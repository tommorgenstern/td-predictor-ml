{"custom_id": "keras#cb30210e6cdc6302d4ca53b24ad48ec0fdca2c0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 16688 | Contributors (this commit): 121 | Commits (past 90d): 27 | Contributors (cumulative): 121 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2011,7 +2011,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                          'information of where went wrong, or file a '\n                          'issue/bug to `tf.keras`.')\n       callbacks.on_predict_end()\n-    all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, potentially_ragged_concat, outputs)\n+    all_outputs = tf.__internal__.nest.map_structure_up_to(\n+      batch_outputs, potentially_ragged_concat, outputs\n+    )\n \n     # If originally PSS strategy was used, then replace it back since predict\n     # is running under `OneDeviceStrategy` after the swap and once its done\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e2736905b85892ae8d38a0563a504c1d6de3d33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 7 | Files Changed: 7 | Hunks: 14 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 282 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -21,6 +21,7 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n+# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.Adadelta', v1=[])\n class Adadelta(optimizer.Optimizer):\n@@ -39,7 +40,7 @@ class Adadelta(optimizer.Optimizer):\n   don't have to set an initial learning rate. In this version, the initial\n   learning rate can be set, as in most other Keras optimizers.\n \n-  Attributes:\n+  Args:\n     learning_rate: Initial value for the learning rate:\n       either a floating point value,\n       or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n\n@@ -22,6 +22,7 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n+# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.Adagrad', v1=[])\n class Adagrad(optimizer.Optimizer):\n@@ -32,7 +33,7 @@ class Adagrad(optimizer.Optimizer):\n   updated during training. The more updates a parameter receives,\n   the smaller the updates.\n \n-  Attributes:\n+  Args:\n     learning_rate: Initial value for the learning rate:\n       either a floating point value,\n       or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n\n@@ -21,6 +21,7 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n+# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.Adam', v1=[])\n class Adam(optimizer.Optimizer):\n@@ -36,7 +37,7 @@ class Adam(optimizer.Optimizer):\n   gradients, and is well suited for problems that are large in terms of\n   data/parameters*\".\n \n-  Attributes:\n+  Args:\n     learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n       `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n       that takes no arguments and returns the actual value to use. The\n\n@@ -21,6 +21,7 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n+# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.AdamW', v1=[])\n class AdamW(optimizer.Optimizer):\n@@ -39,7 +40,7 @@ class AdamW(optimizer.Optimizer):\n   gradients, and is well suited for problems that are large in terms of\n   data/parameters*\".\n \n-  Attributes:\n+  Args:\n     learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n       `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n       that takes no arguments and returns the actual value to use. The\n\n@@ -518,6 +518,7 @@ class _BaseOptimizer(tf.Module):\n     return cls(**config)\n \n \n+# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.experimental.Optimizer\", v1=[])\n class Optimizer(_BaseOptimizer):\n   \"\"\"Abstract optimizer base class.\n@@ -525,7 +526,7 @@ class Optimizer(_BaseOptimizer):\n   This class supports distributed training. If you want to implement your own\n   optimizer, please subclass this class instead of _BaseOptimizer.\n \n-  Attributes:\n+  Args:\n     name: string. The name to use for momentum accumulator weights created by\n       the optimizer.\n     clipnorm: float. If set, the gradient of each weight is individually\n\n@@ -19,6 +19,7 @@ from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n \n \n+# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n class RMSprop(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the RMSprop algorithm.\n@@ -33,7 +34,7 @@ class RMSprop(optimizer.Optimizer):\n   The centered version additionally maintains a moving average of the\n   gradients, and uses that average to estimate the variance.\n \n-  Attributes:\n+  Args:\n     learning_rate: Initial value for the learning rate:\n       either a floating point value,\n       or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n\n@@ -21,6 +21,7 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n+# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.SGD', v1=[])\n class SGD(optimizer.Optimizer):\n@@ -46,7 +47,7 @@ class SGD(optimizer.Optimizer):\n   w = w + momentum * velocity - learning_rate * g\n   ```\n \n-  Attributes:\n+  Args:\n     learning_rate: A `Tensor`, floating point value, or a schedule that is a\n       `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n       that takes no arguments and returns the actual value to use. The\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c959a0670a2bcb12dc7a1717ce7416ff1f7cc27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 636 | Lines Deleted: 785 | Files Changed: 27 | Hunks: 304 | Methods Changed: 207 | Complexity Δ (Sum/Max): -6/1 | Churn Δ: 1421 | Churn Cumulative: 41612 | Contributors (this commit): 39 | Commits (past 90d): 82 | Contributors (cumulative): 157 | DMM Complexity: 0.2708333333333333\n\nDIFF:\n@@ -490,7 +490,7 @@ class KerasModelsXLATest(tf.test.TestCase, parameterized.TestCase):\n         self.num_outputs = num_outputs\n \n       def build(self, input_shape):\n-        self.kernel = self.add_variable(\n+        self.kernel = self.add_weight(\n             \"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n \n       @tf.function(jit_compile=True)\n\n@@ -267,53 +267,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n   \"\"\"\n \n-  # See tf.Module for the usage of this property.\n-  # The key for _obj_reference_counts_dict is a Trackable, which could be a\n-  # variable or layer etc. tf.Module._flatten will fail to flatten the key\n-  # since it is trying to convert Trackable to a string. This attribute can be\n-  # ignored even after the fix of nest lib, since the trackable object should\n-  # already been available as individual attributes. _obj_reference_counts_dict\n-  # just contains a copy of them.\n-  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(\n-      ('_obj_reference_counts_dict',),\n-      tf.Module._TF_MODULE_IGNORED_PROPERTIES\n-  ))\n-\n-  # When loading from a SavedModel, Layers typically can be revived into a\n-  # generic Layer wrapper. Sometimes, however, layers may implement methods\n-  # that go beyond this wrapper, as in the case of PreprocessingLayers'\n-  # `adapt` method. When this is the case, layer implementers can override\n-  # must_restore_from_config to return True; layers with this property must\n-  # be restored into their actual objects (and will fail if the object is\n-  # not available to the restoration code).\n-  _must_restore_from_config = False\n-\n-  def _get_cell_name(self):\n-    canonical_name = get_canonical_name_for_symbol(\n-        self.__class__, api_name='keras', add_prefix_to_v1_names=True)\n-    if canonical_name is not None:\n-      return 'tf.{}'.format(canonical_name)\n-    return self.__class__.__module__ + '.' + self.__class__.__name__\n-\n-  def _instrument_layer_creation(self):\n-    self._instrumented_keras_api = False\n-    self._instrumented_keras_layer_class = False\n-    self._instrumented_keras_model_class = False\n-    if not getattr(self, '_disable_keras_instrumentation', False):\n-      keras_api_gauge.get_cell('layer').set(True)\n-      self._instrumented_keras_api = True\n-      if getattr(self, '_is_model_for_instrumentation', False):\n-        keras_models_gauge.get_cell(self._get_cell_name()).set(True)\n-        self._instrumented_keras_model_class = True\n-      else:\n-        keras_layers_gauge.get_cell(self._get_cell_name()).set(True)\n-        self._instrumented_keras_layer_class = True\n-    else:\n-      # This is a legacy layer that has disabled instrumentation\n-      # as a native keras object. We still instrument this as\n-      # legacy usage.\n-      keras_api_gauge.get_cell('legacy_layer').set(True)\n-\n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self,\n                trainable=True,\n@@ -531,29 +484,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     \"\"\"\n     return inputs\n \n-  @doc_controls.for_subclass_implementers\n-  def _add_trackable(self, trackable_object, trainable):\n-    \"\"\"Adds a Trackable object to this layer's state.\n-\n-    Args:\n-      trackable_object: The tf.tracking.Trackable object to add.\n-      trainable: Boolean, whether the variable should be part of the layer's\n-        \"trainable_variables\" (e.g. variables, biases) or\n-        \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n-\n-    Returns:\n-      The TrackableWeightHandler used to track this object.\n-    \"\"\"\n-    if isinstance(trackable_object, base_layer_utils.TrackableWeightHandler):\n-      handler = trackable_object\n-    else:\n-      handler = base_layer_utils.TrackableWeightHandler(trackable_object)\n-    if trainable:\n-      self._trainable_weights.append(handler)\n-    else:\n-      self._non_trainable_weights.append(handler)\n-    return handler\n-\n   @doc_controls.for_subclass_implementers\n   def add_weight(self,\n                  name=None,\n@@ -868,7 +798,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         raise TypeError('Only TensorSpec signature types are supported. '\n                         f'Received: {s}.')\n       return s.shape\n-    input_shape = tf.nest.map_structure(check_type_return_shape, input_signature)\n+    input_shape = tf.nest.map_structure(\n+        check_type_return_shape, input_signature)\n     output_shape = self.compute_output_shape(input_shape)\n     dtype = self._compute_dtype\n     if dtype is None:\n@@ -880,82 +811,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         lambda s: tf.TensorSpec(dtype=dtype, shape=s),\n         output_shape)\n \n-  def _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs):\n-    if self.dynamic:\n-      # We will use static shape inference to return symbolic tensors\n-      # matching the specifications of the layer outputs.\n-      # Since `self.dynamic` is True, we will never attempt to\n-      # run the underlying TF graph (which is disconnected).\n-      # TODO(fchollet): consider py_func as an alternative, which\n-      # would enable us to run the underlying graph if needed.\n-      input_signature = tf.nest.map_structure(\n-          lambda x: tf.TensorSpec(shape=x.shape, dtype=x.dtype),\n-          inputs)\n-      output_signature = self.compute_output_signature(input_signature)\n-      return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)\n-    else:\n-      return self._infer_output_signature(inputs, args, kwargs, input_masks)\n-\n-  def _infer_output_signature(self, inputs, args, kwargs, input_masks):\n-    \"\"\"Call the layer on input KerasTensors and returns output KerasTensors.\"\"\"\n-\n-    call_fn = self.call\n-    # Wrapping `call` function in autograph to allow for dynamic control\n-    # flow and control dependencies in call. We are limiting this to\n-    # subclassed layers as autograph is strictly needed only for\n-    # subclassed layers and models.\n-    # tf_convert will respect the value of autograph setting in the\n-    # enclosing tf.function, if any.\n-    if (base_layer_utils.is_subclassed(self) and\n-        not base_layer_utils.from_saved_model(self)):\n-      call_fn = tf.__internal__.autograph.tf_convert(\n-          self.call, tf.__internal__.autograph.control_status_ctx())\n-\n-    call_fn = traceback_utils.inject_argument_info_in_traceback(\n-        call_fn,\n-        object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n-\n-    # We enter a scratch graph and build placeholder inputs inside of it that\n-    # match the input args.\n-    # We then call the layer inside of the scratch graph to identify the\n-    # output signatures, then we build KerasTensors corresponding to those\n-    # outputs.\n-    scratch_graph = tf.__internal__.FuncGraph(str(self.name) + '_scratch_graph')\n-    with scratch_graph.as_default():\n-      inputs = tf.nest.map_structure(\n-          keras_tensor.keras_tensor_to_placeholder, inputs)\n-      args = tf.nest.map_structure(\n-          keras_tensor.keras_tensor_to_placeholder, args)\n-      kwargs = tf.nest.map_structure(\n-          keras_tensor.keras_tensor_to_placeholder, kwargs)\n-      input_masks = tf.nest.map_structure(\n-          keras_tensor.keras_tensor_to_placeholder, input_masks)\n-\n-      with backend.name_scope(self._name_scope()):  # pylint: disable=not-callable\n-        with autocast_variable.enable_auto_cast_variables(\n-            self._compute_dtype_object):\n-          # Build layer if applicable (if the `build` method has been\n-          # overridden).\n-          # TODO(kaftan): do we maybe_build here, or have we already done it?\n-          self._maybe_build(inputs)\n-          inputs = self._maybe_cast_inputs(inputs)\n-          outputs = call_fn(inputs, *args, **kwargs)\n-\n-        self._handle_activity_regularization(inputs, outputs)\n-      self._set_mask_metadata(inputs, outputs, input_masks,\n-                              build_graph=False)\n-      outputs = tf.nest.map_structure(\n-          keras_tensor.keras_tensor_from_tensor, outputs)\n-\n-    self._set_save_spec(inputs, args, kwargs)\n-    if hasattr(self, '_set_inputs') and not self.inputs:\n-      # TODO(kaftan): figure out if we need to do this at all\n-      # Subclassed network: explicitly set metadata normally set by\n-      # a call to self._set_inputs().\n-      self._set_inputs(inputs, outputs)\n-    del scratch_graph\n-    return outputs\n-\n   @generic_utils.default\n   def compute_mask(self, inputs, mask=None):  # pylint: disable=unused-argument\n     \"\"\"Computes an output mask tensor.\n@@ -1104,146 +959,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         return outputs\n \n-  def _functional_construction_call(self, inputs, args, kwargs, input_list):\n-    call_context = base_layer_utils.call_context()\n-\n-    # Accept NumPy and scalar inputs by converting to Tensors.\n-    if any(isinstance(x, (\n-        tf.Tensor, np.ndarray, float, int)) for x in input_list):\n-\n-      def _convert_non_tensor(x):\n-        # Don't call `ops.convert_to_tensor` on all `inputs` because\n-        # `SparseTensors` can't be converted to `Tensor`.\n-        if isinstance(x, (tf.Tensor, np.ndarray, float, int)):\n-          return tf.convert_to_tensor(x)\n-        return x\n-\n-      inputs = tf.nest.map_structure(_convert_non_tensor, inputs)\n-      input_list = tf.nest.flatten(inputs)\n-\n-    # Handle `mask` propagation from previous layer to current layer. Masks can\n-    # be propagated explicitly via the `mask` argument, or implicitly via\n-    # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n-    # explicitly take priority.\n-    mask_arg_passed_by_framework = False\n-    input_masks, mask_is_implicit = self._get_input_masks(\n-        inputs, input_list, args, kwargs)\n-    if self._expects_mask_arg and mask_is_implicit:\n-      kwargs['mask'] = input_masks\n-      mask_arg_passed_by_framework = True\n-\n-    # If `training` argument is None or not explicitly passed,\n-    # propagate `training` value from this layer's calling layer.\n-    training_value = None\n-    training_arg_passed_by_framework = False\n-    # Priority 1: `training` was explicitly passed a non-None value.\n-    if self._call_arg_was_passed('training', args, kwargs):\n-      training_value = self._get_call_arg_value('training', args, kwargs)\n-      if not self._expects_training_arg:\n-        kwargs.pop('training')\n-\n-    if training_value is None:\n-      # Priority 2: `training` was passed to a parent layer.\n-      if call_context.training is not None:\n-        training_value = call_context.training\n-      # Priority 3: `learning_phase()` has been set.\n-      elif backend.global_learning_phase_is_set():\n-        training_value = backend.learning_phase()\n-        # Force the training_value to be bool type which matches to the contract\n-        # for layer/model call args.\n-        if tf.is_tensor(training_value):\n-          training_value = tf.cast(training_value, tf.bool)\n-        else:\n-          training_value = bool(training_value)\n-      # Priority 4: trace layer with the default training argument specified\n-      # in the `call` signature (or in inference mode if the `call` signature\n-      # specifies no non-None default).\n-      else:\n-        training_value = self._default_training_arg\n-      # In cases (2), (3), (4) the training argument is passed automatically\n-      # by the framework, and will not be hard-coded into the model.\n-      if self._expects_training_arg:\n-        args, kwargs = self._set_call_arg_value('training', training_value,\n-                                                args, kwargs)\n-        training_arg_passed_by_framework = True\n-\n-    with call_context.enter(\n-        layer=self, inputs=inputs, build_graph=True, training=training_value):\n-      # Check input assumptions set after layer building, e.g. input shape.\n-      outputs = self._keras_tensor_symbolic_call(\n-          inputs, input_masks, args, kwargs)\n-\n-      if outputs is None:\n-        raise ValueError('A layer\\'s `call` method should return a '\n-                         'Tensor or a list of Tensors, not None '\n-                         '(layer: ' + self.name + ').')\n-      if training_arg_passed_by_framework:\n-        args, kwargs = self._set_call_arg_value(\n-            'training', None, args, kwargs, pop_kwarg_if_none=True)\n-      if mask_arg_passed_by_framework:\n-        kwargs.pop('mask')\n-      # Node connectivity does not special-case the first argument.\n-      outputs = self._set_connectivity_metadata((inputs,) + args, kwargs,\n-                                                outputs)\n-      return outputs\n-\n-  def _set_training_mode(self, args, kwargs, call_context):\n-    training_mode = None\n-    if self._expects_training_arg:\n-      # (1) `training` was passed to this `Layer.call`.\n-      if self._call_arg_was_passed('training', args, kwargs):\n-        training_mode = self._get_call_arg_value('training', args, kwargs)\n-      # If no `training` arg was passed, or `None` was explicitly passed,\n-      # the framework will make a decision about the training mode is.\n-      if training_mode is None:\n-        call_ctx_training = call_context.training\n-        # (2) `training` mode is inferred from an outer `Layer.call`.\n-        if call_ctx_training is not None:\n-          training_mode = call_ctx_training\n-        # (3) User set `tf.keras.backend.set_learning_phase`.\n-        elif backend.global_learning_phase_is_set():\n-          training_mode = backend.learning_phase()\n-          # Ensure value is a `bool` or `tf.bool`.\n-          if isinstance(training_mode, bool):\n-            pass\n-          elif tf.is_tensor(training_mode):\n-            training_mode = tf.cast(training_mode, tf.bool)\n-          else:\n-            training_mode = bool(training_mode)\n-        # (4) We default to using `call`'s default value for `training`,\n-        # or treating the layer as if it is in inference if no non-None default\n-        # is specified in the `call` signature.\n-        else:\n-          training_mode = self._default_training_arg\n-\n-        # For case (2), (3), (4) `training` arg is passed by framework.\n-        args, kwargs = self._set_call_arg_value('training', training_mode, args,\n-                                                kwargs)\n-    else:\n-      if 'training' in kwargs:\n-        # `training` was passed to this `Layer` but is not needed for\n-        # `Layer.call`. It will set the default mode for inner `Layer.call`s.\n-        training_mode = kwargs.pop('training')\n-      else:\n-        # Grab the current `training` mode from any outer `Layer.call`.\n-        training_mode = call_context.training\n-\n-    return args, kwargs, training_mode\n-\n-  def _autographed_call(self):\n-    # Wrapping `call` function in autograph to allow for dynamic control\n-    # flow and control dependencies in call. We are limiting this to\n-    # subclassed layers as autograph is strictly needed only for\n-    # subclassed layers and models.\n-    # tf_convert will respect the value of autograph setting in the\n-    # enclosing tf.function, if any.\n-    if (base_layer_utils.is_subclassed(self) and\n-        not base_layer_utils.from_saved_model(self)):\n-      return tf.__internal__.autograph.tf_convert(\n-          self.call, tf.__internal__.autograph.control_status_ctx())\n-    else:\n-      return self.call\n-\n   @property\n   def dtype(self):\n     \"\"\"The dtype of the layer weights.\n@@ -1352,7 +1067,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def input_spec(self, value):\n     for v in tf.nest.flatten(value):\n-      if v is not None and not isinstance(v, InputSpec):\n+      if v is not None and not isinstance(v, input_spec.InputSpec):\n         raise TypeError('Layer input_spec must be an instance of InputSpec. '\n                         'Got: {}'.format(v))\n     self._input_spec = value\n@@ -1531,9 +1246,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     Args:\n       losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n         may also be zero-argument callables which create a loss tensor.\n-      **kwargs: Additional keyword arguments for backward compatibility.\n-        Accepted values:\n-          inputs - Deprecated, will be automatically inferred.\n+      **kwargs: Used for backwards compatibility only.\n     \"\"\"\n     kwargs.pop('inputs', None)\n     if kwargs:\n@@ -1594,16 +1307,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # Possible a loss was added in a Layer's `build`.\n         self._losses.append(symbolic_loss)\n \n-  def _clear_losses(self):\n-    \"\"\"Used every step in eager to reset losses.\"\"\"\n-    # Set to thread local directly to avoid Layer.__setattr__ overhead.\n-    if not getattr(self, '_self_tracked_trackables',\n-                   None):  # Fast path for single Layer.\n-      self._thread_local._eager_losses = []\n-    else:\n-      for layer in self._flatten_layers():\n-        layer._thread_local._eager_losses = []\n-\n   @property\n   def metrics(self):\n     \"\"\"List of metrics added using the `add_metric()` API.\n@@ -1752,7 +1455,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       self._graph_network_add_metric(value, aggregation, name)\n \n   @doc_controls.do_not_doc_inheritable\n-  def add_update(self, updates, inputs=None):\n+  def add_update(self, updates):\n     \"\"\"Add update op(s), potentially dependent on layer inputs.\n \n     Weight updates (for instance, the updates of the moving mean and variance\n@@ -1771,12 +1474,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         that returns an update op. A zero-arg callable should be passed in\n         order to disable running the updates by setting `trainable=False`\n         on this Layer, when executing in Eager mode.\n-      inputs: Deprecated, will be automatically inferred.\n     \"\"\"\n-    if inputs is not None:\n-      tf_logging.warning(\n-          '`add_update` `inputs` kwarg has been deprecated. You no longer need '\n-          'to pass a value to `inputs` as it is being automatically inferred.')\n     call_context = base_layer_utils.call_context()\n     # No need to run updates during Functional API construction.\n     if call_context.in_keras_graph:\n@@ -1931,44 +1629,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     \"\"\"\n     pass\n \n-  @doc_controls.do_not_generate_docs\n-  def get_updates_for(self, inputs):\n-    \"\"\"Deprecated, do NOT use!\n-\n-    Retrieves updates relevant to a specific set of inputs.\n-\n-    Args:\n-      inputs: Input tensor or list/tuple of input tensors.\n-\n-    Returns:\n-      List of update ops of the layer that depend on `inputs`.\n-    \"\"\"\n-    warnings.warn(\n-        '`layer.get_updates_for` is deprecated and '\n-        'will be removed in a future version. '\n-        'Please use `layer.updates` method instead.',\n-        stacklevel=2)\n-    return self.updates\n-\n-  @doc_controls.do_not_generate_docs\n-  def get_losses_for(self, inputs):\n-    \"\"\"Deprecated, do NOT use!\n-\n-    Retrieves losses relevant to a specific set of inputs.\n-\n-    Args:\n-      inputs: Input tensor or list/tuple of input tensors.\n-\n-    Returns:\n-      List of loss tensors of the layer that depend on `inputs`.\n-    \"\"\"\n-    warnings.warn(\n-        '`layer.get_losses_for` is deprecated and '\n-        'will be removed in a future version. '\n-        'Please use `layer.losses` instead.',\n-        stacklevel=2)\n-    return self.losses\n-\n   @doc_controls.do_not_doc_inheritable\n   def get_input_mask_at(self, node_index):\n     \"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n@@ -2258,53 +1918,57 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                            'Use `get_output_shape_at(node_index)` '\n                            'instead.' % self.name)\n \n+  @property\n+  def dtype_policy(self):\n+    \"\"\"The dtype policy associated with this layer.\n+\n+    This is an instance of a `tf.keras.mixed_precision.Policy`.\n+    \"\"\"\n+    return self._dtype_policy\n+\n+  @property\n+  def compute_dtype(self):\n+    \"\"\"The dtype of the layer's computations.\n+\n+    This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n+    mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n+    the weights.\n+\n+    Layers automatically cast their inputs to the compute dtype, which causes\n+    computations and the output to be in the compute dtype as well. This is done\n+    by the base Layer class in `Layer.__call__`, so you do not have to insert\n+    these casts if implementing your own layer.\n+\n+    Layers often perform certain internal computations in higher precision when\n+    `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n+    will still typically be float16 or bfloat16 in such cases.\n+\n+    Returns:\n+      The layer's compute dtype.\n+    \"\"\"\n+    return self._dtype_policy.compute_dtype\n+\n+  @property\n+  def variable_dtype(self):\n+    \"\"\"Alias of `Layer.dtype`, the dtype of the weights.\"\"\"\n+    return self.dtype\n+\n   @property\n   @doc_controls.do_not_doc_inheritable\n   def inbound_nodes(self):\n-    \"\"\"Deprecated, do NOT use! Only for compatibility with external Keras.\"\"\"\n+    \"\"\"Return Functional API nodes upstream of this layer.\"\"\"\n     return self._inbound_nodes\n \n   @property\n   @doc_controls.do_not_doc_inheritable\n   def outbound_nodes(self):\n-    \"\"\"Deprecated, do NOT use! Only for compatibility with external Keras.\"\"\"\n+    \"\"\"Return Functional API nodes downstream of this layer.\"\"\"\n     return self._outbound_nodes\n \n   ##############################################################################\n   # Methods & attributes below are public aliases of other methods.            #\n   ##############################################################################\n \n-  @doc_controls.do_not_doc_inheritable\n-  def apply(self, inputs, *args, **kwargs):\n-    \"\"\"Deprecated, do NOT use!\n-\n-    This is an alias of `self.__call__`.\n-\n-    Args:\n-      inputs: Input tensor(s).\n-      *args: additional positional arguments to be passed to `self.call`.\n-      **kwargs: additional keyword arguments to be passed to `self.call`.\n-\n-    Returns:\n-      Output tensor(s).\n-    \"\"\"\n-    warnings.warn(\n-        '`layer.apply` is deprecated and '\n-        'will be removed in a future version. '\n-        'Please use `layer.__call__` method instead.',\n-        stacklevel=2)\n-    return self.__call__(inputs, *args, **kwargs)\n-\n-  @doc_controls.do_not_doc_inheritable\n-  def add_variable(self, *args, **kwargs):\n-    \"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n-    warnings.warn(\n-        '`layer.add_variable` is deprecated and '\n-        'will be removed in a future version. '\n-        'Please use `layer.add_weight` method instead.',\n-        stacklevel=2)\n-    return self.add_weight(*args, **kwargs)\n-\n   @property\n   @doc_controls.do_not_generate_docs\n   def variables(self):\n@@ -2330,10 +1994,316 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   def non_trainable_variables(self):\n     return self.non_trainable_weights\n \n+  @doc_controls.do_not_doc_inheritable\n+  def add_variable(self, *args, **kwargs):\n+    \"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n+    warnings.warn(\n+        '`layer.add_variable` is deprecated and '\n+        'will be removed in a future version. '\n+        'Please use the `layer.add_weight()` method instead.',\n+        stacklevel=2)\n+    return self.add_weight(*args, **kwargs)\n+\n   ##############################################################################\n   # Methods & attributes below are all private and only used by the framework. #\n   ##############################################################################\n \n+  # See tf.Module for the usage of this property.\n+  # The key for _obj_reference_counts_dict is a Trackable, which could be a\n+  # variable or layer etc. tf.Module._flatten will fail to flatten the key\n+  # since it is trying to convert Trackable to a string. This attribute can be\n+  # ignored even after the fix of nest lib, since the trackable object should\n+  # already been available as individual attributes. _obj_reference_counts_dict\n+  # just contains a copy of them.\n+  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(\n+      ('_obj_reference_counts_dict',),\n+      tf.Module._TF_MODULE_IGNORED_PROPERTIES\n+  ))\n+\n+  # When loading from a SavedModel, Layers typically can be revived into a\n+  # generic Layer wrapper. Sometimes, however, layers may implement methods\n+  # that go beyond this wrapper, as in the case of PreprocessingLayers'\n+  # `adapt` method. When this is the case, layer implementers can override\n+  # must_restore_from_config to return True; layers with this property must\n+  # be restored into their actual objects (and will fail if the object is\n+  # not available to the restoration code).\n+  _must_restore_from_config = False\n+\n+  def _get_cell_name(self):\n+    canonical_name = get_canonical_name_for_symbol(\n+        self.__class__, api_name='keras', add_prefix_to_v1_names=True)\n+    if canonical_name is not None:\n+      return 'tf.{}'.format(canonical_name)\n+    return self.__class__.__module__ + '.' + self.__class__.__name__\n+\n+  def _instrument_layer_creation(self):\n+    self._instrumented_keras_api = False\n+    self._instrumented_keras_layer_class = False\n+    self._instrumented_keras_model_class = False\n+    if not getattr(self, '_disable_keras_instrumentation', False):\n+      keras_api_gauge.get_cell('layer').set(True)\n+      self._instrumented_keras_api = True\n+      if getattr(self, '_is_model_for_instrumentation', False):\n+        keras_models_gauge.get_cell(self._get_cell_name()).set(True)\n+        self._instrumented_keras_model_class = True\n+      else:\n+        keras_layers_gauge.get_cell(self._get_cell_name()).set(True)\n+        self._instrumented_keras_layer_class = True\n+    else:\n+      # This is a legacy layer that has disabled instrumentation\n+      # as a native keras object. We still instrument this as\n+      # legacy usage.\n+      keras_api_gauge.get_cell('legacy_layer').set(True)\n+\n+  @doc_controls.for_subclass_implementers\n+  def _add_trackable(self, trackable_object, trainable):\n+    \"\"\"Adds a Trackable object to this layer's state.\n+\n+    Args:\n+      trackable_object: The tf.tracking.Trackable object to add.\n+      trainable: Boolean, whether the variable should be part of the layer's\n+        \"trainable_variables\" (e.g. variables, biases) or\n+        \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n+\n+    Returns:\n+      The TrackableWeightHandler used to track this object.\n+    \"\"\"\n+    if isinstance(trackable_object, base_layer_utils.TrackableWeightHandler):\n+      handler = trackable_object\n+    else:\n+      handler = base_layer_utils.TrackableWeightHandler(trackable_object)\n+    if trainable:\n+      self._trainable_weights.append(handler)\n+    else:\n+      self._non_trainable_weights.append(handler)\n+    return handler\n+\n+  def _clear_losses(self):\n+    \"\"\"Used every step in eager to reset losses.\"\"\"\n+    # Set to thread local directly to avoid Layer.__setattr__ overhead.\n+    if not getattr(self, '_self_tracked_trackables',\n+                   None):  # Fast path for single Layer.\n+      self._thread_local._eager_losses = []\n+    else:\n+      for layer in self._flatten_layers():\n+        layer._thread_local._eager_losses = []\n+\n+  def _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs):\n+    if self.dynamic:\n+      # We will use static shape inference to return symbolic tensors\n+      # matching the specifications of the layer outputs.\n+      # Since `self.dynamic` is True, we will never attempt to\n+      # run the underlying TF graph (which is disconnected).\n+      # TODO(fchollet): consider py_func as an alternative, which\n+      # would enable us to run the underlying graph if needed.\n+      input_signature = tf.nest.map_structure(\n+          lambda x: tf.TensorSpec(shape=x.shape, dtype=x.dtype),\n+          inputs)\n+      output_signature = self.compute_output_signature(input_signature)\n+      return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)\n+    else:\n+      return self._infer_output_signature(inputs, args, kwargs, input_masks)\n+\n+  def _infer_output_signature(self, inputs, args, kwargs, input_masks):\n+    \"\"\"Call the layer on input KerasTensors and returns output KerasTensors.\"\"\"\n+\n+    call_fn = self.call\n+    # Wrapping `call` function in autograph to allow for dynamic control\n+    # flow and control dependencies in call. We are limiting this to\n+    # subclassed layers as autograph is strictly needed only for\n+    # subclassed layers and models.\n+    # tf_convert will respect the value of autograph setting in the\n+    # enclosing tf.function, if any.\n+    if (base_layer_utils.is_subclassed(self) and\n+        not base_layer_utils.from_saved_model(self)):\n+      call_fn = tf.__internal__.autograph.tf_convert(\n+          self.call, tf.__internal__.autograph.control_status_ctx())\n+\n+    call_fn = traceback_utils.inject_argument_info_in_traceback(\n+        call_fn,\n+        object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n+\n+    # We enter a scratch graph and build placeholder inputs inside of it that\n+    # match the input args.\n+    # We then call the layer inside of the scratch graph to identify the\n+    # output signatures, then we build KerasTensors corresponding to those\n+    # outputs.\n+    scratch_graph = tf.__internal__.FuncGraph(str(self.name) + '_scratch_graph')\n+    with scratch_graph.as_default():\n+      inputs = tf.nest.map_structure(\n+          keras_tensor.keras_tensor_to_placeholder, inputs)\n+      args = tf.nest.map_structure(\n+          keras_tensor.keras_tensor_to_placeholder, args)\n+      kwargs = tf.nest.map_structure(\n+          keras_tensor.keras_tensor_to_placeholder, kwargs)\n+      input_masks = tf.nest.map_structure(\n+          keras_tensor.keras_tensor_to_placeholder, input_masks)\n+\n+      with backend.name_scope(self._name_scope()):  # pylint: disable=not-callable\n+        with autocast_variable.enable_auto_cast_variables(\n+            self._compute_dtype_object):\n+          # Build layer if applicable (if the `build` method has been\n+          # overridden).\n+          # TODO(kaftan): do we maybe_build here, or have we already done it?\n+          self._maybe_build(inputs)\n+          inputs = self._maybe_cast_inputs(inputs)\n+          outputs = call_fn(inputs, *args, **kwargs)\n+\n+        self._handle_activity_regularization(inputs, outputs)\n+      self._set_mask_metadata(inputs, outputs, input_masks,\n+                              build_graph=False)\n+      outputs = tf.nest.map_structure(\n+          keras_tensor.keras_tensor_from_tensor, outputs)\n+\n+    self._set_save_spec(inputs, args, kwargs)\n+    if hasattr(self, '_set_inputs') and not self.inputs:\n+      # TODO(kaftan): figure out if we need to do this at all\n+      # Subclassed network: explicitly set metadata normally set by\n+      # a call to self._set_inputs().\n+      self._set_inputs(inputs, outputs)\n+    del scratch_graph\n+    return outputs\n+\n+  def _functional_construction_call(self, inputs, args, kwargs, input_list):\n+    call_context = base_layer_utils.call_context()\n+\n+    # Accept NumPy and scalar inputs by converting to Tensors.\n+    if any(isinstance(x, (\n+        tf.Tensor, np.ndarray, float, int)) for x in input_list):\n+\n+      def _convert_non_tensor(x):\n+        # Don't call `ops.convert_to_tensor` on all `inputs` because\n+        # `SparseTensors` can't be converted to `Tensor`.\n+        if isinstance(x, (tf.Tensor, np.ndarray, float, int)):\n+          return tf.convert_to_tensor(x)\n+        return x\n+\n+      inputs = tf.nest.map_structure(_convert_non_tensor, inputs)\n+      input_list = tf.nest.flatten(inputs)\n+\n+    # Handle `mask` propagation from previous layer to current layer. Masks can\n+    # be propagated explicitly via the `mask` argument, or implicitly via\n+    # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n+    # explicitly take priority.\n+    mask_arg_passed_by_framework = False\n+    input_masks, mask_is_implicit = self._get_input_masks(\n+        inputs, input_list, args, kwargs)\n+    if self._expects_mask_arg and mask_is_implicit:\n+      kwargs['mask'] = input_masks\n+      mask_arg_passed_by_framework = True\n+\n+    # If `training` argument is None or not explicitly passed,\n+    # propagate `training` value from this layer's calling layer.\n+    training_value = None\n+    training_arg_passed_by_framework = False\n+    # Priority 1: `training` was explicitly passed a non-None value.\n+    if self._call_arg_was_passed('training', args, kwargs):\n+      training_value = self._get_call_arg_value('training', args, kwargs)\n+      if not self._expects_training_arg:\n+        kwargs.pop('training')\n+\n+    if training_value is None:\n+      # Priority 2: `training` was passed to a parent layer.\n+      if call_context.training is not None:\n+        training_value = call_context.training\n+      # Priority 3: `learning_phase()` has been set.\n+      elif backend.global_learning_phase_is_set():\n+        training_value = backend.learning_phase()\n+        # Force the training_value to be bool type which matches to the contract\n+        # for layer/model call args.\n+        if tf.is_tensor(training_value):\n+          training_value = tf.cast(training_value, tf.bool)\n+        else:\n+          training_value = bool(training_value)\n+      # Priority 4: trace layer with the default training argument specified\n+      # in the `call` signature (or in inference mode if the `call` signature\n+      # specifies no non-None default).\n+      else:\n+        training_value = self._default_training_arg\n+      # In cases (2), (3), (4) the training argument is passed automatically\n+      # by the framework, and will not be hard-coded into the model.\n+      if self._expects_training_arg:\n+        args, kwargs = self._set_call_arg_value('training', training_value,\n+                                                args, kwargs)\n+        training_arg_passed_by_framework = True\n+\n+    with call_context.enter(\n+        layer=self, inputs=inputs, build_graph=True, training=training_value):\n+      # Check input assumptions set after layer building, e.g. input shape.\n+      outputs = self._keras_tensor_symbolic_call(\n+          inputs, input_masks, args, kwargs)\n+\n+      if outputs is None:\n+        raise ValueError('A layer\\'s `call` method should return a '\n+                         'Tensor or a list of Tensors, not None '\n+                         '(layer: ' + self.name + ').')\n+      if training_arg_passed_by_framework:\n+        args, kwargs = self._set_call_arg_value(\n+            'training', None, args, kwargs, pop_kwarg_if_none=True)\n+      if mask_arg_passed_by_framework:\n+        kwargs.pop('mask')\n+      # Node connectivity does not special-case the first argument.\n+      outputs = self._set_connectivity_metadata((inputs,) + args, kwargs,\n+                                                outputs)\n+      return outputs\n+\n+  def _set_training_mode(self, args, kwargs, call_context):\n+    training_mode = None\n+    if self._expects_training_arg:\n+      # (1) `training` was passed to this `Layer.call`.\n+      if self._call_arg_was_passed('training', args, kwargs):\n+        training_mode = self._get_call_arg_value('training', args, kwargs)\n+      # If no `training` arg was passed, or `None` was explicitly passed,\n+      # the framework will make a decision about the training mode is.\n+      if training_mode is None:\n+        call_ctx_training = call_context.training\n+        # (2) `training` mode is inferred from an outer `Layer.call`.\n+        if call_ctx_training is not None:\n+          training_mode = call_ctx_training\n+        # (3) User set `tf.keras.backend.set_learning_phase`.\n+        elif backend.global_learning_phase_is_set():\n+          training_mode = backend.learning_phase()\n+          # Ensure value is a `bool` or `tf.bool`.\n+          if isinstance(training_mode, bool):\n+            pass\n+          elif tf.is_tensor(training_mode):\n+            training_mode = tf.cast(training_mode, tf.bool)\n+          else:\n+            training_mode = bool(training_mode)\n+        # (4) We default to using `call`'s default value for `training`,\n+        # or treating the layer as if it is in inference if no non-None default\n+        # is specified in the `call` signature.\n+        else:\n+          training_mode = self._default_training_arg\n+\n+        # For case (2), (3), (4) `training` arg is passed by framework.\n+        args, kwargs = self._set_call_arg_value('training', training_mode, args,\n+                                                kwargs)\n+    else:\n+      if 'training' in kwargs:\n+        # `training` was passed to this `Layer` but is not needed for\n+        # `Layer.call`. It will set the default mode for inner `Layer.call`s.\n+        training_mode = kwargs.pop('training')\n+      else:\n+        # Grab the current `training` mode from any outer `Layer.call`.\n+        training_mode = call_context.training\n+\n+    return args, kwargs, training_mode\n+\n+  def _autographed_call(self):\n+    # Wrapping `call` function in autograph to allow for dynamic control\n+    # flow and control dependencies in call. We are limiting this to\n+    # subclassed layers as autograph is strictly needed only for\n+    # subclassed layers and models.\n+    # tf_convert will respect the value of autograph setting in the\n+    # enclosing tf.function, if any.\n+    if (base_layer_utils.is_subclassed(self) and\n+        not base_layer_utils.from_saved_model(self)):\n+      return tf.__internal__.autograph.tf_convert(\n+          self.call, tf.__internal__.autograph.control_status_ctx())\n+    else:\n+      return self.call\n+\n   @property\n   def _inbound_nodes(self):\n     return self._inbound_nodes_value\n@@ -2389,46 +2359,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     else:\n       self._compute_dtype_object = None\n \n-  @property\n-  def dtype_policy(self):\n-    \"\"\"The dtype policy associated with this layer.\n-\n-    This is an instance of a `tf.keras.mixed_precision.Policy`.\n-    \"\"\"\n-    return self._dtype_policy\n-\n-  @property\n-  def compute_dtype(self):\n-    \"\"\"The dtype of the layer's computations.\n-\n-    This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n-    mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n-    the weights.\n-\n-    Layers automatically cast their inputs to the compute dtype, which causes\n-    computations and the output to be in the compute dtype as well. This is done\n-    by the base Layer class in `Layer.__call__`, so you do not have to insert\n-    these casts if implementing your own layer.\n-\n-    Layers often perform certain internal computations in higher precision when\n-    `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n-    will still typically be float16 or bfloat16 in such cases.\n-\n-    Returns:\n-      The layer's compute dtype.\n-    \"\"\"\n-    return self._dtype_policy.compute_dtype\n-\n   @property\n   def _compute_dtype(self):\n     \"\"\"Deprecated alias of `compute_dtype`.\"\"\"\n     return self._dtype_policy.compute_dtype\n \n-  @property\n-  def variable_dtype(self):\n-    \"\"\"Alias of `Layer.dtype`, the dtype of the weights.\"\"\"\n-    return self.dtype\n-\n   def _maybe_cast_inputs(self, inputs, input_list=None):\n     \"\"\"Maybe casts the inputs to the compute dtype.\n \n@@ -2857,7 +2792,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # Exclude @property.setters from tracking\n         hasattr(self.__class__, name)):\n       try:\n-        super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(name, value)  # pylint: disable=bad-super-call\n+        super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n+            name, value)  # pylint: disable=bad-super-call\n       except AttributeError:\n         raise AttributeError(\n             ('Can\\'t set the attribute \"{}\", likely because it conflicts with '\n@@ -2976,7 +2912,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                                     None)\n             if subtrackables:\n               deque.extendleft(reversed(subtrackables))\n-        elif isinstance(trackable_obj, tf.__internal__.tracking.TrackableDataStructure):\n+        elif isinstance(trackable_obj,\n+                        tf.__internal__.tracking.TrackableDataStructure):\n           # Data structures are introspected even with `recursive=False`.\n           tracked_values = trackable_obj._values\n           if tracked_values:\n@@ -3454,8 +3391,3 @@ class BaseRandomLayer(Layer):\n     children.update(super()._trackable_children(save_type, **kwargs))\n     children['_random_generator'] = self._random_generator\n     return children\n-\n-\n-# Avoid breaking users who directly import this symbol from this file.\n-# TODO(fchollet): remove this.\n-InputSpec = input_spec.InputSpec  # pylint:disable=invalid-name\n\n@@ -1230,13 +1230,11 @@ class NestedTrackingTest(tf.test.TestCase):\n       inputs = tf.ones((3, 1))\n       _ = layer(inputs)\n       self.assertEqual(len(layer.losses), 3)\n-      self.assertLen(layer.get_losses_for(None), 3)\n     else:\n       inputs = input_layer.Input((1,))\n       _ = layer(inputs)\n       self.assertEqual(len(layer.losses), 3)\n       self.assertEqual(len(layer.updates), 3)\n-      self.assertLen(layer.get_losses_for(None), 3)\n \n   def test_attribute_reassignment(self):\n     l = base_layer.Layer()\n\n@@ -13,15 +13,14 @@\n # limitations under the License.\n # ==============================================================================\n # pylint: disable=protected-access\n+# pylint: disable=g-bad-import-order\n \"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n-import collections\n import functools\n import itertools\n import threading\n-import warnings\n \n import numpy as np\n from keras import backend\n@@ -884,7 +883,7 @@ class Layer(base_layer.Layer):\n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def input_spec(self, value):\n     for v in tf.nest.flatten(value):\n-      if v is not None and not isinstance(v, base_layer.InputSpec):\n+      if v is not None and not isinstance(v, input_spec.InputSpec):\n         raise TypeError('Layer input_spec must be an instance of InputSpec. '\n                         'Got: {}'.format(v))\n     self._input_spec = value\n@@ -993,9 +992,6 @@ class Layer(base_layer.Layer):\n     model.add_loss(lambda: tf.reduce_mean(x.kernel))\n     ```\n \n-    The `get_losses_for` method allows to retrieve the losses relevant to a\n-    specific set of inputs.\n-\n     Args:\n       losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n         may also be zero-argument callables which create a loss tensor.\n@@ -1132,7 +1128,7 @@ class Layer(base_layer.Layer):\n       self._graph_network_add_metric(value, aggregation, name)\n \n   @doc_controls.for_subclass_implementers\n-  def add_update(self, updates, inputs=None):\n+  def add_update(self, updates):\n     \"\"\"Add update op(s), potentially dependent on layer inputs.\n \n     Weight updates (for instance, the updates of the moving mean and variance\n@@ -1154,12 +1150,7 @@ class Layer(base_layer.Layer):\n         that returns an update op. A zero-arg callable should be passed in\n         order to disable running the updates by setting `trainable=False`\n         on this Layer, when executing in Eager mode.\n-      inputs: Deprecated, will be automatically inferred.\n     \"\"\"\n-    if inputs is not None:\n-      tf_logging.warning(\n-          '`add_update` `inputs` kwarg has been deprecated. You no longer need '\n-          'to pass a value to `inputs` as it is being automatically inferred.')\n     call_context = base_layer_utils.call_context()\n \n     if (tf.distribute.has_strategy() and\n@@ -1661,37 +1652,6 @@ class Layer(base_layer.Layer):\n   # Methods & attributes below are public aliases of other methods.            #\n   ##############################################################################\n \n-  @doc_controls.do_not_doc_inheritable\n-  def apply(self, inputs, *args, **kwargs):\n-    \"\"\"Deprecated, do NOT use!\n-\n-    This is an alias of `self.__call__`.\n-\n-    Args:\n-      inputs: Input tensor(s).\n-      *args: additional positional arguments to be passed to `self.call`.\n-      **kwargs: additional keyword arguments to be passed to `self.call`.\n-\n-    Returns:\n-      Output tensor(s).\n-    \"\"\"\n-    warnings.warn(\n-        '`layer.apply` is deprecated and '\n-        'will be removed in a future version. '\n-        'Please use `layer.__call__` method instead.',\n-        stacklevel=2)\n-    return self.__call__(inputs, *args, **kwargs)\n-\n-  @doc_controls.do_not_doc_inheritable\n-  def add_variable(self, *args, **kwargs):\n-    \"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n-    warnings.warn(\n-        '`layer.add_variable` is deprecated and '\n-        'will be removed in a future version. '\n-        'Please use `layer.add_weight` method instead.',\n-        stacklevel=2)\n-    return self.add_weight(*args, **kwargs)\n-\n   @property\n   def variables(self):\n     \"\"\"Returns the list of all layer variables/weights.\n@@ -1968,8 +1928,8 @@ class Layer(base_layer.Layer):\n     if not self._should_compute_mask:\n       return None\n \n-    input_masks = tf.nest.map_structure(lambda t: getattr(t, '_keras_mask', None),\n-                                     inputs)\n+    input_masks = tf.nest.map_structure(\n+        lambda t: getattr(t, '_keras_mask', None), inputs)\n     if generic_utils.is_all_none(input_masks):\n       return None\n     return input_masks\n@@ -2188,7 +2148,8 @@ class Layer(base_layer.Layer):\n         # Exclude @property.setters from tracking\n         hasattr(self.__class__, name)):\n       try:\n-        super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(name, value)  # pylint: disable=bad-super-call\n+        super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n+            name, value)  # pylint: disable=bad-super-call\n       except AttributeError:\n         raise AttributeError(\n             ('Can\\'t set the attribute \"{}\", likely because it conflicts with '\n@@ -2364,33 +2325,3 @@ class Layer(base_layer.Layer):\n     state['_thread_local'] = threading.local()\n     # Bypass Trackable logic as `__dict__` already contains this info.\n     object.__setattr__(self, '__dict__', state)\n-\n-\n-class KerasHistory(\n-    collections.namedtuple('KerasHistory',\n-                           ['layer', 'node_index', 'tensor_index'])):\n-  \"\"\"Tracks the Layer call that created a Tensor, for Keras Graph Networks.\n-\n-  During construction of Keras Graph Networks, this metadata is added to\n-  each Tensor produced as the output of a Layer, starting with an\n-  `InputLayer`. This allows Keras to track how each Tensor was produced, and\n-  this information is later retraced by the `keras.engine.Network` class to\n-  reconstruct the Keras Graph Network.\n-\n-  Attributes:\n-    layer: The Layer that produced the Tensor.\n-    node_index: The specific call to the Layer that produced this Tensor. Layers\n-      can be called multiple times in order to share weights. A new node is\n-      created every time a Tensor is called.\n-    tensor_index: The output index for this Tensor. Always zero if the Layer\n-      that produced this Tensor only has one output. Nested structures of\n-      Tensors are deterministically assigned an index via `nest.flatten`.\n-  \"\"\"\n-  # Added to maintain memory and performance characteristics of `namedtuple`\n-  # while subclassing.\n-  __slots__ = ()\n-\n-\n-# Avoid breaking users who directly import this symbol from this file.\n-# TODO(fchollet): remove this.\n-InputSpec = input_spec.InputSpec  # pylint:disable=invalid-name\n\n@@ -17,7 +17,6 @@\n import warnings\n \n from keras import backend\n-from keras import initializers\n from keras import layers\n from keras import losses\n from keras import models\n@@ -59,22 +58,21 @@ class NetworkConstructionTest(test_combinations.TestCase):\n     class MyLayer(layers.Layer):\n \n       def build(self, input_shape):\n-        self.a = self.add_variable('a',\n+        self.a = self.add_weight('a',\n                                  (1, 1),\n                                  'float32',\n                                  trainable=False)\n-        self.b = self.add_variable('b',\n+        self.b = self.add_weight('b',\n                                  (1, 1),\n                                  'float32',\n                                  trainable=False)\n-        self.add_update(tf.compat.v1.assign_add(self.a, [[1.]],\n-                                             name='unconditional_update'))\n+        self.add_update(tf.compat.v1.assign_add(\n+            self.a, [[1.]], name='unconditional_update'))\n         self.built = True\n \n       def call(self, inputs):\n-        self.add_update(tf.compat.v1.assign_add(self.b, inputs,\n-                                             name='conditional_update'),\n-                        inputs=True)\n+        self.add_update(\n+            tf.compat.v1.assign_add(self.b, inputs, name='conditional_update'))\n         return inputs + 1\n \n     with tf.Graph().as_default():\n@@ -103,7 +101,7 @@ class NetworkConstructionTest(test_combinations.TestCase):\n       network.add_update(tf.compat.v1.assign_add(layer.a, [[1]]))\n       self.assertEqual(len(network.updates), 6)\n \n-      network.add_update(tf.compat.v1.assign_add(layer.b, x4), inputs=True)\n+      network.add_update(tf.compat.v1.assign_add(layer.b, x4))\n       self.assertEqual(len(network.updates), 7)\n \n   @test_combinations.generate(test_combinations.combine(mode=['graph']))\n@@ -726,7 +724,7 @@ class NetworkConstructionTest(test_combinations.TestCase):\n       a = tf.constant([2] * 32)\n       mask = tf.constant([0, 1] * 16)\n       a._keras_mask = mask\n-      b = MaskedLayer().apply(a)\n+      b = MaskedLayer()(a)\n       self.assertTrue(hasattr(b, '_keras_mask'))\n       self.assertAllEqual(\n           self.evaluate(tf.ones_like(mask)),\n@@ -1443,7 +1441,8 @@ class NetworkConstructionTest(test_combinations.TestCase):\n     self.assertEqual('subclassed', model.name)\n     self.assertTrue(model.dynamic)\n     self.assertTrue(model.trainable)\n-    w = model.add_weight('w', [], initializer=tf.compat.v1.constant_initializer(1))\n+    w = model.add_weight(\n+        'w', [], initializer=tf.compat.v1.constant_initializer(1))\n     self.assertEqual(tf.int64, w.dtype)\n \n   def test_disconnected_inputs(self):\n\n@@ -3419,7 +3419,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestLayer(layers_module.Layer):\n \n       def build(self, input_shape):\n-        self.a = self.add_variable(\n+        self.a = self.add_weight(\n             'a', (1, 1), initializer='ones', trainable=False)\n         self.built = True\n \n@@ -3668,7 +3668,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class Bias(layers_module.Layer):\n \n       def build(self, input_shape):\n-        self.bias = self.add_variable('bias', (1,), initializer='zeros')\n+        self.bias = self.add_weight('bias', (1,), initializer='zeros')\n         self.mae = metrics_module.MeanAbsoluteError(name='mae_1')\n \n       def call(self, inputs):\n\n@@ -140,7 +140,7 @@ def no_manual_dependency_tracking_scope(obj):\n   class TestLayer(tf.keras.Layer):\n     def build():\n       with no_manual_dependency_tracking_scope(self):\n-        var = self.add_variable(\"name1\")  # Creates a var and doesn't track it\n+        var = self.add_weight(\"name1\")  # Creates a var and doesn't track it\n       self._track_trackable(\"name2\", var)  # We track variable with name `name2`\n \n   Args:\n\n@@ -267,7 +267,7 @@ class RNNCell(base_layer.Layer):\n \n   def build(self, _):\n     # This tells the parent Layer object that it's OK to call\n-    # self.add_variable() inside the call() method.\n+    # self.add_weight() inside the call() method.\n     pass\n \n   def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n@@ -454,10 +454,10 @@ class BasicRNNCell(LayerRNNCell):\n     _check_supported_dtypes(self.dtype)\n \n     input_depth = inputs_shape[-1]\n-    self._kernel = self.add_variable(\n+    self._kernel = self.add_weight(\n         _WEIGHTS_VARIABLE_NAME,\n         shape=[input_depth + self._num_units, self._num_units])\n-    self._bias = self.add_variable(\n+    self._bias = self.add_weight(\n         _BIAS_VARIABLE_NAME,\n         shape=[self._num_units],\n         initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n@@ -567,21 +567,21 @@ class GRUCell(LayerRNNCell):\n           f\"received shape: {inputs_shape}\")\n     _check_supported_dtypes(self.dtype)\n     input_depth = inputs_shape[-1]\n-    self._gate_kernel = self.add_variable(\n+    self._gate_kernel = self.add_weight(\n         \"gates/%s\" % _WEIGHTS_VARIABLE_NAME,\n         shape=[input_depth + self._num_units, 2 * self._num_units],\n         initializer=self._kernel_initializer)\n-    self._gate_bias = self.add_variable(\n+    self._gate_bias = self.add_weight(\n         \"gates/%s\" % _BIAS_VARIABLE_NAME,\n         shape=[2 * self._num_units],\n         initializer=(self._bias_initializer\n                      if self._bias_initializer is not None else\n                      tf.compat.v1.constant_initializer(1.0, dtype=self.dtype)))\n-    self._candidate_kernel = self.add_variable(\n+    self._candidate_kernel = self.add_weight(\n         \"candidate/%s\" % _WEIGHTS_VARIABLE_NAME,\n         shape=[input_depth + self._num_units, self._num_units],\n         initializer=self._kernel_initializer)\n-    self._candidate_bias = self.add_variable(\n+    self._candidate_bias = self.add_weight(\n         \"candidate/%s\" % _BIAS_VARIABLE_NAME,\n         shape=[self._num_units],\n         initializer=(self._bias_initializer\n@@ -751,10 +751,10 @@ class BasicLSTMCell(LayerRNNCell):\n     _check_supported_dtypes(self.dtype)\n     input_depth = inputs_shape[-1]\n     h_depth = self._num_units\n-    self._kernel = self.add_variable(\n+    self._kernel = self.add_weight(\n         _WEIGHTS_VARIABLE_NAME,\n         shape=[input_depth + h_depth, 4 * self._num_units])\n-    self._bias = self.add_variable(\n+    self._bias = self.add_weight(\n         _BIAS_VARIABLE_NAME,\n         shape=[4 * self._num_units],\n         initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n@@ -981,7 +981,7 @@ class LSTMCell(LayerRNNCell):\n     maybe_partitioner = (\n         tf.compat.v1.fixed_size_partitioner(self._num_unit_shards)\n         if self._num_unit_shards is not None else None)\n-    self._kernel = self.add_variable(\n+    self._kernel = self.add_weight(\n         _WEIGHTS_VARIABLE_NAME,\n         shape=[input_depth + h_depth, 4 * self._num_units],\n         initializer=self._initializer,\n@@ -990,23 +990,23 @@ class LSTMCell(LayerRNNCell):\n       initializer = tf.compat.v1.zeros_initializer\n     else:\n       initializer = tf.compat.v1.zeros_initializer(dtype=self.dtype)\n-    self._bias = self.add_variable(\n+    self._bias = self.add_weight(\n         _BIAS_VARIABLE_NAME,\n         shape=[4 * self._num_units],\n         initializer=initializer)\n     if self._use_peepholes:\n-      self._w_f_diag = self.add_variable(\n+      self._w_f_diag = self.add_weight(\n           \"w_f_diag\", shape=[self._num_units], initializer=self._initializer)\n-      self._w_i_diag = self.add_variable(\n+      self._w_i_diag = self.add_weight(\n           \"w_i_diag\", shape=[self._num_units], initializer=self._initializer)\n-      self._w_o_diag = self.add_variable(\n+      self._w_o_diag = self.add_weight(\n           \"w_o_diag\", shape=[self._num_units], initializer=self._initializer)\n \n     if self._num_proj is not None:\n       maybe_proj_partitioner = (\n           tf.compat.v1.fixed_size_partitioner(self._num_proj_shards)\n           if self._num_proj_shards is not None else None)\n-      self._proj_kernel = self.add_variable(\n+      self._proj_kernel = self.add_weight(\n           \"projection/%s\" % _WEIGHTS_VARIABLE_NAME,\n           shape=[self._num_units, self._num_proj],\n           initializer=self._initializer,\n\n@@ -596,7 +596,7 @@ class RNNTest(test_combinations.TestCase):\n                                        tf.ones_like(cells[0].kernel))\n     # TODO(b/128682878): Remove when RNNCells are __call__'d.\n     with base_layer_utils.call_context().enter(layer, x, True, None):\n-      cells[0].add_update(update_1, inputs=x)\n+      cells[0].add_update(update_1)\n       cells[0].add_update(update_2)\n     self.assertEqual(len(layer.updates), 2)\n \n\n@@ -868,10 +868,10 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n       assert not layer.updates\n       # TODO(b/128684069): Remove when Wrapper sublayers are __call__'d.\n       with base_layer_utils.call_context().enter(layer, x, True, None):\n-        layer.forward_layer.add_update(x_reachable_update, inputs=x)\n-        layer.forward_layer.add_update(1, inputs=None)\n-        layer.backward_layer.add_update(x_reachable_update, inputs=x)\n-        layer.backward_layer.add_update(1, inputs=None)\n+        layer.forward_layer.add_update(x_reachable_update)\n+        layer.forward_layer.add_update(1)\n+        layer.backward_layer.add_update(x_reachable_update)\n+        layer.backward_layer.add_update(1)\n       assert len(layer.updates) == 4\n \n   def test_Bidirectional_losses(self):\n@@ -887,7 +887,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n \n     loss = x * x\n     layer.forward_layer.add_loss(loss)\n-    layer.backward_layer.add_loss(loss, inputs=x)\n+    layer.backward_layer.add_loss(loss)\n     assert len(layer.losses) == 8\n \n   def test_Bidirectional_with_constants(self):\n\n@@ -23,7 +23,7 @@ import tensorflow.compat.v2 as tf\n import copy\n import warnings\n from keras import backend\n-from keras.engine import base_layer\n+from keras.engine import base_layer_v1 as base_layer\n from keras.engine import base_layer_utils\n from keras.legacy_tf_layers import variable_scope_shim\n from keras.mixed_precision import policy\n@@ -32,9 +32,6 @@ from tensorflow.python.ops import variable_scope as vs\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.python.util.tf_export import tf_export\n \n-# Avoid breaking users who directly import this symbol from this file.\n-# TODO(fchollet): remove this.\n-InputSpec = base_layer.InputSpec  # pylint: disable=invalid-name\n \n _KERAS_STYLE_SCOPE = False\n \n@@ -239,6 +236,9 @@ class Layer(base_layer.Layer):\n       self._scope = None\n     self._current_scope = None\n \n+  def apply(self, *args, **kwargs):\n+    return self(*args, **kwargs)\n+\n   # We no longer track graph in tf.layers layers. This property is only kept to\n   # maintain API backward compatibility.\n   @property\n\n@@ -28,8 +28,8 @@ from keras import backend\n from keras.testing_infra import test_combinations\n from keras.engine import base_layer as keras_base_layer\n from keras.engine import input_spec\n-from keras.legacy_tf_layers import base as base_layers\n-from keras.legacy_tf_layers import core as core_layers\n+from keras.legacy_tf_layers import base as base_tf_layers\n+from keras.legacy_tf_layers import core as core_tf_layers\n \n \n class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n@@ -37,7 +37,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testLayerProperties(self):\n-    layer = base_layers.Layer(name='my_layer')\n+    layer = base_tf_layers.Layer(name='my_layer')\n     self.assertEqual(layer.variables, [])\n     self.assertEqual(layer.trainable_variables, [])\n     self.assertEqual(layer.non_trainable_variables, [])\n@@ -46,7 +46,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(layer.updates, [])\n       self.assertEqual(layer.losses, [])\n     self.assertEqual(layer.built, False)\n-    layer = base_layers.Layer(name='my_layer', trainable=False)\n+    layer = base_tf_layers.Layer(name='my_layer', trainable=False)\n     self.assertEqual(layer.trainable, False)\n \n     # Assert that the layer was not instrumented as a Keras layer\n@@ -60,8 +60,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInt64Layer(self):\n-    layer = base_layers.Layer(name='my_layer', dtype='int64')\n-    layer.add_variable('my_var', [2, 2])\n+    layer = base_tf_layers.Layer(name='my_layer', dtype='int64')\n+    layer.add_weight('my_var', [2, 2])\n     self.assertEqual(layer.name, 'my_layer')\n \n   @test_combinations.generate(\n@@ -69,34 +69,34 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def testKerasStyleAddWeight(self):\n     keras_layer = keras_base_layer.Layer(name='keras_layer')\n     with backend.name_scope('foo'):\n-      keras_variable = keras_layer.add_variable(\n+      keras_variable = keras_layer.add_weight(\n           'my_var', [2, 2], initializer=tf.compat.v1.zeros_initializer())\n     self.assertEqual(keras_variable.name, 'foo/my_var:0')\n \n     with backend.name_scope('baz'):\n-      old_style_layer = base_layers.Layer(name='my_layer')\n+      old_style_layer = base_tf_layers.Layer(name='my_layer')\n       # Test basic variable creation.\n-      variable = old_style_layer.add_variable(\n+      variable = old_style_layer.add_weight(\n           'my_var', [2, 2], initializer=tf.compat.v1.zeros_initializer())\n     self.assertEqual(variable.name, 'my_layer/my_var:0')\n \n-    with base_layers.keras_style_scope():\n-      layer = base_layers.Layer(name='my_layer')\n+    with base_tf_layers.keras_style_scope():\n+      layer = base_tf_layers.Layer(name='my_layer')\n     # Assert that the layer was not instrumented as a Keras layer\n     self.assertFalse(layer._instrumented_keras_api)\n     # Test basic variable creation.\n     with backend.name_scope('bar'):\n-      variable = layer.add_variable(\n+      variable = layer.add_weight(\n           'my_var', [2, 2], initializer=tf.compat.v1.zeros_initializer())\n     self.assertEqual(variable.name, 'bar/my_var:0')\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testAddWeight(self):\n-    layer = base_layers.Layer(name='my_layer')\n+    layer = base_tf_layers.Layer(name='my_layer')\n \n     # Test basic variable creation.\n-    variable = layer.add_variable(\n+    variable = layer.add_weight(\n         'my_var', [2, 2], initializer=tf.compat.v1.zeros_initializer())\n     self.assertEqual(variable.name, 'my_layer/my_var:0')\n     self.assertEqual(layer.variables, [variable])\n@@ -109,7 +109,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     # Test non-trainable variable creation.\n     # layer.add_variable should work even outside `build` and `call`.\n-    variable_2 = layer.add_variable(\n+    variable_2 = layer.add_weight(\n         'non_trainable_var', [2, 2],\n         initializer=tf.compat.v1.zeros_initializer(),\n         trainable=False)\n@@ -122,7 +122,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n           len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)), 1)\n \n     regularizer = lambda x: tf.reduce_sum(x) * 1e-3\n-    _ = layer.add_variable(\n+    _ = layer.add_weight(\n         'reg_var', [2, 2],\n         initializer=tf.compat.v1.zeros_initializer(),\n         regularizer=regularizer)\n@@ -131,7 +131,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     added_variable = [False]\n \n     # Test that sync `ON_READ` variables are defaulted to be non-trainable.\n-    variable_3 = layer.add_variable(\n+    variable_3 = layer.add_weight(\n         'sync_on_read_var', [2, 2],\n         initializer=tf.compat.v1.zeros_initializer(),\n         synchronization=tf.VariableSynchronization.ON_READ,\n@@ -141,7 +141,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     @tf.function\n     def function_adds_weight():\n       if not added_variable[0]:\n-        layer.add_variable(\n+        layer.add_weight(\n             'reg_var_from_function', [2, 2],\n             initializer=tf.compat.v1.zeros_initializer(),\n             regularizer=regularizer)\n@@ -151,14 +151,14 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(len(layer.losses), 2)\n \n   def testInvalidTrainableSynchronizationCombination(self):\n-    layer = base_layers.Layer(name='my_layer')\n+    layer = base_tf_layers.Layer(name='my_layer')\n \n     with self.assertRaisesRegex(\n         ValueError, 'Synchronization value can be set to '\n         'VariableSynchronization.ON_READ only for non-trainable variables. '\n         'You have specified trainable=True and '\n         'synchronization=VariableSynchronization.ON_READ.'):\n-      _ = layer.add_variable(\n+      _ = layer.add_weight(\n           'v', [2, 2],\n           initializer=tf.compat.v1.zeros_initializer(),\n           synchronization=tf.VariableSynchronization.ON_READ,\n@@ -173,26 +173,27 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n             tf.compat.v1.get_variable_scope(),\n             partitioner=partitioner,\n             reuse=reuse):\n-          layer = base_layers.Layer(name='my_layer')\n-          _ = layer.add_variable(\n+          layer = base_tf_layers.Layer(name='my_layer')\n+          _ = layer.add_weight(\n               'reg_part_var', [4, 4],\n               initializer=tf.compat.v1.zeros_initializer(),\n               regularizer=regularizer)\n       self.assertEqual(\n-          len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)), 3)\n+          len(tf.compat.v1.get_collection(\n+              tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)), 3)\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testCall(self):\n \n-    class MyLayer(base_layers.Layer):\n+    class MyLayer(base_tf_layers.Layer):\n \n       def call(self, inputs):\n         return tf.square(inputs)\n \n     layer = MyLayer(name='my_layer')\n     inputs = tf.random.uniform((5,), seed=1)\n-    outputs = layer.apply(inputs)\n+    outputs = layer(inputs)\n     self.assertEqual(layer.built, True)\n     if not tf.executing_eagerly():\n       # op is only supported in GRAPH mode\n@@ -202,7 +203,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testDeepCopy(self):\n \n-    class MyLayer(base_layers.Layer):\n+    class MyLayer(base_tf_layers.Layer):\n \n       def call(self, inputs):\n         return tf.square(inputs)\n@@ -210,7 +211,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = MyLayer(name='my_layer')\n     layer._private_tensor = tf.random.uniform(())\n     inputs = tf.random.uniform((5,), seed=1)\n-    outputs = layer.apply(inputs)\n+    outputs = layer(inputs)\n     self.assertEqual(layer.built, True)\n     if not tf.executing_eagerly():\n       # op only supported in GRAPH mode.\n@@ -225,56 +226,56 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testScopeNaming(self):\n \n-    class PrivateLayer(base_layers.Layer):\n+    class PrivateLayer(base_tf_layers.Layer):\n \n       def call(self, inputs):\n         return inputs\n \n     inputs = tf.random.uniform((5,))\n     default_layer = PrivateLayer()\n-    _ = default_layer.apply(inputs)\n+    _ = default_layer(inputs)\n     self.assertEqual(default_layer._scope.name, 'private_layer')\n     default_layer1 = PrivateLayer()\n-    default_layer1.apply(inputs)\n+    default_layer1(inputs)\n     self.assertEqual(default_layer1._scope.name, 'private_layer_1')\n     my_layer = PrivateLayer(name='my_layer')\n-    my_layer.apply(inputs)\n+    my_layer(inputs)\n     self.assertEqual(my_layer._scope.name, 'my_layer')\n     my_layer1 = PrivateLayer(name='my_layer')\n-    my_layer1.apply(inputs)\n+    my_layer1(inputs)\n     self.assertEqual(my_layer1._scope.name, 'my_layer_1')\n     my_layer2 = PrivateLayer(name='my_layer')\n-    my_layer2.apply(inputs)\n+    my_layer2(inputs)\n     self.assertEqual(my_layer2._scope.name, 'my_layer_2')\n     # Name scope shouldn't affect names.\n     with backend.name_scope('some_name_scope'):\n       default_layer2 = PrivateLayer()\n-      default_layer2.apply(inputs)\n+      default_layer2(inputs)\n       self.assertEqual(default_layer2._scope.name, 'private_layer_2')\n       my_layer3 = PrivateLayer(name='my_layer')\n-      my_layer3.apply(inputs)\n+      my_layer3(inputs)\n       self.assertEqual(my_layer3._scope.name, 'my_layer_3')\n       other_layer = PrivateLayer(name='other_layer')\n-      other_layer.apply(inputs)\n+      other_layer(inputs)\n       self.assertEqual(other_layer._scope.name, 'other_layer')\n     # Variable scope gets added to scope names.\n     with tf.compat.v1.variable_scope('var_scope'):\n       default_layer_scoped = PrivateLayer()\n-      default_layer_scoped.apply(inputs)\n+      default_layer_scoped(inputs)\n       self.assertEqual(default_layer_scoped._scope.name,\n                        'var_scope/private_layer')\n       my_layer_scoped = PrivateLayer(name='my_layer')\n-      my_layer_scoped.apply(inputs)\n+      my_layer_scoped(inputs)\n       self.assertEqual(my_layer_scoped._scope.name, 'var_scope/my_layer')\n       my_layer_scoped1 = PrivateLayer(name='my_layer')\n-      my_layer_scoped1.apply(inputs)\n+      my_layer_scoped1(inputs)\n       self.assertEqual(my_layer_scoped1._scope.name, 'var_scope/my_layer_1')\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecNdimCheck(self):\n \n-    class CustomerLayer(base_layers.Layer):\n+    class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomerLayer, self).__init__()\n@@ -285,19 +286,19 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomerLayer()\n     with self.assertRaisesRegex(ValueError, r'expected ndim=2'):\n-      layer.apply(tf.constant([1]))\n+      layer(tf.constant([1]))\n \n     # Note that we re-create the layer since in Eager mode, input spec checks\n     # only happen on first call.\n     # Works\n     layer = CustomerLayer()\n-    layer.apply(tf.constant([[1], [2]]))\n+    layer(tf.constant([[1], [2]]))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecMinNdimCheck(self):\n \n-    class CustomLayer(base_layers.Layer):\n+    class CustomLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomLayer, self).__init__()\n@@ -308,20 +309,20 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomLayer()\n     with self.assertRaisesRegex(ValueError, r'expected min_ndim=2'):\n-      layer.apply(tf.constant([1]))\n+      layer(tf.constant([1]))\n \n     # Works\n     layer = CustomLayer()\n-    layer.apply(tf.constant([[1], [2]]))\n+    layer(tf.constant([[1], [2]]))\n \n     layer = CustomLayer()\n-    layer.apply(tf.constant([[[1], [2]]]))\n+    layer(tf.constant([[[1], [2]]]))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecMaxNdimCheck(self):\n \n-    class CustomerLayer(base_layers.Layer):\n+    class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomerLayer, self).__init__()\n@@ -332,20 +333,20 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomerLayer()\n     with self.assertRaisesRegex(ValueError, r'expected max_ndim=2'):\n-      layer.apply(tf.constant([[[1], [2]]]))\n+      layer(tf.constant([[[1], [2]]]))\n \n     # Works\n     layer = CustomerLayer()\n-    layer.apply(tf.constant([1]))\n+    layer(tf.constant([1]))\n \n     layer = CustomerLayer()\n-    layer.apply(tf.constant([[1], [2]]))\n+    layer(tf.constant([[1], [2]]))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecDtypeCheck(self):\n \n-    class CustomerLayer(base_layers.Layer):\n+    class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomerLayer, self).__init__()\n@@ -356,17 +357,17 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomerLayer()\n     with self.assertRaisesRegex(ValueError, r'expected dtype=float32'):\n-      layer.apply(tf.constant(1, dtype=tf.int32))\n+      layer(tf.constant(1, dtype=tf.int32))\n \n     # Works\n     layer = CustomerLayer()\n-    layer.apply(tf.constant(1.0, dtype=tf.float32))\n+    layer(tf.constant(1.0, dtype=tf.float32))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecAxesCheck(self):\n \n-    class CustomerLayer(base_layers.Layer):\n+    class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomerLayer, self).__init__()\n@@ -377,19 +378,19 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomerLayer()\n     with self.assertRaisesRegex(ValueError, r'expected axis'):\n-      layer.apply(tf.constant([1, 2, 3]))\n+      layer(tf.constant([1, 2, 3]))\n \n     # Works\n     layer = CustomerLayer()\n-    layer.apply(tf.constant([1, 2]))\n+    layer(tf.constant([1, 2]))\n     layer = CustomerLayer()\n-    layer.apply(tf.constant([[1, 2], [3, 4], [5, 6]]))\n+    layer(tf.constant([[1, 2], [3, 4], [5, 6]]))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecShapeCheck(self):\n \n-    class CustomerLayer(base_layers.Layer):\n+    class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomerLayer, self).__init__()\n@@ -400,17 +401,17 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomerLayer()\n     with self.assertRaisesRegex(ValueError, r'expected shape'):\n-      layer.apply(tf.constant([[1, 2]]))\n+      layer(tf.constant([[1, 2]]))\n \n     # Works\n     layer = CustomerLayer()\n-    layer.apply(tf.constant([[1, 2, 3], [4, 5, 6]]))\n+    layer(tf.constant([[1, 2, 3], [4, 5, 6]]))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testNoInputSpec(self):\n \n-    class CustomerLayer(base_layers.Layer):\n+    class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n         super(CustomerLayer, self).__init__()\n@@ -421,21 +422,21 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     layer = CustomerLayer()\n \n-    layer.apply(tf.constant(1))\n+    layer(tf.constant(1))\n \n     # Works\n     if not tf.executing_eagerly():\n-      layer.apply(tf.compat.v1.placeholder('int32'))\n-      layer.apply(tf.compat.v1.placeholder('int32', shape=(2, 3)))\n+      layer(tf.compat.v1.placeholder('int32'))\n+      layer(tf.compat.v1.placeholder('int32', shape=(2, 3)))\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n   def test_count_params(self):\n-    dense = core_layers.Dense(16)\n+    dense = core_tf_layers.Dense(16)\n     dense.build((None, 4))\n     self.assertEqual(dense.count_params(), 16 * 4 + 16)\n \n-    dense = core_layers.Dense(16)\n+    dense = core_tf_layers.Dense(16)\n     with self.assertRaises(ValueError):\n       dense.count_params()\n \n@@ -443,7 +444,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       test_combinations.combine(mode=['graph', 'eager']))\n   def testDictInputOutput(self):\n \n-    class DictLayer(base_layers.Layer):\n+    class DictLayer(base_tf_layers.Layer):\n \n       def call(self, inputs):\n         return {'l' + key: inputs[key] for key in inputs}\n@@ -452,7 +453,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     if tf.executing_eagerly():\n       i1 = tf.constant(3)\n       i2 = tf.constant(4.0)\n-      result = layer.apply({'abel': i1, 'ogits': i2})\n+      result = layer({'abel': i1, 'ogits': i2})\n       self.assertTrue(isinstance(result, dict))\n       self.assertEqual(set(['label', 'logits']), set(result.keys()))\n       self.assertEqual(3, result['label'].numpy())\n@@ -460,25 +461,25 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     else:\n       i1 = tf.compat.v1.placeholder('int32')\n       i2 = tf.compat.v1.placeholder('float32')\n-      result = layer.apply({'abel': i1, 'ogits': i2})\n+      result = layer({'abel': i1, 'ogits': i2})\n       self.assertTrue(isinstance(result, dict))\n       self.assertEqual(set(['label', 'logits']), set(result.keys()))\n \n   def testActivityRegularizer(self):\n     with tf.Graph().as_default():\n       regularizer = tf.reduce_sum\n-      layer = base_layers.Layer(activity_regularizer=regularizer)\n+      layer = base_tf_layers.Layer(activity_regularizer=regularizer)\n       x = tf.compat.v1.placeholder('int32')\n-      layer.apply(x)\n+      layer(x)\n       self.assertEqual(len(layer.get_losses_for(x)), 1)\n \n   def testNameScopeIsConsistentWithVariableScope(self):\n     # Github issue 13429.\n \n-    class MyLayer(base_layers.Layer):\n+    class MyLayer(base_tf_layers.Layer):\n \n       def build(self, input_shape):\n-        self.my_var = self.add_variable('my_var', (), tf.float32)\n+        self.my_var = self.add_weight('my_var', (), tf.float32)\n         self.built = True\n \n       def call(self, inputs):\n@@ -486,7 +487,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n     def _gen_layer(x, name=None):\n       layer = MyLayer(name=name)\n-      out = layer.apply(x)\n+      out = layer(x)\n       return layer, out\n \n     # unnamed layer\n@@ -530,10 +531,10 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(op2.name, 'name_3/my_op:0')\n \n   def testVariablesAreLiftedFromFunctionBuildingGraphs(self):\n-    class MyLayer(base_layers.Layer):\n+    class MyLayer(base_tf_layers.Layer):\n \n       def build(self, input_shape):\n-        self.my_var = self.add_variable('my_var', (), tf.float32)\n+        self.my_var = self.add_weight('my_var', (), tf.float32)\n         self.built = True\n \n       def call(self, inputs):\n@@ -548,21 +549,21 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n         # Create a variable by invoking build through __call__ and assert that\n         # it is both tracked and lifted into the outer graph.\n         inputs = tf.compat.v1.placeholder(tf.float32, (), 'inputs')\n-        layer.apply(inputs)\n+        layer(inputs)\n         self.assertEqual(len(layer.variables), 1)\n         self.assertEqual(len(layer.trainable_variables), 1)\n         self.assertEqual(layer.variables[0].graph, outer_graph)\n \n   def testGetUpdateFor(self):\n \n-    class MyLayer(base_layers.Layer):\n+    class MyLayer(base_tf_layers.Layer):\n \n       def build(self, input_shape):\n-        self.a = self.add_variable('a',\n+        self.a = self.add_weight('a',\n                                  (),\n                                  tf.float32,\n                                  trainable=False)\n-        self.b = self.add_variable('b',\n+        self.b = self.add_weight('b',\n                                  (),\n                                  tf.float32,\n                                  trainable=False)\n@@ -570,15 +571,15 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n         self.built = True\n \n       def call(self, inputs):\n-        self.add_update(tf.compat.v1.assign_add(self.a, inputs, name='a_update'),\n-                        inputs=True)\n+        self.add_update(\n+            tf.compat.v1.assign_add(self.a, inputs, name='a_update'))\n         return inputs + 1\n \n     with tf.Graph().as_default():\n       layer = MyLayer()\n       inputs = tf.compat.v1.placeholder(tf.float32, (), 'inputs')\n       intermediate_inputs = inputs + 1\n-      outputs = layer.apply(intermediate_inputs)\n+      outputs = layer(intermediate_inputs)\n \n       self.assertEqual(len(layer.updates), 2)\n       self.assertEqual(len(layer.get_updates_for(None)), 1)\n@@ -589,7 +590,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       # Call same layer on new input, creating one more conditional update\n       inputs = tf.compat.v1.placeholder(tf.float32, (), 'inputs')\n       intermediate_inputs = inputs + 1\n-      outputs = layer.apply(intermediate_inputs)\n+      outputs = layer(intermediate_inputs)\n \n       self.assertEqual(len(layer.updates), 3)\n       self.assertEqual(len(layer.get_updates_for(None)), 1)\n@@ -600,14 +601,14 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testGetLossesFor(self):\n \n-    class MyLayer(base_layers.Layer):\n+    class MyLayer(base_tf_layers.Layer):\n \n       def build(self, input_shape):\n-        self.a = self.add_variable('a',\n+        self.a = self.add_weight('a',\n                                  (),\n                                  tf.float32,\n                                  trainable=False)\n-        self.b = self.add_variable('b',\n+        self.b = self.add_weight('b',\n                                  (),\n                                  tf.float32,\n                                  trainable=False)\n@@ -622,7 +623,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       layer = MyLayer()\n       inputs = tf.compat.v1.placeholder(tf.float32, (), 'inputs')\n       intermediate_inputs = inputs + 1\n-      outputs = layer.apply(intermediate_inputs)\n+      outputs = layer(intermediate_inputs)\n \n       self.assertEqual(len(layer.losses), 2)\n       self.assertEqual(len(layer.get_losses_for(None)), 1)\n@@ -633,7 +634,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       # Call same layer on new input, creating one more conditional loss\n       inputs = tf.compat.v1.placeholder(tf.float32, (), 'inputs')\n       intermediate_inputs = inputs + 1\n-      outputs = layer.apply(intermediate_inputs)\n+      outputs = layer(intermediate_inputs)\n \n       self.assertEqual(len(layer.losses), 3)\n       self.assertEqual(len(layer.get_losses_for(None)), 1)\n@@ -643,7 +644,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(len(layer.get_losses_for([outputs])), 0)\n \n \n-class IdentityLayer(base_layers.Layer):\n+class IdentityLayer(base_tf_layers.Layer):\n   \"\"\"A layer returns the identity of it's input.\"\"\"\n \n   def call(self, inputs):\n\n@@ -291,7 +291,7 @@ def conv1d(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.Conv2D'])\n@@ -572,7 +572,7 @@ def conv2d(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.Conv3D'])\n@@ -855,7 +855,7 @@ def conv3d(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.SeparableConv1D'])\n@@ -1306,7 +1306,7 @@ def separable_conv1d(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.separable_conv2d'])\n@@ -1473,7 +1473,7 @@ def separable_conv2d(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.Conv2DTranspose'])\n@@ -1733,7 +1733,7 @@ def conv2d_transpose(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.Conv3DTranspose'])\n@@ -1984,7 +1984,7 @@ def conv3d_transpose(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n # Aliases\n\n@@ -54,7 +54,7 @@ class ConvTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.Conv2D(32, [3, 3], activation=tf.nn.relu)\n-    output = layer.apply(images)\n+    output = layer(images)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'conv2d/Relu')\n     self.assertListEqual(output.get_shape().as_list(),\n@@ -73,7 +73,7 @@ class ConvTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.Conv2D(32, 3)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height - 2, width - 2, 32])\n     self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 4, 32])\n@@ -84,7 +84,7 @@ class ConvTest(tf.test.TestCase):\n       height, width = 7, 9\n       images = tf.random.uniform((5, 4, height, width))\n       layer = conv_layers.Conv2D(32, [3, 3], data_format='channels_first')\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, 32, height - 2, width - 2])\n       self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 4, 32])\n@@ -97,20 +97,20 @@ class ConvTest(tf.test.TestCase):\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n           'should be defined. The input_shape received is'):\n-        _ = layer.apply(images)\n+        _ = layer(images)\n \n       images = tf.compat.v1.placeholder(tf.float32, (5, None, 7, 9))\n       layer = conv_layers.Conv2D(32, [3, 3], data_format='channels_first')\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n           'should be defined. The input_shape received is'):\n-        _ = layer.apply(images)\n+        _ = layer(images)\n \n   def testConv2DPaddingSame(self):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 32), seed=1)\n     layer = conv_layers.Conv2D(64, images.get_shape()[1:3], padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, height, width, 64])\n \n   def testCreateConvWithStrides(self):\n@@ -118,19 +118,19 @@ class ConvTest(tf.test.TestCase):\n     # Test strides tuple\n     images = tf.random.uniform((5, height, width, 3), seed=1)\n     layer = conv_layers.Conv2D(32, [3, 3], strides=(2, 2), padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height / 2, width / 2, 32])\n \n     # Test strides integer\n     layer = conv_layers.Conv2D(32, [3, 3], strides=2, padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height / 2, width / 2, 32])\n \n     # Test unequal strides\n     layer = conv_layers.Conv2D(32, [3, 3], strides=(2, 1), padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height / 2, width, 32])\n \n@@ -138,7 +138,7 @@ class ConvTest(tf.test.TestCase):\n     width = 7\n     data = tf.random.uniform((5, width, 4))\n     layer = conv_layers.Conv1D(32, 3, activation=tf.nn.relu)\n-    output = layer.apply(data)\n+    output = layer(data)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'conv1d/Relu')\n     self.assertListEqual(output.get_shape().as_list(), [5, width - 2, 32])\n@@ -156,7 +156,7 @@ class ConvTest(tf.test.TestCase):\n       width = 7\n       data = tf.random.uniform((5, 4, width))\n       layer = conv_layers.Conv1D(32, 3, data_format='channels_first')\n-      output = layer.apply(data)\n+      output = layer(data)\n       self.assertListEqual(output.get_shape().as_list(), [5, 32, width - 2])\n       self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 4, 32])\n       self.assertListEqual(layer.bias.get_shape().as_list(), [32])\n@@ -168,20 +168,20 @@ class ConvTest(tf.test.TestCase):\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n           'should be defined. The input_shape received is'):\n-        _ = layer.apply(data)\n+        _ = layer(data)\n \n       data = tf.compat.v1.placeholder(tf.float32, (5, None, 4))\n       layer = conv_layers.Conv1D(32, 3, data_format='channels_first')\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n           'should be defined. The input_shape received is'):\n-        _ = layer.apply(data)\n+        _ = layer(data)\n \n   def testCreateConv3D(self):\n     depth, height, width = 6, 7, 9\n     volumes = tf.random.uniform((5, depth, height, width, 4))\n     layer = conv_layers.Conv3D(32, [3, 3, 3], activation=tf.nn.relu)\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'conv3d/Relu')\n     self.assertListEqual(output.get_shape().as_list(),\n@@ -196,7 +196,7 @@ class ConvTest(tf.test.TestCase):\n       with self.assertRaisesRegex(\n           ValueError, 'The channel dimension of the inputs '\n           'should be defined. The input_shape received is'):\n-        _ = layer.apply(volumes)\n+        _ = layer(volumes)\n \n   def testConv2DKernelRegularizer(self):\n     with tf.Graph().as_default():\n@@ -204,7 +204,7 @@ class ConvTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.Conv2D(32, [3, 3], kernel_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -217,7 +217,7 @@ class ConvTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.Conv2D(32, [3, 3], bias_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -229,7 +229,7 @@ class ConvTest(tf.test.TestCase):\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.Conv2D(\n         32, [3, 3], activation=tf.nn.relu, use_bias=False)\n-    output = layer.apply(images)\n+    output = layer(images)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'conv2d/Relu')\n     self.assertListEqual(output.get_shape().as_list(),\n@@ -241,14 +241,14 @@ class ConvTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.Conv2D(32, [3, 3], dilation_rate=3)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 1, 3, 32])\n     self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 4, 32])\n     self.assertListEqual(layer.bias.get_shape().as_list(), [32])\n \n     # Test tuple dilation rate\n     layer = conv_layers.Conv2D(32, [3, 3], dilation_rate=(1, 3))\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, height - 2, 3, 32])\n \n   def testFunctionalConv2DReuse(self):\n@@ -370,7 +370,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n     length = 9\n     data = tf.random.uniform((5, length, 4))\n     layer = conv_layers.SeparableConv1D(32, 3, activation=tf.nn.relu)\n-    output = layer.apply(data)\n+    output = layer(data)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'separable_conv1d/Relu')\n     self.assertEqual(output.get_shape().as_list(), [5, length - 2, 32])\n@@ -382,7 +382,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n     length = 9\n     data = tf.random.uniform((5, length, 4))\n     layer = conv_layers.SeparableConv1D(32, 3, depth_multiplier=2)\n-    output = layer.apply(data)\n+    output = layer(data)\n     self.assertEqual(output.get_shape().as_list(), [5, length - 2, 32])\n     self.assertEqual(layer.depthwise_kernel.get_shape().as_list(), [3, 4, 2])\n     self.assertEqual(layer.pointwise_kernel.get_shape().as_list(), [1, 8, 32])\n@@ -393,7 +393,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n       length = 9\n       data = tf.random.uniform((5, 4, length))\n       layer = conv_layers.SeparableConv1D(32, 3, data_format='channels_first')\n-      output = layer.apply(data)\n+      output = layer(data)\n       self.assertEqual(output.get_shape().as_list(), [5, 32, length - 2])\n       self.assertEqual(layer.depthwise_kernel.get_shape().as_list(), [3, 4, 1])\n       self.assertEqual(layer.pointwise_kernel.get_shape().as_list(), [1, 4, 32])\n@@ -404,14 +404,14 @@ class SeparableConv1DTest(tf.test.TestCase):\n     data = tf.random.uniform((5, length, 32), seed=1)\n     layer = conv_layers.SeparableConv1D(\n         64, length, padding='same')\n-    output = layer.apply(data)\n+    output = layer(data)\n     self.assertEqual(output.get_shape().as_list(), [5, length, 64])\n \n   def testCreateSeparableConv1DWithStrides(self):\n     length = 10\n     data = tf.random.uniform((5, length, 3), seed=1)\n     layer = conv_layers.SeparableConv1D(32, 3, strides=2, padding='same')\n-    output = layer.apply(data)\n+    output = layer(data)\n     self.assertEqual(output.get_shape().as_list(), [5, length // 2, 32])\n \n   def testCreateSeparableConv1DWithStridesChannelsFirst(self):\n@@ -421,7 +421,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n       data = tf.random.uniform((5, 3, length), seed=1)\n       layer = conv_layers.SeparableConv1D(\n           32, 3, strides=2, padding='same', data_format=data_format)\n-      output = layer.apply(data)\n+      output = layer(data)\n       self.assertEqual(output.get_shape().as_list(), [5, 32, length // 2])\n \n   def testFunctionalConv1DReuse(self):\n@@ -459,7 +459,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n       data = tf.random.uniform((5, length, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.SeparableConv1D(32, 3, depthwise_regularizer=reg)\n-      layer.apply(data)\n+      layer(data)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -472,7 +472,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n       data = tf.random.uniform((5, length, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.SeparableConv1D(32, 3, pointwise_regularizer=reg)\n-      layer.apply(data)\n+      layer(data)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -485,7 +485,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n       data = tf.random.uniform((5, length, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.SeparableConv1D(32, 3, bias_regularizer=reg)\n-      layer.apply(data)\n+      layer(data)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -498,7 +498,7 @@ class SeparableConv1DTest(tf.test.TestCase):\n       data = tf.random.uniform((5, length, 4))\n       layer = conv_layers.SeparableConv1D(\n           32, 3, activation=tf.nn.relu, use_bias=False)\n-      output = layer.apply(data)\n+      output = layer(data)\n       self.assertEqual(output.op.name, 'separable_conv1d/Relu')\n       self.assertEqual(layer.bias, None)\n \n@@ -547,7 +547,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.SeparableConv2D(32, [3, 3], activation=tf.nn.relu)\n-    output = layer.apply(images)\n+    output = layer(images)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'separable_conv2d/Relu')\n     self.assertListEqual(output.get_shape().as_list(),\n@@ -562,7 +562,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.SeparableConv2D(32, [3, 3], depth_multiplier=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height - 2, width - 2, 32])\n     self.assertListEqual(layer.depthwise_kernel.get_shape().as_list(),\n@@ -575,7 +575,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.SeparableConv2D(32, 3)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height - 2, width - 2, 32])\n     self.assertListEqual(layer.depthwise_kernel.get_shape().as_list(),\n@@ -590,7 +590,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, 4, height, width))\n       layer = conv_layers.SeparableConv2D(\n           32, [3, 3], data_format='channels_first')\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, 32, height - 2, width - 2])\n       self.assertListEqual(layer.depthwise_kernel.get_shape().as_list(),\n@@ -604,7 +604,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n     images = tf.random.uniform((5, height, width, 32), seed=1)\n     layer = conv_layers.SeparableConv2D(\n         64, images.get_shape()[1:3], padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, height, width, 64])\n \n   def testCreateSeparableConvWithStrides(self):\n@@ -614,20 +614,20 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 3), seed=1)\n       layer = conv_layers.SeparableConv2D(\n           32, [3, 3], strides=(2, 2), padding='same')\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, height / 2, width / 2, 32])\n \n       # Test strides integer\n       layer = conv_layers.SeparableConv2D(32, [3, 3], strides=2, padding='same')\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, height / 2, width / 2, 32])\n \n       # Test unequal strides\n       layer = conv_layers.SeparableConv2D(\n           32, [3, 3], strides=(2, 1), padding='same')\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, height / 2, width, 32])\n \n@@ -639,21 +639,21 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, 3, height, width), seed=1)\n       layer = conv_layers.SeparableConv2D(\n           32, [3, 3], strides=(2, 2), padding='same', data_format=data_format)\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, 32, height / 2, width / 2])\n \n       # Test strides integer\n       layer = conv_layers.SeparableConv2D(32, [3, 3], strides=2, padding='same',\n                                           data_format=data_format)\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, 32, height / 2, width / 2])\n \n       # Test unequal strides\n       layer = conv_layers.SeparableConv2D(\n           32, [3, 3], strides=(2, 1), padding='same', data_format=data_format)\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, 32, height / 2, width])\n \n@@ -713,7 +713,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.SeparableConv2D(32, [3, 3], depthwise_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -726,7 +726,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.SeparableConv2D(32, [3, 3], pointwise_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -739,7 +739,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.SeparableConv2D(32, [3, 3], bias_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -752,7 +752,7 @@ class SeparableConv2DTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       layer = conv_layers.SeparableConv2D(\n           32, [3, 3], activation=tf.nn.relu, use_bias=False)\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertEqual(output.op.name, 'separable_conv2d/Relu')\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, height - 2, width - 2, 32])\n@@ -807,7 +807,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.Conv2DTranspose(32, [3, 3], activation=tf.nn.relu)\n-    output = layer.apply(images)\n+    output = layer(images)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'conv2d_transpose/Relu')\n     self.assertListEqual(output.get_shape().as_list(),\n@@ -827,7 +827,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = conv_layers.Conv2DTranspose(32, 3)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height + 2, width + 2, 32])\n     self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 32, 4])\n@@ -838,7 +838,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n     images = tf.random.uniform((5, 4, height, width))\n     layer = conv_layers.Conv2DTranspose(\n         32, [3, 3], data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, 32, height + 2, width + 2])\n     self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 32, 4])\n@@ -849,7 +849,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n     images = tf.random.uniform((5, height, width, 32), seed=1)\n     layer = conv_layers.Conv2DTranspose(\n         64, images.get_shape()[1:3], padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, height, width, 64])\n \n   def testCreateConv2DTransposeWithStrides(self):\n@@ -858,20 +858,20 @@ class Conv2DTransposeTest(tf.test.TestCase):\n     images = tf.random.uniform((5, height, width, 3), seed=1)\n     layer = conv_layers.Conv2DTranspose(\n         32, [3, 3], strides=(2, 2), padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height * 2, width * 2, 32])\n \n     # Test strides integer\n     layer = conv_layers.Conv2DTranspose(32, [3, 3], strides=2, padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height * 2, width * 2, 32])\n \n     # Test unequal strides\n     layer = conv_layers.Conv2DTranspose(\n         32, [3, 3], strides=(2, 1), padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height * 2, width, 32])\n \n@@ -881,7 +881,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.Conv2DTranspose(32, [3, 3], kernel_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -894,7 +894,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.Conv2DTranspose(32, [3, 3], bias_regularizer=reg)\n-      layer.apply(images)\n+      layer(images)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -907,7 +907,7 @@ class Conv2DTransposeTest(tf.test.TestCase):\n       images = tf.random.uniform((5, height, width, 4))\n       layer = conv_layers.Conv2DTranspose(\n           32, [3, 3], activation=tf.nn.relu, use_bias=False)\n-      output = layer.apply(images)\n+      output = layer(images)\n       self.assertEqual(output.op.name, 'conv2d_transpose/Relu')\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, height + 2, width + 2, 32])\n@@ -1004,7 +1004,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n     depth, height, width = 5, 7, 9\n     volumes = tf.random.uniform((5, depth, height, width, 32))\n     layer = conv_layers.Conv3DTranspose(4, [3, 3, 3], activation=tf.nn.relu)\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     if not tf.executing_eagerly():\n       self.assertEqual(output.op.name, 'conv3d_transpose/Relu')\n     self.assertListEqual(output.get_shape().as_list(),\n@@ -1016,7 +1016,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n     depth, height, width = 5, 7, 9\n     volumes = tf.random.uniform((5, depth, height, width, 32))\n     layer = conv_layers.Conv3DTranspose(4, 3)\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, depth + 2, height + 2, width + 2, 4])\n     self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 3, 4, 32])\n@@ -1028,7 +1028,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n       volumes = tf.random.uniform((5, 32, depth, height, width))\n       layer = conv_layers.Conv3DTranspose(\n           4, [3, 3, 3], data_format='channels_first')\n-      output = layer.apply(volumes)\n+      output = layer(volumes)\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, 4, depth + 2, height + 2, width + 2])\n       self.assertListEqual(layer.kernel.get_shape().as_list(), [3, 3, 3, 4, 32])\n@@ -1039,7 +1039,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n     volumes = tf.random.uniform((5, depth, height, width, 64), seed=1)\n     layer = conv_layers.Conv3DTranspose(\n         32, volumes.get_shape()[1:4], padding='same')\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, depth, height, width, 32])\n \n@@ -1049,20 +1049,20 @@ class Conv3DTransposeTest(tf.test.TestCase):\n     volumes = tf.random.uniform((5, depth, height, width, 32), seed=1)\n     layer = conv_layers.Conv3DTranspose(\n         4, [3, 3, 3], strides=(2, 2, 2), padding='same')\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, depth * 2, height * 2, width * 2, 4])\n \n     # Test strides integer.\n     layer = conv_layers.Conv3DTranspose(4, [3, 3, 3], strides=2, padding='same')\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, depth * 2, height * 2, width * 2, 4])\n \n     # Test unequal strides.\n     layer = conv_layers.Conv3DTranspose(\n         4, [3, 3, 3], strides=(2, 1, 1), padding='same')\n-    output = layer.apply(volumes)\n+    output = layer(volumes)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, depth * 2, height, width, 4])\n \n@@ -1072,7 +1072,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n       volumes = tf.random.uniform((5, depth, height, width, 32))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.Conv3DTranspose(4, [3, 3, 3], kernel_regularizer=reg)\n-      layer.apply(volumes)\n+      layer(volumes)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -1085,7 +1085,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n       volumes = tf.random.uniform((5, depth, height, width, 32))\n       reg = lambda x: 0.1 * tf.reduce_sum(x)\n       layer = conv_layers.Conv3DTranspose(4, [3, 3, 3], bias_regularizer=reg)\n-      layer.apply(volumes)\n+      layer(volumes)\n       loss_keys = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n       self.assertEqual(len(loss_keys), 1)\n       self.evaluate([v.initializer for v in layer.variables])\n@@ -1098,7 +1098,7 @@ class Conv3DTransposeTest(tf.test.TestCase):\n       volumes = tf.random.uniform((5, depth, height, width, 32))\n       layer = conv_layers.Conv3DTranspose(\n           4, [3, 3, 3], activation=tf.nn.relu, use_bias=False)\n-      output = layer.apply(volumes)\n+      output = layer(volumes)\n       self.assertEqual(output.op.name, 'conv3d_transpose/Relu')\n       self.assertListEqual(output.get_shape().as_list(),\n                            [5, depth + 2, height + 2, width + 2, 4])\n\n@@ -258,7 +258,7 @@ def dense(\n                 name=name,\n                 _scope=name,\n                 _reuse=reuse)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.Dropout'])\n@@ -410,7 +410,7 @@ def dropout(inputs,\n       'Please use `tf.keras.layers.Dropout` instead.',\n       stacklevel=2)\n   layer = Dropout(rate, noise_shape=noise_shape, seed=seed, name=name)\n-  return layer.apply(inputs, training=training)\n+  return layer(inputs, training=training)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.Flatten'])\n@@ -538,7 +538,7 @@ def flatten(inputs, name=None, data_format='channels_last'):\n       'Please use `tf.keras.layers.Flatten` instead.',\n       stacklevel=2)\n   layer = Flatten(name=name, data_format=data_format)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n # Aliases\n\n@@ -46,10 +46,10 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n \n     # Test auto-naming\n     dense = core_layers.Dense(2, activation=tf.nn.relu)\n-    dense.apply(tf.random.uniform((5, 2)))\n+    dense(tf.random.uniform((5, 2)))\n     self.assertEqual(dense.name, 'dense_1')\n     dense = core_layers.Dense(2, activation=tf.nn.relu)\n-    dense.apply(tf.random.uniform((5, 2)))\n+    dense(tf.random.uniform((5, 2)))\n     self.assertEqual(dense.name, 'dense_2')\n \n   @tf_test_utils.run_deprecated_v1\n@@ -128,7 +128,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n   def testOutputShape(self):\n     dense = core_layers.Dense(7, activation=tf.nn.relu, name='my_dense')\n     inputs = tf.random.uniform((5, 3), seed=1)\n-    outputs = dense.apply(inputs)\n+    outputs = dense(inputs)\n     self.assertEqual(outputs.get_shape().as_list(), [5, 7])\n \n     inputs = tf.random.uniform((5, 2, 3), seed=1)\n@@ -136,7 +136,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(outputs.get_shape().as_list(), [5, 2, 7])\n \n     inputs = tf.random.uniform((1, 2, 4, 3), seed=1)\n-    outputs = dense.apply(inputs)\n+    outputs = dense(inputs)\n     self.assertEqual(outputs.get_shape().as_list(), [1, 2, 4, 7])\n \n   @tf_test_utils.run_deprecated_v1\n@@ -372,7 +372,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n     dp = core_layers.Dropout(0.5, name='dropout')\n     self.assertEqual(dp.rate, 0.5)\n     self.assertEqual(dp.noise_shape, None)\n-    dp.apply(tf.ones(()))\n+    dp(tf.ones(()))\n     self.assertEqual(dp.name, 'dropout')\n \n   @test_combinations.generate(\n@@ -380,12 +380,12 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n   def testBooleanLearningPhase(self):\n     dp = core_layers.Dropout(0.5)\n     inputs = tf.ones((5, 3))\n-    dropped = dp.apply(inputs, training=True)\n+    dropped = dp(inputs, training=True)\n     if not tf.executing_eagerly():\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n     np_output = self.evaluate(dropped)\n     self.assertAlmostEqual(0., np_output.min())\n-    dropped = dp.apply(inputs, training=False)\n+    dropped = dp(inputs, training=False)\n     np_output = self.evaluate(dropped)\n     self.assertAllClose(np.ones((5, 3)), np_output)\n \n@@ -395,7 +395,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n       dp = core_layers.Dropout(0.5, seed=1)\n       inputs = tf.ones((5, 5))\n       training = tf.compat.v1.placeholder(dtype='bool')\n-      dropped = dp.apply(inputs, training=training)\n+      dropped = dp(inputs, training=training)\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       np_output = sess.run(dropped, feed_dict={training: True})\n       self.assertAlmostEqual(0., np_output.min())\n@@ -408,7 +408,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n     inputs = tf.ones((5, 3, 2))\n     noise_shape = [None, 1, None]\n     dp = core_layers.Dropout(0.5, noise_shape=noise_shape, seed=1)\n-    dropped = dp.apply(inputs, training=True)\n+    dropped = dp(inputs, training=True)\n     self.evaluate(tf.compat.v1.global_variables_initializer())\n     np_output = self.evaluate(dropped)\n     self.assertAlmostEqual(0., np_output.min())\n@@ -418,7 +418,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n     inputs = tf.ones((5, 3, 2))\n     noise_shape = [5, 1, 2]\n     dp = core_layers.Dropout(0.5, noise_shape=noise_shape, seed=1)\n-    dropped = dp.apply(inputs, training=True)\n+    dropped = dp(inputs, training=True)\n     self.evaluate(tf.compat.v1.global_variables_initializer())\n     np_output = self.evaluate(dropped)\n     self.assertAlmostEqual(0., np_output.min())\n@@ -442,7 +442,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n       rate = tf.compat.v1.placeholder(dtype='float32', name='rate')\n       dp = core_layers.Dropout(rate, name='dropout')\n       inputs = tf.ones((5, 5))\n-      dropped = dp.apply(inputs, training=True)\n+      dropped = dp(inputs, training=True)\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       np_output = sess.run(dropped, feed_dict={rate: 0.5})\n       self.assertAlmostEqual(0., np_output.min())\n\n@@ -460,7 +460,7 @@ def batch_normalization(inputs,\n       name=name,\n       _reuse=reuse,\n       _scope=name)\n-  return layer.apply(inputs, training=training)\n+  return layer(inputs, training=training)\n \n \n # Aliases\n\n@@ -44,7 +44,7 @@ class BNTest(tf.test.TestCase):\n     bn_layer = normalization_layers.BatchNormalization(fused=fused)\n     bn_layer._bessels_correction_test_only = False\n     training = not freeze_mode\n-    bn = bn_layer.apply(conv, training=training)\n+    bn = bn_layer(conv, training=training)\n     loss = tf.reduce_sum(tf.abs(bn))\n     optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)\n     if not freeze_mode:\n@@ -254,7 +254,7 @@ class BNTest(tf.test.TestCase):\n     bn = normalization_layers.BatchNormalization(axis=1)\n     inputs = tf.random.uniform((5, 4, 3), seed=1)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     # Verify shape.\n     self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])\n@@ -281,7 +281,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.random.uniform(\n         (5, 4, 3, 3), seed=1, dtype=tf.float16)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     # Verify shape.\n     self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3, 3])\n@@ -311,7 +311,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -354,7 +354,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -396,7 +396,7 @@ class BNTest(tf.test.TestCase):\n       inputs = tf.Variable(\n           np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n       training = tf.compat.v1.placeholder(dtype='bool')\n-      outputs = bn.apply(inputs, training=training)\n+      outputs = bn(inputs, training=training)\n \n       with self.session() as sess:\n         # Test training with placeholder learning phase.\n@@ -437,7 +437,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -478,7 +478,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -519,7 +519,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -561,7 +561,7 @@ class BNTest(tf.test.TestCase):\n       inputs = tf.Variable(\n           np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n       training = tf.compat.v1.placeholder(dtype='bool')\n-      outputs = bn.apply(inputs, training=training)\n+      outputs = bn(inputs, training=training)\n \n       with self.cached_session() as sess:\n         # Test training with placeholder learning phase.\n@@ -602,7 +602,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -643,8 +643,8 @@ class BNTest(tf.test.TestCase):\n         axis=-1, epsilon=epsilon, momentum=0.9)\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)\n-    outputs_training = bn.apply(inputs, training=True)\n-    outputs_infer = bn.apply(inputs, training=False)\n+    outputs_training = bn(inputs, training=True)\n+    outputs_infer = bn(inputs, training=False)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -811,7 +811,7 @@ class BNTest(tf.test.TestCase):\n     bn = normalization_layers.BatchNormalization(axis=1, center=False)\n     inputs = tf.random.uniform((5, 4, 3), seed=1)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     # Verify shape.\n     self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])\n@@ -826,7 +826,7 @@ class BNTest(tf.test.TestCase):\n     bn = normalization_layers.BatchNormalization(axis=1, scale=False)\n     inputs = tf.random.uniform((5, 4, 3), seed=1)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     # Verify shape.\n     self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])\n@@ -842,13 +842,13 @@ class BNTest(tf.test.TestCase):\n     bn = normalization_layers.BatchNormalization(axis=1, beta_regularizer=reg)\n     inputs = tf.random.uniform((5, 4, 3), seed=1)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    _ = bn.apply(inputs, training=training)\n+    _ = bn(inputs, training=training)\n     self.assertEqual(len(bn.losses), 1)\n \n     bn = normalization_layers.BatchNormalization(axis=1, gamma_regularizer=reg)\n     inputs = tf.random.uniform((5, 4, 3), seed=1)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    _ = bn.apply(inputs, training=training)\n+    _ = bn(inputs, training=training)\n     self.assertEqual(len(bn.losses), 1)\n \n   def testConstraints(self):\n@@ -883,7 +883,7 @@ class BNTest(tf.test.TestCase):\n         renorm_clipping={'rmax': rmax, 'rmin': rmin, 'dmax': dmax},\n         renorm_momentum=renorm_momentum)\n     training = tf.compat.v1.placeholder(tf.bool)\n-    yt = bn.apply(xt, training=training)\n+    yt = bn(xt, training=training)\n \n     moving_mean = 0.\n     moving_stddev = 1.\n@@ -934,7 +934,7 @@ class BNTest(tf.test.TestCase):\n         renorm_clipping=None,\n         renorm_momentum=momentum)\n     training = tf.compat.v1.placeholder(tf.bool)\n-    yt = bn.apply(xt, training=training)\n+    yt = bn(xt, training=training)\n     moving_mean = 0.\n     moving_stddev = 1.\n     renorm_mean = 0.\n@@ -989,7 +989,7 @@ class BNTest(tf.test.TestCase):\n         momentum=momentum,\n         adjustment=lambda _: (adjust_scale, adjust_bias))\n     training = tf.compat.v1.placeholder(tf.bool)\n-    yt = bn.apply(xt, training=training)\n+    yt = bn(xt, training=training)\n \n     moving_mean = 0.\n     moving_variance = 1.\n@@ -1040,7 +1040,7 @@ class BNTest(tf.test.TestCase):\n         renorm_momentum=renorm_momentum,\n         adjustment=lambda _: (adjust_scale, adjust_bias))\n     training = tf.compat.v1.placeholder(tf.bool)\n-    yt = bn.apply(xt, training=training)\n+    yt = bn(xt, training=training)\n \n     moving_mean = 0.\n     moving_stddev = 1.\n@@ -1140,7 +1140,7 @@ class BNTest(tf.test.TestCase):\n         beta_initializer=tf.compat.v1.constant_initializer(beta),\n         gamma_initializer=tf.compat.v1.constant_initializer(gamma),\n         virtual_batch_size=virtual_batch_size)\n-    out = bn.apply(inp, training=is_training)\n+    out = bn(inp, training=is_training)\n     ghost_shape = ([virtual_batch_size,\n                     shape[0] // virtual_batch_size,\n                     shape[1]])\n@@ -1194,7 +1194,7 @@ class BNTest(tf.test.TestCase):\n         beta_initializer=tf.compat.v1.constant_initializer(beta),\n         gamma_initializer=tf.compat.v1.constant_initializer(gamma),\n         virtual_batch_size=virtual_batch_size)\n-    out = bn.apply(inp, training=is_training)\n+    out = bn(inp, training=is_training)\n     ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +\n                    shape[1:])\n \n@@ -1248,7 +1248,7 @@ class BNTest(tf.test.TestCase):\n         gamma_initializer=tf.compat.v1.constant_initializer(gamma),\n         virtual_batch_size=virtual_batch_size,\n         fused=False)      # NCHW is unsupported by CPU fused batch norm\n-    out = bn.apply(inp, training=is_training)\n+    out = bn(inp, training=is_training)\n     ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +\n                    shape[1:])\n \n@@ -1305,7 +1305,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 4, 3)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -1346,7 +1346,7 @@ class BNTest(tf.test.TestCase):\n     inputs = tf.Variable(\n         np.random.random((5, 3, 4, 4, 3)) + 100, dtype=tf.float32)\n     training = tf.compat.v1.placeholder(dtype='bool')\n-    outputs = bn.apply(inputs, training=training)\n+    outputs = bn(inputs, training=training)\n \n     with self.cached_session() as sess:\n       # Test training with placeholder learning phase.\n@@ -1400,7 +1400,7 @@ class BNTest(tf.test.TestCase):\n         gamma_initializer=tf.compat.v1.constant_initializer(gamma),\n         virtual_batch_size=virtual_batch_size,\n         fused=False)\n-    out = bn.apply(inp, training=is_training)\n+    out = bn(inp, training=is_training)\n     ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +\n                    shape[1:])\n \n\n@@ -165,7 +165,7 @@ def average_pooling1d(inputs, pool_size, strides,\n                            padding=padding,\n                            data_format=data_format,\n                            name=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.MaxPooling1D'])\n@@ -307,7 +307,7 @@ def max_pooling1d(inputs, pool_size, strides,\n                        padding=padding,\n                        data_format=data_format,\n                        name=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.AveragePooling2D'])\n@@ -452,7 +452,7 @@ def average_pooling2d(inputs,\n   layer = AveragePooling2D(pool_size=pool_size, strides=strides,\n                            padding=padding, data_format=data_format,\n                            name=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.MaxPooling2D'])\n@@ -597,7 +597,7 @@ def max_pooling2d(inputs,\n   layer = MaxPooling2D(pool_size=pool_size, strides=strides,\n                        padding=padding, data_format=data_format,\n                        name=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.AveragePooling3D'])\n@@ -746,7 +746,7 @@ def average_pooling3d(inputs,\n   layer = AveragePooling3D(pool_size=pool_size, strides=strides,\n                            padding=padding, data_format=data_format,\n                            name=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.MaxPooling3D'])\n@@ -893,7 +893,7 @@ def max_pooling3d(inputs,\n   layer = MaxPooling3D(pool_size=pool_size, strides=strides,\n                        padding=padding, data_format=data_format,\n                        name=name)\n-  return layer.apply(inputs)\n+  return layer(inputs)\n \n # Aliases\n \n\n@@ -54,14 +54,14 @@ class PoolingTest(tf.test.TestCase):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = pooling_layers.MaxPooling2D([2, 2], strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 3, 4, 4])\n \n   def testCreateAveragePooling2D(self):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = pooling_layers.AveragePooling2D([2, 2], strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 3, 4, 4])\n \n   @tf_test_utils.run_deprecated_v1\n@@ -71,7 +71,7 @@ class PoolingTest(tf.test.TestCase):\n     layer = pooling_layers.MaxPooling2D([2, 2],\n                                         strides=1,\n                                         data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 2, 6, 8])\n \n   @tf_test_utils.run_deprecated_v1\n@@ -82,7 +82,7 @@ class PoolingTest(tf.test.TestCase):\n                                             strides=(1, 1),\n                                             padding='valid',\n                                             data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [3, 4, 4, 5])\n \n   @tf_test_utils.run_deprecated_v1\n@@ -94,7 +94,7 @@ class PoolingTest(tf.test.TestCase):\n                                             strides=(1, 1),\n                                             padding='valid',\n                                             data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [None, 4, 4, 5])\n \n   def testCreateMaxPooling1D(self):\n@@ -102,7 +102,7 @@ class PoolingTest(tf.test.TestCase):\n     channels = 3\n     images = tf.random.uniform((5, width, channels))\n     layer = pooling_layers.MaxPooling1D(2, strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, width // 2, channels])\n \n@@ -111,7 +111,7 @@ class PoolingTest(tf.test.TestCase):\n     channels = 3\n     images = tf.random.uniform((5, width, channels))\n     layer = pooling_layers.AveragePooling1D(2, strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, width // 2, channels])\n \n@@ -121,7 +121,7 @@ class PoolingTest(tf.test.TestCase):\n     images = tf.random.uniform((5, channels, width))\n     layer = pooling_layers.MaxPooling1D(\n         2, strides=2, data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, channels, width // 2])\n \n@@ -131,7 +131,7 @@ class PoolingTest(tf.test.TestCase):\n     images = tf.random.uniform((5, channels, width))\n     layer = pooling_layers.AveragePooling1D(\n         2, strides=2, data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, channels, width // 2])\n \n@@ -139,14 +139,14 @@ class PoolingTest(tf.test.TestCase):\n     depth, height, width = 6, 7, 9\n     images = tf.random.uniform((5, depth, height, width, 4))\n     layer = pooling_layers.MaxPooling3D([2, 2, 2], strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 3, 3, 4, 4])\n \n   def testCreateAveragePooling3D(self):\n     depth, height, width = 6, 7, 9\n     images = tf.random.uniform((5, depth, height, width, 4))\n     layer = pooling_layers.AveragePooling3D([2, 2, 2], strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 3, 3, 4, 4])\n \n   def testMaxPooling3DChannelsFirst(self):\n@@ -154,7 +154,7 @@ class PoolingTest(tf.test.TestCase):\n     images = tf.random.uniform((5, 2, depth, height, width))\n     layer = pooling_layers.MaxPooling3D(\n         [2, 2, 2], strides=2, data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 2, 3, 3, 4])\n \n   def testAveragePooling3DChannelsFirst(self):\n@@ -162,14 +162,14 @@ class PoolingTest(tf.test.TestCase):\n     images = tf.random.uniform((5, 2, depth, height, width))\n     layer = pooling_layers.AveragePooling3D(\n         [2, 2, 2], strides=2, data_format='channels_first')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 2, 3, 3, 4])\n \n   def testCreateMaxPooling2DIntegerPoolSize(self):\n     height, width = 7, 9\n     images = tf.random.uniform((5, height, width, 4))\n     layer = pooling_layers.MaxPooling2D(2, strides=2)\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 3, 4, 4])\n \n   def testMaxPooling2DPaddingSame(self):\n@@ -177,7 +177,7 @@ class PoolingTest(tf.test.TestCase):\n     images = tf.random.uniform((5, height, width, 4), seed=1)\n     layer = pooling_layers.MaxPooling2D(\n         images.get_shape()[1:3], strides=2, padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 4, 5, 4])\n \n   def testCreatePooling2DWithStrides(self):\n@@ -185,19 +185,19 @@ class PoolingTest(tf.test.TestCase):\n     # Test strides tuple\n     images = tf.random.uniform((5, height, width, 3), seed=1)\n     layer = pooling_layers.MaxPooling2D([2, 2], strides=(2, 2), padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height / 2, width / 2, 3])\n \n     # Test strides integer\n     layer = pooling_layers.MaxPooling2D([2, 2], strides=2, padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height / 2, width / 2, 3])\n \n     # Test unequal strides\n     layer = pooling_layers.MaxPooling2D([2, 2], strides=(2, 1), padding='same')\n-    output = layer.apply(images)\n+    output = layer(images)\n     self.assertListEqual(output.get_shape().as_list(),\n                          [5, height / 2, width, 3])\n \n\n@@ -120,7 +120,7 @@ class CustomLayerNoConfig(keras.layers.Layer):\n         tf.constant(1.0, shape=input_shape[1:]), name=self.name+'_c')\n \n   def call(self, inputs):\n-    self.add_loss(tf.reduce_sum(inputs), inputs=inputs)\n+    self.add_loss(tf.reduce_sum(inputs))\n     self.add_metric(self.sum_metric(inputs))\n     self.add_metric(inputs, aggregation='mean', name='mean')\n \n\n@@ -73,7 +73,7 @@ class LayerWithLearningPhase(keras.engine.base_layer.Layer):\n class LayerWithLoss(keras.layers.Layer):\n \n   def call(self, inputs):\n-    self.add_loss(tf.reduce_sum(inputs), inputs=inputs)\n+    self.add_loss(tf.reduce_sum(inputs))\n     return inputs * 2\n \n \n@@ -155,12 +155,6 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n     if tf.executing_eagerly():\n       self.assertAllClose(sorted(self.evaluate(model.losses)),\n                           sorted(self.evaluate(loaded.losses)))\n-    else:\n-      self.assertAllClose(self.evaluate(model.get_losses_for(None)),\n-                          self.evaluate(loaded.get_losses_for(None)))\n-      self.assertAllClose(\n-          sorted(self.evaluate(model.get_losses_for(input_arr))),\n-          sorted(self.evaluate(loaded.get_losses_for(input_arr))))\n \n   @test_combinations.run_with_all_model_types\n   def test_model_save_and_load(self):\n@@ -915,7 +909,7 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n     class CustomLayer(keras.layers.Layer):\n \n       def call(self, *input_list):\n-        self.add_loss(input_list[-2] * 2, inputs=True)\n+        self.add_loss(input_list[-2] * 2)\n         return sum(input_list[:-1])  # The test's last input is a non-tensor arg\n \n     class CustomModel(keras.Model):\n@@ -932,16 +926,12 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n     inp = [tf.constant(i, shape=[1, 1], dtype=tf.float32)\n            for i in range(1, 5)]\n     expected = model(*inp)\n-    expected_loss = model.get_losses_for(inp)\n     saved_model_dir = self._save_model_dir()\n     model.save(saved_model_dir, save_format='tf')\n     loaded = keras_load.load(saved_model_dir)\n     actual = loaded(*inp)\n-    actual_loss = loaded.get_losses_for(inp)\n     self.assertAllEqual(self.evaluate(expected),\n                         self.evaluate(actual))\n-    self.assertAllEqual(self.evaluate(expected_loss),\n-                        self.evaluate(actual_loss))\n \n   def test_wrapped_layer_training(self):\n     class Custom(keras.models.Model):\n\n@@ -69,7 +69,7 @@ def use_wrapped_call(layer, call_fn, default_training_value=None,\n       args = args[1:]\n \n     outputs, losses = fn(*args, **kwargs)\n-    layer.add_loss(losses, inputs=True)\n+    layer.add_loss(losses)\n \n     # TODO(kathywu): This is a temporary hack. When a network of layers is\n     # revived from SavedModel, only the top-level layer will have losses. This\n\n@@ -638,7 +638,7 @@ def get_model_from_layers(model_layers,\n class Bias(layers.Layer):\n \n   def build(self, input_shape):\n-    self.bias = self.add_variable('bias', (1,), initializer='zeros')\n+    self.bias = self.add_weight('bias', (1,), initializer='zeros')\n \n   def call(self, inputs):\n     return inputs + self.bias\n\n@@ -329,7 +329,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n     class LayerWithLoss(layers.Layer):\n \n       def call(self, inputs):\n-        self.add_loss(tf.reduce_sum(inputs), inputs=inputs)\n+        self.add_loss(tf.reduce_sum(inputs))\n         return inputs * 2\n \n     shared_layer = LayerWithLoss()\n@@ -346,7 +346,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n     class LayerWithLoss(layers.Layer):\n \n       def call(self, inputs):\n-        self.add_loss(tf.reduce_sum(inputs), inputs=inputs)\n+        self.add_loss(tf.reduce_sum(inputs))\n         return inputs * 2\n \n     class LayerWithNestedLayerWithLoss(layers.Layer):\n\n@@ -142,7 +142,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.embedding_dim = embedding_dim\n \n       def build(self, _):\n-        self.embedding = self.add_variable(\n+        self.embedding = self.add_weight(\n             'embedding_kernel',\n             shape=[self.vocab_size, self.embedding_dim],\n             dtype=np.float32,\n\n@@ -29,7 +29,7 @@ class Bias(layers.Layer):\n   \"\"\"Layer that add a bias to its inputs.\"\"\"\n \n   def build(self, input_shape):\n-    self.bias = self.add_variable('bias', (1,), initializer='zeros')\n+    self.bias = self.add_weight('bias', (1,), initializer='zeros')\n \n   def call(self, inputs):\n     return inputs + self.bias\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
