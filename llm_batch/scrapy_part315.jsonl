{"custom_id": "scrapy#585e4a8aee649f2b439c42d91d857ed1fbdf4fa5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 810 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -330,7 +330,8 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_caching_hostname_resolver_finite_execution(self):\n         with MockServer() as mock_server:\n-            log = self.run_script(\"caching_hostname_resolver.py\", mock_server.http_address)\n+            http_address = mock_server.http_address.replace(\"0.0.0.0\", \"127.0.0.1\")\n+            log = self.run_script(\"caching_hostname_resolver.py\", http_address)\n             self.assertIn(\"Spider closed (finished)\", log)\n             self.assertNotIn(\"ERROR: Error downloading\", log)\n             self.assertNotIn(\"TimeoutError\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7187247c01d3629000b8b460eea26064043c5595", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 599 | Contributors (this commit): 27 | Commits (past 90d): 8 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -33,12 +33,12 @@ install_requires = [\n     'itemadapter>=0.1.0',\n ]\n extras_require = {}\n-\n-if has_environment_marker_platform_impl_support():\n-    extras_require[':platform_python_implementation == \"CPython\"'] = [\n+cpython_dependencies = [\n     'lxml>=3.5.0',\n     'PyDispatcher>=2.0.5',\n ]\n+if has_environment_marker_platform_impl_support():\n+    extras_require[':platform_python_implementation == \"CPython\"'] = cpython_dependencies\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n         # Earlier lxml versions are affected by\n         # https://foss.heptapod.net/pypy/pypy/-/issues/2498,\n@@ -49,7 +49,7 @@ if has_environment_marker_platform_impl_support():\n         'PyPyDispatcher>=2.1.0',\n     ]\n else:\n-    install_requires.append('lxml>=3.5.0')\n+    install_requires.extend(cpython_dependencies)\n \n \n setup(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5872a0fad090c3d3f91f6e03a772265aa50f901", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 2474 | Contributors (this commit): 36 | Commits (past 90d): 13 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -452,7 +452,7 @@ class FeedExporter:\n         crawler = getattr(self, 'crawler', None)\n \n         def build_instance(builder, *preargs):\n-            return build_storage(builder, uri, preargs=preargs)\n+            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)\n \n         if crawler and hasattr(feedcls, 'from_crawler'):\n             instance = build_instance(feedcls.from_crawler, crawler)\n\n@@ -680,9 +680,14 @@ class MySpider(scrapy.Spider):\n         )\n         return []\n \"\"\"\n+        with open(os.path.join(self.cwd, \"example.json\"), \"w\") as f1:\n+            f1.write(\"not empty\")\n         args = ['-O', 'example.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n+        with open(os.path.join(self.cwd, \"example.json\")) as f2:\n+            first_line = f2.readline()\n+        self.assertNotEqual(first_line, \"not empty\")\n \n     def test_output_and_overwrite_output(self):\n         spider_code = \"\"\"\n@@ -813,9 +818,14 @@ class MySpider(scrapy.Spider):\n         )\n         return []\n \"\"\"\n+        with open(os.path.join(self.cwd, \"example.json\"), \"w\") as f1:\n+            f1.write(\"not empty\")\n         args = ['-O', 'example.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n+        with open(os.path.join(self.cwd, \"example.json\")) as f2:\n+            first_line = f2.readline()\n+        self.assertNotEqual(first_line, \"not empty\")\n \n     def test_output_and_overwrite_output(self):\n         spider_code = \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9c3188189cffc965797b1b77fc5dc5cfa06b5cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 85 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,7 +25,12 @@ class HttpCompressionMiddleware:\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n             raise NotConfigured\n-        return cls(crawler.stats)\n+        try:\n+            return cls(stats=crawler.stats)\n+        except TypeError:\n+            result = cls()\n+            result.stats = crawler.stats\n+            return result\n \n     def process_request(self, request, spider):\n         request.headers.setdefault('Accept-Encoding',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7327145bf3d45b83b110a46ff79d2e337e36b520", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 812 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,8 +5,6 @@ import re\n import sys\n from subprocess import Popen, PIPE\n from urllib.parse import urlsplit, urlunsplit\n-from unittest import skipIf\n-\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -57,13 +55,14 @@ def _wrong_credentials(proxy_url):\n     return urlunsplit(bad_auth_proxy)\n \n \n-@skipIf(\"pypy\" in sys.executable,\n-        \"mitmproxy does not support PyPy\")\n-@skipIf(platform.system() == 'Windows' and sys.version_info < (3, 7),\n-        \"mitmproxy does not support Windows when running Python < 3.7\")\n class ProxyConnectTestCase(TestCase):\n \n     def setUp(self):\n+        try:\n+            import mitmproxy\n+        except ImportError:\n+            self.skipTest('mitmproxy is not installed')\n+\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n         self._oldenv = os.environ.copy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e7b756727cbd8c207acce3083ae29c85a6c6d51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 815 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,5 @@\n import json\n import os\n-import platform\n import re\n import sys\n from subprocess import Popen, PIPE\n@@ -59,7 +58,7 @@ class ProxyConnectTestCase(TestCase):\n \n     def setUp(self):\n         try:\n-            import mitmproxy\n+            import mitmproxy  # noqa: F401\n         except ImportError:\n             self.skipTest('mitmproxy is not installed')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#906626cf0befe332cb61b9cdda7108d61afec776", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 226 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,12 @@\n import asyncio\n-from unittest import mock\n+from unittest import mock, SkipTest\n \n from pytest import mark\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n+from twisted.python.versions import Version\n \n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n@@ -211,10 +212,24 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n         self.assertFalse(download_func.called)\n \n \n+@mark.usefixtures('reactor_pytest')\n class MiddlewareUsingCoro(ManagerTestCase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n     def test_asyncdef(self):\n+        import twisted\n+        if (\n+            self.reactor_pytest == 'asyncio'\n+            and twisted.version < Version('twisted', 18, 4, 0)\n+        ):\n+            raise SkipTest(\n+                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n+                'hangs when using AsyncIO and Twisted versions lower than '\n+                '18.4.0'\n+            )\n+\n+        from twisted.python.versions import Version\n+\n         resp = Response('http://example.com/index.html')\n \n         class CoroMiddleware:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b5478ae9d6f8d5e0028fe47a63252679c457364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 64 | Churn Cumulative: 1003 | Contributors (this commit): 20 | Commits (past 90d): 9 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,8 +60,9 @@ def install_reactor(reactor_path, event_loop_path=None):\n             if event_loop_path is not None:\n                 event_loop_class = load_object(event_loop_path)\n                 event_loop = event_loop_class()\n+                asyncio.set_event_loop(event_loop)\n             else:\n-                event_loop = asyncio.new_event_loop()\n+                event_loop = asyncio.get_event_loop()\n             asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n\n@@ -0,0 +1,45 @@\n+import asyncio\n+import sys\n+\n+import scrapy\n+\n+from scrapy.crawler import CrawlerProcess\n+from twisted.internet.defer import Deferred\n+\n+\n+class UppercasePipeline:\n+    async def _open_spider(self, spider):\n+        spider.logger.info(\"async pipeline opened!\")\n+        await asyncio.sleep(0.1)\n+\n+    def open_spider(self, spider):\n+        loop = asyncio.get_event_loop()\n+        return Deferred.fromFuture(loop.create_task(self._open_spider(spider)))\n+\n+    def process_item(self, item, spider):\n+        return {\"url\": item[\"url\"].upper()}\n+\n+\n+class UrlSpider(scrapy.Spider):\n+    name = \"url_spider\"\n+    start_urls = [\"data:,\"]\n+    custom_settings = {\n+        \"ITEM_PIPELINES\": {UppercasePipeline: 100},\n+    }\n+\n+    def parse(self, response):\n+        yield {\"url\": response.url}\n+\n+\n+if __name__ == \"__main__\":\n+    try:\n+        ASYNCIO_EVENT_LOOP = sys.argv[1]\n+    except IndexError:\n+        ASYNCIO_EVENT_LOOP = None\n+\n+    process = CrawlerProcess(settings={\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+        \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n+    })\n+    process.crawl(UrlSpider)\n+    process.start()\n\n@@ -364,6 +364,22 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n+    @mark.skipif(sys.implementation.name == \"pypy\", reason=\"uvloop does not support pypy properly\")\n+    @mark.skipif(platform.system() == \"Windows\", reason=\"uvloop does not support Windows\")\n+    def test_custom_loop_asyncio_deferred_signal(self):\n+        log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+        self.assertIn(\"async pipeline opened!\", log)\n+\n+    def test_default_loop_asyncio_deferred_signal(self):\n+        log = self.run_script(\"asyncio_deferred_signal.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertNotIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+        self.assertIn(\"async pipeline opened!\", log)\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerRunner')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea851b910e580e840653274182548314e15b4312", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 232 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import asyncio\n from unittest import mock, SkipTest\n \n from pytest import mark\n+from twisted import version as twisted_version\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial.unittest import TestCase\n@@ -217,10 +218,9 @@ class MiddlewareUsingCoro(ManagerTestCase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n     def test_asyncdef(self):\n-        import twisted\n         if (\n             self.reactor_pytest == 'asyncio'\n-            and twisted.version < Version('twisted', 18, 4, 0)\n+            and twisted_version < Version('twisted', 18, 4, 0)\n         ):\n             raise SkipTest(\n                 'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n@@ -228,8 +228,6 @@ class MiddlewareUsingCoro(ManagerTestCase):\n                 '18.4.0'\n             )\n \n-        from twisted.python.versions import Version\n-\n         resp = Response('http://example.com/index.html')\n \n         class CoroMiddleware:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fea5a118993855808183b32809c2c304db28c364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 238 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -248,6 +248,12 @@ class MiddlewareUsingCoro(ManagerTestCase):\n \n     @mark.only_asyncio()\n     def test_asyncdef_asyncio(self):\n+        if twisted_version < Version('twisted', 18, 4, 0):\n+            raise SkipTest(\n+                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n+                'hangs when using Twisted versions lower than 18.4.0'\n+            )\n+\n         resp = Response('http://example.com/index.html')\n \n         class CoroMiddleware:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3e53027ec35498dcb931404e02689877da6aeb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 48 | Churn Cumulative: 439 | Contributors (this commit): 14 | Commits (past 90d): 16 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,11 @@\n+import warnings\n import zlib\n \n-from scrapy.utils.gz import gunzip\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.exceptions import NotConfigured\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n+from scrapy.utils.gz import gunzip\n \n \n ACCEPTED_ENCODINGS = [b'gzip', b'deflate']\n@@ -28,6 +30,12 @@ class HttpCompressionMiddleware:\n         try:\n             return cls(stats=crawler.stats)\n         except TypeError:\n+            warnings.warn(\n+                \"HttpCompressionMiddleware subclasses must either modify \"\n+                \"their '__init__' method to support a 'stats' parameter or \"\n+                \"reimplement the 'from_crawler' method.\",\n+                ScrapyDeprecationWarning,\n+            )\n             result = cls()\n             result.stats = crawler.stats\n             return result\n\n@@ -1,12 +1,13 @@\n-from io import BytesIO\n-from unittest import TestCase, SkipTest\n-from os.path import join\n from gzip import GzipFile\n+from io import BytesIO\n+from os.path import join\n+from unittest import TestCase, SkipTest\n+from warnings import catch_warnings\n \n from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.gz import gunzip\n from scrapy.utils.test import get_crawler\n@@ -321,3 +322,30 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(response.body, b'')\n         self.assertStatsEqual('httpcompression/response_count', None)\n         self.assertStatsEqual('httpcompression/response_bytes', None)\n+\n+\n+class HttpCompressionSubclassTest(TestCase):\n+\n+    def test_init_missing_stats(self):\n+        class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):\n+\n+            def __init__(self):\n+                super().__init__()\n+\n+        crawler = get_crawler(Spider)\n+        with catch_warnings(record=True) as caught_warnings:\n+            instance = HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n+        messages = tuple(\n+            str(warning.message) for warning in caught_warnings\n+            if warning.category is ScrapyDeprecationWarning\n+        )\n+        self.assertEqual(\n+            messages,\n+            (\n+                (\n+                    \"HttpCompressionMiddleware subclasses must either modify \"\n+                    \"their '__init__' method to support a 'stats' parameter \"\n+                    \"or reimplement the 'from_crawler' method.\"\n+                ),\n+            )\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3095d39740c4818d2c1c98d392255981ebed2a10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 829 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -373,6 +373,9 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n+    # https://twistedmatrix.com/trac/ticket/9766\n+    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n+            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_default_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1941f607ca54694d4822933eba973665baa1b45b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 33 | Churn Cumulative: 121 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,14 @@\n import asyncio\n+from unittest import SkipTest\n \n+from pydispatch import dispatcher\n from pytest import mark\n from testfixtures import LogCapture\n-from twisted.trial import unittest\n-from twisted.python.failure import Failure\n+from twisted import version as twisted_version\n from twisted.internet import defer, reactor\n-from pydispatch import dispatcher\n+from twisted.python.failure import Failure\n+from twisted.python.versions import Version\n+from twisted.trial import unittest\n \n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.utils.test import get_from_asyncio_queue\n@@ -68,6 +71,7 @@ class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n         return d\n \n \n+@mark.usefixtures('reactor_pytest')\n class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n \n     async def ok_handler(self, arg, handlers_called):\n@@ -76,6 +80,19 @@ class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n         await defer.succeed(42)\n         return \"OK\"\n \n+    def test_send_catch_log(self):\n+        if (\n+            self.reactor_pytest == 'asyncio'\n+            and twisted_version < Version('twisted', 18, 4, 0)\n+        ):\n+            raise SkipTest(\n+                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n+                'fails due to a timeout when using AsyncIO and Twisted '\n+                'versions lower than 18.4.0'\n+            )\n+\n+        return super().test_send_catch_log()\n+\n \n @mark.only_asyncio()\n class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n@@ -86,6 +103,16 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n         await asyncio.sleep(0.2)\n         return await get_from_asyncio_queue(\"OK\")\n \n+    def test_send_catch_log(self):\n+        if (twisted_version < Version('twisted', 18, 4, 0):\n+            raise SkipTest(\n+                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n+                'fails due to a timeout when using Twisted versions lower '\n+                'than 18.4.0'\n+            )\n+\n+        return super().test_send_catch_log()\n+\n \n class SendCatchLogTest2(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee98771fa72aa5e109292af5d734e5c49bae64e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 344 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -334,7 +334,7 @@ class HttpCompressionSubclassTest(TestCase):\n \n         crawler = get_crawler(Spider)\n         with catch_warnings(record=True) as caught_warnings:\n-            instance = HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n+            HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n         messages = tuple(\n             str(warning.message) for warning in caught_warnings\n             if warning.category is ScrapyDeprecationWarning\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b28da433384f67d0e6c7d4d557f5bcf268ae846", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 123 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -104,7 +104,7 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n         return await get_from_asyncio_queue(\"OK\")\n \n     def test_send_catch_log(self):\n-        if (twisted_version < Version('twisted', 18, 4, 0):\n+        if twisted_version < Version('twisted', 18, 4, 0):\n             raise SkipTest(\n                 'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n                 'fails due to a timeout when using Twisted versions lower '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2c4a7f9200a06b227a297b9dd2919fe3ec37dbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1668 | Contributors (this commit): 27 | Commits (past 90d): 12 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,7 +98,7 @@ class Command(ScrapyCommand):\n         print(f\"Created spider {name!r} using template {template_name!r} \",\n               end=('' if spiders_module else '\\n'))\n         if spiders_module:\n-            print(\"in module:\\n  {spiders_module.__name__}.{module}\")\n+            print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n \n     def _find_template(self, template):\n         template_file = join(self.templates_dir, f'{template}.tmpl')\n\n@@ -389,8 +389,9 @@ class GenspiderCommandTest(CommandTest):\n     def test_template(self, tplname='crawl'):\n         args = [f'--template={tplname}'] if tplname else []\n         spname = 'test_spider'\n+        spmodule = f\"{self.project_name}.spiders.{spname}\"\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n-        self.assertIn(f\"Created spider {spname!r} using template {tplname!r} in module\", out)\n+        self.assertIn(f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\", out)\n         self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n         modify_time_before = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e98a76ac455a8c69950104766719cde313bbb74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 53 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,9 @@\n import asyncio\n import sys\n \n-import scrapy\n-\n+from scrapy import Spider\n from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.defer import deferred_from_coro\n from twisted.internet.defer import Deferred\n \n \n@@ -14,13 +14,13 @@ class UppercasePipeline:\n \n     def open_spider(self, spider):\n         loop = asyncio.get_event_loop()\n-        return Deferred.fromFuture(loop.create_task(self._open_spider(spider)))\n+        return deferred_from_coro(self._open_spider(spider))\n \n     def process_item(self, item, spider):\n         return {\"url\": item[\"url\"].upper()}\n \n \n-class UrlSpider(scrapy.Spider):\n+class UrlSpider(Spider):\n     name = \"url_spider\"\n     start_urls = [\"data:,\"]\n     custom_settings = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b20cfef1e54d9f22769f6d0ec6ae06031bf86ec3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,7 +13,6 @@ class UppercasePipeline:\n         await asyncio.sleep(0.1)\n \n     def open_spider(self, spider):\n-        loop = asyncio.get_event_loop()\n         return deferred_from_coro(self._open_spider(spider))\n \n     def process_item(self, item, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99cc853d6953d336ca65e0eecc0cb3286306bacf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 2625 | Contributors (this commit): 32 | Commits (past 90d): 7 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -442,6 +442,7 @@ class ScrapyAgent:\n             flags=result[\"flags\"],\n             certificate=result[\"certificate\"],\n             ip_address=result[\"ip_address\"],\n+            protocol=getattr(result[\"txresponse\"], \"version\", None),\n         )\n         if result.get(\"failure\"):\n             result[\"failure\"].value.response = response\n\n@@ -17,8 +17,18 @@ from scrapy.utils.trackref import object_ref\n \n class Response(object_ref):\n \n-    def __init__(self, url, status=200, headers=None, body=b'', flags=None,\n-                 request=None, certificate=None, ip_address=None):\n+    def __init__(\n+        self,\n+        url,\n+        status=200,\n+        headers=None,\n+        body=b\"\",\n+        flags=None,\n+        request=None,\n+        certificate=None,\n+        ip_address=None,\n+        protocol=None,\n+    ):\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n         self._set_body(body)\n@@ -27,6 +37,7 @@ class Response(object_ref):\n         self.flags = [] if flags is None else list(flags)\n         self.certificate = certificate\n         self.ip_address = ip_address\n+        self.protocol = protocol\n \n     @property\n     def cb_kwargs(self):\n@@ -90,7 +101,7 @@ class Response(object_ref):\n         given new values.\n         \"\"\"\n         for x in ['url', 'status', 'headers', 'body',\n-                  'request', 'flags', 'certificate', 'ip_address']:\n+                  'request', 'flags', 'certificate', 'ip_address', 'protocol']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b6b56240c24d02ef69e6cc591ffb2529bc3f36a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 2019 | Contributors (this commit): 26 | Commits (past 90d): 9 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -489,6 +489,13 @@ class Http11TestCase(HttpTestCase):\n     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n         return self.test_download_broken_content_allow_data_loss_via_setting('broken-chunked')\n \n+    def test_protocol(self):\n+        request = Request(self.getURL(\"host\"), method=\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.protocol)\n+        d.addCallback(self.assertEqual, (b\"HTTP\", 1, 1))\n+        return d\n+\n \n class Https11TestCase(Http11TestCase):\n     scheme = 'https'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0fb7bcb2cf1606f63f1863bea254a34386c6a0f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 414 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -100,8 +100,9 @@ class Response(object_ref):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n         \"\"\"\n-        for x in ['url', 'status', 'headers', 'body',\n-                  'request', 'flags', 'certificate', 'ip_address', 'protocol']:\n+        for x in [\n+            \"url\", \"status\", \"headers\", \"body\", \"request\", \"flags\", \"certificate\", \"ip_address\", \"protocol\",\n+        ]:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e9a99e6a1e940864d9157252592f04658eaf851", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 4715 | Contributors (this commit): 40 | Commits (past 90d): 16 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -434,6 +434,11 @@ class ScrapyAgent:\n     def _cb_bodydone(self, result, request, url):\n         headers = Headers(result[\"txresponse\"].headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n+        try:\n+            version = result[\"txresponse\"].version\n+            protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n+        except (AttributeError, TypeError):\n+            protocol = None\n         response = respcls(\n             url=url,\n             status=int(result[\"txresponse\"].code),\n@@ -442,7 +447,7 @@ class ScrapyAgent:\n             flags=result[\"flags\"],\n             certificate=result[\"certificate\"],\n             ip_address=result[\"ip_address\"],\n-            protocol=getattr(result[\"txresponse\"], \"version\", None),\n+            protocol=protocol,\n         )\n         if result.get(\"failure\"):\n             result[\"failure\"].value.response = response\n\n@@ -7,7 +7,7 @@ from twisted.internet.protocol import ClientFactory\n \n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.responsetypes import responsetypes\n \n \n@@ -110,7 +110,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n         respcls = responsetypes.from_args(headers=headers, url=self._url)\n-        return respcls(url=self._url, status=status, headers=headers, body=body)\n+        return respcls(url=self._url, status=status, headers=headers, body=body, protocol=to_unicode(self.version))\n \n     def _set_connection_attributes(self, request):\n         parsed = urlparse_cached(request)\n\n@@ -360,6 +360,13 @@ class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n     download_handler_cls = HTTP10DownloadHandler\n \n+    def test_protocol(self):\n+        request = Request(self.getURL(\"host\"), method=\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.protocol)\n+        d.addCallback(self.assertEqual, \"HTTP/1.0\")\n+        return d\n+\n \n class Https10TestCase(Http10TestCase):\n     scheme = 'https'\n@@ -493,7 +500,7 @@ class Http11TestCase(HttpTestCase):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n         d = self.download_request(request, Spider(\"foo\"))\n         d.addCallback(lambda r: r.protocol)\n-        d.addCallback(self.assertEqual, (b\"HTTP\", 1, 1))\n+        d.addCallback(self.assertEqual, \"HTTP/1.1\")\n         return d\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0368228d7f6391c0df41fca1609c6548613ad6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2225 | Contributors (this commit): 28 | Commits (past 90d): 5 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -437,7 +437,7 @@ class ScrapyAgent:\n         try:\n             version = result[\"txresponse\"].version\n             protocol = f\"{to_unicode(version[0])}/{version[1]}.{version[2]}\"\n-        except (AttributeError, TypeError):\n+        except (AttributeError, TypeError, IndexError):\n             protocol = None\n         response = respcls(\n             url=url,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85604e1078b927c2a875040e29c58b4594c8d386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 10 | Methods Changed: 14 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 124 | Churn Cumulative: 4479 | Contributors (this commit): 32 | Commits (past 90d): 16 | Contributors (cumulative): 53 | DMM Complexity: 1.0\n\nDIFF:\n@@ -319,17 +319,25 @@ class FeedExporter:\n         # Use `largs=log_args` to copy log_args into function's scope\n         # instead of using `log_args` from the outer scope\n         d.addCallback(\n-            lambda _, largs=log_args: logger.info(\n-                logfmt % \"Stored\", largs, extra={'spider': spider}\n-            )\n+            self._handle_store_success, log_args, logfmt, spider, type(slot.storage).__name__\n         )\n         d.addErrback(\n-            lambda f, largs=log_args: logger.error(\n+            self._handle_store_error, log_args, logfmt, spider, type(slot.storage).__name__\n+        )\n+        return d\n+\n+    def _handle_store_error(self, f, largs, logfmt, spider, slot_type):\n+        logger.error(\n             logfmt % \"Error storing\", largs,\n             exc_info=failure_to_exc_info(f), extra={'spider': spider}\n         )\n+        self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n+\n+    def _handle_store_success(self, f, largs, logfmt, spider, slot_type):\n+        logger.info(\n+            logfmt % \"Stored\", largs, extra={'spider': spider}\n         )\n-        return d\n+        self.crawler.stats.inc_value(f\"feedexport/success_count/{slot_type}\")\n \n     def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n         \"\"\"\n\n@@ -8,6 +8,7 @@ import tempfile\n import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n+from contextlib import ExitStack\n from io import BytesIO\n from logging import getLogger\n from pathlib import Path\n@@ -47,6 +48,21 @@ from scrapy.utils.test import (\n )\n \n from tests.mockserver import MockFTPServer, MockServer\n+from tests.spiders import ItemSpider\n+\n+\n+def path_to_url(path):\n+    return urljoin('file:', pathname2url(str(path)))\n+\n+\n+def printf_escape(string):\n+    return string.replace('%', '%%')\n+\n+\n+def build_url(path):\n+    if path[0] != '/':\n+        path = '/' + path\n+    return urljoin('file:', path)\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -620,12 +636,6 @@ class FeedExportTest(FeedExportTestBase):\n     def run_and_export(self, spider_cls, settings):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n \n-        def path_to_url(path):\n-            return urljoin('file:', pathname2url(str(path)))\n-\n-        def printf_escape(string):\n-            return string.replace('%', '%%')\n-\n         FEEDS = settings.get('FEEDS') or {}\n         settings['FEEDS'] = {\n             printf_escape(path_to_url(file_path)): feed_options\n@@ -748,6 +758,69 @@ class FeedExportTest(FeedExportTestBase):\n         result = self._load_until_eof(data['marshal'], load_func=marshal.load)\n         self.assertEqual(expected, result)\n \n+    @defer.inlineCallbacks\n+    def test_stats_file_success(self):\n+        settings = {\n+            \"FEEDS\": {\n+                printf_escape(path_to_url(self._random_temp_filename())): {\n+                    \"format\": \"json\",\n+                }\n+            },\n+        }\n+        crawler = get_crawler(ItemSpider, settings)\n+        with MockServer() as mockserver:\n+            yield crawler.crawl(mockserver=mockserver)\n+        self.assertIn(\"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats())\n+        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1)\n+\n+    @defer.inlineCallbacks\n+    def test_stats_file_failed(self):\n+        settings = {\n+            \"FEEDS\": {\n+                printf_escape(path_to_url(self._random_temp_filename())): {\n+                    \"format\": \"json\",\n+                }\n+            },\n+        }\n+        crawler = get_crawler(ItemSpider, settings)\n+        with ExitStack() as stack:\n+            mockserver = stack.enter_context(MockServer())\n+            stack.enter_context(\n+                mock.patch(\n+                    \"scrapy.extensions.feedexport.FileFeedStorage.store\",\n+                    side_effect=KeyError(\"foo\"))\n+            )\n+            yield crawler.crawl(mockserver=mockserver)\n+        self.assertIn(\"feedexport/failed_count/FileFeedStorage\", crawler.stats.get_stats())\n+        self.assertEqual(crawler.stats.get_value(\"feedexport/failed_count/FileFeedStorage\"), 1)\n+\n+    @defer.inlineCallbacks\n+    def test_stats_multiple_file(self):\n+        settings = {\n+            'AWS_ACCESS_KEY_ID': 'access_key',\n+            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            \"FEEDS\": {\n+                printf_escape(path_to_url(self._random_temp_filename())): {\n+                    \"format\": \"json\",\n+                },\n+                \"s3://bucket/key/foo.csv\": {\n+                    \"format\": \"csv\",\n+                },\n+                \"stdout:\": {\n+                    \"format\": \"xml\",\n+                }\n+            },\n+        }\n+        crawler = get_crawler(ItemSpider, settings)\n+        with MockServer() as mockserver, mock.patch.object(S3FeedStorage, \"store\"):\n+            yield crawler.crawl(mockserver=mockserver)\n+        self.assertIn(\"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats())\n+        self.assertIn(\"feedexport/success_count/S3FeedStorage\", crawler.stats.get_stats())\n+        self.assertIn(\"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats())\n+        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1)\n+        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/S3FeedStorage\"), 1)\n+        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1)\n+\n     @defer.inlineCallbacks\n     def test_export_items(self):\n         # feed exporters use field names from Item\n@@ -1256,11 +1329,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     def run_and_export(self, spider_cls, settings):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n \n-        def build_url(path):\n-            if path[0] != '/':\n-                path = '/' + path\n-            return urljoin('file:', path)\n-\n         FEEDS = settings.get('FEEDS') or {}\n         settings['FEEDS'] = {\n             build_url(file_path): feed\n@@ -1550,6 +1618,22 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         data = yield self.exported_data(items, settings)\n         self.assertEqual(len(items) + 1, len(data['json']))\n \n+    @defer.inlineCallbacks\n+    def test_stats_batch_file_success(self):\n+        settings = {\n+            \"FEEDS\": {\n+                build_url(os.path.join(self._random_temp_filename(), \"json\", self._file_mark)): {\n+                    \"format\": \"json\",\n+                }\n+            },\n+            \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n+        }\n+        crawler = get_crawler(ItemSpider, settings)\n+        with MockServer() as mockserver:\n+            yield crawler.crawl(total=2, mockserver=mockserver)\n+        self.assertIn(\"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats())\n+        self.assertEqual(crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 12)\n+\n     @defer.inlineCallbacks\n     def test_s3_export(self):\n         skip_if_no_boto()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2405df49f14cbc052d73e58a819a87417b2502e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 2037 | Contributors (this commit): 26 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -115,6 +115,7 @@ class FileTestCase(unittest.TestCase):\n             self.assertEqual(response.url, request.url)\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.body, b'0123456789')\n+            self.assertEqual(response.protocol, None)\n \n         request = Request(path_to_file_uri(self.tmpname + '^'))\n         assert request.url.upper().endswith('%5E')\n@@ -976,6 +977,7 @@ class BaseFTPTestCase(unittest.TestCase):\n             self.assertEqual(r.status, 200)\n             self.assertEqual(r.body, b'I have the power!')\n             self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'17']})\n+            self.assertIsNone(r.protocol)\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_download_path_with_spaces(self):\n@@ -1134,3 +1136,10 @@ class DataURITestCase(unittest.TestCase):\n \n         request = Request('data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D')\n         return self.download_request(request, self.spider).addCallback(_test)\n+\n+    def test_protocol(self):\n+        def _test(response):\n+            self.assertIsNone(response.protocol)\n+\n+        request = Request(\"data:,\")\n+        return self.download_request(request, self.spider).addCallback(_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
