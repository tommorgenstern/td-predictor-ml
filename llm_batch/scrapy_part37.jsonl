{"custom_id": "scrapy#9209dbc8825b451ec0129d783e9bb31185f18f91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 96 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ class RobustScrapedItemTestCase(unittest.TestCase):\n         self.item.attribute('children', 'Jimmy', add=True)\n         self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy'])\n \n-        self.item.attribute('children', *['Johnny', 'Rodrigo'], add=True)\n+        self.item.attribute('children', 'Johnny', 'Rodrigo', add=True)\n         self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy', 'Johnny', 'Rodrigo'])\n \n     def test_set_adaptors(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a07c95003fcacaaf2674e3a9cdb243f88fa01f12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 68 | Churn Cumulative: 131 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,6 +69,7 @@ SPIDER_MIDDLEWARES = (\n     'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n     'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n     'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n+    'scrapy.contrib.spidermiddleware.duplicatesfilter.DuplicatesFilterMiddleware',\n     # Spider side\n )\n \n@@ -96,3 +97,5 @@ URLLENGTH_LIMIT = 2083\n WS_ENABLED = 0\n \n SPIDERPROFILER_ENABLED = 0\n+\n+#DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'\n\n@@ -0,0 +1,65 @@\n+\"\"\"\n+DuplicatesFilterMiddleware: Filter out already visited urls\n+\"\"\"\n+\n+from pydispatch import dispatcher\n+\n+from scrapy.core import signals\n+from scrapy.http import Request\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.misc import load_class\n+from scrapy.conf import settings\n+from scrapy import log\n+\n+\n+class DuplicatesFilterMiddleware(object):\n+    \"\"\"Filter out duplicate requests to avoid visiting same page more than once\"\"\"\n+\n+    def __init__(self):\n+        clspath = settings.get('DUPLICATESFILTER_FILTERCLASS')\n+        if not clspath:\n+            raise NotConfigured\n+\n+        self.filter = load_class(clspath)()\n+        dispatcher.connect(self.filter.open, signals.domain_open)\n+        dispatcher.connect(self.filter.close, signals.domain_closed)\n+\n+    def process_spider_output(self, response, result, spider):\n+        domain = spider.domain_name\n+\n+        for req in result:\n+            if isinstance(req, Request):\n+                added = self.filter.add(domain, req)\n+                if not (added or req.dont_filter):\n+                    log.msg('Skipped (already visited): %s' % req, log.DEBUG, domain=domain)\n+                    continue\n+            yield req\n+\n+\n+class SimplePerDomainFilter(dict):\n+    \"\"\"Filter out a request if already seen for same domain\"\"\"\n+\n+    def open(self, domain):\n+        \"\"\"Initialize the resources needed for filtering for this domain\"\"\"\n+        self[domain] = set()\n+\n+    def close(self, domain):\n+        \"\"\"Remove the resources reserved for filtering for this domain\"\"\"\n+        del self[domain]\n+\n+    def add(self, domain, request):\n+        \"\"\"Add the fingerprint of a request to the domain set if a equivalent fingerprint has not been added.\n+        This method will return true if the fingerprint was added and false otherwise.\n+        \"\"\"\n+        fp = request_fingerprint(request)\n+        if fp not in self[domain]:\n+            self[domain].add(fp)\n+            return True\n+        return False\n+\n+    def has(self, domain, request):\n+        \"\"\"Check if a request was already seen for a domain\"\"\"\n+        return request_fingerprint(request) in self[domain]\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33df3c81c2d2c10fbd8e28d9aaf35571c09a8c54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 9 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,9 @@\n+\"\"\"\n+This module contains experimental code that may go into scrapy.contrib in the\n+future, but it's not yet stable enough to go there (either API stable or\n+functionality stable).\n+\n+Subscribe to Scrapy developers mailing list or join the IRC channel if you want\n+to discuss about this code.\n+\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbaf6027304a1c9244437cdd8fbbbb834409c78d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 56 | Files Changed: 6 | Hunks: 30 | Methods Changed: 23 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 117 | Churn Cumulative: 1267 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 9 | DMM Complexity: 0.9333333333333333\n\nDIFF:\n@@ -119,15 +119,6 @@ class RobustScrapedItem(ScrapedItem):\n         else:\n            raise NotImplementedError('You must override _add_single_attributes method in order to join %s values into a single value.' % attrtype.__name__)\n \n-    def _clean_values(self, values):\n-        ret = []\n-        for val in values:\n-            if isinstance(val, tuple):\n-                ret.extend(val)\n-            elif val:\n-                ret.append(val)\n-        return ret\n-\n     def attribute(self, attrname, *values, **kwargs):\n         \"\"\"\n         Set the provided values to the provided attribute (`attrname`) by filtering them\n@@ -146,6 +137,15 @@ class RobustScrapedItem(ScrapedItem):\n         The kwargs parameter is passed to the adaptors pipeline, which manages to transmit\n         it to the adaptors themselves.\n         \"\"\"\n+        def _clean_values(values):\n+            ret = []\n+            for val in values:\n+                if isinstance(val, tuple):\n+                    ret.extend(val)\n+                elif val:\n+                    ret.append(val)\n+            return ret\n+\n         if not values:\n             raise UsageError(\"You must specify at least one value when setting an attribute\")\n         if attrname not in self.ATTRIBUTES:\n@@ -162,7 +162,7 @@ class RobustScrapedItem(ScrapedItem):\n         multivalued = isinstance(attrtype, list)\n         adaptors_pipe = self._adaptors_dict.get(attrname)\n         new_values = [adaptors_pipe(value, kwargs) for value in values] if adaptors_pipe else [values]\n-        new_values = self._clean_values(new_values)\n+        new_values = _clean_values(new_values)\n \n         if old_value and not override:\n             if multivalued:\n\n@@ -1,4 +1,4 @@\n from scrapy.contrib_exp.adaptors.extraction import extract, ExtractImageLinks\n-from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, Unquote\n-from scrapy.contrib_exp.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, Delist, Regex\n-from scrapy.utils.python import flatten\n+from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, unquote\n+from scrapy.contrib_exp.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, delist, Regex\n+\n\n@@ -3,20 +3,25 @@ from scrapy.utils.markup import replace_tags, unquote_markup\n from scrapy.utils.python import str_to_unicode\n from scrapy.item.adaptors import adaptize\n \n-def remove_tags(value):\n+def remove_tags(tags=()):\n     \"\"\"\n-    Removes any tags found in each of the provided list's string.\n-    E.g:\n-      >> remove_tags('<head>my header</head>', '<body>my <b>body</b></body>')\n-      u'my header', u'my body'\n+    Factory that returns an adaptor for removing each\n+    tag in the `tags` parameter found in the given value.\n+    If no `tags` are specified, all of them are removed.\n+\n     Input: string/unicode\n     Output: unicode\n     \"\"\"\n-    return replace_tags(value)\n+    def _remove_tags(value):\n+        return replace_tags(value, tags)\n+    return _remove_tags\n \n _remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n def remove_root(value):\n     \"\"\"\n+    This adaptor removes the root tag of the given string/unicode,\n+    if it's found.\n+\n     Input: string/unicode\n     Output: unicode\n     \"\"\"\n@@ -25,18 +30,20 @@ def remove_root(value):\n         value = m.group(1)\n     return str_to_unicode(value)\n \n-class Unquote(object):\n+def unquote(keep=None):\n     \"\"\"\n-    Receives a list of strings, removes all of the\n-    CDATAs and entities (except the ones in CDATAs) the strings\n-    may have, and returns a new list.\n+    This factory returns an adaptor that\n+    receives a string or unicode, removes all of the\n+    CDATAs and entities (except the ones in CDATAs, and the ones\n+    you specify in the `keep` parameter) and then, returns a new\n+    string or unicode.\n \n     Input: string/unicode\n     Output: string/unicode\n     \"\"\"\n-    def __init__(self, keep=None):\n-        self.keep = [] if keep is None else keep\n+    default_keep = [] if keep is None else keep\n \n-    def __call__(self, value, adaptor_args):\n-        keep = adaptor_args.get('keep_entities', self.keep)\n+    def unquote(value, adaptor_args):\n+        keep = adaptor_args.get('keep_entities', default_keep)\n         return unquote_markup(value, keep=keep)\n+    return unquote\n\n@@ -8,14 +8,15 @@ from scrapy.item.adaptors import adaptize\n \n def to_unicode(value, adaptor_args):\n     \"\"\"\n-    Receives a list of strings, converts\n-    it to unicode, and returns a new list.\n+    Receives a string and converts it to unicode\n+    using the given encoding (if specified, else utf-8 is used)\n+    and returns a new unicode object.\n     E.g:\n-      >> to_unicode(['it costs 20€, or 30£'])\n+      >> to_unicode('it costs 20\\xe2\\x82\\xac, or 30\\xc2\\xa3')\n         [u'it costs 20\\u20ac, or 30\\xa3']\n \n-    Input: iterable of strings\n-    Output: list of unicodes\n+    Input: string\n+    Output: unicode\n     \"\"\"\n     if not isinstance(value, basestring):\n         value = str(value)\n@@ -24,14 +25,13 @@ def to_unicode(value, adaptor_args):\n _clean_spaces_re = re.compile(\"\\s+\", re.U)\n def clean_spaces(value):\n     \"\"\"\n-    Converts multispaces into single spaces for each string\n-    in the provided iterable.\n+    Converts multispaces into single spaces for the given string.\n     E.g:\n-      >> clean_spaces(['Hello   sir'])\n-      [u'Hello sir']\n+      >> clean_spaces(u'Hello   sir')\n+      u'Hello sir'\n \n-    Input: iterable of unicodes\n-    Output: list of unicodes\n+    Input: string/unicode\n+    Output: unicode\n     \"\"\"\n     return _clean_spaces_re.sub(' ', str_to_unicode(value))\n \n@@ -61,20 +61,18 @@ def drop_empty(value):\n     \"\"\"\n     return [ v for v in value if v ]\n \n-class Delist(object):\n+def delist(delimiter=''):\n     \"\"\"\n-    Joins a list with the specified delimiter\n-    in the adaptor's constructor.\n+    This factory returns and adaptor that joins\n+    an iterable with the specified delimiter.\n \n     Input: iterable of strings/unicodes\n     Output: string/unicode\n     \"\"\"\n-    def __init__(self, delimiter=''):\n-        self.delimiter = delimiter\n-\n-    def __call__(self, value, adaptor_args):\n-        delimiter = adaptor_args.get('join_delimiter', self.delimiter)\n-        return delimiter.join(value)\n+    def delist(value, adaptor_args):\n+        delim = adaptor_args.get('join_delimiter', delimiter)\n+        return delim.join(value)\n+    return delist\n \n class Regex(object):\n     \"\"\"\n\n@@ -162,18 +162,18 @@ class AdaptorsTestCase(unittest.TestCase):\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n \n     def test_unquote_all(self):\n-        unquote = adaptors.Unquote()\n+        unquote = adaptors.unquote()\n         self.assertEqual(unquote(u'hello&copy;&amp;welcome&lt;br /&gt;&amp;', {}),\n             u'hello\\xa9&welcome<br />&')\n \n     def test_unquote(self):\n-        unquote = adaptors.Unquote(keep=['amp', 'lt'])\n+        unquote = adaptors.unquote(keep=['amp', 'lt'])\n         self.assertEqual(unquote(u'hello&copy;&amp;welcome&lt;br /&gt;&amp;', {}),\n             u'hello\\xa9&amp;welcome&lt;br />&amp;')\n \n     def test_remove_tags(self):\n-        self.assertEqual(adaptors.remove_tags('<a href=\"lala\">adsaas<br /></a>'), 'adsaas')\n-        self.assertEqual(adaptors.remove_tags('<div id=\"1\"><table>dsadasf</table></div>'), 'dsadasf')\n+        self.assertEqual(adaptors.remove_tags()('<a href=\"lala\">adsaas<br /></a>'), 'adsaas')\n+        self.assertEqual(adaptors.remove_tags()('<div id=\"1\"><table>dsadasf</table></div>'), 'dsadasf')\n \n     def test_remove_root(self):\n         self.assertEqual(adaptors.remove_root('<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>'),\n@@ -191,7 +191,7 @@ class AdaptorsTestCase(unittest.TestCase):\n             [1, 2, 5, 6, 'hi'])\n \n     def test_delist(self):\n-        delist = adaptors.Delist(' ')\n+        delist = adaptors.delist(' ')\n         self.assertEqual(delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.'], {}),\n             'hi there fellas. this is my test.')\n         self.assertEqual(delist(['hi', 'there', 'fellas,', 'this', 'is', 'my', 'test.'], {'join_delimiter': '.'}),\n\n@@ -61,11 +61,11 @@ class RobustScrapedItemTestCase(unittest.TestCase):\n     def test_set_adaptors(self):\n         self.assertEqual(self.item._adaptors_dict, {})\n \n-        delist = adaptors.Delist()\n+        delist = adaptors.delist()\n         self.item.set_adaptors({'name': [adaptors.extract, delist]})\n         self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n         self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n-        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"Delist\")\n+        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"delist\")\n \n         self.item.set_adaptors({'description': [adaptors.extract]})\n         self.assertEqual(self.item._adaptors_dict['description'][0].name, \"extract\")\n@@ -78,11 +78,11 @@ class RobustScrapedItemTestCase(unittest.TestCase):\n         self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n         self.assertEqual(self.item._adaptors_dict['name'][1].name, \"strip\")\n \n-        unquote = adaptors.Unquote()\n+        unquote = adaptors.unquote()\n         self.item.set_attrib_adaptors('name', [adaptors.extract, unquote])\n         self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n         self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n-        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"Unquote\")\n+        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"unquote\")\n \n     def test_add_adaptor(self):\n         self.assertEqual(self.item._adaptors_dict, {})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f0d5a1653ce0a90b94ac0a65554b8231e44e660", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 738 | Lines Deleted: 366 | Files Changed: 43 | Hunks: 143 | Methods Changed: 137 | Complexity Δ (Sum/Max): 104/28 | Churn Δ: 1104 | Churn Cumulative: 6829 | Contributors (this commit): 5 | Commits (past 90d): 205 | Contributors (cumulative): 70 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,18 +11,10 @@ from scrapy import log\n from scrapy.http import Response, Headers\n from scrapy.http.headers import headers_dict_to_raw\n from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest\n+from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.conf import settings\n \n-class CachedResponse(Response):\n-\n-    def __init__(self, *args, **kwargs):\n-        Response.__init__(self, *args, **kwargs)\n-        self.meta['cached'] = True\n-\n-    def __str__(self):\n-        return \"(cached) \" + Response.__str__(self)\n-\n class CacheMiddleware(object):\n     def __init__(self):\n         if not settings['CACHE2_DIR']:\n@@ -129,7 +121,7 @@ class Cache(object):\n                 if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs):\n                     return True\n                 else:\n-                    log.msg('dropping old cached response from %s' % metadata['timestamp'])\n+                    log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG)\n                     return False\n             else:\n                 # disabled cache expiration\n@@ -158,7 +150,10 @@ class Cache(object):\n         headers = Headers(responseheaders)\n         status = metadata['status']\n \n-        response = CachedResponse(url=url, headers=headers, status=status, body=responsebody)\n+        respcls = responsetypes.from_headers(headers)\n+        response = respcls(url=url, headers=headers, status=status, body=responsebody)\n+        response.meta['cached'] = True\n+        response.flags.append('cached')\n         return response\n \n     def store(self, domain, key, request, response):\n@@ -184,7 +179,7 @@ class Cache(object):\n         with open(os.path.join(requestpath, 'response_headers'), 'w') as f:\n             f.write(headers_dict_to_raw(response.headers))\n         with open(os.path.join(requestpath, 'response_body'), 'w') as f:\n-            f.write(response.body.get_content())\n+            f.write(response.body)\n         # request\n         with open(os.path.join(requestpath, 'request_headers'), 'w') as f:\n             f.write(headers_dict_to_raw(request.headers))\n\n@@ -17,7 +17,7 @@ class CompressionMiddleware(object):\n             content_encoding = response.headers.get('Content-Encoding')\n             if content_encoding:\n                 encoding = content_encoding[0].lower()\n-                raw_body = response.body.get_content()\n+                raw_body = response.body\n                 decoded_body = self._decode(raw_body, encoding)\n                 response = response.replace(body=decoded_body)\n                 response.headers['Content-Encoding'] = content_encoding[1:]\n\n@@ -23,10 +23,7 @@ class RedirectMiddleware(object):\n             if status in ['302', '303']:\n                 redirected_url = urljoin(request.url, response.headers['location'][0])\n                 if not getattr(spider, \"no_redirect\", False):\n-                    redirected = request.copy()\n-                    redirected.url = redirected_url\n-                    redirected.method = 'GET'\n-                    redirected.body = None\n+                    redirected = request.replace(url=redirected_url, method='GET', body=None)\n                     # This is needed to avoid redirection loops with requests that contain dont_filter = True\n                     # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n                     if isinstance(redirected.dont_filter, int):\n@@ -45,8 +42,7 @@ class RedirectMiddleware(object):\n             if status in ['301', '307']:\n                 redirected_url = urljoin(request.url, response.headers['location'][0])\n                 if not getattr(spider, \"no_redirect\", False):\n-                    redirected = request.copy()\n-                    redirected.url = redirected_url\n+                    redirected = request.replace(url=redirected_url)\n                     # This is needed to avoid redirection loops with requests that contain dont_filter = True\n                     # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n                     redirected.dont_filter = False\n@@ -57,7 +53,7 @@ class RedirectMiddleware(object):\n \n     def process_response(self, request, response, spider):\n         if isinstance(response, Response):\n-            m = META_REFRESH_RE.search(response.body.to_string()[0:4096])\n+            m = META_REFRESH_RE.search(response.body[0:4096])\n             if m and int(m.group(1)) < META_REFRESH_MAXSEC:\n                 redirected = request.copy()\n                 redirected.url = urljoin(request.url, m.group(2))\n\n@@ -72,7 +72,7 @@ class HistoryMiddleware(object):\n         self.historydata.store(domain, key, url, parentkey, version, post_version)\n \n     def get_version(self, response):\n-        key = hashlib.sha1(response.body.to_string()).hexdigest()\n+        key = hashlib.sha1(response.body).hexdigest()\n \n def urlkey(url):\n     \"\"\"Generate a 'key' for a given url\n\n@@ -102,4 +102,4 @@ class RulesScheduler(Scheduler):\n         return request\n \n     def get_version(self, response):\n-        key = hashlib.sha1(response.body.to_string()).hexdigest()\n+        key = hashlib.sha1(response.body).hexdigest()\n\n@@ -33,7 +33,7 @@ class BaseImagesPipeline(MediaPipeline):\n         mtype = self.MEDIA_TYPE\n         referer = request.headers.get('Referer')\n \n-        if not response or not response.body.to_string():\n+        if not response or not response.body:\n             msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: no-content' \\\n                     % (mtype, request, referer)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n@@ -172,7 +172,7 @@ def save_scaled_image(image, img_path, name, size):\n     thumb.save(filename, 'JPEG')\n \n def save_image_with_thumbnails(response, path, thumbsizes, min_width=0, min_height=0):\n-    memoryfile = StringIO(response.body.to_string())\n+    memoryfile = StringIO(response.body)\n     im = Image.open(memoryfile)\n     if im.mode != 'RGB':\n         log.msg(\"Found non-RGB image during scraping %s\" % path, level=log.WARNING)\n@@ -183,7 +183,7 @@ def save_image_with_thumbnails(response, path, thumbsizes, min_width=0, min_heig\n     except Exception, ex:\n         log.msg(\"Image (processing-error): cannot process %s, so writing direct file: Error: %s\" % (path, ex))\n         f = open(path, 'wb')\n-        f.write(response.body.to_string())\n+        f.write(response.body)\n         f.close()\n     width, height = im.size\n     if width < min_width or height < min_height:\n\n@@ -123,7 +123,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n \n     def s3_store_image(self, response, url, info):\n         \"\"\"Upload image to S3 storage\"\"\"\n-        buf = StringIO(response.body.to_string())\n+        buf = StringIO(response.body)\n         image = Image.open(buf)\n         key = self.s3_image_key(url)\n         _, jpegbuf = self._s3_put_image(image, key, info)\n\n@@ -17,6 +17,6 @@ class ResponseSoup(object):\n def getsoup(response, **kwargs):\n     # TODO: use different cache buckets depending on constructor parameters\n     if 'soup' not in response.cache:\n-        body = response.body.to_string() if response.body is not None else \"\"\n+        body = response.body if response.body is not None else \"\"\n         response.cache['soup'] = BeautifulSoup(body, **kwargs)\n     return response.cache['soup']\n\n\n@@ -1,14 +1,19 @@\n+\"\"\" This module implements the DecompressionMiddleware which tries to recognise\n+and extract the potentially compressed responses that may arrive. \n+\n+NOTE: This middleware needs a better name to avoid confusiong it with the\n+CompressinMiddleware (in contrib.downloadermiddleware.compression).\n+\"\"\"\n+\n import zipfile\n import tarfile\n import gzip\n import bz2\n-try:\n from cStringIO import StringIO\n-except:\n-    from StringIO import StringIO\n \n from scrapy import log\n from scrapy.http import Response\n+from scrapy.core.downloader.responsetypes import responsetypes\n \n class DecompressionMiddleware(object):\n     \"\"\" This middleware tries to recognise and extract the possibly compressed\n@@ -28,7 +33,9 @@ class DecompressionMiddleware(object):\n         except tarfile.ReadError:\n             return False\n         if tar_file.members:\n-            return response.replace(body=tar_file.extractfile(tar_file.members[0]).read())\n+            body = body=tar_file.extractfile(tar_file.members[0]).read()\n+            respcls = self._get_response_class(filename=tar_file.members[0].name, body=body)\n+            return response.replace(body=body, cls=respcls)\n         else:\n             raise self.ArchiveIsEmpty\n \n@@ -39,7 +46,9 @@ class DecompressionMiddleware(object):\n             return False\n         namelist = zip_file.namelist()\n         if namelist:\n-            return response.replace(body=zip_file.read(namelist[0]))\n+            body = zip_file.read(namelist[0])\n+            respcls = self._get_response_class(filename=namelist[0], body=body)\n+            return response.replace(body=body, cls=respcls)\n         else:\n             raise self.ArchiveIsEmpty\n \n@@ -49,21 +58,31 @@ class DecompressionMiddleware(object):\n             decompressed_body = gzip_file.read()\n         except IOError:\n             return False\n-        return response.replace(body=decompressed_body)\n+        respcls = self._get_response_class(body=decompressed_body)\n+        return response.replace(body=decompressed_body, cls=respcls)\n \n     def is_bzip2(self, response):\n         try:\n             decompressed_body = bz2.decompress(self.body)\n         except IOError:\n             return False\n-        return response.replace(body=decompressed_body)\n+        respcls = self._get_response_class(body=decompressed_body)\n+        return response.replace(body=decompressed_body, cls=respcls)\n+\n+    def _get_response_class(self, filename=None, body=None):\n+        respcls = Response\n+        if filename is not None:\n+            respcls = responsetypes.from_filename(filename)\n+        if respcls is Response and body is not None:\n+            respcls = responsetypes.from_body(body)\n+        return respcls\n \n     def extract(self, response):\n         \"\"\" This method tries to decompress the given response, if possible,\n         and returns a tuple containing the resulting response, and the name\n         of the used decompressor \"\"\"\n \n-        self.body = response.body.to_string()\n+        self.body = response.body\n         self.archive = StringIO()\n         self.archive.write(self.body)\n \n\n@@ -23,6 +23,7 @@ from scrapy.utils.defer import defer_succeed\n from scrapy.conf import settings\n \n from scrapy.core.downloader.dnscache import DNSCache\n+from scrapy.core.downloader.responsetypes import responsetypes\n \n default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n default_agent = settings.get('USER_AGENT')\n@@ -63,7 +64,8 @@ def create_factory(request, spider):\n         body = body or ''\n         status = int(factory.status)\n         headers = Headers(factory.response_headers)\n-        r = Response(url=request.url, status=status, headers=headers, body=body)\n+        respcls = responsetypes.from_headers(headers)\n+        r = respcls(url=request.url, status=status, headers=headers, body=body)\n         signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n         signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n         return r\n\n@@ -0,0 +1,72 @@\n+\"\"\"\n+This module implements a class which returns the appropiate Response class\n+based on different criterias.\n+\n+\"\"\"\n+\n+import mimetypes\n+\n+from scrapy.http import Response\n+from scrapy.utils.misc import load_class\n+from scrapy.utils.python import isbinarytext\n+from scrapy.conf import settings\n+\n+class ResponseTypes(object):\n+\n+    CLASSES = {\n+        'text/html': 'scrapy.http.HtmlResponse',\n+        'text/xml': 'scrapy.http.XmlResponse',\n+        'text': 'scrapy.http.TextResponse',\n+    }\n+\n+    def __init__(self):\n+        self.CLASSES.update(settings.get('RESPONSE_CLASSES', {}))\n+        self.classes = {}\n+        for mimetype, cls in self.CLASSES.iteritems():\n+            self.classes[mimetype] = load_class(cls)\n+\n+    def from_mimetype(self, mimetype):\n+        \"\"\"Return the most appropiate Response class for the given mimetype\"\"\"\n+        return self.classes.get(mimetype, self.classes.get(mimetype.split('/')[0], Response))\n+\n+    def from_content_type(self, content_type):\n+        \"\"\"Return the most appropiate Response class from an HTTP Content-Type\n+        header \"\"\"\n+        mimetype = content_type.split(';')[0].strip().lower()\n+        return self.from_mimetype(mimetype)\n+\n+    def from_headers(self, headers):\n+        \"\"\"Return the most appropiate Response class by looking at the HTTP\n+        headers\"\"\"\n+        if 'Content-Type' in headers:\n+            return self.from_content_type(headers['Content-type'][0])\n+        else:\n+            return Response\n+\n+    def from_filename(self, filename):\n+        \"\"\"Return the most appropiate Response class from a file name\"\"\"\n+        return self.from_mimetype(mimetypes.guess_type(filename)[0])\n+\n+    def from_url(self, url):\n+        \"\"\"Return the most appropiate Response class from a URL\"\"\"\n+        return self.from_mimetype(mimetypes.guess_type(url)[0])\n+\n+    def from_body(self, body):\n+        \"\"\"Try to guess the appropiate response based on the body content. \n+        \n+        This method is a bit magic and could be improved in the future, but\n+        it's not meant to be used except for special cases where response types\n+        cannot be guess using more straightforward methods.\n+\n+        \"\"\"\n+        chunk = body[:5000]\n+        if isbinarytext(chunk):\n+            return self.from_mimetype('application/octet-stream')\n+        elif \"<html>\" in chunk.lower():\n+            return self.from_mimetype('text/html')\n+        elif \"<?xml\" in chunk.lower():\n+            return self.from_mimetype('text/xml')\n+        else:\n+            return self.from_mimetype('text')\n+\n+responsetypes = ResponseTypes()\n\n@@ -7,5 +7,11 @@ Request, Response and Url outside this module.\n \n from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n+\n from scrapy.http.request import Request\n+from scrapy.http.request.form import FormRequest\n+\n from scrapy.http.response import Response\n+from scrapy.http.response.html import HtmlResponse\n+from scrapy.http.response.xml import XmlResponse\n+from scrapy.http.response.text import TextResponse\n\n@@ -20,7 +20,7 @@ class Request(object):\n     def __init__(self, url, callback=None, method='GET', headers=None, body=None, \n                  cookies=None, meta=None, encoding='utf-8', dont_filter=None):\n \n-        self.encoding = encoding  # this one has to be set first\n+        self._encoding = encoding  # this one has to be set first\n         self.method = method.upper()\n         self.set_url(url)\n         self.set_body(body)\n@@ -44,19 +44,20 @@ class Request(object):\n     url = property(lambda x: x._url, set_url)\n \n     def set_body(self, body):\n-        # TODO: move dict constructor to another Request class\n-        if isinstance(body, dict):\n-            self._body = urllib.urlencode(body)\n-        elif body is None:\n-            self._body = None\n-        elif isinstance(body, str):\n+        if isinstance(body, str):\n             self._body = body\n         elif isinstance(body, unicode):\n             self._body = body.encode(self.encoding)\n+        elif body is None:\n+            self._body = ''\n         else:\n-            raise TypeError(\"Request body must either str, unicode or None. Got: '%s'\" % type(body).__name__)\n+            raise TypeError(\"Request body must either str or unicode. Got: '%s'\" % type(body).__name__)\n     body = property(lambda x: x._body, set_body)\n \n+    @property\n+    def encoding(self):\n+        return self._encoding\n+\n     def __str__(self):\n         if self.method == 'GET':\n             return \"<%s>\" % self.url\n@@ -108,7 +109,5 @@ class Request(object):\n         if self.headers:\n             s += self.headers.to_string() + \"\\r\\n\"\n         s += \"\\r\\n\"\n-        if self.body:\n         s += self.body\n-            s += \"\\r\\n\"\n         return s\n\n@@ -0,0 +1,22 @@\n+\"\"\"\n+This module implements the FormRequest class which is a more covenient class\n+(that Request) to generate Requests based on form data.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n+import urllib\n+\n+from scrapy.http.request import Request\n+from scrapy.utils.python import unicode_to_str\n+\n+class FormRequest(Request):\n+\n+    def __init__(self, *args, **kwargs):\n+        formdata = kwargs.pop('formdata', None)\n+        Request.__init__(self, *args, **kwargs)\n+        if formdata:\n+            items = formdata.iteritems() if isinstance(formdata, dict) else formdata\n+            query = [(unicode_to_str(k, self.encoding), unicode_to_str(v, self.encoding)) for k, v in items]\n+            self.body = urllib.urlencode(query)\n+            self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n\n@@ -1,186 +0,0 @@\n-\"\"\"\n-This module implements the Response class which is used to represent HTTP\n-esponses in Scrapy.\n-\n-See documentation in docs/ref/request-response.rst\n-\"\"\"\n-\n-import re\n-import copy\n-\n-from twisted.web.http import RESPONSES\n-from BeautifulSoup import UnicodeDammit\n-\n-from scrapy.http.url import Url\n-from scrapy.http.headers import Headers\n-\n-class Response(object):\n-\n-    _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n-\n-    def __init__(self, url, status=200, headers=None, body=None, meta=None):\n-        self.url = Url(url)\n-        self.headers = Headers(headers or {})\n-        self.status = status\n-        if body is not None:\n-            assert isinstance(body, basestring), \\\n-                \"body must be basestring, got %s\" % type(body).__name__\n-            self.body = _ResponseBody(body, self.headers_encoding())\n-        else:\n-            self.body = None\n-        self.cached = False\n-        self.request = None\n-        self.meta = {} if meta is None else dict(meta)\n-        self.cache = {}\n-\n-    def headers_encoding(self):\n-        content_type = self.headers.get('Content-Type')\n-        if content_type:\n-            encoding = self._ENCODING_RE.search(content_type[0])\n-            if encoding:\n-                return encoding.group(1)\n-\n-    def __repr__(self):\n-        return \"Response(url=%s, headers=%s, status=%s, body=%s)\" % \\\n-                (repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n-\n-    def __str__(self):\n-        if self.status == 200:\n-            return \"<%s>\" % (self.url)\n-        else:\n-            return \"<%d %s>\" % (self.status, self.url)\n-\n-    def copy(self):\n-        \"\"\"Return a copy of this Response\"\"\"\n-        return self.replace()\n-\n-    def replace(self, url=None, status=None, headers=None, body=None, meta=None):\n-        \"\"\"Create a new Response with the same attributes except for those\n-        given new values.\n-        \"\"\"\n-        new = self.__class__(url=url or self.url,\n-                             status=status or self.status,\n-                             headers=headers or copy.deepcopy(self.headers),\n-                             meta=meta or self.meta)\n-        if body is None:\n-            new.body = copy.deepcopy(self.body)\n-        else:\n-            new.body = _ResponseBody(body, self.headers_encoding())\n-        return new\n-\n-    def httprepr(self):\n-        \"\"\"\n-        Return raw HTTP response representation (as string). This is provided\n-        only for reference, since it's not the exact stream of bytes that was\n-        received (that's not exposed by Twisted).\n-        \"\"\"\n-\n-        s  = \"HTTP/1.1 %s %s\\r\\n\" % (self.status, RESPONSES[self.status])\n-        if self.headers:\n-            s += self.headers.to_string() + \"\\r\\n\"\n-        s += \"\\r\\n\"\n-        if self.body:\n-            s += self.body.to_string()\n-            s += \"\\r\\n\"\n-        return s\n-\n-class _ResponseBody(object):\n-    \"\"\"The body of an HTTP response. \n-    \n-    WARNING: This is a private class and could be removed in the future without\n-    previous notice. Do not use it this class from outside this module, use\n-    the Response class instead.\n-\n-    Currently, the main purpose of this class is to handle conversion to\n-    unicode and various character encodings.\n-    \"\"\"\n-\n-    _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n-\n-    _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n-    _content_re   = _template % ('content', r'(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)')\n-    _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n-\n-    XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n-\n-    METATAG_RE  = re.compile(r'<meta\\s+%s\\s+%s' % (_httpequiv_re, _content_re), re.I)\n-    METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n-\n-    def __init__(self, content, declared_encoding=None):\n-        self._content = content\n-        self._unicode_content = None\n-        self.declared_encoding = declared_encoding\n-        self._expected_encoding = None\n-        self._actual_encoding = None\n-\n-    def to_string(self, encoding=None):\n-        \"\"\"Get the body as a string. If an encoding is specified, the\n-        body will be encoded using that encoding.\n-        \"\"\"\n-        if encoding in (None, self.declared_encoding, self._actual_encoding):\n-            return self._content\n-        # should we cache this decode?\n-        return self.to_unicode().encode(encoding)\n-\n-    def to_unicode(self):\n-        \"\"\"Return body as unicode string\"\"\"\n-        if self._unicode_content:\n-            return self._unicode_content\n-        proposed = self.get_expected_encoding()\n-        dammit = UnicodeDammit(self._content, [proposed])\n-        self._actual_encoding = dammit.originalEncoding\n-        # FIXME sometimes dammit.unicode fails, even when it recognizes the encoding correctly\n-        self._unicode_content = dammit.unicode\n-        return self._unicode_content\n-\n-    def get_content(self):\n-        \"\"\"Return original content bytes\"\"\"\n-        return self._content\n-\n-    def get_declared_encoding(self):\n-        \"\"\"Get the value of the declared encoding passed to the\n-        constructor.\n-        \"\"\"\n-        return self.declared_encoding\n-\n-    def get_real_encoding(self):\n-        \"\"\"Get the real encoding, by trying first the expected and then the\n-        actual encoding.\n-        \"\"\"\n-        if self.get_expected_encoding():\n-            result = self.get_expected_encoding()\n-        else:\n-            self.to_unicode()\n-            result = self._actual_encoding\n-        return result\n-\n-    def get_expected_encoding(self):\n-        \"\"\"Get the expected encoding for the page. This is the declared\n-        encoding, or the meta tag encoding.\n-        \"\"\"\n-        if self._expected_encoding:\n-            return self._expected_encoding\n-        proposed = self.declared_encoding\n-        if not proposed:\n-            chunk = self._content[:5000]\n-            match = self.XMLDECL_RE.search(chunk) or self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)\n-            if match:\n-                proposed = match.group(\"charset\")\n-        self._expected_encoding = proposed\n-        return proposed\n-\n-    def __repr__(self):\n-        return \"_ResponseBody(content=%s, declared_encoding=%s)\" % (repr(self._content), repr(self.declared_encoding))\n-\n-    def __str__(self):\n-        return self.to_string()\n-\n-    def __unicode__(self):\n-        return self.to_unicode()\n-\n-    def __len__(self):\n-        return len(self._content)\n-\n-    def __eq__(self, other):\n-        return self._content == other._content and self.declared_encoding == other.declared_encoding\n-\n\n@@ -0,0 +1,79 @@\n+\"\"\"\n+This module implements the Response class which is used to represent HTTP\n+responses in Scrapy.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n+import copy\n+\n+from twisted.web.http import RESPONSES\n+\n+from scrapy.http.url import Url\n+from scrapy.http.headers import Headers\n+\n+class Response(object):\n+\n+    def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):\n+        self.url = Url(url)\n+        self.headers = Headers(headers or {})\n+        self.status = status\n+        self.set_body(body)\n+        self.cached = False\n+        self.request = None\n+        self.meta = {} if meta is None else dict(meta)\n+        self.flags = [] if flags is None else list(flags)\n+        self.cache = {}\n+\n+    def set_body(self, body):\n+        if isinstance(body, str):\n+            self._body = body\n+        elif isinstance(body, unicode):\n+            raise TypeError(\"Cannot assign a unicode body to a raw Response. Use TextResponse, HtmlResponse, etc\")\n+        elif body is None:\n+            self._body = ''\n+        else:\n+            raise TypeError(\"Response body must either str or unicode. Got: '%s'\" % type(body).__name__)\n+    body = property(lambda x: x._body, set_body)\n+\n+    def __repr__(self):\n+        return \"%s(url=%s, headers=%s, status=%s, body=%s)\" % \\\n+                (type(self).__name__, repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n+\n+    def __str__(self):\n+        flags = \"(%s) \" % \",\".join(self.flags) if self.flags else \"\"\n+        status = \"%d \" % self.status + \" \" if self.status != 200 else \"\"\n+        return \"%s<%s%s>\" % (flags, status, self.url)\n+\n+    def copy(self):\n+        \"\"\"Return a copy of this Response\"\"\"\n+        return self.replace()\n+\n+    def replace(self, url=None, status=None, headers=None, body=None, meta=None, flags=None, cls=None, **kwargs):\n+        \"\"\"Create a new Response with the same attributes except for those\n+        given new values.\n+        \"\"\"\n+        if cls is None:\n+            cls = self.__class__\n+        new = cls(url=url or self.url,\n+                  status=status or self.status,\n+                  headers=headers or copy.deepcopy(self.headers),\n+                  body=body or self.body,\n+                  meta=meta or self.meta,\n+                  flags=flags or self.flags,\n+                  **kwargs)\n+        return new\n+\n+    def httprepr(self):\n+        \"\"\"\n+        Return raw HTTP response representation (as string). This is provided\n+        only for reference, since it's not the exact stream of bytes that was\n+        received (that's not exposed by Twisted).\n+        \"\"\"\n+\n+        s  = \"HTTP/1.1 %s %s\\r\\n\" % (self.status, RESPONSES[self.status])\n+        if self.headers:\n+            s += self.headers.to_string() + \"\\r\\n\"\n+        s += \"\\r\\n\"\n+        s += self.body\n+        return s\n\n@@ -0,0 +1,35 @@\n+\"\"\"\n+This module implements the HtmlResponse class which adds encoding\n+discovering through HTML encoding declarations to the TextResponse class.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n+import re\n+\n+from BeautifulSoup import UnicodeDammit\n+\n+from scrapy.http.response.text import TextResponse\n+from scrapy.utils.python import memoizemethod\n+\n+class HtmlResponse(TextResponse):\n+\n+    _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n+\n+    _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n+    _content_re   = _template % ('content', r'(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)')\n+    _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n+\n+    METATAG_RE  = re.compile(r'<meta\\s+%s\\s+%s' % (_httpequiv_re, _content_re), re.I)\n+    METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n+\n+    def body_encoding(self):\n+        return self._body_declared_encoding() or self._body_inferred_encoding()\n+\n+    @memoizemethod('cache')\n+    def _body_declared_encoding(self):\n+        chunk = self.body[:5000]\n+        match = self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)\n+        return match.group('charset') if match else None\n+\n+\n\n@@ -0,0 +1,75 @@\n+\"\"\"\n+This module implements the TextResponse class which adds encoding handling and\n+discovering (through HTTP headers) to base Response class.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n+import re\n+\n+from BeautifulSoup import UnicodeDammit\n+\n+from scrapy.http.response import Response\n+from scrapy.utils.python import memoizemethod\n+\n+class TextResponse(Response):\n+\n+    _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n+\n+    def __init__(self, url, status=200, headers=None, body=None, meta=None, flags=None, encoding=None):\n+        self._encoding = encoding\n+        if isinstance(body, unicode):\n+            if encoding is None:\n+                clsname = self.__class__.__name__\n+                raise TypeError(\"To instantiate a %s with unicode body you must specify the encoding\" % clsname)\n+            body = body.encode(encoding)\n+        Response.__init__(self, url, status, headers, body, meta, flags)\n+\n+    def set_body(self, body):\n+        if isinstance(body, str):\n+            self._body = body\n+        elif isinstance(body, unicode):\n+            self._body = body.encode(self._encoding)\n+        elif body is None:\n+            self._body = None\n+        else:\n+            raise TypeError(\"Request body must either str, unicode or None. Got: '%s'\" % type(body).__name__)\n+    body = property(lambda x: x._body, set_body)\n+\n+    def replace(self, *args, **kwargs):\n+        kwargs.setdefault('encoding', self.encoding)\n+        return Response.replace(self, *args, **kwargs)\n+\n+    @property\n+    def encoding(self):\n+        return self._encoding or self.headers_encoding() or self.body_encoding()\n+\n+    @memoizemethod('cache')\n+    def headers_encoding(self, headers=None):\n+        if headers is None:\n+            headers = self.headers\n+        content_type = headers.get('Content-Type')\n+        if content_type:\n+            encoding = self._ENCODING_RE.search(content_type[0])\n+            if encoding:\n+                return encoding.group(1)\n+\n+    @memoizemethod('cache')\n+    def body_as_unicode(self):\n+        \"\"\"Return body as unicode\"\"\"\n+        possible_encodings = (self._encoding, self.headers_encoding(), self._body_declared_encoding())\n+        dammit = UnicodeDammit(self.body, possible_encodings)\n+        self.cache['body_inferred_encoding'] = dammit.originalEncoding\n+        # XXX: sometimes dammit.unicode fails, even when it recognizes the encoding correctly\n+        return dammit.unicode\n+\n+    def body_encoding(self):\n+        return self._body_inferred_encoding()\n+\n+    def _body_inferred_encoding(self):\n+        if 'body_inferred_encoding' not in self.cache:\n+            self.body_as_unicode()\n+        return self.cache['body_inferred_encoding']\n+\n+    def _body_declared_encoding(self):\n+        return None\n\n@@ -0,0 +1,29 @@\n+\"\"\"\n+This module implements the XmlResponse class which adds encoding\n+discovering through XML encoding declarations to the TextResponse class.\n+\n+See documentation in docs/ref/request-response.rst\n+\"\"\"\n+\n+import re\n+\n+from BeautifulSoup import UnicodeDammit\n+\n+from scrapy.http.response.text import TextResponse\n+from scrapy.utils.python import memoizemethod\n+\n+class XmlResponse(TextResponse):\n+\n+    _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n+    _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n+    XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n+\n+    def body_encoding(self):\n+        return self._body_declared_encoding() or self._body_inferred_encoding()\n+\n+    @memoizemethod('cache')\n+    def _body_declared_encoding(self):\n+        chunk = self.body[:5000]\n+        match = self.XMLDECL_RE.search(chunk)\n+        return match.group('charset') if match else None\n+\n\n@@ -58,9 +58,7 @@ class LinkExtractor(FixedSGMLParser):\n \n     def extract_links(self, response):\n         # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body.to_string(),\n-            response.url,\n-            response.body.get_real_encoding())\n+        return self._extract_links(response.body, response.url, response.encoding)\n \n     def reset(self):\n         FixedSGMLParser.reset(self)\n\n@@ -55,7 +55,7 @@ class RegexLinkExtractor(LinkExtractor):\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n             html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.x(xpath_expr).extract()) for xpath_expr in self.restrict_xpaths)\n-            links = self._extract_links(html_slice, response.url, response.body.get_real_encoding())\n+            links = self._extract_links(html_slice, response.url, response.encoding)\n         else:\n             links = LinkExtractor.extract_links(self, response)\n \n\n@@ -133,7 +133,7 @@ class Replay(object):\n             self.passed_new[str(item.guid)] = item\n \n     def response_received(self, response, spider):\n-        key = hashlib.sha1(response.body.to_string()).hexdigest()\n+        key = hashlib.sha1(response.body).hexdigest()\n         if (self.recording or self.updating) and key:\n             self.responses_old[key] = response.copy()\n         elif key:\n\n@@ -5,7 +5,7 @@ import re\n \n from scrapy.item.adaptors import AdaptorPipe\n from scrapy.contrib_exp import adaptors\n-from scrapy.http import Response, Headers\n+from scrapy.http import HtmlResponse, Headers\n from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n from scrapy.link import Link\n \n@@ -43,7 +43,7 @@ class AdaptorsTestCase(unittest.TestCase):\n     def get_selector(self, domain, url, sample_filename, headers=None, selector=HtmlXPathSelector):\n         sample_filename = os.path.join(self.samplesdir, sample_filename)\n         body = file(sample_filename).read()\n-        response = Response(url=url, headers=Headers(headers), status=200, body=body)\n+        response = HtmlResponse(url=url, headers=Headers(headers), status=200, body=body)\n         return selector(response)\n \n     def test_extract(self):\n@@ -125,7 +125,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                            <a onclick=\"javascript: opensomething('dummy/my_html2.html');\">something2</a>\n                          </div>\n                        </body></html>\"\"\"\n-        sample_response = Response('http://foobar.com/dummy', body=test_data)\n+        sample_response = HtmlResponse('http://foobar.com/dummy', body=test_data)\n         sample_selector = HtmlXPathSelector(sample_response)\n         sample_adaptor = adaptors.ExtractImageLinks(response=sample_response)\n \n\n@@ -23,7 +23,7 @@ class ResponseSoupTest(unittest.TestCase):\n \n         # when body is None, an empty soup should be returned\n         r1 = Response('http://www.example.com')\n-        assert r1.body is None\n+        assert r1.body == \"\"\n         assert isinstance(r1.getsoup(), BeautifulSoup)\n \n     def test_response_soup_caching(self):\n\n@@ -1,7 +1,7 @@\n import os\n from unittest import TestCase, main\n-from scrapy.http import Response\n-from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n+from scrapy.http import Response, XmlResponse\n+from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n \n def setUp():\n     datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n@@ -25,19 +25,23 @@ class ScrapyDecompressionTest(TestCase):\n \n     def test_tar(self):\n         response, format = self.middleware.extract(self.test_responses['tar'])\n-        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n+        assert isinstance(response, XmlResponse)\n+        self.assertEqual(response.body, self.uncompressed_body)\n \n     def test_zip(self):\n         response, format = self.middleware.extract(self.test_responses['zip'])\n-        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n+        assert isinstance(response, XmlResponse)\n+        self.assertEqual(response.body, self.uncompressed_body)\n \n     def test_gz(self):\n         response, format = self.middleware.extract(self.test_responses['xml.gz'])\n-        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n+        assert isinstance(response, XmlResponse)\n+        self.assertEqual(response.body, self.uncompressed_body)\n \n     def test_bz2(self):\n         response, format = self.middleware.extract(self.test_responses['xml.bz2'])\n-        self.assertEqual(response.body.to_string(), self.uncompressed_body)\n+        assert isinstance(response, XmlResponse)\n+        self.assertEqual(response.body, self.uncompressed_body)\n \n if __name__ == '__main__':\n     main()\n\n\n@@ -0,0 +1,27 @@\n+import unittest\n+from scrapy.core.scheduler import GroupFilter\n+\n+class GroupFilterTest(unittest.TestCase):\n+\n+    def test_groupfilter(self):\n+        k1 = \"id1\"\n+        k2 = \"id1\"\n+\n+        f = GroupFilter()\n+        f.open(\"mygroup\")\n+        self.assertTrue(f.add(\"mygroup\", k1))\n+        self.assertFalse(f.add(\"mygroup\", k1))\n+        self.assertFalse(f.add(\"mygroup\", k2))\n+\n+        f.open('anothergroup')\n+        self.assertTrue(f.add(\"anothergroup\", k1))\n+        self.assertFalse(f.add(\"anothergroup\", k1))\n+        self.assertFalse(f.add(\"anothergroup\", k2))\n+\n+        f.close('mygroup')\n+        f.open('mygroup')\n+        self.assertTrue(f.add(\"mygroup\", k2))\n+        self.assertFalse(f.add(\"mygroup\", k1))\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -1,6 +1,5 @@\n import unittest\n-from scrapy.http import Request, Headers, Url\n-from scrapy.core.scheduler import GroupFilter\n+from scrapy.http import Request, FormRequest, Headers, Url\n \n class RequestTest(unittest.TestCase):\n \n@@ -30,26 +29,6 @@ class RequestTest(unittest.TestCase):\n         assert r.headers is not headers\n         self.assertEqual(r.headers[\"caca\"], \"coco\")\n \n-    def test_groupfilter(self):\n-        k1 = \"id1\"\n-        k2 = \"id1\"\n-\n-        f = GroupFilter()\n-        f.open(\"mygroup\")\n-        self.assertTrue(f.add(\"mygroup\", k1))\n-        self.assertFalse(f.add(\"mygroup\", k1))\n-        self.assertFalse(f.add(\"mygroup\", k2))\n-\n-        f.open('anothergroup')\n-        self.assertTrue(f.add(\"anothergroup\", k1))\n-        self.assertFalse(f.add(\"anothergroup\", k1))\n-        self.assertFalse(f.add(\"anothergroup\", k2))\n-\n-        f.close('mygroup')\n-        f.open('mygroup')\n-        self.assertTrue(f.add(\"mygroup\", k2))\n-        self.assertFalse(f.add(\"mygroup\", k1))\n-\n     def test_headers(self):\n         # Different ways of setting headers attribute\n         url = 'http://www.scrapy.org'\n@@ -108,7 +87,7 @@ class RequestTest(unittest.TestCase):\n \n     def test_body(self):\n         r1 = Request(url=\"http://www.example.com/\")\n-        assert r1.body is None\n+        assert r1.body == ''\n \n         r2 = Request(url=\"http://www.example.com/\", body=\"\")\n         assert isinstance(r2.body, str)\n@@ -166,7 +145,7 @@ class RequestTest(unittest.TestCase):\n         r2 = r1.replace(method=\"POST\", body=\"New body\", headers=hdrs)\n         self.assertEqual(r1.url, r2.url)\n         self.assertEqual((r1.method, r2.method), (\"GET\", \"POST\"))\n-        self.assertEqual((r1.body, r2.body), (None, \"New body\"))\n+        self.assertEqual((r1.body, r2.body), ('', \"New body\"))\n         self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n \n     def test_httprepr(self):\n@@ -174,7 +153,26 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r1.httprepr(), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n \n         r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n+        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+\n+    def test_form_request(self):\n+\n+        # empty formdata\n+        r1 = FormRequest(\"http://www.example.com\", formdata={})\n+        self.assertEqual(r1.body, '')\n+\n+        # using default encoding (utf-8)\n+        data = {'one': 'two', 'price': '\\xc2\\xa3 100'}\n+        r2 = FormRequest(\"http://www.example.com\", formdata=data)\n+        self.assertEqual(r2.encoding, 'utf-8')\n+        self.assertEqual(r2.body, 'price=%C2%A3+100&one=two')\n+        self.assertEqual(r2.headers['Content-Type'], 'application/x-www-form-urlencoded')\n+\n+        # using custom encoding\n+        data = {'price': u'\\xa3 100'}\n+        r3 = FormRequest(\"http://www.example.com\", formdata=data, encoding='latin1')\n+        self.assertEqual(r3.encoding, 'latin1')\n+        self.assertEqual(r3.body, 'price=%A3+100')\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,6 +1,5 @@\n import unittest\n-from scrapy.http import Response, Headers, Url\n-from scrapy.http.response import _ResponseBody\n+from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers, Url\n \n class ResponseTest(unittest.TestCase):\n \n@@ -9,10 +8,10 @@ class ResponseTest(unittest.TestCase):\n         self.assertRaises(Exception, Response)\n         self.assertTrue(isinstance(Response('http://example.com/'), Response))\n         # body can be str or None but not ResponseBody\n-        self.assertTrue(isinstance(Response('http://example.com/', body=None), Response))\n+        self.assertTrue(isinstance(Response('http://example.com/', body=''), Response))\n         self.assertTrue(isinstance(Response('http://example.com/', body='body'), Response))\n         # test presence of all optional parameters\n-        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=None), Response))\n+        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=''), Response))\n \n         r = Response(\"http://www.example.com\")\n         assert isinstance(r.url, Url)\n@@ -38,9 +37,13 @@ class ResponseTest(unittest.TestCase):\n         \n         r1 = Response(\"http://www.example.com\", body=\"Some body\")\n         r1.meta['foo'] = 'bar'\n+        r1.flags.append('cached')\n         r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n \n+        self.assertEqual(r1.status, r2.status)\n+        self.assertEqual(r1.body, r2.body)\n+\n         assert r1.cache\n         assert not r2.cache\n \n@@ -48,14 +51,14 @@ class ResponseTest(unittest.TestCase):\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n \n+        # make sure flags list is shallow copied\n+        assert r1.flags is not r2.flags, \"flags must be a shallow copy, not identical\"\n+        self.assertEqual(r1.flags, r2.flags)\n+\n         # make sure headers attribute is shallow copied\n         assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n         self.assertEqual(r1.headers, r2.headers)\n \n-        # make sure body is shallow copied\n-        assert r1.body is not r2.body, \"body must be a shallow copy, not identical\"\n-        self.assertEqual(r1.body, r2.body)\n-\n     def test_copy_inherited_classes(self):\n         \"\"\"Test Response children copies preserve their class\"\"\"\n \n@@ -72,38 +75,116 @@ class ResponseTest(unittest.TestCase):\n         hdrs = Headers({\"key\": \"value\"})\n         r1 = Response(\"http://www.example.com\")\n         r2 = r1.replace(status=301, body=\"New body\", headers=hdrs)\n-        assert r1.body is None\n-        assert isinstance(r2.body, _ResponseBody)\n+        assert r1.body == ''\n         self.assertEqual(r1.url, r2.url)\n         self.assertEqual((r1.status, r2.status), (200, 301))\n-        self.assertEqual((r1.body, r2.body.to_string()), (None, \"New body\"))\n+        self.assertEqual((r1.body, r2.body), ('', \"New body\"))\n         self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n \n+        r1 = TextResponse(\"http://www.example.com\", body=\"hello\", encoding=\"cp852\")\n+        r2 = r1.replace(url=\"http://www.example.com/other\")\n+        r3 = r1.replace(url=\"http://www.example.com/other\", encoding=\"latin1\")\n+\n+        assert isinstance(r2, TextResponse)\n+        self.assertEqual(r2.url, \"http://www.example.com/other\")\n+        self.assertEqual(r2.encoding, \"cp852\")\n+        self.assertEqual(r3.url, \"http://www.example.com/other\")\n+        self.assertEqual(r3.encoding, \"latin1\")\n+\n     def test_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(r1.httprepr(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n \n         r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n-        \n-class ResponseBodyTest(unittest.TestCase):\n-    unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n+        self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n \n     def test_encoding(self):\n-        original_string = self.unicode_string.encode('cp1251')\n-        cp1251_body     = _ResponseBody(original_string, 'cp1251')\n+        unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n+        self.assertRaises(TypeError, Response, 'http://www.example.com', body=u'unicode body')\n \n-        # check to_unicode\n-        self.assertTrue(isinstance(cp1251_body.to_unicode(), unicode))\n-        self.assertEqual(cp1251_body.to_unicode(), self.unicode_string)\n+        original_string = unicode_string.encode('cp1251')\n+        r1 = TextResponse('http://www.example.com', body=original_string, encoding='cp1251')\n \n-        # check to_string using default encoding (declared when created)\n-        self.assertTrue(isinstance(cp1251_body.to_string(), str))\n-        self.assertEqual(cp1251_body.to_string(), original_string)\n+        # check body_as_unicode\n+        self.assertTrue(isinstance(r1.body_as_unicode(), unicode))\n+        self.assertEqual(r1.body_as_unicode(), unicode_string)\n+\n+    def _assert_response_values(self, response, encoding, body):\n+        if isinstance(body, unicode):\n+            body_unicode = body\n+            body_str = body.encode(encoding)\n+        else:\n+            body_unicode = body.decode(encoding)\n+            body_str = body\n+\n+        assert isinstance(response.body, str)\n+        self.assertEqual(response.encoding, encoding)\n+        self.assertEqual(response.body, body_str)\n+        self.assertEqual(response.body_as_unicode(), body_unicode)\n+\n+    def test_text_response(self):\n+        r1 = TextResponse(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]}, body=\"\\xc2\\xa3\")\n+        r2 = TextResponse(\"http://www.example.com\", encoding='utf-8', body=u\"\\xa3\")\n+        r3 = TextResponse(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=\"\\xa3\")\n+        r4 = TextResponse(\"http://www.example.com\", body=\"\\xa2\\xa3\")\n+\n+        self.assertEqual(r1.headers_encoding(), \"utf-8\")\n+        self.assertEqual(r2.headers_encoding(), None)\n+        self.assertEqual(r2.encoding, 'utf-8')\n+        self.assertEqual(r3.headers_encoding(), \"iso-8859-1\")\n+        self.assertEqual(r3.encoding, 'iso-8859-1')\n+        self.assertEqual(r4.headers_encoding(), None)\n+        assert r4.body_encoding() is not None and r4.body_encoding() != 'ascii'\n+        self._assert_response_values(r1, 'utf-8', u\"\\xa3\")\n+        self._assert_response_values(r2, 'utf-8', u\"\\xa3\")\n+        self._assert_response_values(r3, 'iso-8859-1', u\"\\xa3\")\n+\n+        # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n+        self.assertRaises(TypeError, TextResponse, \"http://www.example.com\", body=u\"\\xa3\")\n+\n+    def test_html_encoding(self):\n+        \n+        body = \"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n+        </head><body>Price: \\xa3100</body></html>'\n+        \"\"\"\n+        r1 = HtmlResponse(\"http://www.example.com\", body=body)\n+        self._assert_response_values(r1, 'iso-8859-1', body)\n+\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\n+        <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n+        Price: \\xa3100\n+        \"\"\"\n+        r2 = HtmlResponse(\"http://www.example.com\", body=body)\n+        self._assert_response_values(r2, 'iso-8859-1', body)\n+\n+        # for conflicting declarations headers must take precedence\n+        body = \"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n+        </head><body>Price: \\xa3100</body></html>'\n+        \"\"\"\n+        r3 = HtmlResponse(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=body)\n+        self._assert_response_values(r3, 'iso-8859-1', body)\n+\n+        # make sure replace() preserves the encoding of the original response\n+        body = \"New body \\xa3\"\n+        r4 = r3.replace(body=body)\n+        self._assert_response_values(r4, 'iso-8859-1', body)\n+\n+    def test_xml_encoding(self):\n+\n+        body = \"<xml></xml>\"\n+        r1 = XmlResponse(\"http://www.example.com\", body=body)\n+        # XXX: we may want to swtich default XmlResponse encoding to utf-8\n+        self._assert_response_values(r1, 'ascii', body)\n+\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n+        r2 = XmlResponse(\"http://www.example.com\", body=body)\n+        self._assert_response_values(r2, 'iso-8859-1', body)\n+\n+        # make sure replace() preserves the encoding of the original response\n+        body = \"New body \\xa3\"\n+        r3 = r2.replace(body=body)\n+        self._assert_response_values(r3, 'iso-8859-1', body)\n \n-        # check to_string using arbitrary encoding\n-        self.assertTrue(isinstance(cp1251_body.to_string('utf-8'), str))\n-        self.assertEqual(cp1251_body.to_string('utf-8'), self.unicode_string.encode('utf-8'))\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,16 +1,13 @@\n-from unittest import TestCase, main\n+import unittest\n+\n import libxml2\n-from scrapy.http import Response\n \n-class Libxml2Test(TestCase):\n-    def setUp(self):\n-        libxml2.debugMemory(1)\n+from scrapy.http import TextResponse\n+from scrapy.utils.test import libxml2debug\n \n-    def tearDown(self):\n-        libxml2.cleanupParser()\n-        leaked_bytes = libxml2.debugMemory(0)\n-        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n+class Libxml2Test(unittest.TestCase):\n \n+    @libxml2debug\n     def test_xpath(self):\n         #this test will fail in version 2.6.27 but passes on 2.6.29+\n         html = \"<td>1<b>2</b>3</td>\"\n@@ -19,25 +16,21 @@ class Libxml2Test(TestCase):\n         self.assertEquals(result, ['1', '2', '3'])\n         node.freeDoc()\n \n-class ResponseLibxml2DocTest(TestCase):\n-    def setUp(self):\n-        libxml2.debugMemory(1)\n-\n-    def tearDown(self):\n-        libxml2.cleanupParser()\n-        leaked_bytes = libxml2.debugMemory(0)\n-        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n+class ResponseLibxml2DocTest(unittest.TestCase):\n \n+    @libxml2debug\n     def test_getlibxml2doc(self):\n         # test to simulate '\\x00' char in body of html page\n-        #this method don't should raise TypeError Exception\n-        from scrapy.core.manager import scrapymanager\n-        scrapymanager.configure()\n+        #this method shouldn't raise TypeError Exception\n+\n+        # make sure we load the libxml2 extension\n+        from scrapy.extension import extensions\n+        extensions.load() # \n \n         self.body_content = 'test problematic \\x00 body'\n-        response = Response('http://example.com/catalog/product/blabla-123',\n+        response = TextResponse('http://example.com/catalog/product/blabla-123',\n                             headers={'Content-Type': 'text/plain; charset=utf-8'}, body=self.body_content)\n         response.getlibxml2doc()\n \n if __name__ == \"__main__\":\n-    main()\n+    unittest.main()\n\n@@ -1,7 +1,7 @@\n import os\n import unittest\n \n-from scrapy.http.response import Response\n+from scrapy.http import HtmlResponse\n from scrapy.link import LinkExtractor, Link\n from scrapy.link.extractors import RegexLinkExtractor\n from scrapy.contrib.link_extractors import HTMLImageLinkExtractor\n@@ -15,7 +15,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         <p><a href=\"../othercat.html\">Other category</a></p>\n         <p><a href=\"/\" /></p>\n         </body></html>\"\"\"\n-        response = Response(\"http://example.org/somepage/index.html\", body=html)\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n         self.assertEqual(lx.extract_links(response),\n@@ -28,7 +28,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n         <body><p><a href=\"item/12.html\">Item 12</a></p>\n         </body></html>\"\"\"\n-        response = Response(\"http://example.org/somepage/index.html\", body=html)\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n         self.assertEqual(lx.extract_links(response),\n@@ -37,10 +37,10 @@ class LinkExtractorTestCase(unittest.TestCase):\n     def test_extraction_encoding(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'linkextractor_noenc.html'), 'r').read()\n-        response_utf8 = Response(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n-        response_noenc = Response(url='http://example.com/noenc', body=body)\n+        response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n+        response_noenc = HtmlResponse(url='http://example.com/noenc', body=body)\n         body = open(os.path.join(base_path, 'linkextractor_latin1.html'), 'r').read()\n-        response_latin1 = Response(url='http://example.com/latin1', body=body)\n+        response_latin1 = HtmlResponse(url='http://example.com/latin1', body=body)\n \n         lx = LinkExtractor()\n         self.assertEqual(lx.extract_links(response_utf8),\n@@ -67,7 +67,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'regex_linkextractor.html'), 'r').read()\n-        self.response = Response(url='http://example.com/index', body=body)\n+        self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def test_urls_type(self):\n         '''Test that the resulting urls are regular strings and not a unicode objects'''\n@@ -146,7 +146,7 @@ class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'image_linkextractor.html'), 'r').read()\n-        self.response = Response(url='http://example.com/index', body=body)\n+        self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def tearDown(self):\n         del self.response\n\n@@ -28,11 +28,11 @@ class TestSpider(BaseSpider):\n \n     def parse_item(self, response):\n         item = ScrapedItem()\n-        m = self.name_re.search(response.body.to_string())\n+        m = self.name_re.search(response.body)\n         if m:\n             item.name = m.group(1)\n         item.url = response.url\n-        m = self.price_re.search(response.body.to_string())\n+        m = self.price_re.search(response.body)\n         if m:\n             item.price = m.group(1)\n         return [item]\n\n@@ -3,10 +3,12 @@ import unittest\n import libxml2\n \n from scrapy.utils.iterators import csviter, xmliter\n-from scrapy.http import Response\n+from scrapy.http import XmlResponse, TextResponse\n \n class UtilsXmlTestCase(unittest.TestCase):\n     ### NOTE: Encoding issues have been found with BeautifulSoup for utf-16 files, utf-16 test removed ###\n+    # pablo: Tests shouldn't be removed, but commented with proper steps on how\n+    # to reproduce the missing functionality\n     def test_iterator(self):\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n             <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\\\n@@ -20,7 +22,7 @@ class UtilsXmlTestCase(unittest.TestCase):\n               </product>\\\n             </products>\"\"\"\n \n-        response = Response(url=\"http://example.com\", body=body)\n+        response = XmlResponse(url=\"http://example.com\", body=body)\n         attrs = []\n         for x in xmliter(response, 'product'):\n             attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n@@ -53,7 +55,7 @@ class UtilsXmlTestCase(unittest.TestCase):\n                 </channel>\n             </rss>\n         \"\"\"\n-        response = Response(url='http://mydummycompany.com', body=body)\n+        response = XmlResponse(url='http://mydummycompany.com', body=body)\n         my_iter = xmliter(response, 'item')\n \n         node = my_iter.next()\n@@ -83,7 +85,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     def test_iterator_defaults(self):\n         body = open(self.sample_feed_path).read()\n \n-        response = Response(url=\"http://example.com/\", body=body)\n+        response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n         result = [row for row in csv]\n@@ -101,7 +103,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     def test_iterator_delimiter(self):\n         body = open(self.sample_feed_path).read().replace(',', '\\t')\n \n-        response = Response(url=\"http://example.com/\", body=body)\n+        response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response, delimiter='\\t')\n \n         self.assertEqual([row for row in csv],\n@@ -114,7 +116,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         sample = open(self.sample_feed_path).read().splitlines()\n         headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n \n-        response = Response(url=\"http://example.com/\", body=body)\n+        response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response, headers=headers)\n \n         self.assertEqual([row for row in csv],\n@@ -127,7 +129,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         body = open(self.sample_feed_path).read()\n         body = '\\n'.join((body, 'a,b', 'a,b,c,d'))\n \n-        response = Response(url=\"http://example.com/\", body=body)\n+        response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n         self.assertEqual([row for row in csv],\n@@ -139,7 +141,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     def test_iterator_exception(self):\n         body = open(self.sample_feed_path).read()\n \n-        response = Response(url=\"http://example.com/\", body=body)\n+        response = TextResponse(url=\"http://example.com/\", body=body)\n         iter = csviter(response)\n         iter.next()\n         iter.next()\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.utils.python import str_to_unicode, unicode_to_str\n+from scrapy.utils.python import str_to_unicode, unicode_to_str, memoizemethod, isbinarytext\n \n class UtilsPythonTestCase(unittest.TestCase):\n     def test_str_to_unicode(self):\n@@ -29,5 +29,37 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # converting a strange object should raise TypeError\n         self.assertRaises(TypeError, unicode_to_str, unittest)\n \n+    def test_memoizemethod(self):\n+        class A(object):\n+            def __init__(self):\n+                self.cache = {}\n+\n+            @memoizemethod('cache')\n+            def heavyfunc(self, arg1=None, arg2=None):\n+                return [arg1, arg2]\n+\n+        a = A()\n+        one = a.heavyfunc()\n+        two = a.heavyfunc()\n+        three = a.heavyfunc('two')\n+        four = a.heavyfunc('two')\n+        assert one is two\n+        assert one is not three\n+        assert three is four\n+\n+    def test_isbinarytext(self):\n+\n+        # basic tests\n+        assert not isbinarytext(\"hello\")\n+\n+        # utf-16 strings contain null bytes\n+        assert not isbinarytext(u\"hello\".encode('utf-16')) \n+\n+        # one with encoding\n+        assert not isbinarytext(\"<div>Price \\xa3</div>\")\n+\n+        # finally some real binary bytes\n+        assert isbinarytext(\"\\x02\\xa3\")\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,9 +1,9 @@\n import unittest\n-from scrapy.http.response import Response\n+from scrapy.http import Response, TextResponse\n from scrapy.utils.response import body_or_str, get_base_url\n \n class ResponseUtilsTest(unittest.TestCase):\n-    dummy_response = Response(url='http://example.org/', body='dummy_response')\n+    dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n \n     def test_body_or_str_input(self):\n         self.assertTrue(isinstance(body_or_str(self.dummy_response), basestring))\n\n@@ -3,23 +3,17 @@ import unittest\n \n import libxml2\n \n-from scrapy.http import Response\n+from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.utils.test import libxml2debug\n \n class XPathTestCase(unittest.TestCase):\n \n-    def setUp(self):\n-        libxml2.debugMemory(1)\n-\n-    def tearDown(self):\n-        libxml2.cleanupParser()\n-        leaked_bytes = libxml2.debugMemory(0) \n-        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n-\n+    @libxml2debug\n     def test_selector_simple(self):\n         \"\"\"Simple selector tests\"\"\"\n         body = \"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n-        response = Response(url=\"http://example.com\", body=body)\n+        response = TextResponse(url=\"http://example.com\", body=body)\n         xpath = HtmlXPathSelector(response)\n \n         xl = xpath.x('//input')\n@@ -40,6 +34,7 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual([x.extract() for x in xpath.x(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n                          [u'12'])\n \n+    @libxml2debug\n     def test_selector_same_type(self):\n         \"\"\"Test XPathSelector returning the same type in x() method\"\"\"\n         text = '<p>test<p>'\n@@ -48,6 +43,7 @@ class XPathTestCase(unittest.TestCase):\n         assert isinstance(HtmlXPathSelector(text=text).x(\"//p\")[0], \n                           HtmlXPathSelector)\n \n+    @libxml2debug\n     def test_selector_xml_html(self):\n         \"\"\"Test that XML and HTML XPathSelector's behave differently\"\"\"\n \n@@ -60,6 +56,7 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(HtmlXPathSelector(text=text).x(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n \n+    @libxml2debug\n     def test_selector_nested(self):\n         \"\"\"Nested selector tests\"\"\"\n         body = \"\"\"<body>\n@@ -75,7 +72,7 @@ class XPathTestCase(unittest.TestCase):\n                     </div>\n                   </body>\"\"\"\n \n-        response = Response(url=\"http://example.com\", body=body)\n+        response = HtmlResponse(url=\"http://example.com\", body=body)\n         x = HtmlXPathSelector(response)\n \n         divtwo = x.x('//div[@class=\"two\"]')\n@@ -88,6 +85,7 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(divtwo.x(\"./li\").extract(),\n                          [])\n \n+    @libxml2debug\n     def test_selector_re(self):\n         body = \"\"\"<div>Name: Mary\n                     <ul>\n@@ -100,7 +98,7 @@ class XPathTestCase(unittest.TestCase):\n                   </div>\n \n                \"\"\"\n-        response = Response(url=\"http://example.com\", body=body)\n+        response = HtmlResponse(url=\"http://example.com\", body=body)\n         x = HtmlXPathSelector(response)\n \n         name_re = re.compile(\"Name: (\\w+)\")\n@@ -109,6 +107,7 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(x.x(\"//ul/li\").re(\"Age: (\\d+)\"),\n                          [\"10\", \"20\"])\n \n+    @libxml2debug\n     def test_selector_over_text(self):\n         hxs = HtmlXPathSelector(text='<root>lala</root>')\n         self.assertEqual(hxs.extract(),\n@@ -123,6 +122,7 @@ class XPathTestCase(unittest.TestCase):\n                          [u'<root>lala</root>'])\n \n \n+    @libxml2debug\n     def test_selector_namespaces_simple(self):\n         body = \"\"\"\n         <test xmlns:somens=\"http://scrapy.org\">\n@@ -131,7 +131,7 @@ class XPathTestCase(unittest.TestCase):\n         </test>\n         \"\"\"\n \n-        response = Response(url=\"http://example.com\", body=body)\n+        response = XmlResponse(url=\"http://example.com\", body=body)\n         x = XmlXPathSelector(response)\n         \n         x.register_namespace(\"somens\", \"http://scrapy.org\")\n@@ -139,6 +139,7 @@ class XPathTestCase(unittest.TestCase):\n                          ['<somens:a id=\"foo\"/>'])\n \n \n+    @libxml2debug\n     def test_selector_namespaces_multiple(self):\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <BrowseNode xmlns=\"http://webservices.amazon.com/AWSECommerceService/2005-10-05\"\n@@ -149,7 +150,7 @@ class XPathTestCase(unittest.TestCase):\n     <p:SecondTestTag><material/><price>90</price><p:name>Dried Rose</p:name></p:SecondTestTag>\n </BrowseNode>\n         \"\"\"\n-        response = Response(url=\"http://example.com\", body=body)\n+        response = XmlResponse(url=\"http://example.com\", body=body)\n         x = XmlXPathSelector(response)\n \n         x.register_namespace(\"xmlns\", \"http://webservices.amazon.com/AWSECommerceService/2005-10-05\")\n@@ -162,6 +163,7 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(x.x(\"//p:SecondTestTag\").x(\"./xmlns:price/text()\")[0].extract(), '90')\n         self.assertEqual(x.x(\"//p:SecondTestTag/xmlns:material\").extract()[0], '<material/>')\n \n+    @libxml2debug\n     def test_http_header_encoding_precedence(self):\n         # u'\\xa3'     = pound symbol in unicode\n         # u'\\xc2\\xa3' = pound symbol in utf-8\n@@ -176,11 +178,12 @@ class XPathTestCase(unittest.TestCase):\n         html_utf8 = html.encode(encoding)\n \n         headers = {'Content-Type': ['text/html; charset=utf-8']}\n-        response = Response(url=\"http://example.com\", headers=headers, body=html_utf8)\n+        response = HtmlResponse(url=\"http://example.com\", headers=headers, body=html_utf8)\n         x = HtmlXPathSelector(response)\n         self.assertEquals(x.x(\"//span[@id='blank']/text()\").extract(),\n                           [u'\\xa3'])\n \n+    @libxml2debug\n     def test_null_bytes(self):\n         hxs = HtmlXPathSelector(text='<root>la\\x00la</root>')\n         self.assertEqual(hxs.extract(),\n@@ -190,6 +193,7 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(xxs.extract(),\n                          u'<root>lala</root>')\n \n+    @libxml2debug\n     def test_unquote(self):\n         xmldoc = '\\n'.join((\n             '<root>',\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.http import Response\n+from scrapy.http import HtmlResponse\n from scrapy.xpath.extension import ResponseLibxml2\n \n class ResponseLibxml2Test(unittest.TestCase):\n@@ -9,7 +9,7 @@ class ResponseLibxml2Test(unittest.TestCase):\n         ResponseLibxml2()\n \n     def test_response_libxml2_caching(self):\n-        r1 = Response('http://www.example.com', body='<html><head></head><body></body></html>')\n+        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')\n         r2 = r1.copy()\n \n         doc1 = r1.getlibxml2doc()\n\n@@ -111,3 +111,57 @@ def re_rsearch(pattern, text, chunk_size=1024):\n             return (offset + matches[-1].span()[0], offset + matches[-1].span()[1])\n     return None\n \n+def memoizemethod(cacheattr):\n+    \"\"\"A memoize decorator for methods, which caches calls to instance methods\n+    into an attribute of the same instance (which must be dict). Calls with\n+    different arguments will be cached in different buckets.\n+\n+    This has the advantage that, when the instance is collected (by the garbage\n+    collector) the cache is collected as well.\n+\n+    Descriptors are required to implement this functionality. See why at:\n+    http://blog.ianbicking.org/2008/10/24/decorators-and-descriptors/\n+\n+    Example:\n+\n+    class A(object):\n+            def __init__(self):\n+            self.cache = {}\n+            @memoizemethod('cache')\n+        def calculate(self, arg1, arg2):\n+            # expensive code here\n+\n+    All calls to calculate() method of A instances will be cached in their\n+    cache (instance) attribute, which must be a dict.\n+\n+    \"\"\"\n+\n+    class MemoizeMethod(object):\n+\n+        def __init__(self, function):\n+            self.function = function\n+\n+        def __get__(self, obj, objtype=None):\n+            if obj is None:\n+                return self\n+            new_func = self.function.__get__(obj, objtype)\n+            return self.__class__(new_func)\n+\n+        def __call__(self, *args, **kwargs):\n+            method = self.function\n+            cache = getattr(method.im_self, cacheattr)\n+            key = (method.im_func, tuple(args), frozenset(kwargs.items()))\n+            if key not in cache:\n+                cache[key] = method(*args, **kwargs)\n+            return cache[key]\n+\n+    return MemoizeMethod\n+\n+_BINARYCHARS = set(map(chr, range(32))) - set([\"\\0\", \"\\t\", \"\\n\", \"\\r\"])\n+\n+def isbinarytext(text):\n+    \"\"\"Return True if the given text is considered binary, or false\n+    otherwise, by looking for binary bytes at their chars\n+    \"\"\"\n+    assert isinstance(text, str), \"text must be str, got '%s'\" % type(text).__name__\n+    return any(c in _BINARYCHARS for c in text)\n\n@@ -9,7 +9,7 @@ from scrapy.http.response import Response\n def body_or_str(obj, unicode=True):\n     assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n     if isinstance(obj, Response):\n-        return obj.body.to_unicode() if unicode else obj.body.to_string()\n+        return obj.body_as_unicode() if unicode else obj.body\n     elif isinstance(obj, str):\n         return obj.decode('utf-8') if unicode else obj\n     else:\n@@ -22,6 +22,6 @@ def get_base_url(response):\n     # benchmark using timeit we got (for 50 repetitions) 0.0017 seconds\n     # using re and 0.7452 using xpath\n     if 'base_url' not in response.cache:\n-        match = BASEURL_RE.search(response.body.to_string()[0:4096])\n+        match = BASEURL_RE.search(response.body[0:4096])\n         response.cache['base_url'] = match.group(1) if match else response.url\n     return response.cache['base_url']\n\n@@ -0,0 +1,28 @@\n+\"\"\"\n+This module contains some assorted functions used in tests\n+\"\"\"\n+\n+import os\n+\n+import libxml2\n+\n+def libxml2debug(testfunction):\n+    \"\"\"Decorator for debugging libxml2 memory leaks inside a function.\n+    \n+    We've found libxml2 memory leaks are something very weird, and can happen\n+    sometimes depending on the order where tests are run. So this decorator\n+    enables libxml2 memory leaks debugging only when the environment variable\n+    LIBXML2_DEBUGLEAKS is set.\n+\n+    \"\"\"\n+    def newfunc(*args, **kwargs):\n+        libxml2.debugMemory(1)\n+        testfunction(*args, **kwargs)\n+        libxml2.cleanupParser()\n+        leaked_bytes = libxml2.debugMemory(0) \n+        assert leaked_bytes == 0, \"libxml2 memory leak detected: %d bytes\" % leaked_bytes\n+\n+    if 'LIBXML2_DEBUGLEAKS' in os.environ:\n+        return newfunc\n+    else:\n+        return testfunction\n\n@@ -14,18 +14,26 @@ html_parser_options = libxml2.HTML_PARSE_RECOVER + \\\n                       libxml2.HTML_PARSE_NOERROR + \\\n                       libxml2.HTML_PARSE_NOWARNING\n \n+def body_as_utf8(response):\n+    if response.encoding in ('utf-8', 'utf8'):\n+        return response.body\n+    else:\n+        return response.body_as_unicode().encode('utf-8')\n+        \n def xmlDoc_from_html(response):\n     \"\"\"Return libxml2 doc for HTMLs\"\"\"\n+    utf8body = body_as_utf8(response)\n     try:\n-        lxdoc = libxml2.htmlReadDoc(response.body.to_string('utf-8'), response.url, 'utf-8', html_parser_options)\n+        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', html_parser_options)\n     except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.htmlReadDoc(response.body.to_string('utf-8').replace(\"\\x00\", \"\"), response.url, 'utf-8', html_parser_options)\n+        lxdoc = libxml2.htmlReadDoc(utf8body.replace(\"\\x00\", \"\"), response.url, 'utf-8', html_parser_options)\n     return lxdoc\n \n def xmlDoc_from_xml(response):\n+    utf8body = body_as_utf8(response)\n     \"\"\"Return libxml2 doc for XMLs\"\"\"\n     try:\n-        lxdoc = libxml2.readDoc(response.body.to_string('utf-8'), response.url, 'utf-8', xml_parser_options)\n+        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', xml_parser_options)\n     except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.readDoc(response.body.to_string('utf-8').replace(\"\\x00\", \"\"), response.url, 'utf-8', xml_parser_options)\n+        lxdoc = libxml2.readDoc(utf8body.replace(\"\\x00\", \"\"), response.url, 'utf-8', xml_parser_options)\n     return lxdoc\n\n@@ -1,6 +1,6 @@\n import libxml2\n \n-from scrapy.http import Response\n+from scrapy.http import TextResponse\n from scrapy.xpath.extension import Libxml2Document\n from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml\n from scrapy.utils.python import flatten, unicode_to_str\n@@ -28,7 +28,7 @@ class XPathSelector(object):\n                 self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n-            response = Response(url=None, body=unicode_to_str(text))\n+            response = TextResponse(url=None, body=unicode_to_str(text))\n             self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
