{"custom_id": "scrapy#dc6e142096e77a99e5340f36309f40e70f4145cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 251 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,8 +27,8 @@ class SpiderLoader:\n     \"\"\"\n \n     def __init__(self, settings: BaseSettings):\n-        self.spider_modules = settings.getlist(\"SPIDER_MODULES\")\n-        self.warn_only = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n+        self.spider_modules: List[str] = settings.getlist(\"SPIDER_MODULES\")\n+        self.warn_only: bool = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n         self._spiders: Dict[str, Type[Spider]] = {}\n         self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n         self._load_all_spiders()\n@@ -96,7 +96,7 @@ class SpiderLoader:\n             name for name, cls in self._spiders.items() if cls.handles_request(request)\n         ]\n \n-    def list(self):\n+    def list(self) -> List[str]:\n         \"\"\"\n         Return a list with the names of all spiders available in the project.\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89503ae3f1228e9406428a19bc6c49c1f8e9e19b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 34 | Churn Cumulative: 176 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n+from __future__ import annotations\n+\n import logging\n from pathlib import Path\n-from typing import Optional, Set, Type, TypeVar\n+from typing import TYPE_CHECKING, Optional, Set\n from warnings import warn\n \n from twisted.internet.defer import Deferred\n@@ -12,14 +14,16 @@ from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import RequestFingerprinter, referer_str\n \n-BaseDupeFilterTV = TypeVar(\"BaseDupeFilterTV\", bound=\"BaseDupeFilter\")\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n \n \n class BaseDupeFilter:\n     @classmethod\n-    def from_settings(\n-        cls: Type[BaseDupeFilterTV], settings: BaseSettings\n-    ) -> BaseDupeFilterTV:\n+    def from_settings(cls, settings: BaseSettings) -> Self:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -36,9 +40,6 @@ class BaseDupeFilter:\n         pass\n \n \n-RFPDupeFilterTV = TypeVar(\"RFPDupeFilterTV\", bound=\"RFPDupeFilter\")\n-\n-\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n@@ -47,10 +48,12 @@ class RFPDupeFilter(BaseDupeFilter):\n         path: Optional[str] = None,\n         debug: bool = False,\n         *,\n-        fingerprinter=None,\n+        fingerprinter: Optional[RequestFingerprinter] = None,\n     ) -> None:\n         self.file = None\n-        self.fingerprinter = fingerprinter or RequestFingerprinter()\n+        self.fingerprinter: RequestFingerprinter = (\n+            fingerprinter or RequestFingerprinter()\n+        )\n         self.fingerprints: Set[str] = set()\n         self.logdupes = True\n         self.debug = debug\n@@ -62,8 +65,11 @@ class RFPDupeFilter(BaseDupeFilter):\n \n     @classmethod\n     def from_settings(\n-        cls: Type[RFPDupeFilterTV], settings: BaseSettings, *, fingerprinter=None\n-    ) -> RFPDupeFilterTV:\n+        cls,\n+        settings: BaseSettings,\n+        *,\n+        fingerprinter: Optional[RequestFingerprinter] = None,\n+    ) -> Self:\n         debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n         try:\n             return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n@@ -75,11 +81,11 @@ class RFPDupeFilter(BaseDupeFilter):\n                 ScrapyDeprecationWarning,\n             )\n             result = cls(job_dir(settings), debug)\n-            result.fingerprinter = fingerprinter\n+            result.fingerprinter = fingerprinter or RequestFingerprinter()\n             return result\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         try:\n             return cls.from_settings(\n                 crawler.settings,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9960c62b871a4b76d72c57abdead553b6e7178e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2067 | Contributors (this commit): 30 | Commits (past 90d): 3 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -364,6 +364,7 @@ class Scraper:\n                     spider=spider,\n                     exception=output.value,\n                 )\n+            assert ex\n             logkws = self.logformatter.item_error(item, ex, response, spider)\n             logger.log(\n                 *logformatter_adapter(logkws),\n\n@@ -114,7 +114,7 @@ class LogFormatter:\n         }\n \n     def item_error(\n-        self, item: Any, exception, response: Response, spider: Spider\n+        self, item: Any, exception: BaseException, response: Response, spider: Spider\n     ) -> dict:\n         \"\"\"Logs a message when an item causes an error while it is passing\n         through the item pipeline.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5885fc13b7d44ddaf403e8ccdd8dccb14082eee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 149 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class StopDownload(Exception):\n     should be handled by the request errback. Note that 'fail' is a keyword-only argument.\n     \"\"\"\n \n-    def __init__(self, *, fail=True):\n+    def __init__(self, *, fail: bool = True):\n         super().__init__()\n         self.fail = fail\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dca18e2e7e92e7dcc0fb8ae769f82287dd64386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 9 | Methods Changed: 15 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 39 | Churn Cumulative: 232 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: 0.6923076923076923\n\nDIFF:\n@@ -5,6 +5,7 @@ based on different criteria.\n from io import StringIO\n from mimetypes import MimeTypes\n from pkgutil import get_data\n+from typing import Dict, Mapping, Optional, Type, Union\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n@@ -29,15 +30,19 @@ class ResponseTypes:\n         \"text/*\": \"scrapy.http.TextResponse\",\n     }\n \n-    def __init__(self):\n-        self.classes = {}\n-        self.mimetypes = MimeTypes()\n-        mimedata = get_data(\"scrapy\", \"mime.types\").decode(\"utf8\")\n-        self.mimetypes.readfp(StringIO(mimedata))\n+    def __init__(self) -> None:\n+        self.classes: Dict[str, Type[Response]] = {}\n+        self.mimetypes: MimeTypes = MimeTypes()\n+        mimedata = get_data(\"scrapy\", \"mime.types\")\n+        if not mimedata:\n+            raise ValueError(\n+                \"The mime.types file is not found in the Scrapy installation\"\n+            )\n+        self.mimetypes.readfp(StringIO(mimedata.decode(\"utf8\")))\n         for mimetype, cls in self.CLASSES.items():\n             self.classes[mimetype] = load_object(cls)\n \n-    def from_mimetype(self, mimetype):\n+    def from_mimetype(self, mimetype: str) -> Type[Response]:\n         \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n         if mimetype is None:\n             return Response\n@@ -46,7 +51,9 @@ class ResponseTypes:\n         basetype = f\"{mimetype.split('/')[0]}/*\"\n         return self.classes.get(basetype, Response)\n \n-    def from_content_type(self, content_type, content_encoding=None):\n+    def from_content_type(\n+        self, content_type: Union[str, bytes], content_encoding: Optional[bytes] = None\n+    ) -> Type[Response]:\n         \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n         header\"\"\"\n         if content_encoding:\n@@ -56,7 +63,9 @@ class ResponseTypes:\n         )\n         return self.from_mimetype(mimetype)\n \n-    def from_content_disposition(self, content_disposition):\n+    def from_content_disposition(\n+        self, content_disposition: Union[str, bytes]\n+    ) -> Type[Response]:\n         try:\n             filename = (\n                 to_unicode(content_disposition, encoding=\"latin-1\", errors=\"replace\")\n@@ -68,7 +77,7 @@ class ResponseTypes:\n         except IndexError:\n             return Response\n \n-    def from_headers(self, headers):\n+    def from_headers(self, headers: Mapping[bytes, bytes]) -> Type[Response]:\n         \"\"\"Return the most appropriate Response class by looking at the HTTP\n         headers\"\"\"\n         cls = Response\n@@ -81,14 +90,14 @@ class ResponseTypes:\n             cls = self.from_content_disposition(headers[b\"Content-Disposition\"])\n         return cls\n \n-    def from_filename(self, filename):\n+    def from_filename(self, filename: str) -> Type[Response]:\n         \"\"\"Return the most appropriate Response class from a file name\"\"\"\n         mimetype, encoding = self.mimetypes.guess_type(filename)\n         if mimetype and not encoding:\n             return self.from_mimetype(mimetype)\n         return Response\n \n-    def from_body(self, body):\n+    def from_body(self, body: bytes) -> Type[Response]:\n         \"\"\"Try to guess the appropriate response based on the body content.\n         This method is a bit magic and could be improved in the future, but\n         it's not meant to be used except for special cases where response types\n@@ -106,7 +115,13 @@ class ResponseTypes:\n             return self.from_mimetype(\"text/html\")\n         return self.from_mimetype(\"text\")\n \n-    def from_args(self, headers=None, url=None, filename=None, body=None):\n+    def from_args(\n+        self,\n+        headers: Optional[Mapping[bytes, bytes]] = None,\n+        url: Optional[str] = None,\n+        filename: Optional[str] = None,\n+        body: Optional[bytes] = None,\n+    ) -> Type[Response]:\n         \"\"\"Guess the most appropriate Response class based on\n         the given arguments.\"\"\"\n         cls = Response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6d919497d08f5c1ddcdd0c210fcc17bd78f2fe9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1028 | Contributors (this commit): 19 | Commits (past 90d): 1 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,10 @@\n+from __future__ import annotations\n+\n import json\n import logging\n from abc import abstractmethod\n from pathlib import Path\n-from typing import Any, Optional, Type, TypeVar, cast\n+from typing import TYPE_CHECKING, Any, Optional, Type, TypeVar, cast\n \n from twisted.internet.defer import Deferred\n \n@@ -14,6 +16,11 @@ from scrapy.statscollectors import StatsCollector\n from scrapy.utils.job import job_dir\n from scrapy.utils.misc import create_instance, load_object\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -54,7 +61,7 @@ class BaseScheduler(metaclass=BaseSchedulerMeta):\n     \"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler: Crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         \"\"\"\n         Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.\n         \"\"\"\n@@ -325,6 +332,7 @@ class Scheduler(BaseScheduler):\n \n     def _dq(self):\n         \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n+        assert self.dqdir\n         state = self._read_dqs_state(self.dqdir)\n         q = create_instance(\n             self.pqclass,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b50268c100c6ad70ea81f0606aedcb720a12692b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 171 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n+from typing import Any\n \n \n class Link:\n@@ -26,16 +27,20 @@ class Link:\n \n     __slots__ = [\"url\", \"text\", \"fragment\", \"nofollow\"]\n \n-    def __init__(self, url, text=\"\", fragment=\"\", nofollow=False):\n+    def __init__(\n+        self, url: str, text: str = \"\", fragment: str = \"\", nofollow: bool = False\n+    ):\n         if not isinstance(url, str):\n             got = url.__class__.__name__\n             raise TypeError(f\"Link urls must be str objects, got {got}\")\n-        self.url = url\n-        self.text = text\n-        self.fragment = fragment\n-        self.nofollow = nofollow\n+        self.url: str = url\n+        self.text: str = text\n+        self.fragment: str = fragment\n+        self.nofollow: bool = nofollow\n \n-    def __eq__(self, other):\n+    def __eq__(self, other: Any) -> bool:\n+        if not isinstance(other, Link):\n+            raise NotImplementedError\n         return (\n             self.url == other.url\n             and self.text == other.text\n@@ -43,12 +48,12 @@ class Link:\n             and self.nofollow == other.nofollow\n         )\n \n-    def __hash__(self):\n+    def __hash__(self) -> int:\n         return (\n             hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n         )\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return (\n             f\"Link(url={self.url!r}, text={self.text!r}, \"\n             f\"fragment={self.fragment!r}, nofollow={self.nofollow!r})\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a090d951a721881d0c434ebf4207e78a4eadfe1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 26 | Churn Cumulative: 1817 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 40 | DMM Complexity: 0.058823529411764705\n\nDIFF:\n@@ -7,13 +7,11 @@ import io\n import marshal\n import pickle\n import pprint\n-import warnings\n from collections.abc import Mapping\n from xml.sax.saxutils import XMLGenerator\n \n from itemadapter import ItemAdapter, is_item\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import Item\n from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n from scrapy.utils.serialize import ScrapyJSONEncoder\n@@ -330,13 +328,7 @@ class PythonItemExporter(BaseItemExporter):\n     \"\"\"\n \n     def _configure(self, options, dont_fail=False):\n-        self.binary = options.pop(\"binary\", True)\n         super()._configure(options, dont_fail)\n-        if self.binary:\n-            warnings.warn(\n-                \"PythonItemExporter will drop support for binary export in the future\",\n-                ScrapyDeprecationWarning,\n-            )\n         if not self.encoding:\n             self.encoding = \"utf-8\"\n \n@@ -351,18 +343,14 @@ class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_item(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n-        encode_func = to_bytes if self.binary else to_unicode\n         if isinstance(value, (str, bytes)):\n-            return encode_func(value, encoding=self.encoding)\n+            return to_unicode(value, encoding=self.encoding)\n         return value\n \n     def _serialize_item(self, item):\n         for key, value in ItemAdapter(item).items():\n-            key = to_bytes(key) if self.binary else key\n             yield key, self._serialize_value(value)\n \n     def export_item(self, item):\n         result = dict(self._get_serialized_fields(item))\n-        if self.binary:\n-            result = dict(self._serialize_item(result))\n         return result\n\n@@ -7,12 +7,10 @@ import tempfile\n import unittest\n from datetime import datetime\n from io import BytesIO\n-from warnings import catch_warnings, filterwarnings\n \n import lxml.etree\n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.exporters import (\n     BaseItemExporter,\n     CsvItemExporter,\n@@ -143,7 +141,7 @@ class BaseItemExporterDataclassTest(BaseItemExporterTest):\n \n class PythonItemExporterTest(BaseItemExporterTest):\n     def _get_exporter(self, **kwargs):\n-        return PythonItemExporter(binary=False, **kwargs)\n+        return PythonItemExporter(**kwargs)\n \n     def test_invalid_option(self):\n         with self.assertRaisesRegex(TypeError, \"Unexpected options: invalid_option\"):\n@@ -198,14 +196,6 @@ class PythonItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(type(exported[\"age\"][0]), dict)\n         self.assertEqual(type(exported[\"age\"][0][\"age\"][0]), dict)\n \n-    def test_export_binary(self):\n-        with catch_warnings():\n-            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n-            exporter = PythonItemExporter(binary=True)\n-            value = self.item_class(name=\"John\\xa3\", age=\"22\")\n-            expected = {b\"name\": b\"John\\xc2\\xa3\", b\"age\": b\"22\"}\n-            self.assertEqual(expected, exporter.export_item(value))\n-\n     def test_nonstring_types_item(self):\n         item = self._get_nonstring_types_item()\n         ie = self._get_exporter()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f05657e54245eff9f912914014c51b37b0bc6b34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 38 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -37,6 +37,8 @@ class PeriodicLog:\n     @classmethod\n     def from_crawler(cls, crawler):\n         interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n+        if not interval:\n+            raise NotConfigured\n         try:\n             ext_stats = crawler.settings.getdict(\"PERIODIC_LOG_STATS\")\n         except (TypeError, ValueError):\n@@ -57,8 +59,6 @@ class PeriodicLog:\n         ext_timing_enabled = crawler.settings.getbool(\n             \"PERIODIC_LOG_TIMING_ENABLED\", False\n         )\n-        if not interval:\n-            raise NotConfigured\n         if not (ext_stats or ext_delta or ext_timing_enabled):\n             raise NotConfigured\n         o = cls(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#736a4b615c08cc7e610ddb18f5ac782f0aedca61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 873 | Contributors (this commit): 64 | Commits (past 90d): 4 | Contributors (cumulative): 64 | DMM Complexity: None\n\nDIFF:\n@@ -242,7 +242,7 @@ NEWSPIDER_MODULE = \"\"\n \n PERIODIC_LOG_DELTA = None\n PERIODIC_LOG_STATS = None\n-PERIODIC_LOG_TIMING_ENABLED = None\n+PERIODIC_LOG_TIMING_ENABLED = False\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9df67a554e542b3ec8f92491c3840fca5ea182d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 892 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,11 @@ from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n-from scrapy.utils.request import RequestFingerprinter, referer_str\n+from scrapy.utils.request import (\n+    RequestFingerprinter,\n+    RequestFingerprinterProtocol,\n+    referer_str,\n+)\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n@@ -48,10 +52,10 @@ class RFPDupeFilter(BaseDupeFilter):\n         path: Optional[str] = None,\n         debug: bool = False,\n         *,\n-        fingerprinter: Optional[RequestFingerprinter] = None,\n+        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n     ) -> None:\n         self.file = None\n-        self.fingerprinter: RequestFingerprinter = (\n+        self.fingerprinter: RequestFingerprinterProtocol = (\n             fingerprinter or RequestFingerprinter()\n         )\n         self.fingerprints: Set[str] = set()\n@@ -68,7 +72,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         cls,\n         settings: BaseSettings,\n         *,\n-        fingerprinter: Optional[RequestFingerprinter] = None,\n+        fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n     ) -> Self:\n         debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n         try:\n\n@@ -14,6 +14,7 @@ from typing import (\n     Iterable,\n     List,\n     Optional,\n+    Protocol,\n     Tuple,\n     Type,\n     Union,\n@@ -230,6 +231,11 @@ def fingerprint(\n     return cache[cache_key]\n \n \n+class RequestFingerprinterProtocol(Protocol):\n+    def fingerprint(self, request: Request) -> bytes:\n+        ...\n+\n+\n class RequestFingerprinter:\n     \"\"\"Default fingerprinter.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44b15c3004d950021460293c391f2283f2418726", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 2160 | Contributors (this commit): 31 | Commits (past 90d): 8 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union\n+from typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union, cast\n \n from twisted.internet.defer import (\n     Deferred,\n@@ -31,7 +31,6 @@ from scrapy.interfaces import ISpiderLoader\n from scrapy.logformatter import LogFormatter\n from scrapy.settings import BaseSettings, Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n-from scrapy.spiderloader import SpiderLoader\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.log import (\n     LogCounterHandler,\n@@ -182,10 +181,10 @@ class CrawlerRunner:\n     )\n \n     @staticmethod\n-    def _get_spider_loader(settings: BaseSettings) -> SpiderLoader:\n+    def _get_spider_loader(settings: BaseSettings):\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n         cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n-        loader_cls: Type[SpiderLoader] = load_object(cls_path)\n+        loader_cls = load_object(cls_path)\n         excs = (\n             (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n         )\n@@ -211,7 +210,7 @@ class CrawlerRunner:\n         self.bootstrap_failed = False\n \n     @property\n-    def spiders(self) -> SpiderLoader:\n+    def spiders(self):\n         warnings.warn(\n             \"CrawlerRunner.spiders attribute is renamed to \"\n             \"CrawlerRunner.spider_loader.\",\n@@ -293,7 +292,8 @@ class CrawlerRunner:\n     def _create_crawler(self, spidercls: Union[str, Type[Spider]]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n-        return Crawler(spidercls, self.settings)\n+        # temporary cast until self.spider_loader is typed\n+        return Crawler(cast(Type[Spider], spidercls), self.settings)\n \n     def stop(self) -> Deferred:\n         \"\"\"\n@@ -375,7 +375,10 @@ class CrawlerProcess(CrawlerRunner):\n             spidercls = self.spider_loader.load(spidercls)\n         init_reactor = not self._initialized_reactor\n         self._initialized_reactor = True\n-        return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n+        # temporary cast until self.spider_loader is typed\n+        return Crawler(\n+            cast(Type[Spider], spidercls), self.settings, init_reactor=init_reactor\n+        )\n \n     def start(\n         self, stop_after_crawl: bool = True, install_signal_handlers: bool = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34d050cfe5aec94a15f00950fcecbc49cb470fb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 21 | Churn Cumulative: 3761 | Contributors (this commit): 37 | Commits (past 90d): 15 | Contributors (cumulative): 53 | DMM Complexity: 0.0\n\nDIFF:\n@@ -204,16 +204,6 @@ class CrawlerRunner:\n         self._active: Set[defer.Deferred] = set()\n         self.bootstrap_failed = False\n \n-    @property\n-    def spiders(self):\n-        warnings.warn(\n-            \"CrawlerRunner.spiders attribute is renamed to \"\n-            \"CrawlerRunner.spider_loader.\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        return self.spider_loader\n-\n     def crawl(self, crawler_or_spidercls, *args, **kwargs):\n         \"\"\"\n         Run a crawler with the provided arguments.\n\n@@ -20,7 +20,6 @@ from scrapy.extensions.throttle import AutoThrottle\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n-from scrapy.utils.misc import load_object\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer, get_mockserver_env\n@@ -182,16 +181,6 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n         runner = CrawlerRunner()\n         self.assertOptionIsDefault(runner.settings, \"RETRY_ENABLED\")\n \n-    def test_deprecated_attribute_spiders(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            runner = CrawlerRunner(Settings())\n-            spiders = runner.spiders\n-            self.assertEqual(len(w), 1)\n-            self.assertIn(\"CrawlerRunner.spiders\", str(w[0].message))\n-            self.assertIn(\"CrawlerRunner.spider_loader\", str(w[0].message))\n-            sl_cls = load_object(runner.settings[\"SPIDER_LOADER_CLASS\"])\n-            self.assertIsInstance(spiders, sl_cls)\n-\n \n class CrawlerProcessTest(BaseCrawlerTest):\n     def test_crawler_process_accepts_dict(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b06936f111741f16aae8338ea8006435890bb10d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 15 | Churn Cumulative: 8565 | Contributors (this commit): 53 | Commits (past 90d): 12 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -238,7 +238,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n     def getdictorlist(\n         self,\n         name: _SettingsKeyT,\n-        default: Union[Dict[Any, Any], List[Any], None] = None,\n+        default: Union[Dict[Any, Any], List[Any], Tuple[Any], None] = None,\n     ) -> Union[Dict[Any, Any], List[Any]]:\n         \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n \n@@ -271,6 +271,8 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n                 return value_loaded\n             except ValueError:\n                 return value.split(\",\")\n+        if isinstance(value, tuple):\n+            return list(value)\n         assert isinstance(value, (dict, list))\n         return copy.deepcopy(value)\n \n\n@@ -1321,6 +1321,17 @@ class FeedExportTest(FeedExportTestBase):\n         yield self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n+    @defer.inlineCallbacks\n+    def test_export_tuple(self):\n+        items = [\n+            {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n+            {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux\"},\n+        ]\n+\n+        settings = {\"FEED_EXPORT_FIELDS\": (\"foo\", \"baz\")}\n+        rows = [{\"foo\": \"bar1\", \"baz\": \"\"}, {\"foo\": \"bar2\", \"baz\": \"quux\"}]\n+        yield self.assertExported(items, [\"foo\", \"baz\"], rows, settings=settings)\n+\n     @defer.inlineCallbacks\n     def test_export_feed_export_fields(self):\n         # FEED_EXPORT_FIELDS option allows to order export fields\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19867659f3eb8a3e018e9b38fd9520f9fa4392cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 918 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -82,7 +82,7 @@ class TextResponse(Response):\n         Deserialize a JSON document to a Python object.\n         \"\"\"\n         if self._cached_decoded_json is _NONE:\n-            self._cached_decoded_json = json.loads(self.text)\n+            self._cached_decoded_json = json.loads(self.body)\n         return self._cached_decoded_json\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd4292b722edfa5aaf08f6e64035271222771fa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2127 | Contributors (this commit): 27 | Commits (past 90d): 1 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -844,7 +844,7 @@ class TextResponseTest(BaseResponseTest):\n             with mock.patch(\"json.loads\") as mock_json:\n                 for _ in range(2):\n                     json_response.json()\n-                mock_json.assert_called_once_with(json_body.decode())\n+                mock_json.assert_called_once_with(json_body)\n \n \n class HtmlResponseTest(TextResponseTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7355741c7dc6a3bca1be54b32eb00fd3ec1f3ab6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 304 | Contributors (this commit): 18 | Commits (past 90d): 1 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -38,6 +38,7 @@ IGNORED_EXTENSIONS = [\n     \"svg\",\n     \"cdr\",\n     \"ico\",\n+    \"webp\",\n     # audio\n     \"mp3\",\n     \"wma\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df2163ce6aded0ed56eed8b125739193b953ff17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 2156 | Contributors (this commit): 44 | Commits (past 90d): 14 | Contributors (cumulative): 70 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n Extension for collecting core stats like items scraped and start/finish times\n \"\"\"\n-from datetime import datetime\n+from datetime import datetime, timezone\n \n from scrapy import signals\n \n@@ -22,11 +22,11 @@ class CoreStats:\n         return o\n \n     def spider_opened(self, spider):\n-        self.start_time = datetime.utcnow()\n+        self.start_time = datetime.now(tz=timezone.utc)\n         self.stats.set_value(\"start_time\", self.start_time, spider=spider)\n \n     def spider_closed(self, spider, reason):\n-        finish_time = datetime.utcnow()\n+        finish_time = datetime.now(tz=timezone.utc)\n         elapsed_time = finish_time - self.start_time\n         elapsed_time_seconds = elapsed_time.total_seconds()\n         self.stats.set_value(\n\n@@ -8,7 +8,7 @@ import logging\n import re\n import sys\n import warnings\n-from datetime import datetime\n+from datetime import datetime, timezone\n from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n from typing import IO, Any, Callable, Dict, List, Optional, Tuple, Union\n@@ -676,7 +676,7 @@ class FeedExporter:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n-        utc_now = datetime.utcnow()\n+        utc_now = datetime.now(tz=timezone.utc)\n         params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n         params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n         params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n\n@@ -1,4 +1,4 @@\n-from datetime import datetime, timedelta\n+from datetime import datetime, timedelta, timezone\n from pathlib import Path\n \n from cryptography.hazmat.backends import default_backend\n@@ -50,8 +50,8 @@ def generate_keys():\n         .issuer_name(issuer)\n         .public_key(key.public_key())\n         .serial_number(random_serial_number())\n-        .not_valid_before(datetime.utcnow())\n-        .not_valid_after(datetime.utcnow() + timedelta(days=10))\n+        .not_valid_before(datetime.now(tz=timezone.utc))\n+        .not_valid_after(datetime.now(tz=timezone.utc) + timedelta(days=10))\n         .add_extension(\n             SubjectAlternativeName([DNSName(\"localhost\")]),\n             critical=False,\n\n@@ -1,5 +1,5 @@\n import shutil\n-from datetime import datetime\n+from datetime import datetime, timezone\n from pathlib import Path\n \n from twisted.trial import unittest\n@@ -16,7 +16,7 @@ class SpiderStateTest(unittest.TestCase):\n         Path(jobdir).mkdir()\n         try:\n             spider = Spider(name=\"default\")\n-            dt = datetime.now()\n+            dt = datetime.now(tz=timezone.utc)\n \n             ss = SpiderState(jobdir)\n             ss.spider_opened(spider)\n\n@@ -16,7 +16,7 @@ class CoreStatsExtensionTest(unittest.TestCase):\n     @mock.patch(\"scrapy.extensions.corestats.datetime\")\n     def test_core_stats_default_stats_collector(self, mock_datetime):\n         fixed_datetime = datetime(2019, 12, 1, 11, 38)\n-        mock_datetime.utcnow = mock.Mock(return_value=fixed_datetime)\n+        mock_datetime.now = mock.Mock(return_value=fixed_datetime)\n         self.crawler.stats = StatsCollector(self.crawler)\n         ext = CoreStats.from_crawler(self.crawler)\n         ext.spider_opened(self.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e542846e4d9b33f7b6f5b984cea0de13f47e8c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 44 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n import logging\n-from datetime import datetime\n+from datetime import datetime, timezone\n \n from twisted.internet import task\n \n@@ -73,7 +73,7 @@ class PeriodicLog:\n         return o\n \n     def spider_opened(self, spider):\n-        self.time_prev = datetime.utcnow()\n+        self.time_prev = datetime.now(tz=timezone.utc)\n         self.delta_prev = {}\n         self.stats_prev = {}\n \n@@ -102,7 +102,7 @@ class PeriodicLog:\n         return {\"delta\": delta}\n \n     def log_timing(self):\n-        now = datetime.utcnow()\n+        now = datetime.now(tz=timezone.utc)\n         time = {\n             \"log_interval\": self.interval,\n             \"start_time\": self.stats._stats[\"start_time\"],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a72a2550cf83c6c533554d430e6b94adae2bb05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 831 | Contributors (this commit): 37 | Commits (past 90d): 7 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,8 @@ version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n \n \n install_requires = [\n-    \"Twisted>=18.9.0\",\n+    # 23.8.0 incompatibility: https://github.com/scrapy/scrapy/issues/6024\n+    \"Twisted>=18.9.0,<23.8.0\",\n     \"cryptography>=36.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#276bce0641a4fdace701b860e2c151307f2c37c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 6 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 6299 | Contributors (this commit): 48 | Commits (past 90d): 8 | Contributors (cumulative): 91 | DMM Complexity: 1.0\n\nDIFF:\n@@ -154,7 +154,7 @@ class Downloader:\n         self.signals.send_catch_log(\n             signal=signals.request_reached_downloader, request=request, spider=spider\n         )\n-        deferred = Deferred().addBoth(_deactivate)\n+        deferred: Deferred = Deferred().addBoth(_deactivate)\n         slot.queue.append((request, deferred))\n         self._process_queue(spider, slot)\n         return deferred\n\n@@ -220,7 +220,12 @@ class ExecutionEngine:\n                 extra={\"spider\": self.spider},\n             )\n         )\n-        d.addBoth(lambda _: cast(Slot, self.slot).remove_request(request))  # type: ignore[arg-type]\n+\n+        def _remove_request(_: Any) -> None:\n+            assert self.slot\n+            self.slot.remove_request(request)\n+\n+        d.addBoth(_remove_request)\n         d.addErrback(\n             lambda f: logger.info(\n                 \"Error while removing request from slot\",\n@@ -300,8 +305,8 @@ class ExecutionEngine:\n         return self._download(request).addBoth(self._downloaded, request)\n \n     def _downloaded(\n-        self, result: Union[Response, Request], request: Request\n-    ) -> Union[Deferred, Response]:\n+        self, result: Union[Response, Request, Failure], request: Request\n+    ) -> Union[Deferred, Response, Failure]:\n         assert self.slot is not None  # typing\n         self.slot.remove_request(request)\n         return self.download(result) if isinstance(result, Request) else result\n\n@@ -221,7 +221,11 @@ class Scraper:\n         return dfd.addCallback(iterate_spider_output)\n \n     def handle_spider_error(\n-        self, _failure: Failure, request: Request, response: Response, spider: Spider\n+        self,\n+        _failure: Failure,\n+        request: Request,\n+        response: Union[Response, Failure],\n+        spider: Spider,\n     ) -> None:\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n@@ -248,7 +252,7 @@ class Scraper:\n         self,\n         result: Union[Iterable, AsyncIterable],\n         request: Request,\n-        response: Response,\n+        response: Union[Response, Failure],\n         spider: Spider,\n     ) -> Deferred:\n         if not result:\n\n@@ -130,7 +130,11 @@ class LogFormatter:\n         }\n \n     def spider_error(\n-        self, failure: Failure, request: Request, response: Response, spider: Spider\n+        self,\n+        failure: Failure,\n+        request: Request,\n+        response: Union[Response, Failure],\n+        spider: Spider,\n     ) -> dict:\n         \"\"\"Logs an error message from a spider.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1dd5493acd9ab9d548d97853da0abbe1d8acf40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 1002 | Contributors (this commit): 2 | Commits (past 90d): 26 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,6 @@ class AddonManager:\n             which to read the add-on configuration\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n-        enabled: List[Any] = []\n         for clspath in build_component_list(settings[\"ADDONS\"]):\n             try:\n                 addoncls = load_object(clspath)\n@@ -48,7 +47,7 @@ class AddonManager:\n         logger.info(\n             \"Enabled addons:\\n%(addons)s\",\n             {\n-                \"addons\": enabled,\n+                \"addons\": self.addons,\n             },\n             extra={\"crawler\": self.crawler},\n         )\n\n@@ -1,6 +1,7 @@\n import itertools\n import unittest\n from typing import Any, Dict\n+from unittest.mock import patch\n \n from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerRunner\n@@ -156,3 +157,22 @@ class AddonManagerTest(unittest.TestCase):\n             crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n         )\n         self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\n+\n+    def test_logging_message(self):\n+        class LoggedAddon:\n+            def update_settings(self, settings):\n+                pass\n+\n+        with patch(\"scrapy.addons.logger\") as logger_mock:\n+            with patch(\"scrapy.addons.create_instance\") as create_instance_mock:\n+                settings_dict = {\n+                    \"ADDONS\": {LoggedAddon: 1},\n+                }\n+                addon = LoggedAddon()\n+                create_instance_mock.return_value = addon\n+                crawler = get_crawler(settings_dict=settings_dict)\n+                logger_mock.info.assert_called_once_with(\n+                    \"Enabled addons:\\n%(addons)s\",\n+                    {\"addons\": [addon]},\n+                    extra={\"crawler\": crawler},\n+                )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df112a3996fd872b1f6e3fff4a9b989ed4d0aaea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 2203 | Contributors (this commit): 31 | Commits (past 90d): 10 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -104,20 +104,7 @@ class Crawler:\n             crawler=self,\n         )\n \n-        reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n-        event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n-        if init_reactor:\n-            # this needs to be done after the spider settings are merged,\n-            # but before something imports twisted.internet.reactor\n-            if reactor_class:\n-                install_reactor(reactor_class, event_loop)\n-            else:\n-                from twisted.internet import reactor  # noqa: F401\n-            log_reactor_info()\n-        if reactor_class:\n-            verify_installed_reactor(reactor_class)\n-            if is_asyncio_reactor_installed() and event_loop:\n-                verify_installed_asyncio_event_loop(event_loop)\n+        self._init_reactor = init_reactor\n \n         self.crawling: bool = False\n         self.spider: Optional[Spider] = None\n@@ -132,8 +119,24 @@ class Crawler:\n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n             self.spider.update_settings(self.settings)\n-            self.settings.freeze()\n+\n+            reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n+            event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n+            if self._init_reactor:\n+                # this needs to be done after the spider settings are merged,\n+                # but before something imports twisted.internet.reactor\n+                if reactor_class:\n+                    install_reactor(reactor_class, event_loop)\n+                else:\n+                    from twisted.internet import reactor  # noqa: F401\n+                log_reactor_info()\n+            if reactor_class:\n+                verify_installed_reactor(reactor_class)\n+                if is_asyncio_reactor_installed() and event_loop:\n+                    verify_installed_asyncio_event_loop(event_loop)\n+\n             self.extensions: ExtensionManager = ExtensionManager.from_crawler(self)\n+            self.settings.freeze()\n \n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
