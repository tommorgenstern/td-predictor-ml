{"custom_id": "keras#b4cd5fd342d86d3daed78ae071c3394548be4792", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 66 | Churn Cumulative: 66 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,66 @@\n+import pytest\n+import numpy as np\n+\n+from keras.utils.test_utils import get_test_data\n+from keras.utils import np_utils\n+\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n+\n+np.random.seed(1337)\n+\n+input_dim = 10\n+nb_class = 3\n+batch_size = 32\n+nb_epoch = 1\n+verbosity = 0\n+optim = 'adam'\n+loss = 'categorical_crossentropy'\n+\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=400,\n+                                                     nb_test=200,\n+                                                     input_shape=(input_dim,),\n+                                                     classification=True,\n+                                                     nb_class=nb_class)\n+y_train = np_utils.to_categorical(y_train, nb_classes=nb_class)\n+y_test = np_utils.to_categorical(y_test, nb_classes=nb_class)\n+\n+\n+(X_train_reg, y_train_reg), (X_test_reg, y_test_reg) = get_test_data(nb_train=400,\n+                                                                     nb_test=200,\n+                                                                     input_shape=(input_dim,),\n+                                                                     classification=False,\n+                                                                     nb_class=1,\n+                                                                     output_shape=(1,))\n+\n+\n+def test_keras_classifier():\n+    model = Sequential()\n+    model.add(Dense(input_dim, input_shape=(input_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+\n+    sklearn_clf = KerasClassifier(model, optimizer=optim, loss=loss,\n+                                  train_batch_size=batch_size,\n+                                  test_batch_size=batch_size,\n+                                  nb_epoch=nb_epoch)\n+    sklearn_clf.fit(X_train, y_train)\n+    sklearn_clf.score(X_test, y_test)\n+\n+\n+def test_keras_regressor():\n+    model = Sequential()\n+    model.add(Dense(input_dim, input_shape=(input_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('softmax'))\n+\n+    sklearn_regressor = KerasRegressor(model, optimizer=optim, loss=loss,\n+                                       train_batch_size=batch_size,\n+                                       test_batch_size=batch_size,\n+                                       nb_epoch=nb_epoch)\n+    sklearn_regressor.fit(X_train_reg, y_train_reg)\n+    sklearn_regressor.score(X_test_reg, y_test_reg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e46447080fe4ab245142d3a61439091e0794a21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 96 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,6 +32,26 @@ class TestActivations(unittest.TestCase):\n         expected = softmax(test_values)\n         assert_allclose(result, expected, rtol=1e-05)\n \n+    def test_sigmoid(self):\n+        from keras.activations import sigmoid\n+\n+        # Reference sigmoid\n+        def ref_sigmoid(x):\n+            if x >= 0:\n+                return 1 / (1 + np.exp(-x))\n+            else:\n+                return z / (1 + np.exp(x))\n+        vec_sigmoid = np.vectorize(ref_sigmoid)\n+\n+        x = K.placeholder(ndim=2)\n+        sig_out = sigmoid(x)\n+        f = K.function([x], [sig_out])\n+        test_values = get_standard_values()\n+\n+        result = f([test_values])[0]\n+        expected = vec_sigmoid(test_values)\n+        assert_allclose(result, expected, rtol=1e-05)\n+\n     def test_relu(self):\n         '''\n         Relu implementation doesn't depend on the value being\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02b40ccfe3cc237e54cb6ce386ca8823cfd58195", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 9 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 128 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import unittest\n+import pytest\n from keras import backend as K\n import numpy as np\n from numpy.testing import assert_allclose\n@@ -12,9 +12,16 @@ def get_standard_values():\n     return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n \n \n-class TestActivations(unittest.TestCase):\n+# Reference sigmoid, numerically stable\n+def ref_sigmoid(x):\n+    if x >= 0:\n+        return 1 / (1 + np.exp(-x))\n+    else:\n+        z = np.exp(x)\n+        return z / (1 + z)\n+vec_sigmoid = np.vectorize(ref_sigmoid)\n \n-    def test_softmax(self):\n+def test_softmax():\n     from keras.activations import softmax as s\n \n     # Test using a reference implementation of softmax\n@@ -32,17 +39,9 @@ class TestActivations(unittest.TestCase):\n     expected = softmax(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n-    def test_sigmoid(self):\n+def test_sigmoid():\n     from keras.activations import sigmoid\n \n-        # Reference sigmoid\n-        def ref_sigmoid(x):\n-            if x >= 0:\n-                return 1 / (1 + np.exp(-x))\n-            else:\n-                return z / (1 + np.exp(x))\n-        vec_sigmoid = np.vectorize(ref_sigmoid)\n-\n     x = K.placeholder(ndim=2)\n     sig_out = sigmoid(x)\n     f = K.function([x], [sig_out])\n@@ -52,7 +51,7 @@ class TestActivations(unittest.TestCase):\n     expected = vec_sigmoid(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n-    def test_relu(self):\n+def test_relu():\n     '''\n     Relu implementation doesn't depend on the value being\n     a theano variable. Testing ints, floats and theano tensors.\n@@ -69,7 +68,7 @@ class TestActivations(unittest.TestCase):\n     # because no negatives in test values\n     assert_allclose(result, test_values, rtol=1e-05)\n \n-    def test_tanh(self):\n+def test_tanh():\n     from keras.activations import tanh as t\n     test_values = get_standard_values()\n \n@@ -81,7 +80,7 @@ class TestActivations(unittest.TestCase):\n     expected = np.tanh(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n-    def test_linear(self):\n+def test_linear():\n     '''\n     This function does no input validation, it just returns the thing\n     that was passed in.\n@@ -91,6 +90,3 @@ class TestActivations(unittest.TestCase):\n     xs = [1, 5, True, None, 'foo']\n     for x in xs:\n         assert x == l(x)\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ce80e8e74205833453739b2f7071b38825904aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 32 | Churn Cumulative: 160 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,8 +21,15 @@ def ref_sigmoid(x):\n         return z / (1 + z)\n vec_sigmoid = np.vectorize(ref_sigmoid)\n \n-def test_softmax():\n-    from keras.activations import softmax as s\n+\n+# Reference hard sigmoid with slope and shift values from theano, see\n+# https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/sigm.py\n+def ref_hard_sigmoid(x):\n+    x = (x * 0.2) + 0.5\n+    z = 0.0 if x <= 0 else (1.0 if x >= 1 else x)\n+    return z\n+vec_hard_sigmoid = np.vectorize(ref_hard_sigmoid)\n+\n \n # Test using a reference implementation of softmax\n def softmax(values):\n@@ -30,6 +37,10 @@ def test_softmax():\n     e = np.exp(values - m)\n     return e / np.sum(e)\n \n+\n+def test_softmax():\n+    from keras.activations import softmax as s\n+\n     x = K.placeholder(ndim=2)\n     exp = s(x)\n     f = K.function([x], [exp])\n@@ -39,6 +50,7 @@ def test_softmax():\n     expected = softmax(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n+\n def test_sigmoid():\n     from keras.activations import sigmoid\n \n@@ -51,6 +63,20 @@ def test_sigmoid():\n     expected = vec_sigmoid(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n+\n+def test_hard_sigmoid():\n+    from keras.activations import hard_sigmoid\n+\n+    x = K.placeholder(ndim=2)\n+    hard_sig_out = hard_sigmoid(x)\n+    f = K.function([x], [hard_sig_out])\n+    test_values = get_standard_values()\n+\n+    result = f([test_values])[0]\n+    expected = vec_hard_sigmoid(test_values)\n+    assert_allclose(result, expected, rtol=1e-05)\n+\n+\n def test_relu():\n     '''\n     Relu implementation doesn't depend on the value being\n@@ -68,6 +94,7 @@ def test_relu():\n     # because no negatives in test values\n     assert_allclose(result, test_values, rtol=1e-05)\n \n+\n def test_tanh():\n     from keras.activations import tanh as t\n     test_values = get_standard_values()\n@@ -80,6 +107,7 @@ def test_tanh():\n     expected = np.tanh(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n+\n def test_linear():\n     '''\n     This function does no input validation, it just returns the thing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca3622cbb7613d273614adc47fe904af232c243a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 28 | Churn Cumulative: 188 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,23 +31,41 @@ def ref_hard_sigmoid(x):\n vec_hard_sigmoid = np.vectorize(ref_hard_sigmoid)\n \n \n+# Reference softplus implementation\n+def ref_softplus(x):\n+    return np.log(np.ones_like(x) + np.exp(x))\n+\n+\n # Test using a reference implementation of softmax\n-def softmax(values):\n+def ref_softmax(values):\n     m = np.max(values)\n     e = np.exp(values - m)\n     return e / np.sum(e)\n \n \n def test_softmax():\n-    from keras.activations import softmax as s\n+    from keras.activations import softmax\n \n     x = K.placeholder(ndim=2)\n-    exp = s(x)\n-    f = K.function([x], [exp])\n+    softmax_out = softmax(x)\n+    f = K.function([x], [softmax_out])\n     test_values = get_standard_values()\n \n     result = f([test_values])[0]\n-    expected = softmax(test_values)\n+    expected = ref_softmax(test_values)\n+    assert_allclose(result, expected, rtol=1e-05)\n+\n+\n+def test_softplus():\n+    from keras.activations import softplus\n+\n+    x = K.placeholder(ndim=2)\n+    softplus_out = softplus(x)\n+    f = K.function([x], [softplus_out])\n+    test_values = get_standard_values()\n+\n+    result = f([test_values])[0]\n+    expected = ref_softplus(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#40c48dd1743bbc2909b71f1afc55b4c91baf63f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 7 | Churn Cumulative: 4025 | Contributors (this commit): 35 | Commits (past 90d): 34 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -113,6 +113,11 @@ class Sequential(Layer):\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n+    def reset_states(self):\n+        for layer in self.layers:\n+            if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n+                layer.reset_states()\n+\n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n                 \"layers\": [layer.get_config() for layer in self.layers]}\n\n@@ -419,7 +419,7 @@ class Sequential(Model, containers.Sequential):\n \n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._train_with_acc = K.function(train_ins, [train_loss, train_accuracy], updates=updates)\n-        self._predict = K.function(predict_ins, [self.y_test])\n+        self._predict = K.function(predict_ins, [self.y_test], updates=self.updates)\n         self._test = K.function(test_ins, [test_loss])\n         self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82353da4dc66bc702a74c6c233f3e16b7682f9e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 41 | Churn Cumulative: 329 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,44 +80,3 @@ class BatchNormalization(Layer):\n                   \"momentum\": self.momentum}\n         base_config = super(BatchNormalization, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class LRN2D(Layer):\n-    \"\"\"\n-    This code is adapted from pylearn2.\n-    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n-    \"\"\"\n-\n-    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n-        if n % 2 == 0:\n-            raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n-        super(LRN2D, self).__init__(**kwargs)\n-        self.alpha = alpha\n-        self.k = k\n-        self.beta = beta\n-        self.n = n\n-\n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        b, ch, r, c = K.shape(X)\n-        half_n = self.n // 2\n-        input_sqr = K.square(X)\n-        extra_channels = K.zeros((b, ch + 2 * half_n, r, c))\n-        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],\n-                                   input_sqr,\n-                                   extra_channels[:, half_n + ch:, :, :]],\n-                                  axis=1)\n-        scale = self.k\n-        for i in range(self.n):\n-            scale += self.alpha * input_sqr[:, i:i + ch, :, :]\n-        scale = scale ** self.beta\n-        return X / scale\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"alpha\": self.alpha,\n-                  \"k\": self.k,\n-                  \"beta\": self.beta,\n-                  \"n\": self.n}\n-        base_config = super(LRN2D, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2534964b76015eb3d76261b025c7556b354764c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2479 | Contributors (this commit): 11 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -158,7 +158,7 @@ class SimpleRNN(Recurrent):\n         assert len(states) == 1\n         prev_output = states[0]\n         h = K.dot(x, self.W) + self.b\n-        output = self.activation(h * K.dot(prev_output, self.U))\n+        output = self.activation(h + K.dot(prev_output, self.U))\n         return output, [output]\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a80b176e814c6f17e3a2aca96643b03cfb12508", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 26 | Churn Cumulative: 6530 | Contributors (this commit): 41 | Commits (past 90d): 56 | Contributors (cumulative): 53 | DMM Complexity: 0.9285714285714286\n\nDIFF:\n@@ -74,6 +74,14 @@ class Sequential(Layer):\n                 updates += l.get_params()[3]\n         return updates\n \n+    @property\n+    def state_updates(self):\n+        state_updates = []\n+        for l in self.layers:\n+            if l.trainable and getattr(l, 'stateful', False):\n+                state_updates += l.get_params()[3]\n+        return state_updates\n+\n     @property\n     def output_shape(self):\n         return self.layers[-1].output_shape\n@@ -192,6 +200,14 @@ class Graph(Layer):\n                 updates += l.get_params()[3]\n         return updates\n \n+    @property\n+    def state_updates(self):\n+        state_updates = []\n+        for l in self.nodes.values():\n+            if l.trainable and getattr(l, 'stateful', False):\n+                state_updates += l.get_params()[3]\n+        return state_updates\n+\n     def set_previous(self, layer, connection_map={}):\n         if self.nb_input != layer.nb_output:\n             raise Exception('Cannot connect layers: input count does not match output count.')\n\n@@ -79,9 +79,9 @@ class Recurrent(MaskedLayer):\n                                              go_backwards=self.go_backwards,\n                                              masking=masking)\n         if self.stateful:\n-            self.updates = []\n+            self.state_updates = []\n             for i in range(len(states)):\n-                self.updates.append((self.states[i], states[i]))\n+                self.state_updates.append((self.states[i], states[i]))\n \n         if self.return_sequences:\n             return outputs\n\n@@ -406,6 +406,7 @@ class Sequential(Model, containers.Sequential):\n                                              self.constraints,\n                                              train_loss)\n         updates += self.updates\n+        state_updates = self.state_updates()\n \n         if type(self.X_train) == list:\n             train_ins = self.X_train + [self.y, self.weights]\n@@ -419,7 +420,7 @@ class Sequential(Model, containers.Sequential):\n \n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._train_with_acc = K.function(train_ins, [train_loss, train_accuracy], updates=updates)\n-        self._predict = K.function(predict_ins, [self.y_test], updates=self.updates)\n+        self._predict = K.function(predict_ins, [self.y_test], updates=state_updates)\n         self._test = K.function(test_ins, [test_loss])\n         self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy])\n \n@@ -626,12 +627,13 @@ class Graph(Model, containers.Graph):\n         self.optimizer = optimizers.get(optimizer)\n         updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n         updates += self.updates\n+        state_updates = self.state_updates()\n         self.theano_mode = theano_mode\n         self.loss = loss\n \n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._test = K.function(test_ins, [test_loss])\n-        self._predict = K.function(inputs=ins, outputs=ys_test)\n+        self._predict = K.function(inputs=ins, outputs=ys_test, updates=state_updates)\n \n     def train_on_batch(self, data, class_weight={}, sample_weight={}):\n         # data is a dictionary mapping output and input names to arrays\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56fa445ef153e1a1120bd004d9b592b33aeb8e00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 5639 | Contributors (this commit): 33 | Commits (past 90d): 33 | Contributors (cumulative): 39 | DMM Complexity: 0.5\n\nDIFF:\n@@ -79,9 +79,9 @@ class Recurrent(MaskedLayer):\n                                              go_backwards=self.go_backwards,\n                                              masking=masking)\n         if self.stateful:\n-            self.state_updates = []\n+            self.updates = []\n             for i in range(len(states)):\n-                self.state_updates.append((self.states[i], states[i]))\n+                self.updates.append((self.states[i], states[i]))\n \n         if self.return_sequences:\n             return outputs\n\n@@ -406,7 +406,6 @@ class Sequential(Model, containers.Sequential):\n                                              self.constraints,\n                                              train_loss)\n         updates += self.updates\n-        state_updates = self.state_updates()\n \n         if type(self.X_train) == list:\n             train_ins = self.X_train + [self.y, self.weights]\n@@ -420,7 +419,7 @@ class Sequential(Model, containers.Sequential):\n \n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._train_with_acc = K.function(train_ins, [train_loss, train_accuracy], updates=updates)\n-        self._predict = K.function(predict_ins, [self.y_test], updates=state_updates)\n+        self._predict = K.function(predict_ins, [self.y_test], updates=self.state_updates)\n         self._test = K.function(test_ins, [test_loss])\n         self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy])\n \n@@ -627,13 +626,12 @@ class Graph(Model, containers.Graph):\n         self.optimizer = optimizers.get(optimizer)\n         updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n         updates += self.updates\n-        state_updates = self.state_updates()\n         self.theano_mode = theano_mode\n         self.loss = loss\n \n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._test = K.function(test_ins, [test_loss])\n-        self._predict = K.function(inputs=ins, outputs=ys_test, updates=state_updates)\n+        self._predict = K.function(inputs=ins, outputs=ys_test, updates=self.state_updates)\n \n     def train_on_batch(self, data, class_weight={}, sample_weight={}):\n         # data is a dictionary mapping output and input names to arrays\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2780cd5154615cc3a5da865744e5bc6731ff2ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 15 | Churn Cumulative: 916 | Contributors (this commit): 14 | Commits (past 90d): 26 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,6 +82,11 @@ class Sequential(Layer):\n                 state_updates += l.get_params()[3]\n         return state_updates\n \n+    def reset_states(self):\n+        for l in self.layers:\n+            if hasattr(l, 'reset_states') and getattr(l, 'stateful', False):\n+                l.reset_states()\n+\n     @property\n     def output_shape(self):\n         return self.layers[-1].output_shape\n@@ -121,11 +126,6 @@ class Sequential(Layer):\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n-    def reset_states(self):\n-        for layer in self.layers:\n-            if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\n-                layer.reset_states()\n-\n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n                 \"layers\": [layer.get_config() for layer in self.layers]}\n@@ -208,6 +208,11 @@ class Graph(Layer):\n                 state_updates += l.get_params()[3]\n         return state_updates\n \n+    def reset_states(self):\n+        for l in self.nodes.values():\n+            if hasattr(l, 'reset_states') and getattr(l, 'stateful', False):\n+                l.reset_states()\n+\n     def set_previous(self, layer, connection_map={}):\n         if self.nb_input != layer.nb_output:\n             raise Exception('Cannot connect layers: input count does not match output count.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ef35e90fc285680f368dc395fb44c680d47421d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 23/13 | Churn Δ: 69 | Churn Cumulative: 5122 | Contributors (this commit): 35 | Commits (past 90d): 36 | Contributors (cumulative): 43 | DMM Complexity: 0.3469387755102041\n\nDIFF:\n@@ -14,7 +14,7 @@ from . import objectives\n from . import regularizers\n from . import constraints\n from . import callbacks as cbks\n-from .utils.layer_utils import container_from_config\n+from .utils.layer_utils import container_from_config, model_summary\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n \n@@ -344,6 +344,9 @@ class Model(object):\n         config = self.get_config()\n         return json.dumps(config, **kwargs)\n \n+    def summary(self):\n+        model_summary(self)\n+\n \n class Sequential(Model, containers.Sequential):\n     '''\n\n@@ -87,6 +87,65 @@ def container_from_config(original_layer_dict, custom_objects={}):\n         return base_layer\n \n \n+def model_summary(model):\n+    param_count = 0  # param count in the model\n+\n+    def display(objects, positions):\n+        line = ''\n+        for i in range(len(objects)):\n+            line += str(objects[i])\n+            line = line[:positions[i]]\n+            line += ' ' * (positions[i] - len(line))\n+        print(line)\n+\n+    def display_layer_info(layer, name, positions):\n+        layer_type = layer.__class__.__name__\n+        output_shape = layer.output_shape\n+        params = layer.count_params()\n+        to_display = ['%s (%s)' % (layer_type, name), output_shape, params]\n+        display(to_display, positions)\n+\n+    line_length = 80  # total length of printed lines\n+    positions = [30, 60, 80]  # absolute positions of log elements in each line\n+    # header names for the different log elements\n+    to_display = ['Layer (name)', 'Output Shape', 'Param #']\n+\n+    # for sequential models, we start by printing\n+    # the expect input shape\n+    if model.__class__.__name__ == 'Sequential':\n+        print('-' * line_length)\n+        print('Initial input shape: ' + str(model.input_shape))\n+\n+    # print header\n+    print('-' * line_length)\n+    display(to_display, positions)\n+    print('-' * line_length)\n+\n+    if model.__class__.__name__ == 'Sequential':\n+        for layer in model.layers:\n+            name = getattr(layer, 'name', 'Unnamed')\n+            display_layer_info(layer, name, positions)\n+            param_count += layer.count_params()\n+\n+    elif model.__class__.__name__ == 'Graph':\n+        for name in model.input_order:\n+            layer = model.inputs[name]\n+            display_layer_info(layer, name, positions)\n+\n+        for name in model.nodes:\n+            layer = model.nodes[name]\n+            display_layer_info(layer, name, positions)\n+            param_count += layer.count_params()\n+\n+        for name in model.output_order:\n+            layer = model.outputs[name]\n+            display_layer_info(layer, name, positions)\n+\n+    print('-' * line_length)\n+    print('Total params: %s' % param_count)\n+    print('-' * line_length)\n+\n+\n from .generic_utils import get_from_module\n def get_layer(identifier, kwargs=None):\n     return get_from_module(identifier, globals(), 'layer',\n\n@@ -8,6 +8,7 @@ from keras.models import Graph, Sequential\n from keras.layers import containers\n from keras.layers.core import Dense, Activation\n from keras.utils.test_utils import get_test_data\n+from keras.utils.layer_utils import model_summary\n \n X = np.random.random((100, 32))\n X2 = np.random.random((100, 32))\n@@ -90,6 +91,8 @@ def test_1o_1i_2():\n     assert(loss < 2.5)\n \n     graph.get_config(verbose=1)\n+    graph.summary()\n+\n \n \n def test_1o_2i():\n\n@@ -10,6 +10,7 @@ from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n import os\n+from keras.utils.layer_utils import model_summary\n \n input_dim = 32\n nb_hidden = 16\n@@ -36,6 +37,7 @@ def test_sequential():\n     model.add(Dense(nb_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    model.summary()\n \n     model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n     model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7da1523053f2e5f4fa15c87e019b3244c8653a53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 121 | Lines Deleted: 135 | Files Changed: 8 | Hunks: 91 | Methods Changed: 64 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 256 | Churn Cumulative: 1578 | Contributors (this commit): 4 | Commits (past 90d): 37 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,7 @@\n import sys\n-import unittest\n+import pytest\n from numpy.testing import assert_allclose\n import numpy as np\n-import pytest\n \n if sys.version_info.major == 2:\n     from keras.backend import theano_backend as KTH\n@@ -39,7 +38,7 @@ def check_two_tensor_operation(function_name, x_input_shape,\n \n \n @pytest.mark.skipif(sys.version_info.major != 2, reason=\"Requires Python 2.7\")\n-class TestBackend(unittest.TestCase):\n+class TestBackend(object):\n \n     def test_linear_operations(self):\n         check_two_tensor_operation('dot', (4, 2), (2, 4))\n@@ -332,4 +331,4 @@ class TestBackend(unittest.TestCase):\n \n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,11 +1,7 @@\n-# encoding: utf-8\n \"\"\"Test keras.layers.core.Layer.__call__\"\"\"\n \n-from __future__ import print_function\n-\n-import unittest\n+import pytest\n import numpy as np\n-\n from numpy.testing import assert_allclose\n \n from keras import backend as K\n@@ -13,10 +9,7 @@ from keras.layers.core import Dense\n from keras.models import Sequential\n \n \n-class TestCall(unittest.TestCase):\n-    \"\"\"Test __call__ methods\"\"\"\n-\n-    def test_layer_call(self):\n+def test_layer_call():\n     \"\"\"Test keras.layers.core.Layer.__call__\"\"\"\n     nb_samples, input_dim, output_dim = 3, 10, 5\n     layer = Dense(output_dim, input_dim=input_dim)\n@@ -30,7 +23,8 @@ class TestCall(unittest.TestCase):\n     t = np.dot(x, W).astype(K.floatx())\n     assert_allclose(t, y, rtol=.2)\n \n-    def test_sequential_call(self):\n+\n+def test_sequential_call():\n     \"\"\"Test keras.models.Sequential.__call__\"\"\"\n     nb_samples, input_dim, output_dim = 3, 10, 5\n     model = Sequential()\n@@ -49,4 +43,4 @@ class TestCall(unittest.TestCase):\n \n \n if __name__ == '__main__':\n-    unittest.main(verbosity=2)\n+    pytest.main([__file__])\n\n@@ -1,4 +1,4 @@\n-import unittest\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -6,8 +6,7 @@ from keras import backend as K\n from keras.layers import convolutional\n \n \n-class TestConvolutions(unittest.TestCase):\n-    def test_convolution_1d(self):\n+def test_convolution_1d():\n     nb_samples = 9\n     nb_steps = 7\n     input_dim = 10\n@@ -44,7 +43,8 @@ class TestConvolutions(unittest.TestCase):\n                                 assert input.shape[1] == out.shape[1]\n                         layer.get_config()\n \n-    def test_maxpooling_1d(self):\n+\n+def test_maxpooling_1d():\n     nb_samples = 9\n     nb_steps = 7\n     input_dim = 10\n@@ -58,7 +58,8 @@ class TestConvolutions(unittest.TestCase):\n             K.eval(layer.get_output(train))\n         layer.get_config()\n \n-    def test_averagepooling_1d(self):\n+\n+def test_averagepooling_1d():\n     nb_samples = 9\n     nb_steps = 7\n     input_dim = 10\n@@ -72,7 +73,8 @@ class TestConvolutions(unittest.TestCase):\n             K.eval(layer.get_output(train))\n         layer.get_config()\n \n-    def test_convolution_2d(self):\n+\n+def test_convolution_2d():\n     nb_samples = 8\n     nb_filter = 9\n     stack_size = 7\n@@ -110,7 +112,8 @@ class TestConvolutions(unittest.TestCase):\n                                     assert out.shape[2:] == input.shape[2:]\n                             layer.get_config()\n \n-    def test_maxpooling_2d(self):\n+\n+def test_maxpooling_2d():\n     nb_samples = 9\n     stack_size = 7\n     input_nb_row = 11\n@@ -127,7 +130,8 @@ class TestConvolutions(unittest.TestCase):\n             K.eval(layer.get_output(train))\n         layer.get_config()\n \n-    def test_averagepooling_2d(self):\n+\n+def test_averagepooling_2d():\n     nb_samples = 9\n     stack_size = 7\n     input_nb_row = 11\n@@ -144,7 +148,8 @@ class TestConvolutions(unittest.TestCase):\n             K.eval(layer.get_output(train))\n         layer.get_config()\n \n-    def test_zero_padding_2d(self):\n+\n+def test_zero_padding_2d():\n     nb_samples = 9\n     stack_size = 7\n     input_nb_row = 11\n@@ -161,7 +166,8 @@ class TestConvolutions(unittest.TestCase):\n         assert_allclose(out[:, :, 2:-2, 2:-2], 1.)\n     layer.get_config()\n \n-    def test_upsampling_1d(self):\n+\n+def test_upsampling_1d():\n     nb_samples = 9\n     nb_steps = 7\n     input_dim = 10\n@@ -175,7 +181,8 @@ class TestConvolutions(unittest.TestCase):\n             assert out.shape[1] == length * nb_steps\n         layer.get_config()\n \n-    def test_upsampling_2d(self):\n+\n+def test_upsampling_2d():\n     nb_samples = 9\n     stack_size = 7\n     input_nb_row = 11\n@@ -193,5 +200,6 @@ class TestConvolutions(unittest.TestCase):\n                 assert out.shape[3] == length_col * input_nb_col\n         layer.get_config()\n \n+\n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,4 +1,4 @@\n-import unittest\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -6,7 +6,7 @@ from keras import backend as K\n from keras.layers import core\n \n \n-class TestLayerBase(unittest.TestCase):\n+class TestLayerBase(object):\n     def test_input_output(self):\n         nb_samples = 10\n         input_dim = 5\n@@ -36,7 +36,7 @@ class TestLayerBase(unittest.TestCase):\n             assert_allclose(K.eval(layer2.get_output(train)), input)\n \n \n-class TestConfigParams(unittest.TestCase):\n+class TestConfigParams(object):\n     \"\"\"\n     Test the constructor, config and params functions of all layers in core.\n     \"\"\"\n@@ -113,7 +113,7 @@ class TestConfigParams(unittest.TestCase):\n         self._runner(layer)\n \n \n-class TestMasking(unittest.TestCase):\n+class TestMasking(object):\n     \"\"\"Test the Masking class\"\"\"\n \n     def test_sequences(self):\n@@ -131,7 +131,7 @@ class TestMasking(unittest.TestCase):\n \n         # get mask for this input\n         output = func([input_data])[0]\n-        self.assertTrue(np.all(output == expected))\n+        assert(np.all(output == expected))\n \n     def test_non_zero(self):\n         \"\"\"Test masking with non-zero mask value\"\"\"\n@@ -144,7 +144,7 @@ class TestMasking(unittest.TestCase):\n                               dtype=np.int32)\n         output = func([input_data])[0]\n         expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n-        self.assertTrue(np.all(output == expected))\n+        assert(np.all(output == expected))\n \n     def test_non_zero_output(self):\n         \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n@@ -159,8 +159,8 @@ class TestMasking(unittest.TestCase):\n         output = func([input_data])[0]\n         expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n                             [[1, 5], [5, 0], [0, 0], [0, 0]]])\n-        self.assertTrue(np.all(output == expected))\n+        assert(np.all(output == expected))\n \n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,4 +1,4 @@\n-import unittest\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -56,19 +56,17 @@ def _runner(layer_class):\n     assert(out2.max() != out3.max())\n \n \n-class TestRNNS(unittest.TestCase):\n-    \"\"\"\n-    Test all the RNNs using a generic test runner function defined above.\n-    \"\"\"\n-    def test_simple(self):\n+def test_SimpleRNN():\n     _runner(recurrent.SimpleRNN)\n \n-    def test_gru(self):\n+\n+def test_GRU():\n     _runner(recurrent.GRU)\n \n-    def test_lstm(self):\n+\n+def test_LSTM():\n     _runner(recurrent.LSTM)\n \n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,8 +1,10 @@\n-import unittest\n-from keras import backend as K\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n+from keras import backend as K\n+from keras import activations\n+\n \n def get_standard_values():\n     '''\n@@ -12,11 +14,7 @@ def get_standard_values():\n     return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n \n \n-class TestActivations(unittest.TestCase):\n-\n-    def test_softmax(self):\n-        from keras.activations import softmax as s\n-\n+def test_softmax():\n     # Test using a reference implementation of softmax\n     def softmax(values):\n         m = np.max(values)\n@@ -24,24 +22,21 @@ class TestActivations(unittest.TestCase):\n         return e / np.sum(e)\n \n     x = K.placeholder(ndim=2)\n-        exp = s(x)\n-        f = K.function([x], [exp])\n+    f = K.function([x], [activations.softmax(x)])\n     test_values = get_standard_values()\n \n     result = f([test_values])[0]\n     expected = softmax(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n-    def test_relu(self):\n+\n+def test_relu():\n     '''\n     Relu implementation doesn't depend on the value being\n     a theano variable. Testing ints, floats and theano tensors.\n     '''\n-        from keras.activations import relu as r\n-\n     x = K.placeholder(ndim=2)\n-        exp = r(x)\n-        f = K.function([x], [exp])\n+    f = K.function([x], [activations.relu(x)])\n \n     test_values = get_standard_values()\n     result = f([test_values])[0]\n@@ -49,28 +44,28 @@ class TestActivations(unittest.TestCase):\n     # because no negatives in test values\n     assert_allclose(result, test_values, rtol=1e-05)\n \n-    def test_tanh(self):\n-        from keras.activations import tanh as t\n+\n+def test_tanh():\n     test_values = get_standard_values()\n \n     x = K.placeholder(ndim=2)\n-        exp = t(x)\n+    exp = activations.tanh(x)\n     f = K.function([x], [exp])\n \n     result = f([test_values])[0]\n     expected = np.tanh(test_values)\n     assert_allclose(result, expected, rtol=1e-05)\n \n-    def test_linear(self):\n+\n+def test_linear():\n     '''\n     This function does no input validation, it just returns the thing\n     that was passed in.\n     '''\n-        from keras.activations import linear as l\n-\n     xs = [1, 5, True, None, 'foo']\n     for x in xs:\n-            assert x == l(x)\n+        assert(x == activations.linear(x))\n+\n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,26 +1,25 @@\n-import unittest\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n+\n from keras import backend as K\n+from keras import constraints\n \n \n-class TestConstraints(unittest.TestCase):\n-    def setUp(self):\n-        self.some_values = [0.1, 0.5, 3, 8, 1e-7]\n+test_values = [0.1, 0.5, 3, 8, 1e-7]\n np.random.seed(3537)\n-        self.example_array = np.random.random((100, 100)) * 100. - 50.\n-        self.example_array[0, 0] = 0.  # 0 could possibly cause trouble\n+example_array = np.random.random((100, 100)) * 100. - 50.\n+example_array[0, 0] = 0.  # 0 could possibly cause trouble\n \n-    def test_maxnorm(self):\n-        from keras.constraints import maxnorm\n \n-        for m in self.some_values:\n-            norm_instance = maxnorm(m)\n-            normed = norm_instance(K.variable(self.example_array))\n+def test_maxnorm():\n+    for m in test_values:\n+        norm_instance = constraints.maxnorm(m)\n+        normed = norm_instance(K.variable(example_array))\n         assert(np.all(K.eval(normed) < m))\n \n     # a more explicit example\n-        norm_instance = maxnorm(2.0)\n+    norm_instance = constraints.maxnorm(2.0)\n     x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n     x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n                                 [2.0, 0, 0],\n@@ -28,45 +27,39 @@ class TestConstraints(unittest.TestCase):\n     x_normed_actual = K.eval(norm_instance(K.variable(x)))\n     assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n \n-    def test_nonneg(self):\n-        from keras.constraints import nonneg\n \n-        nonneg_instance = nonneg()\n-\n-        normed = nonneg_instance(K.variable(self.example_array))\n+def test_nonneg():\n+    nonneg_instance = constraints.nonneg()\n+    normed = nonneg_instance(K.variable(example_array))\n     assert(np.all(np.min(K.eval(normed), axis=1) == 0.))\n \n-    def test_identity(self):\n-        from keras.constraints import identity\n \n-        identity_instance = identity()\n+def test_identity():\n+    identity_instance = constraints.identity()\n+    normed = identity_instance(example_array)\n+    assert(np.all(normed == example_array))\n \n-        normed = identity_instance(self.example_array)\n-        assert (np.all(normed == self.example_array))\n \n-    def test_identity_oddballs(self):\n+def test_identity_oddballs():\n     \"\"\"\n     test the identity constraint on some more exotic input.\n     this does not need to pass for the desired real life behaviour,\n     but it should in the current implementation.\n     \"\"\"\n-        from keras.constraints import identity\n-        identity_instance = identity()\n-\n+    identity_instance = constraints.identity()\n     oddball_examples = [\"Hello\", [1], -1, None]\n     assert(oddball_examples == identity_instance(oddball_examples))\n \n-    def test_unitnorm(self):\n-        from keras.constraints import unitnorm\n-        unitnorm_instance = unitnorm()\n-\n-        normalized = unitnorm_instance(K.variable(self.example_array))\n \n+def test_unitnorm():\n+    unitnorm_instance = constraints.unitnorm()\n+    normalized = unitnorm_instance(K.variable(example_array))\n     norm_of_normalized = np.sqrt(np.sum(K.eval(normalized)**2, axis=1))\n     # in the unit norm constraint, it should be equal to 1.\n     difference = norm_of_normalized - 1.\n     largest_difference = np.max(np.abs(difference))\n-        assert np.abs(largest_difference) < 10e-5\n+    assert(np.abs(largest_difference) < 10e-5)\n+\n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,28 +1,20 @@\n-import unittest\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n+\n from keras.layers import normalization\n from keras.models import Sequential\n from keras import backend as K\n \n \n-class TestBatchNormalization(unittest.TestCase):\n-    def setUp(self):\n-        self.input_1 = np.arange(10)\n-        self.input_2 = np.zeros(10)\n-        self.input_3 = np.ones((10))\n+input_1 = np.arange(10)\n+input_2 = np.zeros(10)\n+input_3 = np.ones((10))\n+input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n-        self.input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n-    def test_setup(self):\n-        norm_m0 = normalization.BatchNormalization(input_shape=(10, 10))\n-        norm_m1 = normalization.BatchNormalization(input_shape=(10, 10), mode=1)\n-\n-        # mode 3 does not exist\n-        self.assertRaises(Exception,\n-                          normalization.BatchNormalization(input_shape=(10, 10), mode=3))\n-\n-    def test_mode_0(self):\n+def test_batchnorm_mode_0():\n+    np.random.seed(1337)\n     model = Sequential()\n     norm_m0 = normalization.BatchNormalization(input_shape=(10,))\n     model.add(norm_m0)\n@@ -34,26 +26,29 @@ class TestBatchNormalization(unittest.TestCase):\n     norm_m0.input = K.variable(X)\n     out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n \n-        self.assertAlmostEqual(K.eval(K.mean(out)), 0.0, places=1)\n-        self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=1)\n+    assert_allclose(K.eval(K.mean(out)), 0.0, atol=1e-1)\n+    assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n \n-    def test_mode_1(self):\n+\n+def test_batchnorm_mode_1():\n+    np.random.seed(1337)\n     norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n \n-        for inp in [self.input_1, self.input_2, self.input_3]:\n+    for inp in [input_1, input_2, input_3]:\n         norm_m1.input = K.variable(inp)\n         out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n-            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n+        assert_allclose(K.eval(K.mean(out)), 0.0, atol=1e-1)\n         if inp.std() > 0.:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n+            assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n         else:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n+            assert_allclose(K.eval(K.std(out)), 0.0, atol=1e-1)\n \n-    def test_shapes(self):\n+\n+def test_batchnorm_shapes():\n     \"\"\"\n     Test batch normalization with various input shapes\n     \"\"\"\n-        for inp in self.input_shapes:\n+    for inp in input_shapes:\n         norm_m0 = normalization.BatchNormalization(input_shape=inp.shape, mode=0)\n         norm_m0.input = K.variable(inp)\n         out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n@@ -62,35 +57,39 @@ class TestBatchNormalization(unittest.TestCase):\n         norm_m1.input = K.variable(inp)\n         out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n \n-    def test_weight_init(self):\n+\n+def test_batchnorm_weight_init():\n     \"\"\"\n     Test weight initialization\n     \"\"\"\n+    np.random.seed(1337)\n     norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1,\n                                                weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n \n-        for inp in [self.input_1, self.input_2, self.input_3]:\n+    for inp in [input_1, input_2, input_3]:\n         norm_m1.input = K.variable(inp)\n         out = (norm_m1.get_output(train=True) - np.ones(10)) / 1.\n-            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n+        assert_allclose(K.eval(K.mean(out)), 0.0, atol=1e-1)\n         if inp.std() > 0.:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n+            assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n         else:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n+            assert_allclose(K.eval(K.std(out)), 0.0, atol=1e-1)\n \n-        assert_allclose(K.eval(norm_m1.gamma), np.ones(10))\n-        assert_allclose(K.eval(norm_m1.beta), np.ones(10))\n+    assert_allclose(K.eval(norm_m1.gamma), np.ones(10), atol=1e-1)\n+    assert_allclose(K.eval(norm_m1.beta), np.ones(10), atol=1e-1)\n \n-    def test_config(self):\n+\n+def test_batchnorm_config():\n     norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n                                             epsilon=0.1, momentum=0.9)\n     conf = norm.get_config()\n     conf_target = {\"input_shape\": (10, 10),\n                    \"name\": normalization.BatchNormalization.__name__,\n                    \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n-        self.assertDictEqual(conf, conf_target)\n+    assert(conf == conf_target)\n \n-    def test_save_weights(self):\n+\n+def test_batchnorm_save_weights():\n     norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n                                             epsilon=0.1)\n     weights = norm.get_weights()\n@@ -99,4 +98,4 @@ class TestBatchNormalization(unittest.TestCase):\n \n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
