{"custom_id": "keras#641bffc9ba23bcdcf98b46d5fb9a0536f8345e8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 93 | Churn Cumulative: 95 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,2 +1,3 @@\n from keras_core.layers.activations.elu import ELU\n+from keras_core.layers.activations.leaky_relu import LeakyReLU\n from keras_core.layers.activations.relu import ReLU\n\n@@ -0,0 +1,56 @@\n+from keras_core import activations\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.LeakyReLU\")\n+class LeakyReLU(Layer):\n+    \"\"\"Leaky version of a Rectified Linear Unit activation layer.\n+\n+    The layer allows a small gradient when the unit is not active.\n+\n+    Formula:\n+    ``` python\n+    f(x) = alpha * x if x < 0\n+    f(x) = x if x >= 0\n+    ```\n+\n+    Example:\n+    ``` python\n+    leaky_relu_layer = LeakyReLU(negative_slope=0.5)\n+    input = np.array([-10, -5, 0.0, 5, 10])\n+    result = leaky_relu_layer(input)\n+    # result = [-5. , -2.5,  0. ,  5. , 10.]\n+    ```\n+\n+    Args:\n+        negative_slope: Float >= 0.0. Negative slope coefficient.\n+          Defaults to 0.3.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+\n+    \"\"\"\n+\n+    def __init__(self, negative_slope=0.3, **kwargs):\n+        super().__init__(**kwargs)\n+        if negative_slope is None:\n+            raise ValueError(\n+                \"The negative_slope value of a Leaky ReLU layer \"\n+                \"cannot be None, Expecting a float. Received \"\n+                f\"negative_slope: {negative_slope}\"\n+            )\n+        self.supports_masking = True\n+        self.negative_slope = negative_slope\n+\n+    def call(self, inputs):\n+        return activations.leaky_relu(\n+            inputs, negative_slope=self.negative_slope\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"negative_slope\": self.negative_slope})\n+        return config\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -0,0 +1,36 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core.layers.activations import leaky_relu\n+\n+\n+class LeakyReLUTest(testing.TestCase):\n+    def test_relu(self):\n+        self.run_layer_test(\n+            leaky_relu.LeakyReLU,\n+            init_kwargs={\n+                \"negative_slope\": 1,\n+            },\n+            input_shape=(2, 3, 4),\n+            supports_masking=True,\n+        )\n+\n+    def test_leaky_relu_correctness(self):\n+        leaky_relu_layer = leaky_relu.LeakyReLU(negative_slope=0.5)\n+        input = np.array([-10, -5, 0.0, 5, 10])\n+        expected_output = np.array([-5.0, -2.5, 0.0, 5.0, 10.0])\n+        result = leaky_relu_layer(input)\n+        self.assertAllClose(result, expected_output)\n+\n+    def test_invalid_usage(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The negative_slope value of a Leaky ReLU layer cannot be None, \"\n+            \"Expecting a float. Received negative_slope: None\",\n+        ):\n+            self.run_layer_test(\n+                leaky_relu.LeakyReLU,\n+                init_kwargs={\"negative_slope\": None},\n+                input_shape=(2, 3, 4),\n+                supports_masking=True,\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c2a29373304d7e24a61e81233ba98ad2fd3d268", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1477 | Lines Deleted: 2 | Files Changed: 5 | Hunks: 6 | Methods Changed: 43 | Complexity Δ (Sum/Max): 96/51 | Churn Δ: 1479 | Churn Cumulative: 4442 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 10 | DMM Complexity: 0.9647519582245431\n\nDIFF:\n@@ -1,2 +1,3 @@\n from keras_core.layers.activations.elu import ELU\n+from keras_core.layers.activations.leaky_relu import LeakyReLU\n from keras_core.layers.activations.relu import ReLU\n\n@@ -0,0 +1,56 @@\n+from keras_core import activations\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.LeakyReLU\")\n+class LeakyReLU(Layer):\n+    \"\"\"Leaky version of a Rectified Linear Unit activation layer.\n+\n+    The layer allows a small gradient when the unit is not active.\n+\n+    Formula:\n+    ``` python\n+    f(x) = alpha * x if x < 0\n+    f(x) = x if x >= 0\n+    ```\n+\n+    Example:\n+    ``` python\n+    leaky_relu_layer = LeakyReLU(negative_slope=0.5)\n+    input = np.array([-10, -5, 0.0, 5, 10])\n+    result = leaky_relu_layer(input)\n+    # result = [-5. , -2.5,  0. ,  5. , 10.]\n+    ```\n+\n+    Args:\n+        negative_slope: Float >= 0.0. Negative slope coefficient.\n+          Defaults to 0.3.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+\n+    \"\"\"\n+\n+    def __init__(self, negative_slope=0.3, **kwargs):\n+        super().__init__(**kwargs)\n+        if negative_slope is None:\n+            raise ValueError(\n+                \"The negative_slope value of a Leaky ReLU layer \"\n+                \"cannot be None, Expecting a float. Received \"\n+                f\"negative_slope: {negative_slope}\"\n+            )\n+        self.supports_masking = True\n+        self.negative_slope = negative_slope\n+\n+    def call(self, inputs):\n+        return activations.leaky_relu(\n+            inputs, negative_slope=self.negative_slope\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"negative_slope\": self.negative_slope})\n+        return config\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -0,0 +1,36 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core.layers.activations import leaky_relu\n+\n+\n+class LeakyReLUTest(testing.TestCase):\n+    def test_relu(self):\n+        self.run_layer_test(\n+            leaky_relu.LeakyReLU,\n+            init_kwargs={\n+                \"negative_slope\": 1,\n+            },\n+            input_shape=(2, 3, 4),\n+            supports_masking=True,\n+        )\n+\n+    def test_leaky_relu_correctness(self):\n+        leaky_relu_layer = leaky_relu.LeakyReLU(negative_slope=0.5)\n+        input = np.array([-10, -5, 0.0, 5, 10])\n+        expected_output = np.array([-5.0, -2.5, 0.0, 5.0, 10.0])\n+        result = leaky_relu_layer(input)\n+        self.assertAllClose(result, expected_output)\n+\n+    def test_invalid_usage(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The negative_slope value of a Leaky ReLU layer cannot be None, \"\n+            \"Expecting a float. Received negative_slope: None\",\n+        ):\n+            self.run_layer_test(\n+                leaky_relu.LeakyReLU,\n+                init_kwargs={\"negative_slope\": None},\n+                input_shape=(2, 3, 4),\n+                supports_masking=True,\n+            )\n\n@@ -1,3 +1,933 @@\n+\"\"\"Various learning rate schedule functions.\"\"\"\n+\n+import math\n+\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.LearningRateSchedule\")\n class LearningRateSchedule:\n-    # TODO\n-    pass\n+    \"\"\"The learning rate schedule base class.\n+\n+    You can use a learning rate schedule to modulate how the learning rate\n+    of your optimizer changes over time.\n+\n+    Several built-in learning rate schedules are available, such as\n+    `keras_core.optimizers.schedules.ExponentialDecay` or\n+    `keras_core.optimizers.schedules.PiecewiseConstantDecay`:\n+\n+    ```python\n+    lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+        initial_learning_rate=1e-2,\n+        decay_steps=10000,\n+        decay_rate=0.9)\n+    optimizer = keras_core.optimizers.SGD(learning_rate=lr_schedule)\n+    ```\n+\n+    A `LearningRateSchedule` instance can be passed in as the `learning_rate`\n+    argument of any optimizer.\n+\n+    To implement your own schedule object, you should implement the `__call__`\n+    method, which takes a `step` argument (scalar integer tensor, the\n+    current training step count).\n+    Like for any other Keras object, you can also optionally\n+    make your object serializable by implementing the `get_config`\n+    and `from_config` methods.\n+\n+    Example:\n+\n+    ```python\n+    class MyLRSchedule(keras_core.optimizers.schedules.LearningRateSchedule):\n+\n+        def __init__(self, initial_learning_rate):\n+            self.initial_learning_rate = initial_learning_rate\n+\n+        def __call__(self, step):\n+            return self.initial_learning_rate / (step + 1)\n+\n+    optimizer = keras_core.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n+    ```\n+    \"\"\"\n+\n+    def __call__(self, step):\n+        raise NotImplementedError(\n+            f\"Learning rate schedule '{self.__class__.__name__}' \"\n+            \"must override `__call__(self, step)`.\"\n+        )\n+\n+    def get_config(self):\n+        raise NotImplementedError(\n+            f\"Learning rate schedule '{self.__class__.__name__}' \"\n+            \"must override `get_config()` in order to be serializable.\"\n+        )\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        \"\"\"Instantiates a `LearningRateSchedule` from its config.\n+\n+        Args:\n+            config: Output of `get_config()`.\n+\n+        Returns:\n+            A `LearningRateSchedule` instance.\n+        \"\"\"\n+        return cls(**config)\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.ExponentialDecay\")\n+class ExponentialDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses an exponential decay schedule.\n+\n+    When training a model, it is often useful to lower the learning rate as\n+    the training progresses. This schedule applies an exponential decay function\n+    to an optimizer step, given a provided initial learning rate.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning\n+    rate when passed the current optimizer step. This can be useful for changing\n+    the learning rate value across different invocations of optimizer functions.\n+    It is computed as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        return initial_learning_rate * decay_rate ^ (step / decay_steps)\n+    ```\n+\n+    If the argument `staircase` is `True`, then `step / decay_steps` is\n+    an integer division and the decayed learning rate follows a\n+    staircase function.\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate.\n+    Example: When fitting a Keras model, decay every 100000 steps with a base\n+    of 0.96:\n+\n+    ```python\n+    initial_learning_rate = 0.1\n+    lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+        initial_learning_rate,\n+        decay_steps=100000,\n+        decay_rate=0.96,\n+        staircase=True)\n+\n+    model.compile(optimizer=keras_core.optimizers.SGD(learning_rate=lr_schedule),\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.fit(data, labels, epochs=5)\n+    ```\n+\n+    The learning rate schedule is also serializable and deserializable using\n+    `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: A Python integer. Must be positive. See the decay\n+            computation above.\n+        decay_rate: A Python float. The decay rate.\n+        staircase: Boolean.  If `True` decay the learning rate at discrete\n+            intervals.\n+        name: String.  Optional name of the operation.  Defaults to\n+            `\"ExponentialDecay`\".\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        decay_rate,\n+        staircase=False,\n+        name=\"ExponentialDecay\",\n+    ):\n+        super().__init__()\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.decay_rate = decay_rate\n+        self.staircase = staircase\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            decay_steps = ops.cast(self.decay_steps, dtype)\n+            decay_rate = ops.cast(self.decay_rate, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            p = global_step_recomp / decay_steps\n+            if self.staircase:\n+                p = ops.floor(p)\n+            return ops.multiply(initial_learning_rate, ops.power(decay_rate, p))\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"decay_rate\": self.decay_rate,\n+            \"staircase\": self.staircase,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.PiecewiseConstantDecay\")\n+class PiecewiseConstantDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a piecewise constant decay schedule.\n+\n+    The function returns a 1-arg callable to compute the piecewise constant\n+    when passed the current optimizer step. This can be useful for changing the\n+    learning rate value across different invocations of optimizer functions.\n+\n+    Example: use a learning rate that's 1.0 for the first 100001 steps, 0.5\n+        for the next 10000 steps, and 0.1 for any additional steps.\n+\n+    ```python\n+    step = ops.array(0)\n+    boundaries = [100000, 110000]\n+    values = [1.0, 0.5, 0.1]\n+    learning_rate_fn = keras_core.optimizers.schedules.PiecewiseConstantDecay(\n+        boundaries, values)\n+\n+    # Later, whenever we perform an optimization step, we pass in the step.\n+    learning_rate = learning_rate_fn(step)\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate. The learning rate schedule is also serializable and\n+    deserializable using `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        boundaries: A list of Python numbers with strictly increasing\n+            entries, and with all elements having the same type as the\n+            optimizer step.\n+        values: A list of Python numbers that specifies the values for the\n+            intervals defined by `boundaries`. It should have one more\n+            element than `boundaries`, and all elements should have the same\n+            type.\n+        name: A string. Optional name of the operation. Defaults to\n+            `\"PiecewiseConstant\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as the boundary tensors.\n+\n+        The output of the 1-arg function that takes the `step`\n+        is `values[0]` when `step <= boundaries[0]`,\n+        `values[1]` when `step > boundaries[0]` and `step <= boundaries[1]`,\n+        ..., and `values[-1]` when `step > boundaries[-1]`.\n+\n+\n+    Raises:\n+        ValueError: if the number of elements in the `boundaries` and `values`\n+        lists do not match.\n+    \"\"\"\n+\n+    def __init__(self, boundaries, values, name=\"PiecewiseConstant\"):\n+        super().__init__()\n+\n+        if len(boundaries) != len(values) - 1:\n+            raise ValueError(\n+                \"The length of boundaries should be 1 less than the length of \"\n+                f\"values. Received: boundaries={boundaries} of length \"\n+                f\"{len(boundaries)}, and values={values} \"\n+                f\"of length {len(values)}.\"\n+            )\n+\n+        self.boundaries = boundaries\n+        self.values = values\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            boundaries = [ops.convert_to_tensor(x) for x in self.boundaries]\n+            values = [ops.convert_to_tensor(x) for x in self.values]\n+            step = ops.convert_to_tensor(step)\n+\n+            for i, b in enumerate(boundaries):\n+                if b.dtype != step.dtype:\n+                    # We cast the boundaries to have the same type as the step\n+                    b = ops.cast(b, step.dtype)\n+                    boundaries[i] = b\n+\n+            result_dtype = values[0].dtype\n+            result_value = ops.array(0, dtype=result_dtype)\n+\n+            # For each range between boundaries, we check whether the step is\n+            # within that range, cast the resulting boolean to a number,\n+            # and multiply the result by the corresponding value for the range.\n+            # Taking the sum of these yields a piecewise constant function.\n+            step_less_than_first_boundary = ops.cast(\n+                step <= boundaries[0], result_dtype\n+            )\n+            result_value += step_less_than_first_boundary * values[0]\n+\n+            step_greater_than_last_boundary = ops.cast(\n+                step > boundaries[-1], result_dtype\n+            )\n+            result_value += step_greater_than_last_boundary * values[-1]\n+\n+            for low, high, value in zip(\n+                boundaries[:-1], boundaries[1:], values[1:-1]\n+            ):\n+                # Need to bind v here; can do this with lambda v=v: ...\n+                step_in_range = ops.cast(\n+                    (step > low) & (step <= high), result_dtype\n+                )\n+                result_value += step_in_range * value\n+\n+            return result_value\n+\n+    def get_config(self):\n+        return {\n+            \"boundaries\": self.boundaries,\n+            \"values\": self.values,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.PolynomialDecay\")\n+class PolynomialDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a polynomial decay schedule.\n+\n+    It is commonly observed that a monotonically decreasing learning rate, whose\n+    degree of change is carefully chosen, results in a better performing model.\n+    This schedule applies a polynomial decay function to an optimizer step,\n+    given a provided `initial_learning_rate`, to reach an `end_learning_rate`\n+    in the given `decay_steps`.\n+\n+    It requires a `step` value to compute the decayed learning rate. You\n+    can just pass a backend variable that you increment at each training\n+    step.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning rate\n+    when passed the current optimizer step. This can be useful for changing the\n+    learning rate value across different invocations of optimizer functions.\n+    It is computed as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        step = min(step, decay_steps)\n+        return ((initial_learning_rate - end_learning_rate) *\n+                (1 - step / decay_steps) ^ (power)\n+               ) + end_learning_rate\n+    ```\n+\n+    If `cycle` is True then a multiple of `decay_steps` is used, the first one\n+    that is bigger than `step`.\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        decay_steps = decay_steps * ceil(step / decay_steps)\n+        return ((initial_learning_rate - end_learning_rate) *\n+                (1 - step / decay_steps) ^ (power)\n+               ) + end_learning_rate\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate.\n+    Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using\n+    sqrt (i.e. power=0.5):\n+\n+    ```python\n+    ...\n+    starter_learning_rate = 0.1\n+    end_learning_rate = 0.01\n+    decay_steps = 10000\n+    learning_rate_fn = keras_core.optimizers.schedules.PolynomialDecay(\n+        starter_learning_rate,\n+        decay_steps,\n+        end_learning_rate,\n+        power=0.5)\n+\n+    model.compile(optimizer=keras_core.optimizers.SGD(\n+                      learning_rate=learning_rate_fn),\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.fit(data, labels, epochs=5)\n+    ```\n+\n+    The learning rate schedule is also serializable and deserializable using\n+    `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: A Python integer. Must be positive. See the decay\n+            computation above.\n+        end_learning_rate: A Python float. The minimal end learning rate.\n+        power: A Python float. The power of the polynomial. Defaults to\n+            `1.0`.\n+        cycle: A boolean, whether it should cycle beyond decay_steps.\n+        name: String.  Optional name of the operation. Defaults to\n+            `\"PolynomialDecay\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        end_learning_rate=0.0001,\n+        power=1.0,\n+        cycle=False,\n+        name=\"PolynomialDecay\",\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.end_learning_rate = end_learning_rate\n+        self.power = power\n+        self.cycle = cycle\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            end_learning_rate = ops.cast(self.end_learning_rate, dtype)\n+            power = ops.cast(self.power, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            decay_steps_recomp = ops.cast(self.decay_steps, dtype)\n+            if self.cycle:\n+                # Find the first multiple of decay_steps that is bigger than\n+                # global_step. If global_step is zero set the multiplier to 1\n+                multiplier = ops.where(\n+                    ops.equal(global_step_recomp, 0),\n+                    1.0,\n+                    ops.ceil(global_step_recomp / self.decay_steps),\n+                )\n+                decay_steps_recomp = ops.multiply(\n+                    decay_steps_recomp, multiplier\n+                )\n+            else:\n+                # Make sure that the global_step used is not bigger than\n+                # decay_steps.\n+                global_step_recomp = ops.minimum(\n+                    global_step_recomp, decay_steps_recomp\n+                )\n+\n+            p = ops.divide(global_step_recomp, decay_steps_recomp)\n+            return ops.add(\n+                ops.multiply(\n+                    initial_learning_rate - end_learning_rate,\n+                    ops.power(1 - p, power),\n+                ),\n+                end_learning_rate,\n+            )\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"end_learning_rate\": self.end_learning_rate,\n+            \"power\": self.power,\n+            \"cycle\": self.cycle,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.InverseTimeDecay\")\n+class InverseTimeDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses an inverse time decay schedule.\n+\n+    When training a model, it is often useful to lower the learning rate as\n+    the training progresses. This schedule applies the inverse decay function\n+    to an optimizer step, given a provided initial learning rate.\n+    It requires a `step` value to compute the decayed learning rate. You can\n+    just pass a backend variable that you increment at each training step.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning\n+    rate when passed the current optimizer step. This can be useful for changing\n+    the learning rate value across different invocations of optimizer functions.\n+    It is computed as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        return initial_learning_rate / (1 + decay_rate * step / decay_step)\n+    ```\n+\n+    or, if `staircase` is `True`, as:\n+\n+    ```python\n+    def decayed_learning_rate(step):\n+        return initial_learning_rate /\n+               (1 + decay_rate * floor(step / decay_step))\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate.\n+    Example: Fit a Keras model when decaying 1/t with a rate of 0.5:\n+\n+    ```python\n+    ...\n+    initial_learning_rate = 0.1\n+    decay_steps = 1.0\n+    decay_rate = 0.5\n+    learning_rate_fn = keras_core.optimizers.schedules.InverseTimeDecay(\n+        initial_learning_rate, decay_steps, decay_rate)\n+\n+    model.compile(optimizer=keras_core.optimizers.SGD(\n+                      learning_rate=learning_rate_fn),\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+\n+    model.fit(data, labels, epochs=5)\n+    ```\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: How often to apply decay.\n+        decay_rate: A Python number.  The decay rate.\n+        staircase: Whether to apply decay in a discrete staircase, as o\n+        pposed to continuous, fashion.\n+        name: String.  Optional name of the operation.  Defaults to\n+            `\"InverseTimeDecay\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        decay_rate,\n+        staircase=False,\n+        name=\"InverseTimeDecay\",\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.decay_rate = decay_rate\n+        self.staircase = staircase\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            decay_steps = ops.cast(self.decay_steps, dtype)\n+            decay_rate = ops.cast(self.decay_rate, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            p = global_step_recomp / decay_steps\n+            if self.staircase:\n+                p = ops.floor(p)\n+            const = ops.cast(ops.array(1), dtype)\n+            denom = ops.add(const, ops.multiply(decay_rate, p))\n+            return ops.divide(initial_learning_rate, denom)\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"decay_rate\": self.decay_rate,\n+            \"staircase\": self.staircase,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.CosineDecay\")\n+class CosineDecay(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a cosine decay with optional warmup.\n+\n+    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n+    SGDR: Stochastic Gradient Descent with Warm Restarts.\n+\n+    For the idea of a linear warmup of our learning rate,\n+    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n+\n+    When we begin training a model, we often want an initial increase in our\n+    learning rate followed by a decay. If `warmup_target` is an int, this\n+    schedule applies a linear increase per optimizer step to our learning rate\n+    from `initial_learning_rate` to `warmup_target` for a duration of\n+    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n+    learning rate from `warmup_target` to `alpha` for a duration of\n+    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n+    will take our learning rate from `initial_learning_rate` to `alpha`.\n+    It requires a `step` value to  compute the learning rate. You can\n+    just pass a backend variable that you increment at each training step.\n+\n+    The schedule is a 1-arg callable that produces a warmup followed by a\n+    decayed learning rate when passed the current optimizer step. This can be\n+    useful for changing the learning rate value across different invocations of\n+    optimizer functions.\n+\n+    Our warmup is computed as:\n+\n+    ```python\n+    def warmup_learning_rate(step):\n+        completed_fraction = step / warmup_steps\n+        total_delta = target_warmup - initial_learning_rate\n+        return completed_fraction * total_delta\n+    ```\n+\n+    And our decay is computed as:\n+\n+    ```python\n+    if warmup_target is None:\n+        initial_decay_lr = initial_learning_rate\n+    else:\n+        initial_decay_lr = warmup_target\n+\n+    def decayed_learning_rate(step):\n+        step = min(step, decay_steps)\n+        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n+        decayed = (1 - alpha) * cosine_decay + alpha\n+        return initial_decay_lr * decayed\n+    ```\n+\n+    Example usage without warmup:\n+\n+    ```python\n+    decay_steps = 1000\n+    initial_learning_rate = 0.1\n+    lr_decayed_fn = keras_core.optimizers.schedules.CosineDecay(\n+        initial_learning_rate, decay_steps)\n+    ```\n+\n+    Example usage with warmup:\n+\n+    ```python\n+    decay_steps = 1000\n+    initial_learning_rate = 0\n+    warmup_steps = 1000\n+    target_learning_rate = 0.1\n+    lr_warmup_decayed_fn = keras_core.optimizers.schedules.CosineDecay(\n+        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n+        warmup_steps=warmup_steps\n+    )\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate. The learning rate schedule is also serializable and\n+    deserializable using `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        decay_steps: A Python int. Number of steps to decay over.\n+        alpha: A Python float. Minimum learning rate value for decay as a\n+            fraction of `initial_learning_rate`.\n+        name: String. Optional name of the operation.  Defaults to\n+            `\"CosineDecay\"`.\n+        warmup_target: A Python float. The target learning rate for our\n+            warmup phase. Will cast to the `initial_learning_rate` datatype.\n+            Setting to `None` will skip warmup and begins decay phase from\n+            `initial_learning_rate`. Otherwise scheduler will warmup from\n+            `initial_learning_rate` to `warmup_target`.\n+        warmup_steps: A Python int. Number of steps to warmup over.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        alpha=0.0,\n+        name=\"CosineDecay\",\n+        warmup_target=None,\n+        warmup_steps=0,\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.decay_steps = decay_steps\n+        self.alpha = alpha\n+        self.name = name\n+        self.warmup_steps = warmup_steps\n+        self.warmup_target = warmup_target\n+\n+    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n+        with ops.name_scope(self.name):\n+            completed_fraction = step / decay_steps\n+            pi = ops.array(math.pi, dtype=dtype)\n+            cosine_decayed = 0.5 * (1.0 + ops.cos(pi * completed_fraction))\n+            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n+            return ops.multiply(decay_from_lr, decayed)\n+\n+    def _warmup_function(\n+        self, step, warmup_steps, warmup_target, initial_learning_rate\n+    ):\n+        with ops.name_scope(self.name):\n+            completed_fraction = step / warmup_steps\n+            total_step_delta = warmup_target - initial_learning_rate\n+            return total_step_delta * completed_fraction + initial_learning_rate\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            decay_steps = ops.cast(self.decay_steps, dtype)\n+            global_step_recomp = ops.cast(step, dtype)\n+\n+            if self.warmup_target is None:\n+                global_step_recomp = ops.minimum(\n+                    global_step_recomp, decay_steps\n+                )\n+                return self._decay_function(\n+                    global_step_recomp,\n+                    decay_steps,\n+                    initial_learning_rate,\n+                    dtype,\n+                )\n+\n+            warmup_target = ops.cast(self.warmup_target, dtype)\n+            warmup_steps = ops.cast(self.warmup_steps, dtype)\n+\n+            global_step_recomp = ops.minimum(\n+                global_step_recomp, decay_steps + warmup_steps\n+            )\n+\n+            return ops.cond(\n+                global_step_recomp < warmup_steps,\n+                lambda: self._warmup_function(\n+                    global_step_recomp,\n+                    warmup_steps,\n+                    warmup_target,\n+                    initial_learning_rate,\n+                ),\n+                lambda: self._decay_function(\n+                    global_step_recomp - warmup_steps,\n+                    decay_steps,\n+                    warmup_target,\n+                    dtype,\n+                ),\n+            )\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"decay_steps\": self.decay_steps,\n+            \"alpha\": self.alpha,\n+            \"name\": self.name,\n+            \"warmup_target\": self.warmup_target,\n+            \"warmup_steps\": self.warmup_steps,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.CosineDecayRestarts\")\n+class CosineDecayRestarts(LearningRateSchedule):\n+    \"\"\"A `LearningRateSchedule` that uses a cosine decay schedule with restarts.\n+\n+    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n+    SGDR: Stochastic Gradient Descent with Warm Restarts.\n+\n+    When training a model, it is often useful to lower the learning rate as\n+    the training progresses. This schedule applies a cosine decay function with\n+    restarts to an optimizer step, given a provided initial learning rate.\n+    It requires a `step` value to compute the decayed learning rate. You can\n+    just pass a backend variable that you increment at each training step.\n+\n+    The schedule is a 1-arg callable that produces a decayed learning\n+    rate when passed the current optimizer step. This can be useful for changing\n+    the learning rate value across different invocations of optimizer functions.\n+\n+    The learning rate multiplier first decays\n+    from 1 to `alpha` for `first_decay_steps` steps. Then, a warm\n+    restart is performed. Each new warm restart runs for `t_mul` times more\n+    steps and with `m_mul` times initial learning rate as the new learning rate.\n+\n+    Example usage:\n+    ```python\n+    first_decay_steps = 1000\n+    lr_decayed_fn = (\n+        keras_core.optimizers.schedules.CosineDecayRestarts(\n+            initial_learning_rate,\n+            first_decay_steps))\n+    ```\n+\n+    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    as the learning rate. The learning rate schedule is also serializable and\n+    deserializable using `keras_core.optimizers.schedules.serialize` and\n+    `keras_core.optimizers.schedules.deserialize`.\n+\n+    Args:\n+        initial_learning_rate: A Python float. The initial learning rate.\n+        first_decay_steps: A Python integer. Number of steps to decay over.\n+        t_mul: A Python float. Used to derive the number of iterations in\n+            the i-th period.\n+        m_mul: A Python float. Used to derive the initial learning rate of\n+            the i-th period.\n+        alpha: A Python float. Minimum learning rate value as a fraction of\n+            the `initial_learning_rate`.\n+        name: String. Optional name of the operation. Defaults to\n+            `\"SGDRDecay\"`.\n+\n+    Returns:\n+        A 1-arg callable learning rate schedule that takes the current optimizer\n+        step and outputs the decayed learning rate, a scalar tensor of the\n+        same type as `initial_learning_rate`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_learning_rate,\n+        first_decay_steps,\n+        t_mul=2.0,\n+        m_mul=1.0,\n+        alpha=0.0,\n+        name=\"SGDRDecay\",\n+    ):\n+        super().__init__()\n+\n+        self.initial_learning_rate = initial_learning_rate\n+        self.first_decay_steps = first_decay_steps\n+        self._t_mul = t_mul\n+        self._m_mul = m_mul\n+        self.alpha = alpha\n+        self.name = name\n+\n+    def __call__(self, step):\n+        with ops.name_scope(self.name):\n+            initial_learning_rate = ops.convert_to_tensor(\n+                self.initial_learning_rate\n+            )\n+            dtype = initial_learning_rate.dtype\n+            first_decay_steps = ops.cast(self.first_decay_steps, dtype)\n+            alpha = ops.cast(self.alpha, dtype)\n+            t_mul = ops.cast(self._t_mul, dtype)\n+            m_mul = ops.cast(self._m_mul, dtype)\n+\n+            global_step_recomp = ops.cast(step, dtype)\n+            completed_fraction = global_step_recomp / first_decay_steps\n+\n+            def compute_step(completed_fraction, geometric=False):\n+                \"\"\"Helper for `cond` operation.\"\"\"\n+                if geometric:\n+                    i_restart = ops.floor(\n+                        ops.log(1.0 - completed_fraction * (1.0 - t_mul))\n+                        / ops.log(t_mul)\n+                    )\n+\n+                    sum_r = (1.0 - t_mul**i_restart) / (1.0 - t_mul)\n+                    completed_fraction = (\n+                        completed_fraction - sum_r\n+                    ) / t_mul**i_restart\n+\n+                else:\n+                    i_restart = ops.floor(completed_fraction)\n+                    completed_fraction -= i_restart\n+\n+                return i_restart, completed_fraction\n+\n+            i_restart, completed_fraction = ops.cond(\n+                ops.equal(t_mul, 1.0),\n+                lambda: compute_step(completed_fraction, geometric=False),\n+                lambda: compute_step(completed_fraction, geometric=True),\n+            )\n+\n+            m_fac = m_mul**i_restart\n+            cosine_decayed = (\n+                0.5\n+                * m_fac\n+                * (\n+                    1.0\n+                    + ops.cos(\n+                        ops.array(math.pi, dtype=dtype) * completed_fraction\n+                    )\n+                )\n+            )\n+            decayed = (1 - alpha) * cosine_decayed + alpha\n+\n+            return ops.multiply(initial_learning_rate, decayed)\n+\n+    def get_config(self):\n+        return {\n+            \"initial_learning_rate\": self.initial_learning_rate,\n+            \"first_decay_steps\": self.first_decay_steps,\n+            \"t_mul\": self._t_mul,\n+            \"m_mul\": self._m_mul,\n+            \"alpha\": self.alpha,\n+            \"name\": self.name,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.serialize\")\n+def serialize(learning_rate_schedule):\n+    \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible dict.\n+\n+    Args:\n+        learning_rate_schedule: The `LearningRateSchedule` object to serialize.\n+\n+    Returns:\n+        A JSON-serializable dict representing the object's config.\n+\n+    Example:\n+\n+    >>> lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+    ...     0.1, decay_steps=100000, decay_rate=0.96, staircase=True)\n+    >>> keras_core.optimizers.schedules.serialize(lr_schedule)\n+    {'module': 'keras_core.optimizers.schedules',\n+    'class_name': 'ExponentialDecay', 'config': {...},\n+    'registered_name': None}\n+    \"\"\"\n+    return serialization_lib.serialize_keras_object(learning_rate_schedule)\n+\n+\n+@keras_core_export(\"keras_core.optimizers.schedules.deserialize\")\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Instantiates a `LearningRateSchedule` object from a serialized form.\n+\n+    Args:\n+        config: The serialized form of the `LearningRateSchedule`. Dictionary of\n+            the form {'class_name': str, 'config': dict}.\n+        custom_objects: A dictionary mapping class names (or function names) of\n+            custom (non-Keras) objects to class/functions.\n+\n+    Returns:\n+        A `LearningRateSchedule` object.\n+\n+    Example:\n+\n+    ```python\n+    # Configuration for PolynomialDecay\n+    config = {\n+        'class_name': 'PolynomialDecay',\n+        'config': {'cycle': False,\n+            'decay_steps': 10000,\n+            'end_learning_rate': 0.01,\n+            'initial_learning_rate': 0.1,\n+            'name': None,\n+            'power': 0.5\n+        }\n+    }\n+    lr_schedule = keras_core.optimizers.schedules.deserialize(config)\n+    ```\n+    \"\"\"\n+    return serialization_lib.deserialize_keras_object(\n+        config,\n+        module_objects=globals(),\n+        custom_objects=custom_objects,\n+        printable_module_name=\"decay\",\n+    )\n\n@@ -0,0 +1,452 @@\n+\"\"\"Tests for learning rate schedule API.\"\"\"\n+\n+import math\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.schedules import learning_rate_schedule\n+\n+\n+class ExponentialDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.ExponentialDecay(\n+                initial_learning_rate=0.05,\n+                decay_steps=10,\n+                decay_rate=0.96,\n+                staircase=True,\n+                name=\"my_ed\",\n+            )\n+        )\n+\n+    def test_continuous(self):\n+        step = 5\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(0.05, 10, 0.96)\n+        expected = 0.05 * 0.96 ** (5.0 / 10.0)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_staircase(self):\n+        step = backend.Variable(1)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(\n+            0.1, 3, 0.96, staircase=True\n+        )\n+\n+        # No change to learning rate due to staircase\n+        expected = 0.1\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+        expected = 0.1\n+        step.assign(2)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+        # Decayed learning rate\n+        expected = 0.1 * 0.96 ** (100 // 3)\n+        step.assign(100)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_variables(self):\n+        step = backend.Variable(1)\n+        decayed_lr = learning_rate_schedule.ExponentialDecay(\n+            0.1, 3, 0.96, staircase=True\n+        )\n+\n+        # No change to learning rate\n+        step.assign(1)\n+        self.assertAllClose(decayed_lr(step), 0.1, 1e-6)\n+        step.assign(2)\n+        self.assertAllClose(decayed_lr(step), 0.1, 1e-6)\n+        # Decayed learning rate\n+        step.assign(100)\n+        expected = 0.1 * 0.96 ** (100 // 3)\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+\n+class PiecewiseConstantDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.PiecewiseConstantDecay(\n+                boundaries=[10, 20], values=[1, 2, 3], name=\"my_pcd\"\n+            )\n+        )\n+\n+    def test_piecewise_values(self):\n+        x = backend.Variable(-999)\n+        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+            [100, 110, 120], [1.0, 0.1, 0.01, 0.001]\n+        )\n+\n+        self.assertAllClose(decayed_lr(x), 1.0, 1e-6)\n+        x.assign(100)\n+        self.assertAllClose(decayed_lr(x), 1.0, 1e-6)\n+        x.assign(105)\n+        self.assertAllClose(decayed_lr(x), 0.1, 1e-6)\n+        x.assign(110)\n+        self.assertAllClose(decayed_lr(x), 0.1, 1e-6)\n+        x.assign(120)\n+        self.assertAllClose(decayed_lr(x), 0.01, 1e-6)\n+        x.assign(999)\n+        self.assertAllClose(decayed_lr(x), 0.001, 1e-6)\n+\n+    def test_boundary_values(self):\n+        # Test casting boundaries from int32 to int64.\n+        x_int64 = backend.Variable(0, dtype=\"int64\")\n+        boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]\n+        decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n+            boundaries, values\n+        )\n+\n+        self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n+        x_int64.assign(1)\n+        self.assertAllClose(decayed_lr(x_int64), 0.4, 1e-6)\n+        x_int64.assign(2)\n+        self.assertAllClose(decayed_lr(x_int64), 0.5, 1e-6)\n+        x_int64.assign(3)\n+        self.assertAllClose(decayed_lr(x_int64), 0.6, 1e-6)\n+        x_int64.assign(4)\n+        self.assertAllClose(decayed_lr(x_int64), 0.7, 1e-6)\n+\n+\n+class LinearDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.PolynomialDecay(\n+                initial_learning_rate=0.1,\n+                decay_steps=100,\n+                end_learning_rate=0.005,\n+                power=1.0,\n+                cycle=False,\n+                name=\"my_ld\",\n+            )\n+        )\n+\n+    def test_halfway(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.0\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = lr * 0.5\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_end(self):\n+        step = 10\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_halfway_with_end(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = (lr + end_lr) * 0.5\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(lr, 10, end_lr)\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end_with_cycle(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, cycle=True\n+        )\n+        expected = (lr - end_lr) * 0.25 + end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+\n+class SqrtDecayTest(testing.TestCase):\n+    def test_halfway(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.0\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = lr * 0.5**power\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_end(self):\n+        step = 10\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_halfway_with_end(self):\n+        step = 5\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = (lr - end_lr) * 0.5**power + end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power\n+        )\n+        expected = end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_beyond_end_with_cycle(self):\n+        step = 15\n+        lr = 0.05\n+        end_lr = 0.001\n+        power = 0.5\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, 10, end_lr, power=power, cycle=True\n+        )\n+        expected = (lr - end_lr) * 0.25**power + end_lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_begin_with_cycle(self):\n+        lr = 0.001\n+        decay_steps = 10\n+        step = 0\n+        decayed_lr = learning_rate_schedule.PolynomialDecay(\n+            lr, decay_steps, cycle=True\n+        )\n+        expected = lr\n+        self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+\n+class InverseTimeDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.InverseTimeDecay(\n+                initial_learning_rate=0.05,\n+                decay_steps=10,\n+                decay_rate=0.96,\n+                staircase=True,\n+                name=\"my_itd\",\n+            )\n+        )\n+\n+    def test_decay(self):\n+        initial_lr = 0.1\n+        k = 10\n+        decay_rate = 0.96\n+        step = backend.Variable(0)\n+        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+            initial_lr, k, decay_rate\n+        )\n+\n+        for i in range(k + 1):\n+            expected = initial_lr / (1 + i / k * decay_rate)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+            step.assign(step + 1)\n+\n+    def test_staircase(self):\n+        initial_lr = 0.1\n+        k = 10\n+        decay_rate = 0.96\n+        step = backend.Variable(0)\n+        decayed_lr = learning_rate_schedule.InverseTimeDecay(\n+            initial_lr, k, decay_rate, staircase=True\n+        )\n+\n+        for i in range(k + 1):\n+            expected = initial_lr / (1 + decay_rate * (i // k))\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+            step.assign(step + 1)\n+\n+\n+class CosineDecayTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.CosineDecay(\n+                initial_learning_rate=0.05,\n+                decay_steps=10,\n+                alpha=0.1,\n+                warmup_target=0.2,\n+                warmup_steps=2,\n+                name=\"my_cd\",\n+            )\n+        )\n+\n+    def np_cosine_decay(self, step, decay_steps, alpha=0.0):\n+        step = min(step, decay_steps)\n+        completed_fraction = step / decay_steps\n+        decay = 0.5 * (1.0 + math.cos(math.pi * completed_fraction))\n+        return (1.0 - alpha) * decay + alpha\n+\n+    def test_decay(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def linear_warmup(self, step, warmup_steps, initial_lr, target_lr):\n+        completed_fraction = step / warmup_steps\n+        total_delta = target_lr - initial_lr\n+        return completed_fraction * total_delta\n+\n+    def test_warmup(self):\n+        warmup_steps = 1500\n+        initial_lr = 0.0\n+        target_lr = 10.0\n+        for step in range(0, 1500, 250):\n+            lr = learning_rate_schedule.CosineDecay(\n+                initial_lr,\n+                0,\n+                warmup_target=target_lr,\n+                warmup_steps=warmup_steps,\n+            )\n+            expected = self.linear_warmup(\n+                step, warmup_steps, initial_lr, target_lr\n+            )\n+            self.assertAllClose(lr(step), expected)\n+\n+    def test_alpha(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        alpha = 0.1\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps, alpha\n+            )\n+            expected = self.np_cosine_decay(step, num_training_steps, alpha)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_float64(self):\n+        num_training_steps = 1000\n+        initial_lr = np.float64(1.0)\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecay(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_warmup_decay(self):\n+        warmup_steps = 2000\n+        decay_steps = 1000\n+        initial_lr = 0.0\n+        target_lr = 10.0\n+        for step in range(0, 3000, 250):\n+            lr = learning_rate_schedule.CosineDecay(\n+                initial_lr,\n+                decay_steps,\n+                warmup_target=target_lr,\n+                warmup_steps=warmup_steps,\n+            )\n+            if step < warmup_steps + 1:\n+                expected = self.linear_warmup(\n+                    step, warmup_steps, initial_lr, target_lr\n+                )\n+            else:\n+                expected = target_lr * self.np_cosine_decay(\n+                    step - warmup_steps, decay_steps\n+                )\n+            self.assertAllClose(lr(step), expected)\n+\n+\n+class CosineDecayRestartsTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            learning_rate_schedule.CosineDecayRestarts(\n+                initial_learning_rate=0.05,\n+                first_decay_steps=10,\n+                alpha=0.1,\n+                t_mul=3.0,\n+                m_mul=4.0,\n+                name=\"my_cdr\",\n+            )\n+        )\n+\n+    def np_cosine_decay_restarts(\n+        self, step, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0\n+    ):\n+        fac = 1.0\n+        while step >= decay_steps:\n+            step -= decay_steps\n+            decay_steps *= t_mul\n+            fac *= m_mul\n+\n+        completed_fraction = step / decay_steps\n+        decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))\n+        return (1.0 - alpha) * decay + alpha\n+\n+    def test_decay(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay_restarts(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_float64(self):\n+        num_training_steps = 1000\n+        initial_lr = np.float64(1.0)\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps\n+            )\n+            expected = self.np_cosine_decay_restarts(step, num_training_steps)\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_alpha(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        alpha = 0.1\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps, alpha=alpha\n+            )\n+            expected = self.np_cosine_decay_restarts(\n+                step, num_training_steps, alpha=alpha\n+            )\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_mmul(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        m_mul = 0.9\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps, m_mul=m_mul\n+            )\n+            expected = self.np_cosine_decay_restarts(\n+                step, num_training_steps, m_mul=m_mul\n+            )\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n+\n+    def test_tmul(self):\n+        num_training_steps = 1000\n+        initial_lr = 1.0\n+        t_mul = 1.0\n+        for step in range(0, 1500, 250):\n+            decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n+                initial_lr, num_training_steps, t_mul=t_mul\n+            )\n+            expected = self.np_cosine_decay_restarts(\n+                step, num_training_steps, t_mul=t_mul\n+            )\n+            self.assertAllClose(decayed_lr(step), expected, 1e-6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
