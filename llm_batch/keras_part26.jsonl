{"custom_id": "keras#3037183c1b0d850659c68571ea8256108b8fd2eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 78 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 78 | Churn Cumulative: 135 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,51 +0,0 @@\n-import pydot\n-from keras.layers.core import Merge\n-from keras.models import Model\n-from collections import Counter\n-\n-class Grapher(object):\n-\n-    def __init__(self):\n-        self.names = {}\n-        self.class_counts = Counter()\n-\n-    def get_name(self, model):\n-        \"\"\"\n-        returns the name of the model instance. If model does not have a `name` attribute, then it will be assigned\n-        a generic (and unique) identifier based on its class\n-        \"\"\"\n-        if hasattr(model, 'name'):\n-            return model.name\n-        clz = model.__class__.__name__\n-        if model not in self.names:\n-            self.class_counts[clz] += 1\n-            self.names[model] = clz + str(self.class_counts[clz])\n-        return self.names[model]\n-\n-    def add_edge(self, f, t, graph):\n-        if f: graph.add_edge(pydot.Edge(f, t))\n-        return t\n-\n-    def add_model(self, model, graph, parent=None):\n-        \"\"\"\n-        Recursively adds `model` and its components to the pydot graph\n-        \"\"\"\n-        this = self.get_name(model)\n-        if isinstance(model, Model):\n-            parent = self.add_edge(parent, this, graph)\n-            for child in reversed(model.layers):\n-                parent = self.add_model(child, graph, parent)\n-        elif isinstance(model, Merge):\n-            for child in model.models:\n-                self.add_model(child, graph, this)\n-            return self.add_edge(parent, this, graph)\n-        else:\n-            return self.add_edge(parent, this, graph)\n-\n-    def plot(self, model, to_file):\n-        \"\"\"\n-        creates a graph visualizing the structure of `model` and writes it to `to_file`\n-        \"\"\"\n-        graph = pydot.Dot(graph_type='graph')\n-        self.add_model(model, graph)\n-        graph.write_png(to_file)\n\n@@ -1,27 +0,0 @@\n-from keras.utils.dot_utils import Grapher\n-\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge, Flatten\n-from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import GRU\n-\n-ent_lookup = Sequential()\n-ent_lookup.add(Embedding(10, 2))\n-ent_lookup.add(Flatten())\n-\n-rel_lookup = Sequential()\n-rel_lookup.add(Embedding(20, 2))\n-rel_lookup.add(Flatten())\n-\n-word_sequence = Sequential()\n-word_sequence.add(Embedding(10, 5))\n-word_sequence.add(GRU(5, 2))\n-\n-model = Sequential()\n-model.add(Merge([word_sequence, ent_lookup, rel_lookup], mode='concat'))\n-model.add(Activation('relu'))\n-model.add(Dense(6, 2))\n-model.add(Activation('softmax'))\n-\n-g = Grapher()\n-g.plot(model, 'mymodel.png')\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#529306f9f9a353863e7983354b88a52328e9e39a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 23/23 | Churn Δ: 57 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,57 @@\n+import unittest\n+import numpy as np\n+import theano\n+\n+from keras.layers import recurrent\n+\n+def recursive_runner(layer):\n+    layer.input = theano.shared(value=np.ones((10,10,10)))\n+    for train in [True,False]:\n+        layer.get_output(train).eval()\n+\n+class TestRNNS(unittest.TestCase):\n+    def test_simple(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.SimpleRNN(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+    def test_simple_deep(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.SimpleDeepRNN(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+    def test_gru(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.GRU(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+    def test_lstm(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.LSTM(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+\n+    def test_jzs1(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.JZS1(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+    def test_jzs2(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.JZS2(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+    def test_jzs3(self):\n+        for weights in [None, [np.ones((10,5))]]:\n+            for rer_seq in [True, False]:\n+                layer = recurrent.JZS3(10, 5, return_sequences=rer_seq, weights=weights)\n+                recursive_runner(layer)\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#130e5cd8cdd6e0bd5512d3e514c6dde96dd1ac6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 91 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,51 +6,55 @@ from keras.layers import recurrent\n \n def recursive_runner(layer):\n     layer.input = theano.shared(value=np.ones((10,10,10)))\n+\n+    config = layer.get_config()\n+\n     for train in [True,False]:\n-        layer.get_output(train).eval()\n+        out = layer.get_output(train).eval()\n+        mask = layer.get_output_mask(train)\n \n class TestRNNS(unittest.TestCase):\n     def test_simple(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.SimpleRNN(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.SimpleRNN(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n     def test_simple_deep(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.SimpleDeepRNN(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.SimpleDeepRNN(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n     def test_gru(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.GRU(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.GRU(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n     def test_lstm(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.LSTM(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.LSTM(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n \n     def test_jzs1(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.JZS1(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.JZS1(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n     def test_jzs2(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.JZS2(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.JZS2(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n     def test_jzs3(self):\n         for weights in [None, [np.ones((10,5))]]:\n-            for rer_seq in [True, False]:\n-                layer = recurrent.JZS3(10, 5, return_sequences=rer_seq, weights=weights)\n+            for ret_seq in [True, False]:\n+                layer = recurrent.JZS3(10, 5, return_sequences=ret_seq, weights=weights)\n                 recursive_runner(layer)\n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5dac6bd6415838f95cf936e473a70fd00b0c8b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 47 | Churn Cumulative: 138 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,58 +4,43 @@ import theano\n \n from keras.layers import recurrent\n \n-def recursive_runner(layer):\n-    layer.input = theano.shared(value=np.ones((10,10,10)))\n+nb_samples, timesteps, input_dim, output_dim = 3, 3, 10, 5\n \n+\n+def _runner(layer_class):\n+    for weights in [None, [np.ones((input_dim, output_dim))]]:\n+        for ret_seq in [True, False]:\n+            layer = layer_class(input_dim, output_dim, return_sequences=ret_seq, weights=weights)\n+            layer.input = theano.shared(value=np.ones((nb_samples, timesteps, input_dim)))\n             config = layer.get_config()\n \n             for train in [True, False]:\n                 out = layer.get_output(train).eval()\n                 mask = layer.get_output_mask(train)\n \n+\n class TestRNNS(unittest.TestCase):\n     def test_simple(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.SimpleRNN(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n+        _runner(recurrent.SimpleRNN)\n \n     def test_simple_deep(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.SimpleDeepRNN(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n+        _runner(recurrent.SimpleDeepRNN)\n \n     def test_gru(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.GRU(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n+        _runner(recurrent.GRU)\n \n     def test_lstm(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.LSTM(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n-\n+        _runner(recurrent.LSTM)\n \n     def test_jzs1(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.JZS1(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n+        _runner(recurrent.JZS1)\n \n     def test_jzs2(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.JZS2(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n+        _runner(recurrent.JZS2)\n \n     def test_jzs3(self):\n-        for weights in [None, [np.ones((10,5))]]:\n-            for ret_seq in [True, False]:\n-                layer = recurrent.JZS3(10, 5, return_sequences=ret_seq, weights=weights)\n-                recursive_runner(layer)\n+        _runner(recurrent.JZS3)\n+\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f08f59075261bd6c9ba10a5be665e1d242ea6874", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 143 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,11 @@ def _runner(layer_class):\n \n             for train in [True, False]:\n                 out = layer.get_output(train).eval()\n+                if ret_seq:\n+                    assert(out.shape == (nb_samples, timesteps, output_dim))\n+                else:\n+                    assert(out.shape == (nb_samples, output_dim))\n+\n                 mask = layer.get_output_mask(train)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20921b7b131d640d5f70d97f61e95f63a191d706", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 151 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,10 @@ nb_samples, timesteps, input_dim, output_dim = 3, 3, 10, 5\n \n \n def _runner(layer_class):\n+    \"\"\"\n+    All the recurrent layers share the same interface, so we can run through them with a single\n+    function.\n+    \"\"\"\n     for weights in [None, [np.ones((input_dim, output_dim))]]:\n         for ret_seq in [True, False]:\n             layer = layer_class(input_dim, output_dim, return_sequences=ret_seq, weights=weights)\n@@ -16,6 +20,7 @@ def _runner(layer_class):\n \n             for train in [True, False]:\n                 out = layer.get_output(train).eval()\n+                # Make sure the output has the desired shape\n                 if ret_seq:\n                     assert(out.shape == (nb_samples, timesteps, output_dim))\n                 else:\n@@ -25,6 +30,9 @@ def _runner(layer_class):\n \n \n class TestRNNS(unittest.TestCase):\n+    \"\"\"\n+    Test all the RNNs using a generic test runner function defined above.\n+    \"\"\"\n     def test_simple(self):\n         _runner(recurrent.SimpleRNN)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#319412a62dd7237317dd4b54128144fb2d5a9125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 30 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,30 @@\n+import unittest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+import theano\n+\n+from keras.layers import core\n+\n+\n+class TestLayerBase(unittest.TestCase):\n+    def test_input_output(self):\n+        nb_samples = 10\n+        input_dim = 5\n+        layer = core.Layer()\n+\n+        # As long as there is no input, an error should be raised.\n+        for train in [True, False]:\n+            self.assertRaises(AttributeError, layer.get_input, train)\n+            self.assertRaises(AttributeError, layer.get_output, train)\n+\n+        # Once an input is provided, it should be reachable through the\n+        # appropriate getters\n+        input = np.ones((nb_samples, input_dim))\n+        layer.input = theano.shared(value=input)\n+        for train in [True, False]:\n+            assert_allclose(layer.get_input(train).eval(), input)\n+            assert_allclose(layer.get_output(train).eval(), input)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#571448f5cd4ab2c30e2e18f75136f4b3f47cd80b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 21 | Churn Cumulative: 51 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,27 @@ class TestLayerBase(unittest.TestCase):\n             assert_allclose(layer.get_input(train).eval(), input)\n             assert_allclose(layer.get_output(train).eval(), input)\n \n+    def test_connections(self):\n+        nb_samples = 10\n+        input_dim = 5\n+        layer1 = core.Layer()\n+        layer2 = core.Layer()\n+\n+        input = np.ones((nb_samples, input_dim))\n+        layer1.input = theano.shared(value=input)\n+\n+        # As long as there is no previous layer, an error should be raised.\n+        for train in [True, False]:\n+            self.assertRaises(AttributeError, layer2.get_input, train)\n+\n+        # After connecting, input of layer1 should be passed through\n+        layer2.set_previous(layer1)\n+        for train in [True, False]:\n+            assert_allclose(layer2.get_input(train).eval(), input)\n+            assert_allclose(layer2.get_output(train).eval(), input)\n+\n+\n+\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67426a6fa3b7a078a8560d519dea777ba94e59be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 14 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 69 | Churn Cumulative: 120 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -45,6 +45,75 @@ class TestLayerBase(unittest.TestCase):\n             assert_allclose(layer2.get_output(train).eval(), input)\n \n \n+class TestConfigParams(unittest.TestCase):\n+    \"\"\"\n+    Test the constructor, config and params functions of all layers in core.\n+    \"\"\"\n+\n+    def _runner(self, layer):\n+        conf = layer.get_config()\n+        assert (type(conf) == dict)\n+\n+        param = layer.get_params()\n+        # Typically a list or a tuple, but may be any iterable\n+        assert hasattr(param, '__iter__')\n+\n+    def test_base(self):\n+        layer = core.Layer()\n+        self._runner(layer)\n+\n+    def test_masked(self):\n+        layer = core.MaskedLayer()\n+        self._runner(layer)\n+\n+    def test_merge(self):\n+        layer_1 = core.Layer()\n+        layer_2 = core.Layer()\n+        layer = core.Merge([layer_1, layer_2])\n+        self._runner(layer)\n+\n+    def test_dropout(self):\n+        layer = core.Dropout(0.5)\n+        self._runner(layer)\n+\n+    def test_activation(self):\n+        layer = core.Activation('linear')\n+        self._runner(layer)\n+\n+    def test_reshape(self):\n+        layer = core.Reshape(10, 10)\n+        self._runner(layer)\n+\n+    def test_flatten(self):\n+        layer = core.Flatten()\n+        self._runner(layer)\n+\n+    def test_repeat_vector(self):\n+        layer = core.RepeatVector(10)\n+        self._runner(layer)\n+\n+    def test_dense(self):\n+        layer = core.Dense(10, 10)\n+        self._runner(layer)\n+\n+    def test_act_reg(self):\n+        layer = core.ActivityRegularization(0.5, 0.5)\n+        self._runner(layer)\n+\n+    def test_time_dist_dense(self):\n+        layer = core.TimeDistributedDense(10, 10)\n+        self._runner(layer)\n+\n+    def test_autoencoder(self):\n+        layer_1 = core.Layer()\n+        layer_2 = core.Layer()\n+\n+        layer = core.AutoEncoder(layer_1, layer_2)\n+        self._runner(layer)\n+\n+    def test_maxout_dense(self):\n+        layer = core.MaxoutDense(10, 10)\n+        self._runner(layer)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66247a8261f7969effe2f2e61bd4157a0ae890e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 84 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,6 @@ setup(name = 'Keras',\n       url = 'https://github.com/fchollet/keras',\n       download_url = 'https://github.com/fchollet/keras/tarball/0.1.1',\n       license = 'MIT',\n-      install_requires = ['theano', 'pyyaml'],\n+      install_requires = ['theano', 'pyyaml', 'h5py'],\n       packages = find_packages(),\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0174f21ea56d61fe7f125ddacc4efe2d9416fd16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 80 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n-from ..layers.core import Layer\n+from ..layers.core import Layer, MaskedLayer\n from ..utils.theano_utils import shared_zeros, shared_ones, shared_scalars\n import theano.tensor as T\n \n-class LeakyReLU(Layer):\n+class LeakyReLU(MaskedLayer):\n     def __init__(self, alpha=0.3):\n         super(LeakyReLU,self).__init__()\n         self.alpha = alpha\n@@ -16,7 +16,7 @@ class LeakyReLU(Layer):\n             \"alpha\":self.alpha}\n \n \n-class PReLU(Layer):\n+class PReLU(MaskedLayer):\n     '''\n         Reference: \n             Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n@@ -39,16 +39,16 @@ class PReLU(Layer):\n         \"input_shape\":self.input_shape}\n \n \n-class Psoftplus(Layer):\n+class ParametricSoftplus(MaskedLayer):\n     '''\n-        Parametric softplus of the form: alpha * (1 + exp(beta * X))\n+        Parametric Softplus of the form: alpha * (1 + exp(beta * X))\n \n         Reference:\n             Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n             http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n     '''\n     def __init__(self, input_shape):\n-        super(Psoftplus,self).__init__()\n+        super(ParametricSoftplus,self).__init__()\n         self.alphas = shared_scalars(input_shape, 0.2)\n         self.betas = shared_scalars(input_shape, 5.0)\n         self.params = [self.alphas, self.betas]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#84a3b5aecbe30fde7b2ec1981527c336ea749755", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 92 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,10 +47,12 @@ class ParametricSoftplus(MaskedLayer):\n             Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n             http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n     '''\n-    def __init__(self, input_shape):\n+    def __init__(self, input_shape, alpha_init=0.2, beta_init=5.0):\n         super(ParametricSoftplus,self).__init__()\n-        self.alphas = shared_scalars(input_shape, 0.2)\n-        self.betas = shared_scalars(input_shape, 5.0)\n+        self.alpha_init = alpha_init\n+        self.beta_init = beta_init\n+        self.alphas = shared_scalars(input_shape, alpha_init)\n+        self.betas = shared_scalars(input_shape, beta_init)\n         self.params = [self.alphas, self.betas]\n         self.input_shape = input_shape\n \n@@ -60,4 +62,6 @@ class ParametricSoftplus(MaskedLayer):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"input_shape\":self.input_shape}\n+            \"input_shape\":self.input_shape,\n+            \"alpha_init\":self.alpha_init,\n+            \"beta_init\":self.beta_init}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cf5f6f9829681cf3bd1c345d352931452ec54b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 65 | Churn Cumulative: 2639 | Contributors (this commit): 20 | Commits (past 90d): 92 | Contributors (cumulative): 22 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -531,6 +531,7 @@ class Graph(Model, containers.Graph):\n         ys = []\n         ys_train = []\n         ys_test = []\n+        weights = []\n         train_loss = 0.\n         test_loss = 0.\n         for output_name in self.output_order:\n@@ -543,15 +544,18 @@ class Graph(Model, containers.Graph):\n             ys_train.append(y_train)\n             ys_test.append(y_test)\n \n-            train_loss += objectives.get(loss_fn)(y, y_train).mean()\n-            test_loss += objectives.get(loss_fn)(y, y_test).mean()\n+            weight = T.ones_like(y_test)\n+            weights.append(weight)\n+            weighted_loss = weighted_objective(objectives.get(loss_fn))\n+            train_loss += weighted_loss(y, y_train, weight)\n+            test_loss += weighted_loss(y, y_test, weight)\n \n         train_loss.name = 'train_loss'\n         test_loss.name = 'test_loss'\n \n         ins = [self.inputs[name].input for name in self.input_order]\n-        train_ins = ins + ys\n-        test_ins = ins + ys\n+        train_ins = ins + ys + weights\n+        test_ins = ins + ys + weights\n \n         for r in self.regularizers:\n             train_loss = r(train_loss)\n@@ -567,14 +571,18 @@ class Graph(Model, containers.Graph):\n         self._predict = theano.function(inputs=ins, outputs=ys_test,\n             allow_input_downcast=True, mode=theano_mode)\n \n-    def train_on_batch(self, data):\n+    def train_on_batch(self, data, sample_weight={}):\n         # data is a dictionary mapping output and input names to arrays\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n+        sample_weight = [standardize_weights(data[name], sample_weight.get(name)) for name in self.output_order]\n+\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         return self._train(*ins)\n \n     def test_on_batch(self, data):\n         # data is a dictionary mapping input names to arrays\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n+        sample_weight = [standardize_weights(data[name]) for name in self.output_order]\n+\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         return self._test(*ins)\n \n     def predict_on_batch(self, data):\n@@ -583,15 +591,17 @@ class Graph(Model, containers.Graph):\n         return self._predict(*ins)\n \n     def fit(self, data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True):\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n+            validation_split=0., validation_data=None, shuffle=True, sample_weight={}):\n+        sample_weight = [standardize_weights(data[name], sample_weight.get(name)) for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n \n         val_f = None\n         val_ins = None\n         if validation_data or validation_split:\n             val_f = self._test\n         if validation_data:\n-            val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order]\n+            sample_weight = [standardize_weights(validation_data[name]) for name in self.output_order]\n+            val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order] + sample_weight\n \n         f = self._train\n         out_labels = self.output_order\n@@ -602,8 +612,10 @@ class Graph(Model, containers.Graph):\n                             shuffle=shuffle, metrics=metrics)\n         return history\n \n-    def evaluate(self, data, batch_size=128, verbose=0):\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n+    def evaluate(self, data, batch_size=128, verbose=0, sample_weight={}):\n+        sample_weight = [standardize_weights(data[name], sample_weight.get(name)) for name in self.output_order]\n+\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         outs = self._test_loop(self._test, ins, batch_size, verbose)\n         return outs[0]\n \n\n@@ -130,6 +130,35 @@ class TestGraph(unittest.TestCase):\n         print(nloss)\n         assert(loss == nloss)\n \n+    def test_2o_1i_sample_weights(self):\n+        print('test a non-sequential graph with 1 input and 2 outputs with sample weights')\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(16, 1), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', input='dense2')\n+        graph.add_output(name='output2', input='dense3')\n+\n+        weights1 = np.random.uniform(size=y_train.shape[0])\n+        weights2 = np.random.uniform(size=y2_train.shape[0])\n+\n+        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train}, nb_epoch=10,\n+                            sample_weight={'output1': weights1, 'output2': weights2})\n+        out = graph.predict({'input1': X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 2)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n+        loss = graph.train_on_batch({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                                    sample_weight={'output1': weights1, 'output2': weights2})\n+        loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n+                              sample_weight={'output1': weights1, 'output2': weights2})\n+        print(loss)\n+\n     def test_recursive(self):\n         print('test layer-like API')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6289de3717367a6fd041c543f18dbfff826f800b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 37 | Churn Cumulative: 2769 | Contributors (this commit): 20 | Commits (past 90d): 98 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -359,22 +359,18 @@ class Sequential(Model, containers.Sequential):\n         self._test_with_acc = theano.function(test_ins, [test_loss, test_accuracy],\n             allow_input_downcast=True, mode=theano_mode)\n \n-    def train(self, X, y, accuracy=False, sample_weight=None):\n+    def train(self, X, y, accuracy=False, class_weight=None, sample_weight=None):\n         warnings.warn('The \"train\" method is deprecated, use \"train_on_batch\" instead.')\n-        return self.train_on_batch(X, y, accuracy, sample_weight)\n+        return self.train_on_batch(X, y, accuracy, class_weight, sample_weight)\n \n     def test(self, X, y, accuracy=False):\n         warnings.warn('The \"test\" method is deprecated, use \"test_on_batch\" instead.')\n         return self.test_on_batch(X, y, accuracy)\n \n-    def train_on_batch(self, X, y, accuracy=False, sample_weight=None):\n+    def train_on_batch(self, X, y, accuracy=False, class_weight=None, sample_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n-\n-        if sample_weight is None:\n-            sample_weight = np.ones(list(y.shape[0:-1]) + [1])\n-        else:\n-            sample_weight = standardize_y(sample_weight)\n+        sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n \n         ins = X + [y, sample_weight]\n         if accuracy:\n@@ -571,14 +567,15 @@ class Graph(Model, containers.Graph):\n         self._predict = theano.function(inputs=ins, outputs=ys_test,\n             allow_input_downcast=True, mode=theano_mode)\n \n-    def train_on_batch(self, data, sample_weight={}):\n+    def train_on_batch(self, data, class_weight={}, sample_weight={}):\n         # data is a dictionary mapping output and input names to arrays\n-        sample_weight = [standardize_weights(data[name], sample_weight.get(name)) for name in self.output_order]\n-\n+        sample_weight = [standardize_weights(data[name],\n+                                             sample_weight=sample_weight.get(name),\n+                                             class_weight=class_weight.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         return self._train(*ins)\n \n-    def test_on_batch(self, data):\n+    def test_on_batch(self, data, sample_weight={}):\n         # data is a dictionary mapping input names to arrays\n         sample_weight = [standardize_weights(data[name]) for name in self.output_order]\n \n@@ -591,8 +588,10 @@ class Graph(Model, containers.Graph):\n         return self._predict(*ins)\n \n     def fit(self, data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, sample_weight={}):\n-        sample_weight = [standardize_weights(data[name], sample_weight.get(name)) for name in self.output_order]\n+            validation_split=0., validation_data=None, shuffle=True, class_weight={}, sample_weight={}):\n+        sample_weight = [standardize_weights(data[name],\n+                                             sample_weight=sample_weight.get(name),\n+                                             class_weight=class_weight.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n \n         val_f = None\n\n@@ -152,7 +152,8 @@ class TestGraph(unittest.TestCase):\n         out = graph.predict({'input1': X_test})\n         assert(type(out == dict))\n         assert(len(out) == 2)\n-        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test},\n+                                   sample_weight={'output1': weights1, 'output2': weights2})\n         loss = graph.train_on_batch({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n                                     sample_weight={'output1': weights1, 'output2': weights2})\n         loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n\n@@ -1,10 +1,13 @@\n from __future__ import absolute_import\n from __future__ import print_function\n+\n+import numpy as np\n+np.random.seed(1336) # for reproducibility\n+\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.utils import np_utils\n-import numpy as np\n import unittest\n \n nb_classes = 10\n@@ -16,8 +19,6 @@ high_weight = 5\n max_train_samples = 5000\n max_test_samples = 1000\n \n-np.random.seed(1337) # for reproducibility\n-\n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n X_train = X_train.reshape(60000, 784)[:max_train_samples]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a08bf38ec6613930cf843e339b89845888d1af5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 18 | Methods Changed: 11 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 82 | Churn Cumulative: 2631 | Contributors (this commit): 20 | Commits (past 90d): 91 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -378,10 +378,11 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return self._train(*ins)\n \n-    def test_on_batch(self, X, y, accuracy=False):\n+    def test_on_batch(self, X, y, accuracy=False, sample_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = np.ones(y.shape[:-1] + (1,))\n+        sample_weight = standardize_weights(y, sample_weight=sample_weight)\n+\n         ins = X + [y, sample_weight]\n         if accuracy:\n             return self._test_with_acc(*ins)\n\n@@ -5,7 +5,7 @@ import numpy as np\n np.random.seed(1336) # for reproducibility\n \n from keras.datasets import mnist\n-from keras.models import Sequential\n+from keras.models import Sequential, Graph\n from keras.layers.core import Dense, Activation\n from keras.utils import np_utils\n import unittest\n@@ -33,8 +33,14 @@ Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n+class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n+class_weight[weighted_class] = high_weight\n \n-def create_model():\n+sample_weight = np.ones((y_train.shape[0])) * standard_weight\n+sample_weight[y_train == weighted_class] = high_weight\n+\n+\n+def create_sequential_model():\n     model = Sequential()\n     model.add(Dense(784, 50))\n     model.add(Activation('relu'))\n@@ -43,40 +49,81 @@ def create_model():\n     return model\n \n \n-def _test_weights(model, class_weight=None, sample_weight=None):\n+def create_graph_model():\n+    model = Graph()\n+    model.add_input(name='input')\n+    model.add_node(Dense(784, 50, activation='relu'), name='d1', input='input')\n+    model.add_node(Dense(50, 10, activation='softmax'), name='d2', input='d1')\n+    model.add_output(name='output', input='d2')\n+    return model\n+\n+\n+def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0,\n               class_weight=class_weight, sample_weight=sample_weight)\n+    model.train_on_batch(X_train[:32], Y_train[:32],\n+                         class_weight=class_weight, sample_weight=sample_weight[:32] if sample_weight is not None else None)\n+    model.test_on_batch(X_train[:32], Y_train[:32],\n+                        sample_weight=sample_weight[:32] if sample_weight is not None else None)\n     score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n     return score\n \n \n-class TestConcatenation(unittest.TestCase):\n-    def test_loss_weighting(self):\n-        class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n-        class_weight[weighted_class] = high_weight\n+def _test_weights_graph(model, class_weight=None, sample_weight=None):\n+    model.fit({'input': X_train, 'output': Y_train}, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0,\n+              class_weight={'output': class_weight}, sample_weight={'output': sample_weight})\n+    model.train_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n+                         class_weight={'output': class_weight}, sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n+    model.test_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n+                        sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n+    score = model.evaluate({'input': X_test[test_ids, :], 'output': Y_test[test_ids, :]}, verbose=0)\n+    return score\n \n-        sample_weight = np.ones((y_train.shape[0])) * standard_weight\n-        sample_weight[y_train == weighted_class] = high_weight\n \n+class TestLossWeighting(unittest.TestCase):\n+    def test_sequential(self):\n         for loss in ['mae', 'mse', 'categorical_crossentropy']:\n             print('loss:', loss)\n+            print('sequential')\n             # no weights: reference point\n-            model = create_model()\n+            model = create_sequential_model()\n             model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            standard_score = _test_weights(model)\n+            standard_score = _test_weights_sequential(model)\n             # test class_weight\n-            model = create_model()\n+            model = create_sequential_model()\n             model.compile(loss=loss, optimizer='rmsprop')\n-            score = _test_weights(model, class_weight=class_weight)\n+            score = _test_weights_sequential(model, class_weight=class_weight)\n             print('score:', score, ' vs.', standard_score)\n             self.assertTrue(score < standard_score)\n             # test sample_weight\n-            model = create_model()\n+            model = create_sequential_model()\n             model.compile(loss=loss, optimizer='rmsprop')\n-            score = _test_weights(model, sample_weight=sample_weight)\n+            score = _test_weights_sequential(model, sample_weight=sample_weight)\n             print('score:', score, ' vs.', standard_score)\n             self.assertTrue(score < standard_score)\n \n+    def test_graph(self):\n+        for loss in ['mae', 'mse', 'categorical_crossentropy']:\n+            print('loss:', loss)\n+            print('graph')\n+            # no weights: reference point\n+            model = create_graph_model()\n+            model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+            standard_score = _test_weights_graph(model)\n+            # test class_weight\n+            model = create_graph_model()\n+            model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+            score = _test_weights_graph(model, class_weight=class_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+            # test sample_weight\n+            model = create_graph_model()\n+            model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+            score = _test_weights_graph(model, sample_weight=sample_weight)\n+            print('score:', score, ' vs.', standard_score)\n+            self.assertTrue(score < standard_score)\n+\n+\n if __name__ == '__main__':\n     print('Test class_weight and sample_weight')\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d635a60140c11c64db4ac887bc79396484bb55e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 45 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.15\n\nDIFF:\n@@ -2,14 +2,37 @@ from __future__ import print_function\n import numpy as np\n import theano\n \n+def print_graph_layer_shapes(graph, input_shapes):\n+    \"\"\"\n+    Utility function to print the shape of the output at each layer of a Graph\n \n-def print_layer_shapes(model, input_shape):\n+    Arguments:\n+        graph: An instance of models.Graph\n+        input_shapes: A dict that gives a shape for each input to the Graph\n+    \"\"\"\n+    input_vars = [graph.inputs[name].input\n+                  for name in graph.input_order]\n+    output_vars = [graph.outputs[name].get_output()\n+                   for name in graph.output_order]\n+    input_dummy = [np.zeros(input_shapes[name], dtype=np.float32)\n+                   for name in graph.input_order]\n+\n+    print(\"input shapes : \", input_shapes)\n+    for name, l in graph.nodes.items():\n+        shape_f = theano.function(input_vars,\n+                                  l.get_output(train=False).shape,\n+                                  on_unused_input='ignore')\n+        out_shape = shape_f(*input_dummy)\n+        print('shape after', l.get_config()['name'], \"(\", name, \") :\", out_shape)\n+\n+def print_model_layer_shapes(model, input_shapes):\n     \"\"\"\n     Utility function that prints the shape of the output at each layer.\n \n     Arguments:\n         model: An instance of models.Model\n         input_shape: The shape of the input you will provide to the model.\n+                     Either a tuple (for a single input) or a list of tuple\n     \"\"\"\n     # This is to handle the case where a model has been connected to a previous\n     # layer (and therefore get_input would recurse into previous layer's\n@@ -21,10 +44,20 @@ def print_layer_shapes(model, input_shape):\n         raise Exception(\"This function doesn't work on model used as subparts \"\n                         \" for other models\")\n \n-    input_var = model.get_input(train=False)\n-    input_tmp = np.zeros(input_shape, dtype=np.float32)\n-    print(\"input shape : \", input_shape)\n+    # We allow the shortcut input_shapes=(1, 1, 28) instead of\n+    # input_shapes=[(1, 1, 28)].\n+    if not isinstance(input_shapes[0], tuple):\n+        input_shapes = [input_shapes]\n+    input_vars = model.get_input(train=False)\n+    # theano.function excepts a list of variables\n+    if not isinstance(input_vars, list):\n+        input_vars = [input_vars]\n+    input_dummy = [np.zeros(shape, dtype=np.float32)\n+                   for shape in input_shapes]\n+\n+    print(\"input shapes : \", input_shapes)\n     for l in model.layers:\n-        shape_f = theano.function([input_var], l.get_output(train=False).shape)\n-        out_shape = shape_f(input_tmp)\n+        shape_f = theano.function(input_vars,\n+                                  l.get_output(train=False).shape)\n+        out_shape = shape_f(*input_dummy)\n         print('shape after', l.get_config()['name'], \":\", out_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
