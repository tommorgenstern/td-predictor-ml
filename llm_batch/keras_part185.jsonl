{"custom_id": "keras#7481b5d060d77f73dc78e2fde37362d1b4656f2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 25 | Churn Cumulative: 791 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,9 +36,10 @@ result_prefix = args.result_prefix\n # You can tweak these setting to obtain new visual effects.\n settings = {\n     'features': {\n-        'mixed2': 0.5,\n-        'mixed3': 1.,\n-        'mixed4': 1.,\n+        'mixed2': 0.2,\n+        'mixed3': 0.5,\n+        'mixed4': 2.,\n+        'mixed5': 1.5,\n     },\n }\n \n@@ -124,9 +125,11 @@ def resize_img(img, size):\n     return scipy.ndimage.zoom(img, factors, order=1)\n \n \n-def gradient_ascent(x, iterations, step):\n+def gradient_ascent(x, iterations, step, max_loss=None):\n     for i in range(iterations):\n         loss_value, grad_values = eval_loss_and_grads(x)\n+        if max_loss is not None and loss_value > max_loss:\n+            break\n         print('..Loss value at', i, ':', loss_value)\n         x += step * grad_values\n     return x\n@@ -155,10 +158,12 @@ and compare the result to the (resized) original image.\n \"\"\"\n \n \n-step = 0.003  # Gradient ascent step size\n-num_octave = 4  # Number of scales at which to run gradient ascent\n-octave_scale = 1.3  # Size ratio between scales\n+# Playing with these hyperparameters will also allow you to achieve new effects\n+step = 0.01  # Gradient ascent step size\n+num_octave = 3  # Number of scales at which to run gradient ascent\n+octave_scale = 1.4  # Size ratio between scales\n iterations = 20  # Number of ascent steps per scale\n+max_loss = 10.\n \n img = preprocess_image(base_image_path)\n if K.image_data_format() == 'channels_first':\n@@ -176,8 +181,10 @@ shrunk_original_img = resize_img(img, successive_shapes[0])\n for shape in successive_shapes:\n     print('Processing image shape', shape)\n     img = resize_img(img, shape)\n-    img = gradient_ascent(img, iterations=iterations, step=step)\n-\n+    img = gradient_ascent(img,\n+                          iterations=iterations,\n+                          step=step,\n+                          max_loss=max_loss)\n     upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n     same_size_original = resize_img(original_img, shape)\n     lost_detail = same_size_original - upscaled_shrunk_original_img\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#365f621b24631a03f995e3b30e1800d327e42fc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 64 | Files Changed: 3 | Hunks: 38 | Methods Changed: 20 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 177 | Churn Cumulative: 8016 | Contributors (this commit): 42 | Commits (past 90d): 53 | Contributors (cumulative): 48 | DMM Complexity: 0.4\n\nDIFF:\n@@ -396,7 +396,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n             self.bias_o = None\n         self.built = True\n \n-    def get_initial_states(self, inputs):\n+    def get_initial_state(self, inputs):\n         # (samples, timesteps, rows, cols, filters)\n         initial_state = K.zeros_like(inputs)\n         # (samples, rows, cols, filters)\n\n@@ -170,11 +170,16 @@ class Recurrent(Layer):\n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n \n-    # Note on specifying initial states in RNNs\n-        You can specify the initial state of RNN layers by calling them with\n-        the keyword argument `initial_state`. The value of `initial_state`\n-        should be a tensor or list of tensors representing the initial state\n-        of the RNN layer.\n+    # Note on specifying the initial state of RNNs\n+        You can specify the initial state of RNN layers symbolically by\n+        calling them with the keyword argument `initial_state`. The value of\n+        `initial_state` should be a tensor or list of tensors representing\n+        the initial state of the RNN layer.\n+\n+        You can specify the initial state of RNN layers numerically by\n+        calling `reset_states` with the keyword argument `states`. The value of\n+        `states` should be a numpy array or list of numpy arrays representing\n+        the initial state of the RNN layer.\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n@@ -190,7 +195,7 @@ class Recurrent(Layer):\n         self.unroll = unroll\n         self.implementation = implementation\n         self.supports_masking = True\n-        self.input_spec = InputSpec(ndim=3)\n+        self.input_spec = [InputSpec(ndim=3)]\n         self.state_spec = None\n         self.dropout = 0\n         self.recurrent_dropout = 0\n@@ -205,6 +210,8 @@ class Recurrent(Layer):\n \n     def compute_mask(self, inputs, mask):\n         if self.return_sequences:\n+            if isinstance(mask, list):\n+                return mask[0]\n             return mask\n         else:\n             return None\n@@ -215,14 +222,14 @@ class Recurrent(Layer):\n     def get_constants(self, inputs, training=None):\n         return []\n \n-    def get_initial_states(self, inputs):\n+    def get_initial_state(self, inputs):\n         # build an all-zero tensor of shape (samples, output_dim)\n         initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n         initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n         initial_state = K.expand_dims(initial_state)  # (samples, 1)\n         initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n-        initial_states = [initial_state for _ in range(len(self.states))]\n-        return initial_states\n+        initial_state = [initial_state for _ in range(len(self.states))]\n+        return initial_state\n \n     def preprocess_input(self, inputs, training=None):\n         return inputs\n@@ -232,18 +239,28 @@ class Recurrent(Layer):\n         # and if it a Keras tensor,\n         # then add it to the inputs and temporarily\n         # modify the input spec to include the state.\n-        if initial_state is not None:\n-            if hasattr(initial_state, '_keras_history'):\n+        if initial_state is None:\n+            return super(Recurrent, self).__call__(inputs, **kwargs)\n+\n+        if not isinstance(initial_state, (list, tuple)):\n+            initial_state = [initial_state]\n+\n+        is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n+        for tensor in initial_state:\n+            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n+                raise ValueError('The initial state of an RNN layer cannot be'\n+                                 ' specified with a mix of Keras tensors and'\n+                                 ' non-Keras tensors')\n+\n+        if is_keras_tensor:\n             # Compute the full input spec, including state\n             input_spec = self.input_spec\n             state_spec = self.state_spec\n             if not isinstance(state_spec, list):\n                 state_spec = [state_spec]\n-                self.input_spec = [input_spec] + state_spec\n+            self.input_spec = input_spec + state_spec\n \n             # Compute the full inputs, including state\n-                if not isinstance(initial_state, (list, tuple)):\n-                    initial_state = [initial_state]\n             inputs = [inputs] + list(initial_state)\n \n             # Perform the call\n@@ -256,27 +273,27 @@ class Recurrent(Layer):\n             kwargs['initial_state'] = initial_state\n             return super(Recurrent, self).__call__(inputs, **kwargs)\n \n-    def call(self, inputs, mask=None, initial_state=None, training=None):\n+    def call(self, inputs, mask=None, training=None, initial_state=None):\n         # input shape: `(samples, time (padded with zeros), input_dim)`\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec and self.state_spec with complete input shapes.\n-        if initial_state is not None:\n-            if not isinstance(initial_state, (list, tuple)):\n-                initial_states = [initial_state]\n-            else:\n-                initial_states = list(initial_state)\n         if isinstance(inputs, list):\n-            initial_states = inputs[1:]\n+            initial_state = inputs[1:]\n             inputs = inputs[0]\n+        elif initial_state is not None:\n+            pass\n         elif self.stateful:\n-            initial_states = self.states\n+            initial_state = self.states\n         else:\n-            initial_states = self.get_initial_states(inputs)\n+            initial_state = self.get_initial_state(inputs)\n \n-        if len(initial_states) != len(self.states):\n+        if isinstance(mask, list):\n+            mask = mask[0]\n+\n+        if len(initial_state) != len(self.states):\n             raise ValueError('Layer has ' + str(len(self.states)) +\n                              ' states but was passed ' +\n-                             str(len(initial_states)) +\n+                             str(len(initial_state)) +\n                              ' initial states.')\n         input_shape = K.int_shape(inputs)\n         if self.unroll and input_shape[1] is None:\n@@ -295,7 +312,7 @@ class Recurrent(Layer):\n         preprocessed_input = self.preprocess_input(inputs, training=None)\n         last_output, outputs, states = K.rnn(self.step,\n                                              preprocessed_input,\n-                                             initial_states,\n+                                             initial_state,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n                                              constants=constants,\n@@ -317,13 +334,10 @@ class Recurrent(Layer):\n         else:\n             return last_output\n \n-    def reset_states(self, states_value=None):\n+    def reset_states(self, states=None):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n-        if not self.input_spec:\n-            raise RuntimeError('Layer has never been called '\n-                               'and thus has no states.')\n-        batch_size = self.input_spec.shape[0]\n+        batch_size = self.input_spec[0].shape[0]\n         if not batch_size:\n             raise ValueError('If a RNN is stateful, it needs to know '\n                              'its batch size. Specify the batch size '\n@@ -335,30 +349,29 @@ class Recurrent(Layer):\n                              '- If using the functional API, specify '\n                              'the time dimension by passing a '\n                              '`batch_shape` argument to your Input layer.')\n-        if states_value is not None:\n-            if not isinstance(states_value, (list, tuple)):\n-                states_value = [states_value]\n-            if len(states_value) != len(self.states):\n-                raise ValueError('The layer has ' + str(len(self.states)) +\n-                                 ' states, but the `states_value` '\n-                                 'argument passed '\n-                                 'only has ' + str(len(states_value)) +\n-                                 ' entries')\n+        # initialize state if None\n         if self.states[0] is None:\n             self.states = [K.zeros((batch_size, self.units))\n                            for _ in self.states]\n-            if not states_value:\n-                return\n-        for i, state in enumerate(self.states):\n-            if states_value:\n-                value = states_value[i]\n-                if value.shape != (batch_size, self.units):\n-                    raise ValueError(\n-                        'Expected state #' + str(i) +\n-                        ' to have shape ' + str((batch_size, self.units)) +\n-                        ' but got array with shape ' + str(value.shape))\n+        elif states is None:\n+            for state in self.states:\n+                K.set_value(state, np.zeros((batch_size, self.units)))\n         else:\n-                value = np.zeros((batch_size, self.units))\n+            if not isinstance(states, (list, tuple)):\n+                states = [states]\n+            if len(states) != len(self.states):\n+                raise ValueError('Layer ' + self.name + ' expects ' +\n+                                 str(len(self.states)) + ' states, '\n+                                 'but it received ' + str(len(values)) +\n+                                 ' state values. Input received: ' +\n+                                 str(values))\n+            for index, (value, state) in enumerate(zip(states, self.states)):\n+                if value.shape != (batch_size, self.units):\n+                    raise ValueError('State ' + str(index) +\n+                                     ' is incompatible with layer ' +\n+                                     self.name + ': expected shape=' +\n+                                     str((batch_size, self.units)) +\n+                                     ', found shape=' + str(value.shape))\n                 K.set_value(state, value)\n \n     def get_config(self):\n@@ -457,6 +470,7 @@ class SimpleRNN(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+        self.state_spec = InputSpec(shape=(None, self.units))\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -464,8 +478,7 @@ class SimpleRNN(Recurrent):\n \n         batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n-        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n-        self.state_spec = InputSpec(shape=(batch_size, self.units))\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n \n         self.states = [None]\n         if self.stateful:\n@@ -676,6 +689,7 @@ class GRU(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+        self.state_spec = InputSpec(shape=(None, self.units))\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -683,8 +697,7 @@ class GRU(Recurrent):\n \n         batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n-        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n-        self.state_spec = InputSpec(shape=(batch_size, self.units))\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n \n         self.states = [None]\n         if self.stateful:\n@@ -955,6 +968,8 @@ class LSTM(Recurrent):\n \n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+        self.state_spec = [InputSpec(shape=(None, self.units)),\n+                           InputSpec(shape=(None, self.units))]\n \n     def build(self, input_shape):\n         if isinstance(input_shape, list):\n@@ -962,9 +977,7 @@ class LSTM(Recurrent):\n \n         batch_size = input_shape[0] if self.stateful else None\n         self.input_dim = input_shape[2]\n-        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n-        self.state_spec = [InputSpec(shape=(batch_size, self.units)),\n-                           InputSpec(shape=(batch_size, self.units))]\n+        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n \n         self.states = [None, None]\n         if self.stateful:\n\n@@ -177,29 +177,41 @@ def test_from_config(layer_class):\n \n \n @rnn_test\n-def test_specify_initial_state(layer_class):\n+def test_specify_initial_state_keras_tensor(layer_class):\n     num_states = 2 if layer_class is recurrent.LSTM else 1\n \n     # Test with Keras tensor\n     inputs = Input((timesteps, embedding_dim))\n     initial_state = [Input((units,)) for _ in range(num_states)]\n     layer = layer_class(units)\n+    if len(initial_state) == 1:\n+        output = layer(inputs, initial_state=initial_state[0])\n+    else:\n         output = layer(inputs, initial_state=initial_state)\n+    assert initial_state[0] in layer.inbound_nodes[0].input_tensors\n+\n     model = Model([inputs] + initial_state, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_states = [np.random.random((num_samples, units))\n+    initial_state = [np.random.random((num_samples, units))\n                      for _ in range(num_states)]\n     targets = np.random.random((num_samples, units))\n-    model.fit([inputs] + initial_states, targets)\n+    model.fit([inputs] + initial_state, targets)\n+\n+\n+@rnn_test\n+def test_specify_initial_state_non_keras_tensor(layer_class):\n+    num_states = 2 if layer_class is recurrent.LSTM else 1\n \n     # Test with non-Keras tensor\n     inputs = Input((timesteps, embedding_dim))\n-    initial_state = [K.random_normal_variable((units,), 0, 1) for _ in range(num_states)]\n+    initial_state = [K.random_normal_variable((num_samples, units), 0, 1)\n+                     for _ in range(num_states)]\n     layer = layer_class(units)\n     output = layer(inputs, initial_state=initial_state)\n-    model = Model([inputs], output)\n+\n+    model = Model(inputs, output)\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n@@ -221,10 +233,34 @@ def test_reset_states_with_values(layer_class):\n                                atol=1e-4)\n     state_shapes = [K.int_shape(state) for state in layer.states]\n     values = [np.ones(shape) for shape in state_shapes]\n+    if len(values) == 1:\n+        values = values[0]\n     layer.reset_states(values)\n     np.testing.assert_allclose(K.eval(layer.states[0]),\n                                np.ones(K.int_shape(layer.states[0])),\n                                atol=1e-4)\n \n+\n+@rnn_test\n+def test_specify_state_with_masking(layer_class):\n+    ''' This test based on a previously failing issue here:\n+    https://github.com/fchollet/keras/issues/1567\n+    '''\n+    num_states = 2 if layer_class is recurrent.LSTM else 1\n+\n+    inputs = Input((timesteps, embedding_dim))\n+    masked_inputs = Masking()(inputs)\n+    initial_state = [Input((units,)) for _ in range(num_states)]\n+    output = layer_class(units)(inputs, initial_state=initial_state)\n+\n+    model = Model([inputs] + initial_state, output)\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    initial_state = [np.random.random((num_samples, units))\n+                     for _ in range(num_states)]\n+    targets = np.random.random((num_samples, units))\n+    model.fit([inputs] + initial_state, targets)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f9e7bf93cc8afb4cbeaf3182e11dc66154a218b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 7004 | Contributors (this commit): 40 | Commits (past 90d): 40 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -362,9 +362,9 @@ class Recurrent(Layer):\n             if len(states) != len(self.states):\n                 raise ValueError('Layer ' + self.name + ' expects ' +\n                                  str(len(self.states)) + ' states, '\n-                                 'but it received ' + str(len(values)) +\n+                                 'but it received ' + str(len(states)) +\n                                  ' state values. Input received: ' +\n-                                 str(values))\n+                                 str(states))\n             for index, (value, state) in enumerate(zip(states, self.states)):\n                 if value.shape != (batch_size, self.units):\n                     raise ValueError('State ' + str(index) +\n\n@@ -240,6 +240,10 @@ def test_reset_states_with_values(layer_class):\n                                np.ones(K.int_shape(layer.states[0])),\n                                atol=1e-4)\n \n+    # Test fit with invalid data\n+    with pytest.raises(ValueError):\n+        layer.reset_states([1] * (len(layer.states) + 1))\n+\n \n @rnn_test\n def test_specify_state_with_masking(layer_class):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e51d02a94f08d461a831cd85a495d225ace7278", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 80 | Churn Cumulative: 7894 | Contributors (this commit): 34 | Commits (past 90d): 57 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1608,56 +1608,53 @@ class Container(Layer):\n         nodes_depths = {}  # dict {node: depth value}\n         layers_depths = {}  # dict {layer: depth value}\n         layer_indices = {}  # dict {layer: index in traversal}\n+        nodes_in_decreasing_depth = []\n \n-        def make_node_marker(node, depth):\n-            return str(id(node)) + '-' + str(depth)\n-\n-        def build_map_of_graph(tensor, seen_nodes=None, depth=0,\n+        def build_map_of_graph(tensor, finished_nodes, nodes_in_progress,\n                                layer=None, node_index=None, tensor_index=None):\n             \"\"\"Builds a map of the graph of layers.\n \n-            This recursively updates the maps `nodes_depths`,\n-            `layers_depths` and the set `container_nodes`.\n-\n-            Does not try to detect cycles in the graph.\n+            This recursively updates the map `layer_indices`,\n+            the list `nodes_in_decreasing_depth` and the set `container_nodes`.\n \n             # Arguments\n                 tensor: Some tensor in a graph.\n-                seen_nodes: Set of node ids (\"{layer.name}_ib-{node_index}\")\n-                    of nodes seen so far. Useful to prevent infinite loops.\n-                depth: Current depth in the graph (0 = last output).\n+                finished_nodes: Set of nodes whose subgraphs have been traversed\n+                    completely. Useful to prevent duplicated work.\n+                nodes_in_progress: Set of nodes that are currently active on the\n+                    recursion stack. Useful to detect cycles.\n                 layer: Layer from which `tensor` comes from. If not provided,\n                     will be obtained from `tensor._keras_history`.\n                 node_index: Node index from which `tensor` comes from.\n                 tensor_index: Tensor_index from which `tensor` comes from.\n+\n+            # Raises\n+                RuntimeError: if a cycle is detected.\n             \"\"\"\n-            seen_nodes = seen_nodes or set()\n             if not layer or node_index is None or tensor_index is None:\n                 layer, node_index, tensor_index = tensor._keras_history\n             node = layer.inbound_nodes[node_index]\n \n             # Prevent cycles.\n-            seen_nodes.add(make_node_marker(node, depth))\n+            if node in nodes_in_progress:\n+                raise RuntimeError(\n+                    'The tensor ' + str(tensor) + ' at layer \"' +\n+                    layer.name + '\" is part of a cycle.')\n+\n+            # Don't repeat work for shared subgraphs\n+            if node in finished_nodes:\n+                return\n \n             node_key = layer.name + '_ib-' + str(node_index)\n             # Update container_nodes.\n             container_nodes.add(node_key)\n-            # Update nodes_depths.\n-            node_depth = nodes_depths.get(node)\n-            if node_depth is None:\n-                nodes_depths[node] = depth\n-            else:\n-                nodes_depths[node] = max(depth, node_depth)\n-            # Update layers_depths.\n-            previously_seen_depth = layers_depths.get(layer)\n-            if previously_seen_depth is None:\n-                current_depth = depth\n-            else:\n-                current_depth = max(depth, previously_seen_depth)\n-            layers_depths[layer] = current_depth\n+\n+            # Store the traversal order for layer sorting.\n             if layer not in layer_indices:\n                 layer_indices[layer] = len(layer_indices)\n \n+            nodes_in_progress.add(node)\n+\n             # Propagate to all previous tensors connected to this node.\n             for i in range(len(node.inbound_layers)):\n                 x = node.input_tensors[i]\n@@ -1665,15 +1662,34 @@ class Container(Layer):\n                 node_index = node.node_indices[i]\n                 tensor_index = node.tensor_indices[i]\n                 next_node = layer.inbound_nodes[node_index]\n-                # use node_marker to prevent cycles\n-                node_marker = make_node_marker(next_node, current_depth + 1)\n-                if node_marker not in seen_nodes:\n-                    build_map_of_graph(x, seen_nodes, current_depth + 1,\n+                build_map_of_graph(x, finished_nodes, nodes_in_progress,\n                                    layer, node_index, tensor_index)\n \n+            finished_nodes.add(node)\n+            nodes_in_progress.remove(node)\n+\n+            nodes_in_decreasing_depth.append(node)\n+\n+        finished_nodes = set()\n+        nodes_in_progress = set()\n         for x in self.outputs:\n-            seen_nodes = set()\n-            build_map_of_graph(x, seen_nodes, depth=0)\n+            build_map_of_graph(x, finished_nodes, nodes_in_progress)\n+\n+        for node in reversed(nodes_in_decreasing_depth):\n+            # If the depth is not set, the node has no outbound nodes (depth 0).\n+            depth = nodes_depths.setdefault(node, 0)\n+\n+            # Update the depth of inbound nodes.\n+            for i in range(len(node.inbound_layers)):\n+                inbound_layer = node.inbound_layers[i]\n+                node_index = node.node_indices[i]\n+                inbound_node = inbound_layer.inbound_nodes[node_index]\n+                previous_depth = nodes_depths.get(inbound_node, 0)\n+                nodes_depths[inbound_node] = max(depth + 1, previous_depth)\n+\n+            # Update the depth of the corresponding layer\n+            previous_depth = layers_depths.get(node.outbound_layer, 0)\n+            layers_depths[node.outbound_layer] = max(depth, previous_depth)\n \n         # Build a dict {depth: list of nodes with this depth}\n         nodes_by_depth = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54a417f6167841bebcb567c5ac98724f2038afa7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 25 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,10 @@ except ImportError:\n \n \n def _check_pydot():\n-    if not (pydot and pydot.find_graphviz()):\n+    try:\n+        # Attempt to create an image of a blank graph to check the pydot/graphviz installation.\n+        pydot.Dot.create(pydot.Dot())\n+    except Exception:  # pydot raises a generic Exception here, so no specific class can be caught.\n         raise ImportError('Failed to import pydot. You must install pydot'\n                           ' and graphviz for `pydotprint` to work.')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bcbfcc000ce07078786bae5f66f8b30dbe82fb64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 4 | Churn Cumulative: 654 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -100,7 +100,7 @@ def load_data(path='imdb.npz', num_words=None, skip_top=0,\n         for x in xs:\n             nx = []\n             for w in x:\n-                if w >= num_words or w < skip_top:\n+                if skip_top <= w < num_words:\n                     nx.append(w)\n             new_xs.append(nx)\n         xs = new_xs\n\n@@ -84,7 +84,7 @@ def load_data(path='reuters.npz', num_words=None, skip_top=0,\n         for x in xs:\n             nx = []\n             for w in x:\n-                if w >= num_words or w < skip_top:\n+                if skip_top <= w < num_words:\n                     nx.append(w)\n             new_xs.append(nx)\n         xs = new_xs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#653cfd20769db94e3234eb0be979882f12a10288", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 333 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 27 | Methods Changed: 35 | Complexity Δ (Sum/Max): 57/57 | Churn Δ: 338 | Churn Cumulative: 17441 | Contributors (this commit): 124 | Commits (past 90d): 90 | Contributors (cumulative): 150 | DMM Complexity: 0.43137254901960786\n\nDIFF:\n@@ -44,7 +44,7 @@ def set_epsilon(e):\n \n \n def floatx():\n-    \"\"\"Returns the default float type, as a string\n+    \"\"\"Returns the default float type, as a string.\n     (e.g. 'float16', 'float32', 'float64').\n \n     # Returns\n@@ -109,8 +109,7 @@ def cast_to_floatx(x):\n \n \n def image_data_format():\n-    \"\"\"Returns the default image data format\n-    convention ('channels_first' or 'channels_last').\n+    \"\"\"Returns the default image data format convention ('channels_first' or 'channels_last').\n \n     # Returns\n         A string, either `'channels_first'` or `'channels_last'`\n@@ -181,7 +180,7 @@ def set_image_dim_ordering(dim_ordering):\n     \"\"\"Legacy setter for `image_data_format`.\n \n     # Arguments\n-        dim_ordering: string. `'tf'` or `'th'`.\n+        dim_ordering: string. `tf` or `th`.\n \n     # Example\n     ```python\n@@ -192,6 +191,9 @@ def set_image_dim_ordering(dim_ordering):\n         >>> K.image_data_format()\n         'channels_last'\n     ```\n+\n+    # Raises\n+        ValueError if invalid `dim_ordering`\n     \"\"\"\n     global _IMAGE_DATA_FORMAT\n     if dim_ordering not in {'tf', 'th'}:\n@@ -205,6 +207,9 @@ def set_image_dim_ordering(dim_ordering):\n \n def image_dim_ordering():\n     \"\"\"Legacy getter for `image_data_format`.\n+\n+    # Returns\n+        string, one of `'th'`, `'tf'`\n     \"\"\"\n     if _IMAGE_DATA_FORMAT == 'channels_first':\n         return 'th'\n\n@@ -43,6 +43,14 @@ _MANUAL_VAR_INIT = False\n \n \n def get_uid(prefix=''):\n+    \"\"\"Get the uid for the default graph.\n+\n+    # Arguments\n+        prefix: An optional prefix of the graph.\n+\n+    # Returns\n+        A unique identifier for the graph.\n+    \"\"\"\n     global _GRAPH_UID_DICTS\n     graph = tf.get_default_graph()\n     if graph not in _GRAPH_UID_DICTS:\n@@ -52,6 +60,7 @@ def get_uid(prefix=''):\n \n \n def reset_uids():\n+    \"\"\"Reset graph identifiers.\"\"\"\n     global _GRAPH_UID_DICTS\n     _GRAPH_UID_DICTS = {}\n \n@@ -169,6 +178,17 @@ def set_session(session):\n # VARIABLE MANIPULATION\n \n def _convert_string_dtype(dtype):\n+    \"\"\"Get the type from a string.\n+\n+    # Arguments\n+        dtype: A string representation of a type.\n+\n+    # Returns:\n+        The type requested.\n+\n+    # Raises\n+        ValueError if `dtype` is not supported\n+    \"\"\"\n     if dtype == 'float16':\n         return tf.float16\n     if dtype == 'float32':\n@@ -190,6 +210,15 @@ def _convert_string_dtype(dtype):\n \n \n def _to_tensor(x, dtype):\n+    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n+\n+    # Arguments\n+        x: An object to be converted (numpy array, list, tensors).\n+        dtype: The destination type.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n     x = tf.convert_to_tensor(x)\n     if x.dtype != dtype:\n         x = tf.cast(x, dtype)\n@@ -309,6 +338,17 @@ def _initialize_variables():\n \n \n def constant(value, dtype=None, shape=None, name=None):\n+    \"\"\"Creates a constant tensor.\n+\n+    # Arguments\n+        value: A constant value (or list)\n+        dtype: The type of the elements of the resulting tensor.\n+        shape: Optional dimensions of resulting tensor.\n+        name: Optional name for the tensor.\n+\n+    # Returns\n+        A Constant Tensor.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     return tf.constant(value, dtype=dtype, shape=shape, name=name)\n@@ -773,18 +813,54 @@ def cast(x, dtype):\n \n \n def update(x, new_x):\n+    \"\"\"Update the value of `x` to `new_x`.\n+\n+    # Arguments\n+        x: A Variable.\n+        new_x: A tensor of same shape as `x`.\n+\n+    # Returns\n+        The variable `x` updated.\n+    \"\"\"\n     return tf.assign(x, new_x)\n \n \n def update_add(x, increment):\n+    \"\"\"Update the value of `x` by adding `increment`.\n+\n+        # Arguments\n+            x: A Variable.\n+            increment: A tensor of same shape as `x`.\n+\n+        # Returns\n+            The variable `x` updated.\n+        \"\"\"\n     return tf.assign_add(x, increment)\n \n \n def update_sub(x, decrement):\n+    \"\"\"Update the value of `x` by subtracting `decrement`.\n+\n+        # Arguments\n+            x: A Variable.\n+            decrement: A tensor of same shape as `x`.\n+\n+        # Returns\n+            The variable `x` updated.\n+        \"\"\"\n     return tf.assign_sub(x, decrement)\n \n \n def moving_average_update(x, value, momentum):\n+    \"\"\"Compute the moving average of a variable.\n+\n+    # Arguments\n+        x: A Variable.\n+        value: A tensor with the same shape as `variable`.\n+        momentum: The moving average momentum.\n+\n+    # Returns\n+        An Operation to update the variable.\"\"\"\n     return moving_averages.assign_moving_average(\n         x, value, momentum, zero_debias=False)\n \n@@ -2795,6 +2871,16 @@ def in_top_k(predictions, targets, k):\n # CONVOLUTIONS\n \n def _preprocess_deconv_output_shape(x, shape, data_format):\n+    \"\"\"Get the output_shape for the deconvolution.\n+\n+    # Arguments\n+        x: input tensor.\n+        shape: output shape.\n+        data_format: string, one of 'channels_last', 'channels_first'.\n+\n+    # Returns\n+        The output shape.\n+    \"\"\"\n     if data_format == 'channels_first':\n         shape = (shape[0], shape[2], shape[3], shape[1])\n \n@@ -2805,6 +2891,15 @@ def _preprocess_deconv_output_shape(x, shape, data_format):\n \n \n def _preprocess_conv2d_input(x, data_format):\n+    \"\"\"Transpose and cast the input before the conv2d.\n+\n+    # Arguments\n+        x: input tensor.\n+        data_format: string, one of 'channels_last', 'channels_first'.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n     if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n     if data_format == 'channels_first':\n@@ -2817,6 +2912,15 @@ def _preprocess_conv2d_input(x, data_format):\n \n \n def _preprocess_conv3d_input(x, data_format):\n+    \"\"\"Transpose and cast the input before the conv3d.\n+\n+    # Arguments\n+        x: input tensor.\n+        data_format: string, one of 'channels_last', 'channels_first'.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n     if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n     if data_format == 'channels_first':\n@@ -2825,6 +2929,15 @@ def _preprocess_conv3d_input(x, data_format):\n \n \n def _preprocess_conv2d_kernel(kernel, data_format):\n+    \"\"\"Transpose and cast the kernel before the conv2d.\n+\n+    # Arguments\n+        kernel: kernel tensor.\n+        data_format: string, one of 'channels_last', 'channels_first'.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n     if dtype(kernel) == 'float64':\n         kernel = tf.cast(kernel, 'float32')\n     if data_format == 'channels_first':\n@@ -2833,6 +2946,15 @@ def _preprocess_conv2d_kernel(kernel, data_format):\n \n \n def _preprocess_conv3d_kernel(kernel, data_format):\n+    \"\"\"Transpose and cast the kernel before the conv3d.\n+\n+    # Arguments\n+        kernel: kernel tensor.\n+        data_format: string, one of 'channels_last', 'channels_first'.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n     if dtype(kernel) == 'float64':\n         kernel = tf.cast(kernel, 'float32')\n     if data_format == 'channels_first':\n@@ -2841,16 +2963,37 @@ def _preprocess_conv3d_kernel(kernel, data_format):\n \n \n def _preprocess_padding(padding):\n+    \"\"\"Convert keras' padding to tensorflow's padding.\n+\n+    # Arguments\n+        padding: string, one of 'same' , 'valid'\n+\n+    # Returns\n+        a string, one of 'SAME', 'VALID'.\n+\n+    # Raises\n+        ValueError if invalid `padding'`\n+    \"\"\"\n     if padding == 'same':\n         padding = 'SAME'\n     elif padding == 'valid':\n         padding = 'VALID'\n     else:\n-        raise ValueError('Invalid border mode:', padding)\n+        raise ValueError('Invalid padding:', padding)\n     return padding\n \n \n def _postprocess_conv2d_output(x, data_format):\n+    \"\"\"Transpose and cast the output from conv2d if needed.\n+\n+    # Arguments\n+        x: A tensor.\n+        data_format: string, one of \"channels_last\", \"channels_first\".\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n+\n     if data_format == 'channels_first':\n         x = tf.transpose(x, (0, 3, 1, 2))\n \n@@ -2860,6 +3003,15 @@ def _postprocess_conv2d_output(x, data_format):\n \n \n def _postprocess_conv3d_output(x, data_format):\n+    \"\"\"Transpose and cast the output from conv3d if needed.\n+\n+        # Arguments\n+            x: A tensor.\n+            data_format: string, one of \"channels_last\", \"channels_first\".\n+\n+        # Returns\n+            A tensor.\n+        \"\"\"\n     if data_format == 'channels_first':\n         x = tf.transpose(x, (0, 4, 1, 2, 3))\n \n\n@@ -21,6 +21,14 @@ from ..legacy.layers import *\n \n \n def serialize(layer):\n+    \"\"\"Serialize a layer.\n+\n+    # Arguments\n+        layer: a Layer object.\n+\n+    # Returns\n+        dictionary with config.\n+    \"\"\"\n     return {'class_name': layer.__class__.__name__,\n             'config': layer.get_config()}\n \n\n@@ -293,6 +293,9 @@ def model_from_config(config, custom_objects=None):\n \n     # Returns\n         A Keras model instance (uncompiled).\n+\n+    # Raises\n+        TypeError if `config` is not a dictionary\n     \"\"\"\n     if isinstance(config, list):\n         raise TypeError('`model_from_config` expects a dictionary, not a list. '\n@@ -1227,6 +1230,15 @@ class Sequential(Model):\n \n     @classmethod\n     def legacy_from_config(cls, config, layer_cache=None):\n+        \"\"\"Load a model from a legacy configuration.\n+\n+        # Arguments\n+            config: dictionary with configuration.\n+            layer_cache: cache to draw pre-existing layer.\n+\n+        # Returns\n+            The loaded Model.\n+        \"\"\"\n         if not layer_cache:\n             layer_cache = {}\n \n\n@@ -0,0 +1,151 @@\n+import importlib\n+import inspect\n+import re\n+import sys\n+from itertools import compress\n+\n+import pytest\n+\n+modules = ['keras.layers', 'keras.models', 'keras', 'keras.backend.tensorflow_backend']\n+accepted_name = ['from_config']\n+accepted_module = ['keras.legacy.layers', 'keras.utils.generic_utils']\n+\n+# Functions or classes with less than 'MIN_CODE_SIZE' lines can be ignored\n+MIN_CODE_SIZE = 10\n+\n+\n+def handle_class(name, member):\n+    if is_accepted(name, member):\n+        return\n+\n+    if member.__doc__ is None and not member_too_small(member):\n+        raise ValueError(\"{} class doesn't have any documentation\".format(name),\n+                         member.__module__, inspect.getmodule(member).__file__)\n+    for n, met in inspect.getmembers(member):\n+        if inspect.ismethod(met):\n+            handle_method(n, met)\n+\n+\n+def handle_function(name, member):\n+    if is_accepted(name, member):\n+        return\n+    doc = member.__doc__\n+    if doc is None and not member_too_small(member):\n+        raise ValueError(\"{} function doesn't have any documentation\".format(name),\n+                         member.__module__, inspect.getmodule(member).__file__)\n+    args = list(inspect.signature(member).parameters.keys())\n+    assert_args_presence(args, doc, member, name)\n+    assert_function_style(name, member, doc, args)\n+    assert_doc_style(name, member, doc)\n+\n+\n+def assert_doc_style(name, member, doc):\n+    lines = doc.split(\"\\n\")\n+    first_line = lines[0]\n+    if len(first_line.strip()) == 0:\n+        raise ValueError(\"{} the documentation should be on the first line.\".format(name),\n+                         member.__module__)\n+    if first_line.strip()[-1] != '.':\n+        raise ValueError(\"{} first line should end with a '.'\".format(name),\n+                         member.__module__)\n+\n+\n+def assert_function_style(name, member, doc, args):\n+    code = inspect.getsource(member)\n+    has_return = re.findall(r\"\\s*return \\S+\", code, re.MULTILINE)\n+    if has_return and \"# Returns\" not in doc:\n+        innerfunction = [inspect.getsource(x) for x in member.__code__.co_consts if\n+                         inspect.iscode(x)]\n+        return_in_sub = [ret for code_inner in innerfunction for ret in\n+                         re.findall(r\"\\s*return \\S+\", code_inner, re.MULTILINE)]\n+        if len(return_in_sub) < len(has_return):\n+            raise ValueError(\"{} needs a '# Returns' section\".format(name),\n+                             member.__module__)\n+\n+    has_raise = re.findall(r\"^\\s*raise \\S+\", code, re.MULTILINE)\n+    if has_raise and \"# Raises\" not in doc:\n+        innerfunction = [inspect.getsource(x) for x in member.__code__.co_consts if\n+                         inspect.iscode(x)]\n+        raise_in_sub = [ret for code_inner in innerfunction for ret in\n+                        re.findall(r\"\\s*raise \\S+\", code_inner, re.MULTILINE)]\n+        if len(raise_in_sub) < len(has_raise):\n+            raise ValueError(\"{} needs a '# Raises' section\".format(name),\n+                             member.__module__)\n+\n+    if len(args) > 0 and \"# Arguments\" not in doc:\n+        raise ValueError(\"{} needs a '# Arguments' section\".format(name),\n+                         member.__module__)\n+\n+    assert_blank_before(name, member, doc, ['# Arguments', '# Raises', '# Returns'])\n+\n+\n+def assert_blank_before(name, member, doc, keywords):\n+    doc_lines = [x.strip() for x in doc.split('\\n')]\n+    for keyword in keywords:\n+        if keyword in doc_lines:\n+            index = doc_lines.index(keyword)\n+            if doc_lines[index - 1] != '':\n+                raise ValueError(\n+                    \"{} '{}' should have a blank line above.\".format(name, keyword),\n+                    member.__module__)\n+\n+\n+def is_accepted(name, member):\n+    if 'keras' not in str(member.__module__):\n+        return True\n+    return name in accepted_name or member.__module__ in accepted_module\n+\n+\n+def member_too_small(member):\n+    code = inspect.getsource(member).split('\\n')\n+    return len(code) < MIN_CODE_SIZE\n+\n+\n+def assert_args_presence(args, doc, member, name):\n+    args_not_in_doc = [arg not in doc for arg in args]\n+    if any(args_not_in_doc):\n+        raise ValueError(\n+            \"{} {} arguments are not present in documentation \".format(name, list(\n+                compress(args, args_not_in_doc))), member.__module__)\n+    words = doc.replace('*', '').split()\n+    # Check arguments styling\n+    styles = [arg + \":\" not in words for arg in args]\n+    if any(styles):\n+        raise ValueError(\n+            \"{} {} are not style properly 'argument': documentation\".format(name, list(\n+                compress(args, styles))), member.__module__)\n+\n+    # Check arguments order\n+    indexes = [words.index(arg + \":\") for arg in args]\n+    if indexes != sorted(indexes):\n+        raise ValueError(\n+            \"{} arguments order is different from the documentation\".format(name),\n+            member.__module__)\n+\n+\n+def handle_method(name, member):\n+    if name in accepted_name or member.__module__ in accepted_module:\n+        return\n+    handle_function(name, member)\n+\n+\n+def handle_module(mod):\n+    for name, mem in inspect.getmembers(mod):\n+        if inspect.isclass(mem):\n+            handle_class(name, mem)\n+        elif inspect.isfunction(mem):\n+            handle_function(name, mem)\n+        elif 'keras' in name and inspect.ismodule(mem):\n+            # Only test keras' modules\n+            handle_module(mem)\n+\n+\n+@pytest.mark.skipif(sys.version_info < (3, 3), reason=\"requires python3.3\")\n+def test_doc():\n+    for module in modules:\n+        mod = importlib.import_module(module)\n+        handle_module(mod)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bdf05c48ef4ab31c9371cdaf986f647163888fa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4299 | Contributors (this commit): 42 | Commits (past 90d): 40 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -1939,7 +1939,7 @@ class Model(Container):\n         The generator should return the same kind of data\n         as accepted by `test_on_batch`.\n \n-        Arguments:\n+        # Arguments\n             generator: Generator yielding tuples (inputs, targets)\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16aa56bb1d1247adadbef3d5ad39a102d5008486", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 4304 | Contributors (this commit): 42 | Commits (past 90d): 41 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -685,6 +685,8 @@ class Model(Container):\n                 See [losses](/losses).\n                 If the model has multiple outputs, you can use a different loss\n                 on each output by passing a dictionary or a list of losses.\n+                The loss value that will be minimized by the model\n+                will then be the sum of all individual losses.\n             metrics: list of metrics to be evaluated by the model\n                 during training and testing.\n                 Typically you will use `metrics=['accuracy']`.\n@@ -694,6 +696,9 @@ class Model(Container):\n             loss_weights: Optional list or dictionary specifying scalar\n                 coefficients (Python floats) to weight the loss contributions\n                 of different model outputs.\n+                The loss value that will be minimized by the model\n+                will then be the *weighted sum* of all individual losses,\n+                weighted by the `loss_weights` coefficients.\n                 If a list, it is expected to have a 1:1 mapping\n                 to the model's outputs. If a tensor, it is expected to map\n                 output names (strings) to scalar coefficients.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8967d16d005decee9c382cb677b68e4eb0b9e330", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 898 | Contributors (this commit): 10 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,12 @@ def hinge(y_true, y_pred):\n     return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)\n \n \n+def logcosh(y_true, y_pred):\n+    def cosh(x):\n+        return (K.exp(x) + K.exp(-x)) / 2\n+    return K.mean(K.log(cosh(y_pred - y_true)), axis=-1)\n+\n+\n def categorical_crossentropy(y_true, y_pred):\n     return K.categorical_crossentropy(y_pred, y_true)\n \n\n@@ -6,6 +6,7 @@ from .losses import mean_absolute_error\n from .losses import mean_absolute_percentage_error\n from .losses import mean_squared_logarithmic_error\n from .losses import hinge\n+from .losses import logcosh\n from .losses import squared_hinge\n from .losses import categorical_crossentropy\n from .losses import sparse_categorical_crossentropy\n\n@@ -14,7 +14,8 @@ allobj = [losses.mean_squared_error,\n           losses.binary_crossentropy,\n           losses.kullback_leibler_divergence,\n           losses.poisson,\n-          losses.cosine_proximity]\n+          losses.cosine_proximity,\n+          losses.logcosh]\n \n \n def test_objective_shapes_3d():\n\n@@ -17,6 +17,7 @@ all_metrics = [\n     metrics.binary_crossentropy,\n     metrics.poisson,\n     metrics.cosine_proximity,\n+    metrics.logcosh,\n ]\n \n all_sparse_metrics = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
