{"custom_id": "keras#03861428e51b1b6986d51bf6cee65190d4defcf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 31 | Churn Cumulative: 5151 | Contributors (this commit): 21 | Commits (past 90d): 51 | Contributors (cumulative): 28 | DMM Complexity: 0.12903225806451613\n\nDIFF:\n@@ -2553,6 +2553,8 @@ def full_like(x, fill_value, dtype=None):\n \n class GetItem(Operation):\n     def call(self, x, key):\n+        if isinstance(key, list):\n+            key = tuple(key)\n         return x[key]\n \n     def compute_output_spec(self, x, key):\n\n@@ -153,6 +153,15 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__bytes__\",\n             \"config\": {\"value\": obj.decode(\"utf-8\")},\n         }\n+    if isinstance(obj, slice):\n+        return {\n+            \"class_name\": \"__slice__\",\n+            \"config\": {\n+                \"start\": serialize_keras_object(obj.start),\n+                \"stop\": serialize_keras_object(obj.stop),\n+                \"step\": serialize_keras_object(obj.step),\n+            },\n+        }\n     if isinstance(obj, backend.KerasTensor):\n         history = getattr(obj, \"_keras_history\", None)\n         if history:\n@@ -602,6 +611,24 @@ def deserialize_keras_object(\n         return np.array(inner_config[\"value\"], dtype=inner_config[\"dtype\"])\n     if config[\"class_name\"] == \"__bytes__\":\n         return inner_config[\"value\"].encode(\"utf-8\")\n+    if config[\"class_name\"] == \"__slice__\":\n+        return slice(\n+            deserialize_keras_object(\n+                inner_config[\"start\"],\n+                custom_objects=custom_objects,\n+                safe_mode=safe_mode,\n+            ),\n+            deserialize_keras_object(\n+                inner_config[\"stop\"],\n+                custom_objects=custom_objects,\n+                safe_mode=safe_mode,\n+            ),\n+            deserialize_keras_object(\n+                inner_config[\"step\"],\n+                custom_objects=custom_objects,\n+                safe_mode=safe_mode,\n+            ),\n+        )\n     if config[\"class_name\"] == \"__lambda__\":\n         if safe_mode:\n             raise ValueError(\n\n@@ -75,6 +75,8 @@ class SerializationLibTest(testing.TestCase):\n             [\"hello\", 0, \"world\", 1.0, True],\n             {\"1\": \"hello\", \"2\": 0, \"3\": True},\n             {\"1\": \"hello\", \"2\": [True, False]},\n+            slice(None, 20, 1),\n+            slice(None, np.array([0, 1]), 1),\n         ]:\n             serialized, _, reserialized = self.roundtrip(obj)\n             self.assertEqual(serialized, reserialized)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72c2e8c69c9b2fe653c8c55e14820e16f265d3f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 14 | Methods Changed: 13 | Complexity Δ (Sum/Max): 20/7 | Churn Δ: 82 | Churn Cumulative: 6771 | Contributors (this commit): 19 | Commits (past 90d): 108 | Contributors (cumulative): 60 | DMM Complexity: 1.0\n\nDIFF:\n@@ -113,6 +113,13 @@ def append(\n \n \n def arange(start, stop=None, step=1, dtype=None):\n+    if dtype is None:\n+        if hasattr(start, \"dtype\"):\n+            dtype = start.dtype\n+        elif isinstance(start, int):\n+            dtype = \"int32\"\n+        else:\n+            dtype = config.floatx()\n     return jnp.arange(start, stop, step=step, dtype=dtype)\n \n \n\n@@ -1,5 +1,8 @@\n import numpy as np\n \n+from keras_core.backend import config\n+from keras_core.backend import standardize_dtype\n+\n \n def add(x1, x2):\n     return np.add(x1, x2)\n@@ -77,6 +80,13 @@ def append(\n \n \n def arange(start, stop=None, step=None, dtype=None):\n+    if dtype is None:\n+        if hasattr(start, \"dtype\"):\n+            dtype = start.dtype\n+        elif isinstance(start, int):\n+            dtype = \"int32\"\n+        else:\n+            dtype = config.floatx()\n     return np.arange(start, stop, step=step, dtype=dtype)\n \n \n@@ -124,6 +134,7 @@ def argsort(x, axis=-1):\n \n \n def array(x, dtype=None):\n+    dtype = dtype or config.floatx()\n     return np.array(x, dtype=dtype)\n \n \n@@ -271,6 +282,7 @@ def floor(x):\n \n \n def full(shape, fill_value, dtype=None):\n+    dtype = dtype or config.floatx()\n     return np.full(shape, fill_value, dtype=dtype)\n \n \n@@ -592,7 +604,11 @@ def square(x):\n \n \n def sqrt(x):\n-    return np.sqrt(x)\n+    dtype = None\n+    if hasattr(x, \"dtype\"):\n+        if standardize_dtype(x.dtype).startswith(\"int\"):\n+            dtype = config.floatx()\n+    return np.sqrt(x, dtype=dtype)\n \n \n def squeeze(x, axis=None):\n\n@@ -5,6 +5,7 @@ import warnings\n import tensorflow as tf\n from tensorflow.experimental import numpy as tfnp\n \n+from keras_core.backend import config\n from keras_core.backend.tensorflow.core import convert_to_tensor\n \n \n@@ -176,6 +177,13 @@ def append(\n def arange(start, stop=None, step=1, dtype=None):\n     # tfnp.arange has trouble with dynamic Tensors in compiled function.\n     # tf.range does not.\n+    if dtype is None:\n+        if hasattr(start, \"dtype\"):\n+            dtype = start.dtype\n+        elif isinstance(start, int):\n+            dtype = \"int32\"\n+        else:\n+            dtype = config.floatx()\n     return tf.range(start, stop, delta=step, dtype=dtype)\n \n \n@@ -749,6 +757,9 @@ def square(x):\n \n \n def sqrt(x):\n+    x = convert_to_tensor(x)\n+    if tf.as_dtype(x.dtype).is_integer:\n+        x = tf.cast(x, dtype=config.floatx())\n     return tfnp.sqrt(x)\n \n \n\n@@ -1,6 +1,7 @@\n import numpy as np\n import torch\n \n+from keras_core.backend import config\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import get_device\n@@ -91,7 +92,7 @@ def zeros(shape, dtype=\"float32\"):\n \n def zeros_like(x, dtype=None):\n     x = convert_to_tensor(x)\n-    dtype = to_torch_dtype(dtype)\n+    dtype = to_torch_dtype(dtype or x.dtype)\n     return torch.zeros_like(x, dtype=dtype)\n \n \n@@ -160,6 +161,13 @@ def append(\n \n \n def arange(start, stop=None, step=1, dtype=None):\n+    if dtype is None:\n+        if hasattr(start, \"dtype\"):\n+            dtype = start.dtype\n+        elif isinstance(start, int):\n+            dtype = \"int32\"\n+        else:\n+            dtype = config.floatx()\n     dtype = to_torch_dtype(dtype)\n     if stop is None:\n         return torch.arange(end=start, dtype=dtype, device=get_device())\n\n@@ -3571,9 +3571,37 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(len(knp.Split(2)(x)), 2)\n \n     def test_sqrt(self):\n-        x = np.array([[1, 4, 9], [16, 25, 36]])\n-        self.assertAllClose(knp.sqrt(x), np.sqrt(x))\n-        self.assertAllClose(knp.Sqrt()(x), np.sqrt(x))\n+        x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float32\")\n+        ref_y = np.sqrt(x)\n+        y = knp.sqrt(x)\n+        self.assertEqual(standardize_dtype(y.dtype), \"float32\")\n+        self.assertAllClose(y, ref_y)\n+        y = knp.Sqrt()(x)\n+        self.assertEqual(standardize_dtype(y.dtype), \"float32\")\n+        self.assertAllClose(y, ref_y)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"jax\", reason=\"JAX does not support float64.\"\n+    )\n+    def test_sqrt_float64(self):\n+        x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"float64\")\n+        ref_y = np.sqrt(x)\n+        y = knp.sqrt(x)\n+        self.assertEqual(standardize_dtype(y.dtype), \"float64\")\n+        self.assertAllClose(y, ref_y)\n+        y = knp.Sqrt()(x)\n+        self.assertEqual(standardize_dtype(y.dtype), \"float64\")\n+        self.assertAllClose(y, ref_y)\n+\n+    def test_sqrt_int32(self):\n+        x = np.array([[1, 4, 9], [16, 25, 36]], dtype=\"int32\")\n+        ref_y = np.sqrt(x)\n+        y = knp.sqrt(x)\n+        self.assertEqual(standardize_dtype(y.dtype), \"float32\")\n+        self.assertAllClose(y, ref_y)\n+        y = knp.Sqrt()(x)\n+        self.assertEqual(standardize_dtype(y.dtype), \"float32\")\n+        self.assertAllClose(y, ref_y)\n \n     def test_stack(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n@@ -3704,6 +3732,8 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.Arange()(3, 7), np.arange(3, 7))\n         self.assertAllClose(knp.Arange()(3, 7, 2), np.arange(3, 7, 2))\n \n+        self.assertEqual(standardize_dtype(knp.arange(3).dtype), \"int32\")\n+\n     def test_full(self):\n         self.assertAllClose(knp.full([2, 3], 0), np.full([2, 3], 0))\n         self.assertAllClose(knp.full([2, 3], 0.1), np.full([2, 3], 0.1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#809cd52cab46afe9ee57b7bbfcaad4bd707735a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 53 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import tree\n \n from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n+from keras_core.backend.config import backend\n from keras_core.ops.operation import Operation\n from keras_core.utils.nest import pack_sequence_as\n \n@@ -46,10 +47,21 @@ class Function(Operation):\n     def __init__(self, inputs, outputs, name=None):\n         super().__init__(name=name)\n \n+        if backend() == \"tensorflow\":\n+            # Temporary work around for\n+            # https://github.com/keras-team/keras-core/issues/931\n+            # This stop tensorflow from wrapping tf.function output in a\n+            # _DictWrapper object.\n+            _self_setattr_tracking = getattr(\n+                self, \"_self_setattr_tracking\", True\n+            )\n+            self._self_setattr_tracking = False\n         self._inputs_struct = tree.map_structure(lambda x: x, inputs)\n         self._outputs_struct = tree.map_structure(lambda x: x, outputs)\n         self._inputs = tree.flatten(inputs)\n         self._outputs = tree.flatten(outputs)\n+        if backend() == \"tensorflow\":\n+            self._self_setattr_tracking = _self_setattr_tracking\n \n         (nodes, nodes_by_depth, operations, operations_by_depth) = map_graph(\n             self._inputs, self._outputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a284cc163d8c7341baac2c946085ee5a6cd73143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 20 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.6\"\n+__version__ = \"0.1.7\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fdddd6eee909be747182e1702c831792ac16efd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 225 | Lines Deleted: 34 | Files Changed: 4 | Hunks: 25 | Methods Changed: 17 | Complexity Δ (Sum/Max): 34/20 | Churn Δ: 259 | Churn Cumulative: 10946 | Contributors (this commit): 24 | Commits (past 90d): 108 | Contributors (cumulative): 48 | DMM Complexity: 0.5512820512820513\n\nDIFF:\n@@ -1,9 +1,11 @@\n import builtins\n import functools\n+import math\n import warnings\n \n import tensorflow as tf\n from tensorflow.experimental import numpy as tfnp\n+from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n \n from keras_core.backend import config\n from keras_core.backend.tensorflow.core import convert_to_tensor\n@@ -52,22 +54,82 @@ def subtract(x1, x2):\n \n \n def matmul(x1, x2):\n-    if isinstance(x1, tf.SparseTensor):\n+    def with_combined_batch_dimensions(a, b, fn_3d):\n+        batch_shape = (\n+            b.shape[:-2] if isinstance(b, tf.SparseTensor) else a.shape[:-2]\n+        )\n+        batch_size = math.prod(batch_shape)\n+        a_3d = reshape(a, [batch_size] + a.shape[-2:])\n+        b_3d = reshape(b, [batch_size] + b.shape[-2:])\n+        result = fn_3d(a_3d, b_3d)\n+        return reshape(result, batch_shape + result.shape[1:])\n+\n+    def sparse_sparse_matmul(a, b):\n+        dtype = a.values.dtype\n+        # Convert SparseTensors to CSR SparseMatrix.\n+        a_csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n+            a.indices, a.values, a.dense_shape\n+        )\n+        b_csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n+            b.indices, b.values, b.dense_shape\n+        )\n+        # Compute the CSR SparseMatrix matrix multiplication.\n+        result_csr = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\n+            a_csr, b_csr, dtype\n+        )\n+        # Convert the CSR SparseMatrix to a SparseTensor.\n+        res = sparse_csr_matrix_ops.csr_sparse_matrix_to_sparse_tensor(\n+            result_csr, dtype\n+        )\n+        return tf.SparseTensor(res.indices, res.values, res.dense_shape)\n+\n+    def embedding_lookup_sparse_dense_matmul(a, b):\n         # We need at least one id per rows for embedding_lookup_sparse,\n         # otherwise there will be missing rows in the output.\n-        x1, _ = tf.sparse.fill_empty_rows(x1, 0)\n+        a, _ = tf.sparse.fill_empty_rows(a, 0)\n         # We need to split x1 into separate ids and weights tensors. The ids\n         # should be the column indices of x1 and the values of the weights\n-        # can continue to be the actual x1. The column arrangement of ids and\n-        # weights does not matter as we sum over columns. See documentation for\n-        # sparse_ops.sparse_tensor_dense_matmul for details.\n+        # can continue to be the actual x1. The column arrangement of ids\n+        # and weights does not matter as we sum over columns. See details in\n+        # the documentation for sparse_ops.sparse_tensor_dense_matmul.\n         ids = tf.SparseTensor(\n-            indices=x1.indices,\n-            values=x1.indices[:, 1],\n-            dense_shape=x1.dense_shape,\n+            indices=a.indices,\n+            values=a.indices[:, 1],\n+            dense_shape=a.dense_shape,\n         )\n-        weights = x1\n-        return tf.nn.embedding_lookup_sparse(x2, ids, weights, combiner=\"sum\")\n+        return tf.nn.embedding_lookup_sparse(b, ids, a, combiner=\"sum\")\n+\n+    # Either a or b is sparse\n+    def sparse_dense_matmul_3d(a, b):\n+        return tf.map_fn(\n+            lambda x: tf.sparse.sparse_dense_matmul(x[0], x[1]),\n+            elems=(a, b),\n+            fn_output_signature=a.dtype,\n+        )\n+\n+    x1_sparse = isinstance(x1, tf.SparseTensor)\n+    x2_sparse = isinstance(x2, tf.SparseTensor)\n+    if x1_sparse and x2_sparse:\n+        if x1.shape.rank <= 3:\n+            return sparse_sparse_matmul(x1, x2)\n+        else:\n+            return with_combined_batch_dimensions(x1, x2, sparse_sparse_matmul)\n+    elif x1_sparse or x2_sparse:\n+        # Sparse * dense or dense * sparse\n+        sparse_rank = x1.shape.rank if x1_sparse else x2.shape.rank\n+\n+        # Special case: embedding_lookup_sparse for sparse * dense and rank 2\n+        if x1_sparse and sparse_rank == 2:\n+            return embedding_lookup_sparse_dense_matmul(x1, x2)\n+        elif sparse_rank == 2:\n+            return tf.sparse.sparse_dense_matmul(x1, x2)\n+        elif sparse_rank == 3:\n+            return sparse_dense_matmul_3d(x1, x2)\n+        else:\n+            return with_combined_batch_dimensions(\n+                x1, x2, sparse_dense_matmul_3d\n+            )\n+\n     return tfnp.matmul(x1, x2)\n \n \n@@ -354,6 +416,8 @@ def exp(x):\n \n \n def expand_dims(x, axis):\n+    if isinstance(x, tf.SparseTensor):\n+        return tf.sparse.expand_dims(x, axis)\n     return tfnp.expand_dims(x, axis)\n \n \n@@ -764,6 +828,24 @@ def sqrt(x):\n \n \n def squeeze(x, axis=None):\n+    if isinstance(x, tf.SparseTensor):\n+        new_shape = list(x.shape)\n+        gather_indices = list(range(len(new_shape)))\n+        if axis is None:\n+            for i in range(len(new_shape) - 1, -1, -1):\n+                if new_shape[i] == 1:\n+                    del new_shape[i]\n+                    del gather_indices[i]\n+        else:\n+            if new_shape[axis] != 1:\n+                raise ValueError(\n+                    f\"Cannot squeeze axis {axis}, because the \"\n+                    \"dimension is not 1.\"\n+                )\n+            del new_shape[axis]\n+            del gather_indices[axis]\n+        new_indices = tf.gather(x.indices, gather_indices, axis=1)\n+        return tf.SparseTensor(new_indices, x.values, tuple(new_shape))\n     return tfnp.squeeze(x, axis=axis)\n \n \n\n@@ -237,9 +237,6 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n     ):\n         import tensorflow as tf\n \n-        if layer_class == layers.Dot:\n-            pytest.skip(\"Dot layer does not support sparse tensors.\")\n-\n         self.run_layer_test(\n             layer_class,\n             init_kwargs=init_kwargs,\n\n@@ -2395,7 +2395,7 @@ class ExpandDims(Operation):\n         else:\n             axis = self.axis\n         output_shape = x_shape[:axis] + [1] + x_shape[axis:]\n-        return KerasTensor(output_shape, dtype=x.dtype)\n+        return KerasTensor(output_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n @keras_core_export(\n@@ -3368,7 +3368,10 @@ class Matmul(Operation):\n             del output_shape[-2]\n         if len(x2.shape) == 1:\n             del output_shape[-1]\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        x1_sparse = getattr(x1, \"sparse\", True)\n+        x2_sparse = getattr(x2, \"sparse\", True)\n+        output_sparse = x1_sparse and x2_sparse\n+        return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n @keras_core_export([\"keras_core.ops.matmul\", \"keras_core.ops.numpy.matmul\"])\n@@ -5309,7 +5312,7 @@ class Squeeze(Operation):\n         input_shape = list(x.shape)\n         if self.axis is None:\n             output_shape = list(filter((1).__ne__, input_shape))\n-            return KerasTensor(output_shape)\n+            return KerasTensor(output_shape, dtype=x.dtype, sparse=x.sparse)\n         else:\n             if input_shape[self.axis] != 1:\n                 raise ValueError(\n@@ -5317,7 +5320,7 @@ class Squeeze(Operation):\n                     \"is not 1.\"\n                 )\n             del input_shape[self.axis]\n-            return KerasTensor(input_shape, dtype=x.dtype)\n+            return KerasTensor(input_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n @keras_core_export([\"keras_core.ops.squeeze\", \"keras_core.ops.numpy.squeeze\"])\n\n@@ -464,6 +464,23 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.matmul(x, y)\n \n+    def test_matmul_sparse(self):\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((3, 2))\n+        result = knp.matmul(x, y)\n+        self.assertEqual(result.shape, (2, 2))\n+\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((3, 2), sparse=True)\n+        result = knp.matmul(x, y)\n+        self.assertEqual(result.shape, (2, 2))\n+\n+        x = KerasTensor((2, 3), sparse=True)\n+        y = KerasTensor((3, 2), sparse=True)\n+        result = knp.matmul(x, y)\n+        self.assertEqual(result.shape, (2, 2))\n+        self.assertTrue(result.sparse)\n+\n     def test_power(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n@@ -1322,6 +1339,13 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n         self.assertEqual(knp.reshape(x, (3, -1)).shape, (3, None))\n \n+    def test_reshape_sparse(self):\n+        x = KerasTensor([None, 3], sparse=True)\n+        self.assertTrue(knp.reshape(x, (3, 2)).sparse)\n+        self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n+        self.assertTrue(knp.reshape(x, (3, -1)).sparse)\n+        self.assertEqual(knp.reshape(x, (3, -1)).shape, (3, None))\n+\n     def test_roll(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.roll(x, 1).shape, (None, 3))\n@@ -1475,6 +1499,27 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.squeeze(x).shape, (2, 3))\n \n+        x = KerasTensor([2, 1, 3])\n+        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n+        self.assertEqual(knp.squeeze(x, axis=1).shape, (2, 3))\n+        self.assertEqual(knp.squeeze(x, axis=-2).shape, (2, 3))\n+\n+        with self.assertRaises(ValueError):\n+            knp.squeeze(x, axis=0)\n+\n+    def test_squeeze_sparse(self):\n+        x = KerasTensor([2, 3], sparse=True)\n+        self.assertTrue(knp.squeeze(x).sparse)\n+        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n+\n+        x = KerasTensor([2, 1, 3], sparse=True)\n+        self.assertTrue(knp.squeeze(x).sparse)\n+        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n+        self.assertTrue(knp.squeeze(x, axis=1).sparse)\n+        self.assertEqual(knp.squeeze(x, axis=1).shape, (2, 3))\n+        self.assertTrue(knp.squeeze(x, axis=-2).sparse)\n+        self.assertEqual(knp.squeeze(x, axis=-2).shape, (2, 3))\n+\n     def test_transpose(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.transpose(x).shape, (3, 2))\n@@ -1641,6 +1686,15 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knp.expand_dims(x, 1).shape, (2, 1, 3, 4))\n         self.assertEqual(knp.expand_dims(x, -2).shape, (2, 3, 1, 4))\n \n+    def test_expand_dims_sparse(self):\n+        x = KerasTensor([2, 3, 4], sparse=True)\n+        self.assertTrue(knp.expand_dims(x, 0).sparse)\n+        self.assertEqual(knp.expand_dims(x, 0).shape, (1, 2, 3, 4))\n+        self.assertTrue(knp.expand_dims(x, 1).sparse)\n+        self.assertEqual(knp.expand_dims(x, 1).shape, (2, 1, 3, 4))\n+        self.assertTrue(knp.expand_dims(x, -2).sparse)\n+        self.assertEqual(knp.expand_dims(x, -2).shape, (2, 3, 1, 4))\n+\n     def test_expm1(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.expm1(x).shape, (2, 3))\n@@ -1957,30 +2011,45 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Matmul()(x, y), np.matmul(x, y))\n         self.assertAllClose(knp.Matmul()(x, z), np.matmul(x, z))\n \n-    @parameterized.parameters(\n-        (\"float16\",),\n-        (\"float32\",),\n-        (\"float64\",),\n-        (\"uint8\",),\n-        (\"int8\",),\n-        (\"int16\",),\n-        (\"int32\",),\n+    @parameterized.product(\n+        (\n+            {\"x_shape\": (5, 3), \"y_shape\": (3, 4)},\n+            {\"x_shape\": (2, 5, 3), \"y_shape\": (2, 3, 4)},\n+            {\"x_shape\": (2, 2, 5, 3), \"y_shape\": (2, 2, 3, 4)},\n+        ),\n+        dtype=[\"float16\", \"float32\", \"float64\", \"int32\"],\n+        x_sparse=[False, True],\n+        y_sparse=[False, True],\n     )\n     @pytest.mark.skipif(\n         not backend.SUPPORTS_SPARSE_TENSORS,\n         reason=\"Backend does not support sparse tensors.\",\n     )\n-    def test_matmul_sparse(self, dtype):\n+    def test_matmul_sparse(self, dtype, x_shape, y_shape, x_sparse, y_sparse):\n         import tensorflow as tf\n \n+        if x_sparse and y_sparse and dtype in (\"float16\", \"int32\"):\n+            pytest.skip(f\"Sparse sparse matmul unsupported for {dtype}\")\n+\n         rng = np.random.default_rng(0)\n-        x1 = 4 * rng.standard_normal((5, 3))\n-        x1 = tf.sparse.from_dense(tf.cast(tf.nn.dropout(x1, 0.7), dtype=dtype))\n-        x2 = (4 * rng.standard_normal((3, 4))).astype(dtype)\n-        self.assertAllClose(\n-            knp.matmul(x1, x2),\n-            np.matmul(tf.sparse.to_dense(x1).numpy(), x2),\n-        )\n+        if x_sparse:\n+            x = 4 * rng.standard_normal(x_shape)\n+            x = tf.sparse.from_dense(tf.cast(tf.nn.dropout(x, 0.7), dtype))\n+            x_np = tf.sparse.to_dense(x).numpy()\n+        else:\n+            x = x_np = (4 * rng.standard_normal(x_shape)).astype(dtype)\n+        y = y_np = (4 * rng.standard_normal(y_shape)).astype(dtype)\n+        if y_sparse:\n+            y = 4 * rng.standard_normal(y_shape)\n+            y = tf.sparse.from_dense(tf.cast(tf.nn.dropout(y, 0.7), dtype))\n+            y_np = tf.sparse.to_dense(y).numpy()\n+        else:\n+            y = y_np = (4 * rng.standard_normal(y_shape)).astype(dtype)\n+\n+        atol = 0.1 if dtype == \"float16\" else 1e-5\n+        self.assertAllClose(knp.matmul(x, y), np.matmul(x_np, y_np), atol=atol)\n+        if x_sparse and y_sparse:\n+            self.assertIsInstance(knp.matmul(x, y), tf.SparseTensor)\n \n     def test_power(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n@@ -2722,13 +2791,32 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Absolute()(x), np.absolute(x))\n \n     def test_squeeze(self):\n-        x = np.ones([1, 2, 3, 4, 5])\n+        x = np.ones([1, 3, 1, 5])\n         self.assertAllClose(knp.squeeze(x), np.squeeze(x))\n         self.assertAllClose(knp.squeeze(x, axis=0), np.squeeze(x, axis=0))\n \n         self.assertAllClose(knp.Squeeze()(x), np.squeeze(x))\n         self.assertAllClose(knp.Squeeze(axis=0)(x), np.squeeze(x, axis=0))\n \n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_squeeze_sparse(self):\n+        import tensorflow as tf\n+\n+        x = tf.SparseTensor(\n+            indices=[[0, 0, 0, 0], [0, 2, 0, 4]],\n+            values=[1, 2],\n+            dense_shape=(1, 3, 1, 5),\n+        )\n+        x_np = tf.sparse.to_dense(x).numpy()\n+        self.assertAllClose(knp.squeeze(x), np.squeeze(x_np))\n+        self.assertAllClose(knp.squeeze(x, axis=0), np.squeeze(x_np, axis=0))\n+\n+        self.assertAllClose(knp.Squeeze()(x), np.squeeze(x_np))\n+        self.assertAllClose(knp.Squeeze(axis=0)(x), np.squeeze(x_np, axis=0))\n+\n     def test_transpose(self):\n         x = np.ones([1, 2, 3, 4, 5])\n         self.assertAllClose(knp.transpose(x), np.transpose(x))\n@@ -3189,6 +3277,27 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.ExpandDims(1)(x), np.expand_dims(x, 1))\n         self.assertAllClose(knp.ExpandDims(-2)(x), np.expand_dims(x, -2))\n \n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_expand_dims_sparse(self):\n+        import tensorflow as tf\n+\n+        x = tf.SparseTensor(\n+            indices=[[0, 0], [1, 2]],\n+            values=[1, 2],\n+            dense_shape=(2, 3),\n+        )\n+        x_np = tf.sparse.to_dense(x).numpy()\n+        self.assertAllClose(knp.expand_dims(x, 0), np.expand_dims(x_np, 0))\n+        self.assertAllClose(knp.expand_dims(x, 1), np.expand_dims(x_np, 1))\n+        self.assertAllClose(knp.expand_dims(x, -2), np.expand_dims(x_np, -2))\n+\n+        self.assertAllClose(knp.ExpandDims(0)(x), np.expand_dims(x_np, 0))\n+        self.assertAllClose(knp.ExpandDims(1)(x), np.expand_dims(x_np, 1))\n+        self.assertAllClose(knp.ExpandDims(-2)(x), np.expand_dims(x_np, -2))\n+\n     def test_expm1(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.expm1(x), np.expm1(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02a0eea5c3dd22149bce82151b29cbc05bb3e741", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 441 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 23 | Complexity Δ (Sum/Max): 54/35 | Churn Δ: 441 | Churn Cumulative: 456 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 0.8154981549815498\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras_core.callbacks.backup_and_restore_callback import BackupAndRestore\n from keras_core.callbacks.callback import Callback\n from keras_core.callbacks.callback_list import CallbackList\n from keras_core.callbacks.csv_logger import CSVLogger\n\n@@ -0,0 +1,216 @@\n+import os\n+import warnings\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.utils import file_utils\n+\n+\n+@keras_core_export(\"keras_core.callbacks.BackupAndRestore\")\n+class BackupAndRestore(Callback):\n+    \"\"\"Callback to back up and restore the training state.\n+\n+    `BackupAndRestore` callback is intended to recover training from an\n+    interruption that has happened in the middle of a `Model.fit` execution, by\n+    backing up the training states in a temporary checkpoint file, at the end of\n+    each epoch. Each backup overwrites the previously written checkpoint file,\n+    so at any given time there is at most one such checkpoint file for\n+    backup/restoring purpose.\n+\n+    If training restarts before completion, the training state (which includes\n+    the `Model` weights and epoch number) is restored to the most recently saved\n+    state at the beginning of a new `Model.fit` run. At the completion of a\n+    `Model.fit` run, the temporary checkpoint file is deleted.\n+\n+    Note that the user is responsible to bring jobs back after the interruption.\n+    This callback is important for the backup and restore mechanism for fault\n+    tolerance purpose, and the model to be restored from a previous checkpoint\n+    is expected to be the same as the one used to back up. If user changes\n+    arguments passed to compile or fit, the checkpoint saved for fault tolerance\n+    can become invalid.\n+\n+    Example:\n+\n+    >>> class InterruptingCallback(keras.callbacks.Callback):\n+    ...   def on_epoch_begin(self, epoch, logs=None):\n+    ...     if epoch == 4:\n+    ...       raise RuntimeError('Interrupting!')\n+    >>> callback = keras.callbacks.BackupAndRestore(backup_dir=\"/tmp/backup\")\n+    >>> model = keras.models.Sequential([keras.layers.Dense(10)])\n+    >>> model.compile(keras.optimizers.SGD(), loss='mse')\n+    >>> try:\n+    ...   model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,\n+    ...             batch_size=1, callbacks=[callback, InterruptingCallback()],\n+    ...             verbose=0)\n+    ... except:\n+    ...   pass\n+    >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n+    ...                     epochs=10, batch_size=1, callbacks=[callback],\n+    ...                     verbose=0)\n+    >>> # Only 6 more epochs are run, since first training got interrupted at\n+    >>> # zero-indexed epoch 4, second training will continue from 4 to 9.\n+    >>> len(history.history['loss'])\n+    >>> 6\n+\n+    Args:\n+        file_path: String, path to store the checkpoint.\n+          e.g. `backup_dir = os.path.join(working_dir, \"backup\")`.\n+          This is the directory in which the system stores temporary files to\n+          recover the model from jobs terminated unexpectedly. The directory\n+          cannot be reused elsewhere to store other files, e.g. by the\n+          `BackupAndRestore` callback of another training run,\n+          or by another callback\n+          (e.g. `ModelCheckpoint`) of the same training.\n+        save_freq: `\"epoch\"`, integer, or `False`. When set to `\"epoch\"`\n+          the callback saves the checkpoint at the end of each epoch.\n+          When set to an integer, the callback saves the checkpoint every\n+          `save_freq` batches. Set `save_freq` to `False` if only using\n+          preemption checkpointing (with `save_before_preemption=True`).\n+        delete_checkpoint: Boolean, default to True. This `BackupAndRestore`\n+          callback works by saving a checkpoint to back up the training state.\n+          If `delete_checkpoint=True`, the checkpoint will be deleted after\n+          training is finished. Use `False` if you'd like to keep the checkpoint\n+          for future usage.\n+        save_before_preemption: A boolean value instructing whether to turn on\n+          the automatic checkpoint saving for preemption/maintenance events.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        file_path,\n+        save_freq=\"epoch\",\n+        delete_checkpoint=True,\n+        save_before_preemption=False,\n+    ):\n+        super().__init__()\n+        self._current_epoch = 0\n+        self.save_freq = save_freq\n+        self.delete_checkpoint = delete_checkpoint\n+        self.save_before_preemption = save_before_preemption\n+        self._batches_seen_since_last_saving = 0\n+        self._last_batch_seen = 0\n+\n+        if not file_path:\n+            raise ValueError(\"Empty `backup_dir` argument passed\")\n+        self.file_path = file_path\n+\n+        if not save_freq and not save_before_preemption:\n+            raise ValueError(\n+                \"Either `save_freq` or `save_before_preemption` \" \"must be set.\"\n+            )\n+\n+        if self.save_freq != \"epoch\" and not isinstance(self.save_freq, int):\n+            raise ValueError(\n+                f\"Unrecognized save_freq: {self.save_freq}. \"\n+                \"Expected save_freq are 'epoch' or integer values\"\n+            )\n+        if self.save_before_preemption:\n+            warnings.warn(\"`save_before_preemption` not yet implemented\")\n+\n+    def on_train_begin(self, logs=None):\n+        \"\"\"\n+        Get training state from temporary file and restore it\n+        \"\"\"\n+        if self._check_checkpoints_exists(self.file_path):\n+            self._model.load_weights(filepath=self.file_path)\n+\n+    def on_train_end(self, logs=None):\n+        if self.delete_checkpoint and self._check_checkpoints_exists(\n+            self.file_path\n+        ):\n+            self._cleanup_checkpoint()\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        self._current_epoch = epoch\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        if self.save_freq == \"epoch\":\n+            self._save_model(epoch=epoch, batch=None, logs=logs)\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        if self._should_save_on_batch(batch):\n+            self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\n+\n+    def _save_model(self, epoch, batch, logs):\n+        \"\"\"Saves the model.\n+\n+        Args:\n+            epoch: the epoch this iteration is in.\n+            batch: the batch this iteration is in. `None` if the `save_freq`\n+                is set to `\"epoch\"`.\n+            logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n+        \"\"\"\n+        logs = logs or {}\n+\n+        filepath = self._get_file_path(epoch, batch, logs)\n+        # Create host directory if it doesn't exist.\n+        dirname = os.path.dirname(filepath)\n+        if dirname and not file_utils.exists(dirname):\n+            file_utils.makedirs(dirname)\n+\n+        try:\n+            self._model.save_weights(filepath=filepath, overwrite=True)\n+        except IsADirectoryError:  # h5py 3.x\n+            raise IOError(\n+                \"Please specify a non-directory filepath for \"\n+                \"ModelCheckpoint. Filepath used is an existing \"\n+                f\"directory: {filepath}\"\n+            )\n+        except IOError as e:  # h5py 2.x\n+            # `e.errno` appears to be `None` so checking the content of\n+            # `e.args[0]`.\n+            if \"is a directory\" in str(e.args[0]).lower():\n+                raise IOError(\n+                    \"Please specify a non-directory filepath for \"\n+                    \"ModelCheckpoint. Filepath used is an existing \"\n+                    f\"directory: f{filepath}\"\n+                )\n+            # Re-throw the error for any other causes.\n+            raise e\n+\n+    def _get_file_path(self, epoch, batch, logs):\n+        \"\"\"Returns the file path for checkpoint.\"\"\"\n+\n+        try:\n+            # `filepath` may contain placeholders such as\n+            # `{epoch:02d}`,`{batch:02d}` and `{mape:.2f}`. A mismatch between\n+            # logged metrics and the path's placeholders can cause formatting to\n+            # fail.\n+            if batch is None or \"batch\" in logs:\n+                file_path = self.file_path.format(epoch=epoch + 1, **logs)\n+            else:\n+                file_path = self.file_path.format(\n+                    epoch=epoch + 1, batch=batch + 1, **logs\n+                )\n+        except KeyError as e:\n+            raise KeyError(\n+                f'Failed to format this callback filepath: \"{self.file_path}\". '\n+                f\"Reason: {e}\"\n+            )\n+        return file_path\n+\n+    def _should_save_on_batch(self, batch):\n+        \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n+        if self.save_freq == \"epoch\":\n+            return False\n+        if batch <= self._last_batch_seen:  # New epoch.\n+            add_batches = batch + 1  # batches are zero-indexed.\n+        else:\n+            add_batches = batch - self._last_batch_seen\n+        self._batches_seen_since_last_saving += add_batches\n+        self._last_batch_seen = batch\n+\n+        if self._batches_seen_since_last_saving >= self.save_freq:\n+            self._batches_seen_since_last_saving = 0\n+            return True\n+        return False\n+\n+    def _cleanup_checkpoint(self):\n+        \"\"\"\n+        Delete other checkpoint files (if present) in the directory\n+        \"\"\"\n+        if self._check_checkpoints_exists(filepath=self.file_path):\n+            file_utils.rmtree(self.file_path)\n+\n+    def _check_checkpoints_exists(self, filepath):\n+        return file_utils.exists(filepath)\n\n@@ -0,0 +1,224 @@\n+import os\n+\n+import numpy as np\n+import pytest\n+\n+from keras_core import callbacks\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.models import Sequential\n+from keras_core.utils import file_utils\n+\n+\n+class InterruptingCallback(callbacks.Callback):\n+    \"\"\"A callback to intentionally introduce interruption to\n+    training.\"\"\"\n+\n+    def __init__(self, steps_int, epoch_int):\n+        self.batch_count = 0\n+        self.epoch_count = 0\n+        self.steps_int = steps_int\n+        self.epoch_int = epoch_int\n+\n+    def on_epoch_end(self, epoch, log=None):\n+        self.epoch_count += 1\n+        if self.epoch_int is not None and self.epoch_count == self.epoch_int:\n+            raise RuntimeError(\"EpochInterruption\")\n+\n+    def on_batch_end(self, batch, logs=None):\n+        self.batch_count += 1\n+        if self.steps_int is not None and self.batch_count == self.steps_int:\n+            raise RuntimeError(\"StepsInterruption\")\n+\n+\n+class BackupAndRestoreCallbackTest(testing.TestCase):\n+    # Checking for invalid backup_dir\n+    def test_empty_backup_dir(self):\n+        with self.assertRaisesRegex(\n+            ValueError, expected_regex=\"Empty \" \"`backup_dir`\"\n+        ):\n+            callbacks.BackupAndRestore(file_path=None)\n+\n+    # Checking save_freq and save_before_preemption both unset\n+    def test_save_set_error(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            expected_regex=\"`save_freq` or \"\n+            \"`save_before_preemption` \"\n+            \"\"\n+            \"must be set\",\n+        ):\n+            callbacks.BackupAndRestore(\n+                file_path=\"backup_dir\",\n+                save_freq=None,\n+                save_before_preemption=False,\n+            )\n+\n+    # Check invalid save_freq, both string and non integer\n+    def test_save_freq_unknown_error(self):\n+        with self.assertRaisesRegex(\n+            ValueError, expected_regex=\"Unrecognized save_freq\"\n+        ):\n+            callbacks.BackupAndRestore(\n+                file_path=\"backup_dir\", save_freq=\"batch\"\n+            )\n+\n+        with self.assertRaisesRegex(\n+            ValueError, expected_regex=\"Unrecognized save_freq\"\n+        ):\n+            callbacks.BackupAndRestore(file_path=\"backup_dir\", save_freq=0.15)\n+\n+    # Checking if after interruption, correct model params and\n+    # weights are loaded in step-wise backup\n+    @pytest.mark.requires_trainable_backend\n+    def test_best_case_step(self):\n+        def make_model():\n+            np.random.seed(1337)\n+            model = Sequential(\n+                [\n+                    layers.Dense(2, activation=\"relu\"),\n+                    layers.Dense(1),\n+                ]\n+            )\n+            model.compile(\n+                loss=\"mse\",\n+                optimizer=\"sgd\",\n+                metrics=[\"mse\"],\n+            )\n+            return model\n+\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.weights.h5\")\n+        file_utils.rmtree(filepath)\n+        self.assertFalse(os.path.exists(filepath))\n+\n+        model = make_model()\n+        cbk = callbacks.BackupAndRestore(file_path=filepath, save_freq=1)\n+\n+        x_train = np.random.random((10, 3))\n+        y_train = np.random.random((10, 1))\n+\n+        try:\n+            model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=4,\n+                callbacks=[\n+                    cbk,\n+                    InterruptingCallback(steps_int=2, epoch_int=None),\n+                ],\n+                epochs=2,\n+                verbose=0,\n+            )\n+        except RuntimeError:\n+            self.assertTrue(os.path.exists(filepath))\n+            self.assertEqual(cbk._current_epoch, 0)\n+            self.assertEqual(cbk._last_batch_seen, 1)\n+\n+            hist = model.fit(\n+                x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n+            )\n+\n+            self.assertEqual(cbk._current_epoch, 4)\n+            self.assertEqual(hist.epoch[-1], 4)\n+\n+    # Checking if after interruption, correct model params and\n+    # weights are loaded in epoch-wise backup\n+    @pytest.mark.requires_trainable_backend\n+    def test_best_case_epoch(self):\n+        def make_model():\n+            np.random.seed(1337)\n+            model = Sequential(\n+                [\n+                    layers.Dense(2, activation=\"relu\"),\n+                    layers.Dense(1),\n+                ]\n+            )\n+            model.compile(\n+                loss=\"mse\",\n+                optimizer=\"sgd\",\n+                metrics=[\"mse\"],\n+            )\n+            return model\n+\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.weights.h5\")\n+        file_utils.rmtree(filepath)\n+        self.assertFalse(os.path.exists(filepath))\n+\n+        model = make_model()\n+        cbk = callbacks.BackupAndRestore(file_path=filepath, save_freq=\"epoch\")\n+\n+        x_train = np.random.random((10, 3))\n+        y_train = np.random.random((10, 1))\n+\n+        try:\n+            model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=4,\n+                callbacks=[\n+                    cbk,\n+                    InterruptingCallback(steps_int=None, epoch_int=2),\n+                ],\n+                epochs=6,\n+                verbose=0,\n+            )\n+        except RuntimeError:\n+            self.assertEqual(cbk._current_epoch, 1)\n+            self.assertTrue(os.path.exists(filepath))\n+\n+            hist = model.fit(\n+                x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n+            )\n+            self.assertEqual(cbk._current_epoch, 4)\n+            self.assertEqual(hist.epoch[-1], 4)\n+\n+    # Checking if after interruption, when model is deleted\n+    @pytest.mark.requires_trainable_backend\n+    def test_model_deleted_case_epoch(self):\n+        def make_model():\n+            np.random.seed(1337)\n+            model = Sequential(\n+                [\n+                    layers.Dense(2, activation=\"relu\"),\n+                    layers.Dense(1),\n+                ]\n+            )\n+            model.compile(\n+                loss=\"mse\",\n+                optimizer=\"sgd\",\n+                metrics=[\"mse\"],\n+            )\n+            return model\n+\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.weights.h5\")\n+        file_utils.rmtree(filepath)\n+        self.assertFalse(os.path.exists(filepath))\n+\n+        model = make_model()\n+        cbk = callbacks.BackupAndRestore(file_path=filepath, save_freq=\"epoch\")\n+\n+        x_train = np.random.random((10, 3))\n+        y_train = np.random.random((10, 1))\n+\n+        try:\n+            model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=4,\n+                callbacks=[\n+                    cbk,\n+                    InterruptingCallback(steps_int=None, epoch_int=2),\n+                ],\n+                epochs=6,\n+                verbose=0,\n+            )\n+        except RuntimeError:\n+            self.assertEqual(cbk._current_epoch, 1)\n+            self.assertTrue(os.path.exists(filepath))\n+            file_utils.rmtree(filepath)\n+\n+            model.fit(x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5)\n+            self.assertEqual(cbk._current_epoch, 4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#418a0f30368942074b0f159ebc54949b25ce03b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 213 | Files Changed: 2 | Hunks: 43 | Methods Changed: 24 | Complexity Δ (Sum/Max): -19/0 | Churn Δ: 296 | Churn Cumulative: 736 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.5154639175257731\n\nDIFF:\n@@ -1,6 +1,3 @@\n-import os\n-import warnings\n-\n from keras_core.api_export import keras_core_export\n from keras_core.callbacks.callback import Callback\n from keras_core.utils import file_utils\n@@ -53,85 +50,63 @@ class BackupAndRestore(Callback):\n     >>> 6\n \n     Args:\n-        file_path: String, path to store the checkpoint.\n-          e.g. `backup_dir = os.path.join(working_dir, \"backup\")`.\n-          This is the directory in which the system stores temporary files to\n-          recover the model from jobs terminated unexpectedly. The directory\n+        backup_dir: String, path of directory where to store the data\n+            needed to restore the model. The directory\n             cannot be reused elsewhere to store other files, e.g. by the\n             `BackupAndRestore` callback of another training run,\n-          or by another callback\n-          (e.g. `ModelCheckpoint`) of the same training.\n+            or by another callback (e.g. `ModelCheckpoint`)\n+            of the same training run.\n         save_freq: `\"epoch\"`, integer, or `False`. When set to `\"epoch\"`\n           the callback saves the checkpoint at the end of each epoch.\n           When set to an integer, the callback saves the checkpoint every\n-          `save_freq` batches. Set `save_freq` to `False` if only using\n-          preemption checkpointing (with `save_before_preemption=True`).\n-        delete_checkpoint: Boolean, default to True. This `BackupAndRestore`\n+          `save_freq` batches. Set `save_freq=False` only if using\n+          preemption checkpointing (i.e. with `save_before_preemption=True`).\n+        delete_checkpoint: Boolean, defaults to `True`. This `BackupAndRestore`\n           callback works by saving a checkpoint to back up the training state.\n           If `delete_checkpoint=True`, the checkpoint will be deleted after\n           training is finished. Use `False` if you'd like to keep the checkpoint\n           for future usage.\n-        save_before_preemption: A boolean value instructing whether to turn on\n-          the automatic checkpoint saving for preemption/maintenance events.\n     \"\"\"\n \n     def __init__(\n         self,\n-        file_path,\n+        backup_dir,\n         save_freq=\"epoch\",\n         delete_checkpoint=True,\n-        save_before_preemption=False,\n     ):\n         super().__init__()\n-        self._current_epoch = 0\n         self.save_freq = save_freq\n         self.delete_checkpoint = delete_checkpoint\n-        self.save_before_preemption = save_before_preemption\n         self._batches_seen_since_last_saving = 0\n         self._last_batch_seen = 0\n+        self._current_epoch = 0\n \n-        if not file_path:\n+        if not backup_dir:\n             raise ValueError(\"Empty `backup_dir` argument passed\")\n-        self.file_path = file_path\n-\n-        if not save_freq and not save_before_preemption:\n+        self.backup_dir = backup_dir\n+        self._weights_path = file_utils.join(backup_dir, \"latest.weights.h5\")\n+        if save_freq != \"epoch\" and not isinstance(save_freq, int):\n             raise ValueError(\n-                \"Either `save_freq` or `save_before_preemption` \" \"must be set.\"\n+                \"Invalid value for argument `save_freq`. \"\n+                f\"Received: save_freq={save_freq}. \"\n+                \"Expected either 'epoch' or an integer value.\"\n             )\n \n-        if self.save_freq != \"epoch\" and not isinstance(self.save_freq, int):\n-            raise ValueError(\n-                f\"Unrecognized save_freq: {self.save_freq}. \"\n-                \"Expected save_freq are 'epoch' or integer values\"\n-            )\n-        if self.save_before_preemption:\n-            warnings.warn(\"`save_before_preemption` not yet implemented\")\n-\n     def on_train_begin(self, logs=None):\n-        \"\"\"\n-        Get training state from temporary file and restore it\n-        \"\"\"\n-        if self._check_checkpoints_exists(self.file_path):\n-            self._model.load_weights(filepath=self.file_path)\n-\n-    def on_train_end(self, logs=None):\n-        if self.delete_checkpoint and self._check_checkpoints_exists(\n-            self.file_path\n-        ):\n-            self._cleanup_checkpoint()\n-\n-    def on_epoch_begin(self, epoch, logs=None):\n-        self._current_epoch = epoch\n+        \"\"\"Get training state from temporary file and restore it.\"\"\"\n+        if file_utils.exists(self._weights_path):\n+            self.model.load_weights(self._weights_path)\n \n     def on_epoch_end(self, epoch, logs=None):\n+        self._current_epoch = epoch\n         if self.save_freq == \"epoch\":\n-            self._save_model(epoch=epoch, batch=None, logs=logs)\n+            self._save_model()\n \n     def on_train_batch_end(self, batch, logs=None):\n         if self._should_save_on_batch(batch):\n-            self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\n+            self._save_model()\n \n-    def _save_model(self, epoch, batch, logs):\n+    def _save_model(self):\n         \"\"\"Saves the model.\n \n         Args:\n@@ -140,54 +115,10 @@ class BackupAndRestore(Callback):\n                 is set to `\"epoch\"`.\n             logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n         \"\"\"\n-        logs = logs or {}\n-\n-        filepath = self._get_file_path(epoch, batch, logs)\n         # Create host directory if it doesn't exist.\n-        dirname = os.path.dirname(filepath)\n-        if dirname and not file_utils.exists(dirname):\n-            file_utils.makedirs(dirname)\n-\n-        try:\n-            self._model.save_weights(filepath=filepath, overwrite=True)\n-        except IsADirectoryError:  # h5py 3.x\n-            raise IOError(\n-                \"Please specify a non-directory filepath for \"\n-                \"ModelCheckpoint. Filepath used is an existing \"\n-                f\"directory: {filepath}\"\n-            )\n-        except IOError as e:  # h5py 2.x\n-            # `e.errno` appears to be `None` so checking the content of\n-            # `e.args[0]`.\n-            if \"is a directory\" in str(e.args[0]).lower():\n-                raise IOError(\n-                    \"Please specify a non-directory filepath for \"\n-                    \"ModelCheckpoint. Filepath used is an existing \"\n-                    f\"directory: f{filepath}\"\n-                )\n-            # Re-throw the error for any other causes.\n-            raise e\n-\n-    def _get_file_path(self, epoch, batch, logs):\n-        \"\"\"Returns the file path for checkpoint.\"\"\"\n-\n-        try:\n-            # `filepath` may contain placeholders such as\n-            # `{epoch:02d}`,`{batch:02d}` and `{mape:.2f}`. A mismatch between\n-            # logged metrics and the path's placeholders can cause formatting to\n-            # fail.\n-            if batch is None or \"batch\" in logs:\n-                file_path = self.file_path.format(epoch=epoch + 1, **logs)\n-            else:\n-                file_path = self.file_path.format(\n-                    epoch=epoch + 1, batch=batch + 1, **logs\n-                )\n-        except KeyError as e:\n-            raise KeyError(\n-                f'Failed to format this callback filepath: \"{self.file_path}\". '\n-                f\"Reason: {e}\"\n-            )\n-        return file_path\n+        if not file_utils.exists(self.backup_dir):\n+            file_utils.makedirs(self.backup_dir)\n+        self.model.save_weights(filepath=self._weights_path, overwrite=True)\n \n     def _should_save_on_batch(self, batch):\n         \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n@@ -205,12 +136,6 @@ class BackupAndRestore(Callback):\n             return True\n         return False\n \n-    def _cleanup_checkpoint(self):\n-        \"\"\"\n-        Delete other checkpoint files (if present) in the directory\n-        \"\"\"\n-        if self._check_checkpoints_exists(filepath=self.file_path):\n-            file_utils.rmtree(self.file_path)\n-\n-    def _check_checkpoints_exists(self, filepath):\n-        return file_utils.exists(filepath)\n+    def on_train_end(self, logs=None):\n+        if self.delete_checkpoint and file_utils.exists(self.backup_dir):\n+            file_utils.rmtree(self.backup_dir)\n\n@@ -1,5 +1,3 @@\n-import os\n-\n import numpy as np\n import pytest\n \n@@ -11,8 +9,7 @@ from keras_core.utils import file_utils\n \n \n class InterruptingCallback(callbacks.Callback):\n-    \"\"\"A callback to intentionally introduce interruption to\n-    training.\"\"\"\n+    \"\"\"A callback to intentionally interrupt training.\"\"\"\n \n     def __init__(self, steps_int, epoch_int):\n         self.batch_count = 0\n@@ -31,52 +28,23 @@ class InterruptingCallback(callbacks.Callback):\n             raise RuntimeError(\"StepsInterruption\")\n \n \n+class CanaryLayer(layers.Layer):\n+    def __init__(self):\n+        super().__init__()\n+        self.counter = self.add_weight(\n+            shape=(), initializer=\"zeros\", dtype=\"int32\", trainable=False\n+        )\n+\n+    def call(self, x):\n+        self.counter.assign_add(1)\n+        return x\n+\n+\n class BackupAndRestoreCallbackTest(testing.TestCase):\n-    # Checking for invalid backup_dir\n-    def test_empty_backup_dir(self):\n-        with self.assertRaisesRegex(\n-            ValueError, expected_regex=\"Empty \" \"`backup_dir`\"\n-        ):\n-            callbacks.BackupAndRestore(file_path=None)\n-\n-    # Checking save_freq and save_before_preemption both unset\n-    def test_save_set_error(self):\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            expected_regex=\"`save_freq` or \"\n-            \"`save_before_preemption` \"\n-            \"\"\n-            \"must be set\",\n-        ):\n-            callbacks.BackupAndRestore(\n-                file_path=\"backup_dir\",\n-                save_freq=None,\n-                save_before_preemption=False,\n-            )\n-\n-    # Check invalid save_freq, both string and non integer\n-    def test_save_freq_unknown_error(self):\n-        with self.assertRaisesRegex(\n-            ValueError, expected_regex=\"Unrecognized save_freq\"\n-        ):\n-            callbacks.BackupAndRestore(\n-                file_path=\"backup_dir\", save_freq=\"batch\"\n-            )\n-\n-        with self.assertRaisesRegex(\n-            ValueError, expected_regex=\"Unrecognized save_freq\"\n-        ):\n-            callbacks.BackupAndRestore(file_path=\"backup_dir\", save_freq=0.15)\n-\n-    # Checking if after interruption, correct model params and\n-    # weights are loaded in step-wise backup\n-    @pytest.mark.requires_trainable_backend\n-    def test_best_case_step(self):\n-        def make_model():\n-            np.random.seed(1337)\n+    def make_model(self):\n         model = Sequential(\n             [\n-                    layers.Dense(2, activation=\"relu\"),\n+                CanaryLayer(),\n                 layers.Dense(1),\n             ]\n         )\n@@ -87,13 +55,26 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n         )\n         return model\n \n-        temp_dir = self.get_temp_dir()\n-        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.weights.h5\")\n-        file_utils.rmtree(filepath)\n-        self.assertFalse(os.path.exists(filepath))\n+    # Check invalid save_freq, both string and non integer\n+    def test_save_freq_unknown_error(self):\n+        with self.assertRaisesRegex(ValueError, expected_regex=\"Invalid value\"):\n+            callbacks.BackupAndRestore(\n+                backup_dir=\"backup_dir\", save_freq=\"batch\"\n+            )\n \n-        model = make_model()\n-        cbk = callbacks.BackupAndRestore(file_path=filepath, save_freq=1)\n+        with self.assertRaisesRegex(ValueError, expected_regex=\"Invalid value\"):\n+            callbacks.BackupAndRestore(backup_dir=\"backup_dir\", save_freq=0.15)\n+\n+    # Checking if after interruption, correct model params and\n+    # weights are loaded in step-wise backup\n+    @pytest.mark.requires_trainable_backend\n+    def test_best_case_step(self):\n+        temp_dir = self.get_temp_dir()\n+        backup_dir = file_utils.join(temp_dir, \"subdir\")\n+        self.assertFalse(file_utils.exists(backup_dir))\n+\n+        model = self.make_model()\n+        cbk = callbacks.BackupAndRestore(backup_dir, save_freq=1)\n \n         x_train = np.random.random((10, 3))\n         y_train = np.random.random((10, 1))\n@@ -111,9 +92,10 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n                 verbose=0,\n             )\n         except RuntimeError:\n-            self.assertTrue(os.path.exists(filepath))\n+            self.assertTrue(file_utils.exists(backup_dir))\n             self.assertEqual(cbk._current_epoch, 0)\n             self.assertEqual(cbk._last_batch_seen, 1)\n+            self.assertEqual(int(model.layers[0].counter.value), 2)\n \n             hist = model.fit(\n                 x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n@@ -121,33 +103,21 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n \n             self.assertEqual(cbk._current_epoch, 4)\n             self.assertEqual(hist.epoch[-1], 4)\n+            self.assertEqual(int(model.layers[0].counter.value), 17)\n \n     # Checking if after interruption, correct model params and\n     # weights are loaded in epoch-wise backup\n     @pytest.mark.requires_trainable_backend\n     def test_best_case_epoch(self):\n-        def make_model():\n-            np.random.seed(1337)\n-            model = Sequential(\n-                [\n-                    layers.Dense(2, activation=\"relu\"),\n-                    layers.Dense(1),\n-                ]\n-            )\n-            model.compile(\n-                loss=\"mse\",\n-                optimizer=\"sgd\",\n-                metrics=[\"mse\"],\n-            )\n-            return model\n-\n         temp_dir = self.get_temp_dir()\n-        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.weights.h5\")\n-        file_utils.rmtree(filepath)\n-        self.assertFalse(os.path.exists(filepath))\n+        backup_dir = file_utils.join(temp_dir, \"subdir\")\n+        self.assertFalse(file_utils.exists(backup_dir))\n \n-        model = make_model()\n-        cbk = callbacks.BackupAndRestore(file_path=filepath, save_freq=\"epoch\")\n+        model = self.make_model()\n+        self.assertEqual(int(model.layers[0].counter.value), 0)\n+        cbk = callbacks.BackupAndRestore(\n+            backup_dir=backup_dir, save_freq=\"epoch\"\n+        )\n \n         x_train = np.random.random((10, 3))\n         y_train = np.random.random((10, 1))\n@@ -166,59 +136,34 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n             )\n         except RuntimeError:\n             self.assertEqual(cbk._current_epoch, 1)\n-            self.assertTrue(os.path.exists(filepath))\n+            self.assertTrue(file_utils.exists(backup_dir))\n+            self.assertEqual(int(model.layers[0].counter.value), 6)\n \n             hist = model.fit(\n                 x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n             )\n             self.assertEqual(cbk._current_epoch, 4)\n             self.assertEqual(hist.epoch[-1], 4)\n+            self.assertEqual(int(model.layers[0].counter.value), 21)\n \n     # Checking if after interruption, when model is deleted\n     @pytest.mark.requires_trainable_backend\n     def test_model_deleted_case_epoch(self):\n-        def make_model():\n-            np.random.seed(1337)\n-            model = Sequential(\n-                [\n-                    layers.Dense(2, activation=\"relu\"),\n-                    layers.Dense(1),\n-                ]\n-            )\n-            model.compile(\n-                loss=\"mse\",\n-                optimizer=\"sgd\",\n-                metrics=[\"mse\"],\n-            )\n-            return model\n-\n         temp_dir = self.get_temp_dir()\n-        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.weights.h5\")\n-        file_utils.rmtree(filepath)\n-        self.assertFalse(os.path.exists(filepath))\n+        backup_dir = file_utils.join(temp_dir, \"subdir\")\n+        self.assertFalse(file_utils.exists(backup_dir))\n \n-        model = make_model()\n-        cbk = callbacks.BackupAndRestore(file_path=filepath, save_freq=\"epoch\")\n+        model = self.make_model()\n+        cbk = callbacks.BackupAndRestore(backup_dir, save_freq=\"epoch\")\n \n         x_train = np.random.random((10, 3))\n         y_train = np.random.random((10, 1))\n-\n-        try:\n         model.fit(\n             x_train,\n             y_train,\n             batch_size=4,\n-                callbacks=[\n-                    cbk,\n-                    InterruptingCallback(steps_int=None, epoch_int=2),\n-                ],\n-                epochs=6,\n+            callbacks=[cbk],\n+            epochs=2,\n             verbose=0,\n         )\n-        except RuntimeError:\n-            self.assertEqual(cbk._current_epoch, 1)\n-            self.assertTrue(os.path.exists(filepath))\n-            file_utils.rmtree(filepath)\n-\n-            model.fit(x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5)\n-            self.assertEqual(cbk._current_epoch, 4)\n+        self.assertFalse(file_utils.exists(backup_dir))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
