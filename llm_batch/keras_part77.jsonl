{"custom_id": "keras#f3f19146f9c248b9d600c5767d1ad60dc9aa20da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 19 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 14260 | Contributors (this commit): 60 | Commits (past 90d): 130 | Contributors (cumulative): 79 | DMM Complexity: 0.125\n\nDIFF:\n@@ -101,7 +101,6 @@ class Convolution1D(Layer):\n         self.input_length = input_length\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        self.input = K.placeholder(ndim=3)\n         super(Convolution1D, self).__init__(**kwargs)\n \n     def build(self):\n@@ -251,7 +250,6 @@ class Convolution2D(Layer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n-        self.input = K.placeholder(ndim=4)\n         super(Convolution2D, self).__init__(**kwargs)\n \n     def build(self):\n@@ -431,7 +429,6 @@ class Convolution3D(Layer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n-        self.input = K.placeholder(ndim=5)\n         super(Convolution3D, self).__init__(**kwargs)\n \n     def build(self):\n@@ -546,7 +543,6 @@ class _Pooling1D(Layer):\n         self.pool_length = pool_length\n         self.stride = stride\n         self.st = (self.stride, 1)\n-        self.input = K.placeholder(ndim=3)\n         self.pool_size = (pool_length, 1)\n         assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n@@ -646,7 +642,6 @@ class _Pooling2D(Layer):\n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n                  dim_ordering='th', **kwargs):\n         super(_Pooling2D, self).__init__(**kwargs)\n-        self.input = K.placeholder(ndim=4)\n         self.pool_size = tuple(pool_size)\n         if strides is None:\n             strides = self.pool_size\n@@ -786,7 +781,6 @@ class _Pooling3D(Layer):\n     def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n                  dim_ordering='th', **kwargs):\n         super(_Pooling3D, self).__init__(**kwargs)\n-        self.input = K.placeholder(ndim=5)\n         self.pool_size = tuple(pool_size)\n         if strides is None:\n             strides = self.pool_size\n@@ -947,7 +941,6 @@ class UpSampling1D(Layer):\n     def __init__(self, length=2, **kwargs):\n         super(UpSampling1D, self).__init__(**kwargs)\n         self.length = length\n-        self.input = K.placeholder(ndim=3)\n \n     @property\n     def output_shape(self):\n@@ -992,7 +985,6 @@ class UpSampling2D(Layer):\n \n     def __init__(self, size=(2, 2), dim_ordering='th', **kwargs):\n         super(UpSampling2D, self).__init__(**kwargs)\n-        self.input = K.placeholder(ndim=4)\n         self.size = tuple(size)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n@@ -1056,7 +1048,6 @@ class UpSampling3D(Layer):\n             raise Exception(self.__class__.__name__ +\n                             ' is currently only working with Theano backend.')\n         super(UpSampling3D, self).__init__(**kwargs)\n-        self.input = K.placeholder(ndim=5)\n         self.size = tuple(size)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n@@ -1110,7 +1101,6 @@ class ZeroPadding1D(Layer):\n     def __init__(self, padding=1, **kwargs):\n         super(ZeroPadding1D, self).__init__(**kwargs)\n         self.padding = padding\n-        self.input = K.placeholder(ndim=3)\n \n     @property\n     def output_shape(self):\n@@ -1151,7 +1141,6 @@ class ZeroPadding2D(Layer):\n     def __init__(self, padding=(1, 1), dim_ordering='th', **kwargs):\n         super(ZeroPadding2D, self).__init__(**kwargs)\n         self.padding = tuple(padding)\n-        self.input = K.placeholder(ndim=4)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n \n@@ -1209,7 +1198,6 @@ class ZeroPadding3D(Layer):\n                             ' is currently only working with Theano backend.')\n         super(ZeroPadding3D, self).__init__(**kwargs)\n         self.padding = tuple(padding)\n-        self.input = K.placeholder(ndim=5)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n \n\n@@ -388,8 +388,6 @@ class Masking(MaskedLayer):\n     def __init__(self, mask_value=0., **kwargs):\n         super(Masking, self).__init__(**kwargs)\n         self.mask_value = mask_value\n-        if (not hasattr(self, 'input')):\n-            self.input = K.placeholder(ndim=3)\n \n     def get_output_mask(self, train=False):\n         X = self.get_input(train)\n@@ -1000,7 +998,6 @@ class Dense(Layer):\n         self.input_dim = input_dim\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_dim,)\n-        self.input = K.placeholder(ndim=2)\n         super(Dense, self).__init__(**kwargs)\n \n     def build(self):\n@@ -1115,7 +1112,6 @@ class TimeDistributedDense(MaskedLayer):\n         self.input_length = input_length\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        self.input = K.placeholder(ndim=3)\n         super(TimeDistributedDense, self).__init__(**kwargs)\n \n     def build(self):\n@@ -1421,7 +1417,6 @@ class MaxoutDense(Layer):\n         self.input_dim = input_dim\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_dim,)\n-        self.input = K.placeholder(ndim=2)\n         super(MaxoutDense, self).__init__(**kwargs)\n \n     def build(self):\n@@ -1989,7 +1984,6 @@ class Highway(Layer):\n         self.input_dim = input_dim\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_dim,)\n-        self.input = K.placeholder(ndim=2)\n         super(Highway, self).__init__(**kwargs)\n \n     def build(self):\n\n@@ -383,7 +383,6 @@ class GRU(Recurrent):\n         input_shape = self.input_shape\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n-        self.input = K.placeholder(input_shape)\n \n         self.W_z = self.init((input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n@@ -550,7 +549,6 @@ class LSTM(Recurrent):\n         input_shape = self.input_shape\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n-        self.input = K.placeholder(input_shape)\n \n         if self.stateful:\n             self.reset_states()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80927fa9589e587437c4e590cd782aeca538c991", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1313 | Contributors (this commit): 14 | Commits (past 90d): 43 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -788,12 +788,12 @@ def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n         x = x.dimshuffle((0, 3, 1, 2))\n \n     if pool_mode == 'max':\n-        pool_out = pool.max_pool_2d(x, ds=pool_size, st=strides,\n+        pool_out = pool.pool_2d(x, ds=pool_size, st=strides,\n                                 ignore_border=True,\n                                 padding=padding,\n                                 mode='max')\n     elif pool_mode == 'avg':\n-        pool_out = pool.max_pool_2d(x, ds=pool_size, st=strides,\n+        pool_out = pool.pool_2d(x, ds=pool_size, st=strides,\n                                 ignore_border=True,\n                                 padding=padding,\n                                 mode='average_exc_pad')\n@@ -832,7 +832,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n \n     if pool_mode == 'max':\n         # pooling over conv_dim2, conv_dim1 (last two channels)\n-        output = pool.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n+        output = pool.pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n                               ds=(pool_size[1], pool_size[0]),\n                               st=(strides[1], strides[0]),\n                               ignore_border=ignore_border,\n@@ -840,7 +840,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n                               mode='max')\n \n         # pooling over conv_dim3\n-        pool_out = pool.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n+        pool_out = pool.pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n                                 ds=(1, pool_size[2]),\n                                 st=(1, strides[2]),\n                                 ignore_border=ignore_border,\n@@ -849,7 +849,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n \n     elif pool_mode == 'avg':\n         # pooling over conv_dim2, conv_dim1 (last two channels)\n-        output = pool.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n+        output = pool.pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n                               ds=(pool_size[1], pool_size[0]),\n                               st=(strides[1], strides[0]),\n                               ignore_border=ignore_border,\n@@ -857,7 +857,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n                               mode='average_exc_pad')\n \n         # pooling over conv_dim3\n-        pool_out = pool.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n+        pool_out = pool.pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n                                 ds=(1, pool_size[2]),\n                                 st=(1, strides[2]),\n                                 ignore_border=ignore_border,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4361d2246f703524932830c40e4c0a8ecb6b95b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 23 | Churn Cumulative: 9494 | Contributors (this commit): 47 | Commits (past 90d): 125 | Contributors (cumulative): 56 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,6 +43,11 @@ def shape(x):\n     return x.get_shape()\n \n \n+def int_shape(x):\n+    shape = x.get_shape()\n+    return tuple([i.__int__() for i in shape])\n+\n+\n def ndim(x):\n     return len(x.get_shape())\n \n\n@@ -241,8 +241,8 @@ class Layer(object):\n         elif hasattr(self, 'input'):\n             return self.input\n         else:\n-            raise Exception('Layer is not connected' +\n-                            ' and is not an input layer.')\n+            self.input = K.placeholder(shape=self.input_shape)\n+            return self.input\n \n     def supports_masked_input(self):\n         '''Whether or not this layer respects the output mask of its previous\n@@ -1510,6 +1510,12 @@ class Lambda(Layer):\n     @property\n     def output_shape(self):\n         if self._output_shape is None:\n+            # if TensorFlow, we can infer the output shape directly:\n+            if K._BACKEND == 'tensorflow':\n+                # we assume output shape is not dependent on train/test mode\n+                x = self.get_input()\n+                return K.int_shape(x)\n+            # otherwise, we default to the input shape\n             return self.input_shape\n         elif type(self._output_shape) == tuple:\n             nb_samples = self.input_shape[0] if self.input_shape else None\n\n@@ -175,7 +175,7 @@ def test_naming():\n def test_sequences():\n     '''Test masking sequences with zeroes as padding'''\n     # integer inputs, one per timestep, like embeddings\n-    layer = core.Masking()\n+    layer = core.Masking(input_shape=(4, 1))\n     func = K.function([layer.get_input(True)], [layer.get_output_mask()])\n     input_data = np.array([[[1], [2], [3], [0]],\n                            [[0], [4], [5], [0]]], dtype=np.int32)\n@@ -190,7 +190,7 @@ def test_sequences():\n \n def test_non_zero():\n     '''Test masking with non-zero mask value'''\n-    layer = core.Masking(5)\n+    layer = core.Masking(5, input_shape=(4, 2))\n     func = K.function([layer.input], [layer.get_output_mask()])\n     input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n                            [[1, 5], [5, 0], [0, 0], [0, 0]]],\n@@ -202,7 +202,7 @@ def test_non_zero():\n \n def test_non_zero_output():\n     '''Test output of masking layer with non-zero mask value'''\n-    layer = core.Masking(5)\n+    layer = core.Masking(5, input_shape=(4, 2))\n     func = K.function([layer.input], [layer.get_output()])\n \n     input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n@@ -228,6 +228,7 @@ def _runner(layer):\n     layer.trainable = True\n     layer.trainable = False\n \n+\n def test_siamese_all():\n     right_input_layer = core.Dense(7, input_dim=3)\n     left_input_layer = core.Dense(7, input_dim=3)\n@@ -238,6 +239,7 @@ def test_siamese_all():\n         siamese_layer.output_shape\n         siamese_layer.get_output()\n \n+\n @pytest.mark.skipif(K._BACKEND == 'tensorflow',\n                     reason='currently not working with TensorFlow')\n def test_siamese_theano_only():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#943d2d4cf802dd5aceeac376c8468d8ee430817f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 66 | Files Changed: 6 | Hunks: 29 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 179 | Churn Cumulative: 15912 | Contributors (this commit): 64 | Commits (past 90d): 166 | Contributors (cumulative): 106 | DMM Complexity: 0.5813953488372093\n\nDIFF:\n@@ -59,7 +59,8 @@ class PReLU(MaskedLayer):\n \n     def build(self):\n         input_shape = self.input_shape[1:]\n-        self.alphas = self.init(input_shape)\n+        self.alphas = self.init(input_shape,\n+                                name='{}_alphas'.format(self.name))\n         self.trainable_weights = [self.alphas]\n \n         if self.initial_weights is not None:\n@@ -140,8 +141,10 @@ class ParametricSoftplus(MaskedLayer):\n \n     def build(self):\n         input_shape = self.input_shape[1:]\n-        self.alphas = K.variable(self.alpha_init * np.ones(input_shape))\n-        self.betas = K.variable(self.beta_init * np.ones(input_shape))\n+        self.alphas = K.variable(self.alpha_init * np.ones(input_shape),\n+                                 name='{}_alphas'.format(self.name))\n+        self.betas = K.variable(self.beta_init * np.ones(input_shape),\n+                                name='{}_betas'.format(self.name))\n         self.trainable_weights = [self.alphas, self.betas]\n \n         if self.initial_weights is not None:\n@@ -246,25 +249,32 @@ class SReLU(MaskedLayer):\n     '''\n     def __init__(self, t_left_init='zero', a_left_init='glorot_uniform',\n                  t_right_init='glorot_uniform', a_right_init='one', **kwargs):\n-        self.t_left_init = t_left_init\n-        self.a_left_init = a_left_init\n-        self.t_right_init = t_right_init\n-        self.a_right_init = a_right_init\n+        self.t_left_init = initializations.get(t_left_init)\n+        self.a_left_init = initializations.get(a_left_init)\n+        self.t_right_init = initializations.get(t_right_init)\n+        self.a_right_init = initializations.get(a_right_init)\n         super(SReLU, self).__init__(**kwargs)\n \n     def build(self):\n-        params_shape = self.input_shape[1:]\n-        self.t_left = initializations.get(self.t_left_init)(params_shape)\n-        self.a_left = initializations.get(self.a_left_init)(params_shape)\n-        self.t_right = initializations.get(self.t_right_init)(params_shape)\n-        self.a_right = initializations.get(self.a_right_init)(params_shape)\n+        input_shape = self.input_shape[1:]\n+        self.t_left = self.t_left_init(input_shape,\n+                                       name='{}_t_left'.format(self.name))\n+        self.a_left = self.a_left_init(input_shape,\n+                                       name='{}_a_left'.format(self.name))\n+        self.t_right = self.t_right_init(input_shape,\n+                                         name='{}_t_right'.format(self.name))\n+        self.a_right = self.a_right_init(input_shape,\n+                                         name='{}_a_right'.format(self.name))\n         # ensure the the right part is always to the right of the left\n         self.t_right_actual = self.t_left + abs(self.t_right)\n-        self.trainable_weights = [self.t_left, self.a_left, self.t_right, self.a_right]\n+        self.trainable_weights = [self.t_left, self.a_left,\n+                                  self.t_right, self.a_right]\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        Y_left_and_center = self.t_left + K.relu(X - self.t_left, self.a_left, self.t_right_actual - self.t_left)\n+        Y_left_and_center = self.t_left + K.relu(X - self.t_left,\n+                                                 self.a_left,\n+                                                 self.t_right_actual - self.t_left)\n         Y_right = K.relu(X - self.t_right_actual) * self.a_right\n         return Y_left_and_center + Y_right\n \n\n@@ -106,8 +106,8 @@ class Convolution1D(Layer):\n     def build(self):\n         input_dim = self.input_shape[2]\n         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)\n-        self.W = self.init(self.W_shape)\n-        self.b = K.zeros((self.nb_filter,))\n+        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n         self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n@@ -261,8 +261,8 @@ class Convolution2D(Layer):\n             self.W_shape = (self.nb_row, self.nb_col, stack_size, self.nb_filter)\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n-        self.W = self.init(self.W_shape)\n-        self.b = K.zeros((self.nb_filter,))\n+        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n         self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n@@ -444,8 +444,8 @@ class Convolution3D(Layer):\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n-        self.W = self.init(self.W_shape)\n-        self.b = K.zeros((self.nb_filter,))\n+        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n         self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n\n@@ -45,19 +45,26 @@ class Layer(object):\n                           'name'}\n         for kwarg in kwargs:\n             assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n+\n+        if 'name' in kwargs:\n+            self.name = kwargs['name']\n+        else:\n+            self.name = self.__class__.__name__.lower()\n+\n+        if 'cache_enabled' in kwargs:\n+            self.cache_enabled = kwargs['cache_enabled']\n+        else:\n+            self.cache_enabled = True\n+\n         if 'batch_input_shape' in kwargs:\n             self.set_input_shape(tuple(kwargs['batch_input_shape']))\n         elif 'input_shape' in kwargs:\n             self.set_input_shape((None,) + tuple(kwargs['input_shape']))\n-        self.trainable = True\n+\n         if 'trainable' in kwargs:\n             self.trainable = kwargs['trainable']\n-        self.name = self.__class__.__name__.lower()\n-        if 'name' in kwargs:\n-            self.name = kwargs['name']\n-        self.cache_enabled = True\n-        if 'cache_enabled' in kwargs:\n-            self.cache_enabled = kwargs['cache_enabled']\n+        else:\n+            self.trainable = True\n \n     @property\n     def name(self):\n@@ -1003,8 +1010,10 @@ class Dense(Layer):\n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.W = self.init((input_dim, self.output_dim))\n-        self.b = K.zeros((self.output_dim,))\n+        self.W = self.init((input_dim, self.output_dim),\n+                           name='{}_W'.format(self.name))\n+        self.b = K.zeros((self.output_dim,),\n+                         name='{}_b'.format(self.name))\n \n         self.trainable_weights = [self.W, self.b]\n \n@@ -1117,8 +1126,10 @@ class TimeDistributedDense(MaskedLayer):\n     def build(self):\n         input_dim = self.input_shape[2]\n \n-        self.W = self.init((input_dim, self.output_dim))\n-        self.b = K.zeros((self.output_dim,))\n+        self.W = self.init((input_dim, self.output_dim),\n+                           name='{}_W'.format(self.name))\n+        self.b = K.zeros((self.output_dim,),\n+                         name='{}_b'.format(self.name))\n \n         self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n@@ -1422,8 +1433,10 @@ class MaxoutDense(Layer):\n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.W = self.init((self.nb_feature, input_dim, self.output_dim))\n-        self.b = K.zeros((self.nb_feature, self.output_dim))\n+        self.W = self.init((self.nb_feature, input_dim, self.output_dim),\n+                           name='{}_W'.format(self.name))\n+        self.b = K.zeros((self.nb_feature, self.output_dim),\n+                         name='{}_b'.format(self.name))\n \n         self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n@@ -1995,12 +2008,15 @@ class Highway(Layer):\n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.W = self.init((input_dim, input_dim))\n-        self.W_carry = self.init((input_dim, input_dim))\n+        self.W = self.init((input_dim, input_dim),\n+                           name='{}_W'.format(self.name))\n+        self.W_carry = self.init((input_dim, input_dim),\n+                                 name='{}_W_carry'.format(self.name))\n \n-        self.b = K.zeros((input_dim,))\n+        self.b = K.zeros((input_dim,), name='{}_b'.format(self.name))\n         # initialize with a vector of values `transform_bias`\n-        self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias)\n+        self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias,\n+                                  name='{}_b_carry'.format(self.name))\n \n         self.trainable_weights = [self.W, self.b, self.W_carry, self.b_carry]\n \n\n@@ -73,7 +73,8 @@ class Embedding(Layer):\n     def build(self):\n         self.input = K.placeholder(shape=(self.input_shape[0], self.input_length),\n                                    dtype='int32')\n-        self.W = self.init((self.input_dim, self.output_dim))\n+        self.W = self.init((self.input_dim, self.output_dim),\n+                           name='{}_W'.format(self.name))\n         self.trainable_weights = [self.W]\n         self.regularizers = []\n         if self.W_regularizer:\n\n@@ -55,12 +55,15 @@ class BatchNormalization(Layer):\n         input_shape = self.input_shape  # starts with samples axis\n         shape = (input_shape[self.axis],)\n \n-        self.gamma = self.init(shape)\n-        self.beta = K.zeros(shape)\n+        self.gamma = self.init(shape,\n+                               name='{}_gamma'.format(self.name))\n+        self.beta = K.zeros(shape, name='{}_beta'.format(self.name))\n         self.trainable_weights = [self.gamma, self.beta]\n \n-        self.running_mean = K.zeros(shape)\n-        self.running_std = K.ones(shape)\n+        self.running_mean = K.zeros(shape,\n+                                    name='{}_running_mean'.format(self.name))\n+        self.running_std = K.ones(shape,\n+                                  name='{}_running_std'.format(self.name))\n         self.non_trainable_weights = [self.running_mean, self.running_std]\n \n         if self.initial_weights is not None:\n\n@@ -261,9 +261,11 @@ class SimpleRNN(Recurrent):\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n \n-        self.W = self.init((input_dim, self.output_dim))\n-        self.U = self.inner_init((self.output_dim, self.output_dim))\n-        self.b = K.zeros((self.output_dim,))\n+        self.W = self.init((input_dim, self.output_dim),\n+                           name='{}_W'.format(self.name))\n+        self.U = self.inner_init((self.output_dim, self.output_dim),\n+                                 name='{}_U'.format(self.name))\n+        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n \n         def append_regulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n@@ -384,17 +386,23 @@ class GRU(Recurrent):\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n \n-        self.W_z = self.init((input_dim, self.output_dim))\n-        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_z = K.zeros((self.output_dim,))\n+        self.W_z = self.init((input_dim, self.output_dim),\n+                             name='{}_W_z'.format(self.name))\n+        self.U_z = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_z'.format(self.name))\n+        self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n \n-        self.W_r = self.init((input_dim, self.output_dim))\n-        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_r = K.zeros((self.output_dim,))\n+        self.W_r = self.init((input_dim, self.output_dim),\n+                             name='{}_W_r'.format(self.name))\n+        self.U_r = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_r'.format(self.name))\n+        self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))\n \n-        self.W_h = self.init((input_dim, self.output_dim))\n-        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_h = K.zeros((self.output_dim,))\n+        self.W_h = self.init((input_dim, self.output_dim),\n+                             name='{}_W_h'.format(self.name))\n+        self.U_h = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_h'.format(self.name))\n+        self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n \n         def append_regulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n@@ -556,21 +564,30 @@ class LSTM(Recurrent):\n             # initial states: 2 all-zero tensors of shape (output_dim)\n             self.states = [None, None]\n \n-        self.W_i = self.init((input_dim, self.output_dim))\n-        self.U_i = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_i = K.zeros((self.output_dim,))\n+        self.W_i = self.init((input_dim, self.output_dim),\n+                             name='{}_W_i'.format(self.name))\n+        self.U_i = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_i'.format(self.name))\n+        self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n \n-        self.W_f = self.init((input_dim, self.output_dim))\n-        self.U_f = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_f = self.forget_bias_init((self.output_dim,))\n+        self.W_f = self.init((input_dim, self.output_dim),\n+                             name='{}_W_f'.format(self.name))\n+        self.U_f = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_f'.format(self.name))\n+        self.b_f = self.forget_bias_init((self.output_dim,),\n+                                         name='{}_b_f'.format(self.name))\n \n-        self.W_c = self.init((input_dim, self.output_dim))\n-        self.U_c = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_c = K.zeros((self.output_dim,))\n+        self.W_c = self.init((input_dim, self.output_dim),\n+                             name='{}_W_c'.format(self.name))\n+        self.U_c = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_c'.format(self.name))\n+        self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n \n-        self.W_o = self.init((input_dim, self.output_dim))\n-        self.U_o = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_o = K.zeros((self.output_dim,))\n+        self.W_o = self.init((input_dim, self.output_dim),\n+                             name='{}_W_o'.format(self.name))\n+        self.U_o = self.inner_init((self.output_dim, self.output_dim),\n+                                   name='{}_U_o'.format(self.name))\n+        self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n \n         def append_regulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be24159959672c32abb31697e721d96ae6ffaf97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 217 | Lines Deleted: 188 | Files Changed: 2 | Hunks: 61 | Methods Changed: 20 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 405 | Churn Cumulative: 948 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,266 +1,243 @@\n from __future__ import absolute_import\n-import abc\n import copy\n+import inspect\n+import types\n import numpy as np\n \n from ..utils.np_utils import to_categorical\n+from ..models import Sequential\n \n \n class BaseWrapper(object):\n-    \"\"\"\n-    Base class for the Keras scikit-learn wrapper.\n \n-    Warning: This class should not be used directly. Use derived classes instead.\n+    '''Base class for the Keras scikit-learn wrapper.\n \n-    Parameters\n-    ----------\n-    train_batch_size : int, optional\n-        Number of training samples evaluated at a time.\n-    test_batch_size : int, optional\n-        Number of test samples evaluated at a time.\n-    nb_epochs : int, optional\n-        Number of training epochs.\n-    shuffle : boolean, optional\n-        Whether to shuffle the samples at each epoch.\n-    show_accuracy : boolean, optional\n-        Whether to display class accuracy in the logs at each epoch.\n-    validation_split : float [0, 1], optional\n-        Fraction of the data to use as held-out validation data.\n-    validation_data : tuple (X, y), optional\n-        Data to be used as held-out validation data. Will override validation_split.\n-    callbacks : list, optional\n-        List of callbacks to apply during training.\n-    verbose : int, optional\n-        Verbosity level.\n-    \"\"\"\n-    __metaclass__ = abc.ABCMeta\n+    Warning: This class should not be used directly.\n+    Use derived classes instead.\n \n-    @abc.abstractmethod\n-    def __init__(self, model, optimizer, loss,\n-                 train_batch_size=128, test_batch_size=128,\n-                 nb_epoch=100, shuffle=True, show_accuracy=False,\n-                 validation_split=0, validation_data=None, callbacks=None,\n-                 verbose=0,):\n-        self.model = model\n-        self.optimizer = optimizer\n-        self.loss = loss\n-        self.compiled_model_ = None\n-        self.classes_ = []\n-        self.config_ = []\n-        self.weights_ = []\n+    # Arguments\n+        build_fn: callable function or class instance,  optional\n+            Implementing the logic of the model.\n+        sk_params: model parameters & fitting parameters\n \n-        self.train_batch_size = train_batch_size\n-        self.test_batch_size = test_batch_size\n-        self.nb_epoch = nb_epoch\n-        self.shuffle = shuffle\n-        self.show_accuracy = show_accuracy\n-        self.validation_split = validation_split\n-        self.validation_data = validation_data\n-        self.callbacks = [] if callbacks is None else callbacks\n+    The build_fn should construct, compile and return a Keras model, which\n+    will then be used to fit data or predict unknow data. One of the following\n+    three values could be passed to build_fn:\n+    1. A function instance\n+    2. An instance of a class that implements the __call__ function\n+    3. None. This means you implement a class that inherits either\n+    KerasClassifier or KerasRegressor. The __call__ function of the class will\n+    then be treated as the default build_fn\n \n-        self.verbose = verbose\n+    'sk_params' takes both model parameters and fitting parameters. Legal model\n+    parameters are the arguments of 'build_fn'. Note that like all other\n+    estimators in scikit_learn, 'build_fn' should provide defalult velues for\n+    its arguments, so that you could create the estimator without passing any\n+    values to 'sk_params'.\n+\n+    'sk_params' could also accept parameters for calling 'fit', 'predict',\n+    'predict_proba', and 'score' function (e.g., nb_epoch, batch_size).\n+    fitting (predicting) parameters are adopts in the following order:\n+\n+    1. Values passed to the dictionary arguments of\n+    'fit', 'predict', 'predict_proba', and 'score' functions\n+    2. Values passed to 'sk_params'\n+    3. The default values of the keras.models.Sequential's\n+    'fit', 'predict', 'predict_proba' and 'score' functions\n+\n+    When using scikit_learn's grid_search api, legal tunable parameters are\n+    those you could pass to 'sk_params', including fitting parameters.\n+    In other words, you could use grid_search to search for the best\n+    batch_size or nb_epoch as well as the model parameters.\n+    '''\n+\n+    def __init__(self, build_fn=None, **sk_params):\n+        self.build_fn = build_fn\n+        self.sk_params = sk_params\n \n     def get_params(self, deep=True):\n-        \"\"\"\n-        Get parameters for this estimator.\n+        '''Get parameters for this estimator.\n \n-        Parameters\n-        ----------\n+        # Arguments\n             deep: boolean, optional\n                 If True, will return the parameters for this estimator and\n                 contained subobjects that are estimators.\n \n-        Returns\n-        -------\n+        # Returns\n             params : dict\n                 Dictionary of parameter names mapped to their values.\n-        \"\"\"\n-        return {'model': self.model, 'optimizer': self.optimizer, 'loss': self.loss}\n+        '''\n+        res = copy.deepcopy(self.sk_params)\n+        res.update({'build_fn': self.build_fn})\n+        return res\n \n     def set_params(self, **params):\n-        \"\"\"\n-        Set the parameters of this estimator.\n+        '''Set the parameters of this estimator.\n \n-        Parameters\n-        ----------\n+        # Arguments\n         params: dict\n             Dictionary of parameter names mapped to their values.\n \n-        Returns\n-        -------\n+        # Returns\n             self\n-        \"\"\"\n-        for parameter, value in params.items():\n-            setattr(self, parameter, value)\n+        '''\n+        self.sk_params.update(params)\n         return self\n \n-    def fit(self, X, y):\n-        \"\"\"\n-        Fit the model according to the given training data.\n+    def fit(self, X, y, **kwargs):\n+        '''Construct a new model with build_fn and fit the model according\n+        to the given training data.\n \n-        Makes a copy of the un-compiled model definition to use for\n-        compilation and fitting, leaving the original definition\n-        intact.\n-\n-        Parameters\n-        ----------\n+        # Arguments\n             X : array-like, shape = (n_samples, n_features)\n                 Training samples where n_samples in the number of samples\n                 and n_features is the number of features.\n             y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n                 True labels for X.\n+            kwargs: dictionary arguments\n+                Legal arguments are the arguments of Sequential.fit\n \n-        Returns\n-        -------\n+        # Returns\n             history : object\n-            Returns details about the training history at each epoch.\n-        \"\"\"\n-        if len(y.shape) == 1:\n-            self.classes_ = list(np.unique(y))\n-            if self.loss == 'categorical_crossentropy':\n-                y = to_categorical(y)\n+                details about the training history at each epoch.\n+        '''\n+\n+        if self.build_fn is None:\n+            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n+        elif not isinstance(self.build_fn, types.FunctionType):\n+            self.model = self.build_fn(\n+                **self.filter_sk_params(self.build_fn.__call__))\n         else:\n-            self.classes_ = np.arange(0, y.shape[1])\n+            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n \n-        self.compiled_model_ = copy.deepcopy(self.model)\n-        self.compiled_model_.compile(optimizer=self.optimizer, loss=self.loss)\n-        history = self.compiled_model_.fit(\n-            X, y, batch_size=self.train_batch_size, nb_epoch=self.nb_epoch, verbose=self.verbose,\n-            shuffle=self.shuffle, show_accuracy=self.show_accuracy,\n-            validation_split=self.validation_split, validation_data=self.validation_data,\n-            callbacks=self.callbacks)\n+        if self.model.loss.__name__ == 'categorical_crossentropy'\\\n+                and len(y.shape) != 2:\n+            y = to_categorical(y)\n \n-        self.config_ = self.model.get_config()\n-        self.weights_ = self.model.get_weights()\n+        fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n+        fit_args.update(kwargs)\n+\n+        history = self.model.fit(X, y, **fit_args)\n \n         return history\n \n+    def filter_sk_params(self, fn, override={}):\n+        '''Filter sk_params and return those in fn's arguments\n+\n+        # Arguments\n+            fn : arbitrary function\n+            override: dictionary, values to overrid sk_params\n+\n+        # Returns\n+            res : dictionary dictionary containing variabls\n+                in both sk_params and fn's arguments.\n+        '''\n+        res = {}\n+        fn_args = inspect.getargspec(fn)[0]\n+        for name, value in self.sk_params.items():\n+            if name in fn_args:\n+                res.update({name: value})\n+        res.update(override)\n+        return res\n+\n \n class KerasClassifier(BaseWrapper):\n-    \"\"\"\n-    Implementation of the scikit-learn classifier API for Keras.\n \n-    Parameters\n-    ----------\n-    model : object\n-        An un-compiled Keras model object is required to use the scikit-learn wrapper.\n-    optimizer : string\n-        Optimization method used by the model during compilation/training.\n-    loss : string\n-        Loss function used by the model during compilation/training.\n-    \"\"\"\n-    def __init__(self, model, optimizer='adam', loss='categorical_crossentropy', **kwargs):\n-        super(KerasClassifier, self).__init__(model, optimizer, loss, **kwargs)\n+    '''Implementation of the scikit-learn classifier API for Keras.'''\n \n-    def predict(self, X):\n-        \"\"\"\n-        Returns the class predictions for the given test data.\n+    def predict(self, X, **kwargs):\n+        '''# Returns the class predictions for the given test data.\n \n-        Parameters\n-        ----------\n+        # Arguments\n             X : array-like, shape = (n_samples, n_features)\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n+            kwargs: dictionary arguments\n+                Legal arguments are the arguments of Sequential.predict_classes\n \n-        Returns\n-        -------\n+        # Returns\n             preds : array-like, shape = (n_samples)\n                 Class predictions.\n-        \"\"\"\n-        return self.compiled_model_.predict_classes(\n-            X, batch_size=self.test_batch_size, verbose=self.verbose)\n+        '''\n+        kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n+        return self.model.predict_classes(X, **kwargs)\n \n-    def predict_proba(self, X):\n-        \"\"\"\n-        Returns class probability estimates for the given test data.\n+    def predict_proba(self, X, **kwargs):\n+        '''# Returns class probability estimates for the given test data.\n \n-        Parameters\n-        ----------\n+        # Arguments\n             X : array-like, shape = (n_samples, n_features)\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n+            kwargs: dictionary arguments\n+                Legal arguments are the arguments of Sequential.predict_classes\n \n-        Returns\n-        -------\n+        # Returns\n             proba : array-like, shape = (n_samples, n_outputs)\n                 Class probability estimates.\n-        \"\"\"\n-        return self.compiled_model_.predict_proba(\n-            X, batch_size=self.test_batch_size, verbose=self.verbose)\n+        '''\n+        kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n+        return self.model.predict_proba(X, **kwargs)\n \n-    def score(self, X, y):\n-        \"\"\"\n-        Returns the mean accuracy on the given test data and labels.\n+    def score(self, X, y, **kwargs):\n+        '''# Returns the mean accuracy on the given test data and labels.\n \n-        Parameters\n-        ----------\n+        # Arguments\n             X : array-like, shape = (n_samples, n_features)\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n             y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n                 True labels for X.\n+            kwargs: dictionary arguments\n+                Legal arguments are the arguments of Sequential.evaluate\n \n-        Returns\n-        -------\n+        # Returns\n             score : float\n                 Mean accuracy of predictions on X wrt. y.\n-        \"\"\"\n-        loss, accuracy = self.compiled_model_.evaluate(\n-            X, y, batch_size=self.test_batch_size, show_accuracy=True, verbose=self.verbose)\n+        '''\n+        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n+        kwargs.update({'show_accuracy': True})\n+        loss, accuracy = self.model.evaluate(X, y, **kwargs)\n         return accuracy\n \n \n class KerasRegressor(BaseWrapper):\n-    \"\"\"\n-    Implementation of the scikit-learn regressor API for Keras.\n \n-    Parameters\n-    ----------\n-    model : object\n-        An un-compiled Keras model object is required to use the scikit-learn wrapper.\n-    optimizer : string\n-        Optimization method used by the model during compilation/training.\n-    loss : string\n-        Loss function used by the model during compilation/training.\n-    \"\"\"\n-    def __init__(self, model, optimizer='adam', loss='mean_squared_error', **kwargs):\n-        super(KerasRegressor, self).__init__(model, optimizer, loss, **kwargs)\n+    '''Implementation of the scikit-learn regressor API for Keras.'''\n \n-    def predict(self, X):\n-        \"\"\"\n-        Returns predictions for the given test data.\n+    def predict(self, X, **kwargs):\n+        ''' Returns predictions for the given test data.\n \n-        Parameters\n-        ----------\n+        # Arguments\n             X : array-like, shape = (n_samples, n_features)\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n-\n-        Returns\n-        -------\n+            kwargs: dictionary arguments\n+                Legal arguments are the arguments of Sequential.predict\n+        # Returns\n             preds : array-like, shape = (n_samples)\n                 Predictions.\n-        \"\"\"\n-        return self.compiled_model_.predict(\n-            X, batch_size=self.test_batch_size, verbose=self.verbose).ravel()\n+        '''\n+        kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n+        return self.model.predict(X, **kwargs)\n \n-    def score(self, X, y):\n-        \"\"\"\n-        Returns the mean accuracy on the given test data and labels.\n+    def score(self, X, y, **kwargs):\n+        '''Returns the mean accuracy on the given test data and labels.\n \n-        Parameters\n-        ----------\n+        # Arguments\n             X : array-like, shape = (n_samples, n_features)\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n             y : array-like, shape = (n_samples)\n                 True labels for X.\n+            kwargs: dictionary arguments\n+                Legal arguments are the arguments of Sequential.evaluate\n \n-        Returns\n-        -------\n+        # Returns\n             score : float\n-            Loss from predictions on X wrt. y.\n-        \"\"\"\n-        loss = self.compiled_model_.evaluate(\n-            X, y, batch_size=self.test_batch_size, show_accuracy=False, verbose=self.verbose)\n+                Mean accuracy of predictions on X wrt. y.\n+        '''\n+        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n+        kwargs.update({'show_accuracy': False})\n+        loss = self.model.evaluate(X, y, **kwargs)\n         return loss\n\n@@ -37,33 +37,85 @@ y_test = np_utils.to_categorical(y_test, nb_classes=nb_class)\n                                                                      output_shape=(1,))\n \n \n-@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n-def test_keras_classifier():\n+def build_fn_clf(hidden_dims=50):\n     model = Sequential()\n     model.add(Dense(input_dim, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n+    model.add(Dense(hidden_dims))\n+    model.add(Activation('relu'))\n     model.add(Dense(nb_class))\n     model.add(Activation('softmax'))\n-\n-    sklearn_clf = KerasClassifier(model, optimizer=optim, loss=loss,\n-                                  train_batch_size=batch_size,\n-                                  test_batch_size=batch_size,\n-                                  nb_epoch=nb_epoch)\n-    sklearn_clf.fit(X_train, y_train)\n-    sklearn_clf.score(X_test, y_test)\n+    model.compile(optimizer='sgd', loss='categorical_crossentropy',\n+                  class_mode='binary')\n+    return model\n \n \n-@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n-def test_keras_regressor():\n+class Class_build_fn_clf(object):\n+    def __call__(self, hidden_dims):\n+        return build_fn_clf(hidden_dims)\n+\n+\n+class Inherit_class_build_fn_clf(KerasClassifier):\n+    def __call__(self, hidden_dims):\n+        return build_fn_clf(hidden_dims)\n+\n+\n+def build_fn_reg(hidden_dims=50):\n     model = Sequential()\n     model.add(Dense(input_dim, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n+    model.add(Dense(hidden_dims))\n+    model.add(Activation('relu'))\n     model.add(Dense(1))\n-    model.add(Activation('softmax'))\n+    model.add(Activation('linear'))\n+    model.compile(optimizer='sgd', loss='mean_absolute_error')\n+    return model\n \n-    sklearn_regressor = KerasRegressor(model, optimizer=optim, loss=loss,\n-                                       train_batch_size=batch_size,\n-                                       test_batch_size=batch_size,\n-                                       nb_epoch=nb_epoch)\n-    sklearn_regressor.fit(X_train_reg, y_train_reg)\n-    sklearn_regressor.score(X_test_reg, y_test_reg)\n+\n+class Class_build_fn_reg(object):\n+    def __call__(self, hidden_dims):\n+        return build_fn_reg(hidden_dims)\n+\n+\n+class Inherit_class_build_fn_reg(KerasRegressor):\n+    def __call__(self, hidden_dims):\n+        return build_fn_reg(hidden_dims)\n+\n+for fn in [build_fn_clf, Class_build_fn_clf(), Inherit_class_build_fn_clf]:\n+    if fn is Inherit_class_build_fn_clf:\n+        classifier = Inherit_class_build_fn_clf(\n+            build_fn=None, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n+    else:\n+        classifier = KerasClassifier(\n+            build_fn=fn, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    classifier.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+    score = classifier.score(X_train, y_train, batch_size=batch_size)\n+    preds = classifier.predict(X_test, batch_size=batch_size)\n+    proba = classifier.predict_proba(X_test, batch_size=batch_size)\n+\n+\n+for fn in [build_fn_reg, Class_build_fn_reg(), Inherit_class_build_fn_reg]:\n+    if fn is Inherit_class_build_fn_reg:\n+        regressor = Inherit_class_build_fn_reg(\n+            build_fn=None, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n+    else:\n+        regressor = KerasRegressor(\n+            build_fn=fn, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    regressor.fit(X_train_reg, y_train_reg,\n+                  batch_size=batch_size, nb_epoch=nb_epoch)\n+    score = regressor.score(X_train_reg, y_train_reg, batch_size=batch_size)\n+    preds = regressor.predict(X_test, batch_size=batch_size)\n+\n+\n+# Usage of sklearn's grid_search\n+# from sklearn import grid_search\n+# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128], nb_epoch=[2], verbose=[0])\n+# classifier = Inherit_class_build_fn_clf()\n+# clf = grid_search.GridSearchCV(classifier, parameters)\n+# clf.fit(X_train, y_train)\n+# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128], nb_epoch=[2], verbose=[0])\n+# regressor = Inherit_class_build_fn_reg()\n+# reg = grid_search.GridSearchCV(regressor, parameters, scoring='mean_squared_error', n_jobs=1, cv=2, verbose=2)\n+# reg.fit(X_train_reg, y_train_reg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
