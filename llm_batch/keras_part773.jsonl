{"custom_id": "keras#962325d7f7231f6addeeea1a2fe9feba2ddec76f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 22 | Churn Cumulative: 991 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -404,12 +404,10 @@ def _load_state(\n         else:\n             trackable.load_assets(assets_store.get(inner_path))\n \n-    if not failure:\n-        if visited_trackables is not None:\n-            visited_trackables.add(id(trackable))\n-        if id(trackable) in failed_trackables:\n-            failed_trackables.remove(id(trackable))\n-            error_msgs.pop(id(trackable))\n+    if failed_trackables is not None:\n+        currently_failed = len(failed_trackables)\n+    else:\n+        currently_failed = 0\n \n     # Recursively load states for Keras trackables such as layers/optimizers.\n     for child_attr, child_obj in _walk_trackable(trackable):\n@@ -440,6 +438,18 @@ def _load_state(\n                 error_msgs=error_msgs,\n             )\n \n+    if failed_trackables is not None:\n+        newly_failed = len(failed_trackables) - currently_failed\n+    else:\n+        newly_failed = 0\n+\n+    if not failure:\n+        if visited_trackables is not None and newly_failed <= 0:\n+            visited_trackables.add(id(trackable))\n+        if id(trackable) in failed_trackables:\n+            failed_trackables.remove(id(trackable))\n+            error_msgs.pop(id(trackable))\n+\n \n def _save_container_state(\n     container, weights_store, assets_store, inner_path, visited_trackables\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e45c1382c85c8b53676fcb40fc270c839e1690d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -76,7 +76,7 @@ def standardize_reduction(reduction):\n     if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n-            f\"Expected on of {allowed}. Received: \"\n+            f\"Expected one of {allowed}. Received: \"\n             f\"reduction={reduction}\"\n         )\n     return reduction\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3d269b308b40314e8e128a7116a7f3fdf0ca212", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 416 | Lines Deleted: 76 | Files Changed: 8 | Hunks: 43 | Methods Changed: 38 | Complexity Δ (Sum/Max): 45/16 | Churn Δ: 492 | Churn Cumulative: 3740 | Contributors (this commit): 17 | Commits (past 90d): 31 | Contributors (cumulative): 31 | DMM Complexity: 0.696969696969697\n\nDIFF:\n@@ -0,0 +1,103 @@\n+import jax\n+from jax import numpy as jnp\n+\n+from keras.optimizers import base_optimizer\n+\n+\n+class JaxOptimizer(base_optimizer.BaseOptimizer):\n+    \"\"\"A class for JAX specific optimizer logic.\n+\n+    Its purpose is to route around statelessness\n+    requirements in cond ops used for EMA handling\n+    and gradient accumulation handling. We do this\n+    by skipping conditionals entirely.\n+    \"\"\"\n+\n+    def _backend_apply_gradients(self, grads, trainable_variables):\n+        if self.gradient_accumulation_steps:\n+            is_update_step = (\n+                self.iterations + 1\n+            ) % self.gradient_accumulation_steps == 0\n+            steps = self.gradient_accumulation_steps\n+\n+            current_trainable_vars_value = [\n+                v.value for v in trainable_variables\n+            ]\n+            current_optimizer_vars_value = [v.value for v in self.variables]\n+\n+            new_g_accs = jax.lax.cond(\n+                is_update_step,\n+                lambda: [\n+                    jnp.zeros(x.shape, dtype=x.dtype)\n+                    for x in self._accumulated_gradients\n+                ],\n+                lambda: [\n+                    grads[i] + self._accumulated_gradients[i]\n+                    for i in range(len(grads))\n+                ],\n+            )\n+\n+            grads = jax.lax.cond(\n+                is_update_step,\n+                lambda: [\n+                    (grads[i] + self._accumulated_gradients[i]) / steps\n+                    for i in range(len(grads))\n+                ],\n+                lambda: list(grads),\n+            )\n+\n+            self._backend_update_step(\n+                grads, trainable_variables, self.learning_rate\n+            )\n+            new_trainable_vars = jax.lax.cond(\n+                is_update_step,\n+                lambda: [v.value for v in trainable_variables],\n+                lambda: current_trainable_vars_value,\n+            )\n+            new_opt_vars = jax.lax.cond(\n+                is_update_step,\n+                lambda: [v.value for v in self.variables],\n+                lambda: current_optimizer_vars_value,\n+            )\n+\n+            for value, v in zip(new_trainable_vars, trainable_variables):\n+                v.assign(value)\n+\n+            for value, v in zip(new_opt_vars, self.variables):\n+                v.assign(value)\n+\n+            for n_g_acc, g_acc in zip(new_g_accs, self._accumulated_gradients):\n+                g_acc.assign(n_g_acc)\n+\n+        else:\n+            self._backend_update_step(\n+                grads, trainable_variables, self.learning_rate\n+            )\n+\n+        if self.use_ema:\n+            self._update_model_variables_moving_average(\n+                self._trainable_variables\n+            )\n+            if self.ema_overwrite_frequency is not None:\n+                should_overwrite_model_vars = (\n+                    self.iterations + 1\n+                ) % self.ema_overwrite_frequency == 0\n+                should_overwrite_model_vars_int = (\n+                    should_overwrite_model_vars.astype(\"int32\")\n+                )\n+                should_not_overwrite_model_vars_int = jnp.logical_not(\n+                    should_overwrite_model_vars\n+                ).astype(\"int32\")\n+                current_trainable_vars_value = [\n+                    v.value for v in self._trainable_variables\n+                ]\n+                for var, average_var in zip(\n+                    self._trainable_variables,\n+                    self._model_variables_moving_average,\n+                ):\n+                    var.assign(\n+                        average_var * should_overwrite_model_vars_int\n+                        + var.value * should_not_overwrite_model_vars_int\n+                    )\n+\n+        self.iterations.assign_add(1)\n\n@@ -97,27 +97,22 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             variables,\n         )\n \n-    def _internal_apply_gradients(self, grads_and_vars):\n+    def _backend_update_step(self, grads, trainable_variables, learning_rate):\n+        trainable_variables = [\n+            v.value if isinstance(v, backend.Variable) else v\n+            for v in trainable_variables\n+        ]\n         tf.__internal__.distribute.interim.maybe_merge_call(\n-            self._distributed_apply_gradients_fn,\n+            self._distributed_tf_update_step,\n             self._distribution_strategy,\n-            grads_and_vars,\n+            list(zip(grads, trainable_variables)),\n+            learning_rate,\n         )\n \n-    def _distributed_apply_gradients_fn(\n-        self, distribution, grads_and_vars, **kwargs\n+    def _distributed_tf_update_step(\n+        self, distribution, grads_and_vars, learning_rate\n     ):\n-        \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n-\n-        unwrapped_grads_and_vars = []\n-        for g, v in grads_and_vars:\n-            if isinstance(v, backend.Variable):\n-                v = v.value\n-            unwrapped_grads_and_vars.append((g, v))\n-        grads_and_vars = unwrapped_grads_and_vars\n-\n         def apply_grad_to_update_var(var, grad):\n-            learning_rate = self._get_current_learning_rate()\n             return self.update_step(grad, var, learning_rate)\n \n         for grad, var in grads_and_vars:\n@@ -125,36 +120,44 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n                 var, apply_grad_to_update_var, args=(grad,), group=False\n             )\n \n-        if self.use_ema:\n-            _, var_list = zip(*grads_and_vars)\n-            self._update_model_variables_moving_average(var_list)\n-            if self.ema_overwrite_frequency:\n-                # Only when self.ema_overwrite_frequency is not None, we\n-                # overwrite the model variables.\n-                should_overwrite_model_vars = (\n-                    self.iterations + 1\n-                ) % self.ema_overwrite_frequency == 0\n-                tf.cond(\n-                    tf.cast(should_overwrite_model_vars, tf.bool),\n-                    true_fn=lambda: self._overwrite_model_variables_with_average_value(  # noqa: E501\n-                        var_list\n-                    ),\n-                    false_fn=lambda: None,\n-                )\n-        self.iterations.assign_add(1)\n-\n-    def _overwrite_model_variables_with_average_value(self, var_list):\n+    def _overwrite_model_variables_with_average_value(\n+        self, trainable_variables\n+    ):\n         \"\"\"Overwrite model variables with their moving average values.\n \n         This function overwrites variables on each device.\n         Args:\n           var_list: list of model variables.\n         \"\"\"\n-        strategy = self._distribution_strategy\n+        trainable_variables = [\n+            v.value if isinstance(v, backend.Variable) else v\n+            for v in trainable_variables\n+        ]\n         # Override model variable by the stored average value on all devices.\n         for var, average_var in zip(\n-            var_list, self._model_variables_moving_average\n+            trainable_variables, self._model_variables_moving_average\n         ):\n-            strategy.extended.update(\n+            self._distribution_strategy.extended.update(\n                 var, lambda a, b: a.assign(b), args=(average_var,)\n             )\n+\n+    def _backend_increment_gradient_accumulators(self, grads):\n+        def update_accumulator(var, grad):\n+            var.assign(var + grad)\n+\n+        accumulators = [v.value for v in self._accumulated_gradients]\n+\n+        def _distributed_tf_increment_grad_acc(\n+            distribution, grads, accumulators\n+        ):\n+            for grad, var in zip(grads, accumulators):\n+                distribution.extended.update(\n+                    var, update_accumulator, args=(grad,), group=False\n+                )\n+\n+        tf.__internal__.distribute.interim.maybe_merge_call(\n+            _distributed_tf_increment_grad_acc,\n+            self._distribution_strategy,\n+            grads,\n+            accumulators,\n+        )\n\n@@ -138,3 +138,49 @@ class OptimizerDistributeTest(testing.TestCase):\n         optimizer.build(vars)\n         with self.assertRaisesRegex(ValueError, \"not supported\"):\n             optimizer.stateless_apply(optimizer.variables, grads, vars)\n+\n+    def test_ema(self):\n+        with self.strategy.scope():\n+            v = backend.Variable([[3.0, 4.0], [5.0, 6.0]])\n+            grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+            optimizer = SGD(\n+                learning_rate=1.0,\n+                use_ema=True,\n+                ema_momentum=0.9,\n+                ema_overwrite_frequency=3,\n+            )\n+            self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n+            self.assertAllClose(v, [[2.0, 3.0], [4.0, 5.0]])\n+            self.assertAllClose(\n+                optimizer._model_variables_moving_average[0],\n+                [[2.0, 3.0], [4.0, 5.0]],\n+            )\n+            self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n+            self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+            self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n+            self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n+\n+    def test_gradient_accumulation(self):\n+        with self.strategy.scope():\n+            v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+            grads = backend.convert_to_tensor([[1.0, 1.0], [2.0, 2.0]])\n+            optimizer = SGD(learning_rate=1.0, gradient_accumulation_steps=3)\n+            self.assertEqual(optimizer.gradient_accumulation_steps, 3)\n+            self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n+            self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+            self.assertAllClose(\n+                optimizer._accumulated_gradients[0], [[1.0, 1.0], [2.0, 2.0]]\n+            )\n+            self.assertAllClose(optimizer.iterations, 1)\n+            self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n+            self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+            self.assertAllClose(\n+                optimizer._accumulated_gradients[0], [[2.0, 2.0], [4.0, 4.0]]\n+            )\n+            self.assertAllClose(optimizer.iterations, 2)\n+            self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n+            self.assertAllClose(v, [[0.0, 1.0], [1.0, 2.0]])\n+            self.assertAllClose(\n+                optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n+            )\n+            self.assertAllClose(optimizer.iterations, 3)\n\n@@ -1,15 +1,24 @@\n+import torch\n+\n from keras.optimizers.base_optimizer import BaseOptimizer\n from keras.utils import torch_utils\n \n \n class TorchParallelOptimizer(BaseOptimizer):\n     @torch_utils.no_grad\n-    def _internal_apply_gradients(self, grads_and_vars):\n-        grads, trainable_variables = zip(*grads_and_vars)\n-\n+    def _backend_update_step(self, grads, trainable_variables, learning_rate):\n         self._parallel_update_step(\n             grads,\n             trainable_variables,\n-            self._get_current_learning_rate(),\n+            learning_rate,\n         )\n-        self.iterations.assign(self.iterations + 1)\n+\n+    @torch_utils.no_grad\n+    def _backend_reset_gradient_accumulators(self):\n+        acc_list = [v.value for v in self._accumulated_gradients]\n+        torch._foreach_mul_(acc_list, 0.0)\n+\n+    @torch_utils.no_grad\n+    def _backend_increment_gradient_accumulators(self, grads):\n+        acc_list = [v.value for v in self._accumulated_gradients]\n+        torch._foreach_add_(acc_list, grads, alpha=1.0)\n\n@@ -1,6 +1,8 @@\n import re\n import warnings\n \n+import numpy as np\n+\n from keras import backend\n from keras import initializers\n from keras import ops\n@@ -22,6 +24,7 @@ class BaseOptimizer:\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n         name=None,\n         **kwargs,\n     ):\n@@ -43,6 +46,15 @@ class BaseOptimizer:\n         self.clipvalue = clipvalue\n         self.use_ema = use_ema\n         self.loss_scale_factor = loss_scale_factor\n+        self.gradient_accumulation_steps = gradient_accumulation_steps\n+\n+        if gradient_accumulation_steps:\n+            if not gradient_accumulation_steps >= 2:\n+                raise ValueError(\n+                    \"`gradient_accumulation_steps` must be an integer >= 2. \"\n+                    \"Received: gradient_accumulation_steps=\"\n+                    f\"{gradient_accumulation_steps}\"\n+                )\n \n         if use_ema:\n             # Verify the arguments related to EMA.\n@@ -128,6 +140,8 @@ class BaseOptimizer:\n         if self.use_ema:\n             self._model_variables_moving_average = []\n             self._ema_vars_initialized = False\n+        if self.gradient_accumulation_steps:\n+            self._accumulated_gradients = []\n         for i, variable in enumerate(variables):\n             self._trainable_variables_indices[self._var_key(variable)] = i\n             if self.use_ema:\n@@ -137,6 +151,13 @@ class BaseOptimizer:\n                         \"average\",\n                     )\n                 )\n+            if self.gradient_accumulation_steps:\n+                self._accumulated_gradients.append(\n+                    self.add_variable_from_reference(\n+                        variable,\n+                        \"gradient_accumulator\",\n+                    )\n+                )\n         self._trainable_variables = variables[:]\n         self.built = True\n \n@@ -250,11 +271,13 @@ class BaseOptimizer:\n         return self.iterations\n \n     def apply(self, grads, trainable_variables=None):\n-        \"\"\"\n+        \"\"\"Update traininable variables according to provided gradient values.\n+\n         `grads` should be a list of gradient tensors\n         with 1:1 mapping to the list of variables the optimizer was built with.\n \n-        `variables` can be provided on the first call to build the optimizer.\n+        `trainable_variables` can be provided\n+        on the first call to build the optimizer.\n         \"\"\"\n         if len(grads) == 0:\n             # It is possible that the grad is empty. In this case,\n@@ -304,19 +327,100 @@ class BaseOptimizer:\n             self._apply_weight_decay(trainable_variables)\n \n             # Apply gradient updates.\n-            self._internal_apply_gradients(\n-                list(zip(grads, trainable_variables))\n-            )\n-\n+            self._backend_apply_gradients(grads, trainable_variables)\n             # Apply variable constraints after applying gradients.\n             for variable in trainable_variables:\n                 if getattr(variable, \"constraint\", None) is not None:\n                     variable.assign(variable.constraint(variable))\n \n-    def _internal_apply_gradients(self, grads_and_vars):\n-        for grad, var in grads_and_vars:\n-            self.update_step(grad, var, self.learning_rate)\n-        self.iterations.assign(self.iterations + 1)\n+    def _backend_apply_gradients(self, grads, trainable_variables):\n+        \"\"\"Apply method that can be overridden by different backends.\n+\n+        JAX overrides it in order to deal with statelessness in gradient\n+        accumulation and EMA handling.\n+\n+        The below implementation is intended to be generally backend-agnostic,\n+        but may not work with all backends.\n+\n+        This method does 4 things:\n+        - Call the optimizer's update_step() to update trainable variables\n+            and optimizer variables.\n+        - Update EMA variables, if EMA is configured.\n+        - Update gradient accumulators, if gradient accumulation is configured.\n+        - Update the iteration counter.\n+        \"\"\"\n+        if self.gradient_accumulation_steps:\n+            is_update_step = (\n+                self.iterations + 1\n+            ) % self.gradient_accumulation_steps == 0\n+\n+            def _update_step_fn(self, grads, trainable_variables):\n+                # Run update step with accumulated grads + reset accumulators\n+                steps = self.gradient_accumulation_steps\n+                grads = [\n+                    (grads[i] + self._accumulated_gradients[i]) / steps\n+                    for i in range(len(grads))\n+                ]\n+                self._backend_update_step(\n+                    grads, trainable_variables, self.learning_rate\n+                )\n+                self._backend_reset_gradient_accumulators()\n+\n+            def _grad_accumulation_fn(self, grads):\n+                # Update gradient accumulators\n+                self._backend_increment_gradient_accumulators(grads)\n+\n+            ops.cond(\n+                is_update_step,\n+                lambda: _update_step_fn(self, grads, trainable_variables),\n+                lambda: _grad_accumulation_fn(self, grads),\n+            )\n+        else:\n+            # Run udpate step.\n+            self._backend_update_step(\n+                grads, trainable_variables, self.learning_rate\n+            )\n+\n+        if self.use_ema:\n+            self._update_model_variables_moving_average(\n+                self._trainable_variables\n+            )\n+            if self.ema_overwrite_frequency:\n+                # Only when self.ema_overwrite_frequency is not None, we\n+                # overwrite the model variables.\n+                should_overwrite_model_vars = (\n+                    self.iterations + 1\n+                ) % self.ema_overwrite_frequency == 0\n+                ops.cond(\n+                    should_overwrite_model_vars,\n+                    lambda: self._overwrite_model_variables_with_average_value(\n+                        self._trainable_variables\n+                    ),\n+                    lambda: None,\n+                )\n+        # Update iteration counter.\n+        self.iterations.assign_add(1)\n+\n+    def _backend_update_step(self, grads, trainable_variables, learning_rate):\n+        \"\"\"Collective update_step that can be overridden by the backend.\n+\n+        It is overridden by torch for performance reasons, and\n+        by TF to support tf.distribute.\n+        \"\"\"\n+        for grad, var in zip(grads, trainable_variables):\n+            self.update_step(grad, var, learning_rate)\n+\n+    def _backend_reset_gradient_accumulators(self):\n+        for g_acc in self._accumulated_gradients:\n+            g_acc.assign(np.zeros(g_acc.shape, dtype=g_acc.dtype))\n+\n+    def _backend_increment_gradient_accumulators(self, grads):\n+        new_g_accs = [\n+            (grads[i] + self._accumulated_gradients[i])\n+            for i in range(len(grads))\n+        ]\n+        for n_g_acc, g_acc in zip(new_g_accs, self._accumulated_gradients):\n+            g_acc.assign(n_g_acc)\n \n     def stateless_apply(self, optimizer_variables, grads, trainable_variables):\n         self._check_super_called()\n@@ -566,11 +670,11 @@ class BaseOptimizer:\n                 \"Go add it!\"\n             )\n \n-    def _update_model_variables_moving_average(self, var_list):\n+    def _update_model_variables_moving_average(self, trainable_variables):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n         if self.use_ema:\n             for var, average in zip(\n-                var_list, self._model_variables_moving_average\n+                trainable_variables, self._model_variables_moving_average\n             ):\n                 if self._ema_vars_initialized:\n                     average.assign(\n@@ -581,22 +685,22 @@ class BaseOptimizer:\n                     average.assign(var)\n             self._ema_vars_initialized = True\n \n-    def _overwrite_model_variables_with_average_value(self, var_list):\n+    def _overwrite_model_variables_with_average_value(\n+        self, trainable_variables\n+    ):\n         \"\"\"Overwrite model variables with its moving average.\"\"\"\n-        if len(var_list) != len(self._model_variables_moving_average):\n+        if len(trainable_variables) != len(\n+            self._model_variables_moving_average\n+        ):\n             raise ValueError(\n-                f\"The length of model variables ({len(var_list)}) to \"\n-                \"override does not match the length of model variables \"\n+                f\"The length of model variables ({len(trainable_variables)}) \"\n+                \"to override does not match the length of model variables \"\n                 \"stored in the optimizer \"\n                 f\"({len(self._model_variables_moving_average)}). Please \"\n                 \"check if the optimizer was called on your model.\"\n             )\n-        self._overwrite_model_variables_with_average_value_helper(var_list)\n-\n-    def _overwrite_model_variables_with_average_value_helper(self, var_list):\n-        \"\"\"Helper function that overwrites model variables.\"\"\"\n         for var, average_var in zip(\n-            var_list, self._model_variables_moving_average\n+            trainable_variables, self._model_variables_moving_average\n         ):\n             var.assign(average_var)\n \n@@ -656,6 +760,7 @@ class BaseOptimizer:\n             \"ema_momentum\": self.ema_momentum,\n             \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n             \"loss_scale_factor\": self.loss_scale_factor,\n+            \"gradient_accumulation_steps\": self.gradient_accumulation_steps,\n         }\n         return config\n \n@@ -707,9 +812,9 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n             so that their global norm is no higher than this value.\n         use_ema: Boolean, defaults to False. If True, exponential moving average\n             (EMA) is applied. EMA consists of computing an exponential moving\n-          average of the weights of the model (as the weight values change after\n-          each training batch), and periodically overwriting the weights with\n-          their moving average.\n+            average of the weights of the model (as the weight values change\n+            after each training batch), and periodically overwriting the\n+            weights with their moving average.\n         ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n             This is the momentum to use when computing\n             the EMA of the model's weights:\n@@ -719,19 +824,27 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n             `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n             we overwrite the model variable by its moving average.\n             If None, the optimizer\n-          does not overwrite model variables in the middle of training, and you\n-          need to explicitly overwrite the variables at the end of training\n-          by calling `optimizer.finalize_variable_values()`\n-          (which updates the model\n+            does not overwrite model variables in the middle of training,\n+            and you need to explicitly overwrite the variables\n+            at the end of training by calling\n+            `optimizer.finalize_variable_values()` (which updates the model\n             variables in-place). When using the built-in `fit()` training loop,\n             this happens automatically after the last epoch,\n             and you don't need to do anything.\n         loss_scale_factor: Float or `None`. If a float, the scale factor will\n-          be multiplied the loss before computing gradients, and the inverse of\n-          the scale factor will be multiplied by the gradients before updating\n-          variables. Useful for preventing underflow during mixed precision\n-          training. Alternately, `keras.optimizers.LossScaleOptimizer` will\n+            be multiplied the loss before computing gradients, and the inverse\n+            of the scale factor will be multiplied by the gradients before\n+            updating variables. Useful for preventing underflow during\n+            mixed precision training. Alternately,\n+            `keras.optimizers.LossScaleOptimizer` will\n             automatically set a loss scale factor.\n+        gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n+            variables will not be updated at every step; instead they will be\n+            updated every `gradient_accumulation_steps` steps, using the average\n+            value of the gradients since the last update. This is known as\n+            \"gradient accumulation\". This can be useful\n+            when your batch size is very small, in order to reduce gradient\n+            noise at each update step.\n \"\"\"\n \n \n\n@@ -99,7 +99,7 @@ class Nadam(optimizer.Optimizer):\n                 )\n             )\n \n-    def _internal_apply_gradients(self, grads_and_vars):\n+    def _backend_update_step(self, grads, trainable_variables, learning_rate):\n         dtype = self._u_product.dtype\n         self.assign(\n             self._u_product,\n@@ -110,7 +110,7 @@ class Nadam(optimizer.Optimizer):\n                 - 0.5 * ops.power(0.96, ops.cast(self.iterations + 1, dtype))\n             ),\n         )\n-        super()._internal_apply_gradients(grads_and_vars)\n+        super()._backend_update_step(grads, trainable_variables, learning_rate)\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n\n@@ -10,6 +10,10 @@ elif backend.backend() == \"torch\":\n     from keras.backend.torch.optimizers import TorchOptimizer\n \n     BackendOptimizer = TorchOptimizer\n+elif backend.backend() == \"jax\":\n+    from keras.backend.jax.optimizer import JaxOptimizer\n+\n+    BackendOptimizer = JaxOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -7,6 +7,36 @@ from keras import testing\n \n \n class OptimizerTest(testing.TestCase):\n+    def test_iterations_counter(self):\n+        v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+        optimizer = optimizers.Adam(learning_rate=1.0)\n+        self.assertAllClose(optimizer.iterations, 0)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(optimizer.iterations, 1)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(optimizer.iterations, 2)\n+\n+    def test_ema(self):\n+        v = backend.Variable([[3.0, 4.0], [5.0, 6.0]])\n+        grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+        optimizer = optimizers.SGD(\n+            learning_rate=1.0,\n+            use_ema=True,\n+            ema_momentum=0.9,\n+            ema_overwrite_frequency=3,\n+        )\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[2.0, 3.0], [4.0, 5.0]])\n+        self.assertAllClose(\n+            optimizer._model_variables_moving_average[0],\n+            [[2.0, 3.0], [4.0, 5.0]],\n+        )\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n+\n     def test_constraints_are_applied(self):\n         v = backend.Variable(np.random.random((2, 2)) - 1.0)\n         v.constraint = constraints.NonNeg()\n@@ -50,3 +80,35 @@ class OptimizerTest(testing.TestCase):\n                 optimizer_1.variables[i],\n                 optimizer_2.variables[i],\n             )\n+\n+    def test_gradient_accumulation(self):\n+        v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+        optimizer = optimizers.SGD(\n+            learning_rate=1.0, gradient_accumulation_steps=3\n+        )\n+        self.assertEqual(optimizer.gradient_accumulation_steps, 3)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[1.0, 1.0], [1.0, 1.0]]\n+        )\n+        self.assertAllClose(optimizer.iterations, 1)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[2.0, 2.0], [2.0, 2.0]]\n+        )\n+        self.assertAllClose(optimizer.iterations, 2)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[0.0, 1.0], [2.0, 3.0]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n+        )\n+        self.assertAllClose(optimizer.iterations, 3)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertAllClose(v, [[0.0, 1.0], [2.0, 3.0]])\n+        self.assertAllClose(\n+            optimizer._accumulated_gradients[0], [[1.0, 1.0], [1.0, 1.0]]\n+        )\n+        self.assertAllClose(optimizer.iterations, 4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#196596972033273c967705b9203767aedcf51350", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 10 | Churn Cumulative: 1436 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,11 @@ class Cropping2D(Layer):\n         super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(cropping, int):\n+            if cropping < 0:\n+                raise ValueError(\n+                    \"`cropping` cannot be negative. \"\n+                    f\"Received: cropping={cropping}.\"\n+                )\n             self.cropping = ((cropping, cropping), (cropping, cropping))\n         elif hasattr(cropping, \"__len__\"):\n             if len(cropping) != 2:\n\n@@ -62,6 +62,11 @@ class Cropping3D(Layer):\n         super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(cropping, int):\n+            if cropping < 0:\n+                raise ValueError(\n+                    \"`cropping` cannot be negative. \"\n+                    f\"Received: cropping={cropping}.\"\n+                )\n             self.cropping = (\n                 (cropping, cropping),\n                 (cropping, cropping),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7972fcac7941de5b71b47a7209241b9682aa03f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1887 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -783,7 +783,7 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-        self.assertAllClose(outputs, expected, rtol=5e-4)\n+        self.assertAllClose(outputs, expected, rtol=1e-3)\n \n     def test_conv_constraints(self):\n         layer = layers.Conv2D(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fbc05db85513395637fadf21210cb8f199182fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 21 | Files Changed: 6 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 113 | Churn Cumulative: 1330 | Contributors (this commit): 9 | Commits (past 90d): 41 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -571,7 +571,7 @@ def ctc_loss(\n     batch_size, max_target_length = target.shape\n \n     output = output.transpose((1, 0, 2))\n-    target = target.transpose((1, 0))\n+    target = target.transpose((1, 0)).astype(\"int32\")\n \n     logits = jnn.log_softmax(output)\n     mgrid_t, mgrid_b = jnp.meshgrid(\n\n@@ -800,9 +800,9 @@ def ctc_loss(\n     batch element.\n \n     Arguments:\n-        target: Tensor `(batch_size, max_target_length)` containing the\n+        target: Tensor `(batch_size, max_length)` containing the\n             target sequences in integer format.\n-        output: Tensor `(batch_size, max_output_length, num_classes)`\n+        output: Tensor `(batch_size, max_length, num_classes)`\n             containing the output of the softmax.\n         target_length: Tensor `(batch_size,)` containing the sequence length\n             for each target sequence in the batch.\n@@ -819,21 +819,8 @@ def ctc_loss(\n     target = tf.cast(target, dtype=\"int32\")\n     output = tf.convert_to_tensor(output)\n     output = tf.cast(output, dtype=\"float32\")\n-\n-    max_label_len = tf.shape(target)[1]\n-\n-    mask = tf.sequence_mask(target_length, max_label_len)\n-    indices = tf.where(mask)\n-    values = tf.boolean_mask(target, mask)\n-\n-    sparse_target = tf.SparseTensor(\n-        indices=indices,\n-        values=values,\n-        dense_shape=tf.cast(tf.shape(target), dtype=\"int64\"),\n-    )\n-\n     return tf.nn.ctc_loss(\n-        labels=sparse_target,\n+        labels=target,\n         logits=output,\n         label_length=target_length,\n         logit_length=output_length,\n\n@@ -20,6 +20,7 @@ from keras.losses.losses import binary_crossentropy\n from keras.losses.losses import categorical_crossentropy\n from keras.losses.losses import categorical_hinge\n from keras.losses.losses import cosine_similarity\n+from keras.losses.losses import ctc\n from keras.losses.losses import hinge\n from keras.losses.losses import huber\n from keras.losses.losses import kl_divergence\n\n@@ -1867,3 +1867,70 @@ def binary_focal_crossentropy(\n         focal_bce = weight * focal_bce\n \n     return ops.mean(focal_bce, axis=axis)\n+\n+\n+@keras_export(\"keras.losses.CTC\")\n+class CTC(LossFunctionWrapper):\n+    \"\"\"CTC (Connectionist Temporal Classification) loss.\n+\n+    Args:\n+        y_true: A tensor of shape `(batch_size, target_max_length)` containing\n+            the true labels in integer format. `0` always represents\n+            the blank/mask index and should not be used for classes.\n+        y_pred: A tensor of shape `(batch_size, output_max_length, num_classes)`\n+            containing logits (the output of your model).\n+            They should *not* be normalized via softmax.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"sparse_categorical_crossentropy\",\n+    ):\n+        super().__init__(\n+            ctc,\n+            name=name,\n+            reduction=reduction,\n+        )\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"reduction\": self.reduction,\n+        }\n+\n+\n+@keras_export(\"keras.losses.ctc\")\n+def ctc(y_true, y_pred):\n+    \"\"\"CTC (Connectionist Temporal Classification) loss.\n+\n+    Args:\n+        y_true: A tensor of shape `(batch_size, max_length)` containing\n+            the true labels in integer format. `0` always represents\n+            the blank/mask index and should not be used for classes.\n+        y_pred: A tensor of shape `(batch_size, max_length, num_classes)`\n+            containing logits (the output of your model).\n+            They should *not* be normalized via softmax.\n+    \"\"\"\n+    if len(ops.shape(y_true)) != 2:\n+        raise ValueError(\n+            \"Targets `y_true` are expected to be a tensor of shape \"\n+            \"`(batch_size, max_length)` in integer format. \"\n+            f\"Received: y_true.shape={ops.shape(y_true)}\"\n+        )\n+    if len(ops.shape(y_pred)) != 3:\n+        raise ValueError(\n+            \"Logits `y_pred` are expected to be a tensor of shape \"\n+            \"`(batch_size, max_length, num_classes)`. \"\n+            f\"Received: y_pred.shape={ops.shape(y_pred)}\"\n+        )\n+\n+    batch_length = ops.cast(ops.shape(y_true)[0], dtype=\"int32\")\n+    input_length = ops.cast(ops.shape(y_pred)[1], dtype=\"int32\")\n+    label_length = ops.cast(ops.shape(y_true)[1], dtype=\"int32\")\n+    input_length = input_length * ops.ones((batch_length,), dtype=\"int32\")\n+    label_length = label_length * ops.ones((batch_length,), dtype=\"int32\")\n+\n+    return ops.ctc_loss(\n+        y_true, y_pred, label_length, input_length, mask_index=0\n+    )\n\n@@ -1,5 +1,7 @@\n import numpy as np\n+import pytest\n \n+from keras import backend\n from keras import testing\n from keras.losses import losses\n \n@@ -1364,3 +1366,18 @@ class CategoricalFocalCrossentropyTest(testing.TestCase):\n \n         expected_value = 0.06685\n         self.assertAlmostEqual(loss, expected_value, 3)\n+\n+\n+class CTCTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(losses.CTC(name=\"myctc\"))\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy does not support CTC loss\",\n+    )\n+    def test_correctness(self):\n+        logits = (np.arange(24).reshape((2, 4, 3)).astype(\"float32\") - 12) / 100\n+        y_true = np.array(([[1, 2, 1, 0], [1, 2, 0, 2]]))\n+        output = losses.CTC()(y_true, logits)\n+        self.assertAllClose(output, 4.389582)\n\n@@ -1821,10 +1821,10 @@ def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     \"\"\"CTC (Connectionist Temporal Classification) loss.\n \n     Args:\n-        target: A tensor of shape `(batch_size, target_max_length)` containing\n+        target: A tensor of shape `(batch_size, max_length)` containing\n             the true labels in integer format.\n-        output: A tensor of shape `(batch_size, output_max_length, num_classes)`\n-            containing the output from the network.\n+        output: A tensor of shape `(batch_size, max_length, num_classes)`\n+            containing logits (the output of your model).\n         target_length: A tensor of shape `(batch_size,)` containing the\n             true label lengths.\n         output_length: A tensor of shape `(batch_size,)` containing the\n@@ -1837,7 +1837,6 @@ def ctc_loss(target, output, target_length, output_length, mask_index=0):\n         return CtcLoss(mask_index).symbolic_call(\n             target, output, target_length, output_length\n         )\n-\n     return backend.nn.ctc_loss(\n         target, output, target_length, output_length, mask_index\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4a4a139c7aada9f4495620e5a1c5f7ef20d84395", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 46 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -90,7 +90,9 @@ def rnn(\n \n     flattened_inputs = tree.flatten(inputs)\n     time_steps = flattened_inputs[0].shape[0]\n-    time_steps_t = tf.shape(flattened_inputs[0])[0]\n+    time_steps_t = (\n+        tf.shape(flattened_inputs[0])[0] if time_steps is None else time_steps\n+    )\n \n     for input_ in flattened_inputs:\n         input_.shape.with_rank_at_least(3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30ca1953e9876dccf9862bf3fa2cb18bcff720bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 17 | Churn Cumulative: 80 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -468,6 +468,8 @@ class CompileLoss(losses_module.Loss):\n                     \"must be a callable. \"\n                     f\"Received instead:\\nloss={loss} of type {type(loss)}\"\n                 )\n+            if isinstance(y_pred, list) and len(y_pred) == 1:\n+                y_pred = y_pred[0]\n \n         if is_function_like(loss) and tree.is_nested(y_pred):\n             # The model has multiple outputs but only one loss fn\n\n@@ -251,6 +251,21 @@ class TestCompileLoss(testing.TestCase, parameterized.TestCase):\n         value = compile_loss(y_true, y_pred)\n         self.assertAllClose(value, 0.068333, atol=1e-5)\n \n+    def test_single_output_case_with_crossentropy_loss(self):\n+        compile_loss = CompileLoss(loss=\"crossentropy\")\n+\n+        # Test symbolic build\n+        y_true, y_pred = backend.KerasTensor((3, 4)), backend.KerasTensor(\n+            (3, 4)\n+        )\n+        compile_loss.build(y_true, y_pred)\n+        # Test eager build\n+        y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n+        y_pred = np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]])\n+        compile_loss.build(y_true, y_pred)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 0.706595, atol=1e-5)\n+\n     @parameterized.parameters(True, False)\n     def test_list_output_case(self, broadcast):\n         if broadcast:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#095de4388389b68a28a67e9b3cd68d1c1be2db81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 69 | Churn Cumulative: 1340 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,23 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n         super().__init__(*args, **kwargs)\n         self._distribution_strategy = tf.distribute.get_strategy()\n \n+    def build(self, variables):\n+        super().build(variables)\n+        if hasattr(self, \"_ema_vars_initialized\"):\n+            # We must use `backend.Variable` in\n+            # `self._update_model_variables_moving_average` to correctly\n+            # initialize moving averages with the same value as\n+            # `trainable_variables` during the first `self.apply`.\n+            with backend.name_scope(self.name, caller=self):\n+                _ema_vars_initialized = backend.Variable(\n+                    False,\n+                    name=\"_ema_vars_initialized\",\n+                    dtype=\"bool\",\n+                    trainable=False,\n+                )\n+                self._track_variable(_ema_vars_initialized)\n+                self._ema_vars_initialized = _ema_vars_initialized\n+\n     def add_variable_from_reference(self, reference_variable, name=None):\n         if isinstance(reference_variable, backend.Variable):\n             colocate_var = reference_variable.value\n@@ -161,3 +178,26 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             grads,\n             accumulators,\n         )\n+\n+    def _update_model_variables_moving_average(self, trainable_variables):\n+        \"\"\"Update the stored moving average using the latest value.\"\"\"\n+        if self.use_ema:\n+            for var, average in zip(\n+                trainable_variables, self._model_variables_moving_average\n+            ):\n+\n+                def _update_fn(var, average):\n+                    average.assign(\n+                        self.ema_momentum * average\n+                        + (1 - self.ema_momentum) * var\n+                    )\n+\n+                def _assign_fn(var, average):\n+                    average.assign(var)\n+\n+                tf.cond(\n+                    self._ema_vars_initialized,\n+                    lambda: _update_fn(var, average),\n+                    lambda: _assign_fn(var, average),\n+                )\n+            self._ema_vars_initialized.assign(True)\n\n@@ -1,9 +1,13 @@\n import numpy as np\n+import pytest\n \n from keras import backend\n from keras import constraints\n+from keras import layers\n+from keras import models\n from keras import optimizers\n from keras import testing\n+from keras.utils import numerical_utils\n \n \n class OptimizerTest(testing.TestCase):\n@@ -34,8 +38,33 @@ class OptimizerTest(testing.TestCase):\n         )\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+        self.assertAllClose(\n+            optimizer._model_variables_moving_average[0],\n+            [[1.9, 2.9], [3.9, 4.9]],\n+        )\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n+        self.assertAllClose(\n+            optimizer._model_variables_moving_average[0],\n+            [[1.71, 2.71], [3.71, 4.71]],\n+        )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_ema_with_model_fit(self):\n+        x_train = np.ones((10, 3)).astype(\"float32\")\n+        y_train = np.zeros((10,)).astype(\"float32\")\n+        y_train = numerical_utils.to_categorical(y_train)\n+        optimizer = optimizers.SGD(use_ema=True, ema_momentum=0.9999)\n+        model = models.Sequential(\n+            [layers.Dense(2, use_bias=False, kernel_initializer=\"ones\")]\n+        )\n+        model.compile(loss=\"mse\", optimizer=optimizer)\n+        model.fit(x_train, y_train, batch_size=10, epochs=1)\n+        self.assertAllClose(\n+            model.trainable_variables[0].numpy(),\n+            [[0.98, 0.98], [0.98, 0.98], [0.98, 0.98]],\n+            atol=1e-5,\n+        )\n \n     def test_constraints_are_applied(self):\n         v = backend.Variable(np.random.random((2, 2)) - 1.0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
