{"custom_id": "keras#4f3274bc5f8e09a8488e74d569a6bfa4a9b9cf09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1149 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 5 | Methods Changed: 35 | Complexity Δ (Sum/Max): 136/66 | Churn Δ: 1149 | Churn Cumulative: 5402 | Contributors (this commit): 12 | Commits (past 90d): 86 | Contributors (cumulative): 25 | DMM Complexity: 0.5098591549295775\n\nDIFF:\n@@ -588,3 +588,458 @@ def binary_crossentropy(target, output, from_logits=False):\n     bce = target * tf.math.log(output)\n     bce += (1 - target) * tf.math.log(1 - output)\n     return -bce\n+\n+\n+def rnn(\n+    step_function,\n+    inputs,\n+    initial_states,\n+    go_backwards=False,\n+    mask=None,\n+    constants=None,\n+    unroll=False,\n+    input_length=None,\n+    time_major=False,\n+    zero_output_for_mask=False,\n+    return_all_outputs=True,\n+):\n+    \"\"\"Iterates over the time dimension of a tensor.\n+\n+    Args:\n+        step_function: RNN step function.\n+            Args;\n+                `input`; Tensor with shape `(samples, ...)` (no time dimension),\n+                    representing input for the batch of samples at a certain\n+                    time step.\n+                `states`; List of tensors.\n+            Returns;\n+                `output`; Tensor with shape `(samples, output_dim)`\n+                    (no time dimension).\n+                `new_states`; List of tensors, same length and shapes\n+                    as 'states'. The first state in the list must be the\n+                    output tensor at the previous timestep.\n+        inputs: Tensor of temporal data of shape `(samples, time, ...)`\n+            (at least 3D), or nested tensors, and each of which has shape\n+            `(samples, time, ...)`.\n+        initial_states: Tensor with shape `(samples, state_size)`\n+            (no time dimension), containing the initial values for the states\n+            used in the step function. In the case that state_size is in a\n+            nested shape, the shape of initial_states will also follow the\n+            nested structure.\n+        go_backwards: Boolean. If `True`, do the iteration over the time\n+            dimension in reverse order and return the reversed sequence.\n+        mask: Binary tensor with shape `(samples, time, 1)`,\n+            with a zero for every element that is masked.\n+        constants: List of constant values passed at each step.\n+        unroll: Whether to unroll the RNN or to use a symbolic `while_loop`.\n+        input_length: An integer or a 1-D Tensor, depending on whether\n+            the time dimension is fixed-length or not. In case of variable\n+            length input, it is used for masking in case there's no mask\n+            specified.\n+        time_major: Boolean. If `True`, the inputs and outputs will be in shape\n+            `(timesteps, batch, ...)`, whereas in the False case, it will be\n+            `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n+            efficient because it avoids transposes at the beginning and end of\n+            the RNN calculation. However, most TensorFlow data is batch-major,\n+            so by default this function accepts input and emits output in\n+            batch-major form.\n+        zero_output_for_mask: Boolean. If `True`, the output for masked timestep\n+            will be zeros, whereas in the `False` case, output from previous\n+            timestep is returned.\n+        return_all_outputs: Boolean. If `True`, return the recurrent outputs for\n+            all timesteps in the sequence. If `False`, only return the output\n+            for the last timestep (which consumes less memory).\n+\n+    Returns:\n+        A tuple, `(last_output, outputs, new_states)`.\n+            - `last_output`: the latest output of the rnn,\n+                with shape `(samples, ...)`.\n+            - `outputs`:\n+                - If `return_all_outputs=True`: a tensor with shape\n+                  `(samples, time, ...)` where each entry `outputs[s, t]` is the\n+                  output of the step function at time `t` for sample `s`\n+                - Else, a tensor equal to `last_output` with shape\n+                  `(samples, 1, ...)`\n+            - `new_states`: list of tensors, latest states returned by\n+                the step function, of shape `(samples, ...)`.\n+    \"\"\"\n+\n+    def swap_batch_timestep(input_t):\n+        # Swap the batch and timestep dim for the incoming tensor.\n+        axes = list(range(len(input_t.shape)))\n+        axes[0], axes[1] = 1, 0\n+        return tf.transpose(input_t, axes)\n+\n+    if not time_major:\n+        inputs = tf.nest.map_structure(swap_batch_timestep, inputs)\n+\n+    flatted_inputs = tf.nest.flatten(inputs)\n+    time_steps = flatted_inputs[0].shape[0]\n+    batch = flatted_inputs[0].shape[1]\n+    time_steps_t = tf.shape(flatted_inputs[0])[0]\n+\n+    for input_ in flatted_inputs:\n+        input_.shape.with_rank_at_least(3)\n+\n+    if mask is not None:\n+        if mask.dtype != tf.bool:\n+            mask = tf.cast(mask, tf.bool)\n+        if len(mask.shape) == 2:\n+            mask = tf.expand_dims(mask, axis=-1)\n+        if not time_major:\n+            mask = swap_batch_timestep(mask)\n+\n+    if constants is None:\n+        constants = []\n+\n+    # tf.where needs its condition tensor to be the same shape as its two\n+    # result tensors, but in our case the condition (mask) tensor is\n+    # (nsamples, 1), and inputs are (nsamples, ndimensions) or even more.\n+    # So we need to broadcast the mask to match the shape of inputs.\n+    # That's what the tile call does, it just repeats the mask along its\n+    # second dimension n times.\n+    def _expand_mask(mask_t, input_t, fixed_dim=1):\n+        if tf.nest.is_nested(mask_t):\n+            raise ValueError(\n+                f\"mask_t is expected to be tensor, but got {mask_t}\"\n+            )\n+        if tf.nest.is_nested(input_t):\n+            raise ValueError(\n+                f\"input_t is expected to be tensor, but got {input_t}\"\n+            )\n+        rank_diff = len(input_t.shape) - len(mask_t.shape)\n+        for _ in range(rank_diff):\n+            mask_t = tf.expand_dims(mask_t, -1)\n+        multiples = [1] * fixed_dim + input_t.shape.as_list()[fixed_dim:]\n+        return tf.tile(mask_t, multiples)\n+\n+    if unroll:\n+        if not time_steps:\n+            raise ValueError(\"Unrolling requires a fixed number of timesteps.\")\n+        states = tuple(initial_states)\n+        successive_states = []\n+        successive_outputs = []\n+\n+        # Process the input tensors. The input tensor need to be split on the\n+        # time_step dim, and reverse if go_backwards is True. In the case of\n+        # nested input, the input is flattened and then transformed\n+        # individually.  The result of this will be a tuple of lists, each of\n+        # the item in tuple is list of the tensor with shape (batch, feature)\n+        def _process_single_input_t(input_t):\n+            input_t = tf.unstack(input_t)  # unstack for time_step dim\n+            if go_backwards:\n+                input_t.reverse()\n+            return input_t\n+\n+        if tf.nest.is_nested(inputs):\n+            processed_input = tf.nest.map_structure(\n+                _process_single_input_t, inputs\n+            )\n+        else:\n+            processed_input = (_process_single_input_t(inputs),)\n+\n+        def _get_input_tensor(time):\n+            inp = [t_[time] for t_ in processed_input]\n+            return tf.nest.pack_sequence_as(inputs, inp)\n+\n+        if mask is not None:\n+            mask_list = tf.unstack(mask)\n+            if go_backwards:\n+                mask_list.reverse()\n+\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                mask_t = mask_list[i]\n+                output, new_states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                tiled_mask_t = _expand_mask(mask_t, output)\n+\n+                if not successive_outputs:\n+                    prev_output = tf.zeros_like(output)\n+                else:\n+                    prev_output = successive_outputs[-1]\n+\n+                output = tf.where(tiled_mask_t, output, prev_output)\n+\n+                flat_states = tf.nest.flatten(states)\n+                flat_new_states = tf.nest.flatten(new_states)\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, s) for s in flat_states\n+                )\n+                flat_final_states = tuple(\n+                    tf.where(m, s, ps)\n+                    for m, s, ps in zip(\n+                        tiled_mask_t, flat_new_states, flat_states\n+                    )\n+                )\n+                states = tf.nest.pack_sequence_as(states, flat_final_states)\n+\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = tf.stack(successive_outputs)\n+\n+            if zero_output_for_mask:\n+                last_output = tf.where(\n+                    _expand_mask(mask_list[-1], last_output),\n+                    last_output,\n+                    tf.zeros_like(last_output),\n+                )\n+                outputs = tf.where(\n+                    _expand_mask(mask, outputs, fixed_dim=2),\n+                    outputs,\n+                    tf.zeros_like(outputs),\n+                )\n+\n+        else:  # mask is None\n+            for i in range(time_steps):\n+                inp = _get_input_tensor(i)\n+                output, states = step_function(\n+                    inp, tuple(states) + tuple(constants)\n+                )\n+                if return_all_outputs:\n+                    successive_outputs.append(output)\n+                    successive_states.append(states)\n+                else:\n+                    successive_outputs = [output]\n+                    successive_states = [states]\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = tf.stack(successive_outputs)\n+\n+    else:  # Unroll == False\n+        states = tuple(initial_states)\n+\n+        # Create input tensor array, if the inputs is nested tensors, then it\n+        # will be flattened first, and tensor array will be created one per\n+        # flattened tensor.\n+        input_ta = tuple(\n+            tf.TensorArray(\n+                dtype=inp.dtype,\n+                size=time_steps_t,\n+                tensor_array_name=f\"input_ta_{i}\",\n+            )\n+            for i, inp in enumerate(flatted_inputs)\n+        )\n+        input_ta = tuple(\n+            ta.unstack(input_)\n+            if not go_backwards\n+            else ta.unstack(tf.reverse(input_, [0]))\n+            for ta, input_ in zip(input_ta, flatted_inputs)\n+        )\n+\n+        # Get the time(0) input and compute the output for that, the output will\n+        # be used to determine the dtype of output tensor array. Don't read from\n+        # input_ta due to TensorArray clear_after_read default to True.\n+        input_time_zero = tf.nest.pack_sequence_as(\n+            inputs, [inp[0] for inp in flatted_inputs]\n+        )\n+        # output_time_zero is used to determine the cell output shape and its\n+        # dtype.  the value is discarded.\n+        output_time_zero, _ = step_function(\n+            input_time_zero, tuple(initial_states) + tuple(constants)\n+        )\n+\n+        output_ta_size = time_steps_t if return_all_outputs else 1\n+        output_ta = tuple(\n+            tf.TensorArray(\n+                dtype=out.dtype,\n+                size=output_ta_size,\n+                element_shape=out.shape,\n+                tensor_array_name=f\"output_ta_{i}\",\n+            )\n+            for i, out in enumerate(tf.nest.flatten(output_time_zero))\n+        )\n+\n+        time = tf.constant(0, dtype=\"int32\", name=\"time\")\n+\n+        if input_length is None:\n+            max_iterations = time_steps_t\n+        else:\n+            max_iterations = tf.reduce_max(input_length)\n+\n+        while_loop_kwargs = {\n+            \"cond\": lambda time, *_: time < time_steps_t,\n+            \"maximum_iterations\": max_iterations,\n+            \"parallel_iterations\": 32,\n+            \"swap_memory\": True,\n+        }\n+        if mask is not None:\n+            if go_backwards:\n+                mask = tf.reverse(mask, [0])\n+\n+            mask_ta = tf.TensorArray(\n+                dtype=tf.bool, size=time_steps_t, tensor_array_name=\"mask_ta\"\n+            )\n+            mask_ta = mask_ta.unstack(mask)\n+\n+            def masking_fn(time):\n+                return mask_ta.read(time)\n+\n+            def compute_masked_output(mask_t, flat_out, flat_mask):\n+                tiled_mask_t = tuple(\n+                    _expand_mask(mask_t, o, fixed_dim=len(mask_t.shape))\n+                    for o in flat_out\n+                )\n+                return tuple(\n+                    tf.where(m, o, fm)\n+                    for m, o, fm in zip(tiled_mask_t, flat_out, flat_mask)\n+                )\n+\n+        elif isinstance(input_length, tf.Tensor):\n+            if go_backwards:\n+                max_len = tf.reduce_max(input_length, axis=0)\n+                rev_input_length = tf.subtract(max_len - 1, input_length)\n+\n+                def masking_fn(time):\n+                    return tf.less(rev_input_length, time)\n+\n+            else:\n+\n+                def masking_fn(time):\n+                    return tf.greater(input_length, time)\n+\n+            def compute_masked_output(mask_t, flat_out, flat_mask):\n+                return tuple(\n+                    tf.where(mask_t, o, zo)\n+                    for (o, zo) in zip(flat_out, flat_mask)\n+                )\n+\n+        else:\n+            masking_fn = None\n+\n+        if masking_fn is not None:\n+            # Mask for the T output will be base on the output of T - 1. In the\n+            # case T = 0, a zero filled tensor will be used.\n+            flat_zero_output = tuple(\n+                tf.zeros_like(o) for o in tf.nest.flatten(output_time_zero)\n+            )\n+\n+            def _step(time, output_ta_t, prev_output, *states):\n+                \"\"\"RNN step function.\n+\n+                Args:\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    prev_output: tuple of outputs from time - 1.\n+                    *states: List of states.\n+\n+                Returns:\n+                    Tuple: `(time + 1, output_ta_t, output) + tuple(new_states)`\n+                \"\"\"\n+                current_input = tuple(ta.read(time) for ta in input_ta)\n+                # maybe set shape.\n+                current_input = tf.nest.pack_sequence_as(inputs, current_input)\n+                mask_t = masking_fn(time)\n+                output, new_states = step_function(\n+                    current_input, tuple(states) + tuple(constants)\n+                )\n+                # mask output\n+                flat_output = tf.nest.flatten(output)\n+                flat_mask_output = (\n+                    flat_zero_output\n+                    if zero_output_for_mask\n+                    else tf.nest.flatten(prev_output)\n+                )\n+                flat_new_output = compute_masked_output(\n+                    mask_t, flat_output, flat_mask_output\n+                )\n+\n+                # mask states\n+                flat_state = tf.nest.flatten(states)\n+                flat_new_state = tf.nest.flatten(new_states)\n+                flat_final_state = compute_masked_output(\n+                    mask_t, flat_new_state, flat_state\n+                )\n+                new_states = tf.nest.pack_sequence_as(\n+                    new_states, flat_final_state\n+                )\n+\n+                ta_index_to_write = time if return_all_outputs else 0\n+                output_ta_t = tuple(\n+                    ta.write(ta_index_to_write, out)\n+                    for ta, out in zip(output_ta_t, flat_new_output)\n+                )\n+\n+                return (time + 1, output_ta_t, tuple(flat_new_output)) + tuple(\n+                    new_states\n+                )\n+\n+            final_outputs = tf.while_loop(\n+                body=_step,\n+                loop_vars=(time, output_ta, flat_zero_output) + states,\n+                **while_loop_kwargs,\n+            )\n+            # Skip final_outputs[2] which is the output for final timestep.\n+            new_states = final_outputs[3:]\n+        else:\n+\n+            def _step(time, output_ta_t, *states):\n+                \"\"\"RNN step function.\n+\n+                Args:\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    *states: List of states.\n+\n+                Returns:\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n+                current_input = tuple(ta.read(time) for ta in input_ta)\n+                current_input = tf.nest.pack_sequence_as(inputs, current_input)\n+                output, new_states = step_function(\n+                    current_input, tuple(states) + tuple(constants)\n+                )\n+                flat_new_state = tf.nest.flatten(new_states)\n+\n+                flat_output = tf.nest.flatten(output)\n+                ta_index_to_write = time if return_all_outputs else 0\n+                output_ta_t = tuple(\n+                    ta.write(ta_index_to_write, out)\n+                    for ta, out in zip(output_ta_t, flat_output)\n+                )\n+\n+                new_states = tf.nest.pack_sequence_as(\n+                    initial_states, flat_new_state\n+                )\n+                return (time + 1, output_ta_t) + tuple(new_states)\n+\n+            final_outputs = tf.while_loop(\n+                body=_step,\n+                loop_vars=(time, output_ta) + states,\n+                **while_loop_kwargs,\n+            )\n+            new_states = final_outputs[2:]\n+\n+        output_ta = final_outputs[1]\n+\n+        outputs = tuple(o.stack() for o in output_ta)\n+        last_output = tuple(o[-1] for o in outputs)\n+\n+        outputs = tf.nest.pack_sequence_as(output_time_zero, outputs)\n+        last_output = tf.nest.pack_sequence_as(output_time_zero, last_output)\n+\n+    # static shape inference\n+    def set_shape(output_):\n+        if isinstance(output_, tf.Tensor):\n+            shape = output_.shape.as_list()\n+            if return_all_outputs:\n+                shape[0] = time_steps\n+            else:\n+                shape[0] = 1\n+            shape[1] = batch\n+            output_.set_shape(shape)\n+        return output_\n+\n+    outputs = tf.nest.map_structure(set_shape, outputs)\n+\n+    if not time_major:\n+        outputs = tf.nest.map_structure(swap_batch_timestep, outputs)\n+\n+    return last_output, outputs, new_states\n\n@@ -92,3 +92,4 @@ from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras_core.layers.rnn.rnn import RNN\n\n@@ -24,6 +24,14 @@ class RandomCrop(Layer):\n     of integer or floating point dtype. By default, the layer will output\n     floats.\n \n+    **Note:** This layer wraps `tf.keras.layers.RandomCrop`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n     Input shape:\n         3D (unbatched) or 4D (batched) tensor with shape:\n         `(..., height, width, channels)`, in `\"channels_last\"` format.\n\n\n@@ -0,0 +1,446 @@\n+from tensorflow import nest\n+\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+from keras_core.saving import serialization_lib\n+from keras_core.utils import tracking\n+\n+\n+class StackedRNNCells:\n+    pass\n+\n+\n+class DropoutRNNCellMixin:\n+    pass\n+\n+\n+@keras_core_export(\"keras_core.layers.RNN\")\n+class RNN(Layer):\n+    \"\"\"Base class for recurrent layers.\n+\n+    Args:\n+        cell: A RNN cell instance or a list of RNN cell instances.\n+            A RNN cell is a class that has:\n+            - A `call(input_at_t, states_at_t)` method, returning\n+            `(output_at_t, states_at_t_plus_1)`. The call method of the\n+            cell can also take the optional argument `constants`, see\n+            section \"Note on passing external constants\" below.\n+            - A `state_size` attribute. This can be a single integer\n+            (single state) in which case it is the size of the recurrent\n+            state. This can also be a list of integers\n+            (one size per state).\n+            - A `output_size` attribute, a single integer.\n+            - A `get_initial_state(batch_size=None, dtype=None)`\n+            method that creates a tensor meant to be fed to `call()` as the\n+            initial state, if the user didn't specify any initial state\n+            via other means. The returned initial state should have\n+            shape `(batch_size, cell.state_size)`.\n+            The cell might choose to create a tensor full of zeros,\n+            or other values based on the cell's implementation.\n+            `inputs` is the input tensor to the RNN layer, with shape\n+            `(batch_size, timesteps, features)`.\n+            If this method is not implemented\n+            by the cell, the RNN layer will create a zero filled tensor\n+            with shape `(batch_size, cell.state_size)`.\n+            In the case that `cell` is a list of RNN cell instances, the cells\n+            will be stacked on top of each other in the RNN, resulting in an\n+            efficient stacked RNN.\n+        return_sequences: Boolean (default `False`). Whether to return the last\n+            output in the output sequence, or the full sequence.\n+        return_state: Boolean (default `False`).\n+            Whether to return the last state in addition to the output.\n+        go_backwards: Boolean (default `False`).\n+            If `True`, process the input sequence backwards and return the\n+            reversed sequence.\n+        stateful: Boolean (default `False`). If True, the last state\n+            for each sample at index `i` in a batch will be used as initial\n+            state for the sample of index `i` in the following batch.\n+        unroll: Boolean (default `False`).\n+            If True, the network will be unrolled, else a symbolic loop will be\n+            used. Unrolling can speed-up a RNN, although it tends to be more\n+            memory-intensive. Unrolling is only suitable for short sequences.\n+        zero_output_for_mask: Boolean (default `False`).\n+            Whether the output should use zeros for the masked timesteps.\n+            Note that this field is only used when `return_sequences`\n+            is `True` and `mask` is provided.\n+            It can useful if you want to reuse the raw output sequence of\n+            the RNN without interference from the masked timesteps, e.g.,\n+            merging bidirectional RNNs.\n+\n+    Call arguments:\n+        inputs: Input tensor.\n+        initial_state: List of initial state tensors to be passed to the first\n+            call of the cell.\n+        mask: Binary tensor of shape `[batch_size, timesteps]`\n+            indicating whether a given timestep should be masked.\n+            An individual `True` entry indicates that the corresponding\n+            timestep should be utilized, while a `False` entry indicates\n+            that the corresponding timestep should be ignored.\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode or in inference mode. This argument is passed\n+            to the cell when calling it.\n+            This is for use with cells that use dropout.\n+\n+    Input shape:\n+        N-D tensor with shape `[batch_size, timesteps, ...]` or\n+        `[timesteps, batch_size, ...]` when time_major is True.\n+\n+    Output shape:\n+\n+    - If `return_state`: a list of tensors. The first tensor is\n+    the output. The remaining tensors are the last states,\n+    each with shape `[batch_size, state_size]`, where `state_size` could\n+    be a high dimension tensor shape.\n+    - If `return_sequences`: N-D tensor with shape\n+    `[batch_size, timesteps, output_size]`, where `output_size` could\n+    be a high dimension tensor shape, or\n+    `[timesteps, batch_size, output_size]` when `time_major` is True.\n+    - Else, N-D tensor with shape `[batch_size, output_size]`, where\n+    `output_size` could be a high dimension tensor shape.\n+\n+    Masking:\n+\n+    This layer supports masking for input data with a variable number\n+    of timesteps. To introduce masks to your data,\n+    use a `keras_core.layers.Embedding` layer with the `mask_zero` parameter\n+    set to `True`.\n+\n+    Note on using statefulness in RNNs:\n+\n+    You can set RNN layers to be 'stateful', which means that the states\n+    computed for the samples in one batch will be reused as initial states\n+    for the samples in the next batch. This assumes a one-to-one mapping\n+    between samples in different successive batches.\n+\n+    To enable statefulness:\n+\n+    - Specify `stateful=True` in the layer constructor.\n+    - Specify a fixed batch size for your model, by passing\n+    If sequential model:\n+        `batch_input_shape=(...)` to the first layer in your model.\n+    Else for functional model with 1 or more Input layers:\n+        `batch_shape=(...)` to all the first layers in your model.\n+    This is the expected shape of your inputs\n+    *including the batch size*.\n+    It should be a tuple of integers, e.g. `(32, 10, 100)`.\n+    - Specify `shuffle=False` when calling `fit()`.\n+\n+    To reset the states of your model, call `.reset_states()` on either\n+    a specific layer, or on your entire model.\n+\n+    Note on specifying the initial state of RNNs:\n+\n+    You can specify the initial state of RNN layers symbolically by\n+    calling them with the keyword argument `initial_state`. The value of\n+    `initial_state` should be a tensor or list of tensors representing\n+    the initial state of the RNN layer.\n+\n+    You can specify the initial state of RNN layers numerically by\n+    calling `reset_states` with the keyword argument `states`. The value of\n+    `states` should be a numpy array or list of numpy arrays representing\n+    the initial state of the RNN layer.\n+\n+    Examples:\n+\n+    ```python\n+    from keras_core.layers import RNN\n+    from keras_core import operations as ops\n+\n+    # First, let's define a RNN Cell, as a layer subclass.\n+    class MinimalRNNCell(keras_core.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(MinimalRNNCell, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n+                                          initializer='uniform',\n+                                          name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states):\n+            prev_output = states[0]\n+            h = ops.matmul(inputs, self.kernel)\n+            output = h + ops.matmul(prev_output, self.recurrent_kernel)\n+            return output, [output]\n+\n+    # Let's use this cell in a RNN layer:\n+\n+    cell = MinimalRNNCell(32)\n+    x = keras_core.Input((None, 5))\n+    layer = RNN(cell)\n+    y = layer(x)\n+\n+    # Here's how to use the cell to build a stacked RNN:\n+\n+    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n+    x = keras_core.Input((None, 5))\n+    layer = RNN(cells)\n+    y = layer(x)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        cell,\n+        return_sequences=False,\n+        return_state=False,\n+        go_backwards=False,\n+        stateful=False,\n+        unroll=False,\n+        **kwargs,\n+    ):\n+        if isinstance(cell, (list, tuple)):\n+            cell = StackedRNNCells(cell)\n+        if \"call\" not in dir(cell):\n+            raise ValueError(\n+                \"Argument `cell` should have a `call` method. \"\n+                f\"Received: cell={cell}\"\n+            )\n+        if \"state_size\" not in dir(cell):\n+            raise ValueError(\n+                \"The RNN cell should have a `state_size` attribute \"\n+                \"(single integer or list of integers, \"\n+                \"one integer per RNN state). \"\n+                f\"Received: cell={cell}\"\n+            )\n+        # If True, the output for masked timestep will be zeros, whereas in the\n+        # False case, output from previous timestep is returned for masked\n+        # timestep.\n+        self.zero_output_for_mask = kwargs.pop(\"zero_output_for_mask\", False)\n+\n+        super().__init__(**kwargs)\n+        self.cell = cell\n+        self.return_sequences = return_sequences\n+        self.return_state = return_state\n+        self.go_backwards = go_backwards\n+        self.stateful = stateful\n+        self.unroll = unroll\n+\n+        self.supports_masking = True\n+        self.input_spec = None\n+        self.states = None\n+\n+        state_size = getattr(self.cell, \"state_size\", None)\n+        if state_size is None:\n+            raise ValueError(\n+                \"state_size must be specified as property on the RNN cell.\"\n+            )\n+        if not isinstance(state_size, (list, int)):\n+            raise ValueError(\n+                \"state_size must be an integer, or a list of integers \"\n+                \"(one for each state tensor).\"\n+            )\n+        if isinstance(state_size, int):\n+            self.state_size = [state_size]\n+            self.single_state = True\n+        else:\n+            self.state_size = state_size\n+            self.single_state = False\n+\n+    def compute_output_shape(self, sequence_shape, initial_state_shape=None):\n+        if self.single_state:\n+            # Single state case.\n+            state_shape = (sequence_shape[0], self.state_size[0])\n+        else:\n+            # Multiple states case.\n+            state_shape = [(sequence_shape[0], d) for d in self.state_size]\n+\n+        # There can only be one output.\n+        output_size = getattr(self.cell, \"output_size\", None)\n+        if output_size is None:\n+            output_size = self.state_size[0]\n+        if not isinstance(output_size, int):\n+            raise ValueError(\"output_size must be an integer.\")\n+        if self.return_sequences:\n+            output_shape = (sequence_shape[0], sequence_shape[1], output_size)\n+        else:\n+            output_shape = (sequence_shape[0], output_size)\n+        if self.return_state:\n+            return output_shape, state_shape\n+        return output_shape\n+\n+    def compute_mask(self, _, mask):\n+        # Time step masks must be the same for each input.\n+        # This is because the mask for an RNN is of size [batch, time_steps, 1],\n+        # and specifies which time steps should be skipped, and a time step\n+        # must be skipped for all inputs.\n+        mask = nest.flatten(mask)[0]\n+        output_mask = mask if self.return_sequences else None\n+        if self.return_state:\n+            state_mask = [None for _ in self.state_size]\n+            return [output_mask] + state_mask\n+        else:\n+            return output_mask\n+\n+    def build(self, sequence_shape, initial_state_shape=None):\n+        # Build cell (if layer).\n+        step_input_shape = (sequence_shape[0],) + sequence_shape[2:]\n+        if isinstance(self.cell, Layer) and not self.cell.built:\n+            self.cell.build(step_input_shape)\n+            self.cell.built = True\n+        if self.stateful:\n+            if self.states is not None:\n+                self.reset_state()\n+            else:\n+                if sequence_shape[0] is None:\n+                    raise ValueError(\n+                        \"When using `stateful=True` in a RNN, the \"\n+                        \"batch size must be static. Found dynamic \"\n+                        f\"batch size: sequence.shape={sequence_shape}\"\n+                    )\n+                self._create_state_variables(sequence_shape[0])\n+        self.built = True\n+\n+    @tracking.no_automatic_dependency_tracking\n+    def _create_state_variables(self, batch_size):\n+        self.states = [\n+            self.add_variable(\n+                initializer=\"zeros\", shape=(batch_size, d), trainable=False\n+            )\n+            for d in self.state_size\n+        ]\n+\n+    def get_initial_state(self, sequence):\n+        input_shape = ops.shape(sequence)\n+        batch_size = input_shape[0]\n+        dtype = sequence.dtype\n+\n+        get_initial_state_fn = getattr(self.cell, \"get_initial_state\", None)\n+        if get_initial_state_fn:\n+            init_state = get_initial_state_fn(\n+                batch_size=batch_size, dtype=dtype\n+            )\n+        else:\n+            return [\n+                ops.zeros((batch_size, d), dtype=dtype) for d in self.state_size\n+            ]\n+\n+        # RNN expect the states in a list, even if single state.\n+        if not nest.is_nested(init_state):\n+            init_state = [init_state]\n+        # Force the state to be a list in case it is a namedtuple eg\n+        # LSTMStateTuple.\n+        return list(init_state)\n+\n+    def reset_states(self):\n+        # Compatibility alias.\n+        self.reset_state()\n+\n+    def reset_state(self):\n+        if self.states is not None:\n+            for v in self.states:\n+                v.assign(ops.zeros_like(v))\n+\n+    def call(\n+        self,\n+        sequence,\n+        initial_state=None,\n+        mask=None,\n+        training=False,\n+    ):\n+        self._maybe_reset_cell_dropout_mask(self.cell)\n+\n+        timesteps = sequence.shape[1]\n+        if self.unroll and timesteps is None:\n+            raise ValueError(\n+                \"Cannot unroll a RNN if the \"\n+                \"time dimension is undefined. \\n\"\n+                \"- If using a Sequential model, \"\n+                \"specify the time dimension by passing \"\n+                \"an `input_shape` or `batch_input_shape` \"\n+                \"argument to your first layer. If your \"\n+                \"first layer is an Embedding, you can \"\n+                \"also use the `input_length` argument.\\n\"\n+                \"- If using the functional API, specify \"\n+                \"the time dimension by passing a `shape` \"\n+                \"or `batch_shape` argument to your Input layer.\"\n+            )\n+\n+        cell_kwargs = {}\n+        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg():\n+            cell_kwargs[\"training\"] = training\n+        if initial_state is None:\n+            if self.stateful:\n+                initial_state = self.states\n+            else:\n+                initial_state = self.get_initial_state(sequence)\n+        else:\n+            # RNN expect the states in a list, even if single state.\n+            if not nest.is_nested(initial_state):\n+                initial_state = [initial_state]\n+            initial_state = list(initial_state)\n+\n+        def step(inputs, states):\n+            output, new_states = self.cell(inputs, states, **cell_kwargs)\n+            if not nest.is_nested(new_states):\n+                new_states = [new_states]\n+            return output, new_states\n+\n+        last_output, outputs, states = backend.nn.rnn(\n+            step,\n+            sequence,\n+            initial_state,\n+            go_backwards=self.go_backwards,\n+            mask=mask,\n+            unroll=self.unroll,\n+            input_length=timesteps,\n+            zero_output_for_mask=self.zero_output_for_mask,\n+            return_all_outputs=self.return_sequences,\n+        )\n+        assert isinstance(states, (list, tuple))\n+\n+        if self.stateful:\n+            for self_state, state in zip(\n+                nest.flatten(self.states), nest.flatten(states)\n+            ):\n+                self_state.assign(state)\n+\n+        if self.return_sequences:\n+            output = outputs\n+        else:\n+            output = last_output\n+\n+        if self.return_state:\n+            if len(states) == 1:\n+                state = states[0]\n+                return output, state\n+            return output, *states\n+        return output\n+\n+    def _maybe_reset_cell_dropout_mask(self, cell):\n+        if isinstance(cell, DropoutRNNCellMixin):\n+            cell.reset_dropout_mask()\n+            cell.reset_recurrent_dropout_mask()\n+        if isinstance(self.cell, StackedRNNCells):\n+            for cell in self.cell.cells:\n+                self._maybe_reset_cell_dropout_mask(cell)\n+\n+    def get_config(self):\n+        config = {\n+            \"return_sequences\": self.return_sequences,\n+            \"return_state\": self.return_state,\n+            \"go_backwards\": self.go_backwards,\n+            \"stateful\": self.stateful,\n+            \"unroll\": self.unroll,\n+            \"zero_output_for_mask\": self.zero_output_for_mask,\n+        }\n+        config[\"cell\"] = serialization_lib.serialize_keras_object(self.cell)\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        cell = serialization_lib.deserialize_keras_object(\n+            config.pop(\"cell\"), custom_objects=custom_objects\n+        )\n+        layer = cls(cell, **config)\n+        return layer\n\n@@ -0,0 +1,239 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class OneStateRNNCell(layers.Layer):\n+    def __init__(self, units, **kwargs):\n+        self.units = units\n+        self.state_size = units\n+        super().__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        self.kernel = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"ones\",\n+            name=\"kernel\",\n+        )\n+        self.recurrent_kernel = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer=\"ones\",\n+            name=\"recurrent_kernel\",\n+        )\n+        self.built = True\n+\n+    def call(self, inputs, states):\n+        prev_output = states[0]\n+        h = ops.matmul(inputs, self.kernel)\n+        output = h + ops.matmul(prev_output, self.recurrent_kernel)\n+        return output, [output]\n+\n+\n+class TwoStatesRNNCell(layers.Layer):\n+    def __init__(self, units, **kwargs):\n+        self.units = units\n+        self.state_size = [units, units]\n+        self.output_size = units\n+        super().__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        self.kernel = self.add_weight(\n+            shape=(input_shape[-1], self.units),\n+            initializer=\"ones\",\n+            name=\"kernel\",\n+        )\n+        self.recurrent_kernel_1 = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer=\"ones\",\n+            name=\"recurrent_kernel_1\",\n+        )\n+        self.recurrent_kernel_2 = self.add_weight(\n+            shape=(self.units, self.units),\n+            initializer=\"ones\",\n+            name=\"recurrent_kernel_2\",\n+        )\n+        self.built = True\n+\n+    def call(self, inputs, states):\n+        prev_1 = states[0]\n+        prev_2 = states[0]\n+        h = ops.matmul(inputs, self.kernel)\n+        output_1 = h + ops.matmul(prev_1, self.recurrent_kernel_1)\n+        output_2 = h + ops.matmul(prev_2, self.recurrent_kernel_2)\n+        output = output_1 + output_2\n+        return output, [output_1, output_2]\n+\n+\n+class RNNTest(testing.TestCase):\n+    def test_compute_output_shape_single_state(self):\n+        sequence = np.ones((3, 4, 5))\n+        layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 8))\n+\n+        layer = layers.RNN(OneStateRNNCell(8), return_sequences=True)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 4, 8))\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(8), return_sequences=False, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 8))\n+        self.assertEqual(output_shape[1], (3, 8))\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(8), return_sequences=True, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 4, 8))\n+        self.assertEqual(output_shape[1], (3, 8))\n+\n+    def test_compute_output_shape_two_states(self):\n+        sequence = np.ones((3, 4, 5))\n+        layer = layers.RNN(TwoStatesRNNCell(8), return_sequences=False)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 8))\n+\n+        layer = layers.RNN(TwoStatesRNNCell(8), return_sequences=True)\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape, (3, 4, 8))\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(8), return_sequences=False, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 8))\n+        self.assertEqual(output_shape[1][0], (3, 8))\n+        self.assertEqual(output_shape[1][1], (3, 8))\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(8), return_sequences=True, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence.shape)\n+        self.assertEqual(output_shape[0], (3, 4, 8))\n+        self.assertEqual(output_shape[1][0], (3, 8))\n+        self.assertEqual(output_shape[1][1], (3, 8))\n+\n+    def test_forward_pass_single_state(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=False)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), output)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=True)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[[3.0, 3.0], [9.0, 9.0]]]), output)\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state = layer(sequence)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state)\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(2), return_sequences=True, return_state=True\n+        )\n+        output, state = layer(sequence)\n+        self.assertAllClose(np.array([[[3.0, 3.0], [9.0, 9.0]]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state)\n+\n+    def test_forward_pass_two_states(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[18.0, 18.0]]), output)\n+\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=True)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[[6.0, 6.0], [18.0, 18.0]]]), output)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state1, state2 = layer(sequence)\n+        self.assertAllClose(np.array([[18.0, 18.0]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state1)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state2)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), return_sequences=True, return_state=True\n+        )\n+        output, state1, state2 = layer(sequence)\n+        self.assertAllClose(np.array([[[6.0, 6.0], [18.0, 18.0]]]), output)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state1)\n+        self.assertAllClose(np.array([[9.0, 9.0]]), state2)\n+\n+    def test_passing_initial_state_single_state(self):\n+        sequence = np.ones((2, 3, 2))\n+        state = np.ones((2, 2))\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=False)\n+        output = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), output)\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), output)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), state)\n+\n+    def test_passing_initial_state_two_states(self):\n+        sequence = np.ones((2, 3, 2))\n+        state = [np.ones((2, 2)), np.ones((2, 2))]\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n+        output = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[44.0, 44.0], [44.0, 44.0]]), output)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), return_sequences=False, return_state=True\n+        )\n+        output, state_1, state_2 = layer(sequence, initial_state=state)\n+        self.assertAllClose(np.array([[44.0, 44.0], [44.0, 44.0]]), output)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), state_1)\n+        self.assertAllClose(np.array([[22.0, 22.0], [22.0, 22.0]]), state_2)\n+\n+    def test_statefulness_single_state(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(OneStateRNNCell(2), stateful=True)\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), output)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), stateful=True, return_state=True)\n+        layer(sequence)\n+        output, state = layer(sequence)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), output)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), state)\n+\n+    def test_statefulness_two_states(self):\n+        sequence = np.ones((1, 2, 3))\n+        layer = layers.RNN(TwoStatesRNNCell(2), stateful=True)\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[90.0, 90.0]]), output)\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(2), stateful=True, return_state=True\n+        )\n+        layer(sequence)\n+        output, state_1, state_2 = layer(sequence)\n+        self.assertAllClose(np.array([[90.0, 90.0]]), output)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), state_1)\n+        self.assertAllClose(np.array([[45.0, 45.0]]), state_2)\n+\n+    def test_serialization(self):\n+        layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n+        self.run_class_serialization_test(layer)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), return_sequences=False)\n+        self.run_class_serialization_test(layer)\n+\n+    # def test_dropout(self):\n+    #     pass\n+\n+    # def test_cell_stacking(self):\n+    #     pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13039c01b05ee23e27d24d6a6ae7c0cb562c0f2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 723 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,7 @@\n import numpy as np\n+import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import operations as ops\n from keras_core import testing\n@@ -66,6 +68,10 @@ class TwoStatesRNNCell(layers.Layer):\n         return output, [output_1, output_2]\n \n \n+@pytest.mark.skipif(\n+    backend.backend() != \"tensorflow\",\n+    reason=\"Only implemented for TF for now.\",\n+)\n class RNNTest(testing.TestCase):\n     def test_compute_output_shape_single_state(self):\n         sequence = np.ones((3, 4, 5))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
