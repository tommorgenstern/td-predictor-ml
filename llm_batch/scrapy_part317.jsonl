{"custom_id": "scrapy#db10aaf9eb858c8deb35e7ef96538549a8e36be0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 261 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -283,6 +283,7 @@ coverage_ignore_pyobjects = [\n intersphinx_mapping = {\n     'attrs': ('https://www.attrs.org/en/stable/', None),\n     'coverage': ('https://coverage.readthedocs.io/en/stable', None),\n+    'cryptography' : ('https://cryptography.io/en/latest/', None),\n     'cssselect': ('https://cssselect.readthedocs.io/en/latest', None),\n     'itemloaders': ('https://itemloaders.readthedocs.io/en/latest/', None),\n     'pytest': ('https://docs.pytest.org/en/latest', None),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b83a1a6fbfe12c205c7c57830b1d4a6fc1097be0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 500 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,10 @@ Tests borrowed from the twisted.web.client tests.\n \"\"\"\n import os\n import shutil\n+import sys\n+from pkg_resources import parse_version\n \n+import cryptography\n import OpenSSL.SSL\n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n@@ -414,6 +417,8 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n+        if sys.implementation.name == \"pypy\" and parse_version(cryptography.__version__) <= parse_version(\"2.3.1\"):\n+            self.skipTest(\"This does work in PyPy with cryptography<=2.3.1\")\n         s = \"0123456789\" * 10\n         settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'ECDHE-RSA-AES256-GCM-SHA384'})\n         client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6dccf82eaa097f9e40e922d04a8b5ec9a0a9dbd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1038 | Contributors (this commit): 32 | Commits (past 90d): 1 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -160,7 +160,7 @@ def _select_value(ele, n, v):\n     multiple = ele.multiple\n     if v is None and not multiple:\n         # Match browser behaviour on simple select tag without options selected\n-        # And for select tags wihout options\n+        # And for select tags without options\n         o = ele.value_options\n         return (n, o[0]) if o else (None, None)\n     elif v is not None and multiple:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44a7ab5bf06be3bfdf4b4a304b24f544d2f833cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 678 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -86,7 +86,7 @@ class MediaPipeline:\n         info = self.spiderinfo\n         requests = arg_to_iter(self.get_media_requests(item, info))\n         dlist = [self._process_request(r, info, item) for r in requests]\n-        dfd = DeferredList(dlist, consumeErrors=1)\n+        dfd = DeferredList(dlist, consumeErrors=True)\n         return dfd.addCallback(self.item_completed, item, info)\n \n     def _process_request(self, request, info, item):\n\n@@ -105,7 +105,7 @@ def process_parallel(callbacks, input, *a, **kw):\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n-    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n+    d = defer.DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n     d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n     return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e494a3f73318db76d7c56e65bc632cd5875b8825", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1485 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from twisted.web.client import ResponseFailed\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n@@ -458,6 +459,7 @@ class Stream:\n             request=self._request,\n             certificate=self._protocol.metadata['certificate'],\n             ip_address=self._protocol.metadata['ip_address'],\n+            protocol=to_unicode(self._protocol.transport.negotiatedProtocol),\n         )\n \n         self._deferred_response.callback(response)\n\n@@ -22,6 +22,13 @@ class Https2TestCase(Https11TestCase):\n     download_handler_cls = H2DownloadHandler\n     HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n \n+    def test_protocol(self):\n+        request = Request(self.getURL(\"host\"), method=\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.protocol)\n+        d.addCallback(self.assertEqual, \"h2\")\n+        return d\n+\n     @defer.inlineCallbacks\n     def test_download_with_maxsize_very_large_file(self):\n         with mock.patch('scrapy.core.http2.stream.logger') as logger:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f30f53b3cc958770406628411db3d93c925db59e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 2238 | Contributors (this commit): 29 | Commits (past 90d): 3 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -303,11 +303,14 @@ class ScrapyAgent:\n             proxyHost = to_unicode(proxyHost)\n             omitConnectTunnel = b'noconnect' in proxyParams\n             if omitConnectTunnel:\n-                warnings.warn(\"Using HTTPS proxies in the noconnect mode is deprecated. \"\n-                              \"If you use Crawlera, it doesn't require this mode anymore, \"\n-                              \"so you should update scrapy-crawlera to 1.3.0+ \"\n-                              \"and remove '?noconnect' from the Crawlera URL.\",\n-                              ScrapyDeprecationWarning)\n+                warnings.warn(\n+                    \"Using HTTPS proxies in the noconnect mode is deprecated. \"\n+                    \"If you use Zyte Smart Proxy Manager (formerly Crawlera), \"\n+                    \"it doesn't require this mode anymore, so you should \"\n+                    \"update scrapy-crawlera to 1.3.0+ and remove '?noconnect' \"\n+                    \"from the Zyte Smart Proxy Manager URL.\",\n+                    ScrapyDeprecationWarning,\n+                )\n             if scheme == b'https' and not omitConnectTunnel:\n                 proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n                 proxyConf = (proxyHost, proxyPort, proxyAuth)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1024566d85e52f71f5591e53a3a9ee4867148ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 601 | Contributors (this commit): 27 | Commits (past 90d): 1 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ def has_environment_marker_platform_impl_support():\n \n \n install_requires = [\n-    'Twisted>=17.9.0',\n+    'Twisted[http2]>=17.9.0',\n     'cryptography>=2.0',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#536e749eccbdf479c9849b1868a5d95aa1210225", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 1363 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -218,7 +218,6 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.setTimeout(self.IDLE_TIMEOUT)\n \n         destination = self.transport.getPeer()\n-        logger.debug('Connection made to {}'.format(destination))\n         self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n \n         # Initiate H2 Connection\n@@ -347,7 +346,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             elif isinstance(event, SettingsAcknowledged):\n                 self.settings_acknowledged(event)\n             elif isinstance(event, UnknownFrameReceived):\n-                logger.debug('UnknownFrameReceived: frame={}'.format(event.frame))\n+                logger.warning(f'Unknown frame received: {event.frame}')\n \n     # Event handler functions starts here\n     def connection_terminated(self, event: ConnectionTerminated) -> None:\n@@ -357,15 +356,19 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n     def data_received(self, event: DataReceived) -> None:\n         try:\n-            self.streams[event.stream_id].receive_data(event.data, event.flow_controlled_length)\n+            stream = self.streams[event.stream_id]\n         except KeyError:\n-            logger.debug(f'Ignoring server-initiated event {event}')\n+            pass  # We ignore server-initiated events\n+        else:\n+            stream.receive_data(event.data, event.flow_controlled_length)\n \n     def response_received(self, event: ResponseReceived) -> None:\n         try:\n-            self.streams[event.stream_id].receive_headers(event.headers)\n+            stream = self.streams[event.stream_id]\n         except KeyError:\n-            logger.debug(f'Ignoring server-initiated event {event}')\n+            pass  # We ignore server-initiated events\n+        else:\n+            stream.receive_headers(event.headers)\n \n     def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n         self.metadata['settings_acknowledged'] = True\n@@ -381,7 +384,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         try:\n             stream = self.pop_stream(event.stream_id)\n         except KeyError:\n-            logger.debug(f'Ignoring server-initiated event {event}')\n+            pass  # We ignore server-initiated events\n         else:\n             stream.close(StreamCloseReason.ENDED, from_protocol=True)\n \n@@ -389,7 +392,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         try:\n             stream = self.pop_stream(event.stream_id)\n         except KeyError:\n-            logger.debug(f'Ignoring server-initiated event {event}')\n+            pass  # We ignore server-initiated events\n         else:\n             stream.close(StreamCloseReason.RESET, from_protocol=True)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1773eaf5dc2330464cf769b7d874f24033f69707", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 266 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -122,7 +122,6 @@ html_theme = 'sphinx_rtd_theme'\n import sphinx_rtd_theme\n html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n \n-\n # The style sheet to use for HTML and HTML Help pages. A file of that name\n # must exist either in Sphinx' static/ path, or in one of the custom paths\n # given in html_static_path.\n@@ -183,6 +182,10 @@ html_copy_source = True\n # Output file base name for HTML help builder.\n htmlhelp_basename = 'Scrapydoc'\n \n+html_css_files = [\n+    'custom.css',\n+]\n+\n \n # Options for LaTeX output\n # ------------------------\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#248800328cb32f79a214289d59e927b576e21712", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n         )\n         config_parser = ConfigParser()\n         config_parser.read(tox_config_file_path)\n-        pattern = r'Twisted==([\\d.]+)'\n+        pattern = r'Twisted\\[http2\\]==([\\d.]+)'\n         match = re.search(pattern, config_parser['pinned']['deps'])\n         pinned_twisted_version_string = match[1]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e4b291701baa8b74bd13bd47831dd040dd7173f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1475 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -459,7 +459,7 @@ class Stream:\n             request=self._request,\n             certificate=self._protocol.metadata['certificate'],\n             ip_address=self._protocol.metadata['ip_address'],\n-            protocol=to_unicode(self._protocol.transport.negotiatedProtocol),\n+            protocol='h2',\n         )\n \n         self._deferred_response.callback(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76abcedaf4f31a7c76de9e19680dd7499d9eccf4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 32 | Churn Cumulative: 37 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,17 @@\n+import collections\n+\n+\n async def collect_asyncgen(result):\n     results = []\n     async for x in result:\n         results.append(x)\n     return results\n+\n+\n+async def as_async_generator(it):\n+    if isinstance(it, collections.abc.AsyncIterator):\n+        async for r in it:\n+            yield r\n+    else:\n+        for r in it:\n+            yield r\n\n@@ -0,0 +1,20 @@\n+from twisted.trial import unittest\n+\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+\n+\n+class AsyncgenUtilsTest(unittest.TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_as_async_generator(self):\n+        ag = as_async_generator(range(42))\n+        results = []\n+        async for i in ag:\n+            results.append(i)\n+        self.assertEqual(results, list(range(42)))\n+\n+    @deferred_f_from_coro_f\n+    async def test_collect_asyncgen(self):\n+        ag = as_async_generator(range(42))\n+        results = await collect_asyncgen(ag)\n+        self.assertEqual(results, list(range(42)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#acff1eb4960940eb360f3cf00d5b33ed71acc351", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 45 | Churn Cumulative: 461 | Contributors (this commit): 22 | Commits (past 90d): 3 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,6 +124,20 @@ def iter_errback(iterable, errback, *a, **kw):\n             errback(failure.Failure(), *a, **kw)\n \n \n+async def aiter_errback(aiterable, errback, *a, **kw):\n+    \"\"\"Wraps an async iterable calling an errback if an error is caught while\n+    iterating it. Similar to scrapy.utils.defer.iter_errback()\n+    \"\"\"\n+    it = aiterable.__aiter__()\n+    while True:\n+        try:\n+            yield await it.__anext__()\n+        except StopAsyncIteration:\n+            break\n+        except Exception:\n+            errback(failure.Failure(), *a, **kw)\n+\n+\n def deferred_from_coro(o):\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, defer.Deferred):\n\n@@ -2,12 +2,15 @@ from twisted.trial import unittest\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n+from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.defer import (\n     iter_errback,\n+    aiter_errback,\n     mustbe_deferred,\n     process_chain,\n     process_chain_both,\n     process_parallel,\n+    deferred_f_from_coro_f,\n )\n \n \n@@ -117,3 +120,31 @@ class IterErrbackTest(unittest.TestCase):\n         self.assertEqual(out, [0, 1, 2, 3, 4])\n         self.assertEqual(len(errors), 1)\n         self.assertIsInstance(errors[0].value, ZeroDivisionError)\n+\n+\n+class AiterErrbackTest(unittest.TestCase):\n+\n+    @deferred_f_from_coro_f\n+    async def test_aiter_errback_good(self):\n+        async def itergood():\n+            for x in range(10):\n+                yield x\n+\n+        errors = []\n+        out = await collect_asyncgen(aiter_errback(itergood(), errors.append))\n+        self.assertEqual(out, list(range(10)))\n+        self.assertFalse(errors)\n+\n+    @deferred_f_from_coro_f\n+    async def test_iter_errback_bad(self):\n+        async def iterbad():\n+            for x in range(10):\n+                if x == 5:\n+                    1 / 0\n+                yield x\n+\n+        errors = []\n+        out = await collect_asyncgen(aiter_errback(iterbad(), errors.append))\n+        self.assertEqual(out, [0, 1, 2, 3, 4])\n+        self.assertEqual(len(errors), 1)\n+        self.assertIsInstance(errors[0].value, ZeroDivisionError)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e9f498e00fd36c76ac139eb285526c9e70b4054", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 19/12 | Churn Δ: 83 | Churn Cumulative: 1427 | Contributors (this commit): 27 | Commits (past 90d): 4 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from functools import partial, wraps\n from itertools import chain\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.asyncgen import as_async_generator\n from scrapy.utils.decorators import deprecated\n \n \n@@ -355,3 +356,27 @@ class MutableChain:\n     @deprecated(\"scrapy.utils.python.MutableChain.__next__\")\n     def next(self):\n         return self.__next__()\n+\n+\n+async def _async_chain(*iterables):\n+    for it in iterables:\n+        async for o in as_async_generator(it):\n+            yield o\n+\n+\n+class MutableAsyncChain:\n+    \"\"\"\n+    Similar to MutableChain but for async iterables\n+    \"\"\"\n+\n+    def __init__(self, *args):\n+        self.data = _async_chain(*args)\n+\n+    def extend(self, *aiterables):\n+        self.data = _async_chain(self.data, _async_chain(*aiterables))\n+\n+    def __aiter__(self):\n+        return self\n+\n+    async def __anext__(self):\n+        return await self.data.__anext__()\n\n@@ -2,15 +2,18 @@ import functools\n import gc\n import operator\n import platform\n-import unittest\n from datetime import datetime\n from itertools import count\n from warnings import catch_warnings\n \n+from twisted.trial import unittest\n+\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n+from scrapy.utils.defer import deferred_f_from_coro_f, aiter_errback\n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n     WeakKeyCache, get_func_args, to_bytes, to_unicode,\n-    without_none_values, MutableChain)\n+    without_none_values, MutableChain, MutableAsyncChain)\n \n \n __doctests__ = ['scrapy.utils.python']\n@@ -32,6 +35,57 @@ class MutableChainTest(unittest.TestCase):\n         self.assertEqual(list(m), list(range(3, 13)))\n \n \n+class MutableAsyncChainTest(unittest.TestCase):\n+    @staticmethod\n+    async def g1():\n+        for i in range(3):\n+            yield i\n+\n+    @staticmethod\n+    async def g2():\n+        return\n+        yield\n+\n+    @staticmethod\n+    async def g3():\n+        for i in range(7, 10):\n+            yield i\n+\n+    @staticmethod\n+    async def g4():\n+        for i in range(3, 5):\n+            yield i\n+        1 / 0\n+        for i in range(5, 7):\n+            yield i\n+\n+    @staticmethod\n+    async def collect_asyncgen_exc(asyncgen):\n+        results = []\n+        async for x in asyncgen:\n+            results.append(x)\n+        return results\n+\n+    @deferred_f_from_coro_f\n+    async def test_mutableasyncchain(self):\n+        m = MutableAsyncChain(self.g1(), as_async_generator(range(3, 7)))\n+        m.extend(self.g2())\n+        m.extend(self.g3())\n+\n+        self.assertEqual(await m.__anext__(), 0)\n+        results = await collect_asyncgen(m)\n+        self.assertEqual(results, list(range(1, 10)))\n+\n+    @deferred_f_from_coro_f\n+    async def test_mutableasyncchain_exc(self):\n+        m = MutableAsyncChain(self.g1())\n+        m.extend(self.g4())\n+        m.extend(self.g3())\n+\n+        results = await collect_asyncgen(aiter_errback(m, lambda _: None))\n+        self.assertEqual(results, list(range(5)))\n+\n+\n class ToUnicodeTest(unittest.TestCase):\n     def test_converting_an_utf8_encoded_string_to_unicode(self):\n         self.assertEqual(to_unicode(b'lel\\xc3\\xb1e'), 'lel\\xf1e')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d658552f232547cc227e597b0d8489c4762eb9d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 6 | Churn Cumulative: 185 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,5 +55,11 @@ def only_asyncio(request, reactor_pytest):\n         pytest.skip('This test is only run with --reactor=asyncio')\n \n \n+@pytest.fixture(autouse=True)\n+def only_not_asyncio(request, reactor_pytest):\n+    if request.node.get_closest_marker('only_not_asyncio') and reactor_pytest == 'asyncio':\n+        pytest.skip('This test is only run without --reactor=asyncio')\n+\n+\n # Generate localhost certificate files, needed by some tests\n generate_keys()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d66d52d3ed8aab0b4f126169c2855d00f4053907", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 14 | Complexity Δ (Sum/Max): 27/16 | Churn Δ: 122 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.7721518987341772\n\nDIFF:\n@@ -0,0 +1,35 @@\n+# coding: utf-8\n+import inspect\n+\n+\n+def process_normal_iterable_helper(it, in_predicate=None, out_predicate=None, processor=None):\n+    for o in it:\n+        if in_predicate and not in_predicate(o):\n+            continue\n+        if processor is not None:\n+            o = processor(o)\n+        if out_predicate and not out_predicate(o):\n+            continue\n+        yield o\n+\n+\n+async def process_async_iterable_helper(it, in_predicate=None, out_predicate=None, processor=None):\n+    async for o in it:\n+        if in_predicate and not in_predicate(o):\n+            continue\n+        if processor is not None:\n+            o = processor(o)\n+        if out_predicate and not out_predicate(o):\n+            continue\n+        yield o\n+\n+\n+def process_iterable_helper(it, in_predicate=None, out_predicate=None, processor=None):\n+    \"\"\"\n+    For each item in the iterable: skips it if in_predicate is False, applies processor,\n+    skips the result if out_predicate is False, else yields it.\n+    \"\"\"\n+    if inspect.isasyncgen(it):\n+        return process_async_iterable_helper(it, in_predicate, out_predicate, processor)\n+    else:\n+        return process_normal_iterable_helper(it, in_predicate, out_predicate, processor)\n\n@@ -0,0 +1,87 @@\n+import collections\n+\n+from twisted.trial import unittest\n+\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n+from scrapy.utils.defer import deferred_f_from_coro_f\n+from scrapy.utils.middlewares import process_iterable_helper\n+\n+\n+def predicate1(o):\n+    return bool(o % 2)\n+\n+\n+def predicate2(o):\n+    return o < 10\n+\n+\n+def processor(o):\n+    return o * 2\n+\n+\n+class ProcessIterableHelperNormalTest(unittest.TestCase):\n+\n+    def test_normal_in_predicate(self):\n+        iterable1 = iter([1, 2, 3])\n+        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1)\n+        self.assertIsInstance(iterable2, collections.abc.Iterable)\n+        list2 = list(iterable2)\n+        self.assertEqual(list2, [1, 3])\n+\n+    def test_normal_out_predicate(self):\n+        iterable1 = iter([1, 2, 10, 3, 15])\n+        iterable2 = process_iterable_helper(iterable1, out_predicate=predicate2)\n+        self.assertIsInstance(iterable2, collections.abc.Iterable)\n+        list2 = list(iterable2)\n+        self.assertEqual(list2, [1, 2, 3])\n+\n+    def test_normal_processor(self):\n+        iterable1 = iter([1, 2, 3])\n+        iterable2 = process_iterable_helper(iterable1, processor=processor)\n+        self.assertIsInstance(iterable2, collections.abc.Iterable)\n+        list2 = list(iterable2)\n+        self.assertEqual(list2, [2, 4, 6])\n+\n+    def test_normal_combined(self):\n+        iterable1 = iter([1, 2, 10, 3, 6, 18, 5, 15])\n+        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1,\n+                                            out_predicate=predicate2, processor=processor)\n+        self.assertIsInstance(iterable2, collections.abc.Iterable)\n+        list2 = list(iterable2)\n+        self.assertEqual(list2, [2, 6])\n+\n+\n+class ProcessIterableHelperAsyncTest(unittest.TestCase):\n+\n+    @deferred_f_from_coro_f\n+    async def test_async_in_predicate(self):\n+        iterable1 = as_async_generator([1, 2, 3])\n+        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1)\n+        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n+        list2 = await collect_asyncgen(iterable2)\n+        self.assertEqual(list2, [1, 3])\n+\n+    @deferred_f_from_coro_f\n+    async def test_async_out_predicate(self):\n+        iterable1 = as_async_generator([1, 2, 10, 3, 15])\n+        iterable2 = process_iterable_helper(iterable1, out_predicate=predicate2)\n+        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n+        list2 = await collect_asyncgen(iterable2)\n+        self.assertEqual(list2, [1, 2, 3])\n+\n+    @deferred_f_from_coro_f\n+    async def test_async_processor(self):\n+        iterable1 = as_async_generator([1, 2, 3])\n+        iterable2 = process_iterable_helper(iterable1, processor=processor)\n+        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n+        list2 = await collect_asyncgen(iterable2)\n+        self.assertEqual(list2, [2, 4, 6])\n+\n+    @deferred_f_from_coro_f\n+    async def test_async_combined(self):\n+        iterable1 = as_async_generator([1, 2, 10, 3, 6, 18, 5, 15])\n+        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1,\n+                                            out_predicate=predicate2, processor=processor)\n+        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n+        list2 = await collect_asyncgen(iterable2)\n+        self.assertEqual(list2, [2, 6])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92f2c9e308a5eda361229a8e74f74a21b9ff770a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 24 | Churn Cumulative: 1099 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,10 +3,10 @@ Depth Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-\n import logging\n \n from scrapy.http import Request\n+from scrapy.utils.middlewares import process_iterable_helper\n \n logger = logging.getLogger(__name__)\n \n@@ -55,4 +55,4 @@ class DepthMiddleware:\n             if self.verbose_stats:\n                 self.stats.inc_value('request_depth_count/0', spider=spider)\n \n-        return (r for r in result or () if _filter(r))\n+        return process_iterable_helper(result or (), in_predicate=_filter)\n\n@@ -10,6 +10,7 @@ import warnings\n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.middlewares import process_iterable_helper\n \n logger = logging.getLogger(__name__)\n \n@@ -26,11 +27,11 @@ class OffsiteMiddleware:\n         return o\n \n     def process_spider_output(self, response, result, spider):\n-        for x in result:\n-            if isinstance(x, Request):\n+        def in_predicate(x):\n+            if not isinstance(x, Request):\n+                return True\n             if x.dont_filter or self.should_follow(x, spider):\n-                    yield x\n-                else:\n+                return True\n             domain = urlparse_cached(x).hostname\n             if domain and domain not in self.domains_seen:\n                 self.domains_seen.add(domain)\n@@ -39,8 +40,9 @@ class OffsiteMiddleware:\n                     {'domain': domain, 'request': x}, extra={'spider': spider})\n                 self.stats.inc_value('offsite/domains', spider=spider)\n             self.stats.inc_value('offsite/filtered', spider=spider)\n-            else:\n-                yield x\n+            return False\n+\n+        return process_iterable_helper(result or (), in_predicate=in_predicate)\n \n     def should_follow(self, request, spider):\n         regex = self.host_regex\n\n@@ -10,6 +10,7 @@ from w3lib.url import safe_url_string\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n+from scrapy.utils.middlewares import process_iterable_helper\n from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n from scrapy.utils.url import strip_url\n@@ -337,7 +338,7 @@ class RefererMiddleware:\n                 if referrer is not None:\n                     r.headers.setdefault('Referer', referrer)\n             return r\n-        return (_set_referer(r) for r in result or ())\n+        return process_iterable_helper(result or (), processor=_set_referer)\n \n     def request_scheduled(self, request, spider):\n         # check redirected request to patch \"Referer\" header if necessary\n\n@@ -8,6 +8,7 @@ import logging\n \n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.middlewares import process_iterable_helper\n \n logger = logging.getLogger(__name__)\n \n@@ -34,4 +35,4 @@ class UrlLengthMiddleware:\n             else:\n                 return True\n \n-        return (r for r in result or () if _filter(r))\n+        return process_iterable_helper(result or (), in_predicate=_filter)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a1e9359caa0e16b336fdb2944ab7f4a69549896", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 16/15 | Churn Δ: 128 | Churn Cumulative: 1674 | Contributors (this commit): 33 | Commits (past 90d): 4 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n-\n+import collections\n import logging\n from collections import deque\n \n@@ -12,7 +12,16 @@ from scrapy import signals\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy.http import Request, Response\n-from scrapy.utils.defer import defer_fail, defer_succeed, iter_errback, parallel\n+from scrapy.utils.defer import (\n+    aiter_errback,\n+    defer_fail,\n+    defer_succeed,\n+    deferred_from_coro,\n+    iter_errback,\n+    parallel,\n+    parallel_async,\n+)\n+\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n@@ -180,6 +189,11 @@ class Scraper:\n     def handle_spider_output(self, result, request, response, spider):\n         if not result:\n             return defer_succeed(None)\n+        if isinstance(result, collections.abc.AsyncIterable):\n+            it = aiter_errback(result, self.handle_spider_error, request, response, spider)\n+            dfd = deferred_from_coro(parallel_async(it, self.concurrent_items, self._process_spidermw_output,\n+                                     request, response, spider))\n+        else:\n             it = iter_errback(result, self.handle_spider_error, request, response, spider)\n             dfd = parallel(it, self.concurrent_items, self._process_spidermw_output,\n                            request, response, spider)\n\n@@ -75,6 +75,116 @@ def parallel(iterable, count, callable, *args, **named):\n     return defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n+class _AsyncCooperatorAdapter:\n+    \"\"\" A class that wraps an async iterator into a normal iterator suitable\n+    for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n+    it calls the callable directly in the callback, instead of providing a more\n+    generic interface.\n+\n+    On the outside, this class behaves as an iterator that yields Deferreds.\n+    Each Deferred is fired with the result of the callable which was called on\n+    the next result from aiterator. It raises StopIteration when aiterator is\n+    exhausted, as expected.\n+\n+    Cooperator calls __next__() multiple times and waits on the Deferreds\n+    returned from it. As async generators (since Python 3.8) don't support\n+    awaiting on __anext__() several times in parallel, we need to serialize\n+    this. It's done by storing the Deferreds returned from __next__() and\n+    firing the oldest one when a result from __anext__() is available.\n+\n+    The workflow:\n+    1. When __next__() is called for the first time, it creates a Deferred, stores it\n+    in self.waiting_deferreds and returns it. It also makes a Deferred that will wait\n+    for self.aiterator.__anext__() and puts it into self.anext_deferred.\n+    2. If __next__() is called again before self.anext_deferred fires, more Deferreds\n+    are added to self.waiting_deferreds.\n+    3. When self.anext_deferred fires, it either calls _callback() or _errback(). Both\n+    clear self.anext_deferred.\n+    3.1. _callback() calls the callable passing the result value that it takes, pops a\n+    Deferred from self.waiting_deferreds, and if the callable result was a Deferred, it\n+    chains those Deferreds so that the waiting Deferred will fire when the result\n+    Deferred does, otherwise it fires it directly. This causes one awaiting task to\n+    receive a result. If self.waiting_deferreds is still not empty, new __anext__() is\n+    called and self.anext_deferred is populated.\n+    3.2. _errback() checks the exception class. If it's StopAsyncIteration it means\n+    self.aiterator is exhausted and so it sets self.finished and fires all\n+    self.waiting_deferreds. Other exceptions are propagated.\n+    4. If __next__() is called after __anext__() was handled, then if self.finished is\n+    True, it raises StopIteration, otherwise it acts like in step 2, but if\n+    self.anext_deferred is now empty is also populates it with a new __anext__().\n+\n+    Note that CooperativeTask ignores the value returned from the Deferred that it waits\n+    for, so we fire them with None when needed.\n+\n+    It may be possible to write an async iterator-aware replacement for\n+    Cooperator/CooperativeTask and use it instead of this adapter to achieve the same\n+    goal.\n+    \"\"\"\n+    def __init__(self, aiterator, callable, *callable_args, **callable_kwargs):\n+        self.aiterator = aiterator\n+        self.callable = callable\n+        self.callable_args = callable_args\n+        self.callable_kwargs = callable_kwargs\n+        self.finished = False\n+        self.waiting_deferreds = []\n+        self.anext_deferred = None\n+\n+    def _callback(self, result):\n+        # This gets called when the result from aiterator.__anext__() is available.\n+        # It calls the callable on it and sends the result to the oldest waiting Deferred\n+        # (by chaining if the result is a Deferred too or by firing if not).\n+        self.anext_deferred = None\n+        result = self.callable(result, *self.callable_args, **self.callable_kwargs)\n+        d = self.waiting_deferreds.pop(0)\n+        if d.called:\n+            raise ValueError('Deferred in waiting_deferreds already called')\n+        if isinstance(result, defer.Deferred):\n+            result.chainDeferred(d)\n+        else:\n+            d.callback(None)\n+        if self.waiting_deferreds:\n+            self._call_anext()\n+\n+    def _errback(self, failure):\n+        # This gets called on any exceptions in aiterator.__anext__().\n+        # It handles StopAsyncIteration by stopping the iteration and reraises all others.\n+        self.anext_deferred = None\n+        failure.trap(StopAsyncIteration)\n+        self.finished = True\n+        for d in self.waiting_deferreds:\n+            if d.called:\n+                raise ValueError('Deferred in waiting_deferreds already called')\n+            d.callback(None)\n+\n+    def _call_anext(self):\n+        # This starts waiting for the next result from aiterator.\n+        # If aiterator is exhausted, _errback will be called.\n+        self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n+        self.anext_deferred.addCallbacks(self._callback, self._errback)\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        # This puts a new Deferred into self.waiting_deferreds and returns it.\n+        # It also calls __anext__() if needed.\n+        if self.finished:\n+            raise StopIteration\n+        d = defer.Deferred()\n+        self.waiting_deferreds.append(d)\n+        if not self.anext_deferred:\n+            self._call_anext()\n+        return d\n+\n+\n+def parallel_async(async_iterable, count, callable, *args, **named):\n+    \"\"\" Like parallel but for async iterables \"\"\"\n+    coop = task.Cooperator()\n+    work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n+    dl = defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n+    return dl\n+\n+\n def process_chain(callbacks, input, *a, **kw):\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n     d = defer.Deferred()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2152a2a50898b91a44e6b0c282b69b7bfb8d18f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 16 | Files Changed: 7 | Hunks: 22 | Methods Changed: 20 | Complexity Δ (Sum/Max): 28/9 | Churn Δ: 177 | Churn Cumulative: 3365 | Contributors (this commit): 33 | Commits (past 90d): 12 | Contributors (cumulative): 85 | DMM Complexity: 0.9512195121951219\n\nDIFF:\n@@ -3,6 +3,7 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+import inspect\n from itertools import islice\n \n from twisted.python.failure import Failure\n@@ -11,11 +12,11 @@ from scrapy.exceptions import _InvalidOutput\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import mustbe_deferred\n-from scrapy.utils.python import MutableChain\n+from scrapy.utils.python import MutableAsyncChain, MutableChain\n \n \n def _isiterable(possible_iterator):\n-    return hasattr(possible_iterator, '__iter__')\n+    return hasattr(possible_iterator, '__iter__') or hasattr(possible_iterator, '__aiter__')\n \n \n def _fname(f):\n@@ -58,15 +59,31 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return scrape_func(response, request, spider)\n \n         def _evaluate_iterable(iterable, exception_processor_index, recover_to):\n-            try:\n-                for r in iterable:\n-                    yield r\n-            except Exception as ex:\n+            def _process_exception(ex):\n                 exception_result = process_spider_exception(Failure(ex), exception_processor_index)\n                 if isinstance(exception_result, Failure):\n                     raise\n                 recover_to.extend(exception_result)\n \n+            def _evaluate_normal_iterable(iterable):\n+                try:\n+                    for r in iterable:\n+                        yield r\n+                except Exception as ex:\n+                    _process_exception(ex)\n+\n+            async def _evaluate_async_iterable(iterable):\n+                try:\n+                    async for r in iterable:\n+                        yield r\n+                except Exception as ex:\n+                    _process_exception(ex)\n+\n+            if inspect.isasyncgen(iterable):\n+                return _evaluate_async_iterable(iterable)\n+            else:\n+                return _evaluate_normal_iterable(iterable)\n+\n         def process_spider_exception(_failure, start_index=0):\n             exception = _failure.value\n             # don't handle _InvalidOutput exception\n@@ -92,7 +109,11 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         def process_spider_output(result, start_index=0):\n             # items in this iterable do not need to go through the process_spider_output\n             # chain, they went through it already from the process_spider_exception method\n-            recovered = MutableChain()\n+            if inspect.isasyncgen(result):\n+                iter_class = MutableAsyncChain\n+            else:\n+                iter_class = MutableChain\n+            recovered = iter_class()\n \n             method_list = islice(self.methods['process_spider_output'], start_index, None)\n             for method_index, method in enumerate(method_list, start=start_index):\n@@ -113,12 +134,16 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                            f\"iterable, got {type(result)}\")\n                     raise _InvalidOutput(msg)\n \n-            return MutableChain(result, recovered)\n+            return iter_class(result, recovered)\n \n         def process_callback_output(result):\n-            recovered = MutableChain()\n+            if inspect.isasyncgen(result):\n+                iter_class = MutableAsyncChain\n+            else:\n+                iter_class = MutableChain\n+            recovered = iter_class()\n             result = _evaluate_iterable(result, 0, recovered)\n-            return MutableChain(process_spider_output(result), recovered)\n+            return iter_class(process_spider_output(result), recovered)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n\n@@ -290,3 +290,22 @@ def maybeDeferred_coro(f, *args, **kw):\n         return defer.fail(result)\n     else:\n         return defer.succeed(result)\n+\n+\n+def deferred_to_future(d):\n+    \"\"\" Wraps a Deferred into a Future. Requires the asyncio reactor.\n+    \"\"\"\n+    return d.asFuture(asyncio.get_event_loop())\n+\n+\n+def maybe_deferred_to_future(d):\n+    \"\"\" Converts a Deferred to something that can be awaited in a callback or other user coroutine.\n+\n+    If the asyncio reactor is installed, coroutines are wrapped into Futures, and only Futures can be\n+    awaited inside them. Otherwise, coroutines are wrapped into Deferreds and Deferreds can be awaited\n+    directly inside them.\n+    \"\"\"\n+    if not is_asyncio_reactor_installed():\n+        return d\n+    else:\n+        return deferred_to_future(d)\n\n@@ -4,7 +4,6 @@ import logging\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.asyncgen import collect_asyncgen\n \n \n logger = logging.getLogger(__name__)\n@@ -12,14 +11,13 @@ logger = logging.getLogger(__name__)\n \n def iterate_spider_output(result):\n     if inspect.isasyncgen(result):\n-        d = deferred_from_coro(collect_asyncgen(result))\n-        d.addCallback(iterate_spider_output)\n-        return d\n+        return result\n     elif inspect.iscoroutine(result):\n         d = deferred_from_coro(result)\n         d.addCallback(iterate_spider_output)\n         return d\n-    return arg_to_iter(result)\n+    else:\n+        return arg_to_iter(deferred_from_coro(result))\n \n \n def iter_spider_classes(module):\n\n@@ -110,3 +110,10 @@ def mock_google_cloud_storage():\n     bucket_mock.blob.return_value = blob_mock\n \n     return (client_mock, bucket_mock, blob_mock)\n+\n+\n+def get_web_client_agent_req(url):\n+    from twisted.internet import reactor\n+    from twisted.web.client import Agent  # imports twisted.internet.reactor\n+    agent = Agent(reactor)\n+    return agent.request(b'GET', url.encode('utf-8'))\n\n@@ -14,7 +14,8 @@ from scrapy.item import Item\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n from scrapy.spiders.crawl import CrawlSpider, Rule\n-from scrapy.utils.test import get_from_asyncio_queue\n+from scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future\n+from scrapy.utils.test import get_from_asyncio_queue, get_web_client_agent_req\n \n \n class MockServerSpider(Spider):\n@@ -148,6 +149,41 @@ class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n         return reqs\n \n \n+class AsyncDefAsyncioGenExcSpider(SimpleSpider):\n+    name = 'asyncdef_asyncio_gen_exc'\n+\n+    async def parse(self, response):\n+        for i in range(10):\n+            await asyncio.sleep(0.1)\n+            yield {'foo': i}\n+            if i > 5:\n+                raise ValueError(\"Stopping the processing\")\n+\n+\n+class AsyncDefDeferredDirectSpider(SimpleSpider):\n+    name = 'asyncdef_deferred_direct'\n+\n+    async def parse(self, response):\n+        resp = await get_web_client_agent_req(self.mockserver.url(\"/status?n=200\"))\n+        yield {'code': resp.code}\n+\n+\n+class AsyncDefDeferredWrappedSpider(SimpleSpider):\n+    name = 'asyncdef_deferred_wrapped'\n+\n+    async def parse(self, response):\n+        resp = await deferred_to_future(get_web_client_agent_req(self.mockserver.url(\"/status?n=200\")))\n+        yield {'code': resp.code}\n+\n+\n+class AsyncDefDeferredMaybeWrappedSpider(SimpleSpider):\n+    name = 'asyncdef_deferred_wrapped'\n+\n+    async def parse(self, response):\n+        resp = await maybe_deferred_to_future(get_web_client_agent_req(self.mockserver.url(\"/status?n=200\")))\n+        yield {'code': resp.code}\n+\n+\n class AsyncDefAsyncioGenSpider(SimpleSpider):\n \n     name = 'asyncdef_asyncio_gen'\n\n@@ -20,12 +20,16 @@ from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n from tests.spiders import (\n     AsyncDefAsyncioGenComplexSpider,\n+    AsyncDefAsyncioGenExcSpider,\n     AsyncDefAsyncioGenLoopSpider,\n     AsyncDefAsyncioGenSpider,\n     AsyncDefAsyncioReqsReturnSpider,\n     AsyncDefAsyncioReturnSingleElementSpider,\n     AsyncDefAsyncioReturnSpider,\n     AsyncDefAsyncioSpider,\n+    AsyncDefDeferredDirectSpider,\n+    AsyncDefDeferredMaybeWrappedSpider,\n+    AsyncDefDeferredWrappedSpider,\n     AsyncDefSpider,\n     BrokenStartRequestsSpider,\n     BytesReceivedCallbackSpider,\n@@ -430,6 +434,18 @@ class CrawlSpiderTestCase(TestCase):\n         for i in range(10):\n             self.assertIn({'foo': i}, items)\n \n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncgen_parse_exc(self):\n+        log, items, stats = yield self._run_spider(AsyncDefAsyncioGenExcSpider)\n+        log = str(log)\n+        self.assertIn(\"Spider error processing\", log)\n+        self.assertIn(\"ValueError\", log)\n+        itemcount = stats.get_value('item_scraped_count')\n+        self.assertEqual(itemcount, 7)\n+        for i in range(7):\n+            self.assertIn({'foo': i}, items)\n+\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n@@ -449,6 +465,23 @@ class CrawlSpiderTestCase(TestCase):\n         for req_id in range(3):\n             self.assertIn(f\"Got response 200, req_id {req_id}\", str(log))\n \n+    @mark.only_not_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_deferred_direct(self):\n+        _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)\n+        self.assertEqual(items, [{'code': 200}])\n+\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_deferred_wrapped(self):\n+        log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)\n+        self.assertEqual(items, [{'code': 200}])\n+\n+    @defer.inlineCallbacks\n+    def test_async_def_deferred_maybe_wrapped(self):\n+        _, items, _ = yield self._run_spider(AsyncDefDeferredMaybeWrappedSpider)\n+        self.assertEqual(items, [{'code': 200}])\n+\n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_none(self):\n         crawler = self.runner.create_crawler(SingleRequestSpider)\n\n@@ -43,6 +43,23 @@ class RecoverySpider(Spider):\n             raise TabError()\n \n \n+class RecoveryAsyncGenSpider(RecoverySpider):\n+    name = 'RecoveryAsyncGenSpider'\n+\n+    async def parse(self, response):\n+        for r in super().parse(response):\n+            yield r\n+\n+\n+class RecoveryMiddleware:\n+    def process_spider_exception(self, response, exception, spider):\n+        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n+        return [\n+            {'from': 'process_spider_exception'},\n+            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n+        ]\n+\n+\n # ================================================================================\n # (1) exceptions from a spider middleware's process_spider_input method\n class FailProcessSpiderInputMiddleware:\n@@ -307,6 +324,16 @@ class TestSpiderMiddleware(TestCase):\n         self.assertEqual(str(log).count(\"Middleware: TabError exception caught\"), 1)\n         self.assertIn(\"'item_scraped_count': 3\", str(log))\n \n+    @defer.inlineCallbacks\n+    def test_recovery_asyncgen(self):\n+        \"\"\"\n+        Same as test_recovery but with an async callback.\n+        \"\"\"\n+        log = yield self.crawl_log(RecoveryAsyncGenSpider)\n+        self.assertIn(\"Middleware: TabError exception caught\", str(log))\n+        self.assertEqual(str(log).count(\"Middleware: TabError exception caught\"), 1)\n+        self.assertIn(\"'item_scraped_count': 3\", str(log))\n+\n     @defer.inlineCallbacks\n     def test_process_spider_input_without_errback(self):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
