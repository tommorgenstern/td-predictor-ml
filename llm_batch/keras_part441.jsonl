{"custom_id": "keras#19c1fddda6b2f8f95745d5d288767dcfac3a74e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 25 | Churn Cumulative: 5128 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2078,6 +2078,31 @@ class TestExceptionsAndWarnings(test_combinations.TestCase):\n                                 'Unexpected result of `predict_function`.*'):\n       model.predict(np.array([]))\n \n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  @parameterized.named_parameters(\n+      ('dynamic', 0, False),\n+      ('dynamic_multistep', 10, False),\n+      ('static', 0, True),\n+      ('static_multistep', 10, True),\n+  )\n+  def test_predict_structured(self, spe, static_batch):\n+    inputs = layers_module.Input(shape=(2,))\n+    outputs = layers_module.Dense(2)(inputs)\n+    model = training_module.Model(\n+        inputs=inputs,\n+        outputs={'out': outputs},\n+    )\n+    model.compile(\n+        loss='mse',\n+        steps_per_execution=spe,\n+        run_eagerly=test_utils.should_run_eagerly(),\n+    )\n+    xdata = np.random.uniform(size=(8, 2)).astype(np.float32)\n+    dataset = tf.data.Dataset.from_tensor_slices((xdata, xdata))\n+    dataset = dataset.batch(8, drop_remainder=static_batch)\n+    ret = model.predict(dataset, steps=1)\n+    tf.nest.assert_same_structure(ret, {'out': ''})\n+\n   @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_on_batch_error_inconsistent_batch_size(self):\n     input_node1 = layers_module.Input(shape=(5,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5bc434575aa3a42dde4553a18f45806949d58e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 52 | Churn Cumulative: 52 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,52 @@\n+\"\"\"Tests for dataset_utils.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.utils import dataset_utils\n+\n+\n+class TestSplitDataset(tf.test.TestCase):\n+  \n+  def test_invalid_dataset_cases(self):\n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(dataset=None, left_size=5)\n+      \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(dataset=1, left_size=5)\n+      \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(dataset=float(1.2), left_size=5)\n+      \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(dataset=dict({}))\n+      \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset(dataset=float('INF'))\n+\n+  def test_valid_left_size_cases(self):\n+    \n+    dataset = [1,2,3]\n+    splitted_dataset = dataset_utils.split_dataset(dataset, left_size=1,right_size=2)\n+    assert(len(splitted_dataset) == 2)\n+    left_dataset,right_dataset = splitted_dataset\n+    self.assertEqual(len(left_dataset), 1)\n+    self.assertEqual(len(right_dataset), 2)\n+    self.assertEqual(list(left_dataset), [1])\n+    self.assertEqual(list(right_dataset), [2,3])\n+     \n+    \n+  def test_invalid_left_and_right_case(self):\n+    with self.assertRaises(ValueError):\n+      dataset_utils.split_dataset(dataset=[1,2,3], left_size=None)\n+      \n+    with self.assertRaises(ValueError):\n+      dataset_utils.split_dataset([1,2,3], left_size=None,right_size=None)\n+      \n+    with self.assertRaises(ValueError):\n+      dataset_utils.split_dataset([1,2,3], left_size=3,right_size=None)\n+    \n+\n+\n+\n+if __name__ == \"__main__\":\n+  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0e215987c51a4f0e98c96a726276465be2f3071", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 195 | Lines Deleted: 69 | Files Changed: 2 | Hunks: 41 | Methods Changed: 15 | Complexity Δ (Sum/Max): 17/10 | Churn Δ: 264 | Churn Cumulative: 596 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.7641509433962265\n\nDIFF:\n@@ -17,6 +17,7 @@\n \n \n \n+\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n \n@@ -24,7 +25,7 @@ import multiprocessing\n import os\n import time\n import warnings\n-import math\n+from random import Random\n \n import numpy as np\n from tensorflow.python.util.tf_export import keras_export\n@@ -56,49 +57,31 @@ def split_dataset(dataset,\n   Returns:\n       A tuple of two `tf.data.Dataset` objects: the left and right splits.\n   \"\"\"\n-  # TODO (prakashsellathurai) : integrate unit test.\n   \n   if not isinstance(dataset,(tf.data.Dataset,list,tuple)):\n     raise TypeError('`dataset` must be either a tf.data.Dataset object'\n-                   'or a list/tuple of arrays',f'Got {type(dataset)}')\n+                   f' or a list/tuple of arrays. Received : {type(dataset)}')\n     \n   if right_size is None and left_size is None:\n-    raise ValueError('Both `left_size` and `right_size`cannot be `None`'\n-                     'atleast specify one with valid value')\n+    raise ValueError('you must specify either `left_size` or `right_size`'\n+                    ' Received: `left_size`= None, and `right_size`=None')\n       \n-  dataset_as_list = []\n+  dataset_as_list = _convert_dataset_to_list(dataset)\n   \n-  if isinstance(dataset,tf.data.Dataset):\n-    data_size_warning_flag = False\n-    start_time = time.time()\n-    i = 0\n-    for datum in list(dataset):\n-      cur_time = time.time()\n-      # warns user if the dataset is too large to iterate within 10s\n-      if int(cur_time - start_time) > 10 and not data_size_warning_flag:\n-        warnings.warn('Takes too long time to process the `dataset`,'\n-                      'this function is only  for small datasets'\n-                      'that fits within the memory')\n-        data_size_warning_flag = True\n-      dataset_as_list.append(datum)\n-  elif isinstance(dataset,list):\n-    dataset_as_list = dataset.copy()\n-  elif isinstance(dataset,tuple):\n-    dataset_as_list = list(zip(*dataset))\n+  if seed is None:\n+    seed = np.random.randint(1e6)  \n     \n   if shuffle: \n-    if seed:\n-      np.random.seed(seed)\n-    np.random.shuffle(dataset_as_list)\n+    Random(seed).shuffle(dataset_as_list)\n     \n-  total_size = len(dataset_as_list)\n+  total_length = len(dataset_as_list)\n    \n-  left_size,right_size = convert_dataset_split_sizes(\n-    left_size,right_size,total_size\n+  left_size,right_size = _rescale_dataset_split_sizes(\n+    left_size,right_size,total_length\n     )\n \n   left_dataset = dataset_as_list[:left_size]\n-  right_dataset = dataset_as_list[left_size:]\n+  right_dataset = dataset_as_list[-right_size:]\n \n   left_dataset = tf.data.Dataset.from_tensor_slices(left_dataset)\n   right_dataset = tf.data.Dataset.from_tensor_slices(right_dataset)\n@@ -108,71 +91,123 @@ def split_dataset(dataset,\n   \n   return left_dataset, right_dataset\n \n-def convert_dataset_split_sizes(left_size,right_size,total_size):\n-  \"\"\"Helper function to convert  left_size/right_size relative to dataset's size\n+def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n+  \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n+  \"\"\"\n+  \n+  if isinstance(dataset,tuple):\n+    dataset_iterator = list(zip(*dataset))\n+  elif isinstance(dataset,list):\n+    dataset_iterator = dataset.copy()\n+  elif isinstance(dataset,tf.data.Dataset):\n+    dataset_iterator = list(dataset)\n+  else:\n+    raise TypeError('`dataset` must be either a tf.data.Dataset object'\n+                   f' or a list/tuple of arrays. Received : {type(dataset)}'\n+          )\n+  \n+  \n+  dataset_as_list = []\n+  start_time = time.time()\n+  i = 0\n+  for i,datum in enumerate(dataset_iterator):\n+    if data_size_warning_flag:\n+      if i % 10 == 0:\n+        cur_time = time.time()\n+        # warns user if the dataset is too large to iterate within 10s\n+        if int(cur_time - start_time) > 10 and data_size_warning_flag:\n+          warnings.warn('Takes too long time to process the `dataset`,'\n+                        'this function is only  for small datasets '\n+                        '(e.g. < 10,000 samples).'\n+          )\n+          data_size_warning_flag = False\n+    \n+    dataset_as_list.append(datum)\n+      \n+  return dataset_as_list\n+\n+def _rescale_dataset_split_sizes(left_size,right_size,total_length):\n+  \"\"\"Helper function to rescale  left_size/right_size args relative \n+  to dataset's size\n   \"\"\"\n \n   left_size_type = type(left_size) \n   right_size_type = type(right_size)\n \n+  if ((left_size is not None and left_size_type not in [int,float]) and \n+      (right_size is not None and right_size_type not in [int,float])):\n+    raise TypeError('Invalid `left_size` and `right_size` Types. '\n+                     'Expected: integer or float or None. '\n+                     f' Received: {left_size_type} and {right_size_type}')\n \n   if left_size is not None and left_size_type not in [int,float]:\n-    raise ValueError(f'Invalid `left_size` type Got {left_size_type}'\n-                     'It should be one of  float,int or None') \n-  if right_size is not None and right_size_type not in [int,float]: \n-    raise ValueError(f'Invalid `right_size` type Got {right_size_type}'\n-                     'It should be one of  float,int or None') \n+    raise TypeError(f'Invalid `left_size` Type. Received: {left_size_type}.  '\n+                     ' Expected: int or float or None')\n     \n+  if right_size is not None and right_size_type not in [int,float]: \n+    raise TypeError(f'Invalid `right_size` Type. Received: {right_size_type}.'\n+                    ' Expected: int or float or None')\n+    \n+  if left_size == 0 and right_size == 0:\n+    raise ValueError('Invalid `left_size` and `right_size` values. '\n+                     'You must specify either `left_size` or `right_size` with '\n+                     f'value greater than 0 and less than {total_length} '\n+                      'or a float within range [0,1] to split the dataset'\n+                      f'Received: `left_size`={left_size}, '\n+                      f'`right_size`={right_size}')\n   \n   if (left_size_type == int \n-      and (left_size <= 0 or left_size>= total_size)\n+      and (left_size <= 0 or left_size>= total_length)\n       or left_size_type == float \n       and (left_size <= 0 or left_size>= 1) ):\n     raise ValueError('`left_size` should be either a positive integer '\n-                     f'and smaller than {total_size} or a float '\n-                     'within the range `[0, 1]`') \n+                     f'and smaller than {total_length} or a float '\n+                     'within the range `[0, 1]`. Received: left_size='\n+                     f'{left_size}') \n     \n   if (right_size_type == int \n-      and (right_size <= 0 or right_size>= total_size) \n+      and (right_size <= 0 or right_size>= total_length) \n       or right_size_type == float \n       and (right_size <= 0 or right_size>= 1)):\n     raise ValueError('`right_size` should be either a positive integer '\n-                     f'and smaller than {total_size} or'\n-                     'a float within the range `[0, 1]`') \n+                     f'and smaller than {total_length} or '\n+                     'a float within the range `[0, 1]`. Received: right_size='\n+                     f'{right_size}') \n     \n   if right_size_type == left_size_type == float and right_size + left_size > 1:\n     raise ValueError('sum of `left_size` and `right_size`'\n-                     ' should be within `[0,1]`'\n-                    f'Got {right_size + left_size} ,'\n+                     ' should be within `[0,1]`.'\n+                    f'Received: {right_size + left_size} ,'\n                     'reduce the `left_size` or `right_size`')\n \n   if left_size_type == float:\n-    left_size = math.ceil(left_size*total_size)\n+    left_size = round(left_size*total_length)\n   else:\n     left_size = float(left_size)\n \n   if right_size_type == float:\n-    right_size = math.ceil(right_size*total_size)\n+    right_size = round(right_size*total_length)\n   else:\n     right_size = float(right_size)\n \n \n   if left_size is None:\n-    left_size = total_size - right_size\n+    left_size = total_length - right_size\n   elif right_size is None:\n-    right_size = total_size - left_size\n+    right_size = total_length - left_size\n \n-  if left_size + right_size > total_size:\n+  if left_size + right_size > total_length:\n     raise ValueError('The sum of `left_size` and `right_size`'\n-                     f' should be smaller than the samples {total_size} '\n+                     f' should be smaller than the samples {total_length} '\n                      ' reduce `left_size` or `right_size` ' )\n \n   \n-  if left_size == 0:\n-    raise ValueError(f'with dataset of length={total_size}'\n-                     '`left_size`={left_size} and `right_size`={right_size} '\n-                    'resulting left dataset split will be empty, '\n-                    'adjust any of the aforementioned parameters')\n+  for split,side in [(left_size,'left'),(right_size,'right')]:\n+    if split == 0:\n+      raise ValueError(f'with dataset of length={total_length} '\n+                      '`left_size`={left_size} and `right_size`={right_size}, '\n+                      f'resulting {side} dataset split will be empty. '\n+                      'Adjust any of the aforementioned parameters')\n     \n   left_size,right_size = int(left_size) ,int(right_size)\n   return left_size,right_size\n\n@@ -2,31 +2,65 @@\n \n import tensorflow.compat.v2 as tf\n \n+import numpy as np\n+\n from keras.utils import dataset_utils\n \n \n-class TestSplitDataset(tf.test.TestCase):\n+class SplitDatasetTest(tf.test.TestCase):\n   \n-  def test_invalid_dataset_cases(self):\n+  def test_with_list_dataset(self):\n+    dataset = [np.ones(shape=(10,10,10)) for _ in range(10)]\n+    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+                                                             left_size=5,\n+                                                             right_size=5)\n+    self.assertEqual(len(left_dataset), len(right_dataset))\n+    self.assertIsInstance(left_dataset, tf.data.Dataset)\n+    self.assertIsInstance(left_dataset, tf.data.Dataset)\n+        \n+    dataset = [np.ones(shape=(10,10,10)) for _ in range(10)]\n+    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+                                                             left_size=0.6,\n+                                                             right_size=0.4)\n+    self.assertEqual(len(left_dataset), 6)\n+    self.assertEqual(len(right_dataset), 4)\n+\n+    \n+  def test_with_tuple_dataset(self):\n+    dataset = (np.ones(shape=(10,10,10)),np.zeros(shape=(10,10,10)))\n+    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+                                                             left_size=0.75,\n+                                                             right_size=0.25)\n+    self.assertLen(left_dataset, 8)\n+    self.assertLen(right_dataset, 2)\n+    \n+    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+                                                             left_size=0.35,\n+                                                             right_size=0.65)\n+    self.assertLen(left_dataset, 4)\n+    self.assertLen(right_dataset, 6)\n+    self.assertIsInstance(left_dataset, tf.data.Dataset)\n+    self.assertIsInstance(right_dataset, tf.data.Dataset)\n+    \n+  \n+  def test_with_invalid_dataset(self):\n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(dataset=None, left_size=5)  \n-      \n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(dataset=1, left_size=5)  \n-      \n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(dataset=float(1.2), left_size=5)  \n-      \n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(dataset=dict({}))\n-      \n+      dataset_utils.split_dataset(dataset=dict({}), left_size=5) \n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(dataset=float('INF'))\n+      dataset_utils.split_dataset(dataset=float('INF'), left_size=5)\n \n-  def test_valid_left_size_cases(self):\n+  def test_with_valid_left_and_right_sizes(self):\n     \n     dataset = [1,2,3]\n-    splitted_dataset = dataset_utils.split_dataset(dataset, left_size=1,right_size=2)\n+    splitted_dataset = dataset_utils.split_dataset(dataset, \n+                                                   left_size=1,\n+                                                   right_size=2)\n     assert(len(splitted_dataset) == 2)\n     left_dataset,right_dataset = splitted_dataset\n     self.assertEqual(len(left_dataset), 1)\n@@ -35,15 +69,72 @@ class TestSplitDataset(tf.test.TestCase):\n     self.assertEqual(list(right_dataset), [2,3])\n     \n     \n-  def test_invalid_left_and_right_case(self):\n+    dataset = [1,2,3,4,5,6,7,8,9,10]\n+    splitted_dataset = dataset_utils.split_dataset(dataset,\n+                                                   left_size=0.1,\n+                                                   right_size=0.9)\n+    assert(len(splitted_dataset) == 2)\n+    left_dataset,right_dataset = splitted_dataset\n+    self.assertEqual(len(left_dataset), 1 )\n+    self.assertEqual(len(right_dataset), 9 )\n+    self.assertEqual(list(left_dataset), [1])\n+    self.assertEqual(list(right_dataset), [2,3,4,5,6,7,8,9,10])\n+    \n+    dataset = [1,2,3,4,5,6,7,8,9,10]\n+    splitted_dataset = dataset_utils.split_dataset(dataset,\n+                                                   left_size=2,\n+                                                   right_size=5)\n+    assert(len(splitted_dataset) == 2)\n+    left_dataset,right_dataset = splitted_dataset\n+    self.assertEqual(len(left_dataset), 2 )\n+    self.assertEqual(len(right_dataset), 5 )\n+    self.assertEqual(list(left_dataset), [1,2])\n+    self.assertEqual(list(right_dataset), [6,7,8,9,10])\n+    \n+  def test_with_float_left_and_right_sizes(self):\n+    dataset = tf.data.Dataset.from_tensor_slices(np.array([[0.1,0.2,0.3],\n+                                                           [0.4,0.5,0.6],\n+                                                           [0.7,0.8,0.9]]))\n+    left_dataset,right_dataset = dataset_utils.split_dataset(dataset, \n+                                                             left_size=0.8,\n+                                                             right_size=0.2)\n+    self.assertEqual(len(left_dataset), 2)\n+    self.assertEqual(len(right_dataset), 1)\n+    \n+  def test_with_invalid_float_left_and_right_sizes(self):\n+    with self.assertRaises(ValueError):\n+      dataset = [np.ones(shape=(200, 32,32)), np.zeros(shape=(200, 32,32))]\n+      dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n+    with self.assertRaises(ValueError):\n+      dataset = [1]\n+      dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n+\n+    \n+      \n+  def test_with_None_and_zero_left_and_right_size(self):    \n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(dataset=[1,2,3], left_size=None)  \n-      \n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset([1,2,3], left_size=None,right_size=None)\n-      \n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset([1,2,3], left_size=3,right_size=None)\n+    with self.assertRaises(ValueError):\n+      dataset_utils.split_dataset([1,2], left_size=3,right_size=None)\n+    with self.assertRaises(ValueError):\n+      dataset_utils.split_dataset([1,2], left_size=0,right_size=0)\n+      \n+  def test_with_invalid_left_and_right_size_types(self):      \n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset([1,2], left_size='1',right_size='1')\n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset([1,2], left_size=0,right_size='1')\n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset([1,2], left_size='100',right_size=None)\n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset([1,2], right_size='1')\n+    with self.assertRaises(TypeError):\n+      dataset_utils.split_dataset([1,2], left_size=0.5,right_size='1')\n+      \n     \n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6a1bda81886a721413eb21a12fcbd69b3f14dfa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 465 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ def split_dataset(dataset,\n def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n   \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n   \"\"\"\n-  # TODO prakashsellathurai: add support for list of tuples,tuples of nd array\n+  # TODO prakashsellathurai: add  failing test cases for list of tuples,tuples of nd array\n   # TODO prakashsellathurai: add support for Batched  and unbatched tf datasets\n   if isinstance(dataset,tuple):\n     if len(dataset) == 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6efe7aed73bb0a87fcad385628dc0d3662d52ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 8568 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -553,8 +553,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         Note that `trainable` cannot be `True` if `synchronization`\n         is set to `ON_READ`.\n       constraint: Constraint instance (callable).\n-      use_resource: Whether to use [ResourceVariable](\n-      https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables).\n+      use_resource: Whether to use a `ResourceVariable` or not.\n+         See [this guide](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)  # pylint: disable=line-too-long\n+         for more information.\n       synchronization: Indicates when a distributed a variable will be\n         aggregated. Accepted values are constants defined in the class\n         `tf.VariableSynchronization`. By default the synchronization is set to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ac8c9efea6b70fdc6ebd7981f19ac00463d1ad0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 366 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -262,14 +262,6 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n                 directory, ragged=False, output_sequence_length=10, sampling_rate=1.2\n             )\n \n-        with self.assertRaisesRegex(\n-            ValueError,\n-            \"The dataset should be ragged dataset or fixed sequence length dataset\",\n-        ):\n-            _ = audio_dataset.audio_dataset_from_directory(\n-                directory, ragged=False, output_sequence_length=None\n-            )\n-\n         with self.assertRaisesRegex(\n             ValueError, \"Cannot set both `ragged` and `output_sequence_length`\"\n         ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b031891352aa56ead4c38eb2c773f8b4d8b6b85a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 780 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -209,6 +209,16 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n     self.assertEqual(batch[1].shape, (8, 4))\n     self.assertEqual(batch[1].dtype.name, 'float32')\n     \n+    train_dataset, val_dataset = image_dataset.image_dataset_from_directory(\n+        directory, batch_size=10, image_size=(18, 18),\n+        validation_split=0.2, subset='both', seed=1337)\n+    batch = next(iter(train_dataset))\n+    self.assertLen(batch, 2)\n+    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n+    batch = next(iter(val_dataset))\n+    self.assertLen(batch, 2)\n+    self.assertEqual(batch[0].shape, (2, 18, 18, 3))\n+\n   def test_image_dataset_from_directory_color_modes(self):\n     if PIL is None:\n       return  # Skip test if PIL is not available.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6937c383a5e06a653ef0d421cd65679eccd5764d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 158 | Lines Deleted: 84 | Files Changed: 2 | Hunks: 40 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 242 | Churn Cumulative: 961 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: 0.9591836734693877\n\nDIFF:\n@@ -76,50 +76,51 @@ def split_dataset(dataset,\n     \n   total_length = len(dataset_as_list)\n    \n-  left_size,right_size = _rescale_dataset_split_sizes(\n-    left_size,right_size,total_length\n-    )\n+  left_size,right_size = _rescale_dataset_split_sizes(left_size,\n+                                                      right_size,\n+                                                      total_length)\n \n-  left_dataset = dataset_as_list[:left_size]\n-  right_dataset = dataset_as_list[-right_size:]\n+  left_split = dataset_as_list[:left_size]\n+  right_split = dataset_as_list[-right_size:]\n \n-  left_dataset = tf.data.Dataset.from_tensor_slices(left_dataset)\n-  right_dataset = tf.data.Dataset.from_tensor_slices(right_dataset)\n+  left_split = tf.data.Dataset.from_tensor_slices(left_split)\n+  right_split = tf.data.Dataset.from_tensor_slices(right_split)\n   \n-  left_dataset = left_dataset.prefetch(tf.data.AUTOTUNE)\n-  right_dataset = right_dataset.prefetch(tf.data.AUTOTUNE)\n+  left_split = left_split.prefetch(tf.data.AUTOTUNE)\n+  right_split = right_split.prefetch(tf.data.AUTOTUNE)\n   \n-  return left_dataset, right_dataset\n+  return left_split, right_split\n \n def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n   \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n   \"\"\"\n-  # TODO prakashsellathurai: add  failing test cases for list of tuples,tuples of nd array\n-  # TODO prakashsellathurai: add support for Batched  and unbatched tf datasets\n-  if isinstance(dataset,tuple):\n-    if len(dataset) == 0:\n-      raise ValueError('`dataset` must be a non-empty list/tuple of'\n-                       ' numpy.ndarrays or tf.data.Dataset objects.')\n-    dataset_iterator = list(zip(*dataset))\n-  elif isinstance(dataset,list):\n+  # TODO (prakashsellathurai): add support for Batched  and unbatched dict tf datasets\n+  if isinstance(dataset,(tuple,list)):\n     if len(dataset) == 0:\n       raise ValueError('`dataset` must be a non-empty list/tuple of'\n                        ' numpy.ndarrays or tf.data.Dataset objects.')\n+      \n+  \n     if isinstance(dataset[0],np.ndarray):\n-      dataset_iterator = list(zip(*dataset))\n+      if not all(element.shape == dataset[0].shape  for element in dataset):\n+        raise ValueError('all elements of `dataset` must have the same shape.')\n+        \n+      dataset_iterator = iter(zip(*dataset))\n     else:\n-      dataset_iterator = list(dataset)\n+      dataset_iterator = iter(dataset)\n \n   elif isinstance(dataset,np.ndarray):\n-    dataset_iterator = list(dataset)\n+    dataset_iterator = iter(dataset)\n   elif isinstance(dataset,tf.data.Dataset):\n-    dataset_iterator = list(dataset)\n+    if is_batched(dataset):\n+      dataset = dataset.unbatch()\n+    dataset_iterator = iter(dataset)\n   else:\n     raise TypeError('`dataset` must be either a tf.data.Dataset object'\n-                   f' or a list/tuple of arrays. Received : {type(dataset)}'\n-                   )\n+                   f' or a list/tuple of arrays. Received : {type(dataset)}')\n   \n   dataset_as_list = []\n+  \n   try:\n     dataset_iterator = iter(dataset_iterator)\n     first_datum = next(dataset_iterator)\n@@ -129,8 +130,11 @@ def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n                      '`dataset` must be a non-empty list/tuple of'\n                      ' numpy.ndarrays or tf.data.Dataset objects.')\n   \n-    \n-  \n+  if isinstance(first_datum,dict):\n+    raise TypeError('`dataset` must be either a tf.data.Dataset object'\n+                    ' or a list/tuple of arrays. '\n+                    'Received : tf.data.Dataset with dict elements')\n+  else:\n     start_time = time.time()\n     for i,datum in enumerate(dataset_iterator):\n       if data_size_warning_flag:\n@@ -140,8 +144,7 @@ def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n           if int(cur_time - start_time) > 10 and data_size_warning_flag:\n             warnings.warn('Takes too long time to process the `dataset`,'\n                           'this function is only  for small datasets '\n-                        '(e.g. < 10,000 samples).'\n-          )\n+                          '(e.g. < 10,000 samples).')\n             data_size_warning_flag = False\n       \n       dataset_as_list.append(datum)\n@@ -235,6 +238,15 @@ def _rescale_dataset_split_sizes(left_size,right_size,total_length):\n   return left_size,right_size\n \n   \n+def is_batched(tf_dataset):\n+  \"\"\"returns true if given tf dataset  is batched or false if not\n+  \n+  refer: https://stackoverflow.com/a/66101853/8336491\n+  \"\"\"\n+  try:\n+    return tf_dataset.__class__.__name__ == 'BatchDataset'\n+  except : \n+    return False\n \n def index_directory(directory,\n                     labels,\n\n@@ -14,60 +14,107 @@ class SplitDatasetTest(tf.test.TestCase):\n     dataset=np.ones(shape=(200, 32))\n     res = dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n     self.assertLen(res, 2)\n-    left_dataset,right_dataset = res\n-    self.assertIsInstance(left_dataset, tf.data.Dataset)\n-    self.assertIsInstance(right_dataset, tf.data.Dataset)\n+    left_split,right_split = res\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(right_split, tf.data.Dataset)\n+    \n+    self.assertLen(left_split, 160)\n+    self.assertLen(right_split, 40)\n+    self.assertAllEqual(dataset[:160] ,list(left_split))\n+    self.assertAllEqual(dataset[-40:] ,list(right_split))\n     \n-    self.assertLen(left_dataset, 160)\n-    self.assertLen(right_dataset, 40)\n-    self.assertAllEqual(dataset[:160] ,np.array(list(left_dataset)))\n-    self.assertAllEqual(dataset[-40:] ,np.array(list(right_dataset)))\n     \n     # list of numpy array\n     dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(200, 32))]\n     res = dataset_utils.split_dataset(dataset, left_size=4)\n     self.assertLen(res, 2)\n-    left_dataset,right_dataset = res\n-    self.assertIsInstance(left_dataset, tf.data.Dataset)\n-    self.assertIsInstance(right_dataset, tf.data.Dataset)\n-    self.assertLen(left_dataset, 4)\n-    self.assertLen(right_dataset, 196)\n-    self.assertAllEqual(list(zip(*dataset))[:4] ,np.array(list(left_dataset)))\n-    self.assertAllEqual(list(zip(*dataset))[4:] ,np.array(list(right_dataset))) \n-    self.assertAllEqual(np.array(list(left_dataset)+list(right_dataset)), list(zip(*dataset)))\n+    left_split,right_split = res\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(right_split, tf.data.Dataset)\n+    self.assertLen(left_split, 4)\n+    self.assertLen(right_split, 196)\n+    self.assertAllEqual(list(zip(*dataset))[:4] ,list(left_split))\n+    self.assertAllEqual(list(zip(*dataset))[4:] ,list(right_split)) \n+    self.assertAllEqual(list(left_split)+list(right_split), \n+                        list(zip(*dataset)))\n+    \n+    \n+    # fail with invalid shape\n+    with self.assertRaises(ValueError):\n+      dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n+      dataset_utils.split_dataset(dataset, left_size=4)\n+      \n+    with self.assertRaises(ValueError):\n+      dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(201, 32)))\n+      dataset_utils.split_dataset(dataset, left_size=4)\n+    \n+    # list with single np array\n+    dataset=[np.ones(shape=(200, 32))]\n+    left_split,right_split = dataset_utils.split_dataset(dataset, \n+                                                             left_size=4)\n+    self.assertAllEqual(list(zip(*dataset)),\n+                        list(left_split)+list(right_split))\n+    \n+    # Tule of np arrays\n+    dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(200, 32)))\n+    left_split,right_split = dataset_utils.split_dataset(dataset, \n+                                                             left_size=80)\n+    self.assertAllEqual(list(zip(*dataset))[:80] ,list(left_split))\n+    self.assertAllEqual(list(zip(*dataset))[80:] ,list(right_split))\n+    \n+    \n+    # Batched tf.data.Dataset that yields batches of vectors\n+    dataset = tf.data.Dataset.from_tensor_slices(np.ones(shape=(16, 16,3)))\n+    dataset = dataset.batch(10)\n+    left_split,right_split=dataset_utils.split_dataset(dataset,left_size=2)\n+    self.assertAllEqual(list(dataset.unbatch()),\n+                        list(left_split)+list(right_split)) \n+    \n+    # Batched tf.data.Dataset that yields batches of tuples of vectors\n+    dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(32,32), np.random.rand(32,32)))\n+    dataset = dataset.batch(2)\n+    left_split,right_split=dataset_utils.split_dataset(dataset,left_size=5)\n+    self.assertAllEqual(list(dataset.unbatch()),\n+                        list(left_split)+list(right_split))  \n+    \n+\n+\n+    \n+    \n+    \n     \n   def test_with_list_dataset(self):\n     dataset = [np.ones(shape=(10,10,10)) for _ in range(10)]\n-    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+    left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                              left_size=5,\n                                                              right_size=5)\n-    self.assertEqual(len(left_dataset), len(right_dataset))\n-    self.assertIsInstance(left_dataset, tf.data.Dataset)\n-    self.assertIsInstance(left_dataset, tf.data.Dataset)\n+    self.assertEqual(len(left_split), len(right_split))\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n         \n     dataset = [np.ones(shape=(10,10,10)) for _ in range(10)]\n-    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+    left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                              left_size=0.6,\n                                                              right_size=0.4)\n-    self.assertEqual(len(left_dataset), 6)\n-    self.assertEqual(len(right_dataset), 4)\n+    self.assertEqual(len(left_split), 6)\n+    self.assertEqual(len(right_split), 4)\n \n     \n   def test_with_tuple_dataset(self):\n     dataset = (np.ones(shape=(10,10,10)),np.zeros(shape=(10,10,10)))\n-    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+    left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                              left_size=0.75,\n                                                              right_size=0.25)\n-    self.assertLen(left_dataset, 8)\n-    self.assertLen(right_dataset, 2)\n+    self.assertLen(left_split, 8)\n+    self.assertLen(right_split, 2)\n     \n-    left_dataset,right_dataset = dataset_utils.split_dataset(dataset,\n+    left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                              left_size=0.35,\n                                                              right_size=0.65)\n-    self.assertLen(left_dataset, 4)\n-    self.assertLen(right_dataset, 6)\n-    self.assertIsInstance(left_dataset, tf.data.Dataset)\n-    self.assertIsInstance(right_dataset, tf.data.Dataset)\n+    self.assertLen(left_split, 4)\n+    self.assertLen(right_split, 6)\n+    self.assertIsInstance(left_split, tf.data.Dataset)\n+    self.assertIsInstance(right_split, tf.data.Dataset)\n     \n   def test_with_invalid_dataset(self):\n     with self.assertRaises(TypeError):\n@@ -88,11 +135,11 @@ class SplitDatasetTest(tf.test.TestCase):\n                                                    left_size=1,\n                                                    right_size=2)\n     assert(len(splitted_dataset) == 2)\n-    left_dataset,right_dataset = splitted_dataset\n-    self.assertEqual(len(left_dataset), 1)\n-    self.assertEqual(len(right_dataset), 2)\n-    self.assertEqual(list(left_dataset), [1])\n-    self.assertEqual(list(right_dataset), [2,3])\n+    left_split,right_split = splitted_dataset\n+    self.assertEqual(len(left_split), 1)\n+    self.assertEqual(len(right_split), 2)\n+    self.assertEqual(list(left_split), [1])\n+    self.assertEqual(list(right_split), [2,3])\n     \n     \n     dataset=np.ones(shape=(200, 32))\n@@ -129,32 +176,32 @@ class SplitDatasetTest(tf.test.TestCase):\n                                                    left_size=0.1,\n                                                    right_size=0.9)\n     assert(len(splitted_dataset) == 2)\n-    left_dataset,right_dataset = splitted_dataset\n-    self.assertEqual(len(left_dataset), 1 )\n-    self.assertEqual(len(right_dataset), 9 )\n-    self.assertEqual(list(left_dataset), [1])\n-    self.assertEqual(list(right_dataset), [2,3,4,5,6,7,8,9,10])\n+    left_split,right_split = splitted_dataset\n+    self.assertEqual(len(left_split), 1 )\n+    self.assertEqual(len(right_split), 9 )\n+    self.assertEqual(list(left_split), [1])\n+    self.assertEqual(list(right_split), [2,3,4,5,6,7,8,9,10])\n     \n     dataset = np.array([1,2,3,4,5,6,7,8,9,10])\n     splitted_dataset = dataset_utils.split_dataset(dataset,\n                                                    left_size=2,\n                                                    right_size=5)\n     assert(len(splitted_dataset) == 2)\n-    left_dataset,right_dataset = splitted_dataset\n-    self.assertEqual(len(left_dataset), 2 )\n-    self.assertEqual(len(right_dataset), 5 )\n-    self.assertEqual(list(left_dataset), [1,2])\n-    self.assertEqual(list(right_dataset), [6,7,8,9,10])\n+    left_split,right_split = splitted_dataset\n+    self.assertEqual(len(left_split), 2 )\n+    self.assertEqual(len(right_split), 5 )\n+    self.assertEqual(list(left_split), [1,2])\n+    self.assertEqual(list(right_split), [6,7,8,9,10])\n     \n   def test_with_float_left_and_right_sizes(self):\n     dataset = tf.data.Dataset.from_tensor_slices(np.array([[0.1,0.2,0.3],\n                                                            [0.4,0.5,0.6],\n                                                            [0.7,0.8,0.9]]))\n-    left_dataset,right_dataset = dataset_utils.split_dataset(dataset, \n+    left_split,right_split = dataset_utils.split_dataset(dataset, \n                                                              left_size=0.8,\n                                                              right_size=0.2)\n-    self.assertEqual(len(left_dataset), 2)\n-    self.assertEqual(len(right_dataset), 1)\n+    self.assertEqual(len(left_split), 2)\n+    self.assertEqual(len(right_split), 1)\n     \n   def test_with_invalid_float_left_and_right_sizes(self):\n     with self.assertRaises(ValueError):\n@@ -170,25 +217,40 @@ class SplitDatasetTest(tf.test.TestCase):\n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(dataset=np.array([1,2,3]), left_size=None)  \n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size=None,right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size=None,\n+                                  right_size=None)\n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size=3,right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size=3,\n+                                  right_size=None)\n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size=3,right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size=3,\n+                                  right_size=None)\n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,right_size=0)\n       \n   def test_with_invalid_left_and_right_size_types(self):      \n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size='1',right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size='1',\n+                                  right_size='1')\n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size=0,\n+                                  right_size='1')\n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size='100',right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size='100',\n+                                  right_size=None)\n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]),\n+                                  right_size='1')\n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size=0.5,right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]), \n+                                  left_size=0.5,\n+                                  right_size='1')\n       \n     \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4fa61684085e24c74b2d2b36e994df49a18ea70a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 1222 | Files Changed: 8 | Hunks: 11 | Methods Changed: 36 | Complexity Δ (Sum/Max): -86/7 | Churn Δ: 1345 | Churn Cumulative: 3883 | Contributors (this commit): 14 | Commits (past 90d): 30 | Contributors (cumulative): 43 | DMM Complexity: 0.21237458193979933\n\nDIFF:\n@@ -1,320 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras image dataset loading utilities.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-# pylint: disable=g-classes-have-attributes\n-\n-import numpy as np\n-from keras.layers.preprocessing import image_preprocessing\n-from keras.preprocessing import dataset_utils\n-from keras.preprocessing import image as keras_image_ops\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-ALLOWLIST_FORMATS = ('.bmp', '.gif', '.jpeg', '.jpg', '.png')\n-\n-\n-@keras_export('keras.utils.image_dataset_from_directory',\n-              'keras.preprocessing.image_dataset_from_directory',\n-              v1=[])\n-def image_dataset_from_directory(directory,\n-                                 labels='inferred',\n-                                 label_mode='int',\n-                                 class_names=None,\n-                                 color_mode='rgb',\n-                                 batch_size=32,\n-                                 image_size=(256, 256),\n-                                 shuffle=True,\n-                                 seed=None,\n-                                 validation_split=None,\n-                                 subset=None,\n-                                 interpolation='bilinear',\n-                                 follow_links=False,\n-                                 crop_to_aspect_ratio=False,\n-                                 **kwargs):\n-  \"\"\"Generates a `tf.data.Dataset` from image files in a directory.\n-\n-  If your directory structure is:\n-\n-  ```\n-  main_directory/\n-  ...class_a/\n-  ......a_image_1.jpg\n-  ......a_image_2.jpg\n-  ...class_b/\n-  ......b_image_1.jpg\n-  ......b_image_2.jpg\n-  ```\n-\n-  Then calling `image_dataset_from_directory(main_directory, labels='inferred')`\n-  will return a `tf.data.Dataset` that yields batches of images from\n-  the subdirectories `class_a` and `class_b`, together with labels\n-  0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n-\n-  Supported image formats: jpeg, png, bmp, gif.\n-  Animated gifs are truncated to the first frame.\n-\n-  Args:\n-    directory: Directory where the data is located.\n-        If `labels` is \"inferred\", it should contain\n-        subdirectories, each containing images for a class.\n-        Otherwise, the directory structure is ignored.\n-    labels: Either \"inferred\"\n-        (labels are generated from the directory structure),\n-        None (no labels),\n-        or a list/tuple of integer labels of the same size as the number of\n-        image files found in the directory. Labels should be sorted according\n-        to the alphanumeric order of the image file paths\n-        (obtained via `os.walk(directory)` in Python).\n-    label_mode:\n-        - 'int': means that the labels are encoded as integers\n-            (e.g. for `sparse_categorical_crossentropy` loss).\n-        - 'categorical' means that the labels are\n-            encoded as a categorical vector\n-            (e.g. for `categorical_crossentropy` loss).\n-        - 'binary' means that the labels (there can be only 2)\n-            are encoded as `float32` scalars with values 0 or 1\n-            (e.g. for `binary_crossentropy`).\n-        - None (no labels).\n-    class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n-        list of class names (must match names of subdirectories). Used\n-        to control the order of the classes\n-        (otherwise alphanumerical order is used).\n-    color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n-        Whether the images will be converted to\n-        have 1, 3, or 4 channels.\n-    batch_size: Size of the batches of data. Default: 32.\n-      If `None`, the data will not be batched\n-      (the dataset will yield individual samples).\n-    image_size: Size to resize images to after they are read from disk.\n-        Defaults to `(256, 256)`.\n-        Since the pipeline processes batches of images that must all have\n-        the same size, this must be provided.\n-    shuffle: Whether to shuffle the data. Default: True.\n-        If set to False, sorts the data in alphanumeric order.\n-    seed: Optional random seed for shuffling and transformations.\n-    validation_split: Optional float between 0 and 1,\n-        fraction of data to reserve for validation.\n-    subset: One of \"training\", \"validation\" or \"both\".\n-        Only used if `validation_split` is set.\n-    interpolation: String, the interpolation method used when resizing images.\n-      Defaults to `bilinear`. Supports `bilinear`, `nearest`, `bicubic`,\n-      `area`, `lanczos3`, `lanczos5`, `gaussian`, `mitchellcubic`.\n-    follow_links: Whether to visits subdirectories pointed to by symlinks.\n-        Defaults to False.\n-    crop_to_aspect_ratio: If True, resize the images without aspect\n-      ratio distortion. When the original aspect ratio differs from the target\n-      aspect ratio, the output image will be cropped so as to return the largest\n-      possible window in the image (of size `image_size`) that matches\n-      the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n-      aspect ratio may not be preserved.\n-    **kwargs: Legacy keyword arguments.\n-\n-  Returns:\n-    A `tf.data.Dataset` object.\n-      - If `label_mode` is None, it yields `float32` tensors of shape\n-        `(batch_size, image_size[0], image_size[1], num_channels)`,\n-        encoding images (see below for rules regarding `num_channels`).\n-      - Otherwise, it yields a tuple `(images, labels)`, where `images`\n-        has shape `(batch_size, image_size[0], image_size[1], num_channels)`,\n-        and `labels` follows the format described below.\n-\n-  Rules regarding labels format:\n-    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n-      `(batch_size,)`.\n-    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n-      1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `categorial`, the labels are a `float32` tensor\n-      of shape `(batch_size, num_classes)`, representing a one-hot\n-      encoding of the class index.\n-\n-  Rules regarding number of channels in the yielded images:\n-    - if `color_mode` is `grayscale`,\n-      there's 1 channel in the image tensors.\n-    - if `color_mode` is `rgb`,\n-      there are 3 channel in the image tensors.\n-    - if `color_mode` is `rgba`,\n-      there are 4 channel in the image tensors.\n-  \"\"\"\n-  if 'smart_resize' in kwargs:\n-    crop_to_aspect_ratio = kwargs.pop('smart_resize')\n-  if kwargs:\n-    raise TypeError(f'Unknown keywords argument(s): {tuple(kwargs.keys())}')\n-  if labels not in ('inferred', None):\n-    if not isinstance(labels, (list, tuple)):\n-      raise ValueError(\n-          '`labels` argument should be a list/tuple of integer labels, of '\n-          'the same size as the number of image files in the target '\n-          'directory. If you wish to infer the labels from the subdirectory '\n-          'names in the target directory, pass `labels=\"inferred\"`. '\n-          'If you wish to get a dataset that only contains images '\n-          f'(no labels), pass `labels=None`. Received: labels={labels}')\n-    if class_names:\n-      raise ValueError('You can only pass `class_names` if '\n-                       f'`labels=\"inferred\"`. Received: labels={labels}, and '\n-                       f'class_names={class_names}')\n-  if label_mode not in {'int', 'categorical', 'binary', None}:\n-    raise ValueError(\n-        '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n-        f'or None. Received: label_mode={label_mode}')\n-  if labels is None or label_mode is None:\n-    labels = None\n-    label_mode = None\n-  if color_mode == 'rgb':\n-    num_channels = 3\n-  elif color_mode == 'rgba':\n-    num_channels = 4\n-  elif color_mode == 'grayscale':\n-    num_channels = 1\n-  else:\n-    raise ValueError(\n-        '`color_mode` must be one of {\"rgb\", \"rgba\", \"grayscale\"}. '\n-        f'Received: color_mode={color_mode}')\n-  interpolation = image_preprocessing.get_interpolation(interpolation)\n-  dataset_utils.check_validation_split_arg(\n-      validation_split, subset, shuffle, seed)\n-\n-  if seed is None:\n-    seed = np.random.randint(1e6)\n-  image_paths, labels, class_names = dataset_utils.index_directory(\n-      directory,\n-      labels,\n-      formats=ALLOWLIST_FORMATS,\n-      class_names=class_names,\n-      shuffle=shuffle,\n-      seed=seed,\n-      follow_links=follow_links)\n-\n-  if label_mode == 'binary' and len(class_names) != 2:\n-    raise ValueError(\n-        f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n-        f'class_names. Received: class_names={class_names}')\n-\n-  if subset == \"both\":\n-    image_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n-        image_paths, labels, validation_split, \"training\")\n-    image_paths_val, labels_val = dataset_utils.get_training_or_validation_split(\n-        image_paths, labels, validation_split, \"validation\")\n-    if not image_paths_train:\n-      raise ValueError(f'No training images found in directory {directory}. '\n-                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n-    if not image_paths_val:\n-      raise ValueError(f'No validation images found in directory {directory}. '\n-                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n-    train_dataset = paths_and_labels_to_dataset(\n-        image_paths=image_paths_train,\n-        image_size=image_size,\n-        num_channels=num_channels,\n-        labels=labels_train,\n-        label_mode=label_mode,\n-        num_classes=len(class_names),\n-        interpolation=interpolation,\n-        crop_to_aspect_ratio=crop_to_aspect_ratio)\n-    val_dataset = paths_and_labels_to_dataset(\n-        image_paths=image_paths_val,\n-        image_size=image_size,\n-        num_channels=num_channels,\n-        labels=labels_val,\n-        label_mode=label_mode,\n-        num_classes=len(class_names),\n-        interpolation=interpolation,\n-        crop_to_aspect_ratio=crop_to_aspect_ratio)\n-    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n-    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n-    if batch_size is not None:\n-      if shuffle:\n-        # Shuffle locally at each iteration\n-        train_dataset = train_dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-      train_dataset = train_dataset.batch(batch_size)\n-      val_dataset = val_dataset.batch(batch_size)\n-    else:\n-      if shuffle:\n-        train_dataset = train_dataset.shuffle(buffer_size=1024, seed=seed)\n-        val_dataset = val_dataset.shuffle(buffer_size=1024, seed=seed)\n-\n-    # Users may need to reference `class_names`.\n-    train_dataset.class_names = class_names\n-    val_dataset.class_names = class_names\n-    # Include file paths for images as attribute.\n-    train_dataset.file_paths = image_paths_train\n-    val_dataset.file_paths = image_paths_val\n-    dataset = [train_dataset, val_dataset]\n-  else:\n-    image_paths, labels = dataset_utils.get_training_or_validation_split(\n-        image_paths, labels, validation_split, subset)\n-    if not image_paths:\n-      raise ValueError(f'No images found in directory {directory}. '\n-                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n-\n-    dataset = paths_and_labels_to_dataset(\n-        image_paths=image_paths,\n-        image_size=image_size,\n-        num_channels=num_channels,\n-        labels=labels,\n-        label_mode=label_mode,\n-        num_classes=len(class_names),\n-        interpolation=interpolation,\n-        crop_to_aspect_ratio=crop_to_aspect_ratio)\n-    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n-    if batch_size is not None:\n-      if shuffle:\n-        # Shuffle locally at each iteration\n-        dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-      dataset = dataset.batch(batch_size)\n-    else:\n-      if shuffle:\n-        dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n-\n-    # Users may need to reference `class_names`.\n-    dataset.class_names = class_names\n-    # Include file paths for images as attribute.\n-    dataset.file_paths = image_paths\n-  return dataset\n-\n-\n-def paths_and_labels_to_dataset(image_paths,\n-                                image_size,\n-                                num_channels,\n-                                labels,\n-                                label_mode,\n-                                num_classes,\n-                                interpolation,\n-                                crop_to_aspect_ratio=False):\n-  \"\"\"Constructs a dataset of images and labels.\"\"\"\n-  # TODO(fchollet): consider making num_parallel_calls settable\n-  path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n-  args = (image_size, num_channels, interpolation, crop_to_aspect_ratio)\n-  img_ds = path_ds.map(\n-      lambda x: load_image(x, *args), num_parallel_calls=tf.data.AUTOTUNE)\n-  if label_mode:\n-    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n-    img_ds = tf.data.Dataset.zip((img_ds, label_ds))\n-  return img_ds\n-\n-\n-def load_image(path, image_size, num_channels, interpolation,\n-               crop_to_aspect_ratio=False):\n-  \"\"\"Load an image from a path and resize it.\"\"\"\n-  img = tf.io.read_file(path)\n-  img = tf.image.decode_image(\n-      img, channels=num_channels, expand_animations=False)\n-  if crop_to_aspect_ratio:\n-    img = keras_image_ops.smart_resize(img, image_size,\n-                                       interpolation=interpolation)\n-  else:\n-    img = tf.image.resize(img, image_size, method=interpolation)\n-  img.set_shape((image_size[0], image_size[1], num_channels))\n-  return img\n\n@@ -1,385 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for image_dataset.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import os\n-import shutil\n-\n-import numpy as np\n-from keras import keras_parameterized\n-from keras.preprocessing import image as image_preproc\n-from keras.preprocessing import image_dataset\n-\n-try:\n-  import PIL  # pylint:disable=g-import-not-at-top\n-except ImportError:\n-  PIL = None\n-\n-\n-class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n-\n-  def _get_images(self, count=16, color_mode='rgb'):\n-    width = height = 24\n-    imgs = []\n-    for _ in range(count):\n-      if color_mode == 'grayscale':\n-        img = np.random.randint(0, 256, size=(height, width, 1))\n-      elif color_mode == 'rgba':\n-        img = np.random.randint(0, 256, size=(height, width, 4))\n-      else:\n-        img = np.random.randint(0, 256, size=(height, width, 3))\n-      img = image_preproc.array_to_img(img)\n-      imgs.append(img)\n-    return imgs\n-\n-  def _prepare_directory(self,\n-                         num_classes=2,\n-                         grayscale=False,\n-                         nested_dirs=False,\n-                         color_mode='rgb',\n-                         count=16):\n-    # Get a unique temp directory\n-    temp_dir = os.path.join(self.get_temp_dir(), str(np.random.randint(1e6)))\n-    os.mkdir(temp_dir)\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-\n-    # Generate paths to class subdirectories\n-    paths = []\n-    for class_index in range(num_classes):\n-      class_directory = 'class_%s' % (class_index,)\n-      if nested_dirs:\n-        class_paths = [\n-            class_directory, os.path.join(class_directory, 'subfolder_1'),\n-            os.path.join(class_directory, 'subfolder_2'), os.path.join(\n-                class_directory, 'subfolder_1', 'sub-subfolder')\n-        ]\n-      else:\n-        class_paths = [class_directory]\n-      for path in class_paths:\n-        os.mkdir(os.path.join(temp_dir, path))\n-      paths += class_paths\n-\n-    # Save images to the paths\n-    i = 0\n-    for img in self._get_images(color_mode=color_mode, count=count):\n-      path = paths[i % len(paths)]\n-      if color_mode == 'rgb':\n-        ext = 'jpg'\n-      else:\n-        ext = 'png'\n-      filename = os.path.join(path, 'image_%s.%s' % (i, ext))\n-      img.save(os.path.join(temp_dir, filename))\n-      i += 1\n-    return temp_dir\n-\n-  def test_image_dataset_from_directory_standalone(self):\n-    # Test retrieving images without labels from a directory and its subdirs.\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    # Save a few extra images in the parent directory.\n-    directory = self._prepare_directory(count=7, num_classes=2)\n-    for i, img in enumerate(self._get_images(3)):\n-      filename = 'image_%s.jpg' % (i,)\n-      img.save(os.path.join(directory, filename))\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=5, image_size=(18, 18), labels=None)\n-    batch = next(iter(dataset))\n-    # We return plain images\n-    self.assertEqual(batch.shape, (5, 18, 18, 3))\n-    self.assertEqual(batch.dtype.name, 'float32')\n-    # Count samples\n-    batch_count = 0\n-    sample_count = 0\n-    for batch in dataset:\n-      batch_count += 1\n-      sample_count += batch.shape[0]\n-    self.assertEqual(batch_count, 2)\n-    self.assertEqual(sample_count, 10)\n-\n-  def test_image_dataset_from_directory_binary(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode='int')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-    self.assertEqual(batch[1].shape, (8,))\n-    self.assertEqual(batch[1].dtype.name, 'int32')\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode='binary')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-    self.assertEqual(batch[1].shape, (8, 1))\n-    self.assertEqual(batch[1].dtype.name, 'float32')\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode='categorical')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-    self.assertEqual(batch[1].shape, (8, 2))\n-    self.assertEqual(batch[1].dtype.name, 'float32')\n-\n-  def test_static_shape_in_graph(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode='int')\n-    test_case = self\n-\n-    @tf.function\n-    def symbolic_fn(ds):\n-      for x, _ in ds.take(1):\n-        test_case.assertListEqual(x.shape.as_list(), [None, 18, 18, 3])\n-\n-    symbolic_fn(dataset)\n-\n-  def test_sample_count(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=4, count=15)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode=None)\n-    sample_count = 0\n-    for batch in dataset:\n-      sample_count += batch.shape[0]\n-    self.assertEqual(sample_count, 15)\n-\n-  def test_image_dataset_from_directory_multiclass(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=4, count=15)\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode=None)\n-    batch = next(iter(dataset))\n-    self.assertEqual(batch.shape, (8, 18, 18, 3))\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode=None)\n-    sample_count = 0\n-    iterator = iter(dataset)\n-    for batch in dataset:\n-      sample_count += next(iterator).shape[0]\n-    self.assertEqual(sample_count, 15)\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode='int')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-    self.assertEqual(batch[1].shape, (8,))\n-    self.assertEqual(batch[1].dtype.name, 'int32')\n-\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode='categorical')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-    self.assertEqual(batch[1].shape, (8, 4))\n-    self.assertEqual(batch[1].dtype.name, 'float32')\n-    \n-    train_dataset, val_dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=10, image_size=(18, 18),\n-        validation_split=0.2, subset='both', seed=1337)\n-    batch = next(iter(train_dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    batch = next(iter(val_dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (2, 18, 18, 3))\n-\n-  def test_image_dataset_from_directory_color_modes(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=4, color_mode='rgba')\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), color_mode='rgba')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 4))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-\n-    directory = self._prepare_directory(num_classes=4, color_mode='grayscale')\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), color_mode='grayscale')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 1))\n-    self.assertEqual(batch[0].dtype.name, 'float32')\n-\n-  def test_image_dataset_from_directory_validation_split(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2, count=10)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=10, image_size=(18, 18),\n-        validation_split=0.2, subset='training', seed=1337)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=10, image_size=(18, 18),\n-        validation_split=0.2, subset='validation', seed=1337)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (2, 18, 18, 3))\n-    \n-    train_dataset, val_dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=10, image_size=(18, 18),\n-        validation_split=0.2, subset='both', seed=1337)\n-    batch = next(iter(train_dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n-    batch = next(iter(val_dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (2, 18, 18, 3))\n-\n-  def test_image_dataset_from_directory_manual_labels(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2, count=2)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18),\n-        labels=[0, 1], shuffle=False)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertAllClose(batch[1], [0, 1])\n-\n-  def test_image_dataset_from_directory_follow_links(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2, count=25,\n-                                        nested_dirs=True)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=8, image_size=(18, 18), label_mode=None,\n-        follow_links=True)\n-    sample_count = 0\n-    for batch in dataset:\n-      sample_count += batch.shape[0]\n-    self.assertEqual(sample_count, 25)\n-\n-  def test_image_dataset_from_directory_no_images(self):\n-    directory = self._prepare_directory(num_classes=2, count=0)\n-    with self.assertRaisesRegex(ValueError, 'No images found.'):\n-      _ = image_dataset.image_dataset_from_directory(directory)\n-\n-  def test_image_dataset_from_directory_crop_to_aspect_ratio(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2, count=5)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory, batch_size=5, image_size=(18, 18), crop_to_aspect_ratio=True)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (5, 18, 18, 3))\n-\n-  def test_image_dataset_from_directory_errors(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=3, count=5)\n-\n-    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, labels='other')\n-\n-    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, label_mode='other')\n-\n-    with self.assertRaisesRegex(ValueError, '`color_mode` must be one of'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, color_mode='other')\n-\n-    with self.assertRaisesRegex(\n-        ValueError, 'only pass `class_names` if `labels=\"inferred\"`'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, labels=[0, 0, 1, 1, 1],\n-          class_names=['class_0', 'class_1', 'class_2'])\n-\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        'Expected the lengths of `labels` to match the number of files'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, labels=[0, 0, 1, 1])\n-\n-    with self.assertRaisesRegex(\n-        ValueError, '`class_names` passed did not match'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, class_names=['class_0', 'class_2'])\n-\n-    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, label_mode='binary')\n-\n-    with self.assertRaisesRegex(ValueError,\n-                                '`validation_split` must be between 0 and 1'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, validation_split=2)\n-\n-    with self.assertRaisesRegex(ValueError,\n-                                '`subset` must be either \"training\" or'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, validation_split=0.2, subset='other')\n-\n-    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, validation_split=0, subset='training')\n-\n-    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):\n-      _ = image_dataset.image_dataset_from_directory(\n-          directory, validation_split=0.2, subset='training')\n-\n-  def test_image_dataset_from_directory_not_batched(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    directory = self._prepare_directory(num_classes=2, count=2)\n-    dataset = image_dataset.image_dataset_from_directory(\n-        directory,\n-        batch_size=None,\n-        image_size=(18, 18),\n-        label_mode=None,\n-        shuffle=False)\n-    sample = next(iter(dataset))\n-    self.assertEqual(len(sample.shape), 3)\n-\n-if __name__ == '__main__':\n-  tf.compat.v1.enable_v2_behavior()\n-  tf.test.main()\n\n@@ -1,242 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras text dataset generation utilities.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-from keras.preprocessing import dataset_utils\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export('keras.utils.text_dataset_from_directory',\n-              'keras.preprocessing.text_dataset_from_directory',\n-              v1=[])\n-def text_dataset_from_directory(directory,\n-                                labels='inferred',\n-                                label_mode='int',\n-                                class_names=None,\n-                                batch_size=32,\n-                                max_length=None,\n-                                shuffle=True,\n-                                seed=None,\n-                                validation_split=None,\n-                                subset=None,\n-                                follow_links=False):\n-  \"\"\"Generates a `tf.data.Dataset` from text files in a directory.\n-\n-  If your directory structure is:\n-\n-  ```\n-  main_directory/\n-  ...class_a/\n-  ......a_text_1.txt\n-  ......a_text_2.txt\n-  ...class_b/\n-  ......b_text_1.txt\n-  ......b_text_2.txt\n-  ```\n-\n-  Then calling `text_dataset_from_directory(main_directory, labels='inferred')`\n-  will return a `tf.data.Dataset` that yields batches of texts from\n-  the subdirectories `class_a` and `class_b`, together with labels\n-  0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n-\n-  Only `.txt` files are supported at this time.\n-\n-  Args:\n-    directory: Directory where the data is located.\n-        If `labels` is \"inferred\", it should contain\n-        subdirectories, each containing text files for a class.\n-        Otherwise, the directory structure is ignored.\n-    labels: Either \"inferred\"\n-        (labels are generated from the directory structure),\n-        None (no labels),\n-        or a list/tuple of integer labels of the same size as the number of\n-        text files found in the directory. Labels should be sorted according\n-        to the alphanumeric order of the text file paths\n-        (obtained via `os.walk(directory)` in Python).\n-    label_mode:\n-        - 'int': means that the labels are encoded as integers\n-            (e.g. for `sparse_categorical_crossentropy` loss).\n-        - 'categorical' means that the labels are\n-            encoded as a categorical vector\n-            (e.g. for `categorical_crossentropy` loss).\n-        - 'binary' means that the labels (there can be only 2)\n-            are encoded as `float32` scalars with values 0 or 1\n-            (e.g. for `binary_crossentropy`).\n-        - None (no labels).\n-    class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n-        list of class names (must match names of subdirectories). Used\n-        to control the order of the classes\n-        (otherwise alphanumerical order is used).\n-    batch_size: Size of the batches of data. Default: 32.\n-      If `None`, the data will not be batched\n-      (the dataset will yield individual samples).\n-    max_length: Maximum size of a text string. Texts longer than this will\n-      be truncated to `max_length`.\n-    shuffle: Whether to shuffle the data. Default: True.\n-        If set to False, sorts the data in alphanumeric order.\n-    seed: Optional random seed for shuffling and transformations.\n-    validation_split: Optional float between 0 and 1,\n-        fraction of data to reserve for validation.\n-    subset: One of \"training\", \"validation\" or \"both\".\n-        Only used if `validation_split` is set.\n-    follow_links: Whether to visits subdirectories pointed to by symlinks.\n-        Defaults to False.\n-\n-  Returns:\n-    A `tf.data.Dataset` object.\n-      - If `label_mode` is None, it yields `string` tensors of shape\n-        `(batch_size,)`, containing the contents of a batch of text files.\n-      - Otherwise, it yields a tuple `(texts, labels)`, where `texts`\n-        has shape `(batch_size,)` and `labels` follows the format described\n-        below.\n-\n-  Rules regarding labels format:\n-    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n-      `(batch_size,)`.\n-    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n-      1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `categorial`, the labels are a `float32` tensor\n-      of shape `(batch_size, num_classes)`, representing a one-hot\n-      encoding of the class index.\n-  \"\"\"\n-  if labels not in ('inferred', None):\n-    if not isinstance(labels, (list, tuple)):\n-      raise ValueError(\n-          '`labels` argument should be a list/tuple of integer labels, of '\n-          'the same size as the number of text files in the target '\n-          'directory. If you wish to infer the labels from the subdirectory '\n-          'names in the target directory, pass `labels=\"inferred\"`. '\n-          'If you wish to get a dataset that only contains text samples '\n-          f'(no labels), pass `labels=None`. Received: labels={labels}')\n-    if class_names:\n-      raise ValueError('You can only pass `class_names` if '\n-                       f'`labels=\"inferred\"`. Received: labels={labels}, and '\n-                       f'class_names={class_names}')\n-  if label_mode not in {'int', 'categorical', 'binary', None}:\n-    raise ValueError(\n-        '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n-        f'or None. Received: label_mode={label_mode}')\n-  if labels is None or label_mode is None:\n-    labels = None\n-    label_mode = None\n-  dataset_utils.check_validation_split_arg(\n-      validation_split, subset, shuffle, seed)\n-\n-  if seed is None:\n-    seed = np.random.randint(1e6)\n-  file_paths, labels, class_names = dataset_utils.index_directory(\n-      directory,\n-      labels,\n-      formats=('.txt',),\n-      class_names=class_names,\n-      shuffle=shuffle,\n-      seed=seed,\n-      follow_links=follow_links)\n-\n-  if label_mode == 'binary' and len(class_names) != 2:\n-    raise ValueError(\n-        f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n-        f'class_names. Received: class_names={class_names}')\n-\n-  if subset == \"both\":\n-    file_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n-        file_paths, labels, validation_split, \"training\")\n-    file_paths_val, labels_val = dataset_utils.get_training_or_validation_split(\n-        file_paths, labels, validation_split, \"validation\")\n-    if not file_paths_train:\n-      raise ValueError(f'No training text files found in directory {directory}. '\n-                      f'Allowed format: .txt')\n-    if not file_paths_val:\n-      raise ValueError(f'No validation text files found in directory {directory}. '\n-                      f'Allowed format: .txt')\n-    train_dataset = paths_and_labels_to_dataset(\n-        file_paths=file_paths_train,\n-        labels=labels_train,\n-        label_mode=label_mode,\n-        num_classes=len(class_names),\n-        max_length=max_length)\n-    val_dataset = paths_and_labels_to_dataset(\n-        file_paths=file_paths_val,\n-        labels=labels_val,\n-        label_mode=label_mode,\n-        num_classes=len(class_names),\n-        max_length=max_length)\n-\n-    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n-    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n-    if batch_size is not None:\n-      if shuffle:\n-        # Shuffle locally at each iteration\n-        train_dataset = train_dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-      train_dataset = train_dataset.batch(batch_size)\n-      val_dataset = val_dataset.batch(batch_size)\n-    else:\n-      if shuffle:\n-        train_dataset = train_dataset.shuffle(buffer_size=1024, seed=seed)\n-    # Users may need to reference `class_names`.\n-    train_dataset.class_names = class_names\n-    val_dataset.class_names = class_names\n-    dataset = [train_dataset, val_dataset]\n-  else:\n-    file_paths, labels = dataset_utils.get_training_or_validation_split(\n-        file_paths, labels, validation_split, subset)\n-    if not file_paths:\n-      raise ValueError(f'No text files found in directory {directory}. '\n-                      f'Allowed format: .txt')\n-    dataset = paths_and_labels_to_dataset(\n-        file_paths=file_paths,\n-        labels=labels,\n-        label_mode=label_mode,\n-        num_classes=len(class_names),\n-        max_length=max_length)\n-    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n-    if batch_size is not None:\n-      if shuffle:\n-        # Shuffle locally at each iteration\n-        dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-      dataset = dataset.batch(batch_size)\n-    else:\n-      if shuffle:\n-        dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n-    # Users may need to reference `class_names`.\n-    dataset.class_names = class_names\n-  return dataset\n-\t\n-\n-\n-def paths_and_labels_to_dataset(file_paths,\n-                                labels,\n-                                label_mode,\n-                                num_classes,\n-                                max_length):\n-  \"\"\"Constructs a dataset of text strings and labels.\"\"\"\n-  path_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n-  string_ds = path_ds.map(\n-      lambda x: path_to_string_content(x, max_length),\n-      num_parallel_calls=tf.data.AUTOTUNE)\n-  if label_mode:\n-    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n-    string_ds = tf.data.Dataset.zip((string_ds, label_ds))\n-  return string_ds\n-\n-\n-def path_to_string_content(path, max_length):\n-  txt = tf.io.read_file(path)\n-  if max_length is not None:\n-    txt = tf.compat.v1.strings.substr(txt, 0, max_length)\n-  return txt\n\n@@ -1,272 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for text_dataset.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import os\n-import random\n-import shutil\n-import string\n-from keras import keras_parameterized\n-from keras.preprocessing import text_dataset\n-\n-\n-class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n-\n-  def _prepare_directory(self,\n-                         num_classes=2,\n-                         nested_dirs=False,\n-                         count=16,\n-                         length=20):\n-    # Get a unique temp directory\n-    temp_dir = os.path.join(self.get_temp_dir(), str(random.randint(0, 1e6)))\n-    os.mkdir(temp_dir)\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-\n-    # Generate paths to class subdirectories\n-    paths = []\n-    for class_index in range(num_classes):\n-      class_directory = 'class_%s' % (class_index,)\n-      if nested_dirs:\n-        class_paths = [\n-            class_directory, os.path.join(class_directory, 'subfolder_1'),\n-            os.path.join(class_directory, 'subfolder_2'), os.path.join(\n-                class_directory, 'subfolder_1', 'sub-subfolder')\n-        ]\n-      else:\n-        class_paths = [class_directory]\n-      for path in class_paths:\n-        os.mkdir(os.path.join(temp_dir, path))\n-      paths += class_paths\n-\n-    for i in range(count):\n-      path = paths[i % len(paths)]\n-      filename = os.path.join(path, 'text_%s.txt' % (i,))\n-      f = open(os.path.join(temp_dir, filename), 'w')\n-      text = ''.join([random.choice(string.printable) for _ in range(length)])\n-      f.write(text)\n-      f.close()\n-    return temp_dir\n-\n-  def test_text_dataset_from_directory_standalone(self):\n-    # Test retrieving txt files without labels from a directory and its subdirs.\n-    # Save a few extra files in the parent directory.\n-    directory = self._prepare_directory(count=7, num_classes=2)\n-    for i in range(3):\n-      filename = 'text_%s.txt' % (i,)\n-      f = open(os.path.join(directory, filename), 'w')\n-      text = ''.join([random.choice(string.printable) for _ in range(20)])\n-      f.write(text)\n-      f.close()\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=5, label_mode=None, max_length=10)\n-    batch = next(iter(dataset))\n-    # We just return the texts, no labels\n-    self.assertEqual(batch.shape, (5,))\n-    self.assertEqual(batch.dtype.name, 'string')\n-    # Count samples\n-    batch_count = 0\n-    sample_count = 0\n-    for batch in dataset:\n-      batch_count += 1\n-      sample_count += batch.shape[0]\n-    self.assertEqual(batch_count, 2)\n-    self.assertEqual(sample_count, 10)\n-\n-  def test_text_dataset_from_directory_binary(self):\n-    directory = self._prepare_directory(num_classes=2)\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode='int', max_length=10)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    self.assertEqual(batch[0].dtype.name, 'string')\n-    self.assertEqual(len(batch[0].numpy()[0]), 10)  # Test max_length\n-    self.assertEqual(batch[1].shape, (8,))\n-    self.assertEqual(batch[1].dtype.name, 'int32')\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode='binary')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    self.assertEqual(batch[0].dtype.name, 'string')\n-    self.assertEqual(batch[1].shape, (8, 1))\n-    self.assertEqual(batch[1].dtype.name, 'float32')\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode='categorical')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    self.assertEqual(batch[0].dtype.name, 'string')\n-    self.assertEqual(batch[1].shape, (8, 2))\n-    self.assertEqual(batch[1].dtype.name, 'float32')\n-\n-  def test_sample_count(self):\n-    directory = self._prepare_directory(num_classes=4, count=15)\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode=None)\n-    sample_count = 0\n-    for batch in dataset:\n-      sample_count += batch.shape[0]\n-    self.assertEqual(sample_count, 15)\n-\n-  def test_text_dataset_from_directory_multiclass(self):\n-    directory = self._prepare_directory(num_classes=4, count=15)\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode=None)\n-    batch = next(iter(dataset))\n-    self.assertEqual(batch.shape, (8,))\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode=None)\n-    sample_count = 0\n-    iterator = iter(dataset)\n-    for batch in dataset:\n-      sample_count += next(iterator).shape[0]\n-    self.assertEqual(sample_count, 15)\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode='int')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    self.assertEqual(batch[0].dtype.name, 'string')\n-    self.assertEqual(batch[1].shape, (8,))\n-    self.assertEqual(batch[1].dtype.name, 'int32')\n-\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode='categorical')\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    self.assertEqual(batch[0].dtype.name, 'string')\n-    self.assertEqual(batch[1].shape, (8, 4))\n-    self.assertEqual(batch[1].dtype.name, 'float32')\n-\n-  def test_text_dataset_from_directory_validation_split(self):\n-    directory = self._prepare_directory(num_classes=2, count=10)\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=10, validation_split=0.2, subset='training',\n-        seed=1337)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=10, validation_split=0.2, subset='validation',\n-        seed=1337)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (2,))\n-    \n-    train_dataset, val_dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=10, validation_split=0.2, subset='both',\n-        seed=1337)\n-    batch = next(iter(train_dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (8,))\n-    batch = next(iter(val_dataset))\n-    self.assertLen(batch, 2)\n-    self.assertEqual(batch[0].shape, (2,))\n-\n-  def test_text_dataset_from_directory_manual_labels(self):\n-    directory = self._prepare_directory(num_classes=2, count=2)\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, labels=[0, 1], shuffle=False)\n-    batch = next(iter(dataset))\n-    self.assertLen(batch, 2)\n-    self.assertAllClose(batch[1], [0, 1])\n-\n-  def test_text_dataset_from_directory_follow_links(self):\n-    directory = self._prepare_directory(num_classes=2, count=25,\n-                                        nested_dirs=True)\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=8, label_mode=None, follow_links=True)\n-    sample_count = 0\n-    for batch in dataset:\n-      sample_count += batch.shape[0]\n-    self.assertEqual(sample_count, 25)\n-\n-  def test_text_dataset_from_directory_no_files(self):\n-    directory = self._prepare_directory(num_classes=2, count=0)\n-    with self.assertRaisesRegex(ValueError, 'No text files found'):\n-      _ = text_dataset.text_dataset_from_directory(directory)\n-\n-  def test_text_dataset_from_directory_errors(self):\n-    directory = self._prepare_directory(num_classes=3, count=5)\n-\n-    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, labels='other')\n-\n-    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, label_mode='other')\n-\n-    with self.assertRaisesRegex(\n-        ValueError, 'only pass `class_names` if `labels=\"inferred\"`'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, labels=[0, 0, 1, 1, 1],\n-          class_names=['class_0', 'class_1', 'class_2'])\n-\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        'Expected the lengths of `labels` to match the number of files'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, labels=[0, 0, 1, 1])\n-\n-    with self.assertRaisesRegex(\n-        ValueError, '`class_names` passed did not match'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, class_names=['class_0', 'class_2'])\n-\n-    with self.assertRaisesRegex(ValueError, 'there must be exactly 2'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, label_mode='binary')\n-\n-    with self.assertRaisesRegex(ValueError,\n-                                '`validation_split` must be between 0 and 1'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, validation_split=2)\n-\n-    with self.assertRaisesRegex(ValueError,\n-                                '`subset` must be either \"training\" or'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, validation_split=0.2, subset='other')\n-\n-    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, validation_split=0, subset='training')\n-\n-    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):\n-      _ = text_dataset.text_dataset_from_directory(\n-          directory, validation_split=0.2, subset='training')\n-\n-  def test_text_dataset_from_directory_not_batched(self):\n-    directory = self._prepare_directory()\n-    dataset = text_dataset.text_dataset_from_directory(\n-        directory, batch_size=None, label_mode=None, follow_links=True)\n-\n-    sample = next(iter(dataset))\n-    self.assertEqual(len(sample.shape), 0)\n-\n-\n-if __name__ == '__main__':\n-  tf.compat.v1.enable_v2_behavior()\n-  tf.test.main()\n\n@@ -108,7 +108,7 @@ def image_dataset_from_directory(directory,\n     validation_split: Optional float between 0 and 1,\n         fraction of data to reserve for validation.\n     subset: Subset of the data to return.\n-        One of \"training\" or \"validation\".\n+        One of \"training\", \"validation\" or \"both\".\n         Only used if `validation_split` is set.\n     interpolation: String, the interpolation method used when resizing images.\n       Defaults to `bilinear`. Supports `bilinear`, `nearest`, `bicubic`,\n@@ -209,6 +209,62 @@ def image_dataset_from_directory(directory,\n     raise ValueError(f'No images found in directory {directory}. '\n                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n \n+  if subset == \"both\":\n+    image_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n+        image_paths, labels, validation_split, \"training\")\n+    image_paths_val, labels_val = dataset_utils.get_training_or_validation_split(\n+        image_paths, labels, validation_split, \"validation\")\n+    if not image_paths_train:\n+      raise ValueError(f'No training images found in directory {directory}. '\n+                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n+    if not image_paths_val:\n+      raise ValueError(f'No validation images found in directory {directory}. '\n+                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n+    train_dataset = paths_and_labels_to_dataset(\n+        image_paths=image_paths_train,\n+        image_size=image_size,\n+        num_channels=num_channels,\n+        labels=labels_train,\n+        label_mode=label_mode,\n+        num_classes=len(class_names),\n+        interpolation=interpolation,\n+        crop_to_aspect_ratio=crop_to_aspect_ratio)\n+    val_dataset = paths_and_labels_to_dataset(\n+        image_paths=image_paths_val,\n+        image_size=image_size,\n+        num_channels=num_channels,\n+        labels=labels_val,\n+        label_mode=label_mode,\n+        num_classes=len(class_names),\n+        interpolation=interpolation,\n+        crop_to_aspect_ratio=crop_to_aspect_ratio)\n+    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n+    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n+    if batch_size is not None:\n+      if shuffle:\n+        # Shuffle locally at each iteration\n+        train_dataset = train_dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n+      train_dataset = train_dataset.batch(batch_size)\n+      val_dataset = val_dataset.batch(batch_size)\n+    else:\n+      if shuffle:\n+        train_dataset = train_dataset.shuffle(buffer_size=1024, seed=seed)\n+        val_dataset = val_dataset.shuffle(buffer_size=1024, seed=seed)\n+\n+    # Users may need to reference `class_names`.\n+    train_dataset.class_names = class_names\n+    val_dataset.class_names = class_names\n+    # Include file paths for images as attribute.\n+    train_dataset.file_paths = image_paths_train\n+    val_dataset.file_paths = image_paths_val\n+    dataset = [train_dataset, val_dataset]\n+  else:\n+    image_paths, labels = dataset_utils.get_training_or_validation_split(\n+        image_paths, labels, validation_split, subset)\n+    if not image_paths:\n+      raise ValueError(f'No images found in directory {directory}. '\n+                      f'Allowed formats: {ALLOWLIST_FORMATS}')\n+\n     dataset = paths_and_labels_to_dataset(\n         image_paths=image_paths,\n         image_size=image_size,\n\n@@ -249,6 +249,16 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n     self.assertLen(batch, 2)\n     self.assertEqual(batch[0].shape, (2, 18, 18, 3))\n     \n+    train_dataset, val_dataset = image_dataset.image_dataset_from_directory(\n+        directory, batch_size=10, image_size=(18, 18),\n+        validation_split=0.2, subset='both', seed=1337)\n+    batch = next(iter(train_dataset))\n+    self.assertLen(batch, 2)\n+    self.assertEqual(batch[0].shape, (8, 18, 18, 3))\n+    batch = next(iter(val_dataset))\n+    self.assertLen(batch, 2)\n+    self.assertEqual(batch[0].shape, (2, 18, 18, 3))\n+\n   def test_image_dataset_from_directory_manual_labels(self):\n     if PIL is None:\n       return  # Skip test if PIL is not available.\n\n@@ -93,7 +93,7 @@ def text_dataset_from_directory(directory,\n     validation_split: Optional float between 0 and 1,\n         fraction of data to reserve for validation.\n     subset: Subset of the data to return.\n-        One of \"training\" or \"validation\".\n+        One of \"training\", \"validation\" or \"both\".\n         Only used if `validation_split` is set.\n     follow_links: Whether to visits subdirectories pointed to by symlinks.\n         Defaults to False.\n@@ -160,6 +160,51 @@ def text_dataset_from_directory(directory,\n     raise ValueError(f'No text files found in directory {directory}. '\n                      f'Allowed format: .txt')\n \n+  if subset == \"both\":\n+    file_paths_train, labels_train = dataset_utils.get_training_or_validation_split(\n+        file_paths, labels, validation_split, \"training\")\n+    file_paths_val, labels_val = dataset_utils.get_training_or_validation_split(\n+        file_paths, labels, validation_split, \"validation\")\n+    if not file_paths_train:\n+      raise ValueError(f'No training text files found in directory {directory}. '\n+                      f'Allowed format: .txt')\n+    if not file_paths_val:\n+      raise ValueError(f'No validation text files found in directory {directory}. '\n+                      f'Allowed format: .txt')\n+    train_dataset = paths_and_labels_to_dataset(\n+        file_paths=file_paths_train,\n+        labels=labels_train,\n+        label_mode=label_mode,\n+        num_classes=len(class_names),\n+        max_length=max_length)\n+    val_dataset = paths_and_labels_to_dataset(\n+        file_paths=file_paths_val,\n+        labels=labels_val,\n+        label_mode=label_mode,\n+        num_classes=len(class_names),\n+        max_length=max_length)\n+\n+    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n+    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n+    if batch_size is not None:\n+      if shuffle:\n+        # Shuffle locally at each iteration\n+        train_dataset = train_dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n+      train_dataset = train_dataset.batch(batch_size)\n+      val_dataset = val_dataset.batch(batch_size)\n+    else:\n+      if shuffle:\n+        train_dataset = train_dataset.shuffle(buffer_size=1024, seed=seed)\n+    # Users may need to reference `class_names`.\n+    train_dataset.class_names = class_names\n+    val_dataset.class_names = class_names\n+    dataset = [train_dataset, val_dataset]\n+  else:\n+    file_paths, labels = dataset_utils.get_training_or_validation_split(\n+        file_paths, labels, validation_split, subset)\n+    if not file_paths:\n+      raise ValueError(f'No text files found in directory {directory}. '\n+                      f'Allowed format: .txt')\n     dataset = paths_and_labels_to_dataset(\n         file_paths=file_paths,\n         labels=labels,\n@@ -175,7 +220,6 @@ def text_dataset_from_directory(directory,\n     else:\n       if shuffle:\n         dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n-\n     # Users may need to reference `class_names`.\n     dataset.class_names = class_names\n   return dataset\n\n@@ -177,6 +177,16 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n     self.assertLen(batch, 2)\n     self.assertEqual(batch[0].shape, (2,))\n     \n+    train_dataset, val_dataset = text_dataset.text_dataset_from_directory(\n+        directory, batch_size=10, validation_split=0.2, subset='both',\n+        seed=1337)\n+    batch = next(iter(train_dataset))\n+    self.assertLen(batch, 2)\n+    self.assertEqual(batch[0].shape, (8,))\n+    batch = next(iter(val_dataset))\n+    self.assertLen(batch, 2)\n+    self.assertEqual(batch[0].shape, (2,))\n+\n   def test_text_dataset_from_directory_manual_labels(self):\n     directory = self._prepare_directory(num_classes=2, count=2)\n     dataset = text_dataset.text_dataset_from_directory(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
