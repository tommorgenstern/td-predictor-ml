{"custom_id": "keras#0becb1fc8a018236fc7b493969cd1d92a7dc1ae4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 17 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 96 | Churn Cumulative: 4454 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,8 +40,8 @@ from keras.distribute.strategy_combinations import tpu_strategies\n from keras.engine import base_layer_utils\n from keras.mixed_precision import policy\n from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.utils import losses_utils\n from keras.utils import np_utils\n-from tensorflow.python.ops.losses import loss_reduction\n \n _RANDOM_SEED = 1337\n _TRAIN_SIZE = 200\n@@ -2036,9 +2036,9 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n           ],\n           mode=['graph', 'eager'],\n           reduction=[\n-              loss_reduction.ReductionV2.AUTO,\n-              loss_reduction.ReductionV2.SUM_OVER_BATCH_SIZE,\n-              loss_reduction.ReductionV2.SUM\n+              losses_utils.ReductionV2.AUTO,\n+              losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n+              losses_utils.ReductionV2.SUM\n           ]))\n   def test_distribution_strategy_with_loss_reduction_types(\n       self, distribution, reduction):\n@@ -2541,7 +2541,7 @@ class TestDistributionStrategyWithMultipleAddLossAndMetricCalls(\n           optimizer=keras.optimizers.adam_v2.Adam(1e-4),\n           loss=keras.losses.SparseCategoricalCrossentropy(\n               from_logits=True,\n-              reduction=loss_reduction.ReductionV2.SUM_OVER_BATCH_SIZE),\n+              reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE),\n           metrics=[\n               keras.metrics.SparseCategoricalAccuracy(),\n               keras.metrics.SparseCategoricalCrossentropy(from_logits=True),\n\n@@ -30,7 +30,7 @@ from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras.layers.preprocessing import string_lookup\n from keras.optimizer_v2 import rmsprop\n-from tensorflow.python.ops.losses import loss_reduction\n+from keras.utils import losses_utils\n from tensorflow.python.training.server_lib import ClusterSpec\n \n \n@@ -155,7 +155,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n           pred = model(batch_data, training=True)\n           loss = tf.nn.compute_average_loss(\n               keras.losses.BinaryCrossentropy(\n-                  reduction=loss_reduction.ReductionV2.NONE)(labels, pred))\n+                  reduction=losses_utils.ReductionV2.NONE)(labels, pred))\n           gradients = tape.gradient(loss, model.trainable_variables)\n \n         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n@@ -28,7 +28,7 @@ from keras import layers\n from keras import losses\n from keras import metrics\n from keras import testing_utils\n-from tensorflow.python.ops.losses import loss_reduction\n+from keras.utils import losses_utils\n \n \n def get_multi_io_model():\n@@ -557,9 +557,9 @@ class TestMetricsCorrectnessSingleIO(keras_parameterized.TestCase):\n @keras_parameterized.run_with_all_model_types(exclude_models=['sequential'])\n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n @parameterized.parameters([\n-    loss_reduction.ReductionV2.SUM_OVER_BATCH_SIZE,\n-    loss_reduction.ReductionV2.AUTO,\n-    loss_reduction.ReductionV2.SUM\n+    losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n+    losses_utils.ReductionV2.AUTO,\n+    losses_utils.ReductionV2.SUM\n ])\n class TestOutputLossMetrics(keras_parameterized.TestCase):\n \n@@ -615,25 +615,25 @@ class TestOutputLossMetrics(keras_parameterized.TestCase):\n     }\n \n     self.expected_fit_result = {\n-        loss_reduction.ReductionV2.NONE:\n+        losses_utils.ReductionV2.NONE:\n             sum_over_batch_size_fit_result,\n-        loss_reduction.ReductionV2.SUM: {\n+        losses_utils.ReductionV2.SUM: {\n             'loss': [198, 198],\n             'output_1_loss': [82, 82],\n             'output_2_loss': [116, 116],\n         },\n-        loss_reduction.ReductionV2.AUTO:\n+        losses_utils.ReductionV2.AUTO:\n             sum_over_batch_size_fit_result,\n-        loss_reduction.ReductionV2.SUM_OVER_BATCH_SIZE:\n+        losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE:\n             sum_over_batch_size_fit_result,\n     }\n \n     # In the order: 'loss', 'output_1_loss', 'output_2_loss',\n     self.expected_batch_result = {\n-        loss_reduction.ReductionV2.NONE: [144, 56, 88],\n-        loss_reduction.ReductionV2.SUM: [198, 82, 116],\n-        loss_reduction.ReductionV2.AUTO: [144, 56, 88],\n-        loss_reduction.ReductionV2.SUM_OVER_BATCH_SIZE: [144, 56, 88],\n+        losses_utils.ReductionV2.NONE: [144, 56, 88],\n+        losses_utils.ReductionV2.SUM: [198, 82, 116],\n+        losses_utils.ReductionV2.AUTO: [144, 56, 88],\n+        losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE: [144, 56, 88],\n     }\n \n     # 2 + 12 + 36 + 80 + 150 = 280\n@@ -675,7 +675,7 @@ class TestOutputLossMetrics(keras_parameterized.TestCase):\n                                   })\n \n     expected_values = self.expected_batch_result[reduction]\n-    if reduction == loss_reduction.ReductionV2.SUM:\n+    if reduction == losses_utils.ReductionV2.SUM:\n       expected_values = self.expected_single_batch_result\n     self.assertAllClose(result, expected_values)\n \n@@ -688,7 +688,7 @@ class TestOutputLossMetrics(keras_parameterized.TestCase):\n                                      'output_2': self.sample_weight_2,\n                                  })\n     expected_values = self.expected_batch_result[reduction]\n-    if reduction == loss_reduction.ReductionV2.SUM:\n+    if reduction == losses_utils.ReductionV2.SUM:\n       expected_values = self.expected_single_batch_result\n     self.assertAllClose(result, expected_values)\n \n\n@@ -21,14 +21,58 @@ from __future__ import print_function\n import tensorflow as tf\n from keras import backend as K\n from keras.engine import keras_tensor\n-from tensorflow.python.ops.losses import loss_reduction\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# TODO(joshl/psv): Update references to ReductionV2 to point to its\n-# new location.\n-ReductionV2 = loss_reduction.ReductionV2\n-keras_export('keras.losses.Reduction', v1=[], allow_multiple_exports=True)(loss_reduction.ReductionV2)\n+@keras_export('keras.losses.Reduction', v1=[])\n+class ReductionV2(object):\n+  \"\"\"Types of loss reduction.\n+\n+  Contains the following values:\n+\n+  * `AUTO`: Indicates that the reduction option will be determined by the usage\n+     context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When\n+     used with `tf.distribute.Strategy`, outside of built-in training loops such\n+     as `tf.keras` `compile` and `fit`, we expect reduction value to be\n+     `SUM` or `NONE`. Using `AUTO` in that case will raise an error.\n+  * `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis\n+     specified by loss function). When this reduction type used with built-in\n+     Keras training loops like `fit`/`evaluate`, the unreduced vector loss is\n+     passed to the optimizer but the reported loss will be a scalar value.\n+  * `SUM`: Scalar sum of weighted losses.\n+  * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses.\n+     This reduction type is not supported when used with\n+     `tf.distribute.Strategy` outside of built-in training loops like `tf.keras`\n+     `compile`/`fit`.\n+\n+     You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:\n+     ```\n+     with strategy.scope():\n+       loss_obj = tf.keras.losses.CategoricalCrossentropy(\n+           reduction=tf.keras.losses.Reduction.NONE)\n+       ....\n+       loss = tf.reduce_sum(loss_obj(labels, predictions)) *\n+           (1. / global_batch_size)\n+     ```\n+\n+  Please see the [custom training guide](\n+  https://www.tensorflow.org/tutorials/distribute/custom_training) for more\n+  details on this.\n+  \"\"\"\n+\n+  AUTO = 'auto'\n+  NONE = 'none'\n+  SUM = 'sum'\n+  SUM_OVER_BATCH_SIZE = 'sum_over_batch_size'\n+\n+  @classmethod\n+  def all(cls):\n+    return (cls.AUTO, cls.NONE, cls.SUM, cls.SUM_OVER_BATCH_SIZE)\n+\n+  @classmethod\n+  def validate(cls, key):\n+    if key not in cls.all():\n+      raise ValueError('Invalid Reduction Key %s.' % key)\n \n \n def remove_squeezable_dimensions(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb89648ac93c8b8503a1c1059707caad8ec71f78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 85 | Files Changed: 9 | Hunks: 35 | Methods Changed: 21 | Complexity Δ (Sum/Max): 15/5 | Churn Δ: 291 | Churn Cumulative: 24098 | Contributors (this commit): 94 | Commits (past 90d): 31 | Contributors (cumulative): 126 | DMM Complexity: 0.9479166666666666\n\nDIFF:\n@@ -406,12 +406,12 @@ def block3(x,\n       depth_multiplier=c,\n       use_bias=False,\n       name=name + '_2_conv')(x)\n-  x_shape = backend.int_shape(x)[1:-1]\n-  x = layers.Reshape(x_shape + (groups, c, c))(x)\n+  x_shape = backend.shape(x)[:-1]\n+  x = backend.reshape(x, backend.concatenate([x_shape, (groups, c, c)]))\n   x = layers.Lambda(\n       lambda x: sum(x[:, :, :, :, i] for i in range(c)),\n       name=name + '_2_reduce')(x)\n-  x = layers.Reshape(x_shape + (filters,))(x)\n+  x = backend.reshape(x, backend.concatenate([x_shape, (filters,)]))\n   x = layers.BatchNormalization(\n       axis=bn_axis, epsilon=1.001e-5, name=name + '_2_bn')(x)\n   x = layers.Activation('relu', name=name + '_2_relu')(x)\n\n@@ -67,38 +67,38 @@ class KerasLayerBenchmarks(six.with_metaclass(\n           \"activation\": \"relu\"\n       }, {\n           \"input_shape\": (1, 1, 1, 1)\n-      }, 10),\n+      }, 100),\n       (\"Conv2D_normal_shape\", tf.keras.layers.Conv2D, {\n           \"filters\": 1,\n           \"kernel_size\": 1,\n           \"activation\": \"relu\"\n       }, {\n           \"input_shape\": (64, 28, 28, 3)\n-      }, 10),\n+      }, 100),\n       (\"LSTM_small_shape\", tf.keras.layers.LSTM, {\n           \"units\": 1\n       }, {\n           \"input_shape\": (1, 1, 1)\n-      }, 10),\n+      }, 100),\n       (\"LSTM_normal_shape\", tf.keras.layers.LSTM, {\n           \"units\": 4\n       }, {\n           \"input_shape\": (32, 10, 8)\n-      }, 10),\n+      }, 100),\n       (\"Embedding_small_shape\", tf.keras.layers.Embedding, {\n           \"input_dim\": 1,\n           \"output_dim\": 1,\n           \"input_length\": 1\n       }, {\n           \"input\": np.random.randint(1, size=(1, 1))\n-      }, 10),\n+      }, 100),\n       (\"Embedding_normal_shape\", tf.keras.layers.Embedding, {\n           \"input_dim\": 1000,\n           \"output_dim\": 64,\n           \"input_length\": 10\n       }, {\n           \"input\": np.random.randint(1000, size=(32, 10))\n-      }, 10),\n+      }, 100),\n   ])\n \n   def benchmark_layer_call(self, layer_cls, layer_args, inputs, num_iters):\n@@ -162,50 +162,12 @@ class KerasLayerBenchmarks(six.with_metaclass(\n     metadata.update(_get_metadata(name))\n     self.run_report(fn, num_iters, metadata)\n \n-\n-class KerasLayerBenchmarksBackwardXLA(six.with_metaclass(\n-    tf.__internal__.test.ParameterizedBenchmark,\n-    layer_benchmarks_test_base.LayerBenchmarksBase)):\n-\n-  _benchmark_parameters = benchmark_util.generate_benchmark_params_cpu_gpu([\n-      (\"Conv2D_small_shape\", tf.keras.layers.Conv2D, {\n-          \"filters\": 1,\n-          \"kernel_size\": 1,\n-          \"activation\": \"relu\"\n-      }, {\n-          \"input_shape\": (1, 1, 1, 1)\n-      }, 10000),\n-      (\"Conv2D_normal_shape\", tf.keras.layers.Conv2D, {\n-          \"filters\": 1,\n-          \"kernel_size\": 1,\n-          \"activation\": \"relu\"\n-      }, {\n-          \"input_shape\": (64, 28, 28, 3)\n-      }, 10000),\n-      # TODO(b/153480400)\n-      # (\"LSTM_small_shape\", tf.keras.layers.LSTM,\n-      #  {\"units\": 1}, {\"input_shape\": (1, 1, 1)}, 10000),\n-      # (\"LSTM_normal_shape\", tf.keras.layers.LSTM,\n-      #  {\"units\": 4}, {\"input_shape\": (32, 10, 8)}, 10000),\n-      (\"Embedding_small_shape\", tf.keras.layers.Embedding, {\n-          \"input_dim\": 1,\n-          \"output_dim\": 1,\n-          \"input_length\": 1\n-      }, {\n-          \"input\": np.random.randint(1, size=(1, 1))\n-      }, 10),\n-      (\"Embedding_normal_shape\", tf.keras.layers.Embedding, {\n-          \"input_dim\": 1000,\n-          \"output_dim\": 64,\n-          \"input_length\": 10\n-      }, {\n-          \"input\": np.random.randint(1000, size=(32, 10))\n-      }, 10),\n-  ])\n-\n   def benchmark_layer_call_backward_with_xla(\n       self, layer_cls, layer_args, inputs, num_iters):\n     name = benchmark_util.get_benchmark_name(self._get_name())\n+    # TODO(b/153480400)\n+    if layer_cls is tf.keras.layers.LSTM:\n+      return\n     # TODO(b/173461426)\n     if layer_cls is tf.keras.layers.Embedding and name[-1] == \"GPU\":\n       return\n\n@@ -27,6 +27,8 @@ import numpy as np\n \n import keras\n from keras.distribute import strategy_combinations\n+from keras.layers import core\n+from keras.optimizer_v2 import gradient_descent\n \n \n class CustomModel(tf.Module):\n@@ -404,6 +406,38 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n     loss = train_step(input_iterator)\n     loss = distribution.reduce(tf.distribute.ReduceOp.MEAN, loss, axis=0)\n \n+  def test_variable_run_argument(self, distribution):\n+    # Test that variables passed to run() remain variables. Previous behavior\n+    # in TPUStrategy was to cast to Tensor.\n+\n+    with distribution.scope():\n+      optimizer = gradient_descent.SGD(0.1)\n+      net = core.Dense(1, trainable=True)\n+    dataset = tf.data.Dataset.from_tensors([[1.]])\n+    dataset = dataset.repeat()\n+    dataset = dataset.batch(2, drop_remainder=True)\n+\n+    def replica_step(trainable_variables, features):\n+\n+      with tf.GradientTape() as tape:\n+        net_out = net(features[0], training=True)\n+        loss = (net_out - 1.0) * (net_out - 1.0)\n+      gradients = tape.gradient(loss, trainable_variables)\n+      optimizer.apply_gradients(zip(gradients, trainable_variables))\n+      return loss\n+\n+    @tf.function\n+    def step(features):\n+      per_replica_losses = distribution.run(\n+          replica_step,\n+          (net.trainable_variables, features),\n+      )\n+      loss = distribution.reduce(\n+          tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n+      return loss\n+\n+    step(next(iter(dataset)))\n+\n \n class KerasModelsXLATest(tf.test.TestCase, parameterized.TestCase):\n \n\n@@ -41,7 +41,6 @@ from keras.layers.pooling import MaxPooling3D\n # pylint: enable=unused-import\n from keras.utils import conv_utils\n from keras.utils import tf_utils\n-from tensorflow.python.ops import nn_ops\n from tensorflow.python.util.tf_export import keras_export\n # pylint: disable=g-classes-have-attributes\n \n@@ -257,7 +256,7 @@ class Conv(Layer):\n           def _apply_fn(o):\n             return tf.nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n \n-          outputs = nn_ops.squeeze_batch_dims(\n+          outputs = conv_utils.squeeze_batch_dims(\n               outputs, _apply_fn, inner_rank=self.rank + 1)\n         else:\n           outputs = tf.nn.bias_add(\n\n@@ -23,7 +23,6 @@ import tensorflow as tf\n import functools\n import numpy as np\n from keras.engine import base_preprocessing_layer\n-from keras.utils import tf_utils\n from tensorflow.python.util.tf_export import keras_export\n \n # Default key from tf.sparse.cross_hashed\n@@ -61,6 +60,19 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n            [1],\n            [2]])>\n \n+  Example (FarmHash64) with a mask value:\n+\n+  >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n+  ...    mask_value='')\n+  >>> inp = [['A'], ['B'], [''], ['C'], ['D']]\n+  >>> layer(inp)\n+  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n+    array([[1],\n+           [1],\n+           [0],\n+           [2],\n+           [2]])>\n+\n \n   Example (FarmHash64) with list of inputs:\n   >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)\n@@ -104,7 +116,12 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n   Reference: [SipHash with salt](https://www.131002.net/siphash/siphash.pdf)\n \n   Args:\n-    num_bins: Number of hash bins.\n+    num_bins: Number of hash bins. Note that this includes the `mask_value` bin,\n+      so the effective number of bins is `(num_bins - 1)` if `mask_value` is\n+      set.\n+    mask_value: A value that represents masked inputs, which are mapped to\n+      index 0. Defaults to None, meaning no mask term will be added and the\n+      hashing will start at index 0.\n     salt: A single unsigned integer or None.\n       If passed, the hash function used will be SipHash64, with these values\n       used as an additional input (known as a \"salt\" in cryptography).\n@@ -124,12 +141,13 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n \n   \"\"\"\n \n-  def __init__(self, num_bins, salt=None, name=None, **kwargs):\n+  def __init__(self, num_bins, mask_value=None, salt=None, name=None, **kwargs):\n     if num_bins is None or num_bins <= 0:\n       raise ValueError('`num_bins` cannot be `None` or non-positive values.')\n     super(Hashing, self).__init__(name=name, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Hashing').set(True)\n     self.num_bins = num_bins\n+    self.mask_value = mask_value\n     self.strong_hash = True if salt is not None else False\n     if salt is not None:\n       if isinstance(salt, (tuple, list)) and len(salt) == 2:\n@@ -160,39 +178,22 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n     inputs = self._preprocess_inputs(inputs)\n     if isinstance(inputs, (tuple, list)):\n       return self._process_input_list(inputs)\n-    else:\n-      return self._process_single_input(inputs)\n-\n-  def _process_single_input(self, inputs):\n-    # Converts integer inputs to string.\n-    if inputs.dtype.is_integer:\n-      if isinstance(inputs, tf.SparseTensor):\n-        inputs = tf.SparseTensor(\n-            indices=inputs.indices,\n-            values=tf.as_string(inputs.values),\n-            dense_shape=inputs.dense_shape)\n-      else:\n-        inputs = tf.as_string(inputs)\n-    str_to_hash_bucket = self._get_string_to_hash_bucket_fn()\n-    if tf_utils.is_ragged(inputs):\n-      return tf.ragged.map_flat_values(\n-          str_to_hash_bucket, inputs, num_buckets=self.num_bins, name='hash')\n     elif isinstance(inputs, tf.SparseTensor):\n-      sparse_values = inputs.values\n-      sparse_hashed_values = str_to_hash_bucket(\n-          sparse_values, self.num_bins, name='hash')\n       return tf.SparseTensor(\n           indices=inputs.indices,\n-          values=sparse_hashed_values,\n+          values=self._hash_values_to_bins(inputs.values),\n           dense_shape=inputs.dense_shape)\n-    else:\n-      return str_to_hash_bucket(inputs, self.num_bins, name='hash')\n+    return self._hash_values_to_bins(inputs)\n \n   def _process_input_list(self, inputs):\n     # TODO(momernick): support ragged_cross_hashed with corrected fingerprint\n     # and siphash.\n     if any(isinstance(inp, tf.RaggedTensor) for inp in inputs):\n       raise ValueError('Hashing with ragged input is not supported yet.')\n+    if self.mask_value is not None:\n+      raise ValueError(\n+          'Cross hashing with a mask_value is not supported yet, mask_value is '\n+          '{}.'.format(self.mask_value))\n     sparse_inputs = [\n         inp for inp in inputs if isinstance(inp, tf.SparseTensor)\n     ]\n@@ -216,6 +217,24 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n       return tf.sparse.to_dense(sparse_out)\n     return sparse_out\n \n+  def _hash_values_to_bins(self, values):\n+    \"\"\"Converts a non-sparse tensor of values to bin indices.\"\"\"\n+    str_to_hash_bucket = self._get_string_to_hash_bucket_fn()\n+    num_available_bins = self.num_bins\n+    mask = None\n+    # If mask_value is set, the zeroth bin is reserved for it.\n+    if self.mask_value is not None and num_available_bins > 1:\n+      num_available_bins -= 1\n+      mask = tf.equal(values, self.mask_value)\n+    # Convert all values to strings before hashing.\n+    if values.dtype.is_integer:\n+      values = tf.as_string(values)\n+    values = str_to_hash_bucket(values, num_available_bins, name='hash')\n+    if mask is not None:\n+      values = tf.add(values, tf.compat.v1.ones_like(values))\n+      values = tf.compat.v1.where(mask, tf.compat.v1.zeros_like(values), values)\n+    return values\n+\n   def _get_string_to_hash_bucket_fn(self):\n     \"\"\"Returns the string_to_hash_bucket op to use based on `hasher_key`.\"\"\"\n     # string_to_hash_bucket_fast uses FarmHash64 as hash function.\n@@ -264,6 +283,10 @@ class Hashing(base_preprocessing_layer.PreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=tf.int64)\n \n   def get_config(self):\n-    config = {'num_bins': self.num_bins, 'salt': self.salt}\n+    config = {\n+        'num_bins': self.num_bins,\n+        'salt': self.salt,\n+        'mask_value': self.mask_value,\n+    }\n     base_config = super(Hashing, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -45,6 +45,27 @@ class HashingTest(keras_parameterized.TestCase):\n     # Assert equal for hashed output that should be true on all platforms.\n     self.assertAllClose([[0], [0], [1], [0], [0]], output)\n \n+  def test_hash_dense_input_mask_value_farmhash(self):\n+    empty_mask_layer = hashing.Hashing(num_bins=3, mask_value='')\n+    omar_mask_layer = hashing.Hashing(num_bins=3, mask_value='omar')\n+    inp = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n+                      ['skywalker']])\n+    empty_mask_output = empty_mask_layer(inp)\n+    omar_mask_output = omar_mask_layer(inp)\n+    # Outputs should be one more than test_hash_dense_input_farmhash (the zeroth\n+    # bin is now reserved for masks).\n+    self.assertAllClose([[1], [1], [2], [1], [1]], empty_mask_output)\n+    # 'omar' should map to 0.\n+    self.assertAllClose([[0], [1], [2], [1], [1]], omar_mask_output)\n+\n+  def test_hash_dense_multi_inputs_mask_value_farmhash(self):\n+    layer = hashing.Hashing(num_bins=3, mask_value='omar')\n+    inp_1 = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n+                        ['skywalker']])\n+    inp_2 = np.asarray([['A'], ['B'], ['C'], ['D'], ['E']])\n+    with self.assertRaisesRegex(ValueError, 'not supported yet'):\n+      _ = layer([inp_1, inp_2])\n+\n   def test_hash_dense_multi_inputs_farmhash(self):\n     layer = hashing.Hashing(num_bins=2)\n     inp_1 = np.asarray([['omar'], ['stringer'], ['marlo'], ['wire'],\n@@ -129,6 +150,24 @@ class HashingTest(keras_parameterized.TestCase):\n     self.assertAllClose(indices, output.indices)\n     self.assertAllClose([0, 0, 1, 0, 0], output.values)\n \n+  def test_hash_sparse_input_mask_value_farmhash(self):\n+    empty_mask_layer = hashing.Hashing(num_bins=3, mask_value='')\n+    omar_mask_layer = hashing.Hashing(num_bins=3, mask_value='omar')\n+    indices = [[0, 0], [1, 0], [1, 1], [2, 0], [2, 1]]\n+    inp = tf.SparseTensor(\n+        indices=indices,\n+        values=['omar', 'stringer', 'marlo', 'wire', 'skywalker'],\n+        dense_shape=[3, 2])\n+    empty_mask_output = empty_mask_layer(inp)\n+    omar_mask_output = omar_mask_layer(inp)\n+    self.assertAllClose(indices, omar_mask_output.indices)\n+    self.assertAllClose(indices, empty_mask_output.indices)\n+    # Outputs should be one more than test_hash_sparse_input_farmhash (the\n+    # zeroth bin is now reserved for masks).\n+    self.assertAllClose([1, 1, 2, 1, 1], empty_mask_output.values)\n+    # 'omar' should map to 0.\n+    self.assertAllClose([0, 1, 2, 1, 1], omar_mask_output.values)\n+\n   def test_hash_sparse_multi_inputs_farmhash(self):\n     layer = hashing.Hashing(num_bins=2)\n     indices = [[0, 0], [1, 0], [2, 0]]\n@@ -211,6 +250,22 @@ class HashingTest(keras_parameterized.TestCase):\n     model = training.Model(inputs=inp_t, outputs=out_t)\n     self.assertAllClose(out_data, model.predict(inp_data))\n \n+  def test_hash_ragged_input_mask_value(self):\n+    empty_mask_layer = hashing.Hashing(num_bins=3, mask_value='')\n+    omar_mask_layer = hashing.Hashing(num_bins=3, mask_value='omar')\n+    inp_data = tf.ragged.constant(\n+        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],\n+        dtype=tf.string)\n+    empty_mask_output = empty_mask_layer(inp_data)\n+    omar_mask_output = omar_mask_layer(inp_data)\n+    # Outputs should be one more than test_hash_ragged_string_input_farmhash\n+    # (the zeroth bin is now reserved for masks).\n+    expected_output = [[1, 1, 2, 1], [2, 1, 1]]\n+    self.assertAllClose(expected_output, empty_mask_output)\n+    # 'omar' should map to 0.\n+    expected_output = [[0, 1, 2, 1], [2, 1, 1]]\n+    self.assertAllClose(expected_output, omar_mask_output)\n+\n   def test_hash_ragged_string_multi_inputs_farmhash(self):\n     layer = hashing.Hashing(num_bins=2)\n     inp_data_1 = tf.ragged.constant(\n\n@@ -323,8 +323,8 @@ class _NormalizingCombiner(base_preprocessing_layer.Combiner):\n     \"\"\"Convert an accumulator into a dict of output values.\"\"\"\n     return {\n         _COUNT_NAME: accumulator[self.COUNT_IDX],\n-        _MEAN_NAME: accumulator[1],\n-        _VARIANCE_NAME: accumulator[2]\n+        _MEAN_NAME: accumulator[self.MEAN_IDX],\n+        _VARIANCE_NAME: accumulator[self.VAR_IDX]\n     }\n \n   def restore(self, output):\n@@ -347,8 +347,8 @@ class _NormalizingCombiner(base_preprocessing_layer.Combiner):\n     \"\"\"Serialize an accumulator for a remote call.\"\"\"\n     output_dict = {\n         _COUNT_NAME: accumulator[self.COUNT_IDX].tolist(),\n-        _MEAN_NAME: accumulator[1].tolist(),\n-        _VARIANCE_NAME: accumulator[2].tolist()\n+        _MEAN_NAME: accumulator[self.MEAN_IDX].tolist(),\n+        _VARIANCE_NAME: accumulator[self.VAR_IDX].tolist()\n     }\n     return tf.compat.as_bytes(json.dumps(output_dict))\n \n\n@@ -17,11 +17,12 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import tensorflow as tf\n+\n import itertools\n \n import numpy as np\n from six.moves import range  # pylint: disable=redefined-builtin\n-\n from keras import backend\n \n \n@@ -467,3 +468,50 @@ def conv_output_shape(input_shape, kernel_shape, strides, padding):\n   output_shape = tuple(\n       [0 if input_shape[d] == 0 else output_shape[d] for d in dims])\n   return output_shape\n+\n+\n+def squeeze_batch_dims(inp, op, inner_rank):\n+  \"\"\"Returns `unsqueeze_batch(op(squeeze_batch(inp)))`.\n+\n+  Where `squeeze_batch` reshapes `inp` to shape\n+  `[prod(inp.shape[:-inner_rank])] + inp.shape[-inner_rank:]`\n+  and `unsqueeze_batch` does the reverse reshape but on the output.\n+\n+  Args:\n+    inp: A tensor with dims `batch_shape + inner_shape` where `inner_shape`\n+      is length `inner_rank`.\n+    op: A callable that takes a single input tensor and returns a single.\n+      output tensor.\n+    inner_rank: A python integer.\n+\n+  Returns:\n+    `unsqueeze_batch_op(squeeze_batch(inp))`.\n+  \"\"\"\n+  with tf.name_scope('squeeze_batch_dims'):\n+    shape = inp.shape\n+\n+    inner_shape = shape[-inner_rank:]\n+    if not inner_shape.is_fully_defined():\n+      inner_shape = tf.compat.v1.shape(inp)[-inner_rank:]\n+\n+    batch_shape = shape[:-inner_rank]\n+    if not batch_shape.is_fully_defined():\n+      batch_shape = tf.compat.v1.shape(inp)[:-inner_rank]\n+\n+    if isinstance(inner_shape, tf.TensorShape):\n+      inp_reshaped = tf.reshape(inp, [-1] + inner_shape.as_list())\n+    else:\n+      inp_reshaped = tf.reshape(\n+          inp, tf.concat(([-1], inner_shape), axis=-1))\n+\n+    out_reshaped = op(inp_reshaped)\n+\n+    out_inner_shape = out_reshaped.shape[-inner_rank:]\n+    if not out_inner_shape.is_fully_defined():\n+      out_inner_shape = tf.compat.v1.shape(out_reshaped)[-inner_rank:]\n+\n+    out = tf.reshape(\n+        out_reshaped, tf.concat((batch_shape, out_inner_shape), axis=-1))\n+\n+    out.set_shape(inp.shape[:-inner_rank] + out.shape[-inner_rank:])\n+    return out\n\n@@ -261,7 +261,7 @@ class KerasClassifier(BaseWrapper):\n             (instead of `(n_sample, 1)` as in Keras).\n     \"\"\"\n     kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n-    probs = self.model.predict_proba(x, **kwargs)\n+    probs = self.model.predict(x, **kwargs)\n \n     # check if binary classification\n     if probs.shape[1] == 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9941536e4884b73228ca2d08926344b6b8c7c8e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,30 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Thin wrapper to call TensorFlow's API generation script.\n+\n+This file exists to provide a main function for the py_binary in the API\n+generation genrule. It just calls the main function for the actual API\n+generation script in TensorFlow.\n+\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import keras  # pylint: disable=unused-import\n+from tensorflow.python.tools.api.generator import create_python_api\n+\n+if __name__ == '__main__':\n+  create_python_api.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1323ed359221b0a75fccd44d2748f46a2bc9b5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 201 | Lines Deleted: 253 | Files Changed: 15 | Hunks: 74 | Methods Changed: 51 | Complexity Δ (Sum/Max): -21/2 | Churn Δ: 454 | Churn Cumulative: 12985 | Contributors (this commit): 5 | Commits (past 90d): 45 | Contributors (cumulative): 33 | DMM Complexity: 0.16822429906542055\n\nDIFF:\n@@ -34,6 +34,13 @@ def _get_metadata(name):\n   }\n \n \n+def _get_layer_args(layer_cls, layer_args):\n+  # To make benchmark parameters be compatible with GPU platform.\n+  if layer_cls is tf.keras.layers.Bidirectional:\n+    return layer_args.update({\"layer\": tf.keras.layers.LSTM(1)})\n+  return layer_args\n+\n+\n def _get_input_data(inputs):\n   if \"input_shape\" in inputs:\n     return tf.ones(inputs[\"input_shape\"])\n@@ -51,6 +58,148 @@ def _layer_call_backward(layer, x):\n \n   _ = tape.gradient(loss, layer.trainable_variables)\n \n+CORE_LAYERS = [\n+    (\"Dense_small_shape\", tf.keras.layers.Dense,\n+     {\"units\": 32, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 16)}, 100),\n+    (\"Activation_small_shape\", tf.keras.layers.Activation,\n+     {\"activation\": \"relu\"},\n+     {\"input_shape\": (1, 4)}, 100),\n+    (\"Embedding_small_shape\", tf.keras.layers.Embedding,\n+     {\"input_dim\": 1, \"output_dim\": 1, \"input_length\": 1},\n+     {\"input\": np.random.randint(1, size=(1, 1))}, 100),\n+    (\"Embedding_normal_shape\", tf.keras.layers.Embedding,\n+     {\"input_dim\": 1000, \"output_dim\": 64, \"input_length\": 10},\n+     {\"input\": np.random.randint(1000, size=(32, 10))}, 100),\n+    (\"Masking_small_shape\", tf.keras.layers.Masking,\n+     {\"mask_value\": 1}, {\"input_shape\": (1, 1)}, 100),\n+    (\"Lambda_small_shape\", tf.keras.layers.Lambda,\n+     {\"function\": lambda x: x ** 2}, {\"input_shape\": (1, 1)}, 100),\n+    (\"Flatten_small_shape\", tf.keras.layers.Flatten,\n+     {}, {\"input_shape\": (1, 1)}, 100),\n+]\n+\n+CONV_LAYERS = [\n+    (\"Conv1D_small_shape\", tf.keras.layers.Conv1D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"Conv2D_small_shape\", tf.keras.layers.Conv2D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"Conv2D_normal_shape\", tf.keras.layers.Conv2D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (64, 28, 28, 3)}, 100),\n+    (\"Conv3D_small_shape\", tf.keras.layers.Conv3D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"Conv1DTranspose_small_shape\", tf.keras.layers.Conv1DTranspose,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"Conv2DTranspose_small_shape\", tf.keras.layers.Conv2DTranspose,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"Conv3DTranspose_small_shape\", tf.keras.layers.Conv3DTranspose,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"SeparableConv1D_small_shape\", tf.keras.layers.SeparableConv1D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"SeparableConv2D_small_shape\", tf.keras.layers.SeparableConv2D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"DepthwiseConv2D_small_shape\", tf.keras.layers.DepthwiseConv2D,\n+     {\"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1)}, 100),\n+]\n+\n+RECURRENT_LAYERS = [\n+    (\"LSTM_small_shape\", tf.keras.layers.LSTM,\n+     {\"units\": 1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"LSTM_normal_shape\", tf.keras.layers.LSTM,\n+     {\"units\": 4}, {\"input_shape\": (32, 10, 8)}, 100),\n+    (\"GRU_small_shape\", tf.keras.layers.GRU,\n+     {\"units\": 1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"SimpleRNN_small_shape\", tf.keras.layers.SimpleRNN,\n+     {\"units\": 1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"TimeDistributed_small_shape\", tf.keras.layers.TimeDistributed,\n+     {\"layer\": tf.keras.layers.Conv2D(1, 1)},\n+     {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"Bidirectional_small_shape\", tf.keras.layers.Bidirectional,\n+     {}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"ConvLSTM2D_small_shape\", tf.keras.layers.ConvLSTM2D,\n+     {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n+     {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"RNN_small_shape\", tf.keras.layers.RNN,\n+     {\"cell\": tf.keras.layers.LSTMCell(1)}, {\"input_shape\": (1, 1, 1)}, 100),\n+]\n+\n+NORMALIZATION_LAYERS = [\n+    (\"BatchNormalization_small_shape\", tf.keras.layers.BatchNormalization,\n+     {\"axis\": -1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"LayerNormalization_small_shape\", tf.keras.layers.LayerNormalization,\n+     {\"axis\": -1}, {\"input_shape\": (1, 1, 1)}, 100),\n+]\n+\n+REGULARIZATION_LAYERS = [\n+    (\"Dropout_small_shape\", tf.keras.layers.Dropout,\n+     {\"rate\": 0.2}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"SpatialDropout1D_small_shape\", tf.keras.layers.SpatialDropout1D,\n+     {\"rate\": 0.2}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"SpatialDropout2D_small_shape\", tf.keras.layers.SpatialDropout2D,\n+     {\"rate\": 0.2}, {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"SpatialDropout3D_small_shape\", tf.keras.layers.SpatialDropout3D,\n+     {\"rate\": 0.2}, {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"GaussianDropout_small_shape\", tf.keras.layers.GaussianDropout,\n+     {\"rate\": 0.2}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"GaussianNoise_small_shape\", tf.keras.layers.GaussianNoise,\n+     {\"stddev\": 0.1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"ActivityRegularization_small_shape\",\n+     tf.keras.layers.ActivityRegularization,\n+     {\"l1\": 0.3}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"AlphaDropout_small_shape\", tf.keras.layers.AlphaDropout,\n+     {\"rate\": 0.2}, {\"input_shape\": (1, 1, 1)}, 100),\n+]\n+\n+\n+ATTENSION_LAYERS = [\n+    (\"Attention_small_shape\", tf.keras.layers.Attention,\n+     {\"use_scale\": False}, {\"input\": [np.ones((1, 1, 1)), np.ones((1, 1, 1))]},\n+     100),\n+    (\"AdditiveAttention_small_shape\", tf.keras.layers.AdditiveAttention,\n+     {\"use_scale\": True}, {\"input\": [np.ones((1, 1, 1)), np.ones((1, 1, 1))]},\n+     100),\n+]\n+\n+POOLING_LAYERS = [\n+    (\"MaxPooling1D_small_shape\", tf.keras.layers.MaxPooling1D,\n+     {\"pool_size\": 1, \"strides\": 1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"MaxPooling2D_small_shape\", tf.keras.layers.MaxPooling2D,\n+     {\"pool_size\": 1, \"strides\": 1}, {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"MaxPooling3D_small_shape\", tf.keras.layers.MaxPooling3D,\n+     {\"pool_size\": 1, \"strides\": 1}, {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"AveragePooling1D_small_shape\", tf.keras.layers.AveragePooling1D,\n+     {\"pool_size\": 1, \"strides\": 1}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"AveragePooling2D_small_shape\", tf.keras.layers.AveragePooling2D,\n+     {\"pool_size\": 1, \"strides\": 1}, {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"AveragePooling3D_small_shape\", tf.keras.layers.AveragePooling3D,\n+     {\"pool_size\": 1, \"strides\": 1}, {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"GlobalMaxPooling1D_small_shape\", tf.keras.layers.GlobalMaxPooling1D,\n+     {}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"GlobalMaxPooling2D_small_shape\", tf.keras.layers.GlobalMaxPooling2D,\n+     {}, {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"GlobalMaxPooling3D_small_shape\", tf.keras.layers.GlobalMaxPooling3D,\n+     {}, {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+    (\"GlobalAveragePooling1D_small_shape\",\n+     tf.keras.layers.GlobalAveragePooling1D,\n+     {}, {\"input_shape\": (1, 1, 1)}, 100),\n+    (\"GlobalAveragePooling2D_small_shape\",\n+     tf.keras.layers.GlobalAveragePooling2D,\n+     {}, {\"input_shape\": (1, 1, 1, 1)}, 100),\n+    (\"GlobalAveragePooling3D_small_shape\",\n+     tf.keras.layers.GlobalAveragePooling3D,\n+     {}, {\"input_shape\": (1, 1, 1, 1, 1)}, 100),\n+]\n+\n \n class KerasLayerBenchmarks(six.with_metaclass(\n     tf.__internal__.test.ParameterizedBenchmark,\n@@ -60,49 +209,12 @@ class KerasLayerBenchmarks(six.with_metaclass(\n   # the benchmark name. It must follow the convention of\n   # \"{layer_name}_{small|normal|large}_shape\" to make it compatible with\n   # `self.report_benchmark()` method.\n-  _benchmark_parameters = benchmark_util.generate_benchmark_params_cpu_gpu([\n-      (\"Conv2D_small_shape\", tf.keras.layers.Conv2D, {\n-          \"filters\": 1,\n-          \"kernel_size\": 1,\n-          \"activation\": \"relu\"\n-      }, {\n-          \"input_shape\": (1, 1, 1, 1)\n-      }, 100),\n-      (\"Conv2D_normal_shape\", tf.keras.layers.Conv2D, {\n-          \"filters\": 1,\n-          \"kernel_size\": 1,\n-          \"activation\": \"relu\"\n-      }, {\n-          \"input_shape\": (64, 28, 28, 3)\n-      }, 100),\n-      (\"LSTM_small_shape\", tf.keras.layers.LSTM, {\n-          \"units\": 1\n-      }, {\n-          \"input_shape\": (1, 1, 1)\n-      }, 100),\n-      (\"LSTM_normal_shape\", tf.keras.layers.LSTM, {\n-          \"units\": 4\n-      }, {\n-          \"input_shape\": (32, 10, 8)\n-      }, 100),\n-      (\"Embedding_small_shape\", tf.keras.layers.Embedding, {\n-          \"input_dim\": 1,\n-          \"output_dim\": 1,\n-          \"input_length\": 1\n-      }, {\n-          \"input\": np.random.randint(1, size=(1, 1))\n-      }, 100),\n-      (\"Embedding_normal_shape\", tf.keras.layers.Embedding, {\n-          \"input_dim\": 1000,\n-          \"output_dim\": 64,\n-          \"input_length\": 10\n-      }, {\n-          \"input\": np.random.randint(1000, size=(32, 10))\n-      }, 100),\n-  ])\n+  _benchmark_parameters = benchmark_util.generate_benchmark_params_cpu_gpu(\n+      CORE_LAYERS + CONV_LAYERS + RECURRENT_LAYERS + NORMALIZATION_LAYERS +\n+      REGULARIZATION_LAYERS + ATTENSION_LAYERS + POOLING_LAYERS)\n \n   def benchmark_layer_call(self, layer_cls, layer_args, inputs, num_iters):\n-    layer = layer_cls(**layer_args)\n+    layer = layer_cls(**_get_layer_args(layer_cls, layer_args))\n     x = _get_input_data(inputs)\n \n     fn = functools.partial(layer, x)\n@@ -113,7 +225,7 @@ class KerasLayerBenchmarks(six.with_metaclass(\n \n   def benchmark_layer_call_with_function(\n       self, layer_cls, layer_args, inputs, num_iters):\n-    layer = layer_cls(**layer_args)\n+    layer = layer_cls(**_get_layer_args(layer_cls, layer_args))\n     x = _get_input_data(inputs)\n     layer.call = tf.function(layer.call)\n \n@@ -129,7 +241,7 @@ class KerasLayerBenchmarks(six.with_metaclass(\n     # TODO(b/173461426)\n     if layer_cls is tf.keras.layers.Embedding and name[-1] == \"GPU\":\n       return\n-    layer = layer_cls(**layer_args)\n+    layer = layer_cls(**_get_layer_args(layer_cls, layer_args))\n     x = _get_input_data(inputs)\n     layer.call = tf.function(\n         layer.call, jit_compile=True)\n@@ -141,7 +253,7 @@ class KerasLayerBenchmarks(six.with_metaclass(\n \n   def benchmark_layer_call_backward(\n       self, layer_cls, layer_args, inputs, num_iters):\n-    layer = layer_cls(**layer_args)\n+    layer = layer_cls(**_get_layer_args(layer_cls, layer_args))\n     x = _get_input_data(inputs)\n \n     fn = functools.partial(_layer_call_backward, layer, x)\n@@ -152,7 +264,7 @@ class KerasLayerBenchmarks(six.with_metaclass(\n \n   def benchmark_layer_call_backward_with_function(\n       self, layer_cls, layer_args, inputs, num_iters):\n-    layer = layer_cls(**layer_args)\n+    layer = layer_cls(**_get_layer_args(layer_cls, layer_args))\n     x = _get_input_data(inputs)\n     layer.call = tf.function(layer.call)\n \n@@ -166,12 +278,16 @@ class KerasLayerBenchmarks(six.with_metaclass(\n       self, layer_cls, layer_args, inputs, num_iters):\n     name = benchmark_util.get_benchmark_name(self._get_name())\n     # TODO(b/153480400)\n-    if layer_cls is tf.keras.layers.LSTM:\n+    if layer_cls in [\n+        tf.keras.layers.LSTM, tf.keras.layers.Bidirectional,\n+        tf.keras.layers.ConvLSTM2D, tf.keras.layers.GRU, tf.keras.layers.RNN,\n+        tf.keras.layers.SimpleRNN\n+    ]:\n       return\n     # TODO(b/173461426)\n     if layer_cls is tf.keras.layers.Embedding and name[-1] == \"GPU\":\n       return\n-    layer = layer_cls(**layer_args)\n+    layer = layer_cls(**_get_layer_args(layer_cls, layer_args))\n     x = _get_input_data(inputs)\n     layer.call = tf.function(\n         layer.call, jit_compile=True)\n\n@@ -59,10 +59,6 @@ def keras_model_type_combinations():\n   return tf.__internal__.test.combinations.combine(model_type=KERAS_MODEL_TYPES)\n \n \n-def keras_tensor_combinations():\n-  return tf.__internal__.test.combinations.combine(use_keras_tensors=['True', 'False'])\n-\n-\n class KerasModeCombination(tf.__internal__.test.combinations.TestCombination):\n   \"\"\"Combination for Keras test mode.\n \n@@ -102,32 +98,11 @@ class KerasModelTypeCombination(tf.__internal__.test.combinations.TestCombinatio\n     return [tf.__internal__.test.combinations.OptionalParameter('model_type')]\n \n \n-class KerasTensorCombination(tf.__internal__.test.combinations.TestCombination):\n-  \"\"\"Combination for whether KerasTensors are being used or not.\n-\n-  It by default includes `True` and `False`:\n-  running Keras's functional API with KerasTensors\n-  as the inputs, and without.\n-  \"\"\"\n-\n-  def context_managers(self, kwargs):\n-    use_keras_tensors = kwargs.pop('use_keras_tensors', None)\n-\n-    if use_keras_tensors is not None:\n-      return [testing_utils.use_keras_tensors_scope(use_keras_tensors)]\n-    else:\n-      return []\n-\n-  def parameter_modifiers(self):\n-    return [tf.__internal__.test.combinations.OptionalParameter('use_keras_tensors')]\n-\n-\n _defaults = tf.__internal__.test.combinations.generate.keywords['test_combinations']\n generate = functools.partial(\n     tf.__internal__.test.combinations.generate,\n     test_combinations=_defaults +\n-    (KerasModeCombination(), KerasModelTypeCombination(),\n-     KerasTensorCombination()))\n+    (KerasModeCombination(), KerasModelTypeCombination()))\n combine = tf.__internal__.test.combinations.combine\n times = tf.__internal__.test.combinations.times\n NamedObject = tf.__internal__.test.combinations.NamedObject\n\n@@ -73,9 +73,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertTrue(layer._instrumented_keras_layer_class)\n     self.assertFalse(layer._instrumented_keras_model_class)\n \n-  @combinations.generate(combinations.times(\n-      combinations.keras_model_type_combinations(),\n-      combinations.keras_tensor_combinations()))\n+  @combinations.generate(combinations.keras_model_type_combinations())\n   def test_dynamic_layer(self):\n     model = testing_utils.get_model_from_layers([DynamicLayer(dynamic=True)],\n                                                 input_shape=(3,))\n@@ -84,9 +82,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(model.run_eagerly, True)\n     model.train_on_batch(np.random.random((2, 3)), np.random.random((2, 3)))\n \n-  @combinations.generate(combinations.times(\n-      combinations.keras_model_type_combinations(),\n-      combinations.keras_tensor_combinations()))\n+  @combinations.generate(combinations.keras_model_type_combinations())\n   def test_dynamic_layer_error(self):\n     # Functional Models hit the `dyanamic=True` error during construction.\n     # Subclass Models should just throw the original autograph error during\n@@ -105,9 +101,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n         raised_error = True\n     self.assertTrue(raised_error)\n \n-  @combinations.generate(combinations.times(\n-      combinations.keras_model_type_combinations(),\n-      combinations.keras_tensor_combinations()))\n+  @combinations.generate(combinations.keras_model_type_combinations())\n   def test_dynamic_layer_error_running_in_graph_mode(self):\n     with tf.compat.v1.get_default_graph().as_default():\n       model = testing_utils.get_model_from_layers([DynamicLayer(dynamic=True)],\n@@ -272,7 +266,6 @@ class BaseLayerTest(keras_parameterized.TestCase):\n   @combinations.generate(\n       combinations.times(\n           combinations.keras_model_type_combinations(),\n-          combinations.keras_tensor_combinations(),\n           combinations.combine(mode=['graph', 'eager'])))\n   def test_build_with_numpy_data(self):\n     model_layers = [\n@@ -373,8 +366,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n   # b/124459427: can't test with `run_eagerly=True` for now.\n   @combinations.generate(\n       combinations.times(combinations.keras_mode_combinations(),\n-                         combinations.keras_model_type_combinations(),\n-                         combinations.keras_tensor_combinations()))\n+                         combinations.keras_model_type_combinations()))\n   def test_training_arg_in_defun(self):\n     layer = self._get_layer_with_training_arg()\n     model = testing_utils.get_model_from_layers([layer], input_shape=(1,))\n@@ -399,8 +391,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(\n       combinations.times(combinations.keras_mode_combinations(),\n-                         combinations.keras_model_type_combinations(),\n-                         combinations.keras_tensor_combinations()))\n+                         combinations.keras_model_type_combinations()))\n   def test_raw_variable_assignment(self):\n \n     class RawVariableLayer(base_layer.Layer):\n@@ -468,45 +459,27 @@ class BaseLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_layer_names(self):\n-    with testing_utils.use_keras_tensors_scope(False):\n     inputs = input_layer.Input(shape=[2])\n     add1 = inputs + inputs\n     add2 = layers.Add()([inputs, inputs])\n     add3 = inputs + inputs\n     add4 = layers.Add()([inputs, inputs])\n-      model = training_lib.Model(\n-          inputs=[inputs], outputs=[add1, add2, add3, add4])\n+    model = training_lib.Model(inputs=[inputs],\n+                               outputs=[add1, add2, add3, add4])\n     actual_names = [l.name for l in model.layers]\n     graph_names = [\n-          'input_1', 'tf_op_layer_AddV2', 'add', 'tf_op_layer_AddV2_1', 'add_1'\n-      ]\n-      eager_names = [\n         'input_1', 'tf_op_layer_add', 'add', 'tf_op_layer_add_2', 'add_1'\n     ]\n-      for actual, eager, graph in zip(actual_names, graph_names, eager_names):\n-        self.assertIn(actual, {eager, graph})\n-    if tf.executing_eagerly():\n-      backend.clear_session()\n-      with testing_utils.use_keras_tensors_scope(True):\n-        inputs = input_layer.Input(shape=[2])\n-        add1 = inputs + inputs\n-        add2 = layers.Add()([inputs, inputs])\n-        add3 = inputs + inputs\n-        add4 = layers.Add()([inputs, inputs])\n-        model = training_lib.Model(\n-            inputs=[inputs], outputs=[add1, add2, add3, add4])\n-        actual_names = [l.name for l in model.layers]\n-        expected_names = [\n+    eager_names = [\n         'input_1', 'tf.__operators__.add', 'add', 'tf.__operators__.add_1',\n         'add_1'\n     ]\n-        self.assertAllEqual(actual_names, expected_names)\n+    for actual, eager, graph in zip(actual_names, graph_names, eager_names):\n+      self.assertIn(actual, {eager, graph})\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @combinations.generate(combinations.combine(mode=['eager']))\n   def test_layer_names_after_loading(self):\n-    if tf.executing_eagerly():\n     backend.clear_session()\n-      with testing_utils.use_keras_tensors_scope(True):\n     # Mimic loading a model that already contained add layers with\n     # name = 'add_1' and 'tf.__operators__.add'\n     layers.Add(name='add_1')\n\n@@ -106,7 +106,6 @@ class OpLayerTest(keras_parameterized.TestCase):\n         _ = keras.Model(int_values, float_values)\n \n   def test_ragged_op_layer_keras_tensors(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     int_values = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n     float_values = tf.cast(int_values, tf.float32)\n     model = keras.Model(int_values, float_values)\n@@ -120,7 +119,6 @@ class OpLayerTest(keras_parameterized.TestCase):\n     self.assertAllClose(expected, output)\n \n   def test_sparse_op_layer_keras_tensors(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     int_values = keras.Input(shape=(None,), dtype=tf.int32, sparse=True)\n     float_values = tf.cast(int_values, tf.float32)\n     _ = keras.Model(int_values, float_values)\n\n@@ -88,9 +88,9 @@ class Functional(training_lib.Model):\n \n   Args:\n     inputs: List of input tensors (must be created via `tf.keras.Input()`).\n-    outputs: List of outputs tensors.\n+    outputs: List of output tensors.\n     name: String, optional. Name of the model.\n-    trainable: Boolean, whether the model's variables should be trainable.\n+    trainable: Boolean, optional. If the model's variables should be trainable.\n   \"\"\"\n \n   # See tf.Module for the usage of this property.\n@@ -103,7 +103,7 @@ class Functional(training_lib.Model):\n   ))\n \n   @trackable.no_automatic_dependency_tracking\n-  def __init__(self, inputs=None, outputs=None, name=None, trainable=True,\n+  def __init__(self, inputs, outputs, name=None, trainable=True,\n                **kwargs):\n     # This is used by the Model class, since we have some logic to swap the\n     # class in the __new__ method, which will lead to __init__ get invoked\n\n@@ -932,29 +932,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.times(\n-      combinations.keras_mode_combinations(mode='eager'),\n-      combinations.combine(use_keras_tensors=False)))\n-  def test_only_some_in_first_arg_derived_from_keras_layer(self):\n-    class MyAddAll(layers.Layer):\n-\n-      def call(self, inputs):\n-        x = inputs[0]\n-        for inp in inputs[1:]:\n-          if inp is not None:\n-            x = x + inp\n-        return x\n-\n-    input1 = input_layer_lib.Input(10)\n-    input2 = input_layer_lib.Input(10)\n-    layer = MyAddAll()\n-\n-    with self.assertRaisesRegexp(ValueError, 'construct a functional'):\n-      layer([0.0, input1, None, input2, None])\n-\n-  @combinations.generate(combinations.times(\n-      combinations.keras_mode_combinations(mode='eager'),\n-      combinations.combine(use_keras_tensors=True)))\n+  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)\n   def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):\n     # This functionality is unsupported in v1 graphs\n \n@@ -1134,7 +1112,6 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n   @combinations.generate(\n       combinations.times(\n           combinations.keras_mode_combinations(),\n-          combinations.keras_tensor_combinations(),\n           combinations.combine(share_already_used_layer=[True, False])))\n   def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(\n       self, share_already_used_layer):\n@@ -1234,9 +1211,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.times(\n-      combinations.keras_mode_combinations(mode='eager'),\n-      combinations.keras_tensor_combinations()))\n+  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))\n   def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):\n     # This functionality is unsupported in v1 graphs\n \n\n@@ -315,14 +315,18 @@ def Input(  # pylint: disable=invalid-name\n \n   Note that even if eager execution is enabled,\n   `Input` produces a symbolic tensor-like object (i.e. a placeholder).\n-  This symbolic tensor-like object can be used with other\n-  TensorFlow ops, as such:\n+  This symbolic tensor-like object can be used with lower-level\n+  TensorFlow ops that take tensors as inputs, as such:\n \n   ```python\n   x = Input(shape=(32,))\n-  y = tf.square(x)\n+  y = tf.square(x)  # This op will be treated like a layer\n+  model = Model(x, y)\n   ```\n \n+  (This behavior does not work for higher-order TensorFlow APIs such as\n+  control flow and being directly watched by a `tf.GradientTape`).\n+\n   However, the resulting model will not track any variables that were\n   used as inputs to TensorFlow ops. All variable usages must happen within\n   Keras layers to make sure they will be tracked by the model's weights.\n\n@@ -165,7 +165,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testInputTensorArgInTFFunction(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # We use a mutable model container instead of a model python variable,\n     # because python 2.7 does not have `nonlocal`\n     model_container = {}\n@@ -186,7 +185,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testCompositeInputTensorArg(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # Create a Keras Input\n     rt = tf.RaggedTensor.from_row_splits(\n         values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n@@ -202,7 +200,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testCompositeInputTensorArgInTFFunction(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # We use a mutable model container instead of a model python variable,\n     # because python 2.7 does not have `nonlocal`\n     model_container = {}\n@@ -226,7 +223,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testNoMixingArgsWithTypeSpecArg(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     with self.assertRaisesRegexp(\n         ValueError, 'all other args except `name` must be None'):\n       input_layer_lib.Input(\n@@ -255,7 +251,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testTypeSpecArg(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # Create a Keras Input\n     x = input_layer_lib.Input(\n         type_spec=tf.TensorSpec((7, 32), tf.float32))\n@@ -277,7 +272,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testTypeSpecArgInTFFunction(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # We use a mutable model container instead of a model python variable,\n     # because python 2.7 does not have `nonlocal`\n     model_container = {}\n@@ -299,7 +293,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testCompositeTypeSpecArg(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # Create a Keras Input\n     rt = tf.RaggedTensor.from_row_splits(\n         values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n@@ -321,7 +314,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testCompositeTypeSpecArgInTFFunction(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     # We use a mutable model container instead of a model pysthon variable,\n     # because python 2.7 does not have `nonlocal`\n     model_container = {}\n@@ -345,7 +337,6 @@ class InputLayerTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def testCompositeTypeSpecArgWithoutDtype(self):\n-    with testing_utils.use_keras_tensors_scope(True):\n     for assign_variant_dtype in [False, True]:\n       # Create a Keras Input\n       spec = TwoTensorsSpecNoOneDtype(\n\n@@ -20,7 +20,6 @@ from __future__ import print_function\n \n import tensorflow as tf\n from keras import layers\n-from keras import testing_utils\n from keras.engine import keras_tensor\n \n \n@@ -56,7 +55,6 @@ class KerasTensorTest(tf.test.TestCase):\n     self.assertEqual(expected_str, str(kt))\n     self.assertEqual(expected_repr, repr(kt))\n \n-    with testing_utils.use_keras_tensors_scope(True):\n     inp = layers.Input(shape=(3, 5))\n     kt = layers.Dense(10)(inp)\n     expected_str = (\n\n@@ -23,7 +23,6 @@ import tensorflow as tf\n from absl.testing import parameterized\n from keras import keras_parameterized\n from keras import layers\n-from keras import testing_utils\n from keras.engine import training\n \n \n@@ -44,7 +43,6 @@ class RaggedKerasTensorTest(keras_parameterized.TestCase):\n       {'batch_size': 12, 'shape': (2, 3, None, 4, 5, None), 'ragged_rank': 6},\n   )\n   def test_to_placeholder(self, shape, batch_size, ragged_rank):\n-    with testing_utils.use_keras_tensors_scope(True):\n     inp = layers.Input(shape=shape, batch_size=batch_size, ragged=True)\n     self.assertEqual(inp.ragged_rank, ragged_rank)\n     self.assertAllEqual(inp.shape, [batch_size] + list(shape))\n\n@@ -383,14 +383,10 @@ def run_all_keras_modes(test_or_class=None,\n     ImportError: If abseil parameterized is not installed or not included as\n       a target dependency.\n   \"\"\"\n-  skip_keras_tensors = kwargs.pop('skip_keras_tensors', False)\n   if kwargs:\n     raise ValueError('Unrecognized keyword args: {}'.format(kwargs))\n \n   params = [('_v2_function', 'v2_function')]\n-  if not skip_keras_tensors:\n-    params.append(('_v2_function_use_keras_tensors',\n-                   'v2_function_use_keras_tensors'))\n   if not always_skip_eager:\n     params.append(('_v2_eager', 'v2_eager'))\n   if not (always_skip_v1 or tf.__internal__.tf2.enabled()):\n@@ -410,8 +406,6 @@ def run_all_keras_modes(test_or_class=None,\n         _v2_eager_test(f, self, *args, **kwargs)\n       elif run_mode == 'v2_function':\n         _v2_function_test(f, self, *args, **kwargs)\n-      elif run_mode == 'v2_function_use_keras_tensors':\n-        _v2_function_and_kerastensors_test(f, self, *args, **kwargs)\n       else:\n         return ValueError('Unknown run mode %s' % run_mode)\n \n@@ -439,13 +433,6 @@ def _v2_function_test(f, test_or_class, *args, **kwargs):\n       f(test_or_class, *args, **kwargs)\n \n \n-def _v2_function_and_kerastensors_test(f, test_or_class, *args, **kwargs):\n-  with tf.__internal__.eager_context.eager_mode():\n-    with testing_utils.run_eagerly_scope(False):\n-      with testing_utils.use_keras_tensors_scope(True):\n-        f(test_or_class, *args, **kwargs)\n-\n-\n def _test_or_class_decorator(test_or_class, single_method_decorator):\n   \"\"\"Decorate a test or class with a decorator intended for one method.\n \n\n@@ -27,7 +27,6 @@ from absl.testing import parameterized\n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n-from keras.engine import keras_tensor\n \n \n class KerasParameterizedTest(keras_parameterized.TestCase):\n@@ -206,7 +205,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+      @keras_parameterized.run_all_keras_modes()\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n         should_run_eagerly = testing_utils.should_run_eagerly()\n@@ -242,54 +241,6 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n       ts.run(res)\n       self.assertLen(l, 4)\n \n-  def test_run_all_keras_modes_include_keras_tensors(self):\n-    l = []\n-\n-    class ExampleTest(keras_parameterized.TestCase):\n-\n-      def runTest(self):\n-        pass\n-\n-      @keras_parameterized.run_all_keras_modes()\n-      def testBody(self):\n-        mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        l.append((mode, should_run_eagerly,\n-                  keras_tensor.keras_tensors_enabled()))\n-\n-    e = ExampleTest()\n-    if not tf.__internal__.tf2.enabled():\n-      e.testBody_v1_session()\n-    e.testBody_v2_eager()\n-    e.testBody_v2_function()\n-    e.testBody_v2_function_use_keras_tensors()\n-\n-    if not tf.__internal__.tf2.enabled():\n-      self.assertLen(l, 4)\n-      self.assertAllEqual(l, [\n-          (\"graph\", False, False),\n-          (\"eager\", True, keras_tensor._KERAS_TENSORS_ENABLED),\n-          (\"eager\", False, keras_tensor._KERAS_TENSORS_ENABLED),\n-          (\"eager\", False, True),\n-      ])\n-\n-      ts = unittest.makeSuite(ExampleTest)\n-      res = unittest.TestResult()\n-      ts.run(res)\n-      self.assertLen(l, 8)\n-    else:\n-      self.assertLen(l, 3)\n-      self.assertAllEqual(l, [\n-          (\"eager\", True, keras_tensor._KERAS_TENSORS_ENABLED),\n-          (\"eager\", False, keras_tensor._KERAS_TENSORS_ENABLED),\n-          (\"eager\", False, True),\n-      ])\n-\n-      ts = unittest.makeSuite(ExampleTest)\n-      res = unittest.TestResult()\n-      ts.run(res)\n-      self.assertLen(l, 6)\n-\n   def test_run_all_keras_modes_extra_params(self):\n     l = []\n \n@@ -298,7 +249,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+      @keras_parameterized.run_all_keras_modes()\n       @parameterized.named_parameters(\n           [dict(testcase_name=\"_0\", with_brackets=True),\n            dict(testcase_name=\"_1\", with_brackets=False)])\n@@ -348,8 +299,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes(always_skip_v1=True,\n-                                               skip_keras_tensors=True)\n+      @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n         should_run_eagerly = testing_utils.should_run_eagerly()\n@@ -379,7 +329,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n         pass\n \n       @keras_parameterized.run_with_all_model_types\n-      @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+      @keras_parameterized.run_all_keras_modes\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n         should_run_eagerly = testing_utils.should_run_eagerly()\n@@ -431,7 +381,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+      @keras_parameterized.run_all_keras_modes\n       @keras_parameterized.run_with_all_model_types\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n@@ -480,7 +430,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n     l = []\n \n     @keras_parameterized.run_with_all_model_types\n-    @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+    @keras_parameterized.run_all_keras_modes\n     class ExampleTest(keras_parameterized.TestCase):\n \n       def runTest(self):\n@@ -540,7 +490,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+      @keras_parameterized.run_all_keras_modes\n       @parameterized.named_parameters(dict(testcase_name=\"_arg\",\n                                            arg=True))\n       def testBody(self, arg):\n@@ -586,7 +536,7 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n \n     self.assertLen(l, len(expected_combinations) * 2)\n \n-  @keras_parameterized.run_all_keras_modes(skip_keras_tensors=True)\n+  @keras_parameterized.run_all_keras_modes\n   @parameterized.named_parameters(dict(testcase_name=\"argument\",\n                                        arg=True))\n   def test_run_all_keras_modes_extra_params_2(self, arg):\n\n@@ -27,7 +27,6 @@ from keras import activations\n from keras import backend as K\n from keras.engine.input_spec import InputSpec\n from keras.layers import recurrent\n-from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -406,19 +405,6 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n     if _use_new_code():\n       self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'gru')\n \n-  def build(self, input_shape):\n-    super(GRU, self).build(input_shape)\n-\n-    if not all(isinstance(v, resource_variable_ops.ResourceVariable)\n-               for v in self.weights):\n-      # Non-resource variables, such as DistributedVariables and\n-      # AutoCastVariables, do not work properly with the implementation\n-      # selector, which is used when cuDNN is used. However, by chance, such\n-      # variables happen to work in LSTM, so this check is only needed for GRU.\n-      # TODO(b/136512020): Make non-resource variables work with the\n-      # implementation selector.\n-      self._could_use_gpu_kernel = False\n-\n   def call(self, inputs, mask=None, training=None, initial_state=None):\n     # The input should be dense, padded with zeros. If a ragged input is fed\n     # into the layer, it is padded and the row lengths are used for masking.\n\n@@ -23,7 +23,6 @@ import threading\n \n from tensorflow.python.distribute import ps_values as ps_distribute_values\n from tensorflow.python.distribute import values as distribute_values\n-from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.types import core\n \n \n@@ -233,7 +232,7 @@ class AutoCastVariable(tf.Variable, core.Tensor):\n \n     # Fallback to wrapping the returned variable in graph mode if possible\n     assign_var = update_fn(value, use_locking, name, read_value)\n-    if read_value and resource_variable_ops.is_resource_variable(assign_var):\n+    if read_value and tf.__internal__.ops.is_resource_variable(assign_var):\n       return create_autocast_variable(assign_var)\n     return assign_var\n \n@@ -243,7 +242,7 @@ class AutoCastVariable(tf.Variable, core.Tensor):\n       return self\n \n     # Fallback to wrapping the returned variable in graph mode if possible\n-    if resource_variable_ops.is_resource_variable(update_var):\n+    if tf.__internal__.ops.is_resource_variable(update_var):\n       return create_autocast_variable(update_var)\n     return update_var\n \n\n@@ -33,7 +33,6 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n from keras.optimizer_v2 import adam\n-from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.tracking import graph_view\n from tensorflow.python.training.tracking import util as trackable_utils\n@@ -259,7 +258,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       # Optimizer slot variables are created when the original variable is\n       # restored.\n       self.assertAllEqual([1.5], self.evaluate(on_create_m_bias_slot))\n-      dummy_var = resource_variable_ops.ResourceVariable([1.])\n+      dummy_var = tf.Variable([1.])\n       on_create_optimizer.minimize(loss=dummy_var.read_value,\n                                    var_list=[dummy_var])\n       status.assert_existing_objects_matched()\n@@ -445,8 +444,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n \n       def __init__(self):\n         super(Model, self).__init__()\n-        self.w = resource_variable_ops.ResourceVariable(0.0)\n-        self.b = resource_variable_ops.ResourceVariable(0.0)\n+        self.w = tf.Variable(0.0)\n+        self.b = tf.Variable(0.0)\n         self.vars = [self.w, self.b]\n \n       def call(self, x):\n@@ -860,8 +859,7 @@ class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n         self._check_sentinels(root)\n         # Check that there is no error when keys are missing from the name-based\n         # checkpoint.\n-        root.not_in_name_checkpoint = resource_variable_ops.ResourceVariable(\n-            [1.])\n+        root.not_in_name_checkpoint = tf.Variable([1.])\n         status = object_saver.restore(save_path)\n         with self.assertRaises(AssertionError):\n           status.assert_existing_objects_matched()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
