{"custom_id": "keras#21ed4f0003956a2d6d7c80160dce4d9fd6d194e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 46 | Churn Cumulative: 10696 | Contributors (this commit): 64 | Commits (past 90d): 12 | Contributors (cumulative): 67 | DMM Complexity: 0.9615384615384616\n\nDIFF:\n@@ -153,6 +153,39 @@ class ResizingTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (1, 4, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  @parameterized.named_parameters(\n+      ('batch_crop_to_aspect_ratio', True, True),\n+      ('batch_dont_crop_to_aspect_ratio', False, True),\n+      ('single_sample_crop_to_aspect_ratio', True, False),\n+      ('single_sample_dont_crop_to_aspect_ratio', False, False),\n+  )\n+  def test_static_shape_inference(self, crop_to_aspect_ratio, batch):\n+    channels = 3\n+    input_height = 8\n+    input_width = 8\n+    target_height = 4\n+    target_width = 6\n+    layer = image_preprocessing.Resizing(\n+        target_height, target_width, crop_to_aspect_ratio=crop_to_aspect_ratio)\n+    unit_test = self\n+\n+    @tf.function\n+    def tf_function(img):\n+      unit_test.assertListEqual([input_height, input_width, channels],\n+                                img.shape.as_list()[-3:])\n+      img = layer(img)\n+      unit_test.assertListEqual([target_height, target_width, channels],\n+                                img.shape.as_list()[-3:])\n+      return img\n+\n+    with testing_utils.use_gpu():\n+      if batch:\n+        input_shape = (2, input_height, input_width, channels)\n+      else:\n+        input_shape = (input_height, input_width, channels)\n+      img_data = np.random.random(size=input_shape).astype('float32')\n+      tf_function(img_data)\n+\n \n def get_numpy_center_crop(images, expected_height, expected_width):\n   orig_height = images.shape[1]\n\n@@ -113,12 +113,10 @@ def smart_resize(x, size, interpolation='bilinear'):\n           'or `(batch_size, height, width, channels)` but '\n           'got input with incorrect rank, of shape %s' % (img.shape,))\n   shape = tf.compat.v1.shape(img)\n-  if img.shape.rank == 4:\n-    height, width = shape[1], shape[2]\n-    static_num_channels = img.shape[-1]\n-  else:\n-    height, width = shape[0], shape[1]\n+  height, width = shape[-3], shape[-2]\n   target_height, target_width = size\n+  if img.shape.rank is not None:\n+    static_num_channels = img.shape[-1]\n \n   crop_height = tf.cast(\n       tf.cast(width * target_height, 'float32') / target_width, 'int32')\n@@ -146,9 +144,12 @@ def smart_resize(x, size, interpolation='bilinear'):\n       images=img,\n       size=size,\n       method=interpolation)\n-  if img.shape.rank == 4:\n   # Apparent bug in resize_images_v2 may cause shape to be lost\n+  if img.shape.rank is not None:\n+    if img.shape.rank == 4:\n       img.set_shape((None, None, None, static_num_channels))\n+    if img.shape.rank == 3:\n+      img.set_shape((None, None, static_num_channels))\n   if isinstance(x, np.ndarray):\n     return img.numpy()\n   return img\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f604d042f892ded1d47c6ecbb6b8a9e71a0f1367", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 9 | Churn Cumulative: 3205 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -465,7 +465,8 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # expression to evaluate to False instead of True if the shape is undefined;\n     # the expression needs to evaluate to True in that case.\n     if self._split is not None:\n-      if input_shape.ndims > 1 and not input_shape[-1] == 1:  # pylint: disable=g-comparison-negation\n+      if (input_shape is not None and input_shape.ndims > 1 and\n+          not input_shape[-1] == 1):  # pylint: disable=g-comparison-negation\n         raise RuntimeError(\n             \"When using TextVectorization to tokenize strings, the innermost \"\n             \"dimension of the input array must be 1, got shape \"\n\n@@ -453,6 +453,12 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     output = vectorization(tf.ragged.constant(data, inner_shape=(1,)))\n     self.assertAllEqual(expected, output)\n \n+  def test_layer_list_input(self):\n+    layer = text_vectorization.TextVectorization(vocabulary=[\"a\", \"b\", \"c\"])\n+    output = layer([\"a\", \"b\", \"c\"])\n+    expected_output = [[2], [3], [4]]\n+    self.assertEqual(output.numpy().tolist(), expected_output)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationPreprocessingTest(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4a382d66c12a7d20f9e7ad92be9a8e8bd4582bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 67 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 76 | Churn Cumulative: 5416 | Contributors (this commit): 30 | Commits (past 90d): 6 | Contributors (cumulative): 35 | DMM Complexity: 0.1724137931034483\n\nDIFF:\n@@ -18,7 +18,6 @@\n import tensorflow.compat.v2 as tf\n \n import copy\n-import warnings\n from keras import layers as layer_module\n from keras.engine import base_layer\n from keras.engine import functional\n@@ -168,10 +167,6 @@ class Sequential(functional.Functional):\n       origin_layer = layer._keras_history[0]\n       if isinstance(origin_layer, input_layer.InputLayer):\n         layer = origin_layer\n-        logging.warning(\n-            'Please add `keras.layers.InputLayer` instead of `keras.Input` to '\n-            'Sequential model. `keras.Input` is intended to be used by '\n-            'Functional model.')\n \n     if isinstance(layer, tf.Module):\n       if not isinstance(layer, base_layer.Layer):\n@@ -407,60 +402,6 @@ class Sequential(functional.Functional):\n     outputs = self.call(inputs, mask=mask)  # pylint: disable=unexpected-keyword-arg\n     return getattr(outputs, '_keras_mask', None)\n \n-  def predict_proba(self, x, batch_size=32, verbose=0):\n-    \"\"\"Generates class probability predictions for the input samples.\n-\n-    The input samples are processed batch by batch.\n-\n-    Args:\n-        x: input data, as a Numpy array or list of Numpy arrays\n-            (if the model has multiple inputs).\n-        batch_size: integer.\n-        verbose: verbosity mode, 0 or 1.\n-\n-    Returns:\n-        A Numpy array of probability predictions.\n-    \"\"\"\n-    warnings.warn('`model.predict_proba()` is deprecated and '\n-                  'will be removed after 2021-01-01. '\n-                  'Please use `model.predict()` instead.')\n-    preds = self.predict(x, batch_size, verbose)\n-    if preds.min() < 0. or preds.max() > 1.:\n-      logging.warning('Network returning invalid probability values. '\n-                      'The last layer might not normalize predictions '\n-                      'into probabilities '\n-                      '(like softmax or sigmoid would).')\n-    return preds\n-\n-  def predict_classes(self, x, batch_size=32, verbose=0):\n-    \"\"\"Generate class predictions for the input samples.\n-\n-    The input samples are processed batch by batch.\n-\n-    Args:\n-        x: input data, as a Numpy array or list of Numpy arrays\n-            (if the model has multiple inputs).\n-        batch_size: integer.\n-        verbose: verbosity mode, 0 or 1.\n-\n-    Returns:\n-        A numpy array of class predictions.\n-    \"\"\"\n-    warnings.warn('`model.predict_classes()` is deprecated and '\n-                  'will be removed after 2021-01-01. '\n-                  'Please use instead:'\n-                  '* `np.argmax(model.predict(x), axis=-1)`, '\n-                  '  if your model does multi-class classification '\n-                  '  (e.g. if it uses a `softmax` last-layer activation).'\n-                  '* `(model.predict(x) > 0.5).astype(\"int32\")`, '\n-                  '  if your model does binary classification '\n-                  '  (e.g. if it uses a `sigmoid` last-layer activation).')\n-    proba = self.predict(x, batch_size=batch_size, verbose=verbose)\n-    if proba.shape[-1] > 1:\n-      return proba.argmax(axis=-1)\n-    else:\n-      return (proba > 0.5).astype('int32')\n-\n   def get_config(self):\n     layer_configs = []\n     for layer in super(Sequential, self).layers:\n\n@@ -60,7 +60,7 @@ class BaseWrapper(object):\n   `fit`, `predict`, `predict_proba`, and `score` methods\n   2. Values passed to `sk_params`\n   3. The default values of the `keras.models.Sequential`\n-  `fit`, `predict`, `predict_proba` and `score` methods\n+  `fit`, `predict` methods.\n \n   When using scikit-learn's `grid_search` API, legal tunable parameters are\n   those you could pass to `sk_params`, including fitting parameters.\n@@ -83,8 +83,7 @@ class BaseWrapper(object):\n         ValueError: if any member of `params` is not a valid argument.\n     \"\"\"\n     legal_params_fns = [\n-        Sequential.fit, Sequential.predict, Sequential.predict_classes,\n-        Sequential.evaluate\n+        Sequential.fit, Sequential.predict, Sequential.evaluate\n     ]\n     if self.build_fn is None:\n       legal_params_fns.append(self.__call__)\n@@ -228,14 +227,17 @@ class KerasClassifier(BaseWrapper):\n             and `n_features` is the number of features.\n         **kwargs: dictionary arguments\n             Legal arguments are the arguments\n-            of `Sequential.predict_classes`.\n+            of `Sequential.predict`.\n \n     Returns:\n         preds: array-like, shape `(n_samples,)`\n             Class predictions.\n     \"\"\"\n-    kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n-    classes = self.model.predict_classes(x, **kwargs)\n+    proba = self.model.predict(x, **kwargs)\n+    if proba.shape[-1] > 1:\n+      classes = proba.argmax(axis=-1)\n+    else:\n+      classes = (proba > 0.5).astype('int32')\n     return self.classes_[classes]\n \n   def predict_proba(self, x, **kwargs):\n@@ -247,7 +249,7 @@ class KerasClassifier(BaseWrapper):\n             and `n_features` is the number of features.\n         **kwargs: dictionary arguments\n             Legal arguments are the arguments\n-            of `Sequential.predict_classes`.\n+            of `Sequential.predict`.\n \n     Returns:\n         proba: array-like, shape `(n_samples, n_outputs)`\n@@ -257,7 +259,6 @@ class KerasClassifier(BaseWrapper):\n             will return an array of shape `(n_samples, 2)`\n             (instead of `(n_sample, 1)` as in Keras).\n     \"\"\"\n-    kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n     probs = self.model.predict(x, **kwargs)\n \n     # check if binary classification\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70c085ce1a0b9300e6efb06a635364488b77874f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9003 | Contributors (this commit): 63 | Commits (past 90d): 8 | Contributors (cumulative): 63 | DMM Complexity: 0.0\n\nDIFF:\n@@ -117,6 +117,8 @@ def smart_resize(x, size, interpolation='bilinear'):\n   target_height, target_width = size\n   if img.shape.rank is not None:\n     static_num_channels = img.shape[-1]\n+  else:\n+    static_num_channels = None\n \n   crop_height = tf.cast(\n       tf.cast(width * target_height, 'float32') / target_width, 'int32')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95c2966d9dd8b5ba67c9473bb1099a407f3b9bd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 91 | Churn Cumulative: 1413 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -144,6 +144,25 @@ def populate_deserializable_objects():\n \n @keras_export('keras.layers.serialize')\n def serialize(layer):\n+  \"\"\"Serializes a `Layer` object into a JSON-compatible representation.\n+\n+  Args:\n+    layer: The `Layer` object to serialize.\n+\n+  Returns:\n+    A JSON-serializable dict representing the object's config.\n+\n+  Example:\n+\n+  ```python\n+  from pprint import pprint\n+  model = tf.keras.models.Sequential()\n+  model.add(tf.keras.Input(shape=(16,)))\n+  model.add(tf.keras.layers.Dense(32, activation='relu'))\n+\n+  pprint(tf.keras.layers.serialize(model))\n+  # prints the configuration of the model, as a dict.\n+  \"\"\"\n   return generic_utils.serialize_keras_object(layer)\n \n \n@@ -158,6 +177,32 @@ def deserialize(config, custom_objects=None):\n \n   Returns:\n       Layer instance (may be Model, Sequential, Network, Layer...)\n+\n+  Example:\n+\n+  ```python\n+  # Configuration of Dense(32, activation='relu')\n+  config = {\n+    'class_name': 'Dense',\n+    'config': {\n+      'activation': 'relu',\n+      'activity_regularizer': None,\n+      'bias_constraint': None,\n+      'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n+      'bias_regularizer': None,\n+      'dtype': 'float32',\n+      'kernel_constraint': None,\n+      'kernel_initializer': {'class_name': 'GlorotUniform',\n+                             'config': {'seed': None}},\n+      'kernel_regularizer': None,\n+      'name': 'dense',\n+      'trainable': True,\n+      'units': 32,\n+      'use_bias': True\n+    }\n+  }\n+  dense_layer = tf.keras.layers.deserialize(config)\n+  ```\n   \"\"\"\n   populate_deserializable_objects()\n   return generic_utils.deserialize_keras_object(\n\n@@ -1023,11 +1023,57 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n \n @keras_export(\"keras.optimizers.schedules.serialize\")\n def serialize(learning_rate_schedule):\n+  \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible representation.\n+\n+  Args:\n+    learning_rate_schedule: The `LearningRateSchedule` object to serialize.\n+\n+  Returns:\n+    A JSON-serializable dict representing the object's config.\n+\n+  Example:\n+\n+  >>> lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n+  ...   0.1, decay_steps=100000, decay_rate=0.96, staircase=True)\n+  >>> tf.keras.optimizers.schedules.serialize(lr_schedule)\n+  {'class_name': 'ExponentialDecay',\n+   'config': {'decay_rate': 0.96,\n+      'decay_steps': 100000,\n+      'initial_learning_rate': 0.1,\n+      'name': None,\n+      'staircase': True}}\n+  \"\"\"\n   return generic_utils.serialize_keras_object(learning_rate_schedule)\n \n \n @keras_export(\"keras.optimizers.schedules.deserialize\")\n def deserialize(config, custom_objects=None):\n+  \"\"\"Instantiates a `LearningRateSchedule` object from a serialized form.\n+\n+  Args:\n+    config: The serialized form of the `LearningRateSchedule`.\n+      Dictionary of the form {'class_name': str, 'config': dict}.\n+    custom_objects: A dictionary mapping class names (or function names) of\n+      custom (non-Keras) objects to class/functions.\n+\n+  Returns:\n+    A `LearningRateSchedule` object.\n+\n+  Example:\n+\n+  ```python\n+  # Configuration for PolynomialDecay\n+  config = {\n+    'class_name': 'PolynomialDecay',\n+    'config': {'cycle': False,\n+      'decay_steps': 10000,\n+      'end_learning_rate': 0.01,\n+      'initial_learning_rate': 0.1,\n+      'name': None,\n+      'power': 0.5}}\n+  lr_schedule = tf.keras.optimizers.schedules.deserialize(config)\n+  ```\n+  \"\"\"\n   return generic_utils.deserialize_keras_object(\n       config,\n       module_objects=globals(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#214d11055890d5f3dcc83d2af69ad17089b8177d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 882 | Lines Deleted: 340 | Files Changed: 3 | Hunks: 71 | Methods Changed: 21 | Complexity Δ (Sum/Max): 14/10 | Churn Δ: 1222 | Churn Cumulative: 6799 | Contributors (this commit): 23 | Commits (past 90d): 9 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -224,7 +224,9 @@ else:\n   LSTMCellV1 = LSTMCell\n \n # Convolutional-recurrent layers.\n+from keras.layers.convolutional_recurrent import ConvLSTM1D\n from keras.layers.convolutional_recurrent import ConvLSTM2D\n+from keras.layers.convolutional_recurrent import ConvLSTM3D\n \n # CuDNN recurrent layers.\n from keras.layers.cudnn_recurrent import CuDNNLSTM\n\n@@ -35,124 +35,100 @@ from keras.utils import tf_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class ConvRNN2D(RNN):\n-  \"\"\"Base class for convolutional-recurrent layers.\n+class ConvRNN(RNN):\n+  \"\"\"N-Dimensional Base class for convolutional-recurrent layers.\n \n   Args:\n-    cell: A RNN cell instance. A RNN cell is a class that has:\n-      - a `call(input_at_t, states_at_t)` method, returning\n-        `(output_at_t, states_at_t_plus_1)`. The call method of the\n-        cell can also take the optional argument `constants`, see\n-        section \"Note on passing external constants\" below.\n-      - a `state_size` attribute. This can be a single integer\n-        (single state) in which case it is\n-        the number of channels of the recurrent state\n-        (which should be the same as the number of channels of the cell\n-        output). This can also be a list/tuple of integers\n-        (one size per state). In this case, the first entry\n-        (`state_size[0]`) should be the same as\n-        the size of the cell output.\n-    return_sequences: Boolean. Whether to return the last output.\n-      in the output sequence, or the full sequence.\n-    return_state: Boolean. Whether to return the last state\n-      in addition to the output.\n-    go_backwards: Boolean (default False).\n-      If True, process the input sequence backwards and return the\n-      reversed sequence.\n-    stateful: Boolean (default False). If True, the last state\n-      for each sample at index i in a batch will be used as initial\n-      state for the sample of index i in the following batch.\n-    input_shape: Use this argument to specify the shape of the\n-      input when this layer is the first one in a model.\n-\n+    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n+    cell: A RNN cell instance. A RNN cell is a class that has: - a\n+      `call(input_at_t, states_at_t)` method, returning `(output_at_t,\n+      states_at_t_plus_1)`. The call method of the cell can also take the\n+      optional argument `constants`, see section \"Note on passing external\n+      constants\" below. - a `state_size` attribute. This can be a single integer\n+      (single state) in which case it is the number of channels of the recurrent\n+      state (which should be the same as the number of channels of the cell\n+      output). This can also be a list/tuple of integers (one size per state).\n+      In this case, the first entry (`state_size[0]`) should be the same as the\n+      size of the cell output.\n+    return_sequences: Boolean. Whether to return the last output. in the output\n+      sequence, or the full sequence.\n+    return_state: Boolean. Whether to return the last state in addition to the\n+      output.\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards and return the reversed sequence.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    input_shape: Use this argument to specify the shape of the input when this\n+      layer is the first one in a model.\n   Call arguments:\n-    inputs: A 5D tensor.\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked.\n+    inputs: A (2 + `rank`)D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is for use with cells that use dropout.\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n     constants: List of constant tensors to be passed to the cell at each\n       timestep.\n-\n   Input shape:\n-    5D tensor with shape:\n-    `(samples, timesteps, channels, rows, cols)`\n-    if data_format='channels_first' or 5D tensor with shape:\n-    `(samples, timesteps, rows, cols, channels)`\n-    if data_format='channels_last'.\n-\n+    (3 + `rank`)D tensor with shape: `(samples, timesteps, channels,\n+      img_dimensions...)`\n+    if data_format='channels_first' or shape: `(samples, timesteps,\n+      img_dimensions..., channels)` if data_format='channels_last'.\n   Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is\n-      the output. The remaining tensors are the last states,\n-      each 4D tensor with shape:\n-      `(samples, filters, new_rows, new_cols)`\n-      if data_format='channels_first'\n-      or 4D tensor with shape:\n-      `(samples, new_rows, new_cols, filters)`\n-      if data_format='channels_last'.\n-      `rows` and `cols` values might have changed due to padding.\n-    - If `return_sequences`: 5D tensor with shape:\n-      `(samples, timesteps, filters, new_rows, new_cols)`\n-      if data_format='channels_first'\n-      or 5D tensor with shape:\n-      `(samples, timesteps, new_rows, new_cols, filters)`\n-      if data_format='channels_last'.\n-    - Else, 4D tensor with shape:\n-      `(samples, filters, new_rows, new_cols)`\n-      if data_format='channels_first'\n-      or 4D tensor with shape:\n-      `(samples, new_rows, new_cols, filters)`\n-      if data_format='channels_last'.\n-\n-  Masking:\n-    This layer supports masking for input data with a variable number\n-    of timesteps.\n-\n-  Note on using statefulness in RNNs:\n-    You can set RNN layers to be 'stateful', which means that the states\n-    computed for the samples in one batch will be reused as initial states\n-    for the samples in the next batch. This assumes a one-to-one mapping\n-    between samples in different successive batches.\n-    To enable statefulness:\n-      - Specify `stateful=True` in the layer constructor.\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each (2 + `rank`)D tensor with shape: `(samples, filters,\n+        new_img_dimensions...)` if data_format='channels_first'\n+      or shape: `(samples, new_img_dimensions..., filters)` if\n+        data_format='channels_last'. img_dimension values might have changed due\n+        to padding.\n+    - If `return_sequences`: (3 + `rank`)D tensor with shape: `(samples,\n+      timesteps, filters, new_img_dimensions...)` if\n+      data_format='channels_first'\n+      or shape: `(samples, timesteps, new_img_dimensions..., filters)` if\n+        data_format='channels_last'.\n+    - Else, (2 + `rank`)D tensor with shape: `(samples, filters,\n+      new_img_dimensions...)` if data_format='channels_first'\n+      or shape: `(samples, new_img_dimensions..., filters)` if\n+        data_format='channels_last'.\n+  Masking: This layer supports masking for input data with a variable number of\n+    timesteps.\n+  Note on using statefulness in RNNs: You can set RNN layers to be 'stateful',\n+    which means that the states computed for the samples in one batch will be\n+    reused as initial states for the samples in the next batch. This assumes a\n+    one-to-one mapping between samples in different successive batches.\n+    To enable statefulness: - Specify `stateful=True` in the layer constructor.\n       - Specify a fixed batch size for your model, by passing\n-         - If sequential model:\n-            `batch_input_shape=(...)` to the first layer in your model.\n-         - If functional model with 1 or more Input layers:\n-            `batch_shape=(...)` to all the first layers in your model.\n-            This is the expected shape of your inputs\n-            *including the batch size*.\n-            It should be a tuple of integers,\n-            e.g. `(32, 10, 100, 100, 32)`.\n-            Note that the number of rows and columns should be specified\n-            too.\n-      - Specify `shuffle=False` when calling fit().\n-    To reset the states of your model, call `.reset_states()` on either\n-    a specific layer, or on your entire model.\n-\n-  Note on specifying the initial state of RNNs:\n-    You can specify the initial state of RNN layers symbolically by\n-    calling them with the keyword argument `initial_state`. The value of\n-    `initial_state` should be a tensor or list of tensors representing\n-    the initial state of the RNN layer.\n-    You can specify the initial state of RNN layers numerically by\n-    calling `reset_states` with the keyword argument `states`. The value of\n-    `states` should be a numpy array or list of numpy arrays representing\n-    the initial state of the RNN layer.\n-\n-  Note on passing external constants to RNNs:\n-    You can pass \"external\" constants to the cell using the `constants`\n-    keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n-    requires that the `cell.call` method accepts the same keyword argument\n-    `constants`. Such constants can be used to condition the cell\n-    transformation on additional static inputs (not changing over time),\n-    a.k.a. an attention mechanism.\n+          - If sequential model: `batch_input_shape=(...)` to the first layer in\n+            your model.\n+          - If functional model with 1 or more Input layers: `batch_shape=(...)`\n+            to all the first layers in your model. This is the expected shape of\n+            your inputs *including the batch size*. It should be a tuple of\n+            integers, e.g. `(32, 10, 100, 100, 32)`. for rank 2 convolution Note\n+            that the image dimensions should be specified too. - Specify\n+            `shuffle=False` when calling fit(). To reset the states of your\n+            model, call `.reset_states()` on either a specific layer, or on your\n+            entire model.\n+  Note on specifying the initial state of RNNs: You can specify the initial\n+    state of RNN layers symbolically by calling them with the keyword argument\n+    `initial_state`. The value of `initial_state` should be a tensor or list of\n+    tensors representing the initial state of the RNN layer. You can specify the\n+    initial state of RNN layers numerically by calling `reset_states` with the\n+    keyword argument `states`. The value of `states` should be a numpy array or\n+    list of numpy arrays representing the initial state of the RNN layer.\n+  Note on passing external constants to RNNs: You can pass \"external\" constants\n+    to the cell using the `constants` keyword argument of `RNN.__call__` (as\n+    well as `RNN.call`) method. This requires that the `cell.call` method\n+    accepts the same keyword argument `constants`. Such constants can be used to\n+    condition the cell transformation on additional static inputs (not changing\n+    over time), a.k.a. an attention mechanism.\n   \"\"\"\n \n   def __init__(self,\n+               rank,\n                cell,\n                return_sequences=False,\n                return_state=False,\n@@ -161,20 +137,15 @@ class ConvRNN2D(RNN):\n                unroll=False,\n                **kwargs):\n     if unroll:\n-      raise TypeError('Unrolling isn\\'t possible with '\n-                      'convolutional RNNs.')\n+      raise TypeError('Unrolling isn\\'t possible with convolutional RNNs.')\n     if isinstance(cell, (list, tuple)):\n-      # The StackedConvRNN2DCells isn't implemented yet.\n+      # The StackedConvRNN3DCells isn't implemented yet.\n       raise TypeError('It is not possible at the moment to'\n                       'stack convolutional cells.')\n-    super(ConvRNN2D, self).__init__(cell,\n-                                    return_sequences,\n-                                    return_state,\n-                                    go_backwards,\n-                                    stateful,\n-                                    unroll,\n-                                    **kwargs)\n-    self.input_spec = [InputSpec(ndim=5)]\n+    super(ConvRNN, self).__init__(cell, return_sequences, return_state,\n+                                  go_backwards, stateful, unroll, **kwargs)\n+    self.rank = rank\n+    self.input_spec = [InputSpec(ndim=rank + 3)]\n     self.states = None\n     self._num_constants = None\n \n@@ -185,26 +156,23 @@ class ConvRNN2D(RNN):\n \n     cell = self.cell\n     if cell.data_format == 'channels_first':\n-      rows = input_shape[3]\n-      cols = input_shape[4]\n+      img_dims = input_shape[3:]\n     elif cell.data_format == 'channels_last':\n-      rows = input_shape[2]\n-      cols = input_shape[3]\n-    rows = conv_utils.conv_output_length(rows,\n-                                         cell.kernel_size[0],\n+      img_dims = input_shape[2:-1]\n+\n+    norm_img_dims = tuple([\n+        conv_utils.conv_output_length(\n+            img_dims[idx],\n+            cell.kernel_size[idx],\n             padding=cell.padding,\n-                                         stride=cell.strides[0],\n-                                         dilation=cell.dilation_rate[0])\n-    cols = conv_utils.conv_output_length(cols,\n-                                         cell.kernel_size[1],\n-                                         padding=cell.padding,\n-                                         stride=cell.strides[1],\n-                                         dilation=cell.dilation_rate[1])\n+            stride=cell.strides[idx],\n+            dilation=cell.dilation_rate[idx]) for idx in range(len(img_dims))\n+    ])\n \n     if cell.data_format == 'channels_first':\n-      output_shape = input_shape[:2] + (cell.filters, rows, cols)\n+      output_shape = input_shape[:2] + (cell.filters,) + norm_img_dims\n     elif cell.data_format == 'channels_last':\n-      output_shape = input_shape[:2] + (rows, cols, cell.filters)\n+      output_shape = input_shape[:2] + norm_img_dims + (cell.filters,)\n \n     if not self.return_sequences:\n       output_shape = output_shape[:1] + output_shape[2:]\n@@ -212,10 +180,11 @@ class ConvRNN2D(RNN):\n     if self.return_state:\n       output_shape = [output_shape]\n       if cell.data_format == 'channels_first':\n-        output_shape += [(input_shape[0], cell.filters, rows, cols)\n-                         for _ in range(2)]\n+        output_shape += [\n+            (input_shape[0], cell.filters) + norm_img_dims for _ in range(2)\n+        ]\n       elif cell.data_format == 'channels_last':\n-        output_shape += [(input_shape[0], rows, cols, cell.filters)\n+        output_shape += [(input_shape[0],) + norm_img_dims + (cell.filters,)\n                          for _ in range(2)]\n     return output_shape\n \n@@ -232,7 +201,8 @@ class ConvRNN2D(RNN):\n       input_shape = input_shape[0]\n \n     batch_size = input_shape[0] if self.stateful else None\n-    self.input_spec[0] = InputSpec(shape=(batch_size, None) + input_shape[2:5])\n+    self.input_spec[0] = InputSpec(\n+        shape=(batch_size, None) + input_shape[2:self.rank + 3])\n \n     # allow cell (if layer) to build before we set or validate state_spec\n     if isinstance(self.cell, Layer):\n@@ -253,7 +223,7 @@ class ConvRNN2D(RNN):\n       if self.cell.data_format == 'channels_first':\n         ch_dim = 1\n       elif self.cell.data_format == 'channels_last':\n-        ch_dim = 3\n+        ch_dim = self.rank + 1\n       if [spec.shape[ch_dim] for spec in self.state_spec] != state_size:\n         raise ValueError(\n             'An initial_state was passed that is not compatible with '\n@@ -262,20 +232,23 @@ class ConvRNN2D(RNN):\n             '{}'.format([spec.shape for spec in self.state_spec],\n                         self.cell.state_size))\n     else:\n+      img_dims = tuple((None for _ in range(self.rank)))\n       if self.cell.data_format == 'channels_first':\n-        self.state_spec = [InputSpec(shape=(None, dim, None, None))\n-                           for dim in state_size]\n+        self.state_spec = [\n+            InputSpec(shape=(None, dim) + img_dims) for dim in state_size\n+        ]\n       elif self.cell.data_format == 'channels_last':\n-        self.state_spec = [InputSpec(shape=(None, None, None, dim))\n-                           for dim in state_size]\n+        self.state_spec = [\n+            InputSpec(shape=(None,) + img_dims + (dim,)) for dim in state_size\n+        ]\n     if self.stateful:\n       self.reset_states()\n     self.built = True\n \n   def get_initial_state(self, inputs):\n-    # (samples, timesteps, rows, cols, filters)\n+    # (samples, timesteps, img_dims..., filters)\n     initial_state = backend.zeros_like(inputs)\n-    # (samples, rows, cols, filters)\n+    # (samples, img_dims..., filters)\n     initial_state = backend.sum(initial_state, axis=1)\n     shape = list(self.cell.kernel_shape)\n     shape[-1] = self.cell.filters\n@@ -345,7 +318,6 @@ class ConvRNN2D(RNN):\n       else:\n         states = list(states)\n       return [output] + states\n-    else:\n     return output\n \n   def reset_states(self, states=None):\n@@ -377,7 +349,7 @@ class ConvRNN2D(RNN):\n       if self.cell.data_format == 'channels_first':\n         result[1] = nb_channels\n       elif self.cell.data_format == 'channels_last':\n-        result[3] = nb_channels\n+        result[self.rank + 1] = nb_channels\n       else:\n         raise KeyError\n       return tuple(result)\n@@ -419,67 +391,60 @@ class ConvRNN2D(RNN):\n         backend.set_value(state, value)\n \n \n-class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n-  \"\"\"Cell class for the ConvLSTM2D layer.\n+class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n+  \"\"\"Cell class for the ConvLSTM layer.\n \n   Args:\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n+    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n     kernel_size: An integer or tuple/list of n integers, specifying the\n       dimensions of the convolution window.\n-    strides: An integer or tuple/list of n integers,\n-      specifying the strides of the convolution.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to \n-      the left/right or up/down of the input such that output has the same \n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    dilation_rate: An integer or tuple/list of n integers, specifying\n-      the dilation rate to use for dilated convolution.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any `strides` value != 1.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      It defaults to the `image_data_format` value found in your Keras config\n+      file at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied\n       (ie. \"linear\" activation: `a(x) = x`).\n-    recurrent_activation: Activation function to use\n-      for the recurrent step.\n+    recurrent_activation: Activation function to use for the recurrent step.\n     use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix,\n-      used for the linear transformation of the inputs.\n-    recurrent_initializer: Initializer for the `recurrent_kernel`\n-      weights matrix,\n-      used for the linear transformation of the recurrent state.\n-    bias_initializer: Initializer for the bias vector.\n-    unit_forget_bias: Boolean.\n-      If True, add 1 to the bias of the forget gate at initialization.\n-      Use in combination with `bias_initializer=\"zeros\"`.\n-      This is recommended in [Jozefowicz et al., 2015](\n-        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix.\n-    recurrent_regularizer: Regularizer function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_regularizer: Regularizer function applied to the bias vector.\n-    kernel_constraint: Constraint function applied to\n-      the `kernel` weights matrix.\n-    recurrent_constraint: Constraint function applied to\n-      the `recurrent_kernel` weights matrix.\n-    bias_constraint: Constraint function applied to the bias vector.\n-    dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n       the linear transformation of the inputs.\n-    recurrent_dropout: Float between 0 and 1.\n-      Fraction of the units to drop for\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n       the linear transformation of the recurrent state.\n-\n   Call arguments:\n-    inputs: A 4D tensor.\n+    inputs: A (2+ `rank`)D tensor.\n     states:  List of state tensors corresponding to the previous timestep.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. Only relevant when `dropout` or\n@@ -487,12 +452,13 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n   \"\"\"\n \n   def __init__(self,\n+               rank,\n                filters,\n                kernel_size,\n-               strides=(1, 1),\n+               strides=1,\n                padding='valid',\n                data_format=None,\n-               dilation_rate=(1, 1),\n+               dilation_rate=1,\n                activation='tanh',\n                recurrent_activation='hard_sigmoid',\n                use_bias=True,\n@@ -506,16 +472,21 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n                kernel_constraint=None,\n                recurrent_constraint=None,\n                bias_constraint=None,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n                **kwargs):\n-    super(ConvLSTM2DCell, self).__init__(**kwargs)\n+    super(ConvLSTMCell, self).__init__(**kwargs)\n+    self.rank = rank\n+    if self.rank > 3:\n+      raise ValueError('Rank ' + str(self.rank) +\n+                       ' convolutions not currently implemented.')\n     self.filters = filters\n-    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n-    self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n+    self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank,\n+                                                  'kernel_size')\n+    self.strides = conv_utils.normalize_tuple(strides, self.rank, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n+    self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, self.rank,\n                                                     'dilation_rate')\n     self.activation = activations.get(activation)\n     self.recurrent_activation = activations.get(recurrent_activation)\n@@ -534,8 +505,8 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n     self.recurrent_constraint = constraints.get(recurrent_constraint)\n     self.bias_constraint = constraints.get(bias_constraint)\n \n-    self.dropout = min(1., max(0., dropout))\n-    self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n+    self.dropout = min(1.0, max(0.0, dropout))\n+    self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n     self.state_size = (self.filters, self.filters)\n \n   def build(self, input_shape):\n@@ -548,11 +519,11 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n       raise ValueError('The channel dimension of the inputs '\n                        'should be defined. Found `None`.')\n     input_dim = input_shape[channel_axis]\n-    kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n-    self.kernel_shape = kernel_shape\n+    self.kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n     recurrent_kernel_shape = self.kernel_size + (self.filters, self.filters * 4)\n \n-    self.kernel = self.add_weight(shape=kernel_shape,\n+    self.kernel = self.add_weight(\n+        shape=self.kernel_shape,\n         initializer=self.kernel_initializer,\n         name='kernel',\n         regularizer=self.kernel_regularizer,\n@@ -617,12 +588,11 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n       h_tm1_c = h_tm1\n       h_tm1_o = h_tm1\n \n-    (kernel_i, kernel_f,\n-     kernel_c, kernel_o) = tf.split(self.kernel, 4, axis=3)\n-    (recurrent_kernel_i,\n-     recurrent_kernel_f,\n-     recurrent_kernel_c,\n-     recurrent_kernel_o) = tf.split(self.recurrent_kernel, 4, axis=3)\n+    (kernel_i, kernel_f, kernel_c, kernel_o) = tf.split(\n+        self.kernel, 4, axis=self.rank + 1)\n+    (recurrent_kernel_i, recurrent_kernel_f, recurrent_kernel_c,\n+     recurrent_kernel_o) = tf.split(\n+         self.recurrent_kernel, 4, axis=self.rank + 1)\n \n     if self.use_bias:\n       bias_i, bias_f, bias_c, bias_o = tf.split(self.bias, 4)\n@@ -645,69 +615,90 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n     h = o * self.activation(c)\n     return h, [h, c]\n \n+  @property\n+  def _conv_func(self):\n+    if self.rank == 1:\n+      return backend.conv1d\n+    if self.rank == 2:\n+      return backend.conv2d\n+    if self.rank == 3:\n+      return backend.conv3d\n+\n   def input_conv(self, x, w, b=None, padding='valid'):\n-    conv_out = backend.conv2d(x, w, strides=self.strides,\n+    conv_out = self._conv_func(\n+        x,\n+        w,\n+        strides=self.strides,\n         padding=padding,\n         data_format=self.data_format,\n         dilation_rate=self.dilation_rate)\n     if b is not None:\n-      conv_out = backend.bias_add(conv_out, b,\n-                                  data_format=self.data_format)\n+      conv_out = backend.bias_add(conv_out, b, data_format=self.data_format)\n     return conv_out\n \n   def recurrent_conv(self, x, w):\n-    conv_out = backend.conv2d(x, w, strides=(1, 1),\n-                              padding='same',\n-                              data_format=self.data_format)\n+    strides = conv_utils.normalize_tuple(1, self.rank, 'strides')\n+    conv_out = self._conv_func(\n+        x, w, strides=strides, padding='same', data_format=self.data_format)\n     return conv_out\n \n   def get_config(self):\n-    config = {'filters': self.filters,\n-              'kernel_size': self.kernel_size,\n-              'strides': self.strides,\n-              'padding': self.padding,\n-              'data_format': self.data_format,\n-              'dilation_rate': self.dilation_rate,\n-              'activation': activations.serialize(self.activation),\n-              'recurrent_activation': activations.serialize(\n-                  self.recurrent_activation),\n-              'use_bias': self.use_bias,\n-              'kernel_initializer': initializers.serialize(\n-                  self.kernel_initializer),\n-              'recurrent_initializer': initializers.serialize(\n-                  self.recurrent_initializer),\n-              'bias_initializer': initializers.serialize(self.bias_initializer),\n-              'unit_forget_bias': self.unit_forget_bias,\n-              'kernel_regularizer': regularizers.serialize(\n-                  self.kernel_regularizer),\n-              'recurrent_regularizer': regularizers.serialize(\n-                  self.recurrent_regularizer),\n-              'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-              'kernel_constraint': constraints.serialize(\n-                  self.kernel_constraint),\n-              'recurrent_constraint': constraints.serialize(\n-                  self.recurrent_constraint),\n-              'bias_constraint': constraints.serialize(self.bias_constraint),\n-              'dropout': self.dropout,\n-              'recurrent_dropout': self.recurrent_dropout}\n-    base_config = super(ConvLSTM2DCell, self).get_config()\n+    config = {\n+        'filters':\n+            self.filters,\n+        'kernel_size':\n+            self.kernel_size,\n+        'strides':\n+            self.strides,\n+        'padding':\n+            self.padding,\n+        'data_format':\n+            self.data_format,\n+        'dilation_rate':\n+            self.dilation_rate,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'recurrent_activation':\n+            activations.serialize(self.recurrent_activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'recurrent_initializer':\n+            initializers.serialize(self.recurrent_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'unit_forget_bias':\n+            self.unit_forget_bias,\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'recurrent_regularizer':\n+            regularizers.serialize(self.recurrent_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'recurrent_constraint':\n+            constraints.serialize(self.recurrent_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint),\n+        'dropout':\n+            self.dropout,\n+        'recurrent_dropout':\n+            self.recurrent_dropout,\n+    }\n+    base_config = super(ConvLSTMCell, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.layers.ConvLSTM2D')\n-class ConvLSTM2D(ConvRNN2D):\n-  \"\"\"2D Convolutional LSTM layer.\n+class ConvLSTM(ConvRNN):\n+  \"\"\"Abstract N-D Convolutional LSTM layer. (Used as implementation base)\n \n-  A convolutional LSTM is similar to an LSTM, but the input transformations\n-  and recurrent transformations are both convolutional. This layer is typically\n-  used to process timeseries of images (i.e. video-like data).\n-\n-  It is known to perform well for weather data forecasting,\n-  using inputs that are timeseries of 2D grids of sensor values.\n-  It isn't usually applied to regular video data, due to its high computational\n-  cost.\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n \n   Args:\n+    rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n     filters: Integer, the dimensionality of the output space\n       (i.e. the number of output filters in the convolution).\n     kernel_size: An integer or tuple/list of n integers, specifying the\n@@ -777,79 +768,16 @@ class ConvLSTM2D(ConvRNN2D):\n     recurrent_dropout: Float between 0 and 1.\n       Fraction of the units to drop for\n       the linear transformation of the recurrent state.\n-\n-  Call arguments:\n-    inputs: A 5D float tensor (see input shape description below).\n-    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked.\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode. This argument is passed to the cell\n-      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-      are set.\n-    initial_state: List of initial state tensors to be passed to the first\n-      call of the cell.\n-\n-  Input shape:\n-    - If data_format='channels_first'\n-        5D tensor with shape:\n-        `(samples, time, channels, rows, cols)`\n-    - If data_format='channels_last'\n-        5D tensor with shape:\n-        `(samples, time, rows, cols, channels)`\n-\n-  Output shape:\n-    - If `return_state`: a list of tensors. The first tensor is\n-      the output. The remaining tensors are the last states,\n-      each 4D tensor with shape:\n-      `(samples, filters, new_rows, new_cols)`\n-      if data_format='channels_first'\n-      or 4D tensor with shape:\n-      `(samples, new_rows, new_cols, filters)`\n-      if data_format='channels_last'.\n-      `rows` and `cols` values might have changed due to padding.\n-    - If `return_sequences`: 5D tensor with shape:\n-      `(samples, timesteps, filters, new_rows, new_cols)`\n-      if data_format='channels_first'\n-      or 5D tensor with shape:\n-      `(samples, timesteps, new_rows, new_cols, filters)`\n-      if data_format='channels_last'.\n-    - Else, 4D tensor with shape:\n-      `(samples, filters, new_rows, new_cols)`\n-      if data_format='channels_first'\n-      or 4D tensor with shape:\n-      `(samples, new_rows, new_cols, filters)`\n-      if data_format='channels_last'.\n-\n-  Raises:\n-    ValueError: in case of invalid constructor arguments.\n-\n-  References:\n-    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n-    (the current implementation does not include the feedback loop on the\n-    cells output).\n-\n-  Example:\n-\n-  ```python\n-  steps = 10\n-  height = 32\n-  width = 32\n-  input_channels = 3\n-  output_channels = 6\n-\n-  inputs = tf.keras.Input(shape=(steps, height, width, input_channels))\n-  layer = tf.keras.layers.ConvLSTM2D(filters=output_channels, kernel_size=3)\n-  outputs = layer(inputs)\n-  ```\n   \"\"\"\n \n   def __init__(self,\n+               rank,\n                filters,\n                kernel_size,\n-               strides=(1, 1),\n+               strides=1,\n                padding='valid',\n                data_format=None,\n-               dilation_rate=(1, 1),\n+               dilation_rate=1,\n                activation='tanh',\n                recurrent_activation='hard_sigmoid',\n                use_bias=True,\n@@ -868,10 +796,12 @@ class ConvLSTM2D(ConvRNN2D):\n                return_state=False,\n                go_backwards=False,\n                stateful=False,\n-               dropout=0.,\n-               recurrent_dropout=0.,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n                **kwargs):\n-    cell = ConvLSTM2DCell(filters=filters,\n+    cell = ConvLSTMCell(\n+        rank=rank,\n+        filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n         padding=padding,\n@@ -893,7 +823,9 @@ class ConvLSTM2D(ConvRNN2D):\n         dropout=dropout,\n         recurrent_dropout=recurrent_dropout,\n         dtype=kwargs.get('dtype'))\n-    super(ConvLSTM2D, self).__init__(cell,\n+    super(ConvLSTM, self).__init__(\n+        rank,\n+        cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n         go_backwards=go_backwards,\n@@ -902,10 +834,8 @@ class ConvLSTM2D(ConvRNN2D):\n     self.activity_regularizer = regularizers.get(activity_regularizer)\n \n   def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(ConvLSTM2D, self).call(inputs,\n-                                        mask=mask,\n-                                        training=training,\n-                                        initial_state=initial_state)\n+    return super(ConvLSTM, self).call(\n+        inputs, mask=mask, training=training, initial_state=initial_state)\n \n   @property\n   def filters(self):\n@@ -1022,10 +952,506 @@ class ConvLSTM2D(ConvRNN2D):\n               'bias_constraint': constraints.serialize(self.bias_constraint),\n               'dropout': self.dropout,\n               'recurrent_dropout': self.recurrent_dropout}\n-    base_config = super(ConvLSTM2D, self).get_config()\n+    base_config = super(ConvLSTM, self).get_config()\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n   def from_config(cls, config):\n     return cls(**config)\n+\n+\n+@keras_export('keras.layers.ConvLSTM1D')\n+class ConvLSTM1D(ConvLSTM):\n+  \"\"\"1D Convolutional LSTM.\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n+      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n+      defaults to the `image_data_format` value found in your Keras config file\n+      at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. By default hyperbolic tangent\n+      activation function is applied (`tanh(x)`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output in the output\n+      sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state in addition to the\n+      output. (default False)\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A 4D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n+      are set.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+  Input shape: - If data_format='channels_first'\n+        4D tensor with shape: `(samples, time, channels, rows)` - If\n+          data_format='channels_last'\n+        4D tensor with shape: `(samples, time, rows, channels)`\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each 3D tensor with shape: `(samples, filters, new_rows)` if\n+        data_format='channels_first'\n+      or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n+        `rows` values might have changed due to padding.\n+    - If `return_sequences`: 4D tensor with shape: `(samples, timesteps,\n+      filters, new_rows)` if data_format='channels_first'\n+      or shape: `(samples, timesteps, new_rows, filters)` if\n+        data_format='channels_last'.\n+    - Else, 3D tensor with shape: `(samples, filters, new_rows)` if\n+      data_format='channels_first'\n+      or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n+\n+  Raises:\n+    ValueError: in case of invalid constructor arguments.\n+\n+  References:\n+    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n+    (the current implementation does not include the feedback loop on the\n+    cells output).\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTM1D, self).__init__(\n+        rank=1,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        **kwargs)\n+\n+\n+@keras_export('keras.layers.ConvLSTM2D')\n+class ConvLSTM2D(ConvLSTM):\n+  \"\"\"2D Convolutional LSTM.\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n+      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n+      defaults to the `image_data_format` value found in your Keras config file\n+      at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. By default hyperbolic tangent\n+      activation function is applied (`tanh(x)`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output in the output\n+      sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state in addition to the\n+      output. (default False)\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A 5D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n+      are set.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+  Input shape: - If data_format='channels_first'\n+        5D tensor with shape: `(samples, time, channels, rows, cols)` - If\n+          data_format='channels_last'\n+        5D tensor with shape: `(samples, time, rows, cols, channels)`\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n+        data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, filters)` if\n+        data_format='channels_last'. `rows` and `cols` values might have changed\n+        due to padding.\n+    - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,\n+      filters, new_rows, new_cols)` if data_format='channels_first'\n+      or shape: `(samples, timesteps, new_rows, new_cols, filters)` if\n+        data_format='channels_last'.\n+    - Else, 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n+      data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, filters)` if\n+        data_format='channels_last'.\n+\n+  Raises:\n+    ValueError: in case of invalid constructor arguments.\n+\n+  References:\n+    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n+    (the current implementation does not include the feedback loop on the\n+    cells output).\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1),\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTM2D, self).__init__(\n+        rank=2,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        **kwargs)\n+\n+\n+@keras_export('keras.layers.ConvLSTM3D')\n+class ConvLSTM3D(ConvLSTM):\n+  \"\"\"3D Convolutional LSTM.\n+\n+  Similar to an LSTM layer, but the input transformations\n+  and recurrent transformations are both convolutional.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      dimensions of the convolution window.\n+    strides: An integer or tuple/list of n integers, specifying the strides of\n+      the convolution. Specifying any stride value != 1 is incompatible with\n+      specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding evenly to the left/right or up/down\n+      of the input such that output has the same height/width dimension as the\n+      input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n+      corresponds to inputs with shape `(batch, time, channels, ...)`. It\n+      defaults to the `image_data_format` value found in your Keras config file\n+      at `~/.keras/keras.json`. If you never set it, then it will be\n+      \"channels_last\".\n+    dilation_rate: An integer or tuple/list of n integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. By default hyperbolic tangent\n+      activation function is applied (`tanh(x)`).\n+    recurrent_activation: Activation function to use for the recurrent step.\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n+      the linear transformation of the inputs.\n+    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+      matrix, used for the linear transformation of the recurrent state.\n+    bias_initializer: Initializer for the bias vector.\n+    unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n+      initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n+      is recommended in [Jozefowicz et al., 2015](\n+        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix.\n+    recurrent_regularizer: Regularizer function applied to the\n+      `recurrent_kernel` weights matrix.\n+    bias_regularizer: Regularizer function applied to the bias vector.\n+    activity_regularizer: Regularizer function applied to.\n+    kernel_constraint: Constraint function applied to the `kernel` weights\n+      matrix.\n+    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n+      weights matrix.\n+    bias_constraint: Constraint function applied to the bias vector.\n+    return_sequences: Boolean. Whether to return the last output in the output\n+      sequence, or the full sequence. (default False)\n+    return_state: Boolean Whether to return the last state in addition to the\n+      output. (default False)\n+    go_backwards: Boolean (default False). If True, process the input sequence\n+      backwards.\n+    stateful: Boolean (default False). If True, the last state for each sample\n+      at index i in a batch will be used as initial state for the sample of\n+      index i in the following batch.\n+    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n+      transformation of the inputs.\n+    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n+      the linear transformation of the recurrent state.\n+  Call arguments:\n+    inputs: A 6D tensor.\n+    mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n+      given timestep should be masked.\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode. This argument is passed to the cell\n+      when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n+      are set.\n+    initial_state: List of initial state tensors to be passed to the first call\n+      of the cell.\n+  Input shape: - If data_format='channels_first'\n+        6D tensor with shape: `(samples, time, channels, rows, cols, depth)` -\n+          If data_format='channels_last'\n+        5D tensor with shape: `(samples, time, rows, cols, depth, channels)`\n+  Output shape:\n+    - If `return_state`: a list of tensors. The first tensor is the output. The\n+      remaining tensors are the last states,\n+      each 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n+        new_depth)` if data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n+        data_format='channels_last'. `rows`, `cols`, and `depth` values might\n+        have changed due to padding.\n+    - If `return_sequences`: 6D tensor with shape: `(samples, timesteps,\n+      filters, new_rows, new_cols, new_depth)` if data_format='channels_first'\n+      or shape: `(samples, timesteps, new_rows, new_cols, new_depth, filters)`\n+        if data_format='channels_last'.\n+    - Else, 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n+      new_depth)` if data_format='channels_first'\n+      or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n+        data_format='channels_last'.\n+\n+  Raises:\n+    ValueError: in case of invalid constructor arguments.\n+\n+  References:\n+    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n+    (the current implementation does not include the feedback loop on the\n+    cells output).\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1, 1),\n+               activation='tanh',\n+               recurrent_activation='hard_sigmoid',\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               recurrent_initializer='orthogonal',\n+               bias_initializer='zeros',\n+               unit_forget_bias=True,\n+               kernel_regularizer=None,\n+               recurrent_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               recurrent_constraint=None,\n+               bias_constraint=None,\n+               return_sequences=False,\n+               return_state=False,\n+               go_backwards=False,\n+               stateful=False,\n+               dropout=0.0,\n+               recurrent_dropout=0.0,\n+               **kwargs):\n+    super(ConvLSTM3D, self).__init__(\n+        rank=3,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        recurrent_activation=recurrent_activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        recurrent_initializer=recurrent_initializer,\n+        bias_initializer=bias_initializer,\n+        unit_forget_bias=unit_forget_bias,\n+        kernel_regularizer=kernel_regularizer,\n+        recurrent_regularizer=recurrent_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        kernel_constraint=kernel_constraint,\n+        recurrent_constraint=recurrent_constraint,\n+        bias_constraint=bias_constraint,\n+        return_sequences=return_sequences,\n+        return_state=return_state,\n+        go_backwards=go_backwards,\n+        stateful=stateful,\n+        dropout=dropout,\n+        recurrent_dropout=recurrent_dropout,\n+        **kwargs)\n\n@@ -25,7 +25,63 @@ from keras import testing_utils\n \n \n @keras_parameterized.run_all_keras_modes\n-class ConvLSTMTest(keras_parameterized.TestCase):\n+class ConvLSTM1DTest(keras_parameterized.TestCase):\n+\n+  @parameterized.named_parameters(\n+      *testing_utils.generate_combinations_with_testcase_name(\n+          data_format=['channels_first', 'channels_last'],\n+          return_sequences=[True, False]))\n+  def test_conv_lstm(self, data_format, return_sequences):\n+    num_row = 3\n+    filters = 3\n+    num_samples = 1\n+    input_channel = 2\n+    input_num_row = 5\n+    sequence_len = 2\n+    if data_format == 'channels_first':\n+      inputs = np.random.rand(num_samples, sequence_len, input_channel,\n+                              input_num_row)\n+    else:\n+      inputs = np.random.rand(num_samples, sequence_len, input_num_row,\n+                              input_channel)\n+\n+    # test for return state:\n+    x = keras.Input(batch_shape=inputs.shape)\n+    kwargs = {\n+        'data_format': data_format,\n+        'return_sequences': return_sequences,\n+        'return_state': True,\n+        'stateful': True,\n+        'filters': filters,\n+        'kernel_size': num_row,\n+        'padding': 'valid',\n+    }\n+    layer = keras.layers.ConvLSTM1D(**kwargs)\n+    layer.build(inputs.shape)\n+    outputs = layer(x)\n+    _, states = outputs[0], outputs[1:]\n+    self.assertEqual(len(states), 2)\n+    model = keras.models.Model(x, states[0])\n+\n+    state = model.predict(inputs)\n+\n+    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n+\n+    # test for output shape:\n+    testing_utils.layer_test(\n+        keras.layers.ConvLSTM1D,\n+        kwargs={\n+            'data_format': data_format,\n+            'return_sequences': return_sequences,\n+            'filters': filters,\n+            'kernel_size': num_row,\n+            'padding': 'valid'\n+        },\n+        input_shape=inputs.shape)\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ConvLSTM2DTest(keras_parameterized.TestCase):\n \n   @parameterized.named_parameters(\n       *testing_utils.generate_combinations_with_testcase_name(\n@@ -66,8 +122,7 @@ class ConvLSTMTest(keras_parameterized.TestCase):\n     model = keras.models.Model(x, states[0])\n     state = model.predict(inputs)\n \n-    self.assertAllClose(\n-        keras.backend.eval(layer.states[0]), state, atol=1e-4)\n+    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n \n     # test for output shape:\n     testing_utils.layer_test(\n@@ -109,8 +164,7 @@ class ConvLSTMTest(keras_parameterized.TestCase):\n       out1 = model.predict(np.ones_like(inputs))\n \n       # train once so that the states change\n-      model.train_on_batch(np.ones_like(inputs),\n-                           np.random.random(out1.shape))\n+      model.train_on_batch(np.ones_like(inputs), np.random.random(out1.shape))\n       out2 = model.predict(np.ones_like(inputs))\n \n       # if the state is not reset, output should be different\n@@ -231,5 +285,65 @@ class ConvLSTMTest(keras_parameterized.TestCase):\n     model.predict([x_1, x_2])\n \n \n+@keras_parameterized.run_all_keras_modes\n+class ConvLSTM3DTest(keras_parameterized.TestCase):\n+\n+  @parameterized.named_parameters(\n+      *testing_utils.generate_combinations_with_testcase_name(\n+          data_format=['channels_first', 'channels_last'],\n+          return_sequences=[True, False]))\n+  def test_conv_lstm(self, data_format, return_sequences):\n+    num_height = 3\n+    num_width = 3\n+    num_depth = 3\n+    filters = 3\n+    num_samples = 1\n+    input_channel = 2\n+    input_height = 5\n+    input_width = 5\n+    input_depth = 5\n+    sequence_len = 2\n+    if data_format == 'channels_first':\n+      inputs = np.random.rand(num_samples, sequence_len, input_channel,\n+                              input_height, input_width, input_depth)\n+    else:\n+      inputs = np.random.rand(num_samples, sequence_len, input_height,\n+                              input_width, input_depth, input_channel)\n+\n+    # test for return state:\n+    x = keras.Input(batch_shape=inputs.shape)\n+    kwargs = {\n+        'data_format': data_format,\n+        'return_sequences': return_sequences,\n+        'return_state': True,\n+        'stateful': True,\n+        'filters': filters,\n+        'kernel_size': (num_height, num_width, num_depth),\n+        'padding': 'same'\n+    }\n+    layer = keras.layers.ConvLSTM3D(**kwargs)\n+    layer.build(inputs.shape)\n+    outputs = layer(x)\n+    _, states = outputs[0], outputs[1:]\n+    self.assertEqual(len(states), 2)\n+    model = keras.models.Model(x, states[0])\n+\n+    state = model.predict(inputs)\n+\n+    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n+\n+    # test for output shape:\n+    testing_utils.layer_test(\n+        keras.layers.ConvLSTM3D,\n+        kwargs={\n+            'data_format': data_format,\n+            'return_sequences': return_sequences,\n+            'filters': filters,\n+            'kernel_size': (num_height, num_width, num_depth),\n+            'padding': 'valid'\n+        },\n+        input_shape=inputs.shape)\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
