{"custom_id": "keras#376cc8c9b84077153db21dae9423abc9f6659d49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 13 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 964 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -24,6 +24,8 @@ from keras.dtensor import lazy_variable\n from keras.dtensor import utils\n from keras.engine import base_layer\n \n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n+\n # pylint: disable=missing-class-docstring\n \n # We will skip the path for certain attributes when mapping the layout, e.g.\n@@ -42,6 +44,7 @@ def get_current_layout_map():\n   return getattr(_LAYOUT_MAP, 'layout_map', None)\n \n \n+@keras_export('keras.dtensor.experimental.LayoutMap', v1=[])\n class LayoutMap(collections.abc.MutableMapping):\n \n   def __init__(self, mesh=None):\n@@ -105,6 +108,7 @@ class LayoutMap(collections.abc.MutableMapping):\n     return self._default_mesh\n \n \n+@keras_export('keras.dtensor.experimental.layout_map_scope', v1=[])\n @contextlib.contextmanager\n def layout_map_scope(layout_map):\n   \"\"\"Apply the layout to all the tf.Variables created under the scope.\n\n@@ -25,18 +25,20 @@ from keras.optimizers.schedules import learning_rate_schedule\n \n import tensorflow.compat.v2 as tf\n \n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n+\n \n # pylint: disable=protected-access,missing-class-docstring\n class Optimizer(optimizer_lib._BaseOptimizer):\n   \"\"\"DTensor specific optimizers.\n \n-  The major changes for this class are:\n+  The major changes for this class is that all the variable init logic will be\n+  mesh/layout aware.\n \n-  All the variable init logic will be mesh/layout aware.\n-\n-  Note that we didn't subclass optimizer_lib.Optimizer since it contains the\n-  extra logic of handling distribution strategy, which we don't need for DTensor\n   \"\"\"\n+  # Note that we didn't subclass optimizer_lib.Optimizer since it contains the\n+  # extra logic of handling distribution strategy, which we don't need for\n+  # DTensor\n \n   def __init__(self, name, mesh=None):\n     \"\"\"Create a new Optimizer.\n@@ -160,6 +162,7 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n     return dtensor.DVariable(init_val, name='learning_rate')\n \n \n+@keras_export('keras.dtensor.experimental.optimizers.Adadelta', v1=[])\n class Adadelta(Optimizer, adadelta.Adadelta):\n \n   def __init__(self,\n@@ -179,6 +182,7 @@ class Adadelta(Optimizer, adadelta.Adadelta):\n     self.epsilon = epsilon\n \n \n+@keras_export('keras.dtensor.experimental.optimizers.Adagrad', v1=[])\n class Adagrad(Optimizer, adagrad.Adagrad):\n \n   def __init__(self,\n@@ -195,6 +199,7 @@ class Adagrad(Optimizer, adagrad.Adagrad):\n     self.epsilon = epsilon\n \n \n+@keras_export('keras.dtensor.experimental.optimizers.Adam', v1=[])\n class Adam(Optimizer, adam.Adam):\n \n   def __init__(self,\n@@ -215,6 +220,7 @@ class Adam(Optimizer, adam.Adam):\n     self.amsgrad = amsgrad\n \n \n+@keras_export('keras.dtensor.experimental.optimizers.RMSprop', v1=[])\n class RMSprop(Optimizer, rmsprop.RMSprop):\n \n   def __init__(self,\n@@ -236,6 +242,7 @@ class RMSprop(Optimizer, rmsprop.RMSprop):\n     self.centered = centered\n \n \n+@keras_export('keras.dtensor.experimental.optimizers.SGD', v1=[])\n class SGD(Optimizer, sgd.SGD):\n \n   def __init__(self,\n@@ -254,3 +261,10 @@ class SGD(Optimizer, sgd.SGD):\n     self.nesterov = nesterov\n     if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n       raise ValueError('`momentum` must be between [0, 1].')\n+\n+\n+Adadelta.__doc__ = Optimizer.__doc__ + adadelta.Adadelta.__doc__\n+Adagrad.__doc__ = Optimizer.__doc__ + adagrad.Adagrad.__doc__\n+Adam.__doc__ = Optimizer.__doc__ + adam.Adam.__doc__\n+RMSprop.__doc__ = Optimizer.__doc__ + rmsprop.RMSprop.__doc__\n+SGD.__doc__ = Optimizer.__doc__ + sgd.SGD.__doc__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fd4ff0606c69a091ed33d203e14e66f98f2b5a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 147 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -298,7 +298,7 @@ class MergingLayersTest(test_combinations.TestCase):\n     self.assertAllClose(out, x1)\n \n     # A single element must be passed as a list, not by itself.\n-    with self.assertRaises(ValueError):\n+    with self.assertRaisesRegex(ValueError, 'called on a list'):\n       layer(i1)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd0f1f480613ceffd994658e5632d54c660d1e73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 598 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 0.9\n\nDIFF:\n@@ -132,6 +132,15 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n           callbacks=callbacks,\n           use_dataset_creator=use_dataset_creator)\n \n+  def testClusterCoordinatorSingleInstanceWithJitCompileTrue(\n+      self, strategy, use_dataset_creator):\n+    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n+                                \"Graph execution error\"):\n+      model = self._model_fit(strategy,\n+                              use_dataset_creator=use_dataset_creator,\n+                              jit_compile=True)\n+      strategy = model.distribute_strategy\n+\n \n if __name__ == \"__main__\":\n   tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -70,7 +70,7 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                      steps_per_execution=1,\n                      run_eagerly=False,\n                      with_normalization_layer=False,\n-                     use_lookup_layer=False):\n+                     jit_compile=None):\n \n     class ResultAssertingCallback(callbacks_lib.Callback):\n       \"\"\"A callback that asserts the result of the tests.\"\"\"\n@@ -103,7 +103,8 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n         loss=\"binary_crossentropy\",\n         metrics=[self._accuracy_metric],\n         steps_per_execution=steps_per_execution,\n-        run_eagerly=run_eagerly)\n+        run_eagerly=run_eagerly,\n+        jit_compile=jit_compile)\n     return model, [ResultAssertingCallback()]\n \n   def _model_fit(self,\n@@ -120,7 +121,8 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                  callbacks=None,\n                  use_lookup_layer=False,\n                  use_dataset_creator=True,\n-                 verbose=\"auto\"):\n+                 verbose=\"auto\",\n+                 jit_compile=None):\n     if callbacks is None:\n       callbacks = []\n \n@@ -128,7 +130,7 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n                                                    steps_per_execution,\n                                                    run_eagerly,\n                                                    with_normalization_layer,\n-                                                   use_lookup_layer)\n+                                                   jit_compile)\n     callbacks += default_callbacks\n \n     if x is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50d6c9cfe29a46c868430c05c804e2519f9b2d7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 44 | Churn Cumulative: 10441 | Contributors (this commit): 28 | Commits (past 90d): 21 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2188,6 +2188,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\n     \"\"\"Call the layer on input KerasTensors and returns output KerasTensors.\"\"\"\n \n+    keras_tensor_inputs = inputs\n     call_fn = self.call\n     # Wrapping `call` function in autograph to allow for dynamic control\n     # flow and control dependencies in call. We are limiting this to\n@@ -2236,7 +2237,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       outputs = tf.nest.map_structure(\n           keras_tensor.keras_tensor_from_tensor, outputs)\n \n-    self._set_save_spec(inputs, args, kwargs)\n+    self._set_save_spec(keras_tensor_inputs, args, kwargs)\n     if hasattr(self, '_set_inputs') and not self.inputs:\n       # TODO(kaftan): figure out if we need to do this at all\n       # Subclassed network: explicitly set metadata normally set by\n\n@@ -408,6 +408,28 @@ class TestJson(test_combinations.TestCase):\n     self.assertEqual(loaded_layer.sublayers[1].name, 'MySubLayer')\n \n \n+class MaskedTensor(tf.experimental.ExtensionType):\n+  __name__ = 'MaskedTensor_save_test'\n+  values: tf.Tensor\n+  mask: tf.Tensor\n+  class Spec(tf.TypeSpec):\n+\n+    @property\n+    def shape(self):\n+      return self.values.shape\n+\n+    @property\n+    def dtype(self):\n+      return self.values.dtype\n+\n+    def with_shape(self, shape):\n+      values_spec = tf.TensorSpec(\n+          shape, dtype=self.values.dtype, name=self.values.name)\n+      mask_spec = tf.TensorSpec(\n+          shape, dtype=self.mask.dtype, name=self.mask.name)\n+      return MaskedTensor.Spec(values_spec, mask_spec)\n+\n+\n @test_combinations.run_with_all_saved_model_formats\n class TestWholeModelSaving(test_combinations.TestCase):\n \n@@ -1231,6 +1253,25 @@ class TestWholeModelSaving(test_combinations.TestCase):\n     model.save(saved_model_dir)\n     keras.models.load_model(saved_model_dir)\n \n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['eager']))\n+  @test_utils.run_v2_only\n+  def test_save_inputs_spec_with_composite_tensor_names(self):\n+\n+    class KerasModel(keras.Model):\n+\n+      def call(self, inputs):\n+        return inputs\n+\n+    spec = MaskedTensor.Spec(\n+        tf.TensorSpec([None], name='x__values'),\n+        tf.TensorSpec([None], dtype=tf.bool, name='x__mask')\n+    )\n+    km1 = KerasModel()\n+    inputs = keras.Input(type_spec=spec)\n+    km1(inputs)\n+    self.assertEqual(km1.save_spec()[0][0].mask.name, 'x__mask')\n+\n \n # Factory functions to create models that will be serialized inside a Network.\n def _make_graph_network(input_size, output_size):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#793620ae1bdda7e37edd485b034e8962fff57f3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 6517 | Contributors (this commit): 8 | Commits (past 90d): 27 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -228,7 +228,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n   `augment_bounding_box()`, which handles the bounding box augmentation, if the\n   layer supports that.\n \n-  `get_random_tranformation()`, which should produce a random transformation\n+  `get_random_transformation()`, which should produce a random transformation\n   setting. The tranformation object, which could be any type, will be passed to\n   `augment_image`, `augment_label` and `augment_bounding_box`, to coodinate\n   the randomness behavior, eg, in the RandomFlip layer, the image and\n@@ -320,7 +320,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     Args:\n       image: 3D image input tensor to the layer. Forwarded from `layer.call()`.\n       transformation: The transformation object produced by\n-        `get_random_tranformation`. Used to coordinate the randomness between\n+        `get_random_transformation`. Used to coordinate the randomness between\n         image, label and bounding box.\n \n     Returns:\n@@ -335,7 +335,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     Args:\n       label: 1D label to the layer. Forwarded from `layer.call()`.\n       transformation: The transformation object produced by\n-        `get_random_tranformation`. Used to coordinate the randomness between\n+        `get_random_transformation`. Used to coordinate the randomness between\n         image, label and bounding box.\n \n     Returns:\n@@ -350,7 +350,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     Args:\n       bounding_box: 2D bounding boxex to the layer. Forwarded from `call()`.\n       transformation: The transformation object produced by\n-        `get_random_tranformation`. Used to coordinate the randomness between\n+        `get_random_transformation`. Used to coordinate the randomness between\n         image, label and bounding box.\n \n     Returns:\n@@ -359,7 +359,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     raise NotImplementedError()\n \n   @doc_controls.for_subclass_implementers\n-  def get_random_tranformation(self):\n+  def get_random_transformation(self):\n     \"\"\"Produce random transformation config.\n \n     This is used to produce same randomness between image/label/bounding_box.\n@@ -388,7 +388,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n       return inputs\n \n   def _augment(self, inputs):\n-    transformation = self.get_random_tranformation()  # pylint: disable=assignment-from-none\n+    transformation = self.get_random_transformation()  # pylint: disable=assignment-from-none\n     image = inputs.get('images', None)\n     label = inputs.get('labels', None)\n     bounding_box = inputs.get('bounding_boxes', None)\n@@ -790,7 +790,7 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n \n     if transformation is None:\n-      transformation = self.get_random_tranformation()\n+      transformation = self.get_random_transformation()\n     height_translation = transformation['height_translation']\n     width_translation = transformation['width_translation']\n     height_translation = height_translation * img_hd\n@@ -809,7 +809,7 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     output.set_shape(original_shape)\n     return output\n \n-  def get_random_tranformation(self):\n+  def get_random_transformation(self):\n     batch_size = 1\n     height_translation = self._random_generator.random_uniform(\n         shape=[batch_size, 1],\n\n@@ -1872,7 +1872,7 @@ class RandomAddLayer(image_preprocessing.BaseImageAugmentationLayer):\n     self.value_range = value_range\n     self.fixed_value = fixed_value\n \n-  def get_random_tranformation(self):\n+  def get_random_transformation(self):\n     if self.fixed_value:\n       return self.fixed_value\n     return self._random_generator.random_uniform(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#084c7c4b1f8c3641f79c3e3637210141f12cdf6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4688 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -1775,7 +1775,7 @@ def categorical_crossentropy(y_true,\n   \"\"\"\n   if isinstance(axis, bool):\n     raise ValueError(\n-      '`axis` must be of type `int`.')\n+      f'`axis` must be of type `int`. Received: axis={axis} of type {type(axis)}')\n   y_pred = tf.convert_to_tensor(y_pred)\n   y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78566bdf0b70bb4aef0f8d08a2e990e8a4e9389b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 344 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.42857142857142855\n\nDIFF:\n@@ -136,12 +136,7 @@ class AdamW(optimizer.Optimizer):\n     if hasattr(self, '_built') and self._built:\n       return\n     self._built = True\n-\n-    if exclude_from_weight_decay == None:\n-      self._exclude_from_weight_decay = []\n-    else:\n-      self._exclude_from_weight_decay = exclude_from_weight_decay\n-\n+    self._exclude_from_weight_decay = var_list or []\n     self._momentums = []\n     self._velocities = []\n     for var in var_list:\n@@ -222,14 +217,10 @@ class AdamW(optimizer.Optimizer):\n     return config\n \n   def exclude_from_weight_decay(self, var_list):\n-    if self._built:\n-      if var_list == None:\n-        self._exclude_from_weight_decay = []\n+    if not self._built:\n+      self._exclude_from_weight_decay = var_list or []\n     else:\n-        self._exclude_from_weight_decay = var_list\n-    else:\n-      raise ValueError('This optimizer has not yet been built. '\n-                       'Build the optimizer first and than use `exclude_from_weight_decay()`')\n+      raise ValueError('`exclude_from_weight_decay()` must be used only before the optimizer is build.')\n \n AdamW.__doc__ = AdamW.__doc__.replace(\n     '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3f5bec982c568b4d61681753d9524c0e7994f53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 351 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -217,10 +217,11 @@ class AdamW(optimizer.Optimizer):\n     return config\n \n   def exclude_from_weight_decay(self, var_list):\n-    if not self._built:\n-      self._exclude_from_weight_decay = var_list or []\n-    else:\n+    if hasattr(self, '_built') and self._built:\n       raise ValueError('`exclude_from_weight_decay()` must be used only before the optimizer is build.')\n     \n+    self._exclude_from_weight_decay = var_list or []\n+      \n+\n AdamW.__doc__ = AdamW.__doc__.replace(\n     '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#583f7cd4d664f760bbab4dcf283d725ced10fcce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 237 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,11 +72,8 @@ class MeanMetricAsCompositeTensor(keras.metrics.Mean,\n \n   @property\n   def _type_spec(self):\n-    weight_specs = []\n-    for w in self.weights:\n-      weight_specs.append(\n-          resource_variable_ops.VariableSpec(\n-              w.shape, w.dtype, w.name.split(\":\")[0], trainable=False))\n+    weight_specs = [\n+        resource_variable_ops.VariableSpec.from_value(w) for w in self.weights]\n     return MeanMetricSpec(self.get_config(), weight_specs)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c0390bf431ab441581c44a1d84ba8a3bc1768b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 353 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -174,7 +174,7 @@ class AdamW(optimizer.Optimizer):\n     alpha = (lr * tf.sqrt(1 - beta_2_power) / (1 - beta_1_power))\n \n     # Apply step weight decay\n-    if self.weight_decay and variable not in self._exclude_from_weight_decay:\n+    if self.weight_decay != 0 and variable not in self._exclude_from_weight_decay:\n       wd = tf.cast(self.weight_decay, variable.dtype)\n       variable.assign_sub(variable * (1 - lr * wd))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d939b160383bef34cac78f4c2ef44109e0fab20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 6541 | Contributors (this commit): 8 | Commits (past 90d): 29 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -348,7 +348,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     \"\"\"Augment bounding boxes for one image during training.\n \n     Args:\n-      bounding_box: 2D bounding boxex to the layer. Forwarded from `call()`.\n+      bounding_box: 2D bounding boxes to the layer. Forwarded from `call()`.\n       transformation: The transformation object produced by\n         `get_random_transformation`. Used to coordinate the randomness between\n         image, label and bounding box.\n@@ -359,11 +359,17 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     raise NotImplementedError()\n \n   @doc_controls.for_subclass_implementers\n-  def get_random_transformation(self):\n-    \"\"\"Produce random transformation config.\n+  def get_random_transformation(\n+      self, image=None, label=None, bounding_box=None):\n+    \"\"\"Produce random transformation config for one single input.\n \n     This is used to produce same randomness between image/label/bounding_box.\n \n+    Args:\n+      image: 3D image tensor from inputs.\n+      label: optional 1D label tensor from inputs.\n+      bounding_box: optional 2D bounding boxes tensor from inputs.\n+\n     Returns:\n       Any type of object, which will be forwarded to `augment_image`,\n       `augment_label` and `augment_bounding_box` as the `transformation`\n@@ -388,10 +394,11 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n       return inputs\n \n   def _augment(self, inputs):\n-    transformation = self.get_random_transformation()  # pylint: disable=assignment-from-none\n     image = inputs.get('images', None)\n     label = inputs.get('labels', None)\n     bounding_box = inputs.get('bounding_boxes', None)\n+    transformation = self.get_random_transformation(\n+        image=image, label=label, bounding_box=bounding_box)  # pylint: disable=assignment-from-none\n     image = self.augment_image(image, transformation=transformation)\n     result = {'images': image}\n     if label is not None:\n@@ -790,7 +797,7 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n \n     if transformation is None:\n-      transformation = self.get_random_transformation()\n+      transformation = self.get_random_transformation(image=image)\n     height_translation = transformation['height_translation']\n     width_translation = transformation['width_translation']\n     height_translation = height_translation * img_hd\n@@ -809,7 +816,9 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     output.set_shape(original_shape)\n     return output\n \n-  def get_random_transformation(self):\n+  def get_random_transformation(\n+      self, image=None, label=None, bounding_box=None):\n+    del image, label, bounding_box\n     batch_size = 1\n     height_translation = self._random_generator.random_uniform(\n         shape=[batch_size, 1],\n\n@@ -1872,7 +1872,8 @@ class RandomAddLayer(image_preprocessing.BaseImageAugmentationLayer):\n     self.value_range = value_range\n     self.fixed_value = fixed_value\n \n-  def get_random_transformation(self):\n+  def get_random_transformation(\n+      self, image=None, label=None, bounding_box=None):\n     if self.fixed_value:\n       return self.fixed_value\n     return self._random_generator.random_uniform(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d730bca0f772f2ec8d0940465b3f99ec8fc1e019", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 73 | Files Changed: 11 | Hunks: 33 | Methods Changed: 39 | Complexity Δ (Sum/Max): -13/5 | Churn Δ: 118 | Churn Cumulative: 1534 | Contributors (this commit): 4 | Commits (past 90d): 59 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,7 +67,7 @@ class Adadelta(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='Adadelta',\n                **kwargs):\n     super(Adadelta, self).__init__(\n@@ -99,11 +99,6 @@ class Adadelta(optimizer.Optimizer):\n \n   def update_step(self, grad, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     lr = tf.cast(self.learning_rate, variable.dtype)\n \n     var_key = self._var_key(variable)\n\n@@ -62,7 +62,7 @@ class Adagrad(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='Adagrad',\n                **kwargs):\n     super(Adagrad, self).__init__(\n@@ -94,11 +94,6 @@ class Adagrad(optimizer.Optimizer):\n \n   def update_step(self, grad, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     lr = tf.cast(self.learning_rate, variable.dtype)\n \n     var_key = self._var_key(variable)\n\n@@ -92,7 +92,7 @@ class Adam(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='Adam',\n                **kwargs):\n     super(Adam, self).__init__(\n@@ -142,11 +142,6 @@ class Adam(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     beta_1_power = None\n     beta_2_power = None\n     lr = tf.cast(self.learning_rate, variable.dtype)\n\n@@ -80,7 +80,7 @@ class Adamax(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='Adamax',\n                **kwargs):\n     super(Adamax, self).__init__(\n@@ -123,11 +123,6 @@ class Adamax(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     lr = tf.cast(self.learning_rate, variable.dtype)\n     local_step = tf.cast(self.iterations + 1, variable.dtype)\n     beta_1_power = tf.pow(tf.cast(self.beta_1, variable.dtype), local_step)\n\n@@ -99,7 +99,7 @@ class AdamW(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='AdamW',\n                **kwargs):\n     super(AdamW, self).__init__(\n@@ -154,11 +154,6 @@ class AdamW(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     beta_1_power = None\n     beta_2_power = None\n     lr = tf.cast(self.learning_rate, variable.dtype)\n\n@@ -107,7 +107,7 @@ class Ftrl(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='Ftrl',\n                **kwargs):\n     super().__init__(\n@@ -178,11 +178,6 @@ class Ftrl(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n \n     lr = tf.cast(self.learning_rate, variable.dtype)\n     var_key = self._var_key(variable)\n\n@@ -63,7 +63,7 @@ class Nadam(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='Nadam',\n                **kwargs):\n     super().__init__(\n@@ -110,11 +110,6 @@ class Nadam(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     var_dtype = variable.dtype\n     lr = tf.cast(self.learning_rate, var_dtype)\n     local_step = tf.cast(self.iterations + 1, var_dtype)\n\n@@ -41,7 +41,7 @@ class _BaseOptimizer(tf.Module):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                **kwargs):\n     self._name = name\n     self.clipnorm = clipnorm\n@@ -49,6 +49,10 @@ class _BaseOptimizer(tf.Module):\n     self.clipvalue = clipvalue\n     self.use_ema = use_ema\n     self.jit_compile = jit_compile\n+    if not tf.config.list_physical_devices(\"GPU\"):\n+      # Optimizer only benefits from XLA when training on GPU. So if no GPU is\n+      # found, we turn off XLA.\n+      self.jit_compile = False\n     if use_ema:\n       # Verify the arguments related to EMA.\n       if ema_momentum > 1 or ema_momentum < 0:\n@@ -131,12 +135,18 @@ class _BaseOptimizer(tf.Module):\n     Returns:\n       An `Operation` that applies the specified gradients.\n     \"\"\"\n-    return self.update_step(gradient, variable)\n+    return self._update_step(gradient, variable)\n \n   def _update_step(self, gradient, variable):\n-    if self.jit_compile:\n-      self._update_step_xla(gradient, variable, id(self._var_key(variable)))\n-    else:\n+    if getattr(variable, \"_unique_id\", None) is None:\n+      # Variable has no `_unique_id` if called during `model.save()`, in which\n+      # case we do not want to update the variable.\n+      return\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(\n+          f\"The optimizer cannot recognize variable {variable.name}. This \"\n+          f\"usually means that you're reusing an optimizer previously created \"\n+          f\"for a different model. Try creating a new optimizer instance.\")\n     self.update_step(gradient, variable)\n \n   def compute_gradients(self, loss, var_list, tape=None):\n@@ -417,6 +427,10 @@ class _BaseOptimizer(tf.Module):\n     Args:\n       grads_and_vars: List of (gradient, variable) pairs.\n     \"\"\"\n+    if self.jit_compile:\n+      for grad, var in grads_and_vars:\n+        self._update_step_xla(grad, var, id(self._var_key(var)))\n+    else:\n       for grad, var in grads_and_vars:\n         self._update_step(grad, var)\n \n@@ -540,9 +554,10 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n       variables in-place). When using the built-in `fit()` training loop, this\n       happens automatically after the last epoch, and you don't need to do\n       anything.\n-    jit_compile: Boolean, defaults to False. If True, the optimizer will use XLA\n+    jit_compile: Boolean, defaults to True. If True, the optimizer will use XLA\n       compilation. `jit_compile` cannot be True when training with\n-      `tf.distribute.experimental.ParameterServerStrategy`.\n+      `tf.distribute.experimental.ParameterServerStrategy`. Additionally,\n+      if no GPU device is found, this flag will be ignored.\n     **kwargs: keyword arguments only used for backward compatibility.\"\"\"\n \n \n@@ -725,7 +740,7 @@ class Optimizer(_BaseOptimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                **kwargs):\n     \"\"\"Create a new Optimizer.\"\"\"\n \n@@ -838,6 +853,9 @@ class Optimizer(_BaseOptimizer):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n \n     def apply_grad_to_update_var(var, grad):\n+      if self.jit_compile:\n+        return self._update_step_xla(grad, var, id(self._var_key(var)))\n+      else:\n         return self._update_step(grad, var)\n \n     for grad, var in grads_and_vars:\n\n@@ -174,7 +174,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n       optimizer.learning_rate = 2.0\n \n   def testSetIterations(self):\n-    optimizer = adam_new.Adam()\n+    optimizer = adam_new.Adam(jit_compile=False)\n     optimizer.iterations = tf.Variable(2, dtype=tf.int32)\n     self.assertEqual(optimizer.iterations, 2)\n     var_list = [tf.Variable(2.0), tf.Variable(2.0)]\n@@ -227,8 +227,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         ema_momentum=0.5,\n         ema_overwrite_frequency=50)\n     config = optimizer.get_config()\n-    self.assertDictEqual(\n-        config, {\n+    expected_config = {\n         \"learning_rate\": np.float32(0.05),\n         \"beta_1\": 0.7,\n         \"beta_2\": 0.77,\n@@ -240,8 +239,8 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         \"use_ema\": True,\n         \"ema_momentum\": 0.5,\n         \"ema_overwrite_frequency\": 50,\n-            \"jit_compile\": False,\n-        })\n+    }\n+    self.assertDictContainsSubset(expected_config, config)\n     restored_optimizer = adam_new.Adam.from_config(config)\n     self.assertDictEqual(restored_optimizer.get_config(),\n                          optimizer.get_config())\n@@ -317,8 +316,8 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     grads = tf.convert_to_tensor([0, 1., 1.5, 0, 0], dtype=tf.float64)\n     sparse_grads = tf.IndexedSlices(\n         tf.convert_to_tensor([1., 1.5], dtype=tf.float64),\n-        [1, 2],\n-        dense_shape=[len(grads)])\n+        tf.convert_to_tensor([1, 2]),\n+        dense_shape=tf.convert_to_tensor([len(grads)]))\n     for _ in range(5):\n       optimizer_1.apply_gradients(zip([grads], [x1]))\n       optimizer_2.apply_gradients(zip([sparse_grads], [x2]))\n@@ -334,8 +333,8 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n     grads = tf.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n     sparse_grads = tf.IndexedSlices(\n         tf.convert_to_tensor([0, 0.2, 0.4, 0.8], dtype=tf.float64),\n-        [0, 2, 4, 6],\n-        dense_shape=[len(grads)])\n+        tf.convert_to_tensor([0, 2, 4, 6]),\n+        dense_shape=tf.convert_to_tensor([len(grads)]))\n \n     for _ in range(5):\n       self.assertAllClose(x1, x2)\n@@ -446,7 +445,8 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n     # Test the optimizer yields same numerical results when jit_compile is\n     # on and off.\n     with strategy.scope():\n-      optimizer_1 = adam_new.Adam(use_ema=True, ema_overwrite_frequency=1)\n+      optimizer_1 = adam_new.Adam(\n+          jit_compile=False, use_ema=True, ema_overwrite_frequency=1)\n       optimizer_2 = adam_new.Adam(\n           jit_compile=True, use_ema=True, ema_overwrite_frequency=1)\n       model_1 = keras.Sequential([\n\n@@ -82,7 +82,7 @@ class RMSprop(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=100,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='RMSprop',\n                **kwargs):\n     super(RMSprop, self).__init__(\n@@ -126,11 +126,6 @@ class RMSprop(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n     lr = tf.cast(self.learning_rate, variable.dtype)\n \n     var_key = self._var_key(variable)\n\n@@ -101,7 +101,7 @@ class SGD(optimizer.Optimizer):\n                use_ema=False,\n                ema_momentum=0.99,\n                ema_overwrite_frequency=None,\n-               jit_compile=False,\n+               jit_compile=True,\n                name='SGD',\n                **kwargs):\n     super(SGD, self).__init__(\n@@ -142,12 +142,6 @@ class SGD(optimizer.Optimizer):\n \n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-    if self._var_key(variable) not in self._index_dict:\n-      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n-                     f'this usually means you are calling an optimizer '\n-                     f'previously used on a different model. Please try '\n-                     f'creating a new optimizer instance.')\n-\n     lr = tf.cast(self.learning_rate, variable.dtype)\n     m = None\n     var_key = self._var_key(variable)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3345ee876446b2e750e7a7f554e9b4baa38ed37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 277 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -134,12 +134,13 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n \n   def testClusterCoordinatorSingleInstanceWithJitCompileTrue(\n       self, strategy, use_dataset_creator):\n-    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n-                                \"Graph execution error\"):\n     model = self._model_fit(strategy,\n                             use_dataset_creator=use_dataset_creator,\n                             jit_compile=True)\n     strategy = model.distribute_strategy\n+    self.assertIs(\n+        strategy._cluster_coordinator,\n+        tf.distribute.experimental.coordinator.ClusterCoordinator(strategy))\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0594547466f8731dccc85f2de37976dab36d6a88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 42 | Churn Cumulative: 4322 | Contributors (this commit): 35 | Commits (past 90d): 3 | Contributors (cumulative): 40 | DMM Complexity: 0.8076923076923077\n\nDIFF:\n@@ -290,6 +290,15 @@ def get_file(fname=None,\n         os.remove(fpath)\n       raise\n \n+    # Validate download if succeeded and user provided an expected hash\n+    # Security conscious users would get the hash of the file from a separate\n+    # channel and pass it to this API to prevent MITM / corruption:\n+    if os.path.exists(fpath) and file_hash is not None:\n+      if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n+        raise ValueError(\n+            f'Incomplete or corrupted file detected. The {hash_algorithm} '\n+            f'file hash does not match the provided value of {file_hash}.')\n+\n   if untar:\n     if not os.path.exists(untar_fpath):\n       _extract_archive(fpath, datadir, archive_format='tar')\n\n@@ -126,6 +126,39 @@ class TestGetFile(tf.test.TestCase):\n     self.assertEndsWith(path, '.txt')\n     self.assertTrue(os.path.exists(path))\n \n+  def test_get_file_with_integrity_check(self):\n+    \"\"\"Tests get_file with validation before download.\"\"\"\n+    orig_dir = self.get_temp_dir()\n+    file_path = os.path.join(orig_dir, 'test.txt')\n+\n+    with open(file_path, 'w') as text_file:\n+      text_file.write('Float like a butterfly, sting like a bee.')\n+\n+    hashval = keras.utils.data_utils._hash_file(file_path)\n+\n+    origin = urllib.parse.urljoin(\n+        'file://', urllib.request.pathname2url(os.path.abspath(file_path)))\n+\n+    path = keras.utils.data_utils.get_file(\n+        'test.txt', origin, file_hash=hashval)\n+    self.assertTrue(os.path.exists(path))\n+\n+  def test_get_file_with_failed_integrity_check(self):\n+    \"\"\"Tests get_file with validation before download.\"\"\"\n+    orig_dir = self.get_temp_dir()\n+    file_path = os.path.join(orig_dir, 'test.txt')\n+\n+    with open(file_path, 'w') as text_file:\n+      text_file.write('Float like a butterfly, sting like a bee.')\n+\n+    hashval = '0' * 64\n+\n+    origin = urllib.parse.urljoin(\n+        'file://', urllib.request.pathname2url(os.path.abspath(file_path)))\n+\n+    with self.assertRaisesRegex(ValueError, 'Incomplete or corrupted file.*'):\n+      _ = keras.utils.data_utils.get_file('test.txt', origin, file_hash=hashval)\n+\n \n class TestSequence(keras.utils.data_utils.Sequence):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa8af10a160845df30a8b28470e866e02c68bdf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 8002 | Contributors (this commit): 14 | Commits (past 90d): 11 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -3389,8 +3389,9 @@ def resize_images(x, height_factor, width_factor, data_format,\n       height_factor: Positive integer.\n       width_factor: Positive integer.\n       data_format: One of `\"channels_first\"`, `\"channels_last\"`.\n-      interpolation: A string, one of `area`, `bicubic`, `bilinear`,\n-        `gaussian`, `lanczos3`, `lanczos5`, `mitchellcubic`, `nearest`.\n+      interpolation: A string, one of `\"area\"`, `\"bicubic\"`, `\"bilinear\"`,\n+        `\"gaussian\"`, `\"lanczos3\"`, `\"lanczos5\"`, `\"mitchellcubic\"`,\n+        `\"nearest\"`.\n \n   Returns:\n       A tensor.\n@@ -3426,7 +3427,7 @@ def resize_images(x, height_factor, width_factor, data_format,\n     'mitchellcubic': tf.image.ResizeMethod.MITCHELLCUBIC,\n     'nearest': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n   }\n-  interploations_list = '`' + '`, `'.join(interpolations.keys()) + '`'\n+  interploations_list = '\"' + '\", \"'.join(interpolations.keys()) + '\"'\n   if interpolation in interpolations:\n     x = tf.image.resize(x, new_shape, method=interpolations[interpolation])\n   else:\n\n@@ -65,8 +65,8 @@ class UpSampling2D(Layer):\n       It defaults to the `image_data_format` value found in your\n       Keras config file at `~/.keras/keras.json`.\n       If you never set it, then it will be \"channels_last\".\n-    interpolation: A string, one of `area`, `bicubic`, `bilinear`, `gaussian`,\n-      `lanczos3`, `lanczos5`, `mitchellcubic`, `nearest`.\n+    interpolation: A string, one of `\"area\"`, `\"bicubic\"`, `\"bilinear\"`,\n+      `\"gaussian\"`, `\"lanczos3\"`, `\"lanczos5\"`, `\"mitchellcubic\"`, `\"nearest\"`.\n \n   Input shape:\n     4D tensor with shape:\n@@ -101,7 +101,7 @@ class UpSampling2D(Layer):\n       'mitchellcubic': tf.image.ResizeMethod.MITCHELLCUBIC,\n       'nearest': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n     }\n-    interploations_list = '`' + '`, `'.join(interpolations.keys()) + '`'\n+    interploations_list = '\"' + '\", \"'.join(interpolations.keys()) + '\"'\n     if interpolation not in interpolations:\n       raise ValueError('`interpolation` argument should be one of: '\n                        f'{interploations_list}. Received: \"{interpolation}\".')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#318aeb1178122215bd94f0bdcaf6458c33cfac7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 17557 | Contributors (this commit): 125 | Commits (past 90d): 34 | Contributors (cumulative): 131 | DMM Complexity: None\n\nDIFF:\n@@ -63,7 +63,7 @@ class CustomMnistBenchmark(tf.test.Benchmark):\n     return tf.nn.compute_average_loss(\n         per_example_loss, global_batch_size=batch_size)\n \n-  @tf.function(experimental_relax_shapes=True)\n+  @tf.function(reduce_retracing=True)\n   def train_step(self, inputs, model, loss_fn, optimizer, batch_size):\n     \"\"\"Compute loss and optimize model by optimizer.\n \n@@ -85,7 +85,7 @@ class CustomMnistBenchmark(tf.test.Benchmark):\n     optimizer.apply_gradients(zip(grads, model.trainable_weights))\n     return loss\n \n-  @tf.function(experimental_relax_shapes=True)\n+  @tf.function(reduce_retracing=True)\n   def distributed_train_step(self, batch_dataset, model, loss_fn, optimizer,\n                              batch_size, distribution_strategy):\n     \"\"\"Train step in distribution strategy setting.\n\n@@ -1035,7 +1035,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if self._jit_compile:\n         run_step = tf.function(\n-            run_step, jit_compile=True, experimental_relax_shapes=True)\n+            run_step, jit_compile=True, reduce_retracing=True)\n       data = next(iterator)\n       outputs = model.distribute_strategy.run(run_step, args=(data,))\n       outputs = reduce_per_replica(\n@@ -1052,7 +1052,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if not self.run_eagerly:\n         train_function = tf.function(\n-            train_function, experimental_relax_shapes=True)\n+            train_function, reduce_retracing=True)\n         self.train_tf_function = train_function\n \n       if self._cluster_coordinator:\n@@ -1074,7 +1074,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if not self.run_eagerly:\n         train_function = tf.function(\n-            train_function, experimental_relax_shapes=True)\n+            train_function, reduce_retracing=True)\n         self.train_tf_function = train_function\n \n       self.train_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n@@ -1090,7 +1090,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if not self.run_eagerly:\n         train_function = tf.function(\n-            train_function, experimental_relax_shapes=True)\n+            train_function, reduce_retracing=True)\n         self.train_tf_function = train_function\n       self.train_function = train_function\n \n@@ -1540,7 +1540,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if self._jit_compile:\n         run_step = tf.function(\n-            run_step, jit_compile=True, experimental_relax_shapes=True)\n+            run_step, jit_compile=True, reduce_retracing=True)\n \n       data = next(iterator)\n       outputs = model.distribute_strategy.run(run_step, args=(data,))\n@@ -1558,7 +1558,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if not self.run_eagerly:\n         test_function = tf.function(\n-            test_function, experimental_relax_shapes=True)\n+            test_function, reduce_retracing=True)\n \n       if self._cluster_coordinator:\n         self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n@@ -1579,7 +1579,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if not self.run_eagerly:\n         test_function = tf.function(\n-            test_function, experimental_relax_shapes=True)\n+            test_function, reduce_retracing=True)\n \n       self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n           test_function,\n@@ -1594,7 +1594,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if not self.run_eagerly:\n         test_function = tf.function(\n-            test_function, experimental_relax_shapes=True)\n+            test_function, reduce_retracing=True)\n       self.test_function = test_function\n \n     return self.test_function\n@@ -1828,7 +1828,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n       if self._jit_compile:\n         run_step = tf.function(\n-            run_step, jit_compile=True, experimental_relax_shapes=True)\n+            run_step, jit_compile=True, reduce_retracing=True)\n \n       data = next(iterator)\n       outputs = model.distribute_strategy.run(run_step, args=(data,))\n@@ -1861,7 +1861,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     if not self.run_eagerly:\n       predict_function = tf.function(\n-          predict_function, experimental_relax_shapes=True)\n+          predict_function, reduce_retracing=True)\n     self.predict_function = predict_function\n \n     return self.predict_function\n\n@@ -63,7 +63,7 @@ class FunctionTest(tf.test.TestCase):\n \n   def testFunctionRelaxationLosesInnerDimWithKerasLayer(self):\n     layer = tf.keras.layers.Dense(1)\n-    fn = tf.function(experimental_relax_shapes=True)(layer)\n+    fn = tf.function(reduce_retracing=True)(layer)\n \n     with self.captureWritesToStream(sys.stderr) as printed:\n       fn(tf.ones((3, 2)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
