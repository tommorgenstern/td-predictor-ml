{"custom_id": "keras#ccc0aaac3e90ae2d1353e0d95e2f694a5fe776d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 36 | Churn Cumulative: 2686 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,12 +17,12 @@ class AdditiveAttentionTest(testing.TestCase):\n             expected_output_shape=(2, 3, 4),\n             expected_num_trainable_weights=1,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n             run_training_check=False,\n         )\n-        # Sale.\n+        # Scale.\n         self.run_layer_test(\n             layers.AdditiveAttention,\n             init_kwargs={\n@@ -33,7 +33,7 @@ class AdditiveAttentionTest(testing.TestCase):\n             expected_output_shape=(2, 3, 4),\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n             run_training_check=False,\n\n@@ -27,6 +27,7 @@ class Attention(Layer):\n             attention scores.\n         dropout: Float between 0 and 1. Fraction of the units to drop for the\n             attention scores. Defaults to `0.0`.\n+        seed: A Python integer to use as random seed incase of `dropout`.\n         score_mode: Function to use to compute attention scores, one of\n             `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the\n             query and key vectors. `\"concat\"` refers to the hyperbolic tangent\n@@ -66,12 +67,16 @@ class Attention(Layer):\n         use_scale=False,\n         score_mode=\"dot\",\n         dropout=0.0,\n+        seed=None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self.use_scale = use_scale\n         self.score_mode = score_mode\n         self.dropout = dropout\n+        if self.dropout > 0:\n+            self.seed_generator = backend.random.SeedGenerator(seed=seed)\n+\n         if self.score_mode not in [\"dot\", \"concat\"]:\n             raise ValueError(\n                 \"Invalid value for argument score_mode. \"\n@@ -174,8 +179,8 @@ class Attention(Layer):\n             weights = backend.random.dropout(\n                 weights,\n                 self.dropout,\n-                noise_shape=self.noise_shape,\n-                seed=self.seed_generator,\n+                noise_shape=None,\n+                seed=None,\n             )\n         return ops.matmul(weights, value), weights\n \n\n@@ -17,12 +17,12 @@ class AttentionTest(testing.TestCase):\n             expected_output_shape=(2, 3, 4),\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n             run_training_check=False,\n         )\n-        # Sale and concat.\n+        # Scale and concat.\n         self.run_layer_test(\n             layers.Attention,\n             init_kwargs={\n@@ -34,7 +34,7 @@ class AttentionTest(testing.TestCase):\n             expected_output_shape=(2, 3, 4),\n             expected_num_trainable_weights=2,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n             run_training_check=False,\n@@ -99,3 +99,18 @@ class AttentionTest(testing.TestCase):\n \n         with self.assertRaisesRegex(ValueError, \"length 2 or 3\"):\n             layer([tensor, tensor], mask=[tensor])\n+\n+    def test_attention_with_dropout(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        value = np.array([[[1.0, 1.0], [1.0, 1.0]]])\n+        layer_with_dropout = layers.Attention(dropout=0.2)\n+        layer_without_dropout = layers.Attention()\n+\n+        output1, scores1 = layer_with_dropout(\n+            [query, value], return_attention_scores=True, training=True\n+        )\n+        output2, scores2 = layer_without_dropout(\n+            [query, value], return_attention_scores=True, training=True\n+        )\n+        self.assertNotAllClose(output1, output2)\n+        self.assertNotAllClose(scores1, scores2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#17aa60842ea8364636482bf3d36628d36393b70b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1324 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,7 +22,7 @@ class AdditiveAttentionTest(testing.TestCase):\n             supports_masking=True,\n             run_training_check=False,\n         )\n-        # Scale.\n+        # With scale.\n         self.run_layer_test(\n             layers.AdditiveAttention,\n             init_kwargs={\n\n@@ -179,8 +179,7 @@ class Attention(Layer):\n             weights = backend.random.dropout(\n                 weights,\n                 self.dropout,\n-                noise_shape=None,\n-                seed=None,\n+                seed=self.seed_generator,\n             )\n         return ops.matmul(weights, value), weights\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#29fd4eaae2dbe3e0af09627376ae1513fb65eb45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 1140 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -233,6 +233,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n         (y_true_rank is not None)\n         and (y_pred_rank is not None)\n         and (len(y_true.shape) == len(y_pred.shape))\n+        and ops.shape(y_true)[-1] == 1\n     ):\n         y_true = ops.squeeze(y_true, -1)\n         reshape_matches = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72cd8be5c2ccc8b3e357ca3a0d8294b287ccd967", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 152 | Lines Deleted: 18 | Files Changed: 12 | Hunks: 49 | Methods Changed: 34 | Complexity Δ (Sum/Max): 14/6 | Churn Δ: 170 | Churn Cumulative: 9637 | Contributors (this commit): 11 | Commits (past 90d): 32 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import warnings\n from keras import ops\n from keras.api_export import keras_export\n from keras.callbacks.callback import Callback\n+from keras.trainers import compile_utils\n from keras.utils import io_utils\n \n \n@@ -95,27 +96,54 @@ class EarlyStopping(Callback):\n                 stacklevel=2,\n             )\n             mode = \"auto\"\n+        self.mode = mode\n \n-        if mode == \"min\":\n+    def _set_monitor_op(self):\n+        if self.mode == \"min\":\n             self.monitor_op = ops.less\n-        elif mode == \"max\":\n+            return\n+        elif self.mode == \"max\":\n             self.monitor_op = ops.greater\n+            return\n         else:\n-            if (\n-                self.monitor.endswith(\"acc\")\n-                or self.monitor.endswith(\"accuracy\")\n-                or self.monitor.endswith(\"auc\")\n+            metric_name = self.monitor.removeprefix(\"val_\")\n+            if metric_name == \"loss\":\n+                self.monitor_op = ops.less\n+                return\n+            if hasattr(self.model, \"metrics\"):\n+                all_metrics = []\n+                for m in self.model.metrics:\n+                    if isinstance(\n+                        m,\n+                        (\n+                            compile_utils.CompileMetrics,\n+                            compile_utils.MetricsList,\n+                        ),\n                     ):\n+                        all_metrics.extend(m.metrics)\n+                for m in all_metrics:\n+                    if m.name == metric_name:\n+                        if hasattr(m, \"_direction\"):\n+                            if m._direction == \"up\":\n                                 self.monitor_op = ops.greater\n+                                return\n                             else:\n                                 self.monitor_op = ops.less\n-\n-        if self.monitor_op == ops.greater:\n-            self.min_delta *= 1\n-        else:\n-            self.min_delta *= -1\n+                                return\n+        raise ValueError(\n+            f\"EarlyStopping callback received monitor={self.monitor} \"\n+            \"but Keras isn't able to automatically determine whether \"\n+            \"that metric should be maximized or minimized. \"\n+            \"Pass `mode='max'` in order to do early stopping based \"\n+            \"on the highest metric value, or pass `mode='min'` \"\n+            \"in order to use the lowest value.\"\n+        )\n \n     def on_train_begin(self, logs=None):\n+        self._set_monitor_op()\n+        if self.monitor_op == ops.less:\n+            self.min_delta *= -1\n+\n         # Allow instances to be re-used\n         self.wait = 0\n         self.stopped_epoch = 0\n\n@@ -3,6 +3,7 @@ import pytest\n \n from keras import callbacks\n from keras import layers\n+from keras import metrics\n from keras import models\n from keras import testing\n \n@@ -23,7 +24,13 @@ class EarlyStoppingTest(testing.TestCase):\n         model.compile(\n             loss=\"mae\",\n             optimizer=\"adam\",\n-            metrics=[\"mse\"],\n+            metrics=[\n+                \"mse\",\n+                \"acc\",\n+                \"accuracy\",\n+                \"hinge\",\n+                metrics.F1Score(name=\"f1_score\"),\n+            ],\n         )\n \n         cases = [\n@@ -31,7 +38,10 @@ class EarlyStoppingTest(testing.TestCase):\n             (\"min\", \"val_loss\"),\n             (\"auto\", \"val_mse\"),\n             (\"auto\", \"loss\"),\n-            (\"unknown\", \"unknown\"),\n+            (\"auto\", \"acc\"),\n+            (\"auto\", \"val_accuracy\"),\n+            (\"auto\", \"hinge\"),\n+            (\"auto\", \"f1_score\"),\n         ]\n         for mode, monitor in cases:\n             patience = 0\n@@ -46,7 +56,21 @@ class EarlyStoppingTest(testing.TestCase):\n                 batch_size=5,\n                 validation_data=(x_test, y_test),\n                 callbacks=cbks,\n-                epochs=5,\n+                epochs=2,\n+                verbose=0,\n+            )\n+\n+        with self.assertRaises(ValueError):\n+            cbks = [\n+                callbacks.EarlyStopping(patience=patience, monitor=\"unknown\")\n+            ]\n+            model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=5,\n+                validation_data=(x_test, y_test),\n+                callbacks=cbks,\n+                epochs=2,\n                 verbose=0,\n             )\n \n\n@@ -55,6 +55,8 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"accuracy\", dtype=None):\n         super().__init__(fn=accuracy, name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -124,6 +126,8 @@ class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n             fn=binary_accuracy, name=name, dtype=dtype, threshold=threshold\n         )\n         self.threshold = threshold\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\n@@ -214,6 +218,8 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"categorical_accuracy\", dtype=None):\n         super().__init__(fn=categorical_accuracy, name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -299,6 +305,8 @@ class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"sparse_categorical_accuracy\", dtype=None):\n         super().__init__(fn=sparse_categorical_accuracy, name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -376,6 +384,8 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n             k=k,\n         )\n         self.k = k\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype, \"k\": self.k}\n@@ -452,6 +462,8 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n             k=k,\n         )\n         self.k = k\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype, \"k\": self.k}\n\n@@ -338,6 +338,9 @@ class Precision(Metric):\n         self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n     ):\n         super().__init__(name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n+\n         self.init_thresholds = thresholds\n         self.top_k = top_k\n         self.class_id = class_id\n@@ -479,6 +482,9 @@ class Recall(Metric):\n         self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n     ):\n         super().__init__(name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n+\n         self.init_thresholds = thresholds\n         self.top_k = top_k\n         self.class_id = class_id\n@@ -560,6 +566,9 @@ class SensitivitySpecificityBase(Metric):\n         self, value, num_thresholds=200, class_id=None, name=None, dtype=None\n     ):\n         super().__init__(name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n+\n         if num_thresholds <= 0:\n             raise ValueError(\n                 \"Argument `num_thresholds` must be an integer > 0. \"\n@@ -1192,6 +1201,9 @@ class AUC(Metric):\n         label_weights=None,\n         from_logits=False,\n     ):\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n+\n         # Validate configurations.\n         if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n             metrics_utils.AUCCurve\n\n@@ -72,6 +72,8 @@ class FBetaScore(Metric):\n         dtype=None,\n     ):\n         super().__init__(name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n         if average not in (None, \"micro\", \"macro\", \"weighted\"):\n             raise ValueError(\n\n@@ -31,6 +31,8 @@ class Hinge(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"hinge\", dtype=None):\n         super().__init__(fn=hinge, name=name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -62,6 +64,8 @@ class SquaredHinge(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"squared_hinge\", dtype=None):\n         super().__init__(fn=squared_hinge, name=name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -89,6 +93,8 @@ class CategoricalHinge(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"categorical_hinge\", dtype=None):\n         super().__init__(fn=categorical_hinge, name=name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n\n@@ -57,6 +57,8 @@ class _IoUBase(Metric):\n     ):\n         # defaulting to float32 to avoid issues with confusion matrix\n         super().__init__(name=name, dtype=dtype or \"float32\")\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n         self.num_classes = num_classes\n         self.ignore_class = ignore_class\n         self.sparse_y_true = sparse_y_true\n\n@@ -156,6 +156,8 @@ class BinaryCrossentropy(reduction_metrics.MeanMetricWrapper):\n         )\n         self.from_logits = from_logits\n         self.label_smoothing = label_smoothing\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\n@@ -242,6 +244,8 @@ class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n         self.from_logits = from_logits\n         self.label_smoothing = label_smoothing\n         self.axis = axis\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\n@@ -327,6 +331,8 @@ class SparseCategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n         )\n         self.from_logits = from_logits\n         self.axis = axis\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\n\n@@ -1,8 +1,8 @@\n from keras import backend\n from keras import initializers\n+from keras import losses\n from keras import ops\n from keras.api_export import keras_export\n-from keras.losses import loss\n from keras.metrics.metric import Metric\n from keras.saving import serialization_lib\n \n@@ -13,11 +13,13 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n     if sample_weight is not None:\n         sample_weight = ops.cast(sample_weight, dtype=dtype)\n         if mask is not None:\n-            sample_weight = loss.apply_mask(\n+            sample_weight = losses.loss.apply_mask(\n                 sample_weight, mask, dtype=dtype, reduction=\"sum\"\n             )\n         # Update dimensions of weights to match with values if possible.\n-        values, sample_weight = loss.squeeze_to_same_rank(values, sample_weight)\n+        values, sample_weight = losses.loss.squeeze_to_same_rank(\n+            values, sample_weight\n+        )\n         # Reduce values to same ndim as weight array\n         weight_ndim = len(sample_weight.shape)\n         values_ndim = len(values.shape)\n@@ -186,11 +188,20 @@ class MeanMetricWrapper(Mean):\n         self._fn = fn\n         self._fn_kwargs = kwargs\n \n+        # If we are wrapping a Keras loss, register the metric's\n+        # direction as \"down\" (needs to be minimized during training).\n+        if (\n+            self._fn in losses.ALL_OBJECTS\n+            or hasattr(self._fn, \"__class__\")\n+            and self._fn.__class__ in losses.ALL_OBJECTS\n+        ):\n+            self._direction = \"down\"\n+\n     def update_state(self, y_true, y_pred, sample_weight=None):\n         mask = getattr(y_pred, \"_keras_mask\", None)\n         values = self._fn(y_true, y_pred, **self._fn_kwargs)\n         if sample_weight is not None and mask is not None:\n-            sample_weight = loss.apply_mask(\n+            sample_weight = losses.loss.apply_mask(\n                 sample_weight, mask, dtype=self.dtype, reduction=\"sum\"\n             )\n         return super().update_state(values, sample_weight=sample_weight)\n\n@@ -37,6 +37,8 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"mean_squared_error\", dtype=None):\n         super().__init__(fn=mean_squared_error, name=name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -82,6 +84,8 @@ class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"mean_absolute_error\", dtype=None):\n         super().__init__(mean_absolute_error, name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -127,6 +131,8 @@ class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"mean_absolute_percentage_error\", dtype=None):\n         super().__init__(mean_absolute_percentage_error, name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -172,6 +178,8 @@ class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"mean_squared_logarithmic_error\", dtype=None):\n         super().__init__(mean_squared_logarithmic_error, name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -218,6 +226,8 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n \n     def __init__(self, name=\"root_mean_squared_error\", dtype=None):\n         super().__init__(name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def update_state(self, y_true, y_pred, sample_weight=None):\n         \"\"\"Accumulates root mean squared error statistics.\n@@ -293,6 +303,8 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"cosine_similarity\", dtype=None, axis=-1):\n         super().__init__(cosine_similarity, name, dtype=dtype, axis=axis)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -338,6 +350,8 @@ class LogCoshError(reduction_metrics.MeanMetricWrapper):\n \n     def __init__(self, name=\"logcosh\", dtype=None):\n         super().__init__(log_cosh, name, dtype=dtype)\n+        # Metric should be minimized during optimization.\n+        self._direction = \"down\"\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n@@ -404,6 +418,8 @@ class R2Score(reduction_metrics.Metric):\n         dtype=None,\n     ):\n         super().__init__(name=name, dtype=dtype)\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n \n         valid_class_aggregation_values = (\n             None,\n\n@@ -146,6 +146,18 @@ class CompileMetrics(metrics_module.Metric):\n         self.name = \"compile_metrics\"\n         self.output_names = output_names\n \n+    @property\n+    def metrics(self):\n+        if not self.built:\n+            return []\n+        metrics = []\n+        for m in self._flat_metrics + self._flat_weighted_metrics:\n+            if isinstance(m, MetricsList):\n+                metrics.extend(m.metrics)\n+            elif m is not None:\n+                metrics.append(m)\n+        return metrics\n+\n     @property\n     def variables(self):\n         # Avoiding relying on implicit tracking since\n\n@@ -75,6 +75,8 @@ class TestCompileMetrics(testing.TestCase):\n         y_true = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n         y_pred = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n         compile_metrics.build(y_true, y_pred)\n+        self.assertEqual(len(compile_metrics.metrics), 8)\n+\n         # Test eager build\n         y_true = [\n             np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n@@ -86,6 +88,7 @@ class TestCompileMetrics(testing.TestCase):\n         ]\n         sample_weight = np.array([1, 0.0, 1])\n         compile_metrics.build(y_true, y_pred)\n+        self.assertEqual(len(compile_metrics.metrics), 8)\n \n         # Test update / result / reset flow\n         compile_metrics.update_state(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1f8bee5bd945d085e13ab128c7496d5ba8d9e0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 14 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 47 | Churn Cumulative: 151 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -97,19 +97,17 @@ class EarlyStopping(Callback):\n             )\n             mode = \"auto\"\n         self.mode = mode\n+        self.monitor_op = None\n \n     def _set_monitor_op(self):\n         if self.mode == \"min\":\n             self.monitor_op = ops.less\n-            return\n         elif self.mode == \"max\":\n             self.monitor_op = ops.greater\n-            return\n         else:\n             metric_name = self.monitor.removeprefix(\"val_\")\n             if metric_name == \"loss\":\n                 self.monitor_op = ops.less\n-                return\n             if hasattr(self.model, \"metrics\"):\n                 all_metrics = []\n                 for m in self.model.metrics:\n@@ -126,10 +124,9 @@ class EarlyStopping(Callback):\n                         if hasattr(m, \"_direction\"):\n                             if m._direction == \"up\":\n                                 self.monitor_op = ops.greater\n-                                return\n                             else:\n                                 self.monitor_op = ops.less\n-                                return\n+        if self.monitor_op is None:\n             raise ValueError(\n                 f\"EarlyStopping callback received monitor={self.monitor} \"\n                 \"but Keras isn't able to automatically determine whether \"\n@@ -138,22 +135,24 @@ class EarlyStopping(Callback):\n                 \"on the highest metric value, or pass `mode='min'` \"\n                 \"in order to use the lowest value.\"\n             )\n-\n-    def on_train_begin(self, logs=None):\n-        self._set_monitor_op()\n         if self.monitor_op == ops.less:\n             self.min_delta *= -1\n-\n-        # Allow instances to be re-used\n-        self.wait = 0\n-        self.stopped_epoch = 0\n         self.best = (\n             float(\"inf\") if self.monitor_op == ops.less else -float(\"inf\")\n         )\n+\n+    def on_train_begin(self, logs=None):\n+        # Allow instances to be re-used\n+        self.wait = 0\n+        self.stopped_epoch = 0\n         self.best_weights = None\n         self.best_epoch = 0\n \n     def on_epoch_end(self, epoch, logs=None):\n+        if self.monitor_op is None:\n+            # Delay setup until the model's metrics are all built\n+            self._set_monitor_op()\n+\n         current = self.get_monitor_value(logs)\n         if current is None or epoch < self.start_from_epoch:\n             # If no monitor value exists or still in initial warm-up stage.\n\n@@ -5,6 +5,7 @@ from keras import callbacks\n from keras import layers\n from keras import metrics\n from keras import models\n+from keras import ops\n from keras import testing\n \n \n@@ -34,16 +35,16 @@ class EarlyStoppingTest(testing.TestCase):\n         )\n \n         cases = [\n-            (\"max\", \"val_mse\"),\n-            (\"min\", \"val_loss\"),\n-            (\"auto\", \"val_mse\"),\n-            (\"auto\", \"loss\"),\n-            (\"auto\", \"acc\"),\n-            (\"auto\", \"val_accuracy\"),\n-            (\"auto\", \"hinge\"),\n-            (\"auto\", \"f1_score\"),\n+            (\"max\", \"val_mse\", \"max\"),\n+            (\"min\", \"val_loss\", \"min\"),\n+            (\"auto\", \"val_mse\", \"min\"),\n+            (\"auto\", \"loss\", \"min\"),\n+            (\"auto\", \"acc\", \"max\"),\n+            (\"auto\", \"val_accuracy\", \"max\"),\n+            (\"auto\", \"hinge\", \"min\"),\n+            (\"auto\", \"f1_score\", \"max\"),\n         ]\n-        for mode, monitor in cases:\n+        for mode, monitor, expected_mode in cases:\n             patience = 0\n             cbks = [\n                 callbacks.EarlyStopping(\n@@ -59,6 +60,11 @@ class EarlyStoppingTest(testing.TestCase):\n                 epochs=2,\n                 verbose=0,\n             )\n+            if expected_mode == \"max\":\n+                monitor_op = ops.greater\n+            else:\n+                monitor_op = ops.less\n+            self.assertEqual(cbks[0].monitor_op, monitor_op)\n \n         with self.assertRaises(ValueError):\n             cbks = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c32d8a85e4da42ed53a264fb434245e6e585a5d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 144 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -186,7 +186,7 @@ class TorchTrainer(base_trainer.Trainer):\n             # Build all model state with `backend.compute_output_spec`.\n             try:\n                 y_pred = backend.compute_output_spec(self, x)\n-            except:\n+            except Exception as e:\n                 raise RuntimeError(\n                     \"Unable to automatically build the model. \"\n                     \"Please build it yourself before calling \"\n@@ -194,7 +194,9 @@ class TorchTrainer(base_trainer.Trainer):\n                     \"A model is 'built' when its variables have \"\n                     \"been created and its `self.built` attribute \"\n                     \"is True. Usually, calling the model on a batch \"\n-                    \"of data is the right way to build it.\"\n+                    \"of data is the right way to build it.\\n\"\n+                    \"Exception encountered:\\n\"\n+                    f\"'{e}'\"\n                 )\n             if compile_metrics_unbuilt:\n                 # Build all metric state with `backend.compute_output_spec`.\n\n@@ -60,6 +60,19 @@ class Function(Operation):\n         self._outputs_struct = tree.map_structure(lambda x: x, outputs)\n         self._inputs = tree.flatten(inputs)\n         self._outputs = tree.flatten(outputs)\n+        if not self._inputs:\n+            raise ValueError(\n+                \"`inputs` argument cannot be empty. Received:\\n\"\n+                f\"inputs={inputs}\\n\"\n+                f\"outputs={outputs}\"\n+            )\n+        if not self._outputs:\n+            raise ValueError(\n+                \"`outputs` argument cannot be empty. Received:\\n\"\n+                f\"inputs={inputs}\\n\"\n+                f\"outputs={outputs}\"\n+            )\n+\n         if backend() == \"tensorflow\":\n             self._self_setattr_tracking = _self_setattr_tracking\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#344dcdedba4c6135ee73b249481f8a4d6ad73ab0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 35 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,12 @@ class SumTest(testing.TestCase):\n         self.assertEqual(sum_obj.name, \"sum\")\n         self.assertEqual(len(sum_obj.variables), 1)\n         self.assertEqual(sum_obj._dtype, \"float32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        sum_obj2 = reduction_metrics.Sum.from_config(sum_obj.get_config())\n+        self.assertEqual(sum_obj2.name, \"sum\")\n+        self.assertEqual(len(sum_obj2.variables), 1)\n+        self.assertEqual(sum_obj2._dtype, \"float32\")\n \n     def test_unweighted(self):\n         sum_obj = reduction_metrics.Sum(name=\"sum\", dtype=\"float32\")\n@@ -38,7 +43,12 @@ class MeanTest(testing.TestCase):\n         self.assertEqual(mean_obj.name, \"mean\")\n         self.assertEqual(len(mean_obj.variables), 2)\n         self.assertEqual(mean_obj._dtype, \"float32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        mean_obj2 = reduction_metrics.Mean.from_config(mean_obj.get_config())\n+        self.assertEqual(mean_obj2.name, \"mean\")\n+        self.assertEqual(len(mean_obj2.variables), 2)\n+        self.assertEqual(mean_obj2._dtype, \"float32\")\n \n     def test_unweighted(self):\n         mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a7592e737e4d52d0a2c6f97cb4172d12facc46e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 32/30 | Churn Δ: 49 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -140,3 +140,11 @@ def removesuffix(x, suffix):\n         return x[: -len(suffix)]\n     else:\n         return x\n+\n+\n+def remove_by_id(lst, value):\n+    \"\"\"Remove a value from a list by id.\"\"\"\n+    for i, v in enumerate(lst):\n+        if id(v) == id(value):\n+            del lst[i]\n+            return\n\n@@ -2,6 +2,7 @@ from functools import wraps\n \n from keras.backend.common.global_state import get_global_attribute\n from keras.backend.common.global_state import set_global_attribute\n+from keras.utils import python_utils\n \n \n class DotNotTrackScope:\n@@ -91,8 +92,8 @@ class Tracker:\n     def untrack(self, value):\n         for store_name in self.stored_ids.keys():\n             if id(value) in self.stored_ids[store_name]:\n-                self.config[store_name][1].remove(value)\n                 self.stored_ids[store_name].remove(id(value))\n+                python_utils.remove_by_id(self.config[store_name][1], value)\n \n     def lock(self, msg):\n         self.locked = True\n@@ -131,7 +132,10 @@ class TrackedList(list):\n     def remove(self, value):\n         if self.tracker:\n             self.tracker.untrack(value)\n+        try:\n             super().remove(value)\n+        except ValueError:\n+            python_utils.remove_by_id(self, value)\n \n \n class TrackedDict(dict):\n\n@@ -0,0 +1,35 @@\n+from keras import backend\n+from keras import testing\n+from keras.utils import tracking\n+\n+\n+class TrackingTest(testing.TestCase):\n+    def test_untracking_in_tracked_list(self):\n+        tracked_variables = []\n+        tracker = tracking.Tracker(\n+            {\n+                \"variables\": (\n+                    lambda x: isinstance(x, backend.Variable),\n+                    tracked_variables,\n+                ),\n+            }\n+        )\n+        v1 = backend.Variable(1)\n+        v2 = backend.Variable(2)\n+        lst = tracking.TrackedList([], tracker)\n+        lst.append(v1)\n+        lst.append(None)\n+        lst.append(v2)\n+        lst.append(0)\n+\n+        self.assertLen(tracked_variables, 2)\n+        self.assertEqual(tracked_variables[0], v1)\n+        self.assertEqual(tracked_variables[1], v2)\n+\n+        lst.remove(v1)\n+        self.assertLen(lst, 3)\n+        self.assertLen(tracked_variables, 1)\n+\n+        lst.remove(v2)\n+        self.assertLen(lst, 2)\n+        self.assertLen(tracked_variables, 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b49dcb7e2aec5eec3a65917d7f70ab40ac8c122", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 87 | Churn Cumulative: 1340 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,12 @@ class AccuracyTest(testing.TestCase):\n         acc_obj_config = acc_obj.get_config()\n         self.assertEqual(acc_obj_config[\"name\"], \"accuracy\")\n         self.assertEqual(acc_obj_config[\"dtype\"], \"float32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        acc_obj2 = accuracy_metrics.Accuracy.from_config(acc_obj_config)\n+        self.assertEqual(acc_obj2.name, \"accuracy\")\n+        self.assertEqual(len(acc_obj2.variables), 2)\n+        self.assertEqual(acc_obj2._dtype, \"float32\")\n \n     def test_unweighted(self):\n         acc_obj = accuracy_metrics.Accuracy(name=\"accuracy\", dtype=\"float32\")\n@@ -50,7 +55,14 @@ class BinaryAccuracyTest(testing.TestCase):\n         bin_acc_obj_config = bin_acc_obj.get_config()\n         self.assertEqual(bin_acc_obj_config[\"name\"], \"binary_accuracy\")\n         self.assertEqual(bin_acc_obj_config[\"dtype\"], \"float32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        bin_acc_obj2 = accuracy_metrics.BinaryAccuracy.from_config(\n+            bin_acc_obj_config\n+        )\n+        self.assertEqual(bin_acc_obj2.name, \"binary_accuracy\")\n+        self.assertEqual(len(bin_acc_obj2.variables), 2)\n+        self.assertEqual(bin_acc_obj2._dtype, \"float32\")\n \n     def test_unweighted(self):\n         bin_acc_obj = accuracy_metrics.BinaryAccuracy(\n@@ -128,7 +140,14 @@ class CategoricalAccuracyTest(testing.TestCase):\n         cat_acc_obj_config = cat_acc_obj.get_config()\n         self.assertEqual(cat_acc_obj_config[\"name\"], \"categorical_accuracy\")\n         self.assertEqual(cat_acc_obj_config[\"dtype\"], \"float32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        cat_acc_obj2 = accuracy_metrics.CategoricalAccuracy.from_config(\n+            cat_acc_obj_config\n+        )\n+        self.assertEqual(cat_acc_obj2.name, \"categorical_accuracy\")\n+        self.assertEqual(len(cat_acc_obj2.variables), 2)\n+        self.assertEqual(cat_acc_obj2._dtype, \"float32\")\n \n     def test_unweighted(self):\n         cat_acc_obj = accuracy_metrics.CategoricalAccuracy(\n@@ -167,7 +186,16 @@ class SparseCategoricalAccuracyTest(testing.TestCase):\n             sp_cat_acc_obj_config[\"name\"], \"sparse_categorical_accuracy\"\n         )\n         self.assertEqual(sp_cat_acc_obj_config[\"dtype\"], \"float32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        sp_cat_acc_obj2 = (\n+            accuracy_metrics.SparseCategoricalAccuracy.from_config(\n+                sp_cat_acc_obj_config\n+            )\n+        )\n+        self.assertEqual(sp_cat_acc_obj2.name, \"sparse_categorical_accuracy\")\n+        self.assertEqual(len(sp_cat_acc_obj2.variables), 2)\n+        self.assertEqual(sp_cat_acc_obj2._dtype, \"float32\")\n \n     def test_unweighted(self):\n         sp_cat_acc_obj = accuracy_metrics.SparseCategoricalAccuracy(\n@@ -207,7 +235,17 @@ class TopKCategoricalAccuracyTest(testing.TestCase):\n         )\n         self.assertEqual(top_k_cat_acc_obj_config[\"dtype\"], \"float32\")\n         self.assertEqual(top_k_cat_acc_obj_config[\"k\"], 1)\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        top_k_cat_acc_obj2 = (\n+            accuracy_metrics.TopKCategoricalAccuracy.from_config(\n+                top_k_cat_acc_obj_config\n+            )\n+        )\n+        self.assertEqual(top_k_cat_acc_obj2.name, \"top_k_categorical_accuracy\")\n+        self.assertEqual(len(top_k_cat_acc_obj2.variables), 2)\n+        self.assertEqual(top_k_cat_acc_obj2._dtype, \"float32\")\n+        self.assertEqual(top_k_cat_acc_obj2.k, 1)\n \n     def test_unweighted(self):\n         top_k_cat_acc_obj = accuracy_metrics.TopKCategoricalAccuracy(\n@@ -252,7 +290,19 @@ class SparseTopKCategoricalAccuracyTest(testing.TestCase):\n         )\n         self.assertEqual(sp_top_k_cat_acc_obj_config[\"dtype\"], \"float32\")\n         self.assertEqual(sp_top_k_cat_acc_obj_config[\"k\"], 1)\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        sp_top_k_cat_acc_obj2 = (\n+            accuracy_metrics.SparseTopKCategoricalAccuracy.from_config(\n+                sp_top_k_cat_acc_obj_config\n+            )\n+        )\n+        self.assertEqual(\n+            sp_top_k_cat_acc_obj2.name, \"sparse_top_k_categorical_accuracy\"\n+        )\n+        self.assertEqual(len(sp_top_k_cat_acc_obj2.variables), 2)\n+        self.assertEqual(sp_top_k_cat_acc_obj2._dtype, \"float32\")\n+        self.assertEqual(sp_top_k_cat_acc_obj2.k, 1)\n \n     def test_unweighted(self):\n         sp_top_k_cat_acc_obj = accuracy_metrics.SparseTopKCategoricalAccuracy(\n\n@@ -9,7 +9,12 @@ class HingeTest(testing.TestCase):\n         hinge_obj = hinge_metrics.Hinge(name=\"hinge\", dtype=\"int32\")\n         self.assertEqual(hinge_obj.name, \"hinge\")\n         self.assertEqual(hinge_obj._dtype, \"int32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        hinge_obj2 = hinge_metrics.Hinge.from_config(hinge_obj.get_config())\n+        self.assertEqual(hinge_obj2.name, \"hinge\")\n+        self.assertEqual(len(hinge_obj2.variables), 2)\n+        self.assertEqual(hinge_obj2._dtype, \"int32\")\n \n     def test_unweighted(self):\n         hinge_obj = hinge_metrics.Hinge()\n@@ -35,7 +40,14 @@ class SquaredHingeTest(testing.TestCase):\n         )\n         self.assertEqual(sq_hinge_obj.name, \"squared_hinge\")\n         self.assertEqual(sq_hinge_obj._dtype, \"int32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        sq_hinge_obj2 = hinge_metrics.SquaredHinge.from_config(\n+            sq_hinge_obj.get_config()\n+        )\n+        self.assertEqual(sq_hinge_obj2.name, \"squared_hinge\")\n+        self.assertEqual(len(sq_hinge_obj2.variables), 2)\n+        self.assertEqual(sq_hinge_obj2._dtype, \"int32\")\n \n     def test_unweighted(self):\n         sq_hinge_obj = hinge_metrics.SquaredHinge()\n@@ -61,7 +73,14 @@ class CategoricalHingeTest(testing.TestCase):\n         )\n         self.assertEqual(cat_hinge_obj.name, \"cat_hinge\")\n         self.assertEqual(cat_hinge_obj._dtype, \"int32\")\n-        # TODO: Check save and restore config\n+\n+        # Check save and restore config\n+        cat_hinge_obj2 = hinge_metrics.CategoricalHinge.from_config(\n+            cat_hinge_obj.get_config()\n+        )\n+        self.assertEqual(cat_hinge_obj2.name, \"cat_hinge\")\n+        self.assertEqual(len(cat_hinge_obj2.variables), 2)\n+        self.assertEqual(cat_hinge_obj2._dtype, \"int32\")\n \n     def test_unweighted(self):\n         cat_hinge_obj = hinge_metrics.CategoricalHinge()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bd54e82c30344a67c5a014aed5d7d369b53a3fe4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n Title: Character-level recurrent sequence-to-sequence model\n Author: [fchollet](https://twitter.com/fchollet)\n Date created: 2017/09/29\n-Last modified: 2020/04/26\n+Last modified: 2023/11/22\n Description: Character-level recurrent sequence-to-sequence model.\n Accelerator: GPU\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af97cb697ded8f719ce22e9c1fb56e088e0f6ecf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 209 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n Title: Variational AutoEncoder\n Author: [fchollet](https://twitter.com/fchollet)\n Date created: 2020/05/03\n-Last modified: 2020/05/03\n+Last modified: 2023/11/22\n Description: Convolutional Variational AutoEncoder (VAE) trained on MNIST digits.\n Accelerator: GPU\n \"\"\"\n@@ -11,6 +11,9 @@ Accelerator: GPU\n ## Setup\n \"\"\"\n \n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import numpy as np\n import tensorflow as tf\n import keras\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dedab25de62b49b1deb2c823f19020f410acdc6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 211 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -156,7 +156,7 @@ def plot_latent_space(vae, n=30, figsize=15):\n     for i, yi in enumerate(grid_y):\n         for j, xi in enumerate(grid_x):\n             z_sample = np.array([[xi, yi]])\n-            x_decoded = vae.decoder.predict(z_sample)\n+            x_decoded = vae.decoder.predict(z_sample, verbose=0)\n             digit = x_decoded[0].reshape(digit_size, digit_size)\n             figure[\n                 i * digit_size : (i + 1) * digit_size,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bda639d7b16d788285772622db14e4f0b22699f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 718 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -25,6 +25,9 @@ using cycle-consistent adversarial networks.\n ## Setup\n \"\"\"\n \n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import numpy as np\n import matplotlib.pyplot as plt\n import keras\n@@ -637,7 +640,7 @@ cycle_gan_model.compile(\n # Callbacks\n plotter = GANMonitor()\n checkpoint_filepath = (\n-    \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n+    \"cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n )\n model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n     filepath=checkpoint_filepath, save_weights_only=True\n@@ -652,10 +655,7 @@ cycle_gan_model.fit(\n )\n \n \"\"\"\n-Test the performance of the model.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/CycleGAN)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/CycleGAN).\n+## Test the performance of the model.\n \"\"\"\n \n \n@@ -676,7 +676,7 @@ print(\"Weights loaded successfully\")\n \n _, ax = plt.subplots(4, 2, figsize=(10, 15))\n for i, img in enumerate(test_horses.take(4)):\n-    prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()\n+    prediction = cycle_gan_model.gen_G.predict(img)[0]\n     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
