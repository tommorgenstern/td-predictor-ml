{"custom_id": "scrapy#b9d3a2cb96221d3c14c15d1b18330a2778d88e7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1367 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -60,13 +60,15 @@ class AdaptorPipe(list):\n         Execute the adaptor pipeline for this attribute.\n         \"\"\"\n \n+        debug_padding = 0\n         values = [value]\n+\n         for adaptor in self:\n             next_round = []\n             for val in values:\n                 try:\n                     if self.debug:\n-                        print \"  %07s | input >\" % adaptor.name, repr(val)\n+                        print \"%sinput | %s <\" % (' ' * debug_padding, adaptor.name), repr(val)\n \n                     val = adaptor(val, kwargs or {})\n                     if isinstance(val, tuple):\n@@ -75,11 +77,13 @@ class AdaptorPipe(list):\n                         next_round.append(val)\n \n                     if self.debug:\n-                        print \"  %07s | output >\" % adaptor.name, repr(val)\n+                        print \"%soutput | %s >\" % (' ' * debug_padding, adaptor.name), repr(val)\n                 except Exception:\n                     print \"Error in '%s' adaptor. Traceback text:\" % adaptor.name\n                     print format_exc()\n                     return\n+\n+            debug_padding += 2\n             values = next_round\n \n         return tuple(values)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74661d54d0e806c79f71b5e2609ad5aad4dba0b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,6 +15,7 @@ class ResponseTypes(object):\n \n     CLASSES = {\n         'text/html': 'scrapy.http.HtmlResponse',\n+        'application/xml': 'scrapy.http.XmlResponse',\n         'text/xml': 'scrapy.http.XmlResponse',\n         'text': 'scrapy.http.TextResponse',\n     }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f63a661320805e80b0a078752e7a4f6218141418", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 229 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ class RedirectMiddleware(object):\n             status = exception.status\n             response = exception.response\n \n-            if status in ['302', '303']:\n+            if status in [302, 303]:\n                 redirected_url = urljoin(request.url, response.headers['location'][0])\n                 if not getattr(spider, \"no_redirect\", False):\n                     redirected = request.replace(url=redirected_url, method='GET', body=None)\n@@ -34,21 +34,21 @@ class RedirectMiddleware(object):\n                         redirected.dont_filter -= 1\n                     else:\n                         redirected.dont_filter = False\n-                    log.msg(\"Redirecting (%s) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n+                    log.msg(\"Redirecting (%d) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n                     return redirected\n-                log.msg(\"Ignored redirecting (%s) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n+                log.msg(\"Ignored redirecting (%d) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n                 return response\n \n-            if status in ['301', '307']:\n+            if status in [301, 307]:\n                 redirected_url = urljoin(request.url, response.headers['location'][0])\n                 if not getattr(spider, \"no_redirect\", False):\n                     redirected = request.replace(url=redirected_url)\n                     # This is needed to avoid redirection loops with requests that contain dont_filter = True\n                     # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n                     redirected.dont_filter = False\n-                    log.msg(\"Redirecting (%s) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n+                    log.msg(\"Redirecting (%d) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n                     return redirected\n-                log.msg(\"Ignored redirecting (%s) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n+                log.msg(\"Ignored redirecting (%d) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n                 return response\n \n     def process_response(self, request, response, spider):\n\n@@ -31,7 +31,7 @@ class HttpException(Exception):\n             from twisted.web import http\n             message = http.responses.get(int(status))\n \n-        self.status = status\n+        self.status = int(status)\n         self.message = message\n         self.response = response\n         Exception.__init__(self, status, message, response)\n\n@@ -17,7 +17,7 @@ class Response(object):\n     def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):\n         self.url = Url(url)\n         self.headers = Headers(headers or {})\n-        self.status = status\n+        self.status = int(status)\n         self.set_body(body)\n         self.cached = False\n         self.request = None\n@@ -71,7 +71,7 @@ class Response(object):\n         received (that's not exposed by Twisted).\n         \"\"\"\n \n-        s  = \"HTTP/1.1 %s %s\\r\\n\" % (self.status, RESPONSES[self.status])\n+        s  = \"HTTP/1.1 %d %s\\r\\n\" % (self.status, RESPONSES[self.status])\n         if self.headers:\n             s += self.headers.to_string() + \"\\r\\n\"\n         s += \"\\r\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91eff31f183799eaf218e39be5fce077c6b95331", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 212 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,6 +54,7 @@ class Command(ScrapyCommand):\n     def _genspider(self, name, site, template_file):\n         \"\"\" Generate spider \"\"\"\n         tvars = {\n+            'project_name': settings.get('BOT_NAME'),\n             'name': name,\n             'site': site,\n             'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])\n\n@@ -4,7 +4,7 @@ import $project_name\n # - Scrapy settings for $project_name                                    -\n # ---------------------------------------------------------------------------\n \n-BOT_NAME = 'scrapybot'\n+BOT_NAME = '$project_name'\n BOT_VERSION = '1.0'\n \n SPIDER_MODULES = ['$project_name.spiders']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9de6ee5109b45cd82d2785ddc796dc9c4ee67ce7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 328 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,6 +32,12 @@ class ResponseTest(unittest.TestCase):\n         assert r.headers is not headers\n         self.assertEqual(r.headers[\"caca\"], \"coco\")\n \n+        r = Response(\"http://www.example.com\", status=301)\n+        self.assertEqual(r.status, 301)\n+        r = Response(\"http://www.example.com\", status='301')\n+        self.assertEqual(r.status, 301)\n+        self.assertRaises(ValueError, Response, \"http://example.com\", status='lala200')\n+\n     def test_copy(self):\n         \"\"\"Test Response copy\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ed88fd0f3cf189bb4c3b1b8f68d3f184ec50595", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 15 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,13 +36,22 @@ class ResponseTypes(object):\n         mimetype = content_type.split(';')[0].strip().lower()\n         return self.from_mimetype(mimetype)\n \n+    def from_content_disposition(self, content_disposition):\n+        try:\n+            filename = content_disposition.split(';')[1].split('=')[1]\n+            return self.from_filename(filename)\n+        except IndexError:\n+            return Response\n+\n     def from_headers(self, headers):\n         \"\"\"Return the most appropiate Response class by looking at the HTTP\n         headers\"\"\"\n+        cls = Response\n         if 'Content-Type' in headers:\n-            return self.from_content_type(headers['Content-type'][0])\n-        else:\n-            return Response\n+            cls = self.from_content_type(headers['Content-type'][0])\n+        if cls is Response and 'Content-Disposition' in headers:\n+            cls = self.from_content_disposition(headers['Content-Disposition'][0])\n+        return cls\n \n     def from_filename(self, filename):\n         \"\"\"Return the most appropiate Response class from a file name\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8e87bcb52ed0ab5939936e6e7ac979cbcd8ac9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1369 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -73,7 +73,7 @@ class AdaptorPipe(list):\n                     val = adaptor(val, kwargs or {})\n                     if isinstance(val, tuple):\n                         next_round.extend(val)\n-                    else:\n+                    elif val is not None:\n                         next_round.append(val)\n \n                     if self.debug:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57189e1b92bb5ab83a42bca54b8d997459c6b640", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 114 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n from twisted.internet import reactor\n \n-import urlparse\n+import os, urllib, urlparse\n import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n@@ -60,6 +60,11 @@ class Command(ScrapyCommand):\n \n     def get_url(self, url):\n         u = urlparse.urlparse(url)\n+        if not u.scheme:\n+            path = os.path.abspath(url).replace(os.sep, '/')\n+            url = 'file://' + urllib.pathname2url(path)\n+            u = urlparse.urlparse(url)\n+\n         if u.scheme not in ('http', 'https', 'file'):\n             print \"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, url)\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4013497edb717eb1270518148c29e194471c4474", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 74 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,12 @@ TEMPLATES_DIR = '%s/templates' % $project_name.__path__[0]\n ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % $project_name.__path__[0]\n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+\n+# The amount of time (in secs) that the downloader should wait before\n+# downloading consecutive pages from the same spider. This can be used\n+# to throttle the crawling speed to avoid hitting servers too\n+# hard. Decimal numbers are supported. Example:\n+# DOWNLOAD_DELAY = 2.5\n DOWNLOAD_TIMEOUT = 600\n \n # use this spider class as default when no spider was found for a given url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6246cbc37a7a5688d759b5ffa973940a489f5b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 137 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import re, csv\n \n+from scrapy.http import Response\n from scrapy.xpath import XmlXPathSelector\n from scrapy import log\n from scrapy.utils.python import re_rsearch, str_to_unicode\n@@ -28,7 +29,7 @@ def xmliter(obj, nodename):\n         nodetext = header_start + match.group() + header_end\n         yield XmlXPathSelector(text=nodetext).x('//' + nodename)[0]\n \n-def csviter(obj, delimiter=None, headers=None):\n+def csviter(obj, delimiter=None, headers=None, encoding=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n \n     obj can be:\n@@ -41,8 +42,9 @@ def csviter(obj, delimiter=None, headers=None):\n     headers is an iterable that when provided offers the keys\n     for the returned dictionaries, if not the first row is used.\n     \"\"\"\n+    encoding = obj.encoding if isinstance(obj, Response) else encoding or 'utf-8'\n     def _getrow(csv_r):\n-        return [str_to_unicode(field) for field in csv_r.next()]\n+        return [str_to_unicode(field, encoding) for field in csv_r.next()]\n \n     lines = body_or_str(obj, unicode=False).splitlines(True)\n     if delimiter:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e25bfa5d88be33063ff0d2e0c0aac3380cef3570", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 65 | Churn Cumulative: 153 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,6 +39,7 @@ class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n             filename = content_disposition.split(';')[1].split('=')[1]\n+            filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:\n             return Response\n@@ -55,7 +56,8 @@ class ResponseTypes(object):\n \n     def from_filename(self, filename):\n         \"\"\"Return the most appropiate Response class from a file name\"\"\"\n-        return self.from_mimetype(mimetypes.guess_type(filename)[0])\n+        mimetype, encoding = mimetypes.guess_type(filename)\n+        return self.from_mimetype(mimetype) if encoding is None else Response\n \n     def from_url(self, url):\n         \"\"\"Return the most appropiate Response class from a URL\"\"\"\n\n@@ -0,0 +1,61 @@\n+import unittest\n+from scrapy.core.downloader.responsetypes import responsetypes\n+\n+from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse, Headers\n+\n+class ResponseTypesTest(unittest.TestCase):\n+\n+    def test_from_filename(self):\n+        mappings = [\n+            ('data.bin', Response),\n+            ('file.txt', TextResponse),\n+            ('file.xml.gz', Response),\n+            ('file.xml', XmlResponse),\n+            ('file.html', HtmlResponse),\n+        ]\n+        for source, cls in mappings:\n+            retcls = responsetypes.from_filename(source)\n+            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+\n+    def test_from_content_disposition(self):\n+        mappings = [\n+            ('attachment; filename=\"data.xml\"', XmlResponse),\n+            ('attachment; filename=data.xml', XmlResponse),\n+        ]\n+        for source, cls in mappings:\n+            retcls = responsetypes.from_content_disposition(source)\n+            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+\n+    def test_from_content_type(self):\n+        mappings = [\n+            ('text/html; charset=UTF-8', HtmlResponse),\n+            ('text/xml; charset=UTF-8', XmlResponse),\n+            ('application/xml; charset=UTF-8', XmlResponse),\n+            ('application/octet-stream', Response),\n+        ]\n+        for source, cls in mappings:\n+            retcls = responsetypes.from_content_type(source)\n+            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+\n+    def test_from_body(self):\n+        mappings = [\n+            ('\\x03\\x02\\xdf\\xdd\\x23', Response),\n+            ('Some plain text\\ndata with tabs\\t and null bytes\\0', TextResponse),\n+            ('<html><head><title>Hello</title></head>', HtmlResponse),\n+            ('<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n+        ]\n+        for source, cls in mappings:\n+            retcls = responsetypes.from_body(source)\n+            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+        \n+    def test_from_headers(self):\n+        mappings = [\n+            ({'Content-Type': ['text/html; charset=utf-8']}, HtmlResponse),\n+            ({'Content-Type': ['application/octet-stream'], 'Content-Disposition': ['attachment; filename=data.csv']}, TextResponse),\n+        ]\n+        for source, cls in mappings:\n+            source = Headers(source)\n+            retcls = responsetypes.from_headers(source)\n+            assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14e67d59c22ce448b80e6ffd58f6bfddfce32ac7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 22 | Churn Cumulative: 264 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,7 +80,10 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertRaises(StopIteration, iter.next)\n \n class UtilsCsvTestCase(unittest.TestCase):\n-    sample_feed_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds', 'feed-sample3.csv')\n+    sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')\n+    sample_feed_path = os.path.join(sample_feeds_dir, 'feed-sample3.csv')\n+    sample_feed2_path = os.path.join(sample_feeds_dir, 'feed-sample4.csv')\n+    sample_feed3_path = os.path.join(sample_feeds_dir, 'feed-sample5.csv')\n \n     def test_iterator_defaults(self):\n         body = open(self.sample_feed_path).read()\n@@ -150,5 +153,22 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         self.assertRaises(StopIteration, iter.next)\n \n+    def test_iterator_encoding(self):\n+        body1 = open(self.sample_feed2_path).read()\n+        body2 = open(self.sample_feed3_path).read()\n+\n+        response = TextResponse(url=\"http://example.com/\", body=body1, encoding='latin1')\n+        csv = csviter(response)\n+        self.assertEqual([row for row in csv],\n+            [{u'id': u'1', u'name': u'latin1', u'value': u'test'},\n+             {u'id': u'2', u'name': u'something', u'value': u'\\xf1\\xe1\\xe9\\xf3'}])\n+\n+        response = TextResponse(url=\"http://example.com/\", body=body2, encoding='cp852')\n+        csv = csviter(response)\n+        self.assertEqual([row for row in csv],\n+            [{u'id': u'1', u'name': u'cp852', u'value': u'test'},\n+             {u'id': u'2', u'name': u'something', u'value': u'\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557'}])\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#134b867abe67a89f23ad1b3cede09a96fab315fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 160 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,7 +57,10 @@ class ResponseTypes(object):\n     def from_filename(self, filename):\n         \"\"\"Return the most appropiate Response class from a file name\"\"\"\n         mimetype, encoding = mimetypes.guess_type(filename)\n-        return self.from_mimetype(mimetype) if encoding is None else Response\n+        if mimetype and not encoding:\n+            return self.from_mimetype(mimetype)\n+        else:\n+            return Response\n \n     def from_url(self, url):\n         \"\"\"Return the most appropiate Response class from a URL\"\"\"\n\n@@ -12,6 +12,7 @@ class ResponseTypesTest(unittest.TestCase):\n             ('file.xml.gz', Response),\n             ('file.xml', XmlResponse),\n             ('file.html', HtmlResponse),\n+            ('file.unknownext', Response),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_filename(source)\n@@ -57,5 +58,6 @@ class ResponseTypesTest(unittest.TestCase):\n             source = Headers(source)\n             retcls = responsetypes.from_headers(source)\n             assert retcls is cls, \"%s ==> %s != %s\" % (source, retcls, cls)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3d61dc999260bac9dcf06356f0cca17910f2a3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 65 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class ResponseTypesTest(unittest.TestCase):\n     def test_from_headers(self):\n         mappings = [\n             ({'Content-Type': ['text/html; charset=utf-8']}, HtmlResponse),\n-            ({'Content-Type': ['application/octet-stream'], 'Content-Disposition': ['attachment; filename=data.csv']}, TextResponse),\n+            ({'Content-Type': ['application/octet-stream'], 'Content-Disposition': ['attachment; filename=data.txt']}, TextResponse),\n         ]\n         for source, cls in mappings:\n             source = Headers(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0601cd03e8f87347f763bc784b11642143c81c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 40 | Churn Cumulative: 619 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n import unittest\n \n from scrapy.utils.markup import remove_entities, replace_tags, remove_comments\n@@ -27,7 +28,7 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'a < b &illegal; c &#12345678; six')\n         self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=True),\n                          u'a < b  c  six')\n-\n+        self.assertEqual(remove_entities('x&#x2264;y'), u'x\\u2264y')\n \n \n     def test_replace_tags(self):\n\n@@ -3,11 +3,11 @@ Functions for dealing with markup text\n \"\"\"\n \n import re\n-import htmlentitydefs\n+from htmlentitydefs import name2codepoint as entity_defs\n \n from scrapy.utils.python import str_to_unicode\n \n-_ent_re = re.compile(r'&(#?)([^&;]+);')\n+_ent_re = re.compile(r'&(#?(x?))([^&;\\s]+);')\n _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n \n def remove_entities(text, keep=(), remove_illegal=True):\n@@ -18,7 +18,8 @@ def remove_entities(text, keep=(), remove_illegal=True):\n     If 'keep' is passed (with a list of entity names) those entities will\n     be kept (they won't be removed).\n \n-    It supports both numeric (&#nnnn;) and named (&nbsp; &gt;) entities.\n+    It supports both numeric (&#nnnn; and &#hhhh;) and named (&nbsp; &gt;)\n+    entities.\n \n     If remove_illegal is True, entities that can't be converted are removed.\n     If remove_illegal is False, entities that can't be converted are kept \"as\n@@ -28,24 +29,32 @@ def remove_entities(text, keep=(), remove_illegal=True):\n     \"\"\"\n \n     def convert_entity(m):\n-        if m.group(1) == '#':\n+        entity_body = m.group(3)\n+\n+        if m.group(1):\n             try:\n-                return unichr(int(m.group(2)))\n+                if m.group(2):\n+                    number = int(entity_body, 16)\n+                else:\n+                    number = int(entity_body, 10)\n             except ValueError:\n-                if remove_illegal:\n-                    return u''\n+                number = None\n         else:\n-                    return u'&#%s;' % m.group(2)\n+            if entity_body in keep:\n+                return m.group(0)\n+            else:\n+                number = entity_defs.get(entity_body)\n+\n+        if number is not None:\n             try:\n-            if m.group(2) in keep:\n-                return '&%s;' % m.group(2)\n-            else:\n-                return unichr(htmlentitydefs.name2codepoint[m.group(2)])\n-        except KeyError:\n+                return unichr(number)\n+            except ValueError:\n+                pass\n+\n         if remove_illegal:\n             return u''\n         else:\n-                return u'&%s;' % m.group(2)\n+            return m.group(0)\n \n     return _ent_re.sub(convert_entity, str_to_unicode(text))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9fbce4ee211be24614e056bfaf756e06f0db7b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 26 | Churn Cumulative: 439 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,13 +82,14 @@ def remove_tags(text, which_ones=()):\n         which_ones -- is a tuple of which tags we want to remove.\n                       if is empty remove all tags.\n     \"\"\"\n-    if len(which_ones) > 0:\n+    if which_ones:\n         tags = ['<%s>|<%s .*?>|</%s>' % (tag,tag,tag) for tag in which_ones]\n-        reg_exp_remove_tags = '|'.join(tags)\n+        regex = '|'.join(tags)\n     else:\n-        reg_exp_remove_tags = '<.*?>'\n-    re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n-    return re_tags.sub(u'', str_to_unicode(text))\n+        regex = '<.*?>'\n+    retags = re.compile(regex, re.DOTALL | re.IGNORECASE)\n+\n+    return retags.sub(u'', str_to_unicode(text))\n \n def remove_tags_with_content(text, which_ones=()):\n     \"\"\" Remove tags and its content.\n@@ -96,9 +97,13 @@ def remove_tags_with_content(text, which_ones=()):\n         which_ones -- is a tuple of which tags with its content we want to remove.\n                       if is empty do nothing.\n     \"\"\"\n-    tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n-    re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n-    return re_tags_remove.sub(u'', str_to_unicode(text))\n+    text = str_to_unicode(text)\n+    if which_ones:\n+        tags = '|'.join(['<%s.*?</%s>' % (tag,tag) for tag in which_ones])\n+        retags = re.compile(tags, re.DOTALL | re.IGNORECASE)\n+        text = retags.sub(u'', text)\n+    return text\n+    \n \n def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n@@ -106,8 +111,9 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n         which_ones -- is a tuple of which escape chars we want to remove.\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n-    re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n-    return re_escape_chars.sub(u'', str_to_unicode(text))\n+    for ec in which_ones:\n+        text = text.replace(ec, u'')\n+    return str_to_unicode(text)\n \n def unquote_markup(text, keep=(), remove_illegal=True):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc4fcbeab7a9be8b3c2d611cd114972eca14d9a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 286 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -104,5 +104,9 @@ def download_file(request, spider) :\n     \"\"\"Return a deferred for a file download.\"\"\"\n     filepath = request.url.split(\"file://\")[1]\n     with open(filepath) as f:\n-        response = Response(url=request.url, body=f.read())\n+        body = f.read()\n+        respcls = responsetypes.from_filename(filepath)\n+        respcls = responsetypes.from_body(body) if respcls is Response else respcls\n+        response = respcls(url=request.url, body=body)\n+\n     return defer_succeed(response)\n\n@@ -68,12 +68,9 @@ class ResponseTypes(object):\n \n     def from_body(self, body):\n         \"\"\"Try to guess the appropiate response based on the body content.\n-        \n         This method is a bit magic and could be improved in the future, but\n         it's not meant to be used except for special cases where response types\n-        cannot be guess using more straightforward methods.\n-\n-        \"\"\"\n+        cannot be guess using more straightforward methods.\"\"\"\n         chunk = body[:5000]\n         if isbinarytext(chunk):\n             return self.from_mimetype('application/octet-stream')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ca2187e3b0103b447542e45d03be2356884f2fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 12 | Churn Cumulative: 601 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -111,12 +111,6 @@ class RobustScrapedItem(ScrapedItem):\n         return \"%s: GUID=%s, url=%s\" % ( self.__class__.__name__ , self.guid, self.url )\n \n     def _add_single_attributes(self, attrname, attrtype, attributes):\n-        if len(attributes) == 1:\n-            return attributes[0]\n-\n-        if attrtype is basestring:\n-            return ''.join(attributes)\n-        else:\n         raise NotImplementedError('You must override _add_single_attributes method in order to join %s values into a single value.' % attrtype.__name__)\n \n     def attribute(self, attrname, *values, **kwargs):\n@@ -171,7 +165,7 @@ class RobustScrapedItem(ScrapedItem):\n                 new_values.insert(0, old_value)\n \n         if not multivalued:\n-            if add and len(new_values) >= 1:\n+            if add and len(new_values) > 1:\n                 new_values = self._add_single_attributes(attrname, attrtype, new_values)\n             else:\n                 new_values = new_values[0] if new_values else None\n\n@@ -46,9 +46,7 @@ class RobustScrapedItemTestCase(unittest.TestCase):\n \n     def test_attribute_add(self):\n         self.item.attribute('name', 'John')\n-\n-        self.item.attribute('name', 'Doe', add=True)\n-        self.assertEqual(self.item.name, 'JohnDoe')\n+        self.assertRaises(NotImplementedError, self.item.attribute, 'name', 'Doe', add=True)\n \n         self.item.attribute('children', *['Ken', 'Tom'])\n         self.item.attribute('children', 'Bobby')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4e6ed3c2fcf92b37a907279dc31c97d1bf27c3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 202 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']\n \n-ROBOTSTXT_OBEY = True\n+ROBOTSTXT_OBEY = False\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b3050ca0e4071649f0d8d522bfef537528a4bd25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 14 | Churn Cumulative: 716 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,9 @@\n+from pydispatch import dispatcher\n from twisted.internet import defer\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n+from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n from scrapy.utils.request import request_fingerprint\n from scrapy.spider import spiders\n@@ -19,11 +21,13 @@ class MediaPipeline(object):\n \n     def __init__(self):\n         self.domaininfo = {}\n+        dispatcher.connect(self.domain_open, signals.domain_open)\n+        dispatcher.connect(self.domain_closed, signals.domain_closed)\n \n-    def open_domain(self, domain):\n+    def domain_open(self, domain):\n         self.domaininfo[domain] = self.DomainInfo(domain)\n \n-    def close_domain(self, domain):\n+    def domain_closed(self, domain):\n         del self.domaininfo[domain]\n \n     def process_item(self, domain, item):\n\n@@ -30,14 +30,8 @@ class ItemPipelineManager(object):\n \n     def open_domain(self, domain):\n         self.domaininfo[domain] = set()\n-        for pipe in self.pipeline:\n-            if hasattr(pipe, 'open_domain'):\n-                pipe.open_domain(domain)\n \n     def close_domain(self, domain):\n-        for pipe in self.pipeline:\n-            if hasattr(pipe, 'close_domain'):\n-                pipe.close_domain(domain)\n         del self.domaininfo[domain]\n \n     def is_idle(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bcf6e5476b13ecaecd0ca2f4f91874798217880d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 104 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,10 @@ class ResponseTypes(object):\n \n     def from_mimetype(self, mimetype):\n         \"\"\"Return the most appropiate Response class for the given mimetype\"\"\"\n+        if mimetype is not None:\n             return self.classes.get(mimetype, self.classes.get(mimetype.split('/')[0], Response))\n+        else:\n+            return Response\n \n     def from_content_type(self, content_type):\n         \"\"\"Return the most appropiate Response class from an HTTP Content-Type\n\n@@ -65,6 +65,9 @@ class ResponseTypesTest(unittest.TestCase):\n             ({'url': 'http://www.example.com/data.csv'}, TextResponse),\n             # headers takes precedence over url\n             ({'headers': Headers({'Content-Type': ['text/html; charset=utf-8']}), 'url': 'http://www.example.com/item/'}, HtmlResponse),\n+            ({'headers': Headers({'Content-Disposition': ['attachment; filename=\"data.xml.gz\"']}), 'url': 'http://www.example.com/page/'}, Response),\n+\n+\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_args(**source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16efbf87fcb5368a454412501367a9cc1939df75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 7 | Files Changed: 8 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 1175 | Contributors (this commit): 3 | Commits (past 90d): 68 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-Common downloader middleare\n+Common downloader middleware\n \n See documentation in docs/ref/downloader-middleware.rst\n \"\"\"\n\n@@ -32,7 +32,7 @@ class CompressionMiddleware(object):\n                 body = zlib.decompress(body)\n             except zlib.error:\n                 # ugly hack to work with raw deflate content that may\n-                # be sent by microsof servers. For more information, see:\n+                # be sent by microsoft servers. For more information, see:\n                 # http://carsten.codimi.de/gzip.yaws/\n                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                 # http://www.gzip.org/zlib/zlib_faq.html#faq38\n\n@@ -2,7 +2,7 @@\n An extension to retry failed requests that are potentially caused by temporary\n problems such as a connection timeout or HTTP 500 error.\n \n-You can change the behaviour of this moddileware by modifing the scraping settings:\n+You can change the behaviour of this middleware by modifing the scraping settings:\n RETRY_TIMES - how many times to retry a failed page\n RETRY_HTTP_CODES - which HTTP response codes to retry\n \n\n@@ -1,3 +1,10 @@\n+\"\"\"\n+This modules implements the CrawlSpider which is the recommended spider to use\n+for scraping typical web sites that requires crawling pages.\n+\n+See documentation in docs/ref/spiders.rst\n+\"\"\"\n+\n import copy\n \n from scrapy.http import Request\n\n@@ -1,4 +1,10 @@\n-# -*- coding: utf8 -*-\n+\"\"\"\n+This module implements the XMLFeedSpider which is the recommended spider to use\n+for scraping from an XML feed.\n+\n+See documentation in docs/ref/spiders.rst\n+\"\"\"\n+\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n from scrapy.http import Request\n\n@@ -1,5 +1,7 @@\n \"\"\"\n LinkExtractor provides en efficient way to extract links from pages\n+\n+See documentation in docs/ref/link-extractors.rst\n \"\"\"\n \n from scrapy.utils.python import FixedSGMLParser, unique as unique_list\n\n@@ -1,7 +1,8 @@\n \"\"\"\n-This module provides some LinkExtractors, which extend that base LinkExtractor\n-(scrapy.link.LinkExtractor) with some useful features.\n+This module provides some LinkExtractors, which extend the base LinkExtractor\n+(scrapy.link.LinkExtractor) with some addutional useful features.\n \n+See documentation in docs/ref/link-extractors.rst\n \"\"\"\n \n import re\n\n@@ -1,5 +1,7 @@\n \"\"\"\n-Base class for scrapy spiders\n+Base class for Scrapy spiders\n+\n+See documentation in docs/ref/spiders.rst\n \"\"\"\n from zope.interface import Interface, Attribute, invariant, implements\n from twisted.plugin import IPlugin\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#005a642240c3adc6574b564464a3f5e047e5596b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 3 | Churn Cumulative: 118 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.5\n\nDIFF:\n@@ -2,6 +2,7 @@ import os\n import cPickle as pickle\n \n SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')\n+SETTINGS_DISABLED = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n \n class Settings(object):\n     \"\"\"Class to obtain configuration values from settings module\n@@ -16,6 +17,7 @@ class Settings(object):\n     def __init__(self):\n         pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n         self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n+        if not SETTINGS_DISABLED:\n             self.settings = self._import(SETTINGS_MODULE)\n         self.defaults = {}\n         self.global_defaults = self._import('scrapy.conf.default_settings')\n@@ -24,6 +26,7 @@ class Settings(object):\n         return __import__(modulepath, {}, {}, [''])\n \n     def __getitem__(self, opt_name):\n+        if not SETTINGS_DISABLED:\n             if opt_name in self.overrides:\n                 return self.overrides[opt_name]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7dac0e449abc1f07cc6e5786a9d3f105ef1d143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 16 | Files Changed: 10 | Hunks: 16 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 66 | Churn Cumulative: 895 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,10 +3,11 @@\n tasks\"\"\"\n import os\n import shutil\n+import string\n from optparse import OptionParser\n \n import scrapy\n-from scrapy.utils.misc import render_templatefile\n+from scrapy.utils.misc import render_templatefile, string_camelcase\n \n usage = \"\"\"\n scrapy-admin.py [options] [command]\n@@ -17,8 +18,15 @@ Available commands:\n       Starts a new project with name 'project_name'\n \"\"\"\n \n+PROJECT_TEMPLATES_PATH = os.path.join(scrapy.__path__[0], 'templates/project')\n+\n+# This is the list of templatefile's path that are rendered *after copying* to\n+# project directory.\n TEMPLATES = (\n-        'scrapy_settings.py',\n+        'scrapy-ctl.py',\n+        '${project_name}/settings.py',\n+        '${project_name}/items.py',\n+        '${project_name}/pipelines.py',\n         )\n \n def main():\n@@ -33,10 +41,20 @@ def main():\n     if cmd == \"startproject\":\n         if len(args) >= 2:\n             project_name = args[1]\n-            project_tplpath = os.path.join(scrapy.__path__[0], \"conf\", \"project_template\")\n-            shutil.copytree(project_tplpath, project_name)\n+            project_root_path = project_name\n+            project_module_path = '%s/%s' % (project_name, project_name)\n+\n+            roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n+            shutil.copytree(roottpl, project_name)\n+\n+            moduletpl = os.path.join(PROJECT_TEMPLATES_PATH, 'module')\n+            shutil.copytree(moduletpl, '%s/%s' % (project_name, project_name))\n+\n             for path in TEMPLATES:\n-                render_templatefile(os.path.join(project_name, path), project_name=project_name)\n+                tplfile = os.path.join(project_root_path,\n+                        string.Template(path).substitute(project_name=project_name))\n+                render_templatefile(tplfile, project_name=project_name,\n+                        ProjectName=string_camelcase(project_name))\n         else:\n             print \"scrapy-admin.py: missing project name\"\n     else:\n\n@@ -7,7 +7,8 @@ import shutil\n from scrapy.spider import spiders\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n-from scrapy.utils.misc import render_templatefile\n+from scrapy.utils.misc import render_templatefile, string_camelcase\n+\n \n class Command(ScrapyCommand):\n     \"\"\" Childs can define custom tvars \"\"\"\n@@ -54,7 +55,8 @@ class Command(ScrapyCommand):\n     def _genspider(self, name, site, template_file):\n         \"\"\" Generate spider \"\"\"\n         tvars = {\n-            'project_name': settings.get('BOT_NAME'),\n+            'project_name': settings.get('PROJECT_NAME'),\n+            'ProjectName': string_camelcase(settings.get('PROJECT_NAME')),\n             'name': name,\n             'site': site,\n             'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])\n\n@@ -1,4 +0,0 @@\n-#!/usr/bin/env python\n-\n-from scrapy.command.cmdline import execute\n-execute()\n\n\n@@ -2,5 +2,5 @@\n \n from scrapy.item import ScrapedItem\n \n-class MyItem(ScrapedItem):\n+class ${ProjectName}Item(ScrapedItem):\n     pass\n\n@@ -0,0 +1,5 @@\n+# Define yours item pipelines here\n+\n+class ${ProjectName}Pipeline(object):\n+    def process_item(self, domain, item):\n+        return item\n\n@@ -3,8 +3,9 @@ import $project_name\n # ---------------------------------------------------------------------------\n # - Scrapy settings for $project_name                                    -\n # ---------------------------------------------------------------------------\n+PROJECT_NAME = '$project_name'\n \n-BOT_NAME = '$project_name'\n+BOT_NAME = PROJECT_NAME\n BOT_VERSION = '1.0'\n \n SPIDER_MODULES = ['$project_name.spiders']\n@@ -79,9 +80,9 @@ SPIDER_MIDDLEWARES = (\n     # Spider side\n )\n \n-# Item pipelines are usually configured by commands (see conf/commands)\n-#ITEM_PIPELINES = (\n-#)\n+ITEM_PIPELINES = (\n+    '${project_name}.pipelines.${ProjectName}Pipeline',\n+)\n \n #DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n #DEPTH_STATS = 1    # enable depth stats\n\n\n@@ -0,0 +1,7 @@\n+#!/usr/bin/env python\n+\n+import os\n+os.environ.setdefault('SCRAPYSETTINGS_MODULE', '${project_name}.settings')\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n@@ -153,3 +153,8 @@ def items_to_csv(file, items, delimiter=';', headers=None):\n             value = unicode_to_str(value) if isinstance(value, basestring) else value\n             row.append(value)\n         csv_file.writerow(row)\n+\n+\n+CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')\n+def string_camelcase(string):\n+    return CAMELCASE_INVALID_CHARS.sub('', string.title())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
