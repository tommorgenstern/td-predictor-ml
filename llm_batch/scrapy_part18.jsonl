{"custom_id": "scrapy#005044b0d824c6ef67d17985b60715239b4dc29c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,7 +63,8 @@ class WebConsole(server.Site):\n         if not settings.getbool('WEBCONSOLE_ENABLED'):\n             return\n \n-        server.Site.__init__(self, WebConsoleResource())\n+        logfile = settings['WEBCONSOLE_LOGFILE']\n+        server.Site.__init__(self, WebConsoleResource(), logPath=logfile)\n         self.noisy = False\n         port = settings.getint('WEBCONSOLE_PORT')\n         scrapyengine.listenTCP(port, self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#215151dd8628839b53534bf4bd76ba0bfdadebcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 317 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -140,7 +140,9 @@ class ClusterWorker(pb.Root):\n             self._master = None\n             log.msg(\"Lost connection to master\", log.ERROR)\n         else:\n-            deferred.addCallbacks(callback=lambda x: x, errback=lambda reason: log.msg(reason, log.ERROR))\n+            def _eb(failure):\n+                log.msg(\"Error received from ClusterMaster\\n%s\" % failure, level=log.ERROR)\n+            deferred.addErrback(_eb)\n         \n     def remote_set_master(self, master):\n         \"\"\"Set the master for this worker\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce3bbd1a2bfce21a6c4056e5ef58ede2bbc0cf24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 11 | Methods Changed: 13 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 36 | Churn Cumulative: 746 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,8 +23,9 @@ def my_import(name):\n \n class ClusterNodeBroker(pb.Referenceable):\n \n-    def __init__(self, remote, name, master):\n-        self._worker = remote\n+    def __init__(self, worker, name, master):\n+        self.unsafeTracebacks = True\n+        self._worker = worker\n         self.alive = False\n         self.name = name\n         self.master = master\n@@ -101,18 +102,18 @@ class ClusterNodeBroker(pb.Referenceable):\n             self._logfailure(\"Error while running domain=%s\" % domain, failure)\n             self.master.loading.remove(domain)\n             newprio = priority - 1 # increase priority for reschedule\n-            self.master.schedule([domain], dsettings, newprio)\n+            self.master.reschedule([domain], dsettings, newprio, reason=\"error while try to run it\")\n             \n         def _run_callback(status):\n             if status['callresponse'][0] == ResponseCode.NO_FREE_SLOT:\n                 log.msg(\"ClusterMaster: No available slots at worker=%s when trying to run domain=%s\" % (self.name, domain), log.WARNING)\n                 self.master.loading.remove(domain)\n                 newprio = priority - 1 # increase priority for rerunning asap\n-                self.master.schedule([domain], dsettings, newprio)\n+                self.master.reschedule([domain], dsettings, newprio, reason=\"no available slots at worker=%s\" % self.name)\n             elif status['callresponse'][0] == ResponseCode.DOMAIN_ALREADY_RUNNING:\n                 log.msg(\"ClusterMaster: Already running domain=%s at worker=%s\" % (domain, self.name), log.WARNING)\n                 self.master.loading.remove(domain)\n-                self.master.schedule([domain], dsettings, priority)\n+                self.master.reschedule([domain], dsettings, priority, reason=\"domain already running at worker=%s\" % self.name)\n \n         try:\n             log.msg(\"ClusterMaster: Running domain=%s at worker=%s\" % (domain, self.name), log.DEBUG)\n@@ -126,6 +127,7 @@ class ClusterNodeBroker(pb.Referenceable):\n     def remote_update(self, worker_status, domain, domain_status):\n         \"\"\"Called remotely form worker when domains finish to update status\"\"\"\n         self._set_status(worker_status)\n+        raise Exception\n         if domain in self.master.loading and domain_status == \"running\":\n             self.master.loading.remove(domain)\n             self.master.statistics[\"domains\"][\"running\"].add(domain)\n@@ -157,9 +159,9 @@ class ClusterNodeBroker(pb.Referenceable):\n             # when there is no loading domain or in the next status update. This way also we load the nodes softly\n             if self.available and free_slots > 0 and self.master.pending:\n                 pending = self.master.pending.pop(0)\n-                # if domain already running in some node, reschedule with same priority (so will be moved to run later)\n+                # if domain already running in some node, reschedule with same priority (so it will be run later)\n                 if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n-                    self.master.schedule([pending['domain']], pending['settings'], pending['priority'])\n+                    self.master.reschedule([pending['domain']], pending['settings'], pending['priority'], reason=\"domain already running in other worker\")\n                 else:\n                     self.run(pending)\n                     self.master.loading.append(pending['domain'])\n@@ -171,6 +173,7 @@ class ScrapyPBClientFactory(pb.PBClientFactory):\n \n     def __init__(self, master, nodename):\n         pb.PBClientFactory.__init__(self)\n+        self.unsafeTracebacks = True\n         self.master = master\n         self.nodename = nodename\n         \n@@ -234,7 +237,7 @@ class ClusterMaster(object):\n             log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" % (name, hostport, err), log.ERROR)\n         else:\n             def _eb(failure):\n-                self._logfailure(\"Error while loading worker node\", failure)\n+                log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" % (name, hostport, failure.value), log.ERROR)\n \n             d = factory.getRootObject()\n             d.addCallbacks(callback=lambda obj: self.add_node(obj, name), errback=_eb)\n@@ -268,8 +271,9 @@ class ClusterMaster(object):\n     def enable_node(self, name):\n         self.nodes[name].available = True\n \n-    def schedule(self, domains, spider_settings=None, priority=20):\n-        \"\"\"Schedule the given domains, with the given priority\"\"\"\n+    def _schedule(self, domains, spider_settings=None, priority=20):\n+        \"\"\"Private method which performs the schedule of the given domains,\n+        with the given priority. Used for both scheduling and rescheduling.\"\"\"\n         insert_pos = len([p for p in self.pending if ['priority'] <= priority])\n         for domain in domains:\n             pd = self.get_first_pending(domain)\n@@ -283,7 +287,17 @@ class ClusterMaster(object):\n                 final_spider_settings.update(self.global_settings)\n                 final_spider_settings.update(spider_settings or {})\n                 self.pending.insert(insert_pos, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n-                log.msg(\"ClusterMaster: Scheduled domain=%s priority=%s\" % (domain, priority), log.DEBUG)\n+\n+    def schedule(self, domains, spider_settings=None, priority=20):\n+        \"\"\"Schedule the given domains, with the given priority\"\"\"\n+        self._schedule(domains, spider_settings, priority)\n+        log.msg(\"clustermaster: Scheduled domains=%s with priority=%s\" % (','.join(domains), priority), log.DEBUG)\n+\n+    def reschedule(self, domains, spider_settings=None, priority=20, reason=None):\n+        \"\"\"Reschedule the given domains, with the given priority\"\"\"\n+        self._schedule(domains, spider_settings, priority)\n+        log.msg(\"clustermaster: Rescheduled domains=%s with priority=%s reason='%s'\" % (','.join(domains), priority, reason), log.DEBUG)\n+\n \n     def stop(self, domains):\n         \"\"\"Stop the given domains\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91ddfd6f8043335218c30cfbf9e8ee971d06e08f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 747 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -127,7 +127,6 @@ class ClusterNodeBroker(pb.Referenceable):\n     def remote_update(self, worker_status, domain, domain_status):\n         \"\"\"Called remotely form worker when domains finish to update status\"\"\"\n         self._set_status(worker_status)\n-        raise Exception\n         if domain in self.master.loading and domain_status == \"running\":\n             self.master.loading.remove(domain)\n             self.master.statistics[\"domains\"][\"running\"].add(domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06c25096346b06393005386e9e1142b73a432ebd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 228 | Lines Deleted: 145 | Files Changed: 3 | Hunks: 76 | Methods Changed: 53 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 373 | Churn Cumulative: 1260 | Contributors (this commit): 2 | Commits (past 90d): 29 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,104 +1,116 @@\n import re\n+import cPickle as pickle\n \n from traceback import format_exc\n+from urlparse import urlparse\n \n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n from scrapy.utils.python import unique, flatten\n from scrapy.utils.markup import replace_tags, remove_entities\n from scrapy.utils.misc import extract_regex\n+from scrapy.conf import settings\n+from scrapy import log\n+\n+class AdaptorPipe(object):\n+    \"\"\"\n+    Class that represents an item's attribute pipeline.\n+\n+    This class contains a dictionary of attributes, matched with a list\n+    of adaptors to be run for filtering the input before storing.\n+    \"\"\"\n \n-class AdaptorPipe:\n     def __init__(self, adaptors_pipe=None):\n         \"\"\"\n         Receives a dictionary that maps attribute_name to a list of adaptor functions\n         \"\"\"\n         self.pipes = adaptors_pipe or {}\n \n-    def append_adaptor(self, attrname, adaptor):\n+    def set_adaptors(self, attr, adaptors):\n         \"\"\"\n-        Add an adaptor at the end of the provided attribute's pipeline\n+        Set the adaptor pipeline that will be used for the specified attribute\n         \"\"\"\n-        if callable(adaptor):\n-            if self.pipes.get(attrname):\n-                self.pipes[attrname].append(adaptor)\n-            else:\n-                self.pipes[attrname] = [adaptor]\n+        self.pipes[attr] = adaptors\n \n-    def execute(self, attrname, value, debug=False):\n+    def execute(self, attrname, value, kwargs):\n         \"\"\"\n         Execute pipeline for attribute name \"attrname\" and value \"value\".\n         \"\"\"\n-        for function in self.pipes.get(attrname, []):\n+        debug = kwargs.get('debug') or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])\n+\n+        for adaptor in self.pipes.get(attrname, []):\n             try:\n                 if debug:\n-                    print \"  %07s | input >\" % function.func_name, repr(value)\n-                value = function(value)\n+                    print \"  %07s | input >\" % adaptor.__name__, repr(value)\n+                value = adaptor(kwargs)(value)\n                 if debug:\n-                    print \"  %07s | output>\" % function.func_name, repr(value)\n+                    print \"  %07s | output >\" % adaptor.__name__, repr(value)\n        \n             except Exception, e:\n-                print \"Error in '%s' adaptor. Traceback text:\" % function.func_name\n+                print \"Error in '%s' adaptor. Traceback text:\" % adaptor.__name__\n                 print format_exc()\n                 return\n         \n         return value\n \n+class AdaptorFunc(object):\n+    \"\"\"\n+    This is the base class for adaptors.\n+    \n+    An adaptor is just an object subclassed from this class\n+    which defines the __call__ method, and receives/returns only\n+    one value.\n+    \n+    You can send the adaptor some extra options while creating it (just before running it)\n+    through **kwargs, and managing them by overriding the __init__ method, as shown on\n+    the UnquoteAdaptor, for example.\n+    \"\"\"\n+    def __init__(self, kwargs={}):\n+        pass\n+\n+    def __call__(self):\n+        raise NotImplementedError('You must define the __call__ method to create and use an adaptor')\n \n ############\n # Adaptors #\n ############\n-def extract(location):\n+class ExtractAdaptor(AdaptorFunc):\n     \"\"\"\n     This adaptor extracts a list of strings\n     from 'location', which can be either a list (or tuple),\n     or an XPathSelector.\n     \n-    This function *always* returns a list.\n+    This adaptor *always* returns a list.\n     \"\"\"\n+\n+    def __call__(self, location):\n         if not location:\n             return []\n-    elif isinstance(location, XPathSelectorList):\n-        return flatten([extract(o) for o in location])\n-    elif isinstance(location, XPathSelector):\n-        return location.extract()\n+        elif isinstance(location, (XPathSelector, XPathSelectorList)):\n+            return flatten(location.extract())\n         elif isinstance(location, (list, tuple)):\n-        return flatten(location)\n+            return flatten(map(lambda x: x.extract() if isinstance(x, (XPathSelector, XPathSelectorList)) else x, flatten(location)))\n         elif isinstance(location, basestring):\n             return [location]\n \n-def _absolutize_links(rel_links, current_url, base_url):\n-    abs_links = []\n-    for link in rel_links:\n-        if link.startswith('/'):\n-            abs_links.append('%s%s' % (base_url, link))\n-        elif link.startswith('http://'):\n-            abs_links.append(link)\n-        else:\n-            abs_links.append('%s/%s' % (current_url, link))\n-    return abs_links\n-    \n-def extract_links(locations):\n+class ExtractImagesAdaptor(AdaptorFunc):\n     \"\"\"\n     This adaptor receives either an XPathSelector containing\n     the desired locations for finding urls, or a tuple like (xpath, regexp)\n     containing the xpath locations to look in, and a regular expression\n     to parse those locations.\n     \n-    In any case, this adaptor returns a list of absolute urls extracted.\n+    In any case, this adaptor returns a list containing the absolute urls extracted.\n     \"\"\"\n-    ret = []\n-    if locations:\n-        regexp = None\n-        if isinstance(locations, XPathSelector):\n-            locations = XPathSelectorList([locations])\n-        elif isinstance(locations, tuple):\n-            locations, regexp = locations\n \n-        if isinstance(locations, XPathSelectorList):\n-            if regexp:\n-                ret = locations.re(regexp)\n-            else:\n-                for selector in locations:\n+    def __init__(self, kwargs):\n+        self.base_url = kwargs.get('base_url')\n+        self.response = kwargs.get('response')\n+        if not self.response and not self.base_url:\n+            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n+\n+    def extract_from_xpath(self, selector):\n+        ret = []\n+\n         if selector.xmlNode.type == 'element':\n           if selector.xmlNode.name == 'a':\n               children = selector.x('child::*')\n@@ -116,82 +128,141 @@ def extract_links(locations):\n             ret.extend(selector.x('.//@src'))\n         elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n             ret.append(selector)\n-                ret = [selector.extract() for selector in ret]\n-            current_url, base_url = re.search(r'((http://(?:www\\.)?[\\w\\d\\.-]+?)(?:/|$).*)', locations[0].response.url).groups()\n-            ret = _absolutize_links(ret, current_url, base_url)\n+        \n         return ret\n \n-def to_unicode(value):\n+    def absolutize_link(self, base_url, link):\n+        base_url = urlparse(base_url)\n+        ret = []\n+\n+        if link.startswith('/'):\n+            ret.append('http://%s%s' % (base_url.hostname, link))\n+        elif link.startswith('http://'):\n+            ret.append(link)\n+        else:\n+            ret.append('http://%s%s/%s' % (base_url.hostname, base_url.path, link))\n+\n+        return ret\n+\n+    def __call__(self, locations):\n+        rel_links = []\n+        for location in flatten(locations):\n+            if isinstance(location, (XPathSelector, XPathSelectorList)):\n+                rel_links.extend(self.extract_from_xpath(location))\n+            else:\n+                rel_links.append(location)\n+        rel_links = ExtractAdaptor()(rel_links)\n+        \n+        if self.response:\n+            return flatten([self.absolutize_link(self.response.url, link) for link in rel_links])\n+        elif self.base_url:\n+            return flatten([self.absolutize_link(self.base_url, link) for link in rel_links])\n+        else:\n+            abs_links = []\n+            for link in rel_links:\n+                if link.startswith('http://'):\n+                    abs_links.append(link)\n+                else:\n+                    log.msg('Couldnt get the absolute url for \"%s\". Ignoring link...' % link, 'WARNING')\n+            return abs_links\n+                \n+class BoolAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n+        return bool(value)\n+\n+class ToUnicodeAdaptor(AdaptorFunc):\n     \"\"\"\n     Receives a list of strings, converts\n     it to unicode, and returns a new list.\n     \"\"\"\n+    \n+    def __call__(self, value):\n         if isinstance(value, (list, tuple)):\n             return [ unicode(v) for v in value ]\n         else:\n-        raise TypeError('to_unicode adaptor must receive a list or a tuple.')\n+            raise TypeError('ToUnicodeAdaptor must receive either a list or a tuple.')\n     \n-def regex(expr):\n+class RegexAdaptor(AdaptorFunc):\n+    \"\"\"\n+    This adaptor must receive either a list of strings or an XPathSelector\n+    and return a new list with the matches of the given strings with the given regular\n+    expression (which is passed by a keyword argument, and is mandatory for this adaptor).\n     \"\"\"\n-    This factory function returns a ready-to-use\n-    adaptor for the specified regular expression.\n-    This adaptor will accept either an XPathSelectorList\n-    or a list of strings, and will apply the provided regular\n-    expression to each of its members.\n     \n-    This adaptor always returns a list of strings.\n-    \"\"\"\n-    def _regex(value):\n+    def __init__(self, kwargs):\n+        self.regex = kwargs.get('regex')\n+\n+    def __call__(self, value):\n+        if self.regex:\n             if isinstance(value, (XPathSelector, XPathSelectorList)):\n-            return value.re(expr)\n+                return value.re(self.regex)\n             elif isinstance(value, list) and value:\n-            return flatten([extract_regex(expr, string, 'utf-8') for string in value])\n+                return flatten([extract_regex(self.regex, string, 'utf-8') for string in value])\n         return value\n-    return _regex\n \n-def unquote_all(value):\n+class UnquoteAdaptor(AdaptorFunc):\n     \"\"\"\n     Receives a list of strings, removes all of the\n     entities the strings may have, and returns\n     a new list\n     \"\"\"\n-    return [ remove_entities(v) for v in value ]\n \n-def unquote(value):\n-    \"\"\"\n-    Receives a list of strings, removes all of the entities\n-    the strings may have (except for &lt; and &amp;), and\n-    returns a new list\n-    \"\"\"\n-    return [ remove_entities(v, keep=['lt', 'amp']) for v in value ]\n+    def __init__(self, kwargs={}):\n+        self.keep = kwargs.get('keep', ['lt', 'amp'])\n \n-def remove_tags(value):\n+    def __call__(self, value):\n+        return [ remove_entities(v, keep=self.keep) for v in value ]\n+\n+class RemoveTagsAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n         return [ replace_tags(v) for v in value ]\n     \n+class RemoveRootAdaptor(AdaptorFunc):\n     _remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-def _remove_root(value):\n-    m = _remove_root_re.search(value)\n+    def _remove_root(self, value):\n+        m = self._remove_root_re.search(value)\n         if m:\n             value = m.group(1)\n         return value\n \n-def remove_root(value):\n-    return [ _remove_root(v) for v in value ]\n+    def __call__(self, value):\n+        return [ self._remove_root(v) for v in value ]\n     \n+class CleanSpacesAdaptor(AdaptorFunc):\n     _clean_spaces_re = re.compile(\"\\s+\", re.U)\n-def remove_multispaces(value):\n-    return [ _clean_spaces_re.sub(' ', v) for v in value ]\n+    def __call__(self, value):\n+        return [ self._clean_spaces_re.sub(' ', v) for v in value ]\n \n-def strip(value):\n+class StripAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n         return [ v.strip() for v in value ]\n \n-def drop_empty_elements(value):\n+class DropEmptyAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n         return [ v for v in value if v ]\n \n-def delist(value):\n-    return ' '.join(value)\n+class DelistAdaptor(AdaptorFunc):\n+    def __init__(self, kwargs={}):\n+        self.delimiter = kwargs.get('join_delimiter', ' ')\n \n+    def __call__(self, value):\n+        return self.delimiter.join(value)\n     \n+class PickleAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n+        return pickle.dumps(value)\n+\n+class DePickleAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n+        return pickle.loads(value)\n+\n+class UniqueAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n+        return unique(value)\n+\n+class FlattenAdaptor(AdaptorFunc):\n+    def __call__(self, value):\n+        return flatten(value)\n \n #############\n # Pipelines #\n@@ -200,42 +271,38 @@ def delist(value):\n The following methods automatically generate adaptor pipelines\n for some basic datatypes, according to the parameters you pass them.\n \"\"\"\n-def single_pipeline(remove_root=True, remove_tags=True, do_unquote=True):\n-    pipe = [ extract,\n-             unique,\n-             to_unicode,\n-             drop_empty_elements,\n-             remove_multispaces,\n-             strip,\n+def single_pipeline(do_remove_root=True, do_remove_tags=True, do_unquote=True):\n+    pipe = [ ExtractAdaptor,\n+             UniqueAdaptor,\n+             ToUnicodeAdaptor,\n+             DropEmptyAdaptor,\n+             CleanSpacesAdaptor,\n+             StripAdaptor,\n            ]\n-    if remove_root:\n-        pipe.insert(4, remove_root)\n-    if remove_tags:\n-        pipe.insert(5, remove_tags)\n+\n+    if do_remove_root:\n+        pipe.insert(4, RemoveRootAdaptor)\n+    if do_remove_tags:\n+        pipe.insert(5, RemoveTagsAdaptor)\n     if do_unquote:\n-        pipe.append(unquote)\n-    return pipe + [delist]\n+        pipe.append(UnquoteAdaptor)\n+    return pipe + [DelistAdaptor]\n            \n def url_pipeline():\n-    return [ extract_links,\n-             unique,\n-             to_unicode,\n-             drop_empty_elements,\n+    return [ ExtractImagesAdaptor,\n+             UniqueAdaptor,\n+             ToUnicodeAdaptor,\n+             DropEmptyAdaptor,\n            ]\n                \n-def list_pipeline(do_extract=True):\n-    pipe = []\n-    if do_extract:\n-        pipe.append(extract)\n-    return pipe + [ unique,\n-                    to_unicode,\n-                    drop_empty_elements,\n-                    unquote,\n-                    remove_tags,\n-                    remove_root,\n-                    strip,\n+list_pipeline = [ ExtractAdaptor,\n+                  UniqueAdaptor,\n+                  ToUnicodeAdaptor,\n+                  DropEmptyAdaptor,\n+                  UnquoteAdaptor,\n+                  RemoveTagsAdaptor,\n+                  RemoveRootAdaptor,\n+                  StripAdaptor,\n                 ]\n                 \n-def list_join_pipeline(delimiter='\\t'):\n-    return list_pipeline() + [delimiter.join]\n-\n+list_join_pipeline = list_pipeline + [DelistAdaptor]\n\n@@ -1,4 +1,5 @@\n from scrapy.item.adaptors import AdaptorPipe\n+from scrapy.conf import settings\n \n class ScrapedItem(object):\n     \"\"\"\n@@ -7,18 +8,31 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    def setadaptors(self, adaptors_pipe):\n+    def __init__(self, adaptors_pipe={}):\n+        #self.set_adaptors(adaptors_pipe)\n+        pass\n+\n+    def set_adaptors(self, adaptors_pipe):\n         \"\"\"\n-        Set adaptors to use for this item. Receives a dict of adaptors and\n-        returns the item itself.\n+        Set the adaptors to use for this item. Receives a dict of the adaptors\n+        desired for each attribute and returns the item itself.\n         \"\"\"\n-        object.__setattr__(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))\n+        setattr(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))\n         return self\n     \n-    def append_adaptor(self, attrname, adaptor):\n-        self._adaptors_pipe.append_adaptor(attrname, adaptor)\n+    def set_attrib_adaptors(self, attrib, adaptors):\n+        \"\"\"\n+        Set the adaptors (from a list or tuple) to be used for a specific attribute.\n+        \"\"\"\n+        self._adaptors_pipe.set_adaptors(attrib, adaptors)\n \n-    def attribute(self, attrname, value):\n-        value = self._adaptors_pipe.execute(attrname, value)\n-        if not hasattr(self, attrname) or not getattr(self, attrname):\n-            setattr(self, attrname, value)\n+    def attribute(self, attrname, value, **kwargs):\n+        val = self._adaptors_pipe.execute(attrname, value, kwargs)\n+        if not getattr(self, attrname, None) or kwargs.get('override'):\n+            setattr(self, attrname, val)\n+\n+    def __sub__(self, other):\n+        raise NotImplementedError\n+\n+class ItemDelta(object):\n+    pass\n\n@@ -7,17 +7,17 @@ from scrapy.http import Response, ResponseBody\n class AdaptorsTestCase(unittest.TestCase):\n     def test_extract(self):\n         sample_xsel = XmlXPathSelector(text='<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>')\n-        self.assertEqual(adaptors.extract(sample_xsel.x('/')),\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('/')),\n                          ['<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/*')),\n                          ['<tag1>foo<tag2>bar</tag2></tag1>', '<tag3 value=\"mytag\">test</tag3>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/@id')), ['2'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1//text()')),\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/@id')), ['2'])\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag1//text()')),\n                          ['foo', 'bar'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//text()')),\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//text()')),\n                          ['foo', 'bar', 'test'])\n-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag3/@value')), ['mytag'])\n+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag3/@value')), ['mytag'])\n         \n     def test_extract_links(self):\n         test_data = \"\"\"<html><body>\n@@ -32,53 +32,55 @@ class AdaptorsTestCase(unittest.TestCase):\n                        </body></html>\"\"\"\n         sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=ResponseBody(test_data))\n         sample_xsel = XmlXPathSelector(sample_response)\n-        self.assertEqual(adaptors.extract_links(sample_xsel.x('//@href')),\n+        sample_adaptor = adaptors.ExtractImagesAdaptor({'response': sample_response})\n+\n+        self.assertEqual(sample_adaptor(sample_xsel.x('//@href')),\n                          [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n                           u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.html'])\n-        self.assertEqual(adaptors.extract_links(sample_xsel.x('//a')),\n+        self.assertEqual(sample_adaptor(sample_xsel.x('//a')),\n                          [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n                           u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.jpg'])\n-        self.assertEqual(adaptors.extract_links((sample_xsel.x('//a[@onclick]'), r'opensomething\\(\\'(.*?)\\'\\)')),\n+        self.assertEqual(sample_adaptor(sample_xsel.x('//a[@onclick]').re(r'opensomething\\(\\'(.*?)\\'\\)')),\n                          [u'http://foobar.com/my_html1.html', u'http://foobar.com/dummy/my_html2.html'])\n         \n     def test_to_unicode(self):\n-        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'luluñ', 1, 'áé']),\n+        self.assertEqual(adaptors.ToUnicodeAdaptor()(['lala', 'lele', 'luluñ', 1, 'áé']),\n                          [u'lala', u'lele', u'lulu\\xf1', u'1', u'\\xe1\\xe9'])\n         \n     def test_regex(self):\n-        func = adaptors.regex(r'href=\"(.*?)\"')\n-        self.assertEqual(func(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n+        adaptor = adaptors.RegexAdaptor({'regex': r'href=\"(.*?)\"'})\n+        self.assertEqual(adaptor(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n                                   '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n         \n     def test_unquote_all(self):\n-        self.assertEqual(adaptors.unquote_all([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n+        self.assertEqual(adaptors.UnquoteAdaptor({'keep': []})([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n         \n     def test_unquote(self):\n-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n+        self.assertEqual(adaptors.UnquoteAdaptor()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n         \n     def test_remove_tags(self):\n         test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n-        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])\n+        self.assertEqual(adaptors.RemoveTagsAdaptor()(test_data), ['adsaas', 'dsadasf'])\n         \n     def test_remove_root(self):\n-        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n+        self.assertEqual(adaptors.RemoveRootAdaptor()(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n                          ['lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />'])\n         \n     def test_remove_multispaces(self):\n-        self.assertEqual(adaptors.remove_multispaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n+        self.assertEqual(adaptors.CleanSpacesAdaptor()(['  hello,  whats     up?', 'testing testingtesting      testing']),\n                          [' hello, whats up?', 'testing testingtesting testing'])\n         \n     def test_strip(self):\n-        self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n+        self.assertEqual(adaptors.StripAdaptor()([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n                          ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n         \n     def test_drop_empty_elements(self):\n-        self.assertEqual(adaptors.drop_empty_elements([1, 2, None, 5, None, 6, None, 'hi']),\n+        self.assertEqual(adaptors.DropEmptyAdaptor()([1, 2, None, 5, None, 6, None, 'hi']),\n                          [1, 2, 5, 6, 'hi'])\n         \n     def test_delist(self):\n-        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n+        self.assertEqual(adaptors.DelistAdaptor()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n                          'hi there fellas. this is my test.')\n         \n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42e6ed74e501b5c69982abcc78a10e8a51119744", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 17 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 51 | Churn Cumulative: 523 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.22727272727272727\n\nDIFF:\n@@ -6,7 +6,7 @@ useful in some Scrapy implementations\n import hashlib\n \n from pprint import PrettyPrinter\n-from scrapy.item import ScrapedItem\n+from scrapy.item import ScrapedItem, ItemDelta\n from scrapy.core.exceptions import UsageError, DropItem\n \n class ValidationError(DropItem):\n@@ -37,11 +37,12 @@ class RobustScrapedItem(ScrapedItem):\n         'url': basestring,  # the main URL where this item was scraped from\n     }\n     \n-    def __init__(self, data=None):\n+    def __init__(self, data=None, adaptors_pipe={}):\n         \"\"\"\n         A scraped item can be initialised with a dictionary that will be\n         squirted directly into the object.\n         \"\"\"\n+        super(RobustScrapedItem, self).__init__(adaptors_pipe)\n         if isinstance(data, dict):\n             for attr, value in data.iteritems():\n                 setattr(self, attr, value)\n@@ -70,6 +71,9 @@ class RobustScrapedItem(ScrapedItem):\n             self.__dict__.pop(attr, None)\n             return\n \n+        if attr == '_adaptors_pipe':\n+            return object.__setattr__(self, '_adaptors_pipe', value)\n+\n         type1 = self.ATTRIBUTES[attr]\n         if hasattr(type1, '__iter__'):\n             if not hasattr(value, '__iter__'):\n@@ -138,14 +142,14 @@ class RobustScrapedItem(ScrapedItem):\n \n         item.features.append('feature')\n         \"\"\"\n-        if self._version:\n+        if getattr(self, '_version', None):\n             return self._version\n         hash_ = hashlib.sha1()\n         hash_.update(\"\".join([\"\".join([n, str(v)]) for n,v in sorted(self.__dict__.iteritems())]))\n         return hash_.hexdigest()\n \n \n-class RobustItemDelta(object):\n+class RobustItemDelta(ItemDelta):\n     \"\"\"\n     This class represents the difference between\n     a pair of RobustScrapedItems.\n@@ -199,6 +203,9 @@ class RobustItemDelta(object):\n                 return True\n         return False\n \n+    def __nonzero__(self):\n+        return bool(self.diff)\n+\n     def __repr__(self):\n         if self.diff:\n             pp = PrettyPrinter(indent=3)\n\n@@ -1 +1 @@\n-from scrapy.item.models import ScrapedItem\n+from scrapy.item.models import ScrapedItem, ItemDelta\n\n@@ -5,14 +5,16 @@ import cPickle as pickle\n import shutil\n import tempfile\n import tarfile\n+import copy\n \n from pydispatch import dispatcher\n \n-from scrapy.core import signals\n from scrapy import log\n+from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n+from scrapy.item import ScrapedItem\n \n class Replay(object):\n     \"\"\"\n@@ -99,17 +101,35 @@ class Replay(object):\n         self._save()\n         self.cleanup()\n \n-    def item_scraped(self, item, spider):\n-        if self.recording or self.updating:\n-            self.scraped_old[str(item.guid)] = item.copy()\n+    def clean_private(self, src):\n+        if isinstance(src, ScrapedItem):\n+            target = src.__dict__\n+        elif isinstance(src, (list, tuple)):\n+            return type(src)(self.clean_private(elem) for elem in src)\n+        elif isinstance(src, dict):\n+            target = src\n         else:\n-            self.scraped_new[str(item.guid)] = item.copy()\n+            return src\n+        for key, val in target.items():\n+            if key[0] == '_' and key[1] != '_':\n+                del target[key]\n+            else:\n+                target[key] = self.clean_private(val)\n+        return src\n+\n+    def item_scraped(self, item, spider):\n+        item = self.clean_private(copy.deepcopy(item))\n+        if self.recording or self.updating:\n+            self.scraped_old[str(item.guid)] = item\n+        else:\n+            self.scraped_new[str(item.guid)] = item\n \n     def item_passed(self, item, spider):\n+        item = self.clean_private(copy.deepcopy(item))\n         if self.recording or self.updating:\n-            self.passed_old[str(item.guid)] = item.copy()\n+            self.passed_old[str(item.guid)] = item\n         else:\n-            self.passed_new[str(item.guid)] = item.copy()\n+            self.passed_new[str(item.guid)] = item\n \n     def response_received(self, response, spider):\n         #key = response.request.fingerprint()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebe847bc6bea71982a1b165dcd26054433f5f2f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 234 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ class ScrapedItem(object):\n     \"\"\"\n \n     def __init__(self, adaptors_pipe={}):\n-        #self.set_adaptors(adaptors_pipe)\n+        self.set_adaptors(adaptors_pipe)\n         pass\n \n     def set_adaptors(self, adaptors_pipe):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23524ccc86aeeacb49ee7771b98706c4e4a3a7de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 238 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,10 +8,6 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    def __init__(self, adaptors_pipe={}):\n-        self.set_adaptors(adaptors_pipe)\n-        pass\n-\n     def set_adaptors(self, adaptors_pipe):\n         \"\"\"\n         Set the adaptors to use for this item. Receives a dict of the adaptors\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#968a55eaf390285291d7a1bdf49cb5054a740f4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 53 | Churn Cumulative: 120 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,37 +1,42 @@\n \"\"\"\n-RequestLimitMiddleware: Limits the scheduler request queue from the point of\n-view of the spider. That is, if the scheduler queue contains an equal or\n-greater ammount of requests than the specified limit, the new requests\n-(generated by the spider) will be ignored.\n+RequestLimitMiddleware: Limits the scheduler request queue size. When spiders\n+try to schedule more than the allowed amount of requests the new requests\n+(returned by the spider) will be dropped.\n \n-The limit is setted from the spider attribute \"requests_queue_size\". If not\n-found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n-will be applied. If given a value of 0, no limit will be applied.\n+The limit can be set using the spider attribue `requests_queue_size` or the\n+setting \"REQUESTS_QUEUE_SIZE\". If not specified (or 0), no limit will be\n+applied. \n \"\"\"\n \n from scrapy.core.engine import scrapyengine\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.http import Request\n from scrapy import log\n \n class RequestLimitMiddleware(object):\n-    #_last_queue_size = 0\n+\n+    def __init__(self):\n+        self.max_queue_size = settings.getint(\"REQUESTS_QUEUE_SIZE\")\n+        if not self.max_queue_size:\n+            raise NotConfigured\n+\n     def process_result(self, response, result, spider):\n         requests = []\n-        other = []\n-        [requests.append(r) if isinstance(r, Request) else other.append(r) for r in result]\n-\n-        max_pending = spider.requests_queue_size if hasattr(spider,\"requests_queue_size\") else settings.getint(\"REQUESTS_QUEUE_SIZE\")\n-        if not max_pending:\n-            accepted = requests\n+        items = []\n+        for r in result:\n+            if isinstance(r, Request):\n+                requests.append(r)\n             else:\n-            free_slots = max_pending - len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n-            accepted = requests[:free_slots]\n-            dropped = set(requests) - set(accepted)\n-            if dropped:\n-                for r in dropped:\n-                    log.msg(\"Ignoring link (max schedule queue size reached): %s \" % r.url, level=log.WARNING, domain=spider.domain_name)\n-        #actual_size = len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n-        #log.msg(\"queue size: %d (%+d)\" % (actual_size, actual_size - self._last_queue_size) )\n-        #self._last_queue_size = actual_size\n-        return accepted + other\n+                items.append(r)\n+\n+        max_pending = getattr(spider, 'requests_queue_size', self.max_queue_size)\n+        if max_pending:\n+            pending_count = len(scrapyengine.scheduler.pending_requests.get(spider.domain_name, []))\n+            free_slots = max_pending - pending_count\n+            dropped_count = len(requests) - free_slots\n+            if dropped_count > 0:\n+                requests = requests[:free_slots]\n+                log.msg(\"Dropping %d request(s) because the maximum schedule size (%d) has been exceeded\" % \\\n+                        (dropped_count, max_pending), level=log.WARNING, domain=spider.domain_name)\n+        return requests + items\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4dab4eb45a47b1183e7c8c5670965c3c75a74a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 87 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,87 @@\n+\"\"\"\n+This module provides a mechanism for collecting one (or more) sample items per\n+domain.\n+\n+The items are collected in a dict of guid->item and persisted by pickling that\n+dict into a file.\n+\n+This can be useful for testing changes made to the framework or other common\n+code that affects several spiders.\n+\n+It uses the scrapy stats service to keep track of which domains are already\n+sampled.\n+\n+Settings that affect this module:\n+\n+ITEMSAMPLER_FILE\n+  file where to store the pickled dict of scraped items\n+\"\"\"\n+\n+from __future__ import with_statement\n+\n+import cPickle as pickle\n+\n+from pydispatch import dispatcher\n+\n+from scrapy.core.engine import scrapyengine\n+from scrapy.core.exceptions import NotConfigured, DropItem\n+from scrapy.core import signals\n+from scrapy.stats import stats\n+from scrapy import log\n+from scrapy.conf import settings\n+\n+items_per_domain = settings.getint('ITEMSAMPLER_COUNT', 1)\n+close_domain = settings.getbool('ITEMSAMPLER_CLOSE_DOMAIN', False)\n+\n+class ItemSamplerPipeline(object):\n+\n+    def __init__(self):\n+        self.filename = settings['ITEMSAMPLER_FILE']\n+        if not self.filename:\n+            raise NotConfigured\n+        self.items = {}\n+        self.domains_count = 0\n+        self.empty_domains = set()\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+\n+    def process_item(self, domain, response, item):\n+        sampled = stats.getpath(\"%s/items_sampled\" % domain, 0)\n+        if sampled < items_per_domain:\n+            self.items[item.guid] = item\n+            sampled += 1\n+            stats.setpath(\"%s/items_sampled\" % domain, sampled)\n+            if close_domain and sampled == items_per_domain:\n+                scrapyengine.close_domain(domain)\n+        return item\n+\n+    def engine_stopped(self):\n+        with open(self.filename, 'w') as f:\n+            pickle.dump(self.items, f)\n+        if self.empty_domains:\n+            log.msg(\"Empty domains (no items scraped) found: %s\" % \" \".join(self.empty_domains), level=log.WARNING)\n+\n+    def domain_closed(self, domain, spider, status):\n+        if status == 'finished' and not stats.getpath(\"%s/items_sampled\" % domain):\n+            self.empty_domains.add(domain)\n+        self.domains_count += 1\n+        log.msg(\"Sampled %d domains so far (%d empty)\" % (self.domains_count, len(self.empty_domains)), level=log.INFO)\n+\n+\n+class ItemSamplerMiddleware(object):\n+    \"\"\"This middleware drops items and requests (when domain sampling has been\n+    completed) to accelerate the processing of remaining domains\"\"\"\n+\n+    def __init__(self):\n+        if not settings['ITEMSAMPLER_FILE']:\n+            raise NotConfigured\n+\n+    def process_scrape(self, response, spider):\n+        if stats.getpath(\"%s/items_sampled\" % spider.domain_name) >= items_per_domain:\n+            return []\n+\n+    def process_result(self, response, result, spider):\n+        if stats.getpath(\"%s/items_sampled\" % spider.domain_name) >= items_per_domain:\n+            return []\n+        else:\n+            return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4288cb3f176ee79f7e472a84b77e0f2392327638", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 247 | Lines Deleted: 328 | Files Changed: 8 | Hunks: 45 | Methods Changed: 55 | Complexity Δ (Sum/Max): -18/27 | Churn Δ: 575 | Churn Cumulative: 2070 | Contributors (this commit): 2 | Commits (past 90d): 42 | Contributors (cumulative): 11 | DMM Complexity: 0.21568627450980393\n\nDIFF:\n@@ -0,0 +1,4 @@\n+from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages\n+from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, Delist, Regex\n+from scrapy.utils.python import unique, flatten\n\n@@ -0,0 +1,95 @@\n+\"\"\"\n+Adaptors related with extraction of data\n+\"\"\"\n+\n+import urlparse\n+from scrapy.utils.python import flatten\n+from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n+\n+def _extract(locations, extractor='extract'):\n+    if isinstance(locations, (XPathSelector, XPathSelectorList)):\n+        return flatten(getattr(locations, extractor)())\n+    elif hasattr(locations, '__iter__'):\n+        return flatten([getattr(x, extractor)() if isinstance(x, (XPathSelector, XPathSelectorList)) else x for x in flatten(locations)])\n+    elif isinstance(locations, basestring):\n+        return [locations]\n+    else:\n+        return []\n+\n+def extract(locations):\n+    \"\"\"\n+    This adaptor extracts a list of strings\n+    from 'locations', which can be either an iterable,\n+    or an XPathSelector/XPathSelectorList.\n+\n+    Input: XPathSelector, XPathSelectorList, iterable, basestring\n+    Output: list of unicodes\n+    \"\"\"\n+    return _extract(locations)\n+\n+def extract_unquoted(locations):\n+    \"\"\"\n+    This adaptor extracts a list of unquoted strings\n+    from 'locations', which can be either an iterable,\n+    or an XPathSelector/XPathSelectorList.\n+    The difference between this and the extract adaptor is\n+    that this adaptor will only extract text nodes and unquote them.\n+\n+    Input: XPathSelector, XPathSelectorList, iterable, basestring\n+    Output: list of unicodes\n+    \"\"\"\n+    return _extract(locations, 'extract_unquoted')\n+\n+class ExtractImages(object):\n+    \"\"\"\n+    This adaptor receives either an XPathSelector containing\n+    the desired locations for finding urls, or a list of relative\n+    links to be resolved.\n+\n+    Input: XPathSelector, XPathSelectorList, iterable\n+    Output: list of unicodes\n+    \"\"\"\n+    def __init__(self, response=None, base_url=None):\n+        self.response = response\n+        self.base_url = base_url\n+\n+    def extract_from_xpath(self, selector):\n+        ret = []\n+        if selector.xmlNode.type == 'element':\n+          if selector.xmlNode.name == 'a':\n+              children = selector.x('child::*')\n+              if len(children) > 1:\n+                ret.extend(selector.x('.//@href'))\n+                ret.extend(selector.x('.//@src'))\n+              elif len(children) == 1 and children[0].xmlNode.name == 'img':\n+                ret.extend(children.x('@src'))\n+              else:\n+                ret.extend(selector.x('@href'))\n+          elif selector.xmlNode.name == 'img':\n+            ret.extend(selector.x('@src'))\n+          else:\n+            ret.extend(selector.x('.//@href'))\n+            ret.extend(selector.x('.//@src'))\n+        elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n+            ret.append(selector)\n+\n+        return ret\n+\n+    def __call__(self, locations):\n+        if isinstance(locations, tuple) and len(locations) > 1:\n+            self.base_url = locations[1]\n+            locations = locations[0]\n+\n+        if not self.response and not self.base_url:\n+            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n+        \n+        rel_links = []\n+        for location in flatten(locations):\n+            if isinstance(location, (XPathSelector, XPathSelectorList)):\n+                rel_links.extend(self.extract_from_xpath(location))\n+            else:\n+                rel_links.append(location)\n+        rel_links = extract(rel_links)\n+\n+        base_url = self.base_url or self.response.url\n+        return [urlparse.urljoin(base_url, link) for link in rel_links]\n\n@@ -0,0 +1,39 @@\n+import re\n+from scrapy.utils.markup import replace_tags, remove_entities\n+\n+def remove_tags(value):\n+    \"\"\"\n+    Input: iterable with strings\n+    Output: list of strings\n+    \"\"\"\n+    return [ replace_tags(v) for v in value ]\n+\n+def remove_root(value):\n+    \"\"\"\n+    Input: iterable with strings\n+    Output: list of strings\n+    \"\"\"\n+    def _remove_root(value):\n+        _remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n+        m = _remove_root_re.search(value)\n+        if m:\n+            value = m.group(1)\n+        return value\n+    return [ _remove_root(v) for v in value ]\n+\n+class Unquote(object):\n+    \"\"\"\n+    Receives a list of strings, removes all of the\n+    entities the strings may have, and returns\n+    a new list\n+\n+    Input: iterable with strings\n+    Output: list of strings\n+    \"\"\"\n+    def __init__(self, keep=['lt', 'amp']):\n+        self.keep = keep\n+\n+    def __call__(self, value):\n+        return [ remove_entities(v, keep=self.keep) for v in value ]\n+\n+\n\n@@ -0,0 +1,59 @@\n+import re\n+from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n+from scrapy.utils.misc import extract_regex\n+from scrapy.utils.python import flatten\n+\n+def to_unicode(value):\n+    \"\"\"\n+    Receives a list of strings, converts\n+    it to unicode, and returns a new list.\n+\n+    Input: iterable with strings\n+    Output: list of unicodes\n+    \"\"\"\n+    if hasattr(value, '__iter__'):\n+        return [ unicode(v) for v in value ]\n+    else:\n+        raise TypeError('to_unicode must receive an iterable.')\n+\n+def clean_spaces(value):\n+    _clean_spaces_re = re.compile(\"\\s+\", re.U)\n+    return [ _clean_spaces_re.sub(' ', v) for v in value ]\n+\n+def strip_list(value):\n+    return [ v.strip() for v in value ]\n+\n+def drop_empty(value):\n+    return [ v for v in value if v ]\n+\n+class Delist(object):\n+    \"\"\"\n+    Input: iterable with strings\n+    Output: unicode\n+    \"\"\"\n+    def __init__(self, delimiter=' '):\n+        self.delimiter = delimiter\n+    \n+    def __call__(self, value):\n+        return self.delimiter.join(value)\n+\n+class Regex(object):\n+    \"\"\"\n+    This adaptor must receive either a list of strings or an XPathSelector\n+    and return a new list with the matches of the given strings with the given regular\n+    expression (which is passed by a keyword argument, and is mandatory for this adaptor).\n+\n+    Input: XPathSelector, XPathSelectorList, iterable\n+    Output: list of unicodes\n+    \"\"\"\n+    def __init__(self, regex=r''):\n+        self.regex = regex\n+\n+    def __call__(self, value):\n+        if self.regex:\n+            if isinstance(value, (XPathSelector, XPathSelectorList)):\n+                return value.re(self.regex)\n+            elif hasattr(value, '__iter__'):\n+                return flatten([extract_regex(self.regex, string, 'utf-8') for string in value])\n+        return value\n+\n\n@@ -37,12 +37,11 @@ class RobustScrapedItem(ScrapedItem):\n         'url': basestring,  # the main URL where this item was scraped from\n     }\n     \n-    def __init__(self, data=None, adaptors_pipe={}):\n+    def __init__(self, data=None):\n         \"\"\"\n         A scraped item can be initialised with a dictionary that will be\n         squirted directly into the object.\n         \"\"\"\n-        super(RobustScrapedItem, self).__init__(adaptors_pipe)\n         if isinstance(data, dict):\n             for attr, value in data.iteritems():\n                 setattr(self, attr, value)\n@@ -71,8 +70,8 @@ class RobustScrapedItem(ScrapedItem):\n             self.__dict__.pop(attr, None)\n             return\n \n-        if attr == '_adaptors_pipe':\n-            return object.__setattr__(self, '_adaptors_pipe', value)\n+        if attr == '_adaptors_dict':\n+            return object.__setattr__(self, '_adaptors_dict', value)\n \n         type1 = self.ATTRIBUTES[attr]\n         if hasattr(type1, '__iter__'):\n\n@@ -1,17 +1,7 @@\n-import re\n-import cPickle as pickle\n-\n from traceback import format_exc\n-from urlparse import urlparse\n-\n-from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n-from scrapy.utils.python import unique, flatten\n-from scrapy.utils.markup import replace_tags, remove_entities\n-from scrapy.utils.misc import extract_regex\n from scrapy.conf import settings\n-from scrapy import log\n \n-class AdaptorPipe(object):\n+class AdaptorDict(dict):\n     \"\"\"\n     Class that represents an item's attribute pipeline.\n \n@@ -19,290 +9,25 @@ class AdaptorPipe(object):\n     of adaptors to be run for filtering the input before storing.\n     \"\"\"\n \n-    def __init__(self, adaptors_pipe=None):\n-        \"\"\"\n-        Receives a dictionary that maps attribute_name to a list of adaptor functions\n-        \"\"\"\n-        self.pipes = adaptors_pipe or {}\n-\n-    def set_adaptors(self, attr, adaptors):\n-        \"\"\"\n-        Set the adaptor pipeline that will be used for the specified attribute\n-        \"\"\"\n-        self.pipes[attr] = adaptors\n-\n-    def execute(self, attrname, value, kwargs):\n+    def execute(self, attrname, value, debug=False):\n         \"\"\"\n         Execute pipeline for attribute name \"attrname\" and value \"value\".\n         \"\"\"\n-        debug = kwargs.get('debug') or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])\n+        debug = debug or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])\n \n-        for adaptor in self.pipes.get(attrname, []):\n+        for adaptor in self.get(attrname, []):\n+            name = adaptor.__class__.__name__ if hasattr(adaptor, '__class__') else adaptor.__name__\n             try:\n                 if debug:\n-                    print \"  %07s | input >\" % adaptor.__name__, repr(value)\n-                value = adaptor(kwargs)(value)\n+                    print \"  %07s | input >\" % name, repr(value)\n+                value = adaptor(value)\n                 if debug:\n-                    print \"  %07s | output >\" % adaptor.__name__, repr(value)\n+                    print \"  %07s | output >\" % name, repr(value)\n        \n-            except Exception, e:\n-                print \"Error in '%s' adaptor. Traceback text:\" % adaptor.__name__\n+            except Exception:\n+                print \"Error in '%s' adaptor. Traceback text:\" % name\n                 print format_exc()\n                 return\n         \n         return value\n \n-class AdaptorFunc(object):\n-    \"\"\"\n-    This is the base class for adaptors.\n-    \n-    An adaptor is just an object subclassed from this class\n-    which defines the __call__ method, and receives/returns only\n-    one value.\n-    \n-    You can send the adaptor some extra options while creating it (just before running it)\n-    through **kwargs, and managing them by overriding the __init__ method, as shown on\n-    the UnquoteAdaptor, for example.\n-    \"\"\"\n-    def __init__(self, kwargs={}):\n-        pass\n-\n-    def __call__(self):\n-        raise NotImplementedError('You must define the __call__ method to create and use an adaptor')\n-\n-############\n-# Adaptors #\n-############\n-class ExtractAdaptor(AdaptorFunc):\n-    \"\"\"\n-    This adaptor extracts a list of strings\n-    from 'location', which can be either a list (or tuple),\n-    or an XPathSelector.\n-    \n-    This adaptor *always* returns a list.\n-    \"\"\"\n-\n-    def __call__(self, location):\n-        if not location:\n-            return []\n-        elif isinstance(location, (XPathSelector, XPathSelectorList)):\n-            return flatten(location.extract())\n-        elif isinstance(location, (list, tuple)):\n-            return flatten(map(lambda x: x.extract() if isinstance(x, (XPathSelector, XPathSelectorList)) else x, flatten(location)))\n-        elif isinstance(location, basestring):\n-            return [location]\n-\n-class ExtractImagesAdaptor(AdaptorFunc):\n-    \"\"\"\n-    This adaptor receives either an XPathSelector containing\n-    the desired locations for finding urls, or a tuple like (xpath, regexp)\n-    containing the xpath locations to look in, and a regular expression\n-    to parse those locations.\n-    \n-    In any case, this adaptor returns a list containing the absolute urls extracted.\n-    \"\"\"\n-\n-    def __init__(self, kwargs):\n-        self.base_url = kwargs.get('base_url')\n-        self.response = kwargs.get('response')\n-        if not self.response and not self.base_url:\n-            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n-\n-    def extract_from_xpath(self, selector):\n-        ret = []\n-\n-        if selector.xmlNode.type == 'element':\n-          if selector.xmlNode.name == 'a':\n-              children = selector.x('child::*')\n-              if len(children) > 1:\n-                ret.extend(selector.x('.//@href'))\n-                ret.extend(selector.x('.//@src'))\n-              elif len(children) == 1 and children[0].xmlNode.name == 'img':\n-                ret.extend(children.x('@src'))\n-              else:\n-                ret.extend(selector.x('@href'))\n-          elif selector.xmlNode.name == 'img':\n-            ret.extend(selector.x('@src'))\n-          else:\n-            ret.extend(selector.x('.//@href'))\n-            ret.extend(selector.x('.//@src'))\n-        elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n-            ret.append(selector)\n-        \n-        return ret\n-\n-    def absolutize_link(self, base_url, link):\n-        base_url = urlparse(base_url)\n-        ret = []\n-\n-        if link.startswith('/'):\n-            ret.append('http://%s%s' % (base_url.hostname, link))\n-        elif link.startswith('http://'):\n-            ret.append(link)\n-        else:\n-            ret.append('http://%s%s/%s' % (base_url.hostname, base_url.path, link))\n-\n-        return ret\n-\n-    def __call__(self, locations):\n-        rel_links = []\n-        for location in flatten(locations):\n-            if isinstance(location, (XPathSelector, XPathSelectorList)):\n-                rel_links.extend(self.extract_from_xpath(location))\n-            else:\n-                rel_links.append(location)\n-        rel_links = ExtractAdaptor()(rel_links)\n-        \n-        if self.response:\n-            return flatten([self.absolutize_link(self.response.url, link) for link in rel_links])\n-        elif self.base_url:\n-            return flatten([self.absolutize_link(self.base_url, link) for link in rel_links])\n-        else:\n-            abs_links = []\n-            for link in rel_links:\n-                if link.startswith('http://'):\n-                    abs_links.append(link)\n-                else:\n-                    log.msg('Couldnt get the absolute url for \"%s\". Ignoring link...' % link, 'WARNING')\n-            return abs_links\n-                \n-class BoolAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return bool(value)\n-\n-class ToUnicodeAdaptor(AdaptorFunc):\n-    \"\"\"\n-    Receives a list of strings, converts\n-    it to unicode, and returns a new list.\n-    \"\"\"\n-    \n-    def __call__(self, value):\n-        if isinstance(value, (list, tuple)):\n-            return [ unicode(v) for v in value ]\n-        else:\n-            raise TypeError('ToUnicodeAdaptor must receive either a list or a tuple.')\n-    \n-class RegexAdaptor(AdaptorFunc):\n-    \"\"\"\n-    This adaptor must receive either a list of strings or an XPathSelector\n-    and return a new list with the matches of the given strings with the given regular\n-    expression (which is passed by a keyword argument, and is mandatory for this adaptor).\n-    \"\"\"\n-    \n-    def __init__(self, kwargs):\n-        self.regex = kwargs.get('regex')\n-\n-    def __call__(self, value):\n-        if self.regex:\n-            if isinstance(value, (XPathSelector, XPathSelectorList)):\n-                return value.re(self.regex)\n-            elif isinstance(value, list) and value:\n-                return flatten([extract_regex(self.regex, string, 'utf-8') for string in value])\n-        return value\n-\n-class UnquoteAdaptor(AdaptorFunc):\n-    \"\"\"\n-    Receives a list of strings, removes all of the\n-    entities the strings may have, and returns\n-    a new list\n-    \"\"\"\n-\n-    def __init__(self, kwargs={}):\n-        self.keep = kwargs.get('keep', ['lt', 'amp'])\n-\n-    def __call__(self, value):\n-        return [ remove_entities(v, keep=self.keep) for v in value ]\n-\n-class RemoveTagsAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return [ replace_tags(v) for v in value ]\n-    \n-class RemoveRootAdaptor(AdaptorFunc):\n-    _remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-    def _remove_root(self, value):\n-        m = self._remove_root_re.search(value)\n-        if m:\n-            value = m.group(1)\n-        return value\n-\n-    def __call__(self, value):\n-        return [ self._remove_root(v) for v in value ]\n-    \n-class CleanSpacesAdaptor(AdaptorFunc):\n-    _clean_spaces_re = re.compile(\"\\s+\", re.U)\n-    def __call__(self, value):\n-        return [ self._clean_spaces_re.sub(' ', v) for v in value ]\n-\n-class StripAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return [ v.strip() for v in value ]\n-\n-class DropEmptyAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return [ v for v in value if v ]\n-\n-class DelistAdaptor(AdaptorFunc):\n-    def __init__(self, kwargs={}):\n-        self.delimiter = kwargs.get('join_delimiter', ' ')\n-\n-    def __call__(self, value):\n-        return self.delimiter.join(value)\n-    \n-class PickleAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return pickle.dumps(value)\n-\n-class DePickleAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return pickle.loads(value)\n-\n-class UniqueAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return unique(value)\n-\n-class FlattenAdaptor(AdaptorFunc):\n-    def __call__(self, value):\n-        return flatten(value)\n-\n-#############\n-# Pipelines #\n-#############\n-\"\"\"\n-The following methods automatically generate adaptor pipelines\n-for some basic datatypes, according to the parameters you pass them.\n-\"\"\"\n-def single_pipeline(do_remove_root=True, do_remove_tags=True, do_unquote=True):\n-    pipe = [ ExtractAdaptor,\n-             UniqueAdaptor,\n-             ToUnicodeAdaptor,\n-             DropEmptyAdaptor,\n-             CleanSpacesAdaptor,\n-             StripAdaptor,\n-           ]\n-\n-    if do_remove_root:\n-        pipe.insert(4, RemoveRootAdaptor)\n-    if do_remove_tags:\n-        pipe.insert(5, RemoveTagsAdaptor)\n-    if do_unquote:\n-        pipe.append(UnquoteAdaptor)\n-    return pipe + [DelistAdaptor]\n-           \n-def url_pipeline():\n-    return [ ExtractImagesAdaptor,\n-             UniqueAdaptor,\n-             ToUnicodeAdaptor,\n-             DropEmptyAdaptor,\n-           ]\n-               \n-list_pipeline = [ ExtractAdaptor,\n-                  UniqueAdaptor,\n-                  ToUnicodeAdaptor,\n-                  DropEmptyAdaptor,\n-                  UnquoteAdaptor,\n-                  RemoveTagsAdaptor,\n-                  RemoveRootAdaptor,\n-                  StripAdaptor,\n-                ]\n-                \n-list_join_pipeline = list_pipeline + [DelistAdaptor]\n\n@@ -1,4 +1,4 @@\n-from scrapy.item.adaptors import AdaptorPipe\n+from scrapy.item.adaptors import AdaptorDict\n from scrapy.conf import settings\n \n class ScrapedItem(object):\n@@ -8,23 +8,23 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    def set_adaptors(self, adaptors_pipe):\n+    def set_adaptors(self, adaptors_dict, **kwargs):\n         \"\"\"\n         Set the adaptors to use for this item. Receives a dict of the adaptors\n         desired for each attribute and returns the item itself.\n         \"\"\"\n-        setattr(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))\n+        setattr(self, '_adaptors_dict', AdaptorDict(adaptors_dict))\n         return self\n     \n-    def set_attrib_adaptors(self, attrib, adaptors):\n+    def set_attrib_adaptors(self, attrib, adaptors, **kwargs):\n         \"\"\"\n         Set the adaptors (from a list or tuple) to be used for a specific attribute.\n         \"\"\"\n-        self._adaptors_pipe.set_adaptors(attrib, adaptors)\n+        self._adaptors_dict[attrib] = adaptors\n \n-    def attribute(self, attrname, value, **kwargs):\n-        val = self._adaptors_pipe.execute(attrname, value, kwargs)\n-        if not getattr(self, attrname, None) or kwargs.get('override'):\n+    def attribute(self, attrname, value, debug=False, override=False):\n+        val = self._adaptors_dict.execute(attrname, value, debug)\n+        if not getattr(self, attrname, None) or override:\n             setattr(self, attrname, val)\n \n     def __sub__(self, other):\n\n@@ -1,86 +1,84 @@\n # -*- coding: utf8 -*-\n import unittest\n-from scrapy.item import adaptors\n-from scrapy.xpath.selector import XmlXPathSelector\n+from scrapy.contrib import adaptors\n from scrapy.http import Response, ResponseBody\n+from scrapy.xpath.selector import XmlXPathSelector\n \n class AdaptorsTestCase(unittest.TestCase):\n     def test_extract(self):\n         sample_xsel = XmlXPathSelector(text='<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>')\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('/')),\n+        self.assertEqual(adaptors.extract(sample_xsel.x('/')),\n                          ['<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>'])\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/*')),\n+        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),\n                          ['<tag1>foo<tag2>bar</tag2></tag1>', '<tag3 value=\"mytag\">test</tag3>'])\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/@id')), ['2'])\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag1//text()')),\n-                         ['foo', 'bar'])\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//text()')),\n-                         ['foo', 'bar', 'test'])\n-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag3/@value')), ['mytag'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('xml/@id')), ['2'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1//text()')), ['foo', 'bar'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//text()')), ['foo', 'bar', 'test'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//tag3/@value')), ['mytag'])\n         \n     def test_extract_links(self):\n         test_data = \"\"\"<html><body>\n                          <div>\n-                           <a href=\"lala1.html\">lala1</a>\n+                           <a href=\"lala1/lala1.html\">lala1</a>\n                            <a href=\"/lala2.html\">lala2</a>\n-                           <a href=\"http://foobar.com/lala3.html\">lala3</a>\n-                           <a href=\"lala4.html\"><img src=\"lala4.jpg\" /></a>\n+                           <a href=\"http://foobar.com/pepepe/papapa/lala3.html\">lala3</a>\n+                           <a href=\"lala4.html\"><img src=\"/imgs/lala4.jpg\" /></a>\n                            <a onclick=\"javascript: opensomething('/my_html1.html');\">something1</a>\n-                           <a onclick=\"javascript: opensomething('my_html2.html');\">something2</a>\n+                           <a onclick=\"javascript: opensomething('dummy/my_html2.html');\">something2</a>\n                          </div>\n                        </body></html>\"\"\"\n         sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=ResponseBody(test_data))\n         sample_xsel = XmlXPathSelector(sample_response)\n-        sample_adaptor = adaptors.ExtractImagesAdaptor({'response': sample_response})\n+        sample_adaptor = adaptors.ExtractImages(response=sample_response)\n \n         self.assertEqual(sample_adaptor(sample_xsel.x('//@href')),\n-                         [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.html'])\n+                         [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',\n+                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/lala4.html'])\n         self.assertEqual(sample_adaptor(sample_xsel.x('//a')),\n-                         [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n-                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.jpg'])\n+                         [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',\n+                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/imgs/lala4.jpg'])\n         self.assertEqual(sample_adaptor(sample_xsel.x('//a[@onclick]').re(r'opensomething\\(\\'(.*?)\\'\\)')),\n                          [u'http://foobar.com/my_html1.html', u'http://foobar.com/dummy/my_html2.html'])\n         \n     def test_to_unicode(self):\n-        self.assertEqual(adaptors.ToUnicodeAdaptor()(['lala', 'lele', 'luluñ', 1, 'áé']),\n+        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'luluñ', 1, 'áé']),\n                          [u'lala', u'lele', u'lulu\\xf1', u'1', u'\\xe1\\xe9'])\n         \n     def test_regex(self):\n-        adaptor = adaptors.RegexAdaptor({'regex': r'href=\"(.*?)\"'})\n+        adaptor = adaptors.Regex(regex=r'href=\"(.*?)\"')\n         self.assertEqual(adaptor(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n                                   '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n         \n     def test_unquote_all(self):\n-        self.assertEqual(adaptors.UnquoteAdaptor({'keep': []})([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n+        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n         \n     def test_unquote(self):\n-        self.assertEqual(adaptors.UnquoteAdaptor()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n+        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n         \n     def test_remove_tags(self):\n         test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n-        self.assertEqual(adaptors.RemoveTagsAdaptor()(test_data), ['adsaas', 'dsadasf'])\n+        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])\n         \n     def test_remove_root(self):\n-        self.assertEqual(adaptors.RemoveRootAdaptor()(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n+        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n                          ['lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />'])\n         \n     def test_remove_multispaces(self):\n-        self.assertEqual(adaptors.CleanSpacesAdaptor()(['  hello,  whats     up?', 'testing testingtesting      testing']),\n+        self.assertEqual(adaptors.clean_spaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n                          [' hello, whats up?', 'testing testingtesting testing'])\n         \n-    def test_strip(self):\n-        self.assertEqual(adaptors.StripAdaptor()([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n+    def test_strip_list(self):\n+        self.assertEqual(adaptors.strip_list([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n                          ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n         \n     def test_drop_empty_elements(self):\n-        self.assertEqual(adaptors.DropEmptyAdaptor()([1, 2, None, 5, None, 6, None, 'hi']),\n+        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, None, 6, None, 'hi']),\n                          [1, 2, 5, 6, 'hi'])\n         \n     def test_delist(self):\n-        self.assertEqual(adaptors.DelistAdaptor()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n+        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n                          'hi there fellas. this is my test.')\n         \n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
