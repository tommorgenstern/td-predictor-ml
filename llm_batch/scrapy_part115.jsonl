{"custom_id": "scrapy#22283854d49de2a4b01fd114880b486f45c5e3f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 410 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ XPath selectors based on lxml\n \n from lxml import etree\n \n-from scrapy.utils.python import flatten\n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.python import unicode_to_str\n@@ -65,9 +64,9 @@ class XPathSelector(object_ref):\n     def extract(self):\n         try:\n             return etree.tostring(self.root, method=self._tostring_method, \\\n-                encoding=unicode).strip()\n+                encoding=unicode)\n         except (AttributeError, TypeError):\n-            return unicode(self.root).strip()\n+            return unicode(self.root)\n \n     def register_namespace(self, prefix, uri):\n         if self.namespaces is None:\n\n@@ -97,15 +97,21 @@ class XPathSelectorTestCase(unittest.TestCase):\n         x = self.hxs_cls(response)\n \n         divtwo = x.select('//div[@class=\"two\"]')\n-        self.assertEqual(divtwo.select(\"//li\").extract(),\n+        self.assertEqual(map(unicode.strip, divtwo.select(\"//li\").extract()),\n                          [\"<li>one</li>\", \"<li>two</li>\", \"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.select(\"./ul/li\").extract(),\n+        self.assertEqual(map(unicode.strip, divtwo.select(\"./ul/li\").extract()),\n                          [\"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.select(\".//li\").extract(),\n+        self.assertEqual(map(unicode.strip, divtwo.select(\".//li\").extract()),\n                          [\"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n         self.assertEqual(divtwo.select(\"./li\").extract(),\n                          [])\n \n+    @libxml2debug\n+    def test_dont_strip(self):\n+        hxs = self.hxs_cls(text='<div>fff: <a href=\"#\">zzz</a></div>')\n+        self.assertEqual(hxs.select(\"//text()\").extract(),\n+            [u'fff: ', u'zzz'])\n+\n     @libxml2debug\n     def test_selector_namespaces_simple(self):\n         body = \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#836e40896a904edf7678bb8191368257293ea4bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 193 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -161,10 +161,10 @@ def _add_auth_header(request, target):\n def _http_post(request):\n     try:\n         f = urllib2.urlopen(request)\n-        _log(\"Server response (%s):\" % f.getcode())\n+        _log(\"Server response (%s):\" % f.code)\n         print f.read()\n     except urllib2.HTTPError, e:\n-        _log(\"Deploy failed (%s):\" % e.getcode())\n+        _log(\"Deploy failed (%s):\" % e.code)\n         print e.read()\n     except urllib2.URLError, e:\n         _log(\"Deploy failed: %s\" % e)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cc5cba69b3d8a5ac1fc26168e047a17604c55bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 594 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -208,7 +208,7 @@ class Scraper(object):\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n-            log.msg(log.formatter.passed(item, spider), log.INFO, spider=spider)\n+            log.msg(log.formatter.passed(output, spider), log.INFO, spider=spider)\n             return send_catch_log_deferred(signal=signals.item_passed, \\\n                 item=item, spider=spider, output=output)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#20efdc027355e4e8a880de5af5214e3f10db41fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 199 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -59,6 +59,8 @@ class Command(ScrapyCommand):\n             help=\"list available targets\")\n         parser.add_option(\"-l\", \"--list-projects\", metavar=\"TARGET\", \\\n             help=\"list available projects on TARGET\")\n+        parser.add_option(\"--egg\", metavar=\"FILE\",\n+            help=\"use the given egg, instead of building it\")\n \n     def run(self, args, opts):\n         try:\n@@ -79,6 +81,10 @@ class Command(ScrapyCommand):\n             return\n         target, project = _get_target_project(args)\n         version = _get_version(opts)\n+        if opts.egg:\n+            egg = open(opts.egg, 'rb')\n+        else:\n+            _log(\"Bulding egg of %s-%s\" % (project, version))\n             egg = _build_egg()\n         _upload_egg(target, egg, project, version)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f73449fed63698b1607dfbd1f1f5b211b70722ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 101 | Files Changed: 3 | Hunks: 2 | Methods Changed: 15 | Complexity Δ (Sum/Max): -18/0 | Churn Δ: 101 | Churn Cumulative: 202 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,34 +0,0 @@\n-from lxml import html, etree\n-\n-from scrapy.contrib.loader import ItemLoader\n-\n-\n-class LxmlItemLoader(ItemLoader):\n-\n-    def __init__(self, response, item=None, **context):\n-        self.tree = html.fromstring(response.body_as_unicode())\n-        context.update(response=response)\n-        super(LxmlItemLoader, self).__init__(item, **context)\n-\n-    def add_xpath(self, field_name, xpath):\n-        self.add_value(field_name, self._get_xpath(xpath))\n-\n-    def replace_xpath(self, field_name, xpath):\n-        self.replace_value(field_name, self._get_xpath(xpath))\n-\n-    def _get_xpath(self, xpath):\n-        return self._get_values(self.tree.xpath(xpath))\n-\n-    def add_css(self, field_name, css):\n-        self.add_value(field_name, self._get_css(css))\n-\n-    def replace_css(self, field_name, css):\n-        self.replace_value(field_name, self._get_css(css))\n-\n-    def _get_css(self, css):\n-        return self._get_values(self.tree.cssselect(css))\n-\n-    def _get_values(self, elems):\n-        for e in elems:\n-            yield etree.tostring(e) if isinstance(e, etree.ElementBase) else e\n-\n\n@@ -1,67 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.contrib.loader.processor import MapCompose\n-from scrapy.item import Item, Field\n-from scrapy.http import HtmlResponse\n-\n-try:\n-    import lxml\n-except ImportError:\n-    lxml = False\n-\n-\n-class TestItem(Item):\n-    name = Field()\n-\n-\n-if lxml:\n-    from scrapy.contrib_exp.loader.lxmlloader import LxmlItemLoader\n-\n-    class TestLxmlItemLoader(LxmlItemLoader):\n-        default_item_class = TestItem\n-\n-\n-class LxmlItemLoaderTest(unittest.TestCase):\n-    response = HtmlResponse(url=\"\", body='<html><body><div id=\"id\">marta</div><p>paragraph</p></body></html>')\n-\n-    def setUp(self):\n-        if not lxml:\n-            raise unittest.SkipTest(\"lxml is not available\")\n-\n-    def test_constructor_with_response(self):\n-        l = TestLxmlItemLoader(response=self.response)\n-        self.assert_(l.tree)\n-\n-    def test_add_xpath(self):\n-        l = TestLxmlItemLoader(response=self.response)\n-        l.add_xpath('name', '//div')\n-        self.assertEqual(l.get_output_value('name'), [u'<div id=\"id\">marta</div>'])\n-\n-    def test_add_xpath_text(self):\n-        l = TestLxmlItemLoader(response=self.response)\n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), [u'marta'])\n-\n-    def test_replace_xpath(self):\n-        l = TestLxmlItemLoader(response=self.response)\n-        l.add_xpath('name', '//div/text()')\n-        self.assertEqual(l.get_output_value('name'), [u'marta'])\n-        l.replace_xpath('name', '//p/text()')\n-        self.assertEqual(l.get_output_value('name'), [u'paragraph'])\n-\n-    def test_add_css(self):\n-        l = TestLxmlItemLoader(response=self.response)\n-        l.add_css('name', '#id')\n-        self.assertEqual(l.get_output_value('name'), [u'<div id=\"id\">marta</div>'])\n-\n-    def test_replace_css(self):\n-        l = TestLxmlItemLoader(response=self.response)\n-        l.add_css('name', '#id')\n-        self.assertEqual(l.get_output_value('name'), [u'<div id=\"id\">marta</div>'])\n-        l.replace_css('name', 'p')\n-        self.assertEqual(l.get_output_value('name'), [u'<p>paragraph</p>'])\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b76c5c597f0d5d649444cea7a9d357b0b36ebcf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 81 | Churn Cumulative: 373 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,12 +1,11 @@\n-import sqlite3\n-\n from zope.interface import Interface, implements\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.sqlite import JsonSqliteDict\n-from scrapy import log, signals\n+from scrapy.utils.project import sqlite_db\n+from scrapy import signals\n \n class ISpiderContextStorage(Interface):\n \n@@ -24,16 +23,11 @@ class SqliteSpiderContextStorage(object):\n     sqlite_dict_class = JsonSqliteDict\n \n     def __init__(self, database=None, table='contexts'):\n-        try:\n         self.d = self.sqlite_dict_class(database, table)\n-        except sqlite3.Error, e:\n-            self.d = self.sqlite_dict_class(':memory:', table)\n-            log.msg(\"Cannot open SQLite %r - using in-memory context storage \" \\\n-                \"instead. Error was: %r\" % (database, str(e)), log.WARNING)\n \n     @classmethod\n     def from_settings(cls, settings):\n-        return cls(settings['SQLITE_DB'])\n+        return cls(sqlite_db(settings['SQLITE_DB']))\n \n     def get(self, spider):\n         if spider.name in self.d:\n\n@@ -1,10 +1,8 @@\n-import sqlite3\n-\n from zope.interface import implements\n \n-from scrapy import log\n from scrapy.interfaces import ISpiderQueue\n from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n+from scrapy.utils.project import sqlite_db\n \n \n class SqliteSpiderQueue(object):\n@@ -12,16 +10,11 @@ class SqliteSpiderQueue(object):\n     implements(ISpiderQueue)\n \n     def __init__(self, database=None, table='spider_queue'):\n-        try:\n         self.q = JsonSqlitePriorityQueue(database, table)\n-        except sqlite3.Error, e:\n-            self.q = JsonSqlitePriorityQueue(':memory:', table)\n-            log.msg(\"Cannot open SQLite %r - using in-memory spider queue \" \\\n-                \"instead. Error was: %r\" % (database, str(e)), log.WARNING)\n \n     @classmethod\n     def from_settings(cls, settings):\n-        return cls(settings['SQLITE_DB'])\n+        return cls(sqlite_db(settings['SQLITE_DB']))\n \n     def add(self, name, **spider_args):\n         d = spider_args.copy()\n\n@@ -0,0 +1,48 @@\n+from os.path import join, dirname, abspath, isabs, exists\n+from os import makedirs\n+import warnings\n+\n+from scrapy.utils.conf import closest_scrapy_cfg, get_config\n+from scrapy.utils.python import is_writable\n+\n+DATADIR_CFG_SECTION = 'datadir'\n+\n+def inside_project():\n+    return bool(closest_scrapy_cfg())\n+\n+def project_data_dir(project='default'):\n+    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n+    assert inside_project(), \"Not inside project\"\n+    scrapy_cfg = closest_scrapy_cfg()\n+    d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n+    cfg = get_config()\n+    if cfg.has_option(DATADIR_CFG_SECTION, project):\n+        d = cfg.get(DATADIR_CFG_SECTION, project)\n+    if not exists(d):\n+        makedirs(d)\n+    return d\n+\n+def expand_data_path(path):\n+    \"\"\"If path is relative, return the given path inside the project data dir,\n+    otherwise return the path unmodified\n+    \"\"\"\n+    if isabs(path):\n+        return path\n+    return join(project_data_dir(), path)\n+\n+def sqlite_db(path, nonwritable_fallback=True):\n+    \"\"\"Get the SQLite database to use. If path is relative, returns the given\n+    path inside the project data dir, otherwise returns the path unmodified. If\n+    not inside a project returns :memory: to use an in-memory database.\n+\n+    If nonwritable_fallback is True, and the path is not writable it issues a\n+    warning and returns :memory:\n+    \"\"\"\n+    if not inside_project() or path == ':memory:':\n+        db = ':memory:'\n+    else:\n+        db = expand_data_path(path)\n+        if not is_writable(db) and nonwritable_fallback:\n+            warnings.warn(\"%r is not writable - using in-memory SQLite instead\" % db)\n+            db = ':memory:'\n+    return db\n\n@@ -5,6 +5,7 @@ It also contains functions (or functionality) which is in Python versions\n higher than 2.5 which is the lowest version supported by Scrapy.\n \n \"\"\"\n+import os\n import re\n import inspect\n import weakref\n@@ -205,3 +206,12 @@ def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n             v = v.encode(encoding) if isinstance(v, unicode) else v\n         d[k] = v\n     return d\n+\n+def is_writable(path):\n+    \"\"\"Return True if the given path can be written (if it exists) or created\n+    (if it doesn't exist)\n+    \"\"\"\n+    if os.path.exists(path):\n+        return os.access(path, os.W_OK)\n+    else:\n+        return os.access(os.path.dirname(path), os.W_OK)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#130276605b0610175f8317c6e535467de5dbfae0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 257 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -259,10 +259,12 @@ USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n+TELNETCONSOLE_HOST = '0.0.0.0'\n \n WEBSERVICE_ENABLED = True\n WEBSERVICE_LOGFILE = None\n WEBSERVICE_PORT = [6080, 7030]\n+WEBSERVICE_HOST = '0.0.0.0'\n WEBSERVICE_RESOURCES = {}\n WEBSERVICE_RESOURCES_BASE = {\n     'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n\n@@ -39,13 +39,14 @@ class TelnetConsole(protocol.ServerFactory):\n             raise NotConfigured\n         self.noisy = False\n         self.portrange = map(int, settings.getlist('TELNETCONSOLE_PORT'))\n+        self.host = settings['TELNETCONSOLE_HOST']\n         dispatcher.connect(self.start_listening, signals.engine_started)\n         dispatcher.connect(self.stop_listening, signals.engine_stopped)\n \n     def start_listening(self):\n-        self.port = listen_tcp(self.portrange, self)\n-        log.msg(\"Telnet console listening on port %d\" % self.port.getHost().port,\n-            log.DEBUG)\n+        self.port = listen_tcp(self.portrange, self.host, self)\n+        h = self.port.getHost()\n+        log.msg(\"Telnet console listening on %s:%d\" % (h.host, h.port), log.DEBUG)\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n@@ -1,17 +1,17 @@\n from twisted.internet import reactor, error\n \n-def listen_tcp(portrange, factory):\n+def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n     if not hasattr(portrange, '__iter__'):\n-        return reactor.listenTCP(portrange, factory)\n+        return reactor.listenTCP(portrange, factory, interface=host)\n     if not portrange:\n-        return reactor.listenTCP(0, factory)\n+        return reactor.listenTCP(0, factory, interface=host)\n     if len(portrange) == 1:\n-        return reactor.listenTCP(portrange[0], factory)\n+        return reactor.listenTCP(portrange[0], factory, interface=host)\n     for x in range(portrange[0], portrange[1]+1):\n         try:\n-            return reactor.listenTCP(x, factory)\n+            return reactor.listenTCP(x, factory, interface=host)\n         except error.CannotListenError:\n             if x == portrange[1]:\n                 raise\n\n@@ -68,6 +68,7 @@ class WebService(server.Site):\n             raise NotConfigured\n         logfile = settings['WEBSERVICE_LOGFILE']\n         self.portrange = map(int, settings.getlist('WEBSERVICE_PORT'))\n+        self.host = settings['WEBSERVICE_HOST']\n         root = RootResource()\n         reslist = build_component_list(settings['WEBSERVICE_RESOURCES_BASE'], \\\n             settings['WEBSERVICE_RESOURCES'])\n@@ -80,9 +81,9 @@ class WebService(server.Site):\n         dispatcher.connect(self.stop_listening, signals.engine_stopped)\n \n     def start_listening(self):\n-        self.port = listen_tcp(self.portrange, self)\n-        log.msg(\"Web service listening on port %d\" % self.port.getHost().port,\n-            log.DEBUG)\n+        self.port = listen_tcp(self.portrange, self.host, self)\n+        h = self.port.getHost()\n+        log.msg(\"Web service listening on %s:%d\" % (h.host, h.port), log.DEBUG)\n \n     def stop_listening(self):\n         self.port.stopListening()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f69e7a1915ccd2cc3c0107b452414e278edd79d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 32 | Churn Cumulative: 701 | Contributors (this commit): 6 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,12 +14,15 @@ from scrapy.utils.request import request_fingerprint\n from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n+from scrapy.utils.project import data_path\n from scrapy import conf\n \n \n class HttpCacheMiddleware(object):\n \n     def __init__(self, settings=conf.settings):\n+        if not settings.getbool('HTTPCACHE_ENABLED'):\n+            raise NotConfigured\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n@@ -58,10 +61,7 @@ class HttpCacheMiddleware(object):\n class FilesystemCacheStorage(object):\n \n     def __init__(self, settings=conf.settings):\n-        cachedir = settings['HTTPCACHE_DIR']\n-        if not cachedir:\n-            raise NotConfigured\n-        self.cachedir = cachedir\n+        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n \n     def open_spider(self, spider):\n@@ -123,7 +123,7 @@ class FilesystemCacheStorage(object):\n         if not exists(metapath):\n             return # not found\n         mtime = os.stat(rpath).st_mtime\n-        if 0 <= self.expiration_secs < time() - mtime:\n+        if 0 < self.expiration_secs < time() - mtime:\n             return # expired\n         with open(metapath, 'rb') as f:\n             return pickle.load(f)\n\n@@ -148,7 +148,8 @@ FEED_EXPORTERS_BASE = {\n     'xml': 'scrapy.contrib.exporter.XmlItemExporter',\n }\n \n-HTTPCACHE_DIR = ''\n+HTTPCACHE_ENABLED = False\n+HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n\n@@ -22,6 +22,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n \n     def _get_settings(self, **new_settings):\n         settings = {\n+            'HTTPCACHE_ENABLED': True,\n             'HTTPCACHE_DIR': self.tmpdir,\n             'HTTPCACHE_EXPIRATION_SECS': 1,\n             'HTTPCACHE_IGNORE_HTTP_CODES': [],\n@@ -46,17 +47,11 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         time.sleep(2) # wait for cache to expire\n         assert storage.retrieve_response(self.spider, request2) is None\n \n-    def test_storage_expire_immediately(self):\n+    def test_storage_never_expire(self):\n         storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=0)\n         assert storage.retrieve_response(self.spider, self.request) is None\n         storage.store_response(self.spider, self.request, self.response)\n-        time.sleep(0.1) # required for win32\n-        assert storage.retrieve_response(self.spider, self.request) is None\n-\n-    def test_storage_never_expire(self):\n-        storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=-1)\n-        assert storage.retrieve_response(self.spider, self.request) is None\n-        storage.store_response(self.spider, self.request, self.response)\n+        time.sleep(0.5) # give the chance to expire\n         assert storage.retrieve_response(self.spider, self.request)\n \n     def test_middleware(self):\n\n@@ -22,13 +22,11 @@ def project_data_dir(project='default'):\n         makedirs(d)\n     return d\n \n-def expand_data_path(path):\n+def data_path(path):\n     \"\"\"If path is relative, return the given path inside the project data dir,\n     otherwise return the path unmodified\n     \"\"\"\n-    if isabs(path):\n-        return path\n-    return join(project_data_dir(), path)\n+    return path if isabs(path) else join(project_data_dir(), path)\n \n def sqlite_db(path, nonwritable_fallback=True):\n     \"\"\"Get the SQLite database to use. If path is relative, returns the given\n@@ -41,7 +39,7 @@ def sqlite_db(path, nonwritable_fallback=True):\n     if not inside_project() or path == ':memory:':\n         db = ':memory:'\n     else:\n-        db = expand_data_path(path)\n+        db = data_path(path)\n         if not is_writable(db) and nonwritable_fallback:\n             warnings.warn(\"%r is not writable - using in-memory SQLite instead\" % db)\n             db = ':memory:'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a2146d11a049b24d761d362778ae2b82f8ff443", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -81,10 +81,6 @@ class ResponseTypes(object):\n         else:\n             return Response\n \n-    def from_url(self, url):\n-        \"\"\"Return the most appropiate Response class from a URL\"\"\"\n-        return self.from_mimetype(self.mimetypes.guess_type(url)[0])\n-\n     def from_body(self, body):\n         \"\"\"Try to guess the appropiate response based on the body content.\n         This method is a bit magic and could be improved in the future, but\n@@ -106,7 +102,7 @@ class ResponseTypes(object):\n         if headers is not None:\n             cls = self.from_headers(headers)\n         if cls is Response and url is not None:\n-            cls = self.from_url(url)\n+            cls = self.from_filename(url)\n         if cls is Response and filename is not None:\n             cls = self.from_filename(filename)\n         if cls is Response and body is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#369a149d60e3deb2f0c54c078950b6316bf06837", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 210 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -81,12 +81,16 @@ class Command(ScrapyCommand):\n             return\n         target, project = _get_target_project(args)\n         version = _get_version(opts)\n+        tmpdir = None\n         if opts.egg:\n             egg = open(opts.egg, 'rb')\n         else:\n             _log(\"Bulding egg of %s-%s\" % (project, version))\n-            egg = _build_egg()\n+            egg, tmpdir = _build_egg()\n         _upload_egg(target, egg, project, version)\n+        egg.close()\n+        if tmpdir:\n+            shutil.rmtree(tmpdir)\n \n def _log(message):\n     sys.stderr.write(\"%s\\n\" % message)\n@@ -182,13 +186,10 @@ def _build_egg():\n         settings = get_config().get('settings', 'default')\n         _create_default_setup_py(settings=settings)\n     d = tempfile.mkdtemp()\n-    try:\n     f = tempfile.TemporaryFile(dir=d)\n     check_call([sys.executable, 'setup.py', 'bdist_egg', '-d', d], stdout=f)\n     egg = glob.glob(os.path.join(d, '*.egg'))[0]\n-        return open(egg, 'rb')\n-    finally:\n-        shutil.rmtree(d)\n+    return open(egg, 'rb'), d\n \n def _create_default_setup_py(**kwargs):\n     with open('setup.py', 'w') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ba972d8cf833902b29082dc4802604f8d34b9c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 46 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,8 +18,11 @@ def get_spider_list_from_eggfile(eggfile, project):\n             env = os.environ.copy()\n             env['SCRAPY_PROJECT'] = project\n             env['SCRAPY_EGGFILE'] = f.name\n-            proc = Popen(pargs, stdout=PIPE, cwd=tmpdir, env=env)\n-            out = proc.communicate()[0]\n+            proc = Popen(pargs, stdout=PIPE, stderr=PIPE, cwd=tmpdir, env=env)\n+            out, err = proc.communicate()\n+            if proc.returncode:\n+                msg = err.splitlines()[-1] if err else \"unknown error\"\n+                raise RuntimeError(msg)\n             return out.splitlines()\n     finally:\n         shutil.rmtree(tmpdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de4909faca75143b5c5b4d95af7fd226bf09489d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ import os, sys, shutil, pkg_resources\n from subprocess import Popen, PIPE\n from tempfile import NamedTemporaryFile, mkdtemp\n \n-def get_spider_list_from_eggfile(eggfile, project):\n+def get_spider_list_from_eggfile(eggfile, project, eggrunner='scrapyd.eggrunner'):\n     # FIXME: we use a temporary directory here to avoid permissions problems\n     # when running as system service, as \"scrapy list\" command tries to write\n     # the scrapy.db sqlite database in current directory\n@@ -14,15 +14,15 @@ def get_spider_list_from_eggfile(eggfile, project):\n             shutil.copyfileobj(eggfile, f)\n             f.flush()\n             eggfile.seek(0)\n-            pargs = [sys.executable, '-m', 'scrapyd.eggrunner', 'list']\n+            pargs = [sys.executable, '-m', eggrunner, 'list']\n             env = os.environ.copy()\n             env['SCRAPY_PROJECT'] = project\n             env['SCRAPY_EGGFILE'] = f.name\n             proc = Popen(pargs, stdout=PIPE, stderr=PIPE, cwd=tmpdir, env=env)\n             out, err = proc.communicate()\n             if proc.returncode:\n-                msg = err.splitlines()[-1] if err else \"unknown error\"\n-                raise RuntimeError(msg)\n+                msg = err or out or 'unknown error'\n+                raise RuntimeError(msg.splitlines()[-1])\n             return out.splitlines()\n     finally:\n         shutil.rmtree(tmpdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31bbcc94760951a495a19d6e7db0fe7b8d5eaa0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 174 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ def get_spider_list_from_eggfile(eggfile, project, eggrunner='scrapyd.eggrunner'\n     # FIXME: we use a temporary directory here to avoid permissions problems\n     # when running as system service, as \"scrapy list\" command tries to write\n     # the scrapy.db sqlite database in current directory\n-    tmpdir = mkdtemp()\n+    tmpdir = mkdtemp(prefix='eggs-%s-' % project)\n     try:\n         with NamedTemporaryFile(suffix='.egg', dir=tmpdir) as f:\n             shutil.copyfileobj(eggfile, f)\n@@ -32,7 +32,10 @@ def activate_egg(eggpath):\n     to activate a Scrapy egg file. Don't use it from other code as it may\n     leave unwanted side effects.\n     \"\"\"\n+    try:\n         d = pkg_resources.find_distributions(eggpath).next()\n+    except StopIteration:\n+        raise ValueError(\"Unknown or corrupt egg\")\n     d.activate()\n     settings_module = d.get_entry_info('scrapy', 'settings').module_name\n     os.environ['SCRAPY_SETTINGS_MODULE'] = settings_module\n\n@@ -68,7 +68,8 @@ class ListSpiders(WsResource):\n         project = txrequest.args['project'][0]\n         eggstorage = self.root.app.getComponent(IEggStorage)\n         _, eggf = eggstorage.get(project)\n-        spiders = get_spider_list_from_eggfile(eggf, project)\n+        spiders = get_spider_list_from_eggfile(eggf, project, \\\n+            eggrunner=self.root.egg_runner)\n         return {\"status\": \"ok\", \"spiders\": spiders}\n \n class DeleteProject(WsResource):\n@@ -96,6 +97,7 @@ class Root(Resource):\n     def __init__(self, config, app):\n         Resource.__init__(self)\n         self.debug = config.getboolean('debug', False)\n+        self.eggrunner = config.get('egg_runner')\n         self.app = app\n         self.putChild('schedule.json', Schedule(self))\n         self.putChild('addversion.json', AddVersion(self))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bdffadbe35f8f338e4618c93af018d79901994b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,15 +2,10 @@ from __future__ import with_statement\n \n import os, sys, shutil, pkg_resources\n from subprocess import Popen, PIPE\n-from tempfile import NamedTemporaryFile, mkdtemp\n+from tempfile import NamedTemporaryFile\n \n def get_spider_list_from_eggfile(eggfile, project, eggrunner='scrapyd.eggrunner'):\n-    # FIXME: we use a temporary directory here to avoid permissions problems\n-    # when running as system service, as \"scrapy list\" command tries to write\n-    # the scrapy.db sqlite database in current directory\n-    tmpdir = mkdtemp(prefix='eggs-%s-' % project)\n-    try:\n-        with NamedTemporaryFile(suffix='.egg', dir=tmpdir) as f:\n+    with NamedTemporaryFile(suffix='.egg') as f:\n         shutil.copyfileobj(eggfile, f)\n         f.flush()\n         eggfile.seek(0)\n@@ -18,14 +13,12 @@ def get_spider_list_from_eggfile(eggfile, project, eggrunner='scrapyd.eggrunner'\n         env = os.environ.copy()\n         env['SCRAPY_PROJECT'] = project\n         env['SCRAPY_EGGFILE'] = f.name\n-            proc = Popen(pargs, stdout=PIPE, stderr=PIPE, cwd=tmpdir, env=env)\n+        proc = Popen(pargs, stdout=PIPE, stderr=PIPE, env=env)\n         out, err = proc.communicate()\n         if proc.returncode:\n             msg = err or out or 'unknown error'\n             raise RuntimeError(msg.splitlines()[-1])\n         return out.splitlines()\n-    finally:\n-        shutil.rmtree(tmpdir)\n \n def activate_egg(eggpath):\n     \"\"\"Activate a Scrapy egg file. This is meant to be used from egg runners\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37c9d5feff6435d452f75fe82661ea88197e4b4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 114 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class Command(runserver.Command):\n         return \"[options] <list|clear|count|add spider1 ..>\"\n \n     def short_desc(self):\n-        return \"Control execution queue\"\n+        return \"Control the spider queue\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d988ca1ec2561f57e3165ea8d86035328f0f8746", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 33 | Files Changed: 1 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 72 | Churn Cumulative: 282 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,12 +18,6 @@ from scrapy.utils.multipart import encode_multipart\n from scrapy.utils.http import basic_auth_header\n from scrapy.utils.conf import get_config, closest_scrapy_cfg\n \n-_DEFAULT_TARGETS = {\n-    'scrapyd': {\n-        'url': 'http://localhost:6800/',\n-    },\n-}\n-\n _SETUP_PY_TEMPLATE = \\\n \"\"\"# Automatically created by: scrapy deploy\n \n@@ -42,22 +36,24 @@ class Command(ScrapyCommand):\n     requires_project = True\n \n     def syntax(self):\n-        return \"[options] [ <target:project> | -l <target> | -L ]\"\n+        return \"[options] [ [target] | -l | -L <target> ]\"\n \n     def short_desc(self):\n-        return \"Deploy project in Scrapyd server\"\n+        return \"Deploy project in Scrapyd target\"\n \n     def long_desc(self):\n         return \"Deploy the current project into the given Scrapyd server \" \\\n-            \"(aka target) and project.\"\n+            \"(known as target)\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-p\", \"--project\",\n+            help=\"the project name in the target\")\n         parser.add_option(\"-v\", \"--version\",\n             help=\"the version to deploy. Defaults to current timestamp\")\n-        parser.add_option(\"-L\", \"--list-targets\", action=\"store_true\", \\\n+        parser.add_option(\"-l\", \"--list-targets\", action=\"store_true\", \\\n             help=\"list available targets\")\n-        parser.add_option(\"-l\", \"--list-projects\", metavar=\"TARGET\", \\\n+        parser.add_option(\"-L\", \"--list-projects\", metavar=\"TARGET\", \\\n             help=\"list available projects on TARGET\")\n         parser.add_option(\"--egg\", metavar=\"FILE\",\n             help=\"use the given egg, instead of building it\")\n@@ -79,13 +75,16 @@ class Command(ScrapyCommand):\n             projects = json.loads(f.read())['projects']\n             print os.linesep.join(projects)\n             return\n-        target, project = _get_target_project(args)\n-        version = _get_version(opts)\n+        target_name = _get_target_name(args)\n+        target = _get_target(target_name)\n+        project = _get_project(target, opts)\n+        version = _get_version(target, opts)\n         tmpdir = None\n         if opts.egg:\n+            _log(\"Using egg: %s\" % opts.egg)\n             egg = open(opts.egg, 'rb')\n         else:\n-            _log(\"Bulding egg of %s-%s\" % (project, version))\n+            _log(\"Building egg of %s-%s\" % (project, version))\n             egg, tmpdir = _build_egg()\n         _upload_egg(target, egg, project, version)\n         egg.close()\n@@ -93,20 +92,21 @@ class Command(ScrapyCommand):\n             shutil.rmtree(tmpdir)\n \n def _log(message):\n-    sys.stderr.write(\"%s\\n\" % message)\n+    sys.stderr.write(message + os.linesep)\n \n-def _get_target_project(args):\n-    if len(args) >= 1 and ':' in args[0]:\n-        target_name, project = args[0].split(':', 1)\n+def _get_target_name(args):\n+    if len(args) > 1:\n+        raise UsageError(\"Too many arguments: %s\" % ' '.join(args))\n+    elif args:\n+        return args[0]\n     elif len(args) < 1:\n-        target_name = _get_option('deploy', 'target')\n-        project = _get_option('deploy', 'project')\n-        if not target_name or not project:\n-            raise UsageError(\"<target:project> not given and defaults not found\")\n-    else:\n-        raise UsageError(\"%r is not a <target:project>\" % args[0])\n-    target = _get_target(target_name)\n-    return target, project\n+        return 'default'\n+\n+def _get_project(target, opts):\n+    project = opts.project or target.get('project')\n+    if not project:\n+        raise UsageError(\"Missing project\")\n+    return project\n \n def _get_option(section, option, default=None):\n     cfg = get_config()\n@@ -115,10 +115,15 @@ def _get_option(section, option, default=None):\n \n def _get_targets():\n     cfg = get_config()\n-    targets = _DEFAULT_TARGETS.copy()\n+    baset = dict(cfg.items('deploy')) if cfg.has_section('deploy') else {}\n+    targets = {}\n+    if 'url' in baset:\n+        targets['default'] = baset\n     for x in cfg.sections():\n-        if x.startswith('deploy_'):\n-            targets[x[7:]] = dict(cfg.items(x))\n+        if x.startswith('deploy:'):\n+            t = baset.copy()\n+            t.update(cfg.items(x))\n+            targets[x[7:]] = t\n     return targets\n \n def _get_target(name):\n@@ -130,12 +135,13 @@ def _get_target(name):\n def _url(target, action):\n     return urljoin(target['url'], action)\n \n-def _get_version(opts):\n-    if opts.version == 'HG':\n+def _get_version(target, opts):\n+    version = opts.version or target.get('version')\n+    if version == 'HG':\n         p = Popen(['hg', 'tip', '--template', '{rev}'], stdout=PIPE)\n         return 'r%s' % p.communicate()[0]\n-    elif opts.version:\n-        return opts.version\n+    elif version:\n+        return version\n     else:\n         return str(int(time.time()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7698ff2a7bcd11ef219ec9e8f2addb7a8a4737d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,6 +68,8 @@ class TelnetConsole(protocol.ServerFactory):\n             'p': pprint.pprint,\n             'prefs': print_live_refs,\n             'hpy': hpy,\n+            'help': \"This is Scrapy telnet console. For more info see: \" \\\n+                \"http://doc.scrapy.org/topics/telnetconsole.html\", # see #284\n         }\n         send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n         return telnet_vars\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c18f02adee5afdca286976c25a9353c77785545", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 495 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,7 @@ from scrapy.utils.response import open_in_browser\n from scrapy.utils.url import any_to_uri\n from scrapy.utils.console import start_python_console\n from scrapy.settings import Settings\n-from scrapy.http import Request, Response, TextResponse\n+from scrapy.http import Request, Response, HtmlResponse, XmlResponse\n \n class Shell(object):\n \n@@ -80,8 +80,9 @@ class Shell(object):\n         self.vars['item'] = item\n         self.vars['settings'] = self.crawler.settings\n         if url:\n-            if isinstance(response, TextResponse):\n+            if isinstance(response, XmlResponse):\n                 self.vars['xxs'] = XmlXPathSelector(response)\n+            if isinstance(response, HtmlResponse):\n                 self.vars['hxs'] = HtmlXPathSelector(response)\n             self.vars['response'] = response\n             self.vars['request'] = request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08bbbc2f82bbf9079fc7c3112f65d909fc85e6d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 497 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,7 +26,6 @@ class Shell(object):\n \n     def __init__(self, crawler, update_vars=None, inthread=False, code=None):\n         self.crawler = crawler\n-        self.vars = {}\n         self.update_vars = update_vars or (lambda x: None)\n         self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])\n         self.inthread = inthread\n@@ -77,6 +76,7 @@ class Shell(object):\n \n     def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n+        self.vars = {}\n         self.vars['item'] = item\n         self.vars['settings'] = self.crawler.settings\n         if url:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b4cc2d91f407e3a9267a84ec4a0346ab34035b35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 42 | Churn Cumulative: 1741 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -343,7 +343,7 @@ class RecordExtractor(object):\n         The region in the page to be extracted from may be specified using\n         start_index and end_index\n         \"\"\"\n-        ignored_regions = [LabelledRegion(*i) for i in (ignored_regions or [])]\n+        ignored_regions = [i if isinstance(i, LabelledRegion) else LabelledRegion(*i) for i in (ignored_regions or [])]\n         region_elements = sorted(self.extractors + ignored_regions, key=lambda x: _labelled(x).start_index)\n         _, _, attributes = self._doextract(page, region_elements, start_index, \n                 end_index)\n\n@@ -749,6 +749,39 @@ EXTRACT_PAGE22 = u\"\"\"\n </body></html>\n \"\"\"\n \n+ANNOTATED_PAGE23 = u\"\"\"\n+<html><body>\n+<h4>Product</h4>\n+<table>\n+<tr><td>\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Variant 1<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\" data-scrapy-ignore=\"true\">560</b></p>\n+</td></tr>\n+<tr><td>\n+<p>Variant 2<b>570</b></p>\n+</td></tr>\n+<tr><td>\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Variant 3<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\" data-scrapy-ignore=\"true\">580</b></p>\n+</td></tr>\n+</table>\n+</body></html>\n+\"\"\"\n+\n+EXTRACT_PAGE23 = u\"\"\"\n+<html><body>\n+<h4>Product</h4>\n+<table>\n+<tr><td>\n+<p>Variant 1<b>300</b></p>\n+</td></tr>\n+<tr><td>\n+<p>Variant 2<b>320</b></p>\n+</td></tr>\n+<tr><td>\n+<p>Variant 3<b>340</b></p>\n+</td></tr>\n+</table>\n+</body></html>\n+\"\"\"\n \n SAMPLE_DESCRIPTOR1 = ItemDescriptor('test', 'product test', [\n     A('name', \"Product name\", required=True),\n@@ -959,6 +992,13 @@ TEST_DATA = [\n              ],\n              u'image_urls': [u'image.jpg']},\n     ),\n+    ('repeated (variants) with ignore annotations', [ANNOTATED_PAGE23], EXTRACT_PAGE23, None,\n+        {'variants': [\n+            {u'price': [u'300'], u'name': [u'Variant 1']},\n+            {u'price': [u'320'], u'name': [u'Variant 2']},\n+            {u'price': [u'340'], u'name': [u'Variant 3']}\n+            ]},\n+    ),\n ]\n \n class TestExtraction(TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1f419e9e934baa415dc58f0931f4ec5fe04646e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 340 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -274,6 +274,10 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(u'http://www.example.com/caf%E9-con-leche.htm'),\n                                            'http://www.example.com/caf%E9-con-leche.htm')\n \n+        # domains are case insensitive\n+        self.assertEqual(canonicalize_url(\"http://www.EXAMPLE.com\"),\n+                                          \"http://www.example.com\")\n+\n     def test_path_to_file_uri(self):\n         if os.name == 'nt':\n             self.assertEqual(path_to_file_uri(\"C:\\\\windows\\clock.avi\"),\n\n@@ -154,7 +154,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n     query = urllib.urlencode(keyvals)\n     path = urllib.quote(urllib.unquote(path))\n     fragment = '' if not keep_fragments else fragment\n-    return urlparse.urlunparse((scheme, netloc, path, params, query, fragment))\n+    return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n \n def path_to_file_uri(path):\n     \"\"\"Convert local filesystem path to legal File URIs as described in:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
