{"custom_id": "keras#b32e60cfa0a38c1aff12b66521d6324f1eb026b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2800 | Contributors (this commit): 28 | Commits (past 90d): 71 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -406,6 +406,9 @@ class Merge(Layer):\n             l1 = self.layers[0].get_output(train)\n             l2 = self.layers[1].get_output(train)\n             output = T.batched_tensordot(l1, l2, self.dot_axes)\n+            output_shape = list(self.output_shape)\n+            output_shape[0] = l1.shape[0]\n+            output = output.reshape(tuple(output_shape))\n             return output\n         elif self.mode == 'cos':\n             l1 = self.layers[0].get_output(train)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28882a868dc8a3da12004a9303756cc6980293cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 1281 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -125,7 +125,8 @@ class SimpleRNN(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(SimpleRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -224,7 +225,8 @@ class SimpleDeepRNN(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(SimpleDeepRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -339,7 +341,8 @@ class GRU(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(GRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -472,7 +475,8 @@ class LSTM(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -591,7 +595,8 @@ class JZS1(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(JZS1, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -712,7 +717,8 @@ class JZS2(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(JZS2, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -825,6 +831,7 @@ class JZS3(Recurrent):\n                   \"truncate_gradient\": self.truncate_gradient,\n                   \"return_sequences\": self.return_sequences,\n                   \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+                  \"input_length\": self.input_length,\n+                  \"go_backwards\": self.go_backwards}\n         base_config = super(JZS3, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4884595e4dcc0b4c12fd5a00ad0df8c4deaf1cfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 90 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,18 +13,12 @@ from keras.datasets import imdb\n \n '''\n     Train a Bidirectional LSTM on the IMDB sentiment classification task.\n-    The dataset is actually too small for bidirectional LSTM to be of any advantage\n-    compared to simpler, much faster methods such as TF-IDF+LogReg.\n-    Bidirectional LSTM may be not suited for a simple text classification task.\n-    Notes:\n-    - RNNs are tricky. And in particular Bidirectional RNNs you may experiment\n-    with different Recurrent layers and different parameters to find the best configuration\n-    for your task.\n \n     GPU command:\n         THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_bidirectional_lstm.py\n \n     Output after 4 epochs on CPU: ~0.8146\n+    Time per epoch on CPU (Core i7): ~150s.\n '''\n \n max_features = 20000\n@@ -32,7 +26,8 @@ maxlen = 100  # cut texts after this number of words (among top max_features mos\n batch_size = 32\n \n print(\"Loading data...\")\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n+                                                      test_split=0.2)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n@@ -47,8 +42,9 @@ y_test = np.array(y_test)\n print('Build model...')\n model = Graph()\n model.add_input(name='input', input_shape=(1,), dtype=int)\n-model.add_node(Embedding(max_features, 128, input_length=maxlen), name='embedding', input='input')\n-model.add_node(LSTM(64), name='forward', input='embedding' )  # You can change these two layers with GRU\n+model.add_node(Embedding(max_features, 128, input_length=maxlen),\n+               name='embedding', input='input')\n+model.add_node(LSTM(64), name='forward', input='embedding')\n model.add_node(LSTM(64, go_backwards=True), name='backward', input='embedding')\n model.add_node(Dropout(0.5), name='dropout', inputs=['forward', 'backward'])\n model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='dropout')\n@@ -58,7 +54,11 @@ model.add_output(name='output', input='sigmoid')\n model.compile('adam', {'output': 'binary_crossentropy'})\n \n print(\"Train...\")\n-model.fit({'input':X_train, 'output':y_train}, batch_size=batch_size, nb_epoch=4)\n-acc = accuracy(y_test, np.round(np.array(model.predict({'input':X_test}, batch_size=batch_size)['output'])))\n+model.fit({'input': X_train, 'output': y_train},\n+          batch_size=batch_size,\n+          nb_epoch=4,\n+          show_accuracy=True)\n+acc = accuracy(y_test,\n+               np.round(np.array(model.predict({'input': X_test},\n+                                               batch_size=batch_size)['output'])))\n print('Test accuracy:', acc)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e3c43d3eef80b5f59cf282d4a7fa00d5baaa6b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 93 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -56,8 +56,7 @@ model.compile('adam', {'output': 'binary_crossentropy'})\n print(\"Train...\")\n model.fit({'input': X_train, 'output': y_train},\n           batch_size=batch_size,\n-          nb_epoch=4,\n-          show_accuracy=True)\n+          nb_epoch=4)\n acc = accuracy(y_test,\n                np.round(np.array(model.predict({'input': X_test},\n                                                batch_size=batch_size)['output'])))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b0e2b0c79862079636db444ae81252a55be8e5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 23/23 | Churn Δ: 19 | Churn Cumulative: 2819 | Contributors (this commit): 28 | Commits (past 90d): 72 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -923,9 +923,8 @@ class Lambda(Layer):\n     output_shape - Expected output shape from function. Could be a tuple or a function of the shape of the input\n     \"\"\"\n \n-    def __init__(self, function, output_shape=None, ndim=2):\n+    def __init__(self, function, output_shape=None):\n         super(Lambda, self).__init__()\n-        self.input = ndim_tensor(ndim)\n         py3 = sys.version_info[0] == 3\n         if py3:\n             self.function = marshal.dumps(function.__code__)\n@@ -946,14 +945,14 @@ class Lambda(Layer):\n         if self._ouput_shape is None:\n             return self.input_shape\n         elif type(self._output_shape) == tuple:\n-            return self._output_shape\n+            return self.input_shape[0] + self._output_shape\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(self.previous.output_shape)\n+            shape = output_shape_func(self.previous.output_shape[1:])\n             if type(shape) not in {list, tuple}:\n                 raise Exception(\"output_shape function must return a tuple\")\n-            return tuple(shape)\n+            return self.input_shape[0] + tuple(shape)\n \n     def get_output(self, train=False):\n         func = marshal.loads(self.function)\n@@ -1019,18 +1018,18 @@ class LambdaMerge(Lambda):\n \n     @property\n     def output_shape(self):\n+        input_shapes = [layer.output_shape for layer in self.layers]\n         if self._output_shape is None:\n-            return self.layers[0].input_shape\n+            return input_shapes[0]\n         elif type(self._output_shape) == tuple:\n-            return self._output_shape\n+            return input_shapes[0][0] + self._output_shape\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n-            input_shapes = [layer.output_shape for layer in self.layers]\n-            shape = output_shape_func(input_shapes)\n+            shape = output_shape_func([shape[1:] for shape in input_shapes])\n             if type(shape) not in {list, tuple}:\n                 raise Exception(\"output_shape function must return a tuple\")\n-            return tuple(shape)\n+            return input_shapes[0][0] + tuple(shape)\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bde97055c01ff8e2cb2047569ab5bf9ed16e75d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2827 | Contributors (this commit): 28 | Commits (past 90d): 73 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -945,14 +945,14 @@ class Lambda(Layer):\n         if self._ouput_shape is None:\n             return self.input_shape\n         elif type(self._output_shape) == tuple:\n-            return self.input_shape[0] + self._output_shape\n+            return (self.input_shape[0], ) + self._output_shape\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n             shape = output_shape_func(self.previous.output_shape[1:])\n             if type(shape) not in {list, tuple}:\n                 raise Exception(\"output_shape function must return a tuple\")\n-            return self.input_shape[0] + tuple(shape)\n+            return (self.input_shape[0], ) + tuple(shape)\n \n     def get_output(self, train=False):\n         func = marshal.loads(self.function)\n@@ -1022,14 +1022,14 @@ class LambdaMerge(Lambda):\n         if self._output_shape is None:\n             return input_shapes[0]\n         elif type(self._output_shape) == tuple:\n-            return input_shapes[0][0] + self._output_shape\n+            return (input_shapes[0][0], ) + self._output_shape\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n             shape = output_shape_func([shape[1:] for shape in input_shapes])\n             if type(shape) not in {list, tuple}:\n                 raise Exception(\"output_shape function must return a tuple\")\n-            return input_shapes[0][0] + tuple(shape)\n+            return (input_shapes[0][0], ) + tuple(shape)\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#17d2f5f9605cb103883e45c70748f34f2c743155", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 3 | Churn Cumulative: 541 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -347,8 +347,7 @@ class TestSequential(unittest.TestCase):\n         assert(loss == nloss)\n         \n         print ('test serializing')\n-        func = None # Make sure that the model has the function code, not just the function name.\n-        activation = None\n+        del func, activation # Make sure that the model has the function code, not just the function name.\n         sys.setrecursionlimit(50000)\n         model_str = pickle.dumps(model)\n         model = pickle.loads(model_str)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1dc1d25a580f888f02c9449535c16b2d8dbb1549", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 205 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 209 | Churn Cumulative: 420 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.4666666666666667\n\nDIFF:\n@@ -0,0 +1,201 @@\n+from __future__ import print_function\n+from keras.models import Sequential\n+from keras.layers.embeddings import Embedding\n+from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n+from keras.layers.recurrent import LSTM\n+from keras.datasets.data_utils import get_file\n+from keras.preprocessing.sequence import pad_sequences\n+import tarfile\n+import numpy as np\n+import re\n+\n+\"\"\"\n+Train a memory network on the bAbI dataset.\n+\n+References:\n+- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n+  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n+  http://arxiv.org/abs/1503.08895\n+\n+- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n+  \"End-To-End Memory Networks\",\n+  http://arxiv.org/abs/1503.08895\n+\n+Reaches 93% accuracy on task 'single_supporting_fact_10k' after 70 epochs.\n+Time per epoch: 3s on CPU (core i7).\n+\"\"\"\n+\n+\n+def tokenize(sent):\n+    '''Return the tokens of a sentence including punctuation.\n+\n+    >>> tokenize('Bob dropped the apple. Where is the apple?')\n+    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n+    '''\n+    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n+\n+\n+def parse_stories(lines, only_supporting=False):\n+    '''Parse stories provided in the bAbi tasks format\n+\n+    If only_supporting is true, only the sentences that support the answer are kept.\n+    '''\n+    data = []\n+    story = []\n+    for line in lines:\n+        line = line.decode('utf-8').strip()\n+        nid, line = line.split(' ', 1)\n+        nid = int(nid)\n+        if nid == 1:\n+            story = []\n+        if '\\t' in line:\n+            q, a, supporting = line.split('\\t')\n+            q = tokenize(q)\n+            substory = None\n+            if only_supporting:\n+                # Only select the related substory\n+                supporting = map(int, supporting.split())\n+                substory = [story[i - 1] for i in supporting]\n+            else:\n+                # Provide all the substories\n+                substory = [x for x in story if x]\n+            data.append((substory, q, a))\n+            story.append('')\n+        else:\n+            sent = tokenize(line)\n+            story.append(sent)\n+    return data\n+\n+\n+def get_stories(f, only_supporting=False, max_length=None):\n+    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n+\n+    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n+    '''\n+    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n+    flatten = lambda data: reduce(lambda x, y: x + y, data)\n+    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n+    return data\n+\n+\n+def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n+    X = []\n+    Xq = []\n+    Y = []\n+    for story, query, answer in data:\n+        x = [word_idx[w] for w in story]\n+        xq = [word_idx[w] for w in query]\n+        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n+        y[word_idx[answer]] = 1\n+        X.append(x)\n+        Xq.append(xq)\n+        Y.append(y)\n+    return (pad_sequences(X, maxlen=story_maxlen),\n+            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n+\n+\n+path = get_file('babi-tasks-v1-2.tar.gz',\n+                origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n+tar = tarfile.open(path)\n+\n+challenges = {\n+    # QA1 with 10,000 samples\n+    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n+    # QA2 with 10,000 samples\n+    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n+}\n+challenge_type = 'single_supporting_fact_10k'\n+challenge = challenges[challenge_type]\n+\n+print('Extracting stories for the challenge:', challenge_type)\n+train_stories = get_stories(tar.extractfile(challenge.format('train')))\n+test_stories = get_stories(tar.extractfile(challenge.format('test')))\n+\n+vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train_stories + test_stories)))\n+# Reserve 0 for masking via pad_sequences\n+vocab_size = len(vocab) + 1\n+story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n+query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n+\n+print('-')\n+print('Vocab size:', vocab_size, 'unique words')\n+print('Story max length:', story_maxlen, 'words')\n+print('Query max length:', query_maxlen, 'words')\n+print('Number of training stories:', len(train_stories))\n+print('Number of test stories:', len(test_stories))\n+print('-')\n+print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n+print(train_stories[0])\n+print('-')\n+print('Vectorizing the word sequences...')\n+\n+word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n+inputs_train, queries_train, answers_train = vectorize_stories(train_stories, word_idx, story_maxlen, query_maxlen)\n+inputs_test, queries_test, answers_test = vectorize_stories(test_stories, word_idx, story_maxlen, query_maxlen)\n+\n+print('-')\n+print('inputs: integer tensor of shape (samples, max_length)')\n+print('inputs_train shape:', inputs_train.shape)\n+print('inputs_test shape:', inputs_test.shape)\n+print('-')\n+print('queries: integer tensor of shape (samples, max_length)')\n+print('queries_train shape:', queries_train.shape)\n+print('queries_test shape:', queries_test.shape)\n+print('-')\n+print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n+print('answers_train shape:', answers_train.shape)\n+print('answers_test shape:', answers_test.shape)\n+print('-')\n+print('Compiling...')\n+\n+# embed the input sequence into a sequence of vectors\n+input_encoder_m = Sequential()\n+input_encoder_m.add(Embedding(input_dim=vocab_size,\n+                              output_dim=64,\n+                              input_length=story_maxlen))\n+# output: (samples, story_maxlen, embedding_dim)\n+# embed the question into a single vector\n+question_encoder = Sequential()\n+question_encoder.add(Embedding(input_dim=vocab_size,\n+                               output_dim=64,\n+                               input_length=query_maxlen))\n+# output: (samples, query_maxlen, embedding_dim)\n+# compute a 'match' between input sequence elements (which are vectors)\n+# and the question vector\n+match = Sequential()\n+match.add(Merge([input_encoder_m, question_encoder],\n+                mode='dot',\n+                dot_axes=[(2,), (2,)]))\n+# output: (samples, story_maxlen, query_maxlen)\n+# embed the input into a single vector with size = story_maxlen:\n+input_encoder_c = Sequential()\n+input_encoder_c.add(Embedding(input_dim=vocab_size,\n+                              output_dim=query_maxlen,\n+                              input_length=story_maxlen))\n+# output: (samples, story_maxlen, query_maxlen)\n+# sum the match vector with the input vector:\n+response = Sequential()\n+response.add(Merge([match, input_encoder_c], mode='sum'))\n+# output: (samples, story_maxlen, query_maxlen)\n+response.add(Permute((2, 1)))  # output: (samples, query_maxlen, story_maxlen)\n+\n+# concatenate the match vector with the question vector,\n+# and do logistic regression on top\n+answer = Sequential()\n+answer.add(Merge([response, question_encoder], mode='concat', concat_axis=-1))\n+# the original paper uses a matrix multiplication for this reduction step.\n+# we choose to use a RNN instead.\n+answer.add(LSTM(64))\n+# one regularization layer -- more would probably be needed.\n+answer.add(Dropout(0.25))\n+answer.add(Dense(vocab_size))\n+# we output a probability distribution over the vocabulary\n+answer.add(Activation('softmax'))\n+\n+answer.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n+# Note: you could use a Graph model to avoid repeat the input twice\n+answer.fit([inputs_train, queries_train, inputs_train], answers_train,\n+           batch_size=32,\n+           nb_epoch=70,\n+           show_accuracy=True,\n+           validation_data=([inputs_test, queries_test, inputs_test], answers_test))\n\n@@ -126,14 +126,14 @@ def get_stories(f, only_supporting=False, max_length=None):\n     return data\n \n \n-def vectorize_stories(data):\n+def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n     X = []\n     Xq = []\n     Y = []\n     for story, query, answer in data:\n         x = [word_idx[w] for w in story]\n         xq = [word_idx[w] for w in query]\n-        y = np.zeros(vocab_size)\n+        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n         y[word_idx[answer]] = 1\n         X.append(x)\n         Xq.append(xq)\n@@ -168,8 +168,8 @@ word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n \n-X, Xq, Y = vectorize_stories(train)\n-tX, tXq, tY = vectorize_stories(test)\n+X, Xq, Y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n+tX, tXq, tY = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n \n print('vocab = {}'.format(vocab))\n print('X.shape = {}'.format(X.shape))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9596ae68d693b45a087321b615ee517415c1cb5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 205 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 209 | Churn Cumulative: 629 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.4666666666666667\n\nDIFF:\n@@ -0,0 +1,201 @@\n+from __future__ import print_function\n+from keras.models import Sequential\n+from keras.layers.embeddings import Embedding\n+from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n+from keras.layers.recurrent import LSTM\n+from keras.datasets.data_utils import get_file\n+from keras.preprocessing.sequence import pad_sequences\n+import tarfile\n+import numpy as np\n+import re\n+\n+\"\"\"\n+Train a memory network on the bAbI dataset.\n+\n+References:\n+- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n+  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n+  http://arxiv.org/abs/1503.08895\n+\n+- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n+  \"End-To-End Memory Networks\",\n+  http://arxiv.org/abs/1503.08895\n+\n+Reaches 93% accuracy on task 'single_supporting_fact_10k' after 70 epochs.\n+Time per epoch: 3s on CPU (core i7).\n+\"\"\"\n+\n+\n+def tokenize(sent):\n+    '''Return the tokens of a sentence including punctuation.\n+\n+    >>> tokenize('Bob dropped the apple. Where is the apple?')\n+    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n+    '''\n+    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n+\n+\n+def parse_stories(lines, only_supporting=False):\n+    '''Parse stories provided in the bAbi tasks format\n+\n+    If only_supporting is true, only the sentences that support the answer are kept.\n+    '''\n+    data = []\n+    story = []\n+    for line in lines:\n+        line = line.decode('utf-8').strip()\n+        nid, line = line.split(' ', 1)\n+        nid = int(nid)\n+        if nid == 1:\n+            story = []\n+        if '\\t' in line:\n+            q, a, supporting = line.split('\\t')\n+            q = tokenize(q)\n+            substory = None\n+            if only_supporting:\n+                # Only select the related substory\n+                supporting = map(int, supporting.split())\n+                substory = [story[i - 1] for i in supporting]\n+            else:\n+                # Provide all the substories\n+                substory = [x for x in story if x]\n+            data.append((substory, q, a))\n+            story.append('')\n+        else:\n+            sent = tokenize(line)\n+            story.append(sent)\n+    return data\n+\n+\n+def get_stories(f, only_supporting=False, max_length=None):\n+    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n+\n+    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n+    '''\n+    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n+    flatten = lambda data: reduce(lambda x, y: x + y, data)\n+    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n+    return data\n+\n+\n+def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n+    X = []\n+    Xq = []\n+    Y = []\n+    for story, query, answer in data:\n+        x = [word_idx[w] for w in story]\n+        xq = [word_idx[w] for w in query]\n+        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n+        y[word_idx[answer]] = 1\n+        X.append(x)\n+        Xq.append(xq)\n+        Y.append(y)\n+    return (pad_sequences(X, maxlen=story_maxlen),\n+            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n+\n+\n+path = get_file('babi-tasks-v1-2.tar.gz',\n+                origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n+tar = tarfile.open(path)\n+\n+challenges = {\n+    # QA1 with 10,000 samples\n+    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n+    # QA2 with 10,000 samples\n+    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n+}\n+challenge_type = 'single_supporting_fact_10k'\n+challenge = challenges[challenge_type]\n+\n+print('Extracting stories for the challenge:', challenge_type)\n+train_stories = get_stories(tar.extractfile(challenge.format('train')))\n+test_stories = get_stories(tar.extractfile(challenge.format('test')))\n+\n+vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train_stories + test_stories)))\n+# Reserve 0 for masking via pad_sequences\n+vocab_size = len(vocab) + 1\n+story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n+query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n+\n+print('-')\n+print('Vocab size:', vocab_size, 'unique words')\n+print('Story max length:', story_maxlen, 'words')\n+print('Query max length:', query_maxlen, 'words')\n+print('Number of training stories:', len(train_stories))\n+print('Number of test stories:', len(test_stories))\n+print('-')\n+print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n+print(train_stories[0])\n+print('-')\n+print('Vectorizing the word sequences...')\n+\n+word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n+inputs_train, queries_train, answers_train = vectorize_stories(train_stories, word_idx, story_maxlen, query_maxlen)\n+inputs_test, queries_test, answers_test = vectorize_stories(test_stories, word_idx, story_maxlen, query_maxlen)\n+\n+print('-')\n+print('inputs: integer tensor of shape (samples, max_length)')\n+print('inputs_train shape:', inputs_train.shape)\n+print('inputs_test shape:', inputs_test.shape)\n+print('-')\n+print('queries: integer tensor of shape (samples, max_length)')\n+print('queries_train shape:', queries_train.shape)\n+print('queries_test shape:', queries_test.shape)\n+print('-')\n+print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n+print('answers_train shape:', answers_train.shape)\n+print('answers_test shape:', answers_test.shape)\n+print('-')\n+print('Compiling...')\n+\n+# embed the input sequence into a sequence of vectors\n+input_encoder_m = Sequential()\n+input_encoder_m.add(Embedding(input_dim=vocab_size,\n+                              output_dim=64,\n+                              input_length=story_maxlen))\n+# output: (samples, story_maxlen, embedding_dim)\n+# embed the question into a single vector\n+question_encoder = Sequential()\n+question_encoder.add(Embedding(input_dim=vocab_size,\n+                               output_dim=64,\n+                               input_length=query_maxlen))\n+# output: (samples, query_maxlen, embedding_dim)\n+# compute a 'match' between input sequence elements (which are vectors)\n+# and the question vector\n+match = Sequential()\n+match.add(Merge([input_encoder_m, question_encoder],\n+                mode='dot',\n+                dot_axes=[(2,), (2,)]))\n+# output: (samples, story_maxlen, query_maxlen)\n+# embed the input into a single vector with size = story_maxlen:\n+input_encoder_c = Sequential()\n+input_encoder_c.add(Embedding(input_dim=vocab_size,\n+                              output_dim=query_maxlen,\n+                              input_length=story_maxlen))\n+# output: (samples, story_maxlen, query_maxlen)\n+# sum the match vector with the input vector:\n+response = Sequential()\n+response.add(Merge([match, input_encoder_c], mode='sum'))\n+# output: (samples, story_maxlen, query_maxlen)\n+response.add(Permute((2, 1)))  # output: (samples, query_maxlen, story_maxlen)\n+\n+# concatenate the match vector with the question vector,\n+# and do logistic regression on top\n+answer = Sequential()\n+answer.add(Merge([response, question_encoder], mode='concat', concat_axis=-1))\n+# the original paper uses a matrix multiplication for this reduction step.\n+# we choose to use a RNN instead.\n+answer.add(LSTM(64))\n+# one regularization layer -- more would probably be needed.\n+answer.add(Dropout(0.25))\n+answer.add(Dense(vocab_size))\n+# we output a probability distribution over the vocabulary\n+answer.add(Activation('softmax'))\n+\n+answer.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n+# Note: you could use a Graph model to avoid repeat the input twice\n+answer.fit([inputs_train, queries_train, inputs_train], answers_train,\n+           batch_size=32,\n+           nb_epoch=70,\n+           show_accuracy=True,\n+           validation_data=([inputs_test, queries_test, inputs_test], answers_test))\n\n@@ -126,14 +126,14 @@ def get_stories(f, only_supporting=False, max_length=None):\n     return data\n \n \n-def vectorize_stories(data):\n+def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n     X = []\n     Xq = []\n     Y = []\n     for story, query, answer in data:\n         x = [word_idx[w] for w in story]\n         xq = [word_idx[w] for w in query]\n-        y = np.zeros(vocab_size)\n+        y = np.zeros(len(word_idx) + 1)  # let's not forget that index 0 is reserved\n         y[word_idx[answer]] = 1\n         X.append(x)\n         Xq.append(xq)\n@@ -168,8 +168,8 @@ word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n \n-X, Xq, Y = vectorize_stories(train)\n-tX, tXq, tY = vectorize_stories(test)\n+X, Xq, Y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n+tX, tXq, tY = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n \n print('vocab = {}'.format(vocab))\n print('X.shape = {}'.format(X.shape))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91ea495b56b634a40168ad9d7489b677b3dc6f89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 403 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n from keras.layers.recurrent import LSTM\n from keras.datasets.data_utils import get_file\n from keras.preprocessing.sequence import pad_sequences\n+from functools import reduce\n import tarfile\n import numpy as np\n import re\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db60707cb8c1461b9437418685e66891cbf4efc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -24/0 | Churn Δ: 7 | Churn Cumulative: 2834 | Contributors (this commit): 29 | Commits (past 90d): 74 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -257,10 +257,11 @@ class TimeDistributedMerge(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        if self.mode == 'sum' or self.mode == 'ave':\n-            s = theano.tensor.sum(X, axis=1)\n         if self.mode == 'ave':\n-                s /= X.shape[1]\n+            s = theano.tensor.mean(X, axis=1)\n+            return s\n+        if self.mode == 'sum':\n+            s = theano.tensor.sum(X, axis=1)\n             return s\n         elif self.mode == 'mul':\n             s = theano.tensor.mul(X, axis=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e4e317bcb4a82cf873e0da9a799ee61155236de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 484 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 20 | Complexity Δ (Sum/Max): 40/34 | Churn Δ: 484 | Churn Cumulative: 484 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.8613445378151261\n\nDIFF:\n@@ -0,0 +1,164 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(123)\n+import matplotlib.pyplot as plt\n+\n+from theano import function\n+\n+from keras.models import Sequential\n+from keras.layers.core import TimeDistributedDense, Activation\n+from keras.layers.recurrent import LSTM\n+from keras.optimizers import Adam\n+\n+from seya.sandbox.ntm import NeuralTuringMachine as NTM\n+\n+\"\"\"\n+Copy Problem defined in Graves et. al [0]\n+After 3000 gradients updates, the accuracy becomes >92%.\n+\n+\n+[0]: http://arxiv.org/pdf/1410.5401v2.pdf\n+\"\"\"\n+\n+batch_size = 100\n+\n+h_dim = 128\n+n_slots = 121\n+m_length = 20\n+input_dim = 8\n+lr = 1e-3\n+clipnorm = 10\n+\n+##### Neural Turing Machine ######\n+\n+ntm = NTM(h_dim, n_slots=n_slots, m_length=m_length, shift_range=3,\n+          inner_rnn='lstm', return_sequences=True, input_dim=input_dim)\n+model = Sequential()\n+model.add(ntm)\n+model.add(TimeDistributedDense(input_dim))\n+model.add(Activation('sigmoid'))\n+\n+sgd = Adam(lr=lr, clipnorm=clipnorm)\n+model.compile(loss='binary_crossentropy', optimizer=sgd)\n+\n+# LSTM - Run this for comparison\n+\n+# sgd = Adam(lr=lr, clipnorm=clipnorm)\n+#\n+# model4 = Sequential()\n+# model4.add(LSTM(input_dim=input_dim, output_dim=h_dim*2, return_sequences=True))\n+# model4.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n+# model4.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n+# model4.add(TimeDistributedDense(input_dim))\n+# model4.add(Activation('sigmoid'))\n+#\n+# model4.compile(loss='binary_crossentropy', optimizer=sgd)\n+\n+###### DATASET ########\n+\n+def get_sample(batch_size=128, n_bits=8, max_size=20, min_size=1):\n+    # generate samples with random length\n+    inp = np.zeros((batch_size, 2*max_size-1, n_bits))\n+    out = np.zeros((batch_size, 2*max_size-1, n_bits))\n+    sw = np.zeros((batch_size, 2*max_size-1, 1))\n+    for i in range(batch_size):\n+        t = np.random.randint(low=min_size, high=max_size)\n+        x = np.random.uniform(size=(t, n_bits)) > .5\n+        for j,f in enumerate(x.sum(axis=-1)): # remove fake flags\n+            if f>=n_bits:\n+                x[j, :] = 0.\n+        del_flag = np.ones((1, n_bits))\n+        inp[i, :t+1] = np.concatenate([x, del_flag], axis=0)\n+        out[i, t+1:(2*t+1)] = x\n+        sw[i, t+1:(2*t+1)] = 1\n+    return inp, out, sw\n+\n+def show_pattern(inp, out, sw, file_name='ntm_output.png'):\n+    ''' Helper function to visualize results '''\n+    plt.figure(figsize=(10, 10))\n+    plt.subplot(131)\n+    plt.imshow(inp>.5)\n+    plt.subplot(132)\n+    plt.imshow(out>.5)\n+    plt.subplot(133)\n+    plt.imshow(sw>.5)\n+    plt.savefig(file_name)\n+    plt.close()\n+\n+# Show data example:\n+inp, out, sw = get_sample(1, 8, 20)\n+\n+plt.subplot(131)\n+plt.title('input')\n+plt.imshow(inp[0], cmap='gray')\n+plt.subplot(132)\n+plt.title('desired')\n+plt.imshow(out[0], cmap='gray')\n+plt.subplot(133)\n+plt.title('sample_weight')\n+plt.imshow(sw[0], cmap='gray')\n+\n+# training uses sequences of length 1 to 20. Test uses series of length 100.\n+def test_model(model, file_name, min_size=100):\n+    I, V, sw = get_sample(batch_size=500, n_bits=input_dim, max_size=min_size+1, min_size=min_size)\n+    Y = np.asarray(model.predict(I, batch_size=100) > .5).astype('float64')\n+    acc = (V[:, -min_size:, :] == Y[:, -min_size:, :]).mean() * 100\n+    show_pattern(Y[0], V[0], sw[0], file_name)\n+    return acc\n+\n+##### TRAIN ######\n+nb_epoch = 4000\n+progbar = generic_utils.Progbar(nb_epoch)\n+ACC = []\n+for e in range(nb_epoch):\n+    I, V, sw = get_sample(n_bits=input_dim, max_size=20, min_size=1, batch_size=100)\n+\n+    loss = trained.train_on_batch(I, V, sample_weight=sw)\n+\n+    progbar.add(1, values=[(\"loss\", loss)])\n+\n+    if e % 500 == 0:\n+        print(\"\")\n+        acc = test_model(trained, filename)\n+        l = []\n+        for a in [acc,]:\n+            print(\"acc: {}\".format(a))\n+            l.append(a)\n+        ACC.append(l)\n+\n+##### VISUALIZATION #####\n+X = model.get_input()\n+Y = ntm.get_full_output()[0:3] # (memory over time, read_vectors, write_vectors)\n+F = function([X], Y, allow_input_downcast=True)\n+\n+inp, out, sw = get_sample(1, 8, 21, 20)\n+mem, read, write = F(inp.astype('float32'))\n+Y = model.predict(inp)\n+\n+plt.figure(figsize=(15, 12))\n+\n+plt.subplot(221)\n+plt.imshow(write[0])\n+plt.xlabel('memory location')\n+plt.ylabel('time')\n+plt.title('write')\n+\n+plt.subplot(222)\n+plt.imshow(read[0])\n+plt.title('read')\n+\n+plt.subplot(223)\n+plt.title('desired')\n+plt.imshow(out[0])\n+\n+plt.subplot(224)\n+plt.imshow(Y[0]>.5)\n+plt.title('output')\n+\n+plt.figure(figsize=(15, 10))\n+plt.subplot(325)\n+plt.ylabel('time')\n+plt.xlabel('location')\n+plt.title('memory evolving in time (avg value per location)')\n+plt.imshow(mem[0].mean(axis=-1))\n\n@@ -0,0 +1,320 @@\n+import numpy as np\n+from scipy.linalg import circulant\n+\n+import theano\n+import theano.tensor as T\n+floatX = theano.config.floatX\n+\n+from keras.layers.recurrent import Recurrent, GRU, LSTM\n+from keras.utils.theano_utils import shared_zeros, alloc_zeros_matrix, shared_scalar\n+tol = 1e-4\n+\n+\n+def _update_controller(self, inp, h_tm1, M, mask):\n+    \"\"\"We have to update the inner RNN inside the NTM, this\n+    is the function to do it. Pretty much copy+pasta from Keras\n+    \"\"\"\n+    x = T.concatenate([inp, M], axis=-1)\n+    # get inputs\n+    if self.inner_rnn == 'gru':\n+        x_z = T.dot(x, self.rnn.W_z) + self.rnn.b_z\n+        x_r = T.dot(x, self.rnn.W_r) + self.rnn.b_r\n+        x_h = T.dot(x, self.rnn.W_h) + self.rnn.b_h\n+\n+    elif self.inner_rnn == 'lstm':\n+        xi = T.dot(x, self.rnn.W_i) + self.rnn.b_i\n+        xf = T.dot(x, self.rnn.W_f) + self.rnn.b_f\n+        xc = T.dot(x, self.rnn.W_c) + self.rnn.b_c\n+        xo = T.dot(x, self.rnn.W_o) + self.rnn.b_o\n+\n+    elif self.inner_rnn == 'simple':\n+        x = T.dot(x, self.rnn.W) + self.rnn.b\n+\n+    # update state\n+    if self.inner_rnn == 'gru':\n+        h = self.rnn._step(x_z, x_r, x_h, 1., h_tm1[0],\n+                           self.rnn.U_z,\n+                           self.rnn.U_r,\n+                           self.rnn.U_h)\n+        h = mask[:, None] * h + (1-mask[:, None])*h_tm1[0]\n+        h = (h, )\n+\n+    elif self.inner_rnn == 'lstm':\n+        h = self.rnn._step(xi, xf, xo, xc, 1.,\n+                           h_tm1[1], h_tm1[0],\n+                           self.rnn.U_i, self.rnn.U_f,\n+                           self.rnn.U_o, self.rnn.U_c)\n+        h = h[::-1]\n+        h = tuple([mask[:, None]*h[i] +\n+                   (1-mask[:, None])*h_tm1[i] for i in range(len(h))])\n+\n+    elif self.inner_rnn == 'simple':\n+        h = self.rnn._step(x, 1, h_tm1[0], self.rnn.U)\n+        h = mask[:, None] * h + (1-mask[:, None])*h_tm1[0]\n+        h = (h, )\n+\n+    return h\n+\n+\n+def _circulant(leng, n_shifts):\n+    \"\"\"\n+    I confess, I'm actually proud of this hack. I hope you enjoy!\n+    This will generate a tensor with `n_shifts` of rotated versions the\n+    identity matrix. When this tensor is multiplied by a vector\n+    the result are `n_shifts` shifted versions of that vector. Since\n+    everything is done with inner products, everything is differentiable.\n+\n+    Paramters:\n+    ----------\n+    leng: int > 0, number of memory locations\n+    n_shifts: int > 0, number of allowed shifts (if 1, no shift)\n+\n+    Returns:\n+    --------\n+    shift operation, a tensor with dimensions (n_shifts, leng, leng)\n+    \"\"\"\n+    eye = np.eye(leng)\n+    shifts = range(n_shifts//2, -n_shifts//2, -1)\n+    C = np.asarray([np.roll(eye, s, axis=1) for s in shifts])\n+    return theano.shared(C.astype(theano.config.floatX))\n+\n+\n+def _renorm(x):\n+    return x / (x.sum(axis=1, keepdims=True))\n+\n+\n+def _softmax(x):\n+    wt = x.flatten(ndim=2)\n+    w = T.nnet.softmax(wt)\n+    return w.reshape(x.shape)  # T.clip(s, 0, 1)\n+\n+\n+def _cosine_distance(M, k):\n+    dot = (M * k[:, None, :]).sum(axis=-1)\n+    nM = T.sqrt((M**2).sum(axis=-1))\n+    nk = T.sqrt((k**2).sum(axis=-1, keepdims=True))\n+    return dot / (nM * nk)\n+\n+\n+class NeuralTuringMachine(Recurrent):\n+    \"\"\" Neural Turing Machines\n+\n+    Non obvious parameter:\n+    ----------------------\n+    shift_range: int, number of available shifts, ex. if 3, avilable shifts are\n+                 (-1, 0, 1)\n+    n_slots: number of memory locations\n+    m_length: memory length at each location\n+\n+    Known issues:\n+    -------------\n+    Theano may complain when n_slots == 1.\n+\n+    \"\"\"\n+    def __init__(self, output_dim, n_slots, m_length, shift_range=3,\n+                 inner_rnn='gru', truncate_gradient=-1, return_sequences=False,\n+                 init='glorot_uniform', inner_init='orthogonal',\n+                 input_dim=None, input_length=None, **kwargs):\n+        self.output_dim = output_dim\n+        self.n_slots = n_slots\n+        self.m_length = m_length\n+        self.shift_range = shift_range\n+        self.init = init\n+        self.inner_init = inner_init\n+        self.inner_rnn = inner_rnn\n+        self.return_sequences = return_sequences\n+        self.truncate_gradient = truncate_gradient\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n+        super(NeuralTuringMachine, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_leng, input_dim = self.input_shape[1:]\n+        self.input = T.tensor3()\n+\n+        if self.inner_rnn == 'gru':\n+            self.rnn = GRU(\n+                input_dim=input_dim+self.m_length,\n+                input_length=input_leng,\n+                output_dim=self.output_dim, init=self.init,\n+                inner_init=self.inner_init)\n+        elif self.inner_rnn == 'lstm':\n+            self.rnn = LSTM(\n+                input_dim=input_dim+self.m_length,\n+                input_length=input_leng,\n+                output_dim=self.output_dim, init=self.init,\n+                inner_init=self.inner_init)\n+        else:\n+            raise ValueError('this inner_rnn is not implemented yet.')\n+\n+        self.rnn.build()\n+\n+        # initial memory, state, read and write vecotrs\n+        self.M = theano.shared((.001*np.ones((1,)).astype(floatX)))\n+        self.init_h = shared_zeros((self.output_dim))\n+        self.init_wr = self.rnn.init((self.n_slots,))\n+        self.init_ww = self.rnn.init((self.n_slots,))\n+\n+        # write\n+        self.W_e = self.rnn.init((self.output_dim, self.m_length))  # erase\n+        self.b_e = shared_zeros((self.m_length))\n+        self.W_a = self.rnn.init((self.output_dim, self.m_length))  # add\n+        self.b_a = shared_zeros((self.m_length))\n+\n+        # get_w  parameters for reading operation\n+        self.W_k_read = self.rnn.init((self.output_dim, self.m_length))\n+        self.b_k_read = self.rnn.init((self.m_length, ))\n+        self.W_c_read = self.rnn.init((self.output_dim, 3))  # 3 = beta, g, gamma see eq. 5, 7, 9\n+        self.b_c_read = shared_zeros((3))\n+        self.W_s_read = self.rnn.init((self.output_dim, self.shift_range))\n+        self.b_s_read = shared_zeros((self.shift_range))  # b_s lol! not intentional\n+\n+        # get_w  parameters for writing operation\n+        self.W_k_write = self.rnn.init((self.output_dim, self.m_length))\n+        self.b_k_write = self.rnn.init((self.m_length, ))\n+        self.W_c_write = self.rnn.init((self.output_dim, 3))  # 3 = beta, g, gamma see eq. 5, 7, 9\n+        self.b_c_write = shared_zeros((3))\n+        self.W_s_write = self.rnn.init((self.output_dim, self.shift_range))\n+        self.b_s_write = shared_zeros((self.shift_range))\n+\n+        self.C = _circulant(self.n_slots, self.shift_range)\n+\n+        self.params = self.rnn.params + [\n+            self.W_e, self.b_e,\n+            self.W_a, self.b_a,\n+            self.W_k_read, self.b_k_read,\n+            self.W_c_read, self.b_c_read,\n+            self.W_s_read, self.b_s_read,\n+            self.W_k_write, self.b_k_write,\n+            self.W_s_write, self.b_s_write,\n+            self.W_c_write, self.b_c_write,\n+            self.M,\n+            self.init_h, self.init_wr, self.init_ww]\n+\n+        if self.inner_rnn == 'lstm':\n+            self.init_c = shared_zeros((self.output_dim))\n+            self.params = self.params + [self.init_c, ]\n+\n+    def _read(self, w, M):\n+        return (w[:, :, None]*M).sum(axis=1)\n+\n+    def _write(self, w, e, a, M, mask):\n+        Mtilda = M * (1 - w[:, :, None]*e[:, None, :])\n+        Mout = Mtilda + w[:, :, None]*a[:, None, :]\n+        return mask[:, None, None]*Mout + (1-mask[:, None, None])*M\n+\n+    def _get_content_w(self, beta, k, M):\n+        num = beta[:, None] * _cosine_distance(M, k)\n+        return _softmax(num)\n+\n+    def _get_location_w(self, g, s, C, gamma, wc, w_tm1, mask):\n+        wg = g[:, None] * wc + (1-g[:, None])*w_tm1\n+        Cs = (C[None, :, :, :] * wg[:, None, None, :]).sum(axis=3)\n+        wtilda = (Cs * s[:, :, None]).sum(axis=1)\n+        wout = _renorm(wtilda ** gamma[:, None])\n+        return mask[:, None] * wout + (1-mask[:, None])*w_tm1\n+\n+    def _get_controller_output(self, h, W_k, b_k, W_c, b_c, W_s, b_s):\n+        k = T.tanh(T.dot(h, W_k) + b_k)  # + 1e-6\n+        c = T.dot(h, W_c) + b_c\n+        beta = T.nnet.relu(c[:, 0]) + 1e-6\n+        g = T.nnet.sigmoid(c[:, 1])\n+        gamma = T.nnet.relu(c[:, 2]) + 1\n+        s = T.nnet.softmax(T.dot(h, W_s) + b_s)\n+        return k, beta, g, gamma, s\n+\n+    def _get_initial_states(self, batch_size):\n+        # init_M = self.M + T.unbroadcast(alloc_zeros_matrix(batch_size, self.n_slots, self.m_length), 0, 1, 2)\n+        # init_M = self.M.dimshuffle('x', 0, 1).repeat(batch_size, axis=0)\n+        init_M = self.M.dimshuffle(0, 'x', 'x').repeat(\n+           batch_size, axis=0).repeat(self.n_slots, axis=1).repeat(\n+               self.m_length, axis=2)\n+\n+        init_h = self.init_h.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n+        init_wr = self.init_wr.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n+        init_ww = self.init_ww.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n+        if self.inner_rnn == 'lstm':\n+            init_c = self.init_c.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n+            return init_M, T.nnet.softmax(init_wr), T.nnet.softmax(init_ww), init_h, init_c\n+        else:\n+            return init_M, T.nnet.softmax(init_wr), T.nnet.softmax(init_ww), init_h\n+\n+    def _step(self, x, mask, M_tm1, wr_tm1, ww_tm1, *args):\n+        # read\n+        if self.inner_rnn == 'lstm':\n+            h_tm1 = args[0:2][::-1]  # (cell_tm1, h_tm1)\n+        else:\n+            h_tm1 = args[0:1]  # (h_tm1, )\n+        k_read, beta_read, g_read, gamma_read, s_read = self._get_controller_output(\n+            h_tm1[-1], self.W_k_read, self.b_k_read, self.W_c_read, self.b_c_read,\n+            self.W_s_read, self.b_s_read)\n+        wc_read = self._get_content_w(beta_read, k_read, M_tm1)\n+        wr_t = self._get_location_w(g_read, s_read, self.C, gamma_read,\n+                                    wc_read, wr_tm1, mask)\n+        M_read = self._read(wr_t, M_tm1)\n+\n+        # update controller\n+        h_t = _update_controller(self, x, h_tm1, M_read, mask)\n+\n+        # write\n+        k_write, beta_write, g_write, gamma_write, s_write = self._get_controller_output(\n+            h_t[-1], self.W_k_write, self.b_k_write, self.W_c_write,\n+            self.b_c_write, self.W_s_write, self.b_s_write)\n+        wc_write = self._get_content_w(beta_write, k_write, M_tm1)\n+        ww_t = self._get_location_w(g_write, s_write, self.C, gamma_write,\n+                                    wc_write, ww_tm1, mask)\n+        e = T.nnet.sigmoid(T.dot(h_t[-1], self.W_e) + self.b_e)\n+        a = T.tanh(T.dot(h_t[-1], self.W_a) + self.b_a)\n+        M_t = self._write(ww_t, e, a, M_tm1, mask)\n+\n+        return (M_t, wr_t, ww_t) + h_t\n+\n+    def get_output(self, train=False):\n+        outputs = self.get_full_output(train)\n+\n+        if self.return_sequences:\n+            return outputs[-1]\n+        else:\n+            return outputs[-1][:, -1]\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        if self.return_sequences:\n+            return input_shape[0], input_shape[1], self.output_dim\n+        else:\n+            return input_shape[0], self.output_dim\n+\n+    def get_full_output(self, train=False):\n+        \"\"\"\n+        This method is for research and visualization purposes. Use it as\n+        X = model.get_input()  # full model\n+        Y = ntm.get_output()    # this layer\n+        F = theano.function([X], Y, allow_input_downcast=True)\n+        [memory, read_address, write_address, rnn_state] = F(x)\n+\n+        if inner_rnn == \"lstm\" use it as\n+        [memory, read_address, write_address, rnn_cell, rnn_state] = F(x)\n+\n+        \"\"\"\n+        X = self.get_input(train)\n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)[:, :, 0]\n+        X = X.dimshuffle((1, 0, 2))\n+\n+        init_states = self._get_initial_states(X.shape[1])\n+        outputs, updates = theano.scan(self._step,\n+                                       sequences=[X, padded_mask],\n+                                       outputs_info=init_states,\n+                                       non_sequences=self.params,\n+                                       truncate_gradient=self.truncate_gradient)\n+\n+        out = [outputs[0].dimshuffle((1, 0, 2, 3)),\n+               outputs[1].dimshuffle(1, 0, 2),\n+               outputs[2].dimshuffle((1, 0, 2)),\n+               outputs[3].dimshuffle((1, 0, 2))]\n+        if self.inner_rnn == 'lstm':\n+            out + [outputs[4].dimshuffle((1, 0, 2))]\n+        return out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
