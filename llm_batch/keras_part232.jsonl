{"custom_id": "keras#4c79e3e16c88b2714c894f0e73f6b2e536d74123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 710 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -208,7 +208,7 @@ def deeper2net_conv2d(teacher_w):\n     kh, kw, num_channel, filters = teacher_w.shape\n     student_w = np.zeros_like(teacher_w)\n     for i in range(filters):\n-        student_w[(kh - 1) / 2, (kw - 1) / 2, i, i] = 1.\n+        student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1.\n     student_b = np.zeros(filters)\n     return student_w, student_b\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e6c3f77b0b10b0d76778109a40d6d3282f1cadd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 258 | Lines Deleted: 89 | Files Changed: 6 | Hunks: 74 | Methods Changed: 31 | Complexity Δ (Sum/Max): 27/14 | Churn Δ: 347 | Churn Cumulative: 14130 | Contributors (this commit): 167 | Commits (past 90d): 77 | Contributors (cumulative): 211 | DMM Complexity: 0.5178571428571429\n\nDIFF:\n@@ -2039,8 +2039,8 @@ def batch_get_value(xs):\n def set_value(x, value):\n     if (isinstance(x, C.variables.Parameter) or\n        isinstance(x, C.variables.Constant)):\n-        if isinstance(value, float):\n-            value = np.full(x.shape, value)\n+        if isinstance(value, (float, int)):\n+            value = np.full(x.shape, value, dtype=floatx())\n         x.value = value\n     else:\n         raise NotImplementedError\n\n@@ -18,6 +18,7 @@ from collections import OrderedDict\n from collections import Iterable\n from .utils.generic_utils import Progbar\n from . import backend as K\n+from .engine.topology import Layer\n \n try:\n     import requests\n@@ -202,8 +203,20 @@ class BaseLogger(Callback):\n     \"\"\"Callback that accumulates epoch averages of metrics.\n \n     This callback is automatically applied to every Keras model.\n+\n+    # Arguments\n+        stateful_metrics: Iterable of string names of metrics that\n+            should *not* be averaged over an epoch.\n+            Metrics in this list will be logged as-is in `on_epoch_end`.\n+            All others will be averaged in `on_epoch_end`.\n     \"\"\"\n \n+    def __init__(self, stateful_metrics=None):\n+        if stateful_metrics:\n+            self.stateful_metrics = set(stateful_metrics)\n+        else:\n+            self.stateful_metrics = set()\n+\n     def on_epoch_begin(self, epoch, logs=None):\n         self.seen = 0\n         self.totals = {}\n@@ -214,6 +227,9 @@ class BaseLogger(Callback):\n         self.seen += batch_size\n \n         for k, v in logs.items():\n+            if k in self.stateful_metrics:\n+                self.totals[k] = v\n+            else:\n                 if k in self.totals:\n                     self.totals[k] += v * batch_size\n                 else:\n@@ -224,6 +240,9 @@ class BaseLogger(Callback):\n             for k in self.params['metrics']:\n                 if k in self.totals:\n                     # Make value available to next callbacks.\n+                    if k in self.stateful_metrics:\n+                        logs[k] = self.totals[k]\n+                    else:\n                         logs[k] = self.totals[k] / self.seen\n \n \n@@ -250,12 +269,17 @@ class ProgbarLogger(Callback):\n         count_mode: One of \"steps\" or \"samples\".\n             Whether the progress bar should\n             count samples seen or steps (batches) seen.\n+        stateful_metrics: Iterable of string names of metrics that\n+            should *not* be averaged over an epoch.\n+            Metrics in this list will be logged as-is.\n+            All others will be averaged over time (e.g. loss, etc).\n \n     # Raises\n         ValueError: In case of invalid `count_mode`.\n     \"\"\"\n \n-    def __init__(self, count_mode='samples'):\n+    def __init__(self, count_mode='samples',\n+                 stateful_metrics=None):\n         super(ProgbarLogger, self).__init__()\n         if count_mode == 'samples':\n             self.use_steps = False\n@@ -263,6 +287,10 @@ class ProgbarLogger(Callback):\n             self.use_steps = True\n         else:\n             raise ValueError('Unknown `count_mode`: ' + str(count_mode))\n+        if stateful_metrics:\n+            self.stateful_metrics = set(stateful_metrics)\n+        else:\n+            self.stateful_metrics = set()\n \n     def on_train_begin(self, logs=None):\n         self.verbose = self.params['verbose']\n@@ -277,7 +305,8 @@ class ProgbarLogger(Callback):\n                 target = self.params['samples']\n             self.target = target\n             self.progbar = Progbar(target=self.target,\n-                                   verbose=self.verbose)\n+                                   verbose=self.verbose,\n+                                   stateful_metrics=self.stateful_metrics)\n         self.seen = 0\n \n     def on_batch_begin(self, batch, logs=None):\n\n@@ -11,6 +11,7 @@ import numpy as np\n from scipy.sparse import issparse\n \n from .topology import Container\n+from .topology import Layer\n from .. import backend as K\n from .. import optimizers\n from .. import losses\n@@ -807,7 +808,7 @@ class Model(Container):\n                 self._feed_sample_weight_modes.append(self.sample_weight_modes[i])\n \n         # Prepare metrics.\n-        self.metrics = metrics\n+        self.metrics = metrics or []\n         self.weighted_metrics = weighted_metrics\n         self.metrics_names = ['loss']\n         self.metrics_tensors = []\n@@ -850,14 +851,8 @@ class Model(Container):\n         # contains tuples (metrics for output, names of metrics).\n         nested_metrics = _collect_metrics(metrics, self.output_names)\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n-\n-        def append_metric(layer_index, metric_name, metric_tensor):\n-            \"\"\"Helper function used in loop below.\"\"\"\n-            if len(self.output_names) > 1:\n-                metric_name = self.output_names[layer_index] + '_' + metric_name\n-            self.metrics_names.append(metric_name)\n-            self.metrics_tensors.append(metric_tensor)\n-\n+        self.metrics_updates = []\n+        self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:\n@@ -881,37 +876,60 @@ class Model(Container):\n                                self.loss_functions[i] == losses.binary_crossentropy):\n                                 # case: binary accuracy/crossentropy\n                                 if metric in ('accuracy', 'acc'):\n-                                    acc_fn = metrics_module.binary_accuracy\n+                                    metric_fn = metrics_module.binary_accuracy\n                                 elif metric in ('crossentropy', 'ce'):\n-                                    acc_fn = metrics_module.binary_crossentropy\n+                                    metric_fn = metrics_module.binary_crossentropy\n                             elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n                                 # case: categorical accuracy/crossentropy with sparse targets\n                                 if metric in ('accuracy', 'acc'):\n-                                    acc_fn = metrics_module.sparse_categorical_accuracy\n+                                    metric_fn = metrics_module.sparse_categorical_accuracy\n                                 elif metric in ('crossentropy', 'ce'):\n-                                    acc_fn = metrics_module.sparse_categorical_crossentropy\n+                                    metric_fn = metrics_module.sparse_categorical_crossentropy\n                             else:\n                                 # case: categorical accuracy/crossentropy\n                                 if metric in ('accuracy', 'acc'):\n-                                    acc_fn = metrics_module.categorical_accuracy\n+                                    metric_fn = metrics_module.categorical_accuracy\n                                 elif metric in ('crossentropy', 'ce'):\n-                                    acc_fn = metrics_module.categorical_crossentropy\n+                                    metric_fn = metrics_module.categorical_crossentropy\n                             if metric in ('accuracy', 'acc'):\n                                     suffix = 'acc'\n                             elif metric in ('crossentropy', 'ce'):\n                                     suffix = 'ce'\n-                            weighted_metric_fn = _weighted_masked_objective(acc_fn)\n+                            weighted_metric_fn = _weighted_masked_objective(metric_fn)\n                             metric_name = metric_name_prefix + suffix\n                         else:\n                             metric_fn = metrics_module.get(metric)\n                             weighted_metric_fn = _weighted_masked_objective(metric_fn)\n-                            metric_name = metric_name_prefix + metric_fn.__name__\n+                            # Get metric name as string\n+                            if hasattr(metric_fn, 'name'):\n+                                metric_name = metric_fn.name\n+                            else:\n+                                metric_name = metric_fn.__name__\n+                            metric_name = metric_name_prefix + metric_name\n \n                         with K.name_scope(metric_name):\n                             metric_result = weighted_metric_fn(y_true, y_pred,\n                                                                weights=weights,\n                                                                mask=masks[i])\n-                        append_metric(i, metric_name, metric_result)\n+\n+                        # Append to self.metrics_names, self.metric_tensors,\n+                        # self.stateful_metric_names\n+                        if len(self.output_names) > 1:\n+                            metric_name = self.output_names[i] + '_' + metric_name\n+                        # Dedupe name\n+                        j = 1\n+                        base_metric_name = metric_name\n+                        while metric_name in self.metrics_names:\n+                            metric_name = base_metric_name + '_' + str(j)\n+                            j += 1\n+                        self.metrics_names.append(metric_name)\n+                        self.metrics_tensors.append(metric_result)\n+\n+                        # Keep track of state updates created by\n+                        # stateful metrics (i.e. metrics layers).\n+                        if isinstance(metric_fn, Layer):\n+                            self.stateful_metric_names.append(metric_name)\n+                            self.metrics_updates += metric_fn.updates\n \n                 handle_metrics(output_metrics)\n                 handle_metrics(output_weighted_metrics, weights=weights)\n@@ -970,7 +988,7 @@ class Model(Container):\n                     training_updates = self.optimizer.get_updates(\n                         params=self._collected_trainable_weights,\n                         loss=self.total_loss)\n-                updates = self.updates + training_updates\n+                updates = self.updates + training_updates + self.metrics_updates\n                 # Gets loss and metrics. Updates weights at each call.\n                 self.train_function = K.function(inputs,\n                                                  [self.total_loss] + self.metrics_tensors,\n@@ -989,7 +1007,7 @@ class Model(Container):\n             # Does update the network states.\n             self.test_function = K.function(inputs,\n                                             [self.total_loss] + self.metrics_tensors,\n-                                            updates=self.state_updates,\n+                                            updates=self.state_updates + self.metrics_updates,\n                                             name='test_function',\n                                             **self._function_kwargs)\n \n@@ -1110,14 +1128,19 @@ class Model(Container):\n             index_array = np.arange(num_train_samples)\n \n         self.history = cbks.History()\n-        callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]\n+        _callbacks = [cbks.BaseLogger(\n+            stateful_metrics=self.stateful_metric_names)]\n         if verbose:\n             if steps_per_epoch is not None:\n                 count_mode = 'steps'\n             else:\n                 count_mode = 'samples'\n-            callbacks.insert(1, cbks.ProgbarLogger(count_mode))\n-        callbacks = cbks.CallbackList(callbacks)\n+            _callbacks.append(\n+                cbks.ProgbarLogger(\n+                    count_mode,\n+                    stateful_metrics=self.stateful_metric_names))\n+        _callbacks += (callbacks or []) + [self.history]\n+        callbacks = cbks.CallbackList(_callbacks)\n         out_labels = out_labels or []\n \n         # it's possible to callback a different model than self\n@@ -1150,6 +1173,10 @@ class Model(Container):\n                 indices_for_conversion_to_dense.append(i)\n \n         for epoch in range(initial_epoch, epochs):\n+            # Reset stateful metrics\n+            for m in self.metrics:\n+                if isinstance(m, Layer):\n+                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:\n@@ -1248,14 +1275,21 @@ class Model(Container):\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n         \"\"\"\n+\n+        if hasattr(self, 'metrics'):\n+            for m in self.metrics:\n+                if isinstance(m, Layer):\n+                    m.reset_states()\n         num_samples = self._check_num_samples(ins, batch_size,\n                                               steps,\n                                               'steps')\n         if verbose == 1:\n             if steps is not None:\n-                progbar = Progbar(target=steps)\n+                progbar = Progbar(target=steps,\n+                                  stateful_metrics=self.stateful_metric_names)\n             else:\n-                progbar = Progbar(target=num_samples)\n+                progbar = Progbar(target=num_samples,\n+                                  stateful_metrics=self.stateful_metric_names)\n \n         indices_for_conversion_to_dense = []\n         for i in range(len(self._feed_inputs)):\n@@ -1334,6 +1368,17 @@ class Model(Container):\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         \"\"\"\n+\n+        if hasattr(self, 'metrics'):\n+            for m in self.metrics:\n+                if isinstance(m, Layer):\n+                    m.reset_states()\n+            stateful_metric_indices = [\n+                i for i, name in enumerate(self.metrics_names)\n+                if str(name) in self.stateful_metric_names]\n+        else:\n+            stateful_metric_indices = []\n+\n         num_samples = self._check_num_samples(ins, batch_size,\n                                               steps,\n                                               'steps')\n@@ -1359,6 +1404,9 @@ class Model(Container):\n                         for _ in enumerate(batch_outs):\n                             outs.append(0.)\n                     for i, batch_out in enumerate(batch_outs):\n+                        if i in stateful_metric_indices:\n+                            outs[i] = batch_out\n+                        else:\n                             outs[i] += batch_out\n                 else:\n                     if step == 0:\n@@ -1367,6 +1415,7 @@ class Model(Container):\n                 if verbose == 1:\n                     progbar.update(step + 1)\n             for i in range(len(outs)):\n+                if i not in stateful_metric_indices:\n                     outs[i] /= steps\n         else:\n             batches = _make_batches(num_samples, batch_size)\n@@ -1387,6 +1436,9 @@ class Model(Container):\n                         for batch_out in enumerate(batch_outs):\n                             outs.append(0.)\n                     for i, batch_out in enumerate(batch_outs):\n+                        if i in stateful_metric_indices:\n+                            outs[i] = batch_out\n+                        else:\n                             outs[i] += batch_out * len(batch_ids)\n                 else:\n                     if batch_index == 0:\n@@ -1396,6 +1448,7 @@ class Model(Container):\n                 if verbose == 1:\n                     progbar.update(batch_end)\n             for i in range(len(outs)):\n+                if i not in stateful_metric_indices:\n                     outs[i] /= num_samples\n         if len(outs) == 1:\n             return outs[0]\n@@ -1454,20 +1507,6 @@ class Model(Container):\n                                  str(x[0].shape[0]) + ' samples')\n         return x, y, sample_weights\n \n-    def _get_deduped_metrics_names(self):\n-        out_labels = self.metrics_names\n-\n-        # Rename duplicated metrics name\n-        # (can happen with an output layer shared among multiple dataflows).\n-        deduped_out_labels = []\n-        for i, label in enumerate(out_labels):\n-            new_label = label\n-            if out_labels.count(label) > 1:\n-                dup_idx = out_labels[:i].count(label)\n-                new_label += '_' + str(dup_idx + 1)\n-            deduped_out_labels.append(new_label)\n-        return deduped_out_labels\n-\n     def fit(self,\n             x=None,\n             y=None,\n@@ -1651,7 +1690,7 @@ class Model(Container):\n         f = self.train_function\n \n         # Prepare display labels.\n-        out_labels = self._get_deduped_metrics_names()\n+        out_labels = self.metrics_names\n \n         if do_validation:\n             self._make_test_function()\n@@ -2079,15 +2118,20 @@ class Model(Container):\n                              ' the `keras.utils.Sequence` class.')\n \n         # Prepare display labels.\n-        out_labels = self._get_deduped_metrics_names()\n+        out_labels = self.metrics_names\n         callback_metrics = out_labels + ['val_' + n for n in out_labels]\n \n         # prepare callbacks\n         self.history = cbks.History()\n-        callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]\n+        _callbacks = [cbks.BaseLogger(\n+            stateful_metrics=self.stateful_metric_names)]\n         if verbose:\n-            callbacks.insert(1, cbks.ProgbarLogger(count_mode='steps'))\n-        callbacks = cbks.CallbackList(callbacks)\n+            _callbacks.append(\n+                cbks.ProgbarLogger(\n+                    count_mode='steps',\n+                    stateful_metrics=self.stateful_metric_names))\n+        _callbacks += (callbacks or []) + [self.history]\n+        callbacks = cbks.CallbackList(_callbacks)\n \n         # it's possible to callback a different model than self:\n         if hasattr(self, 'callback_model') and self.callback_model:\n\n@@ -20,6 +20,7 @@ from .losses import kullback_leibler_divergence\n from .losses import poisson\n from .losses import cosine_proximity\n from .utils.generic_utils import deserialize_keras_object\n+from .utils.generic_utils import serialize_keras_object\n \n \n def binary_accuracy(y_true, y_pred):\n@@ -56,20 +57,22 @@ cosine = cosine_proximity\n \n \n def serialize(metric):\n-    return metric.__name__\n+    return serialize_keras_object(metric)\n \n \n-def deserialize(name, custom_objects=None):\n-    return deserialize_keras_object(name,\n+def deserialize(config, custom_objects=None):\n+    return deserialize_keras_object(config,\n                                     module_objects=globals(),\n                                     custom_objects=custom_objects,\n                                     printable_module_name='metric function')\n \n \n def get(identifier):\n-    if isinstance(identifier, six.string_types):\n-        identifier = str(identifier)\n-        return deserialize(identifier)\n+    if isinstance(identifier, dict):\n+        config = {'class_name': str(identifier), 'config': {}}\n+        return deserialize(config)\n+    elif isinstance(identifier, six.string_types):\n+        return deserialize(str(identifier))\n     elif callable(identifier):\n         return identifier\n     else:\n\n@@ -13,6 +13,7 @@ import marshal\n import types as python_types\n import inspect\n import codecs\n+import collections\n \n _GLOBAL_CUSTOM_OBJECTS = {}\n \n@@ -286,23 +287,34 @@ class Progbar(object):\n \n     # Arguments\n         target: Total number of steps expected, None if unknown.\n+        width: Progress bar width on screen.\n+        verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n+        stateful_metrics: Iterable of string names of metrics that\n+            should *not* be averaged over time. Metrics in this list\n+            will be displayed as-is. All others will be averaged\n+            by the progbar before display.\n         interval: Minimum visual progress update interval (in seconds).\n     \"\"\"\n \n-    def __init__(self, target, width=30, verbose=1, interval=0.05):\n-        self.width = width\n+    def __init__(self, target, width=30, verbose=1, interval=0.05,\n+                 stateful_metrics=None):\n         self.target = target\n-        self.sum_values = {}\n-        self.unique_values = []\n-        self.start = time.time()\n-        self.last_update = 0\n-        self.interval = interval\n-        self.total_width = 0\n-        self.seen_so_far = 0\n+        self.width = width\n         self.verbose = verbose\n+        self.interval = interval\n+        if stateful_metrics:\n+            self.stateful_metrics = set(stateful_metrics)\n+        else:\n+            self.stateful_metrics = set()\n+\n         self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n                                   sys.stdout.isatty()) or\n                                  'ipykernel' in sys.modules)\n+        self._total_width = 0\n+        self._seen_so_far = 0\n+        self._values = collections.OrderedDict()\n+        self._start = time.time()\n+        self._last_update = 0\n \n     def update(self, current, values=None):\n         \"\"\"Updates the progress bar.\n@@ -311,27 +323,31 @@ class Progbar(object):\n             current: Index of current step.\n             values: List of tuples:\n                 `(name, value_for_last_step)`.\n-                The progress bar will display averages for these values.\n+                If `name` is in `stateful_metrics`,\n+                `value_for_last_step` will be displayed as-is.\n+                Else, an average of the metric over time will be displayed.\n         \"\"\"\n         values = values or []\n         for k, v in values:\n-            if k not in self.sum_values:\n-                self.sum_values[k] = [v * (current - self.seen_so_far),\n-                                      current - self.seen_so_far]\n-                self.unique_values.append(k)\n+            if k not in self.stateful_metrics:\n+                if k not in self._values:\n+                    self._values[k] = [v * (current - self._seen_so_far),\n+                                       current - self._seen_so_far]\n                 else:\n-                self.sum_values[k][0] += v * (current - self.seen_so_far)\n-                self.sum_values[k][1] += (current - self.seen_so_far)\n-        self.seen_so_far = current\n+                    self._values[k][0] += v * (current - self._seen_so_far)\n+                    self._values[k][1] += (current - self._seen_so_far)\n+            else:\n+                self._values[k] = v\n+        self._seen_so_far = current\n \n         now = time.time()\n-        info = ' - %.0fs' % (now - self.start)\n+        info = ' - %.0fs' % (now - self._start)\n         if self.verbose == 1:\n-            if (now - self.last_update < self.interval and\n+            if (now - self._last_update < self.interval and\n                     self.target is not None and current < self.target):\n                 return\n \n-            prev_total_width = self.total_width\n+            prev_total_width = self._total_width\n             if self._dynamic_display:\n                 sys.stdout.write('\\b' * prev_total_width)\n                 sys.stdout.write('\\r')\n@@ -355,11 +371,11 @@ class Progbar(object):\n             else:\n                 bar = '%7d/Unknown' % current\n \n-            self.total_width = len(bar)\n+            self._total_width = len(bar)\n             sys.stdout.write(bar)\n \n             if current:\n-                time_per_unit = (now - self.start) / current\n+                time_per_unit = (now - self._start) / current\n             else:\n                 time_per_unit = 0\n             if self.target is not None and current < self.target:\n@@ -380,21 +396,21 @@ class Progbar(object):\n                 else:\n                     info += ' %.0fus/step' % (time_per_unit * 1e6)\n \n-            for k in self.unique_values:\n+            for k in self._values:\n                 info += ' - %s:' % k\n-                if isinstance(self.sum_values[k], list):\n+                if isinstance(self._values[k], list):\n                     avg = np.mean(\n-                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n+                        self._values[k][0] / max(1, self._values[k][1]))\n                     if abs(avg) > 1e-3:\n                         info += ' %.4f' % avg\n                     else:\n                         info += ' %.4e' % avg\n                 else:\n-                    info += ' %s' % self.sum_values[k]\n+                    info += ' %s' % self._values[k]\n \n-            self.total_width += len(info)\n-            if prev_total_width > self.total_width:\n-                info += (' ' * (prev_total_width - self.total_width))\n+            self._total_width += len(info)\n+            if prev_total_width > self._total_width:\n+                info += (' ' * (prev_total_width - self._total_width))\n \n             if self.target is not None and current >= self.target:\n                 info += '\\n'\n@@ -404,10 +420,10 @@ class Progbar(object):\n \n         elif self.verbose == 2:\n             if self.target is None or current >= self.target:\n-                for k in self.unique_values:\n+                for k in self._values:\n                     info += ' - %s:' % k\n                     avg = np.mean(\n-                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n+                        self._values[k][0] / max(1, self._values[k][1]))\n                     if avg > 1e-3:\n                         info += ' %.4f' % avg\n                     else:\n@@ -417,7 +433,7 @@ class Progbar(object):\n                 sys.stdout.write(info)\n                 sys.stdout.flush()\n \n-        self.last_update = now\n+        self._last_update = now\n \n     def add(self, n, values=None):\n-        self.update(self.seen_so_far + n, values)\n+        self.update(self._seen_so_far + n, values)\n\n@@ -1,8 +1,10 @@\n import pytest\n import numpy as np\n \n+import keras\n from keras import metrics\n from keras import backend as K\n+from keras.utils.test_utils import keras_test\n \n all_metrics = [\n     metrics.binary_accuracy,\n@@ -26,6 +28,7 @@ all_sparse_metrics = [\n ]\n \n \n+@keras_test\n def test_metrics():\n     y_a = K.variable(np.random.random((6, 7)))\n     y_b = K.variable(np.random.random((6, 7)))\n@@ -35,6 +38,7 @@ def test_metrics():\n         assert K.eval(output).shape == (6,)\n \n \n+@keras_test\n def test_sparse_metrics():\n     for metric in all_sparse_metrics:\n         y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n@@ -66,7 +70,8 @@ def test_invalid_get():\n \n \n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"keras cntk backend does not support top_k yet\")\n+                    reason='CNTK backend does not support top_k yet')\n+@keras_test\n def test_top_k_categorical_accuracy():\n     y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n     y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n@@ -82,7 +87,8 @@ def test_top_k_categorical_accuracy():\n \n \n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"keras cntk backend does not support top_k yet\")\n+                    reason='CNTK backend does not support top_k yet')\n+@keras_test\n def test_sparse_top_k_categorical_accuracy():\n     y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n     y_true = K.variable(np.array([[1], [0]]))\n@@ -100,5 +106,76 @@ def test_sparse_top_k_categorical_accuracy():\n     assert failure_result == 0\n \n \n+@keras_test\n+def test_stateful_metrics():\n+    np.random.seed(1334)\n+\n+    class BinaryTruePositives(keras.layers.Layer):\n+        \"\"\"Stateful Metric to count the total true positives over all batches.\n+\n+        Assumes predictions and targets of shape `(samples, 1)`.\n+\n+        # Arguments\n+            threshold: Float, lower limit on prediction value that counts as a\n+                positive class prediction.\n+            name: String, name for the metric.\n+        \"\"\"\n+\n+        def __init__(self, name='true_positives', **kwargs):\n+            super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n+            self.true_positives = K.variable(value=0, dtype='int32')\n+\n+        def reset_states(self):\n+            K.set_value(self.true_positives, 0)\n+\n+        def __call__(self, y_true, y_pred):\n+            \"\"\"Computes the number of true positives in a batch.\n+\n+            # Arguments\n+                y_true: Tensor, batch_wise labels\n+                y_pred: Tensor, batch_wise predictions\n+\n+            # Returns\n+                The total number of true positives seen this epoch at the\n+                    completion of the batch.\n+            \"\"\"\n+            y_true = K.cast(y_true, 'int32')\n+            y_pred = K.cast(K.round(y_pred), 'int32')\n+            correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')\n+            true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')\n+            current_true_pos = self.true_positives * 1\n+            self.add_update(K.update_add(self.true_positives,\n+                                         true_pos),\n+                            inputs=[y_true, y_pred])\n+            return current_true_pos + true_pos\n+\n+    metric_fn = BinaryTruePositives()\n+    config = metrics.serialize(metric_fn)\n+    metric_fn = metrics.deserialize(\n+        config, custom_objects={'BinaryTruePositives': BinaryTruePositives})\n+\n+    # Test on simple model\n+    inputs = keras.Input(shape=(2,))\n+    outputs = keras.layers.Dense(1, activation='sigmoid')(inputs)\n+    model = keras.Model(inputs, outputs)\n+    model.compile(optimizer='sgd',\n+                  loss='binary_crossentropy',\n+                  metrics=['acc', metric_fn])\n+\n+    # Test fit, evaluate\n+    samples = 1000\n+    x = np.random.random((samples, 2))\n+    y = np.random.randint(2, size=(samples, 1))\n+    model.fit(x, y, epochs=1, batch_size=10)\n+    outs = model.evaluate(x, y, batch_size=10)\n+    preds = model.predict(x)\n+\n+    def ref_true_pos(y_true, y_pred):\n+        return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))\n+\n+    # Test correctness (e.g. updates should have been run)\n+    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9f26a92f4fdc1f1e170a4203a7c13abf3af86e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1371 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -224,7 +224,12 @@ class KerasClassifier(BaseWrapper):\n                 Class predictions.\n         \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n-        classes = self.model.predict_classes(x, **kwargs)\n+\n+        proba = self.model.predict(x, **kwargs)\n+        if proba.shape[-1] > 1:\n+            classes = proba.argmax(axis=-1)\n+        else:\n+            classes = (proba > 0.5).astype('int32')\n         return self.classes_[classes]\n \n     def predict_proba(self, x, **kwargs):\n@@ -247,7 +252,7 @@ class KerasClassifier(BaseWrapper):\n                 (instead of `(n_sample, 1)` as in Keras).\n         \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n-        probs = self.model.predict_proba(x, **kwargs)\n+        probs = self.model.predict(x, **kwargs)\n \n         # check if binary classification\n         if probs.shape[1] == 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6958abb6566b1374d029c9dcbd361c37baebc3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 222 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 222 | Churn Cumulative: 222 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.6764705882352942\n\nDIFF:\n@@ -0,0 +1,222 @@\n+\"\"\"Train a simple CNN-Capsule Network on the CIFAR10 small images dataset.\n+\n+Without Data Augmentation:\n+It gets to 75% validation accuracy in 10 epochs,\n+and 79% after 15 epochs, and overfitting after 20 epochs\n+\n+With Data Augmentation:\n+It gets to 75% validation accuracy in 10 epochs,\n+and 79% after 15 epochs, and 83% after 30 epcohs.\n+In my test, highest validation accuracy is 83.79% after 50 epcohs.\n+\n+This is a fast Implement, just 20s/epcoh with a gtx 1070 gpu.\n+\"\"\"\n+\n+from __future__ import print_function\n+from keras import backend as K\n+from keras.engine.topology import Layer\n+from keras import activations\n+from keras import utils\n+from keras.datasets import cifar10\n+from keras.models import Model\n+from keras.layers import *\n+from keras.preprocessing.image import ImageDataGenerator\n+\n+\n+# the squashing function.\n+# we use 0.5 in stead of 1 in hinton's paper.\n+# if 1, the norm of vector will be zoomed out.\n+# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n+# and be zoomed out while original norm is greater than 0.5.\n+def squash(x, axis=-1):\n+    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n+    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n+    return scale * x\n+\n+\n+# define our own softmax function instead of K.softmax\n+# because K.softmax can not specify axis.\n+def softmax(x, axis=-1):\n+    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n+    return ex / K.sum(ex, axis=axis, keepdims=True)\n+\n+\n+# define the margin loss like hinge loss\n+def margin_loss(y_true, y_pred):\n+    lamb, margin = 0.5, 0.1\n+    return y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n+        1 - y_true) * K.square(K.relu(y_pred - margin))\n+\n+\n+class Capsule(Layer):\n+    \"\"\"A Capsule Implement with Pure Keras\n+    There are two vesions of Capsule.\n+    One is like dense layer (for the fixed-shape input),\n+    and the other is like timedistributed dense (for various length input).\n+\n+    The input shape of Capsule must be (batch_size,\n+                                        input_num_capsule,\n+                                        input_dim_capsule\n+                                       )\n+    and the output shape is (batch_size,\n+                             num_capsule,\n+                             dim_capsule\n+                            )\n+\n+    Capsule Implement is from https://github.com/bojone/Capsule/\n+    Capsule Paper: https://arxiv.org/abs/1710.09829\n+    \"\"\"\n+\n+    def __init__(self,\n+                 num_capsule,\n+                 dim_capsule,\n+                 routings=3,\n+                 share_weights=True,\n+                 activation='squash',\n+                 **kwargs):\n+        super(Capsule, self).__init__(**kwargs)\n+        self.num_capsule = num_capsule\n+        self.dim_capsule = dim_capsule\n+        self.routings = routings\n+        self.share_weights = share_weights\n+        if activation == 'squash':\n+            self.activation = squash\n+        else:\n+            self.activation = activations.get(activation)\n+\n+    def build(self, input_shape):\n+        input_dim_capsule = input_shape[-1]\n+        if self.share_weights:\n+            self.kernel = self.add_weight(\n+                name='capsule_kernel',\n+                shape=(1, input_dim_capsule,\n+                       self.num_capsule * self.dim_capsule),\n+                initializer='glorot_uniform',\n+                trainable=True)\n+        else:\n+            input_num_capsule = input_shape[-2]\n+            self.kernel = self.add_weight(\n+                name='capsule_kernel',\n+                shape=(input_num_capsule, input_dim_capsule,\n+                       self.num_capsule * self.dim_capsule),\n+                initializer='glorot_uniform',\n+                trainable=True)\n+\n+    def call(self, inputs):\n+        \"\"\"Following the routing algorithm from Hinton's paper,\n+        but replace b = b + <u,v> with b = <u,v>.\n+\n+        This change can improve the feature representation of Capsule.\n+\n+        However, you can replace\n+            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n+        with\n+            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n+        to realize a standard routing.\n+        \"\"\"\n+\n+        if self.share_weights:\n+            hat_inputs = K.conv1d(inputs, self.kernel)\n+        else:\n+            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n+\n+        batch_size = K.shape(inputs)[0]\n+        input_num_capsule = K.shape(inputs)[1]\n+        hat_inputs = K.reshape(hat_inputs,\n+                               (batch_size, input_num_capsule,\n+                                self.num_capsule, self.dim_capsule))\n+        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n+\n+        b = K.zeros_like(hat_inputs[:, :, :, 0])\n+        for i in range(self.routings):\n+            c = softmax(b, 1)\n+            if K.backend() == 'theano':\n+                o = K.sum(o, axis=1)\n+            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n+            if i < self.routings - 1:\n+                b = K.batch_dot(o, hat_inputs, [2, 3])\n+                if K.backend() == 'theano':\n+                    o = K.sum(o, axis=1)\n+\n+        return o\n+\n+    def compute_output_shape(self, input_shape):\n+        return (None, self.num_capsule, self.dim_capsule)\n+\n+\n+batch_size = 128\n+num_classes = 10\n+epochs = 100\n+(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n+\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+y_train = utils.to_categorical(y_train, num_classes)\n+y_test = utils.to_categorical(y_test, num_classes)\n+\n+# A common Conv2D model\n+input_image = Input(shape=(None, None, 3))\n+x = Conv2D(64, (3, 3), activation='relu')(input_image)\n+x = Conv2D(64, (3, 3), activation='relu')(x)\n+x = AveragePooling2D((2, 2))(x)\n+x = Conv2D(128, (3, 3), activation='relu')(x)\n+x = Conv2D(128, (3, 3), activation='relu')(x)\n+\n+\n+\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n+then connect a Capsule layer.\n+\n+the output of final model is the lengths of 10 Capsule, whose dim=16.\n+\n+the length of Capsule is the proba,\n+so the probelm becomes a 10 two-classification problems\n+\"\"\"\n+\n+x = Reshape((-1, 128))(x)\n+capsule = Capsule(10, 16, 3, True)(x)\n+output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n+model = Model(inputs=input_image, outputs=output)\n+\n+# we use a margin loss\n+model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n+model.summary()\n+\n+# we can compare the performance with or without data augmentation\n+data_augmentation = True\n+\n+if not data_augmentation:\n+    print('Not using data augmentation.')\n+    model.fit(\n+        x_train,\n+        y_train,\n+        batch_size=batch_size,\n+        epochs=epochs,\n+        validation_data=(x_test, y_test),\n+        shuffle=True)\n+else:\n+    print('Using real-time data augmentation.')\n+    # This will do preprocessing and realtime data augmentation:\n+    datagen = ImageDataGenerator(\n+        featurewise_center=False,  # set input mean to 0 over the dataset\n+        samplewise_center=False,  # set each sample mean to 0\n+        featurewise_std_normalization=False,  # divide inputs by dataset std\n+        samplewise_std_normalization=False,  # divide each input by its std\n+        zca_whitening=False,  # apply ZCA whitening\n+        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n+        width_shift_range=0.1,  # randomly shift images horizontally\n+        height_shift_range=0.1,  # randomly shift images vertically\n+        horizontal_flip=True,  # randomly flip images\n+        vertical_flip=False)  # randomly flip images\n+\n+    # Compute quantities required for feature-wise normalization\n+    # (std, mean, and principal components if ZCA whitening is applied).\n+    datagen.fit(x_train)\n+\n+    # Fit the model on the batches generated by datagen.flow().\n+    model.fit_generator(\n+        datagen.flow(x_train, y_train, batch_size=batch_size),\n+        epochs=epochs,\n+        validation_data=(x_test, y_test),\n+        workers=4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b08f3335aa9e03c450b4c286a9c2bd304db188b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2770 | Contributors (this commit): 47 | Commits (past 90d): 17 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -221,7 +221,7 @@ def apply_transform(x,\n         x_channel,\n         final_affine_matrix,\n         final_offset,\n-        order=0,\n+        order=1,\n         mode=fill_mode,\n         cval=cval) for x_channel in x]\n     x = np.stack(channel_images, axis=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c42f9b0ad497878de9793361464c89c0b90b9efd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 1754 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -658,25 +658,26 @@ def test_recursion_with_bn_and_loss():\n \n @keras_test\n def test_activity_regularization_with_model_composition():\n-    # Ensures that the shape of the input to the activity\n-    # regularizer is the same under model composition.\n-    # Tests for regressions of issue #9267\n-    wrong_size = 10\n-    correct_size = 2\n-    def reg(x):\n-        assert x.shape[-1] == correct_size\n-        return 0\n \n-    net_a_input = Input([2])\n+    def reg(x):\n+        return K.sum(x)\n+\n+    net_a_input = Input((2,))\n     net_a = net_a_input\n-    net_a = Dense(wrong_size)(net_a)\n-    net_a = Dense(correct_size, activity_regularizer=reg)(net_a)\n+    net_a = Dense(2, kernel_initializer='ones',\n+                  use_bias=False,\n+                  activity_regularizer=reg)(net_a)\n     model_a = Model([net_a_input], [net_a])\n \n-    net_b_input = Input([2])\n-    net_b = Dense(10)(model_a(net_b_input))\n+    net_b_input = Input((2,))\n+    net_b = model_a(net_b_input)\n     model_b = Model([net_b_input], [net_b])\n \n+    model_b.compile(optimizer='sgd', loss=None)\n+    x = np.ones((1, 2))\n+    loss = model_b.evaluate(x)\n+    assert loss == 4\n+\n \n @keras_test\n def test_shared_layer_depth_is_correct():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d54eeb3967f3364955478c9520ed9ba05c4f9f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 426 | Contributors (this commit): 18 | Commits (past 90d): 9 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.1.3'\r\n+__version__ = '2.1.4'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.1.3',\n+      version='2.1.4',\n       description='Deep Learning for humans',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.1.3',\n+      download_url='https://github.com/keras-team/keras/tarball/2.1.4',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e41378a2e1962b35df3f8dee90aed848286809b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8556 | Contributors (this commit): 58 | Commits (past 90d): 16 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -2632,7 +2632,7 @@ class Container(Layer):\n                 or a mismatch in the shape of the weight\n                 (only valid when `by_name`=True).\n             reshape: Reshape weights to fit the layer when the correct number\n-            of values are present but the shape does not match.\n+                of weight arrays is present but their shape does not match.\n \n \n         # Raises\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed204e116d6e8f8dc1f700a6f6f2026c6437c420", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 224 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -171,7 +171,7 @@ then connect a Capsule layer.\n the output of final model is the lengths of 10 Capsule, whose dim=16.\n \n the length of Capsule is the proba,\n-so the probelm becomes a 10 two-classification problems\n+so the problem becomes a 10 two-classification problem.\n \"\"\"\n \n x = Reshape((-1, 128))(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3301fae7ef22801780d0f79a7a0a32c34507a09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9212 | Contributors (this commit): 50 | Commits (past 90d): 19 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -1906,7 +1906,7 @@ class LSTMCell(Layer):\n \n \n class LSTM(RNN):\n-    \"\"\"Long-Short Term Memory layer - Hochreiter 1997.\n+    \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n \n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
