{"custom_id": "scrapy#c382f2fc8a25f13a894e2c4fcb476500a412aed1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 85 | Churn Cumulative: 220 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.6944444444444444\n\nDIFF:\n@@ -1,19 +1,32 @@\n-import unittest\n+from twisted.trial import unittest\n \n from scrapy.utils.pqueue import PriorityQueue\n-from scrapy.utils.queue import MemoryQueue\n+from scrapy.utils.queue import MemoryQueue, DiskQueue\n \n \n class TestMemoryQueue(MemoryQueue):\n \n-    def __init__(self):\n-        super(TestMemoryQueue, self).__init__()\n+    def __init__(self, *a, **kw):\n+        super(TestMemoryQueue, self).__init__(*a, **kw)\n         self.closed = False\n \n     def close(self):\n+        super(TestMemoryQueue, self).close()\n         self.closed = True\n \n-class PriorityQueueTest(unittest.TestCase):\n+\n+class TestDiskQueue(DiskQueue):\n+\n+    def __init__(self, *a, **kw):\n+        super(TestDiskQueue, self).__init__(*a, **kw)\n+        self.closed = False\n+\n+    def close(self):\n+        super(TestDiskQueue, self).close()\n+        self.closed = True\n+\n+\n+class MemoryPriorityQueueTest(unittest.TestCase):\n \n     def setUp(self):\n         qfactory = lambda x: TestMemoryQueue()\n@@ -64,6 +77,13 @@ class PriorityQueueTest(unittest.TestCase):\n         self.assertEqual(sorted(self.q.close()), [1, 2, 3])\n         assert all(q.closed for q in iqueues)\n \n+    def test_close_return_active(self):\n+        self.q.push('b', 1)\n+        self.q.push('c', 2)\n+        self.q.push('a', 3)\n+        self.q.pop()\n+        self.assertEqual(sorted(self.q.close()), [2, 3])\n+\n     def test_popped_internal_queues_closed(self):\n         self.q.push('a', 3)\n         self.q.push('b', 1)\n@@ -72,3 +92,34 @@ class PriorityQueueTest(unittest.TestCase):\n         self.assertEqual(self.q.pop(), 'b')\n         self.q.close()\n         assert p1queue.closed\n+\n+\n+class DiskPriorityQueueTest(MemoryPriorityQueueTest):\n+\n+    def setUp(self):\n+        qfactory = lambda x: TestDiskQueue(self.mktemp())\n+        self.q = PriorityQueue(qfactory)\n+\n+    def test_nonserializable_object_one(self):\n+        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n+        self.assertEqual(self.q.close(), [])\n+\n+    def test_nonserializable_object_many_close(self):\n+        self.q.push('a', 3)\n+        self.q.push('b', 1)\n+        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n+        self.q.push('c', 2)\n+        self.assertEqual(self.q.pop(), 'b')\n+        self.assertEqual(sorted(self.q.close()), [2, 3])\n+\n+    def test_nonserializable_object_many_pop(self):\n+        self.q.push('a', 3)\n+        self.q.push('b', 1)\n+        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n+        self.q.push('c', 2)\n+        self.assertEqual(self.q.pop(), 'b')\n+        self.assertEqual(self.q.pop(), 'c')\n+        self.assertEqual(self.q.pop(), 'a')\n+        self.assertEqual(self.q.pop(), None)\n+        self.assertEqual(self.q.close(), [])\n+\n\n@@ -20,17 +20,14 @@ class PriorityQueue(object):\n         self.queues = {}\n         self.qfactory = qfactory\n         for p in startprios:\n-            q = self.qfactory(p)\n-            if q:\n-                self.queues[p] = q\n+            self.queues[p] = self.qfactory(p)\n         self.curprio = min(startprios) if startprios else None\n \n     def push(self, obj, priority=0):\n-        try:\n+        if priority not in self.queues:\n+            self.queues[priority] = self.qfactory(priority)\n         q = self.queues[priority]\n-        except KeyError:\n-            self.queues[priority] = q = self.qfactory(priority)\n-        q.push(obj)\n+        q.push(obj) # this may fail (eg. serialization error)\n         if priority < self.curprio or self.curprio is None:\n             self.curprio = priority\n \n@@ -39,17 +36,20 @@ class PriorityQueue(object):\n             return\n         q = self.queues[self.curprio]\n         m = q.pop()\n-        if not q:\n-            q = self.queues.pop(self.curprio)\n+        if len(q) == 0:\n+            del self.queues[self.curprio]\n             q.close()\n-            prios = self.queues.keys()\n+            prios = [p for p, q in self.queues.items() if len(q) > 0]\n             self.curprio = min(prios) if prios else None\n         return m\n \n     def close(self):\n-        for q in self.queues.values():\n+        active = []\n+        for p, q in self.queues.items():\n+            if len(q):\n+                active.append(p)\n             q.close()\n-        return self.queues.keys()\n+        return active\n \n     def __len__(self):\n         return sum(len(x) for x in self.queues.values()) if self.queues else 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27dd68a69093a2fae3d86d88631c3906fe070130", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 34 | Churn Cumulative: 114 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,33 @@\n+from __future__ import with_statement\n+\n+import os, cPickle as pickle\n+\n+from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n+from scrapy.xlib.pydispatch import dispatcher\n+\n+class SpiderState(object):\n+    \"\"\"Stores and loads spider state\"\"\"\n+\n+    def __init__(self, jobdir):\n+        self.statefn = os.path.join(jobdir, 'spider.state')\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        jobdir = crawler.settings.get('JOBDIR')\n+        if not jobdir:\n+            raise NotConfigured\n+        return cls(jobdir)\n+\n+    def spider_closed(self, spider):\n+        with open(self.statefn, 'wb') as f:\n+            pickle.dump(spider.state, f, protocol=2)\n+\n+    def spider_opened(self, spider):\n+        if os.path.exists(self.statefn):\n+            with open(self.statefn) as f:\n+                spider.state = pickle.load(f)\n+        else:\n+            spider.state = {}\n\n@@ -144,6 +144,7 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.closespider.CloseSpider': 0,\n     'scrapy.contrib.feedexport.FeedExporter': 0,\n     'scrapy.contrib.logstats.LogStats': 0,\n+    'scrapy.contrib.spiderstate.SpiderState': 0,\n }\n \n FEED_URI = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1dbc62b455fbacd323bfe7aeef0951a6f597417", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 2127 | Contributors (this commit): 7 | Commits (past 90d): 45 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.\n For more information see docs/topics/architecture.rst\n \n \"\"\"\n+import warnings\n from time import time\n \n from twisted.internet import defer\n@@ -13,7 +14,7 @@ from scrapy import log, signals\n from scrapy.stats import stats\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import DontCloseSpider\n+from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n@@ -59,7 +60,10 @@ class ExecutionEngine(object):\n         self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n         self.downloader = Downloader(crawler)\n         self.scraper = Scraper(crawler)\n-        self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS')\n+        self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS', 1)\n+        if self._concurrent_spiders != 1:\n+            warnings.warn(\"CONCURRENT_SPIDERS settings is deprecated, use \" \\\n+                \"Scrapyd max_proc config instead\", ScrapyDeprecationWarning)\n         self._spider_closed_callback = spider_closed_callback\n \n     @defer.inlineCallbacks\n\n@@ -31,8 +31,6 @@ CONCURRENT_REQUESTS = 16\n CONCURRENT_REQUESTS_PER_DOMAIN = 8\n CONCURRENT_REQUESTS_PER_IP = 0\n \n-CONCURRENT_SPIDERS = 8\n-\n COOKIES_ENABLED = True\n COOKIES_DEBUG = False\n \n\n@@ -30,7 +30,6 @@ class Environment(object):\n         dbpath = os.path.join(self.dbs_dir, '%s.db' % project)\n         env['SCRAPY_SQLITE_DB'] = dbpath\n         env['SCRAPY_LOG_FILE'] = self._get_log_file(message)\n-        env['SCRAPY_CONCURRENT_SPIDERS'] = '1' # scrapyd runs one spider per process\n         return env\n \n     def _get_log_file(self, message):\n\n@@ -29,7 +29,6 @@ class EnvironmentTest(unittest.TestCase):\n         self.assertEqual(env['SCRAPY_SLOT'], '3')\n         self.assertEqual(env['SCRAPY_SPIDER'], 'myspider')\n         self.assertEqual(env['SCRAPY_JOB'], 'ID')\n-        self.assertEqual(env['SCRAPY_CONCURRENT_SPIDERS'], '1')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID.log')))\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c8d30c6ffa6185bb98ffd4f74c64a9b1d265f852", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 17 | Files Changed: 5 | Hunks: 19 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 39 | Churn Cumulative: 525 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from w3lib.url import urljoin_rfc\n+from urlparse import urljoin\n \n from scrapy import log\n from scrapy.http import HtmlResponse\n@@ -22,19 +22,19 @@ class RedirectMiddleware(object):\n             return response\n         if request.method.upper() == 'HEAD':\n             if response.status in [301, 302, 303, 307] and 'Location' in response.headers:\n-                redirected_url = urljoin_rfc(request.url, response.headers['location'])\n+                redirected_url = urljoin(request.url, response.headers['location'])\n                 redirected = request.replace(url=redirected_url)\n                 return self._redirect(redirected, request, spider, response.status)\n             else:\n                 return response\n \n         if response.status in [302, 303] and 'Location' in response.headers:\n-            redirected_url = urljoin_rfc(request.url, response.headers['location'])\n+            redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = self._redirect_request_using_get(request, redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n         if response.status in [301, 307] and 'Location' in response.headers:\n-            redirected_url = urljoin_rfc(request.url, response.headers['location'])\n+            redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n\n@@ -3,8 +3,9 @@ HTMLParser-based link extractor\n \"\"\"\n \n from HTMLParser import HTMLParser\n+from urlparse import urljoin\n \n-from w3lib.url import safe_url_string, urljoin_rfc\n+from w3lib.url import safe_url_string\n \n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list\n@@ -27,9 +28,11 @@ class HtmlParserLinkExtractor(HTMLParser):\n         links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n \n         ret = []\n-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n+        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n         for link in links:\n-            link.url = urljoin_rfc(base_url, link.url, response_encoding)\n+            if isinstance(link.url, unicode):\n+                link.url = link.url.encode(response_encoding)\n+            link.url = urljoin(base_url, link.url)\n             link.url = safe_url_string(link.url, response_encoding)\n             link.text = link.text.decode(response_encoding)\n             ret.append(link)\n\n@@ -3,7 +3,7 @@ This module implements the HtmlImageLinkExtractor for extracting\n image links only.\n \"\"\"\n \n-from w3lib.url import urljoin_rfc\n+from urlparse import urljoin\n from scrapy.link import Link\n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.python import unicode_to_str, flatten\n@@ -51,7 +51,7 @@ class HTMLImageLinkExtractor(object):\n     def extract_links(self, response):\n         xs = HtmlXPathSelector(response)\n         base_url = xs.select('//base/@href').extract()\n-        base_url = urljoin_rfc(response.url, base_url[0]) if base_url else response.url\n+        base_url = urljoin(response.url, base_url[0].encode(response.encoding)) if base_url else response.url\n \n         links = []\n         for location in self.locations:\n@@ -67,7 +67,7 @@ class HTMLImageLinkExtractor(object):\n \n         seen, ret = set(), []\n         for link in links:\n-            link.url = urljoin_rfc(base_url, link.url, response.encoding)\n+            link.url = urljoin(base_url, link.url)\n             if self.unique:\n                 if link.url in seen:\n                     continue\n\n@@ -1,6 +1,6 @@\n import re\n+from urlparse import urljoin\n \n-from w3lib.url import urljoin_rfc\n from w3lib.html import remove_tags, remove_entities, replace_escape_chars\n \n from scrapy.link import Link\n@@ -18,9 +18,9 @@ class RegexLinkExtractor(SgmlLinkExtractor):\n     \"\"\"High performant link extractor\"\"\"\n \n     def _extract_links(self, response_text, response_url, response_encoding):\n-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n+        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n \n-        clean_url = lambda u: urljoin_rfc(base_url, remove_entities(clean_link(u.decode(response_encoding))))\n+        clean_url = lambda u: urljoin(base_url, remove_entities(clean_link(u.decode(response_encoding))))\n         clean_text = lambda t: replace_escape_chars(remove_tags(t.decode(response_encoding))).strip()\n \n         links_text = linkre.findall(response_text)\n\n@@ -3,9 +3,9 @@ SGMLParser-based Link extractors\n \"\"\"\n \n import re\n-from urlparse import urlparse\n+from urlparse import urlparse, urljoin\n \n-from w3lib.url import safe_url_string, urljoin_rfc\n+from w3lib.url import safe_url_string\n \n from scrapy.selector import HtmlXPathSelector\n from scrapy.link import Link\n@@ -33,9 +33,11 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n \n         ret = []\n         if base_url is None:\n-            base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n+            base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n         for link in self.links:\n-            link.url = urljoin_rfc(base_url, link.url, response_encoding)\n+            if isinstance(link.url, unicode):\n+                link.url = link.url.encode(response_encoding)\n+            link.url = urljoin(base_url, link.url)\n             link.url = safe_url_string(link.url, response_encoding)\n             link.text = str_to_unicode(link.text, response_encoding, errors='replace')\n             ret.append(link)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0ec23993011477802d82fdd491a720324670c58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 572 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,6 +40,10 @@ class Slot(object):\n \n def _get_concurrency_delay(concurrency, spider, settings):\n     delay = settings.getfloat('DOWNLOAD_DELAY')\n+    if hasattr(spider, 'DOWNLOAD_DELAY'):\n+        warnings.warn(\"%s.DOWNLOAD_DELAY attribute is deprecated, use %s.download_delay instead\" %\n+            (type(spider).__name__, type(spider).__name__))\n+        delay = spider.DOWNLOAD_DELAY\n     if hasattr(spider, 'download_delay'):\n         delay = spider.download_delay\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17cc90e3fecfd38d6e1456d508212c4fe7d08e7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 42 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,19 +7,20 @@ from scrapy.exceptions import NotConfigured\n from scrapy.xlib.pydispatch import dispatcher\n \n class SpiderState(object):\n-    \"\"\"Stores and loads spider state\"\"\"\n+    \"\"\"Store and load spider state during a scraping job\"\"\"\n \n     def __init__(self, jobdir):\n         self.statefn = os.path.join(jobdir, 'spider.state')\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         jobdir = crawler.settings.get('JOBDIR')\n         if not jobdir:\n             raise NotConfigured\n-        return cls(jobdir)\n+        obj = cls(jobdir)\n+        dispatcher.connect(obj.spider_closed, signal=signals.spider_closed)\n+        dispatcher.connect(obj.spider_opened, signal=signals.spider_opened)\n+        return obj\n \n     def spider_closed(self, spider):\n         with open(self.statefn, 'wb') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bff3d314692646cee3f6e7fb1eb9a52eb638c6fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 200 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -32,8 +32,7 @@ class Schedule(WsResource):\n         jobid = uuid.uuid1().hex\n         args['_job'] = jobid\n         self.root.scheduler.schedule(project, spider, **args)\n-        jobids = {spider: jobid}\n-        return {\"status\": \"ok\", \"jobs\": jobids}\n+        return {\"status\": \"ok\", \"jobid\": jobid}\n \n class AddVersion(WsResource):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f1b1c05f8fa98fca544e02eeda8b5cdba526cbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 187 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,7 @@ class OffsiteMiddleware(object):\n     def process_spider_output(self, response, result, spider):\n         for x in result:\n             if isinstance(x, Request):\n-                if self.should_follow(x, spider):\n+                if x.dont_filter or self.should_follow(x, spider):\n                     yield x\n                 else:\n                     domain = urlparse_cached(x).hostname\n\n@@ -20,8 +20,10 @@ class TestOffsiteMiddleware(TestCase):\n \n         onsite_reqs = [Request('http://scrapytest.org/1'),\n                        Request('http://scrapy.org/1'),\n-                       Request('http://sub.scrapy.org/1')]\n-        offsite_reqs = [Request('http://scrapy2.org')]\n+                       Request('http://sub.scrapy.org/1'),\n+                       Request('http://offsite.tld/letmepass', dont_filter=True)]\n+        offsite_reqs = [Request('http://scrapy2.org'),\n+                       Request('http://offsite.tld/')]\n         reqs = onsite_reqs + offsite_reqs\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e43afeaea4c91ca49ca642c31d419282f5ae075", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 209 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ def build(suffix):\n         with open(ifn, 'w') as of:\n             of.write(s)\n \n-    env={'SCRAPY_VERSION_FROM_HG': '1'}\n+    env={'SCRAPY_VERSION_FROM_GIT': '1'}\n     check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)+$(date +%s) \"Automatic build\"', \\\n         shell=True, env=env)\n     check_call('debuild -us -uc -b', shell=True)\n\n@@ -86,6 +86,10 @@ if os.environ.get('SCRAPY_VERSION_FROM_HG'):\n     rev = Popen([\"hg\", \"tip\", \"--template\", \"{rev}\"], stdout=PIPE).communicate()[0]\n     with open('scrapy/__init__.py', 'a') as f:\n         f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev)\n+elif os.environ.get('SCRAPY_VERSION_FROM_GIT'):\n+    rev = Popen(\"git log --oneline | wc -l\", shell=True, stdout=PIPE).communicate()[0]\n+    with open('scrapy/__init__.py', 'a') as f:\n+        f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev.strip())\n version = __import__('scrapy').__version__\n \n setup_args = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43ae7bdd8992debea24a725e739729094c84abda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 27 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,27 @@\n+import os\n+from datetime import datetime\n+from twisted.trial import unittest\n+\n+from scrapy.contrib.spiderstate import SpiderState\n+from scrapy.spider import BaseSpider\n+\n+\n+class SpiderStateTest(unittest.TestCase):\n+\n+    def test_store_load(self):\n+        jobdir = self.mktemp()\n+        os.mkdir(jobdir)\n+        spider = BaseSpider(name='default')\n+        dt = datetime.now()\n+\n+        ss = SpiderState(jobdir)\n+        ss.spider_opened(spider)\n+        spider.state['one'] = 1\n+        spider.state['dt'] = dt\n+        ss.spider_closed(spider)\n+\n+        spider2 = BaseSpider(name='default')\n+        ss2 = SpiderState(jobdir)\n+        ss2.spider_opened(spider2)\n+        self.assertEqual(spider.state, {'one': 1, 'dt': dt})\n+        ss2.spider_closed(spider2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b00b9cb12afa097e55f51167614f0985816a482", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 217 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,9 @@ def build(suffix):\n         with open(ifn, 'w') as of:\n             of.write(s)\n \n-    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)-r$(hg tip --template \"{rev}\")+$(date +%s) \"Automatic build\"', shell=True)\n+    env={'SCRAPY_VERSION_FROM_GIT': '1'}\n+    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)+$(date +%s) \"Automatic build\"', \\\n+        shell=True, env=env)\n     check_call('debuild -us -uc -b', shell=True)\n \n def clean(suffix):\n\n@@ -86,6 +86,10 @@ if os.environ.get('SCRAPY_VERSION_FROM_HG'):\n     rev = Popen([\"hg\", \"tip\", \"--template\", \"{rev}\"], stdout=PIPE).communicate()[0]\n     with open('scrapy/__init__.py', 'a') as f:\n         f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev)\n+elif os.environ.get('SCRAPY_VERSION_FROM_GIT'):\n+    rev = Popen(\"git log --oneline | wc -l\", shell=True, stdout=PIPE).communicate()[0]\n+    with open('scrapy/__init__.py', 'a') as f:\n+        f.write(\"\\n__version__ = '.'.join(map(str, version_info)) + '.%s'\" % rev.strip())\n version = __import__('scrapy').__version__\n \n setup_args = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#509b05db571aac659c51ec36b75f83166a2eda49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 48 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,8 @@ def clean_link(link_text):\n class RegexLinkExtractor(SgmlLinkExtractor):\n     \"\"\"High performant link extractor\"\"\"\n \n-    def _extract_links(self, response_text, response_url, response_encoding):\n+    def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n+        if base_url is None:\n             base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n \n         clean_url = lambda u: urljoin(base_url, remove_entities(clean_link(u.decode(response_encoding))))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d788ba8a44e0538188872977a8705c1209a09194", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 500 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,10 +124,11 @@ def execute(argv=None):\n     parser.usage = \"scrapy %s %s\" % (cmdname, cmd.syntax())\n     parser.description = cmd.long_desc()\n     settings.defaults.update(cmd.default_settings)\n-    cmd.set_crawler(crawler)\n+    cmd.settings = settings\n     cmd.add_options(parser)\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n+    cmd.set_crawler(crawler)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n \n\n@@ -5,12 +5,9 @@ Base class for Scrapy commands\n from __future__ import with_statement\n \n import os\n-import sys\n from optparse import OptionGroup\n \n-import scrapy\n from scrapy import log\n-from scrapy.conf import settings\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n \n@@ -23,7 +20,11 @@ class ScrapyCommand(object):\n \n     exitcode = 0\n \n+    def __init__(self):\n+        self.settings = None # set in scrapy.cmdline\n+\n     def set_crawler(self, crawler):\n+        assert not hasattr(self, '_crawler'), \"crawler already set\"\n         self._crawler = crawler\n \n     @property\n@@ -68,7 +69,7 @@ class ScrapyCommand(object):\n             help=\"log file. if omitted stderr will be used\")\n         group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", \\\n             default=None, \\\n-            help=\"log level (default: %s)\" % settings['LOGLEVEL'])\n+            help=\"log level (default: %s)\" % self.settings['LOG_LEVEL'])\n         group.add_option(\"--nolog\", action=\"store_true\", \\\n             help=\"disable logging completely\")\n         group.add_option(\"--profile\", metavar=\"FILE\", default=None, \\\n@@ -83,20 +84,20 @@ class ScrapyCommand(object):\n         \n     def process_options(self, args, opts):\n         try:\n-            settings.overrides.update(arglist_to_dict(opts.set))\n+            self.settings.overrides.update(arglist_to_dict(opts.set))\n         except ValueError:\n             raise UsageError(\"Invalid -s value, use -s NAME=VALUE\", print_help=False)\n \n         if opts.logfile:\n-            settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOG_FILE'] = opts.logfile\n+            self.settings.overrides['LOG_ENABLED'] = True\n+            self.settings.overrides['LOG_FILE'] = opts.logfile\n \n         if opts.loglevel:\n-            settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOG_LEVEL'] = opts.loglevel\n+            self.settings.overrides['LOG_ENABLED'] = True\n+            self.settings.overrides['LOG_LEVEL'] = opts.loglevel\n \n         if opts.nolog:\n-            settings.overrides['LOG_ENABLED'] = False\n+            self.settings.overrides['LOG_ENABLED'] = False\n \n         if opts.pidfile:\n             with open(opts.pidfile, \"w\") as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcc50201e3a42d4dbe8c5988fda1b99cd2d8beee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 91 | Churn Cumulative: 239 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,8 @@\n-# TODO: this extension is currently broken and needs to be ported after the\n-# downloader refactoring introduced in r2732\n-\n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.utils.python import setattr_default\n-from scrapy.conf import settings\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.resolver import dnscache\n \n class AutoThrottle(object):\n     \"\"\"\n@@ -61,56 +58,74 @@ class AutoThrottle(object):\n \n     \"\"\"\n \n-    # TODO: convert these to settings\n-    START_DELAY = 5.0\n-    MAX_CONCURRENCY = 8\n-    CONCURRENCY_CHECK_PERIOD = 10\n-\n-    DEBUG = 0\n-\n-    def __init__(self):\n+    def __init__(self, crawler):\n+        settings = crawler.settings    \n         if not settings.getbool('AUTOTHROTTLE_ENABLED'):\n             raise NotConfigured\n+        self.crawler = crawler\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.response_received, signal=signals.response_received)\n-        self.last_latencies = {}\n-        self.last_lat = {}\n+        self.START_DELAY = settings.getfloat(\"AUTOTHROTTLE_START_DELAY\", 5.0)\n+        self.CONCURRENCY_CHECK_PERIOD = settings.getint(\"AUTOTHROTTLE_CONCURRENCY_CHECK_PERIOD\", 10)\n+        self.MAX_CONCURRENCY = settings.getint(\"AUTOTHROTTLE_MAX_CONCURRENCY\", 8)\n+        self.DEBUG = settings.getint(\"AUTOTHROTTLE_DEBUG\", False)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n \n     def spider_opened(self, spider):\n         spider.download_delay = self.START_DELAY\n         spider.max_concurrent_requests = 1\n-        self.last_latencies[spider] = [self.START_DELAY]\n-        self.last_lat[spider] = self.START_DELAY, 0.0\n-\n-    def spider_closed(self, spider):\n-        del self.last_latencies[spider]\n-        del self.last_lat[spider]\n+        self.last_latencies = [self.START_DELAY]\n+        self.last_lat = self.START_DELAY, 0.0\n         \n     def response_received(self, response, spider):\n+        slot = self._get_slot(response.request)\n         latency = response.meta.get('download_latency')\n-        if not latency:\n-            return\n-        spider.download_delay = (spider.download_delay + latency) / 2.0\n-        self._check_concurrency(spider, latency)\n-        if self.DEBUG:\n-            print \"conc:%2d | delay:%5d ms | latency:%5d ms | size:%6d bytes\" % \\\n-                (spider.max_concurrent_requests, spider.download_delay*1000, \\\n-                latency*1000, len(response.body))\n         \n-    def _check_concurrency(self, spider, latency):\n-        latencies = self.last_latencies[spider]\n+        if not latency or not slot:\n+            return\n+\n+        self._adjust_delay(slot, latency, response)\n+        self._check_concurrency(slot, latency)\n+\n+        if self.DEBUG:\n+            spider.log(\"conc:%2d | delay:%5d ms | latency:%5d ms | size:%6d bytes\" % \\\n+                (slot.concurrency, slot.delay*1000, \\\n+                latency*1000, len(response.body)))\n+\n+    def _get_slot(self, request):\n+        downloader = self.crawler.engine.downloader\n+        key = urlparse_cached(request).hostname or ''\n+        if downloader.ip_concurrency:\n+            key = dnscache.get(key, key)\n+        return downloader.slots.get(key)\n+\n+    def _check_concurrency(self, slot, latency):\n+        latencies = self.last_latencies\n         latencies.append(latency)\n         if len(latencies) == self.CONCURRENCY_CHECK_PERIOD:\n             curavg, curdev = avg_stdev(latencies)\n-            preavg, predev = self.last_lat[spider]\n-            self.last_lat[spider] = curavg, curdev\n+            preavg, predev = self.last_lat\n+            self.last_lat = curavg, curdev\n             del latencies[:]\n             if curavg > preavg + predev:\n-                if spider.max_concurrent_requests > 1:\n-                    spider.max_concurrent_requests -= 1\n-            elif spider.max_concurrent_requests < self.MAX_CONCURRENCY:\n-                spider.max_concurrent_requests += 1\n+                if slot.concurrency > 1:\n+                    slot.concurrency -= 1\n+            elif slot.concurrency < self.MAX_CONCURRENCY:\n+                slot.concurrency += 1\n+\n+    def _adjust_delay(self, slot, latency, response):\n+        \"\"\"Define delay adjustment policy\"\"\"\n+        # if latency is bigger than old delay, then use latency instead of mean. Works better with problematic sites\n+        new_delay = (slot.delay + latency) / 2.0 if latency < slot.delay else latency\n+\n+        # dont adjust delay if response status != 200 and new delay is smaller than old one,\n+        # as error pages (and redirections) are usually small and so tend to reduce latency, thus provoking a positive feedback\n+        # by reducing delay instead of increase.\n+        if response.status == 200 or new_delay > slot.delay:\n+            slot.delay = new_delay\n \n def avg_stdev(lst):\n     \"\"\"Return average and standard deviation of the given list\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7c436343a77048f79f2df037a8fb14d41917ddb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 315 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -150,6 +150,9 @@ def _get_version(target, opts):\n     if version == 'HG':\n         p = Popen(['hg', 'tip', '--template', '{rev}'], stdout=PIPE)\n         return 'r%s' % p.communicate()[0]\n+    elif version == 'GIT':\n+        p = Popen(['git', 'rev-parse', 'HEAD'], stdout=PIPE)\n+        return '%s' % p.communicate()[0].strip('\\n')\n     elif version:\n         return version\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfddc314cec18e6998c4abac9e02b5562219b39f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 25 | Churn Cumulative: 94 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,26 +9,28 @@ from scrapy.xlib.pydispatch import dispatcher\n class SpiderState(object):\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n \n-    def __init__(self, jobdir):\n-        self.statefn = os.path.join(jobdir, 'spider.state')\n+    def __init__(self, jobdir=None):\n+        self.jobdir = jobdir\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        jobdir = crawler.settings.get('JOBDIR')\n-        if not jobdir:\n-            raise NotConfigured\n-        obj = cls(jobdir)\n+        obj = cls(crawler.settings.get('JOBDIR'))\n         dispatcher.connect(obj.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(obj.spider_opened, signal=signals.spider_opened)\n         return obj\n \n     def spider_closed(self, spider):\n+        if self.jobdir:\n             with open(self.statefn, 'wb') as f:\n                 pickle.dump(spider.state, f, protocol=2)\n \n     def spider_opened(self, spider):\n-        if os.path.exists(self.statefn):\n+        if self.jobdir and os.path.exists(self.statefn):\n             with open(self.statefn) as f:\n                 spider.state = pickle.load(f)\n         else:\n             spider.state = {}\n+\n+    @property\n+    def statefn(self):\n+        return os.path.join(self.jobdir, 'spider.state')\n\n@@ -25,3 +25,12 @@ class SpiderStateTest(unittest.TestCase):\n         ss2.spider_opened(spider2)\n         self.assertEqual(spider.state, {'one': 1, 'dt': dt})\n         ss2.spider_closed(spider2)\n+\n+    def test_state_attribute(self):\n+        # state attribute must be present if jobdir is not set, to provide a\n+        # consistent interface \n+        spider = BaseSpider(name='default')\n+        ss = SpiderState()\n+        ss.spider_opened(spider)\n+        self.assertEqual(spider.state, {})\n+        ss.spider_closed(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f850a4478415b60516752696d08588ec5aa0428c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 355 | Lines Deleted: 71 | Files Changed: 7 | Hunks: 56 | Methods Changed: 44 | Complexity Δ (Sum/Max): 39/12 | Churn Δ: 426 | Churn Cumulative: 1299 | Contributors (this commit): 4 | Commits (past 90d): 47 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,8 @@\n from __future__ import with_statement\n \n+import os\n from os.path import join, exists\n \n-from scrapy.utils.queue import MemoryQueue\n from scrapy.utils.pqueue import PriorityQueue\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n from scrapy.utils.misc import load_object\n@@ -13,10 +13,11 @@ from scrapy import log\n \n class Scheduler(object):\n \n-    def __init__(self, dupefilter, jobdir=None, dqclass=None, logunser=False):\n+    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False):\n         self.df = dupefilter\n-        self.dqdir = join(jobdir, 'requests.queue') if jobdir else None\n+        self.dqdir = self._dqdir(jobdir)\n         self.dqclass = dqclass\n+        self.mqclass = mqclass\n         self.logunser = logunser\n \n     @classmethod\n@@ -24,8 +25,9 @@ class Scheduler(object):\n         dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n         dupefilter = dupefilter_cls.from_settings(settings)\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n+        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n         logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')\n-        return cls(dupefilter, job_dir(settings), dqclass, logunser)\n+        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser)\n \n     def has_pending_requests(self):\n         return len(self) > 0\n@@ -81,7 +83,7 @@ class Scheduler(object):\n                 return request_from_dict(d, self.spider)\n \n     def _newmq(self, priority):\n-        return MemoryQueue()\n+        return self.mqclass()\n \n     def _newdq(self, priority):\n         return self.dqclass(join(self.dqdir, 'p%s' % priority))\n@@ -98,3 +100,10 @@ class Scheduler(object):\n             log.msg(\"Resuming crawl (%d requests scheduled)\" % len(q), \\\n                 spider=self.spider)\n         return q\n+\n+    def _dqdir(self, jobdir):\n+        if jobdir:\n+            dqdir = join(jobdir, 'requests.queue')\n+            if not exists(dqdir):\n+                os.makedirs(dqdir)\n+            return dqdir\n\n@@ -45,7 +45,7 @@ DEFAULT_RESPONSE_ENCODING = 'ascii'\n \n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n-DEPTH_PRIORITY = 1\n+DEPTH_PRIORITY = 0\n \n DNSCACHE_ENABLED = True\n \n@@ -224,7 +224,8 @@ RETRY_PRIORITY_ADJUST = -1\n ROBOTSTXT_OBEY = False\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleDiskQueue'\n+SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'\n+SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'\n \n SELECTORS_BACKEND = None # possible values: libxml2, lxml\n \n\n@@ -1,33 +1,39 @@\n \"\"\"\n-Scheduler disk-based queues\n+Scheduler queues\n \"\"\"\n \n import marshal, cPickle as pickle\n \n-from scrapy.utils.queue import DiskQueue\n+from scrapy.utils import queue\n \n+def _serializable_queue(queue_class, serialize, deserialize):\n \n-class PickleDiskQueue(DiskQueue):\n+    class SerializableQueue(queue_class):\n \n         def push(self, obj):\n+            s = serialize(obj)\n+            super(SerializableQueue, self).push(s)\n+\n+        def pop(self):\n+            s = super(SerializableQueue, self).pop()\n+            if s:\n+                return deserialize(s)\n+\n+    return SerializableQueue\n+\n+def _pickle_serialize(obj):\n     try:\n-            s = pickle.dumps(obj, protocol=2)\n+        return pickle.dumps(obj, protocol=2)\n     except pickle.PicklingError, e:\n         raise ValueError(str(e))\n-        super(PickleDiskQueue, self).push(s)\n \n-    def pop(self):\n-        s = super(PickleDiskQueue, self).pop()\n-        if s:\n-            return pickle.loads(s)\n-\n-\n-class MarshalDiskQueue(DiskQueue):\n-\n-    def push(self, obj):\n-        super(MarshalDiskQueue, self).push(marshal.dumps(obj))\n-\n-    def pop(self):\n-        s = super(MarshalDiskQueue, self).pop()\n-        if s:\n-            return marshal.loads(s)\n+PickleFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue, \\\n+    _pickle_serialize, pickle.loads)\n+PickleLifoDiskQueue = _serializable_queue(queue.LifoDiskQueue, \\\n+    _pickle_serialize, pickle.loads)\n+MarshalFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue, \\\n+    marshal.dumps, marshal.loads)\n+MarshalLifoDiskQueue = _serializable_queue(queue.LifoDiskQueue, \\\n+    marshal.dumps, marshal.loads)\n+FifoMemoryQueue = queue.FifoMemoryQueue\n+LifoMemoryQueue = queue.LifoMemoryQueue\n\n@@ -1,5 +1,5 @@\n from scrapy.tests import test_utils_queue as t\n-from scrapy.squeue import MarshalDiskQueue, PickleDiskQueue\n+from scrapy.squeue import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue\n from scrapy.item import Item, Field\n from scrapy.http import Request\n from scrapy.contrib.loader import ItemLoader\n@@ -14,12 +14,12 @@ class TestLoader(ItemLoader):\n     default_item_class = TestItem\n     name_out = staticmethod(test_processor)\n \n-class MarshalDiskQueueTest(t.DiskQueueTest):\n+class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n \n     chunksize = 100000\n \n     def queue(self):\n-        return MarshalDiskQueue(self.qdir, chunksize=self.chunksize)\n+        return MarshalFifoDiskQueue(self.qdir, chunksize=self.chunksize)\n \n     def test_serialize(self):\n         q = self.queue()\n@@ -34,25 +34,25 @@ class MarshalDiskQueueTest(t.DiskQueueTest):\n         q = self.queue()\n         self.assertRaises(ValueError, q.push, lambda x: x)\n \n-class ChunkSize1MarshalDiskQueueTest(MarshalDiskQueueTest):\n+class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 1\n \n-class ChunkSize2MarshalDiskQueueTest(MarshalDiskQueueTest):\n+class ChunkSize2MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 2\n \n-class ChunkSize3MarshalDiskQueueTest(MarshalDiskQueueTest):\n+class ChunkSize3MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 3\n \n-class ChunkSize4MarshalDiskQueueTest(MarshalDiskQueueTest):\n+class ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 4\n \n \n-class PickleDiskQueueTest(MarshalDiskQueueTest):\n+class PickleFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n \n     chunksize = 100000\n \n     def queue(self):\n-        return PickleDiskQueue(self.qdir, chunksize=self.chunksize)\n+        return PickleFifoDiskQueue(self.qdir, chunksize=self.chunksize)\n \n     def test_serialize_item(self):\n         q = self.queue()\n@@ -81,15 +81,66 @@ class PickleDiskQueueTest(MarshalDiskQueueTest):\n         self.assertEqual(r.url, r2.url)\n         assert r2.meta['request'] is r2\n \n-class ChunkSize1PickleDiskQueueTest(PickleDiskQueueTest):\n+class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 1\n \n-class ChunkSize2PickleDiskQueueTest(PickleDiskQueueTest):\n+class ChunkSize2PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 2\n \n-class ChunkSize3PickleDiskQueueTest(PickleDiskQueueTest):\n+class ChunkSize3PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 3\n \n-class ChunkSize4PickleDiskQueueTest(PickleDiskQueueTest):\n+class ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 4\n \n+\n+class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):\n+\n+    def queue(self):\n+        return MarshalLifoDiskQueue(self.path)\n+\n+    def test_serialize(self):\n+        q = self.queue()\n+        q.push('a')\n+        q.push(123)\n+        q.push({'a': 'dict'})\n+        self.assertEqual(q.pop(), {'a': 'dict'})\n+        self.assertEqual(q.pop(), 123)\n+        self.assertEqual(q.pop(), 'a')\n+\n+    def test_nonserializable_object(self):\n+        q = self.queue()\n+        self.assertRaises(ValueError, q.push, lambda x: x)\n+\n+\n+class PickleLifoDiskQueueTest(MarshalLifoDiskQueueTest):\n+\n+    def queue(self):\n+        return PickleLifoDiskQueue(self.path)\n+\n+    def test_serialize_item(self):\n+        q = self.queue()\n+        i = TestItem(name='foo')\n+        q.push(i)\n+        i2 = q.pop()\n+        assert isinstance(i2, TestItem)\n+        self.assertEqual(i, i2)\n+\n+    def test_serialize_loader(self):\n+        q = self.queue()\n+        l = TestLoader()\n+        q.push(l)\n+        l2 = q.pop()\n+        assert isinstance(l2, TestLoader)\n+        assert l2.default_item_class is TestItem\n+        self.assertEqual(l2.name_out('x'), 'xx')\n+\n+    def test_serialize_request_recursive(self):\n+        q = self.queue()\n+        r = Request('http://www.example.com')\n+        r.meta['request'] = r\n+        q.push(r)\n+        r2 = q.pop()\n+        assert isinstance(r2, Request)\n+        self.assertEqual(r.url, r2.url)\n+        assert r2.meta['request'] is r2\n\n@@ -1,36 +1,32 @@\n from twisted.trial import unittest\n \n from scrapy.utils.pqueue import PriorityQueue\n-from scrapy.utils.queue import MemoryQueue, DiskQueue\n+from scrapy.utils.queue import FifoMemoryQueue, LifoMemoryQueue, FifoDiskQueue, LifoDiskQueue\n \n \n-class TestMemoryQueue(MemoryQueue):\n+def track_closed(cls):\n+    \"\"\"Wraps a queue class to track down if close() method was called\"\"\"\n+\n+    class TrackingClosed(cls):\n \n         def __init__(self, *a, **kw):\n-        super(TestMemoryQueue, self).__init__(*a, **kw)\n+            super(TrackingClosed, self).__init__(*a, **kw)\n             self.closed = False\n \n         def close(self):\n-        super(TestMemoryQueue, self).close()\n+            super(TrackingClosed, self).close()\n             self.closed = True\n \n-\n-class TestDiskQueue(DiskQueue):\n-\n-    def __init__(self, *a, **kw):\n-        super(TestDiskQueue, self).__init__(*a, **kw)\n-        self.closed = False\n-\n-    def close(self):\n-        super(TestDiskQueue, self).close()\n-        self.closed = True\n+    return TrackingClosed\n \n \n-class MemoryPriorityQueueTest(unittest.TestCase):\n+class FifoMemoryPriorityQueueTest(unittest.TestCase):\n \n     def setUp(self):\n-        qfactory = lambda x: TestMemoryQueue()\n-        self.q = PriorityQueue(qfactory)\n+        self.q = PriorityQueue(self.qfactory)\n+\n+    def qfactory(self, prio):\n+        return track_closed(FifoMemoryQueue)()\n \n     def test_push_pop_noprio(self):\n         self.q.push('a')\n@@ -94,11 +90,39 @@ class MemoryPriorityQueueTest(unittest.TestCase):\n         assert p1queue.closed\n \n \n-class DiskPriorityQueueTest(MemoryPriorityQueueTest):\n+class LifoMemoryPriorityQueueTest(FifoMemoryPriorityQueueTest):\n+\n+    def qfactory(self, prio):\n+        return track_closed(LifoMemoryQueue)()\n+\n+    def test_push_pop_noprio(self):\n+        self.q.push('a')\n+        self.q.push('b')\n+        self.q.push('c')\n+        self.assertEqual(self.q.pop(), 'c')\n+        self.assertEqual(self.q.pop(), 'b')\n+        self.assertEqual(self.q.pop(), 'a')\n+        self.assertEqual(self.q.pop(), None)\n+\n+    def test_push_pop_prio(self):\n+        self.q.push('a', 3)\n+        self.q.push('b', 1)\n+        self.q.push('c', 2)\n+        self.q.push('d', 1)\n+        self.assertEqual(self.q.pop(), 'd')\n+        self.assertEqual(self.q.pop(), 'b')\n+        self.assertEqual(self.q.pop(), 'c')\n+        self.assertEqual(self.q.pop(), 'a')\n+        self.assertEqual(self.q.pop(), None)\n+\n+\n+class FifoDiskPriorityQueueTest(FifoMemoryPriorityQueueTest):\n \n     def setUp(self):\n-        qfactory = lambda x: TestDiskQueue(self.mktemp())\n-        self.q = PriorityQueue(qfactory)\n+        self.q = PriorityQueue(self.qfactory)\n+\n+    def qfactory(self, prio):\n+        return track_closed(FifoDiskQueue)(self.mktemp())\n \n     def test_nonserializable_object_one(self):\n         self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n@@ -123,3 +147,37 @@ class DiskPriorityQueueTest(MemoryPriorityQueueTest):\n         self.assertEqual(self.q.pop(), None)\n         self.assertEqual(self.q.close(), [])\n \n+\n+class FifoDiskPriorityQueueTest(FifoMemoryPriorityQueueTest):\n+\n+    def qfactory(self, prio):\n+        return track_closed(FifoDiskQueue)(self.mktemp())\n+\n+    def test_nonserializable_object_one(self):\n+        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n+        self.assertEqual(self.q.close(), [])\n+\n+    def test_nonserializable_object_many_close(self):\n+        self.q.push('a', 3)\n+        self.q.push('b', 1)\n+        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n+        self.q.push('c', 2)\n+        self.assertEqual(self.q.pop(), 'b')\n+        self.assertEqual(sorted(self.q.close()), [2, 3])\n+\n+    def test_nonserializable_object_many_pop(self):\n+        self.q.push('a', 3)\n+        self.q.push('b', 1)\n+        self.assertRaises(TypeError, self.q.push, lambda x: x, 0)\n+        self.q.push('c', 2)\n+        self.assertEqual(self.q.pop(), 'b')\n+        self.assertEqual(self.q.pop(), 'c')\n+        self.assertEqual(self.q.pop(), 'a')\n+        self.assertEqual(self.q.pop(), None)\n+        self.assertEqual(self.q.close(), [])\n+\n+\n+class LifoDiskPriorityQueueTest(LifoMemoryPriorityQueueTest):\n+\n+    def qfactory(self, prio):\n+        return track_closed(LifoDiskQueue)(self.mktemp())\n\n@@ -1,12 +1,12 @@\n import os, glob\n from twisted.trial import unittest\n \n-from scrapy.utils.queue import MemoryQueue, DiskQueue\n+from scrapy.utils.queue import FifoMemoryQueue, LifoMemoryQueue, FifoDiskQueue, LifoDiskQueue\n \n-class MemoryQueueTest(unittest.TestCase):\n+class FifoMemoryQueueTest(unittest.TestCase):\n \n     def queue(self):\n-        return MemoryQueue()\n+        return FifoMemoryQueue()\n \n     def test_empty(self):\n         \"\"\"Empty queue test\"\"\"\n@@ -52,7 +52,56 @@ class MemoryQueueTest(unittest.TestCase):\n         self.assertEqual(len(q), 0)\n \n \n-class DiskQueueTest(MemoryQueueTest):\n+class LifoMemoryQueueTest(unittest.TestCase):\n+\n+    def queue(self):\n+        return LifoMemoryQueue()\n+\n+    def test_empty(self):\n+        \"\"\"Empty queue test\"\"\"\n+        q = self.queue()\n+        assert q.pop() is None\n+\n+    def test_push_pop1(self):\n+        \"\"\"Basic push/pop test\"\"\"\n+        q = self.queue()\n+        q.push('a')\n+        q.push('b')\n+        q.push('c')\n+        self.assertEqual(q.pop(), 'c')\n+        self.assertEqual(q.pop(), 'b')\n+        self.assertEqual(q.pop(), 'a')\n+        self.assertEqual(q.pop(), None)\n+\n+    def test_push_pop2(self):\n+        \"\"\"Test interleaved push and pops\"\"\"\n+        q = self.queue()\n+        q.push('a')\n+        q.push('b')\n+        q.push('c')\n+        q.push('d')\n+        self.assertEqual(q.pop(), 'd')\n+        self.assertEqual(q.pop(), 'c')\n+        q.push('e')\n+        self.assertEqual(q.pop(), 'e')\n+        self.assertEqual(q.pop(), 'b')\n+        self.assertEqual(q.pop(), 'a')\n+\n+    def test_len(self):\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        q.push('a')\n+        self.assertEqual(len(q), 1)\n+        q.push('b')\n+        q.push('c')\n+        self.assertEqual(len(q), 3)\n+        q.pop()\n+        q.pop()\n+        q.pop()\n+        self.assertEqual(len(q), 0)\n+\n+\n+class FifoDiskQueueTest(FifoMemoryQueueTest):\n \n     chunksize = 100000\n \n@@ -60,7 +109,7 @@ class DiskQueueTest(MemoryQueueTest):\n         self.qdir = self.mktemp()\n \n     def queue(self):\n-        return DiskQueue(self.qdir, chunksize=self.chunksize)\n+        return FifoDiskQueue(self.qdir, chunksize=self.chunksize)\n \n     def test_close_open(self):\n         \"\"\"Test closing and re-opening keeps state\"\"\"\n@@ -108,14 +157,68 @@ class DiskQueueTest(MemoryQueueTest):\n         assert not os.path.exists(self.qdir)\n \n \n-class ChunkSize1DiskQueueTest(DiskQueueTest):\n+class ChunkSize1FifoDiskQueueTest(FifoDiskQueueTest):\n     chunksize = 1\n \n-class ChunkSize2DiskQueueTest(DiskQueueTest):\n+class ChunkSize2FifoDiskQueueTest(FifoDiskQueueTest):\n     chunksize = 2\n \n-class ChunkSize3DiskQueueTest(DiskQueueTest):\n+class ChunkSize3FifoDiskQueueTest(FifoDiskQueueTest):\n     chunksize = 3\n \n-class ChunkSize4DiskQueueTest(DiskQueueTest):\n+class ChunkSize4FifoDiskQueueTest(FifoDiskQueueTest):\n     chunksize = 4\n+\n+\n+class LifoDiskQueueTest(LifoMemoryQueueTest):\n+\n+    def setUp(self):\n+        self.path = self.mktemp()\n+\n+    def queue(self):\n+        return LifoDiskQueue(self.path)\n+\n+    def test_close_open(self):\n+        \"\"\"Test closing and re-opening keeps state\"\"\"\n+        q = self.queue()\n+        q.push('a')\n+        q.push('b')\n+        q.push('c')\n+        q.push('d')\n+        self.assertEqual(q.pop(), 'd')\n+        self.assertEqual(q.pop(), 'c')\n+        q.close()\n+        del q\n+        q = self.queue()\n+        self.assertEqual(len(q), 2)\n+        q.push('e')\n+        self.assertEqual(q.pop(), 'e')\n+        self.assertEqual(q.pop(), 'b')\n+        q.close()\n+        del q\n+        q = self.queue()\n+        self.assertEqual(q.pop(), 'a')\n+        self.assertEqual(len(q), 0)\n+\n+    def test_cleanup(self):\n+        \"\"\"Test queue file is removed if queue is empty\"\"\"\n+        q = self.queue()\n+        assert os.path.exists(self.path)\n+        for x in range(5):\n+            q.push(str(x))\n+        for x in range(5):\n+            q.pop()\n+        q.close()\n+        assert not os.path.exists(self.path)\n+\n+    def test_file_size_shrinks(self):\n+        \"\"\"Test size of queue file shrinks when popping items\"\"\"\n+        q = self.queue()\n+        q.push('a')\n+        q.push('b')\n+        q.close()\n+        size = os.path.getsize(self.path)\n+        q = self.queue()\n+        q.pop()\n+        q.close()\n+        self.assertLess(os.path.getsize(self.path), size)\n\n@@ -8,7 +8,7 @@ from collections import deque\n from scrapy.utils.py26 import json\n \n \n-class MemoryQueue(object):\n+class FifoMemoryQueue(object):\n     \"\"\"Memory FIFO queue.\"\"\"\n \n     def __init__(self):\n@@ -28,7 +28,14 @@ class MemoryQueue(object):\n         return len(self.q)\n \n \n-class DiskQueue(object):\n+class LifoMemoryQueue(FifoMemoryQueue):\n+    \"\"\"Memory LIFO queue.\"\"\"\n+\n+    def push(self, obj):\n+        self.q.append(obj)\n+\n+\n+class FifoDiskQueue(object):\n     \"\"\"Persistent FIFO queue.\"\"\"\n \n     szhdr_format = \">L\"\n@@ -119,3 +126,52 @@ class DiskQueue(object):\n         os.remove(os.path.join(self.path, 'info.json'))\n         if not os.listdir(self.path):\n             os.rmdir(self.path)\n+\n+\n+\n+class LifoDiskQueue(object):\n+    \"\"\"Persistent LIFO queue.\"\"\"\n+\n+    SIZE_FORMAT = \">L\"\n+    SIZE_SIZE = struct.calcsize(SIZE_FORMAT)\n+\n+    def __init__(self, path):\n+        self.path = path\n+        if os.path.exists(path):\n+            self.f = open(path, 'rb+')\n+            qsize = self.f.read(self.SIZE_SIZE)\n+            self.size, = struct.unpack(self.SIZE_FORMAT, qsize)\n+            self.f.seek(0, os.SEEK_END)\n+        else:\n+            self.f = open(path, 'wb+')\n+            self.f.write(struct.pack(self.SIZE_FORMAT, 0))\n+            self.size = 0\n+\n+    def push(self, string):\n+        self.f.write(string)\n+        ssize = struct.pack(self.SIZE_FORMAT, len(string))\n+        self.f.write(ssize)\n+        self.size += 1\n+\n+    def pop(self):\n+        if not self.size:\n+            return\n+        self.f.seek(-self.SIZE_SIZE, os.SEEK_END)\n+        size, = struct.unpack(self.SIZE_FORMAT, self.f.read())\n+        self.f.seek(-size-self.SIZE_SIZE, os.SEEK_END)\n+        data = self.f.read(size)\n+        self.f.seek(-size, os.SEEK_CUR)\n+        self.f.truncate()\n+        self.size -= 1\n+        return data\n+\n+    def close(self):\n+        if self.size:\n+            self.f.seek(0)\n+            self.f.write(struct.pack(self.SIZE_FORMAT, self.size))\n+        self.f.close()\n+        if not self.size:\n+            os.remove(self.path)\n+\n+    def __len__(self):\n+        return self.size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
