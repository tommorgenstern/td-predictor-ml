{"custom_id": "scrapy#50862629138d8d6a10d3f8ceed2db01d1b58abdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 303 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,10 +41,7 @@ def load_object(path):\n         raise ValueError(\"Error loading object '%s': not a full path\" % path)\n \n     module, name = path[:dot], path[dot+1:]\n-    try:\n     mod = import_module(module)\n-    except ImportError as e:\n-        raise ImportError(\"Error loading object '%s': %s\" % (path, e))\n \n     try:\n         obj = getattr(mod, name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33a7c1d4380944d0a6e2efc017b85b1341fc72a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 423 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -6,9 +6,9 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n-from six.moves.urllib.parse import ParseResult, urlunparse, urldefrag, urlparse\n+from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n+                                    urlparse, parse_qsl)\n import urllib\n-import cgi\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this move doesn't break old code\n from w3lib.url import *\n@@ -54,7 +54,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n     \"\"\"\n \n     scheme, netloc, path, params, query, fragment = parse_url(url)\n-    keyvals = cgi.parse_qsl(query, keep_blank_values)\n+    keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urllib.urlencode(keyvals)\n     path = safe_url_string(_unquotepath(path)) or '/'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7db6bbce27e694243a96307e149c7092195da9b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 2066 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,16 +4,14 @@ This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.\n For more information see docs/topics/architecture.rst\n \n \"\"\"\n-import warnings\n from time import time\n \n from twisted.internet import defer\n from twisted.python.failure import Failure\n \n from scrapy import log, signals\n-from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning\n+from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\n@@ -63,10 +61,6 @@ class ExecutionEngine(object):\n         downloader_cls = load_object(self.settings['DOWNLOADER'])\n         self.downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n-        self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS', 1)\n-        if self._concurrent_spiders != 1:\n-            warnings.warn(\"CONCURRENT_SPIDERS settings is deprecated, use \" \\\n-                \"Scrapyd max_proc config instead\", ScrapyDeprecationWarning)\n         self._spider_closed_callback = spider_closed_callback\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22278056199244c3c87128ab9e5cf4743bb908c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2072 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -263,10 +263,8 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: slot.scheduler.close(reason))\n         dfd.addErrback(log.err, spider=spider)\n \n-        # XXX: spider_stats argument was added for backwards compatibility with\n-        # stats collection refactoring added in 0.15. it should be removed in 0.17.\n-        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(signal=signals.spider_closed, \\\n-            spider=spider, reason=reason, spider_stats=self.crawler.stats.get_stats()))\n+        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n+            signal=signals.spider_closed, spider=spider, reason=reason))\n         dfd.addErrback(log.err, spider=spider)\n \n         dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b422312a38b517d77f920ace68f24cbe0f60faaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 57 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008-2013, Scrapy developers'\n+copyright = u'2008-2014, Scrapy developers'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa84730e702a1d94ddeee0acbb81ac7394348b98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 14 | Methods Changed: 16 | Complexity Δ (Sum/Max): 17/9 | Churn Δ: 108 | Churn Cumulative: 1427 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 25 | DMM Complexity: 0.7313432835820896\n\nDIFF:\n@@ -9,7 +9,7 @@ from six.moves.urllib.parse import urldefrag\n from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n-from twisted.web.iweb import IBodyProducer\n+from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n from twisted.web.http import PotentialDataLoss\n from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \\\n@@ -19,6 +19,7 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n+from scrapy import log\n \n \n class HTTP11DownloadHandler(object):\n@@ -29,10 +30,14 @@ class HTTP11DownloadHandler(object):\n         self._pool._factory.noisy = False\n         self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n         self._contextFactory = self._contextFactoryClass()\n+        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n+        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n-        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool)\n+        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool,\n+            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n+            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize))\n         return agent.download_request(request)\n \n     def close(self):\n@@ -131,11 +136,14 @@ class ScrapyAgent(object):\n     _ProxyAgent = ProxyAgent\n     _TunnelingAgent = TunnelingAgent\n \n-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None):\n+    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n+                 maxsize=0, warnsize=0):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n         self._pool = pool\n+        self._maxsize = maxsize\n+        self._warnsize = warnsize\n \n     def _get_agent(self, request, timeout):\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n@@ -197,11 +205,25 @@ class ScrapyAgent(object):\n         if txresponse.length == 0:\n             return txresponse, '', None\n \n+        maxsize = request.meta.get('download_maxsize', self._maxsize)\n+        warnsize = request.meta.get('download_warnsize', self._warnsize)\n+        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n+\n+        if maxsize and expected_size > maxsize:\n+            log.msg(\"Expected response size (%s) larger than download max size (%s).\" % (expected_size, maxsize),\n+                    logLevel=log.ERROR)\n+            txresponse._transport._producer.loseConnection()\n+            raise defer.CancelledError()\n+\n+        if warnsize and expected_size > warnsize:\n+            log.msg(\"Expected response size (%s) larger than downlod warn size (%s).\" % (expected_size, warnsize),\n+                    logLevel=log.WARNING)\n+\n         def _cancel(_):\n             txresponse._transport._producer.loseConnection()\n \n         d = defer.Deferred(_cancel)\n-        txresponse.deliverBody(_ResponseReader(d, txresponse, request))\n+        txresponse.deliverBody(_ResponseReader(d, txresponse, request, maxsize, warnsize))\n         return d\n \n     def _cb_bodydone(self, result, request, url):\n@@ -232,14 +254,27 @@ class _RequestBodyProducer(object):\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished, txresponse, request):\n+    def __init__(self, finished, txresponse, request, maxsize, warnsize):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n         self._bodybuf = BytesIO()\n+        self._maxsize  = maxsize\n+        self._warnsize  = warnsize\n+        self._bytes_received = 0\n \n     def dataReceived(self, bodyBytes):\n         self._bodybuf.write(bodyBytes)\n+        self._bytes_received += len(bodyBytes)\n+\n+        if self._maxsize and self._bytes_received > self._maxsize:\n+            log.msg(\"Received (%s) bytes larger than download max size (%s).\" % (self._bytes_received, self._maxsize),\n+                    logLevel=log.ERROR)\n+            self._finished.cancel()\n+\n+        if self._warnsize and self._bytes_received > self._warnsize:\n+            log.msg(\"Received (%s) bytes larger than download warn size (%s).\" % (self._bytes_received, self._warnsize),\n+                    logLevel=log.WARNING)\n \n     def connectionLost(self, reason):\n         if self._finished.called:\n\n@@ -66,6 +66,9 @@ DOWNLOAD_HANDLERS_BASE = {\n \n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n+DOWNLOAD_MAXSIZE = 1073741824   # 1024m\n+DOWNLOAD_WARNSIZE = 33554432    # 32m\n+\n DOWNLOADER = 'scrapy.core.downloader.Downloader'\n \n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n\n@@ -30,6 +30,8 @@ from scrapy import optional_features\n from scrapy.utils.test import get_crawler\n from scrapy.exceptions import NotConfigured\n \n+from tests.mockserver import MockServer\n+from tests.spiders import SingleRequestSpider\n \n class DummyDH(object):\n \n@@ -211,6 +213,64 @@ class Http11TestCase(HttpTestCase):\n     if 'http11' not in optional_features:\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n+    def test_download_without_maxsize_limit(self):\n+        request = Request(self.getURL('file'))\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, \"0123456789\")\n+        return d\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize_per_req(self):\n+        meta = {'download_maxsize': 2}\n+        request = Request(self.getURL('file'), meta=meta)\n+        d = self.download_request(request, Spider('foo'))\n+        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_small_maxsize_per_spider(self):\n+        request = Request(self.getURL('file'))\n+        d = self.download_request(request, Spider('foo', download_maxsize=2))\n+        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+    def test_download_with_large_maxsize_per_spider(self):\n+        request = Request(self.getURL('file'))\n+        d = self.download_request(request, Spider('foo', download_maxsize=100))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, \"0123456789\")\n+        return d\n+\n+\n+class Http11MockServerTestCase(unittest.TestCase):\n+    \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n+    if 'http11' not in optional_features:\n+        skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_content_length(self):\n+        crawler = get_crawler(SingleRequestSpider)\n+        # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n+        # download it\n+        yield crawler.crawl(seed=Request(url='http://localhost:8998/partial', meta={'download_maxsize': 1000}))\n+        failure = crawler.spider.meta['failure']\n+        self.assertIsInstance(failure.value, defer.CancelledError)\n+\n+    @defer.inlineCallbacks\n+    def test_download(self):\n+        crawler = get_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=Request(url='http://localhost:8998'))\n+        failure = crawler.spider.meta.get('failure')\n+        self.assertTrue(failure == None)\n+        reason = crawler.spider.meta['close_reason']\n+        self.assertTrue(reason, 'finished')\n+\n \n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a69f042d1064c4beea46fa70084835dd6c91c143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 45 | Churn Cumulative: 377 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,8 +66,8 @@ DOWNLOAD_HANDLERS_BASE = {\n \n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n-DOWNLOAD_MAXSIZE = 1073741824   # 1024m\n-DOWNLOAD_WARNSIZE = 33554432    # 32m\n+DOWNLOAD_MAXSIZE = 1024*1024*1024   # 1024m\n+DOWNLOAD_WARNSIZE = 32*1024*1024    # 32m\n \n DOWNLOADER = 'scrapy.core.downloader.Downloader'\n \n\n@@ -1,9 +1,10 @@\n from __future__ import print_function\n import sys, time, random, urllib, os, json\n from subprocess import Popen, PIPE\n-from twisted.web.server import Site, NOT_DONE_YET\n-from twisted.web.resource import Resource\n+from twisted.web.server import Site, NOT_DONE_YET, GzipEncoderFactory\n+from twisted.web.resource import Resource, EncodingResourceWrapper\n from twisted.internet import reactor, defer, ssl\n+from twisted.web.test.test_webclient import PayloadResource\n from scrapy import twisted_version\n \n \n@@ -167,6 +168,8 @@ class Root(Resource):\n         self.putChild(\"drop\", Drop())\n         self.putChild(\"raw\", Raw())\n         self.putChild(\"echo\", Echo())\n+        self.putChild('payload', PayloadResource())\n+        self.putChild(\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n \n     def getChild(self, name, request):\n         return self\n\n@@ -220,6 +220,20 @@ class Http11TestCase(HttpTestCase):\n         d.addCallback(self.assertEquals, \"0123456789\")\n         return d\n \n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize(self):\n+        request = Request(self.getURL('file'))\n+\n+        # 10 is minimal size for this request and the limit is only counted on\n+        # response body. (regardless of headers)\n+        d = self.download_request(request, Spider('foo', download_maxsize=10))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEquals, \"0123456789\")\n+        yield d\n+\n+        d = self.download_request(request, Spider('foo', download_maxsize=9))\n+        yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n     @defer.inlineCallbacks\n     def test_download_with_maxsize_per_req(self):\n         meta = {'download_maxsize': 2}\n@@ -271,6 +285,26 @@ class Http11MockServerTestCase(unittest.TestCase):\n         reason = crawler.spider.meta['close_reason']\n         self.assertTrue(reason, 'finished')\n \n+    @defer.inlineCallbacks\n+    def test_download_gzip_response(self):\n+        crawler = get_crawler(SingleRequestSpider)\n+        body = '1'*100 # PayloadResource requires body length to be 100\n+        request = Request('http://localhost:8998/payload', method='POST', body=body, meta={'download_maxsize': 50})\n+        yield crawler.crawl(seed=request)\n+        failure = crawler.spider.meta['failure']\n+        # download_maxsize < 100, hence the CancelledError\n+        self.assertIsInstance(failure.value, defer.CancelledError)\n+\n+        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')\n+        request = request.replace(url='http://localhost:8998/xpayload')\n+        yield crawler.crawl(seed=request)\n+\n+        # download_maxsize = 50 is enough for the gzipped response\n+        failure = crawler.spider.meta.get('failure')\n+        self.assertTrue(failure == None)\n+        reason = crawler.spider.meta['close_reason']\n+        self.assertTrue(reason, 'finished')\n+\n \n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7910fa017243c6f64ecb55b4272894f6eb5d35ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 73 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class DownloadTimeoutMiddleware(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls(crawler.settings['DOWNLOAD_TIMEOUT'])\n+        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n\n@@ -8,8 +8,8 @@ from scrapy.utils.test import get_crawler\n \n class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n \n-    def get_request_spider_mw(self):\n-        crawler = get_crawler(Spider)\n+    def get_request_spider_mw(self, settings=None):\n+        crawler = get_crawler(Spider, settings)\n         spider = crawler._create_spider('foo')\n         request = Request('http://scrapytest.org/')\n         return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n@@ -20,6 +20,12 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta.get('download_timeout'), 180)\n \n+    def test_string_download_timeout(self):\n+        req, spider, mw = self.get_request_spider_mw({'DOWNLOAD_TIMEOUT': '20.1'})\n+        mw.spider_opened(spider)\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta.get('download_timeout'), 20.1)\n+\n     def test_spider_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         spider.download_timeout = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd193827546d2e20029c28961622ae7def7d541d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 17 | Churn Cumulative: 137 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,10 @@\n from __future__ import print_function\n import sys, time, random, urllib, os, json\n+import six\n from subprocess import Popen, PIPE\n-from twisted.web.server import Site, NOT_DONE_YET, GzipEncoderFactory\n-from twisted.web.resource import Resource, EncodingResourceWrapper\n+from twisted.web.server import Site, NOT_DONE_YET\n+from twisted.web.resource import Resource\n from twisted.internet import reactor, defer, ssl\n-from twisted.web.test.test_webclient import PayloadResource\n from scrapy import twisted_version\n \n \n@@ -168,6 +168,11 @@ class Root(Resource):\n         self.putChild(\"drop\", Drop())\n         self.putChild(\"raw\", Raw())\n         self.putChild(\"echo\", Echo())\n+\n+        if six.PY2 and twisted_version > (12, 3, 0):\n+            from twisted.web.test.test_webclient import PayloadResource\n+            from twisted.web.server import GzipEncoderFactory\n+            from twisted.web.resource import EncodingResourceWrapper\n             self.putChild('payload', PayloadResource())\n             self.putChild(\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n \n\n@@ -1,5 +1,6 @@\n import os\n import twisted\n+import six\n \n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n@@ -287,6 +288,9 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_gzip_response(self):\n+\n+        if six.PY2 and twisted_version > (12, 3, 0):\n+\n             crawler = get_crawler(SingleRequestSpider)\n             body = '1'*100 # PayloadResource requires body length to be 100\n             request = Request('http://localhost:8998/payload', method='POST', body=body, meta={'download_maxsize': 50})\n@@ -304,6 +308,8 @@ class Http11MockServerTestCase(unittest.TestCase):\n             self.assertTrue(failure == None)\n             reason = crawler.spider.meta['close_reason']\n             self.assertTrue(reason, 'finished')\n+        else:\n+            raise unittest.SkipTest(\"xpayload and payload endpoint only enabled for twisted > 12.3.0 and python 2.x\")\n \n \n class UriResource(resource.Resource):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e04b0aff74632702d78f7729b855c0737d50dfcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 18 | Churn Cumulative: 2482 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -173,7 +173,9 @@ class ExecutionEngine(object):\n     def schedule(self, request, spider):\n         self.signals.send_catch_log(signal=signals.request_scheduled,\n                 request=request, spider=spider)\n-        return self.slot.scheduler.enqueue_request(request)\n+        if not self.slot.scheduler.enqueue_request(request):\n+            self.signals.send_catch_log(signal=signals.request_dropped,\n+                                        request=request, spider=spider)\n \n     def download(self, request, spider):\n         slot = self.slot\n\n@@ -47,7 +47,7 @@ class Scheduler(object):\n     def enqueue_request(self, request):\n         if not request.dont_filter and self.df.request_seen(request):\n             self.df.log(request, self.spider)\n-            return\n+            return False\n         dqok = self._dqpush(request)\n         if dqok:\n             self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n@@ -55,6 +55,7 @@ class Scheduler(object):\n             self._mqpush(request)\n             self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n         self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n+        return True\n \n     def next_request(self):\n         request = self.mqs.pop()\n\n@@ -12,6 +12,7 @@ spider_idle = object()\n spider_closed = object()\n spider_error = object()\n request_scheduled = object()\n+request_dropped = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n\n@@ -79,6 +79,7 @@ class CrawlerRun(object):\n         self.spider = None\n         self.respplug = []\n         self.reqplug = []\n+        self.reqdropped = []\n         self.itemresp = []\n         self.signals_catched = {}\n \n@@ -95,6 +96,7 @@ class CrawlerRun(object):\n         self.crawler = get_crawler(TestSpider)\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n+        self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n         self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n         self.crawler.crawl(start_urls=start_urls)\n         self.spider = self.crawler.spider\n@@ -123,6 +125,9 @@ class CrawlerRun(object):\n     def request_scheduled(self, request, spider):\n         self.reqplug.append((request, spider))\n \n+    def request_dropped(self, request, spider):\n+        self.reqdropped.append((request, spider))\n+\n     def response_downloaded(self, response, spider):\n         self.respplug.append((response, spider))\n \n@@ -161,6 +166,11 @@ class EngineTest(unittest.TestCase):\n         urls_requested = set([rq[0].url for rq in self.run.reqplug])\n         urls_expected = set([self.run.geturl(p) for p in paths_expected])\n         assert urls_expected <= urls_requested\n+        scheduled_requests_count = len(self.run.reqplug)\n+        dropped_requests_count = len(self.run.reqdropped)\n+        responses_count = len(self.run.respplug)\n+        self.assertEqual(scheduled_requests_count,\n+                         dropped_requests_count + responses_count)\n \n     def _assert_downloaded_responses(self):\n         # response tests\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fdb6bb07c0cbe2cf664993555c376eeff750f28d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 30 | Churn Cumulative: 52 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,6 +59,12 @@ class TestSpider(Spider):\n             item['price'] = m.group(1)\n         return item\n \n+\n+class TestDupeFilterSpider(TestSpider):\n+    def make_requests_from_url(self, url):\n+        return Request(url)  # dont_filter=False\n+\n+\n def start_test_site(debug=False):\n     root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n@@ -75,25 +81,28 @@ def start_test_site(debug=False):\n class CrawlerRun(object):\n     \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, with_dupefilter=False):\n         self.spider = None\n         self.respplug = []\n         self.reqplug = []\n         self.reqdropped = []\n         self.itemresp = []\n         self.signals_catched = {}\n+        self.spider_class = TestSpider if not with_dupefilter else \\\n+            TestDupeFilterSpider\n \n     def run(self):\n         self.port = start_test_site()\n         self.portno = self.port.getHost().port\n \n-        start_urls = [self.geturl(\"/\"), self.geturl(\"/redirect\")]\n+        start_urls = [self.geturl(\"/\"), self.geturl(\"/redirect\"),\n+                      self.geturl(\"/redirect\")]  # a duplicate\n \n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n                 dispatcher.connect(self.record_signal, signal)\n \n-        self.crawler = get_crawler(TestSpider)\n+        self.crawler = get_crawler(self.spider_class)\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n@@ -146,10 +155,14 @@ class EngineTest(unittest.TestCase):\n         self.run = CrawlerRun()\n         yield self.run.run()\n         self._assert_visited_urls()\n-        self._assert_scheduled_requests()\n+        self._assert_scheduled_requests(urls_to_visit=8)\n         self._assert_downloaded_responses()\n         self._assert_scraped_items()\n         self._assert_signals_catched()\n+        self.run = CrawlerRun(with_dupefilter=True)\n+        yield self.run.run()\n+        self._assert_scheduled_requests(urls_to_visit=7)\n+        self._assert_dropped_requests()\n \n     def _assert_visited_urls(self):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\",\n@@ -158,8 +171,8 @@ class EngineTest(unittest.TestCase):\n         urls_expected = set([self.run.geturl(p) for p in must_be_visited])\n         assert urls_expected <= urls_visited, \"URLs not visited: %s\" % list(urls_expected - urls_visited)\n \n-    def _assert_scheduled_requests(self):\n-        self.assertEqual(6, len(self.run.reqplug))\n+    def _assert_scheduled_requests(self, urls_to_visit=None):\n+        self.assertEqual(urls_to_visit, len(self.run.reqplug))\n \n         paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n \n@@ -172,9 +185,12 @@ class EngineTest(unittest.TestCase):\n         self.assertEqual(scheduled_requests_count,\n                          dropped_requests_count + responses_count)\n \n+    def _assert_dropped_requests(self):\n+        self.assertEqual(len(self.run.reqdropped), 1)\n+\n     def _assert_downloaded_responses(self):\n         # response tests\n-        self.assertEqual(6, len(self.run.respplug))\n+        self.assertEqual(8, len(self.run.respplug))\n \n         for response, _ in self.run.respplug:\n             if self.run.getpath(response.url) == '/item999.html':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82b187f283b51c9c71e97d240e115b4abe09deec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 236 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from urlparse import unquote\n+\n from scrapy import optional_features\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n@@ -54,8 +56,8 @@ class S3DownloadHandler(object):\n         signed_headers = self.conn.make_request(\n                 method=request.method,\n                 bucket=bucket,\n-                key=p.path,\n-                query_args=p.query,\n+                key=unquote(p.path),\n+                query_args=unquote(p.query),\n                 headers=request.headers,\n                 data=request.body)\n         httpreq = request.replace(url=url, headers=signed_headers)\n\n@@ -482,6 +482,21 @@ class S3TestCase(unittest.TestCase):\n         self.assertEqual(httpreq.headers['Authorization'], \\\n                 'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n \n+    def test_request_signing7(self):\n+        # ensure that spaces are quoted properly before signing\n+        req = Request(\n+            (\"s3://johnsmith/photos/my puppy.jpg\"\n+             \"?response-content-disposition=my puppy.jpg\"),\n+            method='GET',\n+            headers={\n+                'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',\n+            })\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(\n+            httpreq.headers['Authorization'],\n+            'AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')\n+\n+\n class FTPTestCase(unittest.TestCase):\n \n     username = \"scrapy\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4cb03eded95dbd27af8dd90495fa4c277ef0b48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 679 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -81,8 +81,8 @@ class LxmlParserLinkExtractor(object):\n class LxmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None,\n-                 deny_extensions=None):\n+                 restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=True,\n+                 unique=True, process_value=None, deny_extensions=None):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n@@ -90,8 +90,8 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n             unique=unique, process=process_value)\n \n         super(LxmlLinkExtractor, self).__init__(lx, allow, deny,\n-            allow_domains, deny_domains, restrict_xpaths, canonicalize,\n-            deny_extensions)\n+            allow_domains, deny_domains, restrict_xpaths, restrict_css,\n+            canonicalize, deny_extensions)\n \n     def extract_links(self, response):\n         html = Selector(response)\n\n@@ -98,8 +98,8 @@ class BaseSgmlLinkExtractor(SGMLParser):\n class SgmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None,\n-                 deny_extensions=None):\n+                 restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,\n+                 process_value=None, deny_extensions=None):\n \n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n@@ -116,8 +116,8 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n                 unique=unique, process_value=process_value)\n \n         super(SgmlLinkExtractor, self).__init__(lx, allow, deny,\n-            allow_domains, deny_domains, restrict_xpaths, canonicalize,\n-            deny_extensions)\n+            allow_domains, deny_domains, restrict_xpaths, restrict_css,\n+            canonicalize, deny_extensions)\n \n         # FIXME: was added to fix a RegexLinkExtractor testcase\n         self.base_url = None\n\n@@ -5,6 +5,7 @@ scrapy.contrib.linkextractor).\n import re\n from six.moves.urllib.parse import urlparse\n \n+from scrapy.selector.csstranslator import ScrapyHTMLTranslator\n from scrapy.utils.url import url_is_from_any_domain\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension\n from scrapy.utils.misc import arg_to_iter\n@@ -38,8 +39,10 @@ _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'fil\n \n class FilteringLinkExtractor(object):\n \n+    _csstranslator = ScrapyHTMLTranslator()\n+\n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n-                 restrict_xpaths, canonicalize, deny_extensions):\n+                 restrict_xpaths, restrict_css, canonicalize, deny_extensions):\n \n         self.link_extractor = link_extractor\n \n@@ -50,6 +53,9 @@ class FilteringLinkExtractor(object):\n         self.deny_domains = set(arg_to_iter(deny_domains))\n \n         self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n+        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,\n+                                          arg_to_iter(restrict_css)))\n+\n         self.canonicalize = canonicalize\n         if deny_extensions is None:\n             deny_extensions = IGNORED_EXTENSIONS\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#403fc686b861b193a13a9abe038eda92aebf5ce8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -284,6 +284,21 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.org/foo', text=u'>\\u4eac<\\u4e1c',\n                                fragment='', nofollow=False)])\n \n+    def test_restrict_css(self):\n+        lx = self.extractor_cls(restrict_css=('#subwrapper a',))\n+        self.assertEqual(lx.extract_links(self.response), [\n+            Link(url='http://example.com/sample2.html', text=u'sample 2')\n+        ])\n+\n+    def test_restrict_css_and_restrict_xpaths_together(self):\n+        lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]', ),\n+                                restrict_css=('#subwrapper + a', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)], [\n+            Link(url='http://example.com/sample1.html', text=u''),\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+        ])\n+\n     def test_area_tag_with_unicode_present(self):\n         body = \"\"\"<html><body>\\xbe\\xa9<map><area href=\"http://example.org/foo\" /></map></body></html>\"\"\"\n         response = HtmlResponse(\"http://example.org\", body=body, encoding='utf-8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22247cf79152dbeb635b77f445ebd359c6d910db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 703 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -81,17 +81,18 @@ class LxmlParserLinkExtractor(object):\n class LxmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=True,\n-                 unique=True, process_value=None, deny_extensions=None):\n+                 tags=('a', 'area'), attrs=('href',), canonicalize=True,\n+                 unique=True, process_value=None, deny_extensions=None, restrict_css=()):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n         lx = LxmlParserLinkExtractor(tag=tag_func, attr=attr_func,\n             unique=unique, process=process_value)\n \n-        super(LxmlLinkExtractor, self).__init__(lx, allow, deny,\n-            allow_domains, deny_domains, restrict_xpaths, restrict_css,\n-            canonicalize, deny_extensions)\n+        super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n+            allow_domains=allow_domains, deny_domains=deny_domains,\n+            restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n+            canonicalize=canonicalize, deny_extensions=deny_extensions)\n \n     def extract_links(self, response):\n         html = Selector(response)\n\n@@ -98,8 +98,8 @@ class BaseSgmlLinkExtractor(SGMLParser):\n class SgmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,\n-                 process_value=None, deny_extensions=None):\n+                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,\n+                 process_value=None, deny_extensions=None, restrict_css=()):\n \n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n@@ -115,9 +115,10 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n             lx = BaseSgmlLinkExtractor(tag=tag_func, attr=attr_func,\n                 unique=unique, process_value=process_value)\n \n-        super(SgmlLinkExtractor, self).__init__(lx, allow, deny,\n-            allow_domains, deny_domains, restrict_xpaths, restrict_css,\n-            canonicalize, deny_extensions)\n+        super(SgmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n+            allow_domains=allow_domains, deny_domains=deny_domains,\n+            restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n+            canonicalize=canonicalize, deny_extensions=deny_extensions)\n \n         # FIXME: was added to fix a RegexLinkExtractor testcase\n         self.base_url = None\n\n@@ -42,7 +42,7 @@ class FilteringLinkExtractor(object):\n     _csstranslator = ScrapyHTMLTranslator()\n \n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n-                 restrict_xpaths, restrict_css, canonicalize, deny_extensions):\n+                 restrict_xpaths, canonicalize, deny_extensions, restrict_css):\n \n         self.link_extractor = link_extractor\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82d138e87e68b1e198dfdc3b0d7f62d0de2e1ceb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 38 | Churn Cumulative: 139 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,18 +2,18 @@ from scrapy.http import Response\n from scrapy.selector import Selector\n \n \n-def xmliter_lxml(obj, nodename, namespace=None):\n+def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n     from lxml import etree\n     reader = _StreamReader(obj)\n     tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n     iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n-    selxpath = '//' + ('x:%s' % nodename if namespace else nodename)\n+    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n     for _, node in iterable:\n         nodetext = etree.tostring(node)\n         node.clear()\n         xs = Selector(text=nodetext, type='xml')\n         if namespace:\n-            xs.register_namespace('x', namespace)\n+            xs.register_namespace(prefix, namespace)\n         yield xs.xpath(selxpath)[0]\n \n \n\n@@ -124,6 +124,38 @@ class LxmlXmliterTestCase(XmliterTestCase):\n         node = next(namespace_iter)\n         self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item2.jpg'])\n \n+    def test_xmliter_namespaces_prefix(self):\n+        body = \"\"\"\\\n+        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        <root>\n+            <h:table xmlns:h=\"http://www.w3.org/TR/html4/\">\n+              <h:tr>\n+                <h:td>Apples</h:td>\n+                <h:td>Bananas</h:td>\n+              </h:tr>\n+            </h:table>\n+\n+            <f:table xmlns:f=\"http://www.w3schools.com/furniture\">\n+              <f:name>African Coffee Table</f:name>\n+              <f:width>80</f:width>\n+              <f:length>120</f:length>\n+            </f:table>\n+\n+        </root>\n+        \"\"\"\n+        response = XmlResponse(url='http://mydummycompany.com', body=body)\n+        my_iter = self.xmliter(response, 'table', 'http://www.w3.org/TR/html4/', 'h')\n+\n+        node = next(my_iter)\n+        self.assertEqual(len(node.xpath('h:tr/h:td').extract()), 2)\n+        self.assertEqual(node.xpath('h:tr/h:td[1]/text()').extract(), ['Apples'])\n+        self.assertEqual(node.xpath('h:tr/h:td[2]/text()').extract(), ['Bananas'])\n+\n+        my_iter = self.xmliter(response, 'table', 'http://www.w3schools.com/furniture', 'f')\n+\n+        node = next(my_iter)\n+        self.assertEqual(node.xpath('f:name/text()').extract(), ['African Coffee Table'])\n+\n \n class UtilsCsvTestCase(unittest.TestCase):\n     sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#880c9e52b5c0a88df39e5fda6546771f4729bc47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ class CachingThreadedResolver(ThreadedResolver):\n     def getHostByName(self, name, timeout = (1, 3, 11, 45)):\n         if name in dnscache:\n             return defer.succeed(dnscache[name])\n-        d = ThreadedResolver.getHostByName(self, name, timeout)\n+        d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n         d.addCallback(self._cache_result, name)\n         return d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09ba4ff68a7ecb00036d02ffda9ee48f48b69402", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 1117 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,6 +32,7 @@ class HTTP11DownloadHandler(object):\n         self._contextFactory = self._contextFactoryClass()\n         self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n         self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n+        self._disconnect_timeout = 1\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n@@ -41,7 +42,24 @@ class HTTP11DownloadHandler(object):\n         return agent.download_request(request)\n \n     def close(self):\n-        return self._pool.closeCachedConnections()\n+        d = self._pool.closeCachedConnections()\n+        # closeCachedConnections will hang on network or server issues, so\n+        # we'll manually timeout the deferred.\n+        #\n+        # Twisted issue addressing this problem can be found here:\n+        # https://twistedmatrix.com/trac/ticket/7738.\n+        #\n+        # closeCachedConnections doesn't handle external errbacks, so we'll\n+        # issue a callback after `_disconnect_timeout` seconds.\n+        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n+\n+        def cancel_delayed_call(result):\n+            if delayed_call.active():\n+                delayed_call.cancel()\n+            return result\n+\n+        d.addBoth(cancel_delayed_call)\n+        return d\n \n \n class TunnelError(Exception):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
