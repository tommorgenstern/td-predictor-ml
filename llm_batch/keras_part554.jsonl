{"custom_id": "keras#602cdf11c13d265b8af4f9b01bb52191e3bdab88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1266 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -297,7 +297,7 @@ def serialize_with_public_class(cls, inner_config=None):\n \n     Called to check and retrieve the config of any class that has a public\n     Keras API or has been registered as serializable via\n-    `keras.utils.register_keras_serializable()`.\n+    `keras.saving.register_keras_serializable()`.\n     \"\"\"\n     # This gets the `keras.*` exported name, such as \"keras.optimizers.Adam\".\n     keras_api_name = tf_export.get_canonical_name_for_symbol(\n@@ -333,7 +333,7 @@ def serialize_with_public_fn(fn, config, fn_module_name=None):\n \n     Called to check and retrieve the config of any function that has a public\n     Keras API or has been registered as serializable via\n-    `keras.utils.register_keras_serializable()`. If function's module name is\n+    `keras.saving.register_keras_serializable()`. If function's module name is\n     already known, returns corresponding config.\n     \"\"\"\n     if fn_module_name:\n@@ -417,7 +417,7 @@ def deserialize_keras_object(\n       \"keras.engine.compile_utils\". Built-in Keras classes\n       expect to have prefix `keras`.\n     - `registered_name`: String. The key the class is registered under via\n-      `keras.utils.register_keras_serializable(package, name)` API. The key has\n+      `keras.saving.register_keras_serializable(package, name)` API. The key has\n       the format of '{package}>{name}', where `package` and `name` are the\n       arguments passed to `register_keras_serializable()`. If `name` is not\n       provided, it uses the class name. If `registered_name` successfully\n@@ -469,7 +469,7 @@ def deserialize_keras_object(\n     loss:\n \n     ```python\n-    @keras.utils.register_keras_serializable(package='my_package')\n+    @keras.saving.register_keras_serializable(package='my_package')\n     class ModifiedMeanSquaredError(keras.losses.MeanSquaredError):\n       ...\n \n@@ -581,7 +581,7 @@ def deserialize_keras_object(\n                         f\"`{config['class_name']}`. If \"\n                         f\"`{config['class_name']}` is a custom class, please \"\n                         \"register it using the \"\n-                        \"`@keras.utils.register_keras_serializable()` \"\n+                        \"`@keras.saving.register_keras_serializable()` \"\n                         \"decorator.\"\n                     )\n                 config = config[\"class_name\"]\n@@ -792,6 +792,6 @@ def _retrieve_class_or_fn(\n     raise TypeError(\n         f\"Could not locate {obj_type} '{name}'. \"\n         \"Make sure custom classes are decorated with \"\n-        \"`@keras.utils.register_keras_serializable()`. \"\n+        \"`@keras.saving.register_keras_serializable()`. \"\n         f\"Full object config: {full_config}\"\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1925ecdd2c0ecf3fbdc101deb73b2625e007549", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1138 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -109,12 +109,12 @@ class Hashing(base_layer.Layer):\n         bin, so the effective number of bins is `(num_bins - 1)` if `mask_value`\n         is set.\n       mask_value: A value that represents masked inputs, which are mapped to\n-        index 0. None means no mask term will be added and the\n+        index 0. `None` means no mask term will be added and the\n         hashing will start at index 0. Defaults to `None`.\n       salt: A single unsigned integer or None.\n         If passed, the hash function used will be SipHash64, with these values\n         used as an additional input (known as a \"salt\" in cryptography).\n-        These should be non-zero. If None, uses the FarmHash64 hash function.\n+        These should be non-zero. If `None`, uses the FarmHash64 hash function.\n         It also supports tuple/list of 2 unsigned integer numbers, see\n         reference paper for details. Defaults to `None`.\n       output_mode: Specification for the output of the layer. Values can bes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58796708934cf85db3106d44f9373099a3f6dc02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 17865 | Contributors (this commit): 24 | Commits (past 90d): 32 | Contributors (cumulative): 52 | DMM Complexity: None\n\nDIFF:\n@@ -756,7 +756,7 @@ def preprocess_input(x, data_format=None):\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. `None` means\n         the global setting `tf.keras.backend.image_data_format()` is used\n-        (unless you changed it, it uses \"channels_last\").{mode}.\n+        (unless you changed it, it uses \"channels_last\").\n         Defaults to `None`.\n \n     Returns:\n\n@@ -854,7 +854,7 @@ def preprocess_input(x, data_format=None):\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. `None` means\n         the global setting `tf.keras.backend.image_data_format()` is used\n-        (unless you changed it, it uses \"channels_last\").{mode}.\n+        (unless you changed it, it uses \"channels_last\").\n         Defaults to `None`.\n \n     Returns:\n\n@@ -1345,7 +1345,7 @@ def preprocess_input(x, data_format=None):\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. `None` means\n         the global setting `tf.keras.backend.image_data_format()` is used\n-        (unless you changed it, it uses \"channels_last\").{mode}.\n+        (unless you changed it, it uses \"channels_last\").\n         Defaults to `None`.\n \n     Returns:\n\n@@ -1821,7 +1821,7 @@ def preprocess_input(x, data_format=None):\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. `None` means\n         the global setting `tf.keras.backend.image_data_format()` is used\n-        (unless you changed it, it uses \"channels_last\").{mode}.\n+        (unless you changed it, it uses \"channels_last\").\n         Defaults to `None`.\n \n     Returns:\n\n@@ -961,7 +961,7 @@ def preprocess_input(x, data_format=None):\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. `None` means\n         the global setting `tf.keras.backend.image_data_format()` is used\n-        (unless you changed it, it uses \"channels_last\").{mode}.\n+        (unless you changed it, it uses \"channels_last\").\n         Defaults to `None`.\n \n     Returns:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5719f7bfa93a5aa498c362c9ee43ef48f50995a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 488 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -44,8 +44,8 @@ class SharpnessAwareMinimization(Model):\n       rho: float. The gradients scaling factor. Defaults to `0.05`.\n       num_batch_splits: int. The number of mini batches to\n         split into from each data batch. If None, batches are not split into\n-        sub-batches. Defaults to None.\n-      name: string. The name of the SAM model. Defaults to None.\n+        sub-batches. Defaults to `None`.\n+      name: string. The name of the SAM model. Defaults to `None`.\n \n     Reference:\n       [Pierre Foret et al., 2020](https://arxiv.org/abs/2010.01412)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af5d96a90206585b046e25d2ee694f607196e966", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2464 | Contributors (this commit): 27 | Commits (past 90d): 3 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -154,8 +154,8 @@ def hashing_trick(\n     Args:\n         text: Input text (string).\n         n: Dimension of the hashing space.\n-        hash_function: when None uses a python `hash` function, can be 'md5' or\n-            any function that takes in input a string and returns a int.\n+        hash_function: When `None` uses a python `hash` function. Can be 'md5'\n+            or any function that takes in input a string and returns a int.\n             Note that 'hash' is not a stable hashing function, so\n             it is not consistent across different runs, while 'md5'\n             is a stable hashing function. Defaults to `None`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8622cee02a7984437ee973e99635290102a24e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 4012 | Contributors (this commit): 12 | Commits (past 90d): 15 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -124,7 +124,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \n   Args:\n     include_top: Whether to include the fully-connected\n-      layer at the top of the network. Defaults to `True`.\n+      layer at the top of the network. Defaults to True.\n     weights: One of `None` (random initialization),\n       `\"imagenet\"` (pre-training on ImageNet-1k), or the path to the weights\n       file to be loaded. Defaults to `\"imagenet\"`.\n@@ -135,7 +135,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n       if `include_top` is False.\n       It should have exactly 3 inputs channels.\n     pooling: Optional pooling mode for feature extraction\n-      when `include_top` is `False`.\n+      when `include_top` is `False`. Defaults to None.\n       - `None` means that the output of the model will be\n         the 4D tensor output of the last convolutional layer.\n       - `avg` means that global average pooling\n@@ -144,16 +144,16 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n         the output of the model will be a 2D tensor.\n       - `max` means that global max pooling will\n         be applied.\n-      Defaults to `None`.\n     classes: Optional number of classes to classify images\n       into, only to be specified if `include_top` is True, and\n-      if no `weights` argument is specified. 1000 is how many\n-      ImageNet classes there are. Defaults to `1000`.\n+      if no `weights` argument is specified. Defaults to 1000 (number of\n+      ImageNet classes).\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      Defaults to `\"softmax\"`.\n       When loading pretrained weights, `classifier_activation` can only\n-      be `None` or `\"softmax\"`. Defaults to `\"softmax\"`.\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n@@ -754,10 +754,10 @@ def preprocess_input(x, data_format=None):\n \n     Args:\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n-      data_format: Optional data format of the image tensor/array. `None` means\n-        the global setting `tf.keras.backend.image_data_format()` is used\n-        (unless you changed it, it uses \"channels_last\").{mode}.\n-        Defaults to `None`.\n+      data_format: Optional data format of the image tensor/array. Defaults to\n+        None, in which case the global setting\n+        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+        it defaults to \"channels_last\").{mode}\n \n     Returns:\n       Unchanged `numpy.array` or `tf.Tensor`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19dcdeb0dd5d3f310a0f1f1c862706c97dc6af61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5512 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -406,7 +406,7 @@ class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n     Args:\n       inner_optimizer: The `tf.keras.optimizers.Optimizer` or\n         `tf.keras.optimizers.experimental.Optimizer` instance to wrap.\n-      dynamic: Bool indicating whether dynamic loss scaling is used.  If True,\n+      dynamic: Bool indicating whether dynamic loss scaling is used. If `True`,\n         the loss scale will be dynamically updated over time using an algorithm\n         that keeps the loss scale at approximately its optimal value. If False,\n         a single fixed loss scale is used and  `initial_scale` must be\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1801651eda912e2d16914141a2e81b994ccf4afb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 529 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -43,14 +43,14 @@ class DeterministicRandomTestTool(object):\n     This applies both to the stateful random operations used for creating and\n     initializing variables, and to the stateful random operations used in\n     computation (such as for dropout layers).\n-    \"\"\"\n \n-    def __init__(self, seed: int = 42, mode=\"constant\"):\n-        \"\"\"\n     Args:\n       mode: Set mode to 'constant' or 'num_random_ops'. Defaults to\n         'constant'.\n+      seed: The random seed to use.\n     \"\"\"\n+\n+    def __init__(self, seed: int = 42, mode=\"constant\"):\n         if mode not in {\"constant\", \"num_random_ops\"}:\n             raise ValueError(\n                 \"Mode arg must be 'constant' or 'num_random_ops'. \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7d08cd26903f0253382ff1ed081a0b5cd98b58e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 5131 | Contributors (this commit): 26 | Commits (past 90d): 12 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1315,9 +1315,7 @@ class DataHandler:\n         )\n \n         if self._inferred_steps == 0:\n-            raise ValueError(\n-                \"Expected input data to `fit()` to be non-empty.\"\n-            )\n+            raise ValueError(\"Expected input data to `fit()` to be non-empty.\")\n \n     def _configure_dataset_and_inferred_steps(\n         self, strategy, x, steps_per_epoch, class_weight, distribute\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#263660d4fda7bdb77626c1bff85c61468c793b2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 2384 | Contributors (this commit): 36 | Commits (past 90d): 10 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -358,9 +358,8 @@ def gelu(x, approximate=False):\n         if `approximate` is `False`.\n \n     Reference:\n-        - [Gaussian Error Linear Units (GELUs)](\n-            https://arxiv.org/abs/1606.08415)\n-    \"\"\"\n+        - [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)\n+    \"\"\"  # noqa: E501\n     return tf.nn.gelu(x, approximate)\n \n \n@@ -537,7 +536,7 @@ def serialize(activation, use_legacy_format=False):\n     Args:\n         activation : Function object.\n         use_legacy_format: Boolean, whether to use the legacy format for\n-            serialization.\n+            serialization. Defaults to False.\n \n     Returns:\n         String denoting the name attribute of the input function\n@@ -614,7 +613,7 @@ def deserialize(name, custom_objects=None, use_legacy_format=False):\n         custom_objects: Optional `{function_name: function_obj}`\n             dictionary listing user-provided activation functions.\n         use_legacy_format: Boolean, whether to use the legacy format for\n-            deserialization.\n+            deserialization. Defaults to False.\n \n     Returns:\n         Corresponding activation function.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ebc45c042053ddf9d078d9908b34f36aff06ea2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 5160 | Contributors (this commit): 46 | Commits (past 90d): 18 | Contributors (cumulative): 58 | DMM Complexity: 0.0\n\nDIFF:\n@@ -573,7 +573,8 @@ def serialize(activation, use_legacy_format=False):\n             f\"Unknown activation function '{activation}' cannot be \"\n             \"serialized due to invalid function name. Make sure to use \"\n             \"an activation name that matches the references defined in \"\n-            \"activations.py or use `@keras.utils.register_keras_serializable()`\"\n+            \"activations.py or use \"\n+            \"`@keras.saving.register_keras_serializable()` \"\n             \"to register any custom activations. \"\n             f\"config={fn_config}\"\n         )\n\n@@ -86,7 +86,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n     1. Any callable can be passed to this Layer, but if you want to serialize\n        this object you should only pass functions that are registered Keras\n-       serializables (see `tf.keras.utils.register_keras_serializable` for more\n+       serializables (see `tf.keras.saving.register_keras_serializable` for more\n        details).\n     2. When using a custom callable for `standardize`, the data received\n        by the callable will be exactly as passed to this layer. The callable\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36c57e1959239646dfbcaf821b09df4c9fb0b708", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2629 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -670,11 +670,11 @@ def get_training_or_validation_split(samples, labels, validation_split, subset):\n \n     num_val_samples = int(validation_split * len(samples))\n     if subset == \"training\":\n-        print(f\"Using {len(samples) - num_val_samples} files for training.\")\n+        io_utils.print_msg(f\"Using {len(samples) - num_val_samples} files for training.\")\n         samples = samples[:-num_val_samples]\n         labels = labels[:-num_val_samples]\n     elif subset == \"validation\":\n-        print(f\"Using {num_val_samples} files for validation.\")\n+        io_utils.print_msg(f\"Using {num_val_samples} files for validation.\")\n         samples = samples[-num_val_samples:]\n         labels = labels[-num_val_samples:]\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eab55ee08cf316242ab065ec46cf8c5e9f395f32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 18643 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -691,6 +691,11 @@ def _current_graph(op_input_list, graph=None):\n     if graph and not isinstance(graph, tf.Graph):\n         raise TypeError(f\"Input graph needs to be a Graph: {graph}\")\n \n+    def _is_symbolic_tensor(tensor):\n+        if hasattr(tf, \"is_symbolic_tensor\"):\n+            return tf.is_symbolic_tensor(tensor)\n+        return type(tensor) == tf.Tensor\n+\n     # 1. We validate that all of the inputs are from the same graph. This is\n     #    either the supplied graph parameter, or the first one selected from one\n     #    the graph-element-valued inputs. In the latter case, we hold onto\n@@ -698,14 +703,9 @@ def _current_graph(op_input_list, graph=None):\n     #    informative error if a mismatch is found.\n     original_graph_element = None\n     for op_input in op_input_list:\n-        # Determine if this is a valid graph_element.\n-        # TODO(joshl): Note that we exclude subclasses of Tensor. Need to clean\n-        # this up.\n         if isinstance(\n-            op_input, (tf.Operation, tf.Tensor, tf.__internal__.CompositeTensor)\n-        ) and (\n-            (not isinstance(op_input, tf.Tensor)) or type(op_input) == tf.Tensor\n-        ):\n+            op_input, (tf.Operation, tf.__internal__.CompositeTensor)\n+        ) or _is_symbolic_tensor(op_input):\n             graph_element = op_input\n         else:\n             graph_element = _as_graph_element(op_input)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66652038407390cba9124f041f7f79b6ead9451b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 303/299 | Churn Δ: 57 | Churn Cumulative: 23591 | Contributors (this commit): 124 | Commits (past 90d): 12 | Contributors (cumulative): 146 | DMM Complexity: 0.7872340425531915\n\nDIFF:\n@@ -1366,6 +1366,14 @@ class ModelCheckpoint(Callback):\n                     f\"Got {options}.\"\n                 )\n         else:\n+            if filepath and filepath.endswith(\".keras\") and options is not None:\n+                raise ValueError(\n+                    \"The native Keras format does not support \"\n+                    \"the `options` argument. Please remove \"\n+                    \"the `options` argument, or use the SavedModel \"\n+                    \"format by removing the `.keras` extension from \"\n+                    \"the model filepath.\"\n+                )\n             if options is None or isinstance(\n                 options, tf.saved_model.SaveOptions\n             ):\n@@ -1563,6 +1571,8 @@ class ModelCheckpoint(Callback):\n                         self.model.save_weights(\n                             filepath, overwrite=True, options=self._options\n                         )\n+                    elif filepath.endswith(\".keras\"):\n+                        self.model.save(filepath, overwrite=True)\n                     else:\n                         self.model.save(\n                             filepath, overwrite=True, options=self._options\n\n@@ -1362,7 +1362,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         assert not os.path.exists(filepath)\n \n     @test_utils.run_v2_only\n-    def test_ModelCheckpoint_subclass_save_weights_false(self):\n+    def test_ModelCheckpoint_subclass_SavedModel_save_weights_false(self):\n         model = test_utils.get_small_subclass_mlp(NUM_HIDDEN, NUM_CLASSES)\n         model.compile(\n             loss=\"categorical_crossentropy\",\n@@ -1388,6 +1388,33 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         # Check that the filepath is a SavedModel directory.\n         self.assertIn(\"saved_model.pb\", os.listdir(filepath))\n \n+    @test_utils.run_v2_only\n+    def test_ModelCheckpoint_subclass_KerasV3_save_weights_false(self):\n+        model = test_utils.get_small_subclass_mlp(NUM_HIDDEN, NUM_CLASSES)\n+        model.compile(\n+            loss=\"categorical_crossentropy\",\n+            optimizer=\"rmsprop\",\n+            metrics=[\"acc\"],\n+        )\n+        temp_dir = self.get_temp_dir()\n+        self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n+        filepath = os.path.join(temp_dir, \"checkpoint.keras\")\n+        cbks = [\n+            keras.callbacks.ModelCheckpoint(filepath, save_weights_only=False)\n+        ]\n+\n+        (x_train, y_train), _ = test_utils.get_test_data(\n+            train_samples=TRAIN_SAMPLES,\n+            test_samples=TEST_SAMPLES,\n+            input_shape=(INPUT_DIM,),\n+            num_classes=NUM_CLASSES,\n+        )\n+        y_train = np_utils.to_categorical(y_train, num_classes=NUM_CLASSES)\n+\n+        model.fit(x_train, y_train, callbacks=cbks, epochs=1, verbose=0)\n+\n+        assert os.path.exists(filepath)\n+\n     def _get_dummy_resource_for_model_checkpoint_testing(self):\n         def get_input_datasets():\n             # Simple training input.\n@@ -1628,6 +1655,24 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         ):\n             model.fit(train_ds, epochs=1, callbacks=[callback])\n \n+    def test_ModelCheckpoint_KerasV3_save_options_error(self):\n+        (\n+            model,\n+            train_ds,\n+            callback,\n+            filepath,\n+        ) = self._get_dummy_resource_for_model_checkpoint_testing()\n+\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"temp.keras\")\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"The native Keras format does not support\"\n+        ):\n+            _ = keras.callbacks.ModelCheckpoint(\n+                filepath=filepath, options=tf.saved_model.SaveOptions()\n+            )\n+\n     def test_ModelCheckpoint_with_bad_path_placeholders(self):\n         (\n             model,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a66368339d515846dc0787516f42fd3e085e9d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 9516 | Contributors (this commit): 26 | Commits (past 90d): 19 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1273,8 +1273,9 @@ class DataHandler:\n \n         if steps_per_epoch == 0:\n             raise ValueError(\n-                \"Got argument `steps_per_epoch=0` passed to `fit()`.\"\n-                \"Try checking the argument and `Model.fit()` documentation.\"\n+                \"Unexpected value for `steps_per_epoch`. Received value is 0. \"\n+                \"Please check the docstring for `model.fit()` for supported \"\n+                \"values.\"\n             )\n \n         self._steps_per_epoch = steps_per_epoch\n@@ -1315,7 +1316,7 @@ class DataHandler:\n         )\n \n         if self._inferred_steps == 0:\n-            raise ValueError(\"Expected input data to `fit()` to be non-empty.\")\n+            raise ValueError(\"Expected input data to be non-empty.\")\n \n     def _configure_dataset_and_inferred_steps(\n         self, strategy, x, steps_per_epoch, class_weight, distribute\n\n@@ -1445,7 +1445,10 @@ class DataHandlerTest(test_combinations.TestCase):\n     def test_error_if_zero_steps_per_epoch(self):\n         data = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3]).batch(1)\n \n-        with self.assertRaisesRegex(ValueError, \"`steps_per_epoch=0`\"):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Unexpected value for `steps_per_epoch`. Received value is 0.\",\n+        ):\n             data_adapter.DataHandler(\n                 data, initial_epoch=0, epochs=2, steps_per_epoch=0\n             )\n@@ -1457,7 +1460,7 @@ class DataHandlerTest(test_combinations.TestCase):\n \n         with self.assertRaisesWithLiteralMatch(\n             ValueError,\n-            \"Expected input data to `fit()` to be non-empty.\",\n+            \"Expected input data to be non-empty.\",\n         ):\n             data_adapter.DataHandler(x[idx], y[idx])\n \n@@ -1466,7 +1469,7 @@ class DataHandlerTest(test_combinations.TestCase):\n \n         with self.assertRaisesWithLiteralMatch(\n             ValueError,\n-            \"Expected input data to `fit()` to be non-empty.\",\n+            \"Expected input data to be non-empty.\",\n         ):\n             data_adapter.DataHandler(data)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#032282b8a8f446fd8c42c043b87b20033778e87d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 223 | Files Changed: 6 | Hunks: 11 | Methods Changed: 16 | Complexity Δ (Sum/Max): -6/7 | Churn Δ: 290 | Churn Cumulative: 2791 | Contributors (this commit): 6 | Commits (past 90d): 60 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,10 +1,12 @@\n+import copy\n+\n from tensorflow import nest\n \n-from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.core.input_layer import InputLayer\n from keras_core.models.functional import Functional\n from keras_core.models.model import Model\n+from keras_core.saving import serialization_lib\n from keras_core.utils import tracking\n \n \n@@ -162,8 +164,40 @@ class Sequential(Model):\n         return True\n \n     def get_config(self):\n-        raise NotImplementedError\n+        layer_configs = []\n+        for layer in super().layers:\n+            # `super().layers` include the InputLayer if available (it is\n+            # filtered out of `self.layers`).\n+            layer_configs.append(\n+                serialization_lib.serialize_keras_object(layer)\n+            )\n+        config = Model.get_config(self)\n+        config[\"name\"] = self.name\n+        config[\"layers\"] = copy.deepcopy(layer_configs)\n+        if self._functional is not None:\n+            config[\"build_input_shape\"] = self._layers[0].batch_shape\n+        return config\n \n     @classmethod\n-    def from_config(cls, config):\n-        raise NotImplementedError\n+    def from_config(cls, config, custom_objects=None):\n+        if \"name\" in config:\n+            name = config[\"name\"]\n+            build_input_shape = config.get(\"build_input_shape\")\n+            layer_configs = config[\"layers\"]\n+        else:\n+            name = None\n+            layer_configs = config\n+        model = cls(name=name)\n+        for layer_config in layer_configs:\n+            layer = serialization_lib.deserialize_keras_object(\n+                layer_config,\n+                custom_objects=custom_objects,\n+            )\n+            model.add(layer)\n+        if (\n+            not model._functional\n+            and build_input_shape\n+            and isinstance(build_input_shape, (tuple, list))\n+        ):\n+            model.build(build_input_shape)\n+        return model\n\n@@ -170,5 +170,30 @@ class SequentialTest(testing.TestCase):\n         model.summary()\n \n     def test_serialization(self):\n-        # TODO\n-        pass\n+        # Unbuilt deferred\n+        model = Sequential(name=\"seq\")\n+        model.add(layers.Dense(4))\n+        model.add(layers.Dense(5))\n+        self.run_class_serialization_test(model)\n+\n+        # Built deferred\n+        model.build((2, 3))\n+        self.run_class_serialization_test(model)\n+\n+        # Regular\n+        model = Sequential(name=\"seq\")\n+        model.add(Input(shape=(2,), batch_size=3))\n+        model.add(layers.Dense(4))\n+        model.add(layers.Dense(5))\n+        self.run_class_serialization_test(model)\n+\n+        # Weird\n+        class DictLayer(layers.Layer):\n+            def call(self, inputs):\n+                assert isinstance(inputs, dict)\n+                return inputs\n+\n+        model = Sequential([DictLayer()])\n+        self.run_class_serialization_test(\n+            model, custom_objects={\"DictLayer\": DictLayer}\n+        )\n\n@@ -1,3 +1,2 @@\n-from keras_core.optimizers.adam import Adam\n from keras_core.optimizers.optimizer import Optimizer\n from keras_core.optimizers.sgd import SGD\n\n@@ -1,149 +0,0 @@\n-from keras_core import operations as ops\n-from keras_core.optimizers import optimizer\n-\n-\n-class Adam(optimizer.Optimizer):\n-    \"\"\"Optimizer that implements the Adam algorithm.\n-\n-    Adam optimization is a stochastic gradient descent method that is based on\n-    adaptive estimation of first-order and second-order moments.\n-\n-    According to\n-    [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n-    the method is \"*computationally\n-    efficient, has little memory requirement, invariant to diagonal rescaling of\n-    gradients, and is well suited for problems that are large in terms of\n-    data/parameters*\".\n-\n-    Args:\n-      learning_rate: A `Tensor`, floating point value, a schedule that is a\n-        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        learning rate. Defaults to `0.001`.\n-      beta_1: A float value or a constant float tensor, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n-      beta_2: A float value or a constant float tensor, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 2nd moment estimates. Defaults to\n-        `0.999`.\n-      epsilon: A small constant for numerical stability. This epsilon is\n-        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        `1e-7`.\n-      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n-        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-      {{base_optimizer_keyword_args}}\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        beta_1=0.9,\n-        beta_2=0.999,\n-        epsilon=1e-7,\n-        amsgrad=False,\n-        weight_decay=None,\n-        clipnorm=None,\n-        clipvalue=None,\n-        global_clipnorm=None,\n-        use_ema=False,\n-        ema_momentum=0.99,\n-        ema_overwrite_frequency=None,\n-        name=\"Adam\",\n-        **kwargs\n-    ):\n-        super().__init__(\n-            learning_rate=learning_rate,\n-            name=name,\n-            weight_decay=weight_decay,\n-            clipnorm=clipnorm,\n-            clipvalue=clipvalue,\n-            global_clipnorm=global_clipnorm,\n-            use_ema=use_ema,\n-            ema_momentum=ema_momentum,\n-            ema_overwrite_frequency=ema_overwrite_frequency,\n-            **kwargs\n-        )\n-        self.beta_1 = beta_1\n-        self.beta_2 = beta_2\n-        self.epsilon = epsilon\n-        self.amsgrad = amsgrad\n-\n-    def build(self, var_list):\n-        \"\"\"Initialize optimizer variables.\n-\n-        Adam optimizer has 3 types of variables: momentums, velocities and\n-        velocity_hat (only set when amsgrad is applied),\n-\n-        Args:\n-          var_list: list of model variables to build Adam variables on.\n-        \"\"\"\n-        if self.built:\n-            return\n-        super().build(var_list)\n-        self._momentums = []\n-        self._velocities = []\n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"m\"\n-                )\n-            )\n-            self._velocities.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"v\"\n-                )\n-            )\n-        if self.amsgrad:\n-            self._velocity_hats = []\n-            for var in var_list:\n-                self._velocity_hats.append(\n-                    self.add_variable_from_reference(\n-                        reference_variable=var, name=\"vhat\"\n-                    )\n-                )\n-\n-    def update_step(self, gradient, variable, learning_rate):\n-        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        beta_1_power = None\n-        beta_2_power = None\n-        lr = ops.cast(learning_rate, variable.dtype)\n-        gradient = ops.cast(gradient, variable.dtype)\n-        local_step = ops.cast(self.iterations + 1, variable.dtype)\n-        beta_1_power = ops.power(\n-            ops.cast(self.beta_1, variable.dtype), local_step\n-        )\n-        beta_2_power = ops.power(\n-            ops.cast(self.beta_2, variable.dtype), local_step\n-        )\n-\n-        m = self._momentums[self._get_variable_index(variable)]\n-        v = self._velocities[self._get_variable_index(variable)]\n-\n-        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n-\n-        m.assign(m + (gradient - m) * (1 - self.beta_1))\n-        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n-        if self.amsgrad:\n-            v_hat = self._velocity_hats[self._get_variable_index(variable)]\n-            v_hat.assign(ops.maximum(v_hat, v))\n-            v = v_hat\n-        variable.assign(variable - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"beta_1\": self.beta_1,\n-                \"beta_2\": self.beta_2,\n-                \"epsilon\": self.epsilon,\n-                \"amsgrad\": self.amsgrad,\n-            }\n-        )\n-        return config\n-\n-\n-Adam.__doc__ = Adam.__doc__.replace(\n-    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n-)\n\n@@ -1,65 +0,0 @@\n-import numpy as np\n-\n-from keras_core import backend\n-from keras_core import operations as ops\n-from keras_core import testing\n-from keras_core.optimizers.adam import Adam\n-\n-\n-class AdamTest(testing.TestCase):\n-    def test_config(self):\n-        optimizer = Adam(\n-            learning_rate=0.5,\n-            beta_1=0.5,\n-            beta_2=0.67,\n-            epsilon=1e-5,\n-            amsgrad=True,\n-        )\n-        self.run_class_serialization_test(optimizer)\n-\n-    def test_single_step(self):\n-        optimizer = Adam(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n-        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n-        optimizer.apply_gradients(zip([grads], [vars]))\n-        self.assertAllClose(vars, [0.5, 1.5, 2.5, 3.5], rtol=1e-4, atol=1e-4)\n-\n-    def test_weight_decay(self):\n-        grads, var1, var2, var3 = (\n-            np.zeros(()),\n-            backend.Variable(2.0),\n-            backend.Variable(2.0, name=\"exclude\"),\n-            backend.Variable(2.0),\n-        )\n-        optimizer_1 = Adam(learning_rate=1.0, weight_decay=0.004)\n-        optimizer_1.apply_gradients(zip([grads], [var1]))\n-\n-        optimizer_2 = Adam(learning_rate=1.0, weight_decay=0.004)\n-        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n-        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n-\n-        optimizer_3 = Adam(learning_rate=1.0, weight_decay=0.004)\n-        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n-        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n-\n-        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n-        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n-        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n-\n-    def test_clip_norm(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-    #     optimizer = Adam(clipnorm=1)\n-    #     grad = [np.array([100.0, 100.0])]\n-    #     clipped_grad = optimizer._clip_gradients(grad)\n-    #     self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n-\n-    def test_clip_value(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-    #     optimizer = Adam(clipvalue=1)\n-    #     grad = [np.array([100.0, 100.0])]\n-    #     clipped_grad = optimizer._clip_gradients(grad)\n-    #     self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -135,7 +135,7 @@ class Optimizer:\n         \"\"\"Add an all-zeros variable with the shape and dtype of a reference variable.\"\"\"\n         initializer = initializers.Zeros()\n         name = name or auto_name(self.__class__.__name__)\n-        return self.add_variable(\n+        self.add_variable(\n             shape=reference_variable.shape,\n             initializer=initializer,\n             dtype=reference_variable.dtype,\n@@ -475,7 +475,7 @@ class Optimizer:\n             \"use_ema\": self.use_ema,\n             \"ema_momentum\": self.ema_momentum,\n             \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n-            # TODO:support `jit_compile` \"jit_compile\": self.jit_compile,\n+            \"jit_compile\": self.jit_compile,\n         }\n         return config\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2677f8f00c3ef14809bad0fb2ee030d07f8aa6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 555 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -174,18 +174,22 @@ class SequentialTest(testing.TestCase):\n         model = Sequential(name=\"seq\")\n         model.add(layers.Dense(4))\n         model.add(layers.Dense(5))\n-        self.run_class_serialization_test(model)\n+        revived = self.run_class_serialization_test(model)\n+        self.assertLen(revived.layers, 2)\n \n         # Built deferred\n         model.build((2, 3))\n-        self.run_class_serialization_test(model)\n+        revived = self.run_class_serialization_test(model)\n+        self.assertLen(revived.layers, 2)\n \n         # Regular\n         model = Sequential(name=\"seq\")\n         model.add(Input(shape=(2,), batch_size=3))\n         model.add(layers.Dense(4))\n         model.add(layers.Dense(5))\n-        self.run_class_serialization_test(model)\n+        model.add(layers.Dense(6))\n+        revived = self.run_class_serialization_test(model)\n+        self.assertLen(revived.layers, 3)\n \n         # Weird\n         class DictLayer(layers.Layer):\n@@ -194,6 +198,7 @@ class SequentialTest(testing.TestCase):\n                 return inputs\n \n         model = Sequential([DictLayer()])\n-        self.run_class_serialization_test(\n+        revived = self.run_class_serialization_test(\n             model, custom_objects={\"DictLayer\": DictLayer}\n         )\n+        self.assertLen(revived.layers, 1)\n\n@@ -49,6 +49,7 @@ class TestCase(unittest.TestCase):\n         )\n         self.assertEqual(config_json, revived_config_json)\n         self.assertEqual(ref_dir, dir(revived_instance))\n+        return revived_instance\n \n     def run_layer_test(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#969dfa2372859ba5c74d492ae4f67548244a8e0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 16/6 | Churn Δ: 113 | Churn Cumulative: 1868 | Contributors (this commit): 5 | Commits (past 90d): 63 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,8 +58,6 @@ class Layer(Operation):\n         self._seed_generators = []\n         self._losses = []\n         self._variables = []\n-        self._trainable_variables = []\n-        self._non_trainable_variables = []\n         self._supports_masking = not utils.is_default(self.compute_mask)\n         self._build_shapes_dict = None\n         self._call_signature_parameters = [\n@@ -448,6 +446,39 @@ class Layer(Operation):\n         losses.extend(weight_regularization_losses)\n         return losses\n \n+    def save_own_variables(self, store):\n+        \"\"\"Saves the state of the layer.\n+\n+        You can override this method to take full control of how the state of\n+        the layer is saved upon calling `model.save()`.\n+\n+        Args:\n+            store: Dict where the state of the model will be saved.\n+        \"\"\"\n+        all_vars = self._variables\n+        for i, v in enumerate(all_vars):\n+            store[f\"{i}\"] = np.array(v)\n+\n+    def load_own_variables(self, store):\n+        \"\"\"Loads the state of the layer.\n+\n+        You can override this method to take full control of how the state of\n+        the layer is loaded upon calling `keras.models.load_model()`.\n+\n+        Args:\n+            store: Dict from which the state of the model will be loaded.\n+        \"\"\"\n+        all_vars = self._variables\n+        if len(store.keys()) != len(all_vars):\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Expected: {[v.name for v in all_vars]}\"\n+            )\n+        for i, v in enumerate(all_vars):\n+            v.assign(store[f\"{i}\"])\n+\n     def _clear_losses(self):\n         if backend.in_stateless_scope():\n             scope = backend.get_stateless_scope()\n\n@@ -1,6 +1,8 @@\n import re\n import warnings\n \n+import numpy as np\n+\n from keras_core import backend\n from keras_core import initializers\n from keras_core import operations as ops\n@@ -275,6 +277,29 @@ class Optimizer:\n     def learning_rate(self):\n         return self._get_current_learning_rate()\n \n+    def save_own_variables(self, store):\n+        \"\"\"Get the state of this optimizer object.\"\"\"\n+        for i, variable in enumerate(self.variables):\n+            store[str(i)] = np.array(variable)\n+\n+    def load_own_variables(self, store):\n+        \"\"\"Set the state of this optimizer object.\"\"\"\n+        if len(store.keys()) != len(self.variables):\n+            msg = (\n+                f\"Skipping variable loading for optimizer '{self.name}', \"\n+                f\"because it has {len(self.variables)} variables whereas \"\n+                f\"the saved optimizer has {len(store.keys())} variables. \"\n+            )\n+            if len(self.variables) == 0:\n+                msg += (\n+                    \"This is likely because the optimizer has not been \"\n+                    \"called/built yet.\"\n+                )\n+            warnings.warn(msg, stacklevel=2)\n+            return\n+        for i, variable in enumerate(self.variables):\n+            variable.assign(store[str(i)])\n+\n     def _get_current_learning_rate(self):\n         if isinstance(\n             self._learning_rate, learning_rate_schedule.LearningRateSchedule\n\n@@ -3,6 +3,7 @@ import warnings\n from keras_core import backend\n from keras_core import metrics as metrics_module\n from keras_core import operations as ops\n+from keras_core.saving import serialization_lib\n from keras_core.trainers.compile_utils import CompileLoss\n from keras_core.trainers.compile_utils import CompileMetrics\n from keras_core.utils import tracking\n@@ -48,6 +49,16 @@ class Trainer:\n         self.compiled = True\n         self._loss_tracker = metrics_module.Mean(name=\"loss\")\n \n+        self._compile_config = serialization_lib.SerializableDict(\n+            optimizer=optimizer,\n+            loss=loss,\n+            loss_weights=loss_weights,\n+            metrics=metrics,\n+            weighted_metrics=weighted_metrics,\n+            run_eagerly=run_eagerly,\n+            jit_compile=jit_compile,\n+        )\n+\n     @property\n     def jit_compile(self):\n         return self._jit_compile\n@@ -249,12 +260,44 @@ class Trainer:\n         raise NotImplementedError\n \n     def get_compile_config(self):\n-        # TODO\n-        pass\n+        \"\"\"Returns a serialized config with information for compiling the model.\n \n-    def compile_from_config(self):\n-        # TODO\n-        return {}\n+        This method returns a config dictionary containing all the information\n+        (optimizer, loss, metrics, etc.) with which the model was compiled.\n+\n+        Returns:\n+            A dict containing information for compiling the model.\n+        \"\"\"\n+        if self.compiled and hasattr(self, \"_compile_config\"):\n+            return self._compile_config.serialize()\n+\n+    def compile_from_config(self, config):\n+        \"\"\"Compiles the model with the information given in config.\n+\n+        This method uses the information in the config (optimizer, loss,\n+        metrics, etc.) to compile the model.\n+\n+        Args:\n+            config: Dict containing information for compiling the model.\n+        \"\"\"\n+        has_overridden_compile = self.__class__.compile != Trainer.compile\n+        if has_overridden_compile:\n+            warnings.warn(\n+                \"`compile()` was not called as part of model loading \"\n+                \"because the model's `compile()` method is custom. \"\n+                \"All subclassed Models that have `compile()` \"\n+                \"overridden should also override \"\n+                \"`get_compile_config()` and `compile_from_config(config)`. \"\n+                \"Alternatively, you can \"\n+                \"call `compile()` manually after loading.\",\n+                stacklevel=2,\n+            )\n+            return\n+        config = serialization_lib.deserialize_keras_object(config)\n+        self.compile(**config)\n+        if hasattr(self, \"optimizer\") and self.built:\n+            # Create optimizer variables.\n+            self.optimizer.build(self.trainable_variables)\n \n     def _should_eval(self, epoch, validation_freq):\n         epoch = epoch + 1  # one-index the user-facing epoch.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
