{"custom_id": "keras#1d265f4b67b1e2056f81e4ae9c6db437540d02dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,7 +16,6 @@ class DenseTest(testing.TestCase):\n             },\n             input_shape=(2, 3),\n             expected_output_shape=(2, 4),\n-            expected_output=None,\n             expected_num_trainable_weights=1,\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n@@ -34,7 +33,6 @@ class DenseTest(testing.TestCase):\n             },\n             input_shape=(2, 3, 4),\n             expected_output_shape=(2, 3, 5),\n-            expected_output=None,\n             expected_num_trainable_weights=2,\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28a076a76385fcfc30d6b51fb424ced7d54adb1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 67 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,10 @@\n+import numpy as np\n+\n from keras_core import testing\n from keras_core.layers.core.dense import Dense\n \n-class DenseTest(testing.TestCase):\n \n+class DenseTest(testing.TestCase):\n     def test_basics(self):\n         # 2D case, no bias.\n         self.run_layer_test(\n@@ -41,5 +43,15 @@ class DenseTest(testing.TestCase):\n         )\n \n     def test_correctness(self):\n-        # TODO\n-        pass\n+        layer = Dense(units=2, activation=\"relu\")\n+        layer.build((1, 2))\n+        layer.set_weights(\n+            [\n+                np.array([[1.0, -2.0], [3.0, -4.0]]),\n+                np.array([5.0, -6.0]),\n+            ]\n+        )\n+        inputs = np.array(\n+            [[-1.0, 2.0]],\n+        )\n+        self.assertAllClose(layer(inputs), [[10.0, 0.0]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b88084efdbb534bdd3bfa08378972f0942333dc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from keras_core.layers.core.dense import Dense\n \n \n class DenseTest(testing.TestCase):\n-    def test_basics(self):\n+    def test_dense_basics(self):\n         # 2D case, no bias.\n         self.run_layer_test(\n             Dense,\n@@ -42,7 +42,7 @@ class DenseTest(testing.TestCase):\n             supports_masking=False,\n         )\n \n-    def test_correctness(self):\n+    def test_dense_correctness(self):\n         layer = Dense(units=2, activation=\"relu\")\n         layer.build((1, 2))\n         layer.set_weights(\n\n@@ -7,9 +7,20 @@ from keras_core import testing\n \n \n class DropoutTest(testing.TestCase):\n-    def test_dropout_supports_masking(self):\n-        dropout = layers.Dropout(0.5)\n-        self.assertEqual(True, dropout.supports_masking)\n+    def test_dropout_basics(self):\n+        self.run_layer_test(\n+            layers.Dropout,\n+            init_kwargs={\n+                \"rate\": 0.2,\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=1,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n \n     def test_dropout_rescaling(self):\n         inputs = np.ones((20, 500))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#183f360112ff320da5e1eb4ee0d8f76fd2af00f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 11 | Methods Changed: 10 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 90 | Churn Cumulative: 279 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras_core.layers.core.activation import Activation\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.input_layer import Input\n from keras_core.layers.core.input_layer import InputLayer\n\n@@ -0,0 +1,39 @@\n+from keras_core import activations\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Activation\")\n+class Activation(Layer):\n+    \"\"\"Applies an activation function to an output.\n+\n+    Args:\n+        activation: Activation function. It could be\n+            a callable, or the name of an activation\n+            from the `keras_core.activations` namespace.\n+\n+    Example:\n+\n+    >>> layer = keras_core.layers.Activation('relu')\n+    >>> layer([-3.0, -1.0, 0.0, 2.0])\n+    [0.0, 0.0, 0.0, 2.0]\n+    >>> layer = keras_core.layers.Activation(keras_core.activations.relu)\n+    >>> layer([-3.0, -1.0, 0.0, 2.0])\n+    [0.0, 0.0, 0.0, 2.0]\n+    \"\"\"\n+\n+    def __init__(self, activation, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.activation = activations.get(activation)\n+\n+    def call(self, inputs):\n+        return self.activation(inputs)\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\"activation\": activations.serialize(self.activation)}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,33 @@\n+from keras_core import activations\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class ActivationTest(testing.TestCase):\n+    def test_dense_basics(self):\n+        self.run_layer_test(\n+            layers.Activation,\n+            init_kwargs={\n+                \"activation\": \"relu\",\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+        self.run_layer_test(\n+            layers.Activation,\n+            init_kwargs={\n+                \"activation\": activations.gelu,\n+            },\n+            input_shape=(2, 2),\n+            expected_output_shape=(2, 2),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n\n@@ -19,9 +19,9 @@ class Dense(Layer):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        name=None,\n+        **kwargs,\n     ):\n-        super().__init__(name=name)\n+        super().__init__(**kwargs)\n         self.units = units\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n@@ -57,6 +57,11 @@ class Dense(Layer):\n             x = x + self.bias\n         return self.activation(x)\n \n+    def compute_output_shape(self, input_shape):\n+        output_shape = list(input_shape)\n+        output_shape[-1] = self.units\n+        return tuple(output_shape)\n+\n     def get_config(self):\n         base_config = super().get_config()\n         config = {\n\n@@ -1,14 +1,14 @@\n import numpy as np\n \n+from keras_core import layers\n from keras_core import testing\n-from keras_core.layers.core.dense import Dense\n \n \n class DenseTest(testing.TestCase):\n     def test_dense_basics(self):\n         # 2D case, no bias.\n         self.run_layer_test(\n-            Dense,\n+            layers.Dense,\n             init_kwargs={\n                 \"units\": 4,\n                 \"activation\": \"relu\",\n@@ -26,7 +26,7 @@ class DenseTest(testing.TestCase):\n         )\n         # 3D case, some regularizers.\n         self.run_layer_test(\n-            Dense,\n+            layers.Dense,\n             init_kwargs={\n                 \"units\": 5,\n                 \"activation\": \"sigmoid\",\n@@ -43,7 +43,7 @@ class DenseTest(testing.TestCase):\n         )\n \n     def test_dense_correctness(self):\n-        layer = Dense(units=2, activation=\"relu\")\n+        layer = layers.Dense(units=2, activation=\"relu\")\n         layer.build((1, 2))\n         layer.set_weights(\n             [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c690e1b5a68d86d3c748cf20261d75084808d86c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from keras_core.layers.core.activation import Activation\n+from keras_core.layers.activations.activation import Activation\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.input_layer import Input\n from keras_core.layers.core.input_layer import InputLayer\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1181f444f2ec4b18d0d70fe8da1e6bfb060cac76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 98 | Churn Cumulative: 456 | Contributors (this commit): 4 | Commits (past 90d): 36 | Contributors (cumulative): 7 | DMM Complexity: 0.32\n\nDIFF:\n@@ -3,10 +3,59 @@ from keras_core import constraints\n from keras_core import initializers\n from keras_core import operations as ops\n from keras_core import regularizers\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n \n \n+@keras_core_export(\"keras_core.layers.Dense\")\n class Dense(Layer):\n+    \"\"\"Just your regular densely-connected NN layer.\n+\n+    `Dense` implements the operation:\n+    `output = activation(dot(input, kernel) + bias)`\n+    where `activation` is the element-wise activation function\n+    passed as the `activation` argument, `kernel` is a weights matrix\n+    created by the layer, and `bias` is a bias vector created by the layer\n+    (only applicable if `use_bias` is `True`).\n+\n+    Note: If the input to the layer has a rank greater than 2, `Dense`\n+    computes the dot product between the `inputs` and the `kernel` along the\n+    last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n+    For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n+    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n+    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n+    `batch_size * d0` such sub-tensors). The output in this case will have\n+    shape `(batch_size, d0, units)`.\n+\n+    Args:\n+        units: Positive integer, dimensionality of the output space.\n+        activation: Activation function to use.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix.\n+        bias_initializer: Initializer for the bias vector.\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix.\n+        bias_regularizer: Regularizer function applied to the bias vector.\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+        kernel_constraint: Constraint function applied to\n+            the `kernel` weights matrix.\n+        bias_constraint: Constraint function applied to the bias vector.\n+\n+    Input shape:\n+        N-D tensor with shape: `(batch_size, ..., input_dim)`.\n+        The most common situation would be\n+        a 2D input with shape `(batch_size, input_dim)`.\n+\n+    Output shape:\n+        N-D tensor with shape: `(batch_size, ..., units)`.\n+        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n+        the output would have shape `(batch_size, units)`.\n+    \"\"\"\n+\n     def __init__(\n         self,\n         units,\n@@ -32,9 +81,7 @@ class Dense(Layer):\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n \n-        if activity_regularizer:\n-            # TODO\n-            raise ValueError(\"activity_regularizer not yet supported.\")\n+        self.input_spec = InputSpec(min_ndim=2)\n \n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n@@ -49,6 +96,7 @@ class Dense(Layer):\n                 initializer=self.bias_initializer,\n                 regularizer=self.bias_regularizer,\n             )\n+        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n         self.built = True\n \n     def call(self, inputs):\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core import layers\n from keras_core import testing\n+from keras_core.backend import keras_tensor\n \n \n class DenseTest(testing.TestCase):\n@@ -55,3 +56,9 @@ class DenseTest(testing.TestCase):\n             [[-1.0, 2.0]],\n         )\n         self.assertAllClose(layer(inputs), [[10.0, 0.0]])\n+\n+    def test_dense_errors(self):\n+        with self.assertRaisesRegex(ValueError, \"incompatible with the layer\"):\n+            layer = layers.Dense(units=2, activation=\"relu\")\n+            layer(keras_tensor.KerasTensor((1, 2)))\n+            layer(keras_tensor.KerasTensor((1, 3)))\n\n@@ -13,6 +13,7 @@ And some more magic:\n - add_loss\n - metric tracking\n - RNG seed tracking\n+- activity regularization\n \"\"\"\n import collections\n import inspect\n@@ -24,6 +25,7 @@ from tensorflow import nest\n \n from keras_core import backend\n from keras_core import initializers\n+from keras_core import regularizers\n from keras_core import utils\n from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n@@ -38,8 +40,11 @@ from keras_core.utils.tracking import Tracker\n \n @keras_core_export([\"keras_core.Layer\", \"keras_core.layers.Layer\"])\n class Layer(Operation):\n-    def __init__(self, trainable=True, dtype=None, name=None):\n+    def __init__(\n+        self, activity_regularizer=None, trainable=True, dtype=None, name=None\n+    ):\n         super().__init__(name=name)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n         self._trainable = trainable\n         if dtype is None:\n             dtype = backend.floatx()\n@@ -316,9 +321,16 @@ class Layer(Operation):\n             kwargs[\"training\"] = training\n \n         # TODO: Populate mask argument(s)\n+\n+        # Call the layer.\n         with backend.name_scope(self.name):\n             outputs = super().__call__(*args, **kwargs)\n+            # Record activity regularizer loss.\n+            if self.activity_regularizer is not None:\n+                self.add_loss(self.activity_regularizer(outputs))\n+\n         # TODO: Set masks on outputs\n+        # self._set_mask_metadata(inputs, outputs, previous_mask)\n \n         # Destroy call context if we created it\n         self._maybe_reset_call_context()\n@@ -566,6 +578,29 @@ class Layer(Operation):\n                 deque.extendleft(layer._layers)\n         return layers\n \n+    def _set_mask_metadata(self, inputs, outputs, previous_mask):\n+        # Many `Layer`s don't need to call `compute_mask`.\n+        # This method is optimized to do as little work as needed for the common\n+        # case.\n+        if not self._supports_masking:\n+            return\n+\n+        flat_outputs = nest.flatten(outputs)\n+\n+        mask_already_computed = all(\n+            getattr(x, \"_keras_mask\", None) is not None for x in flat_outputs\n+        )\n+        if mask_already_computed:\n+            return\n+\n+        output_masks = self.compute_mask(inputs, previous_mask)\n+        if output_masks is None:\n+            return\n+\n+        flat_masks = nest.flatten(output_masks)\n+        for tensor, mask in zip(flat_outputs, flat_masks):\n+            tensor._keras_mask = mask\n+\n \n def get_arguments_dict(fn, *args, **kwargs):\n     \"\"\"Return a dict mapping argument names to their values.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de595e39a37a6b748f9ea40d63b0f3b5da7dd7c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 109 | Churn Cumulative: 1426 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.backend.floatx()` is used,\n             which default to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`).\n-        seed: TODO\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n@@ -39,7 +46,14 @@ def uniform(shape, minval=0.0, maxval=None, dtype=None, seed=None):\n             supported. If not specified, `keras.backend.floatx()` is used,\n             which default to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n-        seed: TODO\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n@@ -60,7 +74,14 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.backend.floatx()` is used,\n             which default to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n-        seed: TODO\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = draw_seed(seed)\n\n@@ -23,7 +23,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.backend.floatx()` is used,\n             which default to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`).\n-        seed: TODO\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n@@ -45,7 +52,14 @@ def uniform(shape, minval=0.0, maxval=None, dtype=None, seed=None):\n             supported. If not specified, `keras.backend.floatx()` is used,\n             which default to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n-        seed: TODO\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n@@ -70,7 +84,14 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.backend.floatx()` is used,\n             which default to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n-        seed: TODO\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n     dtype = dtype or floatx()\n     seed = tf_draw_seed(seed)\n\n@@ -13,6 +13,7 @@ from keras_core.initializers.random_initializers import LecunNormal\n from keras_core.initializers.random_initializers import LecunUniform\n from keras_core.initializers.random_initializers import RandomNormal\n from keras_core.initializers.random_initializers import RandomUniform\n+from keras_core.initializers.random_initializers import TruncatedNormal\n from keras_core.initializers.random_initializers import VarianceScaling\n from keras_core.saving import serialization_lib\n from keras_core.utils.naming import to_snake_case\n@@ -29,6 +30,7 @@ ALL_OBJECTS = {\n     LecunNormal,\n     LecunUniform,\n     RandomNormal,\n+    TruncatedNormal,\n     RandomUniform,\n     VarianceScaling,\n }\n\n@@ -55,6 +55,59 @@ class RandomNormal(Initializer):\n         return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": self.seed}\n \n \n+@keras_core_export(\"keras_core.initializers.TruncatedNormal\")\n+class TruncatedNormal(Initializer):\n+    \"\"\"Initializer that generates a truncated normal distribution.\n+\n+    The values generated are similar to values from a\n+    `RandomNormal` initializer, except that values more\n+    than two standard deviations from the mean are\n+    discarded and re-drawn.\n+\n+    Examples:\n+\n+    >>> # Standalone usage:\n+    >>> initializer = TruncatedNormal(mean=0., stddev=1.)\n+    >>> values = initializer(shape=(2, 2))\n+\n+    >>> # Usage in a Keras layer:\n+    >>> initializer = TruncatedNormal(mean=0., stddev=1.)\n+    >>> layer = Dense(3, kernel_initializer=initializer)\n+\n+    Args:\n+        mean: A python scalar or a scalar keras tensor. Mean of the random values to\n+            generate.\n+        stddev: A python scalar or a scalar keras tensor. Standard deviation of the\n+           random values to generate.\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n+\n+    def __init__(self, mean=0.0, stddev=1.0, seed=None):\n+        self.mean = mean\n+        self.stddev = stddev\n+        self.seed = seed or random.make_default_seed()\n+        super().__init__()\n+\n+    def __call__(self, shape, dtype=None):\n+        return random.truncated_normal(\n+            shape=shape,\n+            mean=self.mean,\n+            stddev=self.stddev,\n+            seed=self.seed,\n+            dtype=dtype,\n+        )\n+\n+    def get_config(self):\n+        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": self.seed}\n+\n+\n @keras_core_export(\"keras_core.initializers.RandomUniform\")\n class RandomUniform(Initializer):\n     \"\"\"Random uniform initializer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2b847f6b4dfc9f1cb0b2e189b231fa838f6c3f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 285 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -11,12 +11,12 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n     Args:\n         shape: The shape of the random values to generate.\n-        mean: Floats, default to 0. Mean of the random values to generate.\n-        stddev: Floats, default to 1. Standard deviation of the random values\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n             to generate.\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n-            which default to `float32` unless you configured it otherwise (via\n+            which defaults to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`).\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n@@ -33,18 +33,18 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     return sample * stddev + mean\n \n \n-def uniform(shape, minval=0.0, maxval=None, dtype=None, seed=None):\n+def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the uniform distribution.\n \n     Args:\n         shape: The shape of the random values to generate.\n-        minval: Floats, default to 0. Lower bound of the range of\n+        minval: Floats, defaults to 0. Lower bound of the range of\n             random values to generate (inclusive).\n-        minval: Floats, default to None. Upper bound of the range of\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n             random values to generate (exclusive).\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n-            which default to `float32` unless you configured it otherwise (via\n+            which defaults to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n@@ -67,12 +67,12 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n     Args:\n         shape: The shape of the random values to generate.\n-        mean: Floats, default to 0. Mean of the random values to generate.\n-        stddev: Floats, default to 1. Standard deviation of the random values\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n             to generate.\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n-            which default to `float32` unless you configured it otherwise (via\n+            which defaults to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n\n@@ -16,12 +16,12 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n     Args:\n         shape: The shape of the random values to generate.\n-        mean: Floats, default to 0. Mean of the random values to generate.\n-        stddev: Floats, default to 1. Standard deviation of the random values\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n             to generate.\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n-            which default to `float32` unless you configured it otherwise (via\n+            which defaults to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`).\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n@@ -39,18 +39,18 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     )\n \n \n-def uniform(shape, minval=0.0, maxval=None, dtype=None, seed=None):\n+def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the uniform distribution.\n \n     Args:\n         shape: The shape of the random values to generate.\n-        minval: Floats, default to 0. Lower bound of the range of\n+        minval: Floats, defaults to 0. Lower bound of the range of\n             random values to generate (inclusive).\n-        minval: Floats, default to None. Upper bound of the range of\n+        minval: Floats, defaults to 1. Upper bound of the range of\n             random values to generate (exclusive).\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n-            which default to `float32` unless you configured it otherwise (via\n+            which defaults to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n@@ -77,12 +77,12 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n     Args:\n         shape: The shape of the random values to generate.\n-        mean: Floats, default to 0. Mean of the random values to generate.\n-        stddev: Floats, default to 1. Standard deviation of the random values\n+        mean: Floats, defaults to 0. Mean of the random values to generate.\n+        stddev: Floats, defaults to 1. Standard deviation of the random values\n             to generate.\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n-            which default to `float32` unless you configured it otherwise (via\n+            which defaults to `float32` unless you configured it otherwise (via\n             `keras.backend.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a6b690b198b47fb113eaa05eb2e2496714732b26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 144 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n         shape: The shape of the random values to generate.\n         minval: Floats, defaults to 0. Lower bound of the range of\n             random values to generate (inclusive).\n-        minval: Floats, defaults to 1. Upper bound of the range of\n+        maxval: Floats, defaults to 1. Upper bound of the range of\n             random values to generate (exclusive).\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.backend.floatx()` is used,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#17c51418fa3c5678db12d932fe9041c728bb2cdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 193 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 18 | Methods Changed: 16 | Complexity Δ (Sum/Max): 24/12 | Churn Δ: 201 | Churn Cumulative: 592 | Contributors (this commit): 5 | Commits (past 90d): 50 | Contributors (cumulative): 14 | DMM Complexity: 0.8090909090909091\n\nDIFF:\n@@ -4,7 +4,7 @@ from keras_core import testing\n \n \n class ActivationTest(testing.TestCase):\n-    def test_dense_basics(self):\n+    def test_activation_basics(self):\n         self.run_layer_test(\n             layers.Activation,\n             init_kwargs={\n\n@@ -24,15 +24,16 @@ class Functional(Function, Model):\n     Symbolic add_loss\n     \"\"\"\n \n+    def __new__(cls, *args, **kwargs):\n+        # Skip Model.__new__.\n+        return Function.__new__(cls)\n+\n     @tracking.no_automatic_dependency_tracking\n     def __init__(self, inputs, outputs, name=None, **kwargs):\n         # This is used by the Model class, since we have some logic to swap the\n         # class in the __new__ method, which will lead to __init__ get invoked\n         # twice. Using the skip_init to skip one of the invocation of __init__\n         # to avoid any side effects\n-        skip_init = kwargs.pop(\"skip_init\", False)\n-        if skip_init:\n-            return\n         if isinstance(inputs, dict):\n             for k, v in inputs.items():\n                 if not isinstance(v, backend.KerasTensor):\n\n@@ -1,6 +1,9 @@\n+import inspect\n+\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.utils import python_utils\n from keras_core.utils import summary_utils\n \n if backend.backend() == \"tensorflow\":\n@@ -32,11 +35,11 @@ class Model(Trainer, Layer):\n \n     def __new__(cls, *args, **kwargs):\n         # Signature detection\n-        if functional_init_arguments(args, kwargs) and cls == Model:\n+        if functional_init_arguments(args, kwargs):\n             # Functional model\n             from keras_core.models import functional\n \n-            return functional.Functional(*args, **kwargs, skip_init=True)\n+            return functional.Functional(*args, **kwargs)\n         return Layer.__new__(cls)\n \n     def __init__(self, trainable=True, name=None, dtype=None):\n@@ -152,6 +155,32 @@ class Model(Trainer, Layer):\n     def export(self, filepath):\n         raise NotImplementedError\n \n+    @python_utils.default\n+    def get_config(self):\n+        # Prepare base arguments\n+        config = {\n+            \"name\": self.name,\n+            \"trainable\": self.trainable,\n+        }\n+        # Check whether the class has a constructor compatible with a Functional\n+        # model or if it has a custom constructor.\n+        if functional_like_constructor(self.__class__):\n+            # Only return a Functional config if the constructor is the same\n+            # as that of a Functional model. This excludes subclassed Functional\n+            # models with a custom __init__.\n+            config = {**config, **get_functional_config(self)}\n+        else:\n+            # Try to autogenerate config\n+            xtra_args = set(config.keys())\n+            if getattr(self, \"_auto_get_config\", False):\n+                config.update(self._auto_config.config)\n+            # Remove args non explicitly supported\n+            argspec = inspect.getfullargspec(self.__init__)\n+            if argspec.varkw != \"kwargs\":\n+                for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n+                    config.pop(key, None)\n+        return config\n+\n \n def functional_init_arguments(args, kwargs):\n     return (\n@@ -159,3 +188,11 @@ def functional_init_arguments(args, kwargs):\n         or (len(args) == 1 and \"outputs\" in kwargs)\n         or (\"inputs\" in kwargs and \"outputs\" in kwargs)\n     )\n+\n+\n+def functional_like_constructor(cls):\n+    raise NotImplementedError\n+\n+\n+def get_functional_config(model):\n+    raise NotImplementedError\n\n@@ -1,12 +1,26 @@\n+import inspect\n+import textwrap\n+\n+from tensorflow import nest\n+\n from keras_core import backend\n from keras_core.backend.keras_tensor import any_symbolic_tensors\n from keras_core.operations.node import Node\n+from keras_core.saving import serialization_lib\n+from keras_core.utils import python_utils\n from keras_core.utils.naming import auto_name\n \n \n class Operation:\n     def __init__(self, name=None):\n-        self.name = name or auto_name(self.__class__.__name__)\n+        if name is None:\n+            name = auto_name(self.__class__.__name__)\n+        if not isinstance(name, str):\n+            raise ValueError(\n+                \"Argument `name` should be a string. \"\n+                f\"Received instead: name={name} (of type {type(name)})\"\n+            )\n+        self.name = name\n         self._inbound_nodes = []\n         self._outbound_nodes = []\n \n@@ -44,12 +58,123 @@ class Operation:\n                 f\"Error encountered: {e}\"\n             )\n \n+    def __new__(cls, *args, **kwargs):\n+        \"\"\"We override __new__ to saving serializable constructor arguments.\n+\n+        These arguments are used to auto-generate an object serialization config,\n+        which enables user-created subclasses to be serializable out of the box\n+        in most cases without forcing the user to manually implement\n+        `get_config()`.\n+        \"\"\"\n+        # Generate a config to be returned by default by `get_config()`.\n+        arg_names = inspect.getfullargspec(cls.__init__).args\n+        kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n+        instance = super(Operation, cls).__new__(cls)\n+        # For safety, we only rely on auto-configs for a small set of\n+        # serializable types.\n+        supported_types = (str, int, float, bool, type(None))\n+        try:\n+            flat_arg_values = nest.flatten(kwargs)\n+            auto_config = True\n+            for value in flat_arg_values:\n+                if not isinstance(value, supported_types):\n+                    auto_config = False\n+                    break\n+        except TypeError:\n+            auto_config = False\n+        try:\n+            if auto_config:\n+                instance._auto_config = serialization_lib.SerializableDict(\n+                    **kwargs\n+                )\n+            else:\n+                instance._auto_config = None\n+        except RecursionError:\n+            # Setting an instance attribute in __new__ has the potential\n+            # to trigger an infinite recursion if a subclass overrides\n+            # setattr in an unsafe way.\n+            pass\n+        return instance\n+\n+    @python_utils.default\n     def get_config(self):\n-        return {\"name\": self.name}\n+        \"\"\"Returns the config of the object.\n+\n+        An object config is a Python dictionary (serializable)\n+        containing the information needed to re-insstantiate it.\n+\n+        Returns:\n+            Python dictionary.\n+        \"\"\"\n+        config = {\n+            \"name\": self.name,\n+        }\n+\n+        if not python_utils.is_default(self.get_config):\n+            # In this case the subclass implements get_config()\n+            return config\n+\n+        # In this case the subclass doesn't implement get_config():\n+        # Let's see if we can autogenerate it.\n+        if getattr(self, \"_auto_config\", None) is not None:\n+            xtra_args = set(config.keys())\n+            config.update(self._auto_config.config)\n+            # Remove args non explicitly supported\n+            argspec = inspect.getfullargspec(self.__init__)\n+            if argspec.varkw != \"kwargs\":\n+                for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n+                    config.pop(key, None)\n+            return config\n+        else:\n+            raise NotImplementedError(\n+                textwrap.dedent(\n+                    f\"\"\"\n+        Object {self.__class__.__name__} was created by passing\n+        non-serializable argument values in `__init__()`,\n+        and therefore the object must override `get_config()` in\n+        order to be serializable. Please implement `get_config()`.\n+\n+        Example:\n+\n+        class CustomLayer(keras.layers.Layer):\n+            def __init__(self, arg1, arg2, **kwargs):\n+                super().__init__(**kwargs)\n+                self.arg1 = arg1\n+                self.arg2 = arg2\n+\n+            def get_config(self):\n+                config = super().get_config()\n+                config.update({\n+                    \"arg1\": self.arg1,\n+                    \"arg2\": self.arg2,\n+                })\n+                return config\"\"\"\n+                )\n+            )\n \n     @classmethod\n     def from_config(cls, config):\n+        \"\"\"Creates a layer from its config.\n+\n+        This method is the reverse of `get_config`,\n+        capable of instantiating the same layer from the config\n+        dictionary. It does not handle layer connectivity\n+        (handled by Network), nor weights (handled by `set_weights`).\n+\n+        Args:\n+            config: A Python dictionary, typically the\n+                output of get_config.\n+\n+        Returns:\n+            A layer instance.\n+        \"\"\"\n+        try:\n             return cls(**config)\n+        except Exception as e:\n+            raise TypeError(\n+                f\"Error when deserializing class '{cls.__name__}' using \"\n+                f\"config={config}.\\n\\nException encountered: {e}\"\n+            )\n \n     def __repr__(self):\n         return f\"<Operation name={self.name}>\"\n\n@@ -26,6 +26,21 @@ class OpWithMultipleOutputs(operation.Operation):\n         )\n \n \n+class OpWithCustomConstructor(operation.Operation):\n+    def __init__(self, alpha, mode=\"foo\"):\n+        super().__init__()\n+        self.alpha = alpha\n+        self.mode = mode\n+\n+    def call(self, x):\n+        if self.mode == \"foo\":\n+            return x\n+        return self.alpha * x\n+\n+    def compute_output_spec(self, x):\n+        return keras_tensor.KerasTensor(x.shape, x.dtype)\n+\n+\n class OperationTest(testing.TestCase):\n     def test_symbolic_call(self):\n         x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n@@ -118,6 +133,13 @@ class OperationTest(testing.TestCase):\n         op = OpWithMultipleOutputs.from_config(config)\n         self.assertEqual(op.name, \"test_op\")\n \n+    def test_autoconfig(self):\n+        op = OpWithCustomConstructor(alpha=0.2, mode=\"bar\")\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"mode\": \"bar\"})\n+        revived = OpWithCustomConstructor.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+\n     def test_input_conversion(self):\n         x = np.ones((2,))\n         y = np.ones((2,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#389318ee9d387b88bc8dfd3a9dbb52863fc6799d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 37 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/7 | Churn Δ: 73 | Churn Cumulative: 470 | Contributors (this commit): 5 | Commits (past 90d): 36 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import inspect\n import warnings\n \n from tensorflow import nest\n@@ -7,6 +8,7 @@ from keras_core import operations as ops\n from keras_core.layers.layer import Layer\n from keras_core.models.model import Model\n from keras_core.operations.function import Function\n+from keras_core.utils import python_utils\n from keras_core.utils import tracking\n \n \n@@ -184,6 +186,32 @@ class Functional(Function, Model):\n         # Symbolic only. TODO\n         raise NotImplementedError\n \n+    @python_utils.default\n+    def get_config(self):\n+        # Prepare base arguments\n+        config = {\n+            \"name\": self.name,\n+            \"trainable\": self.trainable,\n+        }\n+        # Check whether the class has a constructor compatible with a Functional\n+        # model or if it has a custom constructor.\n+        if functional_like_constructor(self.__class__):\n+            # Only return a Functional config if the constructor is the same\n+            # as that of a Functional model. This excludes subclassed Functional\n+            # models with a custom __init__.\n+            config = {**config, **get_functional_config(self)}\n+        else:\n+            # Try to autogenerate config\n+            xtra_args = set(config.keys())\n+            if getattr(self, \"_auto_get_config\", False):\n+                config.update(self._auto_config.config)\n+            # Remove args non explicitly supported\n+            argspec = inspect.getfullargspec(self.__init__)\n+            if argspec.varkw != \"kwargs\":\n+                for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n+                    config.pop(key, None)\n+        return config\n+\n \n def operation_fn(operation, training):\n     def call(*args, **kwargs):\n@@ -195,3 +223,11 @@ def operation_fn(operation, training):\n         return operation(*args, **kwargs)\n \n     return call\n+\n+\n+def functional_like_constructor(cls):\n+    raise NotImplementedError\n+\n+\n+def get_functional_config(model):\n+    raise NotImplementedError\n\n@@ -1,9 +1,6 @@\n-import inspect\n-\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n-from keras_core.utils import python_utils\n from keras_core.utils import summary_utils\n \n if backend.backend() == \"tensorflow\":\n@@ -155,32 +152,6 @@ class Model(Trainer, Layer):\n     def export(self, filepath):\n         raise NotImplementedError\n \n-    @python_utils.default\n-    def get_config(self):\n-        # Prepare base arguments\n-        config = {\n-            \"name\": self.name,\n-            \"trainable\": self.trainable,\n-        }\n-        # Check whether the class has a constructor compatible with a Functional\n-        # model or if it has a custom constructor.\n-        if functional_like_constructor(self.__class__):\n-            # Only return a Functional config if the constructor is the same\n-            # as that of a Functional model. This excludes subclassed Functional\n-            # models with a custom __init__.\n-            config = {**config, **get_functional_config(self)}\n-        else:\n-            # Try to autogenerate config\n-            xtra_args = set(config.keys())\n-            if getattr(self, \"_auto_get_config\", False):\n-                config.update(self._auto_config.config)\n-            # Remove args non explicitly supported\n-            argspec = inspect.getfullargspec(self.__init__)\n-            if argspec.varkw != \"kwargs\":\n-                for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n-                    config.pop(key, None)\n-        return config\n-\n \n def functional_init_arguments(args, kwargs):\n     return (\n@@ -188,11 +159,3 @@ def functional_init_arguments(args, kwargs):\n         or (len(args) == 1 and \"outputs\" in kwargs)\n         or (\"inputs\" in kwargs and \"outputs\" in kwargs)\n     )\n-\n-\n-def functional_like_constructor(cls):\n-    raise NotImplementedError\n-\n-\n-def get_functional_config(model):\n-    raise NotImplementedError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
