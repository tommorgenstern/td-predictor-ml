{"custom_id": "keras#0904573a78bea557330a9b1717ef6162855c4866", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 38 | Files Changed: 6 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/5 | Churn Δ: 72 | Churn Cumulative: 3856 | Contributors (this commit): 22 | Commits (past 90d): 32 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,6 @@ from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n from keras.utils import python_utils\n-from keras.utils import shape_utils\n from keras.utils import tree\n \n \n@@ -86,7 +85,7 @@ class Lambda(Layer):\n         if self._output_shape is None:\n             # Leverage backend shape inference\n             try:\n-                inputs = shape_utils.map_shape_structure(\n+                inputs = tree.map_shape_structure(\n                     lambda x: backend.KerasTensor(x, dtype=self.compute_dtype),\n                     input_shape,\n                 )\n@@ -108,7 +107,7 @@ class Lambda(Layer):\n         def _add_batch(shape):\n             return (batch_size,) + shape\n \n-        return shape_utils.map_shape_structure(_add_batch, self._output_shape)\n+        return tree.map_shape_structure(_add_batch, self._output_shape)\n \n     def call(self, inputs, mask=None, training=None):\n         # We must copy for thread safety,\n\n@@ -40,7 +40,6 @@ from keras.utils import summary_utils\n from keras.utils import traceback_utils\n from keras.utils import tracking\n from keras.utils import tree\n-from keras.utils.shape_utils import map_shape_structure\n \n if backend.backend() == \"tensorflow\":\n     from keras.backend.tensorflow.layer import TFLayer as BackendLayer\n@@ -1017,7 +1016,7 @@ class Layer(BackendLayer, Operation):\n                 return KerasTensor(output_shape, dtype=self.compute_dtype)\n             # Case: nested. Could be a tuple/list of shapes, or a dict of\n             # shapes. Could be deeply nested.\n-            return map_shape_structure(\n+            return tree.map_shape_structure(\n                 lambda s: KerasTensor(s, dtype=self.compute_dtype), output_shape\n             )\n \n@@ -1264,7 +1263,7 @@ class Layer(BackendLayer, Operation):\n \n     def _build_by_run_for_single_pos_arg(self, input_shape):\n         # Case: all inputs are in the first arg (possibly nested).\n-        input_tensors = map_shape_structure(\n+        input_tensors = tree.map_shape_structure(\n             lambda s: backend.KerasTensor(s), input_shape\n         )\n         try:\n\n@@ -15,7 +15,6 @@ from keras.backend.common.keras_tensor import KerasTensor\n from keras.models import Model\n from keras.utils import traceback_utils\n from keras.utils import tree\n-from keras.utils.shape_utils import is_shape_tuple\n \n \n class TestCase(unittest.TestCase):\n@@ -230,16 +229,9 @@ class TestCase(unittest.TestCase):\n             elif not isinstance(input_shape, tuple):\n                 raise ValueError(\"The type of input_shape is not supported\")\n         if input_shape is not None and input_dtype is None:\n-            if isinstance(input_shape, tuple) and is_shape_tuple(\n-                input_shape[0]\n-            ):\n-                input_dtype = [\"float32\"] * len(input_shape)\n-            elif isinstance(input_shape, dict):\n-                input_dtype = {k: \"float32\" for k in input_shape.keys()}\n-            elif isinstance(input_shape, list):\n-                input_dtype = [\"float32\"] * len(input_shape)\n-            else:\n-                input_dtype = \"float32\"\n+            input_dtype = tree.map_shape_structure(\n+                lambda _: \"float32\", input_shape\n+            )\n \n         # Serialization test.\n         layer = layer_cls(**init_kwargs)\n@@ -613,6 +605,12 @@ def create_eager_tensors(input_shape, dtype, sparse):\n     return map_shape_dtype_structure(create_fn, input_shape, dtype)\n \n \n+def is_shape_tuple(x):\n+    return isinstance(x, (list, tuple)) and all(\n+        isinstance(e, (int, type(None))) for e in x\n+    )\n+\n+\n def map_shape_dtype_structure(fn, shape, dtype):\n     \"\"\"Variant of tree.map_structure that operates on shape tuples.\"\"\"\n     if is_shape_tuple(shape):\n\n@@ -6,8 +6,8 @@ from keras import backend\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n-from keras.utils import shape_utils\n from keras.utils import tracking\n+from keras.utils import tree\n from keras.utils.module_utils import jax\n \n \n@@ -354,7 +354,7 @@ class JaxLayer(Layer):\n             shape = [d if d is not None else 1 for d in shape]\n             return jax.numpy.ones(shape)\n \n-        init_inputs = shape_utils.map_shape_structure(create_input, input_shape)\n+        init_inputs = tree.map_shape_structure(create_input, input_shape)\n         init_args = []\n         for argument_name in self.init_fn_arguments:\n             if argument_name == \"rng\":\n\n@@ -1,19 +0,0 @@\n-def is_shape_tuple(x):\n-    if isinstance(x, (list, tuple)):\n-        if all(isinstance(e, (int, type(None))) for e in x):\n-            return True\n-    return False\n-\n-\n-def map_shape_structure(fn, struct):\n-    \"\"\"Variant of tree.map_structure that operates on shape tuples.\"\"\"\n-    if is_shape_tuple(struct):\n-        return fn(tuple(struct))\n-    if isinstance(struct, list):\n-        return [map_shape_structure(fn, e) for e in struct]\n-    if isinstance(struct, tuple):\n-        return tuple(map_shape_structure(fn, e) for e in struct)\n-    if isinstance(struct, dict):\n-        return {k: map_shape_structure(fn, v) for k, v in struct.items()}\n-    else:\n-        raise ValueError(f\"Cannot map function to unknown object {struct}\")\n\n@@ -439,6 +439,25 @@ def lists_to_tuples(structure):\n     )\n \n \n+def map_shape_structure(func, structure):\n+    \"\"\"Variant of tree.map_structure that operates on shape tuples.\"\"\"\n+\n+    def is_shape_tuple(x):\n+        return isinstance(x, (list, tuple)) and all(\n+            isinstance(e, (int, type(None))) for e in x\n+        )\n+\n+    if not callable(func):\n+        raise TypeError(f\"`func` must be callable. Received: func={func}\")\n+    return optree.tree_map(\n+        func,\n+        structure,\n+        is_leaf=is_shape_tuple,\n+        none_is_leaf=True,\n+        namespace=\"keras\",\n+    )\n+\n+\n class _MapToNone:\n     \"\"\"A special object used as a sentinel within `traverse`.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0847695f8c8f021201094f4ce8a71e2a55d3f9cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 156 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,7 +47,7 @@ class RandomNormal(Initializer):\n         self.mean = mean\n         self.stddev = stddev\n         self._init_seed = seed\n-        self.seed = seed or random.make_default_seed()\n+        self.seed = seed if seed is not None else random.make_default_seed()\n         super().__init__()\n \n     def __call__(self, shape, dtype=None):\n@@ -107,7 +107,7 @@ class TruncatedNormal(Initializer):\n         self.mean = mean\n         self.stddev = stddev\n         self._init_seed = seed\n-        self.seed = seed or random.make_default_seed()\n+        self.seed = seed if seed is not None else random.make_default_seed()\n         super().__init__()\n \n     def __call__(self, shape, dtype=None):\n@@ -164,7 +164,7 @@ class RandomUniform(Initializer):\n         self.minval = minval\n         self.maxval = maxval\n         self._init_seed = seed\n-        self.seed = seed or random.make_default_seed()\n+        self.seed = seed if seed is not None else random.make_default_seed()\n         super().__init__()\n \n     def __call__(self, shape, dtype=None):\n@@ -268,7 +268,7 @@ class VarianceScaling(Initializer):\n         self.mode = mode\n         self.distribution = distribution\n         self._init_seed = seed\n-        self.seed = seed or random.make_default_seed()\n+        self.seed = seed if seed is not None else random.make_default_seed()\n \n     def __call__(self, shape, dtype=None):\n         scale = self.scale\n@@ -669,7 +669,7 @@ class OrthogonalInitializer(Initializer):\n     def __init__(self, gain=1.0, seed=None):\n         self.gain = gain\n         self._init_seed = seed\n-        self.seed = seed or random.make_default_seed()\n+        self.seed = seed if seed is not None else random.make_default_seed()\n \n     def __call__(self, shape, dtype=None):\n         if len(shape) < 2:\n\n@@ -48,7 +48,9 @@ class RandomCrop(TFDataLayer):\n         super().__init__(name=name, **kwargs)\n         self.height = height\n         self.width = width\n-        self.seed = seed or backend.random.make_default_seed()\n+        self.seed = (\n+            seed if seed is not None else backend.random.make_default_seed()\n+        )\n         self.generator = SeedGenerator(seed)\n         self.data_format = backend.standardize_data_format(data_format)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa3a61bccca5770279ed83efd4737c0857212a9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 563 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -262,7 +262,10 @@ class Dot(Merge):\n \n     def build(self, input_shape):\n         # Used purely for shape validation.\n-        if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n+        if (\n+            not isinstance(input_shape[0], (tuple, list))\n+            or len(input_shape) != 2\n+        ):\n             raise ValueError(\n                 f\"A `Dot` layer should be called on a list of 2 inputs. \"\n                 f\"Received: input_shape={input_shape}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f23781a92248356506a997623a2b3b58a705f54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 302 | Contributors (this commit): 8 | Commits (past 90d): 14 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1111,17 +1111,21 @@ class Layer(BackendLayer, Operation):\n \n     def _check_quantize_args(self, mode, compute_dtype):\n         if not self.built:\n-            raise ValueError(\"Cannot quantize on a layer that isn't yet built.\")\n+            raise ValueError(\n+                \"Cannot quantize a layer that isn't yet built. \"\n+                f\"Layer '{self.name}' (of type '{self.__class__.__name__}') \"\n+                \"is not built yet.\"\n+            )\n         if mode not in (\"int8\",):\n             raise ValueError(\n                 f\"`quantize` must be one of ('int8'). Received: mode={mode}\"\n             )\n         if mode == \"int8\" and compute_dtype == \"float16\":\n             raise ValueError(\n-                f\"mode='{mode}' doesn't work well with \"\n+                f\"Quantization mode='{mode}' doesn't work well with \"\n                 \"compute_dtype='float16'. Consider loading model/layer with \"\n-                \"other dtype policy such as 'mixed_bfloat16' before calling \"\n-                \"`quantize`.\"\n+                \"another dtype policy such as 'mixed_bfloat16' or \"\n+                \"'mixed_float16' before calling `quantize()`.\"\n             )\n \n     def save_own_variables(self, store):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d7917c1c8250e43c86ad4f345304e26f91190bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 133 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 26 | Methods Changed: 7 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 135 | Churn Cumulative: 9650 | Contributors (this commit): 13 | Commits (past 90d): 42 | Contributors (cumulative): 27 | DMM Complexity: 0.45528455284552843\n\nDIFF:\n@@ -104,9 +104,14 @@ def einsum(subscripts, *operands, **kwargs):\n         # `None`.\n         if subscripts in [\n             \"a,b->ab\",\n+            \"ab,b->a\",\n             \"ab,bc->ac\",\n+            \"ab,cb->ac\",\n             \"abc,cd->abd\",\n+            \"abc,dc->abd\",\n+            \"abcd,abde->abce\",\n             \"abcd,abed->abce\",\n+            \"abcd,acbe->adbe\",\n             \"abcd,adbe->acbe\",\n             \"abcd,aecd->acbe\",\n             \"abcd,aecd->aceb\",\n@@ -127,6 +132,13 @@ def einsum(subscripts, *operands, **kwargs):\n             if None in (b, c, d, e):\n                 return False\n             return True\n+        elif subscripts == \"abc,dec->abde\":\n+            _, b1, c1 = operands[0].shape\n+            d2, e2, c2 = operands[1].shape\n+            b, c, d, e = b1, c1 or c2, d2, e2\n+            if None in (b, c, d, e):\n+                return False\n+            return True\n         elif subscripts == \"abcd,cde->abe\":\n             _, b1, c1, d1 = operands[0].shape\n             c2, d2, e2 = operands[1].shape\n@@ -134,6 +146,20 @@ def einsum(subscripts, *operands, **kwargs):\n             if None in (b, c, d, e):\n                 return False\n             return True\n+        elif subscripts == \"abcd,ced->abe\":\n+            _, b1, c1, d1 = operands[0].shape\n+            c2, e2, d2 = operands[1].shape\n+            b, c, d, e = b1, c1 or c2, d1 or d2, e2\n+            if None in (b, c, d, e):\n+                return False\n+            return True\n+        elif subscripts == \"abcd,ecd->abe\":\n+            _, b1, c1, d1 = operands[0].shape\n+            e2, c2, d2 = operands[1].shape\n+            b, c, d, e = b1, c1 or c2, d1 or d2, e2\n+            if None in (b, c, d, e):\n+                return False\n+            return True\n         elif subscripts == \"abcde,aebf->adbcf\":\n             _, b1, c1, d1, e1 = operands[0].shape\n             _, e2, b2, f2 = operands[1].shape\n@@ -160,8 +186,15 @@ def einsum(subscripts, *operands, **kwargs):\n             x = tf.expand_dims(x, axis=-1)\n             y = tf.expand_dims(y, axis=0)\n             return tf.matmul(x, y, output_type=output_type)\n+        elif subscripts == \"ab,b->a\":\n+            y = tf.expand_dims(y, axis=-1)\n+            result = tf.matmul(x, y, output_type=output_type)\n+            return tf.squeeze(result, axis=-1)\n         elif subscripts == \"ab,bc->ac\":\n             return tf.matmul(x, y, output_type=output_type)\n+        elif subscripts == \"ab,cb->ac\":\n+            y = tf.transpose(y, [1, 0])\n+            return tf.matmul(x, y, output_type=output_type)\n         elif subscripts == \"abc,cd->abd\":\n             return tf.matmul(x, y, output_type=output_type)\n         elif subscripts == \"abc,cde->abde\":\n@@ -171,6 +204,9 @@ def einsum(subscripts, *operands, **kwargs):\n             y = tf.reshape(y, [c, -1])\n             result = tf.matmul(x, y, output_type=output_type)\n             return tf.reshape(result, [-1, b, d, e])\n+        elif subscripts == \"abc,dc->abd\":\n+            y = tf.transpose(y, [1, 0])\n+            return tf.matmul(x, y, output_type=output_type)\n         elif subscripts == \"abc,dce->abde\":\n             _, b1, c1 = x.shape\n             d2, c2, e2 = y.shape\n@@ -179,9 +215,24 @@ def einsum(subscripts, *operands, **kwargs):\n             y = tf.reshape(y, [c, -1])\n             result = tf.matmul(x, y, output_type=output_type)\n             return tf.reshape(result, [-1, b, d, e])\n+        elif subscripts == \"abc,dec->abde\":\n+            _, b1, c1 = x.shape\n+            d2, e2, c2 = y.shape\n+            b, c, d, e = b1, c1 or c2, d2, e2\n+            y = tf.transpose(y, [2, 0, 1])  # cde\n+            y = tf.reshape(y, [c, -1])\n+            result = tf.matmul(x, y, output_type=output_type)\n+            return tf.reshape(result, [-1, b, d, e])\n+        elif subscripts == \"abcd,abde->abce\":\n+            return tf.matmul(x, y, output_type=output_type)\n         elif subscripts == \"abcd,abed->abce\":\n             y = tf.transpose(y, [0, 1, 3, 2])\n             return tf.matmul(x, y, output_type=output_type)\n+        elif subscripts == \"abcd,acbe->adbe\":\n+            x = tf.transpose(x, [0, 1, 3, 2])\n+            y = tf.transpose(y, [0, 2, 1, 3])\n+            result = tf.matmul(x, y, output_type=output_type)\n+            return tf.transpose(result, [0, 2, 1, 3])\n         elif subscripts == \"abcd,adbe->acbe\":\n             y = tf.transpose(y, [0, 2, 1, 3])  # abde\n             result = tf.matmul(x, y, output_type=output_type)  # abce\n@@ -202,6 +253,22 @@ def einsum(subscripts, *operands, **kwargs):\n             x = tf.reshape(x, [-1, b, c * d])\n             y = tf.reshape(y, [-1, e])\n             return tf.matmul(x, y, output_type=output_type)\n+        elif subscripts == \"abcd,ced->abe\":\n+            _, b1, c1, d1 = x.shape\n+            c2, e2, d2 = y.shape\n+            b, c, d, e = b1, c1 or c2, d1 or d2, e2\n+            x = tf.reshape(x, [-1, b, c * d])\n+            y = tf.transpose(y, [0, 2, 1])\n+            y = tf.reshape(y, [-1, e])\n+            return tf.matmul(x, y, output_type=output_type)\n+        elif subscripts == \"abcd,ecd->abe\":\n+            _, b1, c1, d1 = x.shape\n+            e2, c2, d2 = y.shape\n+            b, c, d, e = b1, c1 or c2, d1 or d2, e2\n+            x = tf.reshape(x, [-1, b, c * d])\n+            y = tf.transpose(y, [1, 2, 0])\n+            y = tf.reshape(y, [-1, e])\n+            return tf.matmul(x, y, output_type=output_type)\n         elif subscripts == \"abcde,aebf->adbcf\":\n             _, b1, c1, d1, e1 = x.shape\n             _, e2, b2, f2 = y.shape\n\n@@ -372,7 +372,7 @@ class DenseTest(testing.TestCase):\n     def test_quantize_on_unbuilt_layer(self):\n         layer = layers.Dense(units=2)\n         with self.assertRaisesRegex(\n-            ValueError, \"Cannot quantize on a layer that isn't yet built.\"\n+            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n         ):\n             layer.quantize(\"int8\")\n \n\n@@ -480,7 +480,7 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             bias_axes=\"d\",\n         )\n         with self.assertRaisesRegex(\n-            ValueError, \"Cannot quantize on a layer that isn't yet built.\"\n+            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n         ):\n             layer.quantize(\"int8\")\n \n\n@@ -2173,6 +2173,13 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n         )\n \n+        subscripts = \"ab,b->a\"\n+        x = np.arange(6).reshape([2, 3]).astype(\"float32\")\n+        y = np.arange(3).reshape([3]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n         subscripts = \"ab,bc->ac\"\n         x = np.arange(6).reshape([2, 3]).astype(\"float32\")\n         y = np.arange(12).reshape([3, 4]).astype(\"float32\")\n@@ -2180,6 +2187,13 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n         )\n \n+        subscripts = \"ab,cb->ac\"\n+        x = np.arange(6).reshape([2, 3]).astype(\"float32\")\n+        y = np.arange(12).reshape([4, 3]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n         subscripts = \"abc,cd->abd\"\n         x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n         y = np.arange(20).reshape([4, 5]).astype(\"float32\")\n@@ -2194,6 +2208,13 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n         )\n \n+        subscripts = \"abc,dc->abd\"\n+        x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n+        y = np.arange(20).reshape([5, 4]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n         subscripts = \"abc,dce->abde\"\n         x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n         y = np.arange(120).reshape([5, 4, 6]).astype(\"float32\")\n@@ -2201,6 +2222,20 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n         )\n \n+        subscripts = \"abc,dec->abde\"\n+        x = np.arange(24).reshape([2, 3, 4]).astype(\"float32\")\n+        y = np.arange(120).reshape([5, 6, 4]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n+        subscripts = \"abcd,abde->abce\"\n+        x = np.arange(120).reshape([2, 3, 4, 5]).astype(\"float32\")\n+        y = np.arange(180).reshape([2, 3, 5, 6]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n         subscripts = \"abcd,abed->abce\"\n         x = np.arange(120).reshape([2, 3, 4, 5]).astype(\"float32\")\n         y = np.arange(180).reshape([2, 3, 6, 5]).astype(\"float32\")\n@@ -2208,6 +2243,13 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n         )\n \n+        subscripts = \"abcd,acbe->adbe\"\n+        x = np.arange(120).reshape([2, 3, 4, 5]).astype(\"float32\")\n+        y = np.arange(144).reshape([2, 4, 3, 6]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n         subscripts = \"abcd,adbe->acbe\"\n         x = np.arange(120).reshape([2, 3, 4, 5]).astype(\"float32\")\n         y = np.arange(180).reshape([2, 5, 3, 6]).astype(\"float32\")\n@@ -2236,6 +2278,20 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n         )\n \n+        subscripts = \"abcd,ced->abe\"\n+        x = np.arange(120).reshape([2, 3, 4, 5]).astype(\"float32\")\n+        y = np.arange(120).reshape([4, 6, 5]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n+        subscripts = \"abcd,ecd->abe\"\n+        x = np.arange(120).reshape([2, 3, 4, 5]).astype(\"float32\")\n+        y = np.arange(120).reshape([6, 4, 5]).astype(\"float32\")\n+        self.assertAllClose(\n+            knp.einsum(subscripts, x, y), np.einsum(subscripts, x, y)\n+        )\n+\n         subscripts = \"abcde,aebf->adbcf\"\n         x = np.arange(720).reshape([2, 3, 4, 5, 6]).astype(\"float32\")\n         y = np.arange(252).reshape([2, 6, 3, 7]).astype(\"float32\")\n@@ -5668,15 +5724,23 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         dtype1, dtype2 = dtypes\n         for subscripts in [\n             \"a,b->ab\",\n+            \"ab,b->a\",\n             \"ab,bc->ac\",\n+            \"ab,cb->ac\",\n             \"abc,cd->abd\",\n             \"abc,cde->abde\",\n+            \"abc,dc->abd\",\n             \"abc,dce->abde\",\n+            \"abc,dec->abde\",\n+            \"abcd,abde->abce\",\n             \"abcd,abed->abce\",\n+            \"abcd,acbe->adbe\",\n             \"abcd,adbe->acbe\",\n             \"abcd,aecd->acbe\",\n             \"abcd,aecd->aceb\",\n             \"abcd,cde->abe\",\n+            \"abcd,ced->abe\",\n+            \"abcd,ecd->abe\",\n             \"abcde,aebf->adbcf\",\n             \"abcde,afce->acdbf\",\n         ]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#576aa8df2d6fc1b3c512d0c0cd32216899e8abe3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1514 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -833,7 +833,9 @@ class SavingBattleTest(testing.TestCase):\n             def call(self, x):\n                 return self.dense(x)\n \n-        temp_filepath = \"normal_model.weights.h5\"\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"normal_model.weights.h5\"\n+        )\n         model_a = NormalModel()\n         model_a(np.random.random((2, 2)))\n         model_a.save_weights(temp_filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f40e443279cc89198dc15712d334d0806e4c6e92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 421 | Lines Deleted: 70 | Files Changed: 4 | Hunks: 42 | Methods Changed: 26 | Complexity Δ (Sum/Max): 26/14 | Churn Δ: 491 | Churn Cumulative: 4396 | Contributors (this commit): 17 | Commits (past 90d): 33 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import numpy as np\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -128,9 +126,6 @@ class Dense(Layer):\n         self.built = True\n         if self.lora_rank:\n             self.enable_lora(self.lora_rank)\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n-            if self.bias is not None:\n-                self.bias.trainable = False\n \n     @property\n     def kernel(self):\n@@ -194,22 +189,34 @@ class Dense(Layer):\n         self.lora_rank = rank\n \n     def save_own_variables(self, store):\n-        if not self.lora_enabled:\n-            return super().save_own_variables(store)\n-\n-        kernel_value = self.kernel\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        # The keys of the `store` will be saved as determined because the\n+        # default ordering will change after quantization\n+        kernel_value, kernel_scale = self._get_kernel_with_merged_lora()\n         store[\"0\"] = kernel_value\n         if self.use_bias:\n             store[\"1\"] = self.bias\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            store[\"2\"] = kernel_scale\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n-            return super().load_own_variables(store)\n+            self._check_load_own_variables(store)\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        # The keys of the `store` will be saved as determined because the\n+        # default ordering will change after quantization\n         self._kernel.assign(store[\"0\"])\n         if self.use_bias:\n             self.bias.assign(store[\"1\"])\n-        self.lora_kernel_a.assign(np.zeros(self.lora_kernel_a.shape))\n-        self.lora_kernel_b.assign(np.zeros(self.lora_kernel_b.shape))\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            self.kernel_scale.assign(store[\"2\"])\n+        if self.lora_enabled:\n+            self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n+            self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n \n     def get_config(self):\n         base_config = super().get_config()\n@@ -232,6 +239,40 @@ class Dense(Layer):\n             config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n \n+    def _check_load_own_variables(self, store):\n+        all_vars = self._trainable_variables + self._non_trainable_variables\n+        if len(store.keys()) != len(all_vars):\n+            if len(all_vars) == 0 and not self.built:\n+                raise ValueError(\n+                    f\"Layer '{self.name}' was never built \"\n+                    \"and thus it doesn't have any variables. \"\n+                    f\"However the weights file lists {len(store.keys())} \"\n+                    \"variables for this layer.\\n\"\n+                    \"In most cases, this error indicates that either:\\n\\n\"\n+                    \"1. The layer is owned by a parent layer that \"\n+                    \"implements a `build()` method, but calling the \"\n+                    \"parent's `build()` method did NOT create the state of \"\n+                    f\"the child layer '{self.name}'. A `build()` method \"\n+                    \"must create ALL state for the layer, including \"\n+                    \"the state of any children layers.\\n\\n\"\n+                    \"2. You need to implement \"\n+                    \"the `def build_from_config(self, config)` method \"\n+                    f\"on layer '{self.name}', to specify how to rebuild \"\n+                    \"it during loading. \"\n+                    \"In this case, you might also want to implement the \"\n+                    \"method that generates the build config at saving time, \"\n+                    \"`def get_build_config(self)`. \"\n+                    \"The method `build_from_config()` is meant \"\n+                    \"to create the state \"\n+                    \"of the layer (i.e. its variables) upon deserialization.\",\n+                )\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Expected: {[v.name for v in all_vars]}\"\n+            )\n+\n     \"\"\"Quantization-related methods\"\"\"\n \n     def quantized_build(self, input_shape, mode):\n@@ -253,13 +294,34 @@ class Dense(Layer):\n             )\n \n     def quantized_call(self, inputs):\n-        if self.lora_enabled:\n-            raise ValueError(\"`quantized_call` doesn't support lora weights\")\n+        @ops.custom_gradient\n+        def matmul_with_inputs_gradient(inputs, kernel, kernel_scale):\n+            def grad_fn(*args, upstream=None):\n+                if upstream is None:\n+                    (upstream,) = args\n+                float_kernel = ops.divide(\n+                    ops.cast(kernel, dtype=self.compute_dtype),\n+                    kernel_scale,\n+                )\n+                inputs_grad = ops.matmul(upstream, ops.transpose(float_kernel))\n+                return (inputs_grad, None, None)\n+\n             inputs, inputs_scale = self.inputs_quantizer(inputs)\n-        x = ops.matmul(inputs, self.kernel)\n+            x = ops.matmul(inputs, kernel)\n             # De-scale outputs\n             x = ops.cast(x, self.compute_dtype)\n-        x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n+            x = ops.divide(x, ops.multiply(inputs_scale, kernel_scale))\n+            return x, grad_fn\n+\n+        x = matmul_with_inputs_gradient(\n+            inputs,\n+            ops.convert_to_tensor(self._kernel),\n+            ops.convert_to_tensor(self.kernel_scale),\n+        )\n+        if self.lora_enabled:\n+            lora_x = ops.matmul(inputs, self.lora_kernel_a)\n+            lora_x = ops.matmul(lora_x, self.lora_kernel_b)\n+            x = ops.add(x, lora_x)\n         if self.bias is not None:\n             x = ops.add(x, self.bias)\n         if self.activation is not None:\n@@ -267,21 +329,19 @@ class Dense(Layer):\n         return x\n \n     def quantize(self, mode):\n+        import gc\n+\n         self._check_quantize_args(mode, self.compute_dtype)\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n                 raise ValueError(\"`quantize` can only be done once per layer.\")\n-            # Merge lora-related parameters to make use of fully int8 kernel\n-            self._merge_lora_into_kernel()\n             # Configure `self.inputs_quantizer`\n             self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=0\n             )\n-            kernel_scale = ops.cast(\n-                ops.squeeze(kernel_scale, axis=0), self.compute_dtype\n-            )\n+            kernel_scale = ops.squeeze(kernel_scale, axis=0)\n             self._tracker.unlock()\n             self._untrack_variable(self._kernel)\n             self._kernel = self.add_weight(\n@@ -299,8 +359,6 @@ class Dense(Layer):\n                 initializer=lambda shape, dtype: kernel_scale,\n                 trainable=False,\n             )\n-            if self.bias is not None:\n-                self.bias.trainable = False\n             self._tracker.lock()\n         else:\n             NotImplementedError(\n@@ -315,15 +373,24 @@ class Dense(Layer):\n             quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n             self.dtype_policy = dtype_policies.get(quantized_dtype)\n \n-    def _merge_lora_into_kernel(self, untrack=False):\n-        if not self.lora_enabled:\n-            return\n-        # Merge lora-enabled kernel into kernel\n-        self._kernel.assign(self.kernel)\n-        self.lora_enabled = False\n-        if untrack:\n-            self._tracker.unlock()\n-            self.lora_kernel_a = self._untrack_variable(self.lora_kernel_a)\n-            self.lora_kernel_b = self._untrack_variable(self.lora_kernel_b)\n-            self._tracker.lock()\n-            self.lora_rank = None\n+        # Release memory manually because sometimes the backend doesn't\n+        gc.collect()\n+\n+    def _get_kernel_with_merged_lora(self):\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            kernel_value = self._kernel\n+            kernel_scale = self.kernel_scale\n+            if self.lora_enabled:\n+                # Dequantize & quantize to merge lora weights into int8 kernel\n+                # Note that this is a lossy compression\n+                kernel_value = ops.divide(kernel_value, kernel_scale)\n+                kernel_value = ops.add(\n+                    kernel_value,\n+                    ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n+                )\n+                kernel_value, kernel_scale = quantizers.abs_max_quantize(\n+                    kernel_value, axis=0\n+                )\n+                kernel_scale = ops.squeeze(kernel_scale, axis=0)\n+            return kernel_value, kernel_scale\n+        return self.kernel, None\n\n@@ -11,6 +11,7 @@ from keras import ops\n from keras import saving\n from keras import testing\n from keras.backend.common import keras_tensor\n+from keras.export import export_lib\n \n \n class DenseTest(testing.TestCase):\n@@ -305,6 +306,10 @@ class DenseTest(testing.TestCase):\n         with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n             layer.enable_lora(rank=2)\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n+    )\n     def test_quantize_int8(self):\n         layer = layers.Dense(units=16)\n         layer.build((None, 8))\n@@ -362,8 +367,8 @@ class DenseTest(testing.TestCase):\n             },\n             input_shape=(2, 3, 4),\n             expected_output_shape=(2, 3, 5),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=3,\n+            expected_num_trainable_weights=1,\n+            expected_num_non_trainable_weights=2,\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n@@ -384,3 +389,82 @@ class DenseTest(testing.TestCase):\n             ValueError, \"`quantize` can only be done once per layer.\"\n         ):\n             layer.quantize(\"int8\")\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_when_lora_enabled(self):\n+        # Note that saving and loading with lora_enabled and quantized are\n+        # lossy, so we use a weak correctness test for model outputs (atol=0.5).\n+        config = dict(units=16)\n+        layer = layers.Dense(**config)\n+        layer.build((None, 8))\n+        layer.enable_lora(4)\n+        layer.quantize(\"int8\")\n+        self.assertLen(layer.trainable_weights, 3)\n+        self.assertLen(layer.non_trainable_weights, 2)\n+\n+        # Try calling fit()\n+        init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        init_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        x = np.random.random((64, 8))\n+        y = np.random.random((64, 16))\n+        model = models.Sequential([layer])\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y, epochs=2)\n+\n+        final_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        final_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_kernel_value - final_lora_a_kernel_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_kernel_value - final_lora_b_kernel_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertTrue(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+        new_model = models.Sequential([layers.Dense(**config)])\n+        new_model.build((None, 8))\n+        new_model.quantize(\"int8\")\n+        new_model.load_weights(temp_filepath)\n+        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Test export and TFSMLayer reloading when using tensorflow backend\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+            ref_input = tf.random.normal((32, 8))\n+            ref_output = model(ref_input)\n+            export_lib.export_model(model, temp_filepath)\n+            reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n+            self.assertAllClose(\n+                reloaded_layer(ref_input), ref_output, atol=1e-7\n+            )\n+            self.assertLen(reloaded_layer.weights, len(model.weights))\n+            self.assertLen(\n+                reloaded_layer.trainable_weights, len(model.trainable_weights)\n+            )\n+            self.assertLen(\n+                reloaded_layer.non_trainable_weights,\n+                len(model.non_trainable_weights),\n+            )\n\n@@ -184,9 +184,6 @@ class EinsumDense(Layer):\n         self.built = True\n         if self.lora_rank:\n             self.enable_lora(self.lora_rank)\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n-            if self.bias is not None:\n-                self.bias.trainable = False\n \n     @property\n     def kernel(self):\n@@ -248,22 +245,34 @@ class EinsumDense(Layer):\n         self.lora_rank = rank\n \n     def save_own_variables(self, store):\n-        if not self.lora_enabled:\n-            return super().save_own_variables(store)\n-\n-        kernel_value = self.kernel\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        # The keys of the `store` will be saved as determined because the\n+        # default ordering will change after quantization\n+        kernel_value, kernel_scale = self._get_kernel_with_merged_lora()\n         store[\"0\"] = kernel_value\n         if self.bias is not None:\n             store[\"1\"] = self.bias\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            store[\"2\"] = kernel_scale\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n-            return super().load_own_variables(store)\n+            self._check_load_own_variables(store)\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        # The keys of the `store` will be saved as determined because the\n+        # default ordering will change after quantization\n         self._kernel.assign(store[\"0\"])\n         if self.bias is not None:\n             self.bias.assign(store[\"1\"])\n-        self.lora_kernel_a.assign(np.zeros(self.lora_kernel_a.shape))\n-        self.lora_kernel_b.assign(np.zeros(self.lora_kernel_b.shape))\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            self.kernel_scale.assign(store[\"2\"])\n+        if self.lora_enabled:\n+            self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n+            self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n \n     def get_config(self):\n         base_config = super().get_config()\n@@ -290,6 +299,40 @@ class EinsumDense(Layer):\n             config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n \n+    def _check_load_own_variables(self, store):\n+        all_vars = self._trainable_variables + self._non_trainable_variables\n+        if len(store.keys()) != len(all_vars):\n+            if len(all_vars) == 0 and not self.built:\n+                raise ValueError(\n+                    f\"Layer '{self.name}' was never built \"\n+                    \"and thus it doesn't have any variables. \"\n+                    f\"However the weights file lists {len(store.keys())} \"\n+                    \"variables for this layer.\\n\"\n+                    \"In most cases, this error indicates that either:\\n\\n\"\n+                    \"1. The layer is owned by a parent layer that \"\n+                    \"implements a `build()` method, but calling the \"\n+                    \"parent's `build()` method did NOT create the state of \"\n+                    f\"the child layer '{self.name}'. A `build()` method \"\n+                    \"must create ALL state for the layer, including \"\n+                    \"the state of any children layers.\\n\\n\"\n+                    \"2. You need to implement \"\n+                    \"the `def build_from_config(self, config)` method \"\n+                    f\"on layer '{self.name}', to specify how to rebuild \"\n+                    \"it during loading. \"\n+                    \"In this case, you might also want to implement the \"\n+                    \"method that generates the build config at saving time, \"\n+                    \"`def get_build_config(self)`. \"\n+                    \"The method `build_from_config()` is meant \"\n+                    \"to create the state \"\n+                    \"of the layer (i.e. its variables) upon deserialization.\",\n+                )\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Expected: {[v.name for v in all_vars]}\"\n+            )\n+\n     \"\"\"Quantization-related methods\"\"\"\n \n     def quantized_build(self, input_shape, mode):\n@@ -310,6 +353,8 @@ class EinsumDense(Layer):\n                 self._kernel_expand_axes,\n                 self._input_squeeze_axes,\n                 self._kernel_squeeze_axes,\n+                self._custom_gradient_equation,\n+                self._kernel_reverse_transpose_axes,\n             ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n             self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             self._kernel = self.add_weight(\n@@ -335,12 +380,40 @@ class EinsumDense(Layer):\n             )\n \n     def quantized_call(self, inputs):\n-        if self.lora_enabled:\n-            raise ValueError(\"`quantized_call` doesn't support lora weights\")\n+        @ops.custom_gradient\n+        def einsum_with_inputs_gradient(inputs, kernel, kernel_scale):\n+            def grad_fn(*args, upstream=None):\n+                if upstream is None:\n+                    (upstream,) = args\n+                # De-scale kernel\n+                _kernel_scale = kernel_scale  # Overcome UnboundLocalError\n+                if self._kernel_squeeze_axes:\n+                    _kernel_scale = ops.expand_dims(\n+                        _kernel_scale, axis=self._kernel_squeeze_axes\n+                    )\n+                if self._kernel_expand_axes:\n+                    _kernel_scale = ops.squeeze(\n+                        _kernel_scale, axis=self._kernel_expand_axes\n+                    )\n+                _kernel_scale = ops.transpose(\n+                    _kernel_scale, self._kernel_reverse_transpose_axes\n+                )\n+                float_kernel = ops.divide(\n+                    ops.cast(kernel, dtype=self.compute_dtype),\n+                    _kernel_scale,\n+                )\n+                # From https://stackoverflow.com/a/47609896\n+                inputs_grad = ops.einsum(\n+                    self._custom_gradient_equation, upstream, float_kernel\n+                )\n+                return (inputs_grad, None, None)\n+\n             inputs, inputs_scale = self.inputs_quantizer(inputs)\n-        x = ops.einsum(self.equation, inputs, self.kernel)\n+            x = ops.einsum(self.equation, inputs, kernel)\n             # Deal with `inputs_scale`\n-        inputs_scale = ops.transpose(inputs_scale, self._input_transpose_axes)\n+            inputs_scale = ops.transpose(\n+                inputs_scale, self._input_transpose_axes\n+            )\n             if self._input_expand_axes:\n                 inputs_scale = ops.expand_dims(\n                     inputs_scale, axis=self._input_expand_axes\n@@ -352,6 +425,17 @@ class EinsumDense(Layer):\n             # De-scale outputs\n             x = ops.cast(x, self.compute_dtype)\n             x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n+            return x, grad_fn\n+\n+        x = einsum_with_inputs_gradient(\n+            inputs,\n+            ops.convert_to_tensor(self._kernel),\n+            ops.convert_to_tensor(self.kernel_scale),\n+        )\n+        if self.lora_enabled:\n+            lora_kernel = ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n+            lora_x = ops.einsum(self.equation, inputs, lora_kernel)\n+            x = ops.add(x, lora_x)\n         if self.bias is not None:\n             x += self.bias\n         if self.activation is not None:\n@@ -359,12 +443,12 @@ class EinsumDense(Layer):\n         return x\n \n     def quantize(self, mode):\n+        import gc\n+\n         self._check_quantize_args(mode, self.compute_dtype)\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n                 raise ValueError(\"`quantize` can only be done once per layer.\")\n-            # Merge lora-related parameters to make use of fully int8 kernel\n-            self._merge_lora_into_kernel()\n             (\n                 self._input_reduced_axes,\n                 self._kernel_reduced_axes,\n@@ -374,6 +458,8 @@ class EinsumDense(Layer):\n                 self._kernel_expand_axes,\n                 self._input_squeeze_axes,\n                 self._kernel_squeeze_axes,\n+                self._custom_gradient_equation,\n+                self._kernel_reverse_transpose_axes,\n             ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n             # Configure `self.inputs_quantizer`\n             self.inputs_quantizer = quantizers.AbsMaxQuantizer(\n@@ -383,7 +469,6 @@ class EinsumDense(Layer):\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=self._kernel_reduced_axes\n             )\n-            kernel_scale = ops.cast(kernel_scale, self.compute_dtype)\n             kernel_scale = ops.transpose(\n                 kernel_scale, self._kernel_transpose_axes\n             )\n@@ -412,8 +497,6 @@ class EinsumDense(Layer):\n                 initializer=lambda shape, dtype: kernel_scale,\n                 trainable=False,\n             )\n-            if self.bias is not None:\n-                self.bias.trainable = False\n             self._tracker.lock()\n         else:\n             NotImplementedError()\n@@ -425,18 +508,39 @@ class EinsumDense(Layer):\n             quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n             self.dtype_policy = dtype_policies.get(quantized_dtype)\n \n-    def _merge_lora_into_kernel(self, untrack=False):\n-        if not self.lora_enabled:\n-            return\n-        # Merge lora-enabled kernel into kernel\n-        self._kernel.assign(self.kernel)\n-        self.lora_enabled = False\n-        if untrack:\n-            self._tracker.unlock()\n-            self.lora_kernel_a = self._untrack_variable(self.lora_kernel_a)\n-            self.lora_kernel_b = self._untrack_variable(self.lora_kernel_b)\n-            self._tracker.lock()\n-            self.lora_rank = None\n+        # Release memory manually because sometimes the backend doesn't\n+        gc.collect()\n+\n+    def _get_kernel_with_merged_lora(self):\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            kernel_value = self._kernel\n+            kernel_scale = self.kernel_scale\n+            if self.lora_enabled:\n+                # Dequantize & quantize to merge lora weights into int8 kernel\n+                # Note that this is a lossy compression\n+                kernel_value = ops.divide(kernel_value, kernel_scale)\n+                kernel_value = ops.add(\n+                    kernel_value,\n+                    ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n+                )\n+                kernel_value, kernel_scale = quantizers.abs_max_quantize(\n+                    kernel_value, axis=self._kernel_reduced_axes\n+                )\n+                kernel_scale = ops.transpose(\n+                    kernel_scale, self._kernel_transpose_axes\n+                )\n+                if self._kernel_expand_axes:\n+                    kernel_scale = ops.expand_dims(\n+                        kernel_scale, axis=self._kernel_expand_axes\n+                    )\n+                if self._kernel_squeeze_axes:\n+                    kernel_scale = ops.squeeze(\n+                        kernel_scale, axis=self._kernel_squeeze_axes\n+                    )\n+        else:\n+            kernel_value = self.kernel\n+            kernel_scale = None\n+        return kernel_value, kernel_scale\n \n \n def _analyze_einsum_string(equation, bias_axes, input_shape, output_shape):\n@@ -703,6 +807,14 @@ def _analyze_quantization_info(equation, input_shape):\n         except IndexError:\n             weight_squeeze_axes.append(ori_index)\n         weight_transpose_axes.insert(index, ori_index)\n+    # Prepare equation for `einsum_with_inputs_gradient`\n+    custom_gradient_equation = f\"{output_spec},{weight_spec}->{input_spec}\"\n+    weight_reverse_transpose_axes = [\n+        i\n+        for (_, i) in sorted(\n+            (v, i) for (i, v) in enumerate(weight_transpose_axes)\n+        )\n+    ]\n     return (\n         input_reduced_axes,\n         weight_reduced_axes,\n@@ -712,4 +824,6 @@ def _analyze_quantization_info(equation, input_shape):\n         weight_expand_axes,\n         input_squeeze_axes,\n         weight_squeeze_axes,\n+        custom_gradient_equation,\n+        weight_reverse_transpose_axes,\n     )\n\n@@ -10,6 +10,7 @@ from keras import layers\n from keras import models\n from keras import saving\n from keras import testing\n+from keras.export import export_lib\n \n \n class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n@@ -374,6 +375,10 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n+    )\n     def test_quantize_int8(self):\n         layer = layers.EinsumDense(\n             equation=\"ab,bcd->acd\",\n@@ -466,8 +471,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             },\n             input_shape=(2, 3),\n             expected_output_shape=(2, 8, 32),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=3,\n+            expected_num_trainable_weights=1,\n+            expected_num_non_trainable_weights=2,\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=False,\n@@ -496,3 +501,84 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             ValueError, \"`quantize` can only be done once per layer.\"\n         ):\n             layer.quantize(\"int8\")\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_when_lora_enabled(self):\n+        config = dict(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=None,\n+        )\n+        layer = layers.EinsumDense(**config)\n+        layer.build((None, 3))\n+        layer.enable_lora(2)\n+        layer.quantize(\"int8\")\n+        self.assertLen(layer.trainable_weights, 2)\n+        self.assertLen(layer.non_trainable_weights, 2)\n+\n+        # Try calling fit()\n+        init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        init_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        x = np.random.random((64, 3))\n+        y = np.random.random((64, 8, 32))\n+        model = models.Sequential([layer])\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y, epochs=2)\n+\n+        final_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        final_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_kernel_value - final_lora_a_kernel_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_kernel_value - final_lora_b_kernel_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertTrue(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+        new_model = models.Sequential([layers.EinsumDense(**config)])\n+        new_model.build((None, 3))\n+        new_model.quantize(\"int8\")\n+        new_model.load_weights(temp_filepath)\n+        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Test export and TFSMLayer reloading when using tensorflow backend\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+            ref_input = tf.random.normal((32, 3))\n+            ref_output = model(ref_input)\n+            export_lib.export_model(model, temp_filepath)\n+            reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n+            self.assertAllClose(\n+                reloaded_layer(ref_input), ref_output, atol=1e-7\n+            )\n+            self.assertLen(reloaded_layer.weights, len(model.weights))\n+            self.assertLen(\n+                reloaded_layer.trainable_weights, len(model.trainable_weights)\n+            )\n+            self.assertLen(\n+                reloaded_layer.non_trainable_weights,\n+                len(model.non_trainable_weights),\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
