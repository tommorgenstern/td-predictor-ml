{"custom_id": "keras#9bc8ab169a03e2a256962ba23ce86196587da7bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 164 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ def random_shift(x, wrg, hrg, fill_mode=\"nearest\", cval=0.):\n         crop = random.uniform(0., hrg)\n         split = random.uniform(0, 1)\n         crop_top_pixels = int(split*crop*x.shape[2])\n-    x = ndimage.interpolation.shift(x, (0, crop_left_pixels, crop_top_pixels),\n+    x = ndimage.interpolation.shift(x, (0, crop_left_pixels, crop_top_pixels), order=0,\n                                     mode=fill_mode, cval=cval)\n     return x\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1623ea6166e9730e3e51af0417250d5480b03402", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 150 | Lines Deleted: 36 | Files Changed: 2 | Hunks: 36 | Methods Changed: 28 | Complexity Δ (Sum/Max): 14/10 | Churn Δ: 186 | Churn Cumulative: 4537 | Contributors (this commit): 34 | Commits (past 90d): 30 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -101,9 +101,30 @@ def weighted_objective(fn):\n     return weighted\n \n \n-def standardize_weights(y, sample_weight=None, class_weight=None):\n-    '''\n+def standardize_weights(y, sample_weight=None, class_weight=None,\n+                        sample_weight_mode=None):\n+    '''Weight input validation and standardization to a single sample-wise\n+    (or timestep-wise) weight array.\n     '''\n+    if sample_weight_mode is not None:\n+        if sample_weight_mode != 'temporal':\n+            raise Exception('\"sample_weight_mode '\n+                            'should be None or \"temporal\".')\n+        if y.ndim < 3:\n+            raise Exception('Timestep-wise sample weighting (use of '\n+                            'sample_weight_mode=\"temporal\") is restricted to '\n+                            'outputs that are at least 3D, i.e. that have '\n+                            'a time dimension.')\n+        if sample_weight is not None and sample_weight.ndim != 2:\n+            raise Exception('In order to use timestep-wise sample weighting, '\n+                            'you should pass a 2D sample_weight array.')\n+    else:\n+        if sample_weight is not None and sample_weight.ndim != 1:\n+            raise Exception('In order to use timestep-wise sample weights, '\n+                            'you should specify sample_weight_mode=\"temporal\" '\n+                            'in compile(). If you just mean to use '\n+                            'sample-wise weights, make sure your '\n+                            'sample_weight array is 1D.')\n     if sample_weight is not None:\n         assert sample_weight.ndim <= y.ndim\n         assert y.shape[:sample_weight.ndim] == sample_weight.shape\n@@ -121,7 +142,10 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n         weights = np.asarray([class_weight[cls] for cls in y_classes])\n         return weights\n     else:\n+        if sample_weight_mode is None:\n             return np.ones((y.shape[0],))\n+        else:\n+            return np.ones((y.shape[0], y.shape[1]))\n \n \n def model_from_yaml(yaml_string, custom_objects={}):\n@@ -413,7 +437,8 @@ class Sequential(Model, containers.Sequential):\n     Inherits from containers.Sequential.\n     '''\n     def compile(self, optimizer, loss,\n-                class_mode=\"categorical\"):\n+                class_mode=\"categorical\",\n+                sample_weight_mode=None):\n         '''Configure the learning process.\n \n         # Arguments\n@@ -424,8 +449,12 @@ class Sequential(Model, containers.Sequential):\n             class_mode: one of \"categorical\", \"binary\".\n                 This is only used for computing classification accuracy or\n                 using the predict_classes method.\n+            sample_weight_mode: if you need to do timestep-wise\n+                sample weighting (2D weights), set this to \"temporal\".\n+                \"None\" defaults to sample-wise weights (1D).\n         '''\n         self.optimizer = optimizers.get(optimizer)\n+        self.sample_weight_mode = sample_weight_mode\n \n         self.loss = objectives.get(loss)\n         weighted_loss = weighted_objective(self.loss)\n@@ -439,7 +468,10 @@ class Sequential(Model, containers.Sequential):\n \n         # target of model\n         self.y = K.placeholder(ndim=K.ndim(self.y_train))\n-        # weights: one scalar per sample\n+\n+        if self.sample_weight_mode == 'temporal':\n+            self.weights = K.placeholder(ndim=2)\n+        else:\n             self.weights = K.placeholder(ndim=1)\n \n         if hasattr(self.layers[-1], \"get_output_mask\"):\n@@ -524,6 +556,8 @@ class Sequential(Model, containers.Sequential):\n                 or in the case of temporal data,\n                 you can pass a 2D array with shape (samples, sequence_length),\n                 to apply a different weight to every timestep of every sample.\n+                In this case you should make sure to specify\n+                sample_weight_mode=\"temporal\" in compile().\n         '''\n         if type(X) == list:\n             if len(set([len(a) for a in X] + [len(y)])) != 1:\n@@ -569,7 +603,8 @@ class Sequential(Model, containers.Sequential):\n                 X_val = standardize_X(X_val)\n                 y_val = standardize_y(y_val)\n                 sample_weight_val = standardize_weights(y_val,\n-                                                        sample_weight=sample_weight_val)\n+                                                        sample_weight=sample_weight_val,\n+                                                        sample_weight_mode=self.sample_weight_mode)\n             else:\n                 raise Exception('Invalid format for validation data; '\n                                 'provide a tuple (X_val, y_val) or '\n@@ -585,7 +620,8 @@ class Sequential(Model, containers.Sequential):\n             if sample_weight is not None:\n                 sample_weight, sample_weight_val = (slice_X(sample_weight, 0, split_at), slice_X(sample_weight, split_at))\n                 sample_weight_val = standardize_weights(y_val,\n-                                                        sample_weight=sample_weight_val)\n+                                                        sample_weight=sample_weight_val,\n+                                                        sample_weight_mode=self.sample_weight_mode)\n             else:\n                 sample_weight_val = standardize_weights(y_val)\n             val_ins = X_val + [y_val, sample_weight_val]\n@@ -598,7 +634,8 @@ class Sequential(Model, containers.Sequential):\n             out_labels = ['loss']\n \n         sample_weight = standardize_weights(y, class_weight=class_weight,\n-                                            sample_weight=sample_weight)\n+                                            sample_weight=sample_weight,\n+                                            sample_weight_mode=self.sample_weight_mode)\n         ins = X + [y, sample_weight]\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n         return self._fit(f, ins, out_labels=out_labels,\n@@ -685,7 +722,8 @@ class Sequential(Model, containers.Sequential):\n                                                   'as X and y.')\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = standardize_weights(y, sample_weight=sample_weight)\n+        sample_weight = standardize_weights(y, sample_weight=sample_weight,\n+                                            sample_weight_mode=self.sample_weight_mode)\n \n         ins = X + [y, sample_weight]\n         if show_accuracy:\n@@ -724,7 +762,8 @@ class Sequential(Model, containers.Sequential):\n         X = standardize_X(X)\n         y = standardize_y(y)\n         sample_weight = standardize_weights(y, class_weight=class_weight,\n-                                            sample_weight=sample_weight)\n+                                            sample_weight=sample_weight,\n+                                            sample_weight_mode=self.sample_weight_mode)\n         ins = X + [y, sample_weight]\n         if accuracy:\n             return self._train_with_acc(ins)\n@@ -753,7 +792,8 @@ class Sequential(Model, containers.Sequential):\n                                                   'as X and y.')\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = standardize_weights(y, sample_weight=sample_weight)\n+        sample_weight = standardize_weights(y, sample_weight=sample_weight,\n+                                            sample_weight_mode=self.sample_weight_mode)\n \n         ins = X + [y, sample_weight]\n         if accuracy:\n@@ -1031,7 +1071,7 @@ class Graph(Model, containers.Graph):\n \n     Inherits from `containers.Graph`.\n     '''\n-    def compile(self, optimizer, loss):\n+    def compile(self, optimizer, loss, sample_weight_modes={}):\n         '''Configure the learning process.\n \n         # Arguments\n@@ -1040,7 +1080,14 @@ class Graph(Model, containers.Graph):\n             loss: dictionary mapping the name(s) of the output(s) to\n                 a loss function (string name of objective function or\n                 objective function. See [objectives](objectives.md)).\n+            sample_weight_modes: optional dictionary mapping certain\n+                output names to a sample weight mode (\"temporal\" and None\n+                are the only supported modes). If you need to do\n+                timestep-wise loss weighting on one of your graph outputs,\n+                you will need to set the sample weight mode for this output\n+                to \"temporal\".\n         '''\n+        self.sample_weight_modes = sample_weight_modes\n         ys = []\n         ys_train = []\n         ys_test = []\n@@ -1062,6 +1109,9 @@ class Graph(Model, containers.Graph):\n             else:\n                 mask = None\n \n+            if sample_weight_modes.get(output_name) == 'temporal':\n+                weight = K.placeholder(ndim=2)\n+            else:\n                 weight = K.placeholder(ndim=1)\n             weights.append(weight)\n             weighted_loss = weighted_objective(objectives.get(loss_fn))\n@@ -1125,7 +1175,8 @@ class Graph(Model, containers.Graph):\n                             'the same number of samples.')\n \n         sample_weight_list = [standardize_weights(y[i],\n-                                                  sample_weight=sample_weight.get(self.output_order[i])) for i in range(len(self.output_order))]\n+                                                  sample_weight=sample_weight.get(self.output_order[i]),\n+                                                  sample_weight_mode=self.sample_weight_modes.get(self.output_order[i])) for i in range(len(self.output_order))]\n         class_weight_list = [class_weight.get(name) for name in self.output_order]\n \n         val_f = None\n@@ -1151,7 +1202,8 @@ class Graph(Model, containers.Graph):\n \n         sample_weight_list = [standardize_weights(y[i],\n                                                   sample_weight=sample_weight_list[i],\n-                                                  class_weight=class_weight_list[i]) for i in range(len(self.output_order))]\n+                                                  class_weight=class_weight_list[i],\n+                                                  sample_weight_mode=self.sample_weight_modes.get(self.output_order[i])) for i in range(len(self.output_order))]\n         ins = X + y + sample_weight_list\n         history = self._fit(f, ins, out_labels=out_labels,\n                             batch_size=batch_size, nb_epoch=nb_epoch,\n@@ -1166,7 +1218,8 @@ class Graph(Model, containers.Graph):\n         Arguments: see `fit` method.\n         '''\n         sample_weight = [standardize_weights(data[name],\n-                                             sample_weight=sample_weight.get(name)) for name in self.output_order]\n+                                             sample_weight=sample_weight.get(name),\n+                                             sample_weight_mode=self.sample_weight_modes.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         if len(set([len(a) for a in ins])) != 1:\n             raise Exception('All input arrays and target arrays must have '\n@@ -1194,7 +1247,8 @@ class Graph(Model, containers.Graph):\n         '''\n         sample_weight = [standardize_weights(data[name],\n                                              sample_weight=sample_weight.get(name),\n-                                             class_weight=class_weight.get(name)) for name in self.output_order]\n+                                             class_weight=class_weight.get(name),\n+                                             sample_weight_mode=self.sample_weight_modes.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         if len(set([len(a) for a in ins])) != 1:\n             raise Exception('All input arrays and target arrays must have '\n@@ -1207,7 +1261,8 @@ class Graph(Model, containers.Graph):\n         Arguments: see `fit` method.\n         '''\n         sample_weight = [standardize_weights(data[name],\n-                                             sample_weight=sample_weight.get(name)) for name in self.output_order]\n+                                             sample_weight=sample_weight.get(name),\n+                                             sample_weight_mode=self.sample_weight_modes.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n         if len(set([len(a) for a in ins])) != 1:\n             raise Exception('All input arrays and target arrays must have '\n\n@@ -6,7 +6,7 @@ np.random.seed(1337)\n \n from keras.datasets import mnist\n from keras.models import Sequential, Graph\n-from keras.layers.core import Dense, Activation\n+from keras.layers.core import Dense, Activation, RepeatVector, TimeDistributedDense\n from keras.utils import np_utils\n \n nb_classes = 10\n@@ -17,6 +17,8 @@ standard_weight = 1\n high_weight = 5\n max_train_samples = 5000\n max_test_samples = 1000\n+timesteps = 3\n+loss = 'mse'\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n@@ -38,6 +40,14 @@ class_weight[weighted_class] = high_weight\n sample_weight = np.ones((y_train.shape[0])) * standard_weight\n sample_weight[y_train == weighted_class] = high_weight\n \n+temporal_Y_train = np.reshape(Y_train, (len(Y_train), 1, Y_train.shape[1]))\n+temporal_Y_train = np.repeat(temporal_Y_train, timesteps, axis=1)\n+temporal_Y_test = np.reshape(Y_test, (len(Y_test), 1, Y_test.shape[1]))\n+temporal_Y_test = np.repeat(temporal_Y_test, timesteps, axis=1)\n+\n+temporal_sample_weight = np.reshape(sample_weight, (len(sample_weight), 1))\n+temporal_sample_weight = np.repeat(temporal_sample_weight, timesteps, axis=1)\n+\n \n def create_sequential_model():\n     model = Sequential()\n@@ -57,7 +67,28 @@ def create_graph_model():\n     return model\n \n \n-def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n+def create_temporal_sequential_model():\n+    model = Sequential()\n+    model.add(RepeatVector(timesteps, input_shape=(784,)))\n+    model.add(TimeDistributedDense(10))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+def create_temporal_graph_model():\n+    model = Graph()\n+    model.add_input(name='input', input_shape=(784,))\n+    model.add_node(RepeatVector(timesteps),\n+                   name='d1', input='input')\n+    model.add_node(TimeDistributedDense(10, activation='softmax'),\n+                   name='d2', input='d1')\n+    model.add_output(name='output', input='d2')\n+    return model\n+\n+\n+def _test_weights_sequential(model, class_weight=None, sample_weight=None,\n+                             X_train=X_train, Y_train=Y_train,\n+                             X_test=X_test, Y_test=Y_test):\n     if sample_weight is not None:\n         model.fit(X_train, Y_train, batch_size=batch_size,\n                   nb_epoch=nb_epoch // 3, verbose=0,\n@@ -88,7 +119,9 @@ def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n     return score\n \n \n-def _test_weights_graph(model, class_weight=None, sample_weight=None):\n+def _test_weights_graph(model, class_weight=None, sample_weight=None,\n+                        X_train=X_train, Y_train=Y_train,\n+                        X_test=X_test, Y_test=Y_test):\n     model.fit({'input': X_train, 'output': Y_train},\n               batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n               class_weight={'output': class_weight},\n@@ -109,40 +142,66 @@ def _test_weights_graph(model, class_weight=None, sample_weight=None):\n     return score\n \n \n-def test_sequential():\n-    for loss in ['mae', 'mse']:\n # no weights: reference point\n model = create_sequential_model()\n model.compile(loss=loss, optimizer='rmsprop')\n-        standard_score = _test_weights_sequential(model)\n-        # test class_weight\n+standard_score_sequential = _test_weights_sequential(model)\n+\n+model = create_graph_model()\n+model.compile(loss={'output': loss}, optimizer='rmsprop')\n+standard_score_graph = _test_weights_graph(model)\n+\n+\n+def test_sequential_class_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n     score = _test_weights_sequential(model, class_weight=class_weight)\n-        assert(score < standard_score)\n-        # test sample_weight\n+    assert(score < standard_score_sequential)\n+\n+\n+def test_sequential_sample_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n     score = _test_weights_sequential(model, sample_weight=sample_weight)\n-        assert(score < standard_score)\n+    assert(score < standard_score_sequential)\n \n \n-def test_graph():\n-    for loss in ['mae', 'mse']:\n-        # no weights: reference point\n-        model = create_graph_model()\n-        model.compile(loss={'output': loss}, optimizer='rmsprop')\n-        standard_score = _test_weights_graph(model)\n-        # test class_weight\n+def test_sequential_temporal_sample_weights():\n+    # just check it runs\n+    model = create_temporal_sequential_model()\n+    model.compile(loss=loss, optimizer='rmsprop',\n+                  sample_weight_mode='temporal')\n+    score = _test_weights_sequential(model,\n+                                     sample_weight=temporal_sample_weight,\n+                                     Y_train=temporal_Y_train,\n+                                     Y_test=temporal_Y_test)\n+    assert(score < standard_score_sequential)\n+\n+\n+def test_graph_class_weights():\n     model = create_graph_model()\n     model.compile(loss={'output': loss}, optimizer='rmsprop')\n     score = _test_weights_graph(model, class_weight=class_weight)\n-        assert(score < standard_score)\n-        # test sample_weight\n+    assert(score < standard_score_graph)\n+\n+\n+def test_graph_sample_weights():\n     model = create_graph_model()\n     model.compile(loss={'output': loss}, optimizer='rmsprop')\n     score = _test_weights_graph(model, sample_weight=sample_weight)\n-        assert(score < standard_score)\n+    assert(score < standard_score_graph)\n+\n+\n+def test_graph_temporal_sample_weight():\n+    # just check it runs\n+    model = create_temporal_graph_model()\n+    model.compile(loss={'output': loss}, optimizer='rmsprop',\n+                  sample_weight_modes={'output': 'temporal'})\n+    score = _test_weights_graph(model,\n+                                sample_weight=temporal_sample_weight,\n+                                Y_train=temporal_Y_train,\n+                                Y_test=temporal_Y_test)\n+    assert(score < standard_score_graph)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efe4fc72e53cd123996514848786ecba48b8a360", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 19 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 75 | Churn Cumulative: 398 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,7 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,), output_shape=(2\n     '''\n     nb_sample = nb_train + nb_test\n     if classification:\n-        y = np.random.randint(0, nb_class, size=(nb_sample, 1))\n+        y = np.random.randint(0, nb_class, size=(nb_sample,))\n         X = np.zeros((nb_sample,) + input_shape)\n         for i in range(nb_sample):\n             X[i] = np.random.normal(loc=y[i], scale=0.7, size=input_shape)\n\n@@ -4,32 +4,36 @@ import pytest\n import numpy as np\n np.random.seed(1337)\n \n-from keras.datasets import mnist\n+from keras.utils.test_utils import get_test_data\n from keras.models import Sequential, Graph\n-from keras.layers.core import Dense, Activation, RepeatVector, TimeDistributedDense\n+from keras.layers import Dense, Activation, RepeatVector, TimeDistributedDense, GRU\n from keras.utils import np_utils\n \n nb_classes = 10\n batch_size = 128\n nb_epoch = 15\n-weighted_class = 9\n+weighted_class = 5\n standard_weight = 1\n-high_weight = 5\n-max_train_samples = 5000\n-max_test_samples = 1000\n+high_weight = 10\n+train_samples = 5000\n+test_samples = 1000\n timesteps = 3\n+input_dim = 10\n loss = 'mse'\n \n # the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\") / 255\n-X_test = X_test.astype(\"float32\") / 255\n+# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n+# X_train = X_train.reshape(60000, 784)[:max_train_samples]\n+# X_test = X_test.reshape(10000, 784)[:max_test_samples]\n+# X_train = X_train.astype(\"float32\") / 255\n+# X_test = X_test.astype(\"float32\") / 255\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                     nb_test=test_samples,\n+                                                     input_shape=(input_dim,),\n+                                                     classification=True,\n+                                                     nb_class=nb_classes)\n \n # convert class vectors to binary class matrices\n-y_train = y_train[:max_train_samples]\n-y_test = y_test[:max_test_samples]\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n test_ids = np.where(y_test == np.array(weighted_class))[0]\n@@ -40,6 +44,11 @@ class_weight[weighted_class] = high_weight\n sample_weight = np.ones((y_train.shape[0])) * standard_weight\n sample_weight[y_train == weighted_class] = high_weight\n \n+temporal_X_train = np.reshape(X_train, (len(X_train), 1, X_train.shape[1]))\n+temporal_X_train = np.repeat(temporal_X_train, timesteps, axis=1)\n+temporal_X_test = np.reshape(X_test, (len(X_test), 1, X_test.shape[1]))\n+temporal_X_test = np.repeat(temporal_X_test, timesteps, axis=1)\n+\n temporal_Y_train = np.reshape(Y_train, (len(Y_train), 1, Y_train.shape[1]))\n temporal_Y_train = np.repeat(temporal_Y_train, timesteps, axis=1)\n temporal_Y_test = np.reshape(Y_test, (len(Y_test), 1, Y_test.shape[1]))\n@@ -51,36 +60,36 @@ temporal_sample_weight = np.repeat(temporal_sample_weight, timesteps, axis=1)\n \n def create_sequential_model():\n     model = Sequential()\n-    model.add(Dense(50, input_shape=(784,)))\n+    model.add(Dense(32, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(10))\n+    model.add(Dense(nb_classes))\n     model.add(Activation('softmax'))\n     return model\n \n \n def create_graph_model():\n     model = Graph()\n-    model.add_input(name='input', input_shape=(784,))\n-    model.add_node(Dense(50, activation='relu'), name='d1', input='input')\n-    model.add_node(Dense(10, activation='softmax'), name='d2', input='d1')\n+    model.add_input(name='input', input_shape=(input_dim,))\n+    model.add_node(Dense(32, activation='relu'), name='d1', input='input')\n+    model.add_node(Dense(nb_classes, activation='softmax'), name='d2', input='d1')\n     model.add_output(name='output', input='d2')\n     return model\n \n \n def create_temporal_sequential_model():\n     model = Sequential()\n-    model.add(RepeatVector(timesteps, input_shape=(784,)))\n-    model.add(TimeDistributedDense(10))\n+    model.add(GRU(32, input_shape=(timesteps, input_dim), return_sequences=True))\n+    model.add(TimeDistributedDense(nb_classes))\n     model.add(Activation('softmax'))\n     return model\n \n \n def create_temporal_graph_model():\n     model = Graph()\n-    model.add_input(name='input', input_shape=(784,))\n-    model.add_node(RepeatVector(timesteps),\n+    model.add_input(name='input', input_shape=(timesteps, input_dim))\n+    model.add_node(GRU(32, return_sequences=True),\n                    name='d1', input='input')\n-    model.add_node(TimeDistributedDense(10, activation='softmax'),\n+    model.add_node(TimeDistributedDense(nb_classes, activation='softmax'),\n                    name='d2', input='d1')\n     model.add_output(name='output', input='d2')\n     return model\n@@ -167,12 +176,25 @@ def test_sequential_sample_weights():\n \n \n def test_sequential_temporal_sample_weights():\n-    # just check it runs\n     model = create_temporal_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop',\n                   sample_weight_mode='temporal')\n     score = _test_weights_sequential(model,\n                                      sample_weight=temporal_sample_weight,\n+                                     X_train=temporal_X_train,\n+                                     X_test=temporal_X_test,\n+                                     Y_train=temporal_Y_train,\n+                                     Y_test=temporal_Y_test)\n+    assert(score < standard_score_sequential)\n+\n+    # a twist: sample-wise weights with temporal output\n+    model = create_temporal_sequential_model()\n+    model.compile(loss=loss, optimizer='rmsprop',\n+                  sample_weight_mode=None)\n+    score = _test_weights_sequential(model,\n+                                     sample_weight=sample_weight,\n+                                     X_train=temporal_X_train,\n+                                     X_test=temporal_X_test,\n                                      Y_train=temporal_Y_train,\n                                      Y_test=temporal_Y_test)\n     assert(score < standard_score_sequential)\n@@ -193,12 +215,13 @@ def test_graph_sample_weights():\n \n \n def test_graph_temporal_sample_weight():\n-    # just check it runs\n     model = create_temporal_graph_model()\n     model.compile(loss={'output': loss}, optimizer='rmsprop',\n                   sample_weight_modes={'output': 'temporal'})\n     score = _test_weights_graph(model,\n                                 sample_weight=temporal_sample_weight,\n+                                X_train=temporal_X_train,\n+                                X_test=temporal_X_test,\n                                 Y_train=temporal_Y_train,\n                                 Y_test=temporal_Y_test)\n     assert(score < standard_score_graph)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3882f32f099371f9c0e9d924fc9f91c410c8189c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 446 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -330,28 +330,30 @@ class TestBackend(object):\n     def test_random_normal(self):\n         mean = 0.\n         std = 1.\n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        rand = KTF.eval(KTF.random_normal((1000, 1000), mean=mean, std=std))\n         assert(rand.shape == (1000, 1000))\n         assert(np.abs(np.mean(rand) - mean) < 0.01)\n         assert(np.abs(np.std(rand) - std) < 0.01)\n \n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        rand = KTH.eval(KTH.random_normal((1000, 1000), mean=mean, std=std))\n         assert(rand.shape == (1000, 1000))\n         assert(np.abs(np.mean(rand) - mean) < 0.01)\n         assert(np.abs(np.std(rand) - std) < 0.01)\n \n     def test_random_uniform(self):\n-        mean = 0.\n-        std = 1.\n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        min = -1.\n+        max = 1.\n+        rand = KTF.eval(KTF.random_uniform((1000, 1000), min, max))\n         assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n+        assert(np.abs(np.mean(rand)) < 0.01)\n+        assert(np.max(rand) <= max)\n+        assert(np.min(rand) >= min)\n \n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n+        rand = KTH.eval(KTH.random_uniform((1000, 1000), min, max))\n         assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n+        assert(np.abs(np.mean(rand)) < 0.01)\n+        assert(np.max(rand) <= max)\n+        assert(np.min(rand) >= min)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4341c623ff651e847d0d9af2335e8310aa359288", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 369 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -21,12 +21,6 @@ timesteps = 3\n input_dim = 10\n loss = 'mse'\n \n-# the data, shuffled and split between tran and test sets\n-# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n-# X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-# X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-# X_train = X_train.astype(\"float32\") / 255\n-# X_test = X_test.astype(\"float32\") / 255\n (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n                                                      nb_test=test_samples,\n                                                      input_shape=(input_dim,),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#027a0182100a7391c5194c23ab281b27480e25f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 182 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 182 | Churn Cumulative: 445 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,181 @@\n+'''Visualization of the filters of VGG16, via gradient ascent in input space.\n+\n+This script can run on CPU in a few minutes (with the TensorFlow backend).\n+\n+Results example: http://i.imgur.com/4nj4KjN.jpg\n+'''\n+from __future__ import print_function\n+from scipy.misc import imsave\n+import numpy as np\n+import time\n+import os\n+import h5py\n+\n+from keras.models import Sequential\n+from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras import backend as K\n+\n+# dimensions of the generated pictures for each filter.\n+img_width = 128\n+img_height = 128\n+\n+# path to the model weights file.\n+weights_path = 'vgg16_weights.h5'\n+\n+# the name of the layer we want to visualize (see model definition below)\n+filter_name = 'conv5_1'\n+\n+# util function to convert a tensor into a valid image\n+def deprocess_image(x):\n+    # normalize tensor: center on 0., ensure std is 0.1\n+    x -= x.mean()\n+    x /= (x.std() + 1e-5)\n+    x *= 0.1\n+\n+    # clip to [0, 1]\n+    x += 0.5\n+    x = np.clip(x, 0, 1)\n+\n+    # convert to RGB array\n+    x *= 255\n+    x = x.transpose((1, 2, 0))\n+    x = np.clip(x, 0, 255).astype('uint8')\n+    return x\n+\n+# this will contain our generated image\n+input_img = K.placeholder((1, 3, img_width, img_height))\n+\n+# build the VGG16 network with our input_img as input\n+first_layer = ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height))\n+first_layer.input = input_img\n+\n+model = Sequential()\n+model.add(first_layer)\n+model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+# load the weights of the VGG16 networks\n+# (trained on ImageNet, won the ILSVRC competition in 2014)\n+# note: when there is a complete match between your model definition\n+# and your weight savefile, you can simply call model.load_weights(filename)\n+assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n+f = h5py.File(weights_path)\n+for k in range(f.attrs['nb_layers']):\n+    if k >= len(model.layers):\n+        # we don't look at the last (fully-connected) layers in the savefile\n+        break\n+    g = f['layer_{}'.format(k)]\n+    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n+    model.layers[k].set_weights(weights)\n+f.close()\n+print('Model loaded.')\n+\n+# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n+layer_dict = dict([(layer.name, layer) for layer in model.layers])\n+\n+\n+def normalize(x):\n+    # utility function to normalize a tensor by its L2 norm\n+    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n+\n+\n+kept_filters = []\n+for filter_index in range(0, 200):\n+    # we only scan through the first 200 filters,\n+    # but there are actually 512 of them\n+    print('Processing filter %d' % filter_index)\n+    start_time = time.time()\n+\n+    # we build a loss function that maximizes the activation\n+    # of the nth filter of the layer considered\n+    layer_output = layer_dict[filter_name].get_output()\n+    loss = K.mean(layer_output[:, filter_index, :, :])\n+\n+    # we compute the gradient of the input picture wrt this loss\n+    grads = K.gradients(loss, input_img)[0]\n+\n+    # normalization trick: we normalize the gradient\n+    grads = normalize(grads)\n+\n+    # this function returns the loss and grads given the input picture\n+    iterate = K.function([input_img], [loss, grads])\n+\n+    # step size for gradient ascent\n+    step = 1.\n+\n+    # we start from a gray image with some random noise\n+    input_img_data = np.random.random((1, 3, img_width, img_height)) * 20 + 128.\n+\n+    # we run gradient ascent for 12 steps\n+    for i in range(12):\n+        loss_value, grads_value = iterate([input_img_data])\n+        input_img_data += grads_value * step\n+\n+        print('Current loss value:', loss_value)\n+        if loss_value <= 0.:\n+            # some filters get stuck to 0, we can skip them\n+            break\n+\n+    # decode the resulting input image\n+    if loss_value > 0:\n+        img = deprocess_image(input_img_data[0])\n+        kept_filters.append((img, loss_value))\n+    end_time = time.time()\n+    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n+\n+# we will stich the best 64 filters on a 8 x 8 grid.\n+n = 8\n+\n+# the filters that have the highest loss are assumed to be better-looking.\n+# we will only keep the top 64 filters.\n+kept_filters.sort(key=lambda x: x[1], reverse=True)\n+kept_filters = kept_filters[:n * n]\n+\n+# build a black picture with enough space for\n+# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n+margin = 5\n+width = n * img_width + (n - 1) * margin\n+height = n * img_height + (n - 1) * margin\n+stitched_filters = np.zeros((width, height, 3))\n+\n+# fill the picture with our saved filters\n+for i in range(n):\n+    for j in range(n):\n+        img, loss = kept_filters[i * n + j]\n+        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n+                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n+\n+# save the result to disk\n+imsave('stitched_filters_%dx%d.png' % (n, n), stitched_filters)\n\n@@ -121,6 +121,7 @@ model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n # (trained on ImageNet, won the ILSVRC competition in 2014)\n # note: when there is a complete match between your model definition\n # and your weight savefile, you can simply call model.load_weights(filename)\n+assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n f = h5py.File(weights_path)\n for k in range(f.attrs['nb_layers']):\n     if k >= len(model.layers):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92b1948d5106abeafa6f169de0e406a13266ca68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 185 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ img_height = 128\n weights_path = 'vgg16_weights.h5'\n \n # the name of the layer we want to visualize (see model definition below)\n-filter_name = 'conv5_1'\n+layer_name = 'conv5_1'\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n@@ -120,7 +120,7 @@ for filter_index in range(0, 200):\n \n     # we build a loss function that maximizes the activation\n     # of the nth filter of the layer considered\n-    layer_output = layer_dict[filter_name].get_output()\n+    layer_output = layer_dict[layer_name].get_output()\n     loss = K.mean(layer_output[:, filter_index, :, :])\n \n     # we compute the gradient of the input picture wrt this loss\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb2b3ada3d502c740f2704e44362c9bc32be4353", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 194 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,11 @@\n This script can run on CPU in a few minutes (with the TensorFlow backend).\n \n Results example: http://i.imgur.com/4nj4KjN.jpg\n+\n+Before running this script, download the weights for the VGG16 model at:\n+https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\n+(source: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)\n+and make sure the variable `weights_path` in this script matches the location of the file.\n '''\n from __future__ import print_function\n from scipy.misc import imsave\n@@ -138,8 +143,8 @@ for filter_index in range(0, 200):\n     # we start from a gray image with some random noise\n     input_img_data = np.random.random((1, 3, img_width, img_height)) * 20 + 128.\n \n-    # we run gradient ascent for 12 steps\n-    for i in range(12):\n+    # we run gradient ascent for 20 steps\n+    for i in range(20):\n         loss_value, grads_value = iterate([input_img_data])\n         input_img_data += grads_value * step\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb8e8b281e00b91472fb87db95e66dc3f3d87587", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 312 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n classification task.\n \n GPU command:\n-    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n+    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn_lstm.py\n \n Get to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n '''\n\n@@ -4,7 +4,7 @@\n 2- Freeze convolutional layers and fine-tune dense layers\n    for the classification of digits [5..9].\n \n-Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n+Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_transfer_cnn.py\n \n Get to 99.8% test accuracy after 5 epochs\n for the first five digits classifier\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c5d11f1da3c3ecf7c9f29cb30a4515f9b8403b61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 14 | Methods Changed: 13 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 97 | Churn Cumulative: 7876 | Contributors (this commit): 37 | Commits (past 90d): 86 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,7 +88,7 @@ class Layer(object):\n         self.get_input = tmp_input\n         return Y\n \n-    def set_previous(self, layer, connection_map={}):\n+    def set_previous(self, layer):\n         '''Connect a layer to its parent in the computational graph.\n         '''\n         assert self.nb_input == layer.nb_output == 1, 'Cannot connect layers: input count and output count should be 1.'\n@@ -1168,28 +1168,62 @@ class AutoEncoder(Layer):\n \n     autoencoder = Sequential()\n     autoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder,\n-                                output_reconstruction=False))\n+                                output_reconstruction=True))\n+\n+    # training the autoencoder:\n+    autoencoder.compile(optimizer='sgd', loss='mse')\n+    autoencoder.fit(X_train, X_train, nb_epoch=10)\n+\n+    # predicting compressed representations of inputs:\n+    autoencoder.output_reconstruction = False  # the autoencoder has to be recompiled after modifying this property\n+    autoencoder.compile(optimizer='sgd', loss='mse')\n+    representations = autoencoder.predict(X_test)\n+\n+    # the model is still trainable, although it now expects compressed representations as targets:\n+    autoencoder.fit(X_test, representations, nb_epoch=1)  # in this case the loss will be 0, so it's useless\n+\n+    # to keep training against the original inputs, just switch back output_reconstruction to True:\n+    autoencoder.output_reconstruction = False\n+    autoencoder.compile(optimizer='sgd', loss='mse')\n+    autoencoder.fit(X_train, X_train, nb_epoch=10)\n     ```\n     '''\n     def __init__(self, encoder, decoder, output_reconstruction=True,\n                  weights=None, **kwargs):\n         super(AutoEncoder, self).__init__(**kwargs)\n \n-        self.output_reconstruction = output_reconstruction\n+        self._output_reconstruction = output_reconstruction\n         self.encoder = encoder\n         self.decoder = decoder\n \n+        if output_reconstruction:\n             self.decoder.set_previous(self.encoder)\n \n         if weights is not None:\n             self.set_weights(weights)\n \n+        super(AutoEncoder, self).__init__(**kwargs)\n+        self.build()\n+\n+    @property\n+    def output_reconstruction(self):\n+        return self._output_reconstruction\n+\n+    @output_reconstruction.setter\n+    def output_reconstruction(self, value):\n+        self._output_reconstruction = value\n+        self.build()\n+\n     def build(self):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n-        for layer in [self.encoder, self.decoder]:\n+        if self.output_reconstruction:\n+            layers = [self.encoder, self.decoder]\n+        else:\n+            layers = [self.encoder]\n+        for layer in layers:\n             params, regularizers, constraints, updates = layer.get_params()\n             self.regularizers += regularizers\n             self.updates += updates\n@@ -1198,9 +1232,8 @@ class AutoEncoder(Layer):\n                     self.params.append(p)\n                     self.constraints.append(c)\n \n-    def set_previous(self, node, connection_map={}):\n-        self.encoder.set_previous(node, connection_map)\n-        super(AutoEncoder, self).set_previous(node, connection_map)\n+    def set_previous(self, node):\n+        self.encoder.set_previous(node)\n \n     def get_weights(self):\n         weights = []\n@@ -1220,9 +1253,6 @@ class AutoEncoder(Layer):\n     def input(self):\n         return self.encoder.input\n \n-    def _get_hidden(self, train=False):\n-        return self.encoder.get_output(train)\n-\n     @property\n     def input_shape(self):\n         return self.encoder.input_shape\n@@ -1230,15 +1260,15 @@ class AutoEncoder(Layer):\n     @property\n     def output_shape(self):\n         if self.output_reconstruction:\n-            return self.encoder.previous.output_shape\n+            return self.decoder.output_shape\n         else:\n-            return self.decoder.previous.output_shape\n+            return self.encoder.output_shape\n \n     def get_output(self, train=False):\n-        if not train and not self.output_reconstruction:\n-            return self.encoder.get_output(train)\n-\n+        if self.output_reconstruction:\n             return self.decoder.get_output(train)\n+        else:\n+            return self.encoder.get_output(train)\n \n     def get_config(self):\n         return {'name': self.__class__.__name__,\n\n@@ -5,6 +5,7 @@ from numpy.testing import assert_allclose\n \n from keras import backend as K\n from keras.layers import core\n+from keras.layers import containers\n \n \n def test_input_output():\n@@ -114,15 +115,33 @@ def test_autoencoder():\n     _runner(layer)\n \n \n-def test_autoencoder_second_layer():\n-    # regression test for issue #1275\n-    encoder = core.Dense(input_dim=10, output_dim=2)\n-    decoder = core.Dense(input_dim=2, output_dim=10)\n-    model = Sequential()\n-    model.add(core.Dense(input_dim=20, output_dim=10))\n-    model.add(core.AutoEncoder(encoder=encoder, decoder=decoder,\n-                               output_reconstruction=False))\n-    model.compile(loss='mse', optimizer='sgd')\n+def test_autoencoder_advanced():\n+    encoder = containers.Sequential([core.Dense(5, input_shape=(10,))])\n+    decoder = containers.Sequential([core.Dense(10, input_shape=(5,))])\n+    X_train = np.random.random((100, 10))\n+    X_test = np.random.random((100, 10))\n+\n+    autoencoder = Sequential()\n+    autoencoder.add(core.Dense(output_dim=10, input_dim=10))\n+    autoencoder.add(core.AutoEncoder(encoder=encoder, decoder=decoder,\n+                                     output_reconstruction=True))\n+\n+    # training the autoencoder:\n+    autoencoder.compile(optimizer='sgd', loss='mse')\n+    autoencoder.fit(X_train, X_train, nb_epoch=1, batch_size=32)\n+\n+    # predicting compressed representations of inputs:\n+    autoencoder.output_reconstruction = False  # the autoencoder has to be recompiled after modifying this property\n+    autoencoder.compile(optimizer='sgd', loss='mse')\n+    representations = autoencoder.predict(X_test)\n+\n+    # the model is still trainable, although it now expects compressed representations as targets:\n+    autoencoder.fit(X_test, representations, nb_epoch=1, batch_size=32)\n+\n+    # to keep training against the original inputs, just switch back output_reconstruction to True:\n+    autoencoder.output_reconstruction = False\n+    autoencoder.compile(optimizer='sgd', loss='mse')\n+    autoencoder.fit(X_train, X_train, nb_epoch=1)\n \n \n def test_maxout_dense():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
