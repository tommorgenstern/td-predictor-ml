{"custom_id": "scrapy#4d77c3084e64d351e3acf9092b4171d4418925fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 65 | Churn Cumulative: 816 | Contributors (this commit): 10 | Commits (past 90d): 29 | Contributors (cumulative): 18 | DMM Complexity: 0.29545454545454547\n\nDIFF:\n@@ -93,12 +93,28 @@ class FileFeedStorage(object):\n \n class S3FeedStorage(BlockingFeedStorage):\n \n-    def __init__(self, uri):\n+    def __init__(self, uri, access_key=None, secret_key=None):\n+        # BEGIN Backwards compatibility for initialising without keys (and\n+        # without using from_crawler)\n         from scrapy.conf import settings\n+        no_defaults = access_key is None and secret_key is None\n+        if no_defaults and ('AWS_ACCESS_KEY_ID' in settings or\n+                            'AWS_SECRET_ACCESS_KEY' in settings):\n+            import warnings\n+            from scrapy.exceptions import ScrapyDeprecationWarning\n+            warnings.warn(\n+                \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n+                \"without AWS keys is deprecated. Please supply credentials or \"\n+                \"use the `from_crawler()` constructor.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2\n+            )\n+            access_key = settings['AWS_ACCESS_KEY_ID']\n+            secret_key = settings['AWS_SECRET_ACCESS_KEY']\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n-        self.access_key = u.username or settings['AWS_ACCESS_KEY_ID']\n-        self.secret_key = u.password or settings['AWS_SECRET_ACCESS_KEY']\n+        self.access_key = u.username or access_key\n+        self.secret_key = u.password or secret_key\n         self.is_botocore = is_botocore()\n         self.keyname = u.path[1:]  # remove first \"/\"\n         if self.is_botocore:\n@@ -111,6 +127,11 @@ class S3FeedStorage(BlockingFeedStorage):\n             import boto\n             self.connect_s3 = boto.connect_s3\n \n+    @classmethod\n+    def from_crawler(cls, crawler, uri):\n+        return cls(uri, crawler.settings['AWS_ACCESS_KEY_ID'],\n+                   crawler.settings['AWS_SECRET_ACCESS_KEY'])\n+\n     def _store_in_thread(self, file):\n         file.seek(0)\n         if self.is_botocore:\n\n@@ -6,12 +6,14 @@ from io import BytesIO\n import tempfile\n import shutil\n from six.moves.urllib.parse import urlparse\n+import warnings\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n from twisted.internet import defer\n from scrapy.crawler import CrawlerRunner\n from scrapy.settings import Settings\n+from tests import mock\n from tests.mockserver import MockServer\n from w3lib.url import path_to_file_uri\n \n@@ -131,13 +133,47 @@ class BlockingFeedStorageTest(unittest.TestCase):\n \n class S3FeedStorageTest(unittest.TestCase):\n \n+    @mock.patch('scrapy.conf.settings', new={'AWS_ACCESS_KEY_ID': 'conf_key',\n+                'AWS_SECRET_ACCESS_KEY': 'conf_secret'}, create=True)\n+    def test_parse_credentials(self):\n+        try:\n+            import boto\n+        except ImportError:\n+            raise unittest.SkipTest(\"S3FeedStorage requires boto\")\n+        aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',\n+                           'AWS_SECRET_ACCESS_KEY': 'settings_secret'}\n+        crawler = get_crawler(settings_dict=aws_credentials)\n+        # Instantiate with crawler\n+        storage = S3FeedStorage.from_crawler(crawler,\n+                                             's3://mybucket/export.csv')\n+        self.assertEqual(storage.access_key, 'settings_key')\n+        self.assertEqual(storage.secret_key, 'settings_secret')\n+        # Instantiate directly\n+        storage = S3FeedStorage('s3://mybucket/export.csv',\n+                                aws_credentials['AWS_ACCESS_KEY_ID'],\n+                                aws_credentials['AWS_SECRET_ACCESS_KEY'])\n+        self.assertEqual(storage.access_key, 'settings_key')\n+        self.assertEqual(storage.secret_key, 'settings_secret')\n+        # URI priority > settings priority\n+        storage = S3FeedStorage('s3://uri_key:uri_secret@mybucket/export.csv',\n+                                aws_credentials['AWS_ACCESS_KEY_ID'],\n+                                aws_credentials['AWS_SECRET_ACCESS_KEY'])\n+        self.assertEqual(storage.access_key, 'uri_key')\n+        self.assertEqual(storage.secret_key, 'uri_secret')\n+        # Backwards compatibility for initialising without settings\n+        with warnings.catch_warnings(record=True) as w:\n+            storage = S3FeedStorage('s3://mybucket/export.csv')\n+            self.assertEqual(storage.access_key, 'conf_key')\n+            self.assertEqual(storage.secret_key, 'conf_secret')\n+            self.assertTrue('without AWS keys' in str(w[-1].message))\n+\n     @defer.inlineCallbacks\n     def test_store(self):\n         assert_aws_environ()\n         uri = os.environ.get('S3_TEST_FILE_URI')\n         if not uri:\n             raise unittest.SkipTest(\"No S3 URI available for testing\")\n-        storage = S3FeedStorage(uri)\n+        storage = S3FeedStorage(uri, Settings())\n         verifyObject(IFeedStorage, storage)\n         file = storage.open(scrapy.Spider(\"default\"))\n         expected_content = b\"content: \\xe2\\x98\\x83\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11a1f970b7b7b68b3d968df4b29c4269ab220ac6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 516 | Contributors (this commit): 37 | Commits (past 90d): 2 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -234,7 +234,7 @@ REFERRER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = [500, 502, 503, 504, 408]\n+RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5dc9a88c347db3497b03949938184ca339f4e9cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 203 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -64,7 +64,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n                 request.meta.get('handle_httpstatus_all', False)):\n             return response\n \n-        allowed_status = (301, 302, 303, 307)\n+        allowed_status = (301, 302, 303, 307, 308)\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n \n@@ -72,7 +72,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n         redirected_url = urljoin(request.url, location)\n \n-        if response.status in (301, 307) or request.method == 'HEAD':\n+        if response.status in (301, 307, 308) or request.method == 'HEAD':\n             redirected = request.replace(url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n\n@@ -22,12 +22,12 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority > req.priority\n \n-    def test_redirect_301(self):\n-        def _test(method):\n-            url = 'http://www.example.com/301'\n+    def test_redirect_3xx_permanent(self):\n+        def _test(method, status=301):\n+            url = 'http://www.example.com/{}'.format(status)\n             url2 = 'http://www.example.com/redirected'\n             req = Request(url, method=method)\n-            rsp = Response(url, headers={'Location': url2}, status=301)\n+            rsp = Response(url, headers={'Location': url2}, status=status)\n \n             req2 = self.mw.process_response(req, rsp, self.spider)\n             assert isinstance(req2, Request)\n@@ -42,6 +42,10 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         _test('POST')\n         _test('HEAD')\n \n+        _test('GET', status=308)\n+        _test('POST', status=308)\n+        _test('HEAD', status=308)\n+\n     def test_dont_redirect(self):\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fdc10684fc427e5446350cada465ec934330a3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 140 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -322,6 +322,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n             (True, 203, {'Last-Modified': self.yesterday}),\n             (True, 300, {'Last-Modified': self.yesterday}),\n             (True, 301, {'Last-Modified': self.yesterday}),\n+            (True, 308, {'Last-Modified': self.yesterday}),\n             (True, 401, {'Last-Modified': self.yesterday}),\n             (True, 404, {'Cache-Control': 'public, max-age=600'}),\n             (True, 302, {'Expires': self.tomorrow}),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#15a5c533fa6f448b7e5cd72ef099725c2295ef6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 134 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,6 +42,10 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         _test('POST')\n         _test('HEAD')\n \n+        _test('GET', status=307)\n+        _test('POST', status=307)\n+        _test('HEAD', status=307)\n+\n         _test('GET', status=308)\n         _test('POST', status=308)\n         _test('HEAD', status=308)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#219c8aa0b622260b9814379be41132367ef33e39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 38 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/6 | Churn Δ: 88 | Churn Cumulative: 473 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,12 +1,8 @@\n from __future__ import print_function\n-import sys\n-import platform\n-\n-import twisted\n-import OpenSSL\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n+from scrapy.utils.log import scrapy_components_versions\n \n \n class Command(ScrapyCommand):\n@@ -27,38 +23,8 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         if opts.verbose:\n-            import cssselect\n-            import parsel\n-            import lxml.etree\n-            import w3lib\n-\n-            lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n-            libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n-\n-            try:\n-                w3lib_version = w3lib.__version__\n-            except AttributeError:\n-                w3lib_version = \"<1.14.3\"\n-\n-            print(\"Scrapy    : %s\" % scrapy.__version__)\n-            print(\"lxml      : %s\" % lxml_version)\n-            print(\"libxml2   : %s\" % libxml2_version)\n-            print(\"cssselect : %s\" % cssselect.__version__)\n-            print(\"parsel    : %s\" % parsel.__version__)\n-            print(\"w3lib     : %s\" % w3lib_version)\n-            print(\"Twisted   : %s\" % twisted.version.short())\n-            print(\"Python    : %s\" % sys.version.replace(\"\\n\", \"- \"))\n-            print(\"pyOpenSSL : %s\" % self._get_openssl_version())\n-            print(\"Platform  : %s\" % platform.platform())\n+            for name, version in scrapy_components_versions():\n+                print(\"%-9s : %s\" % (name, version))\n         else:\n             print(\"Scrapy %s\" % scrapy.__version__)\n \n-    def _get_openssl_version(self):\n-        try:\n-            openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\\\n-                .decode('ascii', errors='replace')\n-        # pyOpenSSL 0.12 does not expose openssl version\n-        except AttributeError:\n-            openssl = 'Unknown OpenSSL version'\n-\n-        return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n\n@@ -139,10 +139,56 @@ def _get_handler(settings):\n     return handler\n \n \n+def scrapy_components_versions():\n+    import platform\n+\n+    import cssselect\n+    import parsel\n+    import lxml.etree\n+    import twisted\n+    import w3lib\n+\n+    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n+    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n+\n+    try:\n+        w3lib_version = w3lib.__version__\n+    except AttributeError:\n+        w3lib_version = \"<1.14.3\"\n+\n+    return [\n+        (\"Scrapy\", scrapy.__version__),\n+        (\"lxml\", lxml_version),\n+        (\"libxml2\", libxml2_version),\n+        (\"cssselect\", cssselect.__version__),\n+        (\"parsel\", parsel.__version__),\n+        (\"w3lib\", w3lib_version),\n+        (\"Twisted\", twisted.version.short()),\n+        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n+        (\"pyOpenSSL\", _get_openssl_version()),\n+        (\"Platform\",  platform.platform()),\n+    ]\n+\n+\n+def _get_openssl_version():\n+    try:\n+        import OpenSSL\n+        openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\\\n+            .decode('ascii', errors='replace')\n+    # pyOpenSSL 0.12 does not expose openssl version\n+    except AttributeError:\n+        openssl = 'Unknown OpenSSL version'\n+\n+    return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n+\n+\n def log_scrapy_info(settings):\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n-\n+    logger.info(\"Versions: %(versions)s}\",\n+                {'versions': \", \".join(\"%s %s\" % (name, version)\n+                    for name, version in scrapy_components_versions()\n+                    if name != \"Scrapy\")})\n     d = dict(overridden_settings(settings))\n     logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf7ef3e4c3ea70a60f6e5f9e5c8a6d842e2b72b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 44 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/4 | Churn Δ: 91 | Churn Cumulative: 564 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,7 @@ from __future__ import print_function\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.log import scrapy_components_versions\n+from scrapy.utils.versions import scrapy_components_versions\n \n \n class Command(ScrapyCommand):\n\n@@ -11,6 +11,8 @@ from twisted.python import log as twisted_log\n import scrapy\n from scrapy.settings import overridden_settings, Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.versions import scrapy_components_versions\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -139,49 +141,6 @@ def _get_handler(settings):\n     return handler\n \n \n-def scrapy_components_versions():\n-    import platform\n-\n-    import cssselect\n-    import parsel\n-    import lxml.etree\n-    import twisted\n-    import w3lib\n-\n-    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n-    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n-\n-    try:\n-        w3lib_version = w3lib.__version__\n-    except AttributeError:\n-        w3lib_version = \"<1.14.3\"\n-\n-    return [\n-        (\"Scrapy\", scrapy.__version__),\n-        (\"lxml\", lxml_version),\n-        (\"libxml2\", libxml2_version),\n-        (\"cssselect\", cssselect.__version__),\n-        (\"parsel\", parsel.__version__),\n-        (\"w3lib\", w3lib_version),\n-        (\"Twisted\", twisted.version.short()),\n-        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n-        (\"pyOpenSSL\", _get_openssl_version()),\n-        (\"Platform\",  platform.platform()),\n-    ]\n-\n-\n-def _get_openssl_version():\n-    try:\n-        import OpenSSL\n-        openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\\\n-            .decode('ascii', errors='replace')\n-    # pyOpenSSL 0.12 does not expose openssl version\n-    except AttributeError:\n-        openssl = 'Unknown OpenSSL version'\n-\n-    return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n-\n-\n def log_scrapy_info(settings):\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n\n@@ -0,0 +1,44 @@\n+import platform\n+import sys\n+\n+import cssselect\n+import lxml.etree\n+import parsel\n+import twisted\n+import w3lib\n+\n+import scrapy\n+\n+\n+def scrapy_components_versions():\n+    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n+    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n+    try:\n+        w3lib_version = w3lib.__version__\n+    except AttributeError:\n+        w3lib_version = \"<1.14.3\"\n+\n+    return [\n+        (\"Scrapy\", scrapy.__version__),\n+        (\"lxml\", lxml_version),\n+        (\"libxml2\", libxml2_version),\n+        (\"cssselect\", cssselect.__version__),\n+        (\"parsel\", parsel.__version__),\n+        (\"w3lib\", w3lib_version),\n+        (\"Twisted\", twisted.version.short()),\n+        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n+        (\"pyOpenSSL\", _get_openssl_version()),\n+        (\"Platform\",  platform.platform()),\n+    ]\n+\n+\n+def _get_openssl_version():\n+    try:\n+        import OpenSSL\n+        openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\\\n+            .decode('ascii', errors='replace')\n+    # pyOpenSSL 0.12 does not expose openssl version\n+    except AttributeError:\n+        openssl = 'Unknown OpenSSL version'\n+\n+    return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e6b5cc29f15dbf4f1941fca70dd9c126e4ba556", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 27 | Churn Cumulative: 625 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,6 @@ import errno\n import six\n from functools import partial, wraps\n import sys\n-import time\n \n from scrapy.utils.decorators import deprecated\n \n@@ -198,10 +197,30 @@ def binary_is_text(data):\n     return all(c not in _BINARYCHARS for c in data)\n \n \n+def _getargspec_py23(func):\n+    \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n+                                                        defaults)\n+\n+    Identical to inspect.getargspec() in python2, but uses\n+    inspect.getfullargspec() for python3 behind the scenes to avoid\n+    DeprecationWarning.\n+\n+    >>> def f(a, b=2, *ar, **kw):\n+    ...     pass\n+\n+    >>> _getargspec_py23(f)\n+    ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n+    \"\"\"\n+    if six.PY2:\n+        return inspect.getargspec(func)\n+\n+    return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])\n+\n+\n def get_func_args(func, stripself=False):\n     \"\"\"Return the argument name list of a callable\"\"\"\n     if inspect.isfunction(func):\n-        func_args, _, _, _ = inspect.getargspec(func)\n+        func_args, _, _, _ = _getargspec_py23(func)\n     elif inspect.isclass(func):\n         return get_func_args(func.__init__, True)\n     elif inspect.ismethod(func):\n@@ -248,9 +267,9 @@ def get_spec(func):\n     \"\"\"\n \n     if inspect.isfunction(func) or inspect.ismethod(func):\n-        spec = inspect.getargspec(func)\n+        spec = _getargspec_py23(func)\n     elif hasattr(func, '__call__'):\n-        spec = inspect.getargspec(func.__call__)\n+        spec = _getargspec_py23(func.__call__)\n     else:\n         raise TypeError('%s is not callable' % type(func))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01ac8838934071b89c5c711688f3d833387e27d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 46 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ class SitemapSpider(Spider):\n                     if any(x.search(loc) for x in self._follow):\n                         yield Request(loc, callback=self._parse_sitemap)\n             elif s.type == 'urlset':\n-                for loc in iterloc(s):\n+                for loc in iterloc(s, self.sitemap_alternate_links):\n                     for r, c in self._cbs:\n                         if r.search(loc):\n                             yield Request(loc, callback=c)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0cb3085f8453265a8f37de684e62eda8b5398c75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 27 | Churn Cumulative: 235 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -348,6 +348,33 @@ Sitemap: /sitemap-relative-url.xml\n                           'http://example.com/sitemap-uppercase.xml',\n                           'http://www.example.com/sitemap-relative-url.xml'])\n \n+    def test_alternate_url_locs(self):\n+        sitemap = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n+        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n+        <url>\n+            <loc>http://www.example.com/english/</loc>\n+            <xhtml:link rel=\"alternate\" hreflang=\"de\"\n+                href=\"http://www.example.com/deutsch/\"/>\n+            <xhtml:link rel=\"alternate\" hreflang=\"de-ch\"\n+                href=\"http://www.example.com/schweiz-deutsch/\"/>\n+            <xhtml:link rel=\"alternate\" hreflang=\"it\"\n+                href=\"http://www.example.com/italiano/\"/>\n+            <xhtml:link rel=\"alternate\" hreflang=\"it\"/><!-- wrong tag without href -->\n+        </url>\n+    </urlset>\"\"\"\n+        r = TextResponse(url=\"http://www.example.com/sitemap.xml\", body=sitemap)\n+        spider = self.spider_class(\"example.com\")\n+        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n+                         ['http://www.example.com/english/'])\n+\n+        spider.sitemap_alternate_links = True\n+        self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n+                         ['http://www.example.com/english/',\n+                          'http://www.example.com/deutsch/',\n+                          'http://www.example.com/schweiz-deutsch/',\n+                          'http://www.example.com/italiano/'])\n+\n \n class DeprecationTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c016a4309dbb045c17842e329043b7d9951e8f14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 160 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -270,7 +270,7 @@ class ItemMetaTest(unittest.TestCase):\n                 def f(self):\n                     # For rationale of this see:\n                     # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n-                    return __class__\n+                    return __class__  # noqa https://github.com/scrapy/scrapy/issues/2836\n \n             MyItem()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be71f98e92688c759d3af48101617229dcdfe05f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 41 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -87,7 +87,7 @@ class Spider(object_ref):\n         return Request(url, dont_filter=True)\n \n     def parse(self, response):\n-        raise NotImplementedError\n+        raise NotImplementedError('Spider.parse callback is not defined')\n \n     @classmethod\n     def update_settings(cls, settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7adab61a7a5f88c78311cc44a468c7b8d0a4c954", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 14 | Churn Cumulative: 249 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -429,3 +429,17 @@ class DeprecationTest(unittest.TestCase):\n             self.assertEqual(len(requests), 1)\n             self.assertEqual(requests[0].url, 'http://example.com/foo')\n             self.assertEqual(len(w), 1)\n+\n+\n+class NoParseMethodSpiderTest(unittest.TestCase):\n+\n+    spider_class = Spider\n+\n+    def test_undefined_parse_method(self):\n+        spider = self.spider_class('example.com')\n+        text = 'Random text response'\n+        resp = TextResponse(url=\"http://www.example.com/random_url\", body=text)\n+\n+        exc_msg = 'Spider.parse callback is not defined'\n+        with self.assertRaisesRegexp(NotImplementedError, exc_msg):\n+            spider.parse(resp)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12409a0cf6c37ff5c19588bb064690549798bb37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 251 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -437,7 +437,7 @@ class NoParseMethodSpiderTest(unittest.TestCase):\n \n     def test_undefined_parse_method(self):\n         spider = self.spider_class('example.com')\n-        text = 'Random text response'\n+        text = b'Random text'\n         resp = TextResponse(url=\"http://www.example.com/random_url\", body=text)\n \n         exc_msg = 'Spider.parse callback is not defined'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2960c9b5683dceb149823e7f927d8c86ee83deb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 43 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -87,7 +87,7 @@ class Spider(object_ref):\n         return Request(url, dont_filter=True)\n \n     def parse(self, response):\n-        raise NotImplementedError('Spider.parse callback is not defined')\n+        raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))\n \n     @classmethod\n     def update_settings(cls, settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ca61a20512b7306e7266e6667a0c02ae5ebe557", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 188 | Lines Deleted: 188 | Files Changed: 30 | Hunks: 145 | Methods Changed: 115 | Complexity Δ (Sum/Max): -4/5 | Churn Δ: 376 | Churn Cumulative: 6587 | Contributors (this commit): 43 | Commits (past 90d): 36 | Contributors (cumulative): 170 | DMM Complexity: None\n\nDIFF:\n@@ -68,4 +68,4 @@ class CmdlineTest(unittest.TestCase):\n             settingsstr = settingsstr.replace(char, '\"')\n         settingsdict = json.loads(settingsstr)\n         six.assertCountEqual(self, settingsdict.keys(), EXTENSIONS.keys())\n-        self.assertEquals(200, settingsdict[EXT_PATH])\n+        self.assertEqual(200, settingsdict[EXT_PATH])\n\n@@ -98,9 +98,9 @@ class FileTestCase(unittest.TestCase):\n \n     def test_download(self):\n         def _test(response):\n-            self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.status, 200)\n-            self.assertEquals(response.body, b'0123456789')\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.body, b'0123456789')\n \n         request = Request(path_to_file_uri(self.tmpname + '^'))\n         assert request.url.upper().endswith('%5E')\n@@ -241,28 +241,28 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL('file'))\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b\"0123456789\")\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n \n     def test_download_head(self):\n         request = Request(self.getURL('file'), method='HEAD')\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b'')\n+        d.addCallback(self.assertEqual, b'')\n         return d\n \n     def test_redirect_status(self):\n         request = Request(self.getURL('redirect'))\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.status)\n-        d.addCallback(self.assertEquals, 302)\n+        d.addCallback(self.assertEqual, 302)\n         return d\n \n     def test_redirect_status_head(self):\n         request = Request(self.getURL('redirect'), method='HEAD')\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.status)\n-        d.addCallback(self.assertEquals, 302)\n+        d.addCallback(self.assertEqual, 302)\n         return d\n \n     @defer.inlineCallbacks\n@@ -285,24 +285,24 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n-            self.assertEquals(\n+            self.assertEqual(\n                 response.body, to_bytes('%s:%d' % (self.host, self.portno)))\n-            self.assertEquals(request.headers, {})\n+            self.assertEqual(request.headers, {})\n \n         request = Request(self.getURL('host'))\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n     def test_host_header_seted_in_request_headers(self):\n         def _test(response):\n-            self.assertEquals(response.body, b'example.com')\n-            self.assertEquals(request.headers.get('Host'), b'example.com')\n+            self.assertEqual(response.body, b'example.com')\n+            self.assertEqual(request.headers.get('Host'), b'example.com')\n \n         request = Request(self.getURL('host'), headers={'Host': 'example.com'})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b'example.com')\n+        d.addCallback(self.assertEqual, b'example.com')\n         return d\n \n     def test_content_length_zero_bodyless_post_request_headers(self):\n@@ -317,7 +317,7 @@ class HttpTestCase(unittest.TestCase):\n         https://bugs.python.org/issue14721\n         \"\"\"\n         def _test(response):\n-            self.assertEquals(response.body, b'0')\n+            self.assertEqual(response.body, b'0')\n \n         request = Request(self.getURL('contentlength'), method='POST', headers={'Host': 'example.com'})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -327,8 +327,8 @@ class HttpTestCase(unittest.TestCase):\n             import json\n             headers = Headers(json.loads(response.text)['headers'])\n             contentlengths = headers.getlist('Content-Length')\n-            self.assertEquals(len(contentlengths), 1)\n-            self.assertEquals(contentlengths, [b\"0\"])\n+            self.assertEqual(len(contentlengths), 1)\n+            self.assertEqual(contentlengths, [b\"0\"])\n \n         request = Request(self.getURL('echo'), method='POST')\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -338,7 +338,7 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL('payload'), method='POST', body=body)\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, body)\n+        d.addCallback(self.assertEqual, body)\n         return d\n \n \n@@ -364,7 +364,7 @@ class Http11TestCase(HttpTestCase):\n         request = Request(self.getURL('file'))\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b\"0123456789\")\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n \n     def test_response_class_choosing_request(self):\n@@ -374,7 +374,7 @@ class Http11TestCase(HttpTestCase):\n         body = b'Some plain text\\ndata with tabs\\t and null bytes\\0'\n \n         def _test_type(response):\n-            self.assertEquals(type(response), TextResponse)\n+            self.assertEqual(type(response), TextResponse)\n \n         request = Request(self.getURL('nocontenttype'), body=body)\n         d = self.download_request(request, Spider('foo'))\n@@ -389,7 +389,7 @@ class Http11TestCase(HttpTestCase):\n         # response body. (regardless of headers)\n         d = self.download_request(request, Spider('foo', download_maxsize=10))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b\"0123456789\")\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n         yield d\n \n         d = self.download_request(request, Spider('foo', download_maxsize=9))\n@@ -431,14 +431,14 @@ class Http11TestCase(HttpTestCase):\n         request = Request(self.getURL('file'))\n         d = self.download_request(request, Spider('foo', download_maxsize=100))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b\"0123456789\")\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n \n     def test_download_chunked_content(self):\n         request = Request(self.getURL('chunked'))\n         d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, b\"chunked content\\n\")\n+        d.addCallback(self.assertEqual, b\"chunked content\\n\")\n         return d\n \n     def test_download_broken_content_cause_data_loss(self, url='broken'):\n@@ -597,9 +597,9 @@ class HttpProxyTestCase(unittest.TestCase):\n \n     def test_download_with_proxy(self):\n         def _test(response):\n-            self.assertEquals(response.status, 200)\n-            self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, b'http://example.com')\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'http://example.com')\n \n         http_proxy = self.getURL('')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n@@ -607,9 +607,9 @@ class HttpProxyTestCase(unittest.TestCase):\n \n     def test_download_with_proxy_https_noconnect(self):\n         def _test(response):\n-            self.assertEquals(response.status, 200)\n-            self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, b'https://example.com')\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'https://example.com')\n \n         http_proxy = '%s?noconnect' % self.getURL('')\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n@@ -617,9 +617,9 @@ class HttpProxyTestCase(unittest.TestCase):\n \n     def test_download_without_proxy(self):\n         def _test(response):\n-            self.assertEquals(response.status, 200)\n-            self.assertEquals(response.url, request.url)\n-            self.assertEquals(response.body, b'/path/to/resource')\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'/path/to/resource')\n \n         request = Request(self.getURL('path/to/resource'))\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -978,7 +978,7 @@ class DataURITestCase(unittest.TestCase):\n         uri = \"data:,A%20brief%20note\"\n \n         def _test(response):\n-            self.assertEquals(response.url, uri)\n+            self.assertEqual(response.url, uri)\n             self.assertFalse(response.headers)\n \n         request = Request(uri)\n@@ -986,39 +986,39 @@ class DataURITestCase(unittest.TestCase):\n \n     def test_default_mediatype_encoding(self):\n         def _test(response):\n-            self.assertEquals(response.text, 'A brief note')\n-            self.assertEquals(type(response),\n+            self.assertEqual(response.text, 'A brief note')\n+            self.assertEqual(type(response),\n                               responsetypes.from_mimetype(\"text/plain\"))\n-            self.assertEquals(response.encoding, \"US-ASCII\")\n+            self.assertEqual(response.encoding, \"US-ASCII\")\n \n         request = Request(\"data:,A%20brief%20note\")\n         return self.download_request(request, self.spider).addCallback(_test)\n \n     def test_default_mediatype(self):\n         def _test(response):\n-            self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n-            self.assertEquals(type(response),\n+            self.assertEqual(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEqual(type(response),\n                               responsetypes.from_mimetype(\"text/plain\"))\n-            self.assertEquals(response.encoding, \"iso-8859-7\")\n+            self.assertEqual(response.encoding, \"iso-8859-7\")\n \n         request = Request(\"data:;charset=iso-8859-7,%be%d3%be\")\n         return self.download_request(request, self.spider).addCallback(_test)\n \n     def test_text_charset(self):\n         def _test(response):\n-            self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n-            self.assertEquals(response.body, b'\\xbe\\xd3\\xbe')\n-            self.assertEquals(response.encoding, \"iso-8859-7\")\n+            self.assertEqual(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEqual(response.body, b'\\xbe\\xd3\\xbe')\n+            self.assertEqual(response.encoding, \"iso-8859-7\")\n \n         request = Request(\"data:text/plain;charset=iso-8859-7,%be%d3%be\")\n         return self.download_request(request, self.spider).addCallback(_test)\n \n     def test_mediatype_parameters(self):\n         def _test(response):\n-            self.assertEquals(response.text, u'\\u038e\\u03a3\\u038e')\n-            self.assertEquals(type(response),\n+            self.assertEqual(response.text, u'\\u038e\\u03a3\\u038e')\n+            self.assertEqual(type(response),\n                               responsetypes.from_mimetype(\"text/plain\"))\n-            self.assertEquals(response.encoding, \"utf-8\")\n+            self.assertEqual(response.encoding, \"utf-8\")\n \n         request = Request('data:text/plain;foo=%22foo;bar%5C%22%22;'\n                           'charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22'\n@@ -1027,7 +1027,7 @@ class DataURITestCase(unittest.TestCase):\n \n     def test_base64(self):\n         def _test(response):\n-            self.assertEquals(response.text, 'Hello, world.')\n+            self.assertEqual(response.text, 'Hello, world.')\n \n         request = Request('data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D')\n         return self.download_request(request, self.spider).addCallback(_test)\n\n@@ -36,7 +36,7 @@ class CookiesMiddlewareTest(TestCase):\n \n         req2 = Request('http://scrapytest.org/sub1/')\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertEquals(req2.headers.get('Cookie'), b\"C1=value1\")\n+        self.assertEqual(req2.headers.get('Cookie'), b\"C1=value1\")\n \n     def test_setting_false_cookies_enabled(self):\n         self.assertRaises(\n@@ -131,12 +131,12 @@ class CookiesMiddlewareTest(TestCase):\n         # check that cookies are merged back\n         req = Request('http://scrapytest.org/mergeme')\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')\n \n         # check that cookies are merged when dont_merge_cookies is passed as 0\n         req = Request('http://scrapytest.org/mergeme', meta={'dont_merge_cookies': 0})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')\n \n     def test_complex_cookies(self):\n         # merge some cookies into jar\n@@ -157,7 +157,7 @@ class CookiesMiddlewareTest(TestCase):\n         # embed C2 for scrapytest.org/bar\n         req = Request('http://scrapytest.org/bar')\n         self.mw.process_request(req, self.spider)\n-        self.assertEquals(req.headers.get('Cookie'), b'C2=value2')\n+        self.assertEqual(req.headers.get('Cookie'), b'C2=value2')\n \n         # embed nothing for scrapytest.org/baz\n         req = Request('http://scrapytest.org/baz')\n@@ -167,7 +167,7 @@ class CookiesMiddlewareTest(TestCase):\n     def test_merge_request_cookies(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')\n+        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')\n \n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers)\n@@ -181,7 +181,7 @@ class CookiesMiddlewareTest(TestCase):\n     def test_cookiejar_key(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'}, meta={'cookiejar': \"store1\"})\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')\n+        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')\n \n         headers = {'Set-Cookie': 'C1=value1; path=/'}\n         res = Response('http://scrapytest.org/', headers=headers, request=req)\n@@ -193,7 +193,7 @@ class CookiesMiddlewareTest(TestCase):\n \n         req3 = Request('http://scrapytest.org/', cookies={'galleta': 'dulce'}, meta={'cookiejar': \"store2\"})\n         assert self.mw.process_request(req3, self.spider) is None\n-        self.assertEquals(req3.headers.get('Cookie'), b'galleta=dulce')\n+        self.assertEqual(req3.headers.get('Cookie'), b'galleta=dulce')\n \n         headers = {'Set-Cookie': 'C2=value2; path=/'}\n         res2 = Response('http://scrapytest.org/', headers=headers, request=req3)\n@@ -213,16 +213,16 @@ class CookiesMiddlewareTest(TestCase):\n \n         req5_2 = Request('http://scrapytest.org:1104/some-redirected-path')\n         assert self.mw.process_request(req5_2, self.spider) is None\n-        self.assertEquals(req5_2.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req5_2.headers.get('Cookie'), b'C1=value1')\n \n         req5_3 = Request('http://scrapytest.org/some-redirected-path')\n         assert self.mw.process_request(req5_3, self.spider) is None\n-        self.assertEquals(req5_3.headers.get('Cookie'), b'C1=value1')\n+        self.assertEqual(req5_3.headers.get('Cookie'), b'C1=value1')\n \n         #skip cookie retrieval for not http request\n         req6 = Request('file:///scrapy/sometempfile')\n         assert self.mw.process_request(req6, self.spider) is None\n-        self.assertEquals(req6.headers.get('Cookie'), None)\n+        self.assertEqual(req6.headers.get('Cookie'), None)\n \n     def test_local_domain(self):\n         request = Request(\"http://example-host/\", cookies={'currencyCookie': 'USD'})\n\n@@ -22,15 +22,15 @@ class TestDefaultHeadersMiddleware(TestCase):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n         req = Request('http://www.scrapytest.org')\n         mw.process_request(req, spider)\n-        self.assertEquals(req.headers, defaults)\n+        self.assertEqual(req.headers, defaults)\n \n     def test_update_headers(self):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n         headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n         bytes_headers = {b'Accept-Language': [b'es'], b'Test-Header': [b'test']}\n         req = Request('http://www.scrapytest.org', headers=headers)\n-        self.assertEquals(req.headers, bytes_headers)\n+        self.assertEqual(req.headers, bytes_headers)\n \n         mw.process_request(req, spider)\n         defaults.update(bytes_headers)\n-        self.assertEquals(req.headers, defaults)\n+        self.assertEqual(req.headers, defaults)\n\n@@ -18,20 +18,20 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n         req, spider, mw = self.get_request_spider_mw()\n         mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta.get('download_timeout'), 180)\n+        self.assertEqual(req.meta.get('download_timeout'), 180)\n \n     def test_string_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw({'DOWNLOAD_TIMEOUT': '20.1'})\n         mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta.get('download_timeout'), 20.1)\n+        self.assertEqual(req.meta.get('download_timeout'), 20.1)\n \n     def test_spider_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n         spider.download_timeout = 2\n         mw.spider_opened(spider)\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta.get('download_timeout'), 2)\n+        self.assertEqual(req.meta.get('download_timeout'), 2)\n \n     def test_request_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n@@ -39,4 +39,4 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n         mw.spider_opened(spider)\n         req.meta['download_timeout'] = 1\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta.get('download_timeout'), 1)\n+        self.assertEqual(req.meta.get('download_timeout'), 1)\n\n@@ -23,10 +23,10 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n     def test_auth(self):\n         req = Request('http://scrapytest.org/')\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')\n+        self.assertEqual(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')\n \n     def test_auth_already_set(self):\n         req = Request('http://scrapytest.org/',\n                       headers=dict(Authorization='Digest 123'))\n         assert self.mw.process_request(req, self.spider) is None\n-        self.assertEquals(req.headers['Authorization'], b'Digest 123')\n+        self.assertEqual(req.headers['Authorization'], b'Digest 123')\n\n@@ -248,4 +248,4 @@ class HttpCompressionTest(TestCase):\n         response = response.replace(body = None)\n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIs(newresponse, response)\n-        self.assertEquals(response.body, b'')\n+        self.assertEqual(response.body, b'')\n\n@@ -35,8 +35,8 @@ class TestDefaultHeadersMiddleware(TestCase):\n         for url in ('http://e.com', 'https://e.com', 'file:///tmp/a'):\n             req = Request(url)\n             assert mw.process_request(req, spider) is None\n-            self.assertEquals(req.url, url)\n-            self.assertEquals(req.meta, {})\n+            self.assertEqual(req.url, url)\n+            self.assertEqual(req.meta, {})\n \n     def test_enviroment_proxies(self):\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n@@ -48,41 +48,41 @@ class TestDefaultHeadersMiddleware(TestCase):\n                 ('https://e.com', https_proxy), ('file://tmp/a', None)]:\n             req = Request(url)\n             assert mw.process_request(req, spider) is None\n-            self.assertEquals(req.url, url)\n-            self.assertEquals(req.meta.get('proxy'), proxy)\n+            self.assertEqual(req.url, url)\n+            self.assertEqual(req.meta.get('proxy'), proxy)\n \n     def test_proxy_precedence_meta(self):\n         os.environ['http_proxy'] = 'https://proxy.com'\n         mw = HttpProxyMiddleware()\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://new.proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://new.proxy:3128'})\n+        self.assertEqual(req.meta, {'proxy': 'https://new.proxy:3128'})\n \n     def test_proxy_auth(self):\n         os.environ['http_proxy'] = 'https://user:pass@proxy:3128'\n         mw = HttpProxyMiddleware()\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')\n         # proxy from request.meta\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://username:password@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')\n \n     def test_proxy_auth_empty_passwd(self):\n         os.environ['http_proxy'] = 'https://user:@proxy:3128'\n         mw = HttpProxyMiddleware()\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n         # proxy from request.meta\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://username:@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')\n \n     def test_proxy_auth_encoding(self):\n         # utf-8 encoding\n@@ -90,27 +90,27 @@ class TestDefaultHeadersMiddleware(TestCase):\n         mw = HttpProxyMiddleware(auth_encoding='utf-8')\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')\n \n         # proxy from request.meta\n         req = Request('http://scrapytest.org', meta={'proxy': u'https://\\u00FCser:pass@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')\n \n         # default latin-1 encoding\n         mw = HttpProxyMiddleware(auth_encoding='latin-1')\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')\n \n         # proxy from request.meta, latin-1 encoding\n         req = Request('http://scrapytest.org', meta={'proxy': u'https://\\u00FCser:pass@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')\n+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')\n \n     def test_proxy_already_seted(self):\n         os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n@@ -142,4 +142,4 @@ class TestDefaultHeadersMiddleware(TestCase):\n         os.environ['no_proxy'] = '*'\n         req = Request('http://noproxy.com', meta={'proxy': 'http://proxy.com'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.meta, {'proxy': 'http://proxy.com'})\n+        self.assertEqual(req.meta, {'proxy': 'http://proxy.com'})\n\n@@ -166,7 +166,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         resp = Response('http://scrapytest.org/first', headers={'Location': latin1_location}, status=302)\n         req_result = self.mw.process_response(req, resp, self.spider)\n         perc_encoded_utf8_url = 'http://scrapytest.org/a%E7%E3o'\n-        self.assertEquals(perc_encoded_utf8_url, req_result.url)\n+        self.assertEqual(perc_encoded_utf8_url, req_result.url)\n \n     def test_utf8_location(self):\n         req = Request('http://scrapytest.org/first')\n@@ -174,7 +174,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)\n         req_result = self.mw.process_response(req, resp, self.spider)\n         perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n-        self.assertEquals(perc_encoded_utf8_url, req_result.url)\n+        self.assertEqual(perc_encoded_utf8_url, req_result.url)\n \n \n class MetaRefreshMiddlewareTest(unittest.TestCase):\n\n@@ -17,7 +17,7 @@ class UserAgentMiddlewareTest(TestCase):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.headers['User-Agent'], b'default_useragent')\n+        self.assertEqual(req.headers['User-Agent'], b'default_useragent')\n \n     def test_remove_agent(self):\n         # settings UESR_AGENT to None should remove the user agent\n@@ -34,7 +34,7 @@ class UserAgentMiddlewareTest(TestCase):\n         mw.spider_opened(spider)\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.headers['User-Agent'], b'spider_useragent')\n+        self.assertEqual(req.headers['User-Agent'], b'spider_useragent')\n \n     def test_header_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n@@ -43,7 +43,7 @@ class UserAgentMiddlewareTest(TestCase):\n         req = Request('http://scrapytest.org/',\n                       headers={'User-Agent': 'header_useragent'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEquals(req.headers['User-Agent'], b'header_useragent')\n+        self.assertEqual(req.headers['User-Agent'], b'header_useragent')\n \n     def test_no_agent(self):\n         spider, mw = self.get_spider_and_mw(None)\n\n@@ -62,7 +62,7 @@ class WrappedResponseTest(TestCase):\n         self.wrapped = WrappedResponse(self.response)\n \n     def test_info(self):\n-        self.assert_(self.wrapped.info() is self.wrapped)\n+        self.assertTrue(self.wrapped.info() is self.wrapped)\n \n     def test_getheaders(self):\n         self.assertEqual(self.wrapped.getheaders('content-type'), ['text/html'])\n\n@@ -64,9 +64,9 @@ class RequestTest(unittest.TestCase):\n         h = Headers({'key1': u'val1', u'key2': 'val2'})\n         h[u'newkey'] = u'newval'\n         for k, v in h.iteritems():\n-            self.assert_(isinstance(k, bytes))\n+            self.assertTrue(isinstance(k, bytes))\n             for s in v:\n-                self.assert_(isinstance(s, bytes))\n+                self.assertTrue(isinstance(s, bytes))\n \n     def test_eq(self):\n         url = 'http://www.scrapy.org'\n\n@@ -437,7 +437,7 @@ class ProcessorsTest(unittest.TestCase):\n         self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])\n         self.assertEqual(proc(['', 'hello', 'world']), u' hello world')\n         self.assertEqual(proc(['hello', 'world']), u'hello world')\n-        self.assert_(isinstance(proc(['hello', 'world']), six.text_type))\n+        self.assertTrue(isinstance(proc(['hello', 'world']), six.text_type))\n \n     def test_compose(self):\n         proc = Compose(lambda v: v[0], str.upper)\n@@ -482,7 +482,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n     def test_constructor_with_selector(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n-        self.assert_(l.selector is sel)\n+        self.assertTrue(l.selector is sel)\n \n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n@@ -490,21 +490,21 @@ class SelectortemLoaderTest(unittest.TestCase):\n     def test_constructor_with_selector_css(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n-        self.assert_(l.selector is sel)\n+        self.assertTrue(l.selector is sel)\n \n         l.add_css('name', 'div::text')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n     def test_constructor_with_response(self):\n         l = TestItemLoader(response=self.response)\n-        self.assert_(l.selector)\n+        self.assertTrue(l.selector)\n \n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n     def test_constructor_with_response_css(self):\n         l = TestItemLoader(response=self.response)\n-        self.assert_(l.selector)\n+        self.assertTrue(l.selector)\n \n         l.add_css('name', 'div::text')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n@@ -526,7 +526,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_replace_xpath(self):\n         l = TestItemLoader(response=self.response)\n-        self.assert_(l.selector)\n+        self.assertTrue(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n         l.replace_xpath('name', '//p/text()')\n@@ -552,7 +552,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_replace_xpath_re(self):\n         l = TestItemLoader(response=self.response)\n-        self.assert_(l.selector)\n+        self.assertTrue(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n         l.replace_xpath('name', '//div/text()', re='ma')\n@@ -568,7 +568,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_replace_css(self):\n         l = TestItemLoader(response=self.response)\n-        self.assert_(l.selector)\n+        self.assertTrue(l.selector)\n         l.add_css('name', 'div::text')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n         l.replace_css('name', 'p::text')\n@@ -606,7 +606,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_replace_css_re(self):\n         l = TestItemLoader(response=self.response)\n-        self.assert_(l.selector)\n+        self.assertTrue(l.selector)\n         l.add_css('url', 'a::attr(href)')\n         self.assertEqual(l.get_output_value('url'), [u'http://www.scrapy.org'])\n         l.replace_css('url', 'a::attr(href)', re='http://www\\.(.+)')\n\n@@ -81,28 +81,28 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         COLOUR = (0, 127, 255)\n         im = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n         converted, _ = self.pipeline.convert_image(im)\n-        self.assertEquals(converted.mode, 'RGB')\n-        self.assertEquals(converted.getcolors(), [(10000, COLOUR)])\n+        self.assertEqual(converted.mode, 'RGB')\n+        self.assertEqual(converted.getcolors(), [(10000, COLOUR)])\n \n         # check that thumbnail keep image ratio\n         thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n-        self.assertEquals(thumbnail.mode, 'RGB')\n-        self.assertEquals(thumbnail.size, (10, 10))\n+        self.assertEqual(thumbnail.mode, 'RGB')\n+        self.assertEqual(thumbnail.size, (10, 10))\n \n         # transparency case: RGBA and PNG\n         COLOUR = (0, 127, 255, 50)\n         im = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n         converted, _ = self.pipeline.convert_image(im)\n-        self.assertEquals(converted.mode, 'RGB')\n-        self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n+        self.assertEqual(converted.mode, 'RGB')\n+        self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n         # transparency case with palette: P and PNG\n         COLOUR = (0, 127, 255, 50)\n         im = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n         im = im.convert('P')\n         converted, _ = self.pipeline.convert_image(im)\n-        self.assertEquals(converted.mode, 'RGB')\n-        self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n+        self.assertEqual(converted.mode, 'RGB')\n+        self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n \n class DeprecatedImagesPipeline(ImagesPipeline):\n\n@@ -84,7 +84,7 @@ class SelectorTestCase(unittest.TestCase):\n         headers = {'Content-Type': ['text/html; charset=utf-8']}\n         response = HtmlResponse(url=\"http://example.com\", headers=headers, body=html_utf8)\n         x = Selector(response)\n-        self.assertEquals(x.xpath(\"//span[@id='blank']/text()\").extract(),\n+        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").extract(),\n                           [u'\\xa3'])\n \n     def test_badly_encoded_body(self):\n\n@@ -207,7 +207,7 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEquals([r.url for r in output],\n+        self.assertEqual([r.url for r in output],\n                           ['http://example.org/somepage/item/12.html',\n                            'http://example.org/about.html',\n                            'http://example.org/nofollow.html'])\n@@ -234,7 +234,7 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 2)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEquals([r.url for r in output],\n+        self.assertEqual([r.url for r in output],\n                           ['http://example.org/somepage/item/12.html',\n                            'http://example.org/about.html'])\n \n@@ -258,7 +258,7 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEquals([r.url for r in output],\n+        self.assertEqual([r.url for r in output],\n                           ['http://example.org/somepage/item/12.html',\n                            'http://example.org/about.html',\n                            'http://example.org/nofollow.html'])\n\n@@ -25,18 +25,18 @@ class TestDepthMiddleware(TestCase):\n         result = [Request('http://scrapytest.org')]\n \n         out = list(self.mw.process_spider_output(resp, result, self.spider))\n-        self.assertEquals(out, result)\n+        self.assertEqual(out, result)\n \n         rdc = self.stats.get_value('request_depth_count/1', spider=self.spider)\n-        self.assertEquals(rdc, 1)\n+        self.assertEqual(rdc, 1)\n \n         req.meta['depth'] = 1\n \n         out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n-        self.assertEquals(out2, [])\n+        self.assertEqual(out2, [])\n \n         rdm = self.stats.get_value('request_depth_max', spider=self.spider)\n-        self.assertEquals(rdm, 1)\n+        self.assertEqual(rdm, 1)\n \n     def tearDown(self):\n         self.stats.close_spider(self.spider, '')\n\n@@ -67,16 +67,16 @@ class TestHttpErrorMiddleware(TestCase):\n         self.res200, self.res404 = _responses(self.req, [200, 404])\n \n     def test_process_spider_input(self):\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n                 self.mw.process_spider_input(self.res200, self.spider))\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, self.res404, self.spider)\n \n     def test_process_spider_exception(self):\n-        self.assertEquals([],\n+        self.assertEqual([],\n                 self.mw.process_spider_exception(self.res404,\n                         HttpError(self.res404), self.spider))\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n                 self.mw.process_spider_exception(self.res404,\n                         Exception(), self.spider))\n \n@@ -84,11 +84,11 @@ class TestHttpErrorMiddleware(TestCase):\n         res = self.res404.copy()\n         res.request = Request('http://scrapytest.org',\n                               meta={'handle_httpstatus_list': [404]})\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n             self.mw.process_spider_input(res, self.spider))\n \n         self.spider.handle_httpstatus_list = [404]\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n             self.mw.process_spider_input(self.res404, self.spider))\n \n \n@@ -102,11 +102,11 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n                 self.mw.process_spider_input(self.res200, self.spider))\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, self.res404, self.spider)\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n                 self.mw.process_spider_input(self.res402, self.spider))\n \n     def test_meta_overrides_settings(self):\n@@ -117,14 +117,14 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         res402 = self.res402.copy()\n         res402.request = request\n \n-        self.assertEquals(None,\n+        self.assertEqual(None,\n             self.mw.process_spider_input(res404, self.spider))\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, res402, self.spider)\n \n     def test_spider_override_settings(self):\n         self.spider.handle_httpstatus_list = [404]\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n             self.mw.process_spider_input(self.res404, self.spider))\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, self.res402, self.spider)\n@@ -139,9 +139,9 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n                 self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertEquals(None,\n+        self.assertEqual(None,\n                 self.mw.process_spider_input(self.res404, self.spider))\n \n     def test_meta_overrides_settings(self):\n@@ -152,7 +152,7 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         res402 = self.res402.copy()\n         res402.request = request\n \n-        self.assertEquals(None,\n+        self.assertEqual(None,\n             self.mw.process_spider_input(res404, self.spider))\n         self.assertRaises(HttpError,\n                 self.mw.process_spider_input, res402, self.spider)\n\n@@ -37,7 +37,7 @@ class TestOffsiteMiddleware(TestCase):\n         reqs = onsite_reqs + offsite_reqs\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEquals(out, onsite_reqs)\n+        self.assertEqual(out, onsite_reqs)\n \n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n@@ -49,7 +49,7 @@ class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n         res = Response('http://scrapytest.org')\n         reqs = [Request('http://a.com/b.html'), Request('http://b.com/1')]\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEquals(out, reqs)\n+        self.assertEqual(out, reqs)\n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n \n@@ -67,4 +67,4 @@ class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n       res = Response('http://scrapytest.org')\n       reqs = [Request('http://scrapytest.org/1')]\n       out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-      self.assertEquals(out, reqs)\n+      self.assertEqual(out, reqs)\n\n@@ -45,7 +45,7 @@ class TestRefererMiddleware(TestCase):\n             response = self.get_response(origin)\n             request = self.get_request(target)\n             out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n+            self.assertEqual(out[0].headers.get('Referer'), referrer)\n \n \n class MixinDefault(object):\n@@ -490,7 +490,7 @@ class TestSettingsPolicyByName(TestCase):\n             ]:\n             settings = Settings({'REFERRER_POLICY': s})\n             mw = RefererMiddleware(settings)\n-            self.assertEquals(mw.default_policy, p)\n+            self.assertEqual(mw.default_policy, p)\n \n     def test_valid_name_casevariants(self):\n         for s, p in [\n@@ -506,7 +506,7 @@ class TestSettingsPolicyByName(TestCase):\n             ]:\n             settings = Settings({'REFERRER_POLICY': s.upper()})\n             mw = RefererMiddleware(settings)\n-            self.assertEquals(mw.default_policy, p)\n+            self.assertEqual(mw.default_policy, p)\n \n     def test_invalid_name(self):\n         settings = Settings({'REFERRER_POLICY': 'some-custom-unknown-policy'})\n@@ -581,7 +581,7 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n             request = self.get_request(target)\n \n             out = list(self.referrermw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), init_referrer)\n+            self.assertEqual(out[0].headers.get('Referer'), init_referrer)\n \n             for status, url in redirections:\n                 response = Response(request.url, headers={'Location': url}, status=status)\n@@ -589,7 +589,7 @@ class TestReferrerOnRedirect(TestRefererMiddleware):\n                 self.referrermw.request_scheduled(request, self.spider)\n \n             assert isinstance(request, Request)\n-            self.assertEquals(request.headers.get('Referer'), final_referrer)\n+            self.assertEqual(request.headers.get('Referer'), final_referrer)\n \n \n class TestReferrerOnRedirectNoReferrer(TestReferrerOnRedirect):\n\n@@ -17,5 +17,5 @@ class TestUrlLengthMiddleware(TestCase):\n         mw = UrlLengthMiddleware(maxlength=25)\n         spider = Spider('foo')\n         out = list(mw.process_spider_output(res, reqs, spider))\n-        self.assertEquals(out, [short_url_req])\n+        self.assertEqual(out, [short_url_req])\n \n\n@@ -6,7 +6,7 @@ class UrlparseTestCase(unittest.TestCase):\n \n     def test_s3_url(self):\n         p = urlparse('s3://bucket/key/name?param=value')\n-        self.assertEquals(p.scheme, 's3')\n-        self.assertEquals(p.hostname, 'bucket')\n-        self.assertEquals(p.path, '/key/name')\n-        self.assertEquals(p.query, 'param=value')\n+        self.assertEqual(p.scheme, 's3')\n+        self.assertEqual(p.hostname, 'bucket')\n+        self.assertEqual(p.path, '/key/name')\n+        self.assertEqual(p.query, 'param=value')\n\n@@ -202,22 +202,22 @@ class SequenceExcludeTest(unittest.TestCase):\n         seq = range(10, 20, 3)\n         d = SequenceExclude(seq)\n         are_not_in = [v for v in range(10, 20, 3) if v in d]\n-        self.assertEquals([], are_not_in)\n+        self.assertEqual([], are_not_in)\n \n         are_not_in = [v for v in range(10, 20) if v in d]\n-        self.assertEquals([11, 12, 14, 15, 17, 18], are_not_in)\n+        self.assertEqual([11, 12, 14, 15, 17, 18], are_not_in)\n \n     def test_string_seq(self):\n         seq = \"cde\"\n         d = SequenceExclude(seq)\n         chars = \"\".join(v for v in \"abcdefg\" if v in d)\n-        self.assertEquals(\"abfg\", chars)\n+        self.assertEqual(\"abfg\", chars)\n \n     def test_stringset_seq(self):\n         seq = set(\"cde\")\n         d = SequenceExclude(seq)\n         chars = \"\".join(v for v in \"abcdefg\" if v in d)\n-        self.assertEquals(\"abfg\", chars)\n+        self.assertEqual(\"abfg\", chars)\n \n     def test_set(self):\n         \"\"\"Anything that is not in the supplied sequence will evaluate as 'in' the container.\"\"\"\n\n@@ -89,7 +89,7 @@ class IterErrbackTest(unittest.TestCase):\n         errors = []\n         out = list(iter_errback(itergood(), errors.append))\n         self.assertEqual(out, list(range(10)))\n-        self.failIf(errors)\n+        self.assertFalse(errors)\n \n     def test_iter_errback_bad(self):\n         def iterbad():\n\n@@ -252,8 +252,8 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         # explicit type check cuz' we no like stinkin' autocasting! yarrr\n         for result_row in result:\n-            self.assert_(all((isinstance(k, six.text_type) for k in result_row.keys())))\n-            self.assert_(all((isinstance(v, six.text_type) for v in result_row.values())))\n+            self.assertTrue(all((isinstance(k, six.text_type) for k in result_row.keys())))\n+            self.assertTrue(all((isinstance(v, six.text_type) for v in result_row.values())))\n \n     def test_csviter_delimiter(self):\n         body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\\t')\n\n@@ -23,20 +23,20 @@ class UtilsMiscTestCase(unittest.TestCase):\n             'tests.test_utils_misc.test_walk_modules.mod.mod0',\n             'tests.test_utils_misc.test_walk_modules.mod1',\n         ]\n-        self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+        self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n \n         mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')\n         expected = [\n             'tests.test_utils_misc.test_walk_modules.mod',\n             'tests.test_utils_misc.test_walk_modules.mod.mod0',\n         ]\n-        self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+        self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n \n         mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')\n         expected = [\n             'tests.test_utils_misc.test_walk_modules.mod1',\n         ]\n-        self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+        self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n \n         self.assertRaises(ImportError, walk_modules, 'nomodule999')\n \n@@ -51,7 +51,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n                 'testegg.spiders.b',\n                 'testegg'\n             ]\n-            self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n+            self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n         finally:\n             sys.path.remove(egg)\n \n\n@@ -25,14 +25,14 @@ def inside_a_project():\n \n class ProjectUtilsTest(unittest.TestCase):\n     def test_data_path_outside_project(self):\n-        self.assertEquals('.scrapy/somepath', data_path('somepath'))\n-        self.assertEquals('/absolute/path', data_path('/absolute/path'))\n+        self.assertEqual('.scrapy/somepath', data_path('somepath'))\n+        self.assertEqual('/absolute/path', data_path('/absolute/path'))\n \n     def test_data_path_inside_project(self):\n         with inside_a_project() as proj_path:\n             expected = os.path.join(proj_path, '.scrapy', 'somepath')\n-            self.assertEquals(\n+            self.assertEqual(\n                 os.path.realpath(expected),\n                 os.path.realpath(data_path('somepath'))\n             )\n-            self.assertEquals('/absolute/path', data_path('/absolute/path'))\n+            self.assertEqual('/absolute/path', data_path('/absolute/path'))\n\n@@ -97,9 +97,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         a = Obj()\n         b = Obj()\n         # no attributes given return False\n-        self.failIf(equal_attributes(a, b, []))\n+        self.assertFalse(equal_attributes(a, b, []))\n         # not existent attributes\n-        self.failIf(equal_attributes(a, b, ['x', 'y']))\n+        self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n \n         a.x = 1\n         b.x = 1\n@@ -108,7 +108,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n         b.y = 2\n         # obj1 has no attribute y\n-        self.failIf(equal_attributes(a, b, ['x', 'y']))\n+        self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n \n         a.y = 2\n         # equal attributes\n@@ -116,7 +116,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n         a.y = 1\n         # differente attributes\n-        self.failIf(equal_attributes(a, b, ['x', 'y']))\n+        self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n \n         # test callable\n         a.meta = {}\n@@ -134,7 +134,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertTrue(equal_attributes(a, b, [compare_z, 'x']))\n         # fail z equality\n         a.meta['z'] = 2\n-        self.failIf(equal_attributes(a, b, [compare_z, 'x']))\n+        self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))\n \n     def test_weakkeycache(self):\n         class _Weakme(object): pass\n@@ -156,9 +156,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d = {'a': 123, u'b': b'c', u'd': u'e', object(): u'e'}\n         d2 = stringify_dict(d, keys_only=False)\n         self.assertEqual(d, d2)\n-        self.failIf(d is d2)  # shouldn't modify in place\n-        self.failIf(any(isinstance(x, six.text_type) for x in d2.keys()))\n-        self.failIf(any(isinstance(x, six.text_type) for x in d2.values()))\n+        self.assertFalse(d is d2)  # shouldn't modify in place\n+        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n+        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n \n     @unittest.skipUnless(six.PY2, \"deprecated function\")\n     def test_stringify_dict_tuples(self):\n@@ -166,17 +166,17 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d = dict(tuples)\n         d2 = stringify_dict(tuples, keys_only=False)\n         self.assertEqual(d, d2)\n-        self.failIf(d is d2)  # shouldn't modify in place\n-        self.failIf(any(isinstance(x, six.text_type) for x in d2.keys()), d2.keys())\n-        self.failIf(any(isinstance(x, six.text_type) for x in d2.values()))\n+        self.assertFalse(d is d2)  # shouldn't modify in place\n+        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()), d2.keys())\n+        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n \n     @unittest.skipUnless(six.PY2, \"deprecated function\")\n     def test_stringify_dict_keys_only(self):\n         d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}\n         d2 = stringify_dict(d)\n         self.assertEqual(d, d2)\n-        self.failIf(d is d2)  # shouldn't modify in place\n-        self.failIf(any(isinstance(x, six.text_type) for x in d2.keys()))\n+        self.assertFalse(d is d2)  # shouldn't modify in place\n+        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n \n     def test_get_func_args(self):\n         def f1(a, b, c):\n\n@@ -29,7 +29,7 @@ class SendCatchLogTest(unittest.TestCase):\n         self.assertIn('error_handler', record.getMessage())\n         self.assertEqual(record.levelname, 'ERROR')\n         self.assertEqual(result[0][0], self.error_handler)\n-        self.assert_(isinstance(result[0][1], Failure))\n+        self.assertTrue(isinstance(result[0][1], Failure))\n         self.assertEqual(result[1], (self.ok_handler, \"OK\"))\n \n         dispatcher.disconnect(self.error_handler, signal=test_signal)\n\n@@ -71,7 +71,7 @@ class ParseUrlTestCase(unittest.TestCase):\n         for url, test in tests:\n             test = tuple(\n                 to_bytes(x) if not isinstance(x, int) else x for x in test)\n-            self.assertEquals(client._parse(url), test, url)\n+            self.assertEqual(client._parse(url), test, url)\n \n     def test_externalUnicodeInterference(self):\n         \"\"\"\n@@ -258,16 +258,16 @@ class WebClientTestCase(unittest.TestCase):\n     def testPayload(self):\n         s = \"0123456789\" * 10\n         return getPage(self.getURL(\"payload\"), body=s).addCallback(\n-            self.assertEquals, to_bytes(s))\n+            self.assertEqual, to_bytes(s))\n \n     def testHostHeader(self):\n         # if we pass Host header explicitly, it should be used, otherwise\n         # it should extract from url\n         return defer.gatherResults([\n             getPage(self.getURL(\"host\")).addCallback(\n-                self.assertEquals, to_bytes(\"127.0.0.1:%d\" % self.portno)),\n+                self.assertEqual, to_bytes(\"127.0.0.1:%d\" % self.portno)),\n             getPage(self.getURL(\"host\"), headers={\"Host\": \"www.example.com\"}).addCallback(\n-                self.assertEquals, to_bytes(\"www.example.com\"))])\n+                self.assertEqual, to_bytes(\"www.example.com\"))])\n \n     def test_getPage(self):\n         \"\"\"\n@@ -275,7 +275,7 @@ class WebClientTestCase(unittest.TestCase):\n         the body of the response if the default method B{GET} is used.\n         \"\"\"\n         d = getPage(self.getURL(\"file\"))\n-        d.addCallback(self.assertEquals, b\"0123456789\")\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n         return d\n \n     def test_getPageHead(self):\n@@ -298,7 +298,7 @@ class WebClientTestCase(unittest.TestCase):\n         \"\"\"\n         d = getPage(self.getURL(\"host\"), timeout=100)\n         d.addCallback(\n-            self.assertEquals, to_bytes(\"127.0.0.1:%d\" % self.portno))\n+            self.assertEqual, to_bytes(\"127.0.0.1:%d\" % self.portno))\n         return d\n \n     def test_timeoutTriggering(self):\n@@ -326,7 +326,7 @@ class WebClientTestCase(unittest.TestCase):\n         return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)\n \n     def _cbNoSuchFile(self, pageData):\n-        self.assert_(b'404 - No Such Resource' in pageData)\n+        self.assertTrue(b'404 - No Such Resource' in pageData)\n \n     def testFactoryInfo(self):\n         url = self.getURL('file')\n@@ -336,16 +336,16 @@ class WebClientTestCase(unittest.TestCase):\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n     def _cbFactoryInfo(self, ignoredResult, factory):\n-        self.assertEquals(factory.status, b'200')\n-        self.assert_(factory.version.startswith(b'HTTP/'))\n-        self.assertEquals(factory.message, b'OK')\n-        self.assertEquals(factory.response_headers[b'content-length'], b'10')\n+        self.assertEqual(factory.status, b'200')\n+        self.assertTrue(factory.version.startswith(b'HTTP/'))\n+        self.assertEqual(factory.message, b'OK')\n+        self.assertEqual(factory.response_headers[b'content-length'], b'10')\n \n     def testRedirect(self):\n         return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n \n     def _cbRedirect(self, pageData):\n-        self.assertEquals(pageData,\n+        self.assertEqual(pageData,\n                 b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n                 b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n                 b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n@@ -360,6 +360,6 @@ class WebClientTestCase(unittest.TestCase):\n \n     def _check_Encoding(self, response, original_body):\n         content_encoding = to_unicode(response.headers[b'Content-Encoding'])\n-        self.assertEquals(content_encoding, EncodingResource.out_encoding)\n-        self.assertEquals(\n+        self.assertEqual(content_encoding, EncodingResource.out_encoding)\n+        self.assertEqual(\n             response.body.decode(content_encoding), to_unicode(original_body))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
