{"custom_id": "scrapy#bd38a312d4e8885f933de872a070e5828988a02a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 12 | Churn Cumulative: 441 | Contributors (this commit): 2 | Commits (past 90d): 25 | Contributors (cumulative): 4 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -8,6 +8,7 @@ from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n from scrapy.xpath.selector import XmlXPathSelector\n from scrapy.core.exceptions import UsageError\n+from scrapy.utils.iterators import xmliter, csviter\n from scrapy.utils.misc import hash_values\n \n class BasicSpider(BaseSpider):\n@@ -17,7 +18,7 @@ class BasicSpider(BaseSpider):\n     gen_guid_attribs = []\n \n     def set_guid(self, item):\n-        item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n+        item.guid = hash_values(self.domain_name, *[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n    \n class CrawlSpider(BasicSpider):\n     \"\"\"\n@@ -93,6 +94,12 @@ class XMLFeedSpider(BasicSpider):\n     iternodes = True\n     itertag = 'product'\n \n+    def parse_item_wrapper(self, response, xSel):\n+        ret = self.parse_item(response, xSel)\n+        if isinstance(ret, ScrapedItem):\n+            self.set_guid(ret)\n+        return ret\n+\n     def parse(self, response):\n         if not hasattr(self, 'parse_item'):\n             raise NotConfigured('You must define parse_item method in order to scrape this feed')\n@@ -102,5 +109,5 @@ class XMLFeedSpider(BasicSpider):\n         else:\n             nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n \n-        return (self.parse_item(response, xSel) for xSel in nodes)\n+        return (self.parse_item_wrapper(response, xSel) for xSel in nodes)\n \n\n@@ -24,6 +24,7 @@ class ScrapedItem(object):\n \n     def attribute(self, attrname, value, override=False, add=False, debug=False):\n         val = self._adaptors_dict.execute(attrname, value, debug)\n+        if val:\n             curr_val = getattr(self, attrname, None)\n             if not curr_val:\n                 setattr(self, attrname, val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bad79836f934ee16eef3e3a7a828b317da64fab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 270 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class ScrapedItem(object):\n \n     def attribute(self, attrname, value, override=False, add=False, debug=False):\n         val = self._adaptors_dict.execute(attrname, value, debug)\n-        if val:\n+        if not val is None:\n             curr_val = getattr(self, attrname, None)\n             if not curr_val:\n                 setattr(self, attrname, val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#224d3c51858ba01e9fbf640e1b2f73d0e6881b1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 231 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class Command(ScrapyCommand):\n             response = responses[0]\n             spider = spiders.fromurl(response.url)\n             if opts.identify:\n-                result = spider.identify(response)\n+                result = spider.identify_products(response)\n             elif hasattr(spider, 'parse_url'):\n                 result = spider.parse_url(response)\n             else:\n\n@@ -92,7 +92,7 @@ class XMLFeedSpider(BasicSpider):\n     or not using it (which just splits the tags using xpath)\n     \"\"\"\n     iternodes = True\n-    itertag = 'product'\n+    itertag = 'item'\n \n     def parse_item_wrapper(self, response, xSel):\n         ret = self.parse_item(response, xSel)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2461fbeeada6b6d3ec50d7881d29e0ca2cd4764", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 272 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class ScrapedItem(object):\n \n     def attribute(self, attrname, value, override=False, add=False, debug=False):\n         val = self._adaptors_dict.execute(attrname, value, debug)\n-        if not val is None:\n+        if val or val is False:\n             curr_val = getattr(self, attrname, None)\n             if not curr_val:\n                 setattr(self, attrname, val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e54ac52ffef6ff00b4d881d16f164ac802b62d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 20 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ scrapy.http.Response objects\n \"\"\"\n \n from scrapy.xpath import XPathSelector\n+from scrapy.http.response import ResponseBody\n \n def new_response_from_xpaths(response, xpaths):\n     \"\"\"Return a new response constructed by applying the given xpaths to the\n@@ -11,5 +12,5 @@ def new_response_from_xpaths(response, xpaths):\n     \"\"\"\n     xs = XPathSelector(response)\n     parts = [''.join([n for n in xs.x(x).extract()]) for x in xpaths]\n-    new_body = ''.join(parts)\n-    return response.replace(body=new_body)\n+    new_body_content = ''.join(parts)\n+    return response.replace(body=ResponseBody(content=new_body_content, declared_encoding=response.body.get_declared_encoding()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e88890bc3c45da1edd95acf8117d89fca2ce643", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 13 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,13 @@\n+from unittest import TestCase, main\n+from scrapy.spider import spiders\n+\n+class ScrapySpidersTest(TestCase):\n+    def test_load_spiders(self):\n+        \"\"\" Simple test that forces to load the spiders\n+        checks for syntax errors that block the whole framework\n+        \"\"\"\n+        spiders.asdict()\n+\n+if __name__ == \"__main__\":\n+    main()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22d713bba9bcc7145b7154e315a4b8e558ab26cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +0,0 @@\n-from unittest import TestCase, main\n-from scrapy.spider import spiders\n-\n-class ScrapySpidersTest(TestCase):\n-    def test_load_spiders(self):\n-        \"\"\" Simple test that forces to load the spiders\n-        checks for syntax errors that block the whole framework\n-        \"\"\"\n-        spiders.asdict()\n-\n-if __name__ == \"__main__\":\n-    main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6db6b8c52ec034ba432e24b80f48c2b1d6768fa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 3 | Churn Cumulative: 178 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -81,6 +81,9 @@ class CrawlSpider(BasicSpider):\n             if hasattr(self, callback_name):\n                 if extractor.match(response.url):\n                     ret.extend(getattr(self, callback_name)(response))\n+        for entry in ret:\n+            if isinstance(entry, ScrapedItem):\n+                self.set_guid(entry)\n         return ret\n \n class XMLFeedSpider(BasicSpider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5d134053bd5d4b6b1a46c82de0d69e403583fa3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 42 | Churn Cumulative: 220 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,15 +30,14 @@ class CrawlSpider(BasicSpider):\n         self._links_callback = []\n         for attr in dir(self):\n             if attr.startswith('links_'):\n-                suffix = attr.split('_', 1)[1]\n-                value = getattr(self, attr)\n-                callback = getattr(self, 'parse_%s' % suffix, None)\n-                self._links_callback.append((value, callback))\n+                suffix = attr[6:]\n+                extractor = getattr(self, attr)\n+                self._links_callback.append((suffix, extractor))\n \n     def parse(self, response):\n         \"\"\"This function is called by the core for all the start_urls. Do not\n         override this function, override parse_start_url instead.\"\"\"\n-        return self._parse_wrapper(response, self.parse_start_url)\n+        return self._parse_wrapper(response, callback=self.parse_start_url)\n \n     def parse_start_url(self, response):\n         \"\"\"Callback function for processing start_urls. It must return a list\n@@ -48,21 +47,28 @@ class CrawlSpider(BasicSpider):\n     def _links_to_follow(self, response):\n         res = []\n         links_to_follow = {}\n-        for lx, callback in self._links_callback:\n+        for suffix, lx in self._links_callback:\n             links = lx.extract_urls(response)\n             links = self.post_extract_links(links) if hasattr(self, 'post_extract_links') else links\n             for link in links:\n-                links_to_follow[link.url] = (callback, link.text)\n-\n-        for url, (callback, link_text) in links_to_follow.iteritems():\n-            request = Request(url=url, link_text=link_text)\n-            request.append_callback(self._parse_wrapper, callback)\n+                request = Request(url=link.url, link_text=link.text)\n+                request.append_callback(self._parse_wrapper, suffix=suffix)\n                 res.append(request)\n         return res\n \n-    def _parse_wrapper(self, response, callback):\n+    def _parse_wrapper(self, response, **kwargs):\n+        suffix = kwargs.get('suffix')\n+        callback = kwargs.get('callback')\n         res = self._links_to_follow(response)\n-        res += callback(response) if callback else ()\n+\n+        if suffix:\n+            check_fcn = getattr(self, 'check_%s' % suffix, None)\n+            if not check_fcn or check_fcn(response):\n+                parse_fcn = getattr(self, 'parse_%s' % suffix, None)\n+                res += parse_fcn(response) if parse_fcn else ()\n+        elif callback:\n+            res += callback(response)\n+\n         for entry in res:\n             if isinstance(entry, ScrapedItem):\n                self.set_guid(entry)\n@@ -77,10 +83,12 @@ class CrawlSpider(BasicSpider):\n         ret = []\n         for name in extractor_names:\n             extractor = getattr(self, name)\n-            callback_name = 'parse_%s' % name[6:]\n-            if hasattr(self, callback_name):\n-                if extractor.match(response.url):\n-                    ret.extend(getattr(self, callback_name)(response))\n+            check_name = 'check_%s' % name[6:]\n+            parse_name = 'parse_%s' % name[6:]\n+            if extractor.match(response.url) and hasattr(self, parse_name):\n+                if hasattr(self, check_name) and not getattr(self, check_name)(response):\n+                    continue\n+                ret.extend(getattr(self, parse_name)(response))\n         for entry in ret:\n             if isinstance(entry, ScrapedItem):\n                 self.set_guid(entry)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#323241b6d6a017b815bf2dd4ad3d5f2ef5514ee0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 42 | Churn Cumulative: 262 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,14 +30,15 @@ class CrawlSpider(BasicSpider):\n         self._links_callback = []\n         for attr in dir(self):\n             if attr.startswith('links_'):\n-                suffix = attr[6:]\n-                extractor = getattr(self, attr)\n-                self._links_callback.append((suffix, extractor))\n+                suffix = attr.split('_', 1)[1]\n+                value = getattr(self, attr)\n+                callback = getattr(self, 'parse_%s' % suffix, None)\n+                self._links_callback.append((value, callback))\n \n     def parse(self, response):\n         \"\"\"This function is called by the core for all the start_urls. Do not\n         override this function, override parse_start_url instead.\"\"\"\n-        return self._parse_wrapper(response, callback=self.parse_start_url)\n+        return self._parse_wrapper(response, self.parse_start_url)\n \n     def parse_start_url(self, response):\n         \"\"\"Callback function for processing start_urls. It must return a list\n@@ -47,28 +48,21 @@ class CrawlSpider(BasicSpider):\n     def _links_to_follow(self, response):\n         res = []\n         links_to_follow = {}\n-        for suffix, lx in self._links_callback:\n+        for lx, callback in self._links_callback:\n             links = lx.extract_urls(response)\n             links = self.post_extract_links(links) if hasattr(self, 'post_extract_links') else links\n             for link in links:\n-                request = Request(url=link.url, link_text=link.text)\n-                request.append_callback(self._parse_wrapper, suffix=suffix)\n+                links_to_follow[link.url] = (callback, link.text)\n+\n+        for url, (callback, link_text) in links_to_follow.iteritems():\n+            request = Request(url=url, link_text=link_text)\n+            request.append_callback(self._parse_wrapper, callback)\n             res.append(request)\n         return res\n \n-    def _parse_wrapper(self, response, **kwargs):\n-        suffix = kwargs.get('suffix')\n-        callback = kwargs.get('callback')\n+    def _parse_wrapper(self, response, callback):\n         res = self._links_to_follow(response)\n-\n-        if suffix:\n-            check_fcn = getattr(self, 'check_%s' % suffix, None)\n-            if not check_fcn or check_fcn(response):\n-                parse_fcn = getattr(self, 'parse_%s' % suffix, None)\n-                res += parse_fcn(response) if parse_fcn else ()\n-        elif callback:\n-            res += callback(response)\n-\n+        res += callback(response) if callback else ()\n         for entry in res:\n             if isinstance(entry, ScrapedItem):\n                self.set_guid(entry)\n@@ -83,12 +77,10 @@ class CrawlSpider(BasicSpider):\n         ret = []\n         for name in extractor_names:\n             extractor = getattr(self, name)\n-            check_name = 'check_%s' % name[6:]\n-            parse_name = 'parse_%s' % name[6:]\n-            if extractor.match(response.url) and hasattr(self, parse_name):\n-                if hasattr(self, check_name) and not getattr(self, check_name)(response):\n-                    continue\n-                ret.extend(getattr(self, parse_name)(response))\n+            callback_name = 'parse_%s' % name[6:]\n+            if hasattr(self, callback_name):\n+                if extractor.match(response.url):\n+                    ret.extend(getattr(self, callback_name)(response))\n         for entry in ret:\n             if isinstance(entry, ScrapedItem):\n                 self.set_guid(entry)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36080ada86005cf89a3fa445fed086a7db44bc72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 37 | Churn Cumulative: 106 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,32 +18,57 @@ def to_unicode(value):\n         raise TypeError('to_unicode must receive an iterable.')\n \n def clean_spaces(value):\n+    \"\"\"\n+    Converts multispaces into single spaces.\n+    E.g. \"Hello   sir\" would turn into \"Hello sir\".\n+\n+    Input: list of unicodes\n+    Output: list of unicodes\n+    \"\"\"\n     _clean_spaces_re = re.compile(\"\\s+\", re.U)\n     return [ _clean_spaces_re.sub(' ', v) for v in value ]\n \n def strip_list(value):\n+    \"\"\"\n+    Removes any spaces at both the start and the ending\n+    of each string in the provided list.\n+\n+    Input: list of unicodes\n+    Output: list of unicodes\n+    \"\"\"\n     return [ v.strip() for v in value ]\n \n def drop_empty(value):\n+    \"\"\"\n+    Removes any index that evaluates to None\n+    from the provided iterable.\n+\n+    Input: iterable\n+    Output: list\n+    \"\"\"\n     return [ v for v in value if v ]\n \n def canonicalize_urls(value):\n+    \"\"\"\n+    Tries to canonicalize each url in the list you provide.\n+    To see what this implies, check out canonicalize_url's\n+    docstring, at scrapy.utils.url.py\n+\n+    Input: list of unicodes(urls)\n+    Output: list of unicodes(urls)\n+    \"\"\"\n     if hasattr(value, '__iter__'):\n         return [canonicalize_url(url) for url in value]\n     elif isinstance(value, basestring):\n         return canonicalize_url(value)\n     return ''\n \n-class Delist(object):\n+def delist(value, join_delimiter=' '):\n     \"\"\"\n     Input: iterable with strings\n     Output: unicode\n     \"\"\"\n-    def __init__(self, delimiter=' '):\n-        self.delimiter = delimiter\n-    \n-    def __call__(self, value):\n-        return self.delimiter.join(value)\n+    return join_delimiter.join(value)\n \n class Regex(object):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90d28badf696bbe161806337d515cb1d72457b55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 36 | Churn Cumulative: 81 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,26 @@\n+import inspect\n+\n from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages\n-from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex\n+from scrapy.contrib.adaptors.markup import remove_tags, remove_root, unquote\n+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, delist, Regex\n from scrapy.utils.python import unique, flatten\n+\n+def adaptor_gen(*args):\n+    subadaptors = []\n+    arg_mappings = []\n+    for subadaptor in args:\n+        if callable(subadaptor):\n+            if inspect.isfunction(subadaptor):\n+                func_args, varargs, varkw, defaults = inspect.getargspec(subadaptor)\n+                arg_mappings.append(func_args[1:])\n+            else:\n+                arg_mappings.append([])\n+            subadaptors.append(subadaptor)\n+    \n+    def _adaptor(value, **kwargs):\n+        for index, subadaptor in enumerate(subadaptors):\n+            adaptor_args = dict((key, val) for key, val in kwargs.items() if key in arg_mappings[index])\n+            value = subadaptor(value, **adaptor_args)\n+        return value\n+\n+    return _adaptor\n\n@@ -21,7 +21,7 @@ def remove_root(value):\n         return value\n     return [ _remove_root(v) for v in value ]\n \n-class Unquote(object):\n+def unquote(value, keep_entities=None):\n     \"\"\"\n     Receives a list of strings, removes all of the\n     entities the strings may have, and returns\n@@ -30,10 +30,8 @@ class Unquote(object):\n     Input: iterable with strings\n     Output: list of strings\n     \"\"\"\n-    def __init__(self, keep=['lt', 'amp']):\n-        self.keep = keep\n-\n-    def __call__(self, value):\n-        return [ remove_entities(v, keep=self.keep) for v in value ]\n+    if keep_entities is None:\n+        keep_entities = ['lt', 'amp']\n+    return [ remove_entities(v, keep=keep_entities) for v in value ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ef7c46303f33c8bf5f45a9b81a2637be6ef9cf7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 52 | Files Changed: 4 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 13/22 | Churn Δ: 138 | Churn Cumulative: 679 | Contributors (this commit): 2 | Commits (past 90d): 32 | Contributors (cumulative): 8 | DMM Complexity: 0.32142857142857145\n\nDIFF:\n@@ -1 +1 @@\n-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline\n+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n\n@@ -8,7 +8,7 @@ import hashlib\n from pydispatch import dispatcher\n from pprint import PrettyPrinter\n \n-from scrapy.item import ScrapedItem, ItemDelta\n+from scrapy.item import ScrapedItem, ItemDelta, ItemAttribute\n from scrapy.spider import spiders\n from scrapy.core import signals\n from scrapy.core.exceptions import UsageError, DropItem\n@@ -30,11 +30,6 @@ class ValidationPipeline(object):\n         item.validate()\n         return item\n \n-class SetGUIDPipeline(object):\n-    def process_item(self, domain, response, item):\n-        spiders.fromdomain(domain).set_guid(item)\n-        return item\n-\n class RobustScrapedItem(ScrapedItem):\n     \"\"\"\n     A more robust scraped item class with a built-in validation mechanism and \n@@ -42,8 +37,8 @@ class RobustScrapedItem(ScrapedItem):\n     \"\"\"\n \n     ATTRIBUTES = {\n-        'guid': basestring, # a global unique identifier\n-        'url': basestring,  # the main URL where this item was scraped from\n+        'guid': ItemAttribute(attrib_name='guid', attrib_type=basestring), # a global unique identifier\n+        'url': ItemAttribute(attrib_name='url', attrib_type=basestring),  # the main URL where this item was scraped from\n     }\n     \n     def __init__(self, data=None):\n@@ -64,7 +59,7 @@ class RobustScrapedItem(ScrapedItem):\n         # Note that this method is called only when the attribute is not found in \n         # self.__dict__ or the class/instance methods.\n         if attr in self.ATTRIBUTES:  \n-            return None\n+            return () if hasattr(self.ATTRIBUTES[attr].attrib_type, '__iter__') else None\n         else:\n             raise AttributeError(attr)\n \n@@ -72,30 +67,8 @@ class RobustScrapedItem(ScrapedItem):\n         \"\"\"\n         Set an attribute checking it matches the attribute type declared in self.ATTRIBUTES\n         \"\"\"\n-        if not attr.startswith('_') and attr not in self.ATTRIBUTES:\n-            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attr, self.__class__.__name__))\n-\n-        if value is None:\n-            self.__dict__.pop(attr, None)\n-            return\n-\n-        if attr == '_adaptors_dict':\n-            return object.__setattr__(self, '_adaptors_dict', value)\n-\n-        type1 = self.ATTRIBUTES[attr]\n-        if hasattr(type1, '__iter__'):\n-            if not hasattr(value, '__iter__'):\n-                raise TypeError('Attribute \"%s\" must be a sequence' % attr)\n-            type2 = type1[0]\n-            for i in value:\n-                if not isinstance(i, type2):\n-                    raise TypeError('Attribute \"%s\" cannot contain %s, only %s' % (attr, i.__class__.__name__, type2.__name__))\n-        else:\n-            if not isinstance(value, type1):\n-                raise TypeError('Attribute \"%s\" must be %s, not %s' % (attr, type1.__name__, value.__class__.__name__))\n-\n-        self.__dict__[attr] = value\n         self.__dict__['_version'] = None\n+        super(RobustScrapedItem, self).__setattr__(attr, value)\n     \n     def __delattr__(self, attr):\n         \"\"\"\n\n@@ -1 +1 @@\n-from scrapy.item.models import ScrapedItem, ItemDelta\n+from scrapy.item.models import ScrapedItem, ItemDelta, ItemAttribute\n\n@@ -1,5 +1,59 @@\n-from scrapy.item.adaptors import AdaptorDict\n+import types\n+\n+from traceback import format_exc\n+from scrapy import log\n from scrapy.conf import settings\n+from scrapy.core.exceptions import NotConfigured\n+\n+class ItemAttribute(object):\n+    def __init__(self, attrib_name, attrib_type, adaptor=None):\n+        self.attrib_name = attrib_name\n+        self.attrib_type = attrib_type\n+        self.adaptor = adaptor\n+\n+    def adapt(self, value, **kwargs):\n+        debug = kwargs.get('debug') or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])\n+        if not self.adaptor:\n+            if debug:\n+                log.msg('No adaptor defined for attribute %s.' % self.attrib_name, log.WARNING)\n+            return value\n+\n+        try:\n+            if debug:\n+                print \"  %07s | input >\" % self.attrib_name, repr(value)\n+            value = self.adaptor(value, **kwargs)\n+            if debug:\n+                print \"  %07s | output >\" % self.attrib_name, repr(value)\n+        \n+        except Exception:\n+            print \"Error in '%s' adaptor. Traceback text:\" % name\n+            print format_exc()\n+            self.value = None\n+            return False\n+        \n+        return value\n+        \n+    def check(self, value):\n+        if not self.attrib_name:\n+            raise NotConfigured('You must define \"attrib_name\" attribute in order to use an ItemAttribute')\n+\n+        if not self.attrib_type:\n+            raise NotConfigured('You must define \"attrib_type\" attribute in order to use an ItemAttribute')\n+        else:\n+            if hasattr(self.attrib_type, '__iter__'):\n+                if not hasattr(value, '__iter__'):\n+                    raise TypeError('Attribute \"%s\" must be a sequence' % self.attrib_name)\n+                iter_type = self.attrib_type[0]\n+                for i in value:\n+                    if not isinstance(i, iter_type):\n+                        raise TypeError('Attribute \"%s\" cannot contain %s, only %s' % (self.name, i.__class__.__name__, iter_type.__name__))\n+            else:\n+                if not isinstance(value, self.attrib_type):\n+                    raise TypeError('Attribute \"%s\" must be %s, not %s' % (self.attrib_name, self.attrib_type.__name__, value.__class__.__name__))\n+        return True\n+\n+class ItemDelta(object):\n+    pass\n \n class ScrapedItem(object):\n     \"\"\"\n@@ -8,23 +62,32 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    def set_adaptors(self, adaptors_dict, **kwargs):\n-        \"\"\"\n-        Set the adaptors to use for this item. Receives a dict of the adaptors\n-        desired for each attribute and returns the item itself.\n-        \"\"\"\n-        setattr(self, '_adaptors_dict', AdaptorDict(adaptors_dict))\n-        return self\n+    ATTRIBUTES = { 'guid': ItemAttribute(attrib_name='guid', attrib_type=basestring) }\n+    _override_adaptors = { }\n \n-    def set_attrib_adaptors(self, attrib, adaptors, **kwargs):\n-        \"\"\"\n-        Set the adaptors (from a list or tuple) to be used for a specific attribute.\n-        \"\"\"\n-        self._adaptors_dict[attrib] = adaptors\n+    def __setattr__(self, attr, value):\n+        if value is None:\n+            self.__dict__.pop(attr, None)\n+            return\n \n-    def attribute(self, attrname, value, override=False, add=False, debug=False):\n-        val = self._adaptors_dict.execute(attrname, value, debug)\n-        if val or val is False:\n+        if attr in self.ATTRIBUTES:\n+            self.ATTRIBUTES[attr].check(value)\n+        self.__dict__[attr] = value\n+\n+    def set_adaptor(self, attrname, adaptor):\n+        if attrname in self.ATTRIBUTES and callable(adaptor):\n+            self._override_adaptors[attrname] = adaptor\n+        \n+    def attribute(self, attrname, val, **kwargs):\n+        if not attrname in self.ATTRIBUTES and not attrname.startswith('_'):\n+            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attrname, self.__class__.__name__))\n+\n+        override = kwargs.pop('override', False)\n+        add = kwargs.pop('add', False)\n+        adaptor = self._override_adaptors.get(attrname) or self.ATTRIBUTES[attrname].adapt\n+\n+        val = adaptor(val, **kwargs)\n+        if not val is None:\n             curr_val = getattr(self, attrname, None)\n             if not curr_val:\n                 setattr(self, attrname, val)\n@@ -40,5 +103,3 @@ class ScrapedItem(object):\n     def __sub__(self, other):\n         raise NotImplementedError\n \n-class ItemDelta(object):\n-    pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4afa29b26a1c1e63e524421eb26f847cd3491cb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 194 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -52,10 +52,10 @@ class AdaptorsTestCase(unittest.TestCase):\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n         \n     def test_unquote_all(self):\n-        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n+        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;'], keep_entities=[]), [u'hello\\xa9&welcome', u'<br />&'])\n         \n     def test_unquote(self):\n-        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n+        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n         \n     def test_remove_tags(self):\n         test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n@@ -78,7 +78,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                          [1, 2, 5, 6, 'hi'])\n         \n     def test_delist(self):\n-        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n+        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n                          'hi there fellas. this is my test.')\n         \n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fab0e6938cfd2f65140147d363e0cc5b10dd6f48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 300 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ class RobustScrapedItem(ScrapedItem):\n         # Note that this method is called only when the attribute is not found in \n         # self.__dict__ or the class/instance methods.\n         if attr in self.ATTRIBUTES:  \n-            return () if hasattr(self.ATTRIBUTES[attr].attrib_type, '__iter__') else None\n+            return None\n         else:\n             raise AttributeError(attr)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f4053d9fc115e2742df8caf991104da3cac08a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 116 | Files Changed: 7 | Hunks: 21 | Methods Changed: 19 | Complexity Δ (Sum/Max): -20/10 | Churn Δ: 178 | Churn Cumulative: 1134 | Contributors (this commit): 2 | Commits (past 90d): 49 | Contributors (cumulative): 11 | DMM Complexity: 0.41304347826086957\n\nDIFF:\n@@ -1,26 +1,4 @@\n-import inspect\n-\n from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages\n-from scrapy.contrib.adaptors.markup import remove_tags, remove_root, unquote\n-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, delist, Regex\n+from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex\n from scrapy.utils.python import unique, flatten\n-\n-def adaptor_gen(*args):\n-    subadaptors = []\n-    arg_mappings = []\n-    for subadaptor in args:\n-        if callable(subadaptor):\n-            if inspect.isfunction(subadaptor):\n-                func_args, varargs, varkw, defaults = inspect.getargspec(subadaptor)\n-                arg_mappings.append(func_args[1:])\n-            else:\n-                arg_mappings.append([])\n-            subadaptors.append(subadaptor)\n-    \n-    def _adaptor(value, **kwargs):\n-        for index, subadaptor in enumerate(subadaptors):\n-            adaptor_args = dict((key, val) for key, val in kwargs.items() if key in arg_mappings[index])\n-            value = subadaptor(value, **adaptor_args)\n-        return value\n-\n-    return _adaptor\n\n@@ -21,7 +21,7 @@ def remove_root(value):\n         return value\n     return [ _remove_root(v) for v in value ]\n \n-def unquote(value, keep_entities=None):\n+class Unquote(object):\n     \"\"\"\n     Receives a list of strings, removes all of the\n     entities the strings may have, and returns\n@@ -30,8 +30,10 @@ def unquote(value, keep_entities=None):\n     Input: iterable with strings\n     Output: list of strings\n     \"\"\"\n-    if keep_entities is None:\n-        keep_entities = ['lt', 'amp']\n-    return [ remove_entities(v, keep=keep_entities) for v in value ]\n+    def __init__(self, keep=['lt', 'amp']):\n+        self.keep = keep\n+\n+    def __call__(self, value):\n+        return [ remove_entities(v, keep=self.keep) for v in value ]\n \n \n\n@@ -1 +1 @@\n-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline\n\n@@ -8,7 +8,7 @@ import hashlib\n from pydispatch import dispatcher\n from pprint import PrettyPrinter\n \n-from scrapy.item import ScrapedItem, ItemDelta, ItemAttribute\n+from scrapy.item import ScrapedItem, ItemDelta\n from scrapy.spider import spiders\n from scrapy.core import signals\n from scrapy.core.exceptions import UsageError, DropItem\n@@ -30,6 +30,11 @@ class ValidationPipeline(object):\n         item.validate()\n         return item\n \n+class SetGUIDPipeline(object):\n+    def process_item(self, domain, response, item):\n+        spiders.fromdomain(domain).set_guid(item)\n+        return item\n+\n class RobustScrapedItem(ScrapedItem):\n     \"\"\"\n     A more robust scraped item class with a built-in validation mechanism and \n@@ -37,8 +42,8 @@ class RobustScrapedItem(ScrapedItem):\n     \"\"\"\n \n     ATTRIBUTES = {\n-        'guid': ItemAttribute(attrib_name='guid', attrib_type=basestring), # a global unique identifier\n-        'url': ItemAttribute(attrib_name='url', attrib_type=basestring),  # the main URL where this item was scraped from\n+        'guid': basestring, # a global unique identifier\n+        'url': basestring,  # the main URL where this item was scraped from\n     }\n     \n     def __init__(self, data=None):\n@@ -67,8 +72,30 @@ class RobustScrapedItem(ScrapedItem):\n         \"\"\"\n         Set an attribute checking it matches the attribute type declared in self.ATTRIBUTES\n         \"\"\"\n+        if not attr.startswith('_') and attr not in self.ATTRIBUTES:\n+            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attr, self.__class__.__name__))\n+\n+        if value is None:\n+            self.__dict__.pop(attr, None)\n+            return\n+\n+        if attr == '_adaptors_dict':\n+            return object.__setattr__(self, '_adaptors_dict', value)\n+\n+        type1 = self.ATTRIBUTES[attr]\n+        if hasattr(type1, '__iter__'):\n+            if not hasattr(value, '__iter__'):\n+                raise TypeError('Attribute \"%s\" must be a sequence' % attr)\n+            type2 = type1[0]\n+            for i in value:\n+                if not isinstance(i, type2):\n+                    raise TypeError('Attribute \"%s\" cannot contain %s, only %s' % (attr, i.__class__.__name__, type2.__name__))\n+        else:\n+            if not isinstance(value, type1):\n+                raise TypeError('Attribute \"%s\" must be %s, not %s' % (attr, type1.__name__, value.__class__.__name__))\n+\n+        self.__dict__[attr] = value\n         self.__dict__['_version'] = None\n-        super(RobustScrapedItem, self).__setattr__(attr, value)\n     \n     def __delattr__(self, attr):\n         \"\"\"\n\n@@ -1 +1 @@\n-from scrapy.item.models import ScrapedItem, ItemDelta, ItemAttribute\n+from scrapy.item.models import ScrapedItem, ItemDelta\n\n@@ -1,59 +1,5 @@\n-import types\n-\n-from traceback import format_exc\n-from scrapy import log\n+from scrapy.item.adaptors import AdaptorDict\n from scrapy.conf import settings\n-from scrapy.core.exceptions import NotConfigured\n-\n-class ItemAttribute(object):\n-    def __init__(self, attrib_name, attrib_type, adaptor=None):\n-        self.attrib_name = attrib_name\n-        self.attrib_type = attrib_type\n-        self.adaptor = adaptor\n-\n-    def adapt(self, value, **kwargs):\n-        debug = kwargs.get('debug') or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])\n-        if not self.adaptor:\n-            if debug:\n-                log.msg('No adaptor defined for attribute %s.' % self.attrib_name, log.WARNING)\n-            return value\n-\n-        try:\n-            if debug:\n-                print \"  %07s | input >\" % self.attrib_name, repr(value)\n-            value = self.adaptor(value, **kwargs)\n-            if debug:\n-                print \"  %07s | output >\" % self.attrib_name, repr(value)\n-        \n-        except Exception:\n-            print \"Error in '%s' adaptor. Traceback text:\" % name\n-            print format_exc()\n-            self.value = None\n-            return False\n-        \n-        return value\n-        \n-    def check(self, value):\n-        if not self.attrib_name:\n-            raise NotConfigured('You must define \"attrib_name\" attribute in order to use an ItemAttribute')\n-\n-        if not self.attrib_type:\n-            raise NotConfigured('You must define \"attrib_type\" attribute in order to use an ItemAttribute')\n-        else:\n-            if hasattr(self.attrib_type, '__iter__'):\n-                if not hasattr(value, '__iter__'):\n-                    raise TypeError('Attribute \"%s\" must be a sequence' % self.attrib_name)\n-                iter_type = self.attrib_type[0]\n-                for i in value:\n-                    if not isinstance(i, iter_type):\n-                        raise TypeError('Attribute \"%s\" cannot contain %s, only %s' % (self.name, i.__class__.__name__, iter_type.__name__))\n-            else:\n-                if not isinstance(value, self.attrib_type):\n-                    raise TypeError('Attribute \"%s\" must be %s, not %s' % (self.attrib_name, self.attrib_type.__name__, value.__class__.__name__))\n-        return True\n-\n-class ItemDelta(object):\n-    pass\n \n class ScrapedItem(object):\n     \"\"\"\n@@ -62,32 +8,23 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    ATTRIBUTES = { 'guid': ItemAttribute(attrib_name='guid', attrib_type=basestring) }\n-    _override_adaptors = { }\n+    def set_adaptors(self, adaptors_dict, **kwargs):\n+        \"\"\"\n+        Set the adaptors to use for this item. Receives a dict of the adaptors\n+        desired for each attribute and returns the item itself.\n+        \"\"\"\n+        setattr(self, '_adaptors_dict', AdaptorDict(adaptors_dict))\n+        return self\n     \n-    def __setattr__(self, attr, value):\n-        if value is None:\n-            self.__dict__.pop(attr, None)\n-            return\n+    def set_attrib_adaptors(self, attrib, adaptors, **kwargs):\n+        \"\"\"\n+        Set the adaptors (from a list or tuple) to be used for a specific attribute.\n+        \"\"\"\n+        self._adaptors_dict[attrib] = adaptors\n \n-        if attr in self.ATTRIBUTES:\n-            self.ATTRIBUTES[attr].check(value)\n-        self.__dict__[attr] = value\n-\n-    def set_adaptor(self, attrname, adaptor):\n-        if attrname in self.ATTRIBUTES and callable(adaptor):\n-            self._override_adaptors[attrname] = adaptor\n-        \n-    def attribute(self, attrname, val, **kwargs):\n-        if not attrname in self.ATTRIBUTES and not attrname.startswith('_'):\n-            raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attrname, self.__class__.__name__))\n-\n-        override = kwargs.pop('override', False)\n-        add = kwargs.pop('add', False)\n-        adaptor = self._override_adaptors.get(attrname) or self.ATTRIBUTES[attrname].adapt\n-\n-        val = adaptor(val, **kwargs)\n-        if not val is None:\n+    def attribute(self, attrname, value, override=False, add=False, debug=False):\n+        val = self._adaptors_dict.execute(attrname, value, debug)\n+        if val or val is False:\n             curr_val = getattr(self, attrname, None)\n             if not curr_val:\n                 setattr(self, attrname, val)\n@@ -103,3 +40,5 @@ class ScrapedItem(object):\n     def __sub__(self, other):\n         raise NotImplementedError\n \n+class ItemDelta(object):\n+    pass\n\n@@ -52,10 +52,10 @@ class AdaptorsTestCase(unittest.TestCase):\n                                   ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n         \n     def test_unquote_all(self):\n-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;'], keep_entities=[]), [u'hello\\xa9&welcome', u'<br />&'])\n+        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n         \n     def test_unquote(self):\n-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n+        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n         \n     def test_remove_tags(self):\n         test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n@@ -78,7 +78,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                          [1, 2, 5, 6, 'hi'])\n         \n     def test_delist(self):\n-        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n+        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n                          'hi there fellas. this is my test.')\n         \n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01b157d10b47a4121429c931494e9bc06ba67b73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 114 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,12 +63,16 @@ def canonicalize_urls(value):\n         return canonicalize_url(value)\n     return ''\n \n-def delist(value, join_delimiter=' '):\n+class Delist(object):\n     \"\"\"\n     Input: iterable with strings\n     Output: unicode\n     \"\"\"\n-    return join_delimiter.join(value)\n+    def __init__(self, delimiter=' '):\n+        self.delimiter = delimiter\n+    \n+    def __call__(self, value):\n+        return self.delimiter.join(value)\n \n class Regex(object):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79c9ac388d0364ded332cb9765cce45af6036036", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 411 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,14 @@\n+import unittest\n+\n+from scrapy.utils.misc import hash_values\n+from scrapy.core.exceptions import UsageError\n+\n+class UtilsMiscTestCase(unittest.TestCase):\n+    def test_hash_values(self):\n+        self.assertEqual(hash_values('some', 'values', 'to', 'hash'),\n+                         'f37f5dc65beaaea35af05e16e26d439fd150c576')\n+\n+        self.assertRaises(UsageError, hash_values, 'some', None, 'value')\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -2,7 +2,7 @@\n Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n import re\n-import sha\n+import hashlib\n \n from twisted.internet import defer\n \n@@ -104,7 +104,7 @@ def hash_values(*values):\n     >>> hash_values('some', 'values', 'to', 'hash')\n     'f37f5dc65beaaea35af05e16e26d439fd150c576'\n     \"\"\"\n-    hash = sha.new()\n+    hash = hashlib.sha1()\n     for value in values:\n         if value is None:\n             message = \"hash_values was passed None at argument index %d. This is a bug in the calling code\" \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
