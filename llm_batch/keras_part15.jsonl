{"custom_id": "keras#4c3495896e77a5a4ebfeff717feb3d04681d27e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 30 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,12 +4,12 @@ import theano.tensor as T\n import types\n \n def softmax(x):\n-    return T.nnet.softmax(x)\n+    return T.nnet.softmax(x.reshape((-1,x.shape[-1]))).reshape(x.shape)\n \n def time_distributed_softmax(x):\n-    xshape = x.shape\n-    X = x.reshape((xshape[0] * xshape[1], xshape[2]))\n-    return T.nnet.softmax(X).reshape(xshape)\n+    import warnings\n+    warnings.warn(\"time_distributed_softmax is deprecated. Just use softmax!\", DeprecationWarning)\n+    return softmax(x)\n \n def softplus(x):\n     return T.nnet.softplus(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a2ae93ba745fc69f130a42a6a2a79464cdc468f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -8,13 +8,8 @@ from keras.layers.embeddings import Embedding\n from keras.layers.core import Layer\n from keras.layers import containers\n from keras.utils import np_utils\n-\n import numpy as np\n \n-# Try different things here: 'lstm' or 'classical' or 'denoising'\n-# or 'deep_denoising'\n-autoencoder_type = 'deep_denoising'\n-\n nb_classes = 10\n batch_size = 128\n nb_epoch = 5\n@@ -43,6 +38,7 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n print(\"X_train: \", X_train.shape)\n print(\"X_test: \", X_test.shape)\n \n+\n ##########################\n # dense model test       #\n ##########################\n@@ -98,6 +94,12 @@ def build_deep_denoising_autoencoder(autoencoder):\n \n \n \n+# Try different things here: 'lstm' or 'classical' or 'denoising'\n+# or 'deep_denoising'\n+\n+for autoencoder_type in ['classical', 'denoising', 'deep_denoising', 'lstm']:\n+\tprint(autoencoder_type)\n+\tprint('-'*40)\n \t# Build our autoencoder model\n \tautoencoder = Sequential()\n \tif autoencoder_type == 'lstm':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ef71ec94406ff4029710680474b3aa887999650", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1301 | Contributors (this commit): 12 | Commits (past 90d): 75 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -161,7 +161,8 @@ class Dropout(Layer):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"p\":self.p}\n+            \"p\":self.p,\n+            \"mask_val\":self.mask_val.eval()}\n \n \n class Activation(Layer):\n@@ -184,7 +185,8 @@ class Activation(Layer):\n         return {\"name\":self.__class__.__name__,\n             \"activation\":self.activation.__name__,\n             \"target\":self.target,\n-            \"beta\":self.beta}\n+            \"beta\":self.beta,\n+            \"mask_val\":self.mask_val.eval()}\n \n \n class Reshape(Layer):\n@@ -330,7 +332,8 @@ class TimeDistributedDense(Layer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"activation\":self.activation.__name__}\n+            \"activation\":self.activation.__name__,\n+            \"mask_val\":self.mask_val.eval()}\n \n class AutoEncoder(Layer):\n     '''\n\n@@ -81,7 +81,8 @@ class SimpleRNN(Layer):\n             \"inner_init\":self.inner_init.__name__,\n             \"activation\":self.activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences}\n+            \"return_sequences\":self.return_sequences,\n+            \"mask_val\":self.mask_val.eval()}\n \n \n class SimpleDeepRNN(Layer):\n@@ -169,7 +170,8 @@ class SimpleDeepRNN(Layer):\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences}\n+            \"return_sequences\":self.return_sequences,\n+            \"mask_val\":self.mask_val.eval()}\n \n \n \n@@ -277,7 +279,8 @@ class GRU(Layer):\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences}\n+            \"return_sequences\":self.return_sequences,\n+            \"mask_val\":self.mask_val.eval()}\n \n \n \n@@ -399,6 +402,7 @@ class LSTM(Layer):\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences}\n+            \"return_sequences\":self.return_sequences,\n+            \"mask_val\":self.mask_val.eval()}\n         \n \n\n@@ -28,6 +28,9 @@ model.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model\")\n \n+if model.layers[1].get_config()['mask_val'] != default_mask_val:\n+    raise Exception(\"mask_val improperly set in config. Set to %s not %f\" % (model.layers[1].get_config()['mask_val']))\n+\n W0 = model.layers[0].W.get_value()[0,:]\n if (W0 != default_mask_val).any():\n     raise Exception(\"Did not set the mask val properly into the Embedding W matrix, got: \",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13715c49c3f36e109fa99c0d753f1ec49f71e88e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 6 | Churn Cumulative: 60 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,6 +59,12 @@ def orthogonal(shape, scale=1.1):\n     q = q.reshape(shape)\n     return sharedX(scale * q[:shape[0], :shape[1]])\n \n+def identity(shape, scale=1):\n+    if len(shape) != 2 or shape[0] != shape[1]:\n+        raise Exception(\"Identity matrix initialization can only be used for 2D square matrices\")\n+    else:\n+        return sharedX(scale * np.identity(shape[0]))\n+\n def zero(shape):\n     return shared_zeros(shape)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c35780557eee7216ce0393049e4b2c4bb68260a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 301 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 301 | Churn Cumulative: 611 | Contributors (this commit): 6 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -368,3 +368,304 @@ class LSTM(Layer):\n             \"return_sequences\":self.return_sequences}\n \n \n+\n+class MutatedRNN_1(Layer):\n+    '''\n+        Evolved recurrent neural network architectures from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n+        \n+        This corresponds to the `MUT1` architecture described in the paper.\n+        \n+        Takes inputs with shape:\n+        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n+        \n+        and returns outputs with shape:\n+        if not return_sequences:\n+            (nb_samples, output_dim)\n+        if return_sequences:\n+            (nb_samples, max_sample_length, output_dim)\n+        \n+        References:\n+            An Empirical Exploration of Recurrent Network Architectures\n+                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n+    '''\n+    def __init__(self, input_dim, output_dim=128, \n+        init='glorot_uniform', inner_init='orthogonal',\n+        activation='tanh', inner_activation='sigmoid',\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n+\n+        super(MutatedRNN_1,self).__init__()\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        \n+        if self.input_dim != self.output_dim:\n+            raise Exception('input_dim must equal output_dim for this architecture')\n+\n+        self.init = initializations.get(init)\n+        self.inner_init = initializations.get(inner_init)\n+        self.activation = activations.get(activation)\n+        self.inner_activation = activations.get(inner_activation)\n+        self.input = T.tensor3()\n+\n+        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.b_z = shared_zeros((self.output_dim))\n+\n+        self.W_r = self.init((self.input_dim, self.output_dim))\n+        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_r = shared_zeros((self.output_dim))\n+\n+        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_h = shared_zeros((self.output_dim))\n+\n+        self.params = [\n+            self.W_z, self.b_z,\n+            self.W_r, self.U_r, self.b_r,\n+            self.U_h, self.b_h,\n+        ]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def _step(self, \n+        xz_t, xr_t, xh_t, \n+        h_tm1, \n+        u_r, u_h):\n+        z = self.inner_activation(xz_t)\n+        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n+        h_t = hh_t * z + h_tm1 * (1 - z)\n+        return h_t\n+\n+    def get_output(self, train):\n+        X = self.get_input(train) \n+        X = X.dimshuffle((1,0,2)) \n+\n+        x_z = T.dot(X, self.W_z) + self.b_z\n+        x_r = T.dot(X, self.W_r) + self.b_r\n+        x_h = T.tanh(X) + self.b_h\n+        outputs, updates = theano.scan(\n+            self._step, \n+            sequences=[x_z, x_r, x_h], \n+            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n+            non_sequences=[self.U_r, self.U_h],\n+            truncate_gradient=self.truncate_gradient\n+        )\n+        if self.return_sequences:\n+            return outputs.dimshuffle((1,0,2))\n+        return outputs[-1]\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n+            \"output_dim\":self.output_dim,\n+            \"init\":self.init.__name__,\n+            \"inner_init\":self.inner_init.__name__,\n+            \"activation\":self.activation.__name__,\n+            \"inner_activation\":self.inner_activation.__name__,\n+            \"truncate_gradient\":self.truncate_gradient,\n+            \"return_sequences\":self.return_sequences}\n+\n+\n+\n+class MutatedRNN_2(Layer):\n+    '''\n+        Evolved recurrent neural network architectures from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n+        \n+        This corresponds to the `MUT2` architecture described in the paper.\n+        \n+        Takes inputs with shape:\n+        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n+        \n+        and returns outputs with shape:\n+        if not return_sequences:\n+            (nb_samples, output_dim)\n+        if return_sequences:\n+            (nb_samples, max_sample_length, output_dim)\n+        \n+        References:\n+            An Empirical Exploration of Recurrent Network Architectures\n+                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n+    '''\n+    def __init__(self, input_dim, output_dim=128, \n+        init='glorot_uniform', inner_init='orthogonal',\n+        activation='tanh', inner_activation='sigmoid',\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n+\n+        super(MutatedRNN_2,self).__init__()\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        \n+        if self.input_dim != self.output_dim:\n+            raise Exception('input_dim must equal output_dim for this architecture')\n+\n+        self.init = initializations.get(init)\n+        self.inner_init = initializations.get(inner_init)\n+        self.activation = activations.get(activation)\n+        self.inner_activation = activations.get(inner_activation)\n+        self.input = T.tensor3()\n+\n+        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_z = shared_zeros((self.output_dim))\n+\n+        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_r = shared_zeros((self.output_dim))\n+\n+        self.W_h = self.init((self.input_dim, self.output_dim))\n+        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_h = shared_zeros((self.output_dim))\n+\n+        self.params = [\n+            self.W_z, self.U_z, self.b_z,\n+            self.U_r, self.b_r,\n+            self.W_h, self.U_h, self.b_h,\n+        ]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def _step(self, \n+        xz_t, xr_t, xh_t, \n+        h_tm1, \n+        u_z, u_r, u_h):\n+        z = self.inner_activation(xz_t + T.dot(h_tm1, u_z))\n+        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n+        h_t = hh_t * z + h_tm1 * (1 - z)\n+        return h_t\n+\n+    def get_output(self, train):\n+        X = self.get_input(train) \n+        X = X.dimshuffle((1,0,2)) \n+\n+        x_z = T.dot(X, self.W_z) + self.b_z\n+        x_r = X + self.b_r\n+        x_h = T.dot(X, self.W_h) + self.b_h\n+        outputs, updates = theano.scan(\n+            self._step, \n+            sequences=[x_z, x_r, x_h], \n+            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n+            non_sequences=[self.U_z, self.U_r, self.U_h],\n+            truncate_gradient=self.truncate_gradient\n+        )\n+        if self.return_sequences:\n+            return outputs.dimshuffle((1,0,2))\n+        return outputs[-1]\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n+            \"output_dim\":self.output_dim,\n+            \"init\":self.init.__name__,\n+            \"inner_init\":self.inner_init.__name__,\n+            \"activation\":self.activation.__name__,\n+            \"inner_activation\":self.inner_activation.__name__,\n+            \"truncate_gradient\":self.truncate_gradient,\n+            \"return_sequences\":self.return_sequences}\n+\n+\n+\n+class MutatedRNN_3(Layer):\n+    '''\n+        Evolved recurrent neural network architectures from the evaluation of thousands\n+        of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n+        \n+        This corresponds to the `MUT3` architecture described in the paper.\n+        \n+        Takes inputs with shape:\n+        (nb_samples, max_sample_length (samples shorter than this are padded with zeros at the end), input_dim)\n+        \n+        and returns outputs with shape:\n+        if not return_sequences:\n+            (nb_samples, output_dim)\n+        if return_sequences:\n+            (nb_samples, max_sample_length, output_dim)\n+        \n+        References:\n+            An Empirical Exploration of Recurrent Network Architectures\n+                http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n+    '''\n+    def __init__(self, input_dim, output_dim=128, \n+        init='glorot_uniform', inner_init='orthogonal',\n+        activation='tanh', inner_activation='sigmoid',\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n+\n+        super(MutatedRNN_3,self).__init__()\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+\n+        self.init = initializations.get(init)\n+        self.inner_init = initializations.get(inner_init)\n+        self.activation = activations.get(activation)\n+        self.inner_activation = activations.get(inner_activation)\n+        self.input = T.tensor3()\n+\n+        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.U_z = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_z = shared_zeros((self.output_dim))\n+\n+        self.W_r = self.init((self.input_dim, self.output_dim))\n+        self.U_r = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_r = shared_zeros((self.output_dim))\n+\n+        self.W_h = self.init((self.input_dim, self.output_dim))\n+        self.U_h = self.inner_init((self.output_dim, self.output_dim))\n+        self.b_h = shared_zeros((self.output_dim))\n+\n+        self.params = [\n+            self.W_z, self.U_z, self.b_z,\n+            self.W_r, self.U_r, self.b_r,\n+            self.W_h, self.U_h, self.b_h,\n+        ]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def _step(self, \n+        xz_t, xr_t, xh_t, \n+        h_tm1, \n+        u_z, u_r, u_h):\n+        z = self.inner_activation(xz_t + T.dot(T.tanh(h_tm1), u_z))\n+        r = self.inner_activation(xr_t + T.dot(h_tm1, u_r))\n+        hh_t = self.activation(xh_t + T.dot(r * h_tm1, u_h))\n+        h_t = hh_t * z + h_tm1 * (1 - z)\n+        return h_t\n+\n+    def get_output(self, train):\n+        X = self.get_input(train) \n+        X = X.dimshuffle((1,0,2)) \n+\n+        x_z = T.dot(X, self.W_z) + self.b_z\n+        x_r = T.dot(X, self.W_r) + self.b_r\n+        x_h = T.dot(X, self.W_h) + self.b_h\n+        outputs, updates = theano.scan(\n+            self._step, \n+            sequences=[x_z, x_r, x_h], \n+            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n+            non_sequences=[self.U_z, self.U_r, self.U_h],\n+            truncate_gradient=self.truncate_gradient\n+        )\n+        if self.return_sequences:\n+            return outputs.dimshuffle((1,0,2))\n+        return outputs[-1]\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n+            \"output_dim\":self.output_dim,\n+            \"init\":self.init.__name__,\n+            \"inner_init\":self.inner_init.__name__,\n+            \"activation\":self.activation.__name__,\n+            \"inner_activation\":self.inner_activation.__name__,\n+            \"truncate_gradient\":self.truncate_gradient,\n+            \"return_sequences\":self.return_sequences}\n+\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#413663f03fe8da3ec01d35cfa0fcca133fc3116c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 46 | Churn Cumulative: 994 | Contributors (this commit): 11 | Commits (past 90d): 47 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,9 +88,11 @@ class Merge(object):\n         self.regularizers = []\n         self.constraints = []\n         for m in self.models:\n-            self.params += m.params\n-            self.regularizers += m.regularizers\n-            self.constraints += m.constraints\n+            for i in range(len(m.params)):\n+                if not m.params[i] in self.params:\n+                    self.params.append(m.params[i])\n+                    self.regularizers.append(m.regularizers[i])\n+                    self.constraints.append(m.constraints[i])\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints\n@@ -111,10 +113,11 @@ class Merge(object):\n         res = []\n         for i in range(len(self.models)):\n             o = self.models[i].get_input(train)\n-            if type(o) == list:\n-                res += o\n-            else:\n-                res.append(o)\n+            if not type(o) == list:\n+                o = [o]\n+            for output in o:\n+                if output not in res:\n+                    res.append(output)\n         return res\n \n     @property\n\n@@ -180,5 +180,34 @@ model.load_weights('temp.h5')\n score = model.evaluate([X_train, X_train, X_train], Y_train, verbose=0)\n print('score:', score)\n \n+######################\n+# test merge overlap #\n+######################\n+print('Test merge overlap')\n+left = Sequential()\n+left.add(Dense(784, 50))\n+left.add(Activation('relu'))\n \n+model = Sequential()\n+model.add(Merge([left, left], mode='sum'))\n \n+model.add(Dense(50, 10))\n+model.add(Activation('softmax'))\n+\n+model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+score = model.evaluate(X_train, Y_train, verbose=0)\n+print('score:', score)\n+if score < 0.22:\n+    raise Exception('Score too low, learning issue.')\n+preds = model.predict(X_test, verbose=0)\n+classes = model.predict_classes(X_test, verbose=0)\n+\n+model.get_config(verbose=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11e961fdf848808bb63489bd698908d80119910e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 48 | Files Changed: 3 | Hunks: 36 | Methods Changed: 17 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 131 | Churn Cumulative: 1758 | Contributors (this commit): 17 | Commits (past 90d): 91 | Contributors (cumulative): 22 | DMM Complexity: 0.8\n\nDIFF:\n@@ -16,7 +16,7 @@ def standardize_y(y):\n     if not hasattr(y, 'shape'):\n         y = np.asarray(y)\n     if len(y.shape) == 1:\n-        y = np.reshape(y, (len(y), 1))\n+        y = np.expand_dims(y, 1)\n     return y\n \n def make_batches(size, batch_size):\n@@ -41,24 +41,6 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n-def calculate_loss_weights(Y, sample_weight=None, class_weight=None):\n-    if sample_weight is not None:\n-        if isinstance(sample_weight, list):\n-            w = np.array(sample_weight)\n-        else:\n-            w = sample_weight\n-    elif isinstance(class_weight, dict):\n-        if Y.shape[1] > 1:\n-            y_classes = Y.argmax(axis=1)\n-        elif Y.shape[1] == 1:\n-            y_classes = np.reshape(Y, Y.shape[0])\n-        else:\n-            y_classes = Y\n-        w = np.array(list(map(lambda x: class_weight[x], y_classes)))\n-    else:\n-        w = np.ones((Y.shape[0]))\n-    return w\n-\n class Model(object):\n \n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n@@ -75,8 +57,11 @@ class Model(object):\n         # target of model\n         self.y = T.zeros_like(self.y_train)\n \n-        train_loss = self.loss(self.y, self.y_train)\n-        test_score = self.loss(self.y, self.y_test)\n+        self.weights = T.ones_like(self.y_train)\n+\n+        train_loss = self.loss(self.y, self.y_train, self.weights)\n+        test_score = self.loss(self.y, self.y_test, self.weights)\n+\n         \n         if class_mode == \"categorical\":\n             train_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_train, axis=-1)))\n@@ -92,12 +77,12 @@ class Model(object):\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n-            train_ins = self.X_train + [self.y]\n-            test_ins = self.X_test + [self.y]\n+            train_ins = self.X_train + [self.y, self.weights]\n+            test_ins = self.X_test + [self.y, self.weights]\n             predict_ins = self.X_test\n         else:\n-            train_ins = [self.X_train, self.y]\n-            test_ins = [self.X_test, self.y]\n+            train_ins = [self.X_train, self.y, self.weights]\n+            test_ins = [self.X_test, self.y, self.weights]\n             predict_ins = [self.X_test]\n \n         self._train = theano.function(train_ins, train_loss, \n@@ -112,11 +97,16 @@ class Model(object):\n             allow_input_downcast=True, mode=theano_mode)\n \n \n-    def train(self, X, y, accuracy=False):\n+    def train(self, X, y, accuracy=False, weights=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        ins = X + [y]\n+        if weights is None:\n+            weights = np.ones(list(y.shape[0:-1]) + [1])\n+        else:\n+            weights = standardize_y(weights)\n+\n+        ins = X + [y, weights]\n         if accuracy:\n             return self._train_with_acc(*ins)\n         else:\n@@ -134,21 +124,30 @@ class Model(object):\n \n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False):\n+            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False, weights=None):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n+        if weights is not None:\n+            weights = standardize_y(weights)\n \n         do_validation = False\n         if validation_data:\n+            weight_val = None\n             try:\n                 X_val, y_val = validation_data\n             except:\n-                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val). \\\n+                try:\n+                    X_val, y_val, weight_val = validation_data\n+                except:\n+                    raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val, [weight_val]). \\\n                         X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n             do_validation = True\n             X_val = standardize_X(X_val)\n             y_val = standardize_y(y_val)\n+            if weight_val is not None:\n+                weight_val = standardize_y(weight_val)\n+            \n             if verbose:\n                 print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n         else:\n@@ -159,7 +158,10 @@ class Model(object):\n                 do_validation = True\n                 split_at = int(len(y) * (1 - validation_split))\n                 (X, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n-                (y, y_val) = (y[0:split_at], y[split_at:])\n+                (y, y_val) = (y[:split_at], y[split_at:])\n+                if weights is not None:\n+                    (weights, weight_val) = (weights[:split_at], weights[split_at:])\n+\n                 if verbose:\n                     print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n \n@@ -191,13 +193,17 @@ class Model(object):\n                 batch_ids = index_array[batch_start:batch_end]\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n+                if weights is not None:\n+                    weight_batch = weights[batch_ids]\n+                else:\n+                    weight_batch = np.ones(list(y_batch.shape[:-1]) + [1])\n \n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n \n-                ins = X_batch + [y_batch]\n+                ins = X_batch + [y_batch, weight_batch]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n                     batch_logs['accuracy'] = acc\n@@ -212,11 +218,11 @@ class Model(object):\n                     epoch_logs = {}\n                     if do_validation:\n                         if show_accuracy:\n-                            val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, \\\n+                            val_loss, val_acc = self.evaluate(X_val, y_val, weights=weight_val, batch_size=batch_size, \\\n                                 verbose=0, show_accuracy=True)\n                             epoch_logs['val_accuracy'] = val_acc\n                         else:\n-                            val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n+                            val_loss = self.evaluate(X_val, y_val, weights=weight_val, batch_size=batch_size, verbose=0)\n                         epoch_logs['val_loss'] = val_loss\n \n             callbacks.on_epoch_end(epoch, epoch_logs)\n@@ -260,10 +266,13 @@ class Model(object):\n             return (proba > 0.5).astype('int32')\n \n \n-    def evaluate(self, X, y, batch_size=128, show_accuracy=False, verbose=1):\n+    def evaluate(self, X, y, batch_size=128, show_accuracy=False, verbose=1, weights=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n+        if weights is not None:\n+            weights = standardize_y(weights)\n+\n         if show_accuracy:\n             tot_acc = 0.\n         tot_score = 0.\n@@ -275,8 +284,13 @@ class Model(object):\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             X_batch = slice_X(X, batch_start, batch_end)\n             y_batch = y[batch_start:batch_end]\n+            if weights is None:\n+                weight_batch = np.ones(list(y_batch.shape[:-1]) + [1])\n+            else:\n+                weight_batch = weights[batch_start:batch_end]\n             \n-            ins = X_batch + [y_batch]\n+\n+            ins = X_batch + [y_batch, weight_batch]\n             if show_accuracy:\n                 loss, acc = self._test_with_acc(*ins)\n                 tot_acc += acc * len(y_batch)\n\n@@ -7,30 +7,30 @@ from six.moves import range\n epsilon = 1.0e-9\n \n def mean_squared_error(y_true, y_pred):\n-    return T.sqr(y_pred - y_true).mean()\n+    return T.sqr(y_pred - y_true).mean(axis=-1)\n \n def mean_absolute_error(y_true, y_pred):\n-    return T.abs_(y_pred - y_true).mean()\n+    return T.abs_(y_pred - y_true).mean(axis=-1)\n \n def squared_hinge(y_true, y_pred):\n-    return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean()\n+    return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean(axis=-1)\n \n def hinge(y_true, y_pred):\n-    return T.maximum(1. - y_true * y_pred, 0.).mean()\n+    return T.maximum(1. - y_true * y_pred, 0.).mean(axis=-1)\n \n def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     # scale preds so that the class probas of each sample sum to 1\n-    y_pred /= y_pred.sum(axis=1, keepdims=True) \n+    y_pred /= y_pred.sum(axis=-1, keepdims=True) \n     cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n-    return cce.mean()\n+    return cce\n \n def binary_crossentropy(y_true, y_pred):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     bce = T.nnet.binary_crossentropy(y_pred, y_true)\n-    return bce.mean()\n+    return bce\n \n # aliases\n mse = MSE = mean_squared_error\n@@ -38,4 +38,11 @@ mae = MAE = mean_absolute_error\n \n from .utils.generic_utils import get_from_module\n def get(identifier):\n-    return get_from_module(identifier, globals(), 'objective')\n\\ No newline at end of file\n+    obj_fn = get_from_module(identifier, globals(), 'objective')\n+    def weighted_obj_fun(y_true, y_pred, weights):\n+        # it's important that 0 * Inf == 0, not NaN, so I need to mask first\n+        masked_y_true = y_true[weights.nonzero()[:-1]]\n+        masked_y_pred = y_pred[weights.nonzero()[:-1]]\n+        masked_weights = weights[weights.nonzero()]\n+        return (masked_weights * obj_fn(masked_y_true, masked_y_pred)).mean()\n+    return weighted_obj_fun\n\n@@ -123,16 +123,30 @@ model2.add(LSTM(4,4, return_sequences=True))\n model2.add(Activation('tanh'))\n model2.add(GRU(4,4, activation='softmax', return_sequences=True))\n model2.add(SimpleDeepRNN(4,4, depth=2, activation='relu', return_sequences=True)) \n-model2.add(SimpleRNN(4,4, activation='relu', return_sequences=False))\n+model2.add(SimpleRNN(4,4, activation='relu', return_sequences=True))\n model2.compile(loss='categorical_crossentropy',\n-        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_COMPILE)\n+        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model2\")\n \n X2 = np.random.random_integers(1, 4, size=(2,5))\n+y2 = np.random.random((X2.shape[0],X2.shape[1],4))\n+\n ref = model2.predict(X2)\n+ref_eval = model2.evaluate(X2, y2)\n+mask = np.ones((y2.shape[0], y2.shape[1], 1))\n+\n for pre_zeros in range(1,10):\n-    padded = np.concatenate((np.zeros((2, pre_zeros)), X2), axis=1)\n-    pred = model2.predict(padded)\n-    if not np.allclose(ref, pred):\n+    padded_X2 = np.concatenate((np.zeros((X2.shape[0], pre_zeros)), X2), axis=1)\n+    padded_mask = np.concatenate((np.zeros((mask.shape[0], pre_zeros, mask.shape[2])), mask), axis=1)\n+    padded_y2 = np.concatenate((np.zeros((y2.shape[0], pre_zeros, y2.shape[2])), y2), axis=1)\n+\n+    pred = model2.predict(padded_X2)\n+    if not np.allclose(ref[:,-1,:], pred[:,-1,:]):\n         raise Exception(\"Different result after left-padding %d zeros. Ref: %s, Pred: %s\" % (pre_zeros, ref, pred))\n \n+    pad_eval = model2.evaluate(padded_X2, padded_y2, weights=padded_mask)\n+    if not np.allclose([pad_eval], [ref_eval]):\n+        raise Exception(\"Got dissimilar categorical_crossentropy after left-padding %d zeros. Ref: %f, Pred %f\" %\\\n+                (pref_eval, pred_val))\n+\n+        \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4962f1885797f29754ebcbe25de2384cf1420c71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 21 | Churn Cumulative: 1712 | Contributors (this commit): 17 | Commits (past 90d): 86 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,11 +41,20 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n+def weighted_obj_fn(fn):\n+    def weighted(y_true, y_pred, weights):\n+        # it's important that 0 * Inf == 0, not NaN, so I need to mask first\n+        masked_y_true = y_true[weights.nonzero()[:-1]]\n+        masked_y_pred = y_pred[weights.nonzero()[:-1]]\n+        masked_weights = weights[weights.nonzero()]\n+        return (masked_weights * fn(masked_y_true, masked_y_pred)).mean()\n+    return weighted\n+\n class Model(object):\n \n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n-        self.loss = objectives.get(loss)\n+        self.loss = weighted_obj_fn(objectives.get(loss))\n \n         # input of model \n         self.X_train = self.get_input(train=True)\n\n@@ -38,11 +38,5 @@ mae = MAE = mean_absolute_error\n \n from .utils.generic_utils import get_from_module\n def get(identifier):\n-    obj_fn = get_from_module(identifier, globals(), 'objective')\n-    def weighted_obj_fun(y_true, y_pred, weights):\n-        # it's important that 0 * Inf == 0, not NaN, so I need to mask first\n-        masked_y_true = y_true[weights.nonzero()[:-1]]\n-        masked_y_pred = y_pred[weights.nonzero()[:-1]]\n-        masked_weights = weights[weights.nonzero()]\n-        return (masked_weights * obj_fn(masked_y_true, masked_y_pred)).mean()\n-    return weighted_obj_fun\n+    return get_from_module(identifier, globals(), 'objective')\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56e09ad7365ed09caded620f65e44160d6aa02b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/5 | Churn Δ: 61 | Churn Cumulative: 1026 | Contributors (this commit): 11 | Commits (past 90d): 46 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -282,8 +282,9 @@ class ActivityRegularization(Layer):\n         Layer that passes through its input unchanged, but applies an update\n         to the cost function based on the activity.\n     '''\n-    def __init__(self, activity_regularizer):\n+    def __init__(self, activity_regularizer = None):\n         super(ActivityRegularization, self).__init__()\n+        if activity_regularizer is not None:\n             self.cost_update = activity_regularizer(self)\n \n     def get_output(self, train):\n\n@@ -0,0 +1,58 @@\n+import unittest\n+import numpy as np\n+from keras.models import Sequential\n+from keras.layers.core import Merge, Dense, Activation, Flatten, ActivityRegularization\n+from keras.layers.embeddings import Embedding\n+from keras.datasets import mnist\n+from keras.utils import np_utils\n+from keras import regularizers\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 5\n+weighted_class = 9\n+standard_weight = 1\n+high_weight = 5\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+np.random.seed(1337) # for reproducibility\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train = X_train.reshape(60000, 784)[:max_train_samples]\n+X_test = X_test.reshape(10000, 784)[:max_test_samples]\n+X_train = X_train.astype(\"float32\") / 255\n+X_test = X_test.astype(\"float32\") / 255\n+\n+# convert class vectors to binary class matrices\n+y_train = y_train[:max_train_samples]\n+y_test = y_test[:max_test_samples]\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+test_ids = np.where(y_test == np.array(weighted_class))[0]\n+\n+def create_model(weight_reg=None, activity_reg=None):\n+    model = Sequential()\n+    model.add(Dense(784, 50))\n+    model.add(Activation('relu'))\n+    model.add(ActivityRegularization(activity_reg))\n+    model.add(Dense(50, 10, W_regularizer=weight_reg))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+class TestRegularizers(unittest.TestCase):\n+    def test_W_reg(self):\n+        for reg in [regularizers.identity, regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n+            model = create_model(weight_reg=reg)\n+            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+            model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+\n+    def test_A_reg(self):\n+        for reg in [regularizers.activity_l1(), regularizers.activity_l2()]:\n+            model = create_model(activity_reg=reg)\n+            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+            model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59e345501e9eb13b6b50f98d2b78a8cc7c453538", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 116 | Lines Deleted: 42 | Files Changed: 3 | Hunks: 6 | Methods Changed: 23 | Complexity Δ (Sum/Max): 9/10 | Churn Δ: 158 | Churn Cumulative: 2699 | Contributors (this commit): 21 | Commits (past 90d): 122 | Contributors (cumulative): 32 | DMM Complexity: 0.9666666666666667\n\nDIFF:\n@@ -285,7 +285,8 @@ class ActivityRegularization(Layer):\n     def __init__(self, activity_regularizer = None):\n         super(ActivityRegularization, self).__init__()\n         if activity_regularizer is not None:\n-            self.cost_update = activity_regularizer(self)\n+            activity_regularizer.set_layer(self)\n+            self.cost_update = activity_regularizer\n \n     def get_output(self, train):\n         return self.get_input(train)\n\n@@ -91,7 +91,8 @@ class Model(object):\n \n         if hasattr(self, 'cost_updates'):\n             for u in self.cost_updates:\n-                train_loss += u()\n+                if u.modifies_cost:\n+                    train_loss = u(train_loss)\n \n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n \n\n@@ -1,45 +1,117 @@\n from __future__ import absolute_import\n-import theano\n import theano.tensor as T\n-import numpy as np\n-\n-def l1(l=.01):\n-    def l1wrap(g, p):\n-        g += T.sgn(p) * l\n-        return g\n-    return l1wrap\n-\n-def l2(l=.01):\n-    def l2wrap(g, p):\n-        g += p * l\n-        return g\n-    return l2wrap\n-\n-def l1l2(l1=.01, l2=.01):\n-    def l1l2wrap(g, p):\n-        g += T.sgn(p) * l1\n-        g += p * l2\n-        return g\n-    return l1l2wrap\n-\n-def identity(g, p):\n-    return g\n \n \n-def activity_l1(l=.01):\n-    # activity dependent l1 norm\n-    def l1wrap(layer):\n-        # needs to be wrapped twice because input is not present during instantiation\n-        def l1wrap_wrap():\n-            return l * T.sum(T.mean(abs(layer.get_output(True)), axis=0))\n-        return l1wrap_wrap\n-    return l1wrap\n+class Regularizer(object):\n+    def __init__(self):\n+        self.modifies_gradient = False\n+        self.modifies_cost = False\n \n-def activity_l2(l=.01):\n-    # activity dependent l1 norm\n-    def l2wrap(layer):\n-        # needs to be wrapped twice because input is not present during instantiation\n-        def l2wrap_wrap():\n-            return l * T.sum(T.mean(layer.get_output(True)**2, axis=0))\n-        return l2wrap_wrap\n-    return l2wrap\n+    def update_gradient(self, gradient, params):\n+        raise NotImplementedError\n+\n+    def update_cost(self, cost):\n+        raise NotImplementedError\n+\n+\n+class RegularizerWeightsL1(Regularizer):\n+    def __init__(self, l=0.01):\n+        super(Regularizer, self).__init__()\n+\n+        self.l = l\n+        self.modifies_gradient = True\n+\n+    def update_gradient(self, gradient, params):\n+        gradient += T.sgn(params) * self.l\n+        return gradient\n+\n+    def __call__(self, gradient, params):\n+        return self.update_gradient(gradient, params)\n+\n+\n+class RegularizerWeightsL2(Regularizer):\n+    def __init__(self, l=0.01):\n+        super(Regularizer, self).__init__()\n+\n+        self.l = l\n+        self.modifies_gradient = True\n+\n+    def update_gradient(self, gradient, params):\n+        gradient += params * self.l\n+        return gradient\n+\n+    def __call__(self, gradient, params):\n+        return self.update_gradient(gradient, params)\n+\n+\n+class RegularizerWeightsL1L2(Regularizer):\n+    def __init__(self, l1=0.01, l2=0.01):\n+        super(Regularizer, self).__init__()\n+\n+        self.l1 = l1\n+        self.l2 = l2\n+        self.modifies_gradient = True\n+\n+    def update_gradient(self, gradient, params):\n+        gradient += params * self.l2\n+        gradient += T.sgn(params) * self.l1\n+        return gradient\n+\n+    def __call__(self, gradient, params):\n+        return self.update_gradient(gradient, params)\n+\n+class RegularizerIdentity(Regularizer):\n+    def __init__(self):\n+        super(Regularizer, self).__init__()\n+\n+        self.modifies_gradient = True\n+\n+    def update_gradient(self, gradient, params):\n+        return gradient\n+\n+    def __call__(self, gradient, params):\n+        return self.update_gradient(gradient, params)\n+\n+\n+class RegularizerActivityL1(Regularizer):\n+    def __init__(self, l = 0.01):\n+        super(Regularizer, self).__init__()\n+\n+        self.l = l\n+        self.layer = None\n+        self.modifies_cost = True\n+\n+    def set_layer(self, layer):\n+        self.layer = layer\n+\n+    def update_cost(self, cost):\n+        return cost + self.l * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n+\n+    def __call__(self, cost):\n+        return self.update_cost(cost)\n+\n+class RegularizerActivityL2(Regularizer):\n+    def __init__(self, l = 0.01):\n+        super(Regularizer, self).__init__()\n+\n+        self.l = l\n+        self.layer = None\n+        self.modifies_cost = True\n+\n+    def set_layer(self, layer):\n+        self.layer = layer\n+\n+    def update_cost(self, cost):\n+        return cost + self.l * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n+\n+    def __call__(self, cost):\n+        return self.update_cost(cost)\n+\n+#old style variables for backwards compatibility\n+l1 = RegularizerWeightsL1\n+l2 = RegularizerWeightsL2\n+l1l2 = RegularizerWeightsL1L2\n+identity = RegularizerIdentity()\n+\n+activity_l1 = RegularizerActivityL1\n+activity_l2 = RegularizerActivityL2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#182c82743372306e684002940386c32bffc47442", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 53 | Files Changed: 2 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 53 | Churn Cumulative: 106 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,53 +0,0 @@\n-__author__ = 'mccolgan'\n-\n-\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.convolutional import Convolution2D\n-from keras.layers.core import Dense, Flatten, ActivityRegularization\n-from keras.utils import np_utils\n-from keras.regularizers import l2, activity_l1\n-\n-import numpy as np\n-\n-np.random.seed(1337)\n-\n-max_train_samples = 128*8\n-max_test_samples = 1000\n-\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape((-1, 28*28))\n-X_test = X_test.reshape((-1, 28*28))\n-\n-nb_classes = len(np.unique(y_train))\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-model_noreg = Sequential()\n-model_noreg.add(Flatten())\n-model_noreg.add(Dense(28*28, 20, activation='sigmoid'))\n-model_noreg.add(Dense(20, 10, activation='sigmoid'))\n-\n-model_noreg.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-model_noreg.fit(X_train, Y_train)\n-\n-score_noreg = model_noreg.evaluate(X_test, Y_test)\n-score_train_noreg = model_noreg.evaluate(X_train, Y_train)\n-\n-model_reg = Sequential()\n-model_reg.add(Flatten())\n-model_reg.add(Dense(28*28, 20, activation='sigmoid'))\n-model_reg.add(ActivityRegularization(activity_l1(0.1)))\n-model_reg.add(Dense(20, 10, activation='sigmoid'))\n-\n-model_reg.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-model_reg.fit(X_train, Y_train)\n-\n-score_reg = model_reg.evaluate(X_test, Y_test)\n-score_train_reg = model_reg.evaluate(X_train, Y_train)\n-\n-print\n-print\n-print \"Overfitting without regularisation: %f - %f = %f\" % ( score_noreg , score_train_noreg , score_noreg-score_train_noreg)\n-print \"Overfitting with regularisation: %f - %f = %f\" % ( score_reg , score_train_reg , score_reg-score_train_reg)\n\nBinary files /dev/null and b/tests/manual/check_activity_reg.py differ\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
