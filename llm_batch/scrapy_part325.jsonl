{"custom_id": "scrapy#02ae1deaf499e79dc8ececf4d5dcea6daef0ade0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 63 | Churn Cumulative: 446 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ import pickle\n \n from queuelib import queue\n \n+from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n \n@@ -123,30 +124,60 @@ def _pickle_serialize(obj):\n         raise ValueError(str(e)) from e\n \n \n-PickleFifoDiskQueueNonRequest = _serializable_queue(\n+_PickleFifoSerializationDiskQueue = _serializable_queue(\n     _with_mkdir(queue.FifoDiskQueue),\n     _pickle_serialize,\n     pickle.loads\n )\n-PickleLifoDiskQueueNonRequest = _serializable_queue(\n+_PickleLifoSerializationDiskQueue = _serializable_queue(\n     _with_mkdir(queue.LifoDiskQueue),\n     _pickle_serialize,\n     pickle.loads\n )\n-MarshalFifoDiskQueueNonRequest = _serializable_queue(\n+_MarshalFifoSerializationDiskQueue = _serializable_queue(\n     _with_mkdir(queue.FifoDiskQueue),\n     marshal.dumps,\n     marshal.loads\n )\n-MarshalLifoDiskQueueNonRequest = _serializable_queue(\n+_MarshalLifoSerializationDiskQueue = _serializable_queue(\n     _with_mkdir(queue.LifoDiskQueue),\n     marshal.dumps,\n     marshal.loads\n )\n \n-PickleFifoDiskQueue = _scrapy_serialization_queue(PickleFifoDiskQueueNonRequest)\n-PickleLifoDiskQueue = _scrapy_serialization_queue(PickleLifoDiskQueueNonRequest)\n-MarshalFifoDiskQueue = _scrapy_serialization_queue(MarshalFifoDiskQueueNonRequest)\n-MarshalLifoDiskQueue = _scrapy_serialization_queue(MarshalLifoDiskQueueNonRequest)\n+# public queue classes\n+PickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQueue)\n+PickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)\n+MarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)\n+MarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)\n FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)\n LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)\n+\n+\n+# deprecated queue classes\n+_subclass_warn_message = \"{cls} inherits from deprecated class {old}\"\n+_instance_warn_message = \"{cls} is deprecated\"\n+PickleFifoDiskQueueNonRequest = create_deprecated_class(\n+    name=\"PickleFifoDiskQueueNonRequest\",\n+    new_class=_PickleFifoSerializationDiskQueue,\n+    subclass_warn_message=_subclass_warn_message,\n+    instance_warn_message=_instance_warn_message,\n+)\n+PickleLifoDiskQueueNonRequest = create_deprecated_class(\n+    name=\"PickleLifoDiskQueueNonRequest\",\n+    new_class=_PickleLifoSerializationDiskQueue,\n+    subclass_warn_message=_subclass_warn_message,\n+    instance_warn_message=_instance_warn_message,\n+)\n+MarshalFifoDiskQueueNonRequest = create_deprecated_class(\n+    name=\"MarshalFifoDiskQueueNonRequest\",\n+    new_class=_MarshalFifoSerializationDiskQueue,\n+    subclass_warn_message=_subclass_warn_message,\n+    instance_warn_message=_instance_warn_message,\n+)\n+MarshalLifoDiskQueueNonRequest = create_deprecated_class(\n+    name=\"MarshalLifoDiskQueueNonRequest\",\n+    new_class=_MarshalLifoSerializationDiskQueue,\n+    subclass_warn_message=_subclass_warn_message,\n+    instance_warn_message=_instance_warn_message,\n+)\n\n@@ -3,10 +3,10 @@ import sys\n \n from queuelib.tests import test_queue as t\n from scrapy.squeues import (\n-    MarshalFifoDiskQueueNonRequest as MarshalFifoDiskQueue,\n-    MarshalLifoDiskQueueNonRequest as MarshalLifoDiskQueue,\n-    PickleFifoDiskQueueNonRequest as PickleFifoDiskQueue,\n-    PickleLifoDiskQueueNonRequest as PickleLifoDiskQueue\n+    _MarshalFifoSerializationDiskQueue,\n+    _MarshalLifoSerializationDiskQueue,\n+    _PickleFifoSerializationDiskQueue,\n+    _PickleLifoSerializationDiskQueue,\n )\n from scrapy.item import Item, Field\n from scrapy.http import Request\n@@ -53,7 +53,7 @@ class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n     chunksize = 100000\n \n     def queue(self):\n-        return MarshalFifoDiskQueue(self.qpath, chunksize=self.chunksize)\n+        return _MarshalFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)\n \n \n class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n@@ -77,7 +77,7 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n     chunksize = 100000\n \n     def queue(self):\n-        return PickleFifoDiskQueue(self.qpath, chunksize=self.chunksize)\n+        return _PickleFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)\n \n     def test_serialize_item(self):\n         q = self.queue()\n@@ -155,13 +155,13 @@ class LifoDiskQueueTestMixin:\n class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n \n     def queue(self):\n-        return MarshalLifoDiskQueue(self.qpath)\n+        return _MarshalLifoSerializationDiskQueue(self.qpath)\n \n \n class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n \n     def queue(self):\n-        return PickleLifoDiskQueue(self.qpath)\n+        return _PickleLifoSerializationDiskQueue(self.qpath)\n \n     def test_serialize_item(self):\n         q = self.queue()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f500342c8ad4674b191e1fab0d1b2ac944d7d3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 605 | Contributors (this commit): 29 | Commits (past 90d): 4 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -32,6 +32,7 @@ install_requires = [\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'h2>=3.0,<4.0',\n+    'setuptools',\n ]\n extras_require = {}\n cpython_dependencies = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19c7415aae1678631d5ca115a13094b6bd70f245", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 30 | Files Changed: 3 | Hunks: 20 | Methods Changed: 19 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 71 | Churn Cumulative: 963 | Contributors (this commit): 20 | Commits (past 90d): 13 | Contributors (cumulative): 28 | DMM Complexity: 0.5625\n\nDIFF:\n@@ -98,7 +98,7 @@ def get_retry_request(\n             {'request': request, 'retry_times': retry_times, 'reason': reason},\n             extra={'spider': spider}\n         )\n-        new_request = request.copy()\n+        new_request: Request = request.copy()\n         new_request.meta['retry_times'] = retry_times\n         new_request.dont_filter = True\n         if priority_adjust is None:\n\n@@ -4,22 +4,38 @@ requests in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+from typing import Callable, List, Optional, Type, TypeVar, Union\n+\n from w3lib.url import safe_url_string\n \n+from scrapy.http.common import obsolete_setter\n from scrapy.http.headers import Headers\n+from scrapy.utils.curl import curl_to_request_kwargs\n from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n-from scrapy.http.common import obsolete_setter\n-from scrapy.utils.curl import curl_to_request_kwargs\n+\n+\n+RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n \n class Request(object_ref):\n-\n-    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n-                 cookies=None, meta=None, encoding='utf-8', priority=0,\n-                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):\n-\n+    def __init__(\n+        self,\n+        url: str,\n+        callback: Optional[Callable] = None,\n+        method: str = \"GET\",\n+        headers: Optional[dict] = None,\n+        body: Optional[Union[bytes, str]] = None,\n+        cookies: Optional[Union[dict, List[dict]]]=None,\n+        meta: Optional[dict] = None,\n+        encoding: str = \"utf-8\",\n+        priority: int = 0,\n+        dont_filter: bool = False,\n+        errback: Optional[Callable] = None,\n+        flags: Optional[List[str]] = None,\n+        cb_kwargs: Optional[dict] = None,\n+    ) -> None:\n         self._encoding = encoding  # this one has to be set first\n         self.method = str(method).upper()\n         self._set_url(url)\n@@ -44,23 +60,23 @@ class Request(object_ref):\n         self.flags = [] if flags is None else list(flags)\n \n     @property\n-    def cb_kwargs(self):\n+    def cb_kwargs(self) -> dict:\n         if self._cb_kwargs is None:\n             self._cb_kwargs = {}\n         return self._cb_kwargs\n \n     @property\n-    def meta(self):\n+    def meta(self) -> dict:\n         if self._meta is None:\n             self._meta = {}\n         return self._meta\n \n-    def _get_url(self):\n+    def _get_url(self) -> str:\n         return self._url\n \n-    def _set_url(self, url):\n+    def _set_url(self, url: str) -> None:\n         if not isinstance(url, str):\n-            raise TypeError(f'Request url must be str or unicode, got {type(url).__name__}')\n+            raise TypeError(f\"Request url must be str, got {type(url).__name__}\")\n \n         s = safe_url_string(url, self.encoding)\n         self._url = escape_ajax(s)\n@@ -74,34 +90,28 @@ class Request(object_ref):\n \n     url = property(_get_url, obsolete_setter(_set_url, 'url'))\n \n-    def _get_body(self):\n+    def _get_body(self) -> bytes:\n         return self._body\n \n-    def _set_body(self, body):\n-        if body is None:\n-            self._body = b''\n-        else:\n-            self._body = to_bytes(body, self.encoding)\n+    def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n+        self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n \n     body = property(_get_body, obsolete_setter(_set_body, 'body'))\n \n     @property\n-    def encoding(self):\n+    def encoding(self) -> str:\n         return self._encoding\n \n-    def __str__(self):\n+    def __str__(self) -> str:\n         return f\"<{self.method} {self.url}>\"\n \n     __repr__ = __str__\n \n-    def copy(self):\n-        \"\"\"Return a copy of this Request\"\"\"\n+    def copy(self) -> RequestTypeVar:\n         return self.replace()\n \n-    def replace(self, *args, **kwargs):\n-        \"\"\"Create a new Request with the same attributes except for those\n-        given new values.\n-        \"\"\"\n+    def replace(self, *args, **kwargs) -> RequestTypeVar:\n+        \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                   'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n             kwargs.setdefault(x, getattr(self, x))\n@@ -109,7 +119,9 @@ class Request(object_ref):\n         return cls(*args, **kwargs)\n \n     @classmethod\n-    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n+    def from_curl(\n+        cls: Type[RequestTypeVar], curl_command: str, ignore_unknown_options: bool = True, **kwargs\n+    ) -> RequestTypeVar:\n         \"\"\"Create a Request object from a string containing a `cURL\n         <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n         URL, the headers, the cookies and the body. It accepts the same\n@@ -136,7 +148,6 @@ class Request(object_ref):\n \n         To translate a cURL command into a Scrapy request,\n         you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n-\n         \"\"\"\n         request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n         request_kwargs.update(kwargs)\n\n@@ -54,7 +54,7 @@ def _parse_headers_and_cookies(parsed_args):\n     return headers, cookies\n \n \n-def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n+def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool = True) -> dict:\n     \"\"\"Convert a cURL command syntax to Request kwargs.\n \n     :param str curl_command: string containing the curl command\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cec36a9284641bb7b69a2081ad92cd7d4ee25934", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 168 | Lines Deleted: 120 | Files Changed: 6 | Hunks: 38 | Methods Changed: 27 | Complexity Δ (Sum/Max): 37/31 | Churn Δ: 288 | Churn Cumulative: 1481 | Contributors (this commit): 25 | Commits (past 90d): 10 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,17 +4,37 @@ requests in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+import inspect\n+from typing import Optional, Tuple\n+\n from w3lib.url import safe_url_string\n \n+import scrapy\n+from scrapy.http.common import obsolete_setter\n from scrapy.http.headers import Headers\n+from scrapy.utils.curl import curl_to_request_kwargs\n from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n-from scrapy.http.common import obsolete_setter\n-from scrapy.utils.curl import curl_to_request_kwargs\n \n \n class Request(object_ref):\n+    \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n+    executed by the Downloader, thus generating a :class:`Response`.\n+    \"\"\"\n+\n+    attributes: Tuple[str, ...] = (\n+        \"url\", \"callback\", \"method\", \"headers\", \"body\",\n+        \"cookies\", \"meta\", \"encoding\", \"priority\",\n+        \"dont_filter\", \"errback\", \"flags\", \"cb_kwargs\",\n+    )\n+    \"\"\"A tuple of :class:`str` objects containing the name of all public\n+    attributes of the class that are also keyword parameters of the\n+    ``__init__`` method.\n+\n+    Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n+    :func:`~scrapy.utils.request.request_from_dict`.\n+    \"\"\"\n \n     def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                  cookies=None, meta=None, encoding='utf-8', priority=0,\n@@ -99,11 +119,8 @@ class Request(object_ref):\n         return self.replace()\n \n     def replace(self, *args, **kwargs):\n-        \"\"\"Create a new Request with the same attributes except for those\n-        given new values.\n-        \"\"\"\n-        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n-                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n+        \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n+        for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n@@ -136,8 +153,43 @@ class Request(object_ref):\n \n         To translate a cURL command into a Scrapy request,\n         you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n-\n         \"\"\"\n         request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n+\n+    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> dict:\n+        \"\"\"Return a dictionary containing the Request's data.\n+\n+        Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n+\n+        If a spider is given, this method will try to find out the name of the spider methods used as callback\n+        and errback and include them in the output dict, raising an exception if they cannot be found.\n+        \"\"\"\n+        d = {\n+            \"url\": self.url,  # urls are safe (safe_string_url)\n+            \"callback\": _find_method(spider, self.callback) if callable(self.callback) else self.callback,\n+            \"errback\": _find_method(spider, self.errback) if callable(self.errback) else self.errback,\n+            \"headers\": dict(self.headers),\n+        }\n+        for attr in self.attributes:\n+            d.setdefault(attr, getattr(self, attr))\n+        if type(self) is not Request:\n+            d[\"_class\"] = self.__module__ + '.' + self.__class__.__name__\n+        return d\n+\n+\n+def _find_method(obj, func):\n+    \"\"\"Helper function for Request.to_dict\"\"\"\n+    # Only instance methods contain ``__func__``\n+    if obj and hasattr(func, '__func__'):\n+        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n+        for name, obj_func in members:\n+            # We need to use __func__ to access the original function object because instance\n+            # method objects are generated each time attribute is retrieved from instance.\n+            #\n+            # Reference: The standard type hierarchy\n+            # https://docs.python.org/3/reference/datamodel.html\n+            if obj_func.__func__ is func.__func__:\n+                return name\n+    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")\n\n@@ -8,12 +8,16 @@ See documentation in docs/topics/request-response.rst\n import copy\n import json\n import warnings\n+from typing import Tuple\n \n from scrapy.http.request import Request\n from scrapy.utils.deprecate import create_deprecated_class\n \n \n class JsonRequest(Request):\n+\n+    attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n+\n     def __init__(self, *args, **kwargs):\n         dumps_kwargs = copy.deepcopy(kwargs.pop('dumps_kwargs', {}))\n         dumps_kwargs.setdefault('sort_keys', True)\n@@ -36,6 +40,10 @@ class JsonRequest(Request):\n         self.headers.setdefault('Content-Type', 'application/json')\n         self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')\n \n+    @property\n+    def dumps_kwargs(self):\n+        return self._dumps_kwargs\n+\n     def replace(self, *args, **kwargs):\n         body_passed = kwargs.get('body', None) is not None\n         data = kwargs.pop('data', None)\n\n@@ -9,7 +9,7 @@ import pickle\n from queuelib import queue\n \n from scrapy.utils.deprecate import create_deprecated_class\n-from scrapy.utils.reqser import request_to_dict, request_from_dict\n+from scrapy.utils.request import request_from_dict\n \n \n def _with_mkdir(queue_class):\n@@ -68,14 +68,14 @@ def _scrapy_serialization_queue(queue_class):\n             return cls(crawler, key)\n \n         def push(self, request):\n-            request = request_to_dict(request, self.spider)\n+            request = request.to_dict(spider=self.spider)\n             return super().push(request)\n \n         def pop(self):\n             request = super().pop()\n             if not request:\n                 return None\n-            return request_from_dict(request, self.spider)\n+            return request_from_dict(request, spider=self.spider)\n \n         def peek(self):\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n@@ -87,7 +87,7 @@ def _scrapy_serialization_queue(queue_class):\n             request = super().peek()\n             if not request:\n                 return None\n-            return request_from_dict(request, self.spider)\n+            return request_from_dict(request, spider=self.spider)\n \n     return ScrapyRequestQueue\n \n\n@@ -1,95 +1,22 @@\n-\"\"\"\n-Helper functions for serializing (and deserializing) requests.\n-\"\"\"\n-import inspect\n+import warnings\n+from typing import Optional\n \n-from scrapy.http import Request\n-from scrapy.utils.python import to_unicode\n-from scrapy.utils.misc import load_object\n+import scrapy\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.request import request_from_dict as _from_dict\n \n \n-def request_to_dict(request, spider=None):\n-    \"\"\"Convert Request object to a dict.\n-\n-    If a spider is given, it will try to find out the name of the spider method\n-    used in the callback and store that as the callback.\n-    \"\"\"\n-    cb = request.callback\n-    if callable(cb):\n-        cb = _find_method(spider, cb)\n-    eb = request.errback\n-    if callable(eb):\n-        eb = _find_method(spider, eb)\n-    d = {\n-        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n-        'callback': cb,\n-        'errback': eb,\n-        'method': request.method,\n-        'headers': dict(request.headers),\n-        'body': request.body,\n-        'cookies': request.cookies,\n-        'meta': request.meta,\n-        '_encoding': request._encoding,\n-        'priority': request.priority,\n-        'dont_filter': request.dont_filter,\n-        'flags': request.flags,\n-        'cb_kwargs': request.cb_kwargs,\n-    }\n-    if type(request) is not Request:\n-        d['_class'] = request.__module__ + '.' + request.__class__.__name__\n-    return d\n-\n-\n-def request_from_dict(d, spider=None):\n-    \"\"\"Create Request object from a dict.\n-\n-    If a spider is given, it will try to resolve the callbacks looking at the\n-    spider for methods with the same name.\n-    \"\"\"\n-    cb = d['callback']\n-    if cb and spider:\n-        cb = _get_method(spider, cb)\n-    eb = d['errback']\n-    if eb and spider:\n-        eb = _get_method(spider, eb)\n-    request_cls = load_object(d['_class']) if '_class' in d else Request\n-    return request_cls(\n-        url=to_unicode(d['url']),\n-        callback=cb,\n-        errback=eb,\n-        method=d['method'],\n-        headers=d['headers'],\n-        body=d['body'],\n-        cookies=d['cookies'],\n-        meta=d['meta'],\n-        encoding=d['_encoding'],\n-        priority=d['priority'],\n-        dont_filter=d['dont_filter'],\n-        flags=d.get('flags'),\n-        cb_kwargs=d.get('cb_kwargs'),\n+warnings.warn(\n+    (\"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n+     \" and/or scrapy.utils.request.request_from_dict instead\"),\n+    category=ScrapyDeprecationWarning,\n+    stacklevel=2,\n )\n \n \n-def _find_method(obj, func):\n-    # Only instance methods contain ``__func__``\n-    if obj and hasattr(func, '__func__'):\n-        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n-        for name, obj_func in members:\n-            # We need to use __func__ to access the original\n-            # function object because instance method objects\n-            # are generated each time attribute is retrieved from\n-            # instance.\n-            #\n-            # Reference: The standard type hierarchy\n-            # https://docs.python.org/3/reference/datamodel.html\n-            if obj_func.__func__ is func.__func__:\n-                return name\n-    raise ValueError(f\"Function {func} is not an instance method in: {obj}\")\n+def request_to_dict(request: \"scrapy.Request\", spider: Optional[\"scrapy.Spider\"] = None) -> dict:\n+    return request.to_dict(spider=spider)\n \n \n-def _get_method(obj, name):\n-    name = str(name)\n-    try:\n-        return getattr(obj, name)\n-    except AttributeError:\n-        raise ValueError(f\"Method {name!r} not found in: {obj}\")\n+def request_from_dict(d: dict, spider: Optional[\"scrapy.Spider\"] = None) -> \"scrapy.Request\":\n+    return _from_dict(d, spider=spider)\n\n@@ -11,8 +11,9 @@ from weakref import WeakKeyDictionary\n from w3lib.http import basic_auth_header\n from w3lib.url import canonicalize_url\n \n-from scrapy.http import Request\n+from scrapy import Request, Spider\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n@@ -106,3 +107,27 @@ def referer_str(request: Request) -> Optional[str]:\n     if referrer is None:\n         return referrer\n     return to_unicode(referrer, errors='replace')\n+\n+\n+def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -> Request:\n+    \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n+\n+    If a spider is given, it will try to resolve the callbacks looking at the\n+    spider for methods with the same name.\n+    \"\"\"\n+    request_cls = load_object(d[\"_class\"]) if \"_class\" in d else Request\n+    kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}\n+    if d.get(\"callback\") and spider:\n+        kwargs[\"callback\"] = _get_method(spider, d[\"callback\"])\n+    if d.get(\"errback\") and spider:\n+        kwargs[\"errback\"] = _get_method(spider, d[\"errback\"])\n+    return request_cls(**kwargs)\n+\n+\n+def _get_method(obj, name):\n+    \"\"\"Helper function for request_from_dict\"\"\"\n+    name = str(name)\n+    try:\n+        return getattr(obj, name)\n+    except AttributeError:\n+        raise ValueError(f\"Method {name!r} not found in: {obj}\")\n\n@@ -1,8 +1,16 @@\n+import sys\n import unittest\n+import warnings\n+from contextlib import suppress\n \n-from scrapy.http import Request, FormRequest\n-from scrapy.spiders import Spider\n-from scrapy.utils.reqser import request_to_dict, request_from_dict\n+from scrapy import Spider, Request\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.http import FormRequest, JsonRequest\n+from scrapy.utils.request import request_from_dict\n+\n+\n+class CustomRequest(Request):\n+    pass\n \n \n class RequestSerializationTest(unittest.TestCase):\n@@ -27,7 +35,8 @@ class RequestSerializationTest(unittest.TestCase):\n             priority=20,\n             meta={'a': 'b'},\n             cb_kwargs={'k': 'v'},\n-            flags=['testFlag'])\n+            flags=['testFlag'],\n+        )\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_latin1_body(self):\n@@ -39,7 +48,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_serializes_ok(r)\n \n     def _assert_serializes_ok(self, request, spider=None):\n-        d = request_to_dict(request, spider=spider)\n+        d = request.to_dict(spider=spider)\n         request2 = request_from_dict(d, spider=spider)\n         self._assert_same_request(request, request2)\n \n@@ -54,16 +63,21 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertEqual(r1.cookies, r2.cookies)\n         self.assertEqual(r1.meta, r2.meta)\n         self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n+        self.assertEqual(r1.encoding, r2.encoding)\n         self.assertEqual(r1._encoding, r2._encoding)\n         self.assertEqual(r1.priority, r2.priority)\n         self.assertEqual(r1.dont_filter, r2.dont_filter)\n         self.assertEqual(r1.flags, r2.flags)\n+        if isinstance(r1, JsonRequest):\n+            self.assertEqual(r1.dumps_kwargs, r2.dumps_kwargs)\n \n     def test_request_class(self):\n-        r = FormRequest(\"http://www.example.com\")\n-        self._assert_serializes_ok(r, spider=self.spider)\n-        r = CustomRequest(\"http://www.example.com\")\n-        self._assert_serializes_ok(r, spider=self.spider)\n+        r1 = FormRequest(\"http://www.example.com\")\n+        self._assert_serializes_ok(r1, spider=self.spider)\n+        r2 = CustomRequest(\"http://www.example.com\")\n+        self._assert_serializes_ok(r2, spider=self.spider)\n+        r3 = JsonRequest(\"http://www.example.com\", dumps_kwargs={\"indent\": 4})\n+        self._assert_serializes_ok(r3, spider=self.spider)\n \n     def test_callback_serialization(self):\n         r = Request(\"http://www.example.com\", callback=self.spider.parse_item,\n@@ -75,7 +89,7 @@ class RequestSerializationTest(unittest.TestCase):\n                     callback=self.spider.parse_item_reference,\n                     errback=self.spider.handle_error_reference)\n         self._assert_serializes_ok(r, spider=self.spider)\n-        request_dict = request_to_dict(r, self.spider)\n+        request_dict = r.to_dict(spider=self.spider)\n         self.assertEqual(request_dict['callback'], 'parse_item_reference')\n         self.assertEqual(request_dict['errback'], 'handle_error_reference')\n \n@@ -84,7 +98,7 @@ class RequestSerializationTest(unittest.TestCase):\n                     callback=self.spider._TestSpider__parse_item_reference,\n                     errback=self.spider._TestSpider__handle_error_reference)\n         self._assert_serializes_ok(r, spider=self.spider)\n-        request_dict = request_to_dict(r, self.spider)\n+        request_dict = r.to_dict(spider=self.spider)\n         self.assertEqual(request_dict['callback'],\n                          '_TestSpider__parse_item_reference')\n         self.assertEqual(request_dict['errback'],\n@@ -110,18 +124,16 @@ class RequestSerializationTest(unittest.TestCase):\n \n     def test_unserializable_callback1(self):\n         r = Request(\"http://www.example.com\", callback=lambda x: x)\n-        self.assertRaises(ValueError, request_to_dict, r)\n-        self.assertRaises(ValueError, request_to_dict, r, spider=self.spider)\n+        self.assertRaises(ValueError, r.to_dict, spider=self.spider)\n \n     def test_unserializable_callback2(self):\n         r = Request(\"http://www.example.com\", callback=self.spider.parse_item)\n-        self.assertRaises(ValueError, request_to_dict, r)\n+        self.assertRaises(ValueError, r.to_dict, spider=None)\n \n     def test_unserializable_callback3(self):\n         \"\"\"Parser method is removed or replaced dynamically.\"\"\"\n \n         class MySpider(Spider):\n-\n             name = 'my_spider'\n \n             def parse(self, response):\n@@ -130,7 +142,35 @@ class RequestSerializationTest(unittest.TestCase):\n         spider = MySpider()\n         r = Request(\"http://www.example.com\", callback=spider.parse)\n         setattr(spider, 'parse', None)\n-        self.assertRaises(ValueError, request_to_dict, r, spider=spider)\n+        self.assertRaises(ValueError, r.to_dict, spider=spider)\n+\n+    def test_callback_not_available(self):\n+        \"\"\"Callback method is not available in the spider passed to from_dict\"\"\"\n+        spider = TestSpiderDelegation()\n+        r = Request(\"http://www.example.com\", callback=spider.delegated_callback)\n+        d = r.to_dict(spider=spider)\n+        self.assertRaises(ValueError, request_from_dict, d, spider=Spider(\"foo\"))\n+\n+\n+class DeprecatedMethodsRequestSerializationTest(RequestSerializationTest):\n+    def _assert_serializes_ok(self, request, spider=None):\n+        with warnings.catch_warnings(record=True) as caught:\n+            warnings.simplefilter(\"always\")\n+            with suppress(KeyError):\n+                del sys.modules[\"scrapy.utils.reqser\"]  # delete module to reset the deprecation warning\n+\n+            from scrapy.utils.reqser import request_from_dict as _from_dict, request_to_dict as _to_dict\n+\n+            request_copy = _from_dict(_to_dict(request, spider), spider)\n+            self._assert_same_request(request, request_copy)\n+\n+            self.assertEqual(len(caught), 1)\n+            self.assertTrue(issubclass(caught[0].category, ScrapyDeprecationWarning))\n+            self.assertEqual(\n+                \"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n+                \" and/or scrapy.utils.request.request_from_dict instead\",\n+                str(caught[0].message),\n+            )\n \n \n class TestSpiderMixin:\n@@ -177,7 +217,3 @@ class TestSpider(Spider, TestSpiderMixin):\n \n     def __parse_item_private(self, response):\n         pass\n-\n-\n-class CustomRequest(Request):\n-    pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5b1ee810167266fcd259f263dbfc0fe0204761a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 26 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 51 | Churn Cumulative: 1981 | Contributors (this commit): 34 | Commits (past 90d): 13 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from pathlib import Path\n \n import pytest\n+from twisted.web.http import H2_ENABLED\n \n from scrapy.utils.reactor import install_reactor\n \n@@ -25,6 +26,14 @@ for line in open('tests/ignores.txt'):\n     if file_path and file_path[0] != '#':\n         collect_ignore.append(file_path)\n \n+if not H2_ENABLED:\n+    collect_ignore.extend(\n+        (\n+            'scrapy/core/downloader/handlers/http2.py',\n+            *_py_files(\"scrapy/core/http2\"),\n+        )\n+    )\n+\n \n @pytest.fixture()\n def chdir(tmpdir):\n\n@@ -19,7 +19,7 @@ def has_environment_marker_platform_impl_support():\n \n \n install_requires = [\n-    'Twisted[http2]>=17.9.0',\n+    'Twisted>=17.9.0',\n     'cryptography>=2.0',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n@@ -31,7 +31,6 @@ install_requires = [\n     'zope.interface>=4.1.3',\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n-    'h2>=3.0,<4.0',\n     'setuptools',\n ]\n extras_require = {}\n\n@@ -1,5 +1,5 @@\n import json\n-from unittest import mock\n+from unittest import mock, skipIf\n \n from pytest import mark\n from testfixtures import LogCapture\n@@ -7,8 +7,8 @@ from twisted.internet import defer, error, reactor\n from twisted.trial import unittest\n from twisted.web import server\n from twisted.web.error import SchemeNotSupported\n+from twisted.web.http import H2_ENABLED\n \n-from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.misc import create_instance\n@@ -21,11 +21,17 @@ from tests.test_downloader_handlers import (\n )\n \n \n+@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2TestCase(Https11TestCase):\n+\n     scheme = 'https'\n-    download_handler_cls = H2DownloadHandler\n     HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n \n+    @classmethod\n+    def setUpClass(cls):\n+        from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+        cls.download_handler_cls = H2DownloadHandler\n+\n     def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n         d = self.download_request(request, Spider(\"foo\"))\n@@ -187,9 +193,14 @@ class Https2InvalidDNSPattern(Https2TestCase):\n         super(Https2InvalidDNSPattern, self).setUp()\n \n \n+@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2CustomCiphers(Https11CustomCiphers):\n     scheme = 'https'\n-    download_handler_cls = H2DownloadHandler\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+        cls.download_handler_cls = H2DownloadHandler\n \n \n class Http2MockServerTestCase(Http11MockServerTestCase):\n@@ -201,6 +212,7 @@ class Http2MockServerTestCase(Http11MockServerTestCase):\n     }\n \n \n+@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2ProxyTestCase(Http11ProxyTestCase):\n     # only used for HTTPS tests\n     keyfile = 'keys/localhost.key'\n@@ -209,9 +221,13 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n     scheme = 'https'\n     host = u'127.0.0.1'\n \n-    download_handler_cls = H2DownloadHandler\n     expected_http_proxy_request_body = b'/'\n \n+    @classmethod\n+    def setUpClass(cls):\n+        from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+        cls.download_handler_cls = H2DownloadHandler\n+\n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n         self.port = reactor.listenSSL(\n\n@@ -5,10 +5,9 @@ import re\n import shutil\n import string\n from ipaddress import IPv4Address\n-from unittest import mock\n+from unittest import mock, skipIf\n from urllib.parse import urlencode\n \n-from h2.exceptions import InvalidBodyLengthError\n from twisted.internet import reactor\n from twisted.internet.defer import CancelledError, Deferred, DeferredList, inlineCallbacks\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n@@ -17,12 +16,10 @@ from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certif\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n from twisted.web.client import ResponseFailed, URI\n-from twisted.web.http import Request as TxRequest\n+from twisted.web.http import H2_ENABLED, Request as TxRequest\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.static import File\n \n-from scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\n-from scrapy.core.http2.stream import InactiveStreamClosed, InvalidHostname\n from scrapy.http import Request, Response, JsonRequest\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -173,6 +170,7 @@ def get_client_certificate(key_file, certificate_file) -> PrivateCertificate:\n     return PrivateCertificate.loadPEM(pem)\n \n \n+@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2ClientProtocolTestCase(TestCase):\n     scheme = 'https'\n     key_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.key')\n@@ -220,6 +218,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         uri = URI.fromBytes(bytes(self.get_url('/'), 'utf-8'))\n \n         self.conn_closed_deferred = Deferred()\n+        from scrapy.core.http2.protocol import H2ClientFactory\n         h2_client_factory = H2ClientFactory(uri, Settings(), self.conn_closed_deferred)\n         client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)\n         self.client = yield client_endpoint.connect(h2_client_factory)\n@@ -426,6 +425,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         def assert_failure(failure: Failure):\n             self.assertTrue(len(failure.value.reasons) > 0)\n+            from h2.exceptions import InvalidBodyLengthError\n             self.assertTrue(any(\n                 isinstance(error, InvalidBodyLengthError)\n                 for error in failure.value.reasons\n@@ -511,6 +511,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         def assert_inactive_stream(failure):\n             self.assertIsNotNone(failure.check(ResponseFailed))\n+            from scrapy.core.http2.stream import InactiveStreamClosed\n             self.assertTrue(any(\n                 isinstance(e, InactiveStreamClosed)\n                 for e in failure.value.reasons\n@@ -596,6 +597,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         request = Request(url)\n \n         def assert_invalid_hostname(failure: Failure):\n+            from scrapy.core.http2.stream import InvalidHostname\n             self.assertIsNotNone(failure.check(InvalidHostname))\n             error_msg = str(failure.value)\n             self.assertIn('localhost', error_msg)\n@@ -633,6 +635,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         def assert_timeout_error(failure: Failure):\n             for err in failure.value.reasons:\n+                from scrapy.core.http2.protocol import H2ClientProtocol\n                 if isinstance(err, TimeoutError):\n                     self.assertIn(f\"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s\", str(err))\n                     break\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee682af3b06d48815dbdaa27c1177b94aaf679e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2287 | Contributors (this commit): 31 | Commits (past 90d): 4 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -98,8 +98,9 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     with this endpoint comes from the pool and a CONNECT has already been issued\n     for it.\n     \"\"\"\n-\n-    _responseMatcher = re.compile(br'HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,32})')\n+    _truncatedLength = 1000\n+    _responseAnswer = r'HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,' + str(_truncatedLength) + r'})'\n+    _responseMatcher = re.compile(_responseAnswer.encode())\n \n     def __init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n@@ -144,7 +145,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n                 extra = {'status': int(respm.group('status')),\n                          'reason': respm.group('reason').strip()}\n             else:\n-                extra = rcvd_bytes[:32]\n+                extra = rcvd_bytes[:self._truncatedLength]\n             self._tunnelReadyDeferred.errback(\n                 TunnelError('Could not open CONNECT tunnel with proxy '\n                             f'{self._host}:{self._port} [{extra!r}]')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#52d0df5f989903d46e6de4878e1d6a0e87a2c803", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 27 | Files Changed: 4 | Hunks: 31 | Methods Changed: 26 | Complexity Δ (Sum/Max): 12/10 | Churn Δ: 134 | Churn Cumulative: 1782 | Contributors (this commit): 31 | Commits (past 90d): 4 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from w3lib.http import headers_dict_to_raw\n-from scrapy.utils.datatypes import CaselessDict\n+\n+from scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\n from scrapy.utils.python import to_unicode\n \n \n@@ -76,13 +77,13 @@ class Headers(CaselessDict):\n         return headers_dict_to_raw(self)\n \n     def to_unicode_dict(self):\n-        \"\"\" Return headers as a CaselessDict with unicode keys\n+        \"\"\" Return headers as a CaseInsensitiveDict with unicode keys\n         and unicode values. Multiple values are joined with ','.\n         \"\"\"\n-        return CaselessDict(\n-            (to_unicode(key, encoding=self.encoding),\n-             to_unicode(b','.join(value), encoding=self.encoding))\n-            for key, value in self.items())\n+        return CaseInsensitiveDict(\n+            (to_unicode(key, encoding=self.encoding), to_unicode(b','.join(value), encoding=self.encoding))\n+            for key, value in self.items()\n+        )\n \n     def __copy__(self):\n         return self.__class__(self)\n\n@@ -23,7 +23,7 @@ from scrapy.http import Request\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.boto import is_botocore_available\n-from scrapy.utils.datatypes import CaselessDict\n+from scrapy.utils.datatypes import CaseInsensitiveDict\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import md5sum\n@@ -143,7 +143,7 @@ class S3FilesStore:\n         \"\"\" Convert headers to botocore keyword agruments.\n         \"\"\"\n         # This is required while we need to support both boto and botocore.\n-        mapping = CaselessDict({\n+        mapping = CaseInsensitiveDict({\n             'Content-Type': 'ContentType',\n             'Cache-Control': 'CacheControl',\n             'Content-Disposition': 'ContentDisposition',\n\n@@ -6,14 +6,30 @@ This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n import collections\n+import warnings\n import weakref\n from collections.abc import Mapping\n+from typing import Any, AnyStr\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n class CaselessDict(dict):\n \n     __slots__ = ()\n \n+    def __new__(cls, *args, **kwargs):\n+        from scrapy.http.headers import Headers\n+\n+        if issubclass(cls, CaselessDict) and not issubclass(cls, Headers):\n+            warnings.warn(\n+                \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n+                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+        return super().__new__(cls, *args, **kwargs)\n+\n     def __init__(self, seq=None):\n         super().__init__()\n         if seq:\n@@ -63,6 +79,36 @@ class CaselessDict(dict):\n         return dict.pop(self, self.normkey(key), *args)\n \n \n+class CaseInsensitiveDict(collections.UserDict):\n+    \"\"\"A dict-like structure that accepts strings or bytes as keys and allows case-insensitive lookups.\n+\n+    It also allows overriding key and value normalization by defining custom `normkey` and `normvalue` methods.\n+    \"\"\"\n+\n+    def __getitem__(self, key: AnyStr) -> Any:\n+        return super().__getitem__(self.normkey(key))\n+\n+    def __setitem__(self, key: AnyStr, value: Any) -> None:\n+        super().__setitem__(self.normkey(key), self.normvalue(value))\n+\n+    def __delitem__(self, key: AnyStr) -> None:\n+        super().__delitem__(self.normkey(key))\n+\n+    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n+        return super().__contains__(self.normkey(key))\n+\n+    def normkey(self, key: AnyStr) -> AnyStr:\n+        \"\"\"Method to normalize dictionary key access\"\"\"\n+        return key.lower()\n+\n+    def normvalue(self, value: Any) -> Any:\n+        \"\"\"Method to normalize values prior to be set\"\"\"\n+        return value\n+\n+    def __repr__(self) -> str:\n+        return f\"<{self.__class__.__name__}: {super().__repr__()}>\"\n+\n+\n class LocalCache(collections.OrderedDict):\n     \"\"\"Dictionary with a finite number of keys.\n \n\n@@ -1,26 +1,34 @@\n import copy\n import unittest\n+import warnings\n from collections.abc import Mapping, MutableMapping\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n-from scrapy.utils.datatypes import CaselessDict, LocalCache, LocalWeakReferencedCache, SequenceExclude\n+from scrapy.utils.datatypes import (\n+    CaseInsensitiveDict,\n+    CaselessDict,\n+    LocalCache,\n+    LocalWeakReferencedCache,\n+    SequenceExclude,\n+)\n from scrapy.utils.python import garbage_collect\n \n \n __doctests__ = ['scrapy.utils.datatypes']\n \n \n-class CaselessDictTest(unittest.TestCase):\n+class CaseInsensitiveDictMixin:\n \n     def test_init_dict(self):\n         seq = {'red': 1, 'black': 3}\n-        d = CaselessDict(seq)\n+        d = self.dict_class(seq)\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n     def test_init_pair_sequence(self):\n         seq = (('red', 1), ('black', 3))\n-        d = CaselessDict(seq)\n+        d = self.dict_class(seq)\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n@@ -39,7 +47,7 @@ class CaselessDictTest(unittest.TestCase):\n                 return len(self._d)\n \n         seq = MyMapping(red=1, black=3)\n-        d = CaselessDict(seq)\n+        d = self.dict_class(seq)\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n@@ -64,12 +72,12 @@ class CaselessDictTest(unittest.TestCase):\n                 return len(self._d)\n \n         seq = MyMutableMapping(red=1, black=3)\n-        d = CaselessDict(seq)\n+        d = self.dict_class(seq)\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n     def test_caseless(self):\n-        d = CaselessDict()\n+        d = self.dict_class()\n         d['key_Lower'] = 1\n         self.assertEqual(d['KEy_loWer'], 1)\n         self.assertEqual(d.get('KEy_loWer'), 1)\n@@ -79,19 +87,19 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(d.get('key_Lower'), 3)\n \n     def test_delete(self):\n-        d = CaselessDict({'key_lower': 1})\n+        d = self.dict_class({'key_lower': 1})\n         del d['key_LOWER']\n         self.assertRaises(KeyError, d.__getitem__, 'key_LOWER')\n         self.assertRaises(KeyError, d.__getitem__, 'key_lower')\n \n     def test_getdefault(self):\n-        d = CaselessDict()\n+        d = self.dict_class()\n         self.assertEqual(d.get('c', 5), 5)\n         d['c'] = 10\n         self.assertEqual(d.get('c', 5), 10)\n \n     def test_setdefault(self):\n-        d = CaselessDict({'a': 1, 'b': 2})\n+        d = self.dict_class({'a': 1, 'b': 2})\n \n         r = d.setdefault('A', 5)\n         self.assertEqual(r, 1)\n@@ -104,15 +112,15 @@ class CaselessDictTest(unittest.TestCase):\n     def test_fromkeys(self):\n         keys = ('a', 'b')\n \n-        d = CaselessDict.fromkeys(keys)\n+        d = self.dict_class.fromkeys(keys)\n         self.assertEqual(d['A'], None)\n         self.assertEqual(d['B'], None)\n \n-        d = CaselessDict.fromkeys(keys, 1)\n+        d = self.dict_class.fromkeys(keys, 1)\n         self.assertEqual(d['A'], 1)\n         self.assertEqual(d['B'], 1)\n \n-        instance = CaselessDict()\n+        instance = self.dict_class()\n         d = instance.fromkeys(keys)\n         self.assertEqual(d['A'], None)\n         self.assertEqual(d['B'], None)\n@@ -122,18 +130,19 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(d['B'], 1)\n \n     def test_contains(self):\n-        d = CaselessDict()\n+        d = self.dict_class()\n         d['a'] = 1\n         assert 'a' in d\n+        assert 'A' in d\n \n     def test_pop(self):\n-        d = CaselessDict()\n+        d = self.dict_class()\n         d['a'] = 1\n         self.assertEqual(d.pop('A'), 1)\n         self.assertRaises(KeyError, d.pop, 'A')\n \n     def test_normkey(self):\n-        class MyDict(CaselessDict):\n+        class MyDict(self.dict_class):\n             def normkey(self, key):\n                 return key.title()\n \n@@ -142,7 +151,7 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(list(d.keys()), ['Key-One'])\n \n     def test_normvalue(self):\n-        class MyDict(CaselessDict):\n+        class MyDict(self.dict_class):\n             def normvalue(self, value):\n                 if value is not None:\n                     return value + 1\n@@ -171,11 +180,35 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(d.get('key'), 2)\n \n     def test_copy(self):\n-        h1 = CaselessDict({'header1': 'value'})\n+        h1 = self.dict_class({'header1': 'value'})\n         h2 = copy.copy(h1)\n         self.assertEqual(h1, h2)\n         self.assertEqual(h1.get('header1'), h2.get('header1'))\n-        assert isinstance(h2, CaselessDict)\n+        assert isinstance(h2, self.dict_class)\n+\n+\n+class CaseInsensitiveDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n+    dict_class = CaseInsensitiveDict\n+\n+    def test_repr(self):\n+        d = self.dict_class({\"foo\": \"bar\"})\n+        self.assertEqual(repr(d), \"<CaseInsensitiveDict: {'foo': 'bar'}>\")\n+\n+\n+class CaselessDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n+    dict_class = CaselessDict\n+\n+    def test_deprecation_message(self):\n+        with warnings.catch_warnings(record=True) as caught:\n+            self.dict_class({\"foo\": \"bar\"})\n+\n+            self.assertEqual(len(caught), 1)\n+            self.assertTrue(issubclass(caught[0].category, ScrapyDeprecationWarning))\n+            self.assertEqual(\n+                \"scrapy.utils.datatypes.CaselessDict is deprecated,\"\n+                \" please use scrapy.utils.datatypes.CaseInsensitiveDict instead\",\n+                str(caught[0].message),\n+            )\n \n \n class SequenceExcludeTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
