{"custom_id": "keras#e72135b5458cbd34d7f09a6ac4eaf57c9be74d05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 87 | Files Changed: 44 | Hunks: 85 | Methods Changed: 62 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 174 | Churn Cumulative: 8262 | Contributors (this commit): 27 | Commits (past 90d): 131 | Contributors (cumulative): 180 | DMM Complexity: None\n\nDIFF:\n@@ -650,7 +650,7 @@ that handle the parallel processing configuration:\n     `True` if your dataset can be safely pickled.\n - `max_queue_size`: Maximum number of batches to keep in the queue\n     when iterating over the dataset in a multithreaded or\n-    multipricessed setting.\n+    multiprocessed setting.\n     You can reduce this value to reduce the CPU memory consumption of\n     your dataset. It defaults to 10.\n \n\n@@ -325,9 +325,9 @@ def EfficientNet(\n     x = layers.Rescaling(1.0 / 255.0)(x)\n     x = layers.Normalization(axis=bn_axis)(x)\n     if weights == \"imagenet\":\n-        # Note that the normaliztion layer uses square value of STDDEV as the\n+        # Note that the normalization layer uses square value of STDDEV as the\n         # variance for the layer: result = (input - mean) / sqrt(var)\n-        # However, the original implemenetation uses (input - mean) / var to\n+        # However, the original implementation uses (input - mean) / var to\n         # normalize the input, we need to divide another sqrt(var) to match the\n         # original implementation.\n         # See https://github.com/tensorflow/tensorflow/issues/49930 for more\n\n@@ -4,7 +4,7 @@ import re\n import warnings\n \n \n-def _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+def _convert_conv_transpose_padding_args_from_keras_to_jax(\n     kernel_size, stride, dilation_rate, padding, output_padding\n ):\n     \"\"\"Convert the padding arguments from Keras to the ones used by JAX.\n@@ -45,7 +45,7 @@ def _convert_conv_tranpose_padding_args_from_keras_to_jax(\n     return left_pad, right_pad\n \n \n-def _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+def _convert_conv_transpose_padding_args_from_keras_to_torch(\n     kernel_size, stride, dilation_rate, padding, output_padding\n ):\n     \"\"\"Convert the padding arguments from Keras to the ones used by Torch.\n@@ -134,7 +134,7 @@ def compute_conv_transpose_padding_args_for_jax(\n         (\n             pad_left,\n             pad_right,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_jax(\n             kernel_size=kernel_spatial_shape[i],\n             stride=strides_i,\n             dilation_rate=dilation_rate_i,\n@@ -174,7 +174,7 @@ def compute_conv_transpose_padding_args_for_torch(\n         (\n             torch_padding,\n             torch_output_padding,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n             kernel_size=kernel_spatial_shape[i],\n             stride=strides_i,\n             dilation_rate=dilation_rate_i,\n\n@@ -1,8 +1,8 @@\n from keras.src.backend.common.backend_utils import (\n-    _convert_conv_tranpose_padding_args_from_keras_to_jax,\n+    _convert_conv_transpose_padding_args_from_keras_to_jax,\n )\n from keras.src.backend.common.backend_utils import (\n-    _convert_conv_tranpose_padding_args_from_keras_to_torch,\n+    _convert_conv_transpose_padding_args_from_keras_to_torch,\n )\n from keras.src.backend.common.backend_utils import (\n     _get_output_shape_given_tf_padding,\n@@ -22,7 +22,7 @@ class ConvertConvTransposePaddingArgsJAXTest(test_case.TestCase):\n         (\n             left_pad,\n             right_pad,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_jax(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n@@ -37,7 +37,7 @@ class ConvertConvTransposePaddingArgsJAXTest(test_case.TestCase):\n         (\n             left_pad,\n             right_pad,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_jax(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n@@ -54,7 +54,7 @@ class ConvertConvTransposePaddingArgsTorchTest(test_case.TestCase):\n         (\n             torch_padding,\n             torch_output_padding,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n@@ -69,7 +69,7 @@ class ConvertConvTransposePaddingArgsTorchTest(test_case.TestCase):\n         (\n             torch_padding,\n             torch_output_padding,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n@@ -145,7 +145,7 @@ class ComputeConvTransposePaddingArgsForTorchTest(test_case.TestCase):\n         (\n             torch_padding,\n             torch_output_padding,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n@@ -160,7 +160,7 @@ class ComputeConvTransposePaddingArgsForTorchTest(test_case.TestCase):\n         (\n             torch_padding,\n             torch_output_padding,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n@@ -211,7 +211,7 @@ class GetOutputShapeGivenTFPaddingTest(test_case.TestCase):\n     def test_warning_for_inconsistencies(self):\n         \"\"\"Test that a warning is raised for potential inconsistencies\"\"\"\n         with self.assertWarns(Warning):\n-            _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            _convert_conv_transpose_padding_args_from_keras_to_torch(\n                 kernel_size=3,\n                 stride=2,\n                 dilation_rate=1,\n@@ -224,7 +224,7 @@ class GetOutputShapeGivenTFPaddingTest(test_case.TestCase):\n         (\n             torch_padding,\n             torch_output_padding,\n-        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+        ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n             kernel_size=3,\n             stride=2,\n             dilation_rate=1,\n\n@@ -397,7 +397,7 @@ class VariableDtypeShapeNdimRepr(test_case.TestCase):\n         self.assertAllClose(v.__array__(), np.array([1, 2, 3]))\n \n \n-class VariableOpsCorrentnessTest(test_case.TestCase):\n+class VariableOpsCorrectnessTest(test_case.TestCase):\n     \"\"\"Tests for operations on KerasVariable.\"\"\"\n \n     def test_int(self):\n\n@@ -58,7 +58,7 @@ def distribute_variable(value, layout):\n     if layout.is_fully_addressable:\n         return jax.device_put(value, layout)\n     else:\n-        # Need to only distribute the value to local addressible devices, and\n+        # Need to only distribute the value to local addressable devices, and\n         # repack them back into global format.\n         mapping = layout.addressable_devices_indices_map(value.shape)\n         local_values = jax.device_put(\n@@ -94,7 +94,7 @@ def distribute_tensor(tensor, layout):\n     if layout.is_fully_addressable:\n         return jax.device_put(tensor, layout)\n     else:\n-        # Need to only distribute the value to local addressible devices, and\n+        # Need to only distribute the value to local addressable devices, and\n         # repack them back into global format.\n         mapping = layout.addressable_devices_indices_map(tensor.shape)\n         local_values = jax.device_put(\n\n@@ -47,9 +47,9 @@ class JaxDistributionLibTest(testing.TestCase):\n             self.assertEqual(jax_d, converted_jax_device)\n \n     @mock.patch.object(jax.distributed, \"initialize\", return_value=None)\n-    def test_initialize_with_all_job_addresses(self, mock_jax_initialze):\n+    def test_initialize_with_all_job_addresses(self, mock_jax_initialize):\n         backend_dlib.initialize(\"10.0.0.1:1234,10.0.0.2:2345\", 2, 0)\n-        mock_jax_initialze.assert_called_once_with(\n+        mock_jax_initialize.assert_called_once_with(\n             coordinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n         )\n \n@@ -60,9 +60,9 @@ class JaxDistributionLibTest(testing.TestCase):\n             backend_dlib.initialize(\"10.0.0.1:1234,10.0.0.2:2345\", 3, 0)\n \n     @mock.patch.object(jax.distributed, \"initialize\", return_value=None)\n-    def test_initialize_with_coordinater_address(self, mock_jax_initialze):\n+    def test_initialize_with_coordinator_address(self, mock_jax_initialize):\n         backend_dlib.initialize(\"10.0.0.1:1234\", 2, 0)\n-        mock_jax_initialze.assert_called_once_with(\n+        mock_jax_initialize.assert_called_once_with(\n             coordinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n         )\n \n\n@@ -2131,7 +2131,7 @@ def round(x, decimals=0):\n         # int\n         if decimals > 0:\n             return x\n-        # temporarilaly convert to floats\n+        # temporarily convert to floats\n         factor = tf.cast(math.pow(10, decimals), config.floatx())\n         x = tf.cast(x, config.floatx())\n     else:\n\n@@ -47,15 +47,15 @@ def categorical(logits, num_samples, dtype=\"int64\", seed=None):\n \n \n def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n-    intemediate_dtype = dtype\n+    intermediate_dtype = dtype\n     if standardize_dtype(dtype) not in [\"int32\", \"int64\"]:\n-        intemediate_dtype = \"int64\"\n+        intermediate_dtype = \"int64\"\n     seed = _cast_seed(draw_seed(seed))\n     output = tf.random.stateless_uniform(\n         shape=shape,\n         minval=minval,\n         maxval=maxval,\n-        dtype=intemediate_dtype,\n+        dtype=intermediate_dtype,\n         seed=seed,\n     )\n     return tf.cast(output, dtype)\n@@ -109,14 +109,14 @@ def gamma(shape, alpha, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     seed = _cast_seed(draw_seed(seed))\n     # TODO: `tf.random.stateless_gamma` doesn't support bfloat16\n-    intemediate_dtype = dtype\n+    intermediate_dtype = dtype\n     if standardize_dtype(dtype) == \"bfloat16\":\n-        intemediate_dtype = \"float32\"\n+        intermediate_dtype = \"float32\"\n     return tf.cast(\n         tf.random.stateless_gamma(\n             shape,\n             alpha=alpha,\n-            dtype=intemediate_dtype,\n+            dtype=intermediate_dtype,\n             seed=seed,\n         ),\n         dtype,\n@@ -127,16 +127,16 @@ def binomial(shape, counts, probabilities, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     seed = _cast_seed(draw_seed(seed))\n     # TODO: `tf.random.stateless_binomial` doesn't support bfloat16\n-    intemediate_dtype = dtype\n+    intermediate_dtype = dtype\n     if standardize_dtype(dtype) == \"bfloat16\":\n-        intemediate_dtype = \"float32\"\n+        intermediate_dtype = \"float32\"\n     return tf.cast(\n         tf.random.stateless_binomial(\n             shape=shape,\n             seed=seed,\n             counts=counts,\n             probs=probabilities,\n-            output_dtype=intemediate_dtype,\n+            output_dtype=intermediate_dtype,\n         ),\n         dtype,\n     )\n@@ -161,11 +161,11 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n     seed_2 = seed_1 + 12\n \n     # TODO: `tf.random.stateless_gamma` doesn't support bfloat16\n-    intemediate_dtype = dtype\n+    intermediate_dtype = dtype\n     if standardize_dtype(dtype) == \"bfloat16\":\n-        intemediate_dtype = \"float32\"\n-    alpha = tf.convert_to_tensor(alpha, dtype=intemediate_dtype)\n-    beta = tf.convert_to_tensor(beta, dtype=intemediate_dtype)\n+        intermediate_dtype = \"float32\"\n+    alpha = tf.convert_to_tensor(alpha, dtype=intermediate_dtype)\n+    beta = tf.convert_to_tensor(beta, dtype=intermediate_dtype)\n \n     # tensorflow's tf.random.stateless_gamma has a bit of unconventional\n     # implementation of the stateless_gamma function where it checks the\n@@ -180,13 +180,13 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n \n     gamma_a = tf.cast(\n         tf.random.stateless_gamma(\n-            shape=shape, seed=seed_1, alpha=alpha, dtype=intemediate_dtype\n+            shape=shape, seed=seed_1, alpha=alpha, dtype=intermediate_dtype\n         ),\n         dtype,\n     )\n     gamma_b = tf.cast(\n         tf.random.stateless_gamma(\n-            shape=shape, seed=seed_2, alpha=beta, dtype=intemediate_dtype\n+            shape=shape, seed=seed_2, alpha=beta, dtype=intermediate_dtype\n         ),\n         dtype,\n     )\n\n@@ -29,7 +29,7 @@ def inv(x):\n \n def lu_factor(x):\n     LU, pivots = torch.linalg.lu_factor(x)\n-    # torch retuns pivots with 1-based indexing\n+    # torch returns pivots with 1-based indexing\n     return LU, pivots - 1\n \n \n\n@@ -341,7 +341,7 @@ class ModelCheckpoint(Callback):\n         later time of modification (for instance, when epoch/batch is used as\n         formatting option), but not necessarily (when accuracy or loss is used).\n         The tie-breaker is put in the logic as best effort to return the most\n-        recent, and to avoid undeterministic result.\n+        recent, and to avoid nondeterministic result.\n \n         Modified time of a file is obtained with `os.path.getmtime()`.\n \n\n@@ -694,7 +694,7 @@ class LayoutMap(collections.abc.MutableMapping):\n     `TensorLayout` instance.\n \n     In the normal case, the key to query is usually the `variable.path`, which\n-    is the idenifier of the variable.\n+    is the identifier of the variable.\n \n     As shortcut, tuple or list of axis names are also allowed when inserting\n     as value, and will be converted to `TensorLayout`.\n\n@@ -617,7 +617,7 @@ class ExportArchive:\n                 \"the TF runtime in the same environment will not work. \"\n                 \"To use JAX-native serialization for high-performance export \"\n                 \"and serving, please install `tensorflow-gpu` and ensure \"\n-                \"CUDA version compatiblity between your JAX and TF \"\n+                \"CUDA version compatibility between your JAX and TF \"\n                 \"installations.\"\n             )\n             return False\n\n@@ -124,7 +124,7 @@ class GroupedQueryAttentionTest(testing.TestCase, parameterized.TestCase):\n         backend.backend() == \"numpy\",\n         reason=\"Numpy backend does not support masking.\",\n     )\n-    def test_query_mask_progagation(self):\n+    def test_query_mask_propagation(self):\n         \"\"\"Test automatic propagation of the query's mask.\"\"\"\n         layer = layers.GroupedQueryAttention(\n             num_query_heads=2, num_key_value_heads=2, head_dim=2\n\n@@ -94,7 +94,7 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(\n         (\"without_key_same_proj\", (4, 8), (2, 8), None, None),\n         (\"with_key_same_proj\", (4, 8), (2, 8), (2, 3), None),\n-        (\"wihtout_key_different_proj\", (4, 8), (2, 8), None, (3, 4)),\n+        (\"without_key_different_proj\", (4, 8), (2, 8), None, (3, 4)),\n         (\"with_key_different_proj\", (4, 8), (2, 8), (2, 3), (1, 5)),\n         (\"high_dim_same_proj\", (4, 2, 3, 8), (1, 1, 5, 8), (1, 1, 5, 2), None),\n         (\n\n@@ -6,7 +6,7 @@ from keras.src import backend\n from keras.src import layers\n from keras.src import testing\n from keras.src.backend.common.backend_utils import (\n-    _convert_conv_tranpose_padding_args_from_keras_to_torch,\n+    _convert_conv_transpose_padding_args_from_keras_to_torch,\n )\n from keras.src.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n@@ -63,9 +63,9 @@ def np_conv1d_transpose(\n     if h_dilation > 1:\n         # Increase kernel size\n         new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n-        new_kenel_size_tuple = (new_h_kernel,)\n+        new_kernel_size_tuple = (new_h_kernel,)\n         new_kernel_weights = np.zeros(\n-            (*new_kenel_size_tuple, ch_out, ch_in),\n+            (*new_kernel_size_tuple, ch_out, ch_in),\n             dtype=kernel_weights.dtype,\n         )\n         new_kernel_weights[::h_dilation] = kernel_weights\n@@ -140,9 +140,9 @@ def np_conv2d_transpose(\n         # Increase kernel size\n         new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n         new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n-        new_kenel_size_tuple = (new_h_kernel, new_w_kernel)\n+        new_kernel_size_tuple = (new_h_kernel, new_w_kernel)\n         new_kernel_weights = np.zeros(\n-            (*new_kenel_size_tuple, ch_out, ch_in),\n+            (*new_kernel_size_tuple, ch_out, ch_in),\n             dtype=kernel_weights.dtype,\n         )\n         new_kernel_weights[::h_dilation, ::w_dilation] = kernel_weights\n@@ -233,9 +233,9 @@ def np_conv3d_transpose(\n         new_h_kernel = h_kernel + (h_dilation - 1) * (h_kernel - 1)\n         new_w_kernel = w_kernel + (w_dilation - 1) * (w_kernel - 1)\n         new_d_kernel = d_kernel + (d_dilation - 1) * (d_kernel - 1)\n-        new_kenel_size_tuple = (new_h_kernel, new_w_kernel, new_d_kernel)\n+        new_kernel_size_tuple = (new_h_kernel, new_w_kernel, new_d_kernel)\n         new_kernel_weights = np.zeros(\n-            (*new_kenel_size_tuple, ch_out, ch_in),\n+            (*new_kernel_size_tuple, ch_out, ch_in),\n             dtype=kernel_weights.dtype,\n         )\n         new_kernel_weights[::h_dilation, ::w_dilation, ::d_dilation] = (\n@@ -843,7 +843,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             (\n                 torch_padding,\n                 torch_output_padding,\n-            ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            ) = _convert_conv_transpose_padding_args_from_keras_to_torch(\n                 kernel_size=kernel_size,\n                 stride=strides,\n                 dilation_rate=1,\n\n@@ -25,7 +25,7 @@ TEST_PARAMETERS = [\n         \"np_op\": np.add,\n     },\n     {\n-        \"testcase_name\": \"substract\",\n+        \"testcase_name\": \"subtract\",\n         \"layer_class\": layers.Subtract,\n         \"np_op\": np.subtract,\n     },\n\n@@ -90,7 +90,7 @@ class HashedCrossingTest(testing.TestCase, parameterized.TestCase):\n             output = output.numpy()\n         self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n \n-    def test_upsupported_shape_input_fails(self):\n+    def test_unsupported_shape_input_fails(self):\n         with self.assertRaisesRegex(ValueError, \"inputs should have shape\"):\n             layers.HashedCrossing(num_bins=10)(\n                 (np.array([[[1.0]]]), np.array([[[1.0]]]))\n\n@@ -306,7 +306,7 @@ class Normalization(TFDataLayer):\n         inputs = self.backend.core.convert_to_tensor(\n             inputs, dtype=self.compute_dtype\n         )\n-        # Enusre the weights are in the correct backend. Without this, it is\n+        # Ensure the weights are in the correct backend. Without this, it is\n         # possible to cause breakage when using this layer in tf.data.\n         mean = self.convert_weight(self.mean)\n         variance = self.convert_weight(self.variance)\n\n@@ -508,7 +508,7 @@ class TextVectorization(Layer):\n         Args:\n             vocabulary: Either an array or a string path to a text file.\n                 If passing an array, can pass a tuple, list, 1D NumPy array,\n-                or 1D tensor containing the vocbulary terms.\n+                or 1D tensor containing the vocabulary terms.\n                 If passing a file path, the file should contain one line\n                 per term in the vocabulary.\n             idf_weights: A tuple, list, 1D NumPy array, or 1D tensor of inverse\n\n@@ -89,7 +89,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n class MeanAbsoluteErrorTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n-            losses.MeanAbsoluteError(name=\"mymae\")\n+            losses.MeanAbsoluteError(name=\"myname\")\n         )\n \n     def test_all_correct_unweighted(self):\n\n@@ -228,7 +228,7 @@ def _wrap_clone_function(\n             return cache[id(layer)]\n         if recursive:\n             if isinstance(layer, Sequential):\n-                # Note: Sequential doens't support call_function.\n+                # Note: Sequential doesn't support call_function.\n                 clone = clone_model(\n                     layer,\n                     clone_function=clone_function,\n\n@@ -163,7 +163,7 @@ class Functional(Function, Model):\n         return layers\n \n     def call(self, inputs, training=None, mask=None):\n-        # Add support for traning, masking\n+        # Add support for training, masking\n         inputs = self._standardize_inputs(inputs)\n         if mask is None:\n             masks = [None] * len(inputs)\n@@ -523,7 +523,7 @@ def functional_from_config(cls, config, custom_objects=None):\n                 else:\n                     del unprocessed_nodes[layer]\n \n-    # Create lits of input and output tensors and return new class\n+    # Create list of input and output tensors and return new class\n     name = config.get(\"name\")\n     trainable = config.get(\"trainable\")\n \n\n@@ -495,10 +495,10 @@ def switch(index, branches, *operands):\n     Examples:\n \n     >>> add_fn = lambda x, y: x + y\n-    >>> substract_fn = lambda x, y: x - y\n+    >>> subtract_fn = lambda x, y: x - y\n     >>> x = keras.ops.array(2.0)\n     >>> y = keras.ops.array(0.5)\n-    >>> branches = [add_fn, substract_fn]\n+    >>> branches = [add_fn, subtract_fn]\n     >>> keras.ops.switch(0, branches, x, y)\n     2.5\n \n\n@@ -68,7 +68,7 @@ def det(x):\n         x: Input tensor of shape `(..., M, M)`.\n \n     Returns:\n-        A tensor of shape `(...,)` represeting the determinant of `x`.\n+        A tensor of shape `(...,)` representing the determinant of `x`.\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n\n@@ -1945,7 +1945,7 @@ def ctc_decode(\n         A tuple containing:\n         - The tensor representing the list of decoded sequences. If\n             `strategy=\"greedy\"`, the shape is `(1, batch_size, max_length)`. If\n-            `strategy=\"beam_seatch\"`, the shape is\n+            `strategy=\"beam_search\"`, the shape is\n             `(top_paths, batch_size, max_length)`. Note that: `-1` indicates the\n             blank label.\n         - If `strategy=\"greedy\"`, a tensor of shape `(batch_size, 1)`\n\n@@ -2901,7 +2901,7 @@ class GetItem(Operation):\n             if not remaining_shape:\n                 raise ValueError(\n                     f\"Array has shape {x.shape} but slice \"\n-                    f\"has to many indices. Recieved: `{key}`\"\n+                    f\"has to many indices. Received: `{key}`\"\n                 )\n             length = remaining_shape.pop(0)\n             if isinstance(subkey, int):\n@@ -2921,7 +2921,7 @@ class GetItem(Operation):\n                     new_shape.append(length)\n             else:\n                 raise ValueError(\n-                    f\"Unsupported key type for array slice. Recieved: `{key}`\"\n+                    f\"Unsupported key type for array slice. Received: `{key}`\"\n                 )\n         return KerasTensor(tuple(new_shape), dtype=x.dtype)\n \n\n@@ -2088,7 +2088,7 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n             knp.argpartition(x, (1, 3))\n \n \n-class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n+class NumpyTwoInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_add(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         y = np.array([[4, 5, 6], [3, 2, 1]])\n\n@@ -23,7 +23,7 @@ class Adamax(optimizer.Optimizer):\n     ```\n \n     The update rule for parameter `w` with gradient `g` is described at the end\n-    of section 7.1 of the paper (see the referenece section):\n+    of section 7.1 of the paper (see the reference section):\n \n     ```python\n     t += 1\n\n@@ -27,7 +27,7 @@ class LossScaleOptimizer(optimizer.Optimizer):\n     scaling to it. This loss scale is dynamically updated over time as follows:\n     - On any train step, if a nonfinite gradient is encountered, the loss scale\n       is halved, and the train step is skipped.\n-    - If `dynamic_growth_steps` have ocurred since the last time the loss scale\n+    - If `dynamic_growth_steps` have occurred since the last time the loss scale\n       was updated, and no nonfinite gradients have occurred, the loss scale\n       is doubled.\n \n@@ -52,7 +52,7 @@ class LossScaleOptimizer(optimizer.Optimizer):\n     ):\n         if not kwargs.pop(\"dynamic\", True):\n             raise ValueError(\n-                \"LossScaleOptimizer no longer suports `dynamic=False`. \"\n+                \"LossScaleOptimizer no longer supports `dynamic=False`. \"\n                 \"Instead, simply set `loss_scale_factor` directly on the \"\n                 \"`inner_optimizer`.\"\n             )\n\n@@ -204,7 +204,7 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n                 )\n \n         # patch \"_apply_weight_decay\" to exclude this special case.\n-        # patch the optimizer \"assign\" methods to detect sparse udpates.\n+        # patch the optimizer \"assign\" methods to detect sparse updates.\n         # patch the tf.Variable \"assign\" methods to detect direct assign calls.\n         with mock.patch.object(\n             optimizer_to_patch, \"_apply_weight_decay\", autospec=True\n\n@@ -41,7 +41,7 @@ class Quantizer:\n     def get_config(self):\n         \"\"\"Returns the config of the quantizer.\n \n-        An quantizer config is a Python dictionary (serializable)\n+        A quantizer config is a Python dictionary (serializable)\n         containing all configuration parameters of the quantizer.\n         The same quantizer can be reinstantiated later\n         (without any saved state) from this configuration.\n\n@@ -45,7 +45,7 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n \n     Args:\n         logits: 2-D Tensor with shape (batch_size, num_classes). Each row\n-            should define a categorical distibution with the unnormalized\n+            should define a categorical distribution with the unnormalized\n             log-probabilities for all classes.\n         num_samples: Int, the number of independent samples to draw for each\n             row of the input. This will be the second dimension of the output\n\n@@ -126,7 +126,7 @@ def register_keras_serializable(package=\"Custom\", name=None):\n \n     Args:\n         package: The package that this class belongs to. This is used for the\n-            `key` (which is `\"package>name\"`) to idenfify the class. Note that\n+            `key` (which is `\"package>name\"`) to identify the class. Note that\n             this is the first argument passed into the decorator.\n         name: The name to serialize this class under in this package. If not\n             provided or `None`, the class' name will be used (note that this is\n\n@@ -432,7 +432,7 @@ class CompileLoss(losses_module.Loss):\n         # Inferred by `y_pred` and `output_names`\n         self.inferred_output_names = None\n \n-        # Use `Tracker` to track metrcis for individual losses.\n+        # Use `Tracker` to track metrics for individual losses.\n         self._metrics = []\n         self._tracker = Tracker(\n             {\n@@ -530,7 +530,7 @@ class CompileLoss(losses_module.Loss):\n                     \"When providing the `loss_weights` argument, it should \"\n                     \"have equal length of `loss` argument. \"\n                     f\"Received: loss_weights length={len(flat_loss_weights)}, \"\n-                    f\"loss legnth={len(flat_losses)}\"\n+                    f\"loss length={len(flat_losses)}\"\n                 )\n \n         y_true = tree.flatten(y_true)\n\n@@ -1,7 +1,7 @@\n class DataAdapter:\n     \"\"\"Base class for input data adapters.\n \n-    The purpose of a DataAdapter is to provide a unfied interface to\n+    The purpose of a DataAdapter is to provide a unified interface to\n     iterate over input data provided in a variety of formats -- such as\n     NumPy arrays, tf.Tensors, tf.data.Datasets, Keras PyDatasets, etc.\n     \"\"\"\n\n@@ -560,7 +560,7 @@ class OrderedEnqueuer(PyDatasetEnqueuer):\n         self.shuffle = shuffle\n         if self.py_dataset.num_batches is None:\n             # For infinite datasets, `self.indices` is created here once for all\n-            # so that subsquent runs resume from where they stopped.\n+            # so that subsequent runs resume from where they stopped.\n             self.indices = itertools.count()\n \n     def _get_executor_init(self, workers):\n\n@@ -7,7 +7,7 @@ class TFDatasetAdapter(DataAdapter):\n     \"\"\"Adapter that handles `tf.data.Dataset`.\"\"\"\n \n     def __init__(self, dataset, class_weight=None, distribution=None):\n-        \"\"\"Iniitialize the TFDatasetAdapter.\n+        \"\"\"Initialize the TFDatasetAdapter.\n \n         Args:\n             dataset: The input `tf.data.Dataset` instance.\n\n@@ -90,7 +90,7 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n         adapter = tf_dataset_adapter.TFDatasetAdapter(dataset)\n         self.assertEqual(adapter.num_batches, 42)\n \n-        # Test for Infiniate Cardinality\n+        # Test for Infinite Cardinality\n         dataset = tf.data.Dataset.range(42)\n         dataset = dataset.repeat()\n         cardinality = int(dataset.cardinality())\n\n@@ -942,7 +942,7 @@ class Trainer:\n     def _get_metrics_result_or_logs(self, logs):\n         \"\"\"Returns model metrics as a dict if the keys match with input logs.\n \n-        When the training / evalution is performed with an asynchronous steps,\n+        When the training / evaluation is performed with an asynchronous steps,\n         the last scheduled `train / test_step` may not give the latest metrics\n         because it is not guaranteed to be executed the last. This method gets\n         metrics from the model directly instead of relying on the return from\n\n@@ -66,7 +66,7 @@ class AudioDatasetFromDirectoryTest(testing.TestCase):\n         return temp_dir\n \n     def test_audio_dataset_from_directory_standalone(self):\n-        # Test retrieving audio samples withouts labels from a directory and its\n+        # Test retrieving audio samples without labels from a directory and its\n         # subdirs.\n         # Save a few extra audio in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n\n@@ -663,7 +663,7 @@ def index_subdirectory(directory, class_indices, follow_links, formats):\n \n \n def get_training_or_validation_split(samples, labels, validation_split, subset):\n-    \"\"\"Potentially restict samples & labels to a training or validation split.\n+    \"\"\"Potentially restrict samples & labels to a training or validation split.\n \n     Args:\n         samples: List of elements.\n\n@@ -31,7 +31,7 @@ class TestNumericalUtils(testing.TestCase, parameterized.TestCase):\n             np.all(np.argmax(one_hot, -1).reshape(label.shape) == label)\n         )\n \n-    def test_to_categorial_without_num_classes(self):\n+    def test_to_categorical_without_num_classes(self):\n         label = [0, 2, 5]\n         one_hot = numerical_utils.to_categorical(label)\n         self.assertEqual(one_hot.shape, (3, 5 + 1))\n\n@@ -88,7 +88,7 @@ class TimeseriesDatasetTest(testing.TestCase):\n         # results\n         for x, _ in dataset.take(1):\n             self.assertNotAllClose(x, first_seq)\n-        # Check determism with same seed\n+        # Check determinism with same seed\n         dataset = timeseries_dataset_utils.timeseries_dataset_from_array(\n             data,\n             targets,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18ee0a6382267967d83cf1964e6e27d7134a605e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 13/12 | Churn Δ: 81 | Churn Cumulative: 613 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 0.5925925925925926\n\nDIFF:\n@@ -134,6 +134,7 @@ class Trainer:\n                 wrapped in a `LossScaleOptimizer`, which will dynamically\n                 scale the loss to prevent underflow.\n         \"\"\"\n+        self._clear_previous_trainer_metrics()\n         optimizer = optimizers.get(optimizer)\n         self.optimizer = optimizer\n         if (\n@@ -246,12 +247,23 @@ class Trainer:\n \n     @property\n     def metrics(self):\n-        metrics = [self._loss_tracker] if self.compiled else []\n-        metrics.extend(super().metrics)\n-        if self.compiled and self._compile_metrics is not None:\n-            metrics += [self._compile_metrics]\n-        if self.compiled and self._compile_loss is not None:\n+        # Order: loss tracker, individual loss trackers, compiled metrics,\n+        # custom metrcis, sublayer metrics.\n+        metrics = []\n+        if self.compiled:\n+            if self._loss_tracker is not None:\n+                metrics.append(self._loss_tracker)\n+            if self._compile_metrics is not None:\n+                metrics.append(self._compile_metrics)\n+            if self._compile_loss is not None:\n                 metrics.extend(self._compile_loss.metrics)\n+        metrics.extend(self._metrics)\n+        for layer in self._layers:\n+            if isinstance(layer, Trainer):\n+                # All Trainer-related metrics in sublayers should be ignored\n+                # because a new Trainer has been instantiated.\n+                continue\n+            metrics.extend(layer.metrics)\n         return metrics\n \n     @property\n@@ -262,6 +274,35 @@ class Trainer:\n         for m in self.metrics:\n             m.reset_state()\n \n+    def _get_own_metrics(self):\n+        metrics = []\n+        if hasattr(self, \"_loss_tracker\"):\n+            metrics.append(self._loss_tracker)\n+        if (\n+            hasattr(self, \"_compile_metrics\")\n+            and self._compile_metrics is not None\n+        ):\n+            metrics.append(self._compile_metrics)\n+        if hasattr(self, \"_compile_loss\") and self._compile_loss is not None:\n+            metrics.extend(self._compile_loss.metrics)\n+        if hasattr(self, \"_metrics\"):\n+            metrics.extend(self._metrics)\n+        return metrics\n+\n+    def _clear_previous_trainer_metrics(self):\n+        for layer in self._flatten_layers(include_self=False):\n+            if not isinstance(layer, Trainer):\n+                continue\n+            # A sublayer might be a Trainer. In that case, we need to clear\n+            # the Trainer-related metrics, as they are not usable when a\n+            # new Trainer is instantiated.\n+            for m in self._get_own_metrics():\n+                layer._tracker.untrack(m)\n+            layer._loss_tracker = None\n+            layer._compile_metrics = None\n+            layer._compile_loss._metrics = []\n+            layer._metrics = []\n+\n     def compute_loss(\n         self,\n         x=None,\n\n@@ -199,8 +199,8 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         # my_metric.\n         self.assertEqual(len(model.metrics), 3)\n         self.assertEqual(model.metrics[0], model._loss_tracker)\n-        self.assertEqual(model.metrics[1], model.my_metric)\n-        self.assertEqual(model.metrics[2], model._compile_metrics)\n+        self.assertEqual(model.metrics[1], model._compile_metrics)\n+        self.assertEqual(model.metrics[2], model.my_metric)\n \n         # All metrics should have their weights created\n         self.assertEqual(len(model._loss_tracker.variables), 2)\n@@ -227,6 +227,32 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(len(model_weighted.metrics), 3)\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_nested_trainer_metrics(self):\n+        # https://github.com/keras-team/keras/issues/20188\n+        model = ExampleModel(units=3)\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+        self.assertLen(model.metrics, 2)\n+        self.assertEqual(model.metrics[0], model._loss_tracker)\n+        self.assertEqual(model.metrics[1], model._compile_metrics)\n+\n+        inputs = keras.Input((4,))\n+        outputs = model(inputs)\n+        outputs = layers.Dense(8)(outputs)\n+        new_model = models.Model(inputs, outputs)\n+        new_model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+        self.assertLen(new_model.metrics, 2)\n+        self.assertEqual(new_model.metrics[0], new_model._loss_tracker)\n+        self.assertEqual(new_model.metrics[1], new_model._compile_metrics)\n+\n     @pytest.mark.skipif(\n         backend.backend() != \"torch\",\n         reason=\"torch backend runs in eager mode for jit_compile='auto'\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7689bcccbfad926d8ed6a1f22866a71de6e2e12e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 286 | Contributors (this commit): 11 | Commits (past 90d): 10 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -465,7 +465,7 @@ class ExportArchive:\n             variables = tree.flatten(tree.map_structure(tf.Variable, variables))\n         setattr(self._tf_trackable, name, list(variables))\n \n-    def write_out(self, filepath, options=None):\n+    def write_out(self, filepath, options=None, verbose=True):\n         \"\"\"Write the corresponding SavedModel to disk.\n \n         Arguments:\n@@ -473,6 +473,8 @@ class ExportArchive:\n                 Path where to save the artifact.\n             options: `tf.saved_model.SaveOptions` object that specifies\n                 SavedModel saving options.\n+            verbose: whether to print all the variables of an\n+                exported SavedModel.\n \n         **Note on TF-Serving**: all endpoints registered via `add_endpoint()`\n         are made visible for TF-Serving in the SavedModel artifact. In addition,\n@@ -506,7 +508,9 @@ class ExportArchive:\n \n         # Print out available endpoints\n         endpoints = \"\\n\\n\".join(\n-            _print_signature(getattr(self._tf_trackable, name), name)\n+            _print_signature(\n+                getattr(self._tf_trackable, name), name, verbose=verbose\n+            )\n             for name in self._endpoint_names\n         )\n         io_utils.print_msg(\n@@ -625,7 +629,7 @@ class ExportArchive:\n             return True\n \n \n-def export_model(model, filepath):\n+def export_model(model, filepath, verbose=True):\n     export_archive = ExportArchive()\n     export_archive.track(model)\n     if isinstance(model, (Functional, Sequential)):\n@@ -641,7 +645,7 @@ def export_model(model, filepath):\n                 \"It must be called at least once before export.\"\n             )\n         export_archive.add_endpoint(\"serve\", model.__call__, input_signature)\n-    export_archive.write_out(filepath)\n+    export_archive.write_out(filepath, verbose=verbose)\n \n \n def _get_input_signature(model):\n@@ -813,9 +817,9 @@ def _make_tensor_spec(x):\n     return tf.TensorSpec(shape, dtype=x.dtype, name=x.name)\n \n \n-def _print_signature(fn, name):\n+def _print_signature(fn, name, verbose=True):\n     concrete_fn = fn._list_all_concrete_functions()[0]\n-    pprinted_signature = concrete_fn.pretty_printed_signature(verbose=True)\n+    pprinted_signature = concrete_fn.pretty_printed_signature(verbose=verbose)\n     lines = pprinted_signature.split(\"\\n\")\n     lines = [f\"* Endpoint '{name}'\"] + lines[1:]\n     endpoint = \"\\n\".join(lines)\n\n@@ -457,7 +457,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         model_config = serialization_lib.serialize_keras_object(self)\n         return json.dumps(model_config, **kwargs)\n \n-    def export(self, filepath, format=\"tf_saved_model\"):\n+    def export(self, filepath, format=\"tf_saved_model\", verbose=True):\n         \"\"\"Create a TF SavedModel artifact for inference.\n \n         **Note:** This can currently only be used with\n@@ -475,6 +475,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         Args:\n             filepath: `str` or `pathlib.Path` object. Path where to save\n                 the artifact.\n+            verbose: whether to print all the variables of the exported model.\n \n         Example:\n \n@@ -493,7 +494,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         \"\"\"\n         from keras.src.export import export_lib\n \n-        export_lib.export_model(self, filepath)\n+        export_lib.export_model(self, filepath, verbose)\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6931138fb1818afcdcf32f821e5635a8acc03e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 16 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 49 | Churn Cumulative: 487 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,14 @@\n+from pathlib import Path\n+\n import keras\n from keras.src.utils import plot_model\n \n \n-def plot_sequential_model():\n+def assert_file_exists(path):\n+    assert Path(path).is_file(), \"File does not exist\"\n+\n+\n+def test_plot_sequential_model():\n     model = keras.Sequential(\n         [\n             keras.Input((3,)),\n@@ -10,41 +16,60 @@ def plot_sequential_model():\n             keras.layers.Dense(1, activation=\"sigmoid\"),\n         ]\n     )\n-    plot_model(model, \"sequential.png\")\n-    plot_model(model, \"sequential-show_shapes.png\", show_shapes=True)\n+    file_name = \"sequential.png\"\n+    plot_model(model, file_name)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_shapes.png\"\n+    plot_model(model, file_name, show_shapes=True)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_shapes-show_dtype.png\"\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_shapes-show_dtype-show_layer_names.png\"\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"sequential-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -52,12 +77,16 @@ def plot_sequential_model():\n         show_trainable=True,\n         rankdir=\"LR\",\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"sequential-show_layer_activations-show_trainable.png\"\n     plot_model(\n         model,\n-        \"sequential-show_layer_activations-show_trainable.png\",\n+        file_name,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n \n \n def plot_functional_model():\n@@ -329,7 +358,7 @@ def plot_functional_model_with_splits_and_merges():\n \n \n if __name__ == \"__main__\":\n-    plot_sequential_model()\n+    test_plot_sequential_model()\n     plot_functional_model()\n     plot_subclassed_model()\n     plot_nested_functional_model()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa6be07ce86b3f8c5072a3d5ff355b4266baa32d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 129 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 57 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 167 | Churn Cumulative: 654 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,7 +89,7 @@ def test_plot_sequential_model():\n     assert_file_exists(file_name)\n \n \n-def plot_functional_model():\n+def test_plot_functional_model():\n     inputs = keras.Input((3,))\n     x = keras.layers.Dense(4, activation=\"relu\", trainable=False)(inputs)\n     residual = x\n@@ -106,41 +106,61 @@ def plot_functional_model():\n     outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n \n     model = keras.Model(inputs, outputs)\n-    plot_model(model, \"functional.png\")\n-    plot_model(model, \"functional-show_shapes.png\", show_shapes=True)\n+\n+    file_name = \"functional.png\"\n+    plot_model(model, file_name)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_shapes.png\"\n+    plot_model(model, file_name, show_shapes=True)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_shapes-show_dtype.png\"\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_shapes-show_dtype-show_layer_names.png\"\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_shapes-show_dtype-show_layer_activations.png\"\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_shapes-show_dtype-show_layer_activations-show_trainable.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -148,22 +168,31 @@ def plot_functional_model():\n         show_trainable=True,\n         rankdir=\"LR\",\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"functional-show_layer_activations-show_trainable.png\"\n     plot_model(\n         model,\n-        \"functional-show_layer_activations-show_trainable.png\",\n+        file_name,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = (\n+        \"functional-show_shapes-show_layer_activations-show_trainable.png\"\n+    )\n     plot_model(\n         model,\n-        \"functional-show_shapes-show_layer_activations-show_trainable.png\",\n+        file_name,\n         show_shapes=True,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n \n \n-def plot_subclassed_model():\n+def test_plot_subclassed_model():\n     class MyModel(keras.Model):\n         def __init__(self, **kwargs):\n             super().__init__(**kwargs)\n@@ -176,41 +205,60 @@ def plot_subclassed_model():\n     model = MyModel()\n     model.build((None, 3))\n \n-    plot_model(model, \"subclassed.png\")\n-    plot_model(model, \"subclassed-show_shapes.png\", show_shapes=True)\n+    file_name = \"subclassed.png\"\n+    plot_model(model, file_name)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_shapes.png\"\n+    plot_model(model, file_name, show_shapes=True)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_shapes-show_dtype.png\"\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_shapes-show_dtype-show_layer_names.png\"\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_shapes-show_dtype-show_layer_activations.png\"\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -218,22 +266,31 @@ def plot_subclassed_model():\n         show_trainable=True,\n         rankdir=\"LR\",\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"subclassed-show_layer_activations-show_trainable.png\"\n     plot_model(\n         model,\n-        \"subclassed-show_layer_activations-show_trainable.png\",\n+        file_name,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = (\n+        \"subclassed-show_shapes-show_layer_activations-show_trainable.png\"\n+    )\n     plot_model(\n         model,\n-        \"subclassed-show_shapes-show_layer_activations-show_trainable.png\",\n+        file_name,\n         show_shapes=True,\n         show_layer_activations=True,\n         show_trainable=True,\n     )\n+    assert_file_exists(file_name)\n \n \n-def plot_nested_functional_model():\n+def test_plot_nested_functional_model():\n     inputs = keras.Input((3,))\n     x = keras.layers.Dense(4, activation=\"relu\")(inputs)\n     x = keras.layers.Dense(4, activation=\"relu\")(x)\n@@ -254,40 +311,56 @@ def plot_nested_functional_model():\n     outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n     model = keras.Model(inputs, outputs)\n \n-    plot_model(model, \"nested-functional.png\", expand_nested=True)\n+    file_name = \"nested-functional.png\"\n+    plot_model(model, file_name, expand_nested=True)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes.png\"\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes.png\",\n+        file_name,\n         show_shapes=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes-show_dtype.png\"\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes-show_dtype-show_layer_names.png\"\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n         show_layer_activations=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -295,9 +368,12 @@ def plot_nested_functional_model():\n         show_trainable=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_dtype-show_layer_names-show_layer_activations-show_trainable-LR.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         show_layer_names=True,\n@@ -306,24 +382,31 @@ def plot_nested_functional_model():\n         rankdir=\"LR\",\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_layer_activations-show_trainable.png\"\n     plot_model(\n         model,\n-        \"nested-functional-show_layer_activations-show_trainable.png\",\n+        file_name,\n         show_layer_activations=True,\n         show_trainable=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"nested-functional-show_shapes-show_layer_activations-show_trainable.png\"  # noqa: E501\n     plot_model(\n         model,\n-        \"nested-functional-show_shapes-show_layer_activations-show_trainable.png\",  # noqa: E501\n+        file_name,\n         show_shapes=True,\n         show_layer_activations=True,\n         show_trainable=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n \n \n-def plot_functional_model_with_splits_and_merges():\n+def test_plot_functional_model_with_splits_and_merges():\n     class SplitLayer(keras.Layer):\n         def call(self, x):\n             return list(keras.ops.split(x, 2, axis=1))\n@@ -341,25 +424,33 @@ def plot_functional_model_with_splits_and_merges():\n     outputs = ConcatLayer()([a, b])\n     model = keras.Model(inputs, outputs)\n \n-    plot_model(model, \"split-functional.png\", expand_nested=True)\n+    file_name = \"split-functional.png\"\n+    plot_model(model, file_name, expand_nested=True)\n+    assert_file_exists(file_name)\n+\n+    file_name = \"split-functional-show_shapes.png\"\n     plot_model(\n         model,\n-        \"split-functional-show_shapes.png\",\n+        file_name,\n         show_shapes=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n+\n+    file_name = \"split-functional-show_shapes-show_dtype.png\"\n     plot_model(\n         model,\n-        \"split-functional-show_shapes-show_dtype.png\",\n+        file_name,\n         show_shapes=True,\n         show_dtype=True,\n         expand_nested=True,\n     )\n+    assert_file_exists(file_name)\n \n \n if __name__ == \"__main__\":\n     test_plot_sequential_model()\n-    plot_functional_model()\n-    plot_subclassed_model()\n-    plot_nested_functional_model()\n-    plot_functional_model_with_splits_and_merges()\n+    test_plot_functional_model()\n+    test_plot_subclassed_model()\n+    test_plot_nested_functional_model()\n+    test_plot_functional_model_with_splits_and_merges()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
