{"custom_id": "scrapy#167894661d3186b45b93743abdccbfb1c89f9408", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 14 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,11 +12,19 @@ class Libxml2Document(object):\n         self.xpathContext = self.xmlDoc.xpathNewContext()\n \n     def __del__(self):\n-        if hasattr(self, 'xmlDoc'):\n+        # we must call both cleanup functions, so we try/except all exceptions\n+        # to make sure one doesn't prevent the other from being called\n+        # this call sometimes raises a \"NoneType is not callable\" TypeError\n+        # also, these calls sometimes raise a \"NoneType is not callable\"\n+        # TypeError, so the try/except block silences them\n+        try:\n             self.xmlDoc.freeDoc()\n-        if hasattr(self, 'xpathContext'):\n+        except:\n+            pass\n+        try:\n             self.xpathContext.xpathFreeContext()\n+        except:\n+            pass\n \n     def __str__(self):\n         return \"<Libxml2Document %s>\" % self.xmlDoc.name\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0723534655f2b90eba64ddf352d1039290126a4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 134 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -191,10 +191,6 @@ STATS_DEBUG = False\n STATS_SDB_DOMAIN = 'scrapy_stats'\n STATS_SDB_ASYNC = False\n \n-# deprecated settings - the stats web service should be moved to the web console\n-STATS_WSPORT = 8089\n-STATS_WSTIMEOUT = 15\n-\n STATSMAILER_RCPTS = []\n \n TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#996fdb7edf4253ba75daebc515451b82a77054fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 9 | Churn Cumulative: 9 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ tasks\"\"\"\n import os\n import string\n from optparse import OptionParser\n+import re\n \n import scrapy\n from scrapy.utils.misc import render_templatefile, string_camelcase\n@@ -43,6 +44,14 @@ def main():\n     if cmd == \"startproject\":\n         if len(args) >= 2:\n             project_name = args[1]\n+            if not re.search(r'^[_a-zA-Z]\\w*$', project_name): # If it's not a valid directory name.\n+                # Provide a smart error message, depending on the error.\n+                if not re.search(r'^[_a-zA-Z]', project_name):\n+                    message = 'make sure the project_name begins with a letter or underscore'\n+                else:\n+                    message = 'use only numbers, letters and underscores'\n+                print \"scrapy-admin.py: %r is not a valid project name. Please %s.\" % (project_name, message)\n+            else:\n                 project_root_path = project_name\n \n                 roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#558230564864706f7866564ef896e487979c04b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 19 | Churn Cumulative: 242 | Contributors (this commit): 3 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -186,7 +186,7 @@ SPIDER_MIDDLEWARES_BASE = {\n \n STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'\n STATS_ENABLED = True\n-STATS_DEBUG = False\n+STATS_DUMP = False\n \n STATS_SDB_DOMAIN = 'scrapy_stats'\n STATS_SDB_ASYNC = False\n\n@@ -14,7 +14,6 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.stats import stats\n from scrapy.extension import extensions\n from scrapy.conf import settings\n \n@@ -58,10 +57,6 @@ class MemoryDebugger(object):\n         s += \"SCRAPY MEMORY DEBUGGER RESULTS\\n\\n\"\n         for f in figures:\n             s += \"%-30s : %s %s\\n\" % f\n-        s += \"\\n\"\n-        if stats:\n-            s += \"SCRAPING STATS --------------------------------------------------\\n\\n\"\n-            s += pprint.pformat(stats)\n         return s\n \n     def print_or_send_report(self, report):\n\n@@ -17,7 +17,6 @@ from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.stats import stats\n from scrapy.conf import settings\n \n class MemoryUsage(object):\n@@ -120,9 +119,4 @@ class MemoryUsage(object):\n         s += \"\\r\\n\"\n         s += scrapyengine.getstatus()\n         s += \"\\r\\n\"\n-\n-        if stats:\n-            s += \"SCRAPING STATS ------------------------------------------------------ \\r\\n\"\n-            s += \"\\r\\n\"\n-            s += pprint.pformat(stats)\n         self.mail.send(rcpts, subject, s)\n\n@@ -14,7 +14,7 @@ from scrapy.conf import settings\n class StatsCollector(object):\n \n     def __init__(self):\n-        self.debug = settings.getbool('STATS_DEBUG')\n+        self._dump = settings.getbool('STATS_DUMP')\n         self._stats = {None: {}} # None is for global stats\n \n         dispatcher.connect(self.open_domain, signal=signals.domain_open)\n@@ -48,8 +48,8 @@ class StatsCollector(object):\n \n     def close_domain(self, domain, reason):\n         signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n-        if self.debug:\n-            log.msg(pprint.pformat(self[domain]), domain=domain, level=log.DEBUG)\n+        if self._dump:\n+            log.msg(\"Dumping stats:\\n\" + pprint.pformat(self._stats[domain]), domain=domain)\n         del self._stats[domain]\n         signals.send_catch_log(stats_domain_closed, domain=domain, reason=reason)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a24516ed19f0fc3fa103b73a58de16f7fe1d64ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 8 | Churn Cumulative: 8 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,11 +6,7 @@ class Command(ScrapyCommand):\n         return \"\"\n     \n     def short_desc(self):\n-        return \"List available spiders (both enabled and disabled)\"\n+        return \"List available spiders\"\n \n     def run(self, args, opts):\n-        spiders_dict = spiders.asdict()\n-        for n, p in spiders_dict.items():\n-            disabled = \"disabled\" if getattr(p, 'disabled', False) else \"enabled\"\n-            print \"%-30s %-30s %s\" % (n, p.__class__.__name__, disabled)\n-        print \"Total spiders: %d\" % len(spiders_dict)\n+        print \"\\n\".join(spiders.asdict().keys())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd4aed7d7b6231594a02322c2f8910c3c3224b68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 12 | Churn Cumulative: 12 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import sys, pprint\n+import pprint\n \n from scrapy.command import ScrapyCommand\n from scrapy.fetcher import fetch\n@@ -8,12 +8,14 @@ class Command(ScrapyCommand):\n         return \"[options] <url>\"\n \n     def short_desc(self):\n-        return \"Download a URL using the Scrapy downloader\"\n+        return \"Fetch a URL using the Scrapy downloader\"\n+\n+    def long_desc(self):\n+        return \"Fetch a URL using the Scrapy downloader and print its content to stdout. You may want to use --nolog to disable logging\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-s\", \"--source\", dest=\"source\", action=\"store_true\", help=\"output HTTP body only\")\n-        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", help=\"output HTTP headers only\")\n+        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", help=\"print HTTP headers instead of body\")\n \n     def run(self, args, opts):\n         if not args:\n@@ -25,4 +27,4 @@ class Command(ScrapyCommand):\n             if opts.headers:\n                 pprint.pprint(responses[0].headers)\n             else:\n-                sys.stdout.write(str(responses[0].body))\n+                print responses[0].body\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc2c6f99b4d457751a84f550619b32d96438c241", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/12 | Churn Δ: 20 | Churn Cumulative: 36 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,10 +9,10 @@ from scrapy import log\n \n class Command(ScrapyCommand):\n     def syntax(self):\n-        return \"[options] [domain|url] ...\"\n+        return \"[options] <domain|url> ...\"\n \n     def short_desc(self):\n-        return \"Run the web scraping engine from the command line\"\n+        return \"Start crawling a domain or URL\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -15,10 +15,10 @@ class Command(ScrapyCommand):\n     custom_tvars = {}\n \n     def syntax(self):\n-        return \"<spider_name> <spider_domain_name> [--template=template_name]\"\n+        return \"[options] <spider_name> <spider_domain_name>\"\n \n     def short_desc(self):\n-        return \"Generate new spider based on a predefined template\"\n+        return \"Generate new spider based on template passed with --template\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -2,9 +2,6 @@ from scrapy.command import ScrapyCommand\n from scrapy.spider import spiders\n \n class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"\"\n-    \n     def short_desc(self):\n         return \"List available spiders\"\n \n\n@@ -1,16 +1,9 @@\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n \n-\n class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options]\"\n-\n     def short_desc(self):\n-        return \"Start the Scrapy server\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+        return \"Start the Scrapy manager but don't run any spider (idle mode)\"\n \n     def run(self, args, opts):\n         scrapymanager.start(*args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a800b5abab996a8e3819797a1b8d52977999bad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 46 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 46 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,46 +0,0 @@\n-import re\n-\n-from scrapy.command import ScrapyCommand\n-\n-log_crawled_re = re.compile(r'^(.*?) \\[\\w+/(.*?)\\].*Crawled <(.*?)> from <(.*?)>$')\n-log_scraped_re = re.compile('^(.*?) \\[\\w+/(.*?)\\].*Scraped (.*) in <(.*?)>$')\n-log_opendomain_re = re.compile('^(.*?) \\[\\w+.*? Started scraping (.*)$')\n-log_closedomain_re = re.compile('^(.*?) \\[\\w+.*? Finished scraping (.*)$')\n-log_debug_re = re.compile(r'\\[\\w+/(.*?)\\] DEBUG')\n-log_info_re = re.compile(r'\\[\\w+/(.*?)\\] INFO')\n-log_warning_re = re.compile(r'\\[\\w+/(.*?)\\] WARNING')\n-log_error_re = re.compile(r'\\[\\w+/(.*?)\\] ERROR')\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <logfile>\"\n-\n-    def short_desc(self):\n-        return \"Several tools to perform scrapy log analysis\"\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--sitemap\", dest=\"sitemap\", action=\"store_true\", help=\"show sitemap based on given log file\")\n-\n-    def sitemap(self, logfile):\n-        from scrapy.utils.datatypes import Sitemap\n-\n-        sm = Sitemap()\n-        for l in open(logfile).readlines():\n-            m = log_crawled_re.search(l.strip())\n-            if m:\n-                sm.add_node(m.group(3), m.group(4))\n-            m = log_scraped_re.search(l.strip())\n-            if m:\n-                sm.add_item(m.group(4), m.group(3))\n-        print sm.to_string()\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A log file is required\"\n-            return\n-\n-        if opts.sitemap:\n-            self.sitemap(args[0])\n-        else:\n-            print \"No analysis method specified\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5faf725c53c4c3baab6b3dd3fcd6063328f989ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 32/32 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ class Command(ScrapyCommand):\n         return \"[options] <url>\"\n \n     def short_desc(self):\n-        return \"Parse the given URL and print the results\"\n+        return \"Parse the given URL (using the spider) and print the results\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db6afcf3cbc065317c7cbe10fc023da1624b8b15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 21 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,23 +49,14 @@ def getcmdname(argv):\n             return arg\n \n def usage(argv):\n-    s  = \"usage: %s <subcommand> [options] [args]\\n\" % argv[0]\n-    s += \"       %s <subcommand> -h\\n\\n\" % argv[0]\n-    s += \"Built-in subcommands:\\n\"\n+    s  = \"usage: %s <command> [options] [args]\\n\" % argv[0]\n+    s += \"       %s <command> -h\\n\\n\" % argv[0]\n+    s += \"Available commands:\\n\\n\"\n \n-    builtin_cmds = builtin_commands_dict()\n-    custom_cmds = custom_commands_dict()\n+    cmds = builtin_commands_dict()\n+    cmds.update(custom_commands_dict())\n \n-    filtered_builtin_cmds = [(name, cls) for name, cls in builtin_cmds.iteritems() if name not in custom_cmds]\n-\n-    for cmdname, cmdclass in filtered_builtin_cmds:\n-        s += \"  %s %s\\n\" % (cmdname, cmdclass.syntax())\n-        s += \"    %s\\n\" % cmdclass.short_desc()\n-\n-    if custom_cmds:\n-        s += \"\\n\"\n-        s += \"Custom (or overloaded) subcommands:\\n\"\n-        for cmdname, cmdclass in custom_cmds.iteritems():\n+    for cmdname, cmdclass in sorted(cmds.iteritems()):\n         s += \"%s %s\\n\" % (cmdname, cmdclass.syntax())\n         s += \"  %s\\n\" % cmdclass.short_desc()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6648b5da8b30578ea1fefc77de8052ca619ab397", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 19 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,7 @@ from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.extension import extensions\n from scrapy.conf import settings\n+from scrapy import log\n \n class MemoryDebugger(object):\n \n@@ -35,7 +36,7 @@ class MemoryDebugger(object):\n     def engine_stopped(self):\n         figures = self.collect_figures()\n         report = self.create_report(figures)\n-        self.print_or_send_report(report)\n+        self.log_or_send_report(report)\n \n     def collect_figures(self):\n         libxml2.cleanupParser()\n@@ -59,7 +60,7 @@ class MemoryDebugger(object):\n             s += \"%-30s : %s %s\\n\" % f\n         return s\n \n-    def print_or_send_report(self, report):\n+    def log_or_send_report(self, report):\n         if self.rcpts:\n             self.mail.send(self.rcpts, \"Scrapy Memory Debugger results at %s\" % socket.gethostname(), report)\n-        print report\n+        log.msg(report)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7deb9d9b0b97f0d0e0327952896568b10ce30ac7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 86/86 | Churn Δ: 25 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -136,31 +136,6 @@ class MultiValueDict(dict):\n         for key, value in kwargs.iteritems():\n             self.setlistdefault(key, []).append(value)\n \n-class Sitemap(object):\n-    \"\"\"Sitemap class is used to build a map of the traversed pages\"\"\"\n-\n-    def __init__(self):\n-        self._nodes = {}\n-        self._roots = []\n-\n-    def add_node(self, url, parent_url):\n-        if not url in self._nodes:\n-            parent = self._nodes.get(parent_url, None)\n-            node = SiteNode(url)\n-            self._nodes[url] = node\n-            if parent:\n-                parent.add_child(node)\n-            else:\n-                self._roots.append(node)\n-\n-    def add_item(self, url, item):\n-        if url in self._nodes:\n-            self._nodes[url].itemnames.append(str(item))\n-    \n-    def to_string(self):\n-        s = ''.join([n.to_string(0) for n in self._roots])\n-        return s\n-\n class SiteNode(object):\n     \"\"\"Class to represent a site node (page, image or any other file)\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9006d1f04010d188d262bc6109fd50e54994fa63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 137 | Contributors (this commit): 3 | Commits (past 90d): 23 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -96,7 +96,6 @@ EXTENSIONS = [\n     'scrapy.contrib.webconsole.spiderctl.Spiderctl',\n     'scrapy.contrib.webconsole.enginestatus.EngineStatus',\n     'scrapy.contrib.webconsole.stats.StatsDump',\n-    'scrapy.contrib.webconsole.spiderstats.SpiderStats',\n     'scrapy.contrib.spider.reloader.SpiderReloader',\n     'scrapy.contrib.memusage.MemoryUsage',\n     'scrapy.contrib.memdebug.MemoryDebugger',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4a1c4b130a48f8729b9c89fcf0a9800461852bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 42 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,14 +4,11 @@ StatsMailer extension sends an email when a domain finishes scraping.\n Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \"\"\"\n \n-import pprint\n-\n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.stats import stats\n+from scrapy.stats import stats, signals\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n-from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n \n class StatsMailer(object):\n@@ -20,9 +17,12 @@ class StatsMailer(object):\n         self.recipients = settings.getlist(\"STATSMAILER_RCPTS\")\n         if not self.recipients:\n             raise NotConfigured\n-        dispatcher.connect(self.send_stats, signal=signals.domain_closed)\n+        dispatcher.connect(self.send_stats, signal=signals.stats_domain_closing)\n         \n     def send_stats(self, domain):\n         mail = MailSender()\n-        body = pprint.pformat(stats[domain])\n+        body = \"Global stats\\n\\n\"\n+        body += \"\\n\".join(\"%-50s : %s\" % i for i in stats.get_stats().items())\n+        body += \"\\n\\n%s stats\\n\\n\" % domain\n+        body += \"\\n\".join(\"%-50s : %s\" % i for i in stats.get_stats(domain).items())\n         mail.send(self.recipients, \"Scrapy stats for: %s\" % domain, body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2f12327d630996e53cef6bf85091f5e78895e05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 94 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ class StatsCollector(object):\n \n         dispatcher.connect(self.open_domain, signal=signals.domain_open)\n         dispatcher.connect(self.close_domain, signal=signals.domain_closed)\n+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n     def get_value(self, key, default=None, domain=None):\n         return self._stats[domain].get(key, default)\n@@ -49,10 +50,15 @@ class StatsCollector(object):\n     def close_domain(self, domain, reason):\n         signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n         if self._dump:\n-            log.msg(\"Dumping stats:\\n\" + pprint.pformat(self._stats[domain]), domain=domain)\n+            log.msg(\"Dumping stats:\\n\" + pprint.pformat(self.get_stats(domain)), \\\n+                domain=domain)\n         del self._stats[domain]\n         signals.send_catch_log(stats_domain_closed, domain=domain, reason=reason)\n \n+    def engine_stopped(self):\n+        if self._dump:\n+            log.msg(\"Dumping global stats:\\n\" + pprint.pformat(self.get_stats()))\n+\n \n class MemoryStatsCollector(StatsCollector):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97a854a53ae160f64e9f08cf086d7786d67df861", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 12 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 42 | Churn Cumulative: 42 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,4 +3,7 @@\n from scrapy.item import ScrapedItem\n \n class GoogledirItem(ScrapedItem):\n-    pass\n+\n+    def __str__(self):\n+        return \"Google Category: name=%s url=%s\" % (self.name, self.url)\n+\n\n@@ -1,5 +1,15 @@\n-# Define yours item pipelines here\n+from scrapy.core.exceptions import DropItem\n+\n+class FilterWordsPipeline(object):\n+    \"\"\"A pipeline for filtering out items which contain certain words in their\n+    description\"\"\"\n+\n+    # put all words in lowercase\n+    words_to_filter = ['politics', 'religion']\n \n-class GoogledirPipeline(object):\n     def process_item(self, domain, item):\n+        for word in self.words_to_filter:\n+            if word in unicode(item.description).lower():\n+                raise DropItem(\"Contains forbidden word: %s\" % word)\n+        else:\n             return item\n\n@@ -1,4 +1,4 @@\n-# - Scrapy settings for googledir                                    -\n+# - Scrapy settings for googledir project\n \n import googledir\n \n@@ -13,10 +13,5 @@ TEMPLATES_DIR = '%s/templates' % googledir.__path__[0]\n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n-# uncomment if you want to add your own custom scrapy commands\n-#COMMANDS_MODULE = 'googledir.commands'\n-#COMMANDS_SETTINGS_MODULE = 'googledir.conf.commands'\n+ITEM_PIPELINES = ['googledir.pipelines.FilterWordsPipeline']\n \n-# global mail sending settings\n-#MAIL_HOST = 'localhost'\n-#MAIL_FROM = 'scrapybot@localhost'\n\n@@ -1,17 +1,15 @@\n-# -*- coding: utf8 -*-\n-import re\n-\n from scrapy.xpath import HtmlXPathSelector\n-from scrapy.link.extractors import RegexLinkExtractor\n+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.contrib.spiders import CrawlSpider, Rule\n from googledir.items import GoogledirItem\n \n class GoogleDirectorySpider(CrawlSpider):\n-    domain_name = 'google.com'\n-    start_urls = ['http://www.google.com/dirhp']\n+\n+    domain_name = 'directory.google.com'\n+    start_urls = ['http://directory.google.com/']\n \n     rules = (\n-        Rule(RegexLinkExtractor(allow='google.com/[A-Z][a-zA-Z_/]+$'),\n+        Rule(SgmlLinkExtractor(allow='directory.google.com/[A-Z][a-zA-Z_/]+$'),\n             'parse_category',\n             follow=True,\n         ),\n@@ -29,7 +27,7 @@ class GoogleDirectorySpider(CrawlSpider):\n \n             item.name = link.x('a/text()').extract()\n             item.url = link.x('a/@href').extract()\n-            item.description = link.x('font[2]/text()')\n+            item.description = link.x('font[2]/text()').extract()\n             yield item\n \n SPIDER = GoogleDirectorySpider()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92b746d866d2f09a821af67fddb9e64298c84743", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,8 @@ class SimpledbStatsCollector(StatsCollector):\n         super(SimpledbStatsCollector, self).__init__()\n         self._sdbdomain = settings['STATS_SDB_DOMAIN']\n         self._async = settings.getbool('STATS_SDB_ASYNC')\n+        sdb = boto.connect_sdb()\n+        sdb.create_domain(self._sdbdomain)\n         \n     def close_domain(self, domain, reason):\n         if self._sdbdomain:\n@@ -39,7 +41,6 @@ class SimpledbStatsCollector(StatsCollector):\n         sdb_item['domain'] = domain\n         sdb_item['timestamp'] = self._to_sdb_value(ts)\n         sdb = boto.connect_sdb()\n-        sdb.create_domain(self._sdbdomain)\n         sdb.batch_put_attributes(self._sdbdomain, {sdb_item_id: sdb_item})\n \n     def _to_sdb_value(self, obj):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc33ac4f863c93d7a16406fb36cb8c76e00a227e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 107 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import get_func_args\n \n@@ -28,7 +27,7 @@ class ItemAdaptorMeta(type):\n         cls._field_adaptors = cls._field_adaptors.copy()\n     \n         if cls.item_class:\n-            for item_field in cls.item_class.fields.keys():\n+            for item_field in cls.item_class.get_fields():\n                 if item_field in attrs:\n                     adaptor = adaptize(attrs[item_field])\n                     cls._field_adaptors[item_field] = adaptor\n@@ -36,7 +35,7 @@ class ItemAdaptorMeta(type):\n         return cls\n \n     def __getattr__(cls, name):\n-        if name in cls.item_class.fields:\n+        if name in cls.item_class.get_fields():\n             return cls.default_adaptor\n         raise AttributeError(name)\n \n\n@@ -6,10 +6,10 @@ class _ItemMeta(type):\n \n     def __new__(meta, class_name, bases, attrs):\n         cls = type.__new__(meta, class_name, bases, attrs)\n-        cls.fields = cls.fields.copy()\n+        cls._fields = cls._fields.copy()\n         for n, v in attrs.iteritems():\n             if isinstance(v, BaseField):\n-                cls.fields[n] = v\n+                cls._fields[n] = v\n                 delattr(cls, n)\n         return cls\n \n@@ -19,7 +19,7 @@ class Item(ScrapedItem):\n \n     __metaclass__ = _ItemMeta\n \n-    fields = {}\n+    _fields = {}\n \n     def __init__(self, values=None):\n         self._values = {}\n@@ -34,8 +34,8 @@ class Item(ScrapedItem):\n         if name.startswith('_'):\n             return ScrapedItem.__setattr__(self, name, value)\n \n-        if name in self.fields.keys():\n-            self._values[name] = self.fields[name].to_python(value)\n+        if name in self._fields:\n+            self._values[name] = self._fields[name].to_python(value)\n         else:\n             raise AttributeError(name)\n \n@@ -44,7 +44,7 @@ class Item(ScrapedItem):\n             return self._values[name]\n         except KeyError:\n             try:\n-                return self.fields[name].get_default()\n+                return self._fields[name].get_default()\n             except KeyError:\n                 raise AttributeError(name)\n \n@@ -52,6 +52,11 @@ class Item(ScrapedItem):\n         \"\"\"Generate a representation of this item that can be used to\n         reconstruct the item by evaluating it\n         \"\"\"\n-        values = dict((field, getattr(self, field)) for field in self.fields)\n+        values = dict((field, getattr(self, field)) for field in self._fields)\n         return \"%s(%s)\" % (self.__class__.__name__, repr(values))\n \n+    @classmethod\n+    def get_fields(cls):\n+        \"\"\"Returns the item fields\"\"\"\n+        return cls._fields\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9e3f72beeaf2caabbb411cf0297a99c9644dd12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 20 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,7 +37,7 @@ class TextResponse(Response):\n     body = property(lambda x: x._body, set_body)\n \n     def replace(self, *args, **kwargs):\n-        kwargs.setdefault('encoding', self.encoding)\n+        kwargs.setdefault('encoding', getattr(self, '_encoding', None))\n         return Response.replace(self, *args, **kwargs)\n \n     @property\n\n@@ -193,10 +193,20 @@ class ResponseTest(unittest.TestCase):\n         r2 = XmlResponse(\"http://www.example.com\", body=body)\n         self._assert_response_values(r2, 'iso-8859-1', body)\n \n-        # make sure replace() preserves the encoding of the original response\n-        body = \"New body \\xa3\"\n-        r3 = r2.replace(body=body)\n-        self._assert_response_values(r3, 'iso-8859-1', body)\n+        # make sure replace() preserves the explicit encoding passed in the constructor\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n+        r3 = XmlResponse(\"http://www.example.com\", body=body, encoding='utf-8')\n+        body2 = \"New body\"\n+        r4 = r3.replace(body=body2)\n+        self._assert_response_values(r4, 'utf-8', body2)\n+\n+        # make sure replace() rediscovers the encoding (if not given explicitly) when changing the body\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n+        r5 = XmlResponse(\"http://www.example.com\", body=body)\n+        body2 = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?><xml></xml>\"\"\"\n+        r6 = r5.replace(body=body2)\n+        self._assert_response_values(r5, 'iso-8859-1', body)\n+        self._assert_response_values(r6, 'utf-8', body2)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9867e7b2d6f562362a6391633fe9475dc9fd13ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 174 | Contributors (this commit): 3 | Commits (past 90d): 28 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,6 @@ BOT_VERSION = '1.0'\n SPIDER_MODULES = ['googledir.spiders']\n NEWSPIDER_MODULE = 'googledir.spiders'\n TEMPLATES_DIR = '%s/templates' % googledir.__path__[0]\n-ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % googledir.__path__[0]\n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n\n@@ -84,8 +84,6 @@ DOWNLOADER_STATS = True\n \n DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n-ENABLED_SPIDERS_FILE = ''\n-\n EXTENSIONS = [\n     'scrapy.stats.corestats.CoreStats',\n     'scrapy.xpath.extension.ResponseLibxml2',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29c92fe818aa58c50377c7d22568015920023198", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 6 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,11 +15,15 @@ def parse_uri(db_uri):\n     if not (isinstance(db_uri, basestring) and db_uri.startswith('mysql://')):\n         raise URIValidationError(\"Incorrect MySQL URI: %s\" % db_uri)\n \n-    m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/]+)/(?P<db>.*)$\", db_uri)\n+    m = re.search(r\"mysql:\\/\\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\\d+))?/(?P<db>.*)$\", db_uri)\n     if m:\n         d = m.groupdict()\n         if d['passwd'] is None:\n             del(d['passwd'])\n+        if d['port'] is None:\n+            del(d['port'])\n+        else:\n+            d['port'] = int(d['port'])\n         return d\n \n def mysql_connect(db_uri_or_dict, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a10e1dabd8ab5ec911c5f452599d72311541915", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 16 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 50 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,6 @@\n-from __future__ import with_statement\n-\n-import os\n-import string\n import shutil\n+import string\n+from os.path import join, dirname, abspath, exists\n \n from scrapy.spider import spiders\n from scrapy.command import ScrapyCommand\n@@ -11,8 +9,6 @@ from scrapy.utils.misc import render_templatefile, string_camelcase\n \n \n class Command(ScrapyCommand):\n-    \"\"\" Childs can define custom tvars \"\"\"\n-    custom_tvars = {}\n \n     def syntax(self):\n         return \"[options] <spider_name> <spider_domain_name>\"\n@@ -22,49 +18,52 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--template\", dest=\"template\", help=\"Uses a custom template.\", default=\"crawl\")\n-        parser.add_option(\"--force\", dest=\"force\", help=\"If the spider already exists, overwrite it with the template\", action=\"store_true\")\n+        parser.add_option(\"--template\", dest=\"template\", default=\"crawl\",\n+            help=\"Uses a custom template.\")\n+        parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n+            help=\"If the spider already exists, overwrite it with the template\")\n \n     def run(self, args, opts):\n         if len(args) < 2:\n             return False\n \n-        template_file = os.path.join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % opts.template)\n-        if not os.path.exists(template_file):\n-            print \"Template named %s.tmpl does not exist\" % opts.template\n+        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % opts.template)\n+        if not exists(template_file):\n+            print \"Template '%s.tmpl' not found\" % opts.template\n             return\n \n         name = self.normalize_name(args[0])\n-        site = args[1]\n+        domain = args[1]\n         spiders_dict = spiders.asdict()\n-        if site in spiders_dict.keys():\n+        if domain in spiders_dict.keys():\n             if opts.force:\n-                print \"Spider '%s' already exists. Overwriting it...\" % name\n+                print \"Spider '%s' already exists. Overwriting it...\" % domain\n             else:\n-                print \"Spider '%s' already exists\" % name\n+                print \"Spider '%s' already exists\" % domain\n                 return\n-        self._genspider(name, site, template_file)\n+        self._genspider(name, domain, template_file)\n \n     def normalize_name(self, name):\n-        name = name.replace('-', '_') # - are replaced by _, for valid python modules\n-        if name[0] not in string.letters: # name must start with a letter, for valid python modules\n+        # - are replaced by _, for valid python modules\n+        name = name.replace('-', '_')\n+        # name must start with a letter, for valid python modules\n+        if name[0] not in string.letters:\n             name = \"a\" + name\n             print \"Spider names must start with a letter; converted to %s.\" % name\n         return name\n         \n-    def _genspider(self, name, site, template_file):\n-        \"\"\" Generate spider \"\"\"\n+    def _genspider(self, name, domain, template_file):\n+        \"\"\"Generate the spider module, based on the given template\"\"\"\n         tvars = {\n             'project_name': settings.get('PROJECT_NAME'),\n             'ProjectName': string_camelcase(settings.get('PROJECT_NAME')),\n             'name': name,\n-            'site': site,\n+            'site': domain,\n             'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])\n         }\n-        tvars.update(self.custom_tvars)\n \n         spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n-        spiders_dir = os.path.abspath(os.path.dirname(spiders_module.__file__))\n+        spiders_dir = abspath(dirname(spiders_module.__file__))\n         spider_file = '%s/%s.py' % (spiders_dir, name)\n \n         shutil.copyfile(template_file, spider_file)\n\n@@ -0,0 +1 @@\n+LOG_ENABLED = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6fcbd03bff4e871ae908ac1d7142c9249032fe44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 143 | Files Changed: 7 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 143 | Churn Cumulative: 178 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,20 +0,0 @@\n-# Define here the models for your scraped items\n-\n-from scrapy.contrib_exp import newitem \n-\n-from scrapy.contrib_exp.newitem.adaptors import ItemAdaptor, adaptor\n-from scrapy.contrib_exp.adaptors import extract, strip\n-\n-\n-class GoogledirItem(newitem.Item):\n-    name = newitem.StringField()\n-    url = newitem.StringField()\n-    description = newitem.StringField()\n-\n-\n-class GoogledirItemAdaptor(ItemAdaptor):\n-    item_class = GoogledirItem\n-\n-    name = adaptor(extract, strip)\n-    url = adaptor(extract, strip)\n-    description = adaptor(extract, strip)\n\n@@ -1,5 +0,0 @@\n-# Define yours item pipelines here\n-\n-class GoogledirPipeline(object):\n-    def process_item(self, domain, item):\n-        return item\n\n@@ -1,77 +0,0 @@\n-import googledir\n-\n-# ---------------------------------------------------------------------------\n-# - Scrapy settings for googledir                                    -\n-# ---------------------------------------------------------------------------\n-PROJECT_NAME = 'googledir'\n-\n-BOT_NAME = PROJECT_NAME\n-BOT_VERSION = '1.0'\n-\n-SPIDER_MODULES = ['googledir.spiders']\n-NEWSPIDER_MODULE = 'googledir.spiders'\n-TEMPLATES_DIR = '%s/templates' % googledir.__path__[0]\n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-\n-# The amount of time (in secs) that the downloader should wait before\n-# downloading consecutive pages from the same spider. This can be used\n-# to throttle the crawling speed to avoid hitting servers too\n-# hard. Decimal numbers are supported. Example:\n-# DOWNLOAD_DELAY = 2.5\n-DOWNLOAD_TIMEOUT = 600\n-\n-# use this spider class as default when no spider was found for a given url\n-#DEFAULT_SPIDER = 'scrapy.contrib.spiders.generic.GenericSpider'\n-\n-# uncomment if you want to add your own custom scrapy commands\n-#COMMANDS_MODULE = 'googledir.commands'\n-#COMMANDS_SETTINGS_MODULE = 'googledir.conf.commands'\n-\n-#Global timeout between sucessive downloads (can be overrided by spider\n-#attribute download_timeout\n-#DOWNLOAD_TIMEOUT = 0\n-\n-MYSQL_CONNECTION_SETTINGS = {\"charset\": \"utf8\" }\n-MYSQL_CONNECTION_PING_PERIOD = 600\n-\n-SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n-\n-#HTTPCACHE_DIR = '/tmp/cache2'  # if set, enables HTTP cache\n-#HTTPCACHE_IGNORE_MISSING = 0   # ignore requests not in cache\n-#HTTPCACHE_SECTORIZE = 1         # sectorize domains to distribute storage among servers\n-\n-#STATS_ENABLED = 1   # enable stats\n-#STATS_CLEANUP = 0   # cleanup domain stats when a domain is closed (saves memory)\n-#STATS_DEBUG = 0     # log stats on domain closed\n-\n-EXTENSIONS = (\n-    'scrapy.management.web.WebConsole',\n-    'scrapy.management.telnet.TelnetConsole',\n-)\n-\n-ITEM_PIPELINES = (\n-    'googledir.pipelines.GoogledirPipeline',\n-)\n-\n-#DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n-#DEPTH_STATS = 1    # enable depth stats\n-\n-# Limit URL length. See: http://www.boutell.com/newfaq/misc/urllength.html\n-URLLENGTH_LIMIT = 2083\n-\n-#WEBCONSOLE_ENABLED = 1\n-#WEBCONSOLE_PORT = 8060  # if not set uses a dynamic port\n-\n-#TELNETCONSOLE_ENABLED = 1\n-#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n-\n-# global mail sending settings\n-#MAIL_HOST = 'localhost'\n-#MAIL_FROM = 'scrapybot@localhost'\n-\n-# scrapy webservice\n-WS_ENABLED = 0\n-\n-SPIDERPROFILER_ENABLED = 0\n\n\n@@ -1,34 +0,0 @@\n-# -*- coding: utf8 -*-\n-import re\n-\n-from scrapy.xpath import HtmlXPathSelector\n-from scrapy.contrib.spiders import CrawlSpider, rule\n-from scrapy.utils.misc import items_to_csv\n-\n-from googledir.items import GoogledirItem, GoogledirItemAdaptor\n-\n-class GoogleDirectorySpider(CrawlSpider):\n-    domain_name = 'google.com'\n-    start_urls = ['http://www.google.com/dirhp']\n-\n-    rules = (\n-        rule('google.com/[A-Z][a-zA-Z_/]+$', 'parse_category', follow=True),\n-    )\n-    csv_file = open('scraped_items.csv', 'ab+')\n-    \n-    def parse_category(self, response):\n-        # The selector we're going to use in order to extract data from the page\n-        hxs = HtmlXPathSelector(response)\n-\n-        # The path to website links in directory page\n-        links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n-\n-        for link in links:\n-            extractor = GoogledirItemAdaptor()\n-            extractor.name =  link.x('a/text()')\n-            extractor.url = link.x('a/@href')\n-            extractor.description = link.x('font[2]/text()')\n-            item = extractor.item_instance\n-            yield item\n-\n-SPIDER = GoogleDirectorySpider()\n\n@@ -1,7 +0,0 @@\n-#!/usr/bin/env python\n-\n-import os\n-os.environ.setdefault('SCRAPYSETTINGS_MODULE', 'googledir.settings')\n-\n-from scrapy.command.cmdline import execute\n-execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#894a5d81d3f24ac2b9638393ef551f550e72017a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 210 | Lines Deleted: 147 | Files Changed: 4 | Hunks: 83 | Methods Changed: 41 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 357 | Churn Cumulative: 1133 | Contributors (this commit): 3 | Commits (past 90d): 31 | Contributors (cumulative): 12 | DMM Complexity: 0.8\n\nDIFF:\n@@ -27,7 +27,7 @@ class ItemAdaptorMeta(type):\n         cls._field_adaptors = cls._field_adaptors.copy()\n     \n         if cls.item_class:\n-            for item_field in cls.item_class.get_fields():\n+            for item_field in cls.item_class.fields:\n                 if item_field in attrs:\n                     adaptor = adaptize(attrs[item_field])\n                     cls._field_adaptors[item_field] = adaptor\n@@ -35,7 +35,7 @@ class ItemAdaptorMeta(type):\n         return cls\n \n     def __getattr__(cls, name):\n-        if name in cls.item_class.get_fields():\n+        if name in cls.item_class.fields:\n             return cls.default_adaptor\n         raise AttributeError(name)\n \n@@ -61,14 +61,14 @@ class ItemAdaptor(object):\n \n         adaptor_args = {'response': self._response, 'item': self.item_instance}\n         ovalue = fa(value, adaptor_args=adaptor_args)\n-        setattr(self.item_instance, name, ovalue)\n+        self.item_instance[name] = ovalue\n \n     def __getattribute__(self, name):\n         if (name.startswith('_') or name.startswith('item_') \\\n                 or name == 'default_adaptor'):\n             return object.__getattribute__(self, name)\n \n-        return getattr(self.item_instance, name)\n+        return self.item_instance[name]\n \n \n def adaptor(*funcs, **default_adaptor_args):\n\n@@ -1,3 +1,5 @@\n+from UserDict import DictMixin\n+\n from scrapy.item import ScrapedItem\n from scrapy.contrib_exp.newitem.fields import BaseField\n \n@@ -5,58 +7,60 @@ from scrapy.contrib_exp.newitem.fields import BaseField\n class _ItemMeta(type):\n \n     def __new__(meta, class_name, bases, attrs):\n-        cls = type.__new__(meta, class_name, bases, attrs)\n-        cls._fields = cls._fields.copy()\n+        fields = {}\n+        new_attrs = {}\n         for n, v in attrs.iteritems():\n             if isinstance(v, BaseField):\n-                cls._fields[n] = v\n-                delattr(cls, n)\n+                fields[n] = v\n+            else:\n+                new_attrs[n] = v\n+\n+        cls = type.__new__(meta, class_name, bases, new_attrs)\n+        cls.fields = cls.fields.copy()\n+        cls.fields.update(fields)\n         return cls\n \n \n-class Item(ScrapedItem):\n-    \"\"\" This is the base class for all scraped items. \"\"\"\n+class Item(DictMixin, ScrapedItem):\n \n     __metaclass__ = _ItemMeta\n \n-    _fields = {}\n+    fields = {}\n \n-    def __init__(self, values=None):\n+    def __init__(self, *args, **kwargs):\n         self._values = {}\n-        if isinstance(values, dict):\n-            for k, v in values.iteritems():\n-                setattr(self, k, v)\n-        elif values is not None:\n-            raise TypeError(\"Items must be instantiated with dicts, got %s\" % \\\n-                type(values).__name__)\n         \n-    def __setattr__(self, name, value):\n-        if name.startswith('_'):\n-            return ScrapedItem.__setattr__(self, name, value)\n+        # load default values\n+        for name, field in self.fields.iteritems():\n+            default = field.get_default()\n+            if default:\n+                self._values[name] = default\n             \n-        if name in self._fields:\n-            self._values[name] = self._fields[name].to_python(value)\n-        else:\n-            raise AttributeError(name)\n+        # load init values\n+        if args or kwargs: # don't instantiate dict for simple (most common) case\n+            for k, v in dict(*args, **kwargs).iteritems():\n+                self[k] = v\n \n-    def __getattr__(self, name):\n-        try:\n-            return self._values[name]\n-        except KeyError:\n-            try:\n-                return self._fields[name].get_default()\n-            except KeyError:\n-                raise AttributeError(name)\n+    def __getitem__(self, key):\n+        return self._values[key]\n+\n+    def __setitem__(self, key, value):\n+        self._values[key] = self.fields[key].to_python(value)\n+\n+    def __delitem__(self, key):\n+        del self._values[key]\n+\n+    def keys(self):\n+        return self._values.keys()\n \n     def __repr__(self):\n         \"\"\"Generate a representation of this item that can be used to\n         reconstruct the item by evaluating it\n         \"\"\"\n-        values = dict((field, getattr(self, field)) for field in self._fields)\n-        return \"%s(%s)\" % (self.__class__.__name__, repr(values))\n+        values = ', '.join('%s=%r' % field for field in self.iteritems())\n+        return \"%s(%s)\" % (self.__class__.__name__, values)\n \n-    @classmethod\n-    def get_fields(cls):\n-        \"\"\"Returns the item fields\"\"\"\n-        return cls._fields\n+    def get_id(self):\n+        \"\"\"Returns the unique id for this item.\"\"\"\n+        raise NotImplementedError\n \n\n@@ -44,7 +44,7 @@ class ItemAdaptorTest(unittest.TestCase):\n     def test_basic(self):\n         ia = TestAdaptor()\n         ia.name = u'marta'\n-        self.assertEqual(ia.item_instance.name, u'Marta')\n+        self.assertEqual(ia.item_instance['name'], u'Marta')\n         self.assertEqual(ia.name, u'Marta')\n \n     def test_defaultadaptor(self):\n\n@@ -13,50 +13,54 @@ class NewItemTest(unittest.TestCase):\n             name = fields.TextField()\n \n         i = TestItem()\n-        i.name = u'name'\n-        self.assertEqual(i.name, u'name')\n+        i['name'] = u'name'\n+        self.assertEqual(i['name'], u'name')\n \n     def test_init(self):\n         class TestItem(Item):\n             name = fields.TextField()\n \n         i = TestItem()\n-        self.assert_(i.name is None)\n+        self.assertRaises(KeyError, i.__getitem__, 'name')\n \n-        i2 = TestItem({'name': u'john doe'})\n-        self.assertEqual(i2.name, u'john doe')\n+        i2 = TestItem(name=u'john doe')\n+        self.assertEqual(i2['name'], u'john doe')\n \n-        self.assertRaises(TypeError, TestItem, name=u'john doe')\n+        i3 = TestItem({'name': u'john doe'})\n+        self.assertEqual(i3['name'], u'john doe')\n \n-        self.assertRaises(AttributeError, TestItem, {'name': u'john doe',\n+        i4 = TestItem(i3)\n+        self.assertEqual(i4['name'], u'john doe')\n+\n+        self.assertRaises(KeyError, TestItem, {'name': u'john doe',\n                                                'other': u'foo'})\n \n+        self.assertRaises(TypeError, TestItem, name=3)\n+\n     def test_multi(self):\n         class TestMultiItem(Item):\n             name = fields.TextField()\n             names = fields.MultiValuedField(fields.TextField)\n \n         i = TestMultiItem()\n-        i.name = u'name'\n-        i.names = [u'name1', u'name2']\n-        self.assertEqual(i.names, [u'name1', u'name2'])\n+        i['name'] = u'name'\n+        i['names'] = [u'name1', u'name2']\n+        self.assertEqual(i['names'], [u'name1', u'name2'])\n \n     def test_invalid_field(self):\n         class TestItem(Item):\n             pass\n \n         i = TestItem()\n-\n-        self.assertRaises(AttributeError, setattr, i, 'field', 'text')\n-\n-        self.assertRaises(AttributeError, getattr, i, 'field')\n+        self.assertRaises(KeyError, i.__setitem__, 'field', 'text')\n+        self.assertRaises(KeyError, i.__getitem__, 'field')\n \n     def test_default_value(self):\n         class TestItem(Item):\n             name = fields.TextField(default=u'John')\n  \n         i = TestItem()\n-        self.assertEqual(i.name, u'John')\n+        self.assertEqual(i['name'], u'John')\n \n     def test_wrong_default(self):\n         def set_wrong_default():\n@@ -70,8 +74,8 @@ class NewItemTest(unittest.TestCase):\n             name = fields.TextField()\n  \n         i = TestItem()\n-        i.name = (u'John', u'Doe')\n-        self.assertEqual(i.name, u'John Doe')\n+        i['name'] = (u'John', u'Doe')\n+        self.assertEqual(i['name'], u'John Doe')\n \n     def test_repr(self):\n         class TestItem(Item):\n@@ -79,15 +83,15 @@ class NewItemTest(unittest.TestCase):\n             number = fields.IntegerField()\n \n         i = TestItem()\n-        i.name = u'John Doe'\n-        i.number = '123'\n+        i['name'] = u'John Doe'\n+        i['number'] = '123'\n         itemrepr = repr(i)\n         self.assertEqual(itemrepr,\n-                         \"TestItem({'name': u'John Doe', 'number': 123})\")\n+                         \"TestItem(name=u'John Doe', number=123)\")\n \n         i2 = eval(itemrepr)\n-        self.assertEqual(i2.name, 'John Doe')\n-        self.assertEqual(i2.number, 123)\n+        self.assertEqual(i2['name'], 'John Doe')\n+        self.assertEqual(i2['number'], 123)\n \n     def test_private_attr(self):\n         class TestItem(Item):\n@@ -102,18 +106,73 @@ class NewItemTest(unittest.TestCase):\n             name = fields.TextField()\n \n             def get_name(self):\n-                return self.name\n+                return self['name']\n \n             def change_name(self, name):\n-                self.name = name\n+                self['name'] = name\n \n         i = TestItem()\n-        self.assertEqual(i.get_name(), None)\n-        i.name = u'lala'\n+        self.assertRaises(KeyError, i.get_name)\n+        i['name'] = u'lala'\n         self.assertEqual(i.get_name(), u'lala')\n         i.change_name(u'other')\n         self.assertEqual(i.get_name(), 'other')\n \n+    def test_metaclass(self):\n+        class TestItem(Item):\n+            name = fields.TextField()\n+            keys = fields.TextField() \n+            values = fields.TextField() \n+\n+        i = TestItem()\n+        i['name'] = u'John'\n+        self.assertEqual(i.keys(), ['name'])\n+        self.assertEqual(i.values(), ['John'])\n+\n+        i['keys'] = u'Keys'\n+        i['values'] = u'Values'\n+        self.assertEqual(i.keys(), ['keys', 'values', 'name'])\n+        self.assertEqual(i.values(), [u'Keys', u'Values', u'John'])\n+\n+    def test_metaclass_inheritance(self):\n+        class BaseItem(Item):\n+            name = fields.TextField()\n+            keys = fields.TextField() \n+            values = fields.TextField() \n+\n+        class TestItem(BaseItem):\n+            keys = fields.IntegerField()\n+\n+        i = TestItem()\n+        i['keys'] = 3\n+        self.assertEqual(i.keys(), ['keys'])\n+        self.assertEqual(i.values(), [3])\n+\n+    def test_to_dict(self):\n+        class TestItem(Item):\n+            name = fields.TextField()\n+\n+        i = TestItem()\n+        i['name'] = u'John'\n+        self.assertEqual(dict(i), {'name': u'John'})\n+\n+    def test_id(self):\n+        class TestItem(Item):\n+            name = fields.TextField()\n+\n+        i = TestItem()\n+        self.assertRaises(NotImplementedError, i.get_id)\n+\n+        class IdItem(Item):\n+            id = fields.IntegerField()\n+\n+            def get_id(self):\n+                return self['id']\n+\n+        i = IdItem()\n+        i['id'] = 11\n+        self.assertEqual(i.get_id(), 11)\n+\n \n class NewItemFieldsTest(unittest.TestCase):\n     \n@@ -129,20 +188,20 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i = TestItem()\n \n-        i.field = True\n-        self.assert_(i.field is True)\n+        i['field'] = True\n+        self.assert_(i['field'] is True)\n     \n-        i.field = 1\n-        self.assert_(i.field is True)\n+        i['field'] = 1\n+        self.assert_(i['field'] is True)\n \n-        i.field = False\n-        self.assert_(i.field is False)\n+        i['field'] = False\n+        self.assert_(i['field'] is False)\n \n-        i.field = 0\n-        self.assert_(i.field is False)\n+        i['field'] = 0\n+        self.assert_(i['field'] is False)\n \n-        i.field = None\n-        self.assert_(i.field is False)\n+        i['field'] = None\n+        self.assert_(i['field'] is False)\n \n     def test_date_field(self):\n         class TestItem(Item):\n@@ -151,21 +210,21 @@ class NewItemFieldsTest(unittest.TestCase):\n         i = TestItem()\n \n         d_today = datetime.date.today()\n-        i.field = d_today\n-        self.assertEqual(i.field, d_today)\n+        i['field'] = d_today\n+        self.assertEqual(i['field'], d_today)\n \n         dt_today = datetime.datetime.today()\n-        i.field = dt_today\n-        self.assertEqual(i.field, dt_today.date())\n+        i['field'] = dt_today\n+        self.assertEqual(i['field'], dt_today.date())\n  \n-        i.field = '2009-05-21'\n-        self.assertEqual(i.field, datetime.date(2009, 5, 21))\n+        i['field'] = '2009-05-21'\n+        self.assertEqual(i['field'], datetime.date(2009, 5, 21))\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '21-05-2009')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '21-05-2009')\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-51')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '2009-05-51')\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n     def test_datetime_field(self):\n         class TestItem(Item):\n@@ -174,34 +233,34 @@ class NewItemFieldsTest(unittest.TestCase):\n         i = TestItem()\n \n         dt_today = datetime.datetime.today()\n-        i.field = dt_today\n-        self.assertEqual(i.field, dt_today)\n+        i['field'] = dt_today\n+        self.assertEqual(i['field'], dt_today)\n \n         d_today = datetime.date.today()\n-        i.field = d_today\n-        self.assertEqual(i.field, datetime.datetime(d_today.year,\n+        i['field'] = d_today\n+        self.assertEqual(i['field'], datetime.datetime(d_today.year,\n                                                     d_today.month, d_today.day))\n \n-        i.field = '2009-05-21 11:08:10.100'\n-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8, 10,\n+        i['field'] = '2009-05-21 11:08:10.100'\n+        self.assertEqual(i['field'], datetime.datetime(2009, 5, 21, 11, 8, 10,\n                                                     100))\n  \n-        i.field = '2009-05-21 11:08:10'\n-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8, 10))\n+        i['field'] = '2009-05-21 11:08:10'\n+        self.assertEqual(i['field'], datetime.datetime(2009, 5, 21, 11, 8, 10))\n \n-        i.field = '2009-05-21 11:08'\n-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8))\n+        i['field'] = '2009-05-21 11:08'\n+        self.assertEqual(i['field'], datetime.datetime(2009, 5, 21, 11, 8))\n \n-        i.field = '2009-05-21'\n-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21))\n+        i['field'] = '2009-05-21'\n+        self.assertEqual(i['field'], datetime.datetime(2009, 5, 21))\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-21 11:08:10.usecs')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '2009-05-21 11:08:10.usecs')\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '21-05-2009')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '21-05-2009')\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-51')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '2009-05-51')\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n     def test_decimal_field(self):\n         class TestItem(Item):\n@@ -209,15 +268,15 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i = TestItem()\n \n-        i.field = decimal.Decimal('3.14')\n-        self.assertEqual(i.field, decimal.Decimal('3.14'))\n+        i['field'] = decimal.Decimal('3.14')\n+        self.assertEqual(i['field'], decimal.Decimal('3.14'))\n \n-        i.field = '3.14'\n-        self.assertEqual(i.field, decimal.Decimal('3.14'))\n+        i['field'] = '3.14'\n+        self.assertEqual(i['field'], decimal.Decimal('3.14'))\n         \n-        self.assertRaises(decimal.InvalidOperation, setattr, i, 'field', 'text')\n+        self.assertRaises(decimal.InvalidOperation, i.__setitem__, 'field', 'text')\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n     def test_float_field(self):\n         class TestItem(Item):\n@@ -225,15 +284,15 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i = TestItem()\n \n-        i.field = 3.14\n-        self.assertEqual(i.field, 3.14)\n+        i['field'] = 3.14\n+        self.assertEqual(i['field'], 3.14)\n \n-        i.field = '3.14'\n-        self.assertEqual(i.field, 3.14)\n+        i['field'] = '3.14'\n+        self.assertEqual(i['field'], 3.14)\n         \n-        self.assertRaises(ValueError, setattr, i, 'field', 'text')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', 'text')\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n     def test_integer_field(self):\n         class TestItem(Item):\n@@ -241,15 +300,15 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i = TestItem()\n \n-        i.field = 3\n-        self.assertEqual(i.field, 3)\n+        i['field'] = 3\n+        self.assertEqual(i['field'], 3)\n \n-        i.field = '3'\n-        self.assertEqual(i.field, 3)\n+        i['field'] = '3'\n+        self.assertEqual(i['field'], 3)\n \n-        self.assertRaises(ValueError, setattr, i, 'field', 'text')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', 'text')\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n     def test_text_field(self):\n         class TestItem(Item):\n@@ -257,32 +316,32 @@ class NewItemFieldsTest(unittest.TestCase):\n \n         i = TestItem()\n \n-        i.field = u'hello'\n-        self.assertEqual(i.field, u'hello')\n-        self.assert_(isinstance(i.field, unicode))\n+        i['field'] = u'hello'\n+        self.assertEqual(i['field'], u'hello')\n+        self.assert_(isinstance(i['field'], unicode))\n \n         # must be unicode!\n-        self.assertRaises(TypeError, setattr, i, 'field', 'string')\n+        self.assertRaises(TypeError, i.__setitem__, 'field', 'string')\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n         def set_invalid_value():\n-            i.field = 3 \n+            i['field'] = 3 \n \n         self.assertRaises(TypeError, set_invalid_value)\n \n         i = TestItem()\n-        i.field = [u'hello', u'world']\n-        self.assertEqual(i.field, u'hello world')\n-        self.assert_(isinstance(i.field, unicode))\n+        i['field'] = [u'hello', u'world']\n+        self.assertEqual(i['field'], u'hello world')\n+        self.assert_(isinstance(i['field'], unicode))\n \n-        self.assertRaises(TypeError, setattr, i, 'field', [u'hello', 3, u'world']) \n-        self.assertRaises(TypeError, setattr, i, 'field', [u'hello', 'world']) \n+        self.assertRaises(TypeError, i.__setitem__, 'field', [u'hello', 3, u'world']) \n+        self.assertRaises(TypeError, i.__setitem__, 'field', [u'hello', 'world']) \n \n         i = TestItem()\n-        i.field = []\n-        self.assert_(isinstance(i.field, unicode))\n-        self.assertEqual(i.field, '')\n+        i['field'] = []\n+        self.assert_(isinstance(i['field'], unicode))\n+        self.assertEqual(i['field'], '')\n \n     def test_time_field(self):\n         class TestItem(Item):\n@@ -291,27 +350,27 @@ class NewItemFieldsTest(unittest.TestCase):\n         i = TestItem()\n \n         dt_t = datetime.time(11, 8, 10, 100)\n-        i.field = dt_t\n-        self.assertEqual(i.field, dt_t)\n+        i['field'] = dt_t\n+        self.assertEqual(i['field'], dt_t)\n \n-        self.assertRaises(TypeError, setattr, i, 'field', None)\n+        self.assertRaises(TypeError, i.__setitem__, 'field', None)\n \n         dt_dt = datetime.datetime.today()\n-        i.field = dt_dt\n-        self.assertEqual(i.field, dt_dt.time)\n+        i['field'] = dt_dt\n+        self.assertEqual(i['field'], dt_dt.time)\n \n-        i.field = '11:08:10.100'\n-        self.assertEqual(i.field, datetime.time(11, 8, 10, 100))\n+        i['field'] = '11:08:10.100'\n+        self.assertEqual(i['field'], datetime.time(11, 8, 10, 100))\n  \n-        i.field = '11:08:10'\n-        self.assertEqual(i.field, datetime.time(11, 8, 10))\n+        i['field'] = '11:08:10'\n+        self.assertEqual(i['field'], datetime.time(11, 8, 10))\n \n-        i.field = '11:08'\n-        self.assertEqual(i.field, datetime.time(11, 8))\n+        i['field'] = '11:08'\n+        self.assertEqual(i['field'], datetime.time(11, 8))\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '11:08:10.usecs')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '11:08:10.usecs')\n \n-        self.assertRaises(ValueError, setattr, i, 'field', '25:08:10')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', '25:08:10')\n \n-        self.assertRaises(ValueError, setattr, i, 'field', 'string')\n+        self.assertRaises(ValueError, i.__setitem__, 'field', 'string')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
