{"custom_id": "scrapy#e1f4144391faceb5c6488df9d28a424d300203c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 946 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -176,9 +176,6 @@ class Downloader(object):\n \n         return dfd.addBoth(finish_transferring)\n \n-    def is_idle(self):\n-        return not self.slots\n-\n     def close(self):\n         self._slot_gc_loop.stop()\n         for slot in self.slots.itervalues():\n\n@@ -7,7 +7,6 @@ def get_engine_status(engine):\n     global_tests = [\n         \"time()-engine.start_time\",\n         \"engine.has_capacity()\",\n-        \"engine.downloader.is_idle()\",\n         \"len(engine.downloader.slots)\",\n         \"len(engine.downloader.active)\",\n         \"engine.scraper.is_idle()\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4018d25a9b968ce316bf48ab9322720163d14043", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 210 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 18 | Complexity Δ (Sum/Max): 37/35 | Churn Δ: 210 | Churn Cumulative: 603 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.8636363636363636\n\nDIFF:\n@@ -0,0 +1,198 @@\n+\"\"\"Download handlers for http and https schemes\"\"\"\n+\n+from twisted.internet import reactor\n+\n+from scrapy.exceptions import NotSupported\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy import optional_features\n+\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+from scrapy.http import Headers as ScrapyHeaders\n+from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.responsetypes import responsetypes\n+\n+\n+from twisted.internet import defer, reactor, protocol\n+from twisted.web.client import Agent, ProxyAgent, ResponseDone, WebClientContextFactory\n+\n+from twisted.web.http_headers import Headers\n+from twisted.web._newclient import Response\n+\n+from twisted.web.http import PotentialDataLoss\n+from twisted.web._newclient import ResponseFailed\n+from twisted.web.iweb import IBodyProducer\n+from twisted.internet.error import TimeoutError\n+\n+from twisted.internet.endpoints import TCP4ClientEndpoint\n+\n+from time import time\n+from urlparse import urldefrag\n+\n+from zope.interface import implements\n+\n+from urlparse import urlparse, urlunparse, urldefrag\n+\n+def _parsed_url_args(parsed):\n+    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n+    host = parsed.hostname\n+    port = parsed.port\n+    scheme = parsed.scheme\n+    netloc = parsed.netloc\n+    if port is None:\n+        port = 443 if scheme == 'https' else 80\n+    return scheme, netloc, host, port, path\n+\n+def _parse(url):\n+    url = url.strip()\n+    parsed = urlparse(url)\n+    return _parsed_url_args(parsed)\n+\n+\n+class ScrapyAgent(object):\n+    def __init__(self, reactor, contextFactory=WebClientContextFactory(),\n+                 connectTimeout=180, bindAddress=None):\n+        self._reactor = reactor\n+        self._agent = Agent(self._reactor,\n+            contextFactory=contextFactory,\n+            connectTimeout=connectTimeout,\n+            bindAddress=bindAddress)\n+\n+    def bindRequest(self, request):\n+        self._scrapyrequest = request\n+\n+        proxy = self._scrapyrequest.meta.get('proxy')\n+        if proxy is not None and proxy != '':\n+            scheme, _, host, port, _ = _parse(proxy)\n+            endpoint = TCP4ClientEndpoint(self._reactor, host, port)\n+            self._agent = ProxyAgent(endpoint)\n+            self._agent._proxyEndpoint._timeout = request.meta.get('download_timeout') or self._agent._proxyEndpoint._timeout\n+        else:\n+            self._agent._connectTimeout = request.meta.get('download_timeout') or self._agent._connectTimeout\n+\n+    def launch(self):\n+        self._scrapyrequest._tw_start_time = time()\n+        d = self._agent.request(\n+                self._scrapyrequest.method,\n+                urldefrag(self._scrapyrequest.url)[0],\n+                Headers(self._scrapyrequest.headers),\n+                ScrapyAgentRequestBodyProducer(self._scrapyrequest.body) if (self._scrapyrequest.body is not None) else None)\n+        return d\n+\n+\n+class ScrapyAgentRequestBodyProducer(object):\n+    implements(IBodyProducer)\n+\n+    def __init__(self, body):\n+        self.body = body\n+        self.length = len(body)\n+\n+    def startProducing(self, consumer):\n+        consumer.write(self.body)\n+        return defer.succeed(None)\n+\n+    def pauseProducing(self):\n+        pass\n+\n+    def stopProducing(self):\n+        pass\n+\n+\n+from cStringIO import StringIO\n+class ScrapyAgentResponseBodyReader(protocol.Protocol):\n+\n+    def __init__(self, finished, response, scrapyRequest, debug=0):\n+\n+        # finished is the deferred that will be fired\n+        self._finished = finished\n+\n+        self.debug = debug\n+\n+        self.status = int(response.code)\n+        self.resp_headers = list(response.headers.getAllRawHeaders())\n+\n+        self._scrapyrequest = scrapyRequest\n+        self._scrapyrequest._tw_headers_time = time()\n+        self._scrapyrequest.meta['download_latency'] = self._scrapyrequest._tw_headers_time - self._scrapyrequest._tw_start_time\n+\n+        # body\n+        self.bodyBuffer = StringIO()\n+\n+    def dataReceived(self, bodyBytes):\n+        if self.debug > 2:\n+            print \"dataReceived\", len(bodyBytes), [bodyBytes[0:min(len(bodyBytes), 16)]]\n+        self.bodyBuffer.write(bodyBytes)\n+\n+    def connectionLost(self, reason):\n+\n+        if reason.check(PotentialDataLoss):\n+            if self.debug > 2:\n+                print \"PotentialDataLoss\"\n+\n+        elif reason.check(ResponseFailed):\n+            if self.debug > 2:\n+                print \"ResponseFailed\"\n+                print reason.getErrorMessage()\n+                reason.getBriefTraceback()\n+\n+        elif reason.check(ResponseDone):\n+            if self.debug > 2:\n+                print \"ReponseDone\"\n+                print \"connection lost: %s\" % reason\n+                print 'Finished receiving body:', reason.getErrorMessage()\n+\n+        # fire the deferred with Scrapy Response object\n+        self._finished.callback(self._build_response())\n+\n+\n+    def _build_response(self):\n+        headers = ScrapyHeaders(self.resp_headers)\n+        respcls = responsetypes.from_args(headers=headers, url=urldefrag(self._scrapyrequest.url)[0])\n+        return respcls(\n+            url=urldefrag(self._scrapyrequest.url)[0],\n+            status=self.status,\n+            headers=headers,\n+            body=self.bodyBuffer.getvalue())\n+\n+\n+class HttpDownloadHandler(object):\n+\n+    def __init__(self, httpclientfactory=None):\n+        self.debug = False\n+\n+\n+    def download_request(self, request, spider):\n+        \"\"\"Return a deferred for the HTTP download\"\"\"\n+\n+        agent = ScrapyAgent(reactor)\n+        agent.bindRequest(request)\n+\n+        d = agent.launch()\n+        d.addCallback(self._agent_callback, request)\n+        d.addErrback(self._agent_errback, request)\n+\n+        return d\n+\n+\n+    def _agent_callback(self, response, request):\n+        finished = defer.Deferred()\n+        reader = ScrapyAgentResponseBodyReader(finished, response, request, debug = 0)\n+        response.deliverBody(reader)\n+\n+        if request.method != 'HEAD':\n+            return finished\n+        else:\n+            return reader._build_response()\n+\n+\n+    def _agent_errback(self, failure, request):\n+        if self.debug:\n+            print \"HttpDownloadHandler: errback called!\"\n+            print failure.getErrorMessage()\n+            failure.getBriefTraceback()\n+            failure.printTraceback()\n+\n+        if failure.check(TimeoutError):\n+            raise defer.TimeoutError\n\n@@ -12,6 +12,7 @@ from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n+from scrapy.core.downloader.handlers.http11 import HttpDownloadHandler as Http11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n@@ -132,6 +133,11 @@ class HttpTestCase(unittest.TestCase):\n         return d\n \n \n+class Http11TestCase(HttpTestCase):\n+    def setUp(self):\n+        HttpTestCase.setUp(self)\n+        self.download_request = Http11DownloadHandler().download_request\n+\n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n \n@@ -177,6 +183,12 @@ class HttpProxyTestCase(unittest.TestCase):\n         return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n \n \n+class Http11ProxyTestCase(HttpProxyTestCase):\n+    def setUp(self):\n+        HttpProxyTestCase.setUp(self)\n+        self.download_request = Http11DownloadHandler().download_request\n+\n+\n class HttpDownloadHandlerMock(object):\n     def __init__(self, settings):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#46341d5275930c63486529ef080a6e39641eb833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 22 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 73 | Churn Cumulative: 676 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-\"\"\"Download handlers for http and https schemes\"\"\"\n+\"\"\"Download handlers for http scheme\"\"\"\n \n from twisted.internet import reactor\n \n@@ -42,9 +42,10 @@ def _parsed_url_args(parsed):\n     scheme = parsed.scheme\n     netloc = parsed.netloc\n     if port is None:\n-        port = 443 if scheme == 'https' else 80\n+        port = 80\n     return scheme, netloc, host, port, path\n \n+\n def _parse(url):\n     url = url.strip()\n     parsed = urlparse(url)\n@@ -55,26 +56,31 @@ class ScrapyAgent(object):\n     def __init__(self, reactor, contextFactory=WebClientContextFactory(),\n                  connectTimeout=180, bindAddress=None):\n         self._reactor = reactor\n-        self._agent = Agent(self._reactor,\n-            contextFactory=contextFactory,\n-            connectTimeout=connectTimeout,\n-            bindAddress=bindAddress)\n+        self._contextFactory = contextFactory\n+        self._connectTimeout = connectTimeout\n+        self._bindAddress = bindAddress\n \n-    def bindRequest(self, request):\n+    def launchRequest(self, request):\n         self._scrapyrequest = request\n+        request_timeout = request.meta.get('download_timeout') or self._connectTimeout\n \n         proxy = self._scrapyrequest.meta.get('proxy')\n         if proxy is not None and proxy != '':\n             scheme, _, host, port, _ = _parse(proxy)\n-            endpoint = TCP4ClientEndpoint(self._reactor, host, port)\n-            self._agent = ProxyAgent(endpoint)\n-            self._agent._proxyEndpoint._timeout = request.meta.get('download_timeout') or self._agent._proxyEndpoint._timeout\n-        else:\n-            self._agent._connectTimeout = request.meta.get('download_timeout') or self._agent._connectTimeout\n+            endpoint = TCP4ClientEndpoint(self._reactor,\n+                host, port,\n+                timeout=request_timeout,\n+                bindAddress=self._bindAddress)\n+            agent = ProxyAgent(endpoint)\n+\n+        else:\n+            agent = Agent(self._reactor,\n+                contextFactory=self._contextFactory,\n+                connectTimeout=request_timeout,\n+                bindAddress=self._bindAddress)\n \n-    def launch(self):\n         self._scrapyrequest._tw_start_time = time()\n-        d = self._agent.request(\n+        d = agent.request(\n                 self._scrapyrequest.method,\n                 urldefrag(self._scrapyrequest.url)[0],\n                 Headers(self._scrapyrequest.headers),\n@@ -101,23 +107,30 @@ class ScrapyAgentRequestBodyProducer(object):\n \n \n from cStringIO import StringIO\n-class ScrapyAgentResponseBodyReader(protocol.Protocol):\n+class ScrapyAgentResponseReader(protocol.Protocol):\n \n     def __init__(self, finished, response, scrapyRequest, debug=0):\n+        self.debug = debug\n \n         # finished is the deferred that will be fired\n         self._finished = finished\n-\n-        self.debug = debug\n-\n         self.status = int(response.code)\n-        self.resp_headers = list(response.headers.getAllRawHeaders())\n \n         self._scrapyrequest = scrapyRequest\n         self._scrapyrequest._tw_headers_time = time()\n         self._scrapyrequest.meta['download_latency'] = self._scrapyrequest._tw_headers_time - self._scrapyrequest._tw_start_time\n \n-        # body\n+        # twisted.web._newclient.HTTPClientParser already decodes chunked response bodies,\n+        # so prevent extra processing in scrapy.contrib.downloadermiddleware.chunked\n+        # by removing the Transfer-Encoding header if found\n+        txEncodings = response.headers.getRawHeaders('Transfer-Encoding')\n+        if txEncodings is not None and 'chunked' in txEncodings:\n+            # hopefully there's only one Transfer-Encoding header...\n+            response.headers.removeHeader('Transfer-Encoding')\n+\n+        self.resp_headers = list(response.headers.getAllRawHeaders())\n+\n+        # body, if any\n         self.bodyBuffer = StringIO()\n \n     def dataReceived(self, bodyBytes):\n@@ -146,7 +159,6 @@ class ScrapyAgentResponseBodyReader(protocol.Protocol):\n         # fire the deferred with Scrapy Response object\n         self._finished.callback(self._build_response())\n \n-\n     def _build_response(self):\n         headers = ScrapyHeaders(self.resp_headers)\n         respcls = responsetypes.from_args(headers=headers, url=urldefrag(self._scrapyrequest.url)[0])\n@@ -157,31 +169,30 @@ class ScrapyAgentResponseBodyReader(protocol.Protocol):\n             body=self.bodyBuffer.getvalue())\n \n \n-class HttpDownloadHandler(object):\n+class Http11DownloadHandler(object):\n \n     def __init__(self, httpclientfactory=None):\n         self.debug = False\n+        self._httpclientfactory = httpclientfactory\n \n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n \n-        agent = ScrapyAgent(reactor)\n-        agent.bindRequest(request)\n-\n-        d = agent.launch()\n+        agent = ScrapyAgent(reactor, self._httpclientfactory)\n+        d = agent.launchRequest(request)\n         d.addCallback(self._agent_callback, request)\n         d.addErrback(self._agent_errback, request)\n-\n         return d\n \n \n     def _agent_callback(self, response, request):\n         finished = defer.Deferred()\n-        reader = ScrapyAgentResponseBodyReader(finished, response, request, debug = 0)\n-        response.deliverBody(reader)\n+        reader = ScrapyAgentResponseReader(finished, response, request, debug = 0)\n \n-        if request.method != 'HEAD':\n+        # is a response body expected?\n+        if response.length > 0:\n+            response.deliverBody(reader)\n             return finished\n         else:\n             return reader._build_response()\n\n@@ -12,7 +12,7 @@ from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n-from scrapy.core.downloader.handlers.http11 import HttpDownloadHandler as Http11DownloadHandler\n+from scrapy.core.downloader.handlers.http11 import Http11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef0360386921cf7b05f8b5e78554989072def5fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 273 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-\"\"\"Download handlers for http scheme\"\"\"\n+\"\"\"Download handlers for http and https schemes\"\"\"\n \n from twisted.internet import reactor\n \n@@ -42,7 +42,7 @@ def _parsed_url_args(parsed):\n     scheme = parsed.scheme\n     netloc = parsed.netloc\n     if port is None:\n-        port = 80\n+        port = 443 if scheme == 'https' else 80\n     return scheme, netloc, host, port, path\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a7a354f9823872176e75c58329146d5ccb9578b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 141 | Files Changed: 3 | Hunks: 30 | Methods Changed: 19 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 200 | Churn Cumulative: 1072 | Contributors (this commit): 9 | Commits (past 90d): 32 | Contributors (cumulative): 13 | DMM Complexity: 0.4411764705882353\n\nDIFF:\n@@ -1,94 +1,56 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n-from twisted.internet import reactor\n-\n-from scrapy.exceptions import NotSupported\n-from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n-from scrapy.utils.httpobj import urlparse_cached\n-from scrapy import optional_features\n-\n-from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n-from scrapy.http import Headers as ScrapyHeaders\n-from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.responsetypes import responsetypes\n-\n-\n-from twisted.internet import defer, reactor, protocol\n-from twisted.web.client import Agent, ProxyAgent, ResponseDone, WebClientContextFactory\n-\n-from twisted.web.http_headers import Headers\n-from twisted.web._newclient import Response\n-\n-from twisted.web.http import PotentialDataLoss\n-from twisted.web._newclient import ResponseFailed\n-from twisted.web.iweb import IBodyProducer\n-from twisted.internet.error import TimeoutError\n-\n-from twisted.internet.endpoints import TCP4ClientEndpoint\n-\n from time import time\n+from cStringIO import StringIO\n from urlparse import urldefrag\n \n from zope.interface import implements\n+from twisted.internet import defer, reactor, protocol\n+from twisted.web.client import Agent, ProxyAgent, ResponseDone, ResponseFailed\n+from twisted.web.http_headers import Headers\n+from twisted.web.http import PotentialDataLoss\n+from twisted.web.iweb import IBodyProducer\n+from twisted.internet.endpoints import TCP4ClientEndpoint\n \n-from urlparse import urlparse, urlunparse, urldefrag\n-\n-def _parsed_url_args(parsed):\n-    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n-    host = parsed.hostname\n-    port = parsed.port\n-    scheme = parsed.scheme\n-    netloc = parsed.netloc\n-    if port is None:\n-        port = 443 if scheme == 'https' else 80\n-    return scheme, netloc, host, port, path\n-\n-\n-def _parse(url):\n-    url = url.strip()\n-    parsed = urlparse(url)\n-    return _parsed_url_args(parsed)\n+from scrapy.http import Headers as ScrapyHeaders\n+from scrapy.responsetypes import responsetypes\n+from scrapy.core.downloader.webclient import _parse\n \n \n class ScrapyAgent(object):\n-    def __init__(self, reactor, contextFactory=WebClientContextFactory(),\n-                 connectTimeout=180, bindAddress=None):\n-        self._reactor = reactor\n+\n+    def __init__(self, contextFactory=None, connectTimeout=180, bindAddress=None):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n \n     def launchRequest(self, request):\n-        self._scrapyrequest = request\n         request_timeout = request.meta.get('download_timeout') or self._connectTimeout\n \n-        proxy = self._scrapyrequest.meta.get('proxy')\n+        proxy = request.meta.get('proxy')\n         if proxy is not None and proxy != '':\n             scheme, _, host, port, _ = _parse(proxy)\n-            endpoint = TCP4ClientEndpoint(self._reactor,\n+            endpoint = TCP4ClientEndpoint(reactor,\n                 host, port,\n                 timeout=request_timeout,\n                 bindAddress=self._bindAddress)\n             agent = ProxyAgent(endpoint)\n-\n         else:\n-            agent = Agent(self._reactor,\n+            agent = Agent(reactor,\n                 contextFactory=self._contextFactory,\n                 connectTimeout=request_timeout,\n                 bindAddress=self._bindAddress)\n \n-        self._scrapyrequest._tw_start_time = time()\n-        d = agent.request(\n-                self._scrapyrequest.method,\n-                urldefrag(self._scrapyrequest.url)[0],\n-                Headers(self._scrapyrequest.headers),\n-                ScrapyAgentRequestBodyProducer(self._scrapyrequest.body) if (self._scrapyrequest.body is not None) else None)\n-        return d\n+        request._tw_start_time = time()\n+        return agent.request(\n+                request.method,\n+                urldefrag(request.url)[0],\n+                Headers(request.headers),\n+                _RequestBodyProducer(request.body or ''),\n+        )\n \n \n-class ScrapyAgentRequestBodyProducer(object):\n+class _RequestBodyProducer(object):\n     implements(IBodyProducer)\n \n     def __init__(self, body):\n@@ -106,104 +68,60 @@ class ScrapyAgentRequestBodyProducer(object):\n         pass\n \n \n-from cStringIO import StringIO\n-class ScrapyAgentResponseReader(protocol.Protocol):\n+class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished, response, scrapyRequest, debug=0):\n-        self.debug = debug\n-\n-        # finished is the deferred that will be fired\n+    def __init__(self, finished):\n         self._finished = finished\n-        self.status = int(response.code)\n-\n-        self._scrapyrequest = scrapyRequest\n-        self._scrapyrequest._tw_headers_time = time()\n-        self._scrapyrequest.meta['download_latency'] = self._scrapyrequest._tw_headers_time - self._scrapyrequest._tw_start_time\n-\n-        # twisted.web._newclient.HTTPClientParser already decodes chunked response bodies,\n-        # so prevent extra processing in scrapy.contrib.downloadermiddleware.chunked\n-        # by removing the Transfer-Encoding header if found\n-        txEncodings = response.headers.getRawHeaders('Transfer-Encoding')\n-        if txEncodings is not None and 'chunked' in txEncodings:\n-            # hopefully there's only one Transfer-Encoding header...\n-            response.headers.removeHeader('Transfer-Encoding')\n-\n-        self.resp_headers = list(response.headers.getAllRawHeaders())\n-\n-        # body, if any\n-        self.bodyBuffer = StringIO()\n+        self._bodybuf = StringIO()\n \n     def dataReceived(self, bodyBytes):\n-        if self.debug > 2:\n-            print \"dataReceived\", len(bodyBytes), [bodyBytes[0:min(len(bodyBytes), 16)]]\n-        self.bodyBuffer.write(bodyBytes)\n+        self._bodybuf.write(bodyBytes)\n \n     def connectionLost(self, reason):\n-\n-        if reason.check(PotentialDataLoss):\n-            if self.debug > 2:\n-                print \"PotentialDataLoss\"\n-\n-        elif reason.check(ResponseFailed):\n-            if self.debug > 2:\n-                print \"ResponseFailed\"\n-                print reason.getErrorMessage()\n-                reason.getBriefTraceback()\n-\n-        elif reason.check(ResponseDone):\n-            if self.debug > 2:\n-                print \"ReponseDone\"\n-                print \"connection lost: %s\" % reason\n-                print 'Finished receiving body:', reason.getErrorMessage()\n-\n-        # fire the deferred with Scrapy Response object\n-        self._finished.callback(self._build_response())\n-\n-    def _build_response(self):\n-        headers = ScrapyHeaders(self.resp_headers)\n-        respcls = responsetypes.from_args(headers=headers, url=urldefrag(self._scrapyrequest.url)[0])\n-        return respcls(\n-            url=urldefrag(self._scrapyrequest.url)[0],\n-            status=self.status,\n-            headers=headers,\n-            body=self.bodyBuffer.getvalue())\n+        body = self._bodybuf.getvalue()\n+        if reason.check(ResponseDone):\n+            self._finished.callback((body, None))\n+        elif reason.check(PotentialDataLoss, ResponseFailed):\n+            self._finished.callback((body, 'partial_download'))\n+        else:\n+            self._finished.errback(reason)\n \n \n class Http11DownloadHandler(object):\n \n-    def __init__(self, httpclientfactory=None):\n+    def __init__(self):\n         self.debug = False\n-        self._httpclientfactory = httpclientfactory\n-\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n-\n-        agent = ScrapyAgent(reactor, self._httpclientfactory)\n+        agent = ScrapyAgent(reactor)\n         d = agent.launchRequest(request)\n+        d.addBoth(self._download_latency, request, time())\n         d.addCallback(self._agent_callback, request)\n         d.addErrback(self._agent_errback, request)\n         return d\n \n+    def _download_latency(self, any_, request, start_time):\n+        request.meta['download_latency'] = time() - start_time\n+        return any_\n \n-    def _agent_callback(self, response, request):\n+    def _agent_callback(self, txresponse, request):\n+        if txresponse.length == 0:\n+            return self._build_response(('', None), txresponse, request)\n         finished = defer.Deferred()\n-        reader = ScrapyAgentResponseReader(finished, response, request, debug = 0)\n-\n-        # is a response body expected?\n-        if response.length > 0:\n-            response.deliverBody(reader)\n+        finished.addCallback(self._build_response, txresponse, request)\n+        txresponse.deliverBody(_ResponseReader(finished))\n         return finished\n-        else:\n-            return reader._build_response()\n \n+    def _build_response(self, (body, flag), txresponse, request):\n+        if flag is not None:\n+            request.meta[flag] = True\n+        url = urldefrag(request.url)[0]\n+        status = int(txresponse.code)\n+        headers = ScrapyHeaders(txresponse.headers.getAllRawHeaders())\n+        respcls = responsetypes.from_args(headers=headers, url=url)\n+        return respcls(url=url, status=status, headers=headers, body=body)\n \n     def _agent_errback(self, failure, request):\n-        if self.debug:\n-            print \"HttpDownloadHandler: errback called!\"\n-            print failure.getErrorMessage()\n-            failure.getBriefTraceback()\n-            failure.printTraceback()\n-\n-        if failure.check(TimeoutError):\n-            raise defer.TimeoutError\n+        #log.err(failure, 'HTTP11 failure: %s' % request)\n+        return failure\n\n@@ -53,8 +53,8 @@ DOWNLOAD_DELAY = 0\n DOWNLOAD_HANDLERS = {}\n DOWNLOAD_HANDLERS_BASE = {\n     'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n-    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n-    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n+    'http': 'scrapy.core.downloader.handlers.http11.Http11DownloadHandler',\n+    'https': 'scrapy.core.downloader.handlers.http11.Http11DownloadHandler',\n     's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n }\n \n\n@@ -3,7 +3,7 @@ import os\n from twisted.trial import unittest\n from twisted.protocols.policies import WrappingFactory\n from twisted.python.filepath import FilePath\n-from twisted.internet import reactor, defer\n+from twisted.internet import reactor, defer, error\n from twisted.web import server, static, util, resource\n from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         NoLengthResource, HostHeaderResource, \\\n@@ -101,7 +101,7 @@ class HttpTestCase(unittest.TestCase):\n     def test_timeout_download_from_spider(self):\n         request = Request(self.getURL('wait'), meta=dict(download_timeout=0.000001))\n         d = self.download_request(request, BaseSpider('foo'))\n-        return self.assertFailure(d, defer.TimeoutError)\n+        return self.assertFailure(d, defer.TimeoutError, error.TimeoutError)\n \n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e4fe7c63b0f84b893205269c2988c4e223d3c110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 62 | Files Changed: 2 | Hunks: 12 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 138 | Churn Cumulative: 813 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,8 @@ from urlparse import urldefrag\n \n from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n-from twisted.web.client import Agent, ProxyAgent, ResponseDone, ResponseFailed\n+from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n+        ResponseFailed, HTTPConnectionPool\n from twisted.web.http_headers import Headers\n from twisted.web.http import PotentialDataLoss\n from twisted.web.iweb import IBodyProducer\n@@ -15,39 +16,88 @@ from twisted.internet.endpoints import TCP4ClientEndpoint\n from scrapy.http import Headers as ScrapyHeaders\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+from scrapy import log\n+\n+\n+ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+\n+\n+class Http11DownloadHandler(object):\n+\n+    def __init__(self):\n+        self._pool = HTTPConnectionPool(reactor, persistent=False)\n+        self._contextFactory = ClientContextFactory()\n+\n+    def download_request(self, request, spider):\n+        \"\"\"Return a deferred for the HTTP download\"\"\"\n+        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool)\n+        return agent.download_request(request)\n \n \n class ScrapyAgent(object):\n \n-    def __init__(self, contextFactory=None, connectTimeout=180, bindAddress=None):\n+    _Agent = Agent\n+    _ProxyAgent = ProxyAgent\n+\n+    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None):\n         self._contextFactory = contextFactory\n         self._connectTimeout = connectTimeout\n         self._bindAddress = bindAddress\n+        self._pool = pool\n \n-    def launchRequest(self, request):\n-        request_timeout = request.meta.get('download_timeout') or self._connectTimeout\n+    def download_request(self, request):\n+        url = urldefrag(request.url)[0]\n+        method = request.method\n+        headers = Headers(request.headers)\n+        bodyproducer = _RequestBodyProducer(request.body or '')\n+        agent = self._get_agent(request)\n+        start_time = time()\n+        d = agent.request(method, url, headers, bodyproducer)\n+        d.addBoth(self._download_latency, request, start_time)\n+        d.addCallback(self._agentrequest_downloaded, request)\n+        d.addErrback(self._agentrequest_failed, request)\n+        return d\n \n+    def _get_agent(self, request):\n+        timeout = request.meta.get('download_timeout') or self._connectTimeout\n+        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n         proxy = request.meta.get('proxy')\n-        if proxy is not None and proxy != '':\n+        if proxy:\n             scheme, _, host, port, _ = _parse(proxy)\n-            endpoint = TCP4ClientEndpoint(reactor,\n-                host, port,\n-                timeout=request_timeout,\n-                bindAddress=self._bindAddress)\n-            agent = ProxyAgent(endpoint)\n-        else:\n-            agent = Agent(reactor,\n-                contextFactory=self._contextFactory,\n-                connectTimeout=request_timeout,\n-                bindAddress=self._bindAddress)\n+            endpoint = TCP4ClientEndpoint(reactor, host, port, timeout=timeout,\n+                bindAddress=bindaddress)\n+            return self._ProxyAgent(endpoint)\n \n-        request._tw_start_time = time()\n-        return agent.request(\n-                request.method,\n-                urldefrag(request.url)[0],\n-                Headers(request.headers),\n-                _RequestBodyProducer(request.body or ''),\n-        )\n+        return self._Agent(reactor, contextFactory=self._contextFactory,\n+            connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n+\n+    def _download_latency(self, any_, request, start_time):\n+        request.meta['download_latency'] = time() - start_time\n+        return any_\n+\n+    def _agentrequest_downloaded(self, txresponse, request):\n+        if txresponse.length == 0:\n+            return self._build_response(('', None), txresponse, request)\n+        finished = defer.Deferred()\n+        finished.addCallback(self._build_response, txresponse, request)\n+        txresponse.deliverBody(_ResponseReader(finished))\n+        return finished\n+\n+    def _build_response(self, (body, flag), txresponse, request):\n+        if flag is not None:\n+            request.meta[flag] = True\n+        url = urldefrag(request.url)[0]\n+        status = int(txresponse.code)\n+        headers = ScrapyHeaders(txresponse.headers.getAllRawHeaders())\n+        respcls = responsetypes.from_args(headers=headers, url=url)\n+        return respcls(url=url, status=status, headers=headers, body=body)\n+\n+    def _agentrequest_failed(self, failure, request):\n+        # be clear it is an HTTP failure with new downloader\n+        log.err(failure, 'HTTP11 failure: %s' % request)\n+        return failure\n \n \n class _RequestBodyProducer(object):\n@@ -85,43 +135,3 @@ class _ResponseReader(protocol.Protocol):\n             self._finished.callback((body, 'partial_download'))\n         else:\n             self._finished.errback(reason)\n-\n-\n-class Http11DownloadHandler(object):\n-\n-    def __init__(self):\n-        self.debug = False\n-\n-    def download_request(self, request, spider):\n-        \"\"\"Return a deferred for the HTTP download\"\"\"\n-        agent = ScrapyAgent(reactor)\n-        d = agent.launchRequest(request)\n-        d.addBoth(self._download_latency, request, time())\n-        d.addCallback(self._agent_callback, request)\n-        d.addErrback(self._agent_errback, request)\n-        return d\n-\n-    def _download_latency(self, any_, request, start_time):\n-        request.meta['download_latency'] = time() - start_time\n-        return any_\n-\n-    def _agent_callback(self, txresponse, request):\n-        if txresponse.length == 0:\n-            return self._build_response(('', None), txresponse, request)\n-        finished = defer.Deferred()\n-        finished.addCallback(self._build_response, txresponse, request)\n-        txresponse.deliverBody(_ResponseReader(finished))\n-        return finished\n-\n-    def _build_response(self, (body, flag), txresponse, request):\n-        if flag is not None:\n-            request.meta[flag] = True\n-        url = urldefrag(request.url)[0]\n-        status = int(txresponse.code)\n-        headers = ScrapyHeaders(txresponse.headers.getAllRawHeaders())\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n-        return respcls(url=url, status=status, headers=headers, body=body)\n-\n-    def _agent_errback(self, failure, request):\n-        #log.err(failure, 'HTTP11 failure: %s' % request)\n-        return failure\n\n@@ -144,12 +144,16 @@ class ScrapyClientContextFactory(ClientContextFactory):\n     # see https://github.com/scrapy/scrapy/issues/82\n     # and https://github.com/scrapy/scrapy/issues/26\n \n+<<<<<<< HEAD\n     def __init__(self):\n         # see this issue on why we use TLSv1_METHOD by default\n         # https://github.com/scrapy/scrapy/issues/194\n         self.method = SSL.TLSv1_METHOD\n \n     def getContext(self):\n+=======\n+    def getContext(self, hostname, port):\n+>>>>>>> add http connection pool and custom ssl context factory\n         ctx = ClientContextFactory.getContext(self)\n         # Enable all workarounds to SSL bugs as documented by\n         # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab3407289cd493f45f10aba75f76561ebbbfdc64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 601 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n class Http11DownloadHandler(object):\n \n     def __init__(self):\n-        self._pool = HTTPConnectionPool(reactor, persistent=False)\n+        self._pool = HTTPConnectionPool(reactor, persistent=True)\n         self._contextFactory = ClientContextFactory()\n \n     def download_request(self, request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a261700862c496dfabab8a5adc05d841bd95c05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 44 | Churn Cumulative: 454 | Contributors (this commit): 8 | Commits (past 90d): 31 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,20 @@\n+from OpenSSL import SSL\n+from twisted.internet.ssl import ClientContextFactory\n+\n+\n+class ScrapyClientContextFactory(ClientContextFactory):\n+    \"A SSL context factory which is more permissive against SSL bugs.\"\n+    # see https://github.com/scrapy/scrapy/issues/82\n+    # and https://github.com/scrapy/scrapy/issues/26\n+\n+    def __init__(self):\n+        # see this issue on why we use TLSv1_METHOD by default\n+        # https://github.com/scrapy/scrapy/issues/194\n+        self.method = SSL.TLSv1_METHOD\n+\n+    def getContext(self, hostname=None, port=None):\n+        ctx = ClientContextFactory.getContext(self)\n+        # Enable all workarounds to SSL bugs as documented by\n+        # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n+        ctx.set_options(SSL.OP_ALL)\n+        return ctx\n\n@@ -137,25 +137,3 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         self.headers_time = time()\n         self.response_headers = headers\n \n-\n-\n-class ScrapyClientContextFactory(ClientContextFactory):\n-    \"A SSL context factory which is more permissive against SSL bugs.\"\n-    # see https://github.com/scrapy/scrapy/issues/82\n-    # and https://github.com/scrapy/scrapy/issues/26\n-\n-<<<<<<< HEAD\n-    def __init__(self):\n-        # see this issue on why we use TLSv1_METHOD by default\n-        # https://github.com/scrapy/scrapy/issues/194\n-        self.method = SSL.TLSv1_METHOD\n-\n-    def getContext(self):\n-=======\n-    def getContext(self, hostname, port):\n->>>>>>> add http connection pool and custom ssl context factory\n-        ctx = ClientContextFactory.getContext(self)\n-        # Enable all workarounds to SSL bugs as documented by\n-        # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n-        ctx.set_options(SSL.OP_ALL)\n-        return ctx\n\n@@ -63,7 +63,7 @@ DOWNLOAD_TIMEOUT = 180      # 3mins\n DOWNLOADER_DEBUG = False\n \n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n-DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.webclient.ScrapyClientContextFactory'\n+DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n \n DOWNLOADER_MIDDLEWARES = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b34606030dff9c83bbd95532e7fc0b9f1604b9c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 157 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,9 @@\n-\"\"\"Download handlers for http and https schemes\"\"\"\n-\n+\"\"\"Download handlers for http and https schemes\n+\"\"\"\n from twisted.internet import reactor\n-\n from scrapy.utils.misc import load_object\n \n+\n class HttpDownloadHandler(object):\n \n     def __init__(self, settings):\n@@ -19,7 +19,7 @@ class HttpDownloadHandler(object):\n     def _connect(self, factory):\n         host, port = factory.host, factory.port\n         if factory.scheme == 'https':\n-            return reactor.connectSSL(host, port, factory, \\\n+            return reactor.connectSSL(host, port, factory,\n                                       self.ClientContextFactory())\n         else:\n             return reactor.connectTCP(host, port, factory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db232da06839bc6f62daa0a89197cf7e57367bae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 33 | Churn Cumulative: 1045 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,9 @@ class Http11DownloadHandler(object):\n         agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool)\n         return agent.download_request(request)\n \n+    def close(self):\n+        return self._pool.closeCachedConnections()\n+\n \n class ScrapyAgent(object):\n \n\n@@ -47,6 +47,8 @@ class FileTestCase(unittest.TestCase):\n \n class HttpTestCase(unittest.TestCase):\n \n+    download_handler_cls = HttpDownloadHandler\n+\n     def setUp(self):\n         name = self.mktemp()\n         os.mkdir(name)\n@@ -62,10 +64,14 @@ class HttpTestCase(unittest.TestCase):\n         self.wrapper = WrappingFactory(self.site)\n         self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_request = HttpDownloadHandler(Settings()).download_request\n+        self.download_handler = self.download_handler_cls(Settings())\n+        self.download_request = self.download_handler.download_request\n \n+    @defer.inlineCallbacks\n     def tearDown(self):\n-        return self.port.stopListening()\n+        yield self.port.stopListening()\n+        if hasattr(self.download_handler, 'close'):\n+            yield self.download_handler.close()\n \n     def getURL(self, path):\n         return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n@@ -134,9 +140,9 @@ class HttpTestCase(unittest.TestCase):\n \n \n class Http11TestCase(HttpTestCase):\n-    def setUp(self):\n-        HttpTestCase.setUp(self)\n-        self.download_request = Http11DownloadHandler().download_request\n+    \"\"\"HTTP 1.1 test case\"\"\"\n+    download_handler_cls = Http11DownloadHandler\n+\n \n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n@@ -150,15 +156,21 @@ class UriResource(resource.Resource):\n \n class HttpProxyTestCase(unittest.TestCase):\n \n+    download_handler_cls = HttpDownloadHandler\n+\n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_request = HttpDownloadHandler(Settings()).download_request\n+        self.download_handler = self.download_handler_cls(Settings())\n+        self.download_request = self.download_handler.download_request\n \n+    @defer.inlineCallbacks\n     def tearDown(self):\n-        return self.port.stopListening()\n+        yield self.port.stopListening()\n+        if hasattr(self.download_handler, 'close'):\n+            yield self.download_handler.close()\n \n     def getURL(self, path):\n         return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n@@ -184,9 +196,7 @@ class HttpProxyTestCase(unittest.TestCase):\n \n \n class Http11ProxyTestCase(HttpProxyTestCase):\n-    def setUp(self):\n-        HttpProxyTestCase.setUp(self)\n-        self.download_request = Http11DownloadHandler().download_request\n+    download_handler_cls = Http11DownloadHandler\n \n \n class HttpDownloadHandlerMock(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba6545555f1d408fc88ac326cf148bdab02bbf3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 606 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ class ScrapyAgent(object):\n         url = urldefrag(request.url)[0]\n         method = request.method\n         headers = Headers(request.headers)\n-        bodyproducer = _RequestBodyProducer(request.body or '')\n+        bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n         agent = self._get_agent(request)\n         start_time = time()\n         d = agent.request(method, url, headers, bodyproducer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#334ad71a2e78ab0383049d5e1dddf187c3e85848", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 613 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -17,18 +17,15 @@ from scrapy.http import Headers as ScrapyHeaders\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n from scrapy import log\n \n \n-ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n-\n \n class Http11DownloadHandler(object):\n \n-    def __init__(self):\n+    def __init__(self, settings):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n-        self._contextFactory = ClientContextFactory()\n+        self._contextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e62ce35a0234fa441dab0cd65a8482a515335b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 24 | Churn Cumulative: 799 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,7 +70,7 @@ class Downloader(object):\n         self.signals = crawler.signals\n         self.slots = {}\n         self.active = set()\n-        self.handlers = DownloadHandlers(crawler.settings)\n+        self.handlers = DownloadHandlers(crawler)\n         self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n         self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n\n@@ -1,32 +1,42 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n+from twisted.internet import defer\n from scrapy.exceptions import NotSupported, NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n+from scrapy import signals\n \n \n class DownloadHandlers(object):\n \n-    def __init__(self, settings):\n+    def __init__(self, crawler):\n         self._handlers = {}\n         self._notconfigured = {}\n-        handlers = settings.get('DOWNLOAD_HANDLERS_BASE')\n-        handlers.update(settings.get('DOWNLOAD_HANDLERS', {}))\n+        handlers = crawler.settings.get('DOWNLOAD_HANDLERS_BASE')\n+        handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))\n         for scheme, clspath in handlers.iteritems():\n             cls = load_object(clspath)\n             try:\n-                dh = cls(settings)\n+                dh = cls(crawler.settings)\n             except NotConfigured, ex:\n                 self._notconfigured[scheme] = str(ex)\n             else:\n-                self._handlers[scheme] = dh.download_request\n+                self._handlers[scheme] = dh\n+\n+        crawler.signals.connect(self._close, signals.engine_stopped)\n \n     def download_request(self, request, spider):\n         scheme = urlparse_cached(request).scheme\n         try:\n-            handler = self._handlers[scheme]\n+            handler = self._handlers[scheme].download_request\n         except KeyError:\n             msg = self._notconfigured.get(scheme, \\\n                     'no handler available for that scheme')\n             raise NotSupported(\"Unsupported URL scheme '%s': %s\" % (scheme, msg))\n         return handler(request, spider)\n+\n+    @defer.inlineCallbacks\n+    def _close(self, *_a, **_kw):\n+        for dh in self._handlers.values():\n+            if hasattr(dh, 'close'):\n+                yield dh.close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
