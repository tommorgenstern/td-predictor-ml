{"custom_id": "keras#f356569249728f6a056bff34828186ab2b3b939c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 300 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 300 | Churn Cumulative: 300 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,300 @@\n+\"\"\"\n+Title: Few-Shot learning with Reptile\n+Author: [ADMoreau](https://github.com/ADMoreau)\n+Converted to Keras Core By: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2020/05/21\n+Last modified: 2023/07/20\n+Description: Few-shot classification on the Omniglot dataset using Reptile.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+The [Reptile](https://arxiv.org/abs/1803.02999) algorithm was developed by OpenAI to\n+perform model-agnostic meta-learning. Specifically, this algorithm was designed to\n+quickly learn to perform new tasks with minimal training (few-shot learning).\n+The algorithm works by performing Stochastic Gradient Descent using the\n+difference between weights trained on a mini-batch of never-seen-before data and the\n+model weights prior to training over a fixed number of meta-iterations.\n+\"\"\"\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+import matplotlib.pyplot as plt\n+import numpy as np\n+import random\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+\n+\"\"\"\n+## Define the Hyperparameters\n+\"\"\"\n+\n+learning_rate = 0.003\n+meta_step_size = 0.25\n+\n+inner_batch_size = 25\n+eval_batch_size = 25\n+\n+meta_iters = 2000\n+eval_iters = 5\n+inner_iters = 4\n+\n+eval_interval = 1\n+train_shots = 20\n+shots = 5\n+classes = 5\n+\n+\"\"\"\n+## Prepare the data\n+\n+The [Omniglot dataset](https://github.com/brendenlake/omniglot/) is a dataset of 1,623\n+characters taken from 50 different alphabets, with 20 examples for each character.\n+The 20 samples for each character were drawn online via Amazon's Mechanical Turk. For the\n+few-shot learning task, `k` samples (or \"shots\") are drawn randomly from `n` randomly-chosen\n+classes. These `n` numerical values are used to create a new set of temporary labels to use\n+to test the model's ability to learn a new task given few examples. In other words, if you\n+are training on 5 classes, your new class labels will be either 0, 1, 2, 3, or 4.\n+Omniglot is a great dataset for this task since there are many different classes to draw\n+from, with a reasonable number of samples for each class.\n+\"\"\"\n+\n+\n+class Dataset:\n+    # This class will facilitate the creation of a few-shot dataset\n+    # from the Omniglot dataset that can be sampled from quickly while also\n+    # allowing to create new labels at the same time.\n+    def __init__(self, training):\n+        # Download the tfrecord files containing the omniglot data and convert to a\n+        # dataset.\n+        split = \"train\" if training else \"test\"\n+        ds = tfds.load(\n+            \"omniglot\", split=split, as_supervised=True, shuffle_files=False\n+        )\n+        # Iterate over the dataset to get each individual image and its class,\n+        # and put that data into a dictionary.\n+        self.data = {}\n+\n+        def extraction(image, label):\n+            # This function will shrink the Omniglot images to the desired size,\n+            # scale pixel values and convert the RGB image to grayscale\n+            image = tf.image.convert_image_dtype(image, tf.float32)\n+            image = tf.image.rgb_to_grayscale(image)\n+            image = tf.image.resize(image, [28, 28])\n+            return image, label\n+\n+        for image, label in ds.map(extraction):\n+            image = image.numpy()\n+            label = str(label.numpy())\n+            if label not in self.data:\n+                self.data[label] = []\n+            self.data[label].append(image)\n+        self.labels = list(self.data.keys())\n+\n+    def get_mini_dataset(\n+        self, batch_size, repetitions, shots, num_classes, split=False\n+    ):\n+        temp_labels = np.zeros(shape=(num_classes * shots))\n+        temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))\n+        if split:\n+            test_labels = np.zeros(shape=(num_classes))\n+            test_images = np.zeros(shape=(num_classes, 28, 28, 1))\n+\n+        # Get a random subset of labels from the entire label set.\n+        label_subset = random.choices(self.labels, k=num_classes)\n+        for class_idx, class_obj in enumerate(label_subset):\n+            # Use enumerated index value as a temporary label for mini-batch in\n+            # few shot learning.\n+            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n+            # If creating a split dataset for testing, select an extra sample from each\n+            # label to create the test dataset.\n+            if split:\n+                test_labels[class_idx] = class_idx\n+                images_to_split = random.choices(\n+                    self.data[label_subset[class_idx]], k=shots + 1\n+                )\n+                test_images[class_idx] = images_to_split[-1]\n+                temp_images[\n+                    class_idx * shots : (class_idx + 1) * shots\n+                ] = images_to_split[:-1]\n+            else:\n+                # For each index in the randomly selected label_subset, sample the\n+                # necessary number of images.\n+                temp_images[\n+                    class_idx * shots : (class_idx + 1) * shots\n+                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n+\n+        dataset = tf.data.Dataset.from_tensor_slices(\n+            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n+        )\n+        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n+        if split:\n+            return dataset, test_images, test_labels\n+        return dataset\n+\n+\n+import urllib3\n+\n+urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n+train_dataset = Dataset(training=True)\n+test_dataset = Dataset(training=False)\n+\n+\"\"\"\n+## Visualize some examples from the dataset\n+\"\"\"\n+\n+_, axarr = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n+\n+sample_keys = list(train_dataset.data.keys())\n+\n+for a in range(5):\n+    for b in range(5):\n+        temp_image = train_dataset.data[sample_keys[a]][b]\n+        temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n+        temp_image *= 255\n+        temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n+        if b == 2:\n+            axarr[a, b].set_title(\"Class : \" + sample_keys[a])\n+        axarr[a, b].imshow(temp_image, cmap=\"gray\")\n+        axarr[a, b].xaxis.set_visible(False)\n+        axarr[a, b].yaxis.set_visible(False)\n+plt.show()\n+\n+\"\"\"\n+## Build the model\n+\"\"\"\n+\n+\n+def conv_bn(x):\n+    x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\n+    x = layers.BatchNormalization()(x)\n+    return layers.ReLU()(x)\n+\n+\n+inputs = layers.Input(shape=(28, 28, 1))\n+x = conv_bn(inputs)\n+x = conv_bn(x)\n+x = conv_bn(x)\n+x = conv_bn(x)\n+x = layers.Flatten()(x)\n+outputs = layers.Dense(classes, activation=\"softmax\")(x)\n+model = keras.Model(inputs=inputs, outputs=outputs)\n+model.compile()\n+optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+training = []\n+testing = []\n+for meta_iter in range(meta_iters):\n+    frac_done = meta_iter / meta_iters\n+    cur_meta_step_size = (1 - frac_done) * meta_step_size\n+    # Temporarily save the weights from the model.\n+    old_vars = model.get_weights()\n+    # Get a sample from the full dataset.\n+    mini_dataset = train_dataset.get_mini_dataset(\n+        inner_batch_size, inner_iters, train_shots, classes\n+    )\n+    for images, labels in mini_dataset:\n+        with tf.GradientTape() as tape:\n+            preds = model(images)\n+            loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n+        grads = tape.gradient(loss, model.trainable_weights)\n+        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+    new_vars = model.get_weights()\n+    # Perform SGD for the meta step.\n+    for var in range(len(new_vars)):\n+        new_vars[var] = old_vars[var] + (\n+            (new_vars[var] - old_vars[var]) * cur_meta_step_size\n+        )\n+    # After the meta-learning step, reload the newly-trained weights into the model.\n+    model.set_weights(new_vars)\n+    # Evaluation loop\n+    if meta_iter % eval_interval == 0:\n+        accuracies = []\n+        for dataset in (train_dataset, test_dataset):\n+            # Sample a mini dataset from the full dataset.\n+            train_set, test_images, test_labels = dataset.get_mini_dataset(\n+                eval_batch_size, eval_iters, shots, classes, split=True\n+            )\n+            old_vars = model.get_weights()\n+            # Train on the samples and get the resulting accuracies.\n+            for images, labels in train_set:\n+                with tf.GradientTape() as tape:\n+                    preds = model(images)\n+                    loss = keras.losses.sparse_categorical_crossentropy(\n+                        labels, preds\n+                    )\n+                grads = tape.gradient(loss, model.trainable_weights)\n+                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+            test_preds = model.predict(test_images)\n+            test_preds = tf.argmax(test_preds).numpy()\n+            num_correct = (test_preds == test_labels).sum()\n+            # Reset the weights after getting the evaluation accuracies.\n+            model.set_weights(old_vars)\n+            accuracies.append(num_correct / classes)\n+        training.append(accuracies[0])\n+        testing.append(accuracies[1])\n+        if meta_iter % 100 == 0:\n+            print(\n+                \"batch %d: train=%f test=%f\"\n+                % (meta_iter, accuracies[0], accuracies[1])\n+            )\n+\n+\"\"\"\n+## Visualize Results\n+\"\"\"\n+\n+# First, some preprocessing to smooth the training and testing arrays for display.\n+window_length = 100\n+train_s = np.r_[\n+    training[window_length - 1 : 0 : -1],\n+    training,\n+    training[-1:-window_length:-1],\n+]\n+test_s = np.r_[\n+    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]\n+]\n+w = np.hamming(window_length)\n+train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\")\n+test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")\n+\n+# Display the training accuracies.\n+x = np.arange(0, len(test_y), 1)\n+plt.plot(x, test_y, x, train_y)\n+plt.legend([\"test\", \"train\"])\n+plt.grid()\n+\n+train_set, test_images, test_labels = dataset.get_mini_dataset(\n+    eval_batch_size, eval_iters, shots, classes, split=True\n+)\n+for images, labels in train_set:\n+    with tf.GradientTape() as tape:\n+        preds = model(images)\n+        loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n+    grads = tape.gradient(loss, model.trainable_weights)\n+    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n+test_preds = model.predict(test_images)\n+test_preds = tf.argmax(test_preds).numpy()\n+\n+_, axarr = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n+\n+sample_keys = list(train_dataset.data.keys())\n+\n+for i, ax in zip(range(5), axarr):\n+    temp_image = np.stack((test_images[i, :, :, 0],) * 3, axis=2)\n+    temp_image *= 255\n+    temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n+    ax.set_title(\n+        \"Label : {}, Prediction : {}\".format(int(test_labels[i]), test_preds[i])\n+    )\n+    ax.imshow(temp_image, cmap=\"gray\")\n+    ax.xaxis.set_visible(False)\n+    ax.yaxis.set_visible(False)\n+plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#868afdcf2fdb73496c4ba586de1dfdc8e4cfbd19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 10 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 86 | Churn Cumulative: 357 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 0.9868421052631579\n\nDIFF:\n@@ -97,3 +97,84 @@ class SavedModelTest(testing.TestCase):\n             rtol=1e-4,\n             atol=1e-4,\n         )\n+\n+    def test_custom_model_and_layer(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class CustomLayer(layers.Layer):\n+            def __call__(self, inputs):\n+                return inputs\n+\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class Model(models.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.layer = CustomLayer()\n+\n+            @tf.function(input_signature=[tf.TensorSpec([None, 1])])\n+            def call(self, inputs):\n+                return self.layer(inputs)\n+\n+        model = Model()\n+        inp = np.array([[1.0]])\n+        result = model(inp)\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        self.assertAllClose(\n+            result,\n+            restored_model.call(inp),\n+            rtol=1e-4,\n+            atol=1e-4,\n+        )\n+\n+    def test_multi_input_model(self):\n+        input_1 = layers.Input(shape=(3,))\n+        input_2 = layers.Input(shape=(5,))\n+        model = models.Model([input_1, input_2], [input_1, input_2])\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        input_arr_1 = np.random.random((1, 3)).astype(\"float32\")\n+        input_arr_2 = np.random.random((1, 5)).astype(\"float32\")\n+\n+        outputs = restored_model.signatures[\"serving_default\"](\n+            inputs=tf.convert_to_tensor(input_arr_1, dtype=tf.float32),\n+            inputs_1=tf.convert_to_tensor(input_arr_2, dtype=tf.float32),\n+        )\n+\n+        self.assertAllClose(\n+            input_arr_1, outputs[\"output_0\"], rtol=1e-4, atol=1e-4\n+        )\n+        self.assertAllClose(\n+            input_arr_2, outputs[\"output_1\"], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_multi_input_custom_model_and_layer(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class CustomLayer(layers.Layer):\n+            def __call__(self, *input_list):\n+                self.add_loss(input_list[-2] * 2)\n+                return sum(input_list)\n+\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class CustomModel(models.Model):\n+            def build(self, input_shape):\n+                super().build(input_shape)\n+                self.layer = CustomLayer()\n+\n+            @tf.function\n+            def call(self, *inputs):\n+                inputs = list(inputs)\n+                return self.layer(*inputs)\n+\n+        model = CustomModel()\n+        inp = [\n+            tf.constant(i, shape=[1, 1], dtype=tf.float32) for i in range(1, 4)\n+        ]\n+        expected = model(*inp)\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        output = restored_model.call(*inp)\n+        self.assertAllClose(expected, output, rtol=1e-4, atol=1e-4)\n\n@@ -55,8 +55,9 @@ class Concatenate(Merge):\n             # in case self.axis is a negative number\n             concat_axis = self.axis % len(reduced_inputs_shapes[i])\n             #  Skip batch axis.\n-            for axis, axis_value in enumerate(reduced_inputs_shapes[i][1:],\n-                                              start=1):\n+            for axis, axis_value in enumerate(\n+                reduced_inputs_shapes[i][1:], start=1\n+            ):\n                 # Remove squeezable axes (axes with value of 1)\n                 # if not in the axis that will be used for concatenation\n                 # otherwise leave it.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab8545f2cfb1b033ee6390fe160546a3089629a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 216 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -133,7 +133,7 @@ class InitializersTest(testing.TestCase):\n         self.assertEqual(initializer.gain, gain)\n \n         self.assertEqual(values.shape, shape)\n-        array = np.array(values)\n+        array = backend.convert_to_numpy(values)\n         # Making sure that the columns have gain * unit norm value\n         for column in array.T:\n             self.assertAlmostEqual(np.linalg.norm(column), gain * 1.0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b879bcfb0e93ae110edafb77c7b73aafbe987b75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 205 | Lines Deleted: 25 | Files Changed: 8 | Hunks: 12 | Methods Changed: 31 | Complexity Δ (Sum/Max): 38/16 | Churn Δ: 230 | Churn Cumulative: 5710 | Contributors (this commit): 12 | Commits (past 90d): 113 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -201,33 +201,91 @@ class KerasTensor:\n \n         return ops.Power().symbolic_call(other, self)\n \n+    def __floordiv__(self, other):\n+        from keras_core import ops\n+\n+        return ops.FloorDiv().symbolic_call(self, other)\n+\n+    def __rfloordiv__(self, other):\n+        from keras_core import ops\n+\n+        return ops.FloorDiv().symbolic_call(other, self)\n+\n+    def __mod__(self, other):\n+        from keras_core import ops\n+\n+        return ops.Mod().symbolic_call(self, other)\n+\n+    def __rmod__(self, other):\n+        from keras_core import ops\n+\n+        return ops.Mod().symbolic_call(other, self)\n+\n+    def __lt__(self, other):\n+        from keras_core import ops\n+\n+        return ops.Less().symbolic_call(self, other)\n+\n+    def __le__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LessEqual().symbolic_call(self, other)\n+\n+    def __gt__(self, other):\n+        from keras_core import ops\n+\n+        return ops.Greater().symbolic_call(self, other)\n+\n+    def __ge__(self, other):\n+        from keras_core import ops\n+\n+        return ops.GreaterEqual().symbolic_call(self, other)\n+\n+    def __ne__(self, other):\n+        from keras_core import ops\n+\n+        return ops.NotEqual().symbolic_call(self, other)\n+\n+    def __and__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalAnd().symbolic_call(self, other)\n+\n+    def __rand__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalAnd().symbolic_call(other, self)\n+\n+    def __or__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalOr().symbolic_call(self, other)\n+\n+    def __ror__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalOr().symbolic_call(other, self)\n+\n+    def __invert__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalNot().symbolic_call(other, self)\n+\n+    def __xor__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalXor().symbolic_call(self, other)\n+\n+    def __rxor__(self, other):\n+        from keras_core import ops\n+\n+        return ops.LogicalXor().symbolic_call(other, self)\n+\n     def __getitem__(self, key):\n         from keras_core import ops\n \n         return ops.GetItem().symbolic_call(self, key)\n \n-    # TODO\n-    #   \"__floordiv__\",\n-    #   \"__rfloordiv__\",\n-    #   \"__mod__\",\n-    #   \"__rmod__\",\n-    #   \"__lt__\",\n-    #   \"__le__\",\n-    #   \"__gt__\",\n-    #   \"__ge__\",\n-    #   \"__ne__\",\n-    #   \"__eq__\",\n-    #   \"__and__\",\n-    #   \"__rand__\",\n-    #   \"__or__\",\n-    #   \"__ror__\",\n-    #   \"__xor__\",\n-    #   \"__rxor__\",\n-    #   \"__invert__\",\n-    #   \"broadcast_to\"\n-    #   \"astype\"\n-    #   a few more NumPy ones...\n-\n \n def any_symbolic_tensors(args=None, kwargs=None):\n     args = args or ()\n\n@@ -573,3 +573,11 @@ def sum(x, axis=None, keepdims=False):\n \n def eye(N, M=None, k=0, dtype=\"float32\"):\n     return jnp.eye(N, M=M, k=k, dtype=dtype)\n+\n+\n+def floor_divide(x1, x2):\n+    return jnp.floor_divide(x1, x2)\n+\n+\n+def logical_xor(x1, x2):\n+    return jnp.logical_xor(x1, x2)\n\n@@ -569,3 +569,11 @@ def sum(x, axis=None, keepdims=False):\n \n def eye(N, M=None, k=0, dtype=\"float32\"):\n     return np.eye(N, M=M, k=k, dtype=dtype)\n+\n+\n+def floor_divide(x1, x2):\n+    return np.floor_divide(x1, x2)\n+\n+\n+def logical_xor(x1, x2):\n+    return np.logical_xor(x1, x2)\n\n@@ -600,3 +600,11 @@ def sum(x, axis=None, keepdims=False):\n \n def eye(N, M=None, k=0, dtype=\"float32\"):\n     return tfnp.eye(N, M=M, k=k, dtype=dtype)\n+\n+\n+def floor_divide(x1, x2):\n+    return tfnp.floor_divide(x1, x2)\n+\n+\n+def logical_xor(x1, x2):\n+    return tfnp.logical_xor(x1, x2)\n\n@@ -906,3 +906,13 @@ def eye(N, M=None, k=None, dtype=\"float32\"):\n     diag_length = np.maximum(N, M)\n     diag = torch.ones(diag_length, dtype=dtype, device=get_device())\n     return torch.diag(diag, diagonal=k)[:N, :M]\n+\n+\n+def floor_divide(x1, x2):\n+    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    return torch.floor_divide(x1, x2)\n+\n+\n+def logical_xor(x1, x2):\n+    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    return torch.logical_xor(x1, x2)\n\n@@ -3458,3 +3458,43 @@ class Eye(Operation):\n @keras_core_export([\"keras_core.ops.eye\", \"keras_core.ops.numpy.eye\"])\n def eye(N, M=None, k=0, dtype=\"float32\"):\n     return backend.numpy.eye(N, M=M, k=k, dtype=dtype)\n+\n+\n+class FloorDiv(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.floor_divide(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+        return KerasTensor(output_shape, dtype=x1.dtype)\n+\n+\n+@keras_core_export(\n+    [\"keras_core.ops.floor_divide\", \"keras_core.ops.numpy.floor_divide\"]\n+)\n+def floor_divide(x1, x2):\n+    if any_symbolic_tensors((x1, x2)):\n+        return FloorDiv().symbolic_call(x1, x2)\n+    return backend.numpy.floor_divide(x1, x2)\n+\n+\n+class LogicalXor(Operation):\n+    def call(self, x1, x2):\n+        return backend.numpy.logical_xor(x1, x2)\n+\n+    def compute_output_spec(self, x1, x2):\n+        x1_shape = getattr(x1, \"shape\", [])\n+        x2_shape = getattr(x2, \"shape\", [])\n+        output_shape = broadcast_shapes(x1_shape, x2_shape)\n+        return KerasTensor(output_shape, dtype=x1.dtype)\n+\n+\n+@keras_core_export(\n+    [\"keras_core.ops.logical_xor\", \"keras_core.ops.numpy.logical_xor\"]\n+)\n+def logical_xor(x1, x2):\n+    if any_symbolic_tensors((x1, x2)):\n+        return LogicalXor().symbolic_call(x1, x2)\n+    return backend.numpy.logical_xor(x1, x2)\n\n@@ -246,6 +246,16 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         y = KerasTensor([None, 3])\n         self.assertEqual(knp.where(condition, x, y).shape, (2, None, 3))\n \n+    def test_floordiv(self):\n+        x = KerasTensor((None, 3))\n+        y = KerasTensor((2, None))\n+        self.assertEqual(knp.floor_divide(x, y).shape, (2, 3))\n+\n+    def test_xor(self):\n+        x = KerasTensor((None, 3))\n+        y = KerasTensor((2, None))\n+        self.assertEqual(knp.logical_xor(x, y).shape, (2, 3))\n+\n \n class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n     def test_add(self):\n@@ -630,6 +640,26 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.where(condition, x, y).shape, (2, 3))\n \n+    def test_floordiv(self):\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3))\n+        self.assertEqual(knp.floor_divide(x, y).shape, (2, 3))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            y = KerasTensor([2, 3, 4])\n+            knp.floor_divide(x, y)\n+\n+    def test_xor(self):\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3))\n+        self.assertEqual(knp.logical_xor(x, y).shape, (2, 3))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            y = KerasTensor([2, 3, 4])\n+            knp.logical_xor(x, y)\n+\n \n class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n     def test_mean(self):\n@@ -2961,6 +2991,27 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.vstack([x, y]), np.vstack([x, y]))\n         self.assertAllClose(knp.Vstack()([x, y]), np.vstack([x, y]))\n \n+    def test_floordiv(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        y = np.array([[4, 5, 6], [3, 2, 1]])\n+        z = np.array([[[1, 2, 3], [3, 2, 1]]])\n+        self.assertAllClose(knp.floor_divide(x, y), np.floor_divide(x, y))\n+        self.assertAllClose(knp.floor_divide(x, z), np.floor_divide(x, z))\n+\n+        self.assertAllClose(knp.FloorDiv()(x, y), np.floor_divide(x, y))\n+        self.assertAllClose(knp.FloorDiv()(x, z), np.floor_divide(x, z))\n+\n+    def test_xor(self):\n+        x = np.array([[True, False], [True, True]])\n+        y = np.array([[False, False], [True, False]])\n+        self.assertAllClose(knp.logical_xor(x, y), np.logical_xor(x, y))\n+        self.assertAllClose(knp.logical_xor(x, True), np.logical_xor(x, True))\n+        self.assertAllClose(knp.logical_xor(True, x), np.logical_xor(True, x))\n+\n+        self.assertAllClose(knp.LogicalXor()(x, y), np.logical_xor(x, y))\n+        self.assertAllClose(knp.LogicalXor()(x, True), np.logical_xor(x, True))\n+        self.assertAllClose(knp.LogicalXor()(True, x), np.logical_xor(True, x))\n+\n \n class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n     def test_ones(self):\n\n@@ -69,11 +69,8 @@ class SymbolicArgumentsTest(testing.TestCase):\n \n         (value, _) = sym_args.convert(lambda x: x**2)\n         args1 = value[0][0]\n-        args2 = value[0][1]\n \n         self.assertIsInstance(args1, KerasTensor)\n-        self.assertNotEqual(args1, a)\n-        self.assertNotEqual(args2, b)\n \n         mapped_value = tree.map_structure(lambda x: x**2, a)\n         self.assertEqual(mapped_value.shape, args1.shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#080a17cd21c027c716384f5de745a5596c768a30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 634 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3460,7 +3460,7 @@ def eye(N, M=None, k=0, dtype=\"float32\"):\n     return backend.numpy.eye(N, M=M, k=k, dtype=dtype)\n \n \n-class FloorDiv(Operation):\n+class FloorDivide(Operation):\n     def call(self, x1, x2):\n         return backend.numpy.floor_divide(x1, x2)\n \n@@ -3476,7 +3476,7 @@ class FloorDiv(Operation):\n )\n def floor_divide(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n-        return FloorDiv().symbolic_call(x1, x2)\n+        return FloorDivide().symbolic_call(x1, x2)\n     return backend.numpy.floor_divide(x1, x2)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d513acd1f5b72141cd641c9d26c545fe43b61a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 153 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -2998,8 +2998,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.floor_divide(x, y), np.floor_divide(x, y))\n         self.assertAllClose(knp.floor_divide(x, z), np.floor_divide(x, z))\n \n-        self.assertAllClose(knp.FloorDiv()(x, y), np.floor_divide(x, y))\n-        self.assertAllClose(knp.FloorDiv()(x, z), np.floor_divide(x, z))\n+        self.assertAllClose(knp.FloorDivide()(x, y), np.floor_divide(x, y))\n+        self.assertAllClose(knp.FloorDivide()(x, z), np.floor_divide(x, z))\n \n     def test_xor(self):\n         x = np.array([[True, False], [True, True]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a64da12397375880a40175bcdfe3dc3a1c55e30f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 15/5 | Churn Δ: 90 | Churn Cumulative: 2350 | Contributors (this commit): 14 | Commits (past 90d): 113 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -243,5 +243,9 @@ def while_loop(\n     return outputs\n \n \n+def fori_loop(lower, upper, body_fun, init_val):\n+    return jax.lax.fori_loop(lower, upper, body_fun, init_val)\n+\n+\n def stop_gradient(variable):\n     return jax.lax.stop_gradient(variable)\n\n@@ -208,5 +208,12 @@ def while_loop(\n     return loop_vars\n \n \n+def fori_loop(lower, upper, body_fun, init_val):\n+    val = init_val\n+    for i in range(lower, upper):\n+        val = body_fun(i, val)\n+    return val\n+\n+\n def stop_gradient(x):\n     pass\n\n@@ -163,5 +163,13 @@ def while_loop(\n     )\n \n \n+def fori_loop(lower, upper, body_fun, init_val):\n+    return tf.while_loop(\n+        lambda i, val: i < upper,\n+        lambda i, val: (i + 1, body_fun(i, val)),\n+        (lower, init_val),\n+    )[1]\n+\n+\n def stop_gradient(variable):\n     return tf.stop_gradient(variable)\n\n@@ -342,6 +342,13 @@ def while_loop(\n     return loop_vars\n \n \n+def fori_loop(lower, upper, body_fun, init_val):\n+    val = init_val\n+    for i in range(lower, upper):\n+        val = body_fun(i, val)\n+    return val\n+\n+\n def stop_gradient(variable):\n     # We can't use `.requires_grad_(False)` here since it only\n     # works when the tensor is a leaf node in the graph.\n\n@@ -294,6 +294,54 @@ def stop_gradient(variable):\n     return backend.core.stop_gradient(variable)\n \n \n+class ForiLoop(Operation):\n+    def __init__(self, lower, upper, body_fun):\n+        super().__init__()\n+        self.lower = lower\n+        self.upper = upper\n+        self.body_fun = body_fun\n+\n+    def call(self, init_val):\n+        return backend.core.fori_loop(\n+            self.lower,\n+            self.upper,\n+            self.body_fun,\n+            init_val,\n+        )\n+\n+    def compute_output_spec(self, init_val):\n+        return KerasTensor(init_val.shape, dtype=init_val.dtype)\n+\n+\n+@keras_core_export(\"keras_core.ops.fori_loop\")\n+def fori_loop(lower, upper, body_fun, init_val):\n+    \"\"\"For loop implementation.\n+\n+    Args:\n+        lower: The initial value of the loop variable.\n+        upper: The upper bound of the loop variable.\n+        body_fun: A callable that represents the loop body. Must take two\n+            arguments: the loop variable and the loop state. The loop state\n+            should be updated and returned by this function.\n+        init_val: The initial value of the loop state.\n+\n+    Returns:\n+        The final state after the loop.\n+\n+    Example:\n+\n+    >>> lower = 0\n+    >>> upper = 10\n+    >>> body_fun = lambda i, s: (i + 1, s + i)\n+    >>> init_val = 0\n+    >>> keras_core.ops.fori_loop(lower, upper, body_fun, init_val)\n+    45\n+    \"\"\"\n+    if any_symbolic_tensors((lower, upper, init_val)):\n+        return ForiLoop(lower, upper, body_fun).symbolic_call(init_val)\n+    return backend.core.fori_loop(lower, upper, body_fun, init_val)\n+\n+\n @keras_core_export(\"keras_core.ops.shape\")\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n\n@@ -48,6 +48,14 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n             core.slice_update(inputs, start_indices, updates).shape, (4, 4, 4)\n         )\n \n+    def test_fori_loop(self):\n+        def body_fun(i, x):\n+            return x + i\n+\n+        initial_value = KerasTensor((3, 5, 7))\n+        result = core.fori_loop(0, 10, body_fun, initial_value)\n+        self.assertEqual(result.shape, (3, 5, 7))\n+\n \n class CoreOpsCorrectnessTest(testing.TestCase):\n     def test_scatter(self):\n@@ -205,6 +213,14 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(x, np.ones((2, 3)) * 6)\n         self.assertAllClose(y, np.ones((3, 2)) * 6)\n \n+    def test_fori_loop(self):\n+        def body_fun(i, x):\n+            return x + i\n+\n+        initial_value = np.array(0)\n+        result = core.fori_loop(0, 10, body_fun, initial_value)\n+        self.assertAllClose(result, 45)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_stop_gradient(self):\n         class ExampleLayer(layers.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b48e5a239e382e284e75880b9c5776bac25ecb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,9 +14,9 @@ class LazyModule:\n         if self._available is None:\n             try:\n                 self.initialize()\n+                self._available = True\n             except ImportError:\n                 self._available = False\n-            self._available = True\n         return self._available\n \n     def initialize(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4837bde0b027564e0c88639b80728bf405db165e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 739 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -229,6 +229,22 @@ class Absolute(Operation):\n \n @keras_core_export([\"keras_core.ops.absolute\", \"keras_core.ops.numpy.absolute\"])\n def absolute(x):\n+    \"\"\"Compute the absolute value element-wise.\n+\n+    `keras_core.ops.abs` is a shorthand for this function.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        An array containing the absolute value of each element in x.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([-1.2, 1.2])\n+    >>> keras_core.ops.absolute(x)\n+    array([1.2 1.2], shape=(2,), dtype=float32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Absolute().symbolic_call(x)\n     return backend.numpy.absolute(x)\n@@ -240,6 +256,7 @@ class Abs(Absolute):\n \n @keras_core_export([\"keras_core.ops.abs\", \"keras_core.ops.numpy.abs\"])\n def abs(x):\n+    \"\"\"Shorthand for `keras_core.ops.absolute`.\"\"\"\n     return absolute(x)\n \n \n@@ -256,6 +273,32 @@ class Add(Operation):\n \n @keras_core_export([\"keras_core.ops.add\", \"keras_core.ops.numpy.add\"])\n def add(x1, x2):\n+    \"\"\"Add arguments element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        The tensor containing the element-wise sum of `x1` and `x2`.\n+\n+    Examples:\n+\n+    >>> x1 = keras_core.ops.convert_to_tensor([1, 4])\n+    >>> x2 = keras_core.ops.convert_to_tensor([5, 6])\n+    >>> keras_core.ops.add(x1, x2)\n+    array([ 6 10], shape=(2,), dtype=int32)\n+\n+    `keras_core.ops.add` also broadcasts shapes:\n+    >>> x1 = keras_core.ops.convert_to_tensor(\n+    ...     [[5, 4],\n+    ...      [5, 6]]\n+    ... )\n+    >>> x2 = keras_core.ops.convert_to_tensor([5, 6])\n+    >>> keras_core.ops.add(x1, x2)\n+    array([[10 10]\n+           [10 12]], shape=(2, 2), dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Add().symbolic_call(x1, x2)\n     return backend.numpy.add(x1, x2)\n@@ -290,6 +333,37 @@ class All(Operation):\n \n @keras_core_export([\"keras_core.ops.all\", \"keras_core.ops.numpy.all\"])\n def all(x, axis=None, keepdims=False):\n+    \"\"\"Test whether all array elements along a given axis evaluate to `True`.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: An integer or tuple of integers that represent the axis along\n+            which a logical AND reduction is performed. The default\n+            (`axis=None`) is to perform a logical AND over all the dimensions\n+            of the input array. `axis` may be negative, in which case it counts\n+            for the last to the first axis.\n+        keepdims: If `True`, axes which are reduced are left in the result as\n+            dimensions with size one. With this option, the result will\n+            broadcast correctly against the input array. Default is `False`.\n+    \n+    Returns:\n+        The tensor containing the logical AND reduction over the `axis`.\n+\n+    Examples:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([True, False])\n+    >>> keras_core.ops.all(x)\n+    array(False, shape=(), dtype=bool)\n+\n+    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras_core.ops.all(x, axis=0)\n+    array([ True False], shape=(2,), dtype=bool)\n+\n+    `keepdims=True` outputs a tensor with dimensions reduced to one.\n+    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras_core.ops.all(x, keepdims=True)\n+    array([[False]], shape=(1, 1), dtype=bool)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return All(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.all(x, axis=axis, keepdims=keepdims)\n@@ -324,6 +398,37 @@ class Any(Operation):\n \n @keras_core_export([\"keras_core.ops.any\", \"keras_core.ops.numpy.any\"])\n def any(x, axis=None, keepdims=False):\n+    \"\"\"Test whether any array element along a given axis evaluates to True.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: An integer or tuple of integers that represent the axis along\n+            which a logical OR reduction is performed. The default\n+            (`axis=None`) is to perform a logical OR over all the dimensions\n+            of the input array. `axis` may be negative, in which case it counts\n+            for the last to the first axis.\n+        keepdims: If `True`, axes which are reduced are left in the result as\n+            dimensions with size one. With this option, the result will\n+            broadcast correctly against the input array. Default is `False`.\n+    \n+    Returns:\n+        The tensor containing the logical OR reduction over the `axis`.\n+    \n+    Examples:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([True, False])\n+    >>> keras_core.ops.any(x)\n+    array(True, shape=(), dtype=bool)\n+\n+    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras_core.ops.any(x, axis=0)\n+    array([ True  True], shape=(2,), dtype=bool)\n+\n+    `keepdims=True` outputs a tensor with dimensions reduced to one.\n+    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras_core.ops.all(x, keepdims=True)\n+    array([[False]], shape=(1, 1), dtype=bool)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Any(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.any(x, axis=axis, keepdims=keepdims)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6cb4bd88f17e0b7e3221ffe254ab60eeb03b2a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 8 | Churn Cumulative: 1039 | Contributors (this commit): 10 | Commits (past 90d): 46 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -126,7 +126,11 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    if noise_shape is not None:\n+    if (\n+        seed is not None\n+        and not (isinstance(seed, SeedGenerator) and seed._initial_seed is None)\n+        or noise_shape is not None\n+    ):\n         keep_prob = 1.0 - rate\n         noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n         keep_prob_matrix = torch.full(\n\n@@ -35,8 +35,10 @@ class SeedGenerator:\n         else:\n             self.backend = backend\n \n+        self._initial_seed = seed\n         if seed is None:\n             seed = make_default_seed()\n+\n         if not isinstance(seed, int):\n             raise ValueError(\n                 \"Argument `seed` must be an integer. \" f\"Received: seed={seed}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6eb30acced72e980c9d8c2042bdd29326a112867", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 471 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 471 | Churn Cumulative: 471 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7403846153846154\n\nDIFF:\n@@ -0,0 +1,471 @@\n+\"\"\"\n+Title: Keypoint Detection with Transfer Learning\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/05/02\n+Last modified: 2023/07/19\n+Description: Training a keypoint detector with data augmentation and transfer learning.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+Keypoint detection consists of locating key object parts. For example, the key parts\n+of our faces include nose tips, eyebrows, eye corners, and so on. These parts help to\n+represent the underlying object in a feature-rich manner. Keypoint detection has\n+applications that include pose estimation, face detection, etc.\n+\n+In this example, we will build a keypoint detector using the\n+[StanfordExtra dataset](https://github.com/benjiebob/StanfordExtra),\n+using transfer learning. This example requires TensorFlow 2.4 or higher,\n+as well as [`imgaug`](https://imgaug.readthedocs.io/) library,\n+which can be installed using the following command:\n+\"\"\"\n+\n+\"\"\"shell\n+pip install -q -U imgaug\n+\"\"\"\n+\n+\"\"\"\n+## Data collection\n+\"\"\"\n+\n+\"\"\"\n+The StanfordExtra dataset contains 12,000 images of dogs together with keypoints and\n+segmentation maps. It is developed from the [Stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/).\n+It can be downloaded with the command below:\n+\"\"\"\n+\n+\"\"\"shell\n+wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n+\"\"\"\n+\n+\"\"\"\n+Annotations are provided as a single JSON file in the StanfordExtra dataset and one needs\n+to fill [this form](https://forms.gle/sRtbicgxsWvRtRmUA) to get access to it. The\n+authors explicitly instruct users not to share the JSON file, and this example respects this wish:\n+you should obtain the JSON file yourself.\n+\n+The JSON file is expected to be locally available as `stanfordextra_v12.zip`.\n+\n+After the files are downloaded, we can extract the archives.\n+\"\"\"\n+\n+\"\"\"shell\n+tar xf images.tar\n+unzip -qq ~/stanfordextra_v12.zip\n+\"\"\"\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+from keras_core import layers\n+import keras_core as keras\n+\n+from imgaug.augmentables.kps import KeypointsOnImage\n+from imgaug.augmentables.kps import Keypoint\n+import imgaug.augmenters as iaa\n+\n+from PIL import Image\n+from sklearn.model_selection import train_test_split\n+from matplotlib import pyplot as plt\n+import pandas as pd\n+import numpy as np\n+import json\n+import os\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+IMG_SIZE = 224\n+BATCH_SIZE = 64\n+EPOCHS = 5\n+NUM_KEYPOINTS = 24 * 2  # 24 pairs each having x and y coordinates\n+\n+\"\"\"\n+## Load data\n+\n+The authors also provide a metadata file that specifies additional information about the\n+keypoints, like color information, animal pose name, etc. We will load this file in a `pandas`\n+dataframe to extract information for visualization purposes.\n+\"\"\"\n+\n+IMG_DIR = \"Images\"\n+JSON = \"StanfordExtra_V12/StanfordExtra_v12.json\"\n+KEYPOINT_DEF = \"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\n+\n+# Load the ground-truth annotations.\n+with open(JSON) as infile:\n+    json_data = json.load(infile)\n+\n+# Set up a dictionary, mapping all the ground-truth information\n+# with respect to the path of the image.\n+json_dict = {i[\"img_path\"]: i for i in json_data}\n+\n+\"\"\"\n+A single entry of `json_dict` looks like the following:\n+\n+```\n+'n02085782-Japanese_spaniel/n02085782_2886.jpg':\n+{'img_bbox': [205, 20, 116, 201],\n+ 'img_height': 272,\n+ 'img_path': 'n02085782-Japanese_spaniel/n02085782_2886.jpg',\n+ 'img_width': 350,\n+ 'is_multiple_dogs': False,\n+ 'joints': [[108.66666666666667, 252.0, 1],\n+            [147.66666666666666, 229.0, 1],\n+            [163.5, 208.5, 1],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [54.0, 244.0, 1],\n+            [77.33333333333333, 225.33333333333334, 1],\n+            [79.0, 196.5, 1],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [150.66666666666666, 86.66666666666667, 1],\n+            [88.66666666666667, 73.0, 1],\n+            [116.0, 106.33333333333333, 1],\n+            [109.0, 123.33333333333333, 1],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0],\n+            [0, 0, 0]],\n+ 'seg': ...}\n+```\n+\"\"\"\n+\n+\"\"\"\n+In this example, the keys we are interested in are:\n+\n+* `img_path`\n+* `joints`\n+\n+There are a total of 24 entries present inside `joints`. Each entry has 3 values:\n+\n+* x-coordinate\n+* y-coordinate\n+* visibility flag of the keypoints (1 indicates visibility and 0 indicates non-visibility)\n+\n+As we can see `joints` contain multiple `[0, 0, 0]` entries which denote that those\n+keypoints were not labeled. In this example, we will consider both non-visible as well as\n+unlabeled keypoints in order to allow mini-batch learning.\n+\"\"\"\n+\n+# Load the metdata definition file and preview it.\n+keypoint_def = pd.read_csv(KEYPOINT_DEF)\n+keypoint_def.head()\n+\n+# Extract the colours and labels.\n+colours = keypoint_def[\"Hex colour\"].values.tolist()\n+colours = [\"#\" + colour for colour in colours]\n+labels = keypoint_def[\"Name\"].values.tolist()\n+\n+\n+# Utility for reading an image and for getting its annotations.\n+def get_dog(name):\n+    data = json_dict[name]\n+    img_data = plt.imread(os.path.join(IMG_DIR, data[\"img_path\"]))\n+    # If the image is RGBA convert it to RGB.\n+    if img_data.shape[-1] == 4:\n+        img_data = img_data.astype(np.uint8)\n+        img_data = Image.fromarray(img_data)\n+        img_data = np.array(img_data.convert(\"RGB\"))\n+    data[\"img_data\"] = img_data\n+\n+    return data\n+\n+\n+\"\"\"\n+## Visualize data\n+\n+Now, we write a utility function to visualize the images and their keypoints.\n+\"\"\"\n+\n+\n+# Parts of this code come from here:\n+# https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb\n+def visualize_keypoints(images, keypoints):\n+    fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))\n+    [ax.axis(\"off\") for ax in np.ravel(axes)]\n+\n+    for (ax_orig, ax_all), image, current_keypoint in zip(\n+        axes, images, keypoints\n+    ):\n+        ax_orig.imshow(image)\n+        ax_all.imshow(image)\n+\n+        # If the keypoints were formed by `imgaug` then the coordinates need\n+        # to be iterated differently.\n+        if isinstance(current_keypoint, KeypointsOnImage):\n+            for idx, kp in enumerate(current_keypoint.keypoints):\n+                ax_all.scatter(\n+                    [kp.x],\n+                    [kp.y],\n+                    c=colours[idx],\n+                    marker=\"x\",\n+                    s=50,\n+                    linewidths=5,\n+                )\n+        else:\n+            current_keypoint = np.array(current_keypoint)\n+            # Since the last entry is the visibility flag, we discard it.\n+            current_keypoint = current_keypoint[:, :2]\n+            for idx, (x, y) in enumerate(current_keypoint):\n+                ax_all.scatter(\n+                    [x], [y], c=colours[idx], marker=\"x\", s=50, linewidths=5\n+                )\n+\n+    plt.tight_layout(pad=2.0)\n+    plt.show()\n+\n+\n+# Select four samples randomly for visualization.\n+samples = list(json_dict.keys())\n+num_samples = 4\n+selected_samples = np.random.choice(samples, num_samples, replace=False)\n+\n+images, keypoints = [], []\n+\n+for sample in selected_samples:\n+    data = get_dog(sample)\n+    image = data[\"img_data\"]\n+    keypoint = data[\"joints\"]\n+\n+    images.append(image)\n+    keypoints.append(keypoint)\n+\n+visualize_keypoints(images, keypoints)\n+\n+\"\"\"\n+The plots show that we have images of non-uniform sizes, which is expected in most\n+real-world scenarios. However, if we resize these images to have a uniform shape (for\n+instance (224 x 224)) their ground-truth annotations will also be affected. The same\n+applies if we apply any geometric transformation (horizontal flip, for e.g.) to an image.\n+Fortunately, `imgaug` provides utilities that can handle this issue.\n+In the next section, we will write a data generator inheriting the\n+[`keras.utils.Sequence`](https://keras.io/api/utils/python_utils/#sequence-class) class\n+that applies data augmentation on batches of data using `imgaug`.\n+\"\"\"\n+\n+\"\"\"\n+## Prepare data generator\n+\"\"\"\n+\n+\n+class KeyPointsDataset(keras.utils.PyDataset):\n+    def __init__(\n+        self, image_keys, aug, batch_size=BATCH_SIZE, train=True, **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self.image_keys = image_keys\n+        self.aug = aug\n+        self.batch_size = batch_size\n+        self.train = train\n+        self.on_epoch_end()\n+\n+    def __len__(self):\n+        return len(self.image_keys) // self.batch_size\n+\n+    def on_epoch_end(self):\n+        self.indexes = np.arange(len(self.image_keys))\n+        if self.train:\n+            np.random.shuffle(self.indexes)\n+\n+    def __getitem__(self, index):\n+        indexes = self.indexes[\n+            index * self.batch_size : (index + 1) * self.batch_size\n+        ]\n+        image_keys_temp = [self.image_keys[k] for k in indexes]\n+        (images, keypoints) = self.__data_generation(image_keys_temp)\n+\n+        return (images, keypoints)\n+\n+    def __data_generation(self, image_keys_temp):\n+        batch_images = np.empty(\n+            (self.batch_size, IMG_SIZE, IMG_SIZE, 3), dtype=\"int\"\n+        )\n+        batch_keypoints = np.empty(\n+            (self.batch_size, 1, 1, NUM_KEYPOINTS), dtype=\"float32\"\n+        )\n+\n+        for i, key in enumerate(image_keys_temp):\n+            data = get_dog(key)\n+            current_keypoint = np.array(data[\"joints\"])[:, :2]\n+            kps = []\n+\n+            # To apply our data augmentation pipeline, we first need to\n+            # form Keypoint objects with the original coordinates.\n+            for j in range(0, len(current_keypoint)):\n+                kps.append(\n+                    Keypoint(x=current_keypoint[j][0], y=current_keypoint[j][1])\n+                )\n+\n+            # We then project the original image and its keypoint coordinates.\n+            current_image = data[\"img_data\"]\n+            kps_obj = KeypointsOnImage(kps, shape=current_image.shape)\n+\n+            # Apply the augmentation pipeline.\n+            (new_image, new_kps_obj) = self.aug(\n+                image=current_image, keypoints=kps_obj\n+            )\n+            batch_images[i,] = new_image\n+\n+            # Parse the coordinates from the new keypoint object.\n+            kp_temp = []\n+            for keypoint in new_kps_obj:\n+                kp_temp.append(np.nan_to_num(keypoint.x))\n+                kp_temp.append(np.nan_to_num(keypoint.y))\n+\n+            # More on why this reshaping later.\n+            batch_keypoints[i,] = np.array(kp_temp).reshape(1, 1, 24 * 2)\n+\n+        # Scale the coordinates to [0, 1] range.\n+        batch_keypoints = batch_keypoints / IMG_SIZE\n+\n+        return (batch_images, batch_keypoints)\n+\n+\n+\"\"\"\n+To know more about how to operate with keypoints in `imgaug` check out\n+[this document](https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html).\n+\"\"\"\n+\n+\"\"\"\n+## Define augmentation transforms\n+\"\"\"\n+\n+train_aug = iaa.Sequential(\n+    [\n+        iaa.Resize(IMG_SIZE, interpolation=\"linear\"),\n+        iaa.Fliplr(0.3),\n+        # `Sometimes()` applies a function randomly to the inputs with\n+        # a given probability (0.3, in this case).\n+        iaa.Sometimes(0.3, iaa.Affine(rotate=10, scale=(0.5, 0.7))),\n+    ]\n+)\n+\n+test_aug = iaa.Sequential([iaa.Resize(IMG_SIZE, interpolation=\"linear\")])\n+\n+\"\"\"\n+## Create training and validation splits\n+\"\"\"\n+\n+np.random.shuffle(samples)\n+train_keys, validation_keys = (\n+    samples[int(len(samples) * 0.15) :],\n+    samples[: int(len(samples) * 0.15)],\n+)\n+\n+\n+\"\"\"\n+## Data generator investigation\n+\"\"\"\n+\n+train_dataset = KeyPointsDataset(\n+    train_keys, train_aug, workers=2, use_multiprocessing=True\n+)\n+validation_dataset = KeyPointsDataset(\n+    validation_keys, test_aug, train=False, workers=2, use_multiprocessing=True\n+)\n+\n+print(f\"Total batches in training set: {len(train_dataset)}\")\n+print(f\"Total batches in validation set: {len(validation_dataset)}\")\n+\n+sample_images, sample_keypoints = next(iter(train_dataset))\n+assert sample_keypoints.max() == 1.0\n+assert sample_keypoints.min() == 0.0\n+\n+sample_keypoints = sample_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE\n+visualize_keypoints(sample_images[:4], sample_keypoints)\n+\n+\"\"\"\n+## Model building\n+\n+The [Stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) (on which\n+the StanfordExtra dataset is based) was built using the [ImageNet-1k dataset](http://image-net.org/).\n+So, it is likely that the models pretrained on the ImageNet-1k dataset would be useful\n+for this task. We will use a MobileNetV2 pre-trained on this dataset as a backbone to\n+extract meaningful features from the images and then pass those to a custom regression\n+head for predicting coordinates.\n+\"\"\"\n+\n+\n+def get_model():\n+    # Load the pre-trained weights of MobileNetV2 and freeze the weights\n+    backbone = keras.applications.MobileNetV2(\n+        weights=\"imagenet\",\n+        include_top=False,\n+        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n+    )\n+    backbone.trainable = False\n+\n+    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n+    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n+    x = backbone(x)\n+    x = layers.Dropout(0.3)(x)\n+    x = layers.SeparableConv2D(\n+        NUM_KEYPOINTS, kernel_size=5, strides=1, activation=\"relu\"\n+    )(x)\n+    outputs = layers.SeparableConv2D(\n+        NUM_KEYPOINTS, kernel_size=3, strides=1, activation=\"sigmoid\"\n+    )(x)\n+\n+    return keras.Model(inputs, outputs, name=\"keypoint_detector\")\n+\n+\n+\"\"\"\n+Our custom network is fully-convolutional which makes it more parameter-friendly than the\n+same version of the network having fully-connected dense layers.\n+\"\"\"\n+\n+get_model().summary()\n+\n+\"\"\"\n+Notice the output shape of the network: `(None, 1, 1, 48)`. This is why we have reshaped\n+the coordinates as: `batch_keypoints[i, :] = np.array(kp_temp).reshape(1, 1, 24 * 2)`.\n+\"\"\"\n+\n+\"\"\"\n+## Model compilation and training\n+\n+For this example, we will train the network only for five epochs.\n+\"\"\"\n+\n+model = get_model()\n+model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(1e-4))\n+model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS)\n+\n+\"\"\"\n+## Make predictions and visualize them\n+\"\"\"\n+\n+sample_val_images, sample_val_keypoints = next(iter(validation_dataset))\n+sample_val_images = sample_val_images[:4]\n+sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE\n+predictions = model.predict(sample_val_images).reshape(-1, 24, 2) * IMG_SIZE\n+\n+# Ground-truth\n+visualize_keypoints(sample_val_images, sample_val_keypoints)\n+\n+# Predictions\n+visualize_keypoints(sample_val_images, predictions)\n+\n+\"\"\"\n+Predictions will likely improve with more training.\n+\"\"\"\n+\n+\"\"\"\n+## Going further\n+\n+* Try using other augmentation transforms from `imgaug` to investigate how that changes\n+the results.\n+* Here, we transferred the features from the pre-trained network linearly that is we did\n+not [fine-tune](https://keras.io/guides/transfer_learning/) it. You are encouraged to fine-tune it on this task and see if that\n+improves the performance. You can also try different architectures and see how they\n+affect the final performance.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
