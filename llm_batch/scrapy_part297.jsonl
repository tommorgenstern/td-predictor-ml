{"custom_id": "scrapy#3672f5f988cbb29c6bebb7ed535c0d5ad941d868", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 738 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -296,7 +296,7 @@ class StartUrlsSpider(Spider):\n \n     def __init__(self, start_urls):\n         self.start_urls = start_urls\n-        super(StartUrlsSpider, self).__init__(start_urls)\n+        super(StartUrlsSpider, self).__init__(name='StartUrlsSpider')\n \n     def parse(self, response):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a97ac0adf86b67a16f09c41f618f736adb872503", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 55 | Files Changed: 3 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 85 | Churn Cumulative: 1520 | Contributors (this commit): 1 | Commits (past 90d): 23 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,8 @@ from h2.events import (\n     DataReceived, ResponseReceived, SettingsAcknowledged,\n     StreamEnded, StreamReset, WindowUpdated\n )\n+\n+from twisted.internet.ssl import Certificate\n from twisted.internet.protocol import connectionDone, Protocol\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n@@ -115,7 +117,7 @@ class H2ClientProtocol(Protocol):\n         self.destination = self.transport.getPeer()\n         LOGGER.info('Connection made to {}'.format(self.destination))\n \n-        self._metadata['certificate'] = self.transport.getPeerCertificate()\n+        self._metadata['certificate'] = Certificate(self.transport.getPeerCertificate())\n         self._metadata['ip_address'] = ipaddress.ip_address(self.destination.host)\n \n         self.conn.initiate_connection()\n\n@@ -117,7 +117,7 @@ class Stream:\n         self._deferred_response = Deferred()\n \n     def __str__(self):\n-        return \"Stream(id={})\".format(self.stream_id)\n+        return \"Stream(id={})\".format(repr(self.stream_id))\n \n     __repr__ = __str__\n \n@@ -299,9 +299,6 @@ class Stream:\n     def close(self, reason: StreamCloseReason):\n         \"\"\"Based on the reason sent we will handle each case.\n         \"\"\"\n-        # TODO: In case of abruptly stream close\n-        #  Q1. Do we need to send the request again?\n-        #  Q2. What response should we send now?\n         if self.stream_closed_server:\n             raise StreamClosedError(self.stream_id)\n \n@@ -346,10 +343,7 @@ class Stream:\n         and fires the response deferred callback with the\n         generated response instance\"\"\"\n         # TODO:\n-        #  2. Should we fire this in case of\n-        #   2.1 StreamReset in between when data is received partially\n-        #   2.2 Forcefully closed the stream\n-        #  3. Update Client Side Status Codes here\n+        #  1. Update Client Side Status Codes here\n \n         response_cls = responsetypes.from_args(\n             headers=self._response['headers'],\n\n@@ -1,71 +1,50 @@\n-import os\n-import shutil\n+from urllib.parse import urlparse\n \n-from twisted.internet import defer, reactor\n+from twisted.internet import reactor\n from twisted.internet.endpoints import connectProtocol, SSL4ClientEndpoint\n-from twisted.internet.ssl import optionsForClientTLS\n-from twisted.protocols.policies import WrappingFactory\n-from twisted.python.filepath import FilePath\n+from twisted.internet.ssl import CertificateOptions\n from twisted.trial import unittest\n-from twisted.web import static, server\n \n from scrapy.core.http2.protocol import H2ClientProtocol\n-from scrapy.http import Request\n-from tests.mockserver import ssl_context_factory\n+from scrapy.http import Request, Response\n+from tests.mockserver import MockServer\n \n \n class Http2ClientProtocolTestCase(unittest.TestCase):\n     scheme = 'https'\n \n-    # only used for HTTPS tests\n-    file_key = 'keys/localhost.key'\n-    file_certificate = 'keys/localhost.crt'\n-\n     def setUp(self):\n         # Start server for testing\n-        self.path_temp = self.mktemp()\n-        os.mkdir(self.path_temp)\n-        FilePath(self.path_temp).child('file').setContent(b\"0123456789\")\n-        r = static.File(self.path_temp)\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n \n-        self.site = server.Site(r, timeout=None)\n-        self.wrapper = WrappingFactory(self.site)\n-        self.host = 'localhost'\n-        if self.scheme is 'https':\n-            self.port = reactor.listenSSL(\n-                0, self.wrapper,\n-                ssl_context_factory(self.file_key, self.file_certificate),\n-                interface=self.host\n-            )\n+        if self.scheme == 'https':\n+            self.url = urlparse(self.mockserver.https_address)\n         else:\n-            self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n-\n-        self.port_number = self.port.getHost().port\n-\n-        # Connect to the server using the custom HTTP2ClientProtocol\n-        options = optionsForClientTLS(\n-            hostname=self.host,\n-            acceptableProtocols=[b'h2']\n-        )\n+            self.url = urlparse(self.mockserver.http_address)\n \n         self.protocol = H2ClientProtocol()\n \n-        connectProtocol(\n-            endpoint=SSL4ClientEndpoint(reactor, self.host, self.port_number, options),\n-            protocol=self.protocol\n-        )\n+        # Connect to the server using the custom HTTP2ClientProtocol\n+        options = CertificateOptions(acceptableProtocols=[b'h2'])\n+        endpoint = SSL4ClientEndpoint(reactor, self.url.hostname, self.url.port, options)\n+        connectProtocol(endpoint, self.protocol)\n \n     def getURL(self, path):\n-        return \"%s://%s:%d/%s\" % (self.scheme, self.host, self.port_number, path)\n+        return \"{}://{}:{}/{}\".format(self.url.scheme, self.url.hostname, self.url.port, path)\n \n-    @defer.inlineCallbacks\n     def tearDown(self):\n-        yield self.port.stopListening()\n-        shutil.rmtree(self.path_temp)\n+        self.mockserver.__exit__(None, None, None)\n \n     def test_download(self):\n-        request = Request(self.getURL('file'))\n+        request = Request(self.getURL(''))\n+\n+        def assert_response(response: Response):\n+            self.assertEqual(response.body, b'Scrapy mock HTTP server\\n')\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.request, request)\n+            self.assertEqual(response.url, request.url)\n+\n         d = self.protocol.request(request)\n-        d.addCallback(lambda response: response.body)\n-        d.addCallback(self.assertEqual, b\"0123456789\")\n+        d.addCallback(assert_response)\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69f6d038c0bc51a9de706890621d2ce183f79e09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 79 | Files Changed: 4 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 94 | Churn Cumulative: 2060 | Contributors (this commit): 24 | Commits (past 90d): 30 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,9 +9,8 @@ from h2.events import (\n     DataReceived, ResponseReceived, SettingsAcknowledged,\n     StreamEnded, StreamReset, WindowUpdated\n )\n-\n-from twisted.internet.ssl import Certificate\n from twisted.internet.protocol import connectionDone, Protocol\n+from twisted.internet.ssl import Certificate\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.http import Request\n@@ -22,12 +21,7 @@ LOGGER = logging.getLogger(__name__)\n class H2ClientProtocol(Protocol):\n     # TODO:\n     #  1. Check for user-agent while testing\n-    #  2. Add support for cookies\n-    #  3. Handle priority updates (Not required)\n-    #  4. Handle case when received events have StreamID = 0 (applied to H2Connection)\n-    #  1 & 2:\n-    #   - Automatically handled by the Request middleware\n-    #   - request.headers will have 'Set-Cookie' value\n+    #  2. Handle case when received events have StreamID = 0 (applied to H2Connection)\n \n     def __init__(self):\n         config = H2Configuration(client_side=True, header_encoding='utf-8')\n@@ -185,8 +179,6 @@ class H2ClientProtocol(Protocol):\n         self.streams[stream_id].close(StreamCloseReason.ENDED)\n \n     def stream_reset(self, event: StreamReset):\n-        # TODO: event.stream_id was abruptly closed\n-        #  Q. What should be the response? (Failure/Partial/???)\n         self.streams[event.stream_id].close(StreamCloseReason.RESET)\n \n     def window_updated(self, event: WindowUpdated):\n\n@@ -10,11 +10,20 @@ from h2.exceptions import StreamClosedError\n from twisted.internet.defer import Deferred, CancelledError\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n+# for python < 3.8 -- typing.TypedDict is undefined\n+from typing_extensions import TypedDict\n \n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n \n+\n+class _ResponseTypedDict(TypedDict):\n+    body: BytesIO\n+    flow_controlled_size: int\n+    headers: Headers\n+\n+\n LOGGER = logging.getLogger(__name__)\n \n \n@@ -100,7 +109,7 @@ class Stream:\n         # Private variable used to build the response\n         # this response is then converted to appropriate Response class\n         # passed to the response deferred callback\n-        self._response = {\n+        self._response: _ResponseTypedDict = {\n             # Data received frame by frame from the server is appended\n             # and passed to the response Deferred when completely received.\n             'body': BytesIO(),\n\n@@ -1,67 +0,0 @@\n-# This is simple script to test\n-\n-import json\n-\n-from twisted.internet import reactor\n-from twisted.internet.endpoints import connectProtocol, SSL4ClientEndpoint\n-from twisted.internet.ssl import optionsForClientTLS\n-\n-from scrapy.core.http2.protocol import H2ClientProtocol\n-from scrapy.http import Request, Response, JsonRequest\n-\n-try:\n-    with open('data.json', 'r') as f:\n-        JSON_DATA = json.load(f)\n-except:\n-    JSON_DATA = {\n-        \"data\": \"To test for really large amount of data -- Add data.json with lots of data.\",\n-        \"why\": \"To test whether correct data is sent :)\"\n-    }\n-\n-# Use nghttp2 for testing whether basic setup works - for small response\n-HTTPBIN_AUTHORITY = u'nghttp2.org'\n-HTTPBIN_REQUEST_URLS = 1 * [\n-    Request(url='https://nghttp2.org/httpbin/get', method='GET'),\n-    Request(url='https://nghttp2.org/httpbin/post', method='POST'),\n-    JsonRequest(url='https://nghttp2.org/httpbin/anything', method='POST', data=JSON_DATA),\n-]\n-\n-# Use POKE_API for testing large responses\n-POKE_API_AUTHORITY = u'pokeapi.co'\n-POKE_API_REQUESTS = 15 * [\n-    Request(url='https://pokeapi.co/api/v2/pokemon/ditto', method='GET'),\n-    Request(url='https://pokeapi.co/api/v2/pokemon/charizard', method='GET'),\n-    Request(url='https://pokeapi.co/api/v2/pokemon/pikachu', method='GET'),\n-    Request(url='https://pokeapi.co/api/v2/pokemon/DoesNotExist', method='GET'),  # should give 404\n-]\n-\n-AUTHORITY = POKE_API_AUTHORITY\n-REQUEST_URLS = POKE_API_REQUESTS\n-\n-options = optionsForClientTLS(\n-    hostname=AUTHORITY,\n-    acceptableProtocols=[b'h2'],\n-)\n-\n-protocol = H2ClientProtocol()\n-\n-count_responses = 1\n-\n-\n-def print_response(response):\n-    global count_responses\n-    assert isinstance(response, Response)\n-    print('({})\\t{}: ReponseBodySize={}'.format(count_responses, response, len(response.body)))\n-    count_responses = count_responses + 1\n-\n-\n-for request in REQUEST_URLS:\n-    d = protocol.request(request)\n-    d.addCallback(print_response)\n-\n-connectProtocol(\n-    SSL4ClientEndpoint(reactor, AUTHORITY, 443, options),\n-    protocol\n-)\n-\n-reactor.run()\n\n@@ -65,7 +65,7 @@ setup(\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=3.5',\n+    python_requires='>=3.5.2',\n     install_requires=[\n         'Twisted>=17.9.0',\n         'Twisted[http2]>=17.9.0'\n@@ -80,6 +80,8 @@ setup(\n         'w3lib>=1.17.0',\n         'zope.interface>=4.1.3',\n         'protego>=0.1.15',\n+        'itemadapter>=0.1.0',\n+        'typing_extensions>=3.7'\n     ],\n     extras_require=extras_require,\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f4c964aa4949584cfd251cf2aa2c0c7de6cf251", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1286 | Contributors (this commit): 30 | Commits (past 90d): 7 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -74,6 +74,8 @@ class TextResponse(Response):\n \n     def json(self):\n         \"\"\"\n+        .. versionadded:: 2.2\n+\n         Deserialize a JSON document to a Python object.\n         \"\"\"\n         if self._cached_decoded_json is _NONE:\n\n@@ -138,6 +138,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n \n     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n \n+    .. versionchanged:: 2.2\n        Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n        extension has not been implemented correctly).\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3cee74fd401e6a6307b5eb1786e532bb2cd5aa8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 75/65 | Churn Δ: 24 | Churn Cumulative: 3829 | Contributors (this commit): 60 | Commits (past 90d): 29 | Contributors (cumulative): 97 | DMM Complexity: None\n\nDIFF:\n@@ -243,7 +243,7 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        self.storage_batch_item_count = self.settings.get('FEED_STORAGE_BATCH_ITEM_COUNT', None)\n+        self.storage_batch_item_count = self.settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n\n@@ -146,7 +146,7 @@ FEED_STORAGES_BASE = {\n     's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n     'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n }\n-FEED_STORAGE_BATCH_ITEM_COUNT = None\n+FEED_STORAGE_BATCH_ITEM_COUNT = 0\n FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n\n@@ -1144,7 +1144,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['jl']:\n@@ -1160,7 +1160,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         data = yield self.exported_data(items, settings)\n         for batch in data['csv']:\n             got_batch = csv.DictReader(to_unicode(batch).splitlines())\n@@ -1176,7 +1176,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         for batch in data['xml']:\n@@ -1194,7 +1194,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         # XML\n@@ -1219,7 +1219,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import pickle\n@@ -1236,7 +1236,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},\n             },\n         })\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         data = yield self.exported_data(items, settings)\n         import marshal\n@@ -1262,7 +1262,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             'FEED_STORAGE_BATCH_ITEM_COUNT': 2\n         }\n         header = self.MyItem.fields.keys()\n-        yield self.assertExported(items, header, rows, settings=settings)\n+        yield self.assertExported(items, header, rows, settings=Settings(settings))\n \n     def test_wrong_path(self):\n         \"\"\" If path is without %(batch_time)s or %(batch_id)s an exception must be raised \"\"\"\n@@ -1412,14 +1412,14 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             bucket_name=s3_test_bucket_name, prefix=prefix\n         )\n         storage = S3FeedStorage(s3_test_bucket_name, access_key, secret_key)\n-        settings = {\n+        settings = Settings({\n             'FEEDS': {\n                 s3_test_file_uri: {\n                     'format': 'json',\n                 },\n             },\n             'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n-        }\n+        })\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n             self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n@@ -1436,7 +1436,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n         s3 = boto3.resource('s3')\n         my_bucket = s3.Bucket(s3_test_bucket_name)\n-        batch_size = settings['FEED_STORAGE_BATCH_ITEM_COUNT']\n+        batch_size = settings.getint('FEED_STORAGE_BATCH_ITEM_COUNT')\n \n         with MockServer() as s:\n             runner = CrawlerRunner(Settings(settings))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88a52198b90faa0129c8e05072197cdffbb9653b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 44 | Churn Cumulative: 3258 | Contributors (this commit): 27 | Commits (past 90d): 29 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,7 +25,6 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -337,7 +336,9 @@ class FeedExporter:\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n             # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_ITEM_COUNT and close the old one\n-            if self.storage_batch_item_count and slot.itemcount == self.storage_batch_item_count:\n+            if self.feeds[slot.uri_template].get('batch_item_count', self.storage_batch_item_count) \\\n+                    and slot.itemcount == self.feeds[slot.uri_template].get('batch_item_count',\n+                                                                            self.storage_batch_item_count):\n                 uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n                 self._close_slot(slot, spider)\n                 slots.append(self._start_new_batch(\n\n@@ -1327,8 +1327,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     '<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>'\n                 ).encode('latin-1')\n             ],\n-            'csv': ['bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n-                    'bar,foo\\r\\nBAR1,FOO1\\r\\n'.encode('utf-8')],\n+            'csv': ['foo,bar\\r\\nFOO,BAR\\r\\n'.encode('utf-8'),\n+                    'foo,bar\\r\\nFOO1,BAR1\\r\\n'.encode('utf-8')],\n+            'jsonlines': ['{\"foo\": \"FOO\", \"bar\": \"BAR\"}\\n{\"foo\": \"FOO1\", \"bar\": \"BAR1\"}\\n'.encode('utf-8')],\n         }\n \n         settings = {\n@@ -1348,9 +1349,16 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {\n                     'format': 'csv',\n                     'indent': None,\n-                    'fields': ['bar', 'foo'],\n+                    'fields': ['foo', 'bar'],\n                     'encoding': 'utf-8',\n                 },\n+                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {\n+                    'format': 'jsonlines',\n+                    'indent': None,\n+                    'fields': ['foo', 'bar'],\n+                    'encoding': 'utf-8',\n+                    'batch_item_count': 0,\n+                },\n             },\n             'FEED_STORAGE_BATCH_ITEM_COUNT': 1,\n         }\n@@ -1359,6 +1367,31 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 self.assertEqual(expected_batch, got_batch)\n \n+    @defer.inlineCallbacks\n+    def test_batch_item_count_feeds_setting(self):\n+        items = [dict({'foo': u'FOO', 'bar': u'BAR'}), dict({'foo': u'FOO1', 'bar': u'BAR1'})]\n+\n+        formats = {\n+            'jsonlines': ['{\"foo\": \"FOO\", \"bar\": \"BAR\"}\\n'.encode('utf-8'),\n+                          '{\"foo\": \"FOO1\", \"bar\": \"BAR1\"}\\n'.encode('utf-8')],\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'jsonlines', self._file_mark): {\n+                    'format': 'jsonlines',\n+                    'indent': None,\n+                    'fields': ['foo', 'bar'],\n+                    'encoding': 'utf-8',\n+                    'batch_item_count': 1,\n+                },\n+            },\n+        }\n+        data = yield self.exported_data(items, settings)\n+        for fmt, expected in formats.items():\n+            for expected_batch, got_batch in zip(expected, data[fmt]):\n+                self.assertEqual(expected_batch, got_batch)\n+\n     @defer.inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23da8e106822cf2805e8d74c382a31bca70986f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2130 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ import warnings\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from urllib.parse import urldefrag\n+from urllib.parse import urldefrag, urlparse\n \n from twisted.internet import defer, protocol, ssl\n from twisted.internet.endpoints import TCP4ClientEndpoint\n@@ -255,7 +255,7 @@ class ScrapyProxyAgent(Agent):\n             bindAddress=bindAddress,\n             pool=pool,\n         )\n-        self._proxyURI = URI.fromBytes(proxyURI)\n+        self._proxyURI = URI.fromBytes(urlparse(proxyURI)._replace(scheme=b'http').geturl())\n \n     def request(self, method, uri, headers=None, bodyProducer=None):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f53f06020b740b450c7555d562565a27918e8036", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 1564 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -131,6 +131,7 @@ class ContentLengthHeaderResource(resource.Resource):\n     A testing resource which renders itself as the value of the Content-Length\n     header from the request.\n     \"\"\"\n+\n     def render(self, request):\n         return request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n \n@@ -186,6 +187,7 @@ class EmptyContentTypeHeaderResource(resource.Resource):\n     A testing resource which renders itself as the value of request body\n     without content-type header in response.\n     \"\"\"\n+\n     def render(self, request):\n         request.setHeader(\"content-type\", \"\")\n         return request.content.read()\n@@ -733,6 +735,16 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n         timeout = yield self.assertFailure(d, error.TimeoutError)\n         self.assertIn(domain, timeout.osError)\n \n+    def test_download_with_proxy_without_http_scheme(self):\n+        def _test(response):\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'http://example.com')\n+\n+        http_proxy = self.getURL('').replace('http:', '')\n+        request = Request('http://example.com', meta={'proxy': http_proxy})\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n \n class HttpDownloadHandlerMock:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05c2587c6a32b84a94463f2b1187e49f94957aa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2451 | Contributors (this commit): 22 | Commits (past 90d): 16 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -1352,7 +1352,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     'fields': ['foo', 'bar'],\n                     'encoding': 'utf-8',\n                 },\n-                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {\n+                os.path.join(self._random_temp_filename(), 'jsonlines', self._file_mark): {\n                     'format': 'jsonlines',\n                     'indent': None,\n                     'fields': ['foo', 'bar'],\n@@ -1423,8 +1423,8 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         [testenv]\n         setenv =\n             AWS_SECRET_ACCESS_KEY = ABCD\n-            AWS_ACCESS_KEY_ID = ABCD\n-            S3_TEST_BUCKET_NAME = ABCD\n+            AWS_ACCESS_KEY_ID = EFGH\n+            S3_TEST_BUCKET_NAME = IJKL\n         \"\"\"\n         try:\n             import boto3\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#690dd7f38bfd245428bdc618dcca1fbc26284df1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 330 | Lines Deleted: 60 | Files Changed: 4 | Hunks: 44 | Methods Changed: 40 | Complexity Δ (Sum/Max): 34/31 | Churn Δ: 390 | Churn Cumulative: 2458 | Contributors (this commit): 24 | Commits (past 90d): 33 | Contributors (cumulative): 27 | DMM Complexity: 0.9668508287292817\n\nDIFF:\n@@ -129,11 +129,11 @@ class H2ClientProtocol(Protocol):\n         \"\"\"\n         # Pop all streams which were pending and were not yet started\n         for stream_id in list(self.streams):\n-            self.streams[stream_id].close(StreamCloseReason.CONNECTION_LOST)\n+            self.streams[stream_id].close(StreamCloseReason.CONNECTION_LOST, reason)\n \n         self.conn.close_connection()\n \n-        LOGGER.info(\"Connection lost with reason \" + str(reason))\n+        LOGGER.warning(\"Connection lost with reason \" + str(reason))\n \n     def _handle_events(self, events):\n         \"\"\"Private method which acts as a bridge between the events\n\n@@ -19,8 +19,15 @@ from scrapy.responsetypes import responsetypes\n \n \n class _ResponseTypedDict(TypedDict):\n+    # Data received frame by frame from the server is appended\n+    # and passed to the response Deferred when completely received.\n     body: BytesIO\n+\n+    # The amount of data received that counts against the flow control\n+    # window\n     flow_controlled_size: int\n+\n+    # Headers received after sending the request\n     headers: Headers\n \n \n@@ -40,6 +47,9 @@ class StreamCloseReason(IntFlag):\n     # Expected response body size is more than allowed limit\n     MAXSIZE_EXCEEDED = auto()\n \n+    # When the response deferred is cancelled\n+    CANCELLED = auto()\n+\n \n class Stream:\n     \"\"\"Represents a single HTTP/2 Stream.\n@@ -110,20 +120,16 @@ class Stream:\n         # this response is then converted to appropriate Response class\n         # passed to the response deferred callback\n         self._response: _ResponseTypedDict = {\n-            # Data received frame by frame from the server is appended\n-            # and passed to the response Deferred when completely received.\n             'body': BytesIO(),\n-\n-            # The amount of data received that counts against the flow control\n-            # window\n             'flow_controlled_size': 0,\n-\n-            # Headers received after sending the request\n             'headers': Headers({})\n         }\n \n-        # TODO: Add canceller for the Deferred below\n-        self._deferred_response = Deferred()\n+        def _cancel(_):\n+            # Close this stream as gracefully as possible :)\n+            self.reset_stream(StreamCloseReason.CANCELLED)\n+\n+        self._deferred_response = Deferred(_cancel)\n \n     def __str__(self):\n         return \"Stream(id={})\".format(repr(self.stream_id))\n@@ -177,13 +183,6 @@ class Stream:\n             and has initiated request already by sending HEADER frame. If not then\n             stream will raise ProtocolError (raise by h2 state machine).\n          \"\"\"\n-        # TODO:\n-        #  1. Add test for sending very large data\n-        #  2. Add test for small data\n-        #  3. Both (1) and (2) should be tested for\n-        #    3.1 Large number of request\n-        #    3.2 Small number of requests\n-\n         if self.stream_closed_local:\n             raise StreamClosedError(self.stream_id)\n \n@@ -221,7 +220,6 @@ class Stream:\n \n         # End the stream if no more data needs to be send\n         if self.remaining_content_length == 0:\n-            self.stream_closed_local = True\n             self._conn.end_stream(self.stream_id)\n \n         # Write data to transport -- Empty the outstanding data\n@@ -288,7 +286,6 @@ class Stream:\n \n     def reset_stream(self, reason=StreamCloseReason.RESET):\n         \"\"\"Close this stream by sending a RST_FRAME to the remote peer\"\"\"\n-        # TODO: Q. REFUSED_STREAM or CANCEL ?\n         if self.stream_closed_local:\n             raise StreamClosedError(self.stream_id)\n \n@@ -305,14 +302,16 @@ class Stream:\n \n         return expected_size != received_body_size\n \n-    def close(self, reason: StreamCloseReason):\n+    def close(self, reason: StreamCloseReason, failure=None):\n         \"\"\"Based on the reason sent we will handle each case.\n         \"\"\"\n         if self.stream_closed_server:\n             raise StreamClosedError(self.stream_id)\n \n+        self._cb_close(self.stream_id)\n         self.stream_closed_server = True\n \n+        # Do nothing if the response deferred was cancelled\n         flags = None\n         if b'Content-Length' not in self._response['headers']:\n             # Missing Content-Length - PotentialDataLoss\n@@ -320,7 +319,6 @@ class Stream:\n         elif self._is_data_lost():\n             if self._fail_on_dataloss:\n                 self._deferred_response.errback(ResponseFailed([Failure()]))\n-                self._cb_close(self.stream_id)\n                 return\n             else:\n                 flags = ['dataloss']\n@@ -328,10 +326,19 @@ class Stream:\n         if reason is StreamCloseReason.ENDED:\n             self._fire_response_deferred(flags)\n \n-        elif reason in (StreamCloseReason.RESET | StreamCloseReason.CONNECTION_LOST):\n         # Stream was abruptly ended here\n-            self._deferred_response.errback(ResponseFailed([Failure()]))\n+        elif reason is StreamCloseReason.CANCELLED:\n+            # Client has cancelled the request. Remove all the data\n+            # received and fire the response deferred with no flags set\n+            self._response['body'].truncate(0)\n+            self._response['headers'].clear()\n+            self._fire_response_deferred()\n \n+        elif reason in (StreamCloseReason.RESET | StreamCloseReason.CONNECTION_LOST):\n+            if failure is None:\n+                self._deferred_response.errback(ResponseFailed([Failure()]))\n+            else:\n+                self._deferred_response.errback(failure)\n         elif reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n             expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n             error_msg = (\"Cancelling download of {url}: expected response \"\n@@ -345,22 +352,21 @@ class Stream:\n             LOGGER.error(error_msg, error_args)\n             self._deferred_response.errback(CancelledError(error_msg.format(**error_args)))\n \n-        self._cb_close(self.stream_id)\n-\n     def _fire_response_deferred(self, flags=None):\n         \"\"\"Builds response from the self._response dict\n         and fires the response deferred callback with the\n         generated response instance\"\"\"\n-        # TODO:\n-        #  1. Update Client Side Status Codes here\n+        # TODO: Update Client Side Status Codes here\n \n+        body = self._response['body'].getvalue()\n         response_cls = responsetypes.from_args(\n             headers=self._response['headers'],\n             url=self._request.url,\n-            body=self._response['body']\n+            body=body\n         )\n \n-        # If there is no :status in headers then\n+        # If there is no :status in headers\n+        # (happens when client called response_deferred.cancel())\n         # HTTP Status Code: 499 - Client Closed Request\n         status = self._response['headers'].get(':status', '499')\n \n@@ -368,7 +374,7 @@ class Stream:\n             url=self._request.url,\n             status=status,\n             headers=self._response['headers'],\n-            body=self._response['body'].getvalue(),\n+            body=body,\n             request=self._request,\n             flags=flags,\n             certificate=self._conn_metadata['certificate'],\n\n@@ -67,7 +67,6 @@ setup(\n     ],\n     python_requires='>=3.5.2',\n     install_requires=[\n-        'Twisted>=17.9.0',\n         'Twisted[http2]>=17.9.0'\n         'cryptography>=2.0',\n         'cssselect>=0.9.1',\n\n@@ -1,50 +1,315 @@\n-from urllib.parse import urlparse\n+# TODO: Add test cases for\n+#   1. No Content Length response header\n+#   2. Cancel Response Deferred\n+import json\n+import os\n+import random\n+import shutil\n+import string\n \n from twisted.internet import reactor\n-from twisted.internet.endpoints import connectProtocol, SSL4ClientEndpoint\n-from twisted.internet.ssl import CertificateOptions\n-from twisted.trial import unittest\n+from twisted.internet.defer import inlineCallbacks, DeferredList\n+from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint, TCP4ServerEndpoint\n+from twisted.internet.protocol import Factory\n+from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate\n+from twisted.trial.unittest import TestCase\n+from twisted.web.http import Request as TxRequest\n+from twisted.web.resource import Resource\n+from twisted.web.server import Site\n+from twisted.web.static import File\n \n from scrapy.core.http2.protocol import H2ClientProtocol\n-from scrapy.http import Request, Response\n-from tests.mockserver import MockServer\n+from scrapy.http import Request, Response, JsonRequest\n+from tests.mockserver import ssl_context_factory\n \n \n-class Http2ClientProtocolTestCase(unittest.TestCase):\n+def generate_random_string(size):\n+    return ''.join(random.choices(\n+        string.ascii_uppercase + string.digits,\n+        k=size\n+    ))\n+\n+\n+def make_html_body(val):\n+    response = '''<html>\n+<h1>Hello from HTTP2<h1>\n+<p>{}</p>\n+</html>'''.format(val)\n+    return bytes(response, 'utf-8')\n+\n+\n+class Data:\n+    SMALL_SIZE = 1024 * 10  # 10 KB\n+    LARGE_SIZE = (1024 ** 2) * 10  # 10 MB\n+\n+    STR_SMALL = generate_random_string(SMALL_SIZE)\n+    STR_LARGE = generate_random_string(LARGE_SIZE)\n+\n+    EXTRA_SMALL = generate_random_string(1024 * 15)\n+    EXTRA_LARGE = generate_random_string((1024 ** 2) * 15)\n+\n+    HTML_SMALL = make_html_body(STR_SMALL)\n+    HTML_LARGE = make_html_body(STR_LARGE)\n+\n+    JSON_SMALL = {'data': STR_SMALL}\n+    JSON_LARGE = {'data': STR_LARGE}\n+\n+\n+class LeafResource(Resource):\n+    isLeaf = True\n+\n+\n+class GetDataHtmlSmall(LeafResource):\n+    def render_GET(self, request: TxRequest):\n+        request.setHeader('Content-Type', 'text/html; charset=UTF-8')\n+        return Data.HTML_SMALL\n+\n+\n+class GetDataHtmlLarge(LeafResource):\n+    def render_GET(self, request: TxRequest):\n+        request.setHeader('Content-Type', 'text/html; charset=UTF-8')\n+        return Data.HTML_LARGE\n+\n+\n+class PostDataJsonMixin:\n+    @staticmethod\n+    def make_response(request: TxRequest, extra_data: str):\n+        response = {\n+            'request-headers': {},\n+            'request-body': json.loads(request.content.read()),\n+            'extra-data': extra_data\n+        }\n+        for k, v in request.requestHeaders.getAllRawHeaders():\n+            response['request-headers'][k.decode('utf-8')] = v[0].decode('utf-8')\n+\n+        response_bytes = bytes(json.dumps(response), 'utf-8')\n+        request.setHeader('Content-Type', 'application/json')\n+        return response_bytes\n+\n+\n+class PostDataJsonSmall(LeafResource, PostDataJsonMixin):\n+    def render_POST(self, request: TxRequest):\n+        return self.make_response(request, Data.EXTRA_SMALL)\n+\n+\n+class PostDataJsonLarge(LeafResource, PostDataJsonMixin):\n+    def render_POST(self, request: TxRequest):\n+        return self.make_response(request, Data.EXTRA_LARGE)\n+\n+\n+def get_client_certificate(key_file, certificate_file):\n+    with open(key_file, 'r') as key, open(certificate_file, 'r') as certificate:\n+        pem = ''.join(key.readlines()) + ''.join(certificate.readlines())\n+\n+    return PrivateCertificate.loadPEM(pem)\n+\n+\n+class Https2ClientProtocolTestCase(TestCase):\n     scheme = 'https'\n+    key_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.key')\n+    certificate_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.crt')\n \n+    def _init_resource(self):\n+        self.temp_directory = self.mktemp()\n+        os.mkdir(self.temp_directory)\n+        r = File(self.temp_directory)\n+        r.putChild(b'get-data-html-small', GetDataHtmlSmall())\n+        r.putChild(b'get-data-html-large', GetDataHtmlLarge())\n+\n+        r.putChild(b'post-data-json-small', PostDataJsonSmall())\n+        r.putChild(b'post-data-json-large', PostDataJsonLarge())\n+        return r\n+\n+    @inlineCallbacks\n     def setUp(self):\n+        # Initialize resource tree\n+        root = self._init_resource()\n+        self.site = Site(root, timeout=None)\n+\n         # Start server for testing\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n-\n+        self.hostname = u'localhost'\n         if self.scheme == 'https':\n-            self.url = urlparse(self.mockserver.https_address)\n+            context_factory = ssl_context_factory(self.key_file, self.certificate_file)\n+            server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)\n         else:\n-            self.url = urlparse(self.mockserver.http_address)\n+            server_endpoint = TCP4ServerEndpoint(reactor, 0, interface=self.hostname)\n+        self.server = yield server_endpoint.listen(self.site)\n+        self.port_number = self.server.getHost().port\n \n-        self.protocol = H2ClientProtocol()\n-\n-        # Connect to the server using the custom HTTP2ClientProtocol\n-        options = CertificateOptions(acceptableProtocols=[b'h2'])\n-        endpoint = SSL4ClientEndpoint(reactor, self.url.hostname, self.url.port, options)\n-        connectProtocol(endpoint, self.protocol)\n-\n-    def getURL(self, path):\n-        return \"{}://{}:{}/{}\".format(self.url.scheme, self.url.hostname, self.url.port, path)\n+        # Connect H2 client with server\n+        client_certificate = get_client_certificate(self.key_file, self.certificate_file)\n+        client_options = optionsForClientTLS(\n+            hostname=self.hostname,\n+            trustRoot=client_certificate,\n+            acceptableProtocols=[b'h2']\n+        )\n+        h2_client_factory = Factory.forProtocol(H2ClientProtocol)\n+        client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)\n+        self.client = yield client_endpoint.connect(h2_client_factory)\n \n+    @inlineCallbacks\n     def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+        yield self.client.transport.loseConnection()\n+        yield self.client.transport.abortConnection()\n+        yield self.server.stopListening()\n+        shutil.rmtree(self.temp_directory)\n \n-    def test_download(self):\n-        request = Request(self.getURL(''))\n+    def get_url(self, path):\n+        \"\"\"\n+        :param path: Should have / at the starting compulsorily if not empty\n+        :return: Complete url\n+        \"\"\"\n+        assert len(path) > 0 and (path[0] == '/' or path[0] == '&')\n+        return \"{}://{}:{}{}\".format(self.scheme, self.hostname, self.port_number, path)\n \n-        def assert_response(response: Response):\n-            self.assertEqual(response.body, b'Scrapy mock HTTP server\\n')\n-            self.assertEqual(response.status, 200)\n+    @staticmethod\n+    def _check_repeat(get_deferred, count):\n+        d_list = []\n+        for _ in range(count):\n+            d = get_deferred()\n+            d_list.append(d)\n+\n+        return DeferredList(d_list, fireOnOneErrback=True)\n+\n+    def _check_GET(\n+            self,\n+            request: Request,\n+            expected_body,\n+            expected_status\n+    ):\n+        def check_response(response: Response):\n+            self.assertEqual(response.status, expected_status)\n+            self.assertEqual(response.body, expected_body)\n             self.assertEqual(response.request, request)\n             self.assertEqual(response.url, request.url)\n \n-        d = self.protocol.request(request)\n+            content_length = int(response.headers.get('Content-Length'))\n+            self.assertEqual(len(response.body), content_length)\n+\n+        d = self.client.request(request)\n+        d.addCallback(check_response)\n+        d.addErrback(self.fail)\n+        return d\n+\n+    def test_GET_small_body(self):\n+        request = Request(self.get_url('/get-data-html-small'))\n+        return self._check_GET(request, Data.HTML_SMALL, 200)\n+\n+    def test_GET_large_body(self):\n+        request = Request(self.get_url('/get-data-html-large'))\n+        return self._check_GET(request, Data.HTML_LARGE, 200)\n+\n+    def _check_GET_x20(self, *args, **kwargs):\n+        def get_deferred():\n+            return self._check_GET(*args, **kwargs)\n+\n+        return self._check_repeat(get_deferred, 20)\n+\n+    def test_GET_small_body_x20(self):\n+        return self._check_GET_x20(\n+            Request(self.get_url('/get-data-html-small')),\n+            Data.HTML_SMALL,\n+            200\n+        )\n+\n+    def test_GET_large_body_x20(self):\n+        return self._check_GET_x20(\n+            Request(self.get_url('/get-data-html-large')),\n+            Data.HTML_LARGE,\n+            200\n+        )\n+\n+    def _check_POST_json(\n+            self,\n+            request: Request,\n+            expected_request_body,\n+            expected_extra_data,\n+            expected_status: int\n+    ):\n+        d = self.client.request(request)\n+\n+        def assert_response(response: Response):\n+            self.assertEqual(response.status, expected_status)\n+            self.assertEqual(response.request, request)\n+            self.assertEqual(response.url, request.url)\n+\n+            content_length = int(response.headers.get('Content-Length'))\n+            self.assertEqual(len(response.body), content_length)\n+\n+            # Parse the body\n+            body = json.loads(response.body.decode('utf-8'))\n+            self.assertIn('request-body', body)\n+            self.assertIn('extra-data', body)\n+            self.assertIn('request-headers', body)\n+\n+            request_body = body['request-body']\n+            self.assertEqual(request_body, expected_request_body)\n+\n+            extra_data = body['extra-data']\n+            self.assertEqual(extra_data, expected_extra_data)\n+\n+            # Check if headers were sent successfully\n+            request_headers = body['request-headers']\n+            for k, v in request.headers.items():\n+                k_str = k.decode('utf-8')\n+                self.assertIn(k_str, request_headers)\n+                self.assertEqual(request_headers[k_str], v[0].decode('utf-8'))\n+\n         d.addCallback(assert_response)\n         return d\n+\n+    def test_POST_small_json(self):\n+        request = JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL)\n+        return self._check_POST_json(\n+            request,\n+            Data.JSON_SMALL,\n+            Data.EXTRA_SMALL,\n+            200\n+        )\n+\n+    def test_POST_large_json(self):\n+        request = JsonRequest(url=self.get_url('/post-data-json-large'), method='POST', data=Data.JSON_LARGE)\n+        return self._check_POST_json(\n+            request,\n+            Data.JSON_LARGE,\n+            Data.EXTRA_LARGE,\n+            200\n+        )\n+\n+    def _check_POST_json_x20(self, *args, **kwargs):\n+        def get_deferred():\n+            return self._check_POST_json(*args, **kwargs)\n+\n+        return self._check_repeat(get_deferred, 20)\n+\n+    def test_POST_small_json_x20(self):\n+        request = JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL)\n+        return self._check_POST_json_x20(\n+            request,\n+            Data.JSON_SMALL,\n+            Data.EXTRA_SMALL,\n+            200\n+        )\n+\n+    def test_POST_large_json_x20(self):\n+        request = JsonRequest(url=self.get_url('/post-data-json-large'), method='POST', data=Data.JSON_LARGE)\n+        return self._check_POST_json_x20(\n+            request,\n+            Data.JSON_LARGE,\n+            Data.EXTRA_LARGE,\n+            200\n+        )\n+\n+    def test_cancel_request(self):\n+        request = Request(url=self.get_url('/get-data-html-large'))\n+\n+        def assert_response(response: Response):\n+            self.assertEqual(response.status, 499)\n+            self.assertEqual(response.request, request)\n+            self.assertEqual(response.url, request.url)\n+\n+        d = self.client.request(request)\n+        d.addCallback(assert_response)\n+        d.cancel()\n+\n+        return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
