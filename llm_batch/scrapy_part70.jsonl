{"custom_id": "scrapy#e0ccbed40dc7826cc7e8838ba1f98a781bae2082", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 721 | Contributors (this commit): 4 | Commits (past 90d): 41 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -161,11 +161,18 @@ class ExecutionEngine(object):\n         if self.paused:\n             return reactor.callLater(5, self.next_request, domain)\n \n-        if not self.running or self.domain_is_closed(domain) or \\\n-                self.downloader.sites[domain].needs_backout() or  \\\n-                self.scraper.sites[domain].needs_backout():\n-            return\n-        self._next_request(domain)\n+        while not self._needs_backout(domain):\n+            if not self._next_request(domain):\n+                break\n+\n+        if self.domain_is_idle(domain):\n+            self._domain_idle(domain)\n+\n+    def _needs_backout(self, domain):\n+        return not self.running \\\n+            or self.domain_is_closed(domain) \\\n+            or self.downloader.sites[domain].needs_backout() \\\n+            or self.scraper.sites[domain].needs_backout()\n \n     def _next_request(self, domain):\n         # Next pending request from scheduler\n@@ -174,9 +181,7 @@ class ExecutionEngine(object):\n             spider = spiders.fromdomain(domain)\n             dwld = mustbe_deferred(self.download, request, spider)\n             dwld.chainDeferred(deferred).addBoth(lambda _: deferred)\n-            dwld.addErrback(log.err)\n-        elif self.domain_is_idle(domain):\n-            self._domain_idle(domain)\n+            return dwld.addErrback(log.err)\n \n     def domain_is_idle(self, domain):\n         scraper_idle = domain in self.scraper.sites and self.scraper.sites[domain].is_idle()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2463842acbdcc66c116143bc8b08f477ce9bacbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 34 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,12 @@ See documentation in docs/ref/spider-middleware.rst\n class HttpErrorMiddleware(object):\n \n     def process_spider_input(self, response, spider):\n-        if not (200 <= response.status < 300 or \\\n-                response.status in getattr(spider, 'handle_httpstatus_list', [])):\n+        if 200 <= response.status < 300: # common case\n+            return\n+        if 'handle_httpstatus_list' in response.request.meta:\n+            allowed_statuses = response.request.meta['handle_httpstatus_list']\n+        else:\n+            allowed_statuses = getattr(spider, 'handle_httpstatus_list', ())\n+        if response.status in allowed_statuses:\n+            return\n         return []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08194186b4604f9bc9fc0b73bc1aa158a7fe8059", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 90 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,6 +58,14 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(safe_url_string(\"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\"),\n                                          \"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\")\n \n+        safeurl = safe_url_string(u\"http://www.example.com/\\xa3\", encoding='latin-1')\n+        self.assert_(isinstance(safeurl, str))\n+        self.assertEqual(safeurl, \"http://www.example.com/%A3\")\n+\n+        safeurl = safe_url_string(u\"http://www.example.com/\\xa3\", encoding='utf-8')\n+        self.assert_(isinstance(safeurl, str))\n+        self.assertEqual(safeurl, \"http://www.example.com/%C2%A3\")\n+\n     def test_safe_download_url(self):\n         self.assertEqual(safe_download_url('http://www.scrapy.org/../'),\n                          'http://www.scrapy.org/')\n\n@@ -45,19 +45,21 @@ _reserved = ';/?:@&=+$|,#' # RFC 2396 (Generic Syntax)\n _unreserved_marks = \"-_.!~*'()\" # RFC 2396 sec 2.3\n _safe_chars = urllib.always_safe + '%' + _reserved + _unreserved_marks\n \n-def safe_url_string(url, use_encoding='utf8'):\n-    \"\"\"Convert a unicode object (using 'use_encoding' as the encoding), or an already\n-    encoded string into a legal URL.\n+def safe_url_string(url, encoding='utf8'):\n+    \"\"\"Convert the given url into a legal URL by escaping unsafe characters\n+    according to RFC-2396.\n \n-    Illegal characters are escaped (RFC-3986)\n+    If a unicode url is given, it is first converted to str using the given\n+    encoding (which defaults to 'utf-8'). When passing a encoding, you should\n+    use the encoding of the original page (the page from which the url was\n+    extracted from).\n \n-    It is safe to call this function multiple times.\n+    Calling this function on an already \"safe\" url will return the url\n+    unmodified.\n \n-    The use_encoding argument is the encoding to use to determine the numerical\n-    values in the escaping. For urls on html pages, you should use the original\n-    encoding of that page.\n+    Always returns a str.\n     \"\"\"\n-    s = unicode_to_str(url, use_encoding)\n+    s = unicode_to_str(url, encoding)\n     return urllib.quote(s,  _safe_chars)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc87350133201f972b0d64cbcc8de16474b5f9ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 71 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -12,7 +12,7 @@ import cgi\n from scrapy.utils.python import unicode_to_str\n \n def url_is_from_any_domain(url, domains):\n-    \"\"\"Return True if the url belongs to the given domain\"\"\"\n+    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n     host = urlparse.urlparse(url).hostname\n \n     if host:\n@@ -33,13 +33,8 @@ def urljoin_rfc(base, ref, encoding='utf-8'):\n \n     Always returns a str.\n     \"\"\"\n-    # XXX: this code was commented out because its purpose is unknown and\n-    # there's no test or documentation that specifies its behaviour. please\n-    # don't restore this code without adding unittests for it\n-    #if ref.startswith('?'):\n-    #    fpart = urlparse.urlsplit(unicode_to_str(base, encoding))[2].rsplit('/', 1)[-1]\n-    #    ref = ''.join([fpart, ref])\n-    return urlparse.urljoin(unicode_to_str(base, encoding), unicode_to_str(ref, encoding))\n+    return urlparse.urljoin(unicode_to_str(base, encoding), \\\n+        unicode_to_str(ref, encoding))\n \n _reserved = ';/?:@&=+$|,#' # RFC 2396 (Generic Syntax)\n _unreserved_marks = \"-_.!~*'()\" # RFC 2396 sec 2.3\n@@ -88,7 +83,8 @@ def is_url(text):\n \n def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n     \"\"\"Return the value of a url parameter, given the url and parameter name\"\"\"\n-    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)\n+    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], \\\n+        keep_blank_values=keep_blank_values)\n     return queryparams.get(parameter, [default])[0]\n \n def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n@@ -111,7 +107,8 @@ def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n             querylist += [pair]\n             unique[k] = 1\n \n-    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in parameterlist])\n+    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in \\\n+        parameterlist])\n     return '?'.join([base, query])\n \n def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1980a3d9f412304201bd443816eafd38f735b71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 77 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -36,13 +36,13 @@ def urljoin_rfc(base, ref, encoding='utf-8'):\n     return urlparse.urljoin(unicode_to_str(base, encoding), \\\n         unicode_to_str(ref, encoding))\n \n-_reserved = ';/?:@&=+$|,#' # RFC 2396 (Generic Syntax)\n-_unreserved_marks = \"-_.!~*'()\" # RFC 2396 sec 2.3\n+_reserved = ';/?:@&=+$|,#' # RFC 3986 (Generic Syntax)\n+_unreserved_marks = \"-_.!~*'()\" # RFC 3986 sec 2.3\n _safe_chars = urllib.always_safe + '%' + _reserved + _unreserved_marks\n \n def safe_url_string(url, encoding='utf8'):\n     \"\"\"Convert the given url into a legal URL by escaping unsafe characters\n-    according to RFC-2396.\n+    according to RFC-3986.\n \n     If a unicode url is given, it is first converted to str using the given\n     encoding (which defaults to 'utf-8'). When passing a encoding, you should\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45ed662ee5b7e2189cbd70b55aa2df3cf251182d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 53 | Files Changed: 4 | Hunks: 17 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/4 | Churn Δ: 117 | Churn Cumulative: 162 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ MemoryDebugger extension\n See documentation in docs/ref/extensions.rst\n \"\"\"\n \n-import pprint\n import gc\n import socket\n \n@@ -14,8 +13,9 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.extension import extensions\n+from scrapy.stats import stats\n from scrapy.conf import settings\n+from scrapy.utils.memory import get_vmvalue_from_procfs\n from scrapy import log\n \n class MemoryDebugger(object):\n@@ -43,12 +43,13 @@ class MemoryDebugger(object):\n         gc.collect()\n \n         figures = []\n-        if 'MemoryUsage' in extensions.enabled:\n-            memusage = extensions.enabled['MemoryUsage']\n-            memusage.update()\n-            figures.append((\"Memory usage at startup\", int(memusage.data['startup']/1024/1024), \"Mb\"))\n-            figures.append((\"Maximum memory usage\", int(memusage.data['max']/1024/1024), \"Mb\"))\n-            figures.append((\"Memory usage at shutdown\", int(memusage.virtual/1024/1024), \"Mb\"))\n+        if stats.get_value('memusage/startup'):\n+            figures.append((\"Memory usage at startup\", \\\n+                stats.get_value('memusage/startup')/1024/1024, \"Mb\"))\n+            figures.append((\"Maximum memory usage\", \\\n+                stats.get_value('memusage/max')/1024/1024, \"Mb\"))\n+            figures.append((\"Memory usage at shutdown\", \\\n+                get_vmvalue_from_procfs()/1024/1024, \"Mb\"))\n         figures.append((\"Objects in gc.garbage\", len(gc.garbage), \"\"))\n         figures.append((\"libxml2 memory leak\", libxml2.debugMemory(1), \"bytes\"))\n         return figures\n@@ -57,10 +58,11 @@ class MemoryDebugger(object):\n         s = \"\"\n         s += \"SCRAPY MEMORY DEBUGGER RESULTS\\n\\n\"\n         for f in figures:\n-            s += \"%-30s : %s %s\\n\" % f\n+            s += \"%-30s : %d %s\\n\" % f\n         return s\n \n     def log_or_send_report(self, report):\n         if self.rcpts:\n-            self.mail.send(self.rcpts, \"Scrapy Memory Debugger results at %s\" % socket.gethostname(), report)\n+            self.mail.send(self.rcpts, \"Scrapy Memory Debugger results at %s\" % \\\n+                socket.gethostname(), report)\n         log.msg(report)\n\n@@ -4,7 +4,6 @@ MemoryUsage extension\n See documentation in docs/ref/extensions.rst\n \"\"\"\n \n-import sys\n import os\n import socket\n \n@@ -18,25 +17,18 @@ from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n from scrapy.stats import stats\n+from scrapy.utils.memory import get_vmvalue_from_procfs\n \n class MemoryUsage(object):\n     \n-    _proc_status = '/proc/%d/status' % os.getpid()\n-    _scale = {'kB': 1024.0, 'mB': 1024.0*1024.0,\n-              'KB': 1024.0, 'MB': 1024.0*1024.0}\n-\n     def __init__(self):\n         if not settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n-        if sys.platform != 'linux2':\n-            raise NotConfigured(\"MemoryUsage extension is only available on Linux\")\n+        if not os.path.exists('/proc'):\n+            raise NotConfigured\n \n         self.warned = False\n \n-        self.data = {}\n-        self.data['startup'] = 0\n-        self.data['max'] = 0\n-\n         scrapyengine.addtask(self.update, 60.0, now=True)\n \n         self.notify_mails = settings.getlist('MEMUSAGE_NOTIFY')\n@@ -56,40 +48,13 @@ class MemoryUsage(object):\n \n     @property\n     def virtual(self):\n-        return self._vmvalue('VmSize:')\n-\n-    @property\n-    def resident(self):\n-        return self._vmvalue('VmRSS:')\n-        \n-    @property\n-    def stacksize(self):\n-        return self._vmvalue('VmStk:')\n+        return get_vmvalue_from_procfs('VmSize')\n \n     def engine_started(self):\n-        self.data['startup'] = self.virtual\n-        stats.set_value('memusage/startup', int(self.virtual))\n+        stats.set_value('memusage/startup', self.virtual)\n \n     def update(self):\n-        if self.virtual > self.data['max']:\n-            self.data['max'] = self.virtual\n-            stats.set_value('memusage/max', int(self.virtual))\n-\n-    def _vmvalue(self, VmKey):\n-        # get pseudo file  /proc/<pid>/status\n-        try:\n-            t = open(self._proc_status)\n-            v = t.read()\n-            t.close()\n-        except:\n-            return 0.0  # non-Linux?\n-        # get VmKey line e.g. 'VmRSS:  9999  kB\\n ...'\n-        i = v.index(VmKey)\n-        v = v[i:].split(None, 3)  # whitespace\n-        if len(v) < 3:\n-            return 0.0  # invalid format?\n-        # convert Vm value to bytes\n-        return float(v[1]) * self._scale[v[2]]\n+        stats.max_value('memusage/max', self.virtual)\n \n     def _check_limit(self):\n         if self.virtual > self.limit:\n@@ -119,8 +84,8 @@ class MemoryUsage(object):\n \n     def _send_report(self, rcpts, subject):\n         \"\"\"send notification mail with some additional useful info\"\"\"\n-        s = \"Memory usage at engine startup : %dM\\r\\n\" % (self.data['startup']/1024/1024)\n-        s += \"Maximum memory usage           : %dM\\r\\n\" % (self.data['max']/1024/1024)\n+        s = \"Memory usage at engine startup : %dM\\r\\n\" % (stats.get_value('memusage/startup')/1024/1024)\n+        s += \"Maximum memory usage           : %dM\\r\\n\" % (stats.get_value('memusage/max')/1024/1024)\n         s += \"Current memory usage           : %dM\\r\\n\" % (self.virtual/1024/1024)\n \n         s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n\n@@ -0,0 +1,19 @@\n+import os\n+\n+from twisted.trial import unittest\n+\n+from scrapy.utils.memory import get_vmvalue_from_procfs\n+\n+class UtilsMemoryTestCase(unittest.TestCase):\n+\n+    def test_get_vmvalue_from_procfs(self):\n+        if not os.path.exists('/proc'):\n+            raise unittest.SkipTest('/proc filesystem not supported')\n+        vmsize = get_vmvalue_from_procfs('VmSize')\n+        vmrss = get_vmvalue_from_procfs('VmRSS')\n+        assert vmsize > 0\n+        assert vmrss > 0\n+        assert vmsize > vmrss\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -0,0 +1,25 @@\n+import os\n+\n+_vmvalue_scale = {'kB': 1024, 'mB': 1024*1024, 'KB': 1024, 'MB': 1024*1024}\n+\n+def get_vmvalue_from_procfs(vmkey='VmSize', pid=None):\n+    \"\"\"Return virtual memory value (in bytes) for the given pid using the /proc\n+    filesystem. If pid is not given, it default to the current process pid.\n+    Available keys are: VmSize, VmRSS (default), VmStk\n+    \"\"\"\n+    if pid is None:\n+        pid = os.getpid()\n+    try:\n+        t = open('/proc/%d/status' % pid)\n+    except IOError:\n+        raise RuntimeError(\"/proc filesystem not supported\")\n+    v = t.read()\n+    t.close()\n+    # get vmkey line e.g. 'VmRSS:  9999  kB\\n ...'\n+    i = v.index(vmkey + ':')\n+    v = v[i:].split(None, 3)  # whitespace\n+    if len(v) < 3:\n+        return 0  # invalid format?\n+    # convert Vm value to bytes\n+    return float(v[1]) * _vmvalue_scale[v[2]]\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b20837a02820c070f9baea324d3717884b69f371", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 14 | Files Changed: 4 | Hunks: 15 | Methods Changed: 15 | Complexity Δ (Sum/Max): 34/23 | Churn Δ: 74 | Churn Cumulative: 116 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,8 @@ from scrapy.utils.http import headers_dict_to_raw\n class Headers(CaselessDict):\n     \"\"\"Case insensitive http headers dictionary\"\"\"\n \n+    __slots__ = ['encoding']\n+\n     def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n         super(Headers, self).__init__(seq)\n\n@@ -15,14 +15,18 @@ from scrapy.utils.url import safe_url_string\n \n class Request(object):\n \n+    __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', '_cache', \\\n+        'dont_filter', 'headers', 'cookies', 'deferred', 'priority', \\\n+        '__weakref__']\n+\n     def __init__(self, url, callback=None, method='GET', headers=None, body=None, \n                  cookies=None, meta=None, encoding='utf-8', priority=0.0,\n                  dont_filter=False, errback=None):\n \n         self._encoding = encoding  # this one has to be set first\n         self.method = method.upper()\n-        self.set_url(url)\n-        self.set_body(body)\n+        self._set_url(url)\n+        self._set_body(body)\n         self.priority = priority\n \n         if callable(callback):\n@@ -33,10 +37,25 @@ class Request(object):\n         self.headers = Headers(headers or {}, encoding=encoding)\n         self.dont_filter = dont_filter\n \n-        self.meta = {} if meta is None else dict(meta)\n-        self.cache = {}\n+        self._meta = dict(meta) if meta else None\n+        self._cache = None\n \n-    def set_url(self, url):\n+    @property\n+    def meta(self):\n+        if self._meta is None:\n+            self._meta = {}\n+        return self._meta\n+\n+    @property\n+    def cache(self):\n+        if self._cache is None:\n+            self._cache = {}\n+        return self._cache\n+\n+    def _get_url(self):\n+        return self._url\n+\n+    def _set_url(self, url):\n         if isinstance(url, basestring):\n             decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n             self._url = Url(safe_url_string(decoded_url, self.encoding))\n@@ -44,9 +63,13 @@ class Request(object):\n             self._url = url\n         else:\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n-    url = property(lambda x: x._url, set_url)\n \n-    def set_body(self, body):\n+    url = property(_get_url, _set_url)\n+\n+    def _get_body(self):\n+        return self._body\n+\n+    def _set_body(self, body):\n         if isinstance(body, str):\n             self._body = body\n         elif isinstance(body, unicode):\n@@ -55,7 +78,8 @@ class Request(object):\n             self._body = ''\n         else:\n             raise TypeError(\"Request body must either str or unicode. Got: '%s'\" % type(body).__name__)\n-    body = property(lambda x: x._body, set_body)\n+\n+    body = property(_get_body, _set_body)\n \n     @property\n     def encoding(self):\n\n@@ -12,18 +12,35 @@ from scrapy.http.headers import Headers\n \n class Response(object):\n \n+    __slots__ = ['url', 'headers', 'status', '_body', 'request', '_meta', \\\n+        'flags', '_cache', '__weakref__']\n+\n     def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):\n         self.url = Url(url)\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n-        self.set_body(body)\n-        self.cached = False\n+        self._set_body(body)\n         self.request = None\n-        self.meta = {} if meta is None else dict(meta)\n         self.flags = [] if flags is None else list(flags)\n-        self.cache = {}\n+        self._meta = dict(meta) if meta else None\n+        self._cache = None\n \n-    def set_body(self, body):\n+    @property\n+    def meta(self):\n+        if self._meta is None:\n+            self._meta = {}\n+        return self._meta\n+\n+    @property\n+    def cache(self):\n+        if self._cache is None:\n+            self._cache = {}\n+        return self._cache\n+\n+    def _get_body(self):\n+        return self._body\n+\n+    def _set_body(self, body):\n         if isinstance(body, str):\n             self._body = body\n         elif isinstance(body, unicode):\n@@ -32,7 +49,8 @@ class Response(object):\n             self._body = ''\n         else:\n             raise TypeError(\"Response body must either str or unicode. Got: '%s'\" % type(body).__name__)\n-    body = property(lambda x: x._body, set_body)\n+\n+    body = property(_get_body, _set_body)\n \n     def __repr__(self):\n         return \"%s(url=%s, headers=%s, status=%s, body=%s)\" % \\\n\n@@ -16,6 +16,8 @@ class TextResponse(Response):\n \n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n+    __slots__ = ['_encoding']\n+\n     def __init__(self, url, status=200, headers=None, body=None, meta=None, flags=None, encoding=None):\n         self._encoding = encoding\n         if isinstance(body, unicode):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db8b29500ffdd7b66a225058b2d07f6c39784ce7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 50 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,9 +13,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.stats import stats\n from scrapy.conf import settings\n-from scrapy.utils.memory import get_vmvalue_from_procfs\n from scrapy import log\n \n class MemoryDebugger(object):\n@@ -43,13 +41,6 @@ class MemoryDebugger(object):\n         gc.collect()\n \n         figures = []\n-        if stats.get_value('memusage/startup'):\n-            figures.append((\"Memory usage at startup\", \\\n-                stats.get_value('memusage/startup')/1024/1024, \"Mb\"))\n-            figures.append((\"Maximum memory usage\", \\\n-                stats.get_value('memusage/max')/1024/1024, \"Mb\"))\n-            figures.append((\"Memory usage at shutdown\", \\\n-                get_vmvalue_from_procfs()/1024/1024, \"Mb\"))\n         figures.append((\"Objects in gc.garbage\", len(gc.garbage), \"\"))\n         figures.append((\"libxml2 memory leak\", libxml2.debugMemory(1), \"bytes\"))\n         return figures\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62c29e4d69666f6921e19c318d66a1117973e705", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 213 | Files Changed: 1 | Hunks: 1 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 213 | Churn Cumulative: 213 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,213 +0,0 @@\n-# lrucache.py -- a simple LRU (Least-Recently-Used) cache class\n-\n-# Copyright 2004 Evan Prodromou <evan@bad.dynu.ca>\n-# Licensed under the Academic Free License 2.1\n-\n-# arch-tag: LRU cache main module\n-\n-\"\"\"a simple LRU (Least-Recently-Used) cache module\n-\n-This module provides very simple LRU (Least-Recently-Used) cache\n-functionality.\n-\n-An *in-memory cache* is useful for storing the results of an\n-'expensive' process (one that takes a lot of time or resources) for\n-later re-use. Typical examples are accessing data from the filesystem,\n-a database, or a network location. If you know you'll need to re-read\n-the data again, it can help to keep it in a cache.\n-\n-You *can* use a Python dictionary as a cache for some purposes.\n-However, if the results you're caching are large, or you have a lot of\n-possible results, this can be impractical memory-wise.\n-\n-An *LRU cache*, on the other hand, only keeps _some_ of the results in\n-memory, which keeps you from overusing resources. The cache is bounded\n-by a maximum size; if you try to add more values to the cache, it will\n-automatically discard the values that you haven't read or written to\n-in the longest time. In other words, the least-recently-used items are\n-discarded. [1]_\n-\n-.. [1]: 'Discarded' here means 'removed from the cache'.\n-\n-\"\"\"\n-\n-from __future__ import generators\n-import time\n-from heapq import heappush, heappop, heapify\n-\n-__version__ = \"0.2\"\n-__all__ = ['CacheKeyError', 'LRUCache', 'DEFAULT_SIZE']\n-__docformat__ = 'reStructuredText en'\n-\n-DEFAULT_SIZE = 16\n-\"\"\"Default size of a new LRUCache object, if no 'size' argument is given.\"\"\"\n-\n-class CacheKeyError(KeyError):\n-    \"\"\"Error raised when cache requests fail\n-    \n-    When a cache record is accessed which no longer exists (or never did),\n-    this error is raised. To avoid it, you may want to check for the existence\n-    of a cache record before reading or deleting it.\"\"\"\n-    pass\n-\n-class LRUCache(object):\n-    \"\"\"Least-Recently-Used (LRU) cache.\n-    \n-    Instances of this class provide a least-recently-used (LRU) cache. They\n-    emulate a Python mapping type. You can use an LRU cache more or less like\n-    a Python dictionary, with the exception that objects you put into the\n-    cache may be discarded before you take them out.\n-    \n-    Some example usage::\n-\t\n-    cache = LRUCache(32) # new cache\n-    cache['foo'] = get_file_contents('foo') # or whatever\n-\t\n-    if 'foo' in cache: # if it's still in cache...\n-\t    # use cached version\n-        contents = cache['foo']\n-    else:\n-\t    # recalculate\n-        contents = get_file_contents('foo')\n-\t    # store in cache for next time\n-        cache['foo'] = contents\n-\n-    print cache.size # Maximum size\n-\t\n-    print len(cache) # 0 <= len(cache) <= cache.size\n-\t\n-    cache.size = 10 # Auto-shrink on size assignment\n-\t\n-    for i in range(50): # note: larger than cache size\n-        cache[i] = i\n-\t    \n-    if 0 not in cache: print 'Zero was discarded.'\n-\n-    if 42 in cache:\n-        del cache[42] # Manual deletion\n-\n-    for j in cache:   # iterate (in LRU order)\n-        print j, cache[j] # iterator produces keys, not values\n-    \"\"\"\n-    \n-    class __Node(object):\n-        \"\"\"Record of a cached value. Not for public consumption.\"\"\"\n-        \n-        def __init__(self, key, obj, timestamp):\n-            object.__init__(self)\n-            self.key = key\n-            self.obj = obj\n-            self.atime = timestamp\n-            self.mtime = self.atime\n-\t    \n-        def __cmp__(self, other):\n-            return cmp(self.atime, other.atime)\n-\n-        def __repr__(self):\n-            return \"<%s %s => %s (%s)>\" % \\\n-                   (self.__class__, self.key, self.obj, \\\n-                    time.asctime(time.localtime(self.atime)))\n-\n-    def __init__(self, size=DEFAULT_SIZE):\n-        # Check arguments\n-        if size <= 0:\n-            raise ValueError, size\n-        elif type(size) is not type(0):\n-            raise TypeError, size\n-        object.__init__(self)\t\n-        self.__heap = []\n-        self.__dict = {}\n-        self.size = size\n-        \"\"\"Maximum size of the cache.\n-        If more than 'size' elements are added to the cache,\n-        the least-recently-used ones will be discarded.\"\"\"\n-\t\n-    def __len__(self):\n-        return len(self.__heap)\n-    \n-    def __contains__(self, key):\n-        return self.__dict.has_key(key)\n-    \n-    def __setitem__(self, key, obj):\n-        if self.__dict.has_key(key):\n-            node = self.__dict[key]\n-            node.obj = obj\n-            node.atime = time.time()\n-            node.mtime = node.atime\n-            heapify(self.__heap)\n-        else:\n-            # size may have been reset, so we loop\n-            while len(self.__heap) >= self.size:\n-                lru = heappop(self.__heap)\n-                del self.__dict[lru.key]\n-            node = self.__Node(key, obj, time.time())\n-            self.__dict[key] = node\n-            heappush(self.__heap, node)\n-\t\n-    def __getitem__(self, key):\n-        if not self.__dict.has_key(key):\n-            raise CacheKeyError(key)\n-        else:\n-            node = self.__dict[key]\n-            node.atime = time.time()\n-            heapify(self.__heap)\n-            return node.obj\n-\t\n-    def __delitem__(self, key):\n-        if not self.__dict.has_key(key):\n-            raise CacheKeyError(key)\n-        else:\n-            node = self.__dict[key]\n-            del self.__dict[key]\n-            self.__heap.remove(node)\n-            heapify(self.__heap)\n-            return node.obj\n-\n-    def __iter__(self):\n-        copy = self.__heap[:]\n-        while len(copy) > 0:\n-            node = heappop(copy)\n-            yield node.key\n-        raise StopIteration\n-\n-    def __setattr__(self, name, value):\n-        object.__setattr__(self, name, value)\n-        # automagically shrink heap on resize\n-        if name == 'size':\n-            while len(self.__heap) > value:\n-                lru = heappop(self.__heap)\n-                del self.__dict[lru.key]\n-\t    \n-    def __repr__(self):\n-        return \"<%s (%d elements)>\" % (str(self.__class__), len(self.__heap))\n-\n-    def mtime(self, key):\n-        \"\"\"Return the last modification time for the cache record with key.\n-        May be useful for cache instances where the stored values can get\n-        'stale', such as caching file or network resource contents.\"\"\"\n-        if not self.__dict.has_key(key):\n-            raise CacheKeyError(key)\n-        else:\n-            node = self.__dict[key]\n-            return node.mtime\n-\n-if __name__ == \"__main__\":\n-    cache = LRUCache(25)\n-    print cache\n-    for i in range(50):\n-        cache[i] = str(i)\n-    print cache\n-    if 46 in cache:\n-        del cache[46]\n-    print cache\n-    cache.size = 10\n-    print cache\n-    cache[46] = '46'\n-    print cache\n-    print len(cache)\n-    for c in cache:\n-        print c\n-    print cache\n-    print cache.mtime(46)\n-    for c in cache:\n-        print c\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1bf56821441042126217df2eebf56cb19e224a51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59e0a83ad41bc1012ca0d9a9a0d7984679503ffe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 167 | Contributors (this commit): 3 | Commits (past 90d): 31 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -15,8 +15,6 @@ Scrapy developers, if you add a setting here remember to:\n \n from os.path import join, abspath, dirname\n \n-ADAPTORS_DEBUG = False\n-\n BOT_NAME = 'scrapybot'\n BOT_VERSION = '1.0'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48b40bd620319c80cd1196058d7e241a1b6f9307", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 62 | Files Changed: 10 | Hunks: 45 | Methods Changed: 28 | Complexity Δ (Sum/Max): 20/13 | Churn Δ: 162 | Churn Cumulative: 571 | Contributors (this commit): 3 | Commits (past 90d): 25 | Contributors (cumulative): 24 | DMM Complexity: 0.92\n\nDIFF:\n@@ -20,14 +20,14 @@ class GoogleDirectorySpider(CrawlSpider):\n         hxs = HtmlXPathSelector(response)\n \n         # The path to website links in directory page\n-        links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n+        links = hxs.select('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n \n         for link in links:\n             item = GoogledirItem()\n \n-            item.name = link.x('a/text()').extract()\n-            item.url = link.x('a/@href').extract()\n-            item.description = link.x('font[2]/text()').extract()\n+            item.name = link.select('a/text()').extract()\n+            item.url = link.select('a/@href').extract()\n+            item.description = link.select('font[2]/text()').extract()\n             yield item\n \n SPIDER = GoogleDirectorySpider()\n\n@@ -35,14 +35,15 @@ class HTMLImageLinkExtractor(object):\n \n         if selector.xmlNode.type == 'element':\n             if selector.xmlNode.name == 'img':\n-                _add_link(selector.x('@src'), selector.x('@alt') or selector.x('@title'))\n+                _add_link(selector.select('@src'), selector.select('@alt') or \\\n+                          selector.select('@title'))\n             else:\n-                children = selector.x('child::*')\n+                children = selector.select('child::*')\n                 if len(children):\n                     for child in children:\n                         ret.extend(self.extract_from_selector(child, parent=selector))\n                 elif selector.xmlNode.name == 'a' and not parent:\n-                    _add_link(selector.x('@href'), selector.x('@title'))\n+                    _add_link(selector.select('@href'), selector.select('@title'))\n         else:\n             _add_link(selector)\n \n@@ -50,13 +51,13 @@ class HTMLImageLinkExtractor(object):\n \n     def extract_links(self, response):\n         xs = HtmlXPathSelector(response)\n-        base_url = xs.x('//base/@href').extract()\n+        base_url = xs.select('//base/@href').extract()\n         base_url = unicode_to_str(base_url[0]) if base_url else unicode_to_str(response.url)\n \n         links = []\n         for location in self.locations:\n             if isinstance(location, basestring):\n-                selectors = xs.x(location)\n+                selectors = xs.select(location)\n             elif isinstance(location, (XPathSelectorList, HtmlXPathSelector)):\n                 selectors = [location] if isinstance(location, HtmlXPathSelector) else location\n             else:\n\n@@ -93,7 +93,8 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n     def extract_links(self, response):\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n-            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.x(xpath_expr).extract()) for xpath_expr in self.restrict_xpaths)\n+            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \\\n+                for xpath_expr in self.restrict_xpaths)\n             links = self._extract_links(html_slice, response.url, response.encoding)\n         else:\n             links = BaseSgmlLinkExtractor.extract_links(self, response)\n\n@@ -88,6 +88,6 @@ class XPathItemLoader(ItemLoader):\n         self.replace_value(field_name, self._get_values(field_name, xpath, re))\n \n     def _get_values(self, field_name, xpath, re):\n-        x = self.selector.x(xpath)\n+        x = self.selector.select(xpath)\n         return x.re(re) if re else x.extract()\n \n\n@@ -74,11 +74,11 @@ class XMLFeedSpider(InitSpider):\n         elif self.iterator == 'xml':\n             selector = XmlXPathSelector(response)\n             self._register_namespaces(selector)\n-            nodes = selector.x('//%s' % self.itertag)\n+            nodes = selector.select('//%s' % self.itertag)\n         elif self.iterator == 'html':\n             selector = HtmlXPathSelector(response)\n             self._register_namespaces(selector)\n-            nodes = selector.x('//%s' % self.itertag)\n+            nodes = selector.select('//%s' % self.itertag)\n         else:\n             raise NotSupported('Unsupported node iterator')\n \n\n@@ -26,7 +26,7 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n         response = XmlResponse(url=\"http://example.com\", body=body)\n         attrs = []\n         for x in xmliter(response, 'product'):\n-            attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n+            attrs.append((x.select(\"@id\").extract(), x.select(\"name/text()\").extract(), x.select(\"./type/text()\").extract()))\n \n         self.assertEqual(attrs, \n                          [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n@@ -34,7 +34,7 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n     def test_xmliter_text(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n-        self.assertEqual([x.x(\"text()\").extract() for x in xmliter(body, 'product')],\n+        self.assertEqual([x.select(\"text()\").extract() for x in xmliter(body, 'product')],\n                          [[u'one'], [u'two']])\n \n     def test_xmliter_namespaces(self):\n@@ -61,15 +61,15 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n \n         node = my_iter.next()\n         node.register_namespace('g', 'http://base.google.com/ns/1.0')\n-        self.assertEqual(node.x('title/text()').extract(), ['Item 1'])\n-        self.assertEqual(node.x('description/text()').extract(), ['This is item 1'])\n-        self.assertEqual(node.x('link/text()').extract(), ['http://www.mydummycompany.com/items/1'])\n-        self.assertEqual(node.x('g:image_link/text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n-        self.assertEqual(node.x('g:id/text()').extract(), ['ITEM_1'])\n-        self.assertEqual(node.x('g:price/text()').extract(), ['400'])\n-        self.assertEqual(node.x('image_link/text()').extract(), [])\n-        self.assertEqual(node.x('id/text()').extract(), [])\n-        self.assertEqual(node.x('price/text()').extract(), [])\n+        self.assertEqual(node.select('title/text()').extract(), ['Item 1'])\n+        self.assertEqual(node.select('description/text()').extract(), ['This is item 1'])\n+        self.assertEqual(node.select('link/text()').extract(), ['http://www.mydummycompany.com/items/1'])\n+        self.assertEqual(node.select('g:image_link/text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n+        self.assertEqual(node.select('g:id/text()').extract(), ['ITEM_1'])\n+        self.assertEqual(node.select('g:price/text()').extract(), ['400'])\n+        self.assertEqual(node.select('image_link/text()').extract(), [])\n+        self.assertEqual(node.select('id/text()').extract(), [])\n+        self.assertEqual(node.select('price/text()').extract(), [])\n \n     def test_xmliter_exception(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n\n@@ -17,31 +17,31 @@ class XPathSelectorTestCase(unittest.TestCase):\n         response = TextResponse(url=\"http://example.com\", body=body)\n         xpath = HtmlXPathSelector(response)\n \n-        xl = xpath.x('//input')\n+        xl = xpath.select('//input')\n         self.assertEqual(2, len(xl))\n         for x in xl:\n             assert isinstance(x, HtmlXPathSelector)\n \n-        self.assertEqual(xpath.x('//input').extract(),\n-                         [x.extract() for x in xpath.x('//input')])\n+        self.assertEqual(xpath.select('//input').extract(),\n+                         [x.extract() for x in xpath.select('//input')])\n \n-        self.assertEqual([x.extract() for x in xpath.x(\"//input[@name='a']/@name\")],\n+        self.assertEqual([x.extract() for x in xpath.select(\"//input[@name='a']/@name\")],\n                          [u'a'])\n-        self.assertEqual([x.extract() for x in xpath.x(\"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\")],\n+        self.assertEqual([x.extract() for x in xpath.select(\"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\")],\n                          [u'12.0'])\n \n-        self.assertEqual(xpath.x(\"concat('xpath', 'rules')\").extract(),\n+        self.assertEqual(xpath.select(\"concat('xpath', 'rules')\").extract(),\n                          [u'xpathrules'])\n-        self.assertEqual([x.extract() for x in xpath.x(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n+        self.assertEqual([x.extract() for x in xpath.select(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n                          [u'12'])\n \n     @libxml2debug\n     def test_selector_same_type(self):\n         \"\"\"Test XPathSelector returning the same type in x() method\"\"\"\n         text = '<p>test<p>'\n-        assert isinstance(XmlXPathSelector(text=text).x(\"//p\")[0],\n+        assert isinstance(XmlXPathSelector(text=text).select(\"//p\")[0],\n                           XmlXPathSelector)\n-        assert isinstance(HtmlXPathSelector(text=text).x(\"//p\")[0], \n+        assert isinstance(HtmlXPathSelector(text=text).select(\"//p\")[0], \n                           HtmlXPathSelector)\n \n     @libxml2debug\n@@ -51,10 +51,10 @@ class XPathSelectorTestCase(unittest.TestCase):\n         # some text which is parsed differently by XML and HTML flavors\n         text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n \n-        self.assertEqual(XmlXPathSelector(text=text).x(\"//div\").extract(),\n+        self.assertEqual(XmlXPathSelector(text=text).select(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n \n-        self.assertEqual(HtmlXPathSelector(text=text).x(\"//div\").extract(),\n+        self.assertEqual(HtmlXPathSelector(text=text).select(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n \n     @libxml2debug\n@@ -76,14 +76,14 @@ class XPathSelectorTestCase(unittest.TestCase):\n         response = HtmlResponse(url=\"http://example.com\", body=body)\n         x = HtmlXPathSelector(response)\n \n-        divtwo = x.x('//div[@class=\"two\"]')\n-        self.assertEqual(divtwo.x(\"//li\").extract(),\n+        divtwo = x.select('//div[@class=\"two\"]')\n+        self.assertEqual(divtwo.select(\"//li\").extract(),\n                          [\"<li>one</li>\", \"<li>two</li>\", \"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.x(\"./ul/li\").extract(),\n+        self.assertEqual(divtwo.select(\"./ul/li\").extract(),\n                          [\"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.x(\".//li\").extract(),\n+        self.assertEqual(divtwo.select(\".//li\").extract(),\n                          [\"<li>four</li>\", \"<li>five</li>\", \"<li>six</li>\"])\n-        self.assertEqual(divtwo.x(\"./li\").extract(),\n+        self.assertEqual(divtwo.select(\"./li\").extract(),\n                          [])\n \n     @libxml2debug\n@@ -103,9 +103,9 @@ class XPathSelectorTestCase(unittest.TestCase):\n         x = HtmlXPathSelector(response)\n \n         name_re = re.compile(\"Name: (\\w+)\")\n-        self.assertEqual(x.x(\"//ul/li\").re(name_re),\n+        self.assertEqual(x.select(\"//ul/li\").re(name_re),\n                          [\"John\", \"Paul\"])\n-        self.assertEqual(x.x(\"//ul/li\").re(\"Age: (\\d+)\"),\n+        self.assertEqual(x.select(\"//ul/li\").re(\"Age: (\\d+)\"),\n                          [\"10\", \"20\"])\n \n     @libxml2debug\n@@ -119,7 +119,7 @@ class XPathSelectorTestCase(unittest.TestCase):\n                          u'<root>lala</root>')\n \n         xxs = XmlXPathSelector(text='<root>lala</root>')\n-        self.assertEqual(xxs.x('.').extract(),\n+        self.assertEqual(xxs.select('.').extract(),\n                          [u'<root>lala</root>'])\n \n \n@@ -136,7 +136,7 @@ class XPathSelectorTestCase(unittest.TestCase):\n         x = XmlXPathSelector(response)\n         \n         x.register_namespace(\"somens\", \"http://scrapy.org\")\n-        self.assertEqual(x.x(\"//somens:a\").extract(), \n+        self.assertEqual(x.select(\"//somens:a\").extract(), \n                          ['<somens:a id=\"foo\"/>'])\n \n \n@@ -157,12 +157,12 @@ class XPathSelectorTestCase(unittest.TestCase):\n         x.register_namespace(\"xmlns\", \"http://webservices.amazon.com/AWSECommerceService/2005-10-05\")\n         x.register_namespace(\"p\", \"http://www.scrapy.org/product\")\n         x.register_namespace(\"b\", \"http://somens.com\")\n-        self.assertEqual(len(x.x(\"//xmlns:TestTag\")), 1)\n-        self.assertEqual(x.x(\"//b:Operation/text()\").extract()[0], 'hello')\n-        self.assertEqual(x.x(\"//xmlns:TestTag/@b:att\").extract()[0], 'value')\n-        self.assertEqual(x.x(\"//p:SecondTestTag/xmlns:price/text()\").extract()[0], '90')\n-        self.assertEqual(x.x(\"//p:SecondTestTag\").x(\"./xmlns:price/text()\")[0].extract(), '90')\n-        self.assertEqual(x.x(\"//p:SecondTestTag/xmlns:material\").extract()[0], '<material/>')\n+        self.assertEqual(len(x.select(\"//xmlns:TestTag\")), 1)\n+        self.assertEqual(x.select(\"//b:Operation/text()\").extract()[0], 'hello')\n+        self.assertEqual(x.select(\"//xmlns:TestTag/@b:att\").extract()[0], 'value')\n+        self.assertEqual(x.select(\"//p:SecondTestTag/xmlns:price/text()\").extract()[0], '90')\n+        self.assertEqual(x.select(\"//p:SecondTestTag\").select(\"./xmlns:price/text()\")[0].extract(), '90')\n+        self.assertEqual(x.select(\"//p:SecondTestTag/xmlns:material\").extract()[0], '<material/>')\n \n     @libxml2debug\n     def test_selector_invalid_xpath(self):\n@@ -170,7 +170,7 @@ class XPathSelectorTestCase(unittest.TestCase):\n         x = HtmlXPathSelector(response)\n         xpath = \"//test[@foo='bar]\"\n         try:\n-            x.x(xpath)\n+            x.select(xpath)\n         except ValueError, e:\n             assert xpath in str(e), \"Exception message does not contain invalid xpath\"\n         except Exception:\n@@ -195,7 +195,7 @@ class XPathSelectorTestCase(unittest.TestCase):\n         headers = {'Content-Type': ['text/html; charset=utf-8']}\n         response = HtmlResponse(url=\"http://example.com\", headers=headers, body=html_utf8)\n         x = HtmlXPathSelector(response)\n-        self.assertEquals(x.x(\"//span[@id='blank']/text()\").extract(),\n+        self.assertEquals(x.select(\"//span[@id='blank']/text()\").extract(),\n                           [u'\\xa3'])\n \n     @libxml2debug\n@@ -223,13 +223,13 @@ class XPathSelectorTestCase(unittest.TestCase):\n \n         self.assertEqual(xxs.extract_unquoted(), u'')\n \n-        self.assertEqual(xxs.x('/root').extract_unquoted(), [u''])\n-        self.assertEqual(xxs.x('/root/text()').extract_unquoted(), [\n+        self.assertEqual(xxs.select('/root').extract_unquoted(), [u''])\n+        self.assertEqual(xxs.select('/root/text()').extract_unquoted(), [\n             u'\\n  lala\\n  ',\n             u'\\n  pff\\n'])\n \n-        self.assertEqual(xxs.x('//*').extract_unquoted(), [u'', u'', u''])\n-        self.assertEqual(xxs.x('//text()').extract_unquoted(), [\n+        self.assertEqual(xxs.select('//*').extract_unquoted(), [u'', u'', u''])\n+        self.assertEqual(xxs.select('//text()').extract_unquoted(), [\n             u'\\n  lala\\n  ',\n             u'\\n    blabla&more',\n             u'a',\n\n@@ -0,0 +1,24 @@\n+import warnings\n+\n+\n+def deprecated(use_instead=None):\n+    \"\"\"This is a decorator which can be used to mark functions\n+    as deprecated. It will result in a warning being emitted\n+    when the function is used.\"\"\"\n+\n+    def wraps(func):\n+        def new_func(*args, **kwargs):\n+            message = \"Call to deprecated function %s.\" % func.__name__\n+            if use_instead:\n+                message += \" Use %s instead.\" % use_instead\n+\n+            warnings.warn_explicit(\n+                message,\n+                category=DeprecationWarning,\n+                filename=func.func_code.co_filename,\n+                lineno=func.func_code.co_firstlineno + 1\n+            )\n+            return func(*args, **kwargs)\n+        return new_func\n+    return wraps\n+\n\n@@ -27,7 +27,7 @@ def xmliter(obj, nodename):\n     r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = header_start + match.group() + header_end\n-        yield XmlXPathSelector(text=nodetext).x('//' + nodename)[0]\n+        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n \n def csviter(obj, delimiter=None, headers=None, encoding=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n\n@@ -11,6 +11,7 @@ from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml\n from scrapy.xpath.document import Libxml2Document\n from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n+from scrapy.utils.decorator import deprecated\n \n class XPathSelector(object):\n \n@@ -29,7 +30,7 @@ class XPathSelector(object):\n         self.expr = expr\n         self.response = response\n \n-    def x(self, xpath):\n+    def select(self, xpath):\n         \"\"\"Perform the given XPath query on the current XPathSelector and\n         return a XPathSelectorList of the result\"\"\"\n         if hasattr(self.xmlNode, 'xpathEval'):\n@@ -47,7 +48,14 @@ class XPathSelector(object):\n                     expr=xpath, response=self.response)])\n         else:\n             return XPathSelectorList([])\n-    __call__ = x\n+\n+    @deprecated(use_instead='XPathSelector.select')\n+    def __call__(self, xpath):\n+        return self.select(xpath)\n+\n+    @deprecated(use_instead='XPathSelector.select')\n+    def x(self, xpath):\n+        return self.select(xpath)\n \n     def re(self, regex):\n         \"\"\"Return a list of unicode strings by applying the regex over all\n@@ -77,7 +85,7 @@ class XPathSelector(object):\n \n     def extract_unquoted(self):\n         \"\"\"Get unescaped contents from the text node (no entities, no CDATA)\"\"\"\n-        if self.x('self::text()'):\n+        if self.select('self::text()'):\n             return unicode(self.xmlNode.getContent(), 'utf-8', errors='ignore')\n         else:\n             return u''\n@@ -106,10 +114,14 @@ class XPathSelectorList(list):\n     def __getslice__(self, i, j):\n         return XPathSelectorList(list.__getslice__(self, i, j))\n \n-    def x(self, xpath):\n+    def select(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n-        return XPathSelectorList(flatten([x.x(xpath) for x in self]))\n+        return XPathSelectorList(flatten([x.select(xpath) for x in self]))\n+\n+    @deprecated(use_instead='XPathSelectorList.select')\n+    def x(self, xpath):\n+        return self.select(xpath)\n \n     def re(self, regex):\n         \"\"\"Perform the re() method on each XPathSelector of the list, and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
