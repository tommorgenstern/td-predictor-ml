{"custom_id": "keras#f89b3397bbb6377186fe050b421cd2e0fa75ebee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52e3f9835c1a25a8b895773f64524e27a5ae10c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10973 | Contributors (this commit): 57 | Commits (past 90d): 8 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -860,12 +860,12 @@ class Conv3DTranspose(Conv3D):\n         filters: Integer, the dimensionality of the output space\n             (i.e. the number of output filters in the convolution).\n         kernel_size: An integer or tuple/list of 3 integers, specifying the\n-            height and width of the 3D convolution window.\n+            depth, height and width of the 3D convolution window.\n             Can be a single integer to specify the same value for\n             all spatial dimensions.\n         strides: An integer or tuple/list of 3 integers,\n             specifying the strides of the convolution\n-            along the height and width.\n+            along the depth, height and width.\n             Can be a single integer to specify the same value for\n             all spatial dimensions.\n             Specifying any stride value != 1 is incompatible with specifying\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b21764824f181fb7a32fce852e5afcee4042192a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 39 | Churn Cumulative: 1458 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -257,3 +257,35 @@ class Softmax(Layer):\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n+\n+\n+class ReLU(Layer):\n+    \"\"\"Rectified Linear Unit activation function.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        max_value: Float, the maximum output value.\n+    \"\"\"\n+\n+    def __init__(self, max_value=None, **kwargs):\n+        super(ReLU, self).__init__(**kwargs)\n+        self.supports_masking = True\n+        self.max_value = max_value\n+\n+    def call(self, inputs):\n+        return activations.relu(inputs, max_value=self.max_value)\n+\n+    def get_config(self):\n+        config = {'max_value': self.max_value}\n+        base_config = super(ReLU, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -43,5 +43,12 @@ def test_softmax():\n                    input_shape=(2, 3, 4))\n \n \n+@keras_test\n+def test_relu():\n+    for max_value in [None, 1., 6.]:\n+        layer_test(layers.ReLU, kwargs={'max_value': max_value},\n+                   input_shape=(2, 3, 4))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce56322aadc6a861db21e598720c2dce03747ec3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 31 | Methods Changed: 13 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 212 | Churn Cumulative: 4733 | Contributors (this commit): 76 | Commits (past 90d): 17 | Contributors (cumulative): 101 | DMM Complexity: 0.17073170731707318\n\nDIFF:\n@@ -0,0 +1,96 @@\n+'''Trains a simple convnet on the MNIST dataset and embeds test data.\n+\n+The test data is embedded using the weights of the final dense layer, just\n+before the classification head. This embedding can then be visualized using\n+TensorBoard's Embedding Projector.\n+'''\n+\n+from __future__ import print_function\n+\n+from os import makedirs\n+from os.path import exists, join\n+\n+import keras\n+from keras.callbacks import TensorBoard\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers import Dense, Dropout, Flatten\n+from keras.layers import Conv2D, MaxPooling2D\n+from keras import backend as K\n+\n+import numpy as np\n+\n+batch_size = 128\n+num_classes = 10\n+epochs = 12\n+log_dir = './logs'\n+\n+if not exists(log_dir):\n+    makedirs(log_dir)\n+\n+# input image dimensions\n+img_rows, img_cols = 28, 28\n+\n+# the data, split between train and test sets\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n+\n+if K.image_data_format() == 'channels_first':\n+    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n+    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n+    input_shape = (1, img_rows, img_cols)\n+else:\n+    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n+    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n+    input_shape = (img_rows, img_cols, 1)\n+\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+print('x_train shape:', x_train.shape)\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n+\n+# save class labels to disk to color data points in TensorBoard accordingly\n+with open(join(log_dir, 'metadata.tsv'), 'w') as f:\n+    np.savetxt(f, y_test)\n+\n+# convert class vectors to binary class matrices\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+\n+tensorboard = TensorBoard(batch_size=batch_size,\n+                          embeddings_freq=1,\n+                          embeddings_layer_names=['features'],\n+                          embeddings_metadata='metadata.tsv',\n+                          embeddings_data=x_test)\n+\n+model = Sequential()\n+model.add(Conv2D(32, kernel_size=(3, 3),\n+                 activation='relu',\n+                 input_shape=input_shape))\n+model.add(Conv2D(64, (3, 3), activation='relu'))\n+model.add(MaxPooling2D(pool_size=(2, 2)))\n+model.add(Dropout(0.25))\n+model.add(Flatten())\n+model.add(Dense(128, activation='relu', name='features'))\n+model.add(Dropout(0.5))\n+model.add(Dense(num_classes, activation='softmax'))\n+\n+model.compile(loss=keras.losses.categorical_crossentropy,\n+              optimizer=keras.optimizers.Adadelta(),\n+              metrics=['accuracy'])\n+\n+model.fit(x_train, y_train,\n+          batch_size=batch_size,\n+          callbacks=[tensorboard],\n+          epochs=epochs,\n+          verbose=1,\n+          validation_data=(x_test, y_test))\n+score = model.evaluate(x_test, y_test, verbose=0)\n+print('Test loss:', score[0])\n+print('Test accuracy:', score[1])\n+\n+# You can now launch tensorboard with `tensorboard --logdir=./logs` on your\n+# command line and then go to http://localhost:6006/#projector to view the\n+# embeddings\n\n@@ -18,6 +18,7 @@ from collections import OrderedDict\n from collections import Iterable\n from .utils.generic_utils import Progbar\n from . import backend as K\n+from .engine.training_utils import standardize_input_data\n \n try:\n     import requests\n@@ -675,7 +676,9 @@ class TensorBoard(Callback):\n         write_images: whether to write model weights to visualize as\n             image in TensorBoard.\n         embeddings_freq: frequency (in epochs) at which selected embedding\n-            layers will be saved.\n+            layers will be saved. If set to 0, embeddings won't be computed.\n+            Data to be visualized in TensorBoard's Embedding tab must be passed\n+            as `embeddings_data`.\n         embeddings_layer_names: a list of names of layers to keep eye on. If\n             None or empty list all the embedding layer will be watched.\n         embeddings_metadata: a dictionary which maps layer name to a file name\n@@ -683,6 +686,10 @@ class TensorBoard(Callback):\n             [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n             about metadata files format. In case if the same metadata file is\n             used for all embedding layers, string can be passed.\n+        embeddings_data: data to be embedded at layers specified in\n+            `embeddings_layer_names`. Numpy array (if the model has a single\n+            input) or list of Numpy arrays (if the model has multiple inputs).\n+            Learn [more about embeddings](https://www.tensorflow.org/programmers_guide/embedding)\n     \"\"\"\n \n     def __init__(self, log_dir='./logs',\n@@ -693,7 +700,8 @@ class TensorBoard(Callback):\n                  write_images=False,\n                  embeddings_freq=0,\n                  embeddings_layer_names=None,\n-                 embeddings_metadata=None):\n+                 embeddings_metadata=None,\n+                 embeddings_data=None):\n         super(TensorBoard, self).__init__()\n         global tf, projector\n         try:\n@@ -730,6 +738,7 @@ class TensorBoard(Callback):\n         self.embeddings_layer_names = embeddings_layer_names\n         self.embeddings_metadata = embeddings_metadata or {}\n         self.batch_size = batch_size\n+        self.embeddings_data = embeddings_data\n \n     def set_model(self, model):\n         self.model = model\n@@ -800,18 +809,35 @@ class TensorBoard(Callback):\n         else:\n             self.writer = tf.summary.FileWriter(self.log_dir)\n \n-        if self.embeddings_freq:\n+        if self.embeddings_freq and self.embeddings_data is not None:\n+            self.embeddings_data = standardize_input_data(self.embeddings_data, model.input_names)\n+\n             embeddings_layer_names = self.embeddings_layer_names\n \n             if not embeddings_layer_names:\n                 embeddings_layer_names = [layer.name for layer in self.model.layers\n                                           if type(layer).__name__ == 'Embedding']\n+            self.assign_embeddings = []\n+            embeddings_vars = {}\n \n-            embeddings = {layer.name: layer.weights[0]\n-                          for layer in self.model.layers\n-                          if layer.name in embeddings_layer_names}\n+            self.batch_id = batch_id = tf.placeholder(tf.int32)\n+            self.step = step = tf.placeholder(tf.int32)\n \n-            self.saver = tf.train.Saver(list(embeddings.values()))\n+            for layer in self.model.layers:\n+                if layer.name in embeddings_layer_names:\n+                    embedding_input = self.model.get_layer(layer.name).output\n+                    embedding_size = np.prod(embedding_input.shape[1:])\n+                    embedding_input = tf.reshape(embedding_input,\n+                                                 (step, int(embedding_size)))\n+                    shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n+                    embedding = tf.Variable(tf.zeros(shape),\n+                                            name=layer.name + '_embedding')\n+                    embeddings_vars[layer.name] = embedding\n+                    batch = tf.assign(embedding[batch_id:batch_id + step],\n+                                      embedding_input)\n+                    self.assign_embeddings.append(batch)\n+\n+            self.saver = tf.train.Saver(list(embeddings_vars.values()))\n \n             embeddings_metadata = {}\n \n@@ -819,13 +845,11 @@ class TensorBoard(Callback):\n                 embeddings_metadata = self.embeddings_metadata\n             else:\n                 embeddings_metadata = {layer_name: self.embeddings_metadata\n-                                       for layer_name in embeddings.keys()}\n+                                       for layer_name in embeddings_vars.keys()}\n \n             config = projector.ProjectorConfig()\n-            self.embeddings_ckpt_path = os.path.join(self.log_dir,\n-                                                     'keras_embedding.ckpt')\n \n-            for layer_name, tensor in embeddings.items():\n+            for layer_name, tensor in embeddings_vars.items():\n                 embedding = config.embeddings.add()\n                 embedding.tensor_name = tensor.name\n \n@@ -838,8 +862,11 @@ class TensorBoard(Callback):\n         logs = logs or {}\n \n         if not self.validation_data and self.histogram_freq:\n-            raise ValueError('If printing histograms, validation_data must be '\n-                             'provided, and cannot be a generator.')\n+            raise ValueError(\"If printing histograms, validation_data must be \"\n+                             \"provided, and cannot be a generator.\")\n+        if self.embeddings_data is None and self.embeddings_freq:\n+            raise ValueError(\"To visualize embeddings, embeddings_data must \"\n+                             \"be provided.\")\n         if self.validation_data and self.histogram_freq:\n             if epoch % self.histogram_freq == 0:\n \n@@ -869,12 +896,44 @@ class TensorBoard(Callback):\n                     self.writer.add_summary(summary_str, epoch)\n                     i += self.batch_size\n \n-        if self.embeddings_freq and self.embeddings_ckpt_path:\n+        if self.embeddings_freq and self.embeddings_data is not None:\n             if epoch % self.embeddings_freq == 0:\n+                # We need a second forward-pass here because we're passing\n+                # the `embeddings_data` explicitly. This design allows to pass\n+                # arbitrary data as `embeddings_data` and results from the fact\n+                # that we need to know the size of the `tf.Variable`s which\n+                # hold the embeddings in `set_model`. At this point, however,\n+                # the `validation_data` is not yet set.\n+\n+                # More details in this discussion:\n+                # https://github.com/keras-team/keras/pull/7766#issuecomment-329195622\n+\n+                embeddings_data = self.embeddings_data\n+                n_samples = embeddings_data[0].shape[0]\n+\n+                i = 0\n+                while i < n_samples:\n+                    step = min(self.batch_size, n_samples - i)\n+                    batch = slice(i, i + step)\n+\n+                    if type(self.model.input) == list:\n+                        feed_dict = {model_input: embeddings_data[idx][batch]\n+                                     for idx, model_input in enumerate(self.model.input)}\n+                    else:\n+                        feed_dict = {self.model.input: embeddings_data[0][batch]}\n+\n+                    feed_dict.update({self.batch_id: i, self.step: step})\n+\n+                    if self.model.uses_learning_phase:\n+                        feed_dict[K.learning_phase()] = False\n+\n+                    self.sess.run(self.assign_embeddings, feed_dict=feed_dict)\n                     self.saver.save(self.sess,\n-                                self.embeddings_ckpt_path,\n+                                    os.path.join(self.log_dir, 'keras_embedding.ckpt'),\n                                     epoch)\n \n+                    i += self.batch_size\n+\n         for name, value in logs.items():\n             if name in ['batch', 'size']:\n                 continue\n\n@@ -510,17 +510,19 @@ def test_TensorBoard(tmpdir):\n                   metrics=['accuracy'])\n \n     # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq):\n+    def callbacks_factory(histogram_freq, embeddings_freq=1):\n         return [callbacks.TensorBoard(log_dir=filepath,\n                                       histogram_freq=histogram_freq,\n                                       write_images=True, write_grads=True,\n-                                      embeddings_freq=1,\n+                                      embeddings_freq=embeddings_freq,\n                                       embeddings_layer_names=['dense_1'],\n+                                      embeddings_data=X_test,\n                                       batch_size=5)]\n \n     # fit without validation data\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n+              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n+              epochs=3)\n \n     # fit with validation data and accuracy\n     model.fit(X_train, y_train, batch_size=batch_size,\n@@ -529,7 +531,8 @@ def test_TensorBoard(tmpdir):\n \n     # fit generator without validation data\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n-                        callbacks=callbacks_factory(histogram_freq=0))\n+                        callbacks=callbacks_factory(histogram_freq=0,\n+                                                    embeddings_freq=0))\n \n     # fit generator with validation data and accuracy\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n@@ -584,12 +587,13 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n                   metrics=['accuracy'])\n \n     # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq):\n+    def callbacks_factory(histogram_freq, embeddings_freq=1):\n         return [callbacks.TensorBoard(log_dir=filepath,\n                                       histogram_freq=histogram_freq,\n                                       write_images=True, write_grads=True,\n-                                      embeddings_freq=1,\n+                                      embeddings_freq=embeddings_freq,\n                                       embeddings_layer_names=['dense_1'],\n+                                      embeddings_data=X_test,\n                                       batch_size=5)]\n \n     # fit without validation data should raise ValueError if histogram_freq > 0\n@@ -662,17 +666,19 @@ def test_TensorBoard_multi_input_output(tmpdir):\n                   metrics=['accuracy'])\n \n     # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq):\n+    def callbacks_factory(histogram_freq, embeddings_freq=1):\n         return [callbacks.TensorBoard(log_dir=filepath,\n                                       histogram_freq=histogram_freq,\n                                       write_images=True, write_grads=True,\n-                                      embeddings_freq=1,\n+                                      embeddings_freq=embeddings_freq,\n                                       embeddings_layer_names=['dense_1'],\n+                                      embeddings_data=[X_test] * 2,\n                                       batch_size=5)]\n \n     # fit without validation data\n     model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n-              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n+              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n+              epochs=3)\n \n     # fit with validation data and accuracy\n     model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n@@ -681,7 +687,8 @@ def test_TensorBoard_multi_input_output(tmpdir):\n \n     # fit generator without validation data\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n-                        callbacks=callbacks_factory(histogram_freq=0))\n+                        callbacks=callbacks_factory(histogram_freq=0,\n+                                                    embeddings_freq=0))\n \n     # fit generator with validation data and accuracy\n     model.fit_generator(data_generator(True), len(X_train), epochs=2,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e80c1a2dbaceba9a8280c37e4e896599eb927f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 439 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,13 +83,12 @@ class HDF5Matrix(object):\n                 idx = (self.start + key).tolist()\n             else:\n                 raise IndexError\n-        elif isinstance(key, list):\n+        else:\n+            # Assume list/iterable\n             if max(key) + self.start < self.end:\n                 idx = [x + self.start for x in key]\n             else:\n                 raise IndexError\n-        else:\n-            raise IndexError\n         if self.normalizer is not None:\n             return self.normalizer(self.data[idx])\n         else:\n\n@@ -90,8 +90,10 @@ def test_io_utils(in_tmpdir):\n     with pytest.raises(IndexError):\n         X_train[[1000, 1001]]\n     with pytest.raises(IndexError):\n-        X_train[np.array([1000])]\n+        X_train[six.moves.range(1000, 1001)]\n     with pytest.raises(IndexError):\n+        X_train[np.array([1000])]\n+    with pytest.raises(TypeError):\n         X_train[None]\n     assert (X_train[0] == X_train[:1][0]).all()\n     assert (X_train[[0, 1]] == X_train[:2]).all()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#25d0193eaeed91ac899bf8b572d98ce0e0c76d3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 189 | Lines Deleted: 16 | Files Changed: 6 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 32/8 | Churn Δ: 205 | Churn Cumulative: 29954 | Contributors (this commit): 206 | Commits (past 90d): 69 | Contributors (cumulative): 347 | DMM Complexity: 0.424\n\nDIFF:\n@@ -1823,7 +1823,23 @@ def softsign(x):\n     return x / (1 + C.abs(x))\n \n \n-def categorical_crossentropy(target, output, from_logits=False):\n+def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    # Here, unlike other backends, the tensors lack a batch dimension:\n+    axis_without_batch = -1 if axis == -1 else axis - 1\n+    output_dimensions = list(range(len(output.shape)))\n+    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n+        raise ValueError(\n+            '{}{}{}'.format(\n+                'Unexpected channels axis {}. '.format(axis_without_batch),\n+                'Expected to be -1 or one of the axes of `output`, ',\n+                'which has {} dimensions.'.format(len(output.shape))))\n+    # If the channels are not in the last axis, move them to be there:\n+    if axis_without_batch != -1 and axis_without_batch != output_dimensions[-1]:\n+        permutation = output_dimensions[:axis_without_batch]\n+        permutation += output_dimensions[axis_without_batch + 1:]\n+        permutation += [axis_without_batch]\n+        output = C.transpose(output, permutation)\n+        target = C.transpose(target, permutation)\n     if from_logits:\n         result = C.cross_entropy_with_softmax(output, target)\n         # cntk's result shape is (batch, 1), while keras expect (batch, )\n@@ -1836,10 +1852,20 @@ def categorical_crossentropy(target, output, from_logits=False):\n         return -sum(target * C.log(output), axis=-1)\n \n \n-def sparse_categorical_crossentropy(target, output, from_logits=False):\n-    target = C.one_hot(target, output.shape[-1])\n+def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    # Here, unlike other backends, the tensors lack a batch dimension:\n+    axis_without_batch = -1 if axis == -1 else axis - 1\n+    output_dimensions = list(range(len(output.shape)))\n+    if axis_without_batch != -1 and axis_without_batch not in output_dimensions:\n+        raise ValueError(\n+            '{}{}{}'.format(\n+                'Unexpected channels axis {}. '.format(axis_without_batch),\n+                'Expected to be -1 or one of the axes of `output`, ',\n+                'which has {} dimensions.'.format(len(output.shape))))\n+    target = C.one_hot(target, output.shape[axis_without_batch],\n+                       axis=axis_without_batch)\n     target = C.reshape(target, output.shape)\n-    return categorical_crossentropy(target, output, from_logits)\n+    return categorical_crossentropy(target, output, from_logits, axis=axis)\n \n \n class Function(object):\n\n@@ -3173,7 +3173,7 @@ def softsign(x):\n     return tf.nn.softsign(x)\n \n \n-def categorical_crossentropy(target, output, from_logits=False):\n+def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n \n     # Arguments\n@@ -3183,28 +3183,40 @@ def categorical_crossentropy(target, output, from_logits=False):\n             case `output` is expected to be the logits).\n         from_logits: Boolean, whether `output` is the\n             result of a softmax, or is a tensor of logits.\n+        axis: Int specifying the channels axis. `axis=-1`\n+            corresponds to data format `channels_last`,\n+            and `axis=1` corresponds to data format\n+            `channels_first`.\n \n     # Returns\n         Output tensor.\n+\n+    # Raises\n+        ValueError: if `axis` is neither -1 nor one of\n+            the axes of `output`.\n     \"\"\"\n+    output_dimensions = list(range(len(output.get_shape())))\n+    if axis != -1 and axis not in output_dimensions:\n+        raise ValueError(\n+            '{}{}{}'.format(\n+                'Unexpected channels axis {}. '.format(axis),\n+                'Expected to be -1 or one of the axes of `output`, ',\n+                'which has {} dimensions.'.format(len(output.get_shape()))))\n     # Note: tf.nn.softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n         # scale preds so that the class probas of each sample sum to 1\n-        output /= tf.reduce_sum(output,\n-                                len(output.get_shape()) - 1,\n-                                True)\n+        output /= tf.reduce_sum(output, axis, True)\n         # manual computation of crossentropy\n         _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n         output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n-        return - tf.reduce_sum(target * tf.log(output),\n-                               len(output.get_shape()) - 1)\n+        return - tf.reduce_sum(target * tf.log(output), axis)\n     else:\n         return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n                                                        logits=output)\n \n \n-def sparse_categorical_crossentropy(target, output, from_logits=False):\n+def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"Categorical crossentropy with integer targets.\n \n     # Arguments\n@@ -3214,10 +3226,31 @@ def sparse_categorical_crossentropy(target, output, from_logits=False):\n             case `output` is expected to be the logits).\n         from_logits: Boolean, whether `output` is the\n             result of a softmax, or is a tensor of logits.\n+        axis: Int specifying the channels axis. `axis=-1`\n+            corresponds to data format `channels_last`,\n+            and `axis=1` corresponds to data format\n+            `channels_first`.\n \n     # Returns\n         Output tensor.\n+\n+    # Raises\n+        ValueError: if `axis` is neither -1 nor one of\n+            the axes of `output`.\n     \"\"\"\n+    output_dimensions = list(range(len(output.get_shape())))\n+    if axis != -1 and axis not in output_dimensions:\n+        raise ValueError(\n+            '{}{}{}'.format(\n+                'Unexpected channels axis {}. '.format(axis),\n+                'Expected to be -1 or one of the axes of `output`, ',\n+                'which has {} dimensions.'.format(len(output.get_shape()))))\n+    # If the channels are not in the last axis, move them to be there:\n+    if axis != -1 and axis != output_dimensions[-1]:\n+        permutation = output_dimensions[:axis] + output_dimensions[axis + 1:]\n+        permutation += [axis]\n+        output = tf.transpose(output, perm=permutation)\n+\n     # Note: tf.nn.sparse_softmax_cross_entropy_with_logits\n     # expects logits, Keras expects probabilities.\n     if not from_logits:\n\n@@ -1619,7 +1619,20 @@ def softsign(x):\n     return T_softsign(x)\n \n \n-def categorical_crossentropy(target, output, from_logits=False):\n+def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    output_dimensions = list(range(len(int_shape(output))))\n+    if axis != -1 and axis not in output_dimensions:\n+        raise ValueError(\n+            '{}{}{}'.format(\n+                'Unexpected channels axis {}. '.format(axis),\n+                'Expected to be -1 or one of the axes of `output`, ',\n+                'which has {} dimensions.'.format(len(int_shape(output)))))\n+    # If the channels are not in the last axis, move them to be there:\n+    if axis != -1 and axis != output_dimensions[-1]:\n+        permutation = output_dimensions[:axis]\n+        permutation += output_dimensions[axis + 1:] + [axis]\n+        output = permute_dimensions(output, permutation)\n+        target = permute_dimensions(target, permutation)\n     if from_logits:\n         output = T.nnet.softmax(output)\n     else:\n@@ -1630,11 +1643,24 @@ def categorical_crossentropy(target, output, from_logits=False):\n     return T.nnet.categorical_crossentropy(output, target)\n \n \n-def sparse_categorical_crossentropy(target, output, from_logits=False):\n+def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+    output_dimensions = list(range(len(int_shape(output))))\n+    if axis != -1 and axis not in output_dimensions:\n+        raise ValueError(\n+            '{}{}{}'.format(\n+                'Unexpected channels axis {}. '.format(axis),\n+                'Expected to be -1 or one of the axes of `output`, ',\n+                'which has {} dimensions.'.format(len(int_shape(output)))))\n+    # If the channels are not in the last axis, move them to be there:\n+    if axis != -1 and axis != output_dimensions[-1]:\n+        permutation = output_dimensions[:axis]\n+        permutation += output_dimensions[axis + 1:] + [axis]\n+        output = permute_dimensions(output, permutation)\n+        target = permute_dimensions(target, permutation)\n     target = T.cast(T.flatten(target), 'int32')\n     target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n     target = reshape(target, shape(output))\n-    return categorical_crossentropy(target, output, from_logits)\n+    return categorical_crossentropy(target, output, from_logits, axis=-1)\n \n \n def binary_crossentropy(target, output, from_logits=False):\n\n@@ -767,6 +767,11 @@ class Model(Network):\n                 for output_shape, loss_fn in zip(self._feed_output_shapes,\n                                                  self._feed_loss_fns):\n                     if loss_fn is losses.sparse_categorical_crossentropy:\n+                        if K.image_data_format() == 'channels_first' and len(\n+                                output_shape) in [4, 5]:\n+                            feed_output_shapes.append(\n+                                (output_shape[0], 1) + output_shape[2:])\n+                        else:\n                             feed_output_shapes.append(output_shape[:-1] + (1,))\n                     elif (not hasattr(loss_fn, '__name__') or\n                             getattr(losses, loss_fn.__name__, None) is None):\n\n@@ -17,7 +17,8 @@ def to_categorical(y, num_classes=None):\n         num_classes: total number of classes.\n \n     # Returns\n-        A binary matrix representation of the input.\n+        A binary matrix representation of the input. The classes axis\n+        is placed last.\n     \"\"\"\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n\n@@ -7,7 +7,7 @@ import scipy.sparse as sparse\n \n import keras\n from keras import losses\n-from keras.layers import Dense, Dropout\n+from keras.layers import Dense, Dropout, Conv2D\n from keras.engine import Input\n from keras.engine.training import Model\n from keras.engine import training_utils\n@@ -1361,5 +1361,87 @@ def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n     model.test_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n \n \n+@keras_test\n+def test_model_with_crossentropy_losses_channels_first():\n+    \"\"\"Tests use of all crossentropy losses with `channels_first`.\n+\n+    Tests `sparse_categorical_crossentropy`, `categorical_crossentropy`,\n+    and `binary_crossentropy`.\n+    Verifies that evaluate gives the same result with either\n+    `channels_first` or `channels_last` image_data_format.\n+    Tests PR #9715.\n+    \"\"\"\n+    def prepare_simple_model(input_tensor, loss_name, target):\n+        axis = 1 if K.image_data_format() == 'channels_first' else -1\n+        if loss_name == 'sparse_categorical_crossentropy':\n+            loss = lambda y_true, y_pred: K.sparse_categorical_crossentropy(\n+                y_true, y_pred, axis=axis)\n+            num_channels = np.amax(target) + 1\n+            activation = 'softmax'\n+        elif loss_name == 'categorical_crossentropy':\n+            loss = lambda y_true, y_pred: K.categorical_crossentropy(\n+                y_true, y_pred, axis=axis)\n+            num_channels = target.shape[axis]\n+            activation = 'softmax'\n+        elif loss_name == 'binary_crossentropy':\n+            loss = lambda y_true, y_pred: K.binary_crossentropy(y_true, y_pred)\n+            num_channels = target.shape[axis]\n+            activation = 'sigmoid'\n+        predictions = Conv2D(num_channels, 1, activation=activation,\n+                             kernel_initializer='ones',\n+                             bias_initializer='ones')(input_tensor)\n+        simple_model = Model(inputs=input_tensor, outputs=predictions)\n+        simple_model.compile(optimizer='rmsprop', loss=loss)\n+        return simple_model\n+\n+    losses_to_test = ['sparse_categorical_crossentropy',\n+                      'categorical_crossentropy', 'binary_crossentropy']\n+\n+    data_channels_first = np.array([[[[8., 7.1, 0.], [4.5, 2.6, 0.55],\n+                                      [0.9, 4.2, 11.2]]]], dtype=np.float32)\n+    # Labels for testing 4-class sparse_categorical_crossentropy, 4-class\n+    # categorical_crossentropy, and 2-class binary_crossentropy:\n+    labels_channels_first = [np.array([[[[0, 1, 3], [2, 1, 0], [2, 2, 1]]]]),\n+                             np.array([[[[0, 1, 0], [0, 1, 0], [0, 0, 0]],\n+                                        [[1, 0, 0], [0, 0, 1], [0, 1, 0]],\n+                                        [[0, 0, 0], [1, 0, 0], [0, 0, 1]],\n+                                        [[0, 0, 1], [0, 0, 0], [1, 0, 0]]]]),\n+                             np.array([[[[0, 1, 0], [0, 1, 0], [0, 0, 1]],\n+                                        [[1, 0, 1], [1, 0, 1], [1, 1, 0]]]])]\n+    # Compute one loss for each loss function in the list `losses_to_test`:\n+    loss_channels_last = [0., 0., 0.]\n+    loss_channels_first = [0., 0., 0.]\n+\n+    old_data_format = K.image_data_format()\n+\n+    # Evaluate a simple network with channels last, with all three loss\n+    # functions:\n+    K.set_image_data_format('channels_last')\n+    data = np.moveaxis(data_channels_first, 1, -1)\n+    for index, loss_function in enumerate(losses_to_test):\n+        labels = np.moveaxis(labels_channels_first[index], 1, -1)\n+        inputs = Input(shape=(3, 3, 1))\n+        model = prepare_simple_model(inputs, loss_function, labels)\n+        loss_channels_last[index] = model.evaluate(x=data, y=labels,\n+                                                   batch_size=1, verbose=0)\n+\n+    # Evaluate the same network with channels first, with all three loss\n+    # functions:\n+    K.set_image_data_format('channels_first')\n+    data = data_channels_first\n+    for index, loss_function in enumerate(losses_to_test):\n+        labels = labels_channels_first[index]\n+        inputs = Input(shape=(1, 3, 3))\n+        model = prepare_simple_model(inputs, loss_function, labels)\n+        loss_channels_first[index] = model.evaluate(x=data, y=labels,\n+                                                    batch_size=1, verbose=0)\n+\n+    K.set_image_data_format(old_data_format)\n+\n+    assert_allclose(loss_channels_first, loss_channels_last,\n+                    err_msg='{}{}'.format('Computed different losses for ',\n+                                          'channels_first and channels_last.'))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f4ccbb1966f3b14b3c98f4df45e018ac24ede13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 2117 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1258,6 +1258,12 @@ class Network(Layer):\n                 in order to capture the string summary.\n                 It defaults to `print` (prints to stdout).\n         \"\"\"\n+        if not self.built:\n+            raise ValueError(\n+                'This model has never been called, this its weights '\n+                'have not yet been created, so no summary can be displayed. '\n+                'Build the model first '\n+                '(e.g. by calling it on some test data).')\n         return print_layer_summary(self,\n                                    line_length=line_length,\n                                    positions=positions,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a58f7b29026270afbec58c9255750b527ceff27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 476 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.1.6'\r\n+__version__ = '2.2.0'\r\n\n@@ -24,13 +24,13 @@ and is distributed under the MIT liense.\n '''\n \n setup(name='Keras',\n-      version='2.1.6',\n+      version='2.2.0',\n       description='Deep Learning for humans',\n       long_description=long_description,\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.1.6',\n+      download_url='https://github.com/keras-team/keras/tarball/2.2.0',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56b255cc043f57e712a95a90819e5c005a2fee8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 330 | Contributors (this commit): 16 | Commits (past 90d): 7 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ setup(name='Keras',\n                         'six>=1.9.0',\n                         'pyyaml',\n                         'h5py',\n-                        'keras_applications==1.0.1',\n+                        'keras_applications==1.0.2',\n                         'keras_preprocessing==1.0.1'],\n       extras_require={\n           'visualize': ['pydot>=1.2.4'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13548e8b212b504c9f506495b01855de6d9f6117", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 2062 | Contributors (this commit): 24 | Commits (past 90d): 7 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,7 @@ import scipy.sparse as sparse\n \n import keras\n from keras import losses\n-from keras.layers import Dense, Dropout, Conv2D\n+from keras.layers import Activation, Dense, Dropout, Conv2D\n from keras.engine import Input\n from keras.engine.training import Model\n from keras.engine import training_utils\n@@ -1443,5 +1443,24 @@ def test_model_with_crossentropy_losses_channels_first():\n                                           'channels_first and channels_last.'))\n \n \n+@keras_test\n+def test_dynamic_set_inputs():\n+    model = Sequential()\n+    model.add(Dense(16, input_dim=32))\n+    model.add(Activation('relu'))\n+\n+    model2 = Sequential()\n+    model2.add(model.layers[-1])\n+    model2.add(Dense(8))\n+    preds2 = model2.predict([np.random.random((1, 32))])\n+    assert preds2.shape == (1, 8)\n+\n+    model3 = Model(inputs=model.inputs, outputs=model.outputs)\n+    model3.inputs = None\n+    model3._set_inputs(model.inputs)\n+    preds3 = model3.predict([np.random.random((1, 32))])\n+    assert preds3.shape == (1, 16)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbf4283457e38859d6dff5a2f0990f8347799e6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 78 | Churn Cumulative: 2125 | Contributors (this commit): 19 | Commits (past 90d): 14 | Contributors (cumulative): 21 | DMM Complexity: 0.9117647058823529\n\nDIFF:\n@@ -502,6 +502,16 @@ def preprocess_weights_for_loading(layer, weights,\n                                                           original_backend)\n         return forward_weights + backward_weights\n \n+    def convert_nested_time_distributed(weights):\n+        \"\"\"Converts layers nested in `TimeDistributed` wrapper by `preprocess_weights_for_loading()`.\n+\n+        # Arguments\n+            weights: List of weights values (Numpy arrays).\n+        # Returns\n+            A list of weights values (Numpy arrays).\n+        \"\"\"\n+        return preprocess_weights_for_loading(layer.layer, weights, original_keras_version, original_backend)\n+\n     def convert_nested_model(weights):\n         \"\"\"Converts layers nested in `Model` or `Sequential` by `preprocess_weights_for_loading()`.\n \n@@ -535,11 +545,13 @@ def preprocess_weights_for_loading(layer, weights,\n                 weights = weights[num_weights:]\n         return new_weights\n \n-    # Convert layers nested in Bidirectional/Model/Sequential.\n+    # Convert layers nested in Bidirectional/TimeDistributed/Model/Sequential.\n     # Both transformation should be ran for both Keras 1->2 conversion\n     # and for conversion of CuDNN layers.\n     if layer.__class__.__name__ == 'Bidirectional':\n         weights = convert_nested_bidirectional(weights)\n+    if layer.__class__.__name__ == 'TimeDistributed':\n+        weights = convert_nested_time_distributed(weights)\n     elif layer.__class__.__name__ in ['Model', 'Sequential']:\n         weights = convert_nested_model(weights)\n \n\n@@ -644,12 +644,6 @@ def test_load_weights_between_noncudnn_rnn(rnn_type, to_cudnn, bidirectional, im\n         cudnn_rnn_layer_class = CuDNNGRU\n         rnn_layer_kwargs['reset_after'] = True\n \n-    def convert_model(source_model, target_model):\n-        _, fname = tempfile.mkstemp('.h5')\n-        source_model.save_weights(fname)\n-        target_model.load_weights(fname)\n-        os.remove(fname)\n-\n     layer = rnn_layer_class(units, **rnn_layer_kwargs)\n     if bidirectional:\n         layer = Bidirectional(layer)\n@@ -662,9 +656,9 @@ def test_load_weights_between_noncudnn_rnn(rnn_type, to_cudnn, bidirectional, im\n     cudnn_model = _make_nested_model(input_shape, cudnn_layer, model_nest_level, model_type)\n \n     if to_cudnn:\n-        convert_model(model, cudnn_model)\n+        _convert_model_weights(model, cudnn_model)\n     else:\n-        convert_model(cudnn_model, model)\n+        _convert_model_weights(cudnn_model, model)\n \n     assert_allclose(model.predict(inputs), cudnn_model.predict(inputs), atol=1e-4)\n \n@@ -692,6 +686,60 @@ def _make_nested_model(input_shape, layer, level=1, model_type='func'):\n         return make_nested_seq_model(input_shape, layer, level)\n \n \n+def _convert_model_weights(source_model, target_model):\n+    _, fname = tempfile.mkstemp('.h5')\n+    source_model.save_weights(fname)\n+    target_model.load_weights(fname)\n+    os.remove(fname)\n+\n+\n+@keras_test\n+@pytest.mark.parametrize('to_cudnn', [False, True], ids=['from_cudnn', 'to_cudnn'])\n+@pytest.mark.parametrize('rnn_type', ['LSTM', 'GRU'], ids=['LSTM', 'GRU'])\n+@skipif_no_tf_gpu\n+def test_load_weights_between_noncudnn_rnn_time_distributed(rnn_type, to_cudnn):\n+    \"\"\"\n+    Similar test as  test_load_weights_between_noncudnn_rnn() but has different\n+    rank of input due to usage of TimeDistributed. Issue: #10356.\n+    \"\"\"\n+    input_size = 10\n+    steps = 6\n+    timesteps = 6\n+    input_shape = (timesteps, steps, input_size)\n+    units = 2\n+    num_samples = 32\n+    inputs = np.random.random((num_samples,) + input_shape)\n+\n+    rnn_layer_kwargs = {\n+        'recurrent_activation': 'sigmoid',\n+        # ensure biases are non-zero and properly converted\n+        'bias_initializer': 'random_uniform',\n+    }\n+    if rnn_type == 'LSTM':\n+        rnn_layer_class = LSTM\n+        cudnn_rnn_layer_class = CuDNNLSTM\n+    else:\n+        rnn_layer_class = GRU\n+        cudnn_rnn_layer_class = CuDNNGRU\n+        rnn_layer_kwargs['reset_after'] = True\n+\n+    layer = rnn_layer_class(units, **rnn_layer_kwargs)\n+    layer = TimeDistributed(layer)\n+\n+    cudnn_layer = cudnn_rnn_layer_class(units)\n+    cudnn_layer = TimeDistributed(cudnn_layer)\n+\n+    model = _make_nested_model(input_shape, layer)\n+    cudnn_model = _make_nested_model(input_shape, cudnn_layer)\n+\n+    if to_cudnn:\n+        _convert_model_weights(model, cudnn_model)\n+    else:\n+        _convert_model_weights(cudnn_model, model)\n+\n+    assert_allclose(model.predict(inputs), cudnn_model.predict(inputs), atol=1e-4)\n+\n+\n @skipif_no_tf_gpu\n def test_preprocess_weights_for_loading_gru_incompatible():\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#632560d91286bf278228de72e7ce64f6c5aa530c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1121 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -510,7 +510,8 @@ def preprocess_weights_for_loading(layer, weights,\n         # Returns\n             A list of weights values (Numpy arrays).\n         \"\"\"\n-        return preprocess_weights_for_loading(layer.layer, weights, original_keras_version, original_backend)\n+        return preprocess_weights_for_loading(\n+            layer.layer, weights, original_keras_version, original_backend)\n \n     def convert_nested_model(weights):\n         \"\"\"Converts layers nested in `Model` or `Sequential` by `preprocess_weights_for_loading()`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
