{"custom_id": "keras#5148978589b67888418ba0515f327b5fdee7b5ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 35 | Files Changed: 3 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 38 | Churn Cumulative: 3927 | Contributors (this commit): 7 | Commits (past 90d): 60 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -375,12 +375,6 @@ PYTHON_DTYPES_MAP = {\n     str: \"string\",\n }\n \n-PYTHON_DTYPES_MAP = {\n-    bool: \"bool\",\n-    int: \"int\",  # TBD by backend\n-    float: \"float32\",\n-}\n-\n \n def standardize_dtype(dtype):\n     if dtype is None:\n\n@@ -1,16 +1,8 @@\n import torch\n \n-from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import to_torch_dtype\n \n-TORCH_INT_TYPES = (\n-    torch.int8,\n-    torch.int16,\n-    torch.int32,\n-    torch.int64,\n-)\n-\n \n def add(x1, x2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n@@ -39,8 +31,6 @@ def multiply(x1, x2):\n \n def mean(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    # Conversion to float necessary for `torch.mean`\n-    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     return torch.mean(x, axis=axis, keepdims=keepdims)\n \n \n@@ -182,8 +172,6 @@ def array(x, dtype=None):\n \n def average(x, axis=None, weights=None):\n     x = convert_to_tensor(x)\n-    # Conversion to float necessary for `torch.mean`\n-    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     if weights is not None:\n         weights = convert_to_tensor(weights)\n         return torch.sum(torch.mul(x, weights), dim=axis) / torch.sum(\n@@ -390,10 +378,6 @@ def imag(x):\n \n def isclose(x1, x2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n-    if torch.is_floating_point(x1) and not torch.is_floating_point(x2):\n-        x2 = cast(x2, x1.dtype)\n-    if torch.is_floating_point(x2) and not torch.is_floating_point(x1):\n-        x1 = cast(x1, x2.dtype)\n     return torch.isclose(x1, x2)\n \n \n@@ -466,8 +450,6 @@ def log2(x):\n \n def logaddexp(x1, x2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n-    x1 = cast(x1, \"float32\") if x1.dtype in TORCH_INT_TYPES else x1\n-    x2 = cast(x2, \"float32\") if x2.dtype in TORCH_INT_TYPES else x2\n     return torch.logaddexp(x1, x2)\n \n \n@@ -603,8 +585,7 @@ def ravel(x):\n \n \n def real(x):\n-    if not isinstance(x, torch.Tensor):\n-        x = torch.from_numpy(x)  # needed for complex type conversion\n+    x = convert_to_tensor(x)\n     return torch.real(x)\n \n \n@@ -665,8 +646,6 @@ def stack(x, axis=0):\n \n def std(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    # Conversion to float necessary for `torch.std`\n-    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     # Remove Bessel correction to align with numpy\n     return torch.std(x, dim=axis, keepdim=keepdims, unbiased=False)\n \n@@ -697,9 +676,6 @@ def tan(x):\n \n def tensordot(x1, x2, axes=2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n-    # Conversion to long necessary for `torch.tensordot`\n-    x1 = cast(x1, \"int64\") if x1.dtype in TORCH_INT_TYPES else x1\n-    x2 = cast(x2, \"int64\") if x2.dtype in TORCH_INT_TYPES else x2\n     return torch.tensordot(x1, x2, dims=axes)\n \n \n@@ -801,9 +777,7 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n-    x = convert_to_tensor(x, dtype=\"float32\")\n-    # Conversion to float necessary for `torch.var`\n-    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n+    x = convert_to_tensor(x)\n     # Bessel correction removed for numpy compatibility\n     return torch.var(x, dim=axis, keepdim=keepdims, correction=0)\n \n\n@@ -21,7 +21,7 @@ class Trainer:\n     @tracking.no_automatic_dependency_tracking\n     def compile(\n         self,\n-        optimizer,\n+        optimizer=\"rmsprop\",\n         loss=None,\n         loss_weights=None,\n         metrics=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e23c55d26d3b1e51f9b84913652f2744c8a48d02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 36 | Churn Cumulative: 2878 | Contributors (this commit): 4 | Commits (past 90d): 28 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -375,6 +375,12 @@ PYTHON_DTYPES_MAP = {\n     str: \"string\",\n }\n \n+PYTHON_DTYPES_MAP = {\n+    bool: \"bool\",\n+    int: \"int\",  # TBD by backend\n+    float: \"float32\",\n+}\n+\n \n def standardize_dtype(dtype):\n     if dtype is None:\n\n@@ -1,8 +1,16 @@\n import torch\n \n+from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import to_torch_dtype\n \n+TORCH_INT_TYPES = (\n+    torch.int8,\n+    torch.int16,\n+    torch.int32,\n+    torch.int64,\n+)\n+\n \n def add(x1, x2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n@@ -31,6 +39,8 @@ def multiply(x1, x2):\n \n def mean(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n+    # Conversion to float necessary for `torch.mean`\n+    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     return torch.mean(x, axis=axis, keepdims=keepdims)\n \n \n@@ -172,6 +182,8 @@ def array(x, dtype=None):\n \n def average(x, axis=None, weights=None):\n     x = convert_to_tensor(x)\n+    # Conversion to float necessary for `torch.mean`\n+    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     if weights is not None:\n         weights = convert_to_tensor(weights)\n         return torch.sum(torch.mul(x, weights), dim=axis) / torch.sum(\n@@ -378,6 +390,10 @@ def imag(x):\n \n def isclose(x1, x2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    if torch.is_floating_point(x1) and not torch.is_floating_point(x2):\n+        x2 = cast(x2, x1.dtype)\n+    if torch.is_floating_point(x2) and not torch.is_floating_point(x1):\n+        x1 = cast(x1, x2.dtype)\n     return torch.isclose(x1, x2)\n \n \n@@ -450,6 +466,8 @@ def log2(x):\n \n def logaddexp(x1, x2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    x1 = cast(x1, \"float32\") if x1.dtype in TORCH_INT_TYPES else x1\n+    x2 = cast(x2, \"float32\") if x2.dtype in TORCH_INT_TYPES else x2\n     return torch.logaddexp(x1, x2)\n \n \n@@ -585,7 +603,8 @@ def ravel(x):\n \n \n def real(x):\n-    x = convert_to_tensor(x)\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.from_numpy(x)  # needed for complex type conversion\n     return torch.real(x)\n \n \n@@ -646,6 +665,8 @@ def stack(x, axis=0):\n \n def std(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n+    # Conversion to float necessary for `torch.std`\n+    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     # Remove Bessel correction to align with numpy\n     return torch.std(x, dim=axis, keepdim=keepdims, unbiased=False)\n \n@@ -676,6 +697,9 @@ def tan(x):\n \n def tensordot(x1, x2, axes=2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n+    # Conversion to long necessary for `torch.tensordot`\n+    x1 = cast(x1, \"int64\") if x1.dtype in TORCH_INT_TYPES else x1\n+    x2 = cast(x2, \"int64\") if x2.dtype in TORCH_INT_TYPES else x2\n     return torch.tensordot(x1, x2, dims=axes)\n \n \n@@ -777,7 +801,9 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n-    x = convert_to_tensor(x)\n+    x = convert_to_tensor(x, dtype=\"float32\")\n+    # Conversion to float necessary for `torch.var`\n+    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     # Bessel correction removed for numpy compatibility\n     return torch.var(x, dim=axis, keepdim=keepdims, correction=0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#931e008683c04ccfff5139185eab5fd5c6e4d5de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 198 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import os\n+\n import numpy as np\n import pytest\n from absl.testing import parameterized\n@@ -101,6 +103,10 @@ def _get_elephant(target_size):\n     return np.expand_dims(x, axis=0)\n \n \n+@pytest.mark.skipif(\n+    os.environ.get(\"SKIP_APPLICATIONS_TESTS\"),\n+    reason=\"Env variable set to skip.\",\n+)\n class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(MODEL_LIST)\n     def test_application_notop_variable_input_channels(self, app, last_dim, _):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d87dc0981cb47062675818da46b605f1626962e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 141 | Lines Deleted: 42 | Files Changed: 6 | Hunks: 31 | Methods Changed: 15 | Complexity Δ (Sum/Max): 13/4 | Churn Δ: 183 | Churn Cumulative: 5863 | Contributors (this commit): 9 | Commits (past 90d): 80 | Contributors (cumulative): 19 | DMM Complexity: 0.6268656716417911\n\nDIFF:\n@@ -106,8 +106,9 @@ class KerasVariable:\n                 \"The shape of the target variable and \"\n                 \"the shape of the target value in \"\n                 \"`variable.assign(value)` must match. \"\n-                f\"Received: value.shape={value.shape}; \"\n-                f\"variable.shape={self.value.shape}\"\n+                f\"variable.shape={self.value.shape}, \"\n+                f\"Received: value.shape={value.shape}. \"\n+                f\"Target variable: {self}\"\n             )\n         if in_stateless_scope():\n             scope = get_stateless_scope()\n\n@@ -222,24 +222,27 @@ class JAXTrainer(base_trainer.Trainer):\n                     metrics_variables,\n                 ) = state\n \n+                # Update `iterations` var (necessary to make LR schedules work)\n+                # It's always the first variable tracked by the optimizer.\n+                self.optimizer.iterations.assign(optimizer_variables[0])\n+                # Setting _jax_state enables callbacks to force a state sync\n+                # if they need to.\n+                self._jax_state = {\n+                    \"trainable_variables\": trainable_variables,\n+                    \"non_trainable_variables\": non_trainable_variables,\n+                    \"optimizer_variables\": optimizer_variables,\n+                    \"metrics_variables\": metrics_variables,\n+                }\n+\n                 # Callbacks\n                 callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n                     break\n \n-            # Update variable values\n+            # Reattach state to model variables.\n             # NOTE: doing this after each step would be a big performance\n             # bottleneck.\n-            for ref_v, v in zip(self.trainable_variables, trainable_variables):\n-                ref_v.assign(v)\n-            for ref_v, v in zip(\n-                self.non_trainable_variables, non_trainable_variables\n-            ):\n-                ref_v.assign(v)\n-            for ref_v, v in zip(self.optimizer.variables, optimizer_variables):\n-                ref_v.assign(v)\n-            for ref_v, v in zip(self.metrics_variables, metrics_variables):\n-                ref_v.assign(v)\n+            self.jax_state_sync()\n \n             # Override with model metrics instead of last step logs\n             epoch_logs = self._pythonify_logs(self.get_metrics_result())\n@@ -434,16 +437,19 @@ class JAXTrainer(base_trainer.Trainer):\n             # immutable here.\n             _, non_trainable_variables, metrics_variables = state\n \n-            callbacks.on_test_batch_end(step, logs)\n-\n-        for ref_v, v in zip(\n-            self.non_trainable_variables, non_trainable_variables\n-        ):\n+            # Setting _jax_state enables callbacks to force a state sync\n+            # if they need to.\n+            self._jax_state = {\n                 # I wouldn't recommend modifying non-trainable model state\n                 # during evaluate(), but it's allowed.\n-            ref_v.assign(v)\n-        for ref_v, v in zip(self.metrics_variables, metrics_variables):\n-            ref_v.assign(v)\n+                \"non_trainable_variables\": non_trainable_variables,\n+                \"metrics_variables\": metrics_variables,\n+            }\n+            callbacks.on_test_batch_end(step, logs)\n+\n+        # Reattach state back to model.\n+        self.jax_state_sync()\n+\n         logs = self._pythonify_logs(self.get_metrics_result())\n         callbacks.on_test_end(logs)\n \n@@ -557,3 +563,28 @@ class JAXTrainer(base_trainer.Trainer):\n         return tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, np.concatenate, outputs\n         )\n+\n+    def jax_state_sync(self):\n+        if not hasattr(self, \"_jax_state\"):\n+            return\n+\n+        trainable_variables = self._jax_state.get(\"trainable_variables\", None)\n+        non_trainable_variables = self._jax_state.get(\n+            \"non_trainable_variables\", None\n+        )\n+        optimizer_variables = self._jax_state.get(\"optimizer_variables\", None)\n+        metrics_variables = self._jax_state.get(\"metrics_variables\", None)\n+        if trainable_variables:\n+            for ref_v, v in zip(self.trainable_variables, trainable_variables):\n+                ref_v.assign(v)\n+        if non_trainable_variables:\n+            for ref_v, v in zip(\n+                self.non_trainable_variables, non_trainable_variables\n+            ):\n+                ref_v.assign(v)\n+        if optimizer_variables:\n+            for ref_v, v in zip(self.optimizer.variables, optimizer_variables):\n+                ref_v.assign(v)\n+        if metrics_variables:\n+            for ref_v, v in zip(self.metrics_variables, metrics_variables):\n+                ref_v.assign(v)\n\n@@ -8,8 +8,8 @@ from keras_core import initializers\n from keras_core import operations as ops\n from keras_core.optimizers.schedules import learning_rate_schedule\n from keras_core.saving import serialization_lib\n+from keras_core.utils import tracking\n from keras_core.utils.naming import auto_name\n-from keras_core.utils.tracking import Tracker\n \n \n class Optimizer:\n@@ -25,6 +25,8 @@ class Optimizer:\n         ema_overwrite_frequency=None,\n         name=None,\n     ):\n+        self._lock = False\n+\n         self.name = name\n         self.weight_decay = weight_decay\n         self.clipnorm = clipnorm\n@@ -57,13 +59,31 @@ class Optimizer:\n                 f\"be set. Received: clipnorm={self.clipnorm}, \"\n                 f\"global_clipnorm={self.global_clipnorm}\"\n             )\n-\n         self.built = False\n+\n+        # Set up variable tracking.\n+        self._variables = []\n+        self._trainable_variables = []\n+        self._tracker = tracking.Tracker(\n+            {\n+                \"variables\": (\n+                    lambda x: isinstance(x, backend.Variable),\n+                    self._variables,\n+                ),\n+            }\n+        )\n+        self._trainable_variables_indices = {}\n+\n+        # Create iteration variable\n         # Note: dtype=\"int\" will resolve to int32 in JAX\n         # (since int64 is disallowed in JAX) and to int64 in TF.\n-        self.iterations = backend.Variable(\n+        iterations = backend.Variable(\n             0, name=\"iteration\", dtype=\"int\", trainable=False\n         )\n+        self._track_variable(iterations)\n+        self.iterations = iterations\n+\n+        # Create learning rate (schedule or variable)\n         if isinstance(\n             learning_rate, learning_rate_schedule.LearningRateSchedule\n         ):\n@@ -79,24 +99,21 @@ class Optimizer:\n                     \"and returns the corresponding learning rate value). \"\n                     f\"Received instead: learning_rate={learning_rate}\"\n                 )\n-            self._learning_rate = backend.Variable(\n+            learning_rate = backend.Variable(\n                 learning_rate,\n                 name=\"learning_rate\",\n                 dtype=backend.floatx(),\n                 trainable=False,\n             )\n-        self._variables = []\n-        self._trainable_variables = []\n-        self._tracker = Tracker(\n-            {\n-                \"variables\": (\n-                    lambda x: isinstance(x, backend.Variable),\n-                    self._variables,\n-                ),\n-            }\n-        )\n-        self._trainable_variables_indices = {}\n+            self._track_variable(learning_rate)\n+            self._learning_rate = learning_rate\n \n+    def _track_variable(self, variable):\n+        self._variables.append(variable)\n+        # Prevent double-tracking\n+        self._tracker.stored_ids[\"variables\"].add(id(variable))\n+\n+    @tracking.no_automatic_dependency_tracking\n     def build(self, variables):\n         for i, variable in enumerate(variables):\n             self._trainable_variables_indices[self._var_key(variable)] = i\n@@ -121,6 +138,7 @@ class Optimizer:\n         dtype=None,\n         name=None,\n     ):\n+        print(\"add variable\", shape)\n         self._check_super_called()\n         initializer = initializers.get(initializer)\n         variable = backend.Variable(\n@@ -130,9 +148,7 @@ class Optimizer:\n             trainable=False,\n             name=name,\n         )\n-        self._variables.append(variable)\n-        # Prevent double-tracking\n-        self._tracker.stored_ids[\"variables\"].add(self._var_key(variable))\n+        self._track_variable(variable)\n         return variable\n \n     def add_variable_from_reference(self, reference_variable, name=None):\n@@ -436,10 +452,11 @@ class Optimizer:\n                 variable.assign(variable - variable * wd * lr)\n \n     def _check_super_called(self):\n-        if not hasattr(self, \"_tracker\"):\n+        if not hasattr(self, \"_lock\"):\n             raise RuntimeError(\n                 f\"In optimizer '{self.__class__.__name__}', you forgot to call \"\n-                \"`super().__init__()` in the `__init__()` method. \"\n+                \"`super().__init__()` as the first statement \"\n+                \"in the `__init__()` method. \"\n                 \"Go add it!\"\n             )\n \n@@ -554,6 +571,17 @@ class Optimizer:\n                 )\n         return cls(**config)\n \n+    def __setattr__(self, name, value):\n+        # Prevent users from attaching state to the\n+        # layer before `super()` is called -- since that\n+        # state would silently not be tracked.\n+        if name != \"_lock\":\n+            self._check_super_called()\n+        # Track Variables.\n+        if hasattr(self, \"_tracker\"):\n+            value = self._tracker.track(value)\n+        return super().__setattr__(name, value)\n+\n \n base_optimizer_keyword_args = \"\"\"name: String. The name to use\n           for momentum accumulator weights created by\n\n@@ -5,10 +5,42 @@ import math\n import numpy as np\n \n from keras_core import backend\n+from keras_core import callbacks\n+from keras_core import layers\n+from keras_core import optimizers\n from keras_core import testing\n+from keras_core.models import Sequential\n from keras_core.optimizers import schedules\n \n \n+class TestFitLRSchedulesFlow(testing.TestCase):\n+    def test_fit_lr_correctness(self):\n+        model = Sequential([layers.Dense(3)])\n+        optimizer = optimizers.Adam(\n+            learning_rate=schedules.ExponentialDecay(\n+                initial_learning_rate=0.05, decay_steps=1, decay_rate=0.9\n+            )\n+        )\n+        self.assertEqual(len(optimizer.variables), 1)\n+        self.assertEqual(optimizer.variables[0], 0)\n+\n+        class LRTracker(callbacks.Callback):\n+            def __init__(self, optimizer):\n+                self.optimizer = optimizer\n+                self.logs = []\n+\n+            def on_batch_end(self, *args, **kwargs):\n+                self.logs.append(float(self.optimizer.learning_rate))\n+\n+        model.compile(optimizer=optimizer, loss=\"mse\")\n+        x = np.random.random((16, 2))\n+        y = np.random.random((16, 3))\n+        tracker = LRTracker(optimizer)\n+        model.fit(x, y, epochs=1, batch_size=4, callbacks=[tracker])\n+        self.assertEqual(optimizer.variables[0], 4)\n+        self.assertAllClose(tracker.logs, [0.045, 0.0405, 0.03645, 0.032805])\n+\n+\n class ExponentialDecayTest(testing.TestCase):\n     def test_config(self):\n         self.run_class_serialization_test(\n\n@@ -83,6 +83,7 @@ class SGD(optimizer.Optimizer):\n             return\n         super().build(variables)\n         self.momentums = []\n+        if self.momentum != 0:\n             for variable in variables:\n                 self.momentums.append(\n                     self.add_variable_from_reference(\n@@ -95,10 +96,11 @@ class SGD(optimizer.Optimizer):\n         learning_rate = ops.cast(learning_rate, variable.dtype)\n         gradient = ops.cast(gradient, variable.dtype)\n         m = None\n-        momentum = ops.cast(self.momentum, variable.dtype)\n+        if self.momentum != 0:\n             m = self.momentums[self._get_variable_index(variable)]\n \n         if m is not None:\n+            momentum = ops.cast(self.momentum, variable.dtype)\n             m.assign(-gradient * learning_rate + m * momentum)\n             if self.nesterov:\n                 variable.assign(\n\n@@ -20,10 +20,15 @@ class SGDTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n+        self.assertEqual(len(optimizer.variables), 2)\n         grads = np.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(vars, [0.5, -1.0, -0.5, 3.0], rtol=1e-4, atol=1e-4)\n+        self.assertEqual(len(optimizer.variables), 2)\n+        self.assertEqual(optimizer.variables[0], 1)\n+        self.assertEqual(optimizer.variables[1], 0.5)\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2ad0e7cd2f8508892d2d8a7b867f28a91f3faf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 38 | Files Changed: 2 | Hunks: 17 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 83 | Churn Cumulative: 467 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,8 +75,8 @@ class BatchNormalization(Layer):\n             - `training=True`: The layer will normalize its inputs using\n             the mean and variance of the current batch of inputs.\n             - `training=False`: The layer will normalize its inputs using\n-            the mean and variance of its moving statistics,\n-            learned during training.\n+            the mean and variance of its moving statistics, learned during\n+            training.\n \n     Reference:\n \n@@ -102,11 +102,10 @@ class BatchNormalization(Layer):\n \n     Note that:\n \n-    - Setting `trainable` on an model containing other layers will\n-    recursively set the `trainable` value of all inner layers.\n-    - If the value of the `trainable`\n-    attribute is changed after calling `compile()` on a model,\n-    the new value doesn't take effect for this model\n+    - Setting `trainable` on an model containing other layers will recursively\n+        set the `trainable` value of all inner layers.\n+    - If the value of the `trainable` attribute is changed after calling\n+        `compile()` on a model, the new value doesn't take effect for this model\n         until `compile()` is called again.\n     \"\"\"\n \n@@ -189,26 +188,29 @@ class BatchNormalization(Layer):\n         return input_shape\n \n     def call(self, inputs, training=None, mask=None):\n-        # TODO: support masking during stats computation.\n+        broadcast_shape = [1] * len(inputs.shape)\n+        broadcast_shape[self.axis] = inputs.shape[self.axis]\n         if training and self.trainable:\n-            mean = ops.mean(inputs, axis=self._reduction_axes)\n-            variance = ops.var(inputs, axis=self._reduction_axes)\n+            mean = ops.mean(inputs, axis=self._reduction_axes, keepdims=True)\n+            variance = ops.var(inputs, axis=self._reduction_axes, keepdims=True)\n             outputs = (inputs - mean) / ops.sqrt(variance + self.epsilon)\n-            self.moving_variance.assign(\n-                self.moving_variance * self.momentum\n-                + variance * (1.0 - self.momentum)\n-            )\n+            mean = ops.squeeze(mean, self._reduction_axes)\n+            variance = ops.squeeze(variance, self._reduction_axes)\n             self.moving_mean.assign(\n                 self.moving_mean * self.momentum + mean * (1.0 - self.momentum)\n             )\n         else:\n-            outputs = (inputs - self.moving_mean) / ops.sqrt(\n-                self.moving_variance + self.epsilon\n+            moving_mean = ops.reshape(self.moving_mean, broadcast_shape)\n+            moving_variance = ops.reshape(self.moving_variance, broadcast_shape)\n+            outputs = (inputs - moving_mean) / ops.sqrt(\n+                moving_variance + self.epsilon\n             )\n         if self.scale:\n-            outputs = outputs * self.gamma\n+            gamma = ops.reshape(self.gamma, broadcast_shape)\n+            outputs = outputs * gamma\n         if self.center:\n-            outputs = outputs + self.beta\n+            beta = ops.reshape(self.beta, broadcast_shape)\n+            outputs = outputs + beta\n         return outputs\n \n     def get_config(self):\n\n@@ -1,10 +1,11 @@\n import numpy as np\n+from absl.testing import parameterized\n \n from keras_core import layers\n from keras_core import testing\n \n \n-class BatchNormalizationTest(testing.TestCase):\n+class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n     def test_bn_basics(self):\n         # vector case\n         self.run_layer_test(\n@@ -56,27 +57,35 @@ class BatchNormalizationTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    def test_correctness(self):\n+    @parameterized.product(\n+        axis=(-1, 1),\n+        input_shape=((5, 2, 3), (5, 3, 3, 2)),\n+    )\n+    def test_correctness(self, axis, input_shape):\n         # Training\n-        layer = layers.BatchNormalization(axis=-1, momentum=0.8)\n+        layer = layers.BatchNormalization(axis=axis, momentum=0)\n         # Random data centered on 5.0, variance 10.0\n-        x = np.random.normal(loc=5.0, scale=10.0, size=(200, 4, 4, 3))\n-        for _ in range(10):\n-            out = layer(x, training=True)\n+        x = np.random.normal(loc=5.0, scale=10.0, size=input_shape)\n+        out = x\n+        for _ in range(3):\n+            out = layer(out, training=True)\n \n-        out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n-        out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n+        # Assert the normalization is correct.\n+        broadcast_shape = [1] * len(input_shape)\n+        broadcast_shape[axis] = input_shape[axis]\n+        out -= np.reshape(np.array(layer.beta), broadcast_shape)\n+        out /= np.reshape(np.array(layer.gamma), broadcast_shape)\n \n-        self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n-        self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n+        reduction_axes = list(range(len(input_shape)))\n+        del reduction_axes[axis]\n+        reduction_axes = tuple(reduction_axes)\n+        self.assertAllClose(np.mean(out, axis=reduction_axes), 0.0, atol=1e-3)\n+        self.assertAllClose(np.std(out, axis=reduction_axes), 1.0, atol=1e-3)\n \n         # Inference\n-        out = layer(x, training=False)\n-        out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n-        out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n-\n-        self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n-        self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n+        inference_out = layer(x, training=False)\n+        training_out = layer(x, training=True)\n+        self.assertNotAllClose(inference_out, training_out)\n \n     def test_trainable_behavior(self):\n         layer = layers.BatchNormalization(axis=-1, momentum=0.8, epsilon=1e-7)\n@@ -105,7 +114,3 @@ class BatchNormalizationTest(testing.TestCase):\n \n         self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n         self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n-\n-    def test_masking_correctness(self):\n-        # TODO\n-        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#faf3711ee622f678aff81647353b2a902778bab1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 38 | Files Changed: 2 | Hunks: 17 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 83 | Churn Cumulative: 633 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,8 +75,8 @@ class BatchNormalization(Layer):\n             - `training=True`: The layer will normalize its inputs using\n             the mean and variance of the current batch of inputs.\n             - `training=False`: The layer will normalize its inputs using\n-            the mean and variance of its moving statistics,\n-            learned during training.\n+            the mean and variance of its moving statistics, learned during\n+            training.\n \n     Reference:\n \n@@ -102,11 +102,10 @@ class BatchNormalization(Layer):\n \n     Note that:\n \n-    - Setting `trainable` on an model containing other layers will\n-    recursively set the `trainable` value of all inner layers.\n-    - If the value of the `trainable`\n-    attribute is changed after calling `compile()` on a model,\n-    the new value doesn't take effect for this model\n+    - Setting `trainable` on an model containing other layers will recursively\n+        set the `trainable` value of all inner layers.\n+    - If the value of the `trainable` attribute is changed after calling\n+        `compile()` on a model, the new value doesn't take effect for this model\n         until `compile()` is called again.\n     \"\"\"\n \n@@ -189,26 +188,29 @@ class BatchNormalization(Layer):\n         return input_shape\n \n     def call(self, inputs, training=None, mask=None):\n-        # TODO: support masking during stats computation.\n+        broadcast_shape = [1] * len(inputs.shape)\n+        broadcast_shape[self.axis] = inputs.shape[self.axis]\n         if training and self.trainable:\n-            mean = ops.mean(inputs, axis=self._reduction_axes)\n-            variance = ops.var(inputs, axis=self._reduction_axes)\n+            mean = ops.mean(inputs, axis=self._reduction_axes, keepdims=True)\n+            variance = ops.var(inputs, axis=self._reduction_axes, keepdims=True)\n             outputs = (inputs - mean) / ops.sqrt(variance + self.epsilon)\n-            self.moving_variance.assign(\n-                self.moving_variance * self.momentum\n-                + variance * (1.0 - self.momentum)\n-            )\n+            mean = ops.squeeze(mean, self._reduction_axes)\n+            variance = ops.squeeze(variance, self._reduction_axes)\n             self.moving_mean.assign(\n                 self.moving_mean * self.momentum + mean * (1.0 - self.momentum)\n             )\n         else:\n-            outputs = (inputs - self.moving_mean) / ops.sqrt(\n-                self.moving_variance + self.epsilon\n+            moving_mean = ops.reshape(self.moving_mean, broadcast_shape)\n+            moving_variance = ops.reshape(self.moving_variance, broadcast_shape)\n+            outputs = (inputs - moving_mean) / ops.sqrt(\n+                moving_variance + self.epsilon\n             )\n         if self.scale:\n-            outputs = outputs * self.gamma\n+            gamma = ops.reshape(self.gamma, broadcast_shape)\n+            outputs = outputs * gamma\n         if self.center:\n-            outputs = outputs + self.beta\n+            beta = ops.reshape(self.beta, broadcast_shape)\n+            outputs = outputs + beta\n         return outputs\n \n     def get_config(self):\n\n@@ -1,10 +1,11 @@\n import numpy as np\n+from absl.testing import parameterized\n \n from keras_core import layers\n from keras_core import testing\n \n \n-class BatchNormalizationTest(testing.TestCase):\n+class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n     def test_bn_basics(self):\n         # vector case\n         self.run_layer_test(\n@@ -56,27 +57,35 @@ class BatchNormalizationTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    def test_correctness(self):\n+    @parameterized.product(\n+        axis=(-1, 1),\n+        input_shape=((5, 2, 3), (5, 3, 3, 2)),\n+    )\n+    def test_correctness(self, axis, input_shape):\n         # Training\n-        layer = layers.BatchNormalization(axis=-1, momentum=0.8)\n+        layer = layers.BatchNormalization(axis=axis, momentum=0)\n         # Random data centered on 5.0, variance 10.0\n-        x = np.random.normal(loc=5.0, scale=10.0, size=(200, 4, 4, 3))\n-        for _ in range(10):\n-            out = layer(x, training=True)\n+        x = np.random.normal(loc=5.0, scale=10.0, size=input_shape)\n+        out = x\n+        for _ in range(3):\n+            out = layer(out, training=True)\n \n-        out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n-        out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n+        # Assert the normalization is correct.\n+        broadcast_shape = [1] * len(input_shape)\n+        broadcast_shape[axis] = input_shape[axis]\n+        out -= np.reshape(np.array(layer.beta), broadcast_shape)\n+        out /= np.reshape(np.array(layer.gamma), broadcast_shape)\n \n-        self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n-        self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n+        reduction_axes = list(range(len(input_shape)))\n+        del reduction_axes[axis]\n+        reduction_axes = tuple(reduction_axes)\n+        self.assertAllClose(np.mean(out, axis=reduction_axes), 0.0, atol=1e-3)\n+        self.assertAllClose(np.std(out, axis=reduction_axes), 1.0, atol=1e-3)\n \n         # Inference\n-        out = layer(x, training=False)\n-        out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n-        out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n-\n-        self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n-        self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n+        inference_out = layer(x, training=False)\n+        training_out = layer(x, training=True)\n+        self.assertNotAllClose(inference_out, training_out)\n \n     def test_trainable_behavior(self):\n         layer = layers.BatchNormalization(axis=-1, momentum=0.8, epsilon=1e-7)\n@@ -105,7 +114,3 @@ class BatchNormalizationTest(testing.TestCase):\n \n         self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n         self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n-\n-    def test_masking_correctness(self):\n-        # TODO\n-        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5c40f0def41df78e88cfbd9280bc473ddbc6f8c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 69 | Files Changed: 2 | Hunks: 17 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/7 | Churn Δ: 114 | Churn Cumulative: 7198 | Contributors (this commit): 5 | Commits (past 90d): 38 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -294,7 +294,7 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n \n def dot(x, y):\n     x, y = convert_to_tensor(x), convert_to_tensor(y)\n-    if len(x.shape) == 0 or len(y.shape) == 0:\n+    if x.ndim == 0 or y.ndim == 0:\n         return torch.multiply(x, y)\n     return torch.matmul(x, y)\n \n@@ -327,7 +327,7 @@ def expm1(x):\n def flip(x, axis=None):\n     x = convert_to_tensor(x)\n     if axis is None:\n-        axis = tuple(range(len(x.shape)))\n+        axis = tuple(range(x.ndim))\n     if isinstance(axis, int):\n         axis = (axis,)\n     return torch.flip(x, dims=axis)\n@@ -341,23 +341,21 @@ def floor(x):\n def full(shape, fill_value, dtype=None):\n     dtype = to_torch_dtype(dtype)\n     if hasattr(fill_value, \"__len__\"):\n-        raise NotImplementedError(\n-            \"`torch.full()` only accepts scalars for `fill_value`. \"\n-            f\"Received: fill_value={fill_value}\"\n-        )\n-        # TODO: implement conversion of shape into repetitions for `torch.tile``\n-        # return torch.tile(fill_value, reps)\n+        fill_value = convert_to_tensor(fill_value)\n+        reps = shape[-1] // fill_value.shape[-1]  # channels-last reduction\n+        reps_by_dim = (*shape[:-1], reps)\n+        return torch.tile(fill_value, reps_by_dim)\n     return torch.full(size=shape, fill_value=fill_value, dtype=dtype)\n \n \n def full_like(x, fill_value, dtype=None):\n     dtype = to_torch_dtype(dtype)\n     if hasattr(fill_value, \"__len__\"):\n-        raise NotImplementedError(\n-            \"`torch.full()` only accepts scalars for `fill_value`.\"\n+        fill_value = convert_to_tensor(fill_value)\n+        reps_by_dim = tuple(\n+            [x.shape[i] // fill_value.shape[i] for i in range(x.ndim)]\n         )\n-        # TODO: implement conversion of shape into repetitions for `torch.tile``\n-        # return torch.tile(fill_value, reps)\n+        return torch.tile(fill_value, reps_by_dim)\n     x = convert_to_tensor(x)\n     return torch.full_like(input=x, fill_value=fill_value, dtype=dtype)\n \n@@ -433,6 +431,18 @@ def linspace(\n     dtype = to_torch_dtype(dtype)\n     if endpoint is False:\n         stop = stop - ((stop - start) / num)\n+    if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n+        start, stop = convert_to_tensor(start), convert_to_tensor(stop)\n+        stop = cast(stop, dtype) if endpoint is False and dtype else stop\n+        steps = torch.arange(num, dtype=dtype) / (num - 1)\n+\n+        # reshape `steps` to allow for broadcasting\n+        for i in range(start.ndim):\n+            steps = steps.unsqueeze(-1)\n+\n+        # increments from `start` to `stop` in each dimension\n+        linspace = start[None] + steps * (stop - start)[None]\n+    else:\n         linspace = torch.linspace(\n             start=start,\n             end=stop,\n@@ -495,6 +505,19 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n     dtype = to_torch_dtype(dtype)\n     if endpoint is False:\n         stop = stop - ((stop - start) / num)\n+    if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n+        start, stop = convert_to_tensor(start), convert_to_tensor(stop)\n+        stop = cast(stop, dtype) if endpoint is False and dtype else stop\n+        steps = torch.arange(num, dtype=dtype) / (num - 1)\n+\n+        # reshape `steps` to allow for broadcasting\n+        for i in range(start.ndim):\n+            steps = steps.unsqueeze(-1)\n+\n+        # increments from `start` to `stop` in each dimension\n+        linspace = start[None] + steps * (stop - start)[None]\n+        logspace = base**linspace\n+    else:\n         logspace = torch.logspace(\n             start=start,\n             end=stop,\n@@ -715,14 +738,7 @@ def tile(x, repeats):\n \n def trace(x, offset=None, axis1=None, axis2=None):\n     x = convert_to_tensor(x)\n-    # TODO: implement support for these arguments\n-    # API divergence between `np.trace()` and `torch.trace()`\n-    if offset or axis1 or axis2:\n-        raise NotImplementedError(\n-            \"Arguments not supported by `torch.trace: \"\n-            f\"offset={offset}, axis1={axis1}, axis2={axis2}\"\n-        )\n-    return torch.trace(x)\n+    return torch.sum(torch.diagonal(x, offset, axis1, axis2), dim=-1)\n \n \n def tri(N, M=None, k=0, dtype=\"float32\"):\n\n@@ -1713,27 +1713,16 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n             np.array(knp.full_like(x, 2, dtype=\"float32\")),\n             np.full_like(x, 2, dtype=\"float32\"),\n         )\n+        self.assertAllClose(\n+            np.array(knp.full_like(x, np.ones([2, 3]))),\n+            np.full_like(x, np.ones([2, 3])),\n+        )\n \n         self.assertAllClose(np.array(knp.FullLike()(x, 2)), np.full_like(x, 2))\n         self.assertAllClose(\n             np.array(knp.FullLike()(x, 2, dtype=\"float32\")),\n             np.full_like(x, 2, dtype=\"float32\"),\n         )\n-\n-    # TODO: implement conversion of shape into repetitions, pass to\n-    # `torch.tile`, since `torch.full()` only accepts scalars\n-    # for `fill_value`.\"\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.full` only accepts scalars for `fill_value`.\",\n-    )\n-    def test_full_like_without_torch(self):\n-        x = np.array([[1, 2, 3], [3, 2, 1]])\n-        self.assertAllClose(\n-            np.array(knp.full_like(x, np.ones([2, 3]))),\n-            np.full_like(x, np.ones([2, 3])),\n-        )\n-\n         self.assertAllClose(\n             np.array(knp.FullLike()(x, np.ones([2, 3]))),\n             np.full_like(x, np.ones([2, 3])),\n@@ -1834,13 +1823,6 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n             np.linspace(0, 10, 5, endpoint=False),\n         )\n \n-    # TODO: torch.linspace does not support tensor or array\n-    # for start/stop, create manual implementation\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.linspace` has no support for array start/stop.\",\n-    )\n-    def test_linspace_without_torch(self):\n         start = np.zeros([2, 3, 4])\n         stop = np.ones([2, 3, 4])\n         self.assertAllClose(\n@@ -1945,15 +1927,9 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n             np.logspace(0, 10, 5, endpoint=False),\n         )\n \n-    # TODO: torch.logspace does not support tensor or array\n-    # for start/stop, create manual implementation\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.logspace` has no support for array start/stop.\",\n-    )\n-    def test_logspace_without_torch(self):\n         start = np.zeros([2, 3, 4])\n         stop = np.ones([2, 3, 4])\n+\n         self.assertAllClose(\n             np.array(knp.logspace(start, stop, 5, base=10)),\n             np.logspace(start, stop, 5, base=10),\n@@ -3016,13 +2992,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.tile(x, [2, 3])), np.tile(x, [2, 3]))\n         self.assertAllClose(np.array(knp.Tile([2, 3])(x)), np.tile(x, [2, 3]))\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.split` does not support args `offset`, `axis1`, `axis2`\",\n-    )\n     def test_trace(self):\n-        # TODO: implement `torch.trace` support for arguments `offset`,\n-        # `axis1`, `axis2` and delete NotImplementedError\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         self.assertAllClose(np.array(knp.trace(x)), np.trace(x))\n         self.assertAllClose(\n@@ -3092,25 +3062,15 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(\n             np.array(knp.full([2, 3], 0.1)), np.full([2, 3], 0.1)\n         )\n-\n-        self.assertAllClose(np.array(knp.Full()([2, 3], 0)), np.full([2, 3], 0))\n-        self.assertAllClose(\n-            np.array(knp.Full()([2, 3], 0.1)), np.full([2, 3], 0.1)\n-        )\n-\n-    # TODO: implement conversion of shape into repetitions, pass to\n-    # `torch.tile`, since `torch.full()` only accepts scalars\n-    # for `fill_value`.\"\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.full` only accepts scalars for `fill_value`.\",\n-    )\n-    def test_full_without_torch(self):\n         self.assertAllClose(\n             np.array(knp.full([2, 3], np.array([1, 4, 5]))),\n             np.full([2, 3], np.array([1, 4, 5])),\n         )\n \n+        self.assertAllClose(np.array(knp.Full()([2, 3], 0)), np.full([2, 3], 0))\n+        self.assertAllClose(\n+            np.array(knp.Full()([2, 3], 0.1)), np.full([2, 3], 0.1)\n+        )\n         self.assertAllClose(\n             np.array(knp.Full()([2, 3], np.array([1, 4, 5]))),\n             np.full([2, 3], np.array([1, 4, 5])),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#186cbf6b7ce1a0264f2ce8d499e81f06c095c23b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 69 | Files Changed: 2 | Hunks: 17 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/7 | Churn Δ: 114 | Churn Cumulative: 7426 | Contributors (this commit): 5 | Commits (past 90d): 42 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -294,7 +294,7 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n \n def dot(x, y):\n     x, y = convert_to_tensor(x), convert_to_tensor(y)\n-    if len(x.shape) == 0 or len(y.shape) == 0:\n+    if x.ndim == 0 or y.ndim == 0:\n         return torch.multiply(x, y)\n     return torch.matmul(x, y)\n \n@@ -327,7 +327,7 @@ def expm1(x):\n def flip(x, axis=None):\n     x = convert_to_tensor(x)\n     if axis is None:\n-        axis = tuple(range(len(x.shape)))\n+        axis = tuple(range(x.ndim))\n     if isinstance(axis, int):\n         axis = (axis,)\n     return torch.flip(x, dims=axis)\n@@ -341,23 +341,21 @@ def floor(x):\n def full(shape, fill_value, dtype=None):\n     dtype = to_torch_dtype(dtype)\n     if hasattr(fill_value, \"__len__\"):\n-        raise NotImplementedError(\n-            \"`torch.full()` only accepts scalars for `fill_value`. \"\n-            f\"Received: fill_value={fill_value}\"\n-        )\n-        # TODO: implement conversion of shape into repetitions for `torch.tile``\n-        # return torch.tile(fill_value, reps)\n+        fill_value = convert_to_tensor(fill_value)\n+        reps = shape[-1] // fill_value.shape[-1]  # channels-last reduction\n+        reps_by_dim = (*shape[:-1], reps)\n+        return torch.tile(fill_value, reps_by_dim)\n     return torch.full(size=shape, fill_value=fill_value, dtype=dtype)\n \n \n def full_like(x, fill_value, dtype=None):\n     dtype = to_torch_dtype(dtype)\n     if hasattr(fill_value, \"__len__\"):\n-        raise NotImplementedError(\n-            \"`torch.full()` only accepts scalars for `fill_value`.\"\n+        fill_value = convert_to_tensor(fill_value)\n+        reps_by_dim = tuple(\n+            [x.shape[i] // fill_value.shape[i] for i in range(x.ndim)]\n         )\n-        # TODO: implement conversion of shape into repetitions for `torch.tile``\n-        # return torch.tile(fill_value, reps)\n+        return torch.tile(fill_value, reps_by_dim)\n     x = convert_to_tensor(x)\n     return torch.full_like(input=x, fill_value=fill_value, dtype=dtype)\n \n@@ -433,6 +431,18 @@ def linspace(\n     dtype = to_torch_dtype(dtype)\n     if endpoint is False:\n         stop = stop - ((stop - start) / num)\n+    if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n+        start, stop = convert_to_tensor(start), convert_to_tensor(stop)\n+        stop = cast(stop, dtype) if endpoint is False and dtype else stop\n+        steps = torch.arange(num, dtype=dtype) / (num - 1)\n+\n+        # reshape `steps` to allow for broadcasting\n+        for i in range(start.ndim):\n+            steps = steps.unsqueeze(-1)\n+\n+        # increments from `start` to `stop` in each dimension\n+        linspace = start[None] + steps * (stop - start)[None]\n+    else:\n         linspace = torch.linspace(\n             start=start,\n             end=stop,\n@@ -495,6 +505,19 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n     dtype = to_torch_dtype(dtype)\n     if endpoint is False:\n         stop = stop - ((stop - start) / num)\n+    if hasattr(start, \"__len__\") and hasattr(stop, \"__len__\"):\n+        start, stop = convert_to_tensor(start), convert_to_tensor(stop)\n+        stop = cast(stop, dtype) if endpoint is False and dtype else stop\n+        steps = torch.arange(num, dtype=dtype) / (num - 1)\n+\n+        # reshape `steps` to allow for broadcasting\n+        for i in range(start.ndim):\n+            steps = steps.unsqueeze(-1)\n+\n+        # increments from `start` to `stop` in each dimension\n+        linspace = start[None] + steps * (stop - start)[None]\n+        logspace = base**linspace\n+    else:\n         logspace = torch.logspace(\n             start=start,\n             end=stop,\n@@ -715,14 +738,7 @@ def tile(x, repeats):\n \n def trace(x, offset=None, axis1=None, axis2=None):\n     x = convert_to_tensor(x)\n-    # TODO: implement support for these arguments\n-    # API divergence between `np.trace()` and `torch.trace()`\n-    if offset or axis1 or axis2:\n-        raise NotImplementedError(\n-            \"Arguments not supported by `torch.trace: \"\n-            f\"offset={offset}, axis1={axis1}, axis2={axis2}\"\n-        )\n-    return torch.trace(x)\n+    return torch.sum(torch.diagonal(x, offset, axis1, axis2), dim=-1)\n \n \n def tri(N, M=None, k=0, dtype=\"float32\"):\n\n@@ -1713,27 +1713,16 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n             np.array(knp.full_like(x, 2, dtype=\"float32\")),\n             np.full_like(x, 2, dtype=\"float32\"),\n         )\n+        self.assertAllClose(\n+            np.array(knp.full_like(x, np.ones([2, 3]))),\n+            np.full_like(x, np.ones([2, 3])),\n+        )\n \n         self.assertAllClose(np.array(knp.FullLike()(x, 2)), np.full_like(x, 2))\n         self.assertAllClose(\n             np.array(knp.FullLike()(x, 2, dtype=\"float32\")),\n             np.full_like(x, 2, dtype=\"float32\"),\n         )\n-\n-    # TODO: implement conversion of shape into repetitions, pass to\n-    # `torch.tile`, since `torch.full()` only accepts scalars\n-    # for `fill_value`.\"\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.full` only accepts scalars for `fill_value`.\",\n-    )\n-    def test_full_like_without_torch(self):\n-        x = np.array([[1, 2, 3], [3, 2, 1]])\n-        self.assertAllClose(\n-            np.array(knp.full_like(x, np.ones([2, 3]))),\n-            np.full_like(x, np.ones([2, 3])),\n-        )\n-\n         self.assertAllClose(\n             np.array(knp.FullLike()(x, np.ones([2, 3]))),\n             np.full_like(x, np.ones([2, 3])),\n@@ -1834,13 +1823,6 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n             np.linspace(0, 10, 5, endpoint=False),\n         )\n \n-    # TODO: torch.linspace does not support tensor or array\n-    # for start/stop, create manual implementation\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.linspace` has no support for array start/stop.\",\n-    )\n-    def test_linspace_without_torch(self):\n         start = np.zeros([2, 3, 4])\n         stop = np.ones([2, 3, 4])\n         self.assertAllClose(\n@@ -1945,15 +1927,9 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n             np.logspace(0, 10, 5, endpoint=False),\n         )\n \n-    # TODO: torch.logspace does not support tensor or array\n-    # for start/stop, create manual implementation\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.logspace` has no support for array start/stop.\",\n-    )\n-    def test_logspace_without_torch(self):\n         start = np.zeros([2, 3, 4])\n         stop = np.ones([2, 3, 4])\n+\n         self.assertAllClose(\n             np.array(knp.logspace(start, stop, 5, base=10)),\n             np.logspace(start, stop, 5, base=10),\n@@ -3016,13 +2992,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.tile(x, [2, 3])), np.tile(x, [2, 3]))\n         self.assertAllClose(np.array(knp.Tile([2, 3])(x)), np.tile(x, [2, 3]))\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.split` does not support args `offset`, `axis1`, `axis2`\",\n-    )\n     def test_trace(self):\n-        # TODO: implement `torch.trace` support for arguments `offset`,\n-        # `axis1`, `axis2` and delete NotImplementedError\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         self.assertAllClose(np.array(knp.trace(x)), np.trace(x))\n         self.assertAllClose(\n@@ -3092,25 +3062,15 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(\n             np.array(knp.full([2, 3], 0.1)), np.full([2, 3], 0.1)\n         )\n-\n-        self.assertAllClose(np.array(knp.Full()([2, 3], 0)), np.full([2, 3], 0))\n-        self.assertAllClose(\n-            np.array(knp.Full()([2, 3], 0.1)), np.full([2, 3], 0.1)\n-        )\n-\n-    # TODO: implement conversion of shape into repetitions, pass to\n-    # `torch.tile`, since `torch.full()` only accepts scalars\n-    # for `fill_value`.\"\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.full` only accepts scalars for `fill_value`.\",\n-    )\n-    def test_full_without_torch(self):\n         self.assertAllClose(\n             np.array(knp.full([2, 3], np.array([1, 4, 5]))),\n             np.full([2, 3], np.array([1, 4, 5])),\n         )\n \n+        self.assertAllClose(np.array(knp.Full()([2, 3], 0)), np.full([2, 3], 0))\n+        self.assertAllClose(\n+            np.array(knp.Full()([2, 3], 0.1)), np.full([2, 3], 0.1)\n+        )\n         self.assertAllClose(\n             np.array(knp.Full()([2, 3], np.array([1, 4, 5]))),\n             np.full([2, 3], np.array([1, 4, 5])),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
