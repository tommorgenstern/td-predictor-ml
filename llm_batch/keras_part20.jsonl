{"custom_id": "keras#8e1e32a90689c0c6d631e923051ccc31ab9fb8a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 213 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,8 +36,7 @@ def create_model(weight_reg=None, activity_reg=None):\n     model = Sequential()\n     model.add(Dense(784, 50))\n     model.add(Activation('relu'))\n-    model.add(ActivityRegularization(activity_reg))\n-    model.add(Dense(50, 10, W_regularizer=weight_reg))\n+    model.add(Dense(50, 10, W_regularizer=weight_reg, activity_regularizer=activity_reg))\n     model.add(Activation('softmax'))\n     return model\n \n\n@@ -14,7 +14,7 @@ class TestRegularizers(unittest.TestCase):\n         nb_hidden = 10\n \n         print('vector classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(10,),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,),\n             classification=True, nb_class=2)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -36,7 +36,7 @@ class TestRegularizers(unittest.TestCase):\n     def test_vector_reg(self):\n         nb_hidden = 10\n         print('vector regression data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(10,), output_shape=(2,),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,), output_shape=(2,),\n             classification=False)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -53,7 +53,7 @@ class TestRegularizers(unittest.TestCase):\n \n     def test_temporal_clf(self):\n         print('temporal classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(5,10), \n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5,10), \n             classification=True, nb_class=2)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -72,7 +72,7 @@ class TestRegularizers(unittest.TestCase):\n \n     def test_temporal_reg(self):\n         print('temporal regression data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(5, 10), output_shape=(2,),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5, 10), output_shape=(2,),\n             classification=False)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -88,7 +88,7 @@ class TestRegularizers(unittest.TestCase):\n \n     def test_seq_to_seq(self):\n         print('sequence to sequence data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(5, 10), output_shape=(5, 10),\n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(5, 10), output_shape=(5, 10),\n             classification=False)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n@@ -104,7 +104,7 @@ class TestRegularizers(unittest.TestCase):\n \n     def test_img_clf(self):\n         print('image classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=1000, input_shape=(3, 32, 32), \n+        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 32, 32), \n             classification=True, nb_class=2)\n         print('X_train:', X_train.shape)\n         print('X_test:', X_test.shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#522201ec9e00ed2fe135a621bde1b288c59ddd25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 26 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,26 @@\n+import numpy as np\n+\n+def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,), output_shape=(2,), \n+    classification=True, nb_class=2):\n+    '''\n+        classification=True overrides output_shape \n+        (i.e. output_shape is set to (1,)) and the output \n+        consists in integers in [0, nb_class-1].\n+\n+        Otherwise: float output with shape output_shape.\n+    '''\n+    nb_sample = nb_train + nb_test\n+    if classification:\n+        y = np.random.randint(0, nb_class, size=(nb_sample, 1))\n+        X = np.zeros((nb_sample,) + input_shape)\n+        for i in range(nb_sample):\n+            X[i] = np.random.normal(loc=y[i], scale=1.0, size=input_shape)\n+    else:\n+        y_loc = np.random.random((nb_sample,))\n+        X = np.zeros((nb_sample,) + input_shape)\n+        y = np.zeros((nb_sample,) + output_shape)\n+        for i in range(nb_sample):\n+            X[i] = np.random.normal(loc=y_loc[i], scale=1.0, size=input_shape)\n+            y[i] = np.random.normal(loc=y_loc[i], scale=1.0, size=output_shape)\n+\n+    return (X[:nb_train], y[:nb_train]), (X[nb_train:], y[nb_train:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af0899ded76e483bcfdb2d4fe1bc365175421649", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 19 | Methods Changed: 11 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 88 | Churn Cumulative: 329 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,9 +16,10 @@ class Convolution1D(Layer):\n         border_mode='valid', subsample_length=1,\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n+        super(Convolution1D,self).__init__()\n+\n         nb_row = 1\n         nb_col = filter_length\n-        \n         self.nb_filter = nb_filter\n         self.stack_size = stack_size\n         self.filter_length = filter_length\n@@ -53,7 +54,6 @@ class Convolution1D(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-\n         conv_out = theano.tensor.nnet.conv.conv2d(X, self.W,\n             border_mode=self.border_mode, subsample=self.subsample)\n         output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n@@ -71,23 +71,27 @@ class Convolution1D(Layer):\n \n \n class MaxPooling1D(Layer):\n-    def __init__(self, pool_length=2, ignore_border=True):\n+    def __init__(self, pool_length=2, stride=None, ignore_border=True):\n+        super(MaxPooling1D,self).__init__()\n         self.pool_length = pool_length\n+\n+        if stride is not None:\n+            self.stride = (1, stride)\n+\n+        self.input = T.tensor4()\n         self.poolsize = (1, pool_length)\n         self.ignore_border = ignore_border\n \n-        self.input = T.tensor4()\n-        self.params = []\n-\n     def get_output(self, train):\n         X = self.get_input(train)\n-        output = downsample.max_pool_2d(X, self.poolsize, ignore_border=self.ignore_border)\n+        output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.stride, ignore_border=self.ignore_border)\n         return output\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n                 \"pool_length\":self.pool_length,\n-            \"ignore_border\":self.ignore_border}\n+                \"ignore_border\":self.ignore_border,\n+                \"subsample_length\": self.subsample_length}\n \n \n \n@@ -104,6 +108,7 @@ class Convolution2D(Layer):\n         self.border_mode = border_mode\n         self.nb_filter = nb_filter\n         self.stack_size = stack_size\n+\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n \n@@ -132,7 +137,6 @@ class Convolution2D(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-\n         conv_out = theano.tensor.nnet.conv.conv2d(X, self.W,\n             border_mode=self.border_mode, subsample=self.subsample)\n         output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n@@ -151,27 +155,47 @@ class Convolution2D(Layer):\n \n \n class MaxPooling2D(Layer):\n-    def __init__(self, poolsize=(2, 2), ignore_border=True):\n+    def __init__(self, poolsize=(2, 2), stride=None, ignore_border=True):\n         super(MaxPooling2D,self).__init__()\n         self.input = T.tensor4()\n         self.poolsize = poolsize\n+        self.stride = stride\n         self.ignore_border = ignore_border\n \n+\n     def get_output(self, train):\n         X = self.get_input(train)\n-        output = downsample.max_pool_2d(X, self.poolsize, ignore_border=self.ignore_border)\n+        output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.stride, ignore_border=self.ignore_border)\n         return output\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n                 \"poolsize\":self.poolsize,\n-            \"ignore_border\":self.ignore_border}\n+                \"ignore_border\":self.ignore_border,\n+                \"stride\": self.stride}\n \n \n+class ZeroPadding2D(Layer):\n+    def __init__(self, width=1):\n+        super(ZeroPadding2D, self).__init__()\n+        self.width = width\n+        self.input = T.tensor4()\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        width =  self.width\n+        in_shape = X.shape\n+        out_shape = (in_shape[0], in_shape[1], in_shape[2] + 2 * width, in_shape[3] + 2 * width)\n+        out = T.zeros(out_shape)\n+        indices = (slice(None), slice(None), slice(width, in_shape[2] + width),slice(width, in_shape[3] + width))\n+        return T.set_subtensor(out[indices], X)\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+                \"width\":self.width}\n+\n \n-# class ZeroPadding2D(Layer): TODO\n \n # class Convolution3D: TODO\n \n # class MaxPooling3D: TODO\n-        \n\n@@ -67,3 +67,39 @@ class BatchNormalization(Layer):\n             \"input_shape\":self.input_shape,\n             \"epsilon\":self.epsilon,\n             \"mode\":self.mode}\n+\n+class LRN2D(Layer):\n+    \"\"\"\n+    This code is adapted from pylearn2.\n+    License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n+    \"\"\"\n+\n+    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5):\n+        super(LRN2D, self).__init__()\n+        self.alpha = alpha\n+        self.k = k\n+        self.beta = beta\n+        self.n = n\n+        if n % 2 == 0:\n+            raise NotImplementedError(\"Only works with odd n\")\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        input_dim = X.shape\n+        half_n = self.n // 2\n+        input_sqr = T.sqr(X)\n+        b, ch, r, c = input_dim\n+        extra_channels = T.alloc(0., b, ch + 2*half_n, r, c)\n+        input_sqr = T.set_subtensor(extra_channels[:, half_n:half_n+ch, :, :],input_sqr)\n+        scale = self.k\n+        for i in range(self.n):\n+            scale += self.alpha * input_sqr[:, i:i+ch, :, :]\n+        scale = scale ** self.beta\n+        return X / scale\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"alpha\":self.alpha,\n+            \"k\":self.k,\n+            \"beta\":self.beta,\n+            \"n\": self.n}\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e63644cf8246b4fc22942d81d474baadb70d092a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1397 | Contributors (this commit): 11 | Commits (past 90d): 63 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -493,17 +493,17 @@ class DenoisingAutoEncoder(AutoEncoder):\n         super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights, weights)\n         self.corruption_level = corruption_level\n \n-    def _get_corrupted_input(self, input):\n+    def _corrupt_input(self, X):\n         \"\"\"\n             http://deeplearning.net/tutorial/dA.html\n         \"\"\"\n-        return srng.binomial(size=(self.input_dim, 1), n=1,\n+        return X * srng.binomial(size=X.shape, n=1,\n                              p=1-self.corruption_level,\n-                             dtype=theano.config.floatX) * input\n+                             dtype=theano.config.floatX)\n \n     def get_input(self, train=False):\n         uncorrupted_input = super(DenoisingAutoEncoder, self).get_input(train)\n-        return self._get_corrupted_input(uncorrupted_input)\n+        return self._corrupt_input(uncorrupted_input)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n\n@@ -30,7 +30,7 @@ class TestRegularizers(unittest.TestCase):\n         model.add(Dense(nb_hidden, y_train.shape[-1]))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n         self.assertTrue(history.validation_accuracy[-1] > 0.9)\n \n     def test_vector_reg(self):\n@@ -48,7 +48,7 @@ class TestRegularizers(unittest.TestCase):\n         model.add(Activation('relu'))\n         model.add(Dense(nb_hidden, y_train.shape[-1]))\n         model.compile(loss='hinge', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.validation_loss[-1] < 0.75)\n \n     def test_temporal_clf(self):\n@@ -67,7 +67,7 @@ class TestRegularizers(unittest.TestCase):\n         model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n         self.assertTrue(history.validation_accuracy[-1] > 0.9)\n \n     def test_temporal_reg(self):\n@@ -82,7 +82,7 @@ class TestRegularizers(unittest.TestCase):\n         model = Sequential()\n         model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n         model.compile(loss='hinge', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.validation_loss[-1] < 0.75)\n \n \n@@ -98,7 +98,7 @@ class TestRegularizers(unittest.TestCase):\n         model = Sequential()\n         model.add(TimeDistributedDense(X_train.shape[-1], y_train.shape[-1]))\n         model.compile(loss='hinge', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n         self.assertTrue(history.validation_loss[-1] < 0.75)\n \n \n@@ -121,7 +121,7 @@ class TestRegularizers(unittest.TestCase):\n         model.add(Dense(32, y_test.shape[-1]))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n         self.assertTrue(history.validation_accuracy[-1] > 0.9)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7c53725098545256e3b2095c9b05285e3da3676", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1585 | Contributors (this commit): 12 | Commits (past 90d): 81 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,6 @@ class Convolution1D(Layer):\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Convolution1D,self).__init__()\n-\n         nb_row = 1\n         nb_col = filter_length\n         self.nb_filter = nb_filter\n@@ -74,7 +73,6 @@ class MaxPooling1D(Layer):\n     def __init__(self, pool_length=2, stride=None, ignore_border=True):\n         super(MaxPooling1D,self).__init__()\n         self.pool_length = pool_length\n-\n         if stride is not None:\n             self.stride = (1, stride)\n \n@@ -100,8 +98,8 @@ class Convolution2D(Layer):\n         init='glorot_uniform', activation='linear', weights=None,\n         border_mode='valid', subsample=(1, 1),\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n-        super(Convolution2D,self).__init__()\n \n+        super(Convolution2D,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.subsample = subsample\n\n@@ -408,6 +408,7 @@ class TimeDistributedDense(MaskedLayer):\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__}\n \n+\n class AutoEncoder(Layer):\n     '''\n         A customizable autoencoder model.\n\n@@ -68,6 +68,7 @@ class BatchNormalization(Layer):\n             \"epsilon\":self.epsilon,\n             \"mode\":self.mode}\n \n+\n class LRN2D(Layer):\n     \"\"\"\n     This code is adapted from pylearn2.\n@@ -75,20 +76,19 @@ class LRN2D(Layer):\n     \"\"\"\n \n     def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5):\n+        if n % 2 == 0:\n+            raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n         super(LRN2D, self).__init__()\n         self.alpha = alpha\n         self.k = k\n         self.beta = beta\n         self.n = n\n-        if n % 2 == 0:\n-            raise NotImplementedError(\"Only works with odd n\")\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        input_dim = X.shape\n+        b, ch, r, c = X.shape\n         half_n = self.n // 2\n         input_sqr = T.sqr(X)\n-        b, ch, r, c = input_dim\n         extra_channels = T.alloc(0., b, ch + 2*half_n, r, c)\n         input_sqr = T.set_subtensor(extra_channels[:, half_n:half_n+ch, :, :], input_sqr)\n         scale = self.k\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#243d4737d1caaf7d586abc69addef16a08276231", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 274 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,16 +11,15 @@ from ..layers.core import Layer\n \n \n class Convolution1D(Layer):\n-    def __init__(self, nb_filter, stack_size, filter_length,\n+    def __init__(self, input_dim, nb_filter, filter_length,\n         init='uniform', activation='linear', weights=None,\n         border_mode='valid', subsample_length=1,\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Convolution1D,self).__init__()\n-        nb_row = 1\n-        nb_col = filter_length\n+\n         self.nb_filter = nb_filter\n-        self.stack_size = stack_size\n+        self.input_dim = input_dim\n         self.filter_length = filter_length\n         self.subsample_length = subsample_length\n         self.init = initializations.get(init)\n@@ -28,8 +27,8 @@ class Convolution1D(Layer):\n         self.subsample = (1, subsample_length)\n         self.border_mode = border_mode\n \n-        self.input = T.tensor4()\n-        self.W_shape = (nb_filter, stack_size, nb_row, nb_col)\n+        self.input = T.tensor3()\n+        self.W_shape = (nb_filter, input_dim, filter_length, 1)\n         self.W = self.init(self.W_shape)\n         self.b = shared_zeros((nb_filter,))\n \n@@ -53,15 +52,15 @@ class Convolution1D(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        conv_out = theano.tensor.nnet.conv.conv2d(X, self.W,\n-            border_mode=self.border_mode, subsample=self.subsample)\n+        X = theano.tensor.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n+        conv_out = theano.tensor.nnet.conv.conv2d(X, self.W, border_mode=self.border_mode, subsample=self.subsample)\n         output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n-        return output\n+        return theano.tensor.reshape(output, (output.shape[0], output.shape[1], output.shape[2])).dimshuffle(0, 2, 1)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n             \"nb_filter\":self.nb_filter,\n-            \"stack_size\":self.stack_size,\n             \"filter_length\":self.filter_length,\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,\n@@ -73,20 +72,26 @@ class MaxPooling1D(Layer):\n     def __init__(self, pool_length=2, stride=None, ignore_border=True):\n         super(MaxPooling1D,self).__init__()\n         self.pool_length = pool_length\n-        if stride is not None:\n-            self.stride = (1, stride)\n+        self.stride = stride\n+        if self.stride:\n+            self.st = (1, self.stride)\n+        else:\n+            self.st = None\n \n-        self.input = T.tensor4()\n+        self.input = T.tensor3()\n         self.poolsize = (1, pool_length)\n         self.ignore_border = ignore_border\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.stride, ignore_border=self.ignore_border)\n-        return output\n+        X = theano.tensor.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 1, 3, 2)\n+        output = downsample.max_pool_2d(X, ds=self.poolsize, st=self.st, ignore_border=self.ignore_border)\n+        output = output.dimshuffle(0, 1, 3, 2)\n+        return theano.tensor.reshape(output, (output.shape[0], output.shape[1], output.shape[2]))\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n+                \"stride\":self.stride,\n                 \"pool_length\":self.pool_length,\n                 \"ignore_border\":self.ignore_border,\n                 \"subsample_length\": self.subsample_length}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76b9877ccbbfb4ab21de0b68e55497bc3908a478", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 208 | Contributors (this commit): 4 | Commits (past 90d): 19 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,9 @@ def mean_squared_error(y_true, y_pred):\n def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean(axis=-1)\n \n+def mean_squared_logarithmic_error(y_true, y_pred):\n+    return T.sqr(T.log(T.clip(y_pred, epsilon, np.inf) + 1.) - T.log(T.clip(y_true, epsilon, np.inf) + 1.)).mean(axis=-1)\n+\n def squared_hinge(y_true, y_pred):\n     return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean(axis=-1)\n \n@@ -35,6 +38,7 @@ def binary_crossentropy(y_true, y_pred):\n # aliases\n mse = MSE = mean_squared_error\n mae = MAE = mean_absolute_error\n+msle = MSLE = mean_squared_logarithmic_error\n \n from .utils.generic_utils import get_from_module\n def get(identifier):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1cd436574b28063f6490ab669e1b357b124e6a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 19 | Methods Changed: 12 | Complexity Δ (Sum/Max): 56/53 | Churn Δ: 42 | Churn Cumulative: 2626 | Contributors (this commit): 17 | Commits (past 90d): 97 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -121,22 +121,24 @@ class BaseLogger(Callback):\n         self.seen += batch_size\n \n         for k, v in logs.items():\n-            self.log_values.append((k, v))\n             if k in self.totals:\n                 self.totals[k] += v * batch_size\n             else:\n                 self.totals[k] = v * batch_size\n+        for k in self.params['metrics']:\n+            if k in logs:\n+                self.log_values.append((k, logs[k]))\n \n         # skip progbar update for the last batch; will be handled by on_epoch_end\n         if self.verbose and self.seen < self.params['nb_sample']:\n             self.progbar.update(self.seen, self.log_values)\n \n     def on_epoch_end(self, epoch, logs={}):\n-        for k, v in self.totals.items():\n+        for k in self.params['metrics']:\n+            if k in self.totals:\n                 self.log_values.append((k, self.totals[k] / self.seen))\n-                \n-        for k, v in logs.items():\n-            self.log_values.append((k, v))\n+            if k in logs:\n+                self.log_values.append((k, logs[k]))\n         self.progbar.update(self.seen, self.log_values)\n \n \n\n@@ -79,7 +79,7 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n \n class Model(object):\n     def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[], \\\n-        validation_split=0., val_f=None, val_ins=None, shuffle=True):\n+        validation_split=0., val_f=None, val_ins=None, shuffle=True, metrics=[]):\n         '''\n             Abstract fit function for f(*ins). Assume that f returns a list, labelled by out_labels.\n         '''\n@@ -92,7 +92,7 @@ class Model(object):\n             if 0 < validation_split < 1:\n                 do_validation = True\n                 split_at = int(len(ins[0]) * (1 - validation_split))\n-                (ins, ins_val) = (slice_X(ins, 0, split_at), slice_X(ins, split_at))\n+                (ins, val_ins) = (slice_X(ins, 0, split_at), slice_X(ins, split_at))\n                 if verbose:\n                     print(\"Train on %d samples, validate on %d samples\" % (len(ins[0]), len(val_ins[0])))\n \n@@ -113,6 +113,7 @@ class Model(object):\n             'nb_sample': nb_train_sample,\n             'verbose': verbose,\n             'do_validation': do_validation,\n+            'metrics':metrics,\n         })\n         callbacks.on_train_begin()\n \n@@ -165,8 +166,9 @@ class Model(object):\n         nb_sample = len(ins[0])\n         outs = []\n         if verbose == 1:\n-            progbar = Progbar(target=len(X[0]))\n+            progbar = Progbar(target=nb_sample)\n         batches = make_batches(nb_sample, batch_size)\n+        index_array = np.arange(nb_sample)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n             ins_batch = slice_X(ins, batch_ids)\n@@ -189,8 +191,9 @@ class Model(object):\n         nb_sample = len(ins[0])\n         outs = []\n         if verbose == 1:\n-            progbar = Progbar(target=len(X[0]))\n+            progbar = Progbar(target=nb_sample)\n         batches = make_batches(nb_sample, batch_size)\n+        index_array = np.arange(nb_sample)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n             ins_batch = slice_X(ins, batch_ids)\n@@ -211,7 +214,7 @@ class Model(object):\n                 progbar.update(batch_end)\n         for i, out in enumerate(outs):\n             outs[i] /= nb_sample\n-        return out\n+        return outs\n \n \n class Sequential(Model, containers.Sequential):\n@@ -353,14 +356,14 @@ class Sequential(Model, containers.Sequential):\n             out_labels = ['loss']\n \n         ins = X + [y, sample_weight]\n-\n+        metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n         return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=callbacks, \\\n-            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle)\n+            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n \n \n     def predict(self, X, batch_size=128, verbose=0):\n         X = standardize_X(X)\n-        return self._predict_loop(self._predict, X, batch_size, verbose)\n+        return self._predict_loop(self._predict, X, batch_size, verbose)[0]\n \n \n     def predict_proba(self, X, batch_size=128, verbose=1):\n@@ -388,7 +391,11 @@ class Sequential(Model, containers.Sequential):\n             f = self._test_with_acc\n         else:\n             f = self._test\n-        return self._test_loop(f, ins, batch_size, verbose)\n+        outs = self._test_loop(f, ins, batch_size, verbose)\n+        if show_accuracy:\n+            return outs\n+        else:\n+            return outs[0]\n \n \n     def get_config(self, verbose=0):\n@@ -448,7 +455,7 @@ class Sequential(Model, containers.Sequential):\n \n \n class Graph(containers.Graph):\n-    def compile(self, optimizer, loss, theano_mode='DebugMode'):\n+    def compile(self, optimizer, loss, theano_mode=None):\n         # loss is a dictionary mapping output name to loss functions\n         ys = []\n         ys_train = []\n@@ -515,8 +522,9 @@ class Graph(containers.Graph):\n \n         f = self._train\n         out_labels = self.output_order\n+        metrics = self.output_order + ['val_' + m for m in self.output_order]\n         history = self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=callbacks, \\\n-            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle)\n+            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n         return history\n \n     def evaluate(self, data, batch_size=128, verbose=0):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66b8f3717569558e69e18a1ea127204885a623d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 35 | Files Changed: 2 | Hunks: 19 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 94 | Churn Cumulative: 2110 | Contributors (this commit): 17 | Commits (past 90d): 80 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -174,10 +174,13 @@ class Model(object):\n             ins_batch = slice_X(ins, batch_ids)\n \n             batch_outs = f(*ins_batch)\n+            if type(batch_outs) != list:\n+                batch_outs = [batch_outs]\n             if batch_index == 0:\n-                for batch_out in enumerate(batch_outs):\n+                for batch_out in batch_outs:\n                     shape = (nb_sample,) + batch_out.shape[1:]\n                     outs.append(np.zeros(shape))\n+\n             for i, batch_out in enumerate(batch_outs):\n                 outs[i][batch_start:batch_end] = batch_out\n             if verbose == 1:\n@@ -208,7 +211,7 @@ class Model(object):\n             else:\n                 if batch_index == 0:\n                     outs.append(0.)\n-                outs[0] += batch_outs\n+                outs[0] += batch_outs * len(batch_ids)\n \n             if verbose == 1:\n                 progbar.update(batch_end)\n@@ -454,7 +457,7 @@ class Sequential(Model, containers.Sequential):\n         f.close()\n \n \n-class Graph(containers.Graph):\n+class Graph(Model, containers.Graph):\n     def compile(self, optimizer, loss, theano_mode=None):\n         # loss is a dictionary mapping output name to loss functions\n         ys = []\n\n@@ -1,14 +1,21 @@\n+from __future__ import print_function\n from keras.models import Graph, Sequential\n from keras.layers import containers\n from keras.layers.core import Dense\n import numpy as np\n+from keras.utils.test_utils import get_test_data\n \n X = np.random.random((100, 32))\n X2 = np.random.random((100, 32))\n y = np.random.random((100, 4))\n y2 = np.random.random((100, 4))\n \n-print 'test a non-sequential graph with 1 input and 1 output'\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n+    classification=False, output_shape=(4,))\n+(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n+    classification=False, output_shape=(4,))\n+\n+print('test a non-sequential graph with 1 input and 1 output')\n graph = Graph()\n graph.add_input(name='input1', ndim=2)\n \n@@ -19,13 +26,18 @@ graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n graph.compile('rmsprop', {'output1':'mse'})\n \n-out = graph.predict({'input1':X})\n-loss = graph.test({'input1':X, 'output1':y})\n-loss = graph.train({'input1':X, 'output1':y})\n-print loss\n+history = graph.fit({'input1':X_train, 'output1':y_train}, nb_epoch=10)\n+out = graph.predict({'input1':X_test})\n+assert(type(out == dict))\n+assert(len(out) == 1)\n+loss = graph.test_on_batch({'input1':X_test, 'output1':y_test})\n+loss = graph.train_on_batch({'input1':X_test, 'output1':y_test})\n+loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n+print(loss)\n+assert(loss['output1'] < 1.2)\n \n \n-print 'test a more complex non-sequential graph with 1 input and 1 output'\n+print('test a more complex non-sequential graph with 1 input and 1 output')\n graph = Graph()\n graph.add_input(name='input1', ndim=2)\n \n@@ -38,13 +50,16 @@ graph.add_node(Dense(16, 4), name='dense4', inputs=['dense1', 'dense3'], merge_m\n graph.add_output(name='output1', inputs=['dense2', 'dense4'], merge_mode='sum')\n graph.compile('rmsprop', {'output1':'mse'})\n \n-out = graph.predict({'input1':X})\n-loss = graph.test({'input1':X, 'output1':y})\n-loss = graph.train({'input1':X, 'output1':y})\n-print loss\n+history = graph.fit({'input1':X_train, 'output1':y_train}, nb_epoch=4)\n+out = graph.predict({'input1':X_train})\n+assert(type(out == dict))\n+assert(len(out) == 1)\n+loss = graph.test_on_batch({'input1':X_test, 'output1':y_test})\n+loss = graph.train_on_batch({'input1':X_test, 'output1':y_test})\n+loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n+print(loss)\n \n-\n-print 'test a non-sequential graph with 2 inputs and 1 output'\n+print('test a non-sequential graph with 2 inputs and 1 output')\n graph = Graph()\n graph.add_input(name='input1', ndim=2)\n graph.add_input(name='input2', ndim=2)\n@@ -56,13 +71,16 @@ graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n graph.compile('rmsprop', {'output1':'mse'})\n \n-out = graph.predict({'input1':X, 'input2':X2})\n-loss = graph.test({'input1':X, 'input2':X2, 'output1':y})\n-loss = graph.train({'input1':X, 'input2':X2, 'output1':y})\n-print loss\n+history = graph.fit({'input1':X_train, 'input2':X2_train, 'output1':y_train}, nb_epoch=4)\n+out = graph.predict({'input1':X_test, 'input2':X2_test})\n+assert(type(out == dict))\n+assert(len(out) == 1)\n+loss = graph.test_on_batch({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n+loss = graph.train_on_batch({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n+loss = graph.evaluate({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n+print(loss)\n \n-\n-print 'test a non-sequential graph with 1 input and 2 outputs'\n+print('test a non-sequential graph with 1 input and 2 outputs')\n graph = Graph()\n graph.add_input(name='input1', ndim=2)\n \n@@ -74,13 +92,16 @@ graph.add_output(name='output1', input='dense2')\n graph.add_output(name='output2', input='dense3')\n graph.compile('rmsprop', {'output1':'mse', 'output2':'mse'})\n \n-out = graph.predict({'input1':X})\n-loss = graph.test({'input1':X, 'output1':y, 'output2':y2})\n-loss = graph.train({'input1':X, 'output1':y, 'output2':y2})\n-print loss\n+history = graph.fit({'input1':X_train, 'output1':y_train, 'output2':y2_train}, nb_epoch=4)\n+out = graph.predict({'input1':X_test})\n+assert(type(out == dict))\n+assert(len(out) == 2)\n+loss = graph.test_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+loss = graph.train_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+loss = graph.evaluate({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+print(loss)\n \n-\n-print 'test layer-like API'\n+print('test layer-like API')\n \n graph = containers.Graph()\n graph.add_input(name='input1', ndim=2)\n@@ -94,13 +115,13 @@ seq.add(Dense(32, 32, name='first_seq_dense'))\n seq.add(graph)\n seq.add(Dense(4, 4, name='last_seq_dense'))\n \n-print seq.params\n-print seq.layers\n-print 'input:'\n-print seq.get_input()\n-print 'output:'\n-print seq.get_output()\n seq.compile('rmsprop', 'mse')\n \n-loss = seq.fit(X, y, batch_size=10, nb_epoch=1)\n-print loss\n\\ No newline at end of file\n+history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=4)\n+loss = seq.evaluate(X_test, y_test)\n+print(loss)\n+\n+loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n+pred = seq.predict(X_test)\n+pred = seq.predict_proba(X_test)\n+pred = seq.predict_classes(X_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47fd945cf1bf9435ae2009df3338723b438a49b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 212 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,9 @@ def mean_squared_error(y_true, y_pred):\n def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean(axis=-1)\n \n+def mean_absolute_percentage_error(y_true, y_pred):\n+    return T.abs_((y_true - y_pred) / y_true).mean() * 100\n+\n def mean_squared_logarithmic_error(y_true, y_pred):\n     return T.sqr(T.log(T.clip(y_pred, epsilon, np.inf) + 1.) - T.log(T.clip(y_true, epsilon, np.inf) + 1.)).mean(axis=-1)\n \n@@ -38,6 +41,7 @@ def binary_crossentropy(y_true, y_pred):\n # aliases\n mse = MSE = mean_squared_error\n mae = MAE = mean_absolute_error\n+mape = MAPE = mean_absolute_percentage_error\n msle = MSLE = mean_squared_logarithmic_error\n \n from .utils.generic_utils import get_from_module\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#553a7c0265e241976bfabac929dfd971add9266b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 158 | Lines Deleted: 131 | Files Changed: 3 | Hunks: 5 | Methods Changed: 10 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 289 | Churn Cumulative: 2399 | Contributors (this commit): 17 | Commits (past 90d): 82 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -533,14 +533,14 @@ class Graph(Model, containers.Graph):\n     def evaluate(self, data, batch_size=128, verbose=0):\n         ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n         outs = self._test_loop(self._test, ins, batch_size, verbose)\n-        return dict(zip(self.output_order, outs))\n+        return outs[0]\n \n     def predict(self, data, batch_size=128, verbose=0):\n         ins = [data[name] for name in self.input_order]\n         outs = self._predict_loop(self._predict, ins, batch_size, verbose)\n         return dict(zip(self.output_order, outs))\n \n-    def save_weights(self):\n+    def save_weights(self, filepath, overwrite=False):\n         # Save weights from all layers to HDF5\n         import h5py\n         import os.path\n@@ -568,8 +568,7 @@ class Graph(Model, containers.Graph):\n         f.flush()\n         f.close()\n \n-\n-    def load_weights(self):\n+    def load_weights(self, filepath):\n         # Loads weights from HDF5 file\n         import h5py\n         f = h5py.File(filepath)\n\n@@ -0,0 +1,155 @@\n+from __future__ import print_function\n+import unittest\n+import numpy as np\n+np.random.seed(1337)\n+from keras.models import Graph, Sequential\n+from keras.layers import containers\n+from keras.layers.core import Dense\n+from keras.utils.test_utils import get_test_data\n+\n+X = np.random.random((100, 32))\n+X2 = np.random.random((100, 32))\n+y = np.random.random((100, 4))\n+y2 = np.random.random((100, 4))\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n+    classification=False, output_shape=(4,))\n+(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n+    classification=False, output_shape=(4,))\n+\n+class TestRegularizers(unittest.TestCase):\n+    def test_1o_1i(self):\n+        print('test a non-sequential graph with 1 input and 1 output')\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+        graph.compile('rmsprop', {'output1':'mse'})\n+\n+        history = graph.fit({'input1':X_train, 'output1':y_train}, nb_epoch=10)\n+        out = graph.predict({'input1':X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1':X_test, 'output1':y_test})\n+        loss = graph.train_on_batch({'input1':X_test, 'output1':y_test})\n+        loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n+        print(loss)\n+        assert(loss < 1.4)\n+\n+    def test_1o_1i_2(self):\n+        print('test a more complex non-sequential graph with 1 input and 1 output')\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+\n+        graph.add_node(Dense(4, 16), name='dense3', input='dense2')\n+        graph.add_node(Dense(16, 4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n+\n+        graph.add_output(name='output1', inputs=['dense2', 'dense4'], merge_mode='sum')\n+        graph.compile('rmsprop', {'output1':'mse'})\n+\n+        history = graph.fit({'input1':X_train, 'output1':y_train}, nb_epoch=10)\n+        out = graph.predict({'input1':X_train})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1':X_test, 'output1':y_test})\n+        loss = graph.train_on_batch({'input1':X_test, 'output1':y_test})\n+        loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n+        print(loss)\n+        assert(loss < 1.4)\n+\n+    def test_1o_2i(self):\n+        print('test a non-sequential graph with 2 inputs and 1 output')\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+        graph.add_input(name='input2', ndim=2)\n+\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input2')\n+        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+        graph.compile('rmsprop', {'output1':'mse'})\n+\n+        history = graph.fit({'input1':X_train, 'input2':X2_train, 'output1':y_train}, nb_epoch=10)\n+        out = graph.predict({'input1':X_test, 'input2':X2_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n+        loss = graph.train_on_batch({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n+        loss = graph.evaluate({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n+        print(loss)\n+        assert(loss < 1.4)\n+\n+    def test_2o_1i_weights(self):\n+        print('test a non-sequential graph with 1 input and 2 outputs')\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+\n+        graph.add_output(name='output1', input='dense2')\n+        graph.add_output(name='output2', input='dense3')\n+        graph.compile('rmsprop', {'output1':'mse', 'output2':'mse'})\n+\n+        history = graph.fit({'input1':X_train, 'output1':y_train, 'output2':y2_train}, nb_epoch=10)\n+        out = graph.predict({'input1':X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 2)\n+        loss = graph.test_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+        loss = graph.train_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+        loss = graph.evaluate({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+        print(loss)\n+        assert(loss < 2.5)\n+\n+        print('test weight saving')\n+        graph.save_weights('temp.h5', overwrite=True)\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_output(name='output1', input='dense2')\n+        graph.add_output(name='output2', input='dense3')\n+        graph.compile('rmsprop', {'output1':'mse', 'output2':'mse'})\n+        graph.load_weights('temp.h5')\n+        nloss = graph.evaluate({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+    def test_recursive(self):\n+        print('test layer-like API')\n+\n+        graph = containers.Graph()\n+        graph.add_input(name='input1', ndim=2)\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+\n+        seq = Sequential()\n+        seq.add(Dense(32, 32, name='first_seq_dense'))\n+        seq.add(graph)\n+        seq.add(Dense(4, 4, name='last_seq_dense'))\n+\n+        seq.compile('rmsprop', 'mse')\n+\n+        history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=10)\n+        loss = seq.evaluate(X_test, y_test)\n+        print(loss)\n+        assert(loss < 1.4)\n+\n+        loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n+        pred = seq.predict(X_test)\n+\n+if __name__ == '__main__':\n+    print('Test graph model')\n+    unittest.main()\n\\ No newline at end of file\n\n@@ -1,127 +0,0 @@\n-from __future__ import print_function\n-from keras.models import Graph, Sequential\n-from keras.layers import containers\n-from keras.layers.core import Dense\n-import numpy as np\n-from keras.utils.test_utils import get_test_data\n-\n-X = np.random.random((100, 32))\n-X2 = np.random.random((100, 32))\n-y = np.random.random((100, 4))\n-y2 = np.random.random((100, 4))\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n-    classification=False, output_shape=(4,))\n-(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n-    classification=False, output_shape=(4,))\n-\n-print('test a non-sequential graph with 1 input and 1 output')\n-graph = Graph()\n-graph.add_input(name='input1', ndim=2)\n-\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-\n-graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n-graph.compile('rmsprop', {'output1':'mse'})\n-\n-history = graph.fit({'input1':X_train, 'output1':y_train}, nb_epoch=10)\n-out = graph.predict({'input1':X_test})\n-assert(type(out == dict))\n-assert(len(out) == 1)\n-loss = graph.test_on_batch({'input1':X_test, 'output1':y_test})\n-loss = graph.train_on_batch({'input1':X_test, 'output1':y_test})\n-loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n-print(loss)\n-assert(loss['output1'] < 1.2)\n-\n-\n-print('test a more complex non-sequential graph with 1 input and 1 output')\n-graph = Graph()\n-graph.add_input(name='input1', ndim=2)\n-\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-\n-graph.add_node(Dense(4, 16), name='dense3', input='dense2')\n-graph.add_node(Dense(16, 4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n-\n-graph.add_output(name='output1', inputs=['dense2', 'dense4'], merge_mode='sum')\n-graph.compile('rmsprop', {'output1':'mse'})\n-\n-history = graph.fit({'input1':X_train, 'output1':y_train}, nb_epoch=4)\n-out = graph.predict({'input1':X_train})\n-assert(type(out == dict))\n-assert(len(out) == 1)\n-loss = graph.test_on_batch({'input1':X_test, 'output1':y_test})\n-loss = graph.train_on_batch({'input1':X_test, 'output1':y_test})\n-loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n-print(loss)\n-\n-print('test a non-sequential graph with 2 inputs and 1 output')\n-graph = Graph()\n-graph.add_input(name='input1', ndim=2)\n-graph.add_input(name='input2', ndim=2)\n-\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(32, 4), name='dense2', input='input2')\n-graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-\n-graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n-graph.compile('rmsprop', {'output1':'mse'})\n-\n-history = graph.fit({'input1':X_train, 'input2':X2_train, 'output1':y_train}, nb_epoch=4)\n-out = graph.predict({'input1':X_test, 'input2':X2_test})\n-assert(type(out == dict))\n-assert(len(out) == 1)\n-loss = graph.test_on_batch({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n-loss = graph.train_on_batch({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n-loss = graph.evaluate({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n-print(loss)\n-\n-print('test a non-sequential graph with 1 input and 2 outputs')\n-graph = Graph()\n-graph.add_input(name='input1', ndim=2)\n-\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-\n-graph.add_output(name='output1', input='dense2')\n-graph.add_output(name='output2', input='dense3')\n-graph.compile('rmsprop', {'output1':'mse', 'output2':'mse'})\n-\n-history = graph.fit({'input1':X_train, 'output1':y_train, 'output2':y2_train}, nb_epoch=4)\n-out = graph.predict({'input1':X_test})\n-assert(type(out == dict))\n-assert(len(out) == 2)\n-loss = graph.test_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n-loss = graph.train_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n-loss = graph.evaluate({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n-print(loss)\n-\n-print('test layer-like API')\n-\n-graph = containers.Graph()\n-graph.add_input(name='input1', ndim=2)\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n-\n-seq = Sequential()\n-seq.add(Dense(32, 32, name='first_seq_dense'))\n-seq.add(graph)\n-seq.add(Dense(4, 4, name='last_seq_dense'))\n-\n-seq.compile('rmsprop', 'mse')\n-\n-history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=4)\n-loss = seq.evaluate(X_test, y_test)\n-print(loss)\n-\n-loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n-pred = seq.predict(X_test)\n-pred = seq.predict_proba(X_test)\n-pred = seq.predict_classes(X_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
