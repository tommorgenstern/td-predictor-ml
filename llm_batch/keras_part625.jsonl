{"custom_id": "keras#cd3822ab0833e041bab587ecf533103d519c16e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,8 +3,8 @@\n import os\n \n import numpy as np\n-from keras import backend\n \n+from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.datasets.cifar import load_batch\n from keras_core.utils.file_utils import get_file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1cf6c0efdb767f03458e944581c2437049254bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 290 | Contributors (this commit): 6 | Commits (past 90d): 28 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -106,6 +106,8 @@ ALL_OBJECTS_DICT.update(\n     {\n         \"mse\": MeanSquaredError,\n         \"MSE\": MeanSquaredError,\n+        \"mae\": MeanAbsoluteError,\n+        \"MAE\": MeanAbsoluteError,\n     }\n )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7e73c8b9a424e51de15438b88893b402ea519b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 145 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 12 | Complexity Δ (Sum/Max): 22/7 | Churn Δ: 145 | Churn Cumulative: 1568 | Contributors (this commit): 7 | Commits (past 90d): 53 | Contributors (cumulative): 24 | DMM Complexity: 0.82\n\nDIFF:\n@@ -205,3 +205,35 @@ def block_update(inputs, start_indices, updates):\n     ]\n     inputs[tuple(slices)] = updates\n     return inputs\n+\n+\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    loop_vars = tuple(loop_vars)\n+    if maximum_iterations is not None:\n+        current_iter = 0\n+        loop_vars = loop_vars + (current_iter,)\n+\n+        # Unpack list/tuple args. The last argument is `current_iter`.\n+        def _cond(args):\n+            return cond(*args[:-1]) & (args[-1] < maximum_iterations)\n+\n+        def _body(args):\n+            return tuple(body(*args[:-1])) + (args[-1] + 1,)\n+\n+    else:\n+\n+        def _cond(args):\n+            return cond(*args)\n+\n+        def _body(args):\n+            return tuple(body(*args))\n+\n+    outputs = jax.lax.while_loop(_cond, _body, loop_vars)\n+    if maximum_iterations is not None:\n+        outputs = outputs[:-1]\n+    return outputs\n\n@@ -111,3 +111,17 @@ def scatter_update(inputs, indices, updates):\n \n def block_update(inputs, start_indices, updates):\n     return dynamic_update_slice(inputs, updates, start_indices)\n+\n+\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    return tf.while_loop(\n+        cond,\n+        body,\n+        loop_vars,\n+        maximum_iterations=maximum_iterations,\n+    )\n\n@@ -130,3 +130,23 @@ def block_update(inputs, start_indices, updates):\n     ]\n     inputs[slices] = updates\n     return inputs\n+\n+\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    current_iter = 0\n+    iteration_check = (\n+        lambda iter: maximum_iterations is None or iter < maximum_iterations\n+    )\n+    loop_vars = tuple([convert_to_tensor(v) for v in loop_vars])\n+    while cond(*loop_vars) and iteration_check(current_iter):\n+        loop_vars = body(*loop_vars)\n+        if not isinstance(loop_vars, (list, tuple)):\n+            loop_vars = (loop_vars,)\n+        loop_vars = tuple(loop_vars)\n+        current_iter += 1\n+    return loop_vars\n\n@@ -126,3 +126,63 @@ def block_update(inputs, start_indices, updates):\n     if any_symbolic_tensors((inputs, start_indices, updates)):\n         return BlockUpdate().symbolic_call(inputs, start_indices, updates)\n     return backend.core.block_update(inputs, start_indices, updates)\n+\n+\n+class WhileLoop(Operation):\n+    def __init__(self, cond, body, maximum_iterations):\n+        super().__init__()\n+        self.cond = cond\n+        self.body = body\n+        self.maximum_iterations = maximum_iterations\n+\n+    def call(self, loop_vars):\n+        return backend.core.while_loop(\n+            self.cond,\n+            self.body,\n+            loop_vars,\n+            maximum_iterations=self.maximum_iterations,\n+        )\n+\n+    def compute_output_spec(self, loop_vars):\n+        return [KerasTensor(v.shape, dtype=v.dtype) for v in loop_vars]\n+\n+\n+@keras_core_export(\"keras_core.operations.while_loop\")\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    \"\"\"While loop implemetation.\n+\n+    Args:\n+        cond: A callable that represents the termination condition of the loop.\n+            Must have the same number of args as `loop_vars`, and return a bool.\n+        body: A callable that represents the loop body. Must have the same\n+            number of args as `loop_vars`, and return a list/tuple of the same\n+            length, shape and dtype as `loop_vars`.\n+        loop_vars: A list/tuple of tensors, the loop variables.\n+        maximum_iterations: Optional maximum number of iterations of the while\n+            loop to run. If provided, the `cond` output is AND-ed with an\n+            additional condition ensuring the number of iterations executed is\n+            no greater than `maximum_iterations`.\n+\n+    Returns:\n+        A list/tuple of tensors, has the same shape and dtype as `inputs`.\n+\n+    Examples:\n+\n+    >>> i = 0\n+    >>> cond = lambda i: i < 10\n+    >>> body = lambda i: i + 1\n+    >>> keras_core.operations.while_loop(cond, body, [i])[0]\n+    10\n+    \"\"\"\n+\n+    return backend.core.while_loop(\n+        cond,\n+        body,\n+        loop_vars,\n+        maximum_iterations=maximum_iterations,\n+    )\n\n@@ -139,3 +139,22 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         updates = np.zeros([2, 2, 2, 2])\n         outputs = core.block_update(inputs, start_indices, updates)\n         self.assertAllClose(outputs[1:3, 1:3, 2:4, 2:4], np.zeros([2, 2, 2, 2]))\n+\n+    def test_while_loop(self):\n+        def cond(x, y):\n+            return x[0, 0] < 10\n+\n+        def body(x, y):\n+            return x + 1, y + 1\n+\n+        x = np.ones((2, 3))\n+        y = np.ones((3, 2))\n+        x, y = core.while_loop(cond, body, (x, y))\n+        self.assertAllClose(x, np.ones((2, 3)) * 10)\n+        self.assertAllClose(y, np.ones((3, 2)) * 10)\n+\n+        x = np.ones((2, 3))\n+        y = np.ones((3, 2))\n+        x, y = core.while_loop(cond, body, (x, y), maximum_iterations=5)\n+        self.assertAllClose(x, np.ones((2, 3)) * 6)\n+        self.assertAllClose(y, np.ones((3, 2)) * 6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#429c2d67a4f43ac1b0831ba90336a8ce9c0b9cc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 147 | Files Changed: 7 | Hunks: 16 | Methods Changed: 15 | Complexity Δ (Sum/Max): -22/0 | Churn Δ: 165 | Churn Cumulative: 3924 | Contributors (this commit): 9 | Commits (past 90d): 81 | Contributors (cumulative): 34 | DMM Complexity: 0.1978021978021978\n\nDIFF:\n@@ -205,35 +205,3 @@ def block_update(inputs, start_indices, updates):\n     ]\n     inputs[tuple(slices)] = updates\n     return inputs\n-\n-\n-def while_loop(\n-    cond,\n-    body,\n-    loop_vars,\n-    maximum_iterations=None,\n-):\n-    loop_vars = tuple(loop_vars)\n-    if maximum_iterations is not None:\n-        current_iter = 0\n-        loop_vars = loop_vars + (current_iter,)\n-\n-        # Unpack list/tuple args. The last argument is `current_iter`.\n-        def _cond(args):\n-            return cond(*args[:-1]) & (args[-1] < maximum_iterations)\n-\n-        def _body(args):\n-            return tuple(body(*args[:-1])) + (args[-1] + 1,)\n-\n-    else:\n-\n-        def _cond(args):\n-            return cond(*args)\n-\n-        def _body(args):\n-            return tuple(body(*args))\n-\n-    outputs = jax.lax.while_loop(_cond, _body, loop_vars)\n-    if maximum_iterations is not None:\n-        outputs = outputs[:-1]\n-    return outputs\n\n@@ -111,17 +111,3 @@ def scatter_update(inputs, indices, updates):\n \n def block_update(inputs, start_indices, updates):\n     return dynamic_update_slice(inputs, updates, start_indices)\n-\n-\n-def while_loop(\n-    cond,\n-    body,\n-    loop_vars,\n-    maximum_iterations=None,\n-):\n-    return tf.while_loop(\n-        cond,\n-        body,\n-        loop_vars,\n-        maximum_iterations=maximum_iterations,\n-    )\n\n@@ -130,23 +130,3 @@ def block_update(inputs, start_indices, updates):\n     ]\n     inputs[slices] = updates\n     return inputs\n-\n-\n-def while_loop(\n-    cond,\n-    body,\n-    loop_vars,\n-    maximum_iterations=None,\n-):\n-    current_iter = 0\n-    iteration_check = (\n-        lambda iter: maximum_iterations is None or iter < maximum_iterations\n-    )\n-    loop_vars = tuple([convert_to_tensor(v) for v in loop_vars])\n-    while cond(*loop_vars) and iteration_check(current_iter):\n-        loop_vars = body(*loop_vars)\n-        if not isinstance(loop_vars, (list, tuple)):\n-            loop_vars = (loop_vars,)\n-        loop_vars = tuple(loop_vars)\n-        current_iter += 1\n-    return loop_vars\n\n@@ -881,6 +881,14 @@ class BinaryCrossentropyTest(testing.TestCase):\n         self.assertAlmostEqual(loss, 0.0)\n \n     def test_unweighted(self):\n+        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=\"float32\")\n+        y_pred = np.array(\n+            [[0.9, 0.1, 0.2], [0.3, 0.8, 0.1], [0.1, 0.2, 0.7]], dtype=\"float32\"\n+        )\n+        bce_obj = losses.BinaryCrossentropy()\n+        loss = bce_obj(y_true, y_pred)\n+        self.assertAllClose(loss, 0.20046903)\n+\n         y_true = np.array([1, 0, 1, 0]).reshape([2, 2])\n         y_pred = np.array([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n         bce_obj = losses.BinaryCrossentropy()\n\n@@ -57,12 +57,15 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n+@keras_core_export(\"keras_core.metrics.binary_accuracy\")\n def binary_accuracy(y_true, y_pred, threshold=0.5):\n+    y_true = ops.convert_to_tensor(y_true)\n     y_pred = ops.convert_to_tensor(y_pred)\n+    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n     threshold = ops.cast(threshold, y_pred.dtype)\n-    y_pred = ops.cast(y_pred > threshold, y_pred.dtype)\n+    y_pred = ops.cast(y_pred > threshold, y_true.dtype)\n     return ops.mean(\n-        ops.cast(ops.equal(y_true, y_pred), backend.floatx()),\n+        ops.cast(ops.equal(y_true, y_pred), dtype=backend.floatx()),\n         axis=-1,\n     )\n \n@@ -114,12 +117,14 @@ class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n+@keras_core_export(\"keras_core.metrics.categorical_accuracy\")\n def categorical_accuracy(y_true, y_pred):\n     y_true = ops.argmax(y_true, axis=-1)\n \n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_true.dtype)\n+\n     y_true_org_shape = ops.shape(y_true)\n     y_pred_rank = len(y_pred.shape)\n     y_true_rank = len(y_true.shape)\n@@ -198,6 +203,7 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n+@keras_core_export(\"keras_core.metrics.sparse_categorical_accuracy\")\n def sparse_categorical_accuracy(y_true, y_pred):\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -281,6 +287,7 @@ class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n+@keras_core_export(\"keras_core.metrics.top_k_categorical_accuracy\")\n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -357,6 +364,7 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype, \"k\": self.k}\n \n \n+@keras_core_export(\"keras_core.metrics.sparse_top_k_categorical_accuracy\")\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n\n@@ -126,63 +126,3 @@ def block_update(inputs, start_indices, updates):\n     if any_symbolic_tensors((inputs, start_indices, updates)):\n         return BlockUpdate().symbolic_call(inputs, start_indices, updates)\n     return backend.core.block_update(inputs, start_indices, updates)\n-\n-\n-class WhileLoop(Operation):\n-    def __init__(self, cond, body, maximum_iterations):\n-        super().__init__()\n-        self.cond = cond\n-        self.body = body\n-        self.maximum_iterations = maximum_iterations\n-\n-    def call(self, loop_vars):\n-        return backend.core.while_loop(\n-            self.cond,\n-            self.body,\n-            loop_vars,\n-            maximum_iterations=self.maximum_iterations,\n-        )\n-\n-    def compute_output_spec(self, loop_vars):\n-        return [KerasTensor(v.shape, dtype=v.dtype) for v in loop_vars]\n-\n-\n-@keras_core_export(\"keras_core.operations.while_loop\")\n-def while_loop(\n-    cond,\n-    body,\n-    loop_vars,\n-    maximum_iterations=None,\n-):\n-    \"\"\"While loop implemetation.\n-\n-    Args:\n-        cond: A callable that represents the termination condition of the loop.\n-            Must have the same number of args as `loop_vars`, and return a bool.\n-        body: A callable that represents the loop body. Must have the same\n-            number of args as `loop_vars`, and return a list/tuple of the same\n-            length, shape and dtype as `loop_vars`.\n-        loop_vars: A list/tuple of tensors, the loop variables.\n-        maximum_iterations: Optional maximum number of iterations of the while\n-            loop to run. If provided, the `cond` output is AND-ed with an\n-            additional condition ensuring the number of iterations executed is\n-            no greater than `maximum_iterations`.\n-\n-    Returns:\n-        A list/tuple of tensors, has the same shape and dtype as `inputs`.\n-\n-    Examples:\n-\n-    >>> i = 0\n-    >>> cond = lambda i: i < 10\n-    >>> body = lambda i: i + 1\n-    >>> keras_core.operations.while_loop(cond, body, [i])[0]\n-    10\n-    \"\"\"\n-\n-    return backend.core.while_loop(\n-        cond,\n-        body,\n-        loop_vars,\n-        maximum_iterations=maximum_iterations,\n-    )\n\n@@ -139,22 +139,3 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         updates = np.zeros([2, 2, 2, 2])\n         outputs = core.block_update(inputs, start_indices, updates)\n         self.assertAllClose(outputs[1:3, 1:3, 2:4, 2:4], np.zeros([2, 2, 2, 2]))\n-\n-    def test_while_loop(self):\n-        def cond(x, y):\n-            return x[0, 0] < 10\n-\n-        def body(x, y):\n-            return x + 1, y + 1\n-\n-        x = np.ones((2, 3))\n-        y = np.ones((3, 2))\n-        x, y = core.while_loop(cond, body, (x, y))\n-        self.assertAllClose(x, np.ones((2, 3)) * 10)\n-        self.assertAllClose(y, np.ones((3, 2)) * 10)\n-\n-        x = np.ones((2, 3))\n-        y = np.ones((3, 2))\n-        x, y = core.while_loop(cond, body, (x, y), maximum_iterations=5)\n-        self.assertAllClose(x, np.ones((2, 3)) * 6)\n-        self.assertAllClose(y, np.ones((3, 2)) * 6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ea318762054fa810ce9ca0d07ee795d3072039d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 145 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 12 | Complexity Δ (Sum/Max): 22/7 | Churn Δ: 145 | Churn Cumulative: 1858 | Contributors (this commit): 7 | Commits (past 90d): 63 | Contributors (cumulative): 24 | DMM Complexity: 0.82\n\nDIFF:\n@@ -205,3 +205,35 @@ def block_update(inputs, start_indices, updates):\n     ]\n     inputs[tuple(slices)] = updates\n     return inputs\n+\n+\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    loop_vars = tuple(loop_vars)\n+    if maximum_iterations is not None:\n+        current_iter = 0\n+        loop_vars = loop_vars + (current_iter,)\n+\n+        # Unpack list/tuple args. The last argument is `current_iter`.\n+        def _cond(args):\n+            return cond(*args[:-1]) & (args[-1] < maximum_iterations)\n+\n+        def _body(args):\n+            return tuple(body(*args[:-1])) + (args[-1] + 1,)\n+\n+    else:\n+\n+        def _cond(args):\n+            return cond(*args)\n+\n+        def _body(args):\n+            return tuple(body(*args))\n+\n+    outputs = jax.lax.while_loop(_cond, _body, loop_vars)\n+    if maximum_iterations is not None:\n+        outputs = outputs[:-1]\n+    return outputs\n\n@@ -111,3 +111,17 @@ def scatter_update(inputs, indices, updates):\n \n def block_update(inputs, start_indices, updates):\n     return dynamic_update_slice(inputs, updates, start_indices)\n+\n+\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    return tf.while_loop(\n+        cond,\n+        body,\n+        loop_vars,\n+        maximum_iterations=maximum_iterations,\n+    )\n\n@@ -130,3 +130,23 @@ def block_update(inputs, start_indices, updates):\n     ]\n     inputs[slices] = updates\n     return inputs\n+\n+\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    current_iter = 0\n+    iteration_check = (\n+        lambda iter: maximum_iterations is None or iter < maximum_iterations\n+    )\n+    loop_vars = tuple([convert_to_tensor(v) for v in loop_vars])\n+    while cond(*loop_vars) and iteration_check(current_iter):\n+        loop_vars = body(*loop_vars)\n+        if not isinstance(loop_vars, (list, tuple)):\n+            loop_vars = (loop_vars,)\n+        loop_vars = tuple(loop_vars)\n+        current_iter += 1\n+    return loop_vars\n\n@@ -126,3 +126,63 @@ def block_update(inputs, start_indices, updates):\n     if any_symbolic_tensors((inputs, start_indices, updates)):\n         return BlockUpdate().symbolic_call(inputs, start_indices, updates)\n     return backend.core.block_update(inputs, start_indices, updates)\n+\n+\n+class WhileLoop(Operation):\n+    def __init__(self, cond, body, maximum_iterations):\n+        super().__init__()\n+        self.cond = cond\n+        self.body = body\n+        self.maximum_iterations = maximum_iterations\n+\n+    def call(self, loop_vars):\n+        return backend.core.while_loop(\n+            self.cond,\n+            self.body,\n+            loop_vars,\n+            maximum_iterations=self.maximum_iterations,\n+        )\n+\n+    def compute_output_spec(self, loop_vars):\n+        return [KerasTensor(v.shape, dtype=v.dtype) for v in loop_vars]\n+\n+\n+@keras_core_export(\"keras_core.operations.while_loop\")\n+def while_loop(\n+    cond,\n+    body,\n+    loop_vars,\n+    maximum_iterations=None,\n+):\n+    \"\"\"While loop implemetation.\n+\n+    Args:\n+        cond: A callable that represents the termination condition of the loop.\n+            Must have the same number of args as `loop_vars`, and return a bool.\n+        body: A callable that represents the loop body. Must have the same\n+            number of args as `loop_vars`, and return a list/tuple of the same\n+            length, shape and dtype as `loop_vars`.\n+        loop_vars: A list/tuple of tensors, the loop variables.\n+        maximum_iterations: Optional maximum number of iterations of the while\n+            loop to run. If provided, the `cond` output is AND-ed with an\n+            additional condition ensuring the number of iterations executed is\n+            no greater than `maximum_iterations`.\n+\n+    Returns:\n+        A list/tuple of tensors, has the same shape and dtype as `inputs`.\n+\n+    Examples:\n+\n+    >>> i = 0\n+    >>> cond = lambda i: i < 10\n+    >>> body = lambda i: i + 1\n+    >>> keras_core.operations.while_loop(cond, body, [i])[0]\n+    10\n+    \"\"\"\n+\n+    return backend.core.while_loop(\n+        cond,\n+        body,\n+        loop_vars,\n+        maximum_iterations=maximum_iterations,\n+    )\n\n@@ -139,3 +139,22 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         updates = np.zeros([2, 2, 2, 2])\n         outputs = core.block_update(inputs, start_indices, updates)\n         self.assertAllClose(outputs[1:3, 1:3, 2:4, 2:4], np.zeros([2, 2, 2, 2]))\n+\n+    def test_while_loop(self):\n+        def cond(x, y):\n+            return x[0, 0] < 10\n+\n+        def body(x, y):\n+            return x + 1, y + 1\n+\n+        x = np.ones((2, 3))\n+        y = np.ones((3, 2))\n+        x, y = core.while_loop(cond, body, (x, y))\n+        self.assertAllClose(x, np.ones((2, 3)) * 10)\n+        self.assertAllClose(y, np.ones((3, 2)) * 10)\n+\n+        x = np.ones((2, 3))\n+        y = np.ones((3, 2))\n+        x, y = core.while_loop(cond, body, (x, y), maximum_iterations=5)\n+        self.assertAllClose(x, np.ones((2, 3)) * 6)\n+        self.assertAllClose(y, np.ones((3, 2)) * 6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#864e1312bad78b47b87eb194d654e017afdded86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 383 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,6 +82,7 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n \n         def apply_grad_to_update_var(var, grad):\n             learning_rate = self._get_current_learning_rate()\n+            grad = tf.convert_to_tensor(grad)\n             return self.update_step(grad, var, learning_rate)\n \n         for grad, var in grads_and_vars:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#489e279031ca70ccb37059210cc4a35179adf9ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 2640 | Contributors (this commit): 16 | Commits (past 90d): 7 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -540,7 +540,8 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)):\n+            if (tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))\n+                and not subdir.startswith(\".\")):\n                 if subdir.endswith(\"/\"):\n                     subdir = subdir[:-1]\n                 subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed62b30f7b240d49cc0fee8cad4b30780f4c94d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2643 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -540,8 +540,7 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if (tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))\n-                and not subdir.startswith(\".\")):\n+            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)) and not subdir.startswith(\".\"):\n                 if subdir.endswith(\"/\"):\n                     subdir = subdir[:-1]\n                 subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91e5c776d764bfb19befc9ed3f5b7a9590c3ff0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1424 | Contributors (this commit): 4 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -47,6 +47,8 @@ ALL_OBJECTS_DICT.update(\n         \"uniform\": RandomUniform,\n         \"normal\": RandomNormal,\n         \"orthogonal\": OrthogonalInitializer,\n+        \"one\": Ones,\n+        \"zero\": Zeros,\n     }\n )\n \n\n@@ -41,7 +41,7 @@ class RandomNormal(Initializer):\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n \n-    def __init__(self, mean=0.0, stddev=1.0, seed=None):\n+    def __init__(self, mean=0.0, stddev=0.05, seed=None):\n         self.mean = mean\n         self.stddev = stddev\n         self._init_seed = seed\n@@ -96,7 +96,7 @@ class TruncatedNormal(Initializer):\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n \n-    def __init__(self, mean=0.0, stddev=1.0, seed=None):\n+    def __init__(self, mean=0.0, stddev=0.05, seed=None):\n         self.mean = mean\n         self.stddev = stddev\n         self._init_seed = seed\n@@ -148,7 +148,7 @@ class RandomUniform(Initializer):\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n \n-    def __init__(self, minval=0.0, maxval=1.0, seed=None):\n+    def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n         self.minval = minval\n         self.maxval = maxval\n         self._init_seed = seed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f86351f0e93c3b0affa01a29df3330498982f39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2646 | Contributors (this commit): 16 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -540,7 +540,8 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)) and not subdir.startswith(\".\"):\n+            if (tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))\n+            and not subdir.startswith(\".\")):\n                 if subdir.endswith(\"/\"):\n                     subdir = subdir[:-1]\n                 subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#489c599d6f569a6169639ae83318fedba2a81956", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 245 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,6 +60,16 @@ class BinaryAccuracyTest(testing.TestCase):\n         result = bin_acc_obj.result()\n         self.assertAllClose(result, 0.75, atol=1e-3)\n \n+        # Test broadcasting case\n+        bin_acc_obj = accuracy_metrics.BinaryAccuracy(\n+            name=\"binary_accuracy\", dtype=\"float32\"\n+        )\n+        y_true = np.array([1, 1, 0, 0])\n+        y_pred = np.array([[0.98], [1], [0], [0.6]])\n+        bin_acc_obj.update_state(y_true, y_pred)\n+        result = bin_acc_obj.result()\n+        self.assertAllClose(result, 0.75, atol=1e-3)\n+\n     def test_weighted(self):\n         bin_acc_obj = accuracy_metrics.BinaryAccuracy(\n             name=\"binary_accuracy\", dtype=\"float32\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f600f067cb67e8cf17eedccf7170b696a75e4aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 279 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 279 | Churn Cumulative: 279 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,279 @@\n+\"\"\"\n+Title: Text classification from scratch\n+Authors: Mark Omernick, Francois Chollet\n+Date created: 2019/11/06\n+Last modified: 2020/05/17\n+Description: Text sentiment classification starting from raw text files.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example shows how to do text classification starting from raw text (as\n+a set of text files on disk). We demonstrate the workflow on the IMDB sentiment\n+classification dataset (unprocessed version). We use the `TextVectorization` layer for\n+ word splitting & indexing.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core.layers import TextVectorization\n+from keras_core import layers\n+import string\n+import re\n+import os\n+from pathlib import Path\n+\n+\"\"\"\n+## Load the data: IMDB movie review sentiment classification\n+\n+Let's download the data and inspect its structure.\n+\"\"\"\n+\n+fpath = keras.utils.get_file(\n+    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n+)\n+dirpath = Path(fpath).parent.absolute()\n+os.system(f\"tar -xf {fpath} -C {dirpath}\")\n+\n+\n+\"\"\"\n+The `aclImdb` folder contains a `train` and `test` subfolder:\n+\"\"\"\n+\n+os.system(f\"ls {dirpath}/aclImdb\")\n+os.system(f\"ls {dirpath}/aclImdb/train\")\n+os.system(f\"ls {dirpath}/aclImdb/test\")\n+\n+\"\"\"\n+The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n+ which represents one review (either positive or negative):\n+\"\"\"\n+\n+os.system(f\"cat {dirpath}/aclImdb/train/pos/6248_7.txt\")\n+\n+\"\"\"\n+We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:\n+\"\"\"\n+\n+os.system(f\"rm -r {dirpath}/aclImdb/train/unsup\")\n+\n+\n+\"\"\"\n+You can use the utility `keras.utils.text_dataset_from_directory` to\n+generate a labeled `tf.data.Dataset` object from a set of text files on disk filed\n+ into class-specific folders.\n+\n+Let's use it to generate the training, validation, and test datasets. The validation\n+and training datasets are generated from two subsets of the `train` directory, with 20%\n+of samples going to the validation dataset and 80% going to the training dataset.\n+\n+Having a validation dataset in addition to the test dataset is useful for tuning\n+hyperparameters, such as the model architecture, for which the test dataset should not\n+be used.\n+\n+Before putting the model out into the real world however, it should be retrained using all\n+available training data (without creating a validation dataset), so its performance is maximized.\n+\n+When using the `validation_split` & `subset` arguments, make sure to either specify a\n+random seed, or to pass `shuffle=False`, so that the validation & training splits you\n+get have no overlap.\n+\"\"\"\n+\n+batch_size = 32\n+raw_train_ds, raw_val_ds = keras.utils.text_dataset_from_directory(\n+    f\"{dirpath}/aclImdb/train\",\n+    batch_size=batch_size,\n+    validation_split=0.2,\n+    subset=\"both\",\n+    seed=1337,\n+)\n+raw_test_ds = keras.utils.text_dataset_from_directory(\n+    f\"{dirpath}/aclImdb/test\", batch_size=batch_size\n+)\n+\n+print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n+print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n+print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n+\n+\"\"\"\n+Let's preview a few samples:\n+\"\"\"\n+\n+# It's important to take a look at your raw data to ensure your normalization\n+# and tokenization will work as expected. We can do that by taking a few\n+# examples from the training set and looking at them.\n+# This is one of the places where eager execution shines:\n+# we can just evaluate these tensors using .numpy()\n+# instead of needing to evaluate them in a Session/Graph context.\n+for text_batch, label_batch in raw_train_ds.take(1):\n+    for i in range(5):\n+        print(text_batch.numpy()[i])\n+        print(label_batch.numpy()[i])\n+\n+\"\"\"\n+## Prepare the data\n+\n+In particular, we remove `<br />` tags.\n+\"\"\"\n+\n+\n+# Having looked at our data above, we see that the raw text contains HTML break\n+# tags of the form '<br />'. These tags will not be removed by the default\n+# standardizer (which doesn't strip HTML). Because of this, we will need to\n+# create a custom standardization function.\n+def custom_standardization(input_data):\n+    lowercase = tf.strings.lower(input_data)\n+    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n+    return tf.strings.regex_replace(\n+        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n+    )\n+\n+\n+# Model constants.\n+max_features = 20000\n+embedding_dim = 128\n+sequence_length = 500\n+\n+# Now that we have our custom standardization, we can instantiate our text\n+# vectorization layer. We are using this layer to normalize, split, and map\n+# strings to integers, so we set our 'output_mode' to 'int'.\n+# Note that we're using the default split function,\n+# and the custom standardization defined above.\n+# We also set an explicit maximum sequence length, since the CNNs later in our\n+# model won't support ragged sequences.\n+vectorize_layer = TextVectorization(\n+    standardize=custom_standardization,\n+    max_tokens=max_features,\n+    output_mode=\"int\",\n+    output_sequence_length=sequence_length,\n+)\n+\n+# Now that the vocab layer has been created, call `adapt` on a text-only\n+# dataset to create the vocabulary. You don't have to batch, but for very large\n+# datasets this means you're not keeping spare copies of the dataset in memory.\n+\n+# Let's make a text-only dataset (no labels):\n+text_ds = raw_train_ds.map(lambda x, y: x)\n+# Let's call `adapt`:\n+vectorize_layer.adapt(text_ds)\n+\n+\"\"\"\n+## Two options to vectorize the data\n+\n+There are 2 ways we can use our text vectorization layer:\n+\n+**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n+strings, like this:\n+\"\"\"\n+\n+\"\"\"\n+\n+```python\n+text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n+x = vectorize_layer(text_input)\n+x = layers.Embedding(max_features + 1, embedding_dim)(x)\n+...\n+```\n+\n+**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n+ feed it into a model that expects integer sequences as inputs.\n+\n+An important difference between the two is that option 2 enables you to do\n+**asynchronous CPU processing and buffering** of your data when training on GPU.\n+So if you're training the model on GPU, you probably want to go with this option to get\n+the best performance. This is what we will do below.\n+\n+If we were to export our model to production, we'd ship a model that accepts raw\n+strings as input, like in the code snippet for option 1 above. This can be done after\n+training. We do this in the last section.\n+\"\"\"\n+\n+\n+def vectorize_text(text, label):\n+    text = tf.expand_dims(text, -1)\n+    return vectorize_layer(text), label\n+\n+\n+# Vectorize the data.\n+train_ds = raw_train_ds.map(vectorize_text)\n+val_ds = raw_val_ds.map(vectorize_text)\n+test_ds = raw_test_ds.map(vectorize_text)\n+\n+# Do async prefetching / buffering of the data for best performance on GPU.\n+train_ds = train_ds.cache().prefetch(buffer_size=10)\n+val_ds = val_ds.cache().prefetch(buffer_size=10)\n+test_ds = test_ds.cache().prefetch(buffer_size=10)\n+\n+\"\"\"\n+## Build a model\n+\n+We choose a simple 1D convnet starting with an `Embedding` layer.\n+\"\"\"\n+\n+# A integer input for vocab indices.\n+inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n+# Next, we add a layer to map those vocab indices into a space of dimensionality\n+# 'embedding_dim'.\n+x = keras.layers.Embedding(max_features, embedding_dim)(inputs)\n+x = layers.Dropout(0.5)(x)\n+\n+# Conv1D + global max pooling\n+x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n+x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n+x = layers.GlobalMaxPooling1D()(x)\n+\n+# We add a vanilla hidden layer:\n+x = layers.Dense(128, activation=\"relu\")(x)\n+x = layers.Dropout(0.5)(x)\n+\n+# We project onto a single unit output layer, and squash it with a sigmoid:\n+predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n+model = keras.Model(inputs, predictions)\n+model.summary()\n+model.compile(\n+    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n+)\n+\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+epochs = 3\n+\n+# Fit the model using the train and test datasets.\n+model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n+\n+\"\"\"\n+## Evaluate the model on the test set\n+\"\"\"\n+\n+model.evaluate(test_ds)\n+\n+\"\"\"\n+## Make an end-to-end model\n+\n+If you want to obtain a model capable of processing raw strings, you can simply\n+create a new model (using the weights we just trained):\n+\"\"\"\n+\n+# A string input\n+inputs = keras.Input(shape=(1,), dtype=\"string\")\n+# Turn strings into vocab indices\n+indices = vectorize_layer(inputs)\n+# Turn vocab indices into predictions\n+outputs = model(indices)\n+\n+# Our end to end model\n+end_to_end_model = keras.Model(inputs, outputs)\n+end_to_end_model.compile(\n+    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n+)\n+\n+# Test it with `raw_test_ds`, which yields raw strings\n+end_to_end_model.evaluate(raw_test_ds)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0fdc93daa024d4c4ed81c1bfd46fc107a1cd539", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 712 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -457,6 +457,8 @@ def swapaxes(x, axis1, axis2):\n \n \n def take(x, indices, axis=None):\n+    x = convert_to_tensor(x)\n+    indices = convert_to_tensor(indices)\n     return jnp.take(x, indices, axis=axis)\n \n \n\n@@ -90,9 +90,8 @@ class Embedding(Layer):\n     def call(self, inputs):\n         if inputs.dtype != \"int32\" and inputs.dtype != \"int64\":\n             inputs = ops.cast(inputs, \"int32\")\n-        one_hot_data = ops.one_hot(inputs, num_classes=self.input_dim)\n-        out = ops.matmul(one_hot_data, self.embeddings)\n-        return ops.cast(out, dtype=self.compute_dtype)\n+        outputs = ops.take(self.embeddings, inputs, axis=0)\n+        return ops.cast(outputs, dtype=self.compute_dtype)\n \n     def compute_mask(self, inputs, mask=None):\n         if not self.mask_zero:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9870b10b5a747ec4fa4e08037c95ab0d70c4a3a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2652 | Contributors (this commit): 16 | Commits (past 90d): 10 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -540,8 +540,10 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if (tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))\n-                    and not subdir.startswith(\".\")):\n+            if (\n+                tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))\n+                and not subdir.startswith(\".\")\n+            ):\n                 if subdir.endswith(\"/\"):\n                     subdir = subdir[:-1]\n                 subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67329117223e30e094931fbece86f5af21f6dace", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 198 | Files Changed: 3 | Hunks: 22 | Methods Changed: 16 | Complexity Δ (Sum/Max): -36/8 | Churn Δ: 295 | Churn Cumulative: 2298 | Contributors (this commit): 24 | Commits (past 90d): 23 | Contributors (cumulative): 30 | DMM Complexity: 0.9108910891089109\n\nDIFF:\n@@ -1,12 +1,13 @@\n import math\n import re\n+import shutil\n \n+import rich\n from tensorflow import nest\n \n from keras_core import backend\n from keras_core.utils import dtype_utils\n from keras_core.utils import io_utils\n-from keras_core.utils import text_rendering\n \n \n def count_params(weights):\n@@ -46,19 +47,41 @@ def readable_memory_size(weight_memory_size):\n     return \"{:.2f} {}\".format(weight_memory_size, units[-1])\n \n \n+def highlight_number(x):\n+    \"\"\"Themes numbers in a summary using rich markup.\n+\n+    We use a separate color for `None`s, e.g. in a layer shape.\n+    \"\"\"\n+    if x is None:\n+        return f\"[color(45)]{x}[/]\"\n+    else:\n+        return f\"[color(34)]{x}[/]\"\n+\n+\n+def highlight_symbol(x):\n+    \"\"\"Themes keras symbols in a summary using rich markup.\"\"\"\n+    return f\"[color(33)]{x}[/]\"\n+\n+\n+def bold_text(x):\n+    \"\"\"Bolds text using rich markup.\"\"\"\n+    return f\"[bold]{x}[/]\"\n+\n+\n def format_layer_shape(layer):\n     if not layer._inbound_nodes:\n         return \"?\"\n-    output_shapes = None\n+\n+    def format_shape(shape):\n+        highlighted = [highlight_number(x) for x in shape]\n+        return \"(\" + \", \".join(highlighted) + \")\"\n+\n     for i in range(len(layer._inbound_nodes)):\n-        shapes = nest.map_structure(\n-            lambda x: tuple(x.shape), layer._inbound_nodes[i].output_tensors\n+        outputs = layer._inbound_nodes[i].output_tensors\n+        output_shapes = nest.map_structure(\n+            lambda x: format_shape(x.shape), outputs\n         )\n-        if output_shapes is None:\n-            output_shapes = shapes\n-        elif output_shapes != shapes:\n-            return \"multiple\"\n-    if len(output_shapes) == 1 and isinstance(output_shapes[0], tuple):\n+    if len(output_shapes) == 1:\n         output_shapes = output_shapes[0]\n     return str(output_shapes)\n \n@@ -161,18 +184,45 @@ def print_summary(\n         positions = [p * 0.86 for p in positions] + [1.0]\n         header.append(\"Trainable\")\n \n+    # Compute columns widths\n+    line_length = min(line_length, shutil.get_terminal_size().columns - 4)\n+    column_widths = []\n+    current = 0\n+    for pos in positions:\n+        width = int(pos * line_length) - current\n+        if width < 4:\n+            raise ValueError(\"Insufficient console width to print summary.\")\n+        column_widths.append(width)\n+        current += width\n+\n+    # Render summary as a rich table.\n+    columns = []\n+    # Right align parameter counts.\n+    alignment = [\"left\", \"left\", \"right\", \"left\", \"left\"]\n+    for i, name in enumerate(header):\n+        column = rich.table.Column(\n+            name,\n+            justify=alignment[i],\n+            width=column_widths[i],\n+        )\n+        columns.append(column)\n+    table = rich.table.Table(*columns, width=line_length, show_lines=True)\n+\n     def get_layer_fields(layer, prefix=\"\"):\n         output_shape = format_layer_shape(layer)\n         name = prefix + layer.name\n         cls_name = layer.__class__.__name__\n-        if not hasattr(layer, \"built\"):\n-            params = \"0\"\n-        elif not layer.built:\n-            params = \"0 (unbuilt)\"\n-        else:\n-            params = layer.count_params()\n-        fields = [name + \" (\" + cls_name + \")\", output_shape, str(params)]\n+        name = rich.markup.escape(name)\n+        name += f\" ({highlight_symbol(rich.markup.escape(cls_name))})\"\n \n+        if not hasattr(layer, \"built\"):\n+            params = highlight_number(0)\n+        elif not layer.built:\n+            params = highlight_number(0) + \" (unbuilt)\"\n+        else:\n+            params = highlight_number(f\"{layer.count_params():,}\")\n+\n+        fields = [name, output_shape, params]\n         if show_trainable:\n             fields.append(\"Y\" if layer.trainable else \"N\")\n         return fields\n@@ -186,8 +236,8 @@ def print_summary(\n             for kt in node.input_tensors:\n                 keras_history = kt._keras_history\n                 inbound_layer = keras_history.operation\n-                node_index = keras_history.node_index\n-                tensor_index = keras_history.tensor_index\n+                node_index = highlight_number(keras_history.node_index)\n+                tensor_index = highlight_number(keras_history.tensor_index)\n                 if connections:\n                     connections += \", \"\n                 connections += (\n@@ -219,23 +269,11 @@ def print_summary(\n                 )\n         return rows\n \n+    # Render all layers to the rich table.\n     layer_range = get_layer_index_bound_by_layer_name(layers, layer_range)\n-    print_fn(text_rendering.highlight_msg(f' Model: \"{model.name}\"'))\n-    rows = []\n     for layer in layers[layer_range[0] : layer_range[1]]:\n-        rows.extend(print_layer(layer))\n-\n-    # Render summary as a table.\n-    table = text_rendering.TextTable(\n-        header=header,\n-        rows=rows,\n-        positions=positions,\n-        # Left align layer name, center-align everything else\n-        alignments=[\"left\"] + [\"center\" for _ in range(len(header) - 1)],\n-        max_line_length=line_length,\n-    )\n-    table_str = table.make()\n-    print_fn(table_str)\n+        for row in print_layer(layer):\n+            table.add_row(*row)\n \n     # After the table, append information about parameter count and size.\n     if hasattr(model, \"_collected_trainable_weights\"):\n@@ -250,25 +288,41 @@ def print_summary(\n     non_trainable_count = count_params(model.non_trainable_weights)\n     non_trainable_memory_size = weight_memory_size(model.non_trainable_weights)\n \n+    total_count = trainable_count + non_trainable_count\n     total_memory_size = trainable_memory_size + non_trainable_memory_size\n \n-    print_fn(\n-        text_rendering.highlight_msg(\n-            f\" Total params: {trainable_count + non_trainable_count}\"\n+    # Create a rich console for printing. Capture for non-interactive logging.\n+    if io_utils.is_interactive_logging_enabled():\n+        console = rich.console.Console(highlight=False)\n+    else:\n+        console = rich.console.Console(\n+            highlight=False, force_terminal=False, color_system=None\n         )\n+        console.begin_capture()\n+\n+    # Print the to the console.\n+    console.print(bold_text(f'Model: \"{rich.markup.escape(model.name)}\"'))\n+    console.print(table)\n+    console.print(\n+        bold_text(\" Total params: \")\n+        + highlight_number(f\"{total_count:,}\")\n         + f\" ({readable_memory_size(total_memory_size)})\"\n     )\n-    print_fn(\n-        text_rendering.highlight_msg(f\" Trainable params: {trainable_count}\")\n+    console.print(\n+        bold_text(\" Trainable params: \")\n+        + highlight_number(f\"{trainable_count:,}\")\n         + f\" ({readable_memory_size(trainable_memory_size)})\"\n     )\n-    print_fn(\n-        text_rendering.highlight_msg(\n-            f\" Non-trainable params: {non_trainable_count}\"\n-        )\n+    console.print(\n+        bold_text(\" Non-trainable params: \")\n+        + highlight_number(f\"{non_trainable_count:,}\")\n         + f\" ({readable_memory_size(non_trainable_memory_size)})\"\n     )\n \n+    # Output captured summary for non-interactive logging.\n+    if not io_utils.is_interactive_logging_enabled():\n+        io_utils.print_msg(console.end_capture(), line_break=False)\n+\n \n def get_layer_index_bound_by_layer_name(layers, layer_range=None):\n     \"\"\"Get the layer indexes from the model based on layer names.\n\n@@ -1,156 +0,0 @@\n-import shutil\n-\n-\n-class TextTable:\n-    def __init__(\n-        self, header, rows, positions, alignments=None, max_line_length=80\n-    ):\n-        if len(header) != len(positions):\n-            raise ValueError(\"header and positions should be the same length.\")\n-        if not all(p <= 1.0 for p in positions):\n-            raise ValueError(\"All positions should be <= 1.\")\n-        self.alignments = alignments or [\"center\" for _ in header]\n-        if len(self.alignments) != len(header):\n-            raise ValueError(\"header and alignments should be the same length.\")\n-        last_p = 0.0\n-        for p in positions:\n-            if p <= last_p:\n-                raise ValueError(\n-                    \"All consecutive positions should be greater than the last.\"\n-                )\n-            last_p = p\n-        self.header = header\n-        self.rows = rows\n-\n-        # Compute columns widths\n-        line_length = min(\n-            max_line_length, shutil.get_terminal_size().columns - 4\n-        )\n-        column_widths = []\n-        current = 0\n-        for pos in positions:\n-            width = int(pos * line_length) - current\n-            if width < 4:\n-                raise ValueError(\"Insufficient console width to print summary.\")\n-            column_widths.append(width)\n-            current += width\n-        self.column_widths = column_widths\n-\n-    def make_separator(self, left, mid, right, horizontal):\n-        line = mid.join(horizontal * width for width in self.column_widths)\n-        return f\"{left}{line}{right}\"\n-\n-    @staticmethod\n-    def format_field(field):\n-        field = str(field)\n-        field = field.replace(\"\\n\", \" \")\n-        return field\n-\n-    @staticmethod\n-    def maybe_pad(field, alignment):\n-        if alignment == \"left\":\n-            return \" \" + field\n-        if alignment == \"right\":\n-            return field + \" \"\n-        return field\n-\n-    def print_row(\n-        self,\n-        fields,\n-        vertical_separator=\"│\",\n-        alignments=None,\n-        highlight=False,\n-    ):\n-        alignments = alignments or [\"center\" for _ in fields]\n-        lines = []\n-        line_break_chars_post = (\"),\", \"],\")\n-        line_break_chars_pre = (\"(\", \"[\")\n-        for field, width, alignment in zip(\n-            fields, self.column_widths, alignments\n-        ):\n-            field = self.format_field(field)\n-            buffered_width = width - 1\n-            if len(field) < buffered_width:\n-                lines.append([self.maybe_pad(field, alignment)])\n-                continue\n-            subfields = []\n-            while len(field) >= buffered_width:\n-                # Break on certain characters\n-                candidate_cutoffs_pre = [\n-                    field.find(x)\n-                    for x in line_break_chars_pre\n-                    if 0 < field.find(x) < buffered_width\n-                ]\n-                candidate_cutoffs_post = [\n-                    field.find(x) + len(x)\n-                    for x in line_break_chars_post\n-                    if 0 < field.find(x) + len(x) < buffered_width\n-                ]\n-                cutoffs = candidate_cutoffs_pre + candidate_cutoffs_post\n-                if cutoffs:\n-                    cutoff = max(cutoffs)\n-                else:\n-                    cutoff = buffered_width - 1\n-                subfield = field[:cutoff]\n-                field = field[cutoff:]\n-\n-                subfields.append(self.maybe_pad(subfield, alignment))\n-            if field:\n-                subfields.append(self.maybe_pad(field, alignment))\n-            lines.append(subfields)\n-\n-        max_subfield_count = max(len(subs) for subs in lines)\n-        rendered_lines = []\n-        for i in range(max_subfield_count):\n-            fields = []\n-            for subfields in lines:\n-                if len(subfields) < i + 1:\n-                    field = \"\"\n-                else:\n-                    field = subfields[i]\n-                fields.append(field)\n-            aligned_fields = [\n-                self.align_field(field, width, alignment)\n-                for field, width, alignment in zip(\n-                    fields, self.column_widths, alignments\n-                )\n-            ]\n-            if highlight:\n-                aligned_fields = [\n-                    highlight_msg(field) for field in aligned_fields\n-                ]\n-            line = vertical_separator.join(aligned_fields)\n-            line = f\"{vertical_separator}{line}{vertical_separator}\"\n-            rendered_lines.append(line)\n-        return \"\\n\".join(rendered_lines)\n-\n-    @staticmethod\n-    def align_field(field, width, alignment):\n-        if alignment == \"center\":\n-            return field.center(width)\n-        if alignment == \"left\":\n-            return field.ljust(width)\n-        if alignment == \"right\":\n-            return field.rjust(width)\n-\n-    def make(self):\n-        lines = []\n-        # Print header\n-        lines.append(self.make_separator(*\"┏┳┓━\"))\n-        lines.append(\n-            self.print_row(self.header, vertical_separator=\"┃\", highlight=True)\n-        )\n-        lines.append(self.make_separator(*\"┡╇┩━\"))\n-\n-        # Print rows\n-        for i, row in enumerate(self.rows):\n-            lines.append(self.print_row(row, alignments=self.alignments))\n-            if i < len(self.rows) - 1:\n-                lines.append(self.make_separator(*\"├┼┤─\"))\n-\n-        lines.append(self.make_separator(*\"└┴┘─\"))\n-        return \"\\n\".join(lines)\n-\n-\n-def highlight_msg(msg):\n-    return f\"\\x1b[1m{msg}\\x1b[0m\"\n\n@@ -38,6 +38,7 @@ setup(\n         \"absl-py\",\n         \"numpy\",\n         \"packaging\",\n+        \"rich\",\n     ],\n     # Supported Python versions\n     python_requires=\">=3.8\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
