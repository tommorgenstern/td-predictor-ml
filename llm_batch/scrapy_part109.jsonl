{"custom_id": "scrapy#9f01e3e79e5e733d4809821f6f87e170841795fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 17 | Churn Cumulative: 235 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 0.8666666666666667\n\nDIFF:\n@@ -14,3 +14,8 @@ def setup(app):\n         rolename      = \"command\",\n         indextemplate = \"pair: %s; command\",\n     )\n+    app.add_crossref_type(\n+        directivename = \"reqmeta\",\n+        rolename      = \"reqmeta\",\n+        indextemplate = \"pair: %s; reqmeta\",\n+    )\n\n@@ -15,6 +15,8 @@ class RedirectMiddleware(object):\n         self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n+        if 'dont_redirect' in request.meta:\n+            return response\n         if request.method.upper() == 'HEAD':\n             if response.status in [301, 302, 303, 307] and 'Location' in response.headers:\n                 redirected_url = urljoin_rfc(request.url, response.headers['location'])\n\n@@ -37,6 +37,16 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         _test('POST')\n         _test('HEAD')\n \n+    def test_dont_redirect(self):\n+        url = 'http://www.example.com/301'\n+        url2 = 'http://www.example.com/redirected'\n+        req = Request(url, meta={'dont_redirect': True})\n+        rsp = Response(url, headers={'Location': url2}, status=301)\n+\n+        r = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(r, Response)\n+        assert r is rsp\n+\n     def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1c943543a81f051576d3818df0fa15d2d7f0572", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 10 | Churn Cumulative: 90 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,6 +44,8 @@ class RetryMiddleware(object):\n         self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n+        if 'dont_retry' in request.meta:\n+            return response\n         if response.status in self.retry_http_codes:\n             reason = response_status_message(response.status)\n             return self._retry(request, reason, spider) or response\n\n@@ -27,6 +27,14 @@ class RetryTest(unittest.TestCase):\n         # dont retry 404s\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n+    def test_dont_retry(self):\n+        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n+        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n+\n+        # first retry\n+        r = self.mw.process_response(req, rsp, self.spider)\n+        assert r is rsp\n+\n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n         rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b9b3a2b646bd63396edaf74623334d37af49dcd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 35 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,6 @@ from twisted.internet.defer import TimeoutError as UserTimeoutError\n from twisted.web.client import PartialDownloadError\n \n from scrapy import log\n-from scrapy.utils.request import request_fingerprint\n from scrapy.utils.response import response_status_message\n from scrapy.conf import settings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b16eff227b7044143f2e56ae226291eb92bac93e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 215 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 0, 'rc1')\n-__version__ = \"0.10-rc1\"\n+version_info = (0, 10, 0, '')\n+__version__ = \"0.10\"\n \n import sys, os, warnings\n \n\n@@ -78,12 +78,9 @@ scripts = ['bin/scrapy']\n if os.name == 'nt':\n     scripts.append('extras/scrapy.bat')\n \n-# Dynamically calculate the version based on scrapy.__version__\n-version = \".\".join(map(str, __import__('scrapy').version_info[:2]))\n-\n setup_args = {\n     'name': 'Scrapy',\n-    'version': version,\n+    'version': __import__('scrapy').__version__,\n     'url': 'http://scrapy.org',\n     'description': 'A high-level Python Screen Scraping framework',\n     'long_description': 'Scrapy is a high level scraping and web crawling framework for writing spiders to crawl and parse web pages for all kinds of purposes, from information retrieval to monitoring or testing web sites.',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7a958a035f9153a30a2f3ed863f8493f6d546f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 212 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ MemoryUsage extension\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-import os\n import socket\n from pprint import pformat\n \n@@ -18,7 +17,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n from scrapy.stats import stats\n-from scrapy.utils.memory import get_vmvalue_from_procfs\n+from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported\n from scrapy.utils.engine import get_engine_status\n \n class MemoryUsage(object):\n@@ -26,7 +25,7 @@ class MemoryUsage(object):\n     def __init__(self):\n         if not settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n-        if not os.path.exists('/proc'):\n+        if not procfs_supported():\n             raise NotConfigured\n \n         self.warned = False\n\n@@ -1,13 +1,11 @@\n-import os\n-\n from twisted.trial import unittest\n \n-from scrapy.utils.memory import get_vmvalue_from_procfs\n+from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported\n \n class UtilsMemoryTestCase(unittest.TestCase):\n \n     def test_get_vmvalue_from_procfs(self):\n-        if not os.path.exists('/proc'):\n+        if not procfs_supported():\n             raise unittest.SkipTest('/proc filesystem not supported')\n         vmsize = get_vmvalue_from_procfs('VmSize')\n         vmrss = get_vmvalue_from_procfs('VmRSS')\n\n@@ -23,3 +23,10 @@ def get_vmvalue_from_procfs(vmkey='VmSize', pid=None):\n     # convert Vm value to bytes\n     return int(v[1]) * _vmvalue_scale[v[2]]\n \n+def procfs_supported():\n+    try:\n+        open('/proc/%d/status' % os.getpid())\n+    except IOError:\n+        return False\n+    else:\n+        return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf467fc37a91c95cff84d186c2cfa3e68b676d7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class CookiesMiddleware(object):\n         dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n     def process_request(self, request, spider):\n-        if request.meta.get('dont_merge_cookies', False):\n+        if 'dont_merge_cookies' in request.meta:\n             return\n \n         jar = self.jars[spider]\n@@ -31,7 +31,7 @@ class CookiesMiddleware(object):\n         self._debug_cookie(request)\n \n     def process_response(self, request, response, spider):\n-        if request.meta.get('dont_merge_cookies', False):\n+        if 'dont_merge_cookies' in request.meta:\n             return response\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fa96dbcd39a28efda06d6f327e26647133550f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 91 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 0, '')\n-__version__ = \"0.10\"\n+version_info = (0, 11, 0, 'dev')\n+__version__ = \"0.11\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0c7a946bdf93143e0c8f7b4812bf01ad2e9f889", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 95 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 0, '')\n-__version__ = \"0.10\"\n+version_info = (0, 10, 1, 'dev')\n+__version__ = \"0.10.1\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b76cd426907814843da81c8ea14de4f1010242dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 162 | Lines Deleted: 5 | Files Changed: 8 | Hunks: 10 | Methods Changed: 19 | Complexity Δ (Sum/Max): 14/4 | Churn Δ: 167 | Churn Cumulative: 338 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,12 +9,16 @@ class Config(object):\n \n     SECTION = 'scrapyd'\n \n-    def __init__(self):\n+    def __init__(self, values=None):\n+        if values is None:\n             sources = self._getsources()\n             default_config = pkgutil.get_data(__package__, 'default_scrapyd.conf')\n             self.cp = SafeConfigParser()\n             self.cp.readfp(StringIO(default_config))\n             self.cp.read(sources)\n+        else:\n+            self.cp = SafeConfigParser(values)\n+            self.cp.add_section(self.SECTION)\n \n     def _getsources(self):\n         sources = ['/etc/scrapyd/scrapyd.conf', r'c:\\scrapyd\\scrapyd.conf']\n\n@@ -6,11 +6,10 @@ from shutil import copyfileobj, rmtree\n from distutils.version import LooseVersion\n \n from zope.interface import implements\n-from twisted.application.service import Service\n \n from .interfaces import IEggStorage\n \n-class FilesystemEggStorage(Service):\n+class FilesystemEggStorage(object):\n \n     implements(IEggStorage)\n \n\n@@ -12,11 +12,11 @@ class IEggStorage(Interface):\n         project and version. If version is None, the latest version is\n         returned.\"\"\"\n \n-    def list(self, project):\n+    def list(project):\n         \"\"\"Return the list of versions which have eggs stored (for the given\n         project) in order (the latest version is the currently used).\"\"\"\n \n-    def delete(self, project, version=None):\n+    def delete(project, version=None):\n         \"\"\"Delete the egg stored for the given project and version. If should\n         also delete the project if no versions are left\"\"\"\n \n\n\n@@ -0,0 +1,41 @@\n+from cStringIO import StringIO\n+\n+from twisted.trial import unittest\n+\n+from zope.interface.verify import verifyObject\n+\n+from scrapyd.interfaces import IEggStorage\n+from scrapyd.config import Config\n+from scrapyd.eggstorage import FilesystemEggStorage\n+\n+class EggStorageTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        d = self.mktemp()\n+        config = Config(values={'eggs_dir': d})\n+        self.eggst = FilesystemEggStorage(config)\n+\n+    def test_interface(self):\n+        verifyObject(IEggStorage, self.eggst)\n+\n+    def test_put_get_list_delete(self):\n+        self.eggst.put(StringIO(\"egg01\"), 'mybot', '01')\n+        self.eggst.put(StringIO(\"egg03\"), 'mybot', '03')\n+        self.eggst.put(StringIO(\"egg02\"), 'mybot', '02')\n+\n+        self.assertEqual(self.eggst.list('mybot'), ['01', '02', '03'])\n+        self.assertEqual(self.eggst.list('mybot2'), [])\n+\n+        v, f = self.eggst.get('mybot')\n+        self.assertEqual(v, \"03\")\n+        self.assertEqual(f.read(), \"egg03\")\n+\n+        v, f = self.eggst.get('mybot', '02')\n+        self.assertEqual(v, \"02\")\n+        self.assertEqual(f.read(), \"egg02\")\n+\n+        self.eggst.delete('mybot', '02')\n+        self.assertEqual(self.eggst.list('mybot'), ['01', '03'])\n+\n+        self.eggst.delete('mybot')\n+        self.assertEqual(self.eggst.list('mybot'), [])\n\n@@ -0,0 +1,28 @@\n+import os\n+\n+from twisted.trial import unittest\n+\n+from zope.interface.verify import verifyObject\n+\n+from scrapyd.interfaces import IEnvironment\n+from scrapyd.config import Config\n+from scrapyd.environ import Environment\n+\n+class EggStorageTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        d = self.mktemp()\n+        os.mkdir(d)\n+        config = Config(values={'eggs_dir': d, 'logs_dir': d})\n+        self.environ = Environment(config)\n+\n+    def test_interface(self):\n+        verifyObject(IEnvironment, self.environ)\n+\n+    def test_get_environment(self):\n+        msg = {'project': 'mybot'}\n+        slot = 3\n+        env = self.environ.get_environment(msg, slot)\n+        self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n+        self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n+        self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))\n\n@@ -0,0 +1,41 @@\n+import os\n+\n+from twisted.trial import unittest\n+from twisted.internet.defer import Deferred\n+\n+from zope.interface.verify import verifyObject\n+\n+from scrapyd.interfaces import IPoller\n+from scrapyd.config import Config\n+from scrapyd.poller import QueuePoller\n+from scrapyd.utils import get_spider_queues\n+\n+class QueuePollerTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        d = self.mktemp()\n+        eggs_dir = os.path.join(d, 'eggs')\n+        dbs_dir = os.path.join(d, 'dbs')\n+        os.makedirs(eggs_dir)\n+        os.makedirs(dbs_dir)\n+        os.makedirs(os.path.join(eggs_dir, 'mybot1'))\n+        os.makedirs(os.path.join(eggs_dir, 'mybot2'))\n+        config = Config(values={'eggs_dir': eggs_dir, 'dbs_dir': dbs_dir})\n+        self.queues = get_spider_queues(eggs_dir, dbs_dir)\n+        self.poller = QueuePoller(config)\n+\n+    def test_interface(self):\n+        verifyObject(IPoller, self.poller)\n+\n+    def test_poll_next(self):\n+        self.queues['mybot1'].add('spider1')\n+        self.queues['mybot2'].add('spider2')\n+        d1 = self.poller.next()\n+        d2 = self.poller.next()\n+        self.failUnless(isinstance(d1, Deferred))\n+        self.failIf(hasattr(d1, 'result'))\n+        self.poller.poll()\n+        self.queues['mybot1'].pop()\n+        self.poller.poll()\n+        self.failUnlessEqual(d1.result, {'project': 'mybot1'})\n+        self.failUnlessEqual(d2.result, {'project': 'mybot2'})\n\n@@ -0,0 +1,44 @@\n+import os\n+\n+from twisted.trial import unittest\n+\n+from zope.interface.verify import verifyObject\n+\n+from scrapyd.interfaces import ISpiderScheduler\n+from scrapyd.config import Config\n+from scrapyd.scheduler import SpiderScheduler\n+from scrapyd.utils import get_spider_queues\n+\n+class SpiderSchedulerTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        d = self.mktemp()\n+        eggs_dir = self.eggs_dir = os.path.join(d, 'eggs')\n+        dbs_dir = os.path.join(d, 'dbs')\n+        os.mkdir(d)\n+        os.makedirs(eggs_dir)\n+        os.makedirs(dbs_dir)\n+        os.makedirs(os.path.join(eggs_dir, 'mybot1'))\n+        os.makedirs(os.path.join(eggs_dir, 'mybot2'))\n+        config = Config(values={'eggs_dir': eggs_dir, 'dbs_dir': dbs_dir})\n+        self.queues = get_spider_queues(eggs_dir, dbs_dir)\n+        self.sched = SpiderScheduler(config)\n+\n+    def test_interface(self):\n+        verifyObject(ISpiderScheduler, self.sched)\n+\n+    def test_list_update_projects(self):\n+        self.assertEqual(sorted(self.sched.list_projects()), sorted(['mybot1', 'mybot2']))\n+        os.makedirs(os.path.join(self.eggs_dir, 'mybot3'))\n+        self.sched.update_projects()\n+        self.assertEqual(sorted(self.sched.list_projects()), sorted(['mybot1', 'mybot2', 'mybot3']))\n+\n+    def test_schedule(self):\n+        q = self.queues['mybot1']\n+        self.failIf(q.count())\n+        self.sched.schedule('mybot1', 'myspider1', a='b')\n+        self.sched.schedule('mybot2', 'myspider2', c='d')\n+        self.assertEqual(q.pop(), {'name': 'myspider1', 'a': 'b'})\n+        q = self.queues['mybot2']\n+        self.assertEqual(q.pop(), {'name': 'myspider2', 'c': 'd'})\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#833baa60419b708ee0af18fed5c8e2ad31daf929", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 30 | Files Changed: 7 | Hunks: 20 | Methods Changed: 15 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 74 | Churn Cumulative: 395 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,27 +1,9 @@\n-\"\"\"\n-This module can be used to run a Scrapy project contained in an egg file\n-\n-To see all spiders in a project:\n-\n-    python -m scrapyd.eggrunner myproject.egg list\n-\n-To crawl a spider:\n-\n-    python -m scrapyd.eggrunner myproject.egg crawl somespider\n-\"\"\"\n-\n-import sys\n+import sys, os\n \n from scrapyd.eggutils import activate_egg\n \n-def main(eggpath, args):\n-    \"\"\"Run scrapy for the settings module name passed\"\"\"\n+eggpath = os.environ.get('SCRAPY_EGGFILE')\n+if eggpath:\n     activate_egg(eggpath)\n from scrapy.cmdline import execute\n-    execute(['scrapy'] + list(args))\n-\n-if __name__ == '__main__':\n-    if len(sys.argv) < 2:\n-        print \"usage: %s <eggfile> [scrapy_command args ...]\" % sys.argv[0]\n-        sys.exit(1)\n-    main(sys.argv[1], sys.argv[2:])\n+execute()\n\n@@ -26,7 +26,10 @@ class FilesystemEggStorage(object):\n \n     def get(self, project, version=None):\n         if version is None:\n+            try:\n                 version = self.list(project)[-1]\n+            except IndexError:\n+                return None, None\n         return version, open(self._eggpath(project, version), 'rb')\n \n     def list(self, project):\n\n@@ -12,9 +12,10 @@ def get_spider_list_from_eggfile(eggfile, project):\n             shutil.copyfileobj(eggfile, f)\n             f.flush()\n             eggfile.seek(0)\n-            pargs = [sys.executable, '-m', 'scrapyd.eggrunner', f.name, 'list']\n+            pargs = [sys.executable, '-m', 'scrapyd.eggrunner', 'list']\n             env = os.environ.copy()\n             env['SCRAPY_PROJECT'] = project\n+            env['SCRAPY_EGGFILE'] = f.name\n             proc = Popen(pargs, stdout=PIPE, cwd=tmpdir, env=env)\n             out = proc.communicate()[0]\n             return out.splitlines()\n\n@@ -11,11 +11,19 @@ class Environment(object):\n     def __init__(self, config):\n         self.dbs_dir = config.get('dbs_dir', 'dbs')\n         self.logs_dir = config.get('logs_dir', 'logs')\n+        if config.cp.has_section('settings'):\n+            self.settings = dict(config.cp.items('settings'))\n+        else:\n+            self.settings = {}\n \n-    def get_environment(self, message, slot):\n+    def get_environment(self, message, slot, eggpath):\n         project = message['project']\n         env = os.environ.copy()\n         env['SCRAPY_PROJECT'] = project\n+        if eggpath:\n+            env['SCRAPY_EGGFILE'] = eggpath\n+        elif project in self.settings:\n+            env['SCRAPY_SETTINGS_MODULE'] = self.settings[project]\n         dbpath = os.path.join(self.dbs_dir, '%s.db' % project)\n         env['SCRAPY_SQLITE_DB'] = dbpath\n         logpath = os.path.join(self.logs_dir, 'slot%s.log' % slot)\n\n@@ -10,7 +10,8 @@ class IEggStorage(Interface):\n     def get(project, version=None):\n         \"\"\"Return a tuple (version, file) with the the egg for the specified\n         project and version. If version is None, the latest version is\n-        returned.\"\"\"\n+        returned. If no egg is found for the given project/version (None, None)\n+        should be returned.\"\"\"\n \n     def list(project):\n         \"\"\"Return the list of versions which have eggs stored (for the given\n@@ -61,9 +62,13 @@ class ISpiderScheduler(Interface):\n class IEnvironment(Interface):\n     \"\"\"A component to generate the environment of crawler processes\"\"\"\n \n-    def get_environment(message, slot):\n+    def get_environment(message, slot, eggpath):\n         \"\"\"Return the environment variables to use for running the process.\n \n         `message` is the message received from the IPoller.next()\n         `slot` is the Launcher slot where the process will be running.\n+        `eggpath` is the path to an eggfile that contains the project code. The\n+           `eggpath` may be `None` if no egg was found for the project, in\n+           which case the project must be on the python path and its settings\n+           defined in scrapyd.conf [settings] section\n         \"\"\"\n\n@@ -39,14 +39,15 @@ class Launcher(Service):\n     def _spawn_process(self, message, slot):\n         project = message['project']\n         eggpath = self._get_eggpath(project)\n-        args = [sys.executable, '-m', self.egg_runner, eggpath, 'crawl']\n+        args = [sys.executable, '-m', self.egg_runner, 'crawl']\n         e = self.app.getComponent(IEnvironment)\n-        env = e.get_environment(message, slot)\n+        env = e.get_environment(message, slot, eggpath)\n         pp = ScrapyProcessProtocol(eggpath, slot)\n         pp.deferred.addBoth(self._process_finished, eggpath, slot)\n         reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n \n     def _process_finished(self, _, eggpath, slot):\n+        if eggpath:\n             os.remove(eggpath)\n         self._wait_for_project(slot)\n \n\n@@ -14,15 +14,29 @@ class EggStorageTest(unittest.TestCase):\n         d = self.mktemp()\n         os.mkdir(d)\n         config = Config(values={'eggs_dir': d, 'logs_dir': d})\n+        config.cp.add_section('settings')\n+        config.cp.set('settings', 'newbot', 'newbot.settings')\n         self.environ = Environment(config)\n \n     def test_interface(self):\n         verifyObject(IEnvironment, self.environ)\n \n-    def test_get_environment(self):\n+    def test_get_environment_with_eggfile(self):\n         msg = {'project': 'mybot'}\n         slot = 3\n-        env = self.environ.get_environment(msg, slot)\n+        env = self.environ.get_environment(msg, slot, '/path/to/file.egg')\n         self.assertEqual(env['SCRAPY_PROJECT'], 'mybot')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n         self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))\n+        self.assert_(env['SCRAPY_EGGFILE'].endswith('/path/to/file.egg'))\n+        self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n+\n+    def test_get_environment_without_eggfile(self):\n+        msg = {'project': 'newbot'}\n+        slot = 3\n+        env = self.environ.get_environment(msg, slot, None)\n+        self.assertEqual(env['SCRAPY_PROJECT'], 'newbot')\n+        self.assert_(env['SCRAPY_SQLITE_DB'].endswith('newbot.db'))\n+        self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))\n+        self.assertEqual(env['SCRAPY_SETTINGS_MODULE'], 'newbot.settings')\n+        self.failIf('SCRAPY_EGGFILE' in env)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c559b06a850d35924ea15e2bb7b90cb5cfe36e83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 55 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import sys, os\n+import os\n \n from scrapyd.eggutils import activate_egg\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9acc99e723295710d9b42525b97fe2246048398b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 73 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import warnings\n from functools import wraps\n \n-from twisted.internet.defer import maybeDeferred\n+from twisted.internet import defer, threads\n \n \n def deprecated(use_instead=None):\n@@ -24,5 +24,14 @@ def defers(func):\n     \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n     @wraps(func)\n     def wrapped(*a, **kw):\n-        return maybeDeferred(func, *a, **kw)\n+        return defer.maybeDeferred(func, *a, **kw)\n+    return wrapped\n+\n+def inthread(func):\n+    \"\"\"Decorator to call a function in a thread and return a deferred with the\n+    result\n+    \"\"\"\n+    @wraps(func)\n+    def wrapped(*a, **kw):\n+        return threads.deferToThread(func, *a, **kw)\n     return wrapped\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39499a24377770e6eeca91101f1be59e738764e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 23/12 | Churn Δ: 37 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from gzip import GzipFile\n from cStringIO import StringIO\n \n from scrapy.http import Response\n+from scrapy.core.downloader.responsetypes import responsetypes\n \n \n class HttpCompressionMiddleware(object):\n@@ -18,7 +19,9 @@ class HttpCompressionMiddleware(object):\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n-                response = response.replace(body=decoded_body)\n+                respcls = responsetypes.from_args(headers=response.headers, \\\n+                    url=response.url)\n+                response = response.replace(cls=respcls, body=decoded_body)\n                 if not content_encoding:\n                     del response.headers['Content-Encoding']\n \n\n@@ -46,9 +46,11 @@ class ResponseTypes(object):\n             basetype = \"%s/*\" % mimetype.split('/')[0]\n             return self.classes.get(basetype, Response)\n \n-    def from_content_type(self, content_type):\n+    def from_content_type(self, content_type, content_encoding=None):\n         \"\"\"Return the most appropiate Response class from an HTTP Content-Type\n         header \"\"\"\n+        if content_encoding:\n+            return Response\n         mimetype = content_type.split(';')[0].strip().lower()\n         return self.from_mimetype(mimetype)\n \n@@ -65,7 +67,8 @@ class ResponseTypes(object):\n         headers\"\"\"\n         cls = Response\n         if 'Content-Type' in headers:\n-            cls = self.from_content_type(headers['Content-type'])\n+            cls = self.from_content_type(headers['Content-type'], \\\n+                headers.get('Content-Encoding'))\n         if cls is Response and 'Content-Disposition' in headers:\n             cls = self.from_content_disposition(headers['Content-Disposition'])\n         return cls\n\n@@ -2,11 +2,14 @@ from __future__ import with_statement\n \n from unittest import TestCase\n from os.path import join, abspath, dirname\n+from cStringIO import StringIO\n+from gzip import GzipFile\n \n from scrapy.spider import BaseSpider\n-from scrapy.http import Response, Request\n+from scrapy.http import Response, Request, HtmlResponse\n from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware\n from scrapy.tests import tests_datadir\n+from scrapy.utils.encoding import resolve_encoding\n \n \n SAMPLEDIR = join(tests_datadir, 'compressed')\n@@ -96,3 +99,22 @@ class HttpCompressionTest(TestCase):\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n         self.assertEqual(newresponse.headers.getlist('Content-Encoding'), ['uuencode'])\n+\n+    def test_process_response_encoding_inside_body(self):\n+        headers = {\n+            'Content-Type': 'text/html',\n+            'Content-Encoding': 'gzip',\n+        }\n+        f = StringIO()\n+        plainbody = \"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n+        zf = GzipFile(fileobj=f, mode='wb')\n+        zf.write(plainbody)\n+        zf.close()\n+        response = Response(\"http;//www.example.com/\", headers=headers, body=f.getvalue())\n+        request = Request(\"http://www.example.com/\")\n+\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert isinstance(newresponse, HtmlResponse)\n+        self.assertEqual(newresponse.body, plainbody)\n+        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n+\n\n@@ -54,6 +54,7 @@ class ResponseTypesTest(unittest.TestCase):\n         mappings = [\n             ({'Content-Type': ['text/html; charset=utf-8']}, HtmlResponse),\n             ({'Content-Type': ['application/octet-stream'], 'Content-Disposition': ['attachment; filename=data.txt']}, TextResponse),\n+            ({'Content-Type': ['text/html; charset=utf-8'], 'Content-Encoding': ['gzip']}, Response),\n         ]\n         for source, cls in mappings:\n             source = Headers(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5fe9a49e51ba1490dbbb3f32450e18370a898237", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 27 | Churn Cumulative: 67 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import zlib\n from gzip import GzipFile\n from cStringIO import StringIO\n \n-from scrapy.http import Response\n+from scrapy.http import Response, TextResponse\n from scrapy.core.downloader.responsetypes import responsetypes\n \n \n@@ -21,7 +21,12 @@ class HttpCompressionMiddleware(object):\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 respcls = responsetypes.from_args(headers=response.headers, \\\n                     url=response.url)\n-                response = response.replace(cls=respcls, body=decoded_body)\n+                kwargs = dict(cls=respcls, body=decoded_body)\n+                if issubclass(respcls, TextResponse):\n+                    # force recalculating the encoding until we make sure the\n+                    # responsetypes guessing is reliable\n+                    kwargs['encoding'] = None\n+                response = response.replace(**kwargs)\n                 if not content_encoding:\n                     del response.headers['Content-Encoding']\n \n\n@@ -118,3 +118,21 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n \n+    def test_process_response_force_recalculate_encoding(self):\n+        headers = {\n+            'Content-Type': 'text/html',\n+            'Content-Encoding': 'gzip',\n+        }\n+        f = StringIO()\n+        plainbody = \"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n+        zf = GzipFile(fileobj=f, mode='wb')\n+        zf.write(plainbody)\n+        zf.close()\n+        response = HtmlResponse(\"http;//www.example.com/page.html\", headers=headers, body=f.getvalue())\n+        request = Request(\"http://www.example.com/\")\n+\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert isinstance(newresponse, HtmlResponse)\n+        self.assertEqual(newresponse.body, plainbody)\n+        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c01b8651ff20d514f2f2ad8fb4c939b1fee1f3b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 99 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 1, '')\n-__version__ = \"0.10.1\"\n+version_info = (0, 10, 2, '')\n+__version__ = \"0.10.2\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#edfba29dfd063b0b0450f252261fd644dad98842", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 103 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 2, '')\n-__version__ = \"0.10.2\"\n+version_info = (0, 11, 0, 'dev')\n+__version__ = \"0.11\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5908f052fdfbdeaab98d43796d3b69550812242", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 107 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 2, '')\n-__version__ = \"0.10.2\"\n+version_info = (0, 10, 3, 'dev')\n+__version__ = \"0.10.3\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cecbcdc5b47ba4877ed5c11f9a277d3ebdef21d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 92 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,8 @@ class Launcher(Service):\n     def _get_eggpath(self, project):\n         eggstorage = self.app.getComponent(IEggStorage)\n         version, eggf = eggstorage.get(project)\n+        if eggf is None:\n+            return\n         prefix = '%s-%s-' % (project, version)\n         fd, eggpath = mkstemp(prefix=prefix, suffix='.egg')\n         lf = os.fdopen(fd, 'wb')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cad077574bdc808a691b23ac6056fceaea73a9da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 52 | Churn Cumulative: 713 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.15789473684210525\n\nDIFF:\n@@ -272,7 +272,11 @@ class ImagesPipeline(MediaPipeline):\n         stats.inc_value('image_status_count/%s' % status, spider=spider)\n \n     def convert_image(self, image, size=None):\n-        if image.mode != 'RGB':\n+        if image.format == 'PNG' and image.mode == 'RGBA':\n+            background = Image.new('RGBA', image.size, (255, 255, 255))\n+            background.paste(image, image)\n+            image = background.convert('RGB')\n+        elif image.mode != 'RGB':\n             image = image.convert('RGB')\n \n         if size:\n\n@@ -1,4 +1,5 @@\n import os\n+from cStringIO import StringIO\n from tempfile import mkdtemp\n from shutil import rmtree\n \n@@ -7,13 +8,18 @@ from twisted.trial import unittest\n from scrapy.crawler import Crawler\n from scrapy.conf import settings\n \n-\n-class ImagesPipelineTestCase(unittest.TestCase):\n-    def setUp(self):\n try:\n     import Image\n+    skip = False\n except ImportError, e:\n-            raise unittest.SkipTest(e)\n+    skip = True\n+\n+\n+class ImagesPipelineTestCase(unittest.TestCase):\n+\n+    skip = skip\n+\n+    def setUp(self):\n         from scrapy.contrib.pipeline.images import ImagesPipeline\n \n         self.crawler = Crawler(settings)\n@@ -59,8 +65,36 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n         key = 'some/image/key.jpg'\n         path = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n-        self.assertEqual(self.pipeline.store._get_filesystem_path(key),\n-            path)\n+        self.assertEqual(self.pipeline.store._get_filesystem_path(key), path)\n+\n+    def test_convert_image(self):\n+        SIZE = (100, 100)\n+        # straigh forward case: RGB and JPEG\n+        COLOUR = (0, 127, 255)\n+        im = _create_image('JPEG', 'RGB', SIZE, COLOUR)\n+        converted, _ = self.pipeline.convert_image(im)\n+        self.assertEquals(converted.mode, 'RGB')\n+        self.assertEquals(converted.getcolors(), [(10000, COLOUR)])\n+\n+        # check that thumbnail keep image ratio\n+        thumbnail, _ = self.pipeline.convert_image(converted, size=(10, 25))\n+        self.assertEquals(thumbnail.mode, 'RGB')\n+        self.assertEquals(thumbnail.size, (10, 10))\n+\n+        # transparency case: RGBA and PNG\n+        COLOUR = (0, 127, 255, 50)\n+        im = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+        converted, _ = self.pipeline.convert_image(im)\n+        self.assertEquals(converted.mode, 'RGB')\n+        self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n+\n+\n+\n+def _create_image(format, *a, **kw):\n+    buf = StringIO()\n+    Image.new(*a, **kw).save(buf, format)\n+    buf.seek(0)\n+    return Image.open(buf)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f41fd3214e2bb717b5f2236aae2ed24329b9ad34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 345 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ Scrapy engine tests\n This starts a testing web server (using twisted.server.Site) and then crawls it\n with the Scrapy crawler.\n \n-To view the testing web server in a brwoser you can start it by running this\n+To view the testing web server in a browser you can start it by running this\n module with the ``runserver`` argument::\n \n     python test_engine.py runserver\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa86f180b9a76c86867c0acd19d0aff9e85f6f3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 252 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,8 +71,8 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n         self.current_link = None\n \n     def handle_data(self, data):\n-        if self.current_link and not self.current_link.text:\n-            self.current_link.text = data.strip()\n+        if self.current_link:\n+            self.current_link.text = self.current_link.text + data.strip()\n \n     def matches(self, url):\n         \"\"\"This extractor matches with any url, since\n\n@@ -14,6 +14,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         <p><a href=\"/about.html\">About us</a></p>\n         <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n         <p><a href=\"../othercat.html\">Other category</a></p>\n+        <p><a href=\"/\">&gt;&gt;</a></p>\n         <p><a href=\"/\" /></p>\n         </body></html>\"\"\"\n         response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n@@ -23,6 +24,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), \n                           Link(url='http://example.org/about.html', text='About us'),\n                           Link(url='http://example.org/othercat.html', text='Other category'), \n+                          Link(url='http://example.org/', text='>>'),\n                           Link(url='http://example.org/', text='')])\n \n     def test_base_url(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#400c4134af7d14f5f0ebe5efacd0e0129aff8799", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 48 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from __future__ import with_statement\n+\n import os, sys, shutil, pkg_resources\n from subprocess import Popen, PIPE\n from tempfile import NamedTemporaryFile, mkdtemp\n\n@@ -0,0 +1,11 @@\n+import pkgutil, unittest\n+from cStringIO import StringIO\n+\n+from scrapyd.eggutils import get_spider_list_from_eggfile\n+\n+class EggUtilsTest(unittest.TestCase):\n+\n+    def test_get_spider_list_from_eggfile(self):\n+        eggfile = StringIO(pkgutil.get_data(__package__, 'mybot.egg'))\n+        spiders = get_spider_list_from_eggfile(eggfile, 'mybot')\n+        self.assertEqual(set(spiders), set(['spider1', 'spider2']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9288f622f9d24510e83c83ae4e09c43738e9d671", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 73 | Churn Cumulative: 404 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -37,13 +37,23 @@ class FormRequest(Request):\n             self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n \n     @classmethod\n-    def from_response(cls, response, formnumber=0, formdata=None, \n+    def from_response(cls, response, formname=None, formnumber=0, formdata=None, \n                       clickdata=None, dont_click=False, **kwargs):\n         encoding = getattr(response, 'encoding', 'utf-8')\n         forms = ParseFile(StringIO(response.body), response.url,\n                           encoding=encoding, backwards_compat=False)\n         if not forms:\n             raise ValueError(\"No <form> element found in %s\" % response)\n+        \n+        form = None\n+\n+        if formname:\n+            for f in forms:\n+                if f.name == formname:\n+                    form = f\n+                    break\n+\n+        if not form:\n             try:\n                 form = forms[formnumber]\n             except IndexError:\n\n@@ -330,6 +330,67 @@ class FormRequestTest(RequestTest):\n         response = Response(\"http://www.example.com/lala.html\", body=respbody)\n         self.assertRaises(IndexError, self.request_class.from_response, response, formnumber=1)\n \n+    def test_from_response_noformname(self):\n+        respbody = \"\"\"\n+<form action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+<input type=\"hidden\" name=\"two\" value=\"2\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/formname.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, formdata={'two':'3'}, callback=lambda x: x)\n+        self.assertEqual(r1.method, 'POST')\n+        self.assertEqual(r1.headers['Content-type'], 'application/x-www-form-urlencoded')\n+        fs = cgi.FieldStorage(StringIO(r1.body), r1.headers, environ={\"REQUEST_METHOD\": \"POST\"})\n+        self.assertEqual(fs['one'].value, '1')\n+        self.assertEqual(fs['two'].value, '3')\n+\n+\n+    def test_from_response_formname_exists(self):\n+        respbody = \"\"\"\n+<form action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+<input type=\"hidden\" name=\"two\" value=\"2\">\n+</form>\n+<form name=\"form2\" action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"three\" value=\"3\">\n+<input type=\"hidden\" name=\"four\" value=\"4\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/formname.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, formname=\"form2\", callback=lambda x: x)\n+        self.assertEqual(r1.method, 'POST')\n+        fs = cgi.FieldStorage(StringIO(r1.body), r1.headers, environ={\"REQUEST_METHOD\": \"POST\"})\n+        self.assertEqual(fs['three'].value, \"3\")\n+        self.assertEqual(fs['four'].value, \"4\")\n+\n+    def test_from_response_formname_notexist(self):\n+        respbody = \"\"\"\n+<form name=\"form1\" action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+</form>\n+<form name=\"form2\" action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"two\" value=\"2\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/formname.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, formname=\"form3\", callback=lambda x: x)\n+        self.assertEqual(r1.method, 'POST')\n+        fs = cgi.FieldStorage(StringIO(r1.body), r1.headers, environ={\"REQUEST_METHOD\": \"POST\"})\n+        self.assertEqual(fs['one'].value, \"1\")\n+\n+    def test_from_response_formname_errors_formnumber(self):\n+        respbody = \"\"\"\n+<form name=\"form1\" action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+</form>\n+<form name=\"form2\" action=\"post.php\" method=\"POST\">\n+<input type=\"hidden\" name=\"two\" value=\"2\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/formname.html\", body=respbody)\n+        self.assertRaises(IndexError, self.request_class.from_response, response, formname=\"form3\", formnumber=2)\n+\n \n class XmlRpcRequestTest(RequestTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
