{"custom_id": "keras#ad74f7f7b8017ec30d80422ec38d526baaafc8f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 406 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -233,7 +233,7 @@ class Dot(Merge):\n     `(2, 3, 5)` and `(2, 10, 3)`. The batch dimension should be\n     of same size for both the inputs, and `axes` should correspond\n     to the dimensions that have the same size in the corresponding\n-    inputs. e.g. with `axes=[1, 2]`, the dot product of `x`, and `y`\n+    inputs. e.g. with `axes=(1, 2)`, the dot product of `x`, and `y`\n     will result in a tensor with shape `(2, 5, 10)`\n \n     Examples:\n@@ -255,8 +255,9 @@ class Dot(Merge):\n             desired axis from the second input, respectively. Note that the\n             size of the two selected axes must match.\n         normalize: Whether to L2-normalize samples along the dot product axis\n-            before taking the dot product. If set to True, then the output of\n-            the dot product is the cosine proximity between the two samples.\n+            before taking the dot product. If set to `True`, then\n+            the output of the dot product is the cosine proximity\n+            between the two samples.\n         **kwargs: Standard layer keyword arguments.\n \n     Returns:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dea0fc5b6fb8b5e3c404afb58d6c6f830f451ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 518 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 16 | Methods Changed: 17 | Complexity Δ (Sum/Max): 30/19 | Churn Δ: 525 | Churn Cumulative: 5803 | Contributors (this commit): 8 | Commits (past 90d): 60 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -160,7 +160,7 @@ class CosineSimilarity(LossFunctionWrapper):\n     Formula:\n \n     ```python\n-    loss = mean(square(log(y_true + 1) - log(y_pred + 1)))\n+    loss = -sum(l2_norm(y_true) * l2_norm(y_pred))\n     ```\n \n     Args:\n@@ -188,6 +188,7 @@ class Huber(LossFunctionWrapper):\n     \"\"\"Computes the Huber loss between `y_true` & `y_pred`.\n \n     Formula:\n+\n     ```python\n     for x in error:\n         if abs(x) <= delta:\n@@ -1204,9 +1205,9 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     >>> y_pred = np.random.random(size=(2, 3))\n     >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n     \"\"\"\n-    epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n+    epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n     diff = ops.abs((y_true - y_pred) / ops.maximum(ops.abs(y_true), epsilon))\n     return 100.0 * ops.mean(diff, axis=-1)\n@@ -1380,7 +1381,7 @@ def log_cosh(y_true, y_pred):\n     log2 = ops.convert_to_tensor(ops.log(2.0), dtype=y_pred.dtype)\n \n     def _logcosh(x):\n-        return x + ops.softplus(-2.0 * x) - log2\n+        return x + ops.softplus(x * -2.0) - log2\n \n     return ops.mean(_logcosh(y_pred - y_true), axis=-1)\n \n\n@@ -1,17 +1,30 @@\n+from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n+from keras_core.losses.loss import squeeze_to_same_rank\n+from keras_core.losses.losses import log_cosh\n+from keras_core.losses.losses import mean_absolute_error\n+from keras_core.losses.losses import mean_absolute_percentage_error\n from keras_core.losses.losses import mean_squared_error\n+from keras_core.losses.losses import mean_squared_logarithmic_error\n from keras_core.metrics import reduction_metrics\n+from keras_core.utils.numerical_utils import normalize\n \n \n @keras_core_export(\"keras_core.metrics.MeanSquaredError\")\n class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n \n+    Formula:\n+\n+    ```python\n+    loss = mean(square(y_true - y_pred))\n+    ```\n+\n     Args:\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n-    Standalone usage:\n+    Example:\n \n     >>> m = keras_core.metrics.MeanSquaredError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n@@ -24,3 +37,270 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.MeanAbsoluteError\")\n+class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the mean absolute error between the labels and predictions.\n+\n+    Formula:\n+\n+    ```python\n+    loss = mean(abs(y_true - y_pred))\n+    ```\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.MeanAbsoluteError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result()\n+    0.25\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.MeanAbsoluteError()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"mean_absolute_error\", dtype=None):\n+        super().__init__(mean_absolute_error, name, dtype=dtype)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.MeanAbsolutePercentageError\")\n+class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes mean absolute percentage error between `y_true` and `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    loss = 100 * mean(abs((y_true - y_pred) / y_true))\n+    ```\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.MeanAbsolutePercentageError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result()\n+    250000000.0\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    500000000.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.MeanAbsolutePercentageError()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"mean_absolute_percentage_error\", dtype=None):\n+        super().__init__(mean_absolute_percentage_error, name, dtype=dtype)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.MeanSquaredLogarithmicError\")\n+class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes mean squared logarithmic error between `y_true` and `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    loss = mean(square(log(y_true + 1) - log(y_pred + 1)))\n+    ```\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.MeanSquaredLogarithmicError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result()\n+    0.12011322\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    0.24022643\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.MeanSquaredLogarithmicError()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"mean_squared_logarithmic_error\", dtype=None):\n+        super().__init__(mean_squared_logarithmic_error, name, dtype=dtype)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.CosineSimilarity\")\n+class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the cosine similarity between the labels and predictions.\n+\n+    Formula:\n+\n+    ```python\n+    loss = sum(l2_norm(y_true) * l2_norm(y_pred))\n+    ```\n+    See: [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n+    This metric keeps the average cosine similarity between `predictions` and\n+    `labels` over a stream of data.\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        axis: (Optional) Defaults to -1. The dimension along which the cosine\n+            similarity is computed.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n+    >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n+    >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n+    >>> # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n+    >>> #        = ((0. + 0.) +  (0.5 + 0.5)) / 2\n+    >>> m = keras_core.metrics.CosineSimilarity(axis=1)\n+    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n+    >>> m.result()\n+    0.49999997\n+    >>> m.reset_state()\n+    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],\n+    ...                sample_weight=[0.3, 0.7])\n+    >>> m.result()\n+    0.6999999\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.CosineSimilarity(axis=1)])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"cosine_similarity\", dtype=None, axis=-1):\n+        super().__init__(cosine_similarity, name, dtype=dtype, axis=axis)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.LogCoshError\")\n+class LogCoshError(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n+\n+    Formula:\n+\n+    ```python\n+    error = y_pred - y_true\n+    logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1)\n+    ```\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.LogCoshError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result()\n+    0.10844523\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    0.21689045\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[keras_core.metrics.LogCoshError()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"logcosh\", dtype=None):\n+        super().__init__(log_cosh, name, dtype=dtype)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+def cosine_similarity(y_true, y_pred, axis=-1):\n+    \"\"\"Computes the cosine similarity between labels and predictions.\n+\n+    Formula:\n+\n+    ```python\n+    loss = sum(l2_norm(y_true) * l2_norm(y_pred))\n+    ```\n+\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+        axis: Axis along which to determine similarity. Defaults to -1.\n+\n+    Returns:\n+        Cosine similarity tensor.\n+\n+    Example:\n+\n+    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n+    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n+    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n+    [0., 0.99999994, -0.99999994]\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n+    y_true, y_pred = squeeze_to_same_rank(y_true, y_pred)\n+    y_pred = normalize(y_pred, axis=axis)\n+    y_true = normalize(y_true, axis=axis)\n+    return ops.sum(y_true * y_pred, axis=axis)\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n from keras_core import testing\n-from keras_core.metrics import regression_metrics\n+from keras_core.metrics import regression_metrics as metrics\n \n \n class MeanSquaredErrorTest(testing.TestCase):\n@@ -10,7 +10,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n         pass\n \n     def test_unweighted(self):\n-        mse_obj = regression_metrics.MeanSquaredError()\n+        mse_obj = metrics.MeanSquaredError()\n         y_true = np.array(\n             [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n         )\n@@ -23,7 +23,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n         self.assertAllClose(0.5, result, atol=1e-5)\n \n     def test_weighted(self):\n-        mse_obj = regression_metrics.MeanSquaredError()\n+        mse_obj = metrics.MeanSquaredError()\n         y_true = np.array(\n             [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n         )\n@@ -33,3 +33,233 @@ class MeanSquaredErrorTest(testing.TestCase):\n         sample_weight = np.array([1.0, 1.5, 2.0, 2.5])\n         result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(0.54285, result, atol=1e-5)\n+\n+\n+class CosineSimilarityTest(testing.TestCase):\n+    def l2_norm(self, x, axis):\n+        epsilon = 1e-12\n+        square_sum = np.sum(np.square(x), axis=axis, keepdims=True)\n+        x_inv_norm = 1 / np.sqrt(np.maximum(square_sum, epsilon))\n+        return np.multiply(x, x_inv_norm)\n+\n+    def setup(self, axis=1):\n+        self.np_y_true = np.asarray([[1, 9, 2], [-5, -2, 6]], dtype=np.float32)\n+        self.np_y_pred = np.asarray([[4, 8, 12], [8, 1, 3]], dtype=np.float32)\n+\n+        y_true = self.l2_norm(self.np_y_true, axis)\n+        y_pred = self.l2_norm(self.np_y_pred, axis)\n+        self.expected_loss = np.sum(np.multiply(y_true, y_pred), axis=(axis,))\n+\n+        self.y_true = self.np_y_true\n+        self.y_pred = self.np_y_pred\n+\n+    def test_config(self):\n+        cosine_obj = metrics.CosineSimilarity(\n+            axis=2, name=\"my_cos\", dtype=\"int32\"\n+        )\n+        self.assertEqual(cosine_obj.name, \"my_cos\")\n+        self.assertEqual(cosine_obj.dtype, \"int32\")\n+\n+        # Check save and restore config\n+        cosine_obj2 = metrics.CosineSimilarity.from_config(\n+            cosine_obj.get_config()\n+        )\n+        self.assertEqual(cosine_obj2.name, \"my_cos\")\n+        self.assertEqual(cosine_obj2._dtype, \"int32\")\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        cosine_obj = metrics.CosineSimilarity()\n+        loss = cosine_obj(self.y_true, self.y_pred)\n+        expected_loss = np.mean(self.expected_loss)\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        cosine_obj = metrics.CosineSimilarity()\n+        sample_weight = np.asarray([1.2, 3.4])\n+        loss = cosine_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+        expected_loss = np.sum(self.expected_loss * sample_weight) / np.sum(\n+            sample_weight\n+        )\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_axis(self):\n+        self.setup(axis=1)\n+        cosine_obj = metrics.CosineSimilarity(axis=1)\n+        loss = cosine_obj(self.y_true, self.y_pred)\n+        expected_loss = np.mean(self.expected_loss)\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+\n+class MeanAbsoluteErrorTest(testing.TestCase):\n+    def test_config(self):\n+        mae_obj = metrics.MeanAbsoluteError(name=\"my_mae\", dtype=\"int32\")\n+        self.assertEqual(mae_obj.name, \"my_mae\")\n+        self.assertEqual(mae_obj._dtype, \"int32\")\n+\n+        # Check save and restore config\n+        mae_obj2 = metrics.MeanAbsoluteError.from_config(mae_obj.get_config())\n+        self.assertEqual(mae_obj2.name, \"my_mae\")\n+        self.assertEqual(mae_obj2._dtype, \"int32\")\n+\n+    def test_unweighted(self):\n+        mae_obj = metrics.MeanAbsoluteError()\n+        y_true = np.array(\n+            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n+        )\n+        y_pred = np.array(\n+            [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n+        )\n+\n+        mae_obj.update_state(y_true, y_pred)\n+        result = mae_obj.result()\n+        self.assertAllClose(0.5, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        mae_obj = metrics.MeanAbsoluteError()\n+        y_true = np.array(\n+            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n+        )\n+        y_pred = np.array(\n+            [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n+        )\n+        sample_weight = np.array([1.0, 1.5, 2.0, 2.5])\n+        result = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.54285, result, atol=1e-5)\n+\n+\n+class MeanAbsolutePercentageErrorTest(testing.TestCase):\n+    def test_config(self):\n+        mape_obj = metrics.MeanAbsolutePercentageError(\n+            name=\"my_mape\", dtype=\"int32\"\n+        )\n+        self.assertEqual(mape_obj.name, \"my_mape\")\n+        self.assertEqual(mape_obj._dtype, \"int32\")\n+\n+        # Check save and restore config\n+        mape_obj2 = metrics.MeanAbsolutePercentageError.from_config(\n+            mape_obj.get_config()\n+        )\n+        self.assertEqual(mape_obj2.name, \"my_mape\")\n+        self.assertEqual(mape_obj2._dtype, \"int32\")\n+\n+    def test_unweighted(self):\n+        mape_obj = metrics.MeanAbsolutePercentageError()\n+        y_true = np.array(\n+            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n+        )\n+        y_pred = np.array(\n+            [\n+                [0, 0, 1, 1, 0],\n+                [1, 1, 1, 1, 1],\n+                [0, 1, 0, 1, 0],\n+                [1, 1, 1, 1, 1],\n+            ],\n+            dtype=\"float32\",\n+        )\n+\n+        result = mape_obj(y_true, y_pred)\n+        self.assertAllClose(35e7, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        mape_obj = metrics.MeanAbsolutePercentageError()\n+        y_true = np.array(\n+            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n+        )\n+        y_pred = np.array(\n+            [\n+                [0, 0, 1, 1, 0],\n+                [1, 1, 1, 1, 1],\n+                [0, 1, 0, 1, 0],\n+                [1, 1, 1, 1, 1],\n+            ],\n+            dtype=\"float32\",\n+        )\n+\n+        sample_weight = np.array([1.0, 1.5, 2.0, 2.5])\n+        result = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(40e7, result, atol=1e-5)\n+\n+\n+class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n+    def test_config(self):\n+        msle_obj = metrics.MeanSquaredLogarithmicError(\n+            name=\"my_msle\", dtype=\"int32\"\n+        )\n+        self.assertEqual(msle_obj.name, \"my_msle\")\n+        self.assertEqual(msle_obj._dtype, \"int32\")\n+\n+        # Check save and restore config\n+        msle_obj2 = metrics.MeanSquaredLogarithmicError.from_config(\n+            msle_obj.get_config()\n+        )\n+        self.assertEqual(msle_obj2.name, \"my_msle\")\n+        self.assertEqual(msle_obj2._dtype, \"int32\")\n+\n+    def test_unweighted(self):\n+        msle_obj = metrics.MeanSquaredLogarithmicError()\n+        y_true = np.array(\n+            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n+        )\n+        y_pred = np.array(\n+            [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n+        )\n+\n+        msle_obj.update_state(y_true, y_pred)\n+        result = msle_obj.result()\n+        self.assertAllClose(0.24022, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        msle_obj = metrics.MeanSquaredLogarithmicError()\n+        y_true = np.array(\n+            [[0, 1, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0], [0, 0, 0, 0, 1]]\n+        )\n+        y_pred = np.array(\n+            [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n+        )\n+        sample_weight = np.array([1.0, 1.5, 2.0, 2.5])\n+        result = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.26082, result, atol=1e-5)\n+\n+\n+class LogCoshErrorTest(testing.TestCase):\n+    def setup(self):\n+        y_true = np.asarray([[1, 9, 2], [-5, -2, 6]], dtype=np.float32)\n+        y_pred = np.asarray([[4, 8, 12], [8, 1, 3]], dtype=np.float32)\n+\n+        self.batch_size = 6\n+        error = y_pred - y_true\n+        self.expected_results = np.log((np.exp(error) + np.exp(-error)) / 2)\n+\n+        self.y_pred = y_pred\n+        self.y_true = y_true\n+\n+    def test_config(self):\n+        logcosh_obj = metrics.LogCoshError(name=\"logcosh\", dtype=\"int32\")\n+        self.assertEqual(logcosh_obj.name, \"logcosh\")\n+        self.assertEqual(logcosh_obj._dtype, \"int32\")\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        logcosh_obj = metrics.LogCoshError()\n+\n+        logcosh_obj.update_state(self.y_true, self.y_pred)\n+        result = logcosh_obj.result()\n+        expected_result = np.sum(self.expected_results) / self.batch_size\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        logcosh_obj = metrics.LogCoshError(dtype=\"float32\")\n+        sample_weight = np.array([[1.2], [3.4]])\n+        result = logcosh_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+\n+        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n+            (2, 3)\n+        )\n+        expected_result = np.multiply(self.expected_results, sample_weight)\n+        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44c0b83fbb0ba20a1eb8effd0950aa3125c2ce9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 15748 | Contributors (this commit): 70 | Commits (past 90d): 3 | Contributors (cumulative): 70 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1599,11 +1599,11 @@ class ImageDataGenerator:\n             dictionary containing the mapping from class names to class\n             indices can be obtained via the attribute `class_indices`.\n           class_mode: One of \"categorical\", \"binary\", \"sparse\",\n-                \"input\", or None. Default: \"categorical\".\n+            \"input\", or None.\n             Determines the type of label arrays that are returned:\n               - \"categorical\" will be 2D one-hot encoded labels,\n               - \"binary\" will be 1D binary labels,\n-                    \"sparse\" will be 1D integer labels,\n+              - \"sparse\" will be 1D integer labels,\n               - \"input\" will be images identical\n                 to input images (mainly used to work with autoencoders).\n               - If None, no labels are returned\n@@ -1612,9 +1612,10 @@ class ImageDataGenerator:\n                 Please note that in case of class_mode None,\n                 the data still needs to reside in a subdirectory\n                 of `directory` for it to work correctly.\n-            batch_size: Size of the batches of data (default: 32).\n-            shuffle: Whether to shuffle the data (default: True) If set to\n-              False, sorts the data in alphanumeric order.\n+              Defaults to \"categorical\".\n+          batch_size: Size of the batches of data. Defaults to `32`.\n+          shuffle: Whether to shuffle the data If `False`, sorts the\n+            data in alphanumeric order. Defaults to `True`.\n           seed: Optional random seed for shuffling and transformations.\n           save_to_dir: None or str (default: None). This allows you to\n             optionally specify a directory to which to save the augmented\n@@ -1623,10 +1624,10 @@ class ImageDataGenerator:\n           save_prefix: Str. Prefix to use for filenames of saved pictures\n             (only relevant if `save_to_dir` is set).\n           save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n-              \"tif\", \"jpg\" (only relevant if `save_to_dir` is set). Default:\n-              \"png\".\n+            \"tif\", \"jpg\" (only relevant if `save_to_dir` is set).\n+            Defaults to \"png\".\n           follow_links: Whether to follow symlinks inside\n-                class subdirectories (default: False).\n+            class subdirectories. Defaults to `False`.\n           subset: Subset of data (`\"training\"` or `\"validation\"`) if\n             `validation_split` is set in `ImageDataGenerator`.\n           interpolation: Interpolation method used to resample the image if\n@@ -1634,7 +1635,7 @@ class ImageDataGenerator:\n             Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n             If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n             supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n-              `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n+            `\"hamming\"` are also supported. Defaults to `\"nearest\"`.\n           keep_aspect_ratio: Boolean, whether to resize images to a target\n             size without aspect ratio distortion. The image is cropped in\n             the center with target aspect ratio before resizing.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c457e2a6058711767fdb162fd8d18749a501eb11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 127 | Churn Cumulative: 1449 | Contributors (this commit): 6 | Commits (past 90d): 76 | Contributors (cumulative): 11 | DMM Complexity: 0.8732394366197183\n\nDIFF:\n@@ -28,7 +28,8 @@ def draw_seed(seed):\n     from keras_core.backend import convert_to_tensor\n \n     if isinstance(seed, SeedGenerator):\n-        new_seed_value = seed.state.value\n+        # Use * 1 to create a copy\n+        new_seed_value = seed.state.value * 1\n         seed.state.assign(\n             seed.state + convert_to_tensor([0, 1], dtype=\"uint32\")\n         )\n\n@@ -415,15 +415,58 @@ class Layer(Operation):\n         raise NotImplementedError\n \n     def stateless_call(\n-        self, trainable_variables, non_trainable_variables, *args, **kwargs\n+        self,\n+        trainable_variables,\n+        non_trainable_variables,\n+        *args,\n+        return_losses=False,\n+        **kwargs,\n     ):\n-        # TODO: also handle losses\n+        \"\"\"Call the layer without any side effects.\n \n+        Args:\n+            trainable_variables: List of trainable variables of the model.\n+            non_trainable_variables: List of non-trainable variables of the model.\n+            *args: Positional argumets to be passed to `call()`.\n+            return_losses: If `True`, `stateless_call()` will return the list of\n+                losses created during `call()` as part of its return values.\n+            **kwargs: Keyword arguments to be passed to `call()`.\n+\n+        Returns:\n+            A tuple. By default, returns `(outputs, non_trainable_variables)`.\n+                If `return_losses = True`, then returns\n+                `(outputs, non_trainable_variables, losses)`.\n+\n+        Note: `non_trainable_variables` include not only non-trainable weights\n+        such as `BatchNormalization` statistics, but also RNG seed state\n+        (if there are any random operations part of the layer, such as dropout),\n+        and `Metric` state (if there are any metrics attached to the layer).\n+        These are all elements of state of the layer.\n+\n+        Example:\n+\n+        ```python\n+        model = ...\n+        data = ...\n+        trainable_variables = model.trainable_variables\n+        non_trainable_variables = model.non_trainable_variables\n+        # Call the model with zero side effects\n+        outputs, non_trainable_variables = model.stateless_call(\n+            trainable_variables,\n+            non_trainable_variables,\n+            data,\n+        )\n+        # Attach the updated state to the model\n+        # (until you do this, the model is still in its pre-call state).\n+        for ref_var, value in zip(model.non_trainable_variables, non_trainable_variables):\n+            ref_var.assign(value)\n+        ```\n+        \"\"\"\n         self._check_super_called()\n \n         if not self.built:\n             raise ValueError(\n-                \"To call stateless_call, {self.__class__.__name__} must be \"\n+                f\"To call stateless_call, {self.__class__.__name__} must be \"\n                 \"built (i.e. its variables must have been already created). \"\n                 \"You can build it by calling it on some data.\"\n             )\n@@ -452,7 +495,9 @@ class Layer(Operation):\n         mapping = list(trainable_mapping) + list(non_trainable_mapping)\n \n         # Call in stateless scope\n-        with backend.StatelessScope(state_mapping=mapping) as scope:\n+        with backend.StatelessScope(\n+            state_mapping=mapping, collect_losses=return_losses\n+        ) as scope:\n             outputs = self.call(*args, **kwargs)\n \n         # Gather updated non-trainable variables\n@@ -463,6 +508,9 @@ class Layer(Operation):\n                 non_trainable_variables.append(new_v)\n             else:\n                 non_trainable_variables.append(v)\n+\n+        if return_losses:\n+            return outputs, non_trainable_variables, scope.losses[:]\n         return outputs, non_trainable_variables\n \n     def compute_output_spec(self, *args, **kwargs):\n\n@@ -332,3 +332,71 @@ class LayerTest(testing.TestCase):\n         x2._keras_mask = backend.numpy.ones((4,))\n         layer((x1_1, x1_2), x2)\n         layer(x1=(x1_1, x1_2), x2=x2)\n+\n+    def test_stateless_call(self):\n+        class TestLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self._seed_generator = backend.random.SeedGenerator(1337)\n+                self.ntw = self.add_weight(\n+                    shape=(),\n+                    initializer=\"zeros\",\n+                    trainable=False,\n+                )\n+                self.tw = self.add_weight(\n+                    shape=(),\n+                    initializer=\"zeros\",\n+                    trainable=True,\n+                )\n+                self.built = True\n+\n+            def call(self, x):\n+                x = backend.convert_to_tensor(x, dtype=\"float32\")\n+                self.add_loss(ops.sum(x))\n+                self.ntw.assign(ops.sum(x))\n+                x = x + backend.random.normal(\n+                    shape=(), seed=self._seed_generator\n+                )\n+                return x + self.tw + self.ntw\n+\n+        data = np.random.random((3, 4))\n+        layer = TestLayer()\n+        out = layer(data)\n+        layer1 = TestLayer()\n+        out1 = layer1(data)\n+        # Check that the layer is in fact deterministic\n+        self.assertAllClose(out, out1)\n+\n+        # Test stateless_call correctness\n+        layer2 = TestLayer()\n+        trainable_variables = layer2.trainable_variables\n+        non_trainable_variables = layer2.non_trainable_variables\n+        out2, non_trainable_variables = layer2.stateless_call(\n+            trainable_variables, non_trainable_variables, data\n+        )\n+        self.assertAllClose(out1, out2)\n+        self.assertEqual(\n+            len(layer1.non_trainable_variables), len(non_trainable_variables)\n+        )\n+        for ref_v, v in zip(\n+            layer1.non_trainable_variables, non_trainable_variables\n+        ):\n+            self.assertAllClose(ref_v, v)\n+\n+        # Test with loss collection\n+        layer3 = TestLayer()\n+        trainable_variables = layer3.trainable_variables\n+        non_trainable_variables = layer3.non_trainable_variables\n+        out3, non_trainable_variables, losses = layer3.stateless_call(\n+            trainable_variables,\n+            non_trainable_variables,\n+            data,\n+            return_losses=True,\n+        )\n+        self.assertAllClose(out1, out3)\n+        for ref_v, v in zip(\n+            layer1.non_trainable_variables, non_trainable_variables\n+        ):\n+            self.assertAllClose(ref_v, v)\n+        for ref_loss, loss in zip(layer1.losses, losses):\n+            self.assertAllClose(ref_loss, loss)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2c6b1bb1960cc358eccb6e8141399af755cccbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 2863 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -380,6 +380,7 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             \"Received: \"\n             f\"target.shape={target.shape}, output.shape={output.shape}\"\n         )\n+\n     if from_logits:\n         log_prob = jax.nn.log_softmax(output, axis=axis)\n     else:\n\n@@ -17,7 +17,10 @@ def relu6(x):\n \n \n def sigmoid(x):\n-    return tf.nn.sigmoid(x)\n+    logits = x\n+    output = tf.nn.sigmoid(x)\n+    output._keras_logits = logits\n+    return output\n \n \n def tanh(x):\n@@ -66,7 +69,10 @@ def gelu(x, approximate=True):\n \n \n def softmax(x, axis=None):\n-    return tf.nn.softmax(x, axis=axis)\n+    logits = x\n+    output = tf.nn.softmax(x, axis=axis)\n+    output._keras_logits = logits\n+    return output\n \n \n def log_softmax(x, axis=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5185d1b5fe5a850d3e678c118a65da462afd6ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/1 | Churn Δ: 12 | Churn Cumulative: 16278 | Contributors (this commit): 61 | Commits (past 90d): 47 | Contributors (cumulative): 103 | DMM Complexity: 1.0\n\nDIFF:\n@@ -359,6 +359,8 @@ unitnorm = unit_norm\n \n @keras_export(\"keras.constraints.serialize\")\n def serialize(constraint, use_legacy_format=False):\n+    if constraint is None:\n+        return None\n     if not isinstance(constraint, Constraint):\n         warnings.warn(\n             \"The `keras.constraints.serialize()` API should only be used for \"\n\n@@ -138,6 +138,8 @@ globals().update(LOCAL.ALL_OBJECTS)\n @keras_export(\"keras.initializers.serialize\")\n def serialize(initializer, use_legacy_format=False):\n     populate_deserializable_objects()\n+    if initializer is None:\n+        return None\n     if not isinstance(initializer, tuple(LOCAL.ALL_OBJECTS.values())):\n         warnings.warn(\n             \"The `keras.initializers.serialize()` API should only be used for \"\n\n@@ -2854,6 +2854,8 @@ def serialize(loss, use_legacy_format=False):\n     Returns:\n       Loss configuration dictionary.\n     \"\"\"\n+    if loss is None:\n+        return None\n     if not isinstance(loss, Loss):\n         warnings.warn(\n             \"The `keras.losses.serialize()` API should only be used for \"\n\n@@ -139,6 +139,8 @@ def serialize(metric, use_legacy_format=False):\n     Returns:\n       Metric configuration dictionary.\n     \"\"\"\n+    if metric is None:\n+        return None\n     if not isinstance(metric, Metric):\n         warnings.warn(\n             \"The `keras.metrics.serialize()` API should only be used for \"\n\n@@ -87,6 +87,8 @@ def serialize(optimizer, use_legacy_format=False):\n     Returns:\n       Python dict which contains the configuration of the input optimizer.\n     \"\"\"\n+    if optimizer is None:\n+        return None\n     if not isinstance(\n         optimizer,\n         (\n\n@@ -420,6 +420,8 @@ orthogonal_regularizer = OrthogonalRegularizer\n \n @keras_export(\"keras.regularizers.serialize\")\n def serialize(regularizer, use_legacy_format=False):\n+    if regularizer is None:\n+        return None\n     if not isinstance(regularizer, Regularizer):\n         warnings.warn(\n             \"The `keras.regularizers.serialize()` API should only be used for \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e24ddab8e72c41496f9d0805560a3c8414732d37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,16 @@\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import constraints\n+from keras_core import optimizers\n+from keras_core import testing\n+\n+\n+class OptimizerTest(testing.TestCase):\n+    def test_constraints_are_applied(self):\n+        v = backend.Variable(np.random.random((2, 2)) - 1.0)\n+        v.constraint = constraints.NonNeg()\n+        optimizer = optimizers.SGD(learning_rate=0.0001)\n+        grad = backend.numpy.zeros((2, 2))\n+        optimizer.apply_gradients([(grad, v)])\n+        self.assertAlmostEqual(np.min(v), 0.0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b1a1ec8cb24cff68ad8234aa3fae43654ce467b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 1337 | Contributors (this commit): 7 | Commits (past 90d): 36 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n+from keras_core.backend import config\n from keras_core.backend.common import global_state\n-from keras_core.backend.config import floatx\n from keras_core.utils.naming import auto_name\n \n \n@@ -333,8 +333,13 @@ ALLOWED_DTYPES = {\n \n \n def standardize_dtype(dtype):\n+    if dtype == \"int\":\n+        if config.backend() == \"tensorflow\":\n+            dtype = \"int64\"\n+        else:\n+            dtype = \"int32\"\n     if dtype is None:\n-        return floatx()\n+        return config.floatx()\n     if hasattr(dtype, \"name\"):\n         dtype = dtype.name\n     if dtype not in ALLOWED_DTYPES:\n\n@@ -61,8 +61,10 @@ class Optimizer:\n             )\n \n         self.built = False\n+        # Note: dtype=\"int\" will resolve to int32 in JAX\n+        # (since int64 is disallowed in JAX) and to int64 in TF.\n         self.iterations = backend.Variable(\n-            0, name=\"iteration\", dtype=\"int64\", trainable=False\n+            0, name=\"iteration\", dtype=\"int\", trainable=False\n         )\n         if isinstance(\n             learning_rate, learning_rate_schedule.LearningRateSchedule\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c994b749a72e8e43d4e0912831d2b7865aba8dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 626 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 11 | Complexity Δ (Sum/Max): 56/46 | Churn Δ: 626 | Churn Cumulative: 1987 | Contributors (this commit): 9 | Commits (past 90d): 97 | Contributors (cumulative): 19 | DMM Complexity: 0.5954198473282443\n\nDIFF:\n@@ -1,5 +1,6 @@\n from keras_core.layers.activations.activation import Activation\n from keras_core.layers.core.dense import Dense\n+from keras_core.layers.core.einsum_dense import EinsumDense\n from keras_core.layers.core.embedding import Embedding\n from keras_core.layers.core.identity import Identity\n from keras_core.layers.core.input_layer import Input\n\n@@ -0,0 +1,340 @@\n+import re\n+\n+from keras_core import activations\n+from keras_core import constraints\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core import regularizers\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.EinsumDense\")\n+class EinsumDense(Layer):\n+    \"\"\"A layer that uses `einsum` as the backing computation.\n+\n+    This layer can perform einsum calculations of arbitrary dimensionality.\n+\n+    Args:\n+        equation: An equation describing the einsum to perform.\n+            This equation must be a valid einsum string of the form\n+            `ab,bc->ac`, `...ab,bc->...ac`, or\n+            `ab...,bc->ac...` where 'ab', 'bc', and 'ac' can be any valid einsum\n+            axis expression sequence.\n+        output_shape: The expected shape of the output tensor\n+            (excluding the batch dimension and any dimensions\n+            represented by ellipses). You can specify `None` for any dimension\n+            that is unknown or can be inferred from the input shape.\n+        activation: Activation function to use. If you don't specify anything,\n+            no activation is applied\n+            (that is, a \"linear\" activation: `a(x) = x`).\n+        bias_axes: A string containing the output dimension(s)\n+            to apply a bias to. Each character in the `bias_axes` string\n+            should correspond to a character in the output portion\n+            of the `equation` string.\n+        kernel_initializer: Initializer for the `kernel` weights matrix.\n+        bias_initializer: Initializer for the bias vector.\n+        kernel_regularizer: Regularizer function applied to the `kernel` weights\n+            matrix.\n+        bias_regularizer: Regularizer function applied to the bias vector.\n+        kernel_constraint: Constraint function applied to the `kernel` weights\n+            matrix.\n+        bias_constraint: Constraint function applied to the bias vector.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+\n+    Examples:\n+\n+    **Biased dense layer with einsums**\n+\n+    This example shows how to instantiate a standard Keras dense layer using\n+    einsum operations. This example is equivalent to\n+    `keras_core.layers.Dense(64, use_bias=True)`.\n+\n+    >>> layer = keras_core.layers.EinsumDense(\"ab,bc->ac\",\n+    ...                                       output_shape=64,\n+    ...                                       bias_axes=\"c\")\n+    >>> input_tensor = keras_core.Input(shape=[32])\n+    >>> output_tensor = layer(input_tensor)\n+    >>> output_tensor.shape\n+    (None, 64)\n+\n+    **Applying a dense layer to a sequence**\n+\n+    This example shows how to instantiate a layer that applies the same dense\n+    operation to every element in a sequence. Here, the `output_shape` has two\n+    values (since there are two non-batch dimensions in the output); the first\n+    dimension in the `output_shape` is `None`, because the sequence dimension\n+    `b` has an unknown shape.\n+\n+    >>> layer = keras_core.layers.EinsumDense(\"abc,cd->abd\",\n+    ...                                       output_shape=(None, 64),\n+    ...                                       bias_axes=\"d\")\n+    >>> input_tensor = keras_core.Input(shape=[32, 128])\n+    >>> output_tensor = layer(input_tensor)\n+    >>> output_tensor.shape\n+    (None, 32, 64)\n+\n+    **Applying a dense layer to a sequence using ellipses**\n+\n+    This example shows how to instantiate a layer that applies the same dense\n+    operation to every element in a sequence, but uses the ellipsis notation\n+    instead of specifying the batch and sequence dimensions.\n+\n+    Because we are using ellipsis notation and have specified only one axis, the\n+    `output_shape` arg is a single value. When instantiated in this way, the\n+    layer can handle any number of sequence dimensions - including the case\n+    where no sequence dimension exists.\n+\n+    >>> layer = keras_core.layers.EinsumDense(\"...x,xy->...y\",\n+    ...                                       output_shape=64,\n+    ...                                       bias_axes=\"y\")\n+    >>> input_tensor = keras_core.Input(shape=[32, 128])\n+    >>> output_tensor = layer(input_tensor)\n+    >>> output_tensor.shape\n+    (None, 32, 64)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        equation,\n+        output_shape,\n+        activation=None,\n+        bias_axes=None,\n+        kernel_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        kernel_regularizer=None,\n+        bias_regularizer=None,\n+        kernel_constraint=None,\n+        bias_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.equation = equation\n+        if isinstance(output_shape, int):\n+            self.partial_output_shape = (output_shape,)\n+        else:\n+            self.partial_output_shape = tuple(output_shape)\n+        self.bias_axes = bias_axes\n+        self.activation = activations.get(activation)\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+    def build(self, input_shape):\n+        shape_data = _analyze_einsum_string(\n+            self.equation,\n+            self.bias_axes,\n+            input_shape,\n+            self.partial_output_shape,\n+        )\n+        kernel_shape, bias_shape, full_output_shape = shape_data\n+        self.full_output_shape = tuple(full_output_shape)\n+        self.kernel = self.add_weight(\n+            name=\"kernel\",\n+            shape=tuple(kernel_shape),\n+            initializer=self.kernel_initializer,\n+            regularizer=self.kernel_regularizer,\n+            constraint=self.kernel_constraint,\n+            dtype=self.dtype,\n+            trainable=True,\n+        )\n+\n+        if bias_shape is not None:\n+            self.bias = self.add_weight(\n+                name=\"bias\",\n+                shape=tuple(bias_shape),\n+                initializer=self.bias_initializer,\n+                regularizer=self.bias_regularizer,\n+                constraint=self.bias_constraint,\n+                dtype=self.dtype,\n+                trainable=True,\n+            )\n+        else:\n+            self.bias = None\n+        super().build(input_shape)\n+\n+    def compute_output_shape(self, _):\n+        return self.full_output_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"output_shape\": self.partial_output_shape,\n+            \"equation\": self.equation,\n+            \"activation\": activations.serialize(self.activation),\n+            \"bias_axes\": self.bias_axes,\n+            \"kernel_initializer\": initializers.serialize(\n+                self.kernel_initializer\n+            ),\n+            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(\n+                self.kernel_regularizer\n+            ),\n+            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n+            \"activity_regularizer\": regularizers.serialize(\n+                self.activity_regularizer\n+            ),\n+            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n+        }\n+        return {**base_config, **config}\n+\n+    def call(self, inputs):\n+        x = ops.einsum(self.equation, inputs, self.kernel)\n+        if self.bias is not None:\n+            x += self.bias\n+        if self.activation is not None:\n+            x = self.activation(x)\n+        return x\n+\n+\n+def _analyze_einsum_string(equation, bias_axes, input_shape, output_shape):\n+    \"\"\"Analyzes an einsum string to determine the required weight shape.\"\"\"\n+\n+    dot_replaced_string = re.sub(r\"\\.\\.\\.\", \"0\", equation)\n+\n+    # This is the case where no ellipses are present in the string.\n+    split_string = re.match(\n+        \"([a-zA-Z]+),([a-zA-Z]+)->([a-zA-Z]+)\", dot_replaced_string\n+    )\n+    if split_string:\n+        return _analyze_split_string(\n+            split_string, bias_axes, input_shape, output_shape\n+        )\n+\n+    # This is the case where ellipses are present on the left.\n+    split_string = re.match(\n+        \"0([a-zA-Z]+),([a-zA-Z]+)->0([a-zA-Z]+)\", dot_replaced_string\n+    )\n+    if split_string:\n+        return _analyze_split_string(\n+            split_string, bias_axes, input_shape, output_shape, left_elided=True\n+        )\n+\n+    # This is the case where ellipses are present on the right.\n+    split_string = re.match(\n+        \"([a-zA-Z]{2,})0,([a-zA-Z]+)->([a-zA-Z]+)0\", dot_replaced_string\n+    )\n+    if split_string:\n+        return _analyze_split_string(\n+            split_string, bias_axes, input_shape, output_shape\n+        )\n+\n+    raise ValueError(\n+        f\"Invalid einsum equation '{equation}'. Equations must be in the form \"\n+        \"[X],[Y]->[Z], ...[X],[Y]->...[Z], or [X]...,[Y]->[Z]....\"\n+    )\n+\n+\n+def _analyze_split_string(\n+    split_string, bias_axes, input_shape, output_shape, left_elided=False\n+):\n+    \"\"\"Analyze an pre-split einsum string to find the weight shape.\"\"\"\n+    input_spec = split_string.group(1)\n+    weight_spec = split_string.group(2)\n+    output_spec = split_string.group(3)\n+    elided = len(input_shape) - len(input_spec)\n+\n+    if isinstance(output_shape, int):\n+        output_shape = [output_shape]\n+    else:\n+        output_shape = list(output_shape)\n+\n+    output_shape.insert(0, input_shape[0])\n+\n+    if elided > 0 and left_elided:\n+        for i in range(1, elided):\n+            # We already inserted the 0th input dimension at dim 0, so we need\n+            # to start at location 1 here.\n+            output_shape.insert(1, input_shape[i])\n+    elif elided > 0 and not left_elided:\n+        for i in range(len(input_shape) - elided, len(input_shape)):\n+            output_shape.append(input_shape[i])\n+\n+    if left_elided:\n+        # If we have beginning dimensions elided, we need to use negative\n+        # indexing to determine where in the input dimension our values are.\n+        input_dim_map = {\n+            dim: (i + elided) - len(input_shape)\n+            for i, dim in enumerate(input_spec)\n+        }\n+        # Because we've constructed the full output shape already, we don't need\n+        # to do negative indexing.\n+        output_dim_map = {\n+            dim: (i + elided) for i, dim in enumerate(output_spec)\n+        }\n+    else:\n+        input_dim_map = {dim: i for i, dim in enumerate(input_spec)}\n+        output_dim_map = {dim: i for i, dim in enumerate(output_spec)}\n+\n+    for dim in input_spec:\n+        input_shape_at_dim = input_shape[input_dim_map[dim]]\n+        if dim in output_dim_map:\n+            output_shape_at_dim = output_shape[output_dim_map[dim]]\n+            if (\n+                output_shape_at_dim is not None\n+                and output_shape_at_dim != input_shape_at_dim\n+            ):\n+                raise ValueError(\n+                    \"Input shape and output shape do not match at shared \"\n+                    f\"dimension '{dim}'. Input shape is {input_shape_at_dim}, \"\n+                    \"and output shape \"\n+                    f\"is {output_shape[output_dim_map[dim]]}.\"\n+                )\n+\n+    for dim in output_spec:\n+        if dim not in input_spec and dim not in weight_spec:\n+            raise ValueError(\n+                f\"Dimension '{dim}' was specified in the output \"\n+                f\"'{output_spec}' but has no corresponding dim in the input \"\n+                f\"spec '{input_spec}' or weight spec '{output_spec}'\"\n+            )\n+\n+    weight_shape = []\n+    for dim in weight_spec:\n+        if dim in input_dim_map:\n+            weight_shape.append(input_shape[input_dim_map[dim]])\n+        elif dim in output_dim_map:\n+            weight_shape.append(output_shape[output_dim_map[dim]])\n+        else:\n+            raise ValueError(\n+                f\"Weight dimension '{dim}' did not have a match in either \"\n+                f\"the input spec '{input_spec}' or the output \"\n+                f\"spec '{output_spec}'. For this layer, the weight must \"\n+                \"be fully specified.\"\n+            )\n+\n+    if bias_axes is not None:\n+        num_left_elided = elided if left_elided else 0\n+        idx_map = {\n+            char: output_shape[i + num_left_elided]\n+            for i, char in enumerate(output_spec)\n+        }\n+\n+        for char in bias_axes:\n+            if char not in output_spec:\n+                raise ValueError(\n+                    f\"Bias dimension '{char}' was requested, but is not part \"\n+                    f\"of the output spec '{output_spec}'\"\n+                )\n+\n+        first_bias_location = min(\n+            [output_spec.find(char) for char in bias_axes]\n+        )\n+        bias_output_spec = output_spec[first_bias_location:]\n+\n+        bias_shape = [\n+            idx_map[char] if char in bias_axes else 1\n+            for char in bias_output_spec\n+        ]\n+\n+        if not left_elided:\n+            for _ in range(elided):\n+                bias_shape.append(1)\n+    else:\n+        bias_shape = None\n+\n+    return weight_shape, bias_shape, output_shape\n\n@@ -0,0 +1,263 @@\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        {\n+            \"testcase_name\": \"_1d_end_weight\",\n+            \"equation\": \"ab,b->a\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 32),\n+            \"output_shape\": (),\n+            \"expected_kernel_shape\": (32,),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2,),\n+        },\n+        {\n+            \"testcase_name\": \"_2d_middle_weight\",\n+            \"equation\": \"ab,bc->ac\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 32),\n+            \"output_shape\": (64),\n+            \"expected_kernel_shape\": (32, 64),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 64),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_bert\",\n+            \"equation\": \"abc,cde->abde\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (1, 3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_3_bias\",\n+            \"equation\": \"abc,cde->abde\",\n+            \"bias_axes\": \"e\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (1, 3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": (4,),\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_2_bias\",\n+            \"equation\": \"abc,cde->abde\",\n+            \"bias_axes\": \"d\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (1, 3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": (3, 1),\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_1_3_bias\",\n+            \"equation\": \"abc,cde->abde\",\n+            \"bias_axes\": \"be\",\n+            \"input_shape\": (2, 7, 2),\n+            \"output_shape\": (7, 3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": (7, 1, 4),\n+            \"expected_output_shape\": (2, 7, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_bert_projection\",\n+            \"equation\": \"BFNH,NHD->BFD\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 1, 2, 3),\n+            \"output_shape\": (1, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 1, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_2d_bert\",\n+            \"equation\": \"abc,cd->abd\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (1, 4),\n+            \"expected_kernel_shape\": (2, 4),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 1, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_embedding_1d\",\n+            \"equation\": \"i,d->id\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2,),\n+            \"output_shape\": (2,),\n+            \"expected_kernel_shape\": (2,),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 2),\n+        },\n+        {\n+            \"testcase_name\": \"_xlnet_lm\",\n+            \"equation\": \"ibd,nd->ibn\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 2, 1),\n+            \"output_shape\": (2, 2),\n+            \"expected_kernel_shape\": (2, 1),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 2, 2),\n+        },\n+        {\n+            \"testcase_name\": \"_2d_precast\",\n+            \"equation\": \"...b,bc->...c\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 32),\n+            \"output_shape\": (64,),\n+            \"expected_kernel_shape\": (32, 64),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 64),\n+        },\n+        {\n+            \"testcase_name\": \"_2d_precast_elided_input_used_in_output\",\n+            \"equation\": \"...bc,bc->...b\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 32, 64),\n+            \"output_shape\": (32,),\n+            \"expected_kernel_shape\": (32, 64),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 32),\n+        },\n+        {\n+            \"testcase_name\": \"_2d_precast_multiple_elided_dims\",\n+            \"equation\": \"...b,bc->...c\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 3, 32),\n+            \"output_shape\": (64,),\n+            \"expected_kernel_shape\": (32, 64),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 3, 64),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_precast\",\n+            \"equation\": \"...c,cde->...de\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_precast_3_bias\",\n+            \"equation\": \"...c,cde->...de\",\n+            \"bias_axes\": \"e\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": (4,),\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_precast_2_bias\",\n+            \"equation\": \"...c,cde->...de\",\n+            \"bias_axes\": \"d\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": (3, 1),\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_precast_2_3_bias\",\n+            \"equation\": \"...c,cde->...de\",\n+            \"bias_axes\": \"de\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (2, 3, 4),\n+            \"expected_bias_shape\": (3, 4),\n+            \"expected_output_shape\": (2, 1, 3, 4),\n+        },\n+        {\n+            \"testcase_name\": \"_2d_postcast\",\n+            \"equation\": \"bc...,cd->bd...\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 1, 2, 3),\n+            \"output_shape\": (4,),\n+            \"expected_kernel_shape\": (1, 4),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 4, 2, 3),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_postcast\",\n+            \"equation\": \"bc...,cde->bde...\",\n+            \"bias_axes\": None,\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (1, 3, 4),\n+            \"expected_bias_shape\": None,\n+            \"expected_output_shape\": (2, 3, 4, 2),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_postcast_1_bias\",\n+            \"equation\": \"bc...,cde->bde...\",\n+            \"bias_axes\": \"d\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (1, 3, 4),\n+            \"expected_bias_shape\": (3, 1, 1),\n+            \"expected_output_shape\": (2, 3, 4, 2),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_postcast_2_bias\",\n+            \"equation\": \"bc...,cde->bde...\",\n+            \"bias_axes\": \"e\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (1, 3, 4),\n+            \"expected_bias_shape\": (4, 1),\n+            \"expected_output_shape\": (2, 3, 4, 2),\n+        },\n+        {\n+            \"testcase_name\": \"_3d_postcast_1_2_bias\",\n+            \"equation\": \"bc...,cde->bde...\",\n+            \"bias_axes\": \"de\",\n+            \"input_shape\": (2, 1, 2),\n+            \"output_shape\": (3, 4),\n+            \"expected_kernel_shape\": (1, 3, 4),\n+            \"expected_bias_shape\": (3, 4, 1),\n+            \"expected_output_shape\": (2, 3, 4, 2),\n+        },\n+    )\n+    def test_einsum_dense_basics(\n+        self,\n+        equation,\n+        bias_axes,\n+        input_shape,\n+        output_shape,\n+        expected_kernel_shape,\n+        expected_bias_shape,\n+        expected_output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.EinsumDense,\n+            init_kwargs={\n+                \"equation\": equation,\n+                \"output_shape\": output_shape,\n+                \"bias_axes\": bias_axes,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=expected_output_shape,\n+            expected_num_trainable_weights=2\n+            if expected_bias_shape is not None\n+            else 1,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+        layer = layers.EinsumDense(\n+            equation, output_shape=output_shape, bias_axes=bias_axes\n+        )\n+        layer.build(input_shape)\n+        self.assertEqual(layer.kernel.shape, expected_kernel_shape)\n+        if expected_bias_shape is not None:\n+            self.assertEqual(layer.bias.shape, expected_bias_shape)\n\n@@ -527,6 +527,23 @@ class Layer(Operation):\n             else:\n                 # More than one shape: pass them by name.\n                 output_shape = self.compute_output_shape(**shapes_dict)\n+            if (\n+                isinstance(output_shape, list)\n+                and output_shape\n+                and isinstance(output_shape[0], (int, type(None)))\n+            ):\n+                output_shape = tuple(output_shape)\n+            if not isinstance(output_shape, (list, tuple, dict)):\n+                try:\n+                    output_shape = tuple(output_shape)\n+                except:\n+                    raise ValueError(\n+                        \"Method `compute_output_shape()` of layer \"\n+                        f\"{self.__class__.__name__} is returning \"\n+                        \"a type that cannot be interpreted as a shape. \"\n+                        \"It should return a shape tuple. \"\n+                        f\"Received: {output_shape}\"\n+                    )\n             if (\n                 isinstance(output_shape, tuple)\n                 and output_shape\n\n@@ -241,6 +241,11 @@ def create_eager_tensors(input_shape, dtype):\n     elif dtype in [\"int16\", \"int32\", \"int64\"]:\n \n         def create_fn(shape, dtype):\n+            shape = list(shape)\n+            for i in range(len(shape)):\n+                if shape[i] is None:\n+                    shape[i] = 2\n+            shape = tuple(shape)\n             return ops.cast(\n                 random.uniform(shape, dtype=\"float32\") * 3, dtype=dtype\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
