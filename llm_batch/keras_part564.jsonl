{"custom_id": "keras#05674c4731f9e476de08ef5aad96379c2619daac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3501 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1400,9 +1400,9 @@ class AUC(base_metric.Metric):\n         if self.multi_label:\n             if shape.ndims != 2:\n                 raise ValueError(\n-                    \"`y_true` must have rank 2 when `multi_label=True`. \"\n+                    \"`y_pred` must have rank 2 when `multi_label=True`. \"\n                     f\"Found rank {shape.ndims}. \"\n-                    f\"Full shape received for `y_true`: {shape}\"\n+                    f\"Full shape received for `y_pred`: {shape}\"\n                 )\n             self._num_labels = shape[1]\n             variable_shape = tf.TensorShape(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9b4749e33eb6b98360f14f412e56b5aca27a8f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 291 | Lines Deleted: 1010 | Files Changed: 4 | Hunks: 36 | Methods Changed: 28 | Complexity Δ (Sum/Max): -2/15 | Churn Δ: 1301 | Churn Cumulative: 3731 | Contributors (this commit): 7 | Commits (past 90d): 39 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,3 +13,4 @@ from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n+from keras_core.layers.reshaping.reshape import Reshape\n\n@@ -0,0 +1,113 @@\n+import math\n+\n+from keras_core import operations as ops\n+from keras_core.layers.layer import Layer\n+\n+\n+class Reshape(Layer):\n+    \"\"\"Layer that reshapes inputs into the given shape.\n+\n+    Args:\n+        target_shape: Target shape. Tuple of integers, does not include the\n+            samples dimension (batch size).\n+\n+    Input shape:\n+        Arbitrary, although all dimensions in the input shape must be\n+        known/fixed. Use the keyword argument `input_shape` (tuple of integers,\n+        does not include the samples/batch size axis) when using this layer as\n+        the first layer in a model.\n+\n+    Output shape:\n+        `(batch_size, *target_shape)`\n+\n+    Example:\n+\n+    >>> # as first layer in a Sequential model\n+    >>> model = keras_core.Sequential()\n+    >>> model.add(keras_core.layers.Reshape((3, 4), input_shape=(12,)))\n+    >>> # model.output_shape == (None, 3, 4), `None` is the batch size.\n+    >>> model.output_shape\n+    (None, 3, 4)\n+\n+    >>> # as intermediate layer in a Sequential model\n+    >>> model.add(keras_core.layers.Reshape((6, 2)))\n+    >>> model.output_shape\n+    (None, 6, 2)\n+\n+    >>> # also supports shape inference using `-1` as dimension\n+    >>> model.add(keras_core.layers.Reshape((-1, 2, 2)))\n+    >>> model.output_shape\n+    (None, 3, 2, 2)\n+    \"\"\"\n+\n+    def __init__(self, target_shape, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        self.target_shape = tuple(target_shape)\n+\n+    def _fix_unknown_dimension(self, input_shape, output_shape):\n+        \"\"\"Find and replace a missing dimension in an output shape.\n+\n+        Args:\n+            input_shape: Shape of tensor being reshaped as a tuple of ints.\n+            output_shape: Desired shape of the tensor as a tuple of ints. It\n+                contains at most a single `-1` which indicates a dimension that\n+                should be derived from the input shape.\n+\n+        Returns:\n+            The new output shape as a tuple of ints with a -1 replaced with its\n+            computed value.\n+\n+        Raises:\n+            ValueError: If the total tensor size of the output_shape is\n+                different than the input_shape, or more than one unknown\n+                dimension is specified.\n+        \"\"\"\n+        msg = (\n+            \"total size of new tensor must be unchanged, \"\n+            f\"input_shape={input_shape},output_shape={output_shape}\"\n+        )\n+\n+        known_output_size, unknown_dim_index = 1, None\n+        for index, dim in enumerate(output_shape):\n+            if dim == -1:\n+                if unknown_dim_index is None:\n+                    unknown_dim_index = index\n+                else:\n+                    raise ValueError(\n+                        \"There must be at most one unknown dimension in \"\n+                        f\"output_shape. Received: output_shape={output_shape}.\"\n+                    )\n+            else:\n+                known_output_size *= dim\n+\n+        input_size = math.prod(input_shape)\n+        if unknown_dim_index is not None:\n+            if known_output_size == 0 or input_size % known_output_size != 0:\n+                raise ValueError(msg)\n+            result = list(output_shape)\n+            result[unknown_dim_index] = input_size // known_output_size\n+            return tuple(result)\n+        elif input_size != known_output_size:\n+            raise ValueError(msg)\n+        return output_shape\n+\n+    def compute_output_shape(self, input_shape):\n+        output_shape = (input_shape[0],)\n+        if None in input_shape[1:]:\n+            # input shape (partially) unknown? replace -1's with None's\n+            output_shape += tuple(\n+                s if s != -1 else None for s in self.target_shape\n+            )\n+        else:\n+            output_shape += self._fix_unknown_dimension(\n+                input_shape[1:], self.target_shape\n+            )\n+        return output_shape\n+\n+    def call(self, inputs):\n+        return ops.reshape(inputs, (inputs.shape[0],) + self.target_shape)\n+\n+    def get_config(self):\n+        config = {\"target_shape\": self.target_shape}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,75 @@\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class ReshapeTest(testing.TestCase):\n+    def test_reshape(self):\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (8, 1)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 8, 1),\n+        )\n+\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (8,)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 8),\n+        )\n+\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (2, 4)},\n+            input_shape=(3, 8),\n+            expected_output_shape=(3, 2, 4),\n+        )\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (-1, 1)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 8, 1),\n+        )\n+\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (1, -1)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 1, 8),\n+        )\n+\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (-1,)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 8),\n+        )\n+\n+        self.run_layer_test(\n+            layers.Reshape,\n+            init_kwargs={\"target_shape\": (2, -1)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 2, 4),\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_reshape_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(shape=(2, 4))\n+        reshaped = layers.Reshape((8,))(input_layer)\n+        self.assertEqual(reshaped.shape, (None, 8))\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_reshape_sets_static_shape(self):\n+        input_layer = layers.Input(batch_shape=(2, None))\n+        reshaped = layers.Reshape((3, 5))(input_layer)\n+        # Also make sure the batch dim is not lost after reshape.\n+        self.assertEqual(reshaped.shape, (2, 3, 5))\n\n@@ -1,5 +1,3 @@\n-import warnings\n-\n from keras_core import backend\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n@@ -45,10 +43,10 @@ class MeanSquaredError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance.\n     \"\"\"\n \n     def __init__(\n@@ -71,10 +69,10 @@ class MeanAbsoluteError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance.\n     \"\"\"\n \n     def __init__(\n@@ -97,10 +95,10 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance.\n     \"\"\"\n \n     def __init__(\n@@ -127,10 +125,10 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance.\n     \"\"\"\n \n     def __init__(\n@@ -166,10 +164,10 @@ class CosineSimilarity(LossFunctionWrapper):\n     Args:\n         axis: The axis along which the cosine similarity is computed\n             (the features axis). Defaults to -1.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`. Defaults to\n+            `\"sum_over_batch_size\"`.\n+        name: Optional name for the instance.\n     \"\"\"\n \n     def __init__(\n@@ -197,10 +195,10 @@ class Hinge(LossFunctionWrapper):\n     provided we will convert them to -1 or 1.\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to `\"hinge\"`\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"hinge\"):\n@@ -224,10 +222,10 @@ class SquaredHinge(LossFunctionWrapper):\n     provided we will convert them to -1 or 1.\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to `\"squared_hinge\"`\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\"):\n@@ -250,10 +248,11 @@ class CategoricalHinge(LossFunctionWrapper):\n     where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to\n+            `\"categorical_hinge\"`\n     \"\"\"\n \n     def __init__(\n@@ -276,10 +275,10 @@ class KLDivergence(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to 'kl_divergence'.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"kl_divergence\"):\n@@ -300,10 +299,10 @@ class Poisson(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to `\"poisson\"`\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"poisson\"):\n@@ -313,572 +312,6 @@ class Poisson(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.BinaryCrossentropy\")\n-class BinaryCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the cross-entropy loss between true labels and predicted labels.\n-\n-    Use this cross-entropy loss for binary (0 or 1) classification applications.\n-    The loss function requires the following inputs:\n-\n-    - `y_true` (true label): This is either 0 or 1.\n-    - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n-      floating-point value which either represents a\n-      [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n-      when `from_logits=True`) or a probability (i.e, value in [0., 1.] when\n-      `from_logits=False`).\n-\n-    Args:\n-        from_logits: Whether to interpret `y_pred` as a tensor of\n-            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n-            assume that `y_pred` contains probabilities (i.e., values in [0,\n-            1]).\n-        label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.\n-            When > 0, we compute the loss between the predicted labels\n-            and a smoothed version of the true labels, where the smoothing\n-            squeezes the labels towards 0.5. Larger values of\n-            `label_smoothing` correspond to heavier smoothing.\n-        axis: The axis along which to compute crossentropy (the features\n-            axis).  Defaults to -1.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-\n-    Examples:\n-\n-    **Recommended Usage:** (set `from_logits=True`)\n-\n-    With `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n-        ...\n-    )\n-    ```\n-\n-    As a standalone function:\n-\n-    >>> # Example 1: (batch_size = 1, number of samples = 4)\n-    >>> y_true = [0, 1, 0, 0]\n-    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n-    >>> bce(y_true, y_pred)\n-    0.865\n-\n-    >>> # Example 2: (batch_size = 2, number of samples = 4)\n-    >>> y_true = [[0, 1], [0, 0]]\n-    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n-    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n-    >>> bce(y_true, y_pred)\n-    0.865\n-    >>> # Using 'sample_weight' attribute\n-    >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2])\n-    0.243\n-    >>> # Using 'sum' reduction` type.\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n-    ...     reduction=\"sum\")\n-    >>> bce(y_true, y_pred)\n-    1.730\n-    >>> # Using 'none' reduction type.\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n-    ...     reduction=None)\n-    >>> bce(y_true, y_pred)\n-    array([0.235, 1.496], dtype=float32)\n-\n-    **Default Usage:** (set `from_logits=False`)\n-\n-    >>> # Make the following updates to the above \"Recommended Usage\" section\n-    >>> # 1. Set `from_logits=False`\n-    >>> keras_core.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n-    >>> # 2. Update `y_pred` to use probabilities instead of logits\n-    >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        from_logits=False,\n-        label_smoothing=0.0,\n-        axis=-1,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"binary_crossentropy\",\n-    ):\n-        super().__init__(\n-            binary_crossentropy,\n-            name=name,\n-            reduction=reduction,\n-            from_logits=from_logits,\n-            label_smoothing=label_smoothing,\n-            axis=axis,\n-        )\n-        self.from_logits = from_logits\n-\n-\n-@keras_core_export(\"keras_core.losses.BinaryFocalCrossentropy\")\n-class BinaryFocalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes focal cross-entropy loss between true labels and predictions.\n-\n-    Binary cross-entropy loss is often used for binary (0 or 1) classification\n-    tasks. The loss function requires the following inputs:\n-\n-    - `y_true` (true label): This is either 0 or 1.\n-    - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n-      floating-point value which either represents a\n-      [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n-      when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when\n-      `from_logits=False`).\n-\n-    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n-    helps to apply a \"focal factor\" to down-weight easy examples and focus more\n-    on hard examples. By default, the focal tensor is computed as follows:\n-\n-    `focal_factor = (1 - output) ** gamma` for class 1\n-    `focal_factor = output ** gamma` for class 0\n-    where `gamma` is a focusing parameter. When `gamma=0`, this function is\n-    equivalent to the binary crossentropy loss.\n-\n-    Args:\n-        apply_class_balancing: A bool, whether to apply weight balancing on the\n-            binary classes 0 and 1.\n-        alpha: A weight balancing factor for class 1, default is `0.25` as\n-            mentioned in reference [Lin et al., 2018](\n-            https://arxiv.org/pdf/1708.02002.pdf).  The weight for class 0 is\n-            `1.0 - alpha`.\n-        gamma: A focusing parameter used to compute the focal factor, default is\n-            `2.0` as mentioned in the reference\n-            [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n-        from_logits: Whether to interpret `y_pred` as a tensor of\n-            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n-            assume that `y_pred` are probabilities (i.e., values in `[0, 1]`).\n-        label_smoothing: Float in `[0, 1]`. When `0`, no smoothing occurs.\n-            When > `0`, we compute the loss between the predicted labels\n-            and a smoothed version of the true labels, where the smoothing\n-            squeezes the labels towards `0.5`.\n-            Larger values of `label_smoothing` correspond to heavier smoothing.\n-        axis: The axis along which to compute crossentropy (the features axis).\n-            Defaults to `-1`.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-\n-    Examples:\n-\n-    With the `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        loss=keras_core.losses.BinaryFocalCrossentropy(\n-            gamma=2.0, from_logits=True),\n-        ...\n-    )\n-    ```\n-\n-    As a standalone function:\n-\n-    >>> # Example 1: (batch_size = 1, number of samples = 4)\n-    >>> y_true = [0, 1, 0, 0]\n-    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...    gamma=2, from_logits=True)\n-    >>> loss(y_true, y_pred)\n-    0.691\n-\n-    >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     apply_class_balancing=True, gamma=2, from_logits=True)\n-    >>> loss(y_true, y_pred)\n-    0.51\n-\n-    >>> # Example 2: (batch_size = 2, number of samples = 4)\n-    >>> y_true = [[0, 1], [0, 0]]\n-    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n-    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     gamma=3, from_logits=True)\n-    >>> loss(y_true, y_pred)\n-    0.647\n-\n-    >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...      apply_class_balancing=True, gamma=3, from_logits=True)\n-    >>> loss(y_true, y_pred)\n-    0.482\n-\n-    >>> # Using 'sample_weight' attribute with focal effect\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     gamma=3, from_logits=True)\n-    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n-    0.133\n-\n-    >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...      apply_class_balancing=True, gamma=3, from_logits=True)\n-    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n-    0.097\n-\n-    >>> # Using 'sum' reduction` type.\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     gamma=4, from_logits=True,\n-    ...     reduction=\"sum\")\n-    >>> loss(y_true, y_pred)\n-    1.222\n-\n-    >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     apply_class_balancing=True, gamma=4, from_logits=True,\n-    ...     reduction=\"sum\")\n-    >>> loss(y_true, y_pred)\n-    0.914\n-\n-    >>> # Using 'none' reduction type.\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     gamma=5, from_logits=True,\n-    ...     reduction=None)\n-    >>> loss(y_true, y_pred)\n-    array([0.0017 1.1561], dtype=float32)\n-\n-    >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n-    ...     apply_class_balancing=True, gamma=5, from_logits=True,\n-    ...     reduction=None)\n-    >>> loss(y_true, y_pred)\n-    array([0.0004 0.8670], dtype=float32)\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        apply_class_balancing=False,\n-        alpha=0.25,\n-        gamma=2.0,\n-        from_logits=False,\n-        label_smoothing=0.0,\n-        axis=-1,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"binary_focal_crossentropy\",\n-    ):\n-        \"\"\"Initializes `BinaryFocalCrossentropy` instance.\"\"\"\n-        super().__init__(\n-            binary_focal_crossentropy,\n-            apply_class_balancing=apply_class_balancing,\n-            alpha=alpha,\n-            gamma=gamma,\n-            name=name,\n-            reduction=reduction,\n-            from_logits=from_logits,\n-            label_smoothing=label_smoothing,\n-            axis=axis,\n-        )\n-        self.from_logits = from_logits\n-        self.apply_class_balancing = apply_class_balancing\n-        self.alpha = alpha\n-        self.gamma = gamma\n-\n-    def get_config(self):\n-        config = {\n-            \"apply_class_balancing\": self.apply_class_balancing,\n-            \"alpha\": self.alpha,\n-            \"gamma\": self.gamma,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_core_export(\"keras_core.losses.CategoricalCrossentropy\")\n-class CategoricalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the crossentropy loss between the labels and predictions.\n-\n-    Use this crossentropy loss function when there are two or more label\n-    classes. We expect labels to be provided in a `one_hot` representation. If\n-    you want to provide labels as integers, please use\n-    `SparseCategoricalCrossentropy` loss.  There should be `# classes` floating\n-    point values per feature.\n-\n-    In the snippet below, there is `# classes` floating pointing values per\n-    example. The shape of both `y_pred` and `y_true` are\n-    `[batch_size, num_classes]`.\n-\n-    Args:\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n-        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n-            meaning the confidence on label values are relaxed. For example, if\n-            `0.1`, use `0.1 / num_classes` for non-target labels and\n-            `0.9 + 0.1 / num_classes` for target labels.\n-        axis: The axis along which to compute crossentropy (the features\n-            axis). Defaults to -1.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-\n-    Examples:\n-\n-    Standalone usage:\n-\n-    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n-    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n-    >>> cce = keras_core.losses.CategoricalCrossentropy()\n-    >>> cce(y_true, y_pred)\n-    1.177\n-\n-    >>> # Calling with 'sample_weight'.\n-    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n-    0.814\n-\n-    >>> # Using 'sum' reduction type.\n-    >>> cce = keras_core.losses.CategoricalCrossentropy(\n-    ...     reduction=\"sum\")\n-    >>> cce(y_true, y_pred)\n-    2.354\n-\n-    >>> # Using 'none' reduction type.\n-    >>> cce = keras_core.losses.CategoricalCrossentropy(\n-    ...     reduction=None)\n-    >>> cce(y_true, y_pred)\n-    array([0.0513, 2.303], dtype=float32)\n-\n-    Usage with the `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss=keras_core.losses.CategoricalCrossentropy())\n-    ```\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        from_logits=False,\n-        label_smoothing=0.0,\n-        axis=-1,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"categorical_crossentropy\",\n-    ):\n-        super().__init__(\n-            categorical_crossentropy,\n-            name=name,\n-            reduction=reduction,\n-            from_logits=from_logits,\n-            label_smoothing=label_smoothing,\n-            axis=axis,\n-        )\n-\n-\n-@keras_core_export(\"keras_core.losses.CategoricalFocalCrossentropy\")\n-class CategoricalFocalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the alpha balanced focal crossentropy loss.\n-\n-    Use this crossentropy loss function when there are two or more label\n-    classes and if you want to handle class imbalance without using\n-    `class_weights`. We expect labels to be provided in a `one_hot`\n-    representation.\n-\n-    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n-    helps to apply a focal factor to down-weight easy examples and focus more on\n-    hard examples. The general formula for the focal loss (FL)\n-    is as follows:\n-\n-    `FL(p_t) = (1 - p_t) ** gamma * log(p_t)`\n-\n-    where `p_t` is defined as follows:\n-    `p_t = output if y_true == 1, else 1 - output`\n-\n-    `(1 - p_t) ** gamma` is the `modulating_factor`, where `gamma` is a focusing\n-    parameter. When `gamma` = 0, there is no focal effect on the cross entropy.\n-    `gamma` reduces the importance given to simple examples in a smooth manner.\n-\n-    The authors use alpha-balanced variant of focal loss (FL) in the paper:\n-    `FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)`\n-\n-    where `alpha` is the weight factor for the classes. If `alpha` = 1, the\n-    loss won't be able to handle class imbalance properly as all\n-    classes will have the same weight. This can be a constant or a list of\n-    constants. If alpha is a list, it must have the same length as the number\n-    of classes.\n-\n-    The formula above can be generalized to:\n-    `FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)`\n-\n-    where minus comes from `CrossEntropy(y_true, y_pred)` (CE).\n-\n-    Extending this to multi-class case is straightforward:\n-    `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`\n-\n-    In the snippet below, there is `# classes` floating pointing values per\n-    example. The shape of both `y_pred` and `y_true` are\n-    `(batch_size, num_classes)`.\n-\n-    Args:\n-        alpha: A weight balancing factor for all classes, default is `0.25` as\n-            mentioned in the reference. It can be a list of floats or a scalar.\n-            In the multi-class case, alpha may be set by inverse class\n-            frequency by using `compute_class_weight` from `sklearn.utils`.\n-        gamma: A focusing parameter, default is `2.0` as mentioned in the\n-            reference. It helps to gradually reduce the importance given to\n-            simple (easy) examples in a smooth manner.\n-        from_logits: Whether `output` is expected to be a logits tensor. By\n-            default, we consider that `output` encodes a probability\n-            distribution.\n-        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n-            meaning the confidence on label values are relaxed. For example, if\n-            `0.1`, use `0.1 / num_classes` for non-target labels and\n-            `0.9 + 0.1 / num_classes` for target labels.\n-        axis: The axis along which to compute crossentropy (the features\n-            axis). Defaults to -1.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-\n-    Examples:\n-\n-    Standalone usage:\n-\n-    >>> y_true = [[0., 1., 0.], [0., 0., 1.]]\n-    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n-    >>> cce = keras_core.losses.CategoricalFocalCrossentropy()\n-    >>> cce(y_true, y_pred)\n-    0.23315276\n-\n-    >>> # Calling with 'sample_weight'.\n-    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n-    0.1632\n-\n-    >>> # Using 'sum' reduction type.\n-    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n-    ...     reduction=\"sum\")\n-    >>> cce(y_true, y_pred)\n-    0.46631\n-\n-    >>> # Using 'none' reduction type.\n-    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n-    ...     reduction=None)\n-    >>> cce(y_true, y_pred)\n-    array([3.2058331e-05, 4.6627346e-01], dtype=float32)\n-\n-    Usage with the `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=keras_core.losses.CategoricalFocalCrossentropy())\n-    ```\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        alpha=0.25,\n-        gamma=2.0,\n-        from_logits=False,\n-        label_smoothing=0.0,\n-        axis=-1,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"categorical_focal_crossentropy\",\n-    ):\n-        \"\"\"Initializes `CategoricalFocalCrossentropy` instance.\"\"\"\n-        super().__init__(\n-            categorical_focal_crossentropy,\n-            alpha=alpha,\n-            gamma=gamma,\n-            name=name,\n-            reduction=reduction,\n-            from_logits=from_logits,\n-            label_smoothing=label_smoothing,\n-            axis=axis,\n-        )\n-        self.from_logits = from_logits\n-        self.alpha = alpha\n-        self.gamma = gamma\n-\n-    def get_config(self):\n-        config = {\n-            \"alpha\": self.alpha,\n-            \"gamma\": self.gamma,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_core_export(\"keras_core.losses.SparseCategoricalCrossentropy\")\n-class SparseCategoricalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the crossentropy loss between the labels and predictions.\n-\n-    Use this crossentropy loss function when there are two or more label\n-    classes.  We expect labels to be provided as integers. If you want to\n-    provide labels using `one-hot` representation, please use\n-    `CategoricalCrossentropy` loss.  There should be `# classes` floating point\n-    values per feature for `y_pred` and a single floating point value per\n-    feature for `y_true`.\n-\n-    In the snippet below, there is a single floating point value per example for\n-    `y_true` and `# classes` floating pointing values per example for `y_pred`.\n-    The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n-    `[batch_size, num_classes]`.\n-\n-    Args:\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n-        ignore_class: Optional integer. The ID of a class to be ignored during\n-            loss computation. This is useful, for example, in segmentation\n-            problems featuring a \"void\" class (commonly -1 or 255) in\n-            segmentation maps.\n-            By default (`ignore_class=None`), all classes are considered.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-\n-    Examples:\n-\n-    >>> y_true = [1, 2]\n-    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n-    >>> scce = keras_core.losses.SparseCategoricalCrossentropy()\n-    >>> scce(y_true, y_pred)\n-    1.177\n-\n-    >>> # Calling with 'sample_weight'.\n-    >>> scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n-    0.814\n-\n-    >>> # Using 'sum' reduction type.\n-    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n-    ...     reduction=\"sum\")\n-    >>> scce(y_true, y_pred)\n-    2.354\n-\n-    >>> # Using 'none' reduction type.\n-    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n-    ...     reduction=None)\n-    >>> scce(y_true, y_pred)\n-    array([0.0513, 2.303], dtype=float32)\n-\n-    Usage with the `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss=keras_core.losses.SparseCategoricalCrossentropy())\n-    ```\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        from_logits=False,\n-        ignore_class=None,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"sparse_categorical_crossentropy\",\n-    ):\n-        super().__init__(\n-            sparse_categorical_crossentropy,\n-            name=name,\n-            reduction=reduction,\n-            from_logits=from_logits,\n-            ignore_class=ignore_class,\n-        )\n-\n-\n def convert_binary_labels_to_hinge(y_true):\n     \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n     are_zeros = ops.equal(y_true, 0)\n@@ -914,6 +347,12 @@ def hinge(y_true, y_pred):\n     loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)\n     ```\n \n+    Standalone usage:\n+\n+    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n+\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be -1\n             or 1. If binary (0 or 1) labels are provided they will be converted\n@@ -922,12 +361,6 @@ def hinge(y_true, y_pred):\n \n     Returns:\n         Hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, dtype=y_pred.dtype)\n@@ -951,6 +384,12 @@ def squared_hinge(y_true, y_pred):\n     loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)\n     ```\n \n+    Standalone usage:\n+\n+    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n+\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be -1\n             or 1. If binary (0 or 1) labels are provided we will convert them\n@@ -959,12 +398,6 @@ def squared_hinge(y_true, y_pred):\n \n     Returns:\n         Squared hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -991,6 +424,13 @@ def categorical_hinge(y_true, y_pred):\n \n     where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n \n+    Standalone usage:\n+\n+    >>> y_true = np.random.randint(0, 3, size=(2,))\n+    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n+\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be\n             either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor) with\n@@ -999,13 +439,6 @@ def categorical_hinge(y_true, y_pred):\n \n     Returns:\n         Categorical hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = np.random.randint(0, 3, size=(2,))\n-    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -1062,18 +495,18 @@ def mean_absolute_error(y_true, y_pred):\n     loss = mean(abs(y_true - y_pred), axis=-1)\n     ```\n \n+    Standalone usage:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n+\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n         Mean absolute error values with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -1090,16 +523,18 @@ def mean_absolute_error(y_true, y_pred):\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n-    Formula:\n-\n-    ```python\n-    loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)\n-    ```\n+    `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n \n     Division by zero is prevented by dividing by `maximum(y_true, epsilon)`\n     where `epsilon = keras_core.backend.epsilon()`\n     (default to `1e-7`).\n \n+    Standalone usage:\n+\n+    >>> y_true = np.random.random(size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n+\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n@@ -1107,12 +542,6 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     Returns:\n         Mean absolute percentage error values with shape = `[batch_size, d0, ..\n         dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = np.random.random(size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -1141,19 +570,19 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     values and 0 values will be replaced with `keras_core.backend.epsilon()`\n     (default to `1e-7`).\n \n+    Standalone usage:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n+\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n-        Mean squared logarithmic error values with shape = `[batch_size, d0, ..\n+        Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n         dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -1181,6 +610,12 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     similarity will be 0 regardless of the proximity between predictions\n     and targets.\n \n+    Standalone usage:\n+    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n+    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n+    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n+    [-0., -0.99999994, 0.99999994]\n+\n     Args:\n         y_true: Tensor of true targets.\n         y_pred: Tensor of predicted targets.\n@@ -1188,13 +623,6 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n \n     Returns:\n         Cosine similarity tensor.\n-\n-    Example:\n-\n-    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n-    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n-    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n-    [-0., -0.99999994, 0.99999994]\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -1219,14 +647,7 @@ def kl_divergence(y_true, y_pred):\n     loss = y_true * log(y_true / y_pred)\n     ```\n \n-    Args:\n-        y_true: Tensor of true targets.\n-        y_pred: Tensor of predicted targets.\n-\n-    Returns:\n-        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n+    Standalone usage:\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)\n     >>> y_pred = np.random.random(size=(2, 3))\n@@ -1236,6 +657,13 @@ def kl_divergence(y_true, y_pred):\n     >>> y_pred = ops.clip(y_pred, 1e-7, 1)\n     >>> assert np.array_equal(\n     ...     loss, np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n+\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+\n+    Returns:\n+        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, y_pred.dtype)\n@@ -1259,14 +687,7 @@ def poisson(y_true, y_pred):\n     loss = y_pred - y_true * log(y_pred)\n     ```\n \n-    Args:\n-        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-\n-    Returns:\n-        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n+    Standalone usage:\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n@@ -1276,344 +697,15 @@ def poisson(y_true, y_pred):\n     >>> assert np.allclose(\n     ...     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),\n     ...     atol=1e-5)\n+\n+    Args:\n+        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+\n+    Returns:\n+        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     return ops.mean(y_pred - y_true * ops.log(y_pred + epsilon), axis=-1)\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.categorical_crossentropy\",\n-        \"keras_core.losses.categorical_crossentropy\",\n-    ]\n-)\n-def categorical_crossentropy(\n-    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n-):\n-    \"\"\"Computes the categorical crossentropy loss.\n-\n-    Args:\n-        y_true: Tensor of one-hot true targets.\n-        y_pred: Tensor of predicted targets.\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n-        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n-            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n-            and `0.9 + 0.1 / num_classes` for target labels.\n-        axis: Defaults to -1. The dimension along which the entropy is\n-            computed.\n-\n-    Returns:\n-        Categorical crossentropy loss value.\n-\n-    Example:\n-\n-    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n-    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> loss = keras_core.losses.categorical_crossentropy(y_true, y_pred)\n-    >>> assert loss.shape == (2,)\n-    >>> loss.numpy()\n-    array([0.0513, 2.303], dtype=float32)\n-    \"\"\"\n-    if isinstance(axis, bool):\n-        raise ValueError(\n-            \"`axis` must be of type `int`. \"\n-            f\"Received: axis={axis} of type {type(axis)}\"\n-        )\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-\n-    if y_pred.shape[-1] == 1:\n-        warnings.warn(\n-            \"In loss categorical_crossentropy, expected \"\n-            \"y_pred.shape to be (batch_size, num_classes) \"\n-            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n-            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n-            SyntaxWarning,\n-            stacklevel=2,\n-        )\n-\n-    if label_smoothing:\n-        num_classes = ops.cast(ops.shape(y_true)[-1], y_pred.dtype)\n-        y_true = y_true * (1.0 - label_smoothing) + (\n-            label_smoothing / num_classes\n-        )\n-\n-    return ops.categorical_crossentropy(\n-        y_true, y_pred, from_logits=from_logits, axis=axis\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.categorical_focal_crossentropy\",\n-        \"keras_core.losses.categorical_focal_crossentropy\",\n-    ]\n-)\n-def categorical_focal_crossentropy(\n-    y_true,\n-    y_pred,\n-    alpha=0.25,\n-    gamma=2.0,\n-    from_logits=False,\n-    label_smoothing=0.0,\n-    axis=-1,\n-):\n-    \"\"\"Computes the categorical focal crossentropy loss.\n-\n-    Args:\n-        y_true: Tensor of one-hot true targets.\n-        y_pred: Tensor of predicted targets.\n-        alpha: A weight balancing factor for all classes, default is `0.25` as\n-            mentioned in the reference. It can be a list of floats or a scalar.\n-            In the multi-class case, alpha may be set by inverse class\n-            frequency by using `compute_class_weight` from `sklearn.utils`.\n-        gamma: A focusing parameter, default is `2.0` as mentioned in the\n-            reference. It helps to gradually reduce the importance given to\n-            simple examples in a smooth manner. When `gamma` = 0, there is\n-            no focal effect on the categorical crossentropy.\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability\n-            distribution.\n-        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n-            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n-            and `0.9 + 0.1 / num_classes` for target labels.\n-        axis: Defaults to -1. The dimension along which the entropy is\n-            computed.\n-\n-    Returns:\n-        Categorical focal crossentropy loss value.\n-\n-    Example:\n-\n-    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n-    >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]\n-    >>> loss = keras_core.losses.categorical_focal_crossentropy(y_true, y_pred)\n-    >>> assert loss.shape == (2,)\n-    >>> loss\n-    array([2.63401289e-04, 6.75912094e-01], dtype=float32)\n-    \"\"\"\n-    if isinstance(axis, bool):\n-        raise ValueError(\n-            \"`axis` must be of type `int`. \"\n-            f\"Received: axis={axis} of type {type(axis)}\"\n-        )\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-\n-    if y_pred.shape[-1] == 1:\n-        warnings.warn(\n-            \"In loss categorical_focal_crossentropy, expected \"\n-            \"y_pred.shape to be (batch_size, num_classes) \"\n-            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n-            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n-            SyntaxWarning,\n-            stacklevel=2,\n-        )\n-\n-    if label_smoothing:\n-        num_classes = ops.cast(ops.shape(y_true)[-1], y_pred.dtype)\n-        y_true = y_true * (1.0 - label_smoothing) + (\n-            label_smoothing / num_classes\n-        )\n-\n-    return ops.categorical_focal_crossentropy(\n-        target=y_true,\n-        output=y_pred,\n-        alpha=alpha,\n-        gamma=gamma,\n-        from_logits=from_logits,\n-        axis=axis,\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.sparse_categorical_crossentropy\",\n-        \"keras_core.losses.sparse_categorical_crossentropy\",\n-    ]\n-)\n-def sparse_categorical_crossentropy(\n-    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n-):\n-    \"\"\"Computes the sparse categorical crossentropy loss.\n-\n-    Args:\n-        y_true: Ground truth values.\n-        y_pred: The predicted values.\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n-        axis: Defaults to -1. The dimension along which the entropy is\n-            computed.\n-        ignore_class: Optional integer. The ID of a class to be ignored during\n-            loss computation. This is useful, for example, in segmentation\n-            problems featuring a \"void\" class (commonly -1 or 255)\n-            in segmentation maps. By default (`ignore_class=None`),\n-            all classes are considered.\n-\n-    Returns:\n-        Sparse categorical crossentropy loss value.\n-\n-    Examples:\n-\n-    >>> y_true = [1, 2]\n-    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> loss = keras_core.losses.sparse_categorical_crossentropy(y_true, y_pred)\n-    >>> assert loss.shape == (2,)\n-    >>> loss\n-    array([0.0513, 2.303], dtype=float32)\n-\n-    >>> y_true = [[[ 0,  2],\n-    ...            [-1, -1]],\n-    ...           [[ 0,  2],\n-    ...            [-1, -1]]]\n-    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n-    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n-    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n-    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n-    >>> loss = keras_core.losses.sparse_categorical_crossentropy(\n-    ...   y_true, y_pred, ignore_class=-1)\n-    array([[[2.3841855e-07, 2.3841855e-07],\n-            [0.0000000e+00, 0.0000000e+00]],\n-           [[2.3841855e-07, 6.9314730e-01],\n-            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n-    \"\"\"\n-    return ops.sparse_categorical_crossentropy(\n-        y_true,\n-        y_pred,\n-        from_logits=from_logits,\n-        ignore_class=ignore_class,\n-        axis=axis,\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.binary_crossentropy\",\n-        \"keras_core.losses.binary_crossentropy\",\n-    ]\n-)\n-def binary_crossentropy(\n-    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n-):\n-    \"\"\"Computes the binary crossentropy loss.\n-\n-    Args:\n-        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n-        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n-            squeezing them towards 0.5, that is,\n-            using `1. - 0.5 * label_smoothing` for the target class\n-            and `0.5 * label_smoothing` for the non-target class.\n-        axis: The axis along which the mean is computed. Defaults to -1.\n-\n-    Returns:\n-        Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = [[0, 1], [0, 0]]\n-    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n-    >>> loss = keras_core.losses.binary_crossentropy(y_true, y_pred)\n-    >>> assert loss.shape == (2,)\n-    >>> loss\n-    array([0.916 , 0.714], dtype=float32)\n-    \"\"\"\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-\n-    if label_smoothing:\n-        y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n-\n-    return ops.mean(\n-        ops.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n-        axis=axis,\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.metrics.binary_focal_crossentropy\",\n-        \"keras_core.losses.binary_focal_crossentropy\",\n-    ]\n-)\n-def binary_focal_crossentropy(\n-    y_true,\n-    y_pred,\n-    apply_class_balancing=False,\n-    alpha=0.25,\n-    gamma=2.0,\n-    from_logits=False,\n-    label_smoothing=0.0,\n-    axis=-1,\n-):\n-    \"\"\"Computes the binary focal crossentropy loss.\n-\n-    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n-    helps to apply a focal factor to down-weight easy examples and focus more on\n-    hard examples. By default, the focal tensor is computed as follows:\n-\n-    `focal_factor = (1 - output)**gamma` for class 1\n-    `focal_factor = output**gamma` for class 0\n-    where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n-    effect on the binary crossentropy loss.\n-\n-    If `apply_class_balancing == True`, this function also takes into account a\n-    weight balancing factor for the binary classes 0 and 1 as follows:\n-\n-    `weight = alpha` for class 1 (`target == 1`)\n-    `weight = 1 - alpha` for class 0\n-    where `alpha` is a float in the range of `[0, 1]`.\n-\n-    Args:\n-        y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.\n-        y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n-        apply_class_balancing: A bool, whether to apply weight balancing on the\n-            binary classes 0 and 1.\n-        alpha: A weight balancing factor for class 1, default is `0.25` as\n-            mentioned in the reference. The weight for class 0 is `1.0 - alpha`.\n-        gamma: A focusing parameter, default is `2.0` as mentioned in the\n-            reference.\n-        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n-            default, we assume that `y_pred` encodes a probability distribution.\n-        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n-            squeezing them towards 0.5, that is,\n-            using `1. - 0.5 * label_smoothing` for the target class\n-            and `0.5 * label_smoothing` for the non-target class.\n-        axis: The axis along which the mean is computed. Defaults to `-1`.\n-\n-    Returns:\n-        Binary focal crossentropy loss value\n-        with shape = `[batch_size, d0, .. dN-1]`.\n-\n-    Example:\n-\n-    >>> y_true = [[0, 1], [0, 0]]\n-    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n-    >>> loss = keras_core.losses.binary_focal_crossentropy(\n-    ...        y_true, y_pred, gamma=2)\n-    >>> assert loss.shape == (2,)\n-    >>> loss\n-    array([0.330, 0.206], dtype=float32)\n-    \"\"\"\n-    y_pred = ops.convert_to_tensor(y_pred)\n-    y_true = ops.cast(y_true, y_pred.dtype)\n-\n-    if label_smoothing:\n-        y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n-\n-    return ops.mean(\n-        ops.binary_focal_crossentropy(\n-            target=y_true,\n-            output=y_pred,\n-            apply_class_balancing=apply_class_balancing,\n-            alpha=alpha,\n-            gamma=gamma,\n-            from_logits=from_logits,\n-        ),\n-        axis=axis,\n-    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
