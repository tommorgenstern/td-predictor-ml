{"custom_id": "keras#136dfb597717e82d0b12b1693a1a9a10cd419a4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 49 | Churn Cumulative: 2290 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -556,7 +556,12 @@ class VariableAndLossTracker(tf.Module):\n \n   def __init__(self):\n     self._var_store = _EagerVariableStore()  # pylint: disable=protected-access\n-    self._variables = self._var_store._vars  # pylint: disable=protected-access\n+    self._variables = {}\n+\n+    # Because this is a module, self._variables becomes a trackable dict\n+    # object that supports restoration. We must make the var_store share\n+    # this same trackable dict object.\n+    self._var_store._vars = self._variables  # pylint: disable=protected-access\n \n   @tf_contextlib.contextmanager\n   def scope(self):\n@@ -600,18 +605,18 @@ class VariableScopeModule(tf.Module):\n \n   def __init__(self, name=None):\n     super().__init__(name=name)\n-    self._tracker = VariableAndLossTracker()\n+    self._var_tracker = VariableAndLossTracker()\n \n   def forward_pass(self, *args, **kwargs):\n     raise NotImplementedError\n \n   def __call__(self, *args, **kwargs):\n     with self.name_scope:\n-      with self._tracker.scope():\n+      with self._var_tracker.scope():\n         return self.forward_pass(*args, **kwargs)\n \n   def get_compat_v1_regularization_losses(self):\n-    return self._tracker.get_regularization_losses()\n+    return self._var_tracker.get_regularization_losses()\n \n \n class VariableScopeLayer(base_layer.Layer):\n@@ -769,7 +774,7 @@ class VariableScopeLayer(base_layer.Layer):\n   def __init__(self, **kwargs):\n     super().__init__(**kwargs)\n     # Relies on keras layers tracking Modules\n-    self._tracker = VariableAndLossTracker()\n+    self._var_tracker = VariableAndLossTracker()\n     # May need to inspect func to see if it should pass a `training` arg or not\n \n   @property\n@@ -783,13 +788,13 @@ class VariableScopeLayer(base_layer.Layer):\n     raise NotImplementedError\n \n   def call(self, *args, **kwargs):\n-    with self._tracker.scope():\n+    with self._var_tracker.scope():\n       out = self.forward_pass(*args, **kwargs)\n     if not self._eager_losses:\n       # We have to record regularization losses in the call as if they\n       # are activity losses.\n       # So, don't double-count regularization losses if the layer is used\n       # multiple times in a model\n-      for loss in self._tracker.get_regularization_losses().values():\n+      for loss in self._var_tracker.get_regularization_losses().values():\n         self.add_loss(loss)\n     return out\n\n@@ -905,6 +905,36 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 200)\n     self.assertAllEqual(tf.add_n(layer.losses), 6)\n \n+  def test_shim_exporting(self):\n+\n+    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def forward_pass(self, inputs, training=None):\n+        out = core_layers.dense(\n+            inputs,\n+            self.units,\n+            name=\"dense_one\",\n+            kernel_initializer=tf.compat.v1.ones_initializer(),\n+            kernel_regularizer=\"l2\")\n+        with tf.compat.v1.variable_scope(\"nested_scope\"):\n+          out = core_layers.dense(\n+              out,\n+              self.units,\n+              name=\"dense_two\",\n+              kernel_initializer=tf.compat.v1.ones_initializer(),\n+              kernel_regularizer=\"l2\")\n+        return out\n+\n+    layer = WrappedDenseLayer(10)\n+    layer(tf.ones(shape=(5, 5)))\n+\n+    tmp_dir = self.get_temp_dir()\n+    tf.saved_model.save(layer, tmp_dir)\n+\n   def test_module_get_variable(self):\n     # Test the module shim when using `get_variable` (and regularizers) directly\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4da9c94739f9832115a84c0f22098c47f29e533", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 15604 | Contributors (this commit): 68 | Commits (past 90d): 5 | Contributors (cumulative): 68 | DMM Complexity: None\n\nDIFF:\n@@ -2706,7 +2706,7 @@ class LSTM(RNN):\n     recurrent_dropout: Float between 0 and 1.\n       Fraction of the units to drop for\n       the linear transformation of the recurrent state.\n-    return_sequences: Boolean. Whether to return the last output.\n+    return_sequences: Boolean. Whether to return the last output\n       in the output sequence, or the full sequence.\n     return_state: Boolean. Whether to return the last state\n       in addition to the output.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d369e05c1075bb6732537d1a69d1fd4f5d0aa749", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 3134 | Contributors (this commit): 5 | Commits (past 90d): 36 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -38,11 +38,11 @@ class IntegerLookup(index_lookup.IndexLookup):\n   for encoding the output via `output_mode`, and has optional support for\n   out-of-vocabulary (OOV) tokens and masking.\n \n-  The vocabulary for the layer can be supplied on construction or learned via\n-  `adapt()`. During `adapt()`, the layer will analyze a data set, determine the\n-  frequency of individual integer tokens, and create a vocabulary from them. If\n-  the vocabulary is capped in size, the most frequent tokens will be used to\n-  create the vocabulary and all others will be treated as OOV.\n+  The vocabulary for the layer must be either supplied on construction or\n+  learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n+  determine the frequency of individual integer tokens, and create a vocabulary\n+  from them. If the vocabulary is capped in size, the most frequent tokens will\n+  be used to create the vocabulary and all others will be treated as OOV.\n \n   There are two possible output modes for the layer.\n   When `output_mode` is `\"int\"`,\n@@ -61,9 +61,11 @@ class IntegerLookup(index_lookup.IndexLookup):\n   indices and instances of the mask token will be dropped.\n \n   Args:\n-    max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that this size\n-      includes the OOV and mask tokens. Default to None.\n+    max_tokens: Maximum size of the vocabulary for this layer. This should only\n+      be specified when adapting the vocabulary or when setting\n+      `pad_to_max_tokens=True`. If None, there is no cap on the size of the\n+      vocabulary. Note that this size includes the OOV and mask tokens. Defaults\n+      to None.\n     num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n       value is more than 1, OOV inputs are modulated to determine their OOV\n       value. If this value is 0, OOV inputs will cause an error when calling the\n\n@@ -33,12 +33,12 @@ class StringLookup(index_lookup.IndexLookup):\n   This layer translates a set of arbitrary strings into an integer output via a\n   table-based vocabulary lookup.\n \n-  The vocabulary for the layer can be supplied on construction or learned via\n-  `adapt()`. During `adapt()`, the layer will analyze a data set, determine the\n-  frequency of individual strings tokens, and create a vocabulary from them. If\n-  the vocabulary is capped in size, the most frequent tokens will be used to\n-  create the vocabulary and all others will be treated as out-of-vocabulary\n-  (OOV).\n+  The vocabulary for the layer must be either supplied on construction or\n+  learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n+  determine the frequency of individual strings tokens, and create a vocabulary\n+  from them. If the vocabulary is capped in size, the most frequent tokens will\n+  be used to create the vocabulary and all others will be treated as\n+  out-of-vocabulary (OOV).\n \n   There are two possible output modes for the layer.\n   When `output_mode` is `\"int\"`,\n@@ -57,9 +57,11 @@ class StringLookup(index_lookup.IndexLookup):\n   indices and instances of the mask token will be dropped.\n \n   Args:\n-    max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that this size\n-      includes the OOV and mask tokens. Default to None.\n+    max_tokens: Maximum size of the vocabulary for this layer. This should only\n+      be specified when adapting the vocabulary or when setting\n+      `pad_to_max_tokens=True`. If None, there is no cap on the size of the\n+      vocabulary. Note that this size includes the OOV and mask tokens. Defaults\n+      to None.\n     num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n       value is more than 1, OOV inputs are hashed to determine their OOV value.\n       If this value is 0, OOV inputs will cause an error when calling the layer.\n\n@@ -70,13 +70,13 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n   representation (one example = 1D tensor of float values representing data\n   about the example's tokens).\n \n-  If desired, the user can call this layer's `adapt()` method on a dataset.\n-  When this layer is adapted, it will analyze the dataset, determine the\n-  frequency of individual string values, and create a 'vocabulary' from them.\n-  This vocabulary can have unlimited size or be capped, depending on the\n-  configuration options for this layer; if there are more unique values in the\n-  input than the maximum vocabulary size, the most frequent terms will be used\n-  to create the vocabulary.\n+  The vocabulary for the layer must be either supplied on construction or\n+  learned via `adapt()`. When this layer is adapted, it will analyze the\n+  dataset, determine the frequency of individual string values, and create a\n+  vocabulary from them. This vocabulary can have unlimited size or be capped,\n+  depending on the configuration options for this layer; if there are more\n+  unique values in the input than the maximum vocabulary size, the most frequent\n+  terms will be used to create the vocabulary.\n \n   The processing of each example contains the following steps:\n \n@@ -107,8 +107,9 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n      site natively compatible with `tf.strings.split()`.\n \n   Args:\n-    max_tokens: The maximum size of the vocabulary for this layer. If None,\n-      there is no cap on the size of the vocabulary. Note that this vocabulary\n+    max_tokens: Maximum size of the vocabulary for this layer. This should only\n+      be specified when adapting a vocabulary or when setting\n+      `pad_to_max_tokens=True`. Note that this vocabulary\n       contains 1 OOV token, so the effective number of tokens is `(max_tokens -\n       1 - (1 if output_mode == \"int\" else 0))`.\n     standardize: Optional specification for standardization to apply to the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b21c9c4edd6fcc8a948726c1b3641f18de63354", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1620 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -722,9 +722,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         # context. (eager updates execute immediately)\n         with backend._current_graph(update_ops).as_default():  # pylint: disable=protected-access\n           with tf.control_dependencies([tf.group(update_ops)]):\n-            return self._iterations.assign_add(1, read_value=False)\n+            return self.iterations.assign_add(1, read_value=False)\n \n-      return self._iterations.assign_add(1)\n+      return self.iterations.assign_add(1)\n \n   def get_gradients(self, loss, params):\n     \"\"\"Returns gradients of `loss` with respect to `params`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6538c2568124fac7eb70220b06a6b13fd6823e4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 571 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -233,8 +233,9 @@ class Attention(BaseDenseAttention):\n     causal: Boolean. Set to `True` for decoder self-attention. Adds a mask such\n       that position `i` cannot attend to positions `j > i`. This prevents the\n       flow of information from the future towards the past.\n+      Defaults to `False`.\n     dropout: Float between 0 and 1. Fraction of the units to drop for the\n-      attention scores.\n+      attention scores. Defaults to 0.0.\n \n   Call Args:\n \n@@ -372,8 +373,9 @@ class AdditiveAttention(BaseDenseAttention):\n     causal: Boolean. Set to `True` for decoder self-attention. Adds a mask such\n       that position `i` cannot attend to positions `j > i`. This prevents the\n       flow of information from the future towards the past.\n+      Defaults to `False`.\n     dropout: Float between 0 and 1. Fraction of the units to drop for the\n-      attention scores.\n+      attention scores. Defaults to 0.0.\n \n   Call Args:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30c921514a0023d9164273a5883c0476c31629b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 53 | Churn Cumulative: 7061 | Contributors (this commit): 40 | Commits (past 90d): 10 | Contributors (cumulative): 44 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -1080,6 +1080,19 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     cb_list.on_predict_batch_end(logs)\n     cb_list.on_predict_end(logs)\n \n+  def test_verbose_2_logging(self):\n+    data = np.random.random((100, 1))\n+    labels = np.where(data > 0.5, 1, 0)\n+    model = keras.models.Sequential((keras.layers.Dense(\n+        1, input_dim=1, activation='relu'), keras.layers.Dense(\n+            1, activation='sigmoid'),))\n+    model.compile(\n+        optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n+    expected_log = r'(.*- loss:.*- accuracy:.*epoch)+'\n+    with self.captureWritesToStream(sys.stdout) as printed:\n+      model.fit(data, labels, verbose=2, epochs=20)\n+      self.assertRegex(printed.contents(), expected_log)\n+\n   def test_ProgbarLogger_verbose_2_nonblocking(self):\n     # Should only cause a sync block on epoch end methods.\n     callback = keras.callbacks.ProgbarLogger(count_mode='steps')\n\n@@ -856,7 +856,8 @@ class Progbar(object):\n     self._values_order = []\n     self._start = time.time()\n     self._last_update = 0\n-\n+    self._time_at_epoch_start = self._start\n+    self._time_at_epoch_end = None\n     self._time_after_first_step = None\n \n   def update(self, current, values=None, finalize=None):\n@@ -900,6 +901,8 @@ class Progbar(object):\n \n     now = time.time()\n     info = ' - %.0fs' % (now - self._start)\n+    if current == self.target:\n+      self._time_at_epoch_end = now\n     if self.verbose == 1:\n       if now - self._last_update < self.interval and not finalize:\n         return\n@@ -933,12 +936,7 @@ class Progbar(object):\n       time_per_unit = self._estimate_step_duration(current, now)\n \n       if self.target is None or finalize:\n-        if time_per_unit >= 1 or time_per_unit == 0:\n-          info += ' %.0fs/%s' % (time_per_unit, self.unit_name)\n-        elif time_per_unit >= 1e-3:\n-          info += ' %.0fms/%s' % (time_per_unit * 1e3, self.unit_name)\n-        else:\n-          info += ' %.0fus/%s' % (time_per_unit * 1e6, self.unit_name)\n+        info += self._format_time(time_per_unit, self.unit_name)\n       else:\n         eta = time_per_unit * (self.target - current)\n         if eta > 3600:\n@@ -984,8 +982,14 @@ class Progbar(object):\n             info += ' %.4f' % avg\n           else:\n             info += ' %.4e' % avg\n+        if self._time_at_epoch_end:\n+          time_per_epoch = self._time_at_epoch_end - self._time_at_epoch_start\n+          avg_time_per_step = time_per_epoch / self.target\n+          self._time_at_epoch_start = now\n+          self._time_at_epoch_end = None\n+          info += ' -' + self._format_time(time_per_epoch, 'epoch')\n+          info += ' -' + self._format_time(avg_time_per_step, self.unit_name)\n           info += '\\n'\n-\n         sys.stdout.write(info)\n         sys.stdout.flush()\n \n@@ -994,6 +998,26 @@ class Progbar(object):\n   def add(self, n, values=None):\n     self.update(self._seen_so_far + n, values)\n \n+  def _format_time(self, time_per_unit, unit_name):\n+    \"\"\"format a given duration to display to the user.\n+\n+    Given the duration, this function formats it in either milliseconds\n+    or seconds and displays the unit (i.e. ms/step or s/epoch)\n+    Args:\n+      time_per_unit: the duration to display\n+      unit_name: the name of the unit to display\n+    Returns:\n+      a string with the correctly formatted duration and units\n+    \"\"\"\n+    formatted = ''\n+    if time_per_unit >= 1 or time_per_unit == 0:\n+      formatted += ' %.0fs/%s' % (time_per_unit, unit_name)\n+    elif time_per_unit >= 1e-3:\n+      formatted += ' %.0fms/%s' % (time_per_unit * 1e3, unit_name)\n+    else:\n+      formatted += ' %.0fus/%s' % (time_per_unit * 1e6, unit_name)\n+    return formatted\n+\n   def _estimate_step_duration(self, current, now):\n     \"\"\"Estimate the duration of a single step.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed6dae62551510787265a821f4b3d969ab4075d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 165 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 25 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 197 | Churn Cumulative: 1071 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -581,26 +581,136 @@ class VariableAndLossTracker(tf.Module):\n class VariableScopeModule(tf.Module):\n   \"\"\"Wrapper Module to capture `compat.v1.get_variable` and `compat.v1.layers`.\n \n-  See go/tf2-migration-model-bookkeeping for background.\n-\n-  This shim layer allows using large sets of TF1 model-forward-pass code as a\n+  This shim module allows using large sets of TF1 model-forward-pass code as a\n   tf.Module that works in TF2 with TF2 behaviors enabled. To use it,\n   override this class and put your TF1 model's forward pass inside your\n-  implementation for `forward_pass`.\n+  implementation for `forward_pass`. You can then call this module directly\n+  to run your TF1 code with working variable reuse.\n \n-  Limitations:\n+  Caution: Unless `tf.compat.v1.VariableScopeLayer` does not work for your code\n+  because of Keras layers' call signature semantics around the first argument,\n+  and you need a lower-level solution, we recommend using\n+  `tf.compat.v1.VariableScopeLayer` instead. `VariableScopeModule` can be used\n+  if you cannot adopt Keras layers to your use case.\n+\n+  Example of capturing tf.compat.v1.layer-based modeling code as a tf.Module:\n+\n+  ```python\n+  class WrappedDoubleDenseModule(variable_scope_shim.VariableScopeModule):\n+\n+    def __init__(self, units, *args, **kwargs):\n+      super().__init__(*args, **kwargs)\n+      self.units = units\n+\n+    def forward_pass(self, inputs):\n+      with variable_scope.variable_scope(\"double_dense_layer\"):\n+        out = tf.compat.v1.layers.dense(\n+            inputs, self.units, name=\"dense_one\",\n+            kernel_initializer=tf.compat.v1.random_normal_initializer,\n+            kernel_regularizer=\"l2\")\n+        out = tf.compat.v1.layers.dense(\n+            out, self.units, name=\"dense_two\",\n+            kernel_initializer=tf.compat.v1.random_normal_initializer(),\n+            kernel_regularizer=\"l2\")\n+      return out\n+\n+  # Create a layer that can be used as a callable Module\n+  layer = WrappedDoubleDenseModule(10)\n+\n+  # call the layer on inputs\n+  layer(...)\n+\n+  # Variables created/used within `forward_pass` will be tracked\n+  layer.variables\n+  layer.trainable_variables\n+\n+  # Regularization losses created by `tf.compat.v1.get_variable` or by\n+  # `tf.compat.v1.layers` will be tracked as well. Note: all other\n+  # regularization losses must be tracked manually if using modules. If you\n+  # would like fully-automated regularization loss tracking, use\n+  # `tf.compat.v1.VariableScopeLayer` instead.\n+  reg_losses = layer.get_compat_v1_regularization_losses()\n+  ```\n+\n+  Example of capturing tf.compat.v1.get_variable-based modeling code as\n+  a tf.Module:\n+\n+  ```python\n+  class WrappedDoubleDenseModule(variable_scope_shim.VariableScopeModule):\n+\n+    def __init__(self, units, *args, **kwargs):\n+      super().__init__(*args, **kwargs)\n+      self.units = units\n+\n+    def forward_pass(self, inputs):\n+      out = inputs\n+      with variable_scope.variable_scope(\"double_dense_layer\"):\n+        with tf.compat.v1.variable_scope(\"dense_one\"):\n+          # The weights are created with a `regularizer`,\n+          # so the layer should track their regularization losses\n+          kernel = tf.compat.v1.get_variable(\n+              shape=[out.shape[-1], self.units],\n+              regularizer=regularizers.L2(),\n+              initializer=init_ops.ones_initializer(),\n+              name=\"kernel\")\n+          bias = tf.compat.v1.get_variable(\n+              shape=[self.units,],\n+              initializer=init_ops.zeros_initializer(),\n+              name=\"bias\")\n+          out = tf.compat.v1.math.matmul(out, kernel)\n+          out = tf.compat.v1.nn.bias_add(out, bias)\n+        with tf.compat.v1.variable_scope(\"dense_two\"):\n+          kernel = tf.compat.v1.get_variable(\n+              shape=[out.shape[-1], self.units],\n+              regularizer=regularizers.L2(),\n+              initializer=init_ops.ones_initializer(),\n+              name=\"kernel\")\n+          bias = tf.compat.v1.get_variable(\n+              shape=[self.units,],\n+              initializer=init_ops.zeros_initializer(),\n+              name=\"bias\")\n+          out = tf.compat.v1.math.matmul(out, kernel)\n+          out = tf.compat.v1.nn.bias_add(out, bias)\n+      return out\n+\n+  # Create a module that can be used as a callable Module\n+  layer = WrappedDoubleDenseModule(10)\n+\n+  # call the module on inputs\n+  layer(...)\n+\n+  # Variables created/used within the scope will be tracked by the module\n+  layer.variables\n+  layer.trainable_variables\n+\n+  # Regularization losses created by `tf.compat.v1.get_variable` or by\n+  # `tf.compat.v1.layers` will be tracked as well. Note: all other\n+  # regularization losses must be tracked manually if using modules. If you\n+  # would like fully-automated regularization loss tracking, use\n+  # `tf.compat.v1.VariableScopeLayer` instead.\n+  reg_losses = layer.get_compat_v1_regularization_losses()\n+  ```\n+\n+  Caveats:\n     * TF2 will not prune unused variable updates (or unused outputs). You may\n       need to adjust your forward pass code to avoid computations or variable\n       updates that you don't intend to use. (E.g. by adding a flag to the\n       `forward_pass` call signature and branching on it).\n-    * Avoid Nesting variable creation in `tf.function` inside of `forward_pass`\n+    * Avoid Nesting variable creation in tf.function inside of `forward_pass`\n       While the layer may safely be used from inside a `tf.function`, using\n       a function inside of `forward_pass` will break the variable scoping.\n-    * TBD: Nesting keras layers/models or other `VariableScopeModule`s\n-      directly in `forward_pass` may not work correctly just yet.\n-      Support for this/instructions for how to do this is still being worked on.\n-\n-  Coming soon: A better guide, testing/verification guide.\n+    * If you would like to nest modules or other\n+      `VariableScopeModules`s directly in `forward_pass`, you need to\n+      assign them as properties of your VariableScopeModule so that\n+      object-oriented weights will activate in for them.\n+      See the intro to modules, layers, and models\n+      [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info\n+      on this explicit recursive weight tracking.\n+      Only weights made by `tf.compat.v1.get_variable` calls and\n+      `tf.compat.v1.layers` will get implicitly tracked by your\n+      VariableScopeModule without being assigned as properties. Note: if these\n+      modules have their own regularization losses you must track them\n+      manually.\n   \"\"\"\n \n   def __init__(self, name=None):\n@@ -608,6 +718,7 @@ class VariableScopeModule(tf.Module):\n     self._var_tracker = VariableAndLossTracker()\n \n   def forward_pass(self, *args, **kwargs):\n+    \"\"\"Implement this method. It should include your model forward pass.\"\"\"\n     raise NotImplementedError\n \n   def __call__(self, *args, **kwargs):\n@@ -616,18 +727,34 @@ class VariableScopeModule(tf.Module):\n         return self.forward_pass(*args, **kwargs)\n \n   def get_compat_v1_regularization_losses(self):\n+    \"\"\"Dict w/ regularization losses from `get_variable` and `compat.v1.layers`.\n+\n+    VariableScopeModule is able to track regularization losses from\n+    `tf.compat.v1.get_variable` calls and `compat.v1.layers` that are used\n+    within the `forward_pass`. All other regularization losses (such as\n+    those belonging to Keras layers or nested VariableScopeModules)\n+    must be tracked via your own manual mechanism as is standard for tf.Modules.\n+    If you would like more comprehensive regularization loss tracking, use\n+    `VariableScopeModule` instead.\n+\n+    Returns:\n+      A dict of the form { variable_name : regularization_loss } for any\n+      regularization losses specified for variables created and used by\n+      `tf.compat.v1.get_variable` (with the regularizer argument passed)\n+      or `tf.compat.v1.layers` within `forward_pass`.\n+    \"\"\"\n     return self._var_tracker.get_regularization_losses()\n \n \n class VariableScopeLayer(base_layer.Layer):\n   \"\"\"Wrapper Layer to capture `compat.v1.get_variable` and `compat.v1.layers`.\n \n-  See go/tf2-migration-model-bookkeeping for background.\n-\n   This shim layer allows using large sets of TF1 model-forward-pass code as a\n-  Keras layer that works in TF2 with TF2 behaviors enabled. To use it,\n+  Keras layer that works in TF2 with TF2 behaviors enabled. It will capture\n+  both weights and regularization losses of your forward-pass code. To use it,\n   override this class and put your TF1 model's forward pass inside your\n-  implementation for `forward_pass`.\n+  implementation for `forward_pass`. (Unlike standard custom Keras layers,\n+  do not override `call`.)\n \n   Below are some examples, and then more details on the functionality of this\n   shim layer to wrap TF1 model forward passes.\n@@ -641,15 +768,15 @@ class VariableScopeLayer(base_layer.Layer):\n       super().__init__(*args, **kwargs)\n       self.units = units\n \n-    def forward_pass(self, inputs, training=None):\n+    def forward_pass(self, inputs):\n+      with variable_scope.variable_scope(\"double_dense_layer\"):\n         out = tf.compat.v1.layers.dense(\n             inputs, self.units, name=\"dense_one\",\n-          kernel_initializer=init_ops.ones_initializer(),\n+            kernel_initializer=tf.compat.v1.random_normal_initializer,\n             kernel_regularizer=\"l2\")\n-      with variable_scope.variable_scope(\"nested_scope\"):\n         out = tf.compat.v1.layers.dense(\n             out, self.units, name=\"dense_two\",\n-            kernel_initializer=init_ops.ones_initializer(),\n+            kernel_initializer=tf.compat.v1.random_normal_initializer(),\n             kernel_regularizer=\"l2\")\n       return out\n \n@@ -668,8 +795,8 @@ class VariableScopeLayer(base_layer.Layer):\n   reg_losses = layer.losses\n   ```\n \n-  The solution is to wrap the model construction and execution in a keras-style\n-  scope:\n+  Example of capturing tf.compat.v1.get_variable-based modeling code as\n+  a Keras layer:\n \n   ```python\n   class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):\n@@ -678,8 +805,9 @@ class VariableScopeLayer(base_layer.Layer):\n       super().__init__(*args, **kwargs)\n       self.units = units\n \n-    def forward_pass(self, inputs, training=None):\n+    def forward_pass(self, inputs):\n       out = inputs\n+      with tf.compat.v1.variable_scope(\"double_dense_layer\"):\n         with tf.compat.v1.variable_scope(\"dense_one\"):\n           # The weights are created with a `regularizer`,\n           # so the layer should track their regularization losses\n@@ -694,7 +822,6 @@ class VariableScopeLayer(base_layer.Layer):\n               name=\"bias\")\n           out = tf.compat.v1.math.matmul(out, kernel)\n           out = tf.compat.v1.nn.bias_add(out, bias)\n-      with tf.compat.v1.variable_scope(\"nested_scope\"):\n         with tf.compat.v1.variable_scope(\"dense_two\"):\n           kernel = tf.compat.v1.get_variable(\n               shape=[out.shape[-1], self.units],\n@@ -729,15 +856,19 @@ class VariableScopeLayer(base_layer.Layer):\n     creations will get captured by this wrapper layer. Regularization losses\n     are accessible in `layer.losses` after a call just like in a standard\n     Keras layer, and will be captured by any model that includes this layer.\n+    Regularization losses attached to Keras layers/models set as attributes\n+    of your layer will also get captured in the standard Keras regularization\n+    loss tracking.\n \n   Variable scope / variable reuse:\n     variable-scope based reuse in the `forward_pass` will be respected,\n     and work like variable-scope based reuse in TF1.\n \n   Variable Names/Pre-trained checkpoint loading:\n-    variable naming from get_variable and `compat.v1.layer` layers will match\n+    Variable naming from get_variable and `compat.v1.layer` layers will match\n     the TF1 names, so you should be able to re-use your old name-based\n-    checkpoints.\n+    checkpoints. Variable naming for Keras layers/models or for variables\n+    created by `tf.Variable` may change when going to eager execution.\n \n   Training Arg in `forward_pass`:\n     Keras will pass a `training` arg to this layer if `forward_pass` contains\n@@ -756,19 +887,20 @@ class VariableScopeLayer(base_layer.Layer):\n     to pass in a `training` arg., and the semantics applied to\n     the first positional arg in the call signature.\n \n-  Limitations:\n+  Caveats:\n     * TF2 will not prune unused variable updates (or unused outputs). You may\n       need to adjust your forward pass code to avoid computations or variable\n       updates that you don't intend to use. (E.g. by adding a flag to the\n       `forward_pass` call signature and branching on it).\n     * Avoid Nesting variable creation in tf.function inside of `forward_pass`\n-      While the layer may safetely be used from inside a `tf.function`, using\n+      While the layer may safely be used from inside a `tf.function`, using\n       a function inside of `forward_pass` will break the variable scoping.\n-    * TBD: Nesting keras layers/models or other `VariableScopeLayer`s\n-      directly in `forward_pass` may not work correctly just yet.\n-      Support for this/instructions for how to do this is sill being worked on.\n-\n-  Coming soon: A better guide, testing/verification guide.\n+    * If you would like to nest Keras layers/models or other\n+      `VariableScopeLayer`s directly in `forward_pass`, you need to\n+      assign them as attributes of your layer so that Keras's standard\n+      object-oriented weights and loss tracking will kick in.\n+      See the intro to modules, layers, and models\n+      [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info\n   \"\"\"\n \n   def __init__(self, **kwargs):\n@@ -785,6 +917,7 @@ class VariableScopeLayer(base_layer.Layer):\n     return tf_inspect.getfullargspec(self.forward_pass)\n \n   def forward_pass(self, *args, **kwargs):\n+    \"\"\"Implement this method. It should include your model forward pass.\"\"\"\n     raise NotImplementedError\n \n   def call(self, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cffc048a59fb2619c13cb0f4d022f8aa23a89a7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 150 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,6 +26,7 @@ PIP_EXCLUDED_FILES = frozenset([\n     'keras/api/create_python_api_wrapper.py',\n     'keras/applications/efficientnet_weight_update_util.py',\n     'keras/distribute/tpu_strategy_test_utils.py',\n+    'keras/saving/saved_model/create_test_saved_model.py'\n     'keras/tools/pip_package/setup.py',\n     'keras/tools/pip_package/create_pip_helper.py',\n ])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#767670b9b7ffa94afadcdc652ebee18513ba6867", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 70 | Churn Cumulative: 1486 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -801,6 +801,25 @@ class VariableScopeMultithreadedTest(tf.test.TestCase):\n       thread.join()\n \n \n+class CompatV1TemplateScaleByY(variable_scope_shim.VariableScopeLayer):\n+\n+  def __init__(self, **kwargs):\n+    super().__init__(**kwargs)\n+    def my_op(x, scalar_name):\n+      var1 = tf.compat.v1.get_variable(\n+          scalar_name,\n+          shape=[],\n+          regularizer=regularizers.L2(),\n+          initializer=tf.compat.v1.constant_initializer(1.5))\n+      return x * var1\n+    self.scale_by_y = tf.compat.v1.make_template(\n+        \"scale_by_y\", my_op, scalar_name=\"y\")\n+\n+  def forward_pass(self, inputs):\n+    with tf.compat.v1.variable_scope(\"foo\"):\n+      return self.scale_by_y(inputs)\n+\n+\n @combinations.generate(combinations.combine(mode=[\"eager\"]))\n class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n@@ -1117,6 +1136,57 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(out, tf.ones(shape=(1, 5)) * 120)\n     self.assertAllEqual(tf.add_n(layer.losses), 320)\n \n+  def test_compat_v1_make_template_in_shim_eager(self):\n+    # Test the shim when using `compat.v1.make_template`\n+    # Verify it works correctly in eager\n+    layer = CompatV1TemplateScaleByY()\n+    for _ in range(3):\n+      # Use multiple calls to verify that no new weights get created\n+      self.assertAllEqual(layer(tf.ones(shape=(2, 3))),\n+                          tf.constant(1.5, shape=(2, 3)))\n+    self.assertAllEqual({var.name: var for var in layer.weights},\n+                        {\"foo/scale_by_y/y:0\": 1.5})\n+    self.assertAllEqual(tf.add_n(layer.losses),\n+                        regularizers.L2()(layer.weights[0]))\n+\n+  def test_compat_v1_make_template_in_shim_tf_function(self):\n+    # Test the shim when using `compat.v1.make_template`\n+    # Verify it works correctly in a tf.function\n+    # when made outside the function\n+    layer = CompatV1TemplateScaleByY()\n+\n+    @tf.function\n+    def foo(x):\n+      return layer(x), tf.add_n(layer.losses)\n+\n+    for _ in range(3):\n+      # Use multiple calls to verify that no new weights get created\n+      out, loss = foo(tf.ones(shape=(2, 3)))\n+      self.assertAllEqual(out, tf.constant(1.5, shape=(2, 3)))\n+      self.assertAllEqual(loss, regularizers.L2()(layer.weights[0]))\n+    self.assertAllEqual({var.name: var for var in layer.weights},\n+                        {\"foo/scale_by_y/y:0\": 1.5})\n+\n+  def test_compat_v1_make_template_in_trace_in_shim(self):\n+    # Test the shim when using `compat.v1.make_template`\n+    # Verify it works correctly when the make_template/layer/shim\n+    # is created on the first tf.function trace!\n+    layers = {}\n+    @tf.function\n+    def bar(x):\n+      if \"layer\" not in layers:\n+        layers[\"layer\"] = CompatV1TemplateScaleByY()\n+      layer = layers[\"layer\"]\n+      return layer(x), tf.add_n(layer.losses)\n+\n+    for _ in range(3):\n+      # Use multiple calls to verify that no new weights get created\n+      out, loss = bar(tf.ones(shape=(2, 3)))\n+      self.assertAllEqual(out, tf.constant(1.5, shape=(2, 3)))\n+      self.assertAllEqual(loss, regularizers.L2()(layers[\"layer\"].weights[0]))\n+    self.assertAllEqual({var.name: var for var in layers[\"layer\"].weights},\n+                        {\"foo/scale_by_y/y:0\": 1.5})\n+\n   def test_only_track_get_variable(self):\n     # Test the shim does not try tracking or reusing variables\n     # that were not created by get_variable. These variables/modules/layers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00518dcf7f5b7de7064267979d7e9f9916e777eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 7339 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,10 +62,17 @@ _GRAPH = threading.local()\n # A graph which is used for constructing functions in eager mode.\n _CURRENT_SCRATCH_GRAPH = threading.local()\n \n+\n # This is a thread local object that will hold the default internal TF session\n # used by Keras. It can be set manually via `set_session(sess)`.\n-_SESSION = threading.local()\n-_SESSION.session = None\n+class SessionLocal(threading.local):\n+\n+  def __init__(self):\n+    super().__init__()\n+    self.session = None\n+\n+\n+_SESSION = SessionLocal()\n \n \n # A global dictionary mapping graph objects to an index of counters used\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e32215a6ec81ac36cc4813119377aef95de82219", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 6 | Churn Cumulative: 18986 | Contributors (this commit): 77 | Commits (past 90d): 10 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3400,6 +3400,12 @@ class Cropping1D(Layer):\n     return tf.TensorShape([input_shape[0], length, input_shape[2]])\n \n   def call(self, inputs):\n+    if sum(self.cropping) >= inputs.shape[1]:\n+      raise ValueError(\n+        'cropping parameter of Cropping layer is too high,' +\n+        'the result of crop' + str(inputs.shape) + ' with cropping ' +\n+        str(self.cropping) + ' is an empty tensor'\n+      )\n     if self.cropping[1] == 0:\n       return inputs[:, self.cropping[0]:, :]\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6e8d5b910aa8afca0f9c4b5b9435cc356adb78c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 152 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ PIP_EXCLUDED_FILES = frozenset([\n     'keras/api/create_python_api_wrapper.py',\n     'keras/applications/efficientnet_weight_update_util.py',\n     'keras/distribute/tpu_strategy_test_utils.py',\n-    'keras/saving/saved_model/create_test_saved_model.py'\n+    'keras/saving/saved_model/create_test_saved_model.py',\n     'keras/tools/pip_package/setup.py',\n     'keras/tools/pip_package/create_pip_helper.py',\n ])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb7cf4c255593fe773c9387787c9d232ceb88739", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 9 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 55 | Churn Cumulative: 10770 | Contributors (this commit): 96 | Commits (past 90d): 15 | Contributors (cumulative): 104 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2452,7 +2452,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   def _start_trace(self):\n     tf.summary.trace_on(graph=True, profiler=False)\n-    self._start_profiler(logdir=self._train_dir)\n+    self._start_profiler(logdir=self.log_dir)\n     self._is_tracing = True\n \n   def _stop_trace(self, batch=None):\n\n@@ -2204,7 +2204,7 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         callbacks=[tb_cbk])\n \n     events_file_run_basenames = set()\n-    for (dirpath, _, filenames) in os.walk(self.logdir):\n+    for (dirpath, _, filenames) in os.walk(self.train_dir):\n       if any(fn.startswith('events.out.') for fn in filenames):\n         events_file_run_basenames.add(os.path.basename(dirpath))\n     self.assertEqual(events_file_run_basenames, {'train'})\n@@ -2639,7 +2639,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n             _ObservedSummary(logdir=self.train_dir, tag=u'batch_1'),\n         },\n     )\n-    self.assertEqual(1, self._count_trace_file(logdir=self.train_dir))\n+    self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n \n   def test_TensorBoard_autoTrace_outerProfiler(self):\n     \"\"\"Runs a profiler session that interferes with the one from the callback.\n@@ -2692,7 +2692,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n             _ObservedSummary(logdir=self.train_dir, tag=u'batch_2'),\n         },\n     )\n-    self.assertEqual(1, self._count_trace_file(logdir=self.train_dir))\n+    self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n \n   def test_TensorBoard_autoTrace_profileBatchRangeSingle(self):\n     model = self._get_seq_model()\n@@ -2716,7 +2716,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n             _ObservedSummary(logdir=self.train_dir, tag=u'batch_2'),\n         },\n     )\n-    self.assertEqual(1, self._count_trace_file(logdir=self.train_dir))\n+    self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n \n   def test_TensorBoard_autoTrace_profileBatchRangeTwice(self):\n     model = self._get_seq_model()\n@@ -2741,7 +2741,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n         epochs=10,\n         validation_data=(x, y),\n         callbacks=[tb_cbk])\n-    self.assertEqual(2, self._count_trace_file(logdir=self.train_dir))\n+    self.assertEqual(2, self._count_trace_file(logdir=self.logdir))\n \n   # Test case that replicates a Github issue.\n   # https://github.com/tensorflow/tensorflow/issues/37543\n@@ -2794,7 +2794,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n             _ObservedSummary(logdir=self.train_dir, tag=u'batch_3'),\n         },\n     )\n-    self.assertEqual(1, self._count_trace_file(logdir=self.train_dir))\n+    self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n \n   def test_TensorBoard_autoTrace_profileInvalidBatchRange(self):\n     with self.assertRaises(ValueError):\n\n@@ -234,6 +234,42 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n         cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),\n         args=(self, saving_filepath))\n \n+  @tf.__internal__.distribute.combinations.generate(\n+      tf.__internal__.test.combinations.combine(mode=['eager']))\n+  def test_profiler_saves_on_both_chief_and_non_chief(self, mode):\n+\n+    def proc_profiler_saves_on_both_chief_and_non_chief(test_obj):\n+      model, _, train_ds, steps = _model_setup(test_obj, file_format='')\n+      num_epoch = 2\n+\n+      task_config = get_tf_config_task()\n+      saving_filepath = os.path.join(\n+          test_obj.get_temp_dir(),\n+          'logfile_%s_%d' % (task_config['type'], task_config['index']))\n+\n+      # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))\n+\n+      model.fit(\n+          x=train_ds,\n+          epochs=num_epoch,\n+          steps_per_epoch=steps,\n+          callbacks=[\n+              callbacks.TensorBoard(\n+                  log_dir=saving_filepath, profile_batch=[2, 4])\n+          ])\n+\n+      # Profiler dir should be created on both chief and non-chief node\n+      profiler_dir_path = os.path.join(saving_filepath, 'plugins', 'profile')\n+      test_obj.assertTrue(tf.io.gfile.exists(profiler_dir_path))\n+\n+    tf.__internal__.distribute.multi_process_runner.run(\n+        proc_profiler_saves_on_both_chief_and_non_chief,\n+        cluster_spec=\n+        tf.__internal__.distribute.multi_process_runner.create_cluster_spec(\n+            num_workers=2),\n+        args=(self,))\n+\n   @tf.__internal__.distribute.combinations.generate(tf.__internal__.test.combinations.combine(mode=['eager']))\n   def test_tensorboard_saves_on_chief_but_not_otherwise(self, mode):\n \n@@ -257,7 +293,10 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n           x=train_ds,\n           epochs=num_epoch,\n           steps_per_epoch=steps,\n-          callbacks=[callbacks.TensorBoard(log_dir=saving_filepath)])\n+          # disabling profiler by setting profile_batch to zero\n+          callbacks=[\n+              callbacks.TensorBoard(log_dir=saving_filepath, profile_batch=0)\n+          ])\n \n       # If it's chief, the summaries should be saved in the filepath; if not,\n       # the directory should be empty (although created). Using\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6290a9e6812c93b336ed640dc4034e519d9b4b0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 36 | Churn Cumulative: 3063 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,11 @@ from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n+try:\n+  import pandas as pd  # pylint: disable=g-import-not-at-top\n+except ImportError:\n+  pd = None\n+\n keras_data_adapter_gauge = tf.__internal__.monitoring.BoolGauge(\n     \"/tensorflow/api/keras/data_adapters\", \"keras data adapter usage\", \"method\")\n \n@@ -1009,7 +1014,8 @@ def _process_tensorlike(inputs):\n \n   (1) Converts `Numpy` arrays to `Tensor`s.\n   (2) Converts `Scipy` sparse matrices to `SparseTensor`s.\n-  (2) Converts `list`s to `tuple`s (for `tf.data` support).\n+  (3) Converts `pandas.Series` to `Tensor`s\n+  (4) Converts `list`s to `tuple`s (for `tf.data` support).\n \n   Args:\n     inputs: Structure of `Tensor`s, `NumPy` arrays, or tensor-like.\n@@ -1018,7 +1024,10 @@ def _process_tensorlike(inputs):\n     Structure of `Tensor`s or tensor-like.\n   \"\"\"\n \n-  def _convert_numpy_and_scipy(x):\n+  def _convert_single_tensor(x):\n+    if _is_pandas_series(x):\n+      x = np.expand_dims(x.to_numpy(), axis=-1)\n+\n     if isinstance(x, np.ndarray):\n       dtype = None\n       if issubclass(x.dtype.type, np.floating):\n@@ -1028,7 +1037,7 @@ def _process_tensorlike(inputs):\n       return _scipy_sparse_to_sparse_tensor(x)\n     return x\n \n-  inputs = tf.nest.map_structure(_convert_numpy_and_scipy, inputs)\n+  inputs = tf.nest.map_structure(_convert_single_tensor, inputs)\n   return tf.__internal__.nest.list_to_tuple(inputs)\n \n \n@@ -1656,12 +1665,10 @@ def _check_data_cardinality(data):\n \n \n def _get_tensor_types():\n-  try:\n-    import pandas as pd  # pylint: disable=g-import-not-at-top\n-\n-    return (tf.Tensor, np.ndarray, pd.Series, pd.DataFrame)\n-  except ImportError:\n+  if pd is None:\n     return (tf.Tensor, np.ndarray)\n+  else:\n+    return (tf.Tensor, np.ndarray, pd.Series, pd.DataFrame)\n \n \n def _is_scipy_sparse(x):\n@@ -1673,6 +1680,13 @@ def _is_scipy_sparse(x):\n     return False\n \n \n+def _is_pandas_series(x):\n+  if pd is None:\n+    return False\n+  else:\n+    return isinstance(x, pd.Series)\n+\n+\n def _scipy_sparse_to_sparse_tensor(t):\n   \"\"\"Converts a SciPy sparse matrix to a SparseTensor.\"\"\"\n   sparse_coo = t.tocoo()\n\n@@ -183,6 +183,7 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n \n     model_1.compile(optimizer='rmsprop', loss='mse')\n     model_2.compile(optimizer='rmsprop', loss='mse')\n+    model_3.compile(optimizer='rmsprop', loss='mse')\n \n     input_a_np = np.random.random((10, 3))\n     input_b_np = np.random.random((10, 3))\n@@ -191,11 +192,14 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n \n     output_a_df = pd.DataFrame(np.random.random((10, 4)))\n     output_b_df = pd.DataFrame(np.random.random((10, 3)))\n+    output_c_series = pd.DataFrame(np.random.random((10, 4)))[0]\n \n     model_1.fit(input_a_df,\n                 output_a_df)\n     model_2.fit([input_a_df, input_b_df],\n                 [output_a_df, output_b_df])\n+    model_3.fit(input_a_df[[0]],\n+                output_c_series)\n     model_1.fit([input_a_df],\n                 [output_a_df])\n     model_1.fit({'input_a': input_a_df},\n@@ -207,6 +211,8 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n                      output_a_df)\n     model_2.evaluate([input_a_df, input_b_df],\n                      [output_a_df, output_b_df])\n+    model_3.evaluate(input_a_df[[0]],\n+                     output_c_series)\n     model_1.evaluate([input_a_df],\n                      [output_a_df])\n     model_1.evaluate({'input_a': input_a_df},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
