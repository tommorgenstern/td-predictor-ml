{"custom_id": "keras#16590ccce51c44e72b0826f50641bcbb2a09a3d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 41/41 | Churn Δ: 2 | Churn Cumulative: 2960 | Contributors (this commit): 30 | Commits (past 90d): 78 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -1005,7 +1005,7 @@ class Lambda(Layer):\n \n     @property\n     def output_shape(self):\n-        if self._ouput_shape is None:\n+        if self._output_shape is None:\n             return self.input_shape\n         elif type(self._output_shape) == tuple:\n             return (self.input_shape[0], ) + self._output_shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71b00324d86a3de18850ff085783a705d0e6f8fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2964 | Contributors (this commit): 31 | Commits (past 90d): 79 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -986,8 +986,8 @@ class Lambda(Layer):\n     output_shape - Expected output shape from function. Could be a tuple or a function of the shape of the input\n     \"\"\"\n \n-    def __init__(self, function, output_shape=None):\n-        super(Lambda, self).__init__()\n+    def __init__(self, function, output_shape=None, **kwargs):\n+        super(Lambda, self).__init__(**kwargs)\n         py3 = sys.version_info[0] == 3\n         if py3:\n             self.function = marshal.dumps(function.__code__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34838cd3692a9af3d6d61a42de8e2f40d1e8a2d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 13 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 43 | Churn Cumulative: 545 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -45,7 +45,7 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(32, 3, 3, border_mode='full',\n+model.add(Convolution2D(32, 3, 3, border_mode='same',\n                         input_shape=(img_channels, img_rows, img_cols)))\n model.add(Activation('relu'))\n model.add(Convolution2D(32, 3, 3))\n@@ -53,7 +53,7 @@ model.add(Activation('relu'))\n model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n \n-model.add(Convolution2D(64, 3, 3, border_mode='full'))\n+model.add(Convolution2D(64, 3, 3, border_mode='same'))\n model.add(Activation('relu'))\n model.add(Convolution2D(64, 3, 3))\n model.add(Activation('relu'))\n\n@@ -51,7 +51,7 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n model = Sequential()\n \n model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n-                        border_mode='full',\n+                        border_mode='same',\n                         input_shape=(1, img_rows, img_cols)))\n model.add(Activation('relu'))\n model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n\n@@ -12,8 +12,9 @@ from keras.utils import np_utils\n '''\n     Train a simple deep NN on the MNIST dataset.\n \n-    Get to 98.30% test accuracy after 20 epochs (there is *a lot* of margin for parameter tuning).\n-    2 seconds per epoch on a GRID K520 GPU.\n+    Get to 98.40% test accuracy after 20 epochs\n+    (there is *a lot* of margin for parameter tuning).\n+    2 seconds per epoch on a K520 GPU.\n '''\n \n batch_size = 128\n@@ -37,10 +38,10 @@ Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n-model.add(Dense(128, input_shape=(784,)))\n+model.add(Dense(512, input_shape=(784,)))\n model.add(Activation('relu'))\n model.add(Dropout(0.2))\n-model.add(Dense(128))\n+model.add(Dense(512))\n model.add(Activation('relu'))\n model.add(Dropout(0.2))\n model.add(Dense(10))\n@@ -49,7 +50,11 @@ model.add(Activation('softmax'))\n rms = RMSprop()\n model.compile(loss='categorical_crossentropy', optimizer=rms)\n \n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))\n-score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+model.fit(X_train, Y_train,\n+          batch_size=batch_size, nb_epoch=nb_epoch,\n+          show_accuracy=True, verbose=2,\n+          validation_data=(X_test, Y_test))\n+score = model.evaluate(X_test, Y_test,\n+                       show_accuracy=True, verbose=0)\n print('Test score:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -14,11 +14,13 @@ from keras.utils import np_utils\n '''\n     Transfer learning toy example:\n         1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n-        2- Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].\n+        2- Freeze convolutional layers and fine-tune dense layers\n+           for the classification of digits [5..9].\n \n     Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n \n-    Get to 99.8% test accuracy after 5 epochs for the first five digits classifier\n+    Get to 99.8% test accuracy after 5 epochs\n+    for the first five digits classifier\n     and 99.2% for the last five digits after transfer + fine-tuning.\n '''\n \n@@ -56,7 +58,9 @@ def train_model(model, train, test, nb_classes):\n     model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n \n     t = now()\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1,\n+    model.fit(X_train, Y_train,\n+              batch_size=batch_size, nb_epoch=nb_epoch,\n+              show_accuracy=True, verbose=1,\n               validation_data=(X_test, Y_test))\n     print('Training time: %s' % (now() - t))\n     score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n@@ -81,7 +85,7 @@ y_test_gte5 = y_test[y_test >= 5] - 5\n # define two groups of layers: feature (convolutions) and classification (dense)\n feature_layers = [\n     Convolution2D(nb_filters, nb_conv, nb_conv,\n-                  border_mode='full',\n+                  border_mode='valid',\n                   input_shape=(1, img_rows, img_cols)),\n     Activation('relu'),\n     Convolution2D(nb_filters, nb_conv, nb_conv),\n@@ -104,11 +108,15 @@ for l in feature_layers + classification_layers:\n     model.add(l)\n \n # train model for 5-digit classification [0..4]\n-train_model(model, (X_train_lt5, y_train_lt5), (X_test_lt5, y_test_lt5), nb_classes)\n+train_model(model,\n+            (X_train_lt5, y_train_lt5),\n+            (X_test_lt5, y_test_lt5), nb_classes)\n \n # freeze feature layers and rebuild model\n for l in feature_layers:\n     l.trainable = False\n \n # transfer: train dense layers for new classification task [5..9]\n-train_model(model, (X_train_gte5, y_train_gte5), (X_test_gte5, y_test_gte5), nb_classes)\n+train_model(model,\n+            (X_train_gte5, y_train_gte5),\n+            (X_test_gte5, y_test_gte5), nb_classes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f2b5f0458cce4e5588238b86e3e2738952ea849", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 324 | Lines Deleted: 283 | Files Changed: 14 | Hunks: 156 | Methods Changed: 85 | Complexity Δ (Sum/Max): -46/20 | Churn Δ: 607 | Churn Cumulative: 13096 | Contributors (this commit): 62 | Commits (past 90d): 193 | Contributors (cumulative): 135 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,17 +1,24 @@\n import numpy as np\n \n # the type of float to use throughout the session.\n-_FLOATX = 'float32'\n+_FLOATX = 'float64'\n _EPSILON = 10e-8\n \n \n+def epsilon():\n+    return _EPSILON\n+\n+\n+def set_epsilon(e):\n+    global _EPSILON\n+    _EPSILON = e\n+\n+\n def floatx():\n     return _FLOATX\n \n \n def set_floatx(floatx):\n-    '''\n-    '''\n     global _FLOATX\n     if floatx not in {'float32', 'float64'}:\n         raise Exception('Unknown floatx type: ' + str(floatx))\n\n@@ -28,6 +28,9 @@ def variable(value, dtype=_FLOATX, name=None):\n \n \n def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n+    if not shape:\n+        if ndim:\n+            shape = [None for _ in range(ndim)]\n     return tf.placeholder(dtype, shape=shape, name=name)\n \n \n@@ -35,6 +38,10 @@ def shape(x):\n     return x.get_shape()\n \n \n+def ndim(x):\n+    return len(x.get_shape())\n+\n+\n def eval(x):\n     '''Run a graph.\n     '''\n@@ -64,6 +71,10 @@ def count_params(x):\n     return np.prod([shape[i]._value for i in range(len(shape))])\n \n \n+def cast(x, dtype):\n+    return tf.cast(x, dtype)\n+\n+\n # LINEAR ALGEBRA\n \n def dot(x, y):\n@@ -74,7 +85,7 @@ def transpose(x):\n     return tf.transpose(x)\n \n \n-def embedding(reference, indices):\n+def gather(reference, indices):\n     '''reference: a tensor.\n     indices: an int tensor of indices.\n \n@@ -86,13 +97,13 @@ def embedding(reference, indices):\n # ELEMENT-WISE OPERATIONS\n \n def max(x, axis=None, keepdims=False):\n-    if axis is not None:\n+    if axis is not None and axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.reduce_max(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def min(x, axis=None, keepdims=False):\n-    if axis is not None:\n+    if axis is not None and axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n \n@@ -100,7 +111,7 @@ def min(x, axis=None, keepdims=False):\n def sum(x, axis=None, keepdims=False):\n     '''Sum of the values in a tensor, alongside the specified axis.\n     '''\n-    if axis is not None:\n+    if axis is not None and axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n@@ -112,6 +123,10 @@ def prod(x, axis=None, keepdims=False):\n \n \n def std(x, axis=None, keepdims=False):\n+    if axis is not None and axis < 0:\n+        axis = axis % len(x.get_shape())\n+    if x.dtype.base_dtype == tf.bool:\n+        x = tf.cast(x, _FLOATX)\n     m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n     devs_squared = tf.square(x - m)\n     return tf.sqrt(tf.reduce_mean(devs_squared,\n@@ -120,8 +135,10 @@ def std(x, axis=None, keepdims=False):\n \n \n def mean(x, axis=None, keepdims=False):\n-    if axis is not None:\n+    if axis is not None and axis < 0:\n         axis = axis % len(x.get_shape())\n+    if x.dtype.base_dtype == tf.bool:\n+        x = tf.cast(x, _FLOATX)\n     return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n@@ -130,7 +147,7 @@ def any(x, axis=None, keepdims=False):\n \n     Return array of int8 (0s and 1s).\n     '''\n-    if axis is not None:\n+    if axis is not None and axis < 0:\n         axis = axis % len(x.get_shape())\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -138,11 +155,13 @@ def any(x, axis=None, keepdims=False):\n \n \n def argmax(x, axis=-1):\n+    if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.argmax(x, axis)\n \n \n def argmin(x, axis=-1):\n+    if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.argmin(x, axis)\n \n@@ -173,6 +192,10 @@ def round(x):\n     return tf.round(x)\n \n \n+def pow(x, a):\n+    return tf.pow(x, a)\n+\n+\n def clip(x, min_value, max_value):\n     if max_value < min_value:\n         max_value = min_value\n@@ -195,6 +218,7 @@ def minimum(x, y):\n # SHAPE OPERATIONS\n \n def concatenate(tensors, axis=-1):\n+    if axis < 0:\n         axis = axis % len(tensors[0].get_shape())\n     return tf.concat(axis, tensors)\n \n@@ -228,6 +252,9 @@ def tile(x, n):\n \n \n def flatten(x):\n+    '''Turn a n-D tensor into a 2D tensor where\n+    the first dimension is conserved.\n+    '''\n     x = tf.reshape(x, [-1, np.prod(x.get_shape()[1:].as_list())])\n     return x\n \n@@ -244,6 +271,26 @@ def squeeze(x, axis):\n     return tf.squeeze(x, [axis])\n \n \n+def temporal_padding(x, padding=1):\n+    '''Pad the middle dimension of a 3D tensor\n+    with \"padding\" zeros left and right.\n+\n+    Appologies for the inane API, but Theano makes this\n+    really hard.\n+    '''\n+    pattern = [[0, 0], [padding, padding], [0, 0]]\n+    return tf.pad(x, pattern)\n+\n+\n+def spatial_2d_padding(x, padding=(1, 1)):\n+    '''Pad the 2nd and 3rd dimensions of a 4D tensor\n+    with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n+    '''\n+    pattern = [[0, 0], [0, 0],\n+               [padding[0], padding[0]], [padding[1], padding[1]]]\n+    return tf.pad(x, pattern)\n+\n+\n # VALUE MANIPULATION\n \n def get_value(x):\n@@ -253,7 +300,7 @@ def get_value(x):\n \n \n def set_value(x, value):\n-    tf.assign(x, value).op.run(session=_get_session())\n+    tf.assign(x, np.asarray(value)).op.run(session=_get_session())\n \n \n # GRAPH MANIPULATION\n@@ -310,7 +357,7 @@ def relu(x, alpha=0., max_value=None):\n     if max_value is not None:\n         x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n                              tf.cast(max_value, dtype=_FLOATX))\n-    x -= alpha * negative_part\n+    x -= tf.constant(alpha, dtype=_FLOATX) * negative_part\n     return x\n \n \n\n@@ -7,6 +7,8 @@ from .common import _FLOATX, _EPSILON\n \n \n # INTERNAL UTILS\n+theano.config.floatX = _FLOATX\n+\n \n def _on_gpu():\n     '''Returns whether the session is set to\n@@ -62,6 +64,10 @@ def shape(x):\n     return x.shape\n \n \n+def ndim(x):\n+    return x.ndim\n+\n+\n def eval(x):\n     '''Run a graph.\n     '''\n@@ -96,6 +102,10 @@ def count_params(x):\n     return np.prod(x.shape.eval())\n \n \n+def cast(x, dtype):\n+    return T.cast(x, dtype)\n+\n+\n # LINEAR ALGEBRA\n \n '''\n@@ -112,7 +122,7 @@ def transpose(x):\n     return T.transpose(x)\n \n \n-def embedding(reference, indices):\n+def gather(reference, indices):\n     '''reference: a tensor.\n     indices: an int tensor of indices.\n \n@@ -123,6 +133,7 @@ def embedding(reference, indices):\n \n # ELEMENT-WISE OPERATIONS\n \n+\n def max(x, axis=None, keepdims=False):\n     return T.max(x, axis=axis, keepdims=keepdims)\n \n@@ -190,6 +201,10 @@ def round(x):\n     return T.round(x)\n \n \n+def pow(x, a):\n+    return T.pow(x, a)\n+\n+\n def clip(x, min_value, max_value):\n     if max_value < min_value:\n         max_value = min_value\n@@ -244,6 +259,9 @@ def tile(x, n):\n \n \n def flatten(x):\n+    '''Turn a n-D tensor into a 2D tensor where\n+    the first dimension is conserved.\n+    '''\n     x = T.reshape(x, (x.shape[0], T.prod(x.shape) // x.shape[0]))\n     return x\n \n@@ -253,6 +271,9 @@ def expand_dims(x, dim=-1):\n     '''\n     pattern = [i for i in range(x.type.ndim)]\n     if dim < 0:\n+        if x.type.ndim == 0:\n+            dim = 0\n+        else:\n             dim = dim % x.type.ndim + 1\n     pattern.insert(dim, 'x')\n     return x.dimshuffle(pattern)\n@@ -265,8 +286,40 @@ def squeeze(x, axis):\n     return T.squeeze(x)\n \n \n+def temporal_padding(x, padding=1):\n+    '''Pad the middle dimension of a 3D tensor\n+    with \"padding\" zeros left and right.\n+\n+    Appologies for the inane API, but Theano makes this\n+    really hard.\n+    '''\n+    input_shape = x.shape\n+    output_shape = (input_shape[0],\n+                    input_shape[1] + 2 * padding,\n+                    input_shape[2])\n+    output = T.zeros(output_shape)\n+    return T.set_subtensor(output[:, padding:x.shape[1] + padding, :], x)\n+\n+\n+def spatial_2d_padding(x, padding=(1, 1)):\n+    '''Pad the 2nd and 3rd dimensions of a 4D tensor\n+    with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n+    '''\n+    input_shape = x.shape\n+    output_shape = (input_shape[0],\n+                    input_shape[1],\n+                    input_shape[2] + 2 * padding[0],\n+                    input_shape[3] + 2 * padding[1])\n+    output = T.zeros(output_shape)\n+    indices = (slice(None),\n+               slice(None),\n+               slice(padding[0], input_shape[2] + padding[0]),\n+               slice(padding[1], input_shape[3] + padding[1]))\n+    return T.set_subtensor(output[indices], x)\n+\n # VALUE MANIPULATION\n \n+\n def get_value(x):\n     if not hasattr(x, 'get_value'):\n         raise Exception(\"'get_value() can only be called on a variable. \" +\n@@ -287,9 +340,7 @@ class Function(object):\n                                         allow_input_downcast=True, **kwargs)\n \n     def __call__(self, inputs):\n-        if len(inputs) == 1:\n-            inputs = inputs[0]  # Theano is pretty dumb there tbh\n-        return self.function(inputs)\n+        return self.function(*inputs)\n \n \n def function(inputs, outputs, updates=[]):\n\n@@ -27,14 +27,13 @@ class MaxNorm(Constraint):\n \n class NonNeg(Constraint):\n     def __call__(self, p):\n-        p = K.variable(p)\n-        p *= (p >= 0.)\n+        p *= K.cast(p >= 0., K.floatx())\n         return p\n \n \n class UnitNorm(Constraint):\n     def __call__(self, p):\n-        return p / K.sqrt(K.sum(p**2, axis=-1, keepdims=True))\n+        return p / K.sqrt(K.sum(K.square(p), axis=-1, keepdims=True))\n \n identity = Constraint\n maxnorm = MaxNorm\n\n@@ -226,13 +226,12 @@ class Graph(Layer):\n         layer.set_input_shape(input_shape)\n         ndim = len(input_shape) + 1\n         if dtype == 'float':\n-            layer.input = K.placeholder(ndim=ndim)\n+            layer.input = K.placeholder(ndim=ndim, name=name)\n         else:\n             if ndim == 2:\n-                layer.input = K.placeholder(ndim=2, dtype='int32')\n+                layer.input = K.placeholder(ndim=2, dtype='int32', name=name)\n             else:\n                 raise Exception('Type \"int\" can only be used with ndim==2 (Embedding).')\n-        layer.input.name = name\n         self.inputs[name] = layer\n         self.input_config.append({'name': name,\n                                   'input_shape': input_shape,\n\n@@ -1,53 +1,22 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n \n-import theano\n-import theano.tensor as T\n-from theano.tensor.signal import downsample\n-\n+from .. import backend as K\n from .. import activations, initializations, regularizers, constraints\n-from ..utils.theano_utils import shared_zeros, on_gpu\n from ..layers.core import Layer\n \n-if on_gpu():\n-    from theano.sandbox.cuda import dnn\n-\n \n def conv_output_length(input_length, filter_size, border_mode, stride):\n     if input_length is None:\n         return None\n-    assert border_mode in {'same', 'full', 'valid'}\n+    assert border_mode in {'same', 'valid'}\n     if border_mode == 'same':\n         output_length = input_length\n-    elif border_mode == 'full':\n-        output_length = input_length + filter_size - 1\n     elif border_mode == 'valid':\n         output_length = input_length - filter_size + 1\n     return (output_length + stride - 1) // stride\n \n \n-def pool_output_length(input_length, pool_size, ignore_border, stride):\n-    if input_length is None:\n-        return None\n-    if ignore_border:\n-        output_length = input_length - pool_size + 1\n-        output_length = (output_length + stride - 1) // stride\n-    else:\n-        if pool_size == input_length:\n-            output_length = min(input_length, stride - stride % 2)\n-            if output_length <= 0:\n-                output_length = 1\n-        elif stride >= pool_size:\n-            output_length = (input_length + stride - 1) // stride\n-        else:\n-            output_length = (input_length - pool_size + stride - 1) // stride\n-            if output_length <= 0:\n-                output_length = 1\n-            else:\n-                output_length += 1\n-    return output_length\n-\n-\n class Convolution1D(Layer):\n     input_ndim = 3\n \n@@ -57,7 +26,7 @@ class Convolution1D(Layer):\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, input_dim=None, input_length=None, **kwargs):\n \n-        if border_mode not in {'valid', 'full', 'same'}:\n+        if border_mode not in {'valid', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n         self.nb_filter = nb_filter\n         self.filter_length = filter_length\n@@ -82,14 +51,14 @@ class Convolution1D(Layer):\n         self.input_length = input_length\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_length, self.input_dim)\n+        self.input = K.placeholder(ndim=3)\n         super(Convolution1D, self).__init__(**kwargs)\n \n     def build(self):\n         input_dim = self.input_shape[2]\n-        self.input = T.tensor3()\n         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)\n         self.W = self.init(self.W_shape)\n-        self.b = shared_zeros((self.nb_filter,))\n+        self.b = K.zeros((self.nb_filter,))\n         self.params = [self.W, self.b]\n         self.regularizers = []\n \n@@ -116,39 +85,15 @@ class Convolution1D(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n+        X = K.expand_dims(X, -1)  # add a dimension of the right\n+        X = K.permute_dimensions(X, (0, 2, 1, 3))\n+        conv_out = K.conv2d(X, self.W, strides=self.subsample,\n+                            border_mode=self.border_mode, dim_ordering='th')\n \n-        border_mode = self.border_mode\n-        if on_gpu() and dnn.dnn_available():\n-            if border_mode == 'same':\n-                assert(self.subsample_length == 1)\n-                pad_x = (self.filter_length - self.subsample_length) // 2\n-                conv_out = dnn.dnn_conv(img=X,\n-                                        kerns=self.W,\n-                                        border_mode=(pad_x, 0))\n-            else:\n-                conv_out = dnn.dnn_conv(img=X,\n-                                        kerns=self.W,\n-                                        border_mode=border_mode,\n-                                        subsample=self.subsample)\n-        else:\n-            if border_mode == 'same':\n-                assert(self.subsample_length == 1)\n-                border_mode = 'full'\n-\n-            input_shape = self.input_shape\n-            image_shape = (input_shape[0], input_shape[2], input_shape[1], 1)\n-            conv_out = T.nnet.conv.conv2d(X, self.W,\n-                                          border_mode=border_mode,\n-                                          subsample=self.subsample,\n-                                          image_shape=image_shape,\n-                                          filter_shape=self.W_shape)\n-            if self.border_mode == 'same':\n-                shift_x = (self.filter_length - 1) // 2\n-                conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, :]\n-\n-        output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n-        output = T.reshape(output, (output.shape[0], output.shape[1], output.shape[2])).dimshuffle(0, 2, 1)\n+        output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+        output = self.activation(output)\n+        output = K.squeeze(output, 3)  # remove the dummy 3rd dimension\n+        output = K.permute_dimensions(output, (0, 2, 1))\n         return output\n \n     def get_config(self):\n@@ -175,11 +120,11 @@ class Convolution2D(Layer):\n \n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation='linear', weights=None,\n-                 border_mode='valid', subsample=(1, 1),\n+                 border_mode='valid', subsample=(1, 1), dim_ordering='th',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, **kwargs):\n \n-        if border_mode not in {'valid', 'full', 'same'}:\n+        if border_mode not in {'valid', 'same'}:\n             raise Exception('Invalid border mode for Convolution2D:', border_mode)\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n@@ -188,6 +133,7 @@ class Convolution2D(Layer):\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n+        self.dim_ordering = dim_ordering\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n@@ -198,14 +144,14 @@ class Convolution2D(Layer):\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n         self.initial_weights = weights\n+        self.input = K.placeholder(ndim=4)\n         super(Convolution2D, self).__init__(**kwargs)\n \n     def build(self):\n         stack_size = self.input_shape[1]\n-        self.input = T.tensor4()\n         self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)\n         self.W = self.init(self.W_shape)\n-        self.b = shared_zeros((self.nb_filter,))\n+        self.b = K.zeros((self.nb_filter,))\n         self.params = [self.W, self.b]\n         self.regularizers = []\n \n@@ -236,36 +182,13 @@ class Convolution2D(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        border_mode = self.border_mode\n-        if on_gpu() and dnn.dnn_available():\n-            if border_mode == 'same':\n-                assert(self.subsample == (1, 1))\n-                pad_x = (self.nb_row - self.subsample[0]) // 2\n-                pad_y = (self.nb_col - self.subsample[1]) // 2\n-                conv_out = dnn.dnn_conv(img=X,\n-                                        kerns=self.W,\n-                                        border_mode=(pad_x, pad_y))\n-            else:\n-                conv_out = dnn.dnn_conv(img=X,\n-                                        kerns=self.W,\n-                                        border_mode=border_mode,\n-                                        subsample=self.subsample)\n-        else:\n-            if border_mode == 'same':\n-                border_mode = 'full'\n-                assert(self.subsample == (1, 1))\n+        conv_out = K.conv2d(X, self.W, strides=self.subsample,\n+                            border_mode=self.border_mode,\n+                            dim_ordering=self.dim_ordering)\n \n-            conv_out = T.nnet.conv.conv2d(X, self.W,\n-                                          border_mode=border_mode,\n-                                          subsample=self.subsample,\n-                                          image_shape=self.input_shape,\n-                                          filter_shape=self.W_shape)\n-            if self.border_mode == 'same':\n-                shift_x = (self.nb_row - 1) // 2\n-                shift_y = (self.nb_col - 1) // 2\n-                conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, shift_y:X.shape[3] + shift_y]\n-\n-        return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n+        output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+        output = self.activation(output)\n+        return output\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -276,6 +199,7 @@ class Convolution2D(Layer):\n                   \"activation\": self.activation.__name__,\n                   \"border_mode\": self.border_mode,\n                   \"subsample\": self.subsample,\n+                  \"dim_ordering\": self.dim_ordering,\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n@@ -288,7 +212,8 @@ class Convolution2D(Layer):\n class MaxPooling1D(Layer):\n     input_ndim = 3\n \n-    def __init__(self, pool_length=2, stride=None, ignore_border=True, **kwargs):\n+    def __init__(self, pool_length=2, stride=None,\n+                 border_mode='valid', **kwargs):\n         super(MaxPooling1D, self).__init__(**kwargs)\n         if stride is None:\n             stride = pool_length\n@@ -296,28 +221,32 @@ class MaxPooling1D(Layer):\n         self.stride = stride\n         self.st = (self.stride, 1)\n \n-        self.input = T.tensor3()\n+        self.input = K.placeholder(ndim=3)\n         self.pool_size = (pool_length, 1)\n-        self.ignore_border = ignore_border\n+        self.border_mode = border_mode\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        length = pool_output_length(input_shape[1], self.pool_length, self.ignore_border, self.stride)\n+        length = conv_output_length(input_shape[1], self.pool_length,\n+                                    self.border_mode, self.stride)\n         return (input_shape[0], length, input_shape[2])\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n-        output = downsample.max_pool_2d(X, ds=self.pool_size, st=self.st, ignore_border=self.ignore_border)\n-        output = output.dimshuffle(0, 2, 1, 3)\n-        return T.reshape(output, (output.shape[0], output.shape[1], output.shape[2]))\n+        X = K.expand_dims(X, -1)   # add dummy last dimension\n+        X = K.permute_dimensions(X, (0, 2, 1, 3))\n+        output = K.maxpool2d(X, pool_size=self.pool_size, strides=self.st,\n+                             border_mode=self.border_mode,\n+                             dim_ordering='th')\n+        output = K.permute_dimensions(output, (0, 2, 1, 3))\n+        return K.squeeze(output, 3)  # remove dummy last dimension\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n                   \"stride\": self.stride,\n                   \"pool_length\": self.pool_length,\n-                  \"ignore_border\": self.ignore_border}\n+                  \"border_mode\": self.border_mode}\n         base_config = super(MaxPooling1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -325,32 +254,40 @@ class MaxPooling1D(Layer):\n class MaxPooling2D(Layer):\n     input_ndim = 4\n \n-    def __init__(self, pool_size=(2, 2), stride=None, ignore_border=True, **kwargs):\n+    def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n+                 dim_ordering='th', **kwargs):\n         super(MaxPooling2D, self).__init__(**kwargs)\n-        self.input = T.tensor4()\n+        self.input = K.placeholder(ndim=4)\n         self.pool_size = tuple(pool_size)\n-        if stride is None:\n-            stride = self.pool_size\n-        self.stride = tuple(stride)\n-        self.ignore_border = ignore_border\n+        if strides is None:\n+            strides = self.pool_size\n+        self.strides = tuple(strides)\n+        self.border_mode = border_mode\n+        self.dim_ordering = dim_ordering\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        rows = pool_output_length(input_shape[2], self.pool_size[0], self.ignore_border, self.stride[0])\n-        cols = pool_output_length(input_shape[3], self.pool_size[1], self.ignore_border, self.stride[1])\n+        rows = conv_output_length(input_shape[2], self.pool_size[0],\n+                                  self.border_mode, self.strides[0])\n+        cols = conv_output_length(input_shape[3], self.pool_size[1],\n+                                  self.border_mode, self.strides[1])\n         return (input_shape[0], input_shape[1], rows, cols)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = downsample.max_pool_2d(X, ds=self.pool_size, st=self.stride, ignore_border=self.ignore_border)\n+        output = K.maxpool2d(X, pool_size=self.pool_size,\n+                             strides=self.strides,\n+                             border_mode=self.border_mode,\n+                             dim_ordering=self.dim_ordering)\n         return output\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n                   \"pool_size\": self.pool_size,\n-                  \"ignore_border\": self.ignore_border,\n-                  \"stride\": self.stride}\n+                  \"border_mode\": self.border_mode,\n+                  \"strides\": self.strides,\n+                  \"dim_ordering\": self.dim_ordering}\n         base_config = super(MaxPooling2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -361,7 +298,7 @@ class UpSample1D(Layer):\n     def __init__(self, length=2, **kwargs):\n         super(UpSample1D, self).__init__(**kwargs)\n         self.length = length\n-        self.input = T.tensor3()\n+        self.input = K.placeholder(ndim=3)\n \n     @property\n     def output_shape(self):\n@@ -370,7 +307,7 @@ class UpSample1D(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = T.extra_ops.repeat(X, self.length, axis=1)\n+        output = K.concatenate([X] * self.length, axis=1)\n         return output\n \n     def get_config(self):\n@@ -385,18 +322,20 @@ class UpSample2D(Layer):\n \n     def __init__(self, size=(2, 2), **kwargs):\n         super(UpSample2D, self).__init__(**kwargs)\n-        self.input = T.tensor4()\n+        self.input = K.placeholder(ndim=4)\n         self.size = tuple(size)\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1], self.size[0] * input_shape[2], self.size[1] * input_shape[3])\n+        return (input_shape[0], input_shape[1],\n+                self.size[0] * input_shape[2],\n+                self.size[1] * input_shape[3])\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        Y = T.extra_ops.repeat(X, self.size[0], axis=2)\n-        output = T.extra_ops.repeat(Y, self.size[1], axis=3)\n+        output = K.concatenate([X] * self.size[0], axis=2)\n+        output = K.concatenate([output] * self.size[1], axis=3)\n         return output\n \n     def get_config(self):\n@@ -428,22 +367,18 @@ class ZeroPadding1D(Layer):\n     def __init__(self, padding=1, **kwargs):\n         super(ZeroPadding1D, self).__init__(**kwargs)\n         self.padding = padding\n-        self.input = T.tensor3()\n+        self.input = K.placeholder(ndim=3)\n \n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1] + self.padding * 2, input_shape[2])\n+        return (input_shape[0],\n+                input_shape[1] + self.padding * 2,\n+                input_shape[2])\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        input_shape = X.shape\n-        output_shape = (input_shape[0],\n-                        input_shape[1] + 2 * self.padding,\n-                        input_shape[2])\n-        output = T.zeros(output_shape)\n-        # TODO: use concatenation instead\n-        return T.set_subtensor(output[:, self.padding:X.shape[1] + self.padding, :], X)\n+        return K.temporal_padding(X, padding=self.padding)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -453,15 +388,17 @@ class ZeroPadding1D(Layer):\n \n \n class ZeroPadding2D(Layer):\n-    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+    \"\"\"Zero-padding layer for 2D input (e.g. picture).\n \n     Input shape\n     -----------\n-    4D tensor with shape (samples, depth, first_axis_to_pad, second_axis_to_pad)\n+    4D tensor with shape:\n+        (samples, depth, first_axis_to_pad, second_axis_to_pad)\n \n     Output shape\n     ------------\n-    4D tensor with shape (samples, depth, first_padded_axis, second_padded_axis)\n+    4D tensor with shape:\n+        (samples, depth, first_padded_axis, second_padded_axis)\n \n     Arguments\n     ---------\n@@ -474,7 +411,7 @@ class ZeroPadding2D(Layer):\n     def __init__(self, padding=(1, 1), **kwargs):\n         super(ZeroPadding2D, self).__init__(**kwargs)\n         self.padding = tuple(padding)\n-        self.input = T.tensor4()\n+        self.input = K.placeholder(ndim=4)\n \n     @property\n     def output_shape(self):\n@@ -486,18 +423,7 @@ class ZeroPadding2D(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        input_shape = X.shape\n-        output_shape = (input_shape[0],\n-                        input_shape[1],\n-                        input_shape[2] + 2 * self.padding[0],\n-                        input_shape[3] + 2 * self.padding[1])\n-        output = T.zeros(output_shape)\n-        indices = (slice(None),\n-                   slice(None),\n-                   slice(self.padding[0], input_shape[2] + self.padding[0]),\n-                   slice(self.padding[1], input_shape[3] + self.padding[1]))\n-        # TODO: use concatenation instead\n-        return T.set_subtensor(output[indices], X)\n+        return K.spatial_2d_padding(X, padding=self.padding)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n\n@@ -82,7 +82,7 @@ class Layer(object):\n                 raise Exception('Invalid input shape - Layer expects input ndim=' +\n                                 str(self.input_ndim) + ', was provided with input shape ' + str(input_shape))\n         self._input_shape = input_shape\n-        self.input = K.placeholder(ndim=len(self._input_shape))\n+        self.input = K.placeholder(shape=self._input_shape)\n         self.build()\n \n     @property\n@@ -531,8 +531,7 @@ class Reshape(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        new_shape = (X.shape[0],) + self.dims\n-        return K.reshape(X, new_shape)\n+        return K.reshape(X, (-1,) + self.dims)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -640,12 +639,12 @@ class Dense(Layer):\n         self.input_dim = input_dim\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_dim,)\n+        self.input = K.placeholder(ndim=2)\n         super(Dense, self).__init__(**kwargs)\n \n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.input = K.placeholder(ndim=2)\n         self.W = self.init((input_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n \n@@ -748,12 +747,12 @@ class TimeDistributedDense(MaskedLayer):\n         self.input_length = input_length\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_length, self.input_dim)\n+        self.input = K.placeholder(ndim=3)\n         super(TimeDistributedDense, self).__init__(**kwargs)\n \n     def build(self):\n         input_dim = self.input_shape[2]\n \n-        self.input = K.placeholder(ndim=3)\n         self.W = self.init((input_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim))\n \n@@ -912,12 +911,12 @@ class MaxoutDense(Layer):\n         self.input_dim = input_dim\n         if self.input_dim:\n             kwargs['input_shape'] = (self.input_dim,)\n+        self.input = K.placeholder(ndim=2)\n         super(MaxoutDense, self).__init__(**kwargs)\n \n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.input = K.placeholder(ndim=2)\n         self.W = self.init((self.nb_feature, input_dim, self.output_dim))\n         self.b = K.zeros((self.nb_feature, self.output_dim))\n \n\n@@ -37,7 +37,8 @@ class Embedding(Layer):\n         super(Embedding, self).__init__(**kwargs)\n \n     def build(self):\n-        self.input = K.placeholder(ndim=2, dtype='int32')\n+        self.input = K.placeholder(shape=(None, self.input_length),\n+                                   dtype='int32')\n         self.W = self.init((self.input_dim, self.output_dim))\n         self.params = [self.W]\n         self.regularizers = []\n@@ -65,7 +66,7 @@ class Embedding(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        out = K.embedding(self.W, X)\n+        out = K.gather(self.W, X)\n         return out\n \n     def get_config(self):\n\n@@ -29,7 +29,6 @@ class BatchNormalization(Layer):\n     def build(self):\n         input_shape = self.input_shape  # starts with samples axis\n         input_shape = input_shape[1:]\n-        self.input = K.placeholder(ndim=len(input_shape) + 1)\n \n         self.gamma = self.init((input_shape))\n         self.beta = K.zeros(input_shape)\n@@ -41,7 +40,8 @@ class BatchNormalization(Layer):\n         # initialize self.updates: batch mean/std computation\n         X = self.get_input(train=True)\n         m = K.mean(X, axis=0)\n-        std = K.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n+        std = K.mean(K.square(X - m) + self.epsilon, axis=0)\n+        std = K.sqrt(std)\n         mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n         std_update = self.momentum * self.running_std + (1-self.momentum) * std\n         self.updates = [(self.running_mean, mean_update),\n\n@@ -24,6 +24,7 @@ class Recurrent(MaskedLayer):\n         if mask is None:\n             mask = T.ones_like(X.sum(axis=-1))  # is there a better way to do this without a sum?\n \n+        # TODO: reimplement\n         # mask is (nb_samples, time)\n         mask = T.shape_padright(mask)  # (nb_samples, time, 1)\n         mask = T.addbroadcast(mask, -1)  # the new dimension (the '1') is made broadcastable\n\n@@ -1,12 +1,14 @@\n from __future__ import absolute_import\n from __future__ import print_function\n-import theano\n-import theano.tensor as T\n import numpy as np\n-import warnings, time, copy, pprint\n+import warnings\n+import time\n+import copy\n+import pprint\n from six.moves import range\n import six\n \n+from . import backend as K\n from . import optimizers\n from . import objectives\n from . import regularizers\n@@ -69,46 +71,52 @@ def slice_X(X, start=None, stop=None):\n \n def weighted_objective(fn):\n     def weighted(y_true, y_pred, weights, mask=None):\n-        # it's important that 0 * Inf == 0, not NaN, so we need to filter\n-        # those out first\n-        filtered_y_true = y_true[weights.nonzero()[:-1]]\n-        filtered_y_pred = y_pred[weights.nonzero()[:-1]]\n-        filtered_weights = weights[weights.nonzero()]\n-        obj_output = fn(filtered_y_true, filtered_y_pred)\n-        weighted = filtered_weights * obj_output\n-        if mask is None:\n-            # Instead of calling mean() here, we divide by the sum of filtered_weights.\n-            return weighted.sum() / filtered_weights.sum()\n-        else:\n-            filtered_mask = mask[weights.nonzero()[:-1]]\n-            return weighted.sum() / (filtered_mask * filtered_weights).sum()\n+        '''To be called only with non-zero weights.\n+\n+        mask: binary\n+        '''\n+        score_array = fn(y_true, y_pred)\n+        if mask is not None:\n+            score_array *= mask\n+            #  the loss per batch should be proportional\n+            #  to the number of unmasked sampled.\n+            score_array /= K.mean(mask)\n+\n+        # reduce score_array to 1D\n+        ndim = K.ndim(score_array)\n+        for d in range(ndim-1):\n+            score_array = K.mean(score_array, axis=-1)\n+\n+        if weights is not None:\n+            score_array *= weights\n+        return K.mean(score_array)\n     return weighted\n \n \n def standardize_weights(y, sample_weight=None, class_weight=None):\n     if sample_weight is not None:\n-        return standardize_y(sample_weight)\n+        assert len(sample_weight) == len(y)\n+        return sample_weight.flatten()\n     elif isinstance(class_weight, dict):\n-        if len(y.shape) > 3:\n-            raise Exception('class_weight not supported for 4+ dimensional targets.')\n-        yshape = y.shape\n-        y = np.reshape(y, (-1, yshape[-1]))  # for time-distributed data, collapse time and sample\n+        if len(y.shape) > 2:\n+            raise Exception('class_weight not supported for 3+ dimensional targets.')\n         if y.shape[1] > 1:\n             y_classes = y.argmax(axis=1)\n         elif y.shape[1] == 1:\n             y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n-        class_weights = np.asarray([class_weight[cls] for cls in y_classes])\n-        return np.reshape(class_weights, yshape[:-1] + (1,))  # uncollapse initial dimensions\n+        weights = np.asarray([class_weight[cls] for cls in y_classes])\n+        return weights\n     else:\n-        return np.ones(y.shape[:-1] + (1,))\n+        return np.ones((y.shape[0],))\n \n \n def model_from_yaml(yaml_string, custom_objects={}):\n     '''\n         Returns a model generated from a local yaml file,\n-        which is either created by hand or from to_yaml method of Sequential or Graph\n+        which is either created by hand or from to_yaml method\n+        of Sequential or Graph\n     '''\n     import yaml\n     config = yaml.load(yaml_string)\n@@ -144,10 +152,11 @@ def model_from_config(config, custom_objects={}):\n         optimizer = optimizers.get(optimizer_name, optimizer_params)\n \n         if model_name == 'Sequential':\n-            model.compile(loss=loss, optimizer=optimizer, class_mode=class_mode, theano_mode=theano_mode)\n+            model.compile(loss=loss, optimizer=optimizer,\n+                          class_mode=class_mode, theano_mode=theano_mode)\n         elif model_name == 'Graph':\n-            model.compile(loss=loss, optimizer=optimizer, theano_mode=theano_mode)\n-\n+            model.compile(loss=loss, optimizer=optimizer,\n+                          theano_mode=theano_mode)\n     return model\n \n \n@@ -159,10 +168,12 @@ def get_function_name(o):\n \n \n class Model(object):\n-    def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n+    def _fit(self, f, ins, out_labels=[], batch_size=128,\n+             nb_epoch=100, verbose=1, callbacks=[],\n              val_f=None, val_ins=None, shuffle=True, metrics=[]):\n         '''\n-            Abstract fit function for f(*ins). Assume that f returns a list, labelled by out_labels.\n+            Abstract fit function for f(ins).\n+            Assume that f returns a list, labelled by out_labels.\n         '''\n         do_validation = False\n         if val_f and val_ins:\n@@ -204,7 +215,7 @@ class Model(object):\n                 batch_ids = index_array[batch_start:batch_end]\n                 try:\n                     ins_batch = slice_X(ins, batch_ids)\n-                except TypeError as err:\n+                except TypeError:\n                     raise Exception('TypeError while preparing batch. \\\n                         If using HDF5 input data, pass shuffle=\"batch\".\\n')\n \n@@ -212,7 +223,7 @@ class Model(object):\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n-                outs = f(*ins_batch)\n+                outs = f(ins_batch)\n                 if type(outs) != list:\n                     outs = [outs]\n                 for l, o in zip(out_labels, outs):\n@@ -225,7 +236,9 @@ class Model(object):\n                     # validation\n                     if do_validation:\n                         # replace with self._evaluate\n-                        val_outs = self._test_loop(val_f, val_ins, batch_size=batch_size, verbose=0)\n+                        val_outs = self._test_loop(val_f, val_ins,\n+                                                   batch_size=batch_size,\n+                                                   verbose=0)\n                         if type(val_outs) != list:\n                             val_outs = [val_outs]\n                         # same labels assumed\n@@ -253,7 +266,7 @@ class Model(object):\n             batch_ids = index_array[batch_start:batch_end]\n             ins_batch = slice_X(ins, batch_ids)\n \n-            batch_outs = f(*ins_batch)\n+            batch_outs = f(ins_batch)\n             if type(batch_outs) != list:\n                 batch_outs = [batch_outs]\n             if batch_index == 0:\n@@ -281,7 +294,7 @@ class Model(object):\n             batch_ids = index_array[batch_start:batch_end]\n             ins_batch = slice_X(ins, batch_ids)\n \n-            batch_outs = f(*ins_batch)\n+            batch_outs = f(ins_batch)\n             if type(batch_outs) == list:\n                 if batch_index == 0:\n                     for batch_out in enumerate(batch_outs):\n@@ -345,7 +358,8 @@ class Sequential(Model, containers.Sequential):\n             - set_weights\n     '''\n \n-    def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n+    def compile(self, optimizer, loss,\n+                class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n \n         self.loss = objectives.get(loss)\n@@ -359,9 +373,9 @@ class Sequential(Model, containers.Sequential):\n         self.y_test = self.get_output(train=False)\n \n         # target of model\n-        self.y = T.zeros_like(self.y_train)\n-\n-        self.weights = T.ones_like(self.y_train)\n+        self.y = K.placeholder(ndim=K.ndim(self.y_train))\n+        # weights: one scalar per sample\n+        self.weights = K.placeholder(ndim=1)\n \n         if hasattr(self.layers[-1], \"get_output_mask\"):\n             mask = self.layers[-1].get_output_mask()\n@@ -370,17 +384,15 @@ class Sequential(Model, containers.Sequential):\n         train_loss = weighted_loss(self.y, self.y_train, self.weights, mask)\n         test_loss = weighted_loss(self.y, self.y_test, self.weights, mask)\n \n-        train_loss.name = 'train_loss'\n-        test_loss.name = 'test_loss'\n-        self.y.name = 'y'\n-\n         if class_mode == \"categorical\":\n-            train_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_train, axis=-1)))\n-            test_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_test, axis=-1)))\n+            train_accuracy = K.mean(K.equal(K.argmax(self.y, axis=-1),\n+                                            K.argmax(self.y_train, axis=-1)))\n+            test_accuracy = K.mean(K.equal(K.argmax(self.y, axis=-1),\n+                                           K.argmax(self.y_test, axis=-1)))\n \n         elif class_mode == \"binary\":\n-            train_accuracy = T.mean(T.eq(self.y, T.round(self.y_train)))\n-            test_accuracy = T.mean(T.eq(self.y, T.round(self.y_test)))\n+            train_accuracy = K.mean(K.equal(self.y, K.round(self.y_train)))\n+            test_accuracy = K.mean(K.equal(self.y, K.round(self.y_test)))\n         else:\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n@@ -388,39 +400,38 @@ class Sequential(Model, containers.Sequential):\n \n         for r in self.regularizers:\n             train_loss = r(train_loss)\n-        updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n+        updates = self.optimizer.get_updates(self.params,\n+                                             self.constraints,\n+                                             train_loss)\n         updates += self.updates\n \n         if type(self.X_train) == list:\n             train_ins = self.X_train + [self.y, self.weights]\n             test_ins = self.X_test + [self.y, self.weights]\n+            assert type(self.X_test) == list\n             predict_ins = self.X_test\n         else:\n             train_ins = [self.X_train, self.y, self.weights]\n             test_ins = [self.X_test, self.y, self.weights]\n             predict_ins = [self.X_test]\n \n-        self._train = theano.function(train_ins, train_loss, updates=updates,\n-                                      allow_input_downcast=True, mode=theano_mode)\n-        self._train_with_acc = theano.function(train_ins, [train_loss, train_accuracy], updates=updates,\n-                                               allow_input_downcast=True, mode=theano_mode)\n-        self._predict = theano.function(predict_ins, self.y_test,\n-                                        allow_input_downcast=True, mode=theano_mode)\n-        self._test = theano.function(test_ins, test_loss,\n-                                     allow_input_downcast=True, mode=theano_mode)\n-        self._test_with_acc = theano.function(test_ins, [test_loss, test_accuracy],\n-                                              allow_input_downcast=True, mode=theano_mode)\n+        self._train = K.function(train_ins, [train_loss], updates=updates)\n+        self._train_with_acc = K.function(train_ins, [train_loss, train_accuracy], updates=updates)\n+        self._predict = K.function(predict_ins, [self.y_test])\n+        self._test = K.function(test_ins, [test_loss])\n+        self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy])\n \n-    def train_on_batch(self, X, y, accuracy=False, class_weight=None, sample_weight=None):\n+    def train_on_batch(self, X, y, accuracy=False,\n+                       class_weight=None, sample_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n-        sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n-\n+        sample_weight = standardize_weights(y, class_weight=class_weight,\n+                                            sample_weight=sample_weight)\n         ins = X + [y, sample_weight]\n         if accuracy:\n-            return self._train_with_acc(*ins)\n+            return self._train_with_acc(ins)\n         else:\n-            return self._train(*ins)\n+            return self._train(ins)\n \n     def test_on_batch(self, X, y, accuracy=False, sample_weight=None):\n         X = standardize_X(X)\n@@ -429,17 +440,17 @@ class Sequential(Model, containers.Sequential):\n \n         ins = X + [y, sample_weight]\n         if accuracy:\n-            return self._test_with_acc(*ins)\n+            return self._test_with_acc(ins)\n         else:\n-            return self._test(*ins)\n+            return self._test(ins)\n \n     def predict_on_batch(self, X):\n         ins = standardize_X(X)\n-        return self._predict(*ins)\n+        return self._predict(ins)\n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, show_accuracy=False,\n-            class_weight=None, sample_weight=None):\n+            validation_split=0., validation_data=None, shuffle=True,\n+            show_accuracy=False, class_weight=None, sample_weight=None):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n@@ -456,12 +467,13 @@ class Sequential(Model, containers.Sequential):\n                 X_val, y_val = validation_data\n                 X_val = standardize_X(X_val)\n                 y_val = standardize_y(y_val)\n-                sample_weight_val = np.ones(y_val.shape[:-1] + (1,))\n+                sample_weight_val = standardize_weights(y_val)\n             elif len(validation_data) == 3:\n                 X_val, y_val, sample_weight_val = validation_data\n                 X_val = standardize_X(X_val)\n                 y_val = standardize_y(y_val)\n-                sample_weight_val = standardize_weights(y_val, sample_weight=sample_weight_val)\n+                sample_weight_val = standardize_weights(y_val,\n+                                                        sample_weight=sample_weight_val)\n             else:\n                 raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val) or (X_val, y_val, sample_weight). \\\n                     X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n@@ -475,7 +487,7 @@ class Sequential(Model, containers.Sequential):\n                 sample_weight, sample_weight_val = (slice_X(sample_weight, 0, split_at), slice_X(sample_weight, split_at))\n                 sample_weight_val = standardize_weights(y_val, sample_weight=sample_weight_val)\n             else:\n-                sample_weight_val = np.ones(y_val.shape[:-1] + (1,))\n+                sample_weight_val = standardize_weights(y_val)\n             val_ins = X_val + [y_val, sample_weight_val]\n \n         if show_accuracy:\n@@ -488,7 +500,8 @@ class Sequential(Model, containers.Sequential):\n         sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n         ins = X + [y, sample_weight]\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n-        return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n+        return self._fit(f, ins, out_labels=out_labels,\n+                         batch_size=batch_size, nb_epoch=nb_epoch,\n                          verbose=verbose, callbacks=callbacks,\n                          val_f=val_f, val_ins=val_ins,\n                          shuffle=shuffle, metrics=metrics)\n@@ -510,7 +523,8 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return (proba > 0.5).astype('int32')\n \n-    def evaluate(self, X, y, batch_size=128, show_accuracy=False, verbose=1, sample_weight=None):\n+    def evaluate(self, X, y, batch_size=128, show_accuracy=False,\n+                 verbose=1, sample_weight=None):\n         X = standardize_X(X)\n         y = standardize_y(y)\n         sample_weight = standardize_weights(y, sample_weight=sample_weight)\n@@ -585,7 +599,7 @@ class Graph(Model, containers.Graph):\n             output = self.outputs[output_name]\n             y_train = output.get_output(True)\n             y_test = output.get_output(False)\n-            y = T.zeros_like(y_test)\n+            y = K.placeholder(ndim=K.ndim(y_train))\n             ys.append(y)\n             ys_train.append(y_train)\n             ys_test.append(y_test)\n@@ -595,15 +609,12 @@ class Graph(Model, containers.Graph):\n             else:\n                 mask = None\n \n-            weight = T.ones_like(y_test)\n+            weight = K.placeholder(ndim=1)\n             weights.append(weight)\n             weighted_loss = weighted_objective(objectives.get(loss_fn))\n             train_loss += weighted_loss(y, y_train, weight, mask)\n             test_loss += weighted_loss(y, y_test, weight, mask)\n \n-        train_loss.name = 'train_loss'\n-        test_loss.name = 'test_loss'\n-\n         ins = [self.inputs[name].input for name in self.input_order]\n         train_ins = ins + ys + weights\n         test_ins = ins + ys + weights\n@@ -616,12 +627,9 @@ class Graph(Model, containers.Graph):\n         self.theano_mode = theano_mode\n         self.loss = loss\n \n-        self._train = theano.function(train_ins, train_loss, updates=updates,\n-                                      allow_input_downcast=True, mode=theano_mode)\n-        self._test = theano.function(test_ins, test_loss,\n-                                     allow_input_downcast=True, mode=theano_mode)\n-        self._predict = theano.function(inputs=ins, outputs=ys_test,\n-                                        allow_input_downcast=True, mode=theano_mode)\n+        self._train = K.function(train_ins, [train_loss], updates=updates)\n+        self._test = K.function(test_ins, [test_loss])\n+        self._predict = K.function(inputs=ins, outputs=ys_test)\n \n     def train_on_batch(self, data, class_weight={}, sample_weight={}):\n         # data is a dictionary mapping output and input names to arrays\n@@ -629,22 +637,23 @@ class Graph(Model, containers.Graph):\n                                              sample_weight=sample_weight.get(name),\n                                              class_weight=class_weight.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n-        return self._train(*ins)\n+        return self._train(ins)\n \n     def test_on_batch(self, data, sample_weight={}):\n         # data is a dictionary mapping input names to arrays\n         sample_weight = [standardize_weights(data[name],\n                                              sample_weight=sample_weight.get(name)) for name in self.output_order]\n         ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n-        return self._test(*ins)\n+        return self._test(ins)\n \n     def predict_on_batch(self, data):\n         # data is a dictionary mapping input names to arrays\n         ins = [data[name] for name in self.input_order]\n-        return self._predict(*ins)\n+        return self._predict(ins)\n \n     def fit(self, data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n-            validation_split=0., validation_data=None, shuffle=True, class_weight={}, sample_weight={}):\n+            validation_split=0., validation_data=None, shuffle=True,\n+            class_weight={}, sample_weight={}):\n         X = [data[name] for name in self.input_order]\n         y = [standardize_y(data[name]) for name in self.output_order]\n \n\n@@ -8,7 +8,7 @@ def mean_squared_error(y_true, y_pred):\n \n \n def root_mean_squared_error(y_true, y_pred):\n-    return K.mean(K.square(K.square(y_pred - y_true), axis=-1))\n+    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n \n \n def mean_absolute_error(y_true, y_pred):\n\n@@ -34,7 +34,7 @@ class Optimizer(object):\n     def get_gradients(self, loss, params):\n         grads = K.gradients(loss, params)\n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n-            norm = K.sqrt(sum([K.sum(g ** 2) for g in grads]))\n+            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n             grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n         if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n             grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n@@ -95,7 +95,7 @@ class RMSprop(Optimizer):\n \n         for p, g, a, c in zip(params, grads, accumulators, constraints):\n             # update accumulator\n-            new_a = self.rho * a + (1 - self.rho) * g ** 2\n+            new_a = self.rho * a + (1 - self.rho) * K.square(g)\n             self.updates.append((a, new_a))\n \n             new_p = p - self.lr * g / K.sqrt(new_a + self.epsilon)\n@@ -121,7 +121,7 @@ class Adagrad(Optimizer):\n         self.updates = []\n \n         for p, g, a, c in zip(params, grads, accumulators, constraints):\n-            new_a = a + g ** 2  # update accumulator\n+            new_a = a + K.square(g)  # update accumulator\n             self.updates.append((a, new_a))\n             new_p = p - self.lr * g / K.sqrt(new_a + self.epsilon)\n             self.updates.append((p, c(new_p)))  # apply constraints\n@@ -151,7 +151,7 @@ class Adadelta(Optimizer):\n         for p, g, a, d_a, c in zip(params, grads, accumulators,\n                                    delta_accumulators, constraints):\n             # update accumulator\n-            new_a = self.rho * a + (1 - self.rho) * g ** 2\n+            new_a = self.rho * a + (1 - self.rho) * K.square(g)\n             self.updates.append((a, new_a))\n \n             # use the new accumulator and the *old* delta_accumulator\n@@ -161,7 +161,7 @@ class Adadelta(Optimizer):\n             self.updates.append((p, c(new_p)))  # apply constraints\n \n             # update delta_accumulator\n-            new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n+            new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)\n             self.updates.append((d_a, new_d_a))\n         return self.updates\n \n@@ -184,13 +184,15 @@ class Adam(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = K.variable(0)\n         self.lr = K.variable(lr)\n+        self.beta_1 = K.variable(beta_1)\n+        self.beta_2 = K.variable(beta_2)\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n         self.updates = [(self.iterations, self.iterations+1.)]\n \n         t = self.iterations + 1\n-        lr_t = self.lr * K.sqrt(1 - self.beta_2 ** t) / (1 - self.beta_1 ** t)\n+        lr_t = self.lr * K.sqrt(1 - K.pow(self.beta_2, t)) / (1 - K.pow(self.beta_1, t))\n \n         for p, g, c in zip(params, grads, constraints):\n             # zero init of moment\n@@ -199,7 +201,7 @@ class Adam(Optimizer):\n             v = K.variable(np.zeros(K.get_value(p).shape))\n \n             m_t = (self.beta_1 * m) + (1 - self.beta_1) * g\n-            v_t = (self.beta_2 * v) + (1 - self.beta_2) * (g ** 2)\n+            v_t = (self.beta_2 * v) + (1 - self.beta_2) * K.square(g)\n             p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n \n             self.updates.append((m, m_t))\n@@ -210,8 +212,8 @@ class Adam(Optimizer):\n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n                 \"lr\": float(K.get_value(self.lr)),\n-                \"beta_1\": self.beta_1,\n-                \"beta_2\": self.beta_2,\n+                \"beta_1\": float(K.get_value(self.beta_1)),\n+                \"beta_2\": float(K.get_value(self.beta_2)),\n                 \"epsilon\": self.epsilon}\n \n # aliases\n\n@@ -26,7 +26,7 @@ class WeightRegularizer(Regularizer):\n \n     def __call__(self, loss):\n         loss += K.sum(K.abs(self.p)) * self.l1\n-        loss += K.sum(self.p ** 2) * self.l2\n+        loss += K.sum(K.square(self.p)) * self.l2\n         return loss\n \n     def get_config(self):\n@@ -46,7 +46,7 @@ class ActivityRegularizer(Regularizer):\n     def __call__(self, loss):\n         output = self.layer.get_output(True)\n         loss += self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n-        loss += self.l2 * K.sum(K.mean(output ** 2, axis=0))\n+        loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n         return loss\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
