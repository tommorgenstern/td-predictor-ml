{"custom_id": "keras#6bc2571241a600681f8a157c20e29858c2e1546a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 4403 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1200,7 +1200,7 @@ def mean_squared_error(y_true, y_pred):\n     Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+  \n   return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n \n \n@@ -1328,7 +1328,7 @@ def mean_absolute_error(y_true, y_pred):\n     Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+  \n   return backend.mean(tf.abs(y_pred - y_true), axis=-1)\n \n \n@@ -1413,7 +1413,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   first_log = tf.math.log(backend.maximum(y_pred, backend.epsilon()) + 1.)\n   second_log = tf.math.log(backend.maximum(y_true, backend.epsilon()) + 1.)\n   return backend.mean(\n@@ -1470,7 +1470,7 @@ def squared_hinge(y_true, y_pred):\n      Squared hinge loss values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   y_true = _maybe_convert_labels(y_true)\n   return backend.mean(\n       tf.square(tf.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n@@ -1503,7 +1503,7 @@ def hinge(y_true, y_pred):\n     Hinge loss values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   y_true = _maybe_convert_labels(y_true)\n   return backend.mean(tf.maximum(1. - y_true * y_pred, 0.), axis=-1)\n \n@@ -1536,7 +1536,7 @@ def categorical_hinge(y_true, y_pred):\n     Categorical hinge loss values.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   pos = tf.reduce_sum(y_true * y_pred, axis=-1)\n   neg = tf.reduce_max((1. - y_true) * y_pred, axis=-1)\n   zero = tf.cast(0., y_pred.dtype)\n@@ -1608,7 +1608,7 @@ def log_cosh(y_true, y_pred):\n     Logcosh error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n \n   def _logcosh(x):\n     return x + tf.math.softplus(-2. * x) - tf.cast(\n@@ -1651,7 +1651,7 @@ def categorical_crossentropy(y_true,\n     Categorical crossentropy loss value.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   label_smoothing = tf.convert_to_tensor(\n       label_smoothing, dtype=backend.floatx())\n \n@@ -1734,7 +1734,7 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n     Sparse categorical crossentropy loss value.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   return backend.sparse_categorical_crossentropy(\n       y_true, y_pred, from_logits=from_logits, axis=axis)\n \n@@ -1795,7 +1795,7 @@ def binary_crossentropy(y_true,\n     Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   label_smoothing = tf.convert_to_tensor(\n       label_smoothing, dtype=backend.floatx())\n \n@@ -1882,7 +1882,7 @@ def kl_divergence(y_true, y_pred):\n     TypeError: If `y_true` cannot be cast to the `y_pred.dtype`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   y_true = backend.clip(y_true, backend.epsilon(), 1)\n   y_pred = backend.clip(y_pred, backend.epsilon(), 1)\n   return tf.reduce_sum(y_true * tf.math.log(y_true / y_pred), axis=-1)\n@@ -1918,7 +1918,7 @@ def poisson(y_true, y_pred):\n     InvalidArgumentError: If `y_true` and `y_pred` have incompatible shapes.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.cast(y_true, y_pred.dtype)\n+\n   return backend.mean(\n       y_pred - y_true * tf.math.log(y_pred + backend.epsilon()), axis=-1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca3da0cbac39799fc183dec704765ed738978cb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 4423 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1200,7 +1200,7 @@ def mean_squared_error(y_true, y_pred):\n     Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  \n+  y_true = tf.cast(y_true, y_pred.dtype)\n   return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n \n \n@@ -1328,7 +1328,7 @@ def mean_absolute_error(y_true, y_pred):\n     Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-  \n+  y_true = tf.cast(y_true, y_pred.dtype)\n   return backend.mean(tf.abs(y_pred - y_true), axis=-1)\n \n \n@@ -1413,7 +1413,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   first_log = tf.math.log(backend.maximum(y_pred, backend.epsilon()) + 1.)\n   second_log = tf.math.log(backend.maximum(y_true, backend.epsilon()) + 1.)\n   return backend.mean(\n@@ -1470,7 +1470,7 @@ def squared_hinge(y_true, y_pred):\n      Squared hinge loss values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   y_true = _maybe_convert_labels(y_true)\n   return backend.mean(\n       tf.square(tf.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n@@ -1503,7 +1503,7 @@ def hinge(y_true, y_pred):\n     Hinge loss values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   y_true = _maybe_convert_labels(y_true)\n   return backend.mean(tf.maximum(1. - y_true * y_pred, 0.), axis=-1)\n \n@@ -1608,7 +1608,7 @@ def log_cosh(y_true, y_pred):\n     Logcosh error values. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n \n   def _logcosh(x):\n     return x + tf.math.softplus(-2. * x) - tf.cast(\n@@ -1651,7 +1651,7 @@ def categorical_crossentropy(y_true,\n     Categorical crossentropy loss value.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(\n       label_smoothing, dtype=backend.floatx())\n \n@@ -1795,7 +1795,7 @@ def binary_crossentropy(y_true,\n     Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(\n       label_smoothing, dtype=backend.floatx())\n \n@@ -1882,7 +1882,7 @@ def kl_divergence(y_true, y_pred):\n     TypeError: If `y_true` cannot be cast to the `y_pred.dtype`.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   y_true = backend.clip(y_true, backend.epsilon(), 1)\n   y_pred = backend.clip(y_pred, backend.epsilon(), 1)\n   return tf.reduce_sum(y_true * tf.math.log(y_true / y_pred), axis=-1)\n@@ -1918,7 +1918,7 @@ def poisson(y_true, y_pred):\n     InvalidArgumentError: If `y_true` and `y_pred` have incompatible shapes.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   return backend.mean(\n       y_pred - y_true * tf.math.log(y_pred + backend.epsilon()), axis=-1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f175038f932aff5af37539f6f67875021b5647db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4425 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1536,7 +1536,7 @@ def categorical_hinge(y_true, y_pred):\n     Categorical hinge loss values.\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n-\n+  y_true = tf.cast(y_true, y_pred.dtype)\n   pos = tf.reduce_sum(y_true * y_pred, axis=-1)\n   neg = tf.reduce_max((1. - y_true) * y_pred, axis=-1)\n   zero = tf.cast(0., y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb306b4cc446675271e5b15b4a7197efd3b60c34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 405 | Lines Deleted: 57 | Files Changed: 6 | Hunks: 24 | Methods Changed: 48 | Complexity Δ (Sum/Max): 63/34 | Churn Δ: 462 | Churn Cumulative: 25418 | Contributors (this commit): 113 | Commits (past 90d): 55 | Contributors (cumulative): 135 | DMM Complexity: 0.7853881278538812\n\nDIFF:\n@@ -13,6 +13,8 @@\n # limitations under the License.\n # ==============================================================================\n # pylint: disable=protected-access\n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-bad-import-order\n \"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -45,6 +47,7 @@ from keras.utils import layer_utils\n from keras.utils import object_identity\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n+from keras.utils import traceback_utils\n from keras.utils import version_utils\n # A module that only depends on `keras.layers` import these from here.\n from keras.utils.generic_utils import to_snake_case  # pylint: disable=unused-import\n@@ -734,9 +737,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # Check that either the only argument in the `__init__` is  `self`,\n     # or that `get_config` has been overridden:\n     if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):\n-      raise NotImplementedError('Layer %s has arguments in `__init__` and '\n-                                'therefore must override `get_config`.' %\n-                                self.__class__.__name__)\n+      raise NotImplementedError(f'Layer {self.__class__.__name__} '\n+                                'has arguments in `__init__)_` and '\n+                                'therefore must override `get_config()`.')\n     return config\n \n   @classmethod\n@@ -782,7 +785,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       # with the shape the Layer will be called on (these users will have to\n       # implement `compute_output_shape` themselves).\n       self._maybe_build(input_shape)\n-      with tf.__internal__.FuncGraph(str(self.name) + '_scratch_graph').as_default():\n+      graph_name = str(self.name) + '_scratch_graph'\n+      with tf.__internal__.FuncGraph(graph_name).as_default():\n         input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n         def _make_placeholder_like(shape):\n           ph = backend.placeholder(shape=shape, dtype=self.dtype)\n@@ -858,7 +862,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       return self._infer_output_signature(inputs, args, kwargs, input_masks)\n \n   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\n-    \"\"\"TODO(kaftan): Docstring.\"\"\"\n+    \"\"\"Call the layer on input KerasTensors and returns output KerasTensors.\"\"\"\n \n     call_fn = self.call\n     # Wrapping `call` function in autograph to allow for dynamic control\n@@ -869,7 +873,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # enclosing tf.function, if any.\n     if (base_layer_utils.is_subclassed(self) and\n         not base_layer_utils.from_saved_model(self)):\n-      call_fn = tf.__internal__.autograph.tf_convert(self.call, tf.__internal__.autograph.control_status_ctx())\n+      call_fn = tf.__internal__.autograph.tf_convert(\n+          self.call, tf.__internal__.autograph.control_status_ctx())\n+\n+    call_fn = traceback_utils.inject_argument_info_in_traceback(\n+        call_fn,\n+        object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n \n     # We enter a scratch graph and build placeholder inputs inside of it that\n     # match the input args.\n@@ -933,6 +942,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # carry over the input mask\n     return mask\n \n+  @traceback_utils.filter_traceback\n   def __call__(self, *args, **kwargs):\n     \"\"\"Wraps `call`, applying pre- and post-processing steps.\n \n@@ -1034,6 +1044,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       else:\n         name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n         call_fn = self._autographed_call()\n+      call_fn = traceback_utils.inject_argument_info_in_traceback(\n+          call_fn,\n+          object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n \n       with tf.name_scope(name_scope):\n         if not self.built:\n@@ -1190,7 +1203,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # enclosing tf.function, if any.\n     if (base_layer_utils.is_subclassed(self) and\n         not base_layer_utils.from_saved_model(self)):\n-      return tf.__internal__.autograph.tf_convert(self.call, tf.__internal__.autograph.control_status_ctx())\n+      return tf.__internal__.autograph.tf_convert(\n+          self.call, tf.__internal__.autograph.control_status_ctx())\n     else:\n       return self.call\n \n\n@@ -13,13 +13,11 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for TensorFlow 2.0 layer behavior.\"\"\"\n-\n+# pylint: disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n import copy\n import os\n-import sys\n-import traceback\n \n import numpy as np\n from keras import backend\n@@ -1044,15 +1042,7 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n     try:\n       _ = TypeErrorLayer()(inputs)\n     except TypeError as e:\n-      if hasattr(e, 'ag_error_metadata'):\n-        self.assertIn('easily_identifiable_name', str(e))\n-        # See ErrorMetadataBase in autograph/pyct/errors.py\n-        function_name = e.ag_error_metadata.translated_stack[-1].function_name\n-      else:\n-        tb = traceback.extract_tb(sys.exc_info()[2])\n-        last_entry = tb[-1]\n-        function_name = last_entry[2]\n-      self.assertEqual(function_name, 'easily_identifiable_name')\n+      self.assertIn('easily_identifiable_name', str(e))  # pylint: disable=g-assert-in-except\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_summaries_in_tf_function(self):\n\n@@ -179,10 +179,10 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n       list_inputs = []\n       for name in names:\n         if name not in inputs:\n-          raise ValueError('Missing data for input \"%s\". '\n-                           'You passed a data dictionary with keys %s. '\n-                           'Expected the following keys: %s' %\n-                           (name, list(inputs.keys()), names))\n+          raise ValueError(f'Missing data for input \"{name}\". '\n+                           'You passed a data dictionary with keys '\n+                           f'{list(inputs.keys())}. '\n+                           f'Expected the following keys: {names}')\n         list_inputs.append(inputs[name])\n       inputs = list_inputs\n \n@@ -193,13 +193,12 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n     # guarding for is a Layer instance (Functional API), which does not\n     # have a `shape` attribute.\n     if not hasattr(x, 'shape'):\n-      raise TypeError('Inputs to a layer should be tensors. Got: %s' % (x,))\n+      raise TypeError(f'Inputs to a layer should be tensors. Got: {x}')\n \n   if len(inputs) != len(input_spec):\n-    raise ValueError('Layer ' + layer_name + ' expects ' +\n-                     str(len(input_spec)) + ' input(s), '\n-                     'but it received ' + str(len(inputs)) +\n-                     ' input tensors. Inputs received: ' + str(inputs))\n+    raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n+                     f' but it received {len(inputs)} input tensors. '\n+                     f'Inputs received: {inputs}')\n   for input_index, (x, spec) in enumerate(zip(inputs, input_spec)):\n     if spec is None:\n       continue\n@@ -211,34 +210,32 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n     if spec.ndim is not None and not spec.allow_last_axis_squeeze:\n       ndim = shape.rank\n       if ndim != spec.ndim:\n-        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n-                         layer_name + ' is incompatible with the layer: '\n-                         'expected ndim=' + str(spec.ndim) + ', found ndim=' +\n-                         str(ndim) + '. Full shape received: ' +\n-                         str(tuple(shape)))\n+        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n+                         'is incompatible with the layer: '\n+                         f'expected ndim={spec.ndim}, found ndim={ndim}. '\n+                         f'Full shape received: {tuple(shape)}')\n     if spec.max_ndim is not None:\n       ndim = x.shape.rank\n       if ndim is not None and ndim > spec.max_ndim:\n-        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n-                         layer_name + ' is incompatible with the layer: '\n-                         'expected max_ndim=' + str(spec.max_ndim) +\n-                         ', found ndim=' + str(ndim))\n+        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n+                         'is incompatible with the layer: '\n+                         f'expected max_ndim={spec.max_ndim}, '\n+                         f'found ndim={ndim}')\n     if spec.min_ndim is not None:\n       ndim = x.shape.rank\n       if ndim is not None and ndim < spec.min_ndim:\n-        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n-                         layer_name + ' is incompatible with the layer: '\n-                         ': expected min_ndim=' + str(spec.min_ndim) +\n-                         ', found ndim=' + str(ndim) +\n-                         '. Full shape received: ' +\n-                         str(tuple(shape)))\n+        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n+                         'is incompatible with the layer: '\n+                         f'expected min_ndim={spec.min_ndim}, '\n+                         f'found ndim={ndim}. '\n+                         f'Full shape received: {tuple(shape)}')\n     # Check dtype.\n     if spec.dtype is not None:\n       if x.dtype.name != spec.dtype:\n-        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n-                         layer_name + ' is incompatible with the layer: '\n-                         'expected dtype=' + str(spec.dtype) +\n-                         ', found dtype=' + str(x.dtype))\n+        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n+                         'is incompatible with the layer: '\n+                         f'expected dtype={spec.dtype}, '\n+                         f'found dtype={x.dtype}')\n \n     # Check specific shape axes.\n     shape_as_list = shape.as_list()\n@@ -248,10 +245,10 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n           value = value.value\n         if value is not None and shape_as_list[int(axis)] not in {value, None}:\n           raise ValueError(\n-              'Input ' + str(input_index) + ' of layer ' + layer_name + ' is'\n-              ' incompatible with the layer: expected axis ' + str(axis) +\n-              ' of input shape to have value ' + str(value) +\n-              ' but received input with shape ' + display_shape(x.shape))\n+              f'Input {input_index} of layer \"{layer_name}\" is '\n+              f'incompatible with the layer: expected axis {axis}'\n+              f'of input shape to have value {value}, '\n+              f'but received input with shape {display_shape(x.shape)}')\n     # Check shape.\n     if spec.shape is not None and shape.rank is not None:\n       spec_shape = spec.shape\n@@ -263,10 +260,10 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n       for spec_dim, dim in zip(spec_shape, shape_as_list):\n         if spec_dim is not None and dim is not None:\n           if spec_dim != dim:\n-            raise ValueError('Input ' + str(input_index) +\n-                             ' is incompatible with layer ' + layer_name +\n-                             ': expected shape=' + str(spec.shape) +\n-                             ', found shape=' + display_shape(x.shape))\n+            raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n+                             'incompatible with the layer: '\n+                             f'expected shape={spec.shape}, '\n+                             f'found shape={display_shape(x.shape)}')\n \n \n def display_shape(shape):\n\n@@ -2941,8 +2941,7 @@ def _multi_worker_concat(v, strategy):\n     shapes = tf.concat([\n         tf.expand_dims(tf.shape(single_value)[0], axis=0)\n         for single_value in v.values\n-    ],\n-                              axis=0)\n+    ], axis=0)\n     all_shapes = strategy.gather(shapes, axis=0)\n   else:\n     # v is a tensor. This may happen when, say, we have 2x1 multi-worker.\n\n@@ -0,0 +1,156 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Utilities related to Keras exception stack trace prettifying.\"\"\"\n+\n+import inspect\n+import os\n+import sys\n+import traceback\n+import types\n+import tensorflow.compat.v2 as tf\n+\n+\n+_EXCLUDED_PATHS = (\n+    os.path.abspath(os.path.join(__file__, '..', '..')),\n+    os.path.join('tensorflow', 'python'),\n+)\n+\n+\n+def include_frame(fname):\n+  for exclusion in _EXCLUDED_PATHS:\n+    if exclusion in fname:\n+      return False\n+  return True\n+\n+\n+def _process_traceback_frames(tb):\n+  \"\"\"Iterate through traceback frames and return a new, filtered traceback.\"\"\"\n+  last_tb = None\n+  tb_list = list(traceback.walk_tb(tb))\n+  for f, line_no in reversed(tb_list):\n+    if include_frame(f.f_code.co_filename):\n+      last_tb = types.TracebackType(last_tb, f, f.f_lasti, line_no)\n+  if last_tb is None and tb_list:\n+    # If no frames were kept during filtering, create a new traceback\n+    # from the outermost function.\n+    f, line_no = tb_list[-1]\n+    last_tb = types.TracebackType(last_tb, f, f.f_lasti, line_no)\n+  return last_tb\n+\n+\n+def filter_traceback(fn):\n+  \"\"\"Filter out Keras-internal stack trace frames in exceptions raised by fn.\"\"\"\n+  if sys.version_info.major != 3 or sys.version_info.minor < 7:\n+    return fn\n+\n+  def error_handler(*args, **kwargs):\n+    if not tf.debugging.is_traceback_filtering_enabled():\n+      return fn(*args, **kwargs)\n+\n+    filtered_tb = None\n+    try:\n+      return fn(*args, **kwargs)\n+    except Exception as e:  # pylint: disable=broad-except\n+      filtered_tb = _process_traceback_frames(e.__traceback__)\n+      raise e.with_traceback(filtered_tb) from None\n+    finally:\n+      del filtered_tb\n+\n+  return tf.__internal__.decorator.make_decorator(fn, error_handler)\n+\n+\n+def inject_argument_info_in_traceback(fn, object_name=None):\n+  \"\"\"Add information about call argument values to an error message.\n+\n+  Arguments:\n+    fn: Function to wrap. Exceptions raised by the this function will be\n+      re-raised with additional information added to the error message,\n+      displaying the values of the different arguments that the function\n+      was called with.\n+    object_name: String, display name of the class/function being called,\n+      e.g. `'layer \"layer_name\" (LayerClass)'`.\n+\n+  Returns:\n+    A wrapped version of `fn`.\n+  \"\"\"\n+  def error_handler(*args, **kwargs):\n+    signature = None\n+    bound_signature = None\n+    try:\n+      return fn(*args, **kwargs)\n+    except Exception as e:  # pylint: disable=broad-except\n+      if hasattr(e, '_keras_call_info_injected'):\n+        # Only inject info for the innermost failing call\n+        raise e\n+      signature = inspect.signature(fn)\n+      try:\n+        # The first argument is `self`, so filter it out\n+        bound_signature = signature.bind(*args, **kwargs)\n+      except TypeError:\n+        # Likely unbindable arguments\n+        raise e\n+\n+      # Add argument context\n+      arguments_context = []\n+      for arg in list(signature.parameters.values()):\n+        if arg.name in bound_signature.arguments:\n+          value = format_argument_value(bound_signature.arguments[arg.name])\n+        else:\n+          value = arg.default\n+        arguments_context.append(f'  • {arg.name}={value}')\n+\n+      if arguments_context:\n+        arguments_context = '\\n'.join(arguments_context)\n+        # Get original error message and append information to it.\n+        if isinstance(e, tf.errors.OpError):\n+          message = e.message\n+        elif e.args:\n+          # Canonically, the 1st argument in an exception is the error message.\n+          # This works for all built-in Python exceptions.\n+          message = e.args[0]\n+        else:\n+          message = ''\n+        message = (\n+            'Exception encountered when calling '\n+            f'{object_name if object_name else fn.__name__}.\\n\\n'\n+            f'{message}\\n\\n'\n+            f'Call arguments received:\\n{arguments_context}')\n+\n+        # Reraise exception, with added context\n+        if isinstance(e, tf.errors.OpError):\n+          new_e = e.__class__(e.node_def, e.op, message, e.error_code)\n+        else:\n+          try:\n+            # For standard exceptions such as ValueError, TypeError, etc.\n+            new_e = e.__class__(message)\n+          except TypeError:\n+            # For any custom error that doesn't have a standard signature.\n+            new_e = RuntimeError(message)\n+        new_e._keras_call_info_injected = True  # pylint: disable=protected-access\n+      else:\n+        new_e = e\n+      raise new_e.with_traceback(e.__traceback__) from None\n+    finally:\n+      del signature\n+      del bound_signature\n+  return tf.__internal__.decorator.make_decorator(fn, error_handler)\n+\n+\n+def format_argument_value(value):\n+  if isinstance(value, tf.Tensor):\n+    # Simplified representation for eager / graph tensors\n+    # to keep messages readable\n+    return f'tf.Tensor(shape={value.shape}, dtype={value.dtype.name})'\n+  return repr(value)\n\n@@ -0,0 +1,192 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for traceback_utils.\"\"\"\n+\n+from keras import layers\n+from keras.utils import traceback_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class TracebackUtilsTest(tf.test.TestCase):\n+\n+  def test_info_injection_basics(self):\n+    def error_fn(arg_1, arg_2, keyword_arg_1=None, keyword_arg_2=None):\n+      raise ValueError('Original message')\n+\n+    with self.assertRaises(ValueError) as e:\n+      traceback_utils.inject_argument_info_in_traceback(\n+          error_fn, 'ObjName')(1, 2, keyword_arg_1=3, keyword_arg_2=4)\n+    self.assertIn('Original message', str(e.exception))\n+    self.assertIn('Exception encountered when calling ObjName',\n+                  str(e.exception))\n+    self.assertIn('Call arguments received:', str(e.exception))\n+    self.assertIn('arg_1=1', str(e.exception))\n+    self.assertIn('arg_2=2', str(e.exception))\n+    self.assertIn('keyword_arg_1=3', str(e.exception))\n+    self.assertIn('keyword_arg_2=4', str(e.exception))\n+\n+    with self.assertRaises(ValueError) as e:\n+      traceback_utils.inject_argument_info_in_traceback(\n+          error_fn)(1, 2, keyword_arg_1=3, keyword_arg_2=4)\n+    self.assertIn('Exception encountered when calling error_fn',\n+                  str(e.exception))\n+\n+  def test_info_injection_no_args(self):\n+    def error_fn():\n+      raise ValueError('Original message')\n+\n+    with self.assertRaises(ValueError) as e:\n+      traceback_utils.inject_argument_info_in_traceback(error_fn)()\n+    self.assertEqual(str(e.exception).count('Call arguments received:'), 0)\n+\n+  def test_info_injection_unbindable(self):\n+    def error_fn(arg_1, keyword_arg_1=1):\n+      return arg_1 + keyword_arg_1\n+\n+    with self.assertRaises(TypeError) as e:\n+      traceback_utils.inject_argument_info_in_traceback(error_fn)()\n+    self.assertIn('missing 1 required positional argument', str(e.exception))\n+\n+  def test_info_injection_nested(self):\n+    def inner_fn(arg_1):\n+      raise ValueError('Original message')\n+\n+    def outer_fn(arg_1):\n+      return inner_fn(arg_1)\n+\n+    with self.assertRaises(ValueError) as e:\n+      traceback_utils.inject_argument_info_in_traceback(\n+          outer_fn)(1)\n+    self.assertEqual(str(e.exception).count('Call arguments received:'), 1)\n+\n+  def test_info_injection_tf_op_error(self):\n+    def error_fn(arg_1, keyword_arg_1=1):\n+      return arg_1 + keyword_arg_1 + tf.zeros((2, 3))\n+\n+    with self.assertRaises(tf.errors.InvalidArgumentError) as e:\n+      traceback_utils.inject_argument_info_in_traceback(error_fn)(\n+          tf.zeros((3, 3)))\n+    self.assertIn('Incompatible shapes', str(e.exception))\n+    self.assertIn('Call arguments received', str(e.exception))\n+\n+\n+class LayerCallInfoInjectionTest(tf.test.TestCase):\n+\n+  def assert_info_injected(self, fn):\n+    tf.debugging.enable_traceback_filtering()\n+    try:\n+      fn()\n+    except Exception as e:  # pylint: disable=broad-except\n+      # Info should be injected exactly once.\n+      self.assertEqual(str(e).count('Call arguments received:'), 1)  # pylint: disable=g-assert-in-except\n+\n+  def test_custom_layer_call_nested(self):\n+\n+    class InnerLayer(layers.Layer):\n+\n+      def call(self, inputs, training=False, mask=None):\n+        return inputs + tf.zeros((3, 4))\n+\n+    class OuterLayer(layers.Layer):\n+\n+      def __init__(self):\n+        super().__init__()\n+        self.inner = InnerLayer()\n+\n+      def call(self, inputs, training=True):\n+        return self.inner(inputs)\n+\n+    def fn():\n+      layer = OuterLayer()\n+      layer(tf.zeros((3, 5)), training=False)\n+\n+    self.assert_info_injected(fn)\n+\n+  def test_custom_layer_call_eager_dense_input(self):\n+\n+    class MyLayer(layers.Layer):\n+\n+      def call(self, inputs, training=False, mask=None):\n+        return inputs + tf.zeros((3, 4))\n+\n+    def fn():\n+      layer = MyLayer()\n+      layer(tf.zeros((3, 5)), training=False)\n+\n+    self.assert_info_injected(fn)\n+\n+  def test_custom_layer_call_eager_sparse_input(self):\n+\n+    class MyLayer(layers.Layer):\n+\n+      def call(self, inputs, training=False, mask=None):\n+        return inputs + tf.zeros((3, 4))\n+\n+    def fn():\n+      layer = MyLayer()\n+      layer(\n+          tf.SparseTensor(indices=[[0, 0]], values=[1], dense_shape=[3, 5]),\n+          training=False)\n+\n+    self.assert_info_injected(fn)\n+\n+  def test_custom_layer_call_eager_ragged_input(self):\n+\n+    class MyLayer(layers.Layer):\n+\n+      def call(self, inputs, training=False, mask=None):\n+        return inputs + tf.zeros((3, 4))\n+\n+    def fn():\n+      layer = MyLayer()\n+      layer(tf.ragged.constant([[0, 0, 0], [0, 0]]), training=False)\n+\n+    self.assert_info_injected(fn)\n+\n+  def test_custom_layer_call_symbolic(self):\n+\n+    class MyLayer(layers.Layer):\n+\n+      def call(self, inputs, training=False, mask=None):\n+        return inputs + tf.zeros((3, 4))\n+\n+    def fn():\n+      layer = MyLayer()\n+      layer(layers.Input((3, 5)), training=False)\n+\n+    self.assert_info_injected(fn)\n+\n+  def test_custom_layer_call_unbindable(self):\n+\n+    class MyLayer(layers.Layer):\n+\n+      def __init__(self):\n+        super().__init__()\n+        self.input_spec = layers.InputSpec(shape=(3, 4))\n+\n+      def call(self, inputs, training=False, mask=None):\n+        return inputs + tf.zeros((3, 4))\n+\n+    def fn():\n+      layer = MyLayer()\n+      layer(bad=True, arguments=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'The first argument to `Layer.call` must always'):\n+      fn()\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01551c8572211ab9cc893847a686faad3d1ebdc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 20 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 19991 | Contributors (this commit): 111 | Commits (past 90d): 37 | Contributors (cumulative): 125 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,6 +23,7 @@ from keras.engine import keras_tensor\n from keras.engine import node as node_module\n from keras.saving.saved_model import layer_serialization\n from keras.utils import tf_utils\n+from keras.utils import traceback_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -92,6 +93,7 @@ class InputLayer(base_layer.Layer):\n       name: Optional name of the layer (string).\n   \"\"\"\n \n+  @traceback_utils.filter_traceback\n   def __init__(self,\n                input_shape=None,\n                batch_size=None,\n@@ -252,6 +254,7 @@ class InputLayer(base_layer.Layer):\n \n \n @keras_export('keras.Input', 'keras.layers.Input')\n+@traceback_utils.filter_traceback\n def Input(  # pylint: disable=invalid-name\n     shape=None,\n     batch_size=None,\n\n@@ -28,6 +28,7 @@ from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n+from keras.utils import traceback_utils\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -96,6 +97,7 @@ class Sequential(functional.Functional):\n   \"\"\"\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  @traceback_utils.filter_traceback\n   def __init__(self, layers=None, name=None):\n     \"\"\"Creates a `Sequential` model instance.\n \n@@ -146,6 +148,7 @@ class Sequential(functional.Functional):\n     return layers[:]\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  @traceback_utils.filter_traceback\n   def add(self, layer):\n     \"\"\"Adds a layer instance on top of the layer stack.\n \n@@ -230,6 +233,7 @@ class Sequential(functional.Functional):\n     self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  @traceback_utils.filter_traceback\n   def pop(self):\n     \"\"\"Removes the last layer in the model.\n \n\n@@ -44,6 +44,7 @@ from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import object_identity\n from keras.utils import tf_utils\n+from keras.utils import traceback_utils\n from keras.utils import version_utils\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n from keras.utils.io_utils import path_to_string\n@@ -159,6 +160,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       return super(Model, cls).__new__(cls, *args, **kwargs)\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  @traceback_utils.filter_traceback\n   def __init__(self, *args, **kwargs):\n     self._is_model_for_instrumentation = True\n     base_layer.keras_api_gauge.get_cell('model').set(True)\n@@ -447,6 +449,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     raise NotImplementedError('When subclassing the `Model` class, you should '\n                               'implement a `call()` method.')\n \n+  @traceback_utils.filter_traceback\n   def compile(self,\n               optimizer='rmsprop',\n               loss=None,\n@@ -873,6 +876,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     return self.train_function\n \n+  @traceback_utils.filter_traceback\n   def fit(self,\n           x=None,\n           y=None,\n@@ -1349,6 +1353,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     return self.test_function\n \n+  @traceback_utils.filter_traceback\n   def evaluate(self,\n                x=None,\n                y=None,\n@@ -1606,6 +1611,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self.predict_function = predict_function\n     return self.predict_function\n \n+  @traceback_utils.filter_traceback\n   def predict(self,\n               x,\n               batch_size=None,\n@@ -2086,6 +2092,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     with self.distribute_strategy.scope():\n       return super(Model, self).get_weights()\n \n+  @traceback_utils.filter_traceback\n   def save(self,\n            filepath,\n            overwrite=True,\n@@ -2140,6 +2147,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n                     signatures, options, save_traces)\n \n+  @traceback_utils.filter_traceback\n   def save_weights(self,\n                    filepath,\n                    overwrite=True,\n@@ -2257,6 +2265,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           save_relative_paths=True,\n           all_model_checkpoint_paths=[filepath])\n \n+  @traceback_utils.filter_traceback\n   def load_weights(self,\n                    filepath,\n                    by_name=False,\n@@ -2332,7 +2341,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         session = backend.get_session()\n         # Restore existing variables (if any) immediately, and set up a\n         # streaming restore for any variables created in the future.\n-        tf.__internal__.tracking.streaming_restore(status=status, session=session)\n+        tf.__internal__.tracking.streaming_restore(status=status,\n+                                                   session=session)\n       status.assert_nontrivial_match()\n     else:\n       status = None\n\n@@ -21,6 +21,7 @@ from keras.saving.saved_model import load as saved_model_load\n from keras.saving.saved_model import load_context\n from keras.saving.saved_model import save as saved_model_save\n from keras.utils import generic_utils\n+from keras.utils import traceback_utils\n from keras.utils.io_utils import path_to_string\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -33,6 +34,7 @@ except ImportError:\n \n \n @keras_export('keras.models.save_model')\n+@traceback_utils.filter_traceback\n def save_model(model,\n                filepath,\n                overwrite=True,\n@@ -151,6 +153,7 @@ def save_model(model,\n \n \n @keras_export('keras.models.load_model')\n+@traceback_utils.filter_traceback\n def load_model(filepath, custom_objects=None, compile=True, options=None):  # pylint: disable=redefined-builtin\n   \"\"\"Loads a model saved via `model.save()`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b04de21bf488353f7d1f87bd89798481cc8f041", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 26 | Churn Cumulative: 19012 | Contributors (this commit): 77 | Commits (past 90d): 11 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -217,6 +217,9 @@ class Conv(Layer):\n     self.input_spec = InputSpec(min_ndim=self.rank + 2,\n                                 axes={channel_axis: input_channel})\n \n+    # This code is kept for backwards compat, but can be removed after fixing\n+    # the tests listed in b/192260878\n+\n     # Convert Keras formats to TF native formats.\n     if self.padding == 'causal':\n       tf_padding = 'VALID'  # Causal padding handled in `call`.\n@@ -240,13 +243,34 @@ class Conv(Layer):\n         name=tf_op_name)\n     self.built = True\n \n+  def convolution_op(self, inputs, kernel):\n+    if self.padding == 'causal':\n+      tf_padding = 'VALID'  # Causal padding handled in `call`.\n+    elif isinstance(self.padding, str):\n+      tf_padding = self.padding.upper()\n+    else:\n+      tf_padding = self.padding\n+\n+    tf_op_name = self.__class__.__name__\n+    if tf_op_name == 'Conv1D':\n+      tf_op_name = 'conv1d'  # Backwards compat.\n+\n+    return tf.nn.convolution(\n+        inputs,\n+        kernel,\n+        strides=list(self.strides),\n+        padding=tf_padding,\n+        dilations=list(self.dilation_rate),\n+        data_format=self._tf_data_format,\n+        name=tf_op_name)\n+\n   def call(self, inputs):\n     input_shape = inputs.shape\n \n     if self._is_causal:  # Apply causal padding to inputs for Conv1D.\n       inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n \n-    outputs = self._convolution_op(inputs, self.kernel)\n+    outputs = self.convolution_op(inputs, self.kernel)\n \n     if self.use_bias:\n       output_rank = outputs.shape.rank\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e8d286537fd10b328f1a307ff9727b97b8a8219", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 43 | Files Changed: 3 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 79 | Churn Cumulative: 2692 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,7 +114,7 @@ class Dense(Layer):\n     self.units = int(units) if not isinstance(units, int) else units\n     if self.units < 0:\n       raise ValueError(f'Received an invalid value for `units`, expected '\n-                       f'a positive integer, got {units}.')\n+                       f'a positive integer. Received: units={units}')\n     self.activation = activations.get(activation)\n     self.use_bias = use_bias\n     self.kernel_initializer = initializers.get(kernel_initializer)\n@@ -130,14 +130,15 @@ class Dense(Layer):\n   def build(self, input_shape):\n     dtype = tf.as_dtype(self.dtype or K.floatx())\n     if not (dtype.is_floating or dtype.is_complex):\n-      raise TypeError('Unable to build `Dense` layer with non-floating point '\n-                      'dtype %s' % (dtype,))\n+      raise TypeError('A Dense layer can only be built with a floating-point '\n+                      f'dtype. Received: dtype={dtype}')\n \n     input_shape = tf.TensorShape(input_shape)\n     last_dim = tf.compat.dimension_value(input_shape[-1])\n     if last_dim is None:\n-      raise ValueError('The last dimension of the inputs to `Dense` '\n-                       'should be defined. Found `None`.')\n+      raise ValueError('The last dimension of the inputs to a Dense layer '\n+                       'should be defined. Found None. '\n+                       f'Full input shape received: {input_shape}')\n     self.input_spec = InputSpec(min_ndim=2, axes={-1: last_dim})\n     self.kernel = self.add_weight(\n         'kernel',\n@@ -216,9 +217,9 @@ class Dense(Layer):\n     input_shape = tf.TensorShape(input_shape)\n     input_shape = input_shape.with_rank_at_least(2)\n     if tf.compat.dimension_value(input_shape[-1]) is None:\n-      raise ValueError(\n-          'The innermost dimension of input_shape must be defined, but saw: %s'\n-          % (input_shape,))\n+      raise ValueError('The last dimension of the input shape of a Dense layer '\n+                       'should be defined. Found None. '\n+                       f'Received: input_shape={input_shape}')\n     return input_shape[:-1].concatenate(self.units)\n \n   def get_config(self):\n\n@@ -58,9 +58,9 @@ class Permute(Layer):\n     self.dims = tuple(dims)\n     if sorted(dims) != list(range(1, len(dims) + 1)):\n       raise ValueError(\n-          'Invalid permutation `dims` for Permute Layer: %s. '\n-          'The set of indices in `dims` must be consecutive and start from 1.' %\n-          (dims,))\n+          'Invalid permutation argument `dims` for Permute Layer. '\n+          'The set of indices in `dims` must be consecutive and start from 1. '\n+          f'Received dims={dims}')\n     self.input_spec = InputSpec(ndim=len(self.dims) + 1)\n \n   def compute_output_shape(self, input_shape):\n\n@@ -17,7 +17,6 @@\n import tensorflow.compat.v2 as tf\n # pylint: enable=g-bad-import-order\n \n-import textwrap\n from keras import backend as K\n from keras.engine import keras_tensor\n from keras.engine.base_layer import Layer\n@@ -72,15 +71,15 @@ class ClassMethod(Layer):\n \n   def get_config(self):\n     if not self.cls_symbol:\n-      raise ValueError('This Keras class method conversion tried to convert '\n-                       'a method belonging to class %s, a class '\n-                       'that is not an exposed in the TensorFlow API. '\n+      raise ValueError(\n+          'This Keras class method conversion tried to convert '\n+          f'a method belonging to class {self.cls_symbol}, a class '\n+          'that is not publicly exposed in the TensorFlow API. '\n           'To ensure cross-version compatibility of Keras models '\n           'that use op layers, only op layers produced from '\n-                       'exported TF API symbols can be serialized.' %\n-                       self.cls_symbol)\n-    config = {'cls_symbol': self.cls_symbol, 'method_name': self.method_name}\n+          'public TensorFlow API symbols can be serialized.')\n \n+    config = {'cls_symbol': self.cls_symbol, 'method_name': self.method_name}\n     base_config = super(ClassMethod, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n@@ -90,7 +89,7 @@ class ClassMethod(Layer):\n     symbol_name = config.pop('cls_symbol')\n     cls_ref = get_symbol_from_name(symbol_name)\n     if not cls_ref:\n-      raise ValueError('TF symbol `tf.%s` could not be found.' % symbol_name)\n+      raise ValueError(f'TensorFlow symbol `{symbol_name}` could not be found.')\n \n     config['cls_ref'] = cls_ref\n \n@@ -276,17 +275,14 @@ class TFOpLambda(Layer):\n     ]\n     if untracked_new_vars:\n       variable_str = '\\n'.join('  {}'.format(i) for i in untracked_new_vars)\n-      error_str = textwrap.dedent(\"\"\"\n-          The following Variables were created within a Lambda layer ({name})\n-          but are not tracked by said layer:\n-          {variable_str}\n-          The layer cannot safely ensure proper Variable reuse across multiple\n-          calls, and consquently this behavior is disallowed for safety. Lambda\n-          layers are not well suited to stateful computation; instead, writing a\n-          subclassed Layer is the recommend way to define layers with\n-          Variables.\"\"\").format(\n-              name=self.name, variable_str=variable_str)\n-      raise ValueError(error_str)\n+      raise ValueError(\n+          'The following Variables were created within a Lambda layer '\n+          f'({self.name}) but are not tracked by said layer: {variable_str}\\n'\n+          'The layer cannot safely ensure proper Variable reuse '\n+          'across multiple calls, and consquently this behavior is disallowed '\n+          'for safety reasons. Lambda layers are not well suited for stateful '\n+          'computation; instead, writing a subclassed Layer is the recommend '\n+          'way to define layers with Variables.')\n \n     untracked_used_vars = [\n         v for v in accessed_variables if v.ref() not in tracked_weights\n@@ -294,14 +290,10 @@ class TFOpLambda(Layer):\n     if untracked_used_vars and not self._already_warned:\n       variable_str = '\\n'.join('  {}'.format(i) for i in untracked_used_vars)\n       self._warn(\n-          textwrap.dedent(\"\"\"\n-          The following Variables were used a Lambda layer's call ({name}), but\n-          are not present in its tracked objects:\n-          {variable_str}\n-          It is possible that this is intended behavior, but it is more likely\n-          an omission. This is a strong indication that this layer should be\n-          formulated as a subclassed Layer rather than a Lambda layer.\"\"\")\n-          .format(name=self.name, variable_str=variable_str))\n+          'The following Variables were used in a Lambda layer\\'s call '\n+          f'({self.name}), but are not present in its tracked objects: '\n+          f'{variable_str}. This is a strong indication that the Lambda layer '\n+          'should be rewritten as a subclassed Layer.')\n       self._already_warned = True\n \n   def _warn(self, msg):\n@@ -311,15 +303,15 @@ class TFOpLambda(Layer):\n \n   def get_config(self):\n     if not self.symbol:\n-      raise ValueError('This Keras op layer was generated from %s, a method '\n-                       'that is not an exposed in the TensorFlow API. This '\n+      raise ValueError(\n+          f'This Keras op layer was generated from {self.function}, a method '\n+          'that is not publicly exposed in the TensorFlow API. This '\n           'may have happened if the method was explicitly '\n           'decorated to add dispatching support, and it was used '\n           'during Functional model construction. '\n           'To ensure cross-version compatibility of Keras models '\n           'that use op layers, only op layers produced from '\n-                       'exported TF API symbols can be serialized.' %\n-                       self.function)\n+          'public TensorFlow API symbols can be serialized.')\n     config = {'function': self.symbol}\n \n     base_config = super(TFOpLambda, self).get_config()\n@@ -331,7 +323,7 @@ class TFOpLambda(Layer):\n     symbol_name = config['function']\n     function = get_symbol_from_name(symbol_name)\n     if not function:\n-      raise ValueError('TF symbol `tf.%s` could not be found.' % symbol_name)\n+      raise ValueError(f'TF symbol `{symbol_name}` could not be found.')\n \n     config['function'] = function\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
