{"custom_id": "keras#42bdabf76ad0f1b03f2f662b573bdee627d1561f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 2736 | Contributors (this commit): 8 | Commits (past 90d): 44 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,7 +106,7 @@ class _IoUBase(Metric):\n         if len(sample_weight.shape) > 1:\n             sample_weight = ops.reshape(sample_weight, [-1])\n \n-        sample_weight = ops.broadcast_to(sample_weight, y_true.shape)\n+        sample_weight = ops.broadcast_to(sample_weight, ops.shape(y_true))\n \n         if self.ignore_class is not None:\n             ignore_class = ops.convert_to_tensor(\n\n@@ -72,7 +72,9 @@ class Metric:\n             values = ops.cast(values, self.dtype)\n             if sample_weight is not None:\n                 sample_weight = ops.cast(sample_weight, self.dtype)\n-                sample_weight = ops.broadcast_to(sample_weight, values.shape)\n+                sample_weight = ops.broadcast_to(\n+                    sample_weight, ops.shape(values)\n+                )\n                 values = ops.multiply(values, sample_weight)\n             self.true_positives.assign(self.true_positives + ops.sum(values))\n \n\n@@ -195,7 +195,7 @@ def _update_confusion_matrix_variables_optimized(\n         sample_weights = 1.0\n     else:\n         sample_weights = ops.broadcast_to(\n-            ops.cast(sample_weights, dtype=y_pred.dtype), y_pred.shape\n+            ops.cast(sample_weights, dtype=y_pred.dtype), ops.shape(y_pred)\n         )\n         if not multi_label:\n             sample_weights = ops.reshape(sample_weights, [-1])\n@@ -203,7 +203,7 @@ def _update_confusion_matrix_variables_optimized(\n         label_weights = 1.0\n     else:\n         label_weights = ops.expand_dims(label_weights, 0)\n-        label_weights = ops.broadcast_to(label_weights, y_pred.shape)\n+        label_weights = ops.broadcast_to(label_weights, ops.shape(y_pred))\n         if not multi_label:\n             label_weights = ops.reshape(label_weights, [-1])\n     weights = ops.cast(\n@@ -533,7 +533,7 @@ def update_confusion_matrix_variables(\n \n     if label_weights is not None and not multi_label:\n         label_weights = ops.expand_dims(label_weights, 0)\n-        label_weights = ops.broadcast_to(label_weights, y_pred.shape)\n+        label_weights = ops.broadcast_to(label_weights, ops.shape(y_pred))\n         label_weights_tiled = ops.tile(\n             ops.reshape(label_weights, thresh_tiles), data_tiles\n         )\n\n@@ -428,7 +428,6 @@ class R2Score(reduction_metrics.Metric):\n             shape=(),\n             initializer=initializers.Zeros(),\n             name=\"num_samples\",\n-            dtype=\"int32\",\n         )\n         self._built = False\n \n@@ -500,16 +499,19 @@ class R2Score(reduction_metrics.Metric):\n             # Make sure there's a features dimension\n             sample_weight = ops.expand_dims(sample_weight, axis=1)\n \n-        sample_weight = ops.broadcast_to(sample_weight, y_true.shape)\n+        sample_weight = ops.broadcast_to(sample_weight, ops.shape(y_true))\n \n-        weighted_y_true = y_true * sample_weight\n+        weighted_y_true = y_true * ops.cast(sample_weight, y_true.dtype)\n         self.sum.assign(self.sum + ops.sum(weighted_y_true, axis=0))\n         self.squared_sum.assign(\n             self.squared_sum + ops.sum(y_true * weighted_y_true, axis=0)\n         )\n         self.total_mse.assign(\n             self.total_mse\n-            + ops.sum((y_true - y_pred) ** 2 * sample_weight, axis=0)\n+            + ops.sum(\n+                (y_true - y_pred) ** 2 * ops.cast(sample_weight, y_true.dtype),\n+                axis=0,\n+            )\n         )\n         self.count.assign(self.count + ops.sum(sample_weight, axis=0))\n         self.num_samples.assign(self.num_samples + ops.size(y_true))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e060e522d88ca0d4f83e2b779fc959cad64f7f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.1\"\n+__version__ = \"0.1.2\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#139131be8559514f55962010118505eb6ea8568a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 101 | Churn Cumulative: 1084 | Contributors (this commit): 8 | Commits (past 90d): 30 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,74 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch import core\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class Nadam(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Nadam):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+\n+        local_step = ops.cast(self.iterations + 1, dtype)\n+        next_step = ops.cast(self.iterations + 2, dtype)\n+        decay = ops.cast(0.96, dtype)\n+        beta_1 = ops.cast(self.beta_1, dtype)\n+        beta_2 = ops.cast(self.beta_2, dtype)\n+        u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n+        u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n+        u_product_t = self._u_product.value * u_t\n+        u_product_t_1 = u_product_t * u_t_1\n+        beta_2_power = ops.power(beta_2, local_step)\n+\n+        self._u_product.assign(u_product_t)\n+\n+        m_list = [\n+            self._momentums[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+        v_list = [\n+            self._velocities[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+\n+        torch._foreach_mul_(m_list, self.beta_1)\n+        torch._foreach_add_(m_list, grads, alpha=1 - self.beta_1)\n+\n+        torch._foreach_mul_(v_list, self.beta_2)\n+        torch._foreach_add_(\n+            v_list, torch._foreach_mul(grads, grads), alpha=1 - self.beta_2\n+        )\n+\n+        m_hat_list = torch._foreach_add(\n+            torch._foreach_div(\n+                torch._foreach_mul(m_list, u_t_1),\n+                1 - core.convert_to_numpy(u_product_t_1),\n+            ),\n+            torch._foreach_div(\n+                torch._foreach_mul(grads, 1 - u_t),\n+                1 - core.convert_to_numpy(u_product_t),\n+            ),\n+        )\n+\n+        v_hat_list = torch._foreach_div(v_list, 1 - beta_2_power)\n+\n+        torch._foreach_add_(\n+            variables,\n+            torch._foreach_div(\n+                torch._foreach_mul(m_hat_list, lr),\n+                torch._foreach_add(\n+                    torch._foreach_sqrt(v_hat_list), self.epsilon\n+                ),\n+            ),\n+            alpha=-1,\n+        )\n\n@@ -12,6 +12,7 @@ class TorchOptimizer(BaseOptimizer):\n         from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_adamax\n         from keras_core.backend.torch.optimizers import torch_adamw\n+        from keras_core.backend.torch.optimizers import torch_nadam\n         from keras_core.backend.torch.optimizers import torch_rmsprop\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n@@ -21,6 +22,7 @@ class TorchOptimizer(BaseOptimizer):\n             optimizers.Adam: torch_adam.Adam,\n             optimizers.Adamax: torch_adamax.Adamax,\n             optimizers.AdamW: torch_adamw.AdamW,\n+            optimizers.Nadam: torch_nadam.Nadam,\n             optimizers.RMSprop: torch_rmsprop.RMSprop,\n             optimizers.SGD: torch_sgd.SGD,\n         }\n\n@@ -97,6 +97,18 @@ class Nadam(optimizer.Optimizer):\n                 )\n             )\n \n+    def _internal_apply_gradients(self, grads_and_vars):\n+        dtype = self._u_product.dtype\n+        self._u_product.assign(\n+            self._u_product\n+            * self.beta_1\n+            * (\n+                1.0\n+                - 0.5 * ops.power(0.96, ops.cast(self.iterations + 1, dtype))\n+            )\n+        )\n+        super()._internal_apply_gradients(grads_and_vars)\n+\n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n         var_dtype = variable.dtype\n@@ -110,9 +122,7 @@ class Nadam(optimizer.Optimizer):\n         beta_2 = ops.cast(self.beta_2, var_dtype)\n         u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n         u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n-\n-        u_product_t = self._u_product * u_t\n-        self._u_product.assign(u_product_t)\n+        u_product_t = ops.cast(self._u_product, var_dtype)\n \n         u_product_t_1 = u_product_t * u_t_1\n         beta_2_power = ops.power(beta_2, local_step)\n\n@@ -4,6 +4,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.nadam import Nadam\n \n@@ -20,7 +21,7 @@ class NadamTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Nadam(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -29,7 +30,7 @@ class NadamTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -58,8 +59,8 @@ class NadamTest(testing.TestCase):\n         )\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35b5af7b6300e1bf42e82a48e7b8a7acb571ae7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 30 | Churn Cumulative: 3191 | Contributors (this commit): 9 | Commits (past 90d): 52 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,6 +44,9 @@ def mean(x, axis=None, keepdims=False):\n     if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n     # Conversion to float necessary for `torch.mean`\n     x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     return torch.mean(x, axis=axis, keepdims=keepdims)\n@@ -886,6 +889,9 @@ def sum(x, axis=None, keepdims=False):\n     if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n     if axis is not None:\n         return torch.sum(x, axis=axis, keepdim=keepdims)\n     return torch.sum(x)\n\n@@ -26,6 +26,10 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n                 values, axis=list(range(weight_ndim, values_ndim))\n             )\n         values = values * sample_weight\n+        if values_ndim > 1:\n+            sample_weight = reduce_fn(\n+                sample_weight, axis=list(range(1, weight_ndim))\n+            )\n \n     values_ndim = len(values.shape)\n     if values_ndim > 1:\n@@ -127,9 +131,7 @@ class Mean(Metric):\n         else:\n             num_samples = 1\n         if sample_weight is not None:\n-            num_samples = ops.sum(\n-                ops.ones(shape=(num_samples,)) * sample_weight\n-            )\n+            num_samples = ops.sum(sample_weight)\n         self.count.assign(self.count + ops.cast(num_samples, dtype=self.dtype))\n \n     def reset_state(self):\n\n@@ -24,6 +24,12 @@ class SumTest(testing.TestCase):\n         result = sum_obj.result()\n         self.assertAllClose(result, 4.0, atol=1e-3)\n \n+    def test_weighted_nd(self):\n+        sum_obj = reduction_metrics.Sum(name=\"sum\", dtype=\"float32\")\n+        sum_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n+        result = sum_obj.result()\n+        self.assertAllClose(result, 9.0, atol=1e-3)\n+\n \n class MeanTest(testing.TestCase):\n     def test_config(self):\n@@ -45,6 +51,12 @@ class MeanTest(testing.TestCase):\n         result = mean_obj.result()\n         self.assertAllClose(result, 2.0, atol=1e-3)\n \n+    def test_weighted_nd(self):\n+        mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n+        mean_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n+        result = mean_obj.result()\n+        self.assertAllClose(result, 3.0, atol=1e-3)\n+\n \n def mse(y_true, y_pred):\n     return (y_true - y_pred) ** 2\n\n@@ -2068,6 +2068,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.mean(x), np.mean(x))\n         self.assertAllClose(knp.mean(x, axis=1), np.mean(x, axis=1))\n+        self.assertAllClose(knp.mean(x, axis=()), np.mean(x, axis=()))\n+        self.assertAllClose(knp.mean(x, axis=(1,)), np.mean(x, axis=(1,)))\n         self.assertAllClose(\n             knp.mean(x, axis=1, keepdims=True),\n             np.mean(x, axis=1, keepdims=True),\n@@ -2132,6 +2134,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.sum(x), np.sum(x))\n         self.assertAllClose(knp.sum(x, axis=1), np.sum(x, axis=1))\n+        self.assertAllClose(knp.sum(x, axis=(1,)), np.sum(x, axis=(1,)))\n+        self.assertAllClose(knp.sum(x, axis=()), np.sum(x, axis=()))\n         self.assertAllClose(\n             knp.sum(x, axis=1, keepdims=True),\n             np.sum(x, axis=1, keepdims=True),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e70dea48d0110d6520b239fd972f14a613202c06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 151 | Files Changed: 20 | Hunks: 43 | Methods Changed: 37 | Complexity Δ (Sum/Max): -13/1 | Churn Δ: 198 | Churn Cumulative: 19244 | Contributors (this commit): 20 | Commits (past 90d): 421 | Contributors (cumulative): 99 | DMM Complexity: 0.3170731707317073\n\nDIFF:\n@@ -30,7 +30,7 @@ class JAXTrainer(base_trainer.Trainer):\n     ):\n         \"\"\"This method is stateless and is intended for use with jax.grad.\"\"\"\n         kwargs = {}\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             kwargs[\"training\"] = training\n         y_pred, non_trainable_variables, losses = self.stateless_call(\n             trainable_variables,\n@@ -167,7 +167,7 @@ class JAXTrainer(base_trainer.Trainer):\n     def predict_step(self, state, data):\n         trainable_variables, non_trainable_variables = state\n         kwargs = {}\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             kwargs[\"training\"] = False\n \n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n\n@@ -48,7 +48,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         # Forward pass\n         with tf.GradientTape() as tape:\n-            if self._call_has_training_arg():\n+            if self._call_has_training_arg:\n                 y_pred = self(x, training=True)\n             else:\n                 y_pred = self(x)\n@@ -71,7 +71,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n     def test_step(self, data):\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n@@ -83,7 +83,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n     def predict_step(self, data):\n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n\n@@ -44,9 +44,6 @@ def mean(x, axis=None, keepdims=False):\n     if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n-    if axis == () or axis == []:\n-        # Torch handles the empty axis case differently from numpy.\n-        return x\n     # Conversion to float necessary for `torch.mean`\n     x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     return torch.mean(x, axis=axis, keepdims=keepdims)\n@@ -889,9 +886,6 @@ def sum(x, axis=None, keepdims=False):\n     if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n-    if axis == () or axis == []:\n-        # Torch handles the empty axis case differently from numpy.\n-        return x\n     if axis is not None:\n         return torch.sum(x, axis=axis, keepdim=keepdims)\n     return torch.sum(x)\n\n@@ -1,74 +0,0 @@\n-import torch\n-\n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch import core\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n-\n-\n-class Nadam(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Nadam):\n-    def _parallel_update_step(\n-        self,\n-        grads,\n-        variables,\n-        learning_rate,\n-    ):\n-        keras_variables = variables\n-        variables = [v.value for v in variables]\n-\n-        dtype = variables[0].dtype\n-        lr = ops.cast(learning_rate, dtype)\n-\n-        local_step = ops.cast(self.iterations + 1, dtype)\n-        next_step = ops.cast(self.iterations + 2, dtype)\n-        decay = ops.cast(0.96, dtype)\n-        beta_1 = ops.cast(self.beta_1, dtype)\n-        beta_2 = ops.cast(self.beta_2, dtype)\n-        u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n-        u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n-        u_product_t = self._u_product.value * u_t\n-        u_product_t_1 = u_product_t * u_t_1\n-        beta_2_power = ops.power(beta_2, local_step)\n-\n-        self._u_product.assign(u_product_t)\n-\n-        m_list = [\n-            self._momentums[self._get_variable_index(variable)].value\n-            for variable in keras_variables\n-        ]\n-        v_list = [\n-            self._velocities[self._get_variable_index(variable)].value\n-            for variable in keras_variables\n-        ]\n-\n-        torch._foreach_mul_(m_list, self.beta_1)\n-        torch._foreach_add_(m_list, grads, alpha=1 - self.beta_1)\n-\n-        torch._foreach_mul_(v_list, self.beta_2)\n-        torch._foreach_add_(\n-            v_list, torch._foreach_mul(grads, grads), alpha=1 - self.beta_2\n-        )\n-\n-        m_hat_list = torch._foreach_add(\n-            torch._foreach_div(\n-                torch._foreach_mul(m_list, u_t_1),\n-                1 - core.convert_to_numpy(u_product_t_1),\n-            ),\n-            torch._foreach_div(\n-                torch._foreach_mul(grads, 1 - u_t),\n-                1 - core.convert_to_numpy(u_product_t),\n-            ),\n-        )\n-\n-        v_hat_list = torch._foreach_div(v_list, 1 - beta_2_power)\n-\n-        torch._foreach_add_(\n-            variables,\n-            torch._foreach_div(\n-                torch._foreach_mul(m_hat_list, lr),\n-                torch._foreach_add(\n-                    torch._foreach_sqrt(v_hat_list), self.epsilon\n-                ),\n-            ),\n-            alpha=-1,\n-        )\n\n@@ -12,7 +12,6 @@ class TorchOptimizer(BaseOptimizer):\n         from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_adamax\n         from keras_core.backend.torch.optimizers import torch_adamw\n-        from keras_core.backend.torch.optimizers import torch_nadam\n         from keras_core.backend.torch.optimizers import torch_rmsprop\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n@@ -22,7 +21,6 @@ class TorchOptimizer(BaseOptimizer):\n             optimizers.Adam: torch_adam.Adam,\n             optimizers.Adamax: torch_adamax.Adamax,\n             optimizers.AdamW: torch_adamw.AdamW,\n-            optimizers.Nadam: torch_nadam.Nadam,\n             optimizers.RMSprop: torch_rmsprop.RMSprop,\n             optimizers.SGD: torch_sgd.SGD,\n         }\n\n@@ -126,9 +126,12 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n+    if noise_shape is not None:\n         keep_prob = 1.0 - rate\n         noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n-    keep_prob_matrix = torch.full(noise_shape, keep_prob, device=get_device())\n+        keep_prob_matrix = torch.full(\n+            noise_shape, keep_prob, device=get_device()\n+        )\n         generator = torch_seed_generator(seed)\n \n         # Do not use generator during symbolic execution.\n@@ -140,5 +143,12 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n         mask = mask.bool()\n         mask = torch.broadcast_to(mask, inputs.shape)\n         return torch.where(\n-        mask, inputs / keep_prob, torch.zeros_like(inputs, dtype=inputs.dtype)\n+            mask,\n+            inputs / keep_prob,\n+            torch.zeros_like(inputs, dtype=inputs.dtype),\n+        )\n+    # Fast path, unseeded (since torch doesn't support seeding dropout!!!!)\n+    # Using the above implementation is possible, but much slower.\n+    return torch.nn.functional.dropout(\n+        inputs, p=rate, training=True, inplace=False\n     )\n\n@@ -27,7 +27,7 @@ class TorchTrainer(base_trainer.Trainer):\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n \n         # Compute predictions\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=True)\n         else:\n             y_pred = self(x)\n@@ -64,7 +64,7 @@ class TorchTrainer(base_trainer.Trainer):\n             y,\n             sample_weight,\n         ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n@@ -76,7 +76,7 @@ class TorchTrainer(base_trainer.Trainer):\n \n     def predict_step(self, data):\n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n+        if self._call_has_training_arg:\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n\n@@ -250,9 +250,12 @@ class Layer(BackendLayer, Operation):\n         self._losses = []\n         self._loss_ids = set()\n \n-        self._call_signature_parameters = [\n+        call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n         ]\n+        self._call_has_training_arg = \"training\" in call_signature_parameters\n+        self._call_has_mask_arg = \"mask\" in call_signature_parameters\n+\n         self._supports_masking = not utils.is_default(self.compute_mask)\n         # Whether to automatically convert (+ auto-cast) inputs to `call()`.\n         self._convert_input_args = True\n@@ -693,7 +696,7 @@ class Layer(BackendLayer, Operation):\n                 # Get signature default value\n                 training = call_spec.arguments_dict.get(\"training\", None)\n         call_context.training = training\n-        if self._call_has_training_arg() and training is not None:\n+        if self._call_has_training_arg and training is not None:\n             # Only populate arg if it has a concrete value\n             kwargs[\"training\"] = training\n \n@@ -1191,12 +1194,6 @@ class Layer(BackendLayer, Operation):\n                 self.input_spec, arg_0, layer_name=self.name\n             )\n \n-    def _call_has_training_arg(self):\n-        return \"training\" in self._call_signature_parameters\n-\n-    def _call_has_mask_arg(self):\n-        return \"mask\" in self._call_signature_parameters\n-\n     def _get_call_context(self):\n         \"\"\"Returns currently active `CallContext`.\"\"\"\n         layer_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n\n@@ -141,7 +141,7 @@ class LayerTest(testing.TestCase):\n                 self.seed_gen = backend.random.SeedGenerator(seed=1337)\n \n             def call(self, x):\n-                return backend.random.dropout(x, rate=0.5, seed=self.seed_gen)\n+                return x * backend.random.normal(x.shape, seed=self.seed_gen)\n \n         layer = RNGLayer()\n         self.assertEqual(layer.variables, [layer.seed_gen.state])\n@@ -158,8 +158,8 @@ class LayerTest(testing.TestCase):\n                 self.seed_gens.append(backend.random.SeedGenerator(seed=10))\n \n             def call(self, x):\n-                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[0])\n-                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[1])\n+                x = x * backend.random.normal(x.shape, seed=self.seed_gens[0])\n+                x = x * backend.random.normal(x.shape, seed=self.seed_gens[1])\n                 return x\n \n         layer = RNGListLayer()\n\n@@ -198,9 +198,9 @@ class Bidirectional(Wrapper):\n         training=None,\n     ):\n         kwargs = {}\n-        if self.forward_layer._call_has_training_arg():\n+        if self.forward_layer._call_has_training_arg:\n             kwargs[\"training\"] = training\n-        if self.forward_layer._call_has_mask_arg():\n+        if self.forward_layer._call_has_mask_arg:\n             kwargs[\"mask\"] = mask\n \n         if initial_state is not None:\n\n@@ -323,7 +323,7 @@ class RNN(Layer):\n \n     def inner_loop(self, sequences, initial_state, mask, training=False):\n         cell_kwargs = {}\n-        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg():\n+        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg:\n             cell_kwargs[\"training\"] = training\n \n         def step(inputs, states):\n\n@@ -90,7 +90,7 @@ class StackedRNNCells(Layer):\n         new_states = []\n         for cell, states in zip(self.cells, states):\n             states = list(states) if tree.is_nested(states) else [states]\n-            if isinstance(cell, Layer) and cell._call_has_training_arg():\n+            if isinstance(cell, Layer) and cell._call_has_training_arg:\n                 kwargs[\"training\"] = training\n             else:\n                 kwargs.pop(\"training\", None)\n\n@@ -95,9 +95,9 @@ class TimeDistributed(Wrapper):\n \n         def step_function(i):\n             kwargs = {}\n-            if self.layer._call_has_mask_arg() and mask is not None:\n+            if self.layer._call_has_mask_arg and mask is not None:\n                 kwargs[\"mask\"] = mask[i]\n-            if self.layer._call_has_training_arg():\n+            if self.layer._call_has_training_arg:\n                 kwargs[\"training\"] = training\n             return self.layer.call(inputs[i], **kwargs)\n \n\n@@ -26,10 +26,6 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n                 values, axis=list(range(weight_ndim, values_ndim))\n             )\n         values = values * sample_weight\n-        if values_ndim > 1:\n-            sample_weight = reduce_fn(\n-                sample_weight, axis=list(range(1, weight_ndim))\n-            )\n \n     values_ndim = len(values.shape)\n     if values_ndim > 1:\n@@ -131,7 +127,9 @@ class Mean(Metric):\n         else:\n             num_samples = 1\n         if sample_weight is not None:\n-            num_samples = ops.sum(sample_weight)\n+            num_samples = ops.sum(\n+                ops.ones(shape=(num_samples,)) * sample_weight\n+            )\n         self.count.assign(self.count + ops.cast(num_samples, dtype=self.dtype))\n \n     def reset_state(self):\n\n@@ -24,12 +24,6 @@ class SumTest(testing.TestCase):\n         result = sum_obj.result()\n         self.assertAllClose(result, 4.0, atol=1e-3)\n \n-    def test_weighted_nd(self):\n-        sum_obj = reduction_metrics.Sum(name=\"sum\", dtype=\"float32\")\n-        sum_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n-        result = sum_obj.result()\n-        self.assertAllClose(result, 9.0, atol=1e-3)\n-\n \n class MeanTest(testing.TestCase):\n     def test_config(self):\n@@ -51,12 +45,6 @@ class MeanTest(testing.TestCase):\n         result = mean_obj.result()\n         self.assertAllClose(result, 2.0, atol=1e-3)\n \n-    def test_weighted_nd(self):\n-        mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n-        mean_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n-        result = mean_obj.result()\n-        self.assertAllClose(result, 3.0, atol=1e-3)\n-\n \n def mse(y_true, y_pred):\n     return (y_true - y_pred) ** 2\n\n@@ -542,7 +542,7 @@ def operation_fn(operation, training):\n     def call(*args, **kwargs):\n         if (\n             hasattr(operation, \"_call_has_training_arg\")\n-            and operation._call_has_training_arg()\n+            and operation._call_has_training_arg\n             and training is not None\n         ):\n             kwargs[\"training\"] = training\n\n@@ -183,9 +183,9 @@ class Sequential(Model):\n             # end of each iteration `inputs` is set to `outputs` to prepare for\n             # the next layer.\n             kwargs = {}\n-            if layer._call_has_mask_arg():\n+            if layer._call_has_mask_arg:\n                 kwargs[\"mask\"] = mask\n-            if layer._call_has_training_arg() and training is not None:\n+            if layer._call_has_training_arg and training is not None:\n                 kwargs[\"training\"] = training\n             outputs = layer(inputs, **kwargs)\n             inputs = outputs\n\n@@ -2068,8 +2068,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.mean(x), np.mean(x))\n         self.assertAllClose(knp.mean(x, axis=1), np.mean(x, axis=1))\n-        self.assertAllClose(knp.mean(x, axis=()), np.mean(x, axis=()))\n-        self.assertAllClose(knp.mean(x, axis=(1,)), np.mean(x, axis=(1,)))\n         self.assertAllClose(\n             knp.mean(x, axis=1, keepdims=True),\n             np.mean(x, axis=1, keepdims=True),\n@@ -2134,8 +2132,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.sum(x), np.sum(x))\n         self.assertAllClose(knp.sum(x, axis=1), np.sum(x, axis=1))\n-        self.assertAllClose(knp.sum(x, axis=(1,)), np.sum(x, axis=(1,)))\n-        self.assertAllClose(knp.sum(x, axis=()), np.sum(x, axis=()))\n         self.assertAllClose(\n             knp.sum(x, axis=1, keepdims=True),\n             np.sum(x, axis=1, keepdims=True),\n\n@@ -97,18 +97,6 @@ class Nadam(optimizer.Optimizer):\n                 )\n             )\n \n-    def _internal_apply_gradients(self, grads_and_vars):\n-        dtype = self._u_product.dtype\n-        self._u_product.assign(\n-            self._u_product\n-            * self.beta_1\n-            * (\n-                1.0\n-                - 0.5 * ops.power(0.96, ops.cast(self.iterations + 1, dtype))\n-            )\n-        )\n-        super()._internal_apply_gradients(grads_and_vars)\n-\n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n         var_dtype = variable.dtype\n@@ -122,7 +110,9 @@ class Nadam(optimizer.Optimizer):\n         beta_2 = ops.cast(self.beta_2, var_dtype)\n         u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n         u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n-        u_product_t = ops.cast(self._u_product, var_dtype)\n+\n+        u_product_t = self._u_product * u_t\n+        self._u_product.assign(u_product_t)\n \n         u_product_t_1 = u_product_t * u_t_1\n         beta_2_power = ops.power(beta_2, local_step)\n\n@@ -4,7 +4,6 @@\n import numpy as np\n \n from keras_core import backend\n-from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.nadam import Nadam\n \n@@ -21,7 +20,7 @@ class NadamTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Nadam(learning_rate=0.5)\n-        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -30,7 +29,7 @@ class NadamTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            ops.zeros(()),\n+            np.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -59,8 +58,8 @@ class NadamTest(testing.TestCase):\n         )\n \n         x = backend.Variable(np.ones([10]))\n-        grads = ops.arange(0.1, 1.1, 0.1)\n-        first_grads = ops.full((10,), 0.01)\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1019afae58240418501a03e38b870d2593bf28b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 183 | Lines Deleted: 48 | Files Changed: 21 | Hunks: 44 | Methods Changed: 38 | Complexity Δ (Sum/Max): 13/4 | Churn Δ: 231 | Churn Cumulative: 19479 | Contributors (this commit): 20 | Commits (past 90d): 444 | Contributors (cumulative): 119 | DMM Complexity: 0.6829268292682927\n\nDIFF:\n@@ -30,7 +30,7 @@ class JAXTrainer(base_trainer.Trainer):\n     ):\n         \"\"\"This method is stateless and is intended for use with jax.grad.\"\"\"\n         kwargs = {}\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             kwargs[\"training\"] = training\n         y_pred, non_trainable_variables, losses = self.stateless_call(\n             trainable_variables,\n@@ -167,7 +167,7 @@ class JAXTrainer(base_trainer.Trainer):\n     def predict_step(self, state, data):\n         trainable_variables, non_trainable_variables = state\n         kwargs = {}\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             kwargs[\"training\"] = False\n \n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n\n@@ -48,7 +48,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         # Forward pass\n         with tf.GradientTape() as tape:\n-            if self._call_has_training_arg:\n+            if self._call_has_training_arg():\n                 y_pred = self(x, training=True)\n             else:\n                 y_pred = self(x)\n@@ -71,7 +71,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n     def test_step(self, data):\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n@@ -83,7 +83,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n     def predict_step(self, data):\n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n\n@@ -44,6 +44,9 @@ def mean(x, axis=None, keepdims=False):\n     if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n     # Conversion to float necessary for `torch.mean`\n     x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n     return torch.mean(x, axis=axis, keepdims=keepdims)\n@@ -886,6 +889,9 @@ def sum(x, axis=None, keepdims=False):\n     if isinstance(x, (list, tuple)):\n         x = stack(x)\n     x = convert_to_tensor(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n     if axis is not None:\n         return torch.sum(x, axis=axis, keepdim=keepdims)\n     return torch.sum(x)\n\n@@ -0,0 +1,74 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch import core\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class Nadam(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Nadam):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+\n+        local_step = ops.cast(self.iterations + 1, dtype)\n+        next_step = ops.cast(self.iterations + 2, dtype)\n+        decay = ops.cast(0.96, dtype)\n+        beta_1 = ops.cast(self.beta_1, dtype)\n+        beta_2 = ops.cast(self.beta_2, dtype)\n+        u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n+        u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n+        u_product_t = self._u_product.value * u_t\n+        u_product_t_1 = u_product_t * u_t_1\n+        beta_2_power = ops.power(beta_2, local_step)\n+\n+        self._u_product.assign(u_product_t)\n+\n+        m_list = [\n+            self._momentums[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+        v_list = [\n+            self._velocities[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+\n+        torch._foreach_mul_(m_list, self.beta_1)\n+        torch._foreach_add_(m_list, grads, alpha=1 - self.beta_1)\n+\n+        torch._foreach_mul_(v_list, self.beta_2)\n+        torch._foreach_add_(\n+            v_list, torch._foreach_mul(grads, grads), alpha=1 - self.beta_2\n+        )\n+\n+        m_hat_list = torch._foreach_add(\n+            torch._foreach_div(\n+                torch._foreach_mul(m_list, u_t_1),\n+                1 - core.convert_to_numpy(u_product_t_1),\n+            ),\n+            torch._foreach_div(\n+                torch._foreach_mul(grads, 1 - u_t),\n+                1 - core.convert_to_numpy(u_product_t),\n+            ),\n+        )\n+\n+        v_hat_list = torch._foreach_div(v_list, 1 - beta_2_power)\n+\n+        torch._foreach_add_(\n+            variables,\n+            torch._foreach_div(\n+                torch._foreach_mul(m_hat_list, lr),\n+                torch._foreach_add(\n+                    torch._foreach_sqrt(v_hat_list), self.epsilon\n+                ),\n+            ),\n+            alpha=-1,\n+        )\n\n@@ -12,6 +12,7 @@ class TorchOptimizer(BaseOptimizer):\n         from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_adamax\n         from keras_core.backend.torch.optimizers import torch_adamw\n+        from keras_core.backend.torch.optimizers import torch_nadam\n         from keras_core.backend.torch.optimizers import torch_rmsprop\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n@@ -21,6 +22,7 @@ class TorchOptimizer(BaseOptimizer):\n             optimizers.Adam: torch_adam.Adam,\n             optimizers.Adamax: torch_adamax.Adamax,\n             optimizers.AdamW: torch_adamw.AdamW,\n+            optimizers.Nadam: torch_nadam.Nadam,\n             optimizers.RMSprop: torch_rmsprop.RMSprop,\n             optimizers.SGD: torch_sgd.SGD,\n         }\n\n@@ -126,12 +126,9 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    if noise_shape is not None:\n     keep_prob = 1.0 - rate\n     noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n-        keep_prob_matrix = torch.full(\n-            noise_shape, keep_prob, device=get_device()\n-        )\n+    keep_prob_matrix = torch.full(noise_shape, keep_prob, device=get_device())\n     generator = torch_seed_generator(seed)\n \n     # Do not use generator during symbolic execution.\n@@ -143,12 +140,5 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n     mask = mask.bool()\n     mask = torch.broadcast_to(mask, inputs.shape)\n     return torch.where(\n-            mask,\n-            inputs / keep_prob,\n-            torch.zeros_like(inputs, dtype=inputs.dtype),\n-        )\n-    # Fast path, unseeded (since torch doesn't support seeding dropout!!!!)\n-    # Using the above implementation is possible, but much slower.\n-    return torch.nn.functional.dropout(\n-        inputs, p=rate, training=True, inplace=False\n+        mask, inputs / keep_prob, torch.zeros_like(inputs, dtype=inputs.dtype)\n     )\n\n@@ -27,7 +27,7 @@ class TorchTrainer(base_trainer.Trainer):\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n \n         # Compute predictions\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             y_pred = self(x, training=True)\n         else:\n             y_pred = self(x)\n@@ -64,7 +64,7 @@ class TorchTrainer(base_trainer.Trainer):\n             y,\n             sample_weight,\n         ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n@@ -76,7 +76,7 @@ class TorchTrainer(base_trainer.Trainer):\n \n     def predict_step(self, data):\n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg:\n+        if self._call_has_training_arg():\n             y_pred = self(x, training=False)\n         else:\n             y_pred = self(x)\n\n@@ -250,12 +250,9 @@ class Layer(BackendLayer, Operation):\n         self._losses = []\n         self._loss_ids = set()\n \n-        call_signature_parameters = [\n+        self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n         ]\n-        self._call_has_training_arg = \"training\" in call_signature_parameters\n-        self._call_has_mask_arg = \"mask\" in call_signature_parameters\n-\n         self._supports_masking = not utils.is_default(self.compute_mask)\n         # Whether to automatically convert (+ auto-cast) inputs to `call()`.\n         self._convert_input_args = True\n@@ -696,7 +693,7 @@ class Layer(BackendLayer, Operation):\n                 # Get signature default value\n                 training = call_spec.arguments_dict.get(\"training\", None)\n         call_context.training = training\n-        if self._call_has_training_arg and training is not None:\n+        if self._call_has_training_arg() and training is not None:\n             # Only populate arg if it has a concrete value\n             kwargs[\"training\"] = training\n \n@@ -1194,6 +1191,12 @@ class Layer(BackendLayer, Operation):\n                 self.input_spec, arg_0, layer_name=self.name\n             )\n \n+    def _call_has_training_arg(self):\n+        return \"training\" in self._call_signature_parameters\n+\n+    def _call_has_mask_arg(self):\n+        return \"mask\" in self._call_signature_parameters\n+\n     def _get_call_context(self):\n         \"\"\"Returns currently active `CallContext`.\"\"\"\n         layer_call_ctx = global_state.get_global_attribute(\"current_call_ctx\")\n\n@@ -141,7 +141,7 @@ class LayerTest(testing.TestCase):\n                 self.seed_gen = backend.random.SeedGenerator(seed=1337)\n \n             def call(self, x):\n-                return x * backend.random.normal(x.shape, seed=self.seed_gen)\n+                return backend.random.dropout(x, rate=0.5, seed=self.seed_gen)\n \n         layer = RNGLayer()\n         self.assertEqual(layer.variables, [layer.seed_gen.state])\n@@ -158,8 +158,8 @@ class LayerTest(testing.TestCase):\n                 self.seed_gens.append(backend.random.SeedGenerator(seed=10))\n \n             def call(self, x):\n-                x = x * backend.random.normal(x.shape, seed=self.seed_gens[0])\n-                x = x * backend.random.normal(x.shape, seed=self.seed_gens[1])\n+                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[0])\n+                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[1])\n                 return x\n \n         layer = RNGListLayer()\n\n@@ -198,9 +198,9 @@ class Bidirectional(Wrapper):\n         training=None,\n     ):\n         kwargs = {}\n-        if self.forward_layer._call_has_training_arg:\n+        if self.forward_layer._call_has_training_arg():\n             kwargs[\"training\"] = training\n-        if self.forward_layer._call_has_mask_arg:\n+        if self.forward_layer._call_has_mask_arg():\n             kwargs[\"mask\"] = mask\n \n         if initial_state is not None:\n\n@@ -323,7 +323,7 @@ class RNN(Layer):\n \n     def inner_loop(self, sequences, initial_state, mask, training=False):\n         cell_kwargs = {}\n-        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg:\n+        if isinstance(self.cell, Layer) and self.cell._call_has_training_arg():\n             cell_kwargs[\"training\"] = training\n \n         def step(inputs, states):\n\n@@ -90,7 +90,7 @@ class StackedRNNCells(Layer):\n         new_states = []\n         for cell, states in zip(self.cells, states):\n             states = list(states) if tree.is_nested(states) else [states]\n-            if isinstance(cell, Layer) and cell._call_has_training_arg:\n+            if isinstance(cell, Layer) and cell._call_has_training_arg():\n                 kwargs[\"training\"] = training\n             else:\n                 kwargs.pop(\"training\", None)\n\n@@ -95,9 +95,9 @@ class TimeDistributed(Wrapper):\n \n         def step_function(i):\n             kwargs = {}\n-            if self.layer._call_has_mask_arg and mask is not None:\n+            if self.layer._call_has_mask_arg() and mask is not None:\n                 kwargs[\"mask\"] = mask[i]\n-            if self.layer._call_has_training_arg:\n+            if self.layer._call_has_training_arg():\n                 kwargs[\"training\"] = training\n             return self.layer.call(inputs[i], **kwargs)\n \n\n@@ -26,6 +26,10 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n                 values, axis=list(range(weight_ndim, values_ndim))\n             )\n         values = values * sample_weight\n+        if values_ndim > 1:\n+            sample_weight = reduce_fn(\n+                sample_weight, axis=list(range(1, weight_ndim))\n+            )\n \n     values_ndim = len(values.shape)\n     if values_ndim > 1:\n@@ -127,9 +131,7 @@ class Mean(Metric):\n         else:\n             num_samples = 1\n         if sample_weight is not None:\n-            num_samples = ops.sum(\n-                ops.ones(shape=(num_samples,)) * sample_weight\n-            )\n+            num_samples = ops.sum(sample_weight)\n         self.count.assign(self.count + ops.cast(num_samples, dtype=self.dtype))\n \n     def reset_state(self):\n\n@@ -24,6 +24,12 @@ class SumTest(testing.TestCase):\n         result = sum_obj.result()\n         self.assertAllClose(result, 4.0, atol=1e-3)\n \n+    def test_weighted_nd(self):\n+        sum_obj = reduction_metrics.Sum(name=\"sum\", dtype=\"float32\")\n+        sum_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n+        result = sum_obj.result()\n+        self.assertAllClose(result, 9.0, atol=1e-3)\n+\n \n class MeanTest(testing.TestCase):\n     def test_config(self):\n@@ -45,6 +51,12 @@ class MeanTest(testing.TestCase):\n         result = mean_obj.result()\n         self.assertAllClose(result, 2.0, atol=1e-3)\n \n+    def test_weighted_nd(self):\n+        mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n+        mean_obj.update_state([[1, 3], [5, 7]], sample_weight=[[1, 1], [1, 0]])\n+        result = mean_obj.result()\n+        self.assertAllClose(result, 3.0, atol=1e-3)\n+\n \n def mse(y_true, y_pred):\n     return (y_true - y_pred) ** 2\n\n@@ -542,7 +542,7 @@ def operation_fn(operation, training):\n     def call(*args, **kwargs):\n         if (\n             hasattr(operation, \"_call_has_training_arg\")\n-            and operation._call_has_training_arg\n+            and operation._call_has_training_arg()\n             and training is not None\n         ):\n             kwargs[\"training\"] = training\n\n@@ -183,9 +183,9 @@ class Sequential(Model):\n             # end of each iteration `inputs` is set to `outputs` to prepare for\n             # the next layer.\n             kwargs = {}\n-            if layer._call_has_mask_arg:\n+            if layer._call_has_mask_arg():\n                 kwargs[\"mask\"] = mask\n-            if layer._call_has_training_arg and training is not None:\n+            if layer._call_has_training_arg() and training is not None:\n                 kwargs[\"training\"] = training\n             outputs = layer(inputs, **kwargs)\n             inputs = outputs\n\n@@ -2068,6 +2068,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.mean(x), np.mean(x))\n         self.assertAllClose(knp.mean(x, axis=1), np.mean(x, axis=1))\n+        self.assertAllClose(knp.mean(x, axis=()), np.mean(x, axis=()))\n+        self.assertAllClose(knp.mean(x, axis=(1,)), np.mean(x, axis=(1,)))\n         self.assertAllClose(\n             knp.mean(x, axis=1, keepdims=True),\n             np.mean(x, axis=1, keepdims=True),\n@@ -2132,6 +2134,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.sum(x), np.sum(x))\n         self.assertAllClose(knp.sum(x, axis=1), np.sum(x, axis=1))\n+        self.assertAllClose(knp.sum(x, axis=(1,)), np.sum(x, axis=(1,)))\n+        self.assertAllClose(knp.sum(x, axis=()), np.sum(x, axis=()))\n         self.assertAllClose(\n             knp.sum(x, axis=1, keepdims=True),\n             np.sum(x, axis=1, keepdims=True),\n\n@@ -11,7 +11,38 @@ def compute_pooling_output_shape(\n     padding=\"valid\",\n     data_format=\"channels_last\",\n ):\n-    \"\"\"Compute the output shape of pooling ops.\"\"\"\n+    \"\"\"Computes the output shape of pooling operations.\n+\n+    Args:\n+        input_shape: Input shape. Must be a tuple of integers.\n+        pool_size: Size of the pooling operation. Must be a tuple of integers.\n+        strides: Stride of the pooling operation. Must be a tuple of integers.\n+            Defaults to `pool_size`.\n+        padding: Padding method. Available methods are `\"valid\"` or `\"same\"`.\n+            Defaults to `\"valid\"`.\n+        data_format: String, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, weight)`. Defaults to `\"channels_last\"`.\n+\n+    Returns:\n+        Tuple of ints: The output shape of the pooling operation.\n+\n+    Examples:\n+\n+    # Basic usage with square pooling on a single image\n+    >>> compute_pooling_output_shape((1, 4, 4, 1), (2, 2))\n+    (1, 2, 2, 1)\n+\n+    # Strided pooling on a single image with strides different from pool_size\n+    >>> compute_pooling_output_shape((1, 4, 4, 1), (2, 2), strides=(1, 1))\n+    (1, 3, 3, 1)\n+\n+    # Pooling on a batch of images\n+    >>> compute_pooling_output_shape((32, 4, 4, 3), (2, 2))\n+    (32, 2, 2, 3)\n+    \"\"\"\n     strides = pool_size if strides is None else strides\n     input_shape_origin = list(input_shape)\n     input_shape = np.array(input_shape)\n\n@@ -97,6 +97,18 @@ class Nadam(optimizer.Optimizer):\n                 )\n             )\n \n+    def _internal_apply_gradients(self, grads_and_vars):\n+        dtype = self._u_product.dtype\n+        self._u_product.assign(\n+            self._u_product\n+            * self.beta_1\n+            * (\n+                1.0\n+                - 0.5 * ops.power(0.96, ops.cast(self.iterations + 1, dtype))\n+            )\n+        )\n+        super()._internal_apply_gradients(grads_and_vars)\n+\n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n         var_dtype = variable.dtype\n@@ -110,9 +122,7 @@ class Nadam(optimizer.Optimizer):\n         beta_2 = ops.cast(self.beta_2, var_dtype)\n         u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n         u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n-\n-        u_product_t = self._u_product * u_t\n-        self._u_product.assign(u_product_t)\n+        u_product_t = ops.cast(self._u_product, var_dtype)\n \n         u_product_t_1 = u_product_t * u_t_1\n         beta_2_power = ops.power(beta_2, local_step)\n\n@@ -4,6 +4,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.nadam import Nadam\n \n@@ -20,7 +21,7 @@ class NadamTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Nadam(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -29,7 +30,7 @@ class NadamTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -58,8 +59,8 @@ class NadamTest(testing.TestCase):\n         )\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
