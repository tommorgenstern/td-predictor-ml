{"custom_id": "scrapy#4a2a45b49536332966c205fc527115554a3d2828", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 152 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ class OffsiteMiddleware(object):\n \n     def should_follow(self, request, spider):\n         regex = self.host_regex\n-        # hostanme can be None for wrong urls (like javascript links)\n+        # hostname can be None for wrong urls (like javascript links)\n         host = urlparse_cached(request).hostname or ''\n         return bool(regex.search(host))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e42646ce19edc54c82fd1a7caf38058452b51f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 437 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -109,8 +109,6 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n                 pass\n \n         with warnings.catch_warnings(record=True) as w:\n-            # warns only once on instantations in the same lineno\n-            for _ in range(10):\n             _, lineno = Deprecated(), inspect.getlineno(inspect.currentframe())\n             _ = UserClass()  # subclass instances don't warn\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b3be6e210de837c54fc50cf62fc9e34a504c705e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 41 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 152 | Churn Cumulative: 1425 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,47 +1,50 @@\n \"\"\"\n XPath selectors based on lxml\n \"\"\"\n+from scrapy.utils.deprecate import create_deprecated_class\n from .unified import Selector, SelectorList\n \n \n __all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector',\n            'XPathSelectorList']\n \n-\n-class XPathSelector(Selector):\n-    __slots__ = ()\n-    _default_type = 'html'\n-\n-    def __init__(self, *a, **kw):\n-        import warnings\n-        from scrapy.exceptions import ScrapyDeprecationWarning\n-        warnings.warn('%s is deprecated, instantiate scrapy.selector.Selector '\n-                      'instead' % type(self).__name__,\n-                      category=ScrapyDeprecationWarning, stacklevel=1)\n-        super(XPathSelector, self).__init__(*a, **kw)\n-\n-    def css(self, *a, **kw):\n+def _xpathselector_css(self, *a, **kw):\n     raise RuntimeError('.css() method not available for %s, '\n                         'instantiate scrapy.selector.Selector '\n                         'instead' % type(self).__name__)\n \n+XPathSelector = create_deprecated_class(\n+    'XPathSelector',\n+    Selector,\n+    {\n+        '__slots__': (),\n+        '_default_type': 'html',\n+        'css': _xpathselector_css,\n+    },\n+    new_class_path='scrapy.selector.Selector',\n+    old_class_path='scrapy.selector.XPathSelector',\n+)\n \n-class XmlXPathSelector(XPathSelector):\n-    __slots__ = ()\n-    _default_type = 'xml'\n+XmlXPathSelector = create_deprecated_class(\n+    'XmlXPathSelector',\n+    XPathSelector,\n+    clsdict={\n+        '__slots__': (),\n+        '_default_type': 'xml',\n+    },\n+    new_class_path='scrapy.selector.Selector',\n+    old_class_path='scrapy.selector.XmlXPathSelector',\n+)\n \n+HtmlXPathSelector = create_deprecated_class(\n+    'HtmlXPathSelector',\n+    XPathSelector,\n+    clsdict={\n+        '__slots__': (),\n+        '_default_type': 'html',\n+    },\n+    new_class_path='scrapy.selector.Selector',\n+    old_class_path='scrapy.selector.HtmlXPathSelector',\n+)\n \n-class HtmlXPathSelector(XPathSelector):\n-    __slots__ = ()\n-    _default_type = 'html'\n-\n-\n-class XPathSelectorList(SelectorList):\n-\n-    def __init__(self, *a, **kw):\n-        import warnings\n-        from scrapy.exceptions import ScrapyDeprecationWarning\n-        warnings.warn('XPathSelectorList is deprecated, instantiate '\n-                      'scrapy.selector.SelectorList instead',\n-                      category=ScrapyDeprecationWarning, stacklevel=1)\n-        super(XPathSelectorList, self).__init__(*a, **kw)\n+XPathSelectorList = create_deprecated_class('XPathSelectorList', SelectorList)\n\n@@ -1,4 +1,5 @@\n import re\n+import inspect\n import warnings\n import weakref\n from twisted.trial import unittest\n@@ -301,17 +302,83 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n     text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n \n-    def test_warnings(self):\n-        for cls in XPathSelector, HtmlXPathSelector, XPathSelector:\n+    def test_warnings_xpathselector(self):\n+        cls = XPathSelector\n         with warnings.catch_warnings(record=True) as w:\n-                warnings.simplefilter('always')\n-                hs = cls(text=self.text)\n-                assert len(w) == 1, w\n-                assert issubclass(w[0].category, ScrapyDeprecationWarning)\n-                assert 'deprecated' in str(w[-1].message)\n-                hs.select(\"//div\").extract()\n-                assert issubclass(w[1].category, ScrapyDeprecationWarning)\n-                assert 'deprecated' in str(w[-1].message)\n+            class UserClass(cls):\n+                pass\n+\n+            # subclassing must issue a warning\n+            self.assertEqual(len(w), 1, str(cls))\n+            self.assertIn('scrapy.selector.Selector', str(w[0].message))\n+\n+            # subclass instance doesn't issue a warning\n+            usel = UserClass(text=self.text)\n+            self.assertEqual(len(w), 1)\n+\n+            # class instance must issue a warning\n+            sel = cls(text=self.text)\n+            self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n+            self.assertIn('scrapy.selector.Selector', str(w[1].message))\n+\n+            # subclass and instance checks\n+            self.assertTrue(issubclass(cls, Selector))\n+            self.assertTrue(isinstance(sel, Selector))\n+            self.assertTrue(isinstance(usel, Selector))\n+\n+    def test_warnings_xmlxpathselector(self):\n+        cls = XmlXPathSelector\n+        with warnings.catch_warnings(record=True) as w:\n+            class UserClass(cls):\n+                pass\n+\n+            # subclassing must issue a warning\n+            self.assertEqual(len(w), 1, str(cls))\n+            self.assertIn('scrapy.selector.Selector', str(w[0].message))\n+\n+            # subclass instance doesn't issue a warning\n+            usel = UserClass(text=self.text)\n+            self.assertEqual(len(w), 1)\n+\n+            # class instance must issue a warning\n+            sel = cls(text=self.text)\n+            self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n+            self.assertIn('scrapy.selector.Selector', str(w[1].message))\n+\n+            # subclass and instance checks\n+            self.assertTrue(issubclass(cls, Selector))\n+            self.assertTrue(issubclass(cls, XPathSelector))\n+            self.assertTrue(isinstance(sel, Selector))\n+            self.assertTrue(isinstance(usel, Selector))\n+            self.assertTrue(isinstance(sel, XPathSelector))\n+            self.assertTrue(isinstance(usel, XPathSelector))\n+\n+    def test_warnings_htmlxpathselector(self):\n+        cls = HtmlXPathSelector\n+        with warnings.catch_warnings(record=True) as w:\n+            class UserClass(cls):\n+                pass\n+\n+            # subclassing must issue a warning\n+            self.assertEqual(len(w), 1, str(cls))\n+            self.assertIn('scrapy.selector.Selector', str(w[0].message))\n+\n+            # subclass instance doesn't issue a warning\n+            usel = UserClass(text=self.text)\n+            self.assertEqual(len(w), 1)\n+\n+            # class instance must issue a warning\n+            sel = cls(text=self.text)\n+            self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n+            self.assertIn('scrapy.selector.Selector', str(w[1].message))\n+\n+            # subclass and instance checks\n+            self.assertTrue(issubclass(cls, Selector))\n+            self.assertTrue(issubclass(cls, XPathSelector))\n+            self.assertTrue(isinstance(sel, Selector))\n+            self.assertTrue(isinstance(usel, Selector))\n+            self.assertTrue(isinstance(sel, XPathSelector))\n+            self.assertTrue(isinstance(usel, XPathSelector))\n \n     def test_xpathselector(self):\n         with warnings.catch_warnings(record=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b03fe049990962f254d52aea4f56a148cce62fc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 36 | Churn Cumulative: 320 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -7,21 +7,21 @@ from scrapy.http import HtmlResponse\n from scrapy.utils.response import _noscript_re, _script_re\n from w3lib import html\n \n-class AjaxCrawlableMiddleware(object):\n+class AjaxCrawlMiddleware(object):\n     \"\"\"\n     Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n     For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n     \"\"\"\n \n     def __init__(self, settings):\n-        if not settings.getbool('AJAXCRAWLABLE_ENABLED'):\n+        if not settings.getbool('AJAXCRAWL_ENABLED'):\n             raise NotConfigured\n \n         # XXX: Google parses at least first 100k bytes; scrapy's redirect\n         # middleware parses first 4k. 4k turns out to be insufficient\n         # for this middleware, and parsing 100k could be slow.\n         # We use something in between (32K) by default.\n-        self.lookup_bytes = settings.getint('AJAXCRAWLABLE_MAXSIZE', 32768)\n+        self.lookup_bytes = settings.getint('AJAXCRAWL_MAXSIZE', 32768)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -43,13 +43,13 @@ class AjaxCrawlableMiddleware(object):\n             return response\n \n         # scrapy already handles #! links properly\n-        ajax_crawlable = request.replace(url=request.url+'#!')\n-        log.msg(format=\"Downloading AJAX crawlable %(ajax_crawlable)s instead of %(request)s\",\n-                level=log.DEBUG, spider=spider, ajax_crawlable=ajax_crawlable,\n-                request=request)\n+        ajax_crawl_request = request.replace(url=request.url+'#!')\n+        log.msg(format=\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n+                level=log.DEBUG, spider=spider,\n+                ajax_crawl_request=ajax_crawl_request, request=request)\n \n-        ajax_crawlable.meta['ajax_crawlable'] = True\n-        return ajax_crawlable\n+        ajax_crawl_request.meta['ajax_crawlable'] = True\n+        return ajax_crawl_request\n \n     def _has_ajax_crawlable_variant(self, response):\n         \"\"\"\n\n@@ -18,7 +18,7 @@ import sys\n from importlib import import_module\n from os.path import join, abspath, dirname\n \n-AJAXCRAWLABLE_ENABLED = False\n+AJAXCRAWL_ENABLED = False\n \n BOT_NAME = 'scrapybot'\n \n@@ -81,7 +81,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n     'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n-    'scrapy.contrib.downloadermiddleware.ajaxcrawlable.AjaxCrawlableMiddleware': 560,\n+    'scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware': 560,\n     'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n     'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n\n@@ -1,17 +1,17 @@\n import unittest\n \n-from scrapy.contrib.downloadermiddleware.ajaxcrawlable import AjaxCrawlableMiddleware\n+from scrapy.contrib.downloadermiddleware.ajaxcrawl import AjaxCrawlMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n-__doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawlable']\n+__doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawl']\n \n-class AjaxCrawlableMiddlewareTest(unittest.TestCase):\n+class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n         self.spider = BaseSpider('foo')\n-        crawler = get_crawler({'AJAXCRAWLABLE_ENABLED': True})\n-        self.mw = AjaxCrawlableMiddleware.from_crawler(crawler)\n+        crawler = get_crawler({'AJAXCRAWL_ENABLED': True})\n+        self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n \n     def _ajaxcrawlable_body(self):\n         return '<html><head><meta name=\"fragment\" content=\"!\"/></head><body></body></html>'\n@@ -32,7 +32,7 @@ class AjaxCrawlableMiddlewareTest(unittest.TestCase):\n         resp2 = self.mw.process_response(req, resp, self.spider)\n         self.assertIs(resp, resp2)\n \n-    def test_ajax_crawlable(self):\n+    def test_ajaxcrawl(self):\n         req, resp = self._req_resp(\n             'http://example.com/',\n             {'meta': {'foo': 'bar'}},\n@@ -42,7 +42,7 @@ class AjaxCrawlableMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req2.url, 'http://example.com/?_escaped_fragment_=')\n         self.assertEqual(req2.meta['foo'], 'bar')\n \n-    def test_ajax_crawlable_loop(self):\n+    def test_ajaxcrawl_loop(self):\n         req, resp = self._req_resp('http://example.com/', {}, {'body': self._ajaxcrawlable_body()})\n         req2 = self.mw.process_response(req, resp, self.spider)\n         resp2 = HtmlResponse(req2.url, body=resp.body, request=req2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b7a1d8dfd039bb40d0c48c8bcf8f14d81612ec7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 228 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -145,7 +145,7 @@ FEED_EXPORTERS_BASE = {\n HTTPCACHE_ENABLED = False\n HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n-HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmCacheStorage'\n+HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f184ed7bf5cd32fb2b19bf42d4260d81d4d1cdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 302 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -85,7 +85,8 @@ class Selector(object_ref):\n             return SelectorList([])\n \n         try:\n-            result = xpathev(query, namespaces=self.namespaces)\n+            result = xpathev(query, namespaces=self.namespaces,\n+                             smart_strings=False)\n         except etree.XPathError:\n             raise ValueError(\"Invalid XPath: %s\" % query)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5eb336215c573ae49774cab332a882b3a8ca830f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 305 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -57,6 +57,7 @@ class Selector(object_ref):\n         # set:trailing\n         \"set\": \"http://exslt.org/sets\"\n     }\n+    _lxml_smart_strings = False\n \n     def __init__(self, response=None, text=None, type=None, namespaces=None,\n                  _root=None, _expr=None):\n@@ -86,7 +87,7 @@ class Selector(object_ref):\n \n         try:\n             result = xpathev(query, namespaces=self.namespaces,\n-                             smart_strings=False)\n+                             smart_strings=self._lxml_smart_strings)\n         except etree.XPathError:\n             raise ValueError(\"Invalid XPath: %s\" % query)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#001cf39ff436fb44da2ae5e8b60aaef73991310e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 43 | Churn Cumulative: 998 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -297,6 +297,49 @@ class SelectorTestCase(unittest.TestCase):\n         sel.remove_namespaces()\n         self.assertEqual(len(sel.xpath(\"//link/@type\")), 2)\n \n+    def test_smart_strings(self):\n+        \"\"\"Lxml smart strings return values\"\"\"\n+\n+        class SmartStringsSelector(Selector):\n+            _lxml_smart_strings = True\n+\n+        body = \"\"\"<body>\n+                    <div class='one'>\n+                      <ul>\n+                        <li>one</li><li>two</li>\n+                      </ul>\n+                    </div>\n+                    <div class='two'>\n+                      <ul>\n+                        <li>four</li><li>five</li><li>six</li>\n+                      </ul>\n+                    </div>\n+                  </body>\"\"\"\n+\n+        response = HtmlResponse(url=\"http://example.com\", body=body)\n+\n+        # .getparent() is available for text nodes and attributes\n+        # only when smart_strings are on\n+        x = self.sscls(response)\n+        li_text = x.xpath('//li/text()')\n+        self.assertIs(\n+            any(map(lambda e: hasattr(e._root, 'getparent'), li_text)),\n+            False)\n+        div_class = x.xpath('//div/@class')\n+        self.assertIs(\n+            any(map(lambda e: hasattr(e._root, 'getparent'), div_class)),\n+            False)\n+\n+        x = SmartStringsSelector(response)\n+        li_text = x.xpath('//li/text()')\n+        self.assertIs(\n+            all(map(lambda e: hasattr(e._root, 'getparent'), li_text)),\n+            True)\n+        div_class = x.xpath('//div/@class')\n+        self.assertIs(\n+            all(map(lambda e: hasattr(e._root, 'getparent'), div_class)),\n+            True)\n+\n \n class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0e25aec007a36eaea8d5bc2ea7d43ade1293328", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1014 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -322,23 +322,15 @@ class SelectorTestCase(unittest.TestCase):\n         # only when smart_strings are on\n         x = self.sscls(response)\n         li_text = x.xpath('//li/text()')\n-        self.assertIs(\n-            any(map(lambda e: hasattr(e._root, 'getparent'), li_text)),\n-            False)\n+        self.assertFalse(any(map(lambda e: hasattr(e._root, 'getparent'), li_text)))\n         div_class = x.xpath('//div/@class')\n-        self.assertIs(\n-            any(map(lambda e: hasattr(e._root, 'getparent'), div_class)),\n-            False)\n+        self.assertFalse(any(map(lambda e: hasattr(e._root, 'getparent'), div_class)))\n \n         x = SmartStringsSelector(response)\n         li_text = x.xpath('//li/text()')\n-        self.assertIs(\n-            all(map(lambda e: hasattr(e._root, 'getparent'), li_text)),\n-            True)\n+        self.assertTrue(all(map(lambda e: hasattr(e._root, 'getparent'), li_text)))\n         div_class = x.xpath('//div/@class')\n-        self.assertIs(\n-            all(map(lambda e: hasattr(e._root, 'getparent'), div_class)),\n-            True)\n+        self.assertTrue(all(map(lambda e: hasattr(e._root, 'getparent'), div_class)))\n \n \n class DeprecatedXpathSelectorTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#420efe77b2a6f40dbafdfeda7985934e08907724", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 33 | Churn Cumulative: 1052 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,7 +81,9 @@ class ItemLoader(object):\n     def load_item(self):\n         item = self.item\n         for field_name in self._values:\n-            item[field_name] = self.get_output_value(field_name)\n+            value = self.get_output_value(field_name)\n+            if value is not None:\n+                item[field_name] = value\n         return item\n \n     def get_output_value(self, field_name):\n\n@@ -57,6 +57,35 @@ class BasicItemLoaderTest(unittest.TestCase):\n         item = il.load_item()\n         self.assertEqual(item['name'], [u'Marta'])\n \n+    def test_load_item_ignore_none_field_values(self):\n+        def validate_sku(value):\n+            # Let's assume a SKU is only digits.\n+            if value.isdigit():\n+                return value\n+\n+        class MyLoader(ItemLoader):\n+            name_out = Compose(lambda vs: vs[0])  # take first which allows empty values\n+            price_out = Compose(TakeFirst(), float)\n+            sku_out = Compose(TakeFirst(), validate_sku)\n+\n+        valid_fragment = u'SKU: 1234'\n+        invalid_fragment = u'SKU: not available'\n+        sku_re = 'SKU: (.+)'\n+\n+        il = MyLoader(item={})\n+        # Should not return \"sku: None\".\n+        il.add_value('sku', [invalid_fragment], re=sku_re)\n+        # Should not ignore empty values.\n+        il.add_value('name', u'')\n+        il.add_value('price', [u'0'])\n+        self.assertEqual(il.load_item(), {\n+            'name': u'',\n+            'price': 0.0,\n+        })\n+\n+        il.replace_value('sku', [valid_fragment], re=sku_re)\n+        self.assertEqual(il.load_item()['sku'], u'1234')\n+\n     def test_add_value(self):\n         il = TestItemLoader()\n         il.add_value('name', u'marta')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b93412059d9c80c02688e85fc032a1447a97c4b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 272 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,18 +1,17 @@\n-\"\"\"\n-Item Loader\n+\"\"\"Item Loader\n \n See documentation in docs/topics/loaders.rst\n-\"\"\"\n \n+\"\"\"\n from collections import defaultdict\n-import re\n \n from scrapy.item import Item\n from scrapy.selector import Selector\n-from scrapy.utils.misc import arg_to_iter, extract_regex\n-from scrapy.utils.python import flatten\n from scrapy.utils.decorator import deprecated\n from scrapy.utils.deprecate import create_deprecated_class\n+from scrapy.utils.misc import arg_to_iter, extract_regex\n+from scrapy.utils.python import flatten\n+\n from .common import wrap_loader_context\n from .processor import Identity\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#240fdde6675a4a26518ab14d0e7c5a2208741eed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 652 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from twisted.internet import reactor, threads, defer\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n+from scrapy.crawler import Crawler\n from scrapy.item import BaseItem\n from scrapy.spider import Spider\n from scrapy.selector import Selector\n@@ -24,7 +25,7 @@ from scrapy.exceptions import IgnoreRequest\n \n class Shell(object):\n \n-    relevant_classes = (Spider, Request, Response, BaseItem,\n+    relevant_classes = (Crawler, Spider, Request, Response, BaseItem,\n                         Selector, Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n@@ -91,6 +92,7 @@ class Shell(object):\n         self.populate_vars(response, request, spider)\n \n     def populate_vars(self, response=None, request=None, spider=None):\n+        self.vars['crawler'] = self.crawler\n         self.vars['item'] = self.item_class()\n         self.vars['settings'] = self.crawler.settings\n         self.vars['spider'] = spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4081ba238dad2d048e29a336f26312e4da2eabff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 669 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,10 @@\n-\"\"\"\n-Scrapy Shell\n+\"\"\"Scrapy Shell\n \n See documentation in docs/topics/shell.rst\n+\n \"\"\"\n from __future__ import print_function\n+\n import signal\n \n from twisted.internet import reactor, threads, defer\n@@ -11,16 +12,16 @@ from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n from scrapy.crawler import Crawler\n+from scrapy.exceptions import IgnoreRequest\n+from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n-from scrapy.spider import Spider\n from scrapy.selector import Selector\n-from scrapy.utils.spider import create_spider_for_request\n+from scrapy.settings import Settings\n+from scrapy.spider import Spider\n+from scrapy.utils.console import start_python_console\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n-from scrapy.utils.console import start_python_console\n-from scrapy.settings import Settings\n-from scrapy.http import Request, Response\n-from scrapy.exceptions import IgnoreRequest\n+from scrapy.utils.spider import create_spider_for_request\n \n \n class Shell(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f87859a6271820f264884100e955a31d04f48782", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 52 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,8 @@ class RegexLinkExtractor(SgmlLinkExtractor):\n         clean_text = lambda t: replace_escape_chars(remove_tags(t.decode(response_encoding))).strip()\n \n         links_text = linkre.findall(response_text)\n-        urlstext = set([(clean_url(url), clean_text(text)) for url, _, text in links_text])\n+        urlstext = set([(clean_url(url).encode(response_encoding), clean_text(text))\n+                        for url, _, text in links_text])\n+\n \n         return [Link(url, text) for url, text in urlstext]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66829c962f7521b3a6cc0f9ad10174d1d12f7762", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 9 | Churn Cumulative: 675 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -121,7 +121,7 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n             body = u''.join(f\n                             for x in self.restrict_xpaths\n                             for f in sel.xpath(x).extract()\n-                            ).encode(response.encoding)\n+                            ).encode(response.encoding, errors='xmlcharrefreplace')\n         else:\n             body = response.body\n \n\n@@ -236,6 +236,13 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/about.html', text=u'About us\\xa3')])\n \n+    def test_restrict_xpaths_with_html_entities(self):\n+        html = '<html><body><p><a href=\"/&hearts;/you?c=&euro;\">text</a></p></body></html>'\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='iso8859-15')\n+        links = SgmlLinkExtractor(restrict_xpaths='//p').extract_links(response)\n+        self.assertEqual(links,\n+                         [Link(url='http://example.org/%E2%99%A5/you?c=%E2%82%AC', text=u'text')])\n+\n     def test_restrict_xpaths_concat_in_handle_data(self):\n         \"\"\"html entities cause SGMLParser to call handle_data hook twice\"\"\"\n         body = \"\"\"<html><body><div><a href=\"/foo\">&gt;\\xbe\\xa9&lt;\\xb6\\xab</a></body></html>\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a545157c66a3daf69da1a1510aa1d41942f242b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 166 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.25\n\nDIFF:\n@@ -13,9 +13,12 @@ from scrapy import log\n \n class OffsiteMiddleware(object):\n \n+    def __init__(self, stats=False):\n+        self.stats_enabled = stats\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls()\n+        o = cls(stats=True)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n@@ -30,6 +33,15 @@ class OffsiteMiddleware(object):\n                         self.domains_seen.add(domain)\n                         log.msg(format=\"Filtered offsite request to %(domain)r: %(request)s\",\n                                 level=log.DEBUG, spider=spider, domain=domain, request=x)\n+\n+                        if self.stats_enabled:\n+                            spider.crawler.stats.inc_value('offsite/domains',\n+                                spider=spider)\n+\n+                    if self.stats_enabled:\n+                        spider.crawler.stats.inc_value('offsite/filtered',\n+                            spider=spider)\n+\n             else:\n                 yield x\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd5b40593a4303acbc00d36a5212846a6a375952", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 21 | Churn Cumulative: 277 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,12 +13,12 @@ from scrapy import log\n \n class OffsiteMiddleware(object):\n \n-    def __init__(self, stats=False):\n-        self.stats_enabled = stats\n+    def __init__(self, stats):\n+        self.stats = stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls(stats=True)\n+        o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         return o\n \n@@ -33,15 +33,8 @@ class OffsiteMiddleware(object):\n                         self.domains_seen.add(domain)\n                         log.msg(format=\"Filtered offsite request to %(domain)r: %(request)s\",\n                                 level=log.DEBUG, spider=spider, domain=domain, request=x)\n-\n-                        if self.stats_enabled:\n-                            spider.crawler.stats.inc_value('offsite/domains',\n-                                spider=spider)\n-\n-                    if self.stats_enabled:\n-                        spider.crawler.stats.inc_value('offsite/filtered',\n-                            spider=spider)\n-\n+                        self.stats.inc_value('offsite/domains', spider=spider)\n+                    self.stats.inc_value('offsite/filtered', spider=spider)\n             else:\n                 yield x\n \n\n@@ -3,13 +3,15 @@ from unittest import TestCase\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n+from scrapy.utils.test import get_crawler\n \n \n class TestOffsiteMiddleware(TestCase):\n \n     def setUp(self):\n         self.spider = self._get_spider()\n-        self.mw = OffsiteMiddleware()\n+        crawler = get_crawler()\n+        self.mw = OffsiteMiddleware.from_crawler(crawler)\n         self.mw.spider_opened(self.spider)\n \n     def _get_spider(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc157f182be5db911ca9c2da7bd4f95d4b49e136", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 49 | Files Changed: 10 | Hunks: 39 | Methods Changed: 30 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 137 | Churn Cumulative: 3243 | Contributors (this commit): 24 | Commits (past 90d): 22 | Contributors (cumulative): 53 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,8 +25,6 @@ class RegexLinkExtractor(SgmlLinkExtractor):\n         clean_text = lambda t: replace_escape_chars(remove_tags(t.decode(response_encoding))).strip()\n \n         links_text = linkre.findall(response_text)\n-        urlstext = set([(clean_url(url).encode(response_encoding), clean_text(text))\n-                        for url, _, text in links_text])\n-\n-\n-        return [Link(url, text) for url, text in urlstext]\n+        return [Link(clean_url(url).encode(response_encoding),\n+                     clean_text(text))\n+                for url, _, text in links_text]\n\n@@ -1,5 +1,7 @@\n import unittest, json, cPickle as pickle\n from cStringIO import StringIO\n+import lxml.etree\n+import re\n \n from scrapy.item import Item, Field\n from scrapy.utils.python import str_to_unicode\n@@ -139,8 +141,13 @@ class CsvItemExporterTest(BaseItemExporterTest):\n     def _get_exporter(self, **kwargs):\n         return CsvItemExporter(self.output, **kwargs)\n \n+    def assertCsvEqual(self, first, second, msg=None):\n+        csvsplit = lambda csv: [sorted(re.split(r'(,|\\s+)', line))\n+                                for line in csv.splitlines(True)]\n+        return self.assertEqual(csvsplit(first), csvsplit(second), msg)\n+\n     def _check_output(self):\n-        self.assertEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n+        self.assertCsvEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n     def test_header(self):\n         output = StringIO()\n@@ -148,14 +155,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n-        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n+        self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n         output = StringIO()\n         ie = CsvItemExporter(output, fields_to_export=['age'])\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n-        self.assertEqual(output.getvalue(), 'age\\r\\n22\\r\\n')\n+        self.assertCsvEqual(output.getvalue(), 'age\\r\\n22\\r\\n')\n \n         output = StringIO()\n         ie = CsvItemExporter(output)\n@@ -163,14 +170,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.export_item(self.i)\n         ie.export_item(self.i)\n         ie.finish_exporting()\n-        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n+        self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n         output = StringIO()\n         ie = CsvItemExporter(output, include_headers_line=False)\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n-        self.assertEqual(output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n+        self.assertCsvEqual(output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n \n     def test_join_multivalue(self):\n         class TestItem2(Item):\n@@ -183,16 +190,29 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.start_exporting()\n         ie.export_item(i)\n         ie.finish_exporting()\n-        self.assertEqual(output.getvalue(), '\"Mary,Paul\",John\\r\\n')\n+        self.assertCsvEqual(output.getvalue(), '\"Mary,Paul\",John\\r\\n')\n \n class XmlItemExporterTest(BaseItemExporterTest):\n \n     def _get_exporter(self, **kwargs):\n         return XmlItemExporter(self.output, **kwargs)\n \n+    def assertXmlEquivalent(self, first, second, msg=None):\n+        def xmltuple(elem):\n+            children = list(elem.iterchildren())\n+            if children:\n+                return [(child.tag, sorted(xmltuple(child)))\n+                        for child in children]\n+            else:\n+                return [(elem.tag, [(elem.text, ())])]\n+        def xmlsplit(xmlcontent):\n+            doc = lxml.etree.fromstring(xmlcontent)\n+            return xmltuple(doc)\n+        return self.assertEqual(xmlsplit(first), xmlsplit(second), msg)\n+\n     def _check_output(self):\n         expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>'\n-        self.assertEqual(self.output.getvalue(), expected_value)\n+        self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n \n     def test_multivalued_fields(self):\n         output = StringIO()\n@@ -202,7 +222,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         ie.export_item(item)\n         ie.finish_exporting()\n         expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><name><value>John\\xc2\\xa3</value><value>Doe</value></name></item></items>'\n-        self.assertEqual(output.getvalue(), expected_value)\n+        self.assertXmlEquivalent(output.getvalue(), expected_value)\n \n     def test_nested_item(self):\n         output = StringIO()\n@@ -224,7 +244,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                     '</age>'\\\n                     '<name>buz</name>'\\\n                 '</item></items>'\n-        self.assertEqual(output.getvalue(), expected_value)\n+        self.assertXmlEquivalent(output.getvalue(), expected_value)\n \n     def test_nested_list_item(self):\n         output = StringIO()\n@@ -243,7 +263,8 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                     '</age>'\\\n                     '<name>buz</name>'\\\n                 '</item></items>'\n-        self.assertEqual(output.getvalue(), expected_value)\n+        self.assertXmlEquivalent(output.getvalue(), expected_value)\n+\n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n\n@@ -312,12 +312,11 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n     def test_extraction(self):\n         # Default arguments\n         lx = HtmlParserLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(self.response)], [\n-            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        self.assertEqual(lx.extract_links(self.response),\n+                         [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                           Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-            Link(url='http://example.com/sample3.html', text='sample 3 repetition'),\n-            Link(url='http://www.google.com/something', text=''),\n-        ])\n+                          Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n+                          Link(url='http://www.google.com/something', text=u''),])\n \n \n class RegexLinkExtractorTestCase(unittest.TestCase):\n@@ -329,13 +328,10 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n     def test_extraction(self):\n         # Default arguments\n         lx = RegexLinkExtractor()\n-        # Note that RegexLinkExtractor returns links in arbitrary order,\n-        # so we need to sort them for comparison\n-        self.assertEqual(sorted(lx.extract_links(self.response), key=lambda x: x.url), [\n-            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-            Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n-            Link(url='http://www.google.com/something', text=u''),\n-        ])\n+        self.assertEqual(lx.extract_links(self.response),\n+                         [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+                          Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+                          Link(url='http://www.google.com/something', text=u''),])\n \n \n if __name__ == \"__main__\":\n\n@@ -24,21 +24,24 @@ if 'django' in optional_features:\n \n class DjangoItemTest(unittest.TestCase):\n \n+    def assertSortedEqual(self, first, second, msg=None):\n+        return self.assertEqual(sorted(first), sorted(second), msg)\n+\n     def setUp(self):\n         if 'django' not in optional_features:\n             raise unittest.SkipTest(\"Django is not available\")\n \n     def test_base(self):\n         i = BasePersonItem()\n-        self.assertEqual(i.fields.keys(), ['age', 'name'])\n+        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])\n \n     def test_new_fields(self):\n         i = NewFieldPersonItem()\n-        self.assertEqual(i.fields.keys(), ['age', 'other', 'name'])\n+        self.assertSortedEqual(i.fields.keys(), ['age', 'other', 'name'])\n \n     def test_override_field(self):\n         i = OverrideFieldPersonItem()\n-        self.assertEqual(i.fields.keys(), ['age', 'name'])\n+        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])\n \n     def test_custom_primary_key_field(self):\n         \"\"\"\n@@ -46,11 +49,11 @@ class DjangoItemTest(unittest.TestCase):\n         in the field list.\n         \"\"\"\n         i = IdentifiedPersonItem()\n-        self.assertEqual(i.fields.keys(), ['age', 'identifier', 'name'])\n+        self.assertSortedEqual(i.fields.keys(), ['age', 'identifier', 'name'])\n \n     def test_save(self):\n         i = BasePersonItem()\n-        self.assertEqual(i.fields.keys(), ['age', 'name'])\n+        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])\n \n         i['name'] = 'John'\n         i['age'] = '22'\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.contrib.downloadermiddleware.ajaxcrawl import AjaxCrawlMiddleware\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n@@ -9,7 +9,7 @@ __doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawl']\n \n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         crawler = get_crawler({'AJAXCRAWL_ENABLED': True})\n         self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n \n\n@@ -1,4 +1,5 @@\n from unittest import TestCase\n+import re\n \n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n@@ -7,6 +8,12 @@ from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n class CookiesMiddlewareTest(TestCase):\n \n+    def assertCookieValEqual(self, first, second, msg=None):\n+        cookievaleq = lambda cv: re.split(';\\s*', cv)\n+        return self.assertEqual(\n+            sorted(cookievaleq(first)),\n+            sorted(cookievaleq(second)), msg)\n+\n     def setUp(self):\n         self.spider = Spider('foo')\n         self.mw = CookiesMiddleware()\n@@ -86,7 +93,8 @@ class CookiesMiddlewareTest(TestCase):\n \n         req2 = Request('http://scrapytest.org/sub1/')\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertEquals(req2.headers.get('Cookie'), \"C1=value1; galleta=salada\")\n+\n+        self.assertCookieValEqual(req2.headers.get('Cookie'), \"C1=value1; galleta=salada\")\n \n     def test_cookiejar_key(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'}, meta={'cookiejar': \"store1\"})\n@@ -99,8 +107,7 @@ class CookiesMiddlewareTest(TestCase):\n \n         req2 = Request('http://scrapytest.org/', meta=res.meta)\n         assert self.mw.process_request(req2, self.spider) is None\n-        self.assertEquals(req2.headers.get('Cookie'), 'C1=value1; galleta=salada')\n-\n+        self.assertCookieValEqual(req2.headers.get('Cookie'),'C1=value1; galleta=salada')\n \n         req3 = Request('http://scrapytest.org/', cookies={'galleta': 'dulce'}, meta={'cookiejar': \"store2\"})\n         assert self.mw.process_request(req3, self.spider) is None\n@@ -112,7 +119,7 @@ class CookiesMiddlewareTest(TestCase):\n \n         req4 = Request('http://scrapytest.org/', meta=res2.meta)\n         assert self.mw.process_request(req4, self.spider) is None\n-        self.assertEquals(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')\n+        self.assertCookieValEqual(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')\n \n         #cookies from hosts with port\n         req5_1 = Request('http://scrapytest.org:1104/')\n\n@@ -4,6 +4,10 @@ import copy\n from scrapy.http import Headers\n \n class HeadersTest(unittest.TestCase):\n+\n+    def assertSortedEqual(self, first, second, msg=None):\n+        return self.assertEqual(sorted(first), sorted(second), msg)\n+\n     def test_basics(self):\n         h = Headers({'Content-Type': 'text/html', 'Content-Length': 1234})\n         assert h['Content-Type']\n@@ -75,13 +79,13 @@ class HeadersTest(unittest.TestCase):\n         idict = {'Content-Type': 'text/html', 'X-Forwarded-For': ['ip1', 'ip2']}\n \n         h = Headers(idict)\n-        self.assertEqual(dict(h), {'Content-Type': ['text/html'], 'X-Forwarded-For': ['ip1', 'ip2']})\n-        self.assertEqual(h.keys(), ['X-Forwarded-For', 'Content-Type'])\n-        self.assertEqual(h.items(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n-        self.assertEqual(list(h.iteritems()),\n-                [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n+        self.assertDictEqual(dict(h),\n+                             {'Content-Type': ['text/html'], 'X-Forwarded-For': ['ip1', 'ip2']})\n+        self.assertSortedEqual(h.keys(), ['X-Forwarded-For', 'Content-Type'])\n+        self.assertSortedEqual(h.items(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n+        self.assertSortedEqual(h.iteritems(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n \n-        self.assertEqual(h.values(), ['ip2', 'text/html'])\n+        self.assertSortedEqual(h.values(), ['ip2', 'text/html'])\n \n     def test_update(self):\n         h = Headers()\n\n@@ -184,6 +184,9 @@ class FormRequestTest(RequestTest):\n \n     request_class = FormRequest\n \n+    def assertSortedEqual(self, first, second, msg=None):\n+        return self.assertEqual(sorted(first), sorted(second), msg)\n+\n     def test_empty_formdata(self):\n         r1 = self.request_class(\"http://www.example.com\", formdata={})\n         self.assertEqual(r1.body, '')\n@@ -194,7 +197,8 @@ class FormRequestTest(RequestTest):\n         r2 = self.request_class(\"http://www.example.com\", formdata=data)\n         self.assertEqual(r2.method, 'POST')\n         self.assertEqual(r2.encoding, 'utf-8')\n-        self.assertEqual(r2.body, 'price=%C2%A3+100&one=two')\n+        self.assertSortedEqual(r2.body.split('&'),\n+                               'price=%C2%A3+100&one=two'.split('&'))\n         self.assertEqual(r2.headers['Content-Type'], 'application/x-www-form-urlencoded')\n \n     def test_custom_encoding(self):\n@@ -207,7 +211,8 @@ class FormRequestTest(RequestTest):\n         # using multiples values for a single key\n         data = {'price': u'\\xa3 100', 'colours': ['red', 'blue', 'green']}\n         r3 = self.request_class(\"http://www.example.com\", formdata=data)\n-        self.assertEqual(r3.body, 'colours=red&colours=blue&colours=green&price=%C2%A3+100')\n+        self.assertSortedEqual(r3.body.split('&'),\n+            'colours=red&colours=blue&colours=green&price=%C2%A3+100'.split('&'))\n \n     def test_from_response_post(self):\n         response = _buildresponse(\n\n@@ -5,6 +5,9 @@ from scrapy.item import Item, Field\n \n class ItemTest(unittest.TestCase):\n \n+    def assertSortedEqual(self, first, second, msg=None):\n+        return self.assertEqual(sorted(first), sorted(second), msg)\n+\n     def test_simple(self):\n         class TestItem(Item):\n             name = Field()\n@@ -108,8 +111,8 @@ class ItemTest(unittest.TestCase):\n \n         i['keys'] = u'Keys'\n         i['values'] = u'Values'\n-        self.assertEqual(i.keys(), ['keys', 'values', 'name'])\n-        self.assertEqual(i.values(), [u'Keys', u'Values', u'John'])\n+        self.assertSortedEqual(i.keys(), ['keys', 'values', 'name'])\n+        self.assertSortedEqual(i.values(), [u'Keys', u'Values', u'John'])\n \n     def test_metaclass_inheritance(self):\n         class BaseItem(Item):\n\n@@ -169,7 +169,9 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         protocol = client.ScrapyHTTPPageGetter()\n         protocol.factory = factory\n         protocol.makeConnection(transport)\n-        self.assertEqual(transport.value(), testvalue)\n+        self.assertEqual(\n+            set(transport.value().splitlines()),\n+            set(testvalue.splitlines()))\n         return testvalue\n \n     def test_non_standard_line_endings(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
