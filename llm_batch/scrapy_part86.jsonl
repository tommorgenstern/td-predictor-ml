{"custom_id": "scrapy#6405efa5073e2e3c37a997619acb95418a0ce571", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 68 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,9 +16,7 @@ class RedirectMiddleware(object):\n     def process_response(self, request, response, spider):\n         if response.status in [302, 303] and 'Location' in response.headers:\n             redirected_url = urljoin_rfc(request.url, response.headers['location'])\n-            redirected = request.replace(url=redirected_url, method='GET', body='')\n-            redirected.headers.pop('Content-Type', None)\n-            redirected.headers.pop('Content-Length', None)\n+            redirected = self._redirect_request_using_get(request, redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n         if response.status in [301, 307] and 'Location' in response.headers:\n@@ -28,9 +26,7 @@ class RedirectMiddleware(object):\n \n         interval, url = get_meta_refresh(response)\n         if url and interval < self.max_metarefresh_delay:\n-            redirected = request.replace(url=url, method='GET', body='')\n-            redirected.headers.pop('Content-Type', None)\n-            redirected.headers.pop('Content-Length', None)\n+            redirected = self._redirect_request_using_get(request, url)\n             return self._redirect(redirected, request, spider, 'meta refresh')\n \n         return response\n@@ -52,4 +48,10 @@ class RedirectMiddleware(object):\n                     domain=spider.domain_name, level=log.DEBUG)\n             raise IgnoreRequest\n \n+    def _redirect_request_using_get(self, request, redirect_url):\n+        redirected = request.replace(url=redirect_url, method='GET', body='')\n+        redirected.headers.pop('Content-Type', None)\n+        redirected.headers.pop('Content-Length', None)\n+        return redirected\n+\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d7a4c890e25d0e145c6f6f5e1e675aacb67c3ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 201 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,6 +93,12 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = Response(url='http://example.com', body=body)\n         self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/thisTHAT'))\n \n+        # responses without refresh tag should return None None\n+        response = Response(url='http://example.org')\n+        self.assertEqual(get_meta_refresh(response), (None, None))\n+        response = TextResponse(url='http://example.org')\n+        self.assertEqual(get_meta_refresh(response), (None, None))\n+\n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n@@ -47,7 +47,7 @@ def get_meta_refresh(response):\n     If no meta redirect is found, (None, None) is returned.\n     \"\"\"\n     if response not in _metaref_cache:\n-        encoding = getattr(response, 'encoding', 'utf-8')\n+        encoding = getattr(response, 'encoding', None) or 'utf-8'\n         body_chunk = remove_entities(unicode(response.body[0:4096], encoding, \\\n             errors='ignore'))\n         match = META_REFRESH_RE.search(body_chunk)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7296a7b88915c2e22ae59de2b6009ab778c615d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 540 | Contributors (this commit): 3 | Commits (past 90d): 41 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,6 +37,8 @@ DEFAULT_REQUEST_HEADERS = {\n     'Accept-Language': 'en',\n }\n \n+DEFAULT_RESPONSE_ENCODING = 'ascii'\n+\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n\n@@ -11,9 +11,11 @@ from scrapy.xlib.BeautifulSoup import UnicodeDammit\n \n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n+from scrapy.conf import settings\n \n class TextResponse(Response):\n \n+    _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']\n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n     __slots__ = ['_encoding', '_body_inferred_encoding']\n@@ -71,6 +73,8 @@ class TextResponse(Response):\n             self._body_declared_encoding())\n         dammit = UnicodeDammit(self.body, possible_encodings)\n         self._body_inferred_encoding = dammit.originalEncoding\n+        if self._body_inferred_encoding in ('ascii', None):\n+            self._body_inferred_encoding = self._DEFAULT_ENCODING\n         return dammit.unicode\n \n     def body_encoding(self):\n\n@@ -2,6 +2,7 @@ import unittest\n import weakref\n \n from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers\n+from scrapy.conf import settings\n \n \n class BaseResponseTest(unittest.TestCase):\n@@ -138,8 +139,10 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r3.encoding, \"latin1\")\n \n     def test_unicode_url(self):\n-        # instantiate with unicode url without encoding\n-        self.assertRaises(TypeError, self.response_class, u\"http://www.example.com/\")\n+        # instantiate with unicode url without encoding (should set default encoding)\n+        resp = self.response_class(u\"http://www.example.com/\")\n+        self.assertEqual(resp.encoding, settings['DEFAULT_RESPONSE_ENCODING'])\n+\n         # make sure urls are converted to str\n         resp = self.response_class(url=u\"http://www.example.com/\", encoding='utf-8')\n         assert isinstance(resp.url, str)\n@@ -187,7 +190,6 @@ class TextResponseTest(BaseResponseTest):\n         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n         self.assertRaises(TypeError, self.response_class, \"http://www.example.com\", body=u\"\\xa3\")\n \n-\n class HtmlResponseTest(TextResponseTest):\n \n     response_class = HtmlResponse\n@@ -229,8 +231,7 @@ class XmlResponseTest(TextResponseTest):\n \n         body = \"<xml></xml>\"\n         r1 = self.response_class(\"http://www.example.com\", body=body)\n-        # XXX: we may want to swtich default XmlResponse encoding to utf-8\n-        self._assert_response_values(r1, 'ascii', body)\n+        self._assert_response_values(r1, settings['DEFAULT_RESPONSE_ENCODING'], body)\n \n         body = \"\"\"<?xml version=\"1.0\" encoding=\"iso-8859-1\"?><xml></xml>\"\"\"\n         r2 = self.response_class(\"http://www.example.com\", body=body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53ae45ad3d5d02009018b07351e97b654c720f7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,12 +26,15 @@ class Libxml2Document(object_ref):\n         # we must call both cleanup functions, so we try/except all exceptions\n         # to make sure one doesn't prevent the other from being called\n         # this call sometimes raises a \"NoneType is not callable\" TypeError\n-        # also, these calls sometimes raise a \"NoneType is not callable\"\n-        # TypeError, so the try/except block silences them\n+        # so the try/except block silences them\n         try:\n             self.xmlDoc.freeDoc()\n-        finally:\n+        except:\n+            pass\n+        try:\n             self.xpathContext.xpathFreeContext()\n+        except:\n+            pass\n \n     def __str__(self):\n         return \"<Libxml2Document %s>\" % self.xmlDoc.name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7baff2914a3e059845c114ad05f0b92bfe0b6be1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 34/26 | Churn Δ: 10 | Churn Cumulative: 10 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,7 +24,7 @@ def body_as_utf8(response):\n         \n def xmlDoc_from_html(response):\n     \"\"\"Return libxml2 doc for HTMLs\"\"\"\n-    utf8body = body_as_utf8(response)\n+    utf8body = body_as_utf8(response) or ' '\n     try:\n         lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', \\\n             html_parser_options)\n@@ -35,7 +35,7 @@ def xmlDoc_from_html(response):\n \n def xmlDoc_from_xml(response):\n     \"\"\"Return libxml2 doc for XMLs\"\"\"\n-    utf8body = body_as_utf8(response)\n+    utf8body = body_as_utf8(response) or ' '\n     try:\n         lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', \\\n             xml_parser_options)\n\n@@ -241,6 +241,12 @@ class XPathSelectorTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n+    @libxml2debug\n+    def test_empty_bodies(self):\n+        r1 = TextResponse('http://www.example.com', body='')\n+        hxs = HtmlXPathSelector(r1) # shouldn't raise error\n+        xxs = XmlXPathSelector(r1) # shouldn't raise error\n+\n     @libxml2debug\n     def test_weakref_slots(self):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a244d23b89f346c05286e2ec8c82feead316381d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 264 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -37,7 +37,8 @@ class FormRequest(Request):\n             self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n \n     @classmethod\n-    def from_response(cls, response, formnumber=0, formdata=None, clickdata=None, **kwargs):\n+    def from_response(cls, response, formnumber=0, formdata=None, \n+                      clickdata=None, dont_click=False, **kwargs):\n         encoding = getattr(response, 'encoding', 'utf-8')\n         forms = ParseFile(StringIO(response.body), response.url,\n                           encoding=encoding, backwards_compat=False)\n@@ -56,5 +57,9 @@ class FormRequest(Request):\n                 for v2 in v if hasattr(v, '__iter__') else [v]:\n                     form.new_control('text', k, {'value': v2})\n         \n+        if not dont_click:\n             url, body, headers = form.click_request_data(**(clickdata or {}))\n+        else:\n+            url, body, headers = form._switch_click('request_data')\n+\n         return cls(url, method=form.method, body=body, headers=headers, **kwargs)\n\n@@ -292,6 +292,21 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(urlargs['one'], ['1'])\n         self.assertEqual(urlargs['two'], ['2'])\n \n+    def test_from_response_dont_click(self):\n+        respbody = \"\"\"\n+<form action=\"get.php\" method=\"GET\">\n+<input type=\"submit\" name=\"clickeable1\" value=\"clicked1\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+<input type=\"hidden\" name=\"two\" value=\"3\">\n+<input type=\"submit\" name=\"clickeable2\" value=\"clicked2\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, dont_click=True)\n+        urlargs = cgi.parse_qs(urlparse(r1.url).query)\n+        self.assertFalse('clickeable1' in urlargs, urlargs)\n+        self.assertFalse('clickeable2' in urlargs, urlargs)\n+\n     def test_from_response_errors_noform(self):\n         respbody = \"\"\"<html></html>\"\"\"\n         response = Response(\"http://www.example.com/lala.html\", body=respbody)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7a00cd2a1b2333ed485bcc7c92936e359025610", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -90,7 +90,6 @@ class CrawlSpider(InitSpider):\n         matching each case, filters them (if needed), and returns a list of unique\n         requests per response.\n         \"\"\"\n-        requests = []\n         seen = set()\n         for rule in self._rules:\n             links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n@@ -101,8 +100,7 @@ class CrawlSpider(InitSpider):\n                 r = Request(url=link.url)\n                 r.meta['link_text'] = link.text\n                 r.deferred.addCallback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n-                requests.append(r)\n-        return requests\n+                yield r\n \n     def _response_downloaded(self, response, callback, cb_kwargs, follow):\n         \"\"\"\n@@ -110,15 +108,16 @@ class CrawlSpider(InitSpider):\n         to extract links or not from it, and if it will be parsed or not.\n         It returns a list of requests/items.\n         \"\"\"\n-        res = []\n-\n-        if follow and settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n-            res.extend(self._requests_to_follow(response))\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n             cb_res = self.process_results(response, cb_res)\n-            res.extend(iterate_spider_output(cb_res))\n-        return res\n+            for requests_or_item in iterate_spider_output(cb_res):\n+                yield requests_or_item\n+\n+        if follow and settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n+            for request_or_item in self._requests_to_follow(response):\n+                yield request_or_item\n+                \n \n     def _compile_rules(self):\n         \"\"\"Compile the crawling rules\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77b8cfadc77254e30fdfb687fd8ada4eaa00c4f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 31 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -57,9 +57,9 @@ class FormRequest(Request):\n                 for v2 in v if hasattr(v, '__iter__') else [v]:\n                     form.new_control('text', k, {'value': v2})\n         \n-        if not dont_click:\n-            url, body, headers = form.click_request_data(**(clickdata or {}))\n-        else:\n+        if dont_click:\n             url, body, headers = form._switch_click('request_data')\n+        else:\n+            url, body, headers = form.click_request_data(**(clickdata or {}))\n \n         return cls(url, method=form.method, body=body, headers=headers, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5ae94df55cdf2a25422bd5ac040cc7d257b555b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 32 | Churn Cumulative: 101 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 0.4375\n\nDIFF:\n@@ -34,15 +34,24 @@ log_level = DEBUG\n \n started = False\n \n+def _get_log_level(level_name_or_id=None):\n+    if level_name_or_id is None:\n+        lvlname = settings['LOG_LEVEL'] or settings['LOGLEVEL']\n+        return globals()[lvlname]\n+    elif isinstance(level_name_or_id, int) and 0 <= level_name_or_id <= 5:\n+        return level_name_or_id\n+    elif isinstance(level_name_or_id, basestring):\n+        return globals()[level_name_or_id]\n+    else:\n+        raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n+\n def start(logfile=None, loglevel=None, logstdout=None):\n     \"\"\"Initialize and start logging facility\"\"\"\n     global log_level, started\n \n-    # set loglevel\n-    loglevel = loglevel or settings['LOG_LEVEL'] or settings['LOGLEVEL']\n-    log_level = globals()[loglevel] if loglevel else DEBUG\n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n+    log_level = _get_log_level(loglevel)\n     started = True\n \n     # set log observer\n\n@@ -0,0 +1,17 @@\n+import unittest\n+\n+from scrapy import log\n+from scrapy.conf import settings\n+\n+class ItemTest(unittest.TestCase):\n+\n+    def test_get_log_level(self):\n+        default_log_level = getattr(log, settings['LOG_LEVEL'])\n+        self.assertEqual(log._get_log_level(), default_log_level)\n+        self.assertEqual(log._get_log_level('WARNING'), log.WARNING)\n+        self.assertEqual(log._get_log_level(log.WARNING), log.WARNING)\n+        self.assertRaises(ValueError, log._get_log_level, 99999)\n+        self.assertRaises(ValueError, log._get_log_level, object())\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65b60653236c61062155df55d080e218e59090ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 142 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class StatsCollectorTest(unittest.TestCase):\n     def test_signals(self):\n         signals_catched = set()\n \n-        def domain_open(domain):\n+        def domain_opened(domain):\n             assert domain == 'example.com'\n             signals_catched.add(stats_domain_opened)\n \n@@ -66,7 +66,7 @@ class StatsCollectorTest(unittest.TestCase):\n             assert domain_stats == {'test': 1}\n             signals_catched.add(stats_domain_closed)\n \n-        dispatcher.connect(domain_open, signal=stats_domain_opened)\n+        dispatcher.connect(domain_opened, signal=stats_domain_opened)\n         dispatcher.connect(domain_closing, signal=stats_domain_closing)\n         dispatcher.connect(domain_closed, signal=stats_domain_closed)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3f29339128fdd10d4a4831708b4538ceb0ccaa5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 144 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,7 @@ class StatsCollectorTest(unittest.TestCase):\n         assert stats_domain_closing in signals_catched\n         assert stats_domain_closed in signals_catched\n \n-        dispatcher.disconnect(domain_open, signal=stats_domain_opened)\n+        dispatcher.disconnect(domain_opened, signal=stats_domain_opened)\n         dispatcher.disconnect(domain_closing, signal=stats_domain_closing)\n         dispatcher.disconnect(domain_closed, signal=stats_domain_closed)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97c322707a77c225a9d53969b9ea801b5f901e97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 132 | Lines Deleted: 145 | Files Changed: 20 | Hunks: 92 | Methods Changed: 92 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 277 | Churn Cumulative: 3114 | Contributors (this commit): 5 | Commits (past 90d): 101 | Contributors (cumulative): 42 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,12 +23,12 @@ class CloseDomain(object):\n         self.tasks = {}\n \n         if self.timeout:\n-            dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n+            dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         if self.itempassed:\n             dispatcher.connect(self.item_passed, signal=signals.item_passed)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n-    def domain_opened(self, spider):\n+    def spider_opened(self, spider):\n         self.tasks[spider] = reactor.callLater(self.timeout, scrapyengine.close_spider, \\\n             spider=spider, reason='closedomain_timeout')\n         \n@@ -37,7 +37,7 @@ class CloseDomain(object):\n         if self.counts[spider] == self.itempassed:\n             scrapyengine.close_spider(spider, 'closedomain_itempassed')\n \n-    def domain_closed(self, spider):\n+    def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n         tsk = self.tasks.pop(spider, None)\n         if tsk and not tsk.called:\n\n@@ -21,19 +21,19 @@ class DelayedCloseDomain(object):\n             raise NotConfigured\n \n         self.opened_at = defaultdict(time)\n-        dispatcher.connect(self.domain_idle, signal=signals.domain_idle)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_idle, signal=signals.spider_idle)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n-    def domain_idle(self, domain):\n+    def spider_idle(self, spider):\n         try:\n-            lastseen = scrapyengine.downloader.sites[domain].lastseen\n+            lastseen = scrapyengine.downloader.sites[spider].lastseen\n         except KeyError:\n             lastseen = None\n         if not lastseen:\n-            lastseen = self.opened_at[domain]\n+            lastseen = self.opened_at[spider]\n \n         if time() < lastseen + self.delay:\n             raise DontCloseDomain\n \n-    def domain_closed(self, domain):\n-        self.opened_at.pop(domain, None)\n+    def spider_closed(self, spider):\n+        self.opened_at.pop(spider, None)\n\n@@ -16,13 +16,13 @@ class CookiesMiddleware(object):\n \n     def __init__(self):\n         self.jars = defaultdict(CookieJar)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n+        dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n     def process_request(self, request, spider):\n         if request.meta.get('dont_merge_cookies', False):\n             return\n \n-        jar = self.jars[spider.domain_name]\n+        jar = self.jars[spider]\n         cookies = self._get_request_cookies(jar, request)\n         for cookie in cookies:\n             jar.set_cookie_if_ok(cookie, request)\n@@ -37,14 +37,14 @@ class CookiesMiddleware(object):\n             return response\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n-        jar = self.jars[spider.domain_name]\n+        jar = self.jars[spider]\n         jar.extract_cookies(response, request)\n         self._debug_set_cookie(response)\n \n         return response\n \n-    def domain_closed(self, domain):\n-        self.jars.pop(domain, None)\n+    def spider_closed(self, spider):\n+        self.jars.pop(spider, None)\n \n     def _debug_cookie(self, request):\n         \"\"\"log Cookie header for request\"\"\"\n\n@@ -24,10 +24,10 @@ class HttpCacheMiddleware(object):\n             raise NotConfigured\n         self.cache = Cache(settings['HTTPCACHE_DIR'], sectorize=settings.getbool('HTTPCACHE_SECTORIZE'))\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n-        dispatcher.connect(self.open_domain, signal=signals.domain_opened)\n+        dispatcher.connect(self.open_domain, signal=signals.spider_opened)\n \n-    def open_domain(self, domain):\n-        self.cache.open_domain(domain)\n+    def open_domain(self, spider):\n+        self.cache.open_domain(spider.domain_name)\n \n     def process_request(self, request, spider):\n         if not is_cacheable(request):\n\n@@ -26,8 +26,8 @@ class RobotsTxtMiddleware(object):\n         self._spider_netlocs = {}\n         self._useragents = {}\n         self._pending = {}\n-        dispatcher.connect(self.domain_opened, signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n     def process_request(self, request, spider):\n         useragent = self._useragents[spider]\n@@ -52,12 +52,12 @@ class RobotsTxtMiddleware(object):\n         rp.parse(response.body.splitlines())\n         self._parsers[urlparse_cached(response).netloc] = rp\n \n-    def domain_opened(self, spider):\n+    def spider_opened(self, spider):\n         self._spider_netlocs[spider] = set()\n         self._useragents[spider] = getattr(spider, 'user_agent', None) \\\n             or settings['USER_AGENT']\n \n-    def domain_closed(self, domain, spider):\n+    def spider_closed(self, spider):\n         for netloc in self._spider_netlocs[domain]:\n             del self._parsers[netloc]\n         del self._spider_netlocs[domain]\n\n@@ -49,7 +49,7 @@ class ItemSamplerPipeline(object):\n         self.items = {}\n         self.domains_count = 0\n         self.empty_domains = set()\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n     def process_item(self, item, spider):\n@@ -70,7 +70,8 @@ class ItemSamplerPipeline(object):\n         if self.empty_domains:\n             log.msg(\"No products sampled for: %s\" % \" \".join(self.empty_domains), level=log.WARNING)\n \n-    def domain_closed(self, domain, spider, reason):\n+    def spider_closed(self, spider, reason):\n+        domain = spider.domain_name\n         if reason == 'finished' and not stats.get_value(\"items_sampled\", domain=domain):\n             self.empty_domains.add(domain)\n         self.domains_count += 1\n\n@@ -44,10 +44,10 @@ class FSImagesStore(object):\n         self.basedir = basedir\n         self._mkdir(self.basedir)\n         self.created_directories = defaultdict(set)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n+        dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n-    def domain_closed(self, domain):\n-        self.created_directories.pop(domain, None)\n+    def spider_closed(self, spider):\n+        self.created_directories.pop(spider.domain_name, None)\n \n     def persist_image(self, key, image, buf, info):\n         absolute_path = self._get_filesystem_path(key)\n\n@@ -22,14 +22,14 @@ class MediaPipeline(object):\n \n     def __init__(self):\n         self.domaininfo = {}\n-        dispatcher.connect(self.domain_opened, signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n-    def domain_opened(self, spider):\n+    def spider_opened(self, spider):\n         self.domaininfo[spider.domain_name] = self.DomainInfo(spider)\n \n-    def domain_closed(self, domain):\n-        del self.domaininfo[domain]\n+    def spider_closed(self, spider):\n+        del self.domaininfo[spider.domain_name]\n \n     def process_item(self, domain, item):\n         info = self.domaininfo[domain]\n\n@@ -16,14 +16,14 @@ class CachingResolver(object):\n         self.resolver = _CachingThreadedResolver(reactor)\n         reactor.installResolver(self.resolver)\n         dispatcher.connect(self.request_received, signals.request_received)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def request_received(self, request, spider):\n         url_hostname = urlparse_cached(request).hostname\n-        self.spider_hostnames[spider.domain_name].add(url_hostname)\n+        self.spider_hostnames[spider].add(url_hostname)\n \n-    def domain_closed(self, spider):\n-        for hostname in self.spider_hostnames:\n+    def spider_closed(self, spider):\n+        for hostname in self.spider_hostnames[spider]:\n             self.resolver._cache.pop(hostname, None)\n \n \n\n@@ -15,8 +15,8 @@ class OffsiteMiddleware(object):\n \n     def __init__(self):\n         self.host_regexes = {}\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def process_spider_output(self, response, result, spider):\n         return (x for x in result if not isinstance(x, Request) or \\\n@@ -34,9 +34,9 @@ class OffsiteMiddleware(object):\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)\n \n-    def domain_opened(self, spider):\n+    def spider_opened(self, spider):\n         domains = [spider.domain_name] + spider.extra_domain_names\n         self.host_regexes[spider] = self.get_host_regex(domains)\n \n-    def domain_closed(self, spider):\n+    def spider_closed(self, spider):\n         del self.host_regexes[spider]\n\n@@ -23,44 +23,43 @@ class RequestLimitMiddleware(object):\n         self.max_pending = {}\n         self.dropped_count = {}\n \n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n-    def domain_opened(self, domain, spider):\n-        self.max_pending[domain] = getattr(spider, 'requests_queue_size', self.max_queue_size)\n-        self.dropped_count[domain] = 0\n+    def spider_opened(self, spider):\n+        self.max_pending[spider] = getattr(spider, 'requests_queue_size', self.max_queue_size)\n+        self.dropped_count[spider] = 0\n \n-    def domain_closed(self, domain):\n-        dropped_count = self.dropped_count[domain]\n+    def spider_closed(self, spider):\n+        dropped_count = self.dropped_count[spider]\n         if dropped_count:\n-            max_pending = self.max_pending[domain]\n+            max_pending = self.max_pending[spider]\n             log.msg('Dropped %d request(s) because the scheduler queue size limit (%d requests) was exceeded' % \\\n-                    (dropped_count, max_pending), level=log.DEBUG, domain=domain)\n-        del self.dropped_count[domain]\n-        del self.max_pending[domain]\n+                    (dropped_count, max_pending), level=log.DEBUG, spider=spider)\n+        del self.dropped_count[spider]\n+        del self.max_pending[spider]\n \n     def process_spider_output(self, response, result, spider):\n-        domain = spider.domain_name\n-        max_pending = self.max_pending.get(domain, 0)\n+        max_pending = self.max_pending.get(spider, 0)\n         if max_pending:\n-            return imap(lambda v: self._limit_requests(v, domain, max_pending), result)\n+            return imap(lambda v: self._limit_requests(v, spider, max_pending), result)\n         else:\n             return result\n \n-    def _limit_requests(self, request_or_other, domain, max_pending):\n+    def _limit_requests(self, request_or_other, spider, max_pending):\n         if isinstance(request_or_other, Request):\n-            free_slots = max_pending - self._pending_count(domain)\n+            free_slots = max_pending - self._pending_count(spider)\n             if free_slots > 0:\n                 # Scheduler isn't saturated and it is fine to schedule more requests.\n                 return request_or_other\n             else:\n                 # Skip the request and give engine time to handle other tasks.\n-                self.dropped_count[domain] += 1\n+                self.dropped_count[spider] += 1\n                 return None\n         else:\n             # Return others (non-requests) as is.\n             return request_or_other\n \n-    def _pending_count(self, domain):\n-        pending = scrapyengine.scheduler.pending_requests.get(domain, [])\n+    def _pending_count(self, spider):\n+        pending = scrapyengine.scheduler.pending_requests.get(spider, [])\n         return len(pending)\n\n@@ -20,49 +20,49 @@ class LiveStats(object):\n \n     def __init__(self):\n         self.domains = {}\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)\n \n         dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n \n-    def domain_opened(self, domain, spider):\n+    def spider_opened(self, spider):\n         pstats = SpiderStats()\n-        self.domains[spider.domain_name] = pstats\n+        self.domains[spider] = pstats\n         pstats.started = datetime.now().replace(microsecond=0)\n         pstats.finished = None\n \n-    def domain_closed(self, domain, spider):\n-        self.domains[spider.domain_name].finished = datetime.now().replace(microsecond=0)\n+    def spider_closed(self, spider):\n+        self.domains[spider].finished = datetime.now().replace(microsecond=0)\n \n     def item_scraped(self, item, spider):\n-        self.domains[spider.domain_name].scraped += 1\n+        self.domains[spider].scraped += 1\n \n     def response_downloaded(self, response, spider):\n         # sometimes we download responses without opening/closing domains,\n         # for example from scrapy shell\n-        if self.domains.get(spider.domain_name):\n-            self.domains[spider.domain_name].crawled += 1\n+        if self.domains.get(spider):\n+            self.domains[spider].crawled += 1\n             \n     def webconsole_render(self, wc_request):\n         sch = scrapyengine.scheduler\n         dwl = scrapyengine.downloader\n \n-        totdomains = totscraped = totcrawled = totscheduled = totactive = totpending = totdqueued = tottransf = 0\n+        totdomains = totscraped = totcrawled = totscheduled = totactive = totdqueued = tottransf = 0\n         s = banner(self)\n         s += \"<table border='1'>\\n\"\n         s += \"<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\\n\"\n-        for d in sorted(self.domains.keys()):\n-            scheduled = len(sch.pending_requests[d]) if d in sch.pending_requests else 0\n-            active = len(dwl.sites[d].active) if d in dwl.sites else 0\n-            dqueued = len(dwl.sites[d].queue) if d in dwl.sites else 0\n-            transf = len(dwl.sites[d].transferring) if d in dwl.sites else 0\n-            stats = self.domains[d]\n+        for spider in sorted(self.domains.keys()):\n+            scheduled = len(sch.pending_requests[spider]) if spider in sch.pending_requests else 0\n+            active = len(dwl.sites[spider].active) if spider in dwl.sites else 0\n+            dqueued = len(dwl.sites[spider].queue) if spider in dwl.sites else 0\n+            transf = len(dwl.sites[spider].transferring) if spider in dwl.sites else 0\n+            stats = self.domains[spider]\n             runtime = stats.finished - stats.started if stats.finished else datetime.now() - stats.started\n \n             s += '<tr><td>%s</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n' % \\\n-                 (d, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(stats.finished), str(runtime))\n+                 (spider.domain_name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(stats.finished), str(runtime))\n \n             totdomains += 1\n             totscraped += stats.scraped\n\n@@ -18,16 +18,16 @@ class Spiderctl(object):\n     def __init__(self):\n         self.running = {}\n         self.finished = set()\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n         from scrapy.management.web import webconsole_discover_module\n         dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n \n-    def domain_opened(self, spider):\n+    def spider_opened(self, spider):\n         self.running[spider.domain_name] = spider\n \n-    def domain_closed(self, spider):\n+    def spider_closed(self, spider):\n         del self.running[spider.domain_name]\n         self.finished.add(spider.domain_name)\n \n\n@@ -24,8 +24,8 @@ class ShoveItemPipeline(object):\n         self.opts = settings['SHOVEITEM_STORE_OPT'] or {}\n         self.stores = {}\n \n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def process_item(self, domain, item):\n         guid = str(item.guid)\n@@ -43,12 +43,13 @@ class ShoveItemPipeline(object):\n         self.log(domain, item, status)\n         return item\n \n-    def domain_opened(self, domain):\n+    def spider_opened(self, spider):\n+        domain = spider.domain_name\n         uri = Template(self.uritpl).substitute(domain=domain)\n         self.stores[domain] = Shove(uri, **self.opts)\n \n-    def domain_closed(self, domain):\n-        self.stores[domain].sync()\n+    def spider_closed(self, spider):\n+        self.stores[spider.domain_name].sync()\n \n     def log(self, domain, item, status):\n         log.msg(\"Shove (%s): Item guid=%s\" % (status, item.guid), level=log.DEBUG, domain=domain)\n\n@@ -99,7 +99,7 @@ class ExecutionEngine(object):\n             return True\n \n     def next_request(self, spider, now=False):\n-        \"\"\"Scrape the next request for the domain passed.\n+        \"\"\"Scrape the next request for the spider passed.\n \n         The next request to be scraped is retrieved from the scheduler and\n         requested from the downloader.\n@@ -164,7 +164,7 @@ class ExecutionEngine(object):\n     def crawl(self, request, spider):\n         if not request.deferred.callbacks:\n             log.msg(\"Unable to crawl Request with no callback: %s\" % request,\n-                level=log.ERROR, domain=spider.domain_name)\n+                level=log.ERROR, spider=spider)\n             return\n         schd = mustbe_deferred(self.schedule, request, spider)\n         # FIXME: we can't log errors because we would be preventing them from\n@@ -186,7 +186,7 @@ class ExecutionEngine(object):\n         return self.scheduler.enqueue_request(spider, request)\n \n     def _mainloop(self):\n-        \"\"\"Add more domains to be scraped if the downloader has the capacity.\n+        \"\"\"Add more spiders to be scraped if the downloader has the capacity.\n \n         If there is nothing else scheduled then stop the execution engine.\n         \"\"\"\n@@ -198,15 +198,13 @@ class ExecutionEngine(object):\n                 return self._stop_if_idle()\n \n     def download(self, request, spider):\n-        domain = spider.domain_name\n-\n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n                 log.msg(self._crawled_logline(request, response), \\\n-                    level=log.DEBUG, domain=spider.domain_name)\n+                    level=log.DEBUG, spider=spider)\n                 return response\n             elif isinstance(response, Request):\n                 newrequest = response\n@@ -225,7 +223,7 @@ class ExecutionEngine(object):\n                 level = log.ERROR\n             if errmsg:\n                 log.msg(\"Crawling <%s>: %s\" % (request.url, errmsg), \\\n-                    level=level, domain=domain)\n+                    level=level, spider=spider)\n             return Failure(IgnoreRequest(str(exc)))\n \n         def _on_complete(_):\n@@ -241,38 +239,31 @@ class ExecutionEngine(object):\n         return dwld\n \n     def open_spider(self, spider):\n-        domain = spider.domain_name\n-        log.msg(\"Domain opened\", domain=domain)\n+        log.msg(\"Spider opened\", spider=spider)\n         self.next_request(spider)\n \n         self.downloader.open_spider(spider)\n         self.scraper.open_spider(spider)\n-        stats.open_domain(domain)\n+        stats.open_domain(spider.domain_name)\n \n-        # XXX: sent for backwards compatibility (will be removed in Scrapy 0.8)\n-        send_catch_log(signals.domain_open, sender=self.__class__, \\\n-            domain=domain, spider=spider)\n-\n-        send_catch_log(signals.domain_opened, sender=self.__class__, \\\n-            domain=domain, spider=spider)\n+        send_catch_log(signals.spider_opened, sender=self.__class__, spider=spider)\n \n     def _spider_idle(self, spider):\n-        \"\"\"Called when a domain gets idle. This function is called when there\n+        \"\"\"Called when a spider gets idle. This function is called when there\n         are no remaining pages to download or schedule. It can be called\n         multiple times. If some extension raises a DontCloseDomain exception\n-        (in the domain_idle signal handler) the domain is not closed until the\n+        (in the spider_idle signal handler) the spider is not closed until the\n         next loop and this function is guaranteed to be called (at least) once\n-        again for this domain.\n+        again for this spider.\n         \"\"\"\n-        domain = spider.domain_name\n         try:\n-            dispatcher.send(signal=signals.domain_idle, sender=self.__class__, \\\n-                domain=domain, spider=spider)\n+            dispatcher.send(signal=signals.spider_idle, sender=self.__class__, \\\n+                spider=spider)\n         except DontCloseDomain:\n             self.next_request(spider)\n             return\n         except:\n-            log.err(\"Exception catched on domain_idle signal dispatch\")\n+            log.err(\"Exception catched on spider_idle signal dispatch\")\n         if self.spider_is_idle(spider):\n             self.close_spider(spider, reason='finished')\n \n@@ -283,9 +274,8 @@ class ExecutionEngine(object):\n \n     def close_spider(self, spider, reason='cancelled'):\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n-        domain = spider.domain_name\n         if spider not in self.closing:\n-            log.msg(\"Closing domain (%s)\" % reason, domain=domain)\n+            log.msg(\"Closing spider (%s)\" % reason, spider=spider)\n             self.closing[spider] = reason\n             self.downloader.close_spider(spider)\n             self.scheduler.clear_pending_requests(spider)\n@@ -298,7 +288,7 @@ class ExecutionEngine(object):\n         return dlist\n \n     def _finish_closing_spider_if_idle(self, spider):\n-        \"\"\"Call _finish_closing_spider if domain is idle\"\"\"\n+        \"\"\"Call _finish_closing_spider if spider is idle\"\"\"\n         if self.spider_is_idle(spider) or self.killed:\n             return self._finish_closing_spider(spider)\n         else:\n@@ -310,15 +300,14 @@ class ExecutionEngine(object):\n \n     def _finish_closing_spider(self, spider):\n         \"\"\"This function is called after the spider has been closed\"\"\"\n-        domain = spider.domain_name\n         self.scheduler.close_spider(spider)\n         self.scraper.close_spider(spider)\n         reason = self.closing.pop(spider, 'finished')\n-        send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n-            domain=domain, spider=spider, reason=reason)\n-        stats.close_domain(domain, reason=reason)\n+        send_catch_log(signal=signals.spider_closed, sender=self.__class__, \\\n+            spider=spider, reason=reason)\n+        stats.close_domain(spider.domain_name, reason=reason)\n         dfd = defer.maybeDeferred(spiders.close_spider, spider)\n-        dfd.addBoth(log.msg, \"Domain closed (%s)\" % reason, domain=domain)\n+        dfd.addBoth(log.msg, \"Spider closed (%s)\" % reason, spider=spider)\n         reactor.callLater(0, self._mainloop)\n         return dfd\n \n\n@@ -7,9 +7,9 @@ signals here without documenting them there.\n \n engine_started = object()\n engine_stopped = object()\n-domain_opened = object()\n-domain_idle = object()\n-domain_closed = object()\n+spider_opened = object()\n+spider_idle = object()\n+spider_closed = object()\n request_received = object()\n request_uploaded = object()\n response_received = object()\n@@ -17,7 +17,3 @@ response_downloaded = object()\n item_scraped = object()\n item_passed = object()\n item_dropped = object()\n-\n-# XXX: deprecated signals (will be removed in Scrapy 0.8)\n-domain_open = object()\n-\n\n@@ -63,26 +63,27 @@ def start(logfile=None, loglevel=None, logstdout=None):\n         file = open(logfile, 'a') if logfile else sys.stderr\n         log.startLogging(file, setStdout=logstdout)\n \n-def msg(message, level=INFO, component=BOT_NAME, domain=None):\n+def msg(message, level=INFO, component=BOT_NAME, domain=None, spider=None):\n     \"\"\"Log message according to the level\"\"\"\n     if level > log_level:\n         return\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n-        domain=domain)\n-    system = domain if domain else component\n+        domain=domain, spider=spider)\n+    system = domain or spider.domain_name if spider else component\n     msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n     log.msg(msg_txt, system=system)\n \n-def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n+def exc(message, level=ERROR, component=BOT_NAME, domain=None, spider=None):\n     message = message + '\\n' + format_exc()\n-    msg(message, level, component, domain)\n+    msg(message, level, component, domain, spider)\n \n def err(_stuff=None, _why=None, **kwargs):\n     if ERROR > log_level:\n         return\n     domain = kwargs.pop('domain', None)\n+    spider = kwargs.pop('spider', None)\n     component = kwargs.pop('component', BOT_NAME)\n-    kwargs['system'] = domain if domain else component\n+    kwargs['system'] = domain or spider.domain_name if spider else component\n     if _why:\n         _why = unicode_to_str(\"ERROR: %s\" % _why)\n     log.err(_stuff, _why, **kwargs)\n\n@@ -14,7 +14,7 @@ class CookiesMiddlewareTest(TestCase):\n         self.mw = CookiesMiddleware()\n \n     def tearDown(self):\n-        self.mw.domain_closed('scrapytest.org')\n+        self.mw.spider_closed('scrapytest.org')\n         del self.mw\n \n     def test_basic(self):\n\n@@ -89,9 +89,9 @@ class CrawlingSession(object):\n \n             dispatcher.connect(self.record_signal, signals.engine_started)\n             dispatcher.connect(self.record_signal, signals.engine_stopped)\n-            dispatcher.connect(self.record_signal, signals.domain_opened)\n-            dispatcher.connect(self.record_signal, signals.domain_idle)\n-            dispatcher.connect(self.record_signal, signals.domain_closed)\n+            dispatcher.connect(self.record_signal, signals.spider_opened)\n+            dispatcher.connect(self.record_signal, signals.spider_idle)\n+            dispatcher.connect(self.record_signal, signals.spider_closed)\n             dispatcher.connect(self.item_scraped, signals.item_scraped)\n             dispatcher.connect(self.request_received, signals.request_received)\n             dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n@@ -201,16 +201,16 @@ class EngineTest(unittest.TestCase):\n \n         assert signals.engine_started in session.signals_catched\n         assert signals.engine_stopped in session.signals_catched\n-        assert signals.domain_opened in session.signals_catched\n-        assert signals.domain_idle in session.signals_catched\n-        assert signals.domain_closed in session.signals_catched\n+        assert signals.spider_opened in session.signals_catched\n+        assert signals.spider_idle in session.signals_catched\n+        assert signals.spider_closed in session.signals_catched\n \n-        self.assertEqual({'domain': session.domain, 'spider': session.spider},\n-                         session.signals_catched[signals.domain_opened])\n-        self.assertEqual({'domain': session.domain, 'spider': session.spider},\n-                         session.signals_catched[signals.domain_idle])\n-        self.assertEqual({'domain': session.domain, 'spider': session.spider, 'reason': 'finished'},\n-                         session.signals_catched[signals.domain_closed])\n+        self.assertEqual({'spider': session.spider},\n+                         session.signals_catched[signals.spider_opened])\n+        self.assertEqual({'spider': session.spider},\n+                         session.signals_catched[signals.spider_idle])\n+        self.assertEqual({'spider': session.spider, 'reason': 'finished'},\n+                         session.signals_catched[signals.spider_closed])\n \n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n\n@@ -13,7 +13,7 @@ class TestOffsiteMiddleware(TestCase):\n         self.spider.extra_domain_names = ['scrapy.org']\n \n         self.mw = OffsiteMiddleware()\n-        self.mw.domain_opened(self.spider)\n+        self.mw.spider_opened(self.spider)\n \n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n@@ -28,5 +28,5 @@ class TestOffsiteMiddleware(TestCase):\n         self.assertEquals(out, onsite_reqs)\n \n     def tearDown(self):\n-        self.mw.domain_closed(self.spider)\n+        self.mw.spider_closed(self.spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
