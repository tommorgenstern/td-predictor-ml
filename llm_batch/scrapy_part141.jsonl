{"custom_id": "scrapy#30b6c77ce59f79fcccb10527ef8d399bbd693b9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         if opts.verbose:\n             try:\n-                import lxml.etree2\n+                import lxml.etree\n             except ImportError:\n                 lxml_version = libxml2_version = \"(lxml not available)\"\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b4f368c37ee0b42812d5412b98976038ea335d0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 13 | Churn Cumulative: 128 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,11 @@ class Link(object):\n     __slots__ = ['url', 'text', 'fragment', 'nofollow']\n \n     def __init__(self, url, text='', fragment='', nofollow=False):\n+        if isinstance(url, unicode):\n+            import warnings\n+            warnings.warn(\"Do not instantiate Link objects with unicode urls. \" \\\n+                \"Assuming utf-8 encoding (which could be wrong)\")\n+            url = url.encode('utf-8')\n         self.url = url\n         self.text = text\n         self.fragment = fragment\n\n@@ -1,4 +1,5 @@\n import unittest\n+import warnings\n \n from scrapy.link import Link\n \n@@ -41,3 +42,10 @@ class LinkTest(unittest.TestCase):\n         l1 = Link(\"http://www.example.com\", text=\"test\", fragment='something', nofollow=True)\n         l2 = eval(repr(l1))\n         self._assert_same_links(l1, l2)\n+\n+    def test_unicode_url(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            l = Link(u\"http://www.example.com/\\xa3\")\n+            assert isinstance(l.url, str)\n+            assert l.url == 'http://www.example.com/\\xc2\\xa3'\n+            assert len(w) == 1, \"warning not issued\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f530b0b3eb5b4c9639366f891836a3cdcb60623a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 756 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -33,3 +33,10 @@ except ImportError:\n     pass\n else:\n     optional_features.add('boto')\n+\n+try:\n+    import libxml2\n+except ImportError:\n+    pass\n+else:\n+    optional_features.add('libxml2')\n\n@@ -4,9 +4,11 @@ garbage collection to libxml2 documents (xmlDoc).\n \"\"\"\n \n import weakref\n-import libxml2\n from scrapy.utils.trackref import object_ref\n+from scrapy import optional_features\n \n+if 'libxml2' in optional_features:\n+    import libxml2\n     xml_parser_options = libxml2.XML_PARSE_RECOVER + \\\n                          libxml2.XML_PARSE_NOERROR + \\\n                          libxml2.XML_PARSE_NOWARNING\n\n@@ -2,6 +2,8 @@\n XPath selectors based on libxml2\n \"\"\"\n \n+from scrapy import optional_features\n+if 'libxml2' in optional_features:\n     import libxml2\n \n from scrapy.http import TextResponse\n\n@@ -1,13 +1,12 @@\n from twisted.trial import unittest\n \n from scrapy.utils.test import libxml2debug\n+from scrapy import optional_features\n+\n \n class Libxml2Test(unittest.TestCase):\n \n-    try:\n-        import libxml2\n-    except ImportError, e:\n-        skip = str(e)\n+    skip = 'libxml2' not in optional_features\n \n     @libxml2debug\n     def test_libxml2_bug_2_6_27(self):\n\n@@ -2,7 +2,9 @@\n Selectors tests, specific for libxml2 backend\n \"\"\"\n \n-import unittest\n+from twisted.trial import unittest\n+from scrapy import optional_features\n+\n \n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.selector.libxml2sel import XmlXPathSelector, HtmlXPathSelector, \\\n@@ -18,6 +20,8 @@ class Libxml2XPathSelectorTestCase(test_selector.XPathSelectorTestCase):\n     hxs_cls = HtmlXPathSelector\n     xxs_cls = XmlXPathSelector\n \n+    skip = 'libxml2' not in optional_features\n+\n     @libxml2debug\n     def test_null_bytes(self):\n         hxs = HtmlXPathSelector(text='<root>la\\x00la</root>')\n@@ -62,6 +66,8 @@ class Libxml2XPathSelectorTestCase(test_selector.XPathSelectorTestCase):\n \n class Libxml2DocumentTest(unittest.TestCase):\n \n+    skip = 'libxml2' not in optional_features\n+\n     @libxml2debug\n     def test_response_libxml2_caching(self):\n         r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fc573a23086e83772ba3b1856529520626e6c2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 66 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,8 +11,9 @@ class Libxml2Test(unittest.TestCase):\n     @libxml2debug\n     def test_libxml2_bug_2_6_27(self):\n         # this test will fail in version 2.6.27 but passes on 2.6.29+\n+        import libxml2\n         html = \"<td>1<b>2</b>3</td>\"\n-        node = self.libxml2.htmlParseDoc(html, 'utf-8')\n+        node = libxml2.htmlParseDoc(html, 'utf-8')\n         result = [str(r) for r in node.xpathEval('//text()')]\n         self.assertEquals(result, ['1', '2', '3'])\n         node.freeDoc()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b33303779a520a0751648d63977f78890ca11d37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 216 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -67,8 +67,8 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.start_time = datetime.now()\n         self.end_time = None\n         self.env = env\n-        self.logfile = env['SCRAPY_LOG_FILE']\n-        self.itemsfile = env['SCRAPY_FEED_URI']\n+        self.logfile = env.get('SCRAPY_LOG_FILE')\n+        self.itemsfile = env.get('SCRAPY_FEED_URI')\n         self.deferred = defer.Deferred()\n \n     def outReceived(self, data):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d77005047cc701970c8c4114a65423c0dc7c2ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 32 | Churn Cumulative: 632 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,8 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n+from threading import Thread\n+\n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy import log\n@@ -35,12 +37,11 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n-        shell = Shell(self.crawler, update_vars=self.update_vars, inthread=True, \\\n-            code=opts.code)\n-        def err(f):\n-            log.err(f, \"Shell error\")\n-            self.exitcode = 1\n-        d = shell.start(url=url)\n-        d.addErrback(err)\n-        d.addBoth(lambda _: self.crawler.stop())\n-        self.crawler.start()\n+        shell = Shell(self.crawler, update_vars=self.update_vars, code=opts.code)\n+        self._start_crawler_thread()\n+        shell.start(url=url)\n+\n+    def _start_crawler_thread(self):\n+        t = Thread(target=self.crawler.start)\n+        t.daemon = True\n+        t.start()\n\n@@ -7,6 +7,7 @@ See documentation in docs/topics/shell.rst\n import signal\n \n from twisted.internet import reactor, threads\n+from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n from scrapy.item import BaseItem\n@@ -25,24 +26,18 @@ class Shell(object):\n     relevant_classes = (BaseSpider, Request, Response, BaseItem, \\\n         XPathSelector, Settings)\n \n-    def __init__(self, crawler, update_vars=None, inthread=False, code=None):\n+    def __init__(self, crawler, update_vars=None, code=None):\n         self.crawler = crawler\n         self.update_vars = update_vars or (lambda x: None)\n         self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])\n         self.spider = None\n-        self.inthread = inthread\n+        self.inthread = not threadable.isInIOThread()\n         self.code = code\n         self.vars = {}\n \n-    def start(self, *a, **kw):\n+    def start(self, url=None, request=None, response=None, spider=None):\n         # disable accidental Ctrl-C key press from shutting down the engine\n         signal.signal(signal.SIGINT, signal.SIG_IGN)\n-        if self.inthread:\n-            return threads.deferToThread(self._start, *a, **kw)\n-        else:\n-            self._start(*a, **kw)\n-\n-    def _start(self, url=None, request=None, response=None, spider=None):\n         if url:\n             self.fetch(url, spider)\n         elif request:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e3c5f1bf795d200fa6c5e0867486a9cdaf1d763", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 45 | Files Changed: 1 | Hunks: 18 | Methods Changed: 15 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 124 | Churn Cumulative: 315 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,12 @@ class Command(ScrapyCommand):\n \n     requires_project = True\n \n+    spider = None\n+    items = []\n+    requests = []\n+    \n+    first_response = None\n+\n     def syntax(self):\n         return \"[options] <url>\"\n \n@@ -31,17 +37,14 @@ class Command(ScrapyCommand):\n             help=\"use CrawlSpider rules to discover the callback\")\n         parser.add_option(\"-c\", \"--callback\", dest=\"callback\", \\\n             help=\"use this callback for parsing, instead looking for a callback\")\n+        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1, \\\n+            help=\"maximum depth for parsing requests [default: %default]\")\n+        # parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", \\\n+        #     help=\"print each depth level one by one\")\n \n-    def pipeline_process(self, item, spider, opts):\n-        return item\n-\n-    def run_callback(self, spider, response, callback, opts):\n-        cb = callback if callable(callback) else getattr(spider, callback, None)\n-        if not cb:\n-            log.msg('Cannot find callback %r in spider: %s' % (callback, spider.name))\n-            return (), ()\n-\n+    def run_callback(self, response, cb):\n         items, requests = [], []\n+\n         for x in iterate_spider_output(cb(response)):\n             if isinstance(x, BaseItem):\n                 items.append(x)\n@@ -49,61 +52,92 @@ class Command(ScrapyCommand):\n                 requests.append(x)\n         return items, requests\n \n-    def get_callback_from_rules(self, spider, response):\n-        if getattr(spider, 'rules', None):\n-            for rule in spider.rules:\n+    def get_callback_from_rules(self, response):\n+        if getattr(self.spider, 'rules', None):\n+            for rule in self.spider.rules:\n                 if rule.link_extractor.matches(response.url) and rule.callback:\n                     return rule.callback\n         else:\n             log.msg(\"No CrawlSpider rules found in spider %r, please specify \"\n-                \"a callback to use for parsing\" % spider.name, log.ERROR)\n+                \"a callback to use for parsing\" % self.spider.name, log.ERROR)\n \n-    def print_results(self, items, requests, cb_name, opts):\n+    def print_results(self, opts):\n         if not opts.noitems:\n-            print \"# Scraped Items - callback: %s\" % cb_name, \"-\"*60\n-            display.pprint([dict(x) for x in items], colorize=not opts.nocolour)\n+            print \"# Scraped Items \", \"-\"*60\n+            display.pprint([dict(x) for x in self.items], colorize=not opts.nocolour)\n         if not opts.nolinks:\n-            print \"# Requests - callback: %s\" % cb_name, \"-\"*68\n-            display.pprint(requests, colorize=not opts.nocolour)\n+            print \"# Requests \", \"-\"*65\n+            display.pprint(self.requests, colorize=not opts.nocolour)\n \n-    def get_spider(self, request, opts):\n+    def set_spider(self, url, opts):\n         if opts.spider:\n             try:\n-                return self.crawler.spiders.create(opts.spider)\n+                self.spider = self.crawler.spiders.create(opts.spider)\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n-            spider = create_spider_for_request(self.crawler.spiders, request)\n-            if spider:\n-                return spider\n+            self.spider = create_spider_for_request(self.crawler.spiders, url)\n+            if not self.spider:\n                 log.msg('Unable to find spider for: %s' % request, log.ERROR)\n \n-    def get_response_and_spider(self, url, opts):\n-        responses = [] # to collect downloaded responses\n-        request = Request(url, callback=responses.append)\n-        spider = self.get_spider(request, opts)\n-        if not spider:\n-            return None, None\n-        self.crawler.crawl(spider, [request])\n+    def start_parsing(self, url, opts):\n+        request = Request(url, opts.callback)\n+        request = self.prepare_request(request, opts)\n+\n+        self.crawler.crawl(self.spider, [request])\n         self.crawler.start()\n-        if not responses:\n+\n+        if not self.first_response:\n             log.msg('No response downloaded for: %s' % request, log.ERROR, \\\n-                spider=spider)\n-            return None, None\n-        return responses[0], spider\n+                spider=self.spider)\n+\n+    def prepare_request(self, request, opts):\n+        def callback(response):\n+            # memorize first request\n+            if not self.first_response:\n+                self.first_response = response\n+\n+            # determine real callback\n+            cb = response.meta['_callback']\n+            if not cb:\n+                if opts.rules and self.first_response == response:\n+                    cb = self.get_callback_from_rules(response)\n+                else:\n+                    cb = 'parse'\n+\n+            cb = cb if callable(cb) else getattr(self.spider, cb, None)\n+            if not cb:\n+                log.msg('Cannot find callback %r in spider: %s' % (callback, spider.name))\n+\n+            # parse items and requests\n+            items, requests = self.run_callback(response, cb)\n+            self.items += items\n+\n+            depth = response.meta['_depth']\n+            if depth < opts.depth:\n+                for req in requests:\n+                    req.meta['_depth'] = depth + 1\n+                    req.meta['_callback'] = req.callback\n+                    req.callback = callback\n+                return requests\n+            else:\n+                self.requests += requests\n+\n+        request.meta['_depth'] = 1\n+        request.meta['_callback'] = request.callback\n+        request.callback = callback\n+        return request\n \n     def run(self, args, opts):\n+        # parse arguments\n         if not len(args) == 1 or not is_url(args[0]):\n             raise UsageError()\n-        response, spider = self.get_response_and_spider(args[0], opts)\n-        if not response:\n-            return\n-        callback = None\n-        if opts.callback:\n-            callback = opts.callback\n-        elif opts.rules:\n-            callback = self.get_callback_from_rules(spider, response)\n-        items, requests = self.run_callback(spider, response, callback or 'parse', \\\n-            opts)\n-        self.print_results(items, requests, callback, opts)\n+        else:\n+            url = args[0]\n \n+        # prepare spider\n+        self.set_spider(url, opts)\n+\n+        if self.spider and opts.depth > 0:\n+            self.start_parsing(url, opts)\n+            self.print_results(opts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b5cf31198a17ce26f8657ef95345fbaca2d5187", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 81 | Churn Cumulative: 396 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.6216216216216216\n\nDIFF:\n@@ -12,8 +12,8 @@ class Command(ScrapyCommand):\n     requires_project = True\n \n     spider = None\n-    items = []\n-    requests = []\n+    items = {}\n+    requests = {}\n     \n     first_response = None\n \n@@ -39,8 +39,63 @@ class Command(ScrapyCommand):\n             help=\"use this callback for parsing, instead looking for a callback\")\n         parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1, \\\n             help=\"maximum depth for parsing requests [default: %default]\")\n-        # parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", \\\n-        #     help=\"print each depth level one by one\")\n+        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", \\\n+            help=\"print each depth level one by one\")\n+\n+\n+    @property\n+    def max_level(self):\n+        levels = self.items.keys() + self.requests.keys()\n+        if levels: return max(levels)\n+        else: return 0\n+\n+    def add_items(self, lvl, new_items):\n+        old_items = self.items.get(lvl, [])\n+        self.items[lvl] = old_items + new_items\n+\n+    def add_requests(self, lvl, new_reqs):\n+        old_reqs = self.requests.get(lvl, [])\n+        self.requests[lvl] = old_reqs + new_reqs\n+    \n+    def print_items(self, lvl=None, colour=True):\n+        if lvl is None:\n+            items = [item for lst in self.items.values() for item in lst]\n+        else:\n+            items = self.items.get(lvl, [])\n+\n+        print \"# Scraped Items \", \"-\"*60\n+        display.pprint([dict(x) for x in items], colorize=colour)\n+\n+    def print_requests(self, lvl=None, colour=True):\n+        if lvl is None:\n+            levels = self.requests.keys()\n+            if levels:\n+                requests = self.requests[max(levels)]\n+            else:\n+                requests = []\n+        else:\n+            requests = self.requests.get(lvl, [])\n+\n+        print \"# Requests \", \"-\"*65\n+        display.pprint(requests, colorize=colour)\n+\n+    def print_results(self, opts):\n+        colour = not opts.nocolour\n+\n+        if opts.verbose:\n+            for level in xrange(1, self.max_level+1):\n+                print '\\n>>> DEPTH LEVEL: %s <<<' % level\n+                if not opts.noitems:\n+                    self.print_items(level, colour)\n+                if not opts.nolinks:\n+                    self.print_requests(level, colour)\n+        else:\n+            print '\\n>>> STATUS DEPTH LEVEL %s <<<' % self.max_level\n+            if not opts.noitems:\n+                self.print_items(colour=colour)\n+            if not opts.nolinks:\n+                self.print_requests(colour=colour)\n+\n \n     def run_callback(self, response, cb):\n         items, requests = [], []\n@@ -61,14 +116,6 @@ class Command(ScrapyCommand):\n             log.msg(\"No CrawlSpider rules found in spider %r, please specify \"\n                 \"a callback to use for parsing\" % self.spider.name, log.ERROR)\n \n-    def print_results(self, opts):\n-        if not opts.noitems:\n-            print \"# Scraped Items \", \"-\"*60\n-            display.pprint([dict(x) for x in self.items], colorize=not opts.nocolour)\n-        if not opts.nolinks:\n-            print \"# Requests \", \"-\"*65\n-            display.pprint(self.requests, colorize=not opts.nocolour)\n-\n     def set_spider(self, url, opts):\n         if opts.spider:\n             try:\n@@ -110,18 +157,18 @@ class Command(ScrapyCommand):\n                 log.msg('Cannot find callback %r in spider: %s' % (callback, spider.name))\n \n             # parse items and requests\n-            items, requests = self.run_callback(response, cb)\n-            self.items += items\n-\n             depth = response.meta['_depth']\n+\n+            items, requests = self.run_callback(response, cb)\n+            self.add_items(depth, items)\n+            self.add_requests(depth, requests)\n+\n             if depth < opts.depth:\n                 for req in requests:\n                     req.meta['_depth'] = depth + 1\n                     req.meta['_callback'] = req.callback\n                     req.callback = callback\n                 return requests\n-            else:\n-                self.requests += requests\n \n         request.meta['_depth'] = 1\n         request.meta['_callback'] = request.callback\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3bddedfc6dc1a090c819a49da6a5b7fe33742a83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 68 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,8 @@ from twisted.application.internet import TimerService, TCPServer\n from twisted.web import server\n from twisted.python import log\n \n+from scrapy.utils.misc import load_object\n+\n from .interfaces import IEggStorage, IPoller, ISpiderScheduler, IEnvironment\n from .launcher import Launcher\n from .eggstorage import FilesystemEggStorage\n@@ -27,7 +29,10 @@ def application(config):\n     app.setComponent(ISpiderScheduler, scheduler)\n     app.setComponent(IEnvironment, environment)\n \n-    launcher = Launcher(config, app)\n+    laupath = config.get('launcher', 'scrapyd.launcher.Launcher')\n+    laucls = load_object(laupath)\n+    launcher = laucls(config, app)\n+\n     timer = TimerService(5, poller.poll)\n     webservice = TCPServer(http_port, server.Site(Root(config, app)), interface=bind_address)\n     log.msg(\"Scrapyd web console available at http://%s:%s/\" % (bind_address, http_port))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e05cf56852cd53bc91a9b9e1c1024fb7a1dfbeb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 398 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -154,7 +154,7 @@ class Command(ScrapyCommand):\n \n             cb = cb if callable(cb) else getattr(self.spider, cb, None)\n             if not cb:\n-                log.msg('Cannot find callback %r in spider: %s' % (callback, spider.name))\n+                log.msg('Cannot find callback %r in spider: %s' % (callback, self.spider.name))\n \n             # parse items and requests\n             depth = response.meta['_depth']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#179e3810dc29fbb292cc401fb78c1e737ecc098c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 295 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,7 @@ class TelnetConsole(protocol.ServerFactory):\n             'prefs': print_live_refs,\n             'hpy': hpy,\n             'help': \"This is Scrapy telnet console. For more info see: \" \\\n-                \"http://doc.scrapy.org/topics/telnetconsole.html\", # see #284\n+                \"http://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n         }\n         send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n         return telnet_vars\n\n@@ -83,7 +83,7 @@ monitoring)</p>\n <p>Example using <a href=\"http://curl.haxx.se/\">curl</a>:</p>\n <p><code>curl http://localhost:6800/schedule.json -d project=default -d spider=somespider</code></p>\n \n-<p>For more information about the API, see the <a href=\"http://doc.scrapy.org/topics/scrapyd.html\">Scrapyd documentation</a></p>\n+<p>For more information about the API, see the <a href=\"http://doc.scrapy.org/en/latest/topics/scrapyd.html\">Scrapyd documentation</a></p>\n </body>\n </html>\n \"\"\" % vars\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf968c328f41a1d6e8b68fdda4ee06e583af086f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 24 | Churn Cumulative: 275 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,3 +52,11 @@ class Config(object):\n \n     def getboolean(self, option, default=None):\n         return self._getany(self.cp.getboolean, option, default)\n+\n+    def items(self, section, default=None):\n+        try:\n+            return self.cp.items(section)\n+        except (NoSectionError, NoOptionError):\n+            if default is not None:\n+                return default\n+            raise\n\n@@ -2,6 +2,9 @@ from datetime import datetime\n \n from twisted.web import resource, static\n from twisted.application.service import IServiceCollection\n+\n+from scrapy.utils.misc import load_object\n+\n from .interfaces import IPoller, IEggStorage, ISpiderScheduler\n \n from . import webservice\n@@ -16,18 +19,13 @@ class Root(resource.Resource):\n         itemsdir = config.get('items_dir')\n         self.app = app\n         self.putChild('', Home(self))\n-        self.putChild('schedule.json', webservice.Schedule(self))\n-        self.putChild('cancel.json', webservice.Cancel(self))\n-        self.putChild('addversion.json', webservice.AddVersion(self))\n-        self.putChild('listprojects.json', webservice.ListProjects(self))\n-        self.putChild('listversions.json', webservice.ListVersions(self))\n-        self.putChild('listspiders.json', webservice.ListSpiders(self))\n-        self.putChild('delproject.json', webservice.DeleteProject(self))\n-        self.putChild('delversion.json', webservice.DeleteVersion(self))\n-        self.putChild('listjobs.json', webservice.ListJobs(self))\n         self.putChild('logs', static.File(logsdir, 'text/plain'))\n         self.putChild('items', static.File(itemsdir, 'text/plain'))\n         self.putChild('jobs', Jobs(self))\n+        services = config.items('services', ())\n+        for servName, servClsName in services:\n+          servCls = load_object(servClsName)\n+          self.putChild(servName, servCls(self))\n         self.update_projects()\n \n     def update_projects(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#177c81745da7996b5e64a8d7e49b4158ced042b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 50 | Churn Cumulative: 174 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,10 +60,26 @@ class CookiesMiddleware(object):\n                 msg += os.linesep.join(\"Set-Cookie: %s\" % c for c in cl)\n                 log.msg(msg, spider=spider, level=log.DEBUG)\n \n+    def _format_cookie(self, cookie):\n+        # build cookie string\n+        cookie_str = '%s=%s' % (cookie['name'], cookie['value'])\n+\n+        if cookie.get('path', None):\n+            cookie_str += '; Path=%s' % cookie['path']\n+        if cookie.get('domain', None):\n+            cookie_str += '; Domain=%s' % cookie['domain']\n+\n+        return cookie_str\n+\n     def _get_request_cookies(self, jar, request):\n-        headers = {'Set-Cookie': ['%s=%s;' % (k, v) for k, v in request.cookies.iteritems()]}\n+        if isinstance(request.cookies, dict):\n+            cookie_list = [{'name': k, 'value': v} for k, v in \\\n+                    request.cookies.iteritems()]\n+        else:\n+            cookie_list = request.cookies\n+\n+        cookies = map(self._format_cookie, cookie_list)\n+        headers = {'Set-Cookie': cookies}\n         response = Response(request.url, headers=headers)\n-        cookies = jar.make_cookies(response, request)\n-        return cookies\n-\n \n+        return jar.make_cookies(response, request)\n\n@@ -49,6 +49,32 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n \n+    def test_complex_cookies(self):\n+        # merge some cookies into jar\n+        cookies = [{'name': 'C1', 'value': 'value1', 'path': '/foo', 'domain': 'scrapytest.org'},\n+                {'name': 'C2', 'value': 'value2', 'path': '/bar', 'domain': 'scrapytest.org'},\n+                {'name': 'C3', 'value': 'value3', 'path': '/foo', 'domain': 'scrapytest.org'},\n+                {'name': 'C4', 'value': 'value4', 'path': '/foo', 'domain': 'scrapy.org'}]\n+\n+\n+        req = Request('http://scrapytest.org/', cookies=cookies)\n+        self.mw.process_request(req, self.spider)\n+\n+        # embed C1 and C3 for scrapytest.org/foo\n+        req = Request('http://scrapytest.org/foo')\n+        self.mw.process_request(req, self.spider)\n+        assert req.headers.get('Cookie') in ('C1=value1; C3=value3', 'C3=value3; C1=value1')\n+\n+        # embed C2 for scrapytest.org/bar\n+        req = Request('http://scrapytest.org/bar')\n+        self.mw.process_request(req, self.spider)\n+        self.assertEquals(req.headers.get('Cookie'), 'C2=value2')\n+\n+        # embed nothing for scrapytest.org/baz\n+        req = Request('http://scrapytest.org/baz')\n+        self.mw.process_request(req, self.spider)\n+        assert 'Cookie' not in req.headers\n+\n     def test_merge_request_cookies(self):\n         req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})\n         assert self.mw.process_request(req, self.spider) is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb91cad0da2338b4a18446db9edb7b21d3535e2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 106 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,8 +13,8 @@ def inside_project():\n     if scrapy_module is not None:\n         try:\n             __import__(scrapy_module)\n-        except ImportError:\n-            warnings.warn(\"Cannot import scrapy settings module %s\" % scrapy_module)\n+        except ImportError as exc:\n+            warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n         else:\n             return True\n     return bool(closest_scrapy_cfg())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e14c41636f7b149192abae62fc83de7345931bc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 61 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,7 +25,8 @@ class DjangoItem(Item):\n     django_model = None\n \n     def save(self, commit=True):\n-        modelargs = dict((f, self.get(f, None)) for f in self._model_fields)\n+        modelargs = dict((k, self.get(k)) for k in self._values\n+                         if k in self._model_fields)\n         model = self.django_model(**modelargs)\n         if commit:\n             model.save()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0497ac2fc86deae85f3f17f39ac81d1a92555fdc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 123 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,3 +78,7 @@ class DjangoItemTest(unittest.TestCase):\n \n         self.assertEqual(person.name, 'John')\n \n+    def test_default_field_values(self):\n+        i = BasePersonItem()\n+        person = i.save(commit=False)\n+        self.assertEqual(person.name, 'Robot')\n\n@@ -2,7 +2,7 @@ from django.db import models\n \n \n class Person(models.Model):\n-    name = models.CharField(max_length=255)\n+    name = models.CharField(max_length=255, default='Robot')\n     age = models.IntegerField()\n \n     class Meta:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16bee46c707f57bbbedfe77ac3b9e83862c0f112", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,5 +17,5 @@ IGNORED_EXTENSIONS = [\n     'm4a',\n \n     # other\n-    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',\n+    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar', 'ppt',\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3bdd9b7b89a93d46630650c3ead7ea98f08eccee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 28 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,9 @@ IGNORED_EXTENSIONS = [\n     '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv',\n     'm4a',\n \n+    # office suites\n+    'xls', 'ppt', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',\n+\n     # other\n-    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar', 'ppt',\n+    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed8721059db1fc7d18a0613a7d999f156fc674ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 199 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -10,13 +10,11 @@ class FifoMemoryQueue(object):\n \n     def __init__(self):\n         self.q = deque()\n-\n-    def push(self, obj):\n-        self.q.appendleft(obj)\n+        self.push = self.q.append\n \n     def pop(self):\n-        if self.q:\n-            return self.q.pop()\n+        q = self.q\n+        return q.popleft() if q else None\n \n     def close(self):\n         pass\n@@ -28,8 +26,9 @@ class FifoMemoryQueue(object):\n class LifoMemoryQueue(FifoMemoryQueue):\n     \"\"\"Memory LIFO queue.\"\"\"\n \n-    def push(self, obj):\n-        self.q.append(obj)\n+    def pop(self):\n+        q = self.q\n+        return q.pop() if q else None\n \n \n class FifoDiskQueue(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99b76eaa2cd347700de5a989656ec583b82ec460", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 15 | Complexity Δ (Sum/Max): 28/12 | Churn Δ: 149 | Churn Cumulative: 149 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,50 @@\n+from functools import wraps\n+\n+from scrapy.command import ScrapyCommand\n+from scrapy.http import Request\n+\n+from scrapy.contracts import Contract\n+\n+class Command(ScrapyCommand):\n+    requires_project = True\n+\n+    def syntax(self):\n+        return \"[options] <spider>\"\n+\n+    def short_desc(self):\n+        return \"Check contracts for given spider\"\n+\n+    def run(self, args, opts):\n+        self.crawler.engine.has_capacity = lambda: True\n+\n+        for spider in args or self.crawler.spiders.list():\n+            spider = self.crawler.spiders.create(spider)\n+            requests = self.get_requests(spider)\n+            self.crawler.crawl(spider, requests)\n+\n+        self.crawler.start()\n+\n+    def get_requests(self, spider):\n+        requests = []\n+\n+        for key, value in vars(type(spider)).iteritems():\n+            if callable(value) and value.__doc__:\n+                bound_method = value.__get__(spider, type(spider))\n+                request = Request(url='http://scrapy.org', callback=bound_method)\n+\n+                # register contract hooks to the request\n+                contracts = Contract.from_method(value)\n+                for contract in contracts:\n+                    request = contract.prepare_request(request)\n+\n+                # discard anything the request might return\n+                cb = request.callback\n+                @wraps(cb)\n+                def wrapper(response):\n+                    cb(response)\n+\n+                request.callback = wrapper\n+\n+                requests.append(request)\n+\n+        return requests\n\n@@ -0,0 +1,2 @@\n+from .base import Contract, ContractType\n+from .default import *\n\n@@ -0,0 +1,74 @@\n+import re\n+from functools import wraps\n+\n+from scrapy.utils.spider import iterate_spider_output\n+\n+class ContractType(type):\n+    \"\"\" Metaclass for contracts\n+        - automatically registers contracts in the root `Contract` class\n+    \"\"\"\n+\n+    def __new__(meta, name, bases, dct):\n+        # only allow single inheritence\n+        assert len(bases) == 1, 'Multiple inheritance is not allowed'\n+        base = bases[0]\n+\n+        # ascend in inheritence chain\n+        while type(base) not in [type, meta]:\n+            base = type(base)\n+\n+        # register this as a valid contract\n+        cls = type.__new__(meta, name, bases, dct)\n+        if type(base) != type:\n+            base.registered[cls.name] = cls\n+        return cls\n+\n+\n+class Contract(object):\n+    \"\"\" Abstract class for contracts\n+        - keeps a reference of all derived classes in `registered`\n+    \"\"\"\n+\n+    __metaclass__ = ContractType\n+    registered = {}\n+\n+    def __init__(self, method, *args):\n+        self.method = method\n+        self.args = args\n+\n+    @classmethod\n+    def from_method(cls, method):\n+        contracts = []\n+        for line in method.__doc__.split('\\n'):\n+            line = line.strip()\n+\n+            if line.startswith('@'):\n+                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n+                args = re.split(r'[\\,\\s+]', args)\n+                args = filter(lambda x:x, args)\n+\n+                contracts.append(cls.registered[name](method, *args))\n+\n+        return contracts\n+\n+    def prepare_request(self, request):\n+        cb = request.callback\n+        @wraps(cb)\n+        def wrapper(response):\n+            self.pre_process(response)\n+            output = list(iterate_spider_output(cb(response)))\n+            self.post_process(output)\n+            return output\n+\n+        request.callback = wrapper\n+        request = self.modify_request(request)\n+        return request\n+\n+    def modify_request(self, request):\n+        return request\n+\n+    def pre_process(self, response):\n+        pass\n+\n+    def post_process(self, output):\n+        pass\n\n@@ -0,0 +1,23 @@\n+from scrapy.item import BaseItem\n+\n+from .base import Contract\n+\n+\n+# contracts\n+class UrlContract(Contract):\n+    name = 'url'\n+\n+    def modify_request(self, request):\n+        return request.replace(url=self.args[0])\n+\n+class ReturnsRequestContract(Contract):\n+    name = 'returns_request'\n+\n+class ScrapesContract(Contract):\n+    name = 'scrapes'\n+\n+    def post_process(self, output):\n+        for x in output:\n+            if isinstance(x, BaseItem):\n+                for arg in self.args:\n+                    assert arg in x, '%r field is missing' % arg\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36f47a4aec6d6d0dedb26848ffec8112fb189e26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 133 | Files Changed: 24 | Hunks: 75 | Methods Changed: 68 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 244 | Churn Cumulative: 3440 | Contributors (this commit): 11 | Commits (past 90d): 25 | Contributors (cumulative): 57 | DMM Complexity: 0.7692307692307693\n\nDIFF:\n@@ -4,7 +4,6 @@ conditions are met.\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-import warnings\n from collections import defaultdict\n \n from twisted.internet import reactor\n@@ -12,17 +11,15 @@ from twisted.python import log as txlog\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals, log\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.conf import settings\n \n class CloseSpider(object):\n \n     def __init__(self, crawler):\n         self.crawler = crawler\n-        self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n-        self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT')\n-        self.pagecount = settings.getint('CLOSESPIDER_PAGECOUNT')\n-        self.errorcount = settings.getint('CLOSESPIDER_ERRORCOUNT')\n+        self.timeout = crawler.settings.getint('CLOSESPIDER_TIMEOUT')\n+        self.itemcount = crawler.settings.getint('CLOSESPIDER_ITEMCOUNT')\n+        self.pagecount = crawler.settings.getint('CLOSESPIDER_PAGECOUNT')\n+        self.errorcount = crawler.settings.getint('CLOSESPIDER_ERRORCOUNT')\n \n         self.errorcounts = defaultdict(int)\n         self.pagecounts = defaultdict(int)\n\n@@ -4,18 +4,21 @@ from collections import defaultdict\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n-from scrapy.conf import settings\n from scrapy import log\n \n \n class CookiesMiddleware(object):\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n-    debug = settings.getbool('COOKIES_DEBUG')\n \n-    def __init__(self):\n-        if not settings.getbool('COOKIES_ENABLED'):\n-            raise NotConfigured\n+    def __init__(self, debug=False):\n         self.jars = defaultdict(CookieJar)\n+        self.debug = debug\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        if not crawler.settings.getbool('COOKIES_ENABLED'):\n+            raise NotConfigured\n+        return cls(crawler.settings.getbool('COOKIES_DEBUG'))\n \n     def process_request(self, request, spider):\n         if 'dont_merge_cookies' in request.meta:\n\n@@ -3,17 +3,21 @@ DefaultHeaders downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-from scrapy import conf\n from scrapy.utils.python import WeakKeyCache\n \n \n class DefaultHeadersMiddleware(object):\n \n-    def __init__(self, settings=conf.settings):\n+    def __init__(self, settings):\n         self._headers = WeakKeyCache(self._default_headers)\n+        self._settings = settings\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings)\n \n     def _default_headers(self, spider):\n-        return spider.settings.get('DEFAULT_REQUEST_HEADERS').items()\n+        return self._settings.get('DEFAULT_REQUEST_HEADERS').items()\n \n     def process_request(self, request, spider):\n         for k, v in self._headers[spider]:\n\n@@ -8,13 +8,18 @@ from scrapy.utils.python import WeakKeyCache\n \n class DownloadTimeoutMiddleware(object):\n \n-    def __init__(self):\n+    def __init__(self, timeout=180):\n         self._cache = WeakKeyCache(self._download_timeout)\n+        self._timeout = timeout\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings['DOWNLOAD_TIMEOUT'])\n \n     def _download_timeout(self, spider):\n         if hasattr(spider, 'download_timeout'):\n             return spider.download_timeout\n-        return spider.settings.getint('DOWNLOAD_TIMEOUT')\n+        return self._timeout\n \n     def process_request(self, request, spider):\n         timeout = self._cache[spider]\n\n@@ -15,12 +15,11 @@ from scrapy.utils.request import request_fingerprint\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.project import data_path\n-from scrapy import conf\n \n \n class HttpCacheMiddleware(object):\n \n-    def __init__(self, settings=conf.settings):\n+    def __init__(self, settings):\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n         self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n@@ -30,6 +29,10 @@ class HttpCacheMiddleware(object):\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings)\n+\n     def spider_opened(self, spider):\n         self.storage.open_spider(spider)\n \n@@ -66,7 +69,7 @@ class HttpCacheMiddleware(object):\n \n class FilesystemCacheStorage(object):\n \n-    def __init__(self, settings=conf.settings):\n+    def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n \n\n@@ -4,19 +4,22 @@ from scrapy import log\n from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n-from scrapy.conf import settings\n \n \n class RedirectMiddleware(object):\n     \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, settings):\n         if not settings.getbool('REDIRECT_ENABLED'):\n             raise NotConfigured\n         self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')\n         self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n         self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings)\n+\n     def process_response(self, request, response, spider):\n         if 'dont_redirect' in request.meta:\n             return response\n\n@@ -26,7 +26,6 @@ from twisted.internet.defer import TimeoutError as UserTimeoutError\n from scrapy import log\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n-from scrapy.conf import settings\n \n class RetryMiddleware(object):\n \n@@ -37,13 +36,17 @@ class RetryMiddleware(object):\n                            ConnectionLost, TCPTimedOutError,\n                            IOError)\n \n-    def __init__(self):\n+    def __init__(self, settings):\n         if not settings.getbool('RETRY_ENABLED'):\n             raise NotConfigured\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n         self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n         self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings)\n+\n     def process_response(self, request, response, spider):\n         if 'dont_retry' in request.meta:\n             return response\n\n@@ -2,13 +2,14 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.request import request_httprepr\n from scrapy.utils.response import response_httprepr\n from scrapy.stats import stats\n-from scrapy.conf import settings\n \n class DownloaderStats(object):\n \n-    def __init__(self):\n-        if not settings.getbool('DOWNLOADER_STATS'):\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        if not crawler.settings.getbool('DOWNLOADER_STATS'):\n             raise NotConfigured\n+        return cls()\n \n     def process_request(self, request, spider):\n         stats.inc_value('downloader/request_count', spider=spider)\n\n@@ -6,13 +6,18 @@ from scrapy.utils.python import WeakKeyCache\n class UserAgentMiddleware(object):\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n-    def __init__(self):\n+    def __init__(self, user_agent='Scrapy'):\n         self.cache = WeakKeyCache(self._user_agent)\n+        self.user_agent = user_agent\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings['USER_AGENT'])\n \n     def _user_agent(self, spider):\n         if hasattr(spider, 'user_agent'):\n             return spider.user_agent\n-        return spider.settings['USER_AGENT']\n+        return self.user_agent\n \n     def process_request(self, request, spider):\n         ua = self.cache[spider]\n\n@@ -19,7 +19,6 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n \n \n class IFeedStorage(Interface):\n@@ -82,6 +81,7 @@ class FileFeedStorage(object):\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri):\n+        from scrapy.conf import settings\n         try:\n             import boto\n         except ImportError:\n@@ -133,7 +133,8 @@ class SpiderSlot(object):\n \n class FeedExporter(object):\n \n-    def __init__(self):\n+    def __init__(self, settings):\n+        self.settings = settings\n         self.urifmt = settings['FEED_URI']\n         if not self.urifmt:\n             raise NotConfigured\n@@ -152,6 +153,10 @@ class FeedExporter(object):\n         dispatcher.connect(self.close_spider, signals.spider_closed)\n         dispatcher.connect(self.item_scraped, signals.item_scraped)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return FeedExporter(crawler.settings)\n+\n     def open_spider(self, spider):\n         uri = self.urifmt % self._get_uri_params(spider)\n         storage = self._get_storage(uri)\n@@ -179,8 +184,8 @@ class FeedExporter(object):\n         return item\n \n     def _load_components(self, setting_prefix):\n-        conf = dict(settings['%s_BASE' % setting_prefix])\n-        conf.update(settings[setting_prefix])\n+        conf = dict(self.settings['%s_BASE' % setting_prefix])\n+        conf.update(self.settings[setting_prefix])\n         d = {}\n         for k, v in conf.items():\n             try:\n\n@@ -6,12 +6,11 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n-from scrapy import conf\n \n \n class DbmCacheStorage(object):\n \n-    def __init__(self, settings=conf.settings):\n+    def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n         self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'])\n\n@@ -2,7 +2,6 @@ from twisted.internet import task\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n-from scrapy.conf import settings\n from scrapy import log, signals\n \n class Slot(object):\n@@ -16,10 +15,8 @@ class Slot(object):\n class LogStats(object):\n     \"\"\"Log basic scraping stats periodically\"\"\"\n \n-    def __init__(self):\n-        self.interval = settings.getfloat('LOGSTATS_INTERVAL')\n-        if not self.interval:\n-            raise NotConfigured\n+    def __init__(self, interval=60.0):\n+        self.interval = interval\n         self.slots = {}\n         self.multiplier = 60.0 / self.interval\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n@@ -29,6 +26,13 @@ class LogStats(object):\n         dispatcher.connect(self.engine_started, signal=signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n+        if not interval:\n+            raise NotConfigured\n+        return cls(interval)\n+\n     def item_scraped(self, spider):\n         self.slots[spider].items += 1\n \n\n@@ -10,24 +10,27 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n-from scrapy.conf import settings\n from scrapy.stats import stats\n from scrapy.utils.trackref import live_refs\n \n class MemoryDebugger(object):\n \n-    def __init__(self):\n+    def __init__(self, trackrefs=False):\n         try:\n             import libxml2\n             self.libxml2 = libxml2\n         except ImportError:\n             self.libxml2 = None\n-        if not settings.getbool('MEMDEBUG_ENABLED'):\n-            raise NotConfigured\n-\n+        self.trackrefs = trackrefs\n         dispatcher.connect(self.engine_started, signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n+            raise NotConfigured\n+        return cls(crawler.settings.getbool('TRACK_REFS'))\n+\n     def engine_started(self):\n         if self.libxml2:\n             self.libxml2.debugMemory(1)\n@@ -38,7 +41,7 @@ class MemoryDebugger(object):\n             stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))\n         gc.collect()\n         stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))\n-        if settings.getbool('TRACK_REFS'):\n+        if self.trackrefs:\n             for cls, wdict in live_refs.iteritems():\n                 if not wdict:\n                     continue\n\n@@ -10,7 +10,6 @@ import copy\n from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.spider import BaseSpider\n-from scrapy.conf import settings\n \n def identity(x):\n     return x\n@@ -70,11 +69,10 @@ class CrawlSpider(BaseSpider):\n             for requests_or_item in iterate_spider_output(cb_res):\n                 yield requests_or_item\n \n-        if follow and settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):\n+        if follow and self._follow_links:\n             for request_or_item in self._requests_to_follow(response):\n                 yield request_or_item\n \n-\n     def _compile_rules(self):\n         def get_method(method):\n             if callable(method):\n@@ -87,3 +85,7 @@ class CrawlSpider(BaseSpider):\n             rule.callback = get_method(rule.callback)\n             rule.process_links = get_method(rule.process_links)\n             rule.process_request = get_method(rule.process_request)\n+\n+    def set_crawler(self, crawler):\n+        super(CrawlSpider, self).set_crawler(crawler)\n+        self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n\n@@ -9,17 +9,21 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.stats import stats\n from scrapy import signals\n from scrapy.mail import MailSender\n-from scrapy.conf import settings\n from scrapy.exceptions import NotConfigured\n \n class StatsMailer(object):\n \n-    def __init__(self):\n-        self.recipients = settings.getlist(\"STATSMAILER_RCPTS\")\n+    def __init__(self, recipients):\n+        self.recipients = recipients\n         if not self.recipients:\n             raise NotConfigured\n         dispatcher.connect(self.stats_spider_closed, signal=signals.stats_spider_closed)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        recipients = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n+        return cls(recipients)\n+\n     def stats_spider_closed(self, spider, spider_stats):\n         mail = MailSender()\n         body = \"Global stats\\n\\n\"\n\n@@ -184,6 +184,7 @@ class Scraper(object):\n         \"\"\"\n         if spider_failure is download_failure:\n             errmsg = spider_failure.getErrorMessage()\n+            spider_failure.printTraceback()\n             if errmsg:\n                 log.msg(\"Error downloading %s: %s\" % (request, errmsg), log.ERROR, spider=spider)\n             return\n\n@@ -57,25 +57,3 @@ class CrawlerSettings(Settings):\n \n     def __str__(self):\n         return \"<CrawlerSettings module=%r>\" % self.settings_module\n-\n-\n-class SpiderSettings(Settings):\n-\n-    def __init__(self, spider, crawler_settings, **kw):\n-        super(SpiderSettings, self).__init__(**kw)\n-        self.spider = spider\n-        self.cset = crawler_settings\n-\n-    def __getitem__(self, opt_name):\n-        if opt_name in self.cset.overrides:\n-            return self.cset.overrides[opt_name]\n-        if hasattr(self.spider, opt_name):\n-            return getattr(self.spider, opt_name)\n-        if self.cset.settings_module and hasattr(self.cset.settings_module, opt_name):\n-            return getattr(self.cset.settings_module, opt_name)\n-        if opt_name in self.cset.defaults:\n-            return self.cset.defaults[opt_name]\n-        return super(SpiderSettings, self).__getitem__(opt_name)\n-\n-    def __str__(self):\n-        return \"<SpiderSettings spider=%r>\" % self.spider.name\n\n@@ -5,7 +5,6 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n from scrapy import log\n-from scrapy.settings import SpiderSettings\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.trackref import object_ref\n@@ -45,9 +44,7 @@ class BaseSpider(object_ref):\n \n     @property\n     def settings(self):\n-        if not hasattr(self, '_settings'):\n-            self._settings = SpiderSettings(self, self.crawler.settings)\n-        return self._settings\n+        return self.crawler.settings\n \n     def start_requests(self):\n         for url in self.start_urls:\n\n@@ -1,6 +1,5 @@\n from unittest import TestCase\n \n-from scrapy.conf import settings\n from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n@@ -15,7 +14,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         spider.set_crawler(crawler)\n         defaults = dict([(k, [v]) for k, v in \\\n             crawler.settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n-        return defaults, spider, DefaultHeadersMiddleware()\n+        return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)\n \n     def test_process_request(self):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n@@ -23,19 +22,6 @@ class TestDefaultHeadersMiddleware(TestCase):\n         mw.process_request(req, spider)\n         self.assertEquals(req.headers, defaults)\n \n-    def test_spider_default_request_headers(self):\n-        defaults, spider, mw = self.get_defaults_spider_mw()\n-        spider_headers = {'Unexistant-Header': ['value']}\n-        # override one of the global default headers by spider\n-        if defaults:\n-            k = set(defaults).pop()\n-            spider_headers[k] = ['__newvalue__']\n-        spider.DEFAULT_REQUEST_HEADERS = spider_headers\n-\n-        req = Request('http://www.scrapytest.org')\n-        mw.process_request(req, spider)\n-        self.assertEquals(req.headers, dict(spider_headers))\n-\n     def test_update_headers(self):\n         defaults, spider, mw = self.get_defaults_spider_mw()\n         headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}\n\n@@ -13,7 +13,7 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n         spider = BaseSpider('foo')\n         spider.set_crawler(crawler)\n         request = Request('http://scrapytest.org/')\n-        return request, spider, DownloadTimeoutMiddleware()\n+        return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n \n     def test_default_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n@@ -22,13 +22,13 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n \n     def test_spider_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n-        spider.DOWNLOAD_TIMEOUT = 2\n+        spider.download_timeout = 2\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta.get('download_timeout'), 2)\n \n     def test_request_has_download_timeout(self):\n         req, spider, mw = self.get_request_spider_mw()\n-        spider.DOWNLOAD_TIMEOUT = 2\n+        spider.download_timeout = 2\n         req.meta['download_timeout'] = 1\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta.get('download_timeout'), 1)\n\n@@ -4,12 +4,14 @@ from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, HtmlResponse, Headers\n+from scrapy.utils.test import get_crawler\n \n class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n+        crawler = get_crawler()\n         self.spider = BaseSpider('foo')\n-        self.mw = RedirectMiddleware()\n+        self.mw = RedirectMiddleware.from_crawler(crawler)\n \n     def test_priority_adjust(self):\n         req = Request('http://a.com')\n\n@@ -7,11 +7,13 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookup\n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n+from scrapy.utils.test import get_crawler\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n+        crawler = get_crawler()\n         self.spider = BaseSpider('foo')\n-        self.mw = RetryMiddleware()\n+        self.mw = RetryMiddleware.from_crawler(crawler)\n         self.mw.max_retry_times = 2\n \n     def test_priority_adjust(self):\n\n@@ -12,7 +12,7 @@ class UserAgentMiddlewareTest(TestCase):\n         crawler = get_crawler({'USER_AGENT': default_useragent})\n         spider = BaseSpider('foo')\n         spider.set_crawler(crawler)\n-        return spider, UserAgentMiddleware()\n+        return spider, UserAgentMiddleware.from_crawler(crawler)\n \n     def test_default_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n@@ -23,28 +23,28 @@ class UserAgentMiddlewareTest(TestCase):\n     def test_remove_agent(self):\n         # settings UESR_AGENT to None should remove the user agent\n         spider, mw = self.get_spider_and_mw('default_useragent')\n-        spider.USER_AGENT = None\n+        spider.user_agent = None\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n         assert req.headers.get('User-Agent') is None\n \n     def test_spider_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n-        spider.USER_AGENT = 'spider_useragent'\n+        spider.user_agent = 'spider_useragent'\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'spider_useragent')\n \n     def test_header_agent(self):\n         spider, mw = self.get_spider_and_mw('default_useragent')\n-        spider.USER_AGENT = 'spider_useragent'\n+        spider.user_agent = 'spider_useragent'\n         req = Request('http://scrapytest.org/', headers={'User-Agent': 'header_useragent'})\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['User-Agent'], 'header_useragent')\n \n     def test_no_agent(self):\n         spider, mw = self.get_spider_and_mw(None)\n-        spider.USER_AGENT = None\n+        spider.user_agent = None\n         req = Request('http://scrapytest.org/')\n         assert mw.process_request(req, spider) is None\n         assert 'User-Agent' not in req.headers\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.settings import Settings, SpiderSettings\n+from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler\n from scrapy.spider import BaseSpider\n \n@@ -68,35 +68,6 @@ class CrawlerSettingsTest(unittest.TestCase):\n         crawler.settings.overrides['DOWNLOAD_TIMEOUT'] = '15'\n         self.assertEqual(crawler.settings.getint('DOWNLOAD_TIMEOUT'), 15)\n \n-class SpiderSettingsTest(unittest.TestCase):\n-\n-    def test_global_defaults(self):\n-        crawler = get_crawler()\n-        settings = SpiderSettings(BaseSpider('name'), crawler.settings)\n-        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 180)\n-\n-    def test_defaults(self):\n-        crawler = get_crawler()\n-        crawler.settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n-        settings = SpiderSettings(BaseSpider('name'), crawler.settings)\n-        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 99)\n-\n-    def test_crawler_defaults(self):\n-        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n-        settings = SpiderSettings(BaseSpider('name'), crawler.settings)\n-        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 3)\n-\n-    def test_spider_overrides_crawler(self):\n-        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n-        crawler.settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n-        settings = SpiderSettings(BaseSpider('name', DOWNLOAD_TIMEOUT='12'), crawler.settings)\n-        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 12)\n-\n-    def test_overrides_most_precedence(self):\n-        crawler = get_crawler({'DOWNLOAD_TIMEOUT': '3'})\n-        crawler.settings.overrides['DOWNLOAD_TIMEOUT'] = '15'\n-        settings = SpiderSettings(BaseSpider('name', DOWNLOAD_TIMEOUT='12'), crawler.settings)\n-        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 15)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
