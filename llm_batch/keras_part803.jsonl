{"custom_id": "keras#dca1d8a081b5bdf7548953d5ce08596e3495d403", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 48 | Churn Cumulative: 3136 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,6 +44,7 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n     def make_model(self):\n         model = Sequential(\n             [\n+                layers.Input((3,)),\n                 CanaryLayer(),\n                 layers.Dense(1),\n             ]\n\n@@ -4,6 +4,7 @@ import string\n \n import numpy as np\n \n+from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import ops\n@@ -603,6 +604,37 @@ class MultiHeadAttention(Layer):\n \n         return query_shape\n \n+    def compute_output_spec(\n+        self,\n+        query,\n+        value,\n+        key=None,\n+        query_mask=None,\n+        value_mask=None,\n+        key_mask=None,\n+        attention_mask=None,\n+        return_attention_scores=False,\n+        training=None,\n+        use_causal_mask=False,\n+    ):\n+        if key is not None:\n+            key_shape = key.shape\n+        else:\n+            key_shape = None\n+        output_shape = self.compute_output_shape(\n+            query.shape, value.shape, key_shape\n+        )\n+        output_spec = backend.KerasTensor(\n+            output_shape, dtype=self.compute_dtype\n+        )\n+        if return_attention_scores:\n+            length = query.shape[1]\n+            attention_shape = (query.shape[0], self.num_heads, length, length)\n+            return output_spec, backend.KerasTensor(\n+                attention_shape, dtype=self.compute_dtype\n+            )\n+        return output_spec\n+\n \n def _index_to_einsum_variable(i):\n     \"\"\"Coverts an index to a einsum variable name.\n\n@@ -363,3 +363,18 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n         new_model.save_weights(temp_filepath)\n         model.load_weights(temp_filepath)\n         self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+    @parameterized.parameters([((1, 2, 3),), ((2, 3, 5),)])\n+    def test_symbolic_return_attention_scores(self, shape):\n+        mha = layers.MultiHeadAttention(num_heads=4, key_dim=2)\n+        x = layers.Input(batch_shape=shape)\n+        y = layers.Input(batch_shape=shape)\n+        symbolic_out = mha(x, y, return_attention_scores=True)\n+        self.assertLen(symbolic_out, 2)\n+\n+        x = np.random.random(shape)\n+        y = np.random.random(shape)\n+        out = mha(x, y, return_attention_scores=True)\n+        self.assertLen(out, 2)\n+        self.assertEqual(symbolic_out[0].shape, out[0].shape)\n+        self.assertEqual(symbolic_out[1].shape, out[1].shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30622e3fea21fdd1f1192a38206e20d4d450ddd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 13 | Files Changed: 16 | Hunks: 28 | Methods Changed: 26 | Complexity Δ (Sum/Max): 25/8 | Churn Δ: 93 | Churn Cumulative: 7923 | Contributors (this commit): 31 | Commits (past 90d): 80 | Contributors (cumulative): 100 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,8 +13,9 @@ class TorchLayer(torch.nn.Module):\n         self._track_variables()\n \n     def _track_variables(self):\n-        self.torch_params = torch.nn.ParameterList(\n-            [variable.value for variable in self.variables]\n+        # Index given to ParameterDict must be a string\n+        self.torch_params = torch.nn.ParameterDict(\n+            {str(id(variable)): variable.value for variable in self.variables}\n         )\n \n     def parameters(self, recurse=True):\n@@ -38,3 +39,16 @@ class TorchLayer(torch.nn.Module):\n             if not isinstance(self, TorchModuleWrapper):\n                 value = TorchModuleWrapper(value)\n         return name, value\n+\n+    def _post_track_variable(self, variable):\n+        if hasattr(self, \"torch_params\"):\n+            # Index given to ParameterDict must be a string\n+            key = str(id(variable))\n+            if key not in self.torch_params:\n+                self.torch_params[key] = variable.value\n+\n+    def _post_untrack_variable(self, variable):\n+        if hasattr(self, \"torch_params\"):\n+            # Index given to ParameterDict must be a string\n+            key = str(id(variable))\n+            self.torch_params.pop(key)\n\n@@ -40,7 +40,7 @@ class GroupedQueryAttentionTest(testing.TestCase, parameterized.TestCase):\n             expected_output_shape=(2, 8, 16),\n             expected_num_trainable_weights=4,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n             run_training_check=False,\n\n@@ -44,7 +44,7 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n             expected_output_shape=(2, 8, 16),\n             expected_num_trainable_weights=4,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=1,\n             expected_num_losses=0,\n             supports_masking=True,\n             run_training_check=False,\n\n@@ -209,6 +209,8 @@ class DenseTest(testing.TestCase):\n         layer.enable_lora(4)\n         self.assertLen(layer.trainable_weights, 3)\n         self.assertLen(layer.non_trainable_weights, 1)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 4)\n         # Try eager call\n         x = np.random.random((64, 8))\n         y = np.random.random((64, 16))\n@@ -434,6 +436,8 @@ class DenseTest(testing.TestCase):\n         layer.quantize(\"int8\")\n         self.assertLen(layer.trainable_weights, 3)\n         self.assertLen(layer.non_trainable_weights, 2)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 5)\n \n         # Try calling fit()\n         init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n\n@@ -296,6 +296,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         layer.enable_lora(2)\n         self.assertLen(layer.trainable_weights, 2)\n         self.assertLen(layer.non_trainable_weights, 1)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 3)\n         # Try eager call\n         x = np.random.random((64, 3))\n         y = np.random.random((64, 8, 32))\n@@ -532,6 +534,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         layer.quantize(\"int8\")\n         self.assertLen(layer.trainable_weights, 2)\n         self.assertLen(layer.non_trainable_weights, 2)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 4)\n \n         # Try calling fit()\n         init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n\n@@ -121,6 +121,8 @@ class EmbeddingTest(test_case.TestCase):\n         layer.enable_lora(4)\n         self.assertLen(layer.trainable_weights, 2)\n         self.assertLen(layer.non_trainable_weights, 1)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 3)\n         # Try eager call\n         x = np.random.randint(0, 9, size=(64, 3))\n         y = np.random.random((64, 3, 16))\n@@ -323,6 +325,8 @@ class EmbeddingTest(test_case.TestCase):\n         layer.quantize(\"int8\")\n         self.assertLen(layer.trainable_weights, 2)\n         self.assertLen(layer.non_trainable_weights, 2)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 4)\n \n         # Try calling fit()\n         init_lora_a_embeddings_value = layer.lora_embeddings_a.numpy()\n\n@@ -1203,6 +1203,7 @@ class Layer(BackendLayer, Operation):\n             self._tracker.add_to_store(\"non_trainable_variables\", variable)\n         if not self.trainable:\n             variable.trainable = False\n+        self._post_track_variable(variable)\n \n     def _untrack_variable(self, variable):\n         previous_lock_state = self._tracker.locked\n@@ -1210,6 +1211,7 @@ class Layer(BackendLayer, Operation):\n         self._tracker.untrack(variable)\n         if previous_lock_state is True:\n             self._tracker.lock()\n+        self._post_untrack_variable(variable)\n \n     def add_metric(self):\n         # Permanently disabled\n\n@@ -147,7 +147,6 @@ class Discretization(TFDataLayer):\n         self.sparse = sparse\n \n         if self.bin_boundaries:\n-            self.built = True\n             self.summary = None\n         else:\n             self.summary = np.array([[], []], dtype=\"float32\")\n\n@@ -43,9 +43,9 @@ class AlphaDropout(Layer):\n         self.rate = rate\n         self.seed = seed\n         self.noise_shape = noise_shape\n+        if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n-        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -52,7 +52,6 @@ class Dropout(Layer):\n         if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n-        self.built = True\n \n     def call(self, inputs, training=False):\n         if training and self.rate > 0:\n\n@@ -34,6 +34,7 @@ class GaussianDropout(layers.Layer):\n             )\n         self.rate = rate\n         self.seed = seed\n+        if rate > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n \n\n@@ -35,6 +35,7 @@ class GaussianNoise(layers.Layer):\n             )\n         self.stddev = stddev\n         self.seed = seed\n+        if stddev > 0:\n             self.seed_generator = backend.random.SeedGenerator(seed)\n         self.supports_masking = True\n \n\n@@ -95,7 +95,7 @@ class StackedRNNTest(testing.TestCase):\n             expected_output_shape=(2, 3, 5),\n             expected_num_trainable_weights=9,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=3,\n             supports_masking=True,\n         )\n         self.run_layer_test(\n@@ -112,7 +112,7 @@ class StackedRNNTest(testing.TestCase):\n             expected_output_shape=(2, 3, 5),\n             expected_num_trainable_weights=9,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=3,\n             supports_masking=True,\n         )\n         self.run_layer_test(\n@@ -129,7 +129,7 @@ class StackedRNNTest(testing.TestCase):\n             expected_output_shape=(2, 3, 5),\n             expected_num_trainable_weights=9,\n             expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n+            expected_num_seed_generators=3,\n             supports_masking=True,\n         )\n \n\n@@ -280,3 +280,11 @@ class Operation:\n     def _setattr_hook(self, name, value):\n         \"\"\"Can be overridden for per backend post build actions.\"\"\"\n         return name, value\n+\n+    def _post_track_variable(self, variable):\n+        \"\"\"Can be overridden for per backend post track actions.\"\"\"\n+        pass\n+\n+    def _post_untrack_variable(self, variable):\n+        \"\"\"Can be overridden for per backend post untrack actions.\"\"\"\n+        pass\n\n@@ -6,6 +6,7 @@ from keras import backend\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n from keras.utils import jax_utils\n+from keras.utils.naming import auto_name\n \n \n @keras_export(\"keras.random.SeedGenerator\")\n@@ -44,7 +45,11 @@ class SeedGenerator:\n     ```\n     \"\"\"\n \n-    def __init__(self, seed=None, **kwargs):\n+    def __init__(self, seed=None, name=None, **kwargs):\n+        if name is None:\n+            name = auto_name(self.__class__.__name__)\n+        self.name = name\n+\n         custom_backend = kwargs.pop(\"backend\", None)\n         if kwargs:\n             raise ValueError(f\"Unrecognized keyword arguments: {kwargs}\")\n@@ -66,6 +71,7 @@ class SeedGenerator:\n             dtype = kwargs.get(\"dtype\", None)\n             return self.backend.convert_to_tensor([seed, 0], dtype=dtype)\n \n+        with backend.name_scope(self.name, caller=self):\n             self.state = self.backend.Variable(\n                 seed_initializer,\n                 shape=(2,),\n\n@@ -291,9 +291,22 @@ class TestCase(unittest.TestCase):\n                 )\n             if expected_num_seed_generators is not None:\n                 self.assertLen(\n-                    layer._seed_generators,\n+                    get_seed_generators(layer),\n                     expected_num_seed_generators,\n-                    msg=\"Unexpected number of _seed_generators\",\n+                    msg=\"Unexpected number of seed_generators\",\n+                )\n+            if (\n+                backend.backend() == \"torch\"\n+                and expected_num_trainable_weights is not None\n+                and expected_num_non_trainable_weights is not None\n+                and expected_num_seed_generators is not None\n+            ):\n+                self.assertLen(\n+                    layer.torch_params,\n+                    expected_num_trainable_weights\n+                    + expected_num_non_trainable_weights\n+                    + expected_num_seed_generators,\n+                    msg=\"Unexpected number of torch_params\",\n                 )\n \n         def run_output_asserts(layer, output, eager=False):\n@@ -662,3 +675,15 @@ def map_shape_dtype_structure(fn, shape, dtype):\n         raise ValueError(\n             f\"Cannot map function to unknown objects {shape} and {dtype}\"\n         )\n+\n+\n+def get_seed_generators(layer):\n+    \"\"\"Get a List of all seed generators in the layer recursively.\"\"\"\n+    seed_generators = []\n+    seen_ids = set()\n+    for sublayer in layer._flatten_layers(True, True):\n+        for sg in sublayer._seed_generators:\n+            if id(sg) not in seen_ids:\n+                seed_generators.append(sg)\n+                seen_ids.add(id(sg))\n+    return seed_generators\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c30d86711184b88009643cb206bdab4562f790e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 28 | Churn Cumulative: 1562 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -231,7 +231,7 @@ def save_subset_weights_to_hdf5_group(f, weights):\n         weights: List of weight variables.\n     \"\"\"\n     weight_values = [backend.convert_to_numpy(w) for w in weights]\n-    weight_names = [w.name.encode(\"utf8\") for w in weights]\n+    weight_names = [str(w.path).encode(\"utf8\") for w in weights]\n     save_attributes_to_hdf5_group(f, \"weight_names\", weight_names)\n     for name, val in zip(weight_names, weight_values):\n         param_dset = f.create_dataset(name, val.shape, dtype=val.dtype)\n@@ -255,7 +255,7 @@ def save_optimizer_weights_to_hdf5_group(hdf5_group, optimizer):\n         symbolic_weights = getattr(optimizer, \"weights\")\n     if symbolic_weights:\n         weights_group = hdf5_group.create_group(\"optimizer_weights\")\n-        weight_names = [str(w.name).encode(\"utf8\") for w in symbolic_weights]\n+        weight_names = [str(w.path).encode(\"utf8\") for w in symbolic_weights]\n         save_attributes_to_hdf5_group(\n             weights_group, \"weight_names\", weight_names\n         )\n@@ -464,7 +464,7 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                         warnings.warn(\n                             f\"Skipping loading weights for layer #{k} (named \"\n                             f\"{layer.name}) due to mismatch in shape for \"\n-                            f\"weight {symbolic_weights[i].name}. \"\n+                            f\"weight {symbolic_weights[i].path}. \"\n                             f\"Weight expects shape {expected_shape}. \"\n                             \"Received saved weight \"\n                             f\"with shape {received_shape}\",\n@@ -473,7 +473,7 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                         continue\n                     raise ValueError(\n                         f\"Shape mismatch in layer #{k} (named {layer.name}) \"\n-                        f\"for weight {symbolic_weights[i].name}. \"\n+                        f\"for weight {symbolic_weights[i].path}. \"\n                         f\"Weight expects shape {expected_shape}. \"\n                         \"Received saved weight \"\n                         f\"with shape {received_shape}\"\n@@ -513,7 +513,7 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                         warnings.warn(\n                             \"Skipping loading top-level weight for model due \"\n                             \"to mismatch in shape for \"\n-                            f\"weight {symbolic_weights[i].name}. \"\n+                            f\"weight {symbolic_weights[i].path}. \"\n                             f\"Weight expects shape {expected_shape}. \"\n                             \"Received saved weight \"\n                             f\"with shape {received_shape}\",\n@@ -522,7 +522,7 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                     else:\n                         raise ValueError(\n                             \"Shape mismatch in model for top-level weight \"\n-                            f\"{symbolic_weights[i].name}. \"\n+                            f\"{symbolic_weights[i].path}. \"\n                             f\"Weight expects shape {expected_shape}. \"\n                             \"Received saved weight \"\n                             f\"with shape {received_shape}\"\n\n@@ -845,3 +845,19 @@ class SavingBattleTest(testing.TestCase):\n         self.assertAllClose(\n             model_a.dense.kernel.numpy(), model_b.dense.kernel.numpy()\n         )\n+\n+    def test_legacy_h5_format(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"custom_model.h5\")\n+\n+        inputs = keras.Input((32,))\n+        x = MyDense(2)(inputs)\n+        outputs = CustomModelX()(x)\n+        model = keras.Model(inputs, outputs)\n+\n+        x = np.random.random((1, 32))\n+        ref_out = model(x)\n+\n+        model.save(temp_filepath)\n+        new_model = keras.saving.load_model(temp_filepath)\n+        out = new_model(x)\n+        self.assertAllClose(ref_out, out, atol=1e-6)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5187fac82ab979e7dbf39e16ea31c87d075029a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 59 | Churn Cumulative: 259 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.9038461538461539\n\nDIFF:\n@@ -82,11 +82,16 @@ class Tracker:\n                 if not self.is_in_store(store_name, attr):\n                     self.add_to_store(store_name, attr)\n                 return attr\n+        if isinstance(attr, tuple) and hasattr(attr, \"_fields\"):\n+            # Named tuple case.\n+            wrapped_attr = {}\n+            for name, e in attr._asdict().items():\n+                wrapped_attr[name] = self.track(e)\n+            return attr.__class__(**wrapped_attr)\n         if isinstance(attr, tuple):\n             wrapped_attr = []\n             for e in attr:\n                 wrapped_attr.append(self.track(e))\n-            # This should cover tuples and nametuples\n             return attr.__class__(wrapped_attr)\n         elif isinstance(attr, list):\n             return TrackedList(attr, self)\n@@ -120,6 +125,15 @@ class Tracker:\n     def is_in_store(self, store_name, value):\n         return id(value) in self.stored_ids[store_name]\n \n+    def replace_tracked_value(self, store_name, old_value, new_value):\n+        if not self.is_in_store(store_name, old_value):\n+            raise ValueError(f\"Unknown value: {old_value}\")\n+        store_list = self.config[store_name][1]\n+        index = store_list.index(old_value)\n+        store_list[index] = new_value\n+        self.stored_ids[store_name].remove(id(old_value))\n+        self.stored_ids[store_name].add(id(new_value))\n+\n \n @optree.register_pytree_node_class(namespace=\"keras\")\n class TrackedList(list):\n\n@@ -1,3 +1,5 @@\n+import collections\n+\n from keras import backend\n from keras import testing\n from keras.utils import tracking\n@@ -54,3 +56,44 @@ class TrackingTest(testing.TestCase):\n         del lst2[0]\n         self.assertLen(lst2, 1)\n         self.assertLen(tracked_variables, 1)\n+\n+    def test_tuple_tracking(self):\n+        tracked_variables = []\n+        tracker = tracking.Tracker(\n+            {\n+                \"variables\": (\n+                    lambda x: isinstance(x, backend.Variable),\n+                    tracked_variables,\n+                ),\n+            }\n+        )\n+        v1 = backend.Variable(1)\n+        v2 = backend.Variable(2)\n+        tup = (v1, v2)\n+        tup = tracker.track(tup)\n+        self.assertIsInstance(tup, tuple)\n+        self.assertLen(tracked_variables, 2)\n+        self.assertEqual(tracked_variables[0], v1)\n+        self.assertEqual(tracked_variables[1], v2)\n+\n+    def test_namedtuple_tracking(self):\n+        tracked_variables = []\n+        tracker = tracking.Tracker(\n+            {\n+                \"variables\": (\n+                    lambda x: isinstance(x, backend.Variable),\n+                    tracked_variables,\n+                ),\n+            }\n+        )\n+        v1 = backend.Variable(1)\n+        v2 = backend.Variable(2)\n+        nt = collections.namedtuple(\"NT\", [\"x\", \"y\"])\n+        tup = nt(x=v1, y=v2)\n+        tup = tracker.track(tup)\n+        self.assertIsInstance(tup, tuple)\n+        self.assertEqual(tup.x, v1)\n+        self.assertEqual(tup.y, v2)\n+        self.assertLen(tracked_variables, 2)\n+        self.assertEqual(tracked_variables[0], v1)\n+        self.assertEqual(tracked_variables[1], v2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b267f939b70970e38eec20e3977a1bf41cec8cdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 856 | Contributors (this commit): 9 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -102,8 +102,8 @@ class Embedding(Layer):\n         self.lora_rank = lora_rank\n         self.lora_enabled = False\n \n-        self.build()\n         if weights is not None:\n+            self.build()\n             if not (isinstance(weights, list) and len(weights) == 1):\n                 weights = [weights]\n             self.set_weights(weights)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#032cdff18a227877e167b80654704ff746f9c17c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 155 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 18 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 156 | Churn Cumulative: 1009 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -515,7 +515,7 @@ def _extract_patches(\n \n \n class MapCoordinates(Operation):\n-    def __init__(self, order, fill_mode=\"constant\", fill_value=0):\n+    def __init__(self, order=1, fill_mode=\"constant\", fill_value=0):\n         super().__init__()\n         self.order = order\n         self.fill_mode = fill_mode\n\n@@ -602,3 +602,157 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             backend.convert_to_numpy(cropped_image),\n             atol=1e-5,\n         )\n+\n+    def test_rgb_to_grayscale_invalid_rank_two_tensor(self):\n+        rgb_to_gray = kimage.RGBToGrayscale()\n+        invalid_image = np.random.uniform(size=(10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid image rank: expected rank 3\",\n+        ):\n+            rgb_to_gray.compute_output_spec(invalid_image)\n+\n+    def test_rgb_to_grayscale_invalid_rank_five_tensor(self):\n+        rgb_to_gray = kimage.RGBToGrayscale()\n+        invalid_image = np.random.uniform(size=(2, 3, 10, 10, 3))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid image rank: expected rank 3\",\n+        ):\n+            rgb_to_gray.compute_output_spec(invalid_image)\n+\n+    def test_rgb_to_grayscale_valid_rank_three_tensor(self):\n+        rgb_to_gray = kimage.RGBToGrayscale()\n+        valid_image = np.random.uniform(size=(10, 10, 3))\n+        output_spec = rgb_to_gray.compute_output_spec(valid_image)\n+        self.assertEqual(\n+            output_spec.shape,\n+            (10, 10, 1),\n+            \"Output shape should match expected grayscale image shape\",\n+        )\n+\n+    def test_rgb_to_grayscale_valid_rank_four_tensor(self):\n+        rgb_to_gray = kimage.RGBToGrayscale()\n+        valid_image = np.random.uniform(size=(5, 10, 10, 3))\n+        output_spec = rgb_to_gray.compute_output_spec(valid_image)\n+        self.assertEqual(\n+            output_spec.shape,\n+            (5, 10, 10, 1),\n+            \"Output shape should match expected grayscale image shape\",\n+        )\n+\n+    def test_affine_transform_compute_output_spec_image_rank_too_low(self):\n+        affine_transform = kimage.AffineTransform()\n+        # Test with an image of rank 2 (invalid)\n+        image_2d = np.random.uniform(size=(10, 10))\n+        transform_valid = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid image rank: expected rank 3\"\n+        ):\n+            affine_transform.compute_output_spec(image_2d, transform_valid)\n+\n+    def test_affine_transform_compute_output_spec_image_rank_too_high(self):\n+        affine_transform = kimage.AffineTransform()\n+        # Test with an image of rank 5 (invalid)\n+        image_5d = np.random.uniform(size=(2, 10, 10, 3, 1))\n+        transform_valid = np.random.uniform(size=(6,))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid image rank: expected rank 3\"\n+        ):\n+            affine_transform.compute_output_spec(image_5d, transform_valid)\n+\n+    def test_affine_transform_compute_output_spec_transform_rank_too_high(self):\n+        affine_transform = kimage.AffineTransform()\n+        # Test with a valid image rank 3\n+        image_valid = np.random.uniform(size=(10, 10, 3))\n+        # Test with a transform of rank 3 (invalid)\n+        transform_invalid_rank3 = np.random.uniform(size=(2, 3, 2))\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            affine_transform.compute_output_spec(\n+                image_valid, transform_invalid_rank3\n+            )\n+\n+    def test_affine_transform_compute_output_spec_transform_rank_too_low(self):\n+        affine_transform = kimage.AffineTransform()\n+        # Test with a valid image rank 3\n+        image_valid = np.random.uniform(size=(10, 10, 3))\n+        # Test with a transform of rank 0 (invalid)\n+        transform_invalid_rank0 = np.random.uniform(size=())\n+        with self.assertRaisesRegex(\n+            ValueError, \"Invalid transform rank: expected rank 1\"\n+        ):\n+            affine_transform.compute_output_spec(\n+                image_valid, transform_invalid_rank0\n+            )\n+\n+    def test_extract_patches_with_invalid_tuple_size(self):\n+        size = (3, 3, 3)  # Invalid size, too many dimensions\n+        image = np.random.uniform(size=(2, 20, 20, 3))\n+        with self.assertRaisesRegex(\n+            TypeError, \"Expected an int or a tuple of length 2\"\n+        ):\n+            kimage.extract_patches(image, size)\n+\n+    def test_extract_patches_with_incorrect_type_size(self):\n+        size = \"5\"  # Invalid size type\n+        image = np.random.uniform(size=(2, 20, 20, 3))\n+        with self.assertRaisesRegex(\n+            TypeError, \"Expected an int or a tuple of length 2\"\n+        ):\n+            kimage.extract_patches(image, size)\n+\n+    def test_extract_patches_with_integer_size(self):\n+        size = 5\n+        # Use float32 for compatibility with TensorFlow convolution operations\n+        image = np.random.uniform(size=(1, 20, 20, 3)).astype(np.float32)\n+        patches = kimage.extract_patches(image, size)\n+        # Expecting 4x4 patches with each patch having 75 values (5x5x3)\n+        expected_shape = (1, 4, 4, 75)\n+        self.assertEqual(patches.shape, expected_shape)\n+\n+    def test_extract_patches_with_tuple_size(self):\n+        size = (5, 5)\n+        image = np.random.uniform(size=(1, 20, 20, 3)).astype(np.float32)\n+        patches = kimage.extract_patches(image, size)\n+        # Expecting 4x4 patches with each patch having 75 values (5x5x3)\n+        expected_shape = (1, 4, 4, 75)\n+        self.assertEqual(patches.shape, expected_shape)\n+\n+    def test_map_coordinates_image_coordinates_rank_mismatch(self):\n+        map_coordinates = kimage.MapCoordinates()\n+        image = np.random.uniform(size=(10, 10, 3))\n+        coordinates = np.random.uniform(size=(2, 10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"must be the same as the rank of `image`\"\n+        ):\n+            map_coordinates.compute_output_spec(image, coordinates)\n+\n+    def test_map_coordinates_image_coordinates_rank_mismatch_order_zero(self):\n+        map_coordinates = kimage.MapCoordinates(order=0)\n+        image = np.random.uniform(size=(10, 10, 3))\n+        coordinates = np.random.uniform(size=(2, 10, 10))\n+        with self.assertRaisesRegex(\n+            ValueError, \"must be the same as the rank of `image`\"\n+        ):\n+            map_coordinates.compute_output_spec(image, coordinates)\n+\n+    def test_map_coordinates_coordinates_rank_too_low(self):\n+        map_coordinates = kimage.MapCoordinates()\n+        image = np.random.uniform(size=(10, 10, 3))\n+        coordinates = np.random.uniform(size=(3,))\n+        with self.assertRaisesRegex(ValueError, \"expected at least rank 2\"):\n+            map_coordinates.compute_output_spec(image, coordinates)\n+\n+    def test_map_coordinates_valid_input(self):\n+        map_coordinates = kimage.MapCoordinates()\n+        image = np.random.uniform(size=(10, 10, 3))\n+        coordinates = np.random.uniform(size=(3, 10, 10))\n+        output_spec = map_coordinates.compute_output_spec(image, coordinates)\n+        expected_shape = (10, 10)\n+        self.assertEqual(\n+            output_spec.shape,\n+            expected_shape,\n+            \"Output shape should be correct for valid inputs\",\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7dae3e918fbc853cb511dfb680757096d6473aa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 5 | Methods Changed: 10 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 57 | Churn Cumulative: 1852 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -480,7 +480,7 @@ def functional_from_config(cls, config, custom_objects=None):\n         layer(*args, **kwargs)\n \n     def process_layer(layer_data):\n-        \"\"\"Deserializes a layer, then call it on appropriate inputs.\n+        \"\"\"Deserializes a layer and index its inbound nodes.\n \n         Args:\n             layer_data: layer config dict.\n@@ -553,6 +553,9 @@ def functional_from_config(cls, config, custom_objects=None):\n     def get_tensor(layer_name, node_index, tensor_index):\n         assert layer_name in created_layers\n         layer = created_layers[layer_name]\n+        if isinstance(layer, Functional):\n+            # Functional models start out with a built-in node.\n+            node_index -= 1\n         layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n         return layer_output_tensors[tensor_index]\n \n@@ -613,8 +616,9 @@ def serialize_node(node, own_nodes=()):\n         if isinstance(x, backend.KerasTensor):\n             operation, node_index, tensor_index = x._keras_history\n             irrelevant_node_count = 0\n-            for node in operation._inbound_nodes[:node_index]:\n-                if node not in own_nodes:\n+            for i, node in enumerate(operation._inbound_nodes[:node_index]):\n+                node_key = make_node_key(operation, i)\n+                if node_key not in own_nodes:\n                     irrelevant_node_count += 1\n             x._keras_history = KerasHistory(\n                 operation, node_index - irrelevant_node_count, tensor_index\n\n@@ -82,7 +82,7 @@ class Node:\n         self.is_input = not self.arguments.keras_tensors\n \n     def __repr__(self):\n-        return f\"<Node operation={self.operation}, id={id(self)}>\"\n+        return f\"<Node operation={self.operation.name}, id={id(self)}>\"\n \n     @property\n     def input_tensors(self):\n\n@@ -861,3 +861,48 @@ class SavingBattleTest(testing.TestCase):\n         new_model = keras.saving.load_model(temp_filepath)\n         out = new_model(x)\n         self.assertAllClose(ref_out, out, atol=1e-6)\n+\n+    def test_nested_functional_model_saving(self):\n+        def func(in_size=4, out_size=2, name=None):\n+            inputs = keras.layers.Input(shape=(in_size,))\n+            outputs = keras.layers.Dense(out_size)((inputs))\n+            return keras.Model(inputs, outputs=outputs, name=name)\n+\n+        input_a, input_b = keras.Input((4,)), keras.Input((4,))\n+        out_a = func(out_size=2, name=\"func_a\")(input_a)\n+        out_b = func(out_size=3, name=\"func_b\")(input_b)\n+        model = keras.Model([input_a, input_b], outputs=[out_a, out_b])\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"nested_func.keras\")\n+        model.save(temp_filepath)\n+        new_model = keras.saving.load_model(temp_filepath)\n+        x = [np.random.random((2, 4))], np.random.random((2, 4))\n+        ref_out = model(x)\n+        out = new_model(x)\n+        self.assertAllClose(ref_out[0], out[0])\n+        self.assertAllClose(ref_out[1], out[1])\n+\n+    def test_nested_shared_functional_model_saving(self):\n+        def func(in_size=4, out_size=2, name=None):\n+            inputs = keras.layers.Input(shape=(in_size,))\n+            outputs = keras.layers.Dense(out_size)((inputs))\n+            return keras.Model(inputs, outputs=outputs, name=name)\n+\n+        inputs = [keras.Input((4,)), keras.Input((4,))]\n+        func_shared = func(out_size=4, name=\"func_shared\")\n+        shared_a = func_shared(inputs[0])\n+        shared_b = func_shared(inputs[1])\n+        out_a = keras.layers.Dense(2)(shared_a)\n+        out_b = keras.layers.Dense(2)(shared_b)\n+        model = keras.Model(inputs, outputs=[out_a, out_b])\n+\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"nested_shared_func.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = keras.saving.load_model(temp_filepath)\n+        x = [np.random.random((2, 4))], np.random.random((2, 4))\n+        ref_out = model(x)\n+        out = new_model(x)\n+        self.assertAllClose(ref_out[0], out[0])\n+        self.assertAllClose(ref_out[1], out[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e7610ee9757658bef3488471ccaf9963e07a823", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 176 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 22 | Methods Changed: 9 | Complexity Δ (Sum/Max): 16/5 | Churn Δ: 189 | Churn Cumulative: 967 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 0.05263157894736842\n\nDIFF:\n@@ -42,6 +42,7 @@ def resize(\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n+    crop_to_aspect_ratio=False,\n     data_format=\"channels_last\",\n ):\n     if interpolation not in RESIZE_INTERPOLATIONS:\n@@ -55,6 +56,7 @@ def resize(\n             f\"(height, width). Received: size={size}\"\n         )\n     size = tuple(size)\n+    target_height, target_width = size\n     if len(image.shape) == 4:\n         if data_format == \"channels_last\":\n             size = (image.shape[0],) + size + (image.shape[-1],)\n@@ -71,6 +73,48 @@ def resize(\n             \"or rank 4 (batch of images). Received input with shape: \"\n             f\"image.shape={image.shape}\"\n         )\n+\n+    if crop_to_aspect_ratio:\n+        shape = image.shape\n+        if data_format == \"channels_last\":\n+            height, width = shape[-3], shape[-2]\n+        else:\n+            height, width = shape[-2], shape[-1]\n+        crop_height = int(float(width * target_height) / target_width)\n+        crop_height = min(height, crop_height)\n+        crop_width = int(float(height * target_width) / target_height)\n+        crop_width = min(width, crop_width)\n+        crop_box_hstart = int(float(height - crop_height) / 2)\n+        crop_box_wstart = int(float(width - crop_width) / 2)\n+        if data_format == \"channels_last\":\n+            if len(image.shape) == 4:\n+                image = image[\n+                    :,\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                    :,\n+                ]\n+            else:\n+                image = image[\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                    :,\n+                ]\n+        else:\n+            if len(image.shape) == 4:\n+                image = image[\n+                    :,\n+                    :,\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                ]\n+            else:\n+                image = image[\n+                    :,\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                ]\n+\n     return jax.image.resize(\n         image, size, method=interpolation, antialias=antialias\n     )\n\n@@ -41,6 +41,7 @@ def resize(\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n+    crop_to_aspect_ratio=False,\n     data_format=\"channels_last\",\n ):\n     if interpolation not in RESIZE_INTERPOLATIONS:\n@@ -54,6 +55,7 @@ def resize(\n             f\"(height, width). Received: size={size}\"\n         )\n     size = tuple(size)\n+    target_height, target_width = size\n     if len(image.shape) == 4:\n         if data_format == \"channels_last\":\n             size = (image.shape[0],) + size + (image.shape[-1],)\n@@ -70,6 +72,48 @@ def resize(\n             \"or rank 4 (batch of images). Received input with shape: \"\n             f\"image.shape={image.shape}\"\n         )\n+\n+    if crop_to_aspect_ratio:\n+        shape = image.shape\n+        if data_format == \"channels_last\":\n+            height, width = shape[-3], shape[-2]\n+        else:\n+            height, width = shape[-2], shape[-1]\n+        crop_height = int(float(width * target_height) / target_width)\n+        crop_height = min(height, crop_height)\n+        crop_width = int(float(height * target_width) / target_height)\n+        crop_width = min(width, crop_width)\n+        crop_box_hstart = int(float(height - crop_height) / 2)\n+        crop_box_wstart = int(float(width - crop_width) / 2)\n+        if data_format == \"channels_last\":\n+            if len(image.shape) == 4:\n+                image = image[\n+                    :,\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                    :,\n+                ]\n+            else:\n+                image = image[\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                    :,\n+                ]\n+        else:\n+            if len(image.shape) == 4:\n+                image = image[\n+                    :,\n+                    :,\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                ]\n+            else:\n+                image = image[\n+                    :,\n+                    crop_box_hstart : crop_box_hstart + crop_height,\n+                    crop_box_wstart : crop_box_wstart + crop_width,\n+                ]\n+\n     return np.array(\n         jax.image.resize(image, size, method=interpolation, antialias=antialias)\n     )\n\n@@ -41,6 +41,7 @@ def resize(\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n+    crop_to_aspect_ratio=False,\n     data_format=\"channels_last\",\n ):\n     if interpolation not in RESIZE_INTERPOLATIONS:\n@@ -65,6 +66,42 @@ def resize(\n                 \"or rank 4 (batch of images). Received input with shape: \"\n                 f\"image.shape={image.shape}\"\n             )\n+    if crop_to_aspect_ratio:\n+        shape = tf.shape(image)\n+        height, width = shape[-3], shape[-2]\n+        target_height, target_width = size\n+        crop_height = tf.cast(\n+            tf.cast(width * target_height, \"float32\") / target_width,\n+            \"int32\",\n+        )\n+        crop_height = tf.minimum(height, crop_height)\n+        crop_height = tf.cast(crop_height, \"int32\")\n+        crop_width = tf.cast(\n+            tf.cast(height * target_width, \"float32\") / target_height,\n+            \"int32\",\n+        )\n+        crop_width = tf.minimum(width, crop_width)\n+        crop_width = tf.cast(crop_width, \"int32\")\n+\n+        crop_box_hstart = tf.cast(\n+            tf.cast(height - crop_height, \"float32\") / 2, \"int32\"\n+        )\n+        crop_box_wstart = tf.cast(\n+            tf.cast(width - crop_width, \"float32\") / 2, \"int32\"\n+        )\n+        if len(image.shape) == 4:\n+            image = image[\n+                :,\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+                :,\n+            ]\n+        else:\n+            image = image[\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+                :,\n+            ]\n \n     resized = tf.image.resize(\n         image, size, method=interpolation, antialias=antialias\n\n@@ -51,6 +51,7 @@ def resize(\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n+    crop_to_aspect_ratio=False,\n     data_format=\"channels_last\",\n ):\n     try:\n@@ -99,6 +100,30 @@ def resize(\n                 f\"image.shape={image.shape}\"\n             )\n \n+    if crop_to_aspect_ratio:\n+        shape = image.shape\n+        height, width = shape[-2], shape[-1]\n+        target_height, target_width = size\n+        crop_height = int(float(width * target_height) / target_width)\n+        crop_height = min(height, crop_height)\n+        crop_width = int(float(height * target_width) / target_height)\n+        crop_width = min(width, crop_width)\n+        crop_box_hstart = int(float(height - crop_height) / 2)\n+        crop_box_wstart = int(float(width - crop_width) / 2)\n+        if len(image.shape) == 4:\n+            image = image[\n+                :,\n+                :,\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+            ]\n+        else:\n+            image = image[\n+                :,\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+            ]\n+\n     resized = torchvision.transforms.functional.resize(\n         img=image,\n         size=size,\n\n@@ -1,7 +1,6 @@\n from keras import backend\n from keras.api_export import keras_export\n from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras.utils import image_utils\n \n \n @keras_export(\"keras.layers.Resizing\")\n@@ -69,22 +68,13 @@ class Resizing(TFDataLayer):\n \n     def call(self, inputs):\n         size = (self.height, self.width)\n-        if self.crop_to_aspect_ratio:\n-            outputs = image_utils.smart_resize(\n+        return self.backend.image.resize(\n             inputs,\n             size=size,\n             interpolation=self.interpolation,\n             data_format=self.data_format,\n-                backend_module=self.backend,\n+            crop_to_aspect_ratio=self.crop_to_aspect_ratio,\n         )\n-        else:\n-            outputs = self.backend.image.resize(\n-                inputs,\n-                size=size,\n-                interpolation=self.interpolation,\n-                data_format=self.data_format,\n-            )\n-        return outputs\n \n     def compute_output_shape(self, input_shape):\n         input_shape = list(input_shape)\n\n@@ -110,6 +110,7 @@ class Resize(Operation):\n         size,\n         interpolation=\"bilinear\",\n         antialias=False,\n+        crop_to_aspect_ratio=False,\n         data_format=\"channels_last\",\n     ):\n         super().__init__()\n@@ -117,6 +118,7 @@ class Resize(Operation):\n         self.interpolation = interpolation\n         self.antialias = antialias\n         self.data_format = data_format\n+        self.crop_to_aspect_ratio = crop_to_aspect_ratio\n \n     def call(self, image):\n         return backend.image.resize(\n@@ -125,6 +127,7 @@ class Resize(Operation):\n             interpolation=self.interpolation,\n             antialias=self.antialias,\n             data_format=self.data_format,\n+            crop_to_aspect_ratio=self.crop_to_aspect_ratio,\n         )\n \n     def compute_output_spec(self, image):\n@@ -156,6 +159,7 @@ def resize(\n     size,\n     interpolation=\"bilinear\",\n     antialias=False,\n+    crop_to_aspect_ratio=False,\n     data_format=\"channels_last\",\n ):\n     \"\"\"Resize images to size using the specified interpolation method.\n@@ -167,6 +171,13 @@ def resize(\n             `\"bilinear\"`, and `\"bicubic\"`. Defaults to `\"bilinear\"`.\n         antialias: Whether to use an antialiasing filter when downsampling an\n             image. Defaults to `False`.\n+        crop_to_aspect_ratio: If `True`, resize the images without aspect\n+            ratio distortion. When the original aspect ratio differs\n+            from the target aspect ratio, the output image will be\n+            cropped so as to return the\n+            largest possible window in the image (of size `(height, width)`)\n+            that matches the target aspect ratio. By default\n+            (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, height, width, channels)`\n@@ -197,19 +208,31 @@ def resize(\n     >>> y.shape\n     (2, 3, 2, 2)\n     \"\"\"\n-\n+    if len(size) != 2:\n+        raise ValueError(\n+            \"Expected `size` to be a tuple of 2 integers. \"\n+            f\"Received: size={size}\"\n+        )\n+    if len(image.shape) < 3 or len(image.shape) > 4:\n+        raise ValueError(\n+            \"Expected an image array with shape `(height, width, \"\n+            \"channels)`, or `(batch_size, height, width, channels)`, but \"\n+            f\"got input with incorrect rank, of shape {image.shape}.\"\n+        )\n     if any_symbolic_tensors((image,)):\n         return Resize(\n             size,\n             interpolation=interpolation,\n             antialias=antialias,\n             data_format=data_format,\n+            crop_to_aspect_ratio=crop_to_aspect_ratio,\n         ).symbolic_call(image)\n     return backend.image.resize(\n         image,\n         size,\n         interpolation=interpolation,\n         antialias=antialias,\n+        crop_to_aspect_ratio=crop_to_aspect_ratio,\n         data_format=data_format,\n     )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
