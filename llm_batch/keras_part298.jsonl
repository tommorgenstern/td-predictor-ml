{"custom_id": "keras#72a2aa538b2ed3c79c8412a99b213ae1a2df70a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 379 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 19 | Complexity Δ (Sum/Max): 35/35 | Churn Δ: 379 | Churn Cumulative: 379 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.9120879120879121\n\nDIFF:\n@@ -0,0 +1,379 @@\n+\"\"\"Tests for Keras metrics correctness.\"\"\"\n+\n+import numpy as np\n+\n+import keras\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import backend as K\n+\n+\n+def get_multi_io_model():\n+    inp_1 = layers.Input(shape=(1,), name='input_1')\n+    inp_2 = layers.Input(shape=(1,), name='input_2')\n+    dense = layers.Dense(3, kernel_initializer='ones', trainable=False)\n+    x_1 = dense(inp_1)\n+    x_2 = dense(inp_2)\n+    out_1 = layers.Dense(\n+        1, kernel_initializer='ones', name='output_1', trainable=False)(x_1)\n+    out_2 = layers.Dense(\n+        1, kernel_initializer='ones', name='output_2', trainable=False)(x_2)\n+    return keras.Model([inp_1, inp_2], [out_1, out_2])\n+\n+\n+def custom_generator_multi_io(sample_weights=None):\n+    batch_size = 2\n+    num_samples = 4\n+    inputs = np.asarray([[1.], [2.], [3.], [4.]])\n+    targets_1 = np.asarray([[2.], [4.], [6.], [8.]])\n+    targets_2 = np.asarray([[1.], [2.], [3.], [4.]])\n+    w1 = sample_weights[0] if sample_weights else None\n+    w2 = sample_weights[1] if sample_weights else None\n+    i = 0\n+    while True:\n+        batch_index = i * batch_size % num_samples\n+        i += 1\n+        start = batch_index\n+        end = start + batch_size\n+        x = [inputs[start:end], inputs[start:end]]\n+        y = [targets_1[start:end], targets_2[start:end]]\n+        if sample_weights:\n+            w = [\n+                None if w1 is None else w1[start:end],\n+                None if w2 is None else w2[start:end]\n+            ]\n+        else:\n+            w = None\n+        yield x, y, w\n+\n+\n+class TestMetricsCorrectnessMultiIO(object):\n+\n+    def _get_compiled_multi_io_model(self):\n+        model = get_multi_io_model()\n+        model.compile(\n+            optimizer='rmsprop',\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError(name='mean_squared_error')],\n+            weighted_metrics=[\n+                metrics.MeanSquaredError(name='mean_squared_error_2')\n+            ])\n+        return model\n+\n+    def setUp(self):\n+        self.x = np.asarray([[1.], [2.], [3.], [4.]])\n+        self.y1 = np.asarray([[2.], [4.], [6.], [8.]])\n+        self.y2 = np.asarray([[1.], [2.], [3.], [4.]])\n+        self.sample_weight_1 = np.asarray([2., 3., 4., 5.])\n+        self.sample_weight_2 = np.asarray([3.5, 2.5, 1.5, 0.5])\n+        self.class_weight_1 = {2: 2, 4: 3, 6: 4, 8: 5}\n+        self.class_weight_2 = {1: 3.5, 2: 2.5, 3: 1.5, 4: 0.5}\n+\n+        # y_true_1 = [[2.], [4.], [6.], [8.]], y_pred = [[3.], [6.], [9.], [12.]]\n+        # y_true_2 = [[1.], [2.], [3.], [4.]], y_pred = [[3.], [6.], [9.], [12.]]\n+\n+        # Weighted metric `output_1`:\n+        #   Total = ((3 - 2)^2 * 2  + (6 - 4)^2 * 3) +\n+        #           ((9 - 6)^2 * 4 + (12 - 8)^2 * 5)\n+        #         = 130\n+        #   Count = (2 + 3) + (4 + 5)\n+        #   Result = 9.2857141\n+\n+        # Weighted metric `output_2`:\n+        #   Total = ((3 - 1)^2 * 3.5 + (6 - 2)^2 * 2.5) +\n+        #           ((9 - 3)^2 * 1.5 + (12 - 4)^2 * 0.5)\n+        #         = 140\n+        #   Count = (3.5 + 2.5) + (1.5 + 0.5)\n+        #   Result = 17.5\n+\n+        # Loss `output_1` with weights:\n+        #   Total = ((3 - 2)^2 * 2  + (6 - 4)^2 * 3) +\n+        #           ((9 - 6)^2 * 4 + (12 - 8)^2 * 5)\n+        #         = 130\n+        #   Count = 2 + 2\n+        #   Result = 32.5\n+\n+        # Loss `output_1` without weights/Metric `output_1`:\n+        #   Total = ((3 - 2)^2 + (6 - 4)^2) + ((9 - 6)^2 + (12 - 8)^2) = 30\n+        #   Count = 2 + 2\n+        #   Result = 7.5\n+\n+        # Loss `output_2` with weights:\n+        #   Total = ((3 - 1)^2 * 3.5 + (6 - 2)^2 * 2.5) +\n+        #           ((9 - 3)^2 * 1.5 + (12 - 4)^2 * 0.5)\n+        #         = 140\n+        #   Count = 2 + 2\n+        #   Result = 35\n+\n+        # Loss `output_2` without weights/Metric `output_2`:\n+        #   Total = ((3 - 1)^2 + (6 - 2)^2) + ((9 - 3)^2 + (12 - 4)^2) = 120\n+        #   Count = 2 + 2\n+        #   Result = 30\n+\n+        # Total loss with weights = 32.5 + 35 = 67.5\n+        # Total loss without weights = 7.5 + 30 = 37.5\n+\n+        self.expected_fit_result_with_weights = {\n+            'output_1_mean_squared_error': [7.5, 7.5],\n+            'output_2_mean_squared_error': [30, 30],\n+            'output_1_mean_squared_error_2': [9.286, 9.286],\n+            'output_2_mean_squared_error_2': [17.5, 17.5],\n+            'loss': [67.5, 67.5],\n+            'output_1_loss': [32.5, 32.5],\n+            'output_2_loss': [35, 35],\n+        }\n+\n+        self.expected_fit_result_with_weights_output_2 = {\n+            'output_1_mean_squared_error': [7.5, 7.5],\n+            'output_2_mean_squared_error': [30, 30],\n+            'output_1_mean_squared_error_2': [7.5, 7.5],\n+            'output_2_mean_squared_error_2': [17.5, 17.5],\n+            'loss': [42.5, 42.5],\n+            'output_1_loss': [7.5, 7.5],\n+            'output_2_loss': [35, 35],\n+        }\n+\n+        self.expected_fit_result = {\n+            'output_1_mean_squared_error': [7.5, 7.5],\n+            'output_2_mean_squared_error': [30, 30],\n+            'output_1_mean_squared_error_2': [7.5, 7.5],\n+            'output_2_mean_squared_error_2': [30, 30],\n+            'loss': [37.5, 37.5],\n+            'output_1_loss': [7.5, 7.5],\n+            'output_2_loss': [30, 30],\n+        }\n+\n+        # In the order: 'loss', 'output_1_loss', 'output_2_loss',\n+        # 'output_1_mean_squared_error', 'output_1_mean_squared_error_2',\n+        # 'output_2_mean_squared_error', 'output_2_mean_squared_error_2'\n+        self.expected_batch_result_with_weights = [\n+            67.5, 32.5, 35, 7.5, 9.286, 30, 17.5\n+        ]\n+        self.expected_batch_result_with_weights_output_2 = [\n+            42.5, 7.5, 35, 7.5, 7.5, 30, 17.5\n+        ]\n+        self.expected_batch_result = [37.5, 7.5, 30, 7.5, 7.5, 30, 30]\n+\n+    def test_fit(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        history = model.fit([self.x, self.x], [self.y1, self.y2],\n+                            batch_size=2,\n+                            epochs=2,\n+                            shuffle=False)\n+        for key, value in self.expected_fit_result.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+    def test_fit_with_sample_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        history = model.fit([self.x, self.x], [self.y1, self.y2],\n+                            sample_weight={\n+                                'output_1': self.sample_weight_1,\n+                                'output_2': self.sample_weight_2},\n+                            batch_size=2,\n+                            epochs=2,\n+                            shuffle=False)\n+        for key, value in self.expected_fit_result_with_weights.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+        # Set weights for one output (use batch size).\n+        history = model.fit([self.x, self.x], [self.y1, self.y2],\n+                            sample_weight={'output_2': self.sample_weight_2},\n+                            batch_size=2,\n+                            epochs=2,\n+                            shuffle=False)\n+\n+        for key, value in self.expected_fit_result_with_weights_output_2.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+    def test_fit_with_class_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        history = model.fit([self.x, self.x], [self.y1, self.y2],\n+                            class_weight={\n+                                'output_1': self.class_weight_1,\n+                                'output_2': self.class_weight_2},\n+                            batch_size=2,\n+                            epochs=2,\n+                            shuffle=False)\n+        for key, value in self.expected_fit_result_with_weights.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+        # Set weights for one output.\n+        history = model.fit([self.x, self.x], [self.y1, self.y2],\n+                            class_weight={'output_2': self.class_weight_2},\n+                            batch_size=2,\n+                            epochs=2,\n+                            shuffle=False)\n+\n+        for key, value in self.expected_fit_result_with_weights_output_2.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+    def test_eval(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],\n+                                     batch_size=2)\n+        np.allclose(eval_result, self.expected_batch_result, 1e-3)\n+\n+    def test_eval_with_sample_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],\n+                                     batch_size=2,\n+                                     sample_weight={\n+                                         'output_1': self.sample_weight_1,\n+                                         'output_2': self.sample_weight_2})\n+        np.allclose(eval_result, self.expected_batch_result_with_weights,\n+                    1e-3)\n+\n+        # Set weights for one output.\n+        model = self._get_compiled_multi_io_model()\n+        eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],\n+                                     batch_size=2,\n+                                     sample_weight={\n+                                         'output_2': self.sample_weight_2})\n+        np.allclose(eval_result,\n+                    self.expected_batch_result_with_weights_output_2, 1e-3)\n+\n+        # Verify that metric value is same with arbitrary weights and batch size.\n+        x = np.random.random((50, 1))\n+        y = np.random.random((50, 1))\n+        w = np.random.random((50,))\n+        mse1 = model.evaluate([x, x], [y, y], sample_weight=[w, w], batch_size=5)[3]\n+        mse2 = model.evaluate([x, x], [y, y], sample_weight=[w, w],\n+                              batch_size=10)[3]\n+        np.allclose(mse1, mse2, 1e-3)\n+\n+    def test_train_on_batch(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2])\n+        np.allclose(result, self.expected_batch_result, 1e-3)\n+\n+    def test_train_on_batch_with_sample_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n+                                      sample_weight={\n+                                          'output_1': self.sample_weight_1,\n+                                          'output_2': self.sample_weight_2})\n+        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n+\n+        # Set weights for one output.\n+        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n+                                      sample_weight={\n+                                          'output_2': self.sample_weight_2})\n+        np.allclose(result, self.expected_batch_result_with_weights_output_2, 1e-3)\n+\n+    def test_train_on_batch_with_class_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n+                                      class_weight={\n+                                          'output_1': self.class_weight_1,\n+                                          'output_2': self.class_weight_2})\n+        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n+\n+        # Set weights for one output.\n+        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n+                                      class_weight={\n+                                          'output_2': self.class_weight_2})\n+        np.allclose(result,\n+                    self.expected_batch_result_with_weights_output_2, 1e-3)\n+\n+    def test_test_on_batch(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2])\n+        np.allclose(result, self.expected_batch_result, 1e-3)\n+\n+    def test_test_on_batch_with_sample_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],\n+                                     sample_weight={\n+                                         'output_1': self.sample_weight_1,\n+                                         'output_2': self.sample_weight_2})\n+        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n+\n+        # Set weights for one output.\n+        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],\n+                                     sample_weight={\n+                                         'output_2': self.sample_weight_2})\n+        np.allclose(result,\n+                    self.expected_batch_result_with_weights_output_2, 1e-3)\n+\n+    def test_fit_generator(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        history = model.fit_generator(\n+            custom_generator_multi_io(), steps_per_epoch=2, epochs=2)\n+        for key, value in self.expected_fit_result.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+    def test_fit_generator_with_sample_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        history = model.fit_generator(\n+            custom_generator_multi_io(\n+                sample_weights=[self.sample_weight_1, self.sample_weight_2]),\n+            steps_per_epoch=2,\n+            epochs=2)\n+        for key, value in self.expected_fit_result_with_weights.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+        # Set weights for one output.\n+        history = model.fit_generator(\n+            custom_generator_multi_io(sample_weights=[None, self.sample_weight_2]),\n+            steps_per_epoch=2,\n+            epochs=2)\n+        for key, value in self.expected_fit_result_with_weights_output_2.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+    def test_fit_generator_with_class_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        history = model.fit_generator(\n+            custom_generator_multi_io(),\n+            class_weight={\n+                'output_1': self.class_weight_1,\n+                'output_2': self.class_weight_2,\n+            },\n+            steps_per_epoch=2,\n+            epochs=2)\n+        for key, value in self.expected_fit_result_with_weights.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+        # Set weights for one output.\n+        history = model.fit_generator(\n+            custom_generator_multi_io(),\n+            class_weight={'output_2': self.class_weight_2},\n+            steps_per_epoch=2,\n+            epochs=2)\n+        for key, value in self.expected_fit_result_with_weights_output_2.items():\n+            np.allclose(history.history[key], value, 1e-3)\n+\n+    def test_eval_generator(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        eval_result = model.evaluate_generator(custom_generator_multi_io(), steps=2)\n+        np.allclose(eval_result, self.expected_batch_result, 1e-3)\n+\n+    def test_eval_generator_with_sample_weight(self):\n+        self.setUp()\n+        model = self._get_compiled_multi_io_model()\n+        eval_result = model.evaluate_generator(\n+            custom_generator_multi_io(\n+                sample_weights=[self.sample_weight_1, self.sample_weight_2]),\n+            steps=2)\n+        np.allclose(eval_result, self.expected_batch_result_with_weights, 1e-3)\n+\n+        # Set weights for one output.\n+        eval_result = model.evaluate_generator(\n+            custom_generator_multi_io(sample_weights=[None, self.sample_weight_2]),\n+            steps=2)\n+        np.allclose(eval_result,\n+                    self.expected_batch_result_with_weights_output_2, 1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3bda5520b787f84f687bb116c460f3aedada039b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 250 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -101,10 +101,10 @@ y_val = labels[-num_validation_samples:]\n print('Preparing embedding matrix.')\n \n # prepare embedding matrix\n-num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n+num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n for word, i in word_index.items():\n-    if i > MAX_NUM_WORDS:\n+    if i >= MAX_NUM_WORDS:\n         continue\n     embedding_vector = embeddings_index.get(word)\n     if embedding_vector is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ebe3f30f1122a97d4d88855ab5a73618332d81b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 15360 | Contributors (this commit): 133 | Commits (past 90d): 16 | Contributors (cumulative): 148 | DMM Complexity: None\n\nDIFF:\n@@ -4509,9 +4509,10 @@ def control_dependencies(control_inputs):\n     \"\"\"A context manager that specifies control dependencies.\n \n     # Arguments\n-        control_inputs: A list of Operation or Tensor objects which must be executed\n-        or computed before running the operations defined in the context. Can also\n-        be None to clear the control dependencies.\n+        control_inputs: A list of Operation or Tensor objects\n+            which must be executed\n+            or computed before running the operations defined in the context.\n+            Can also be None to clear the control dependencies.\n \n     # Returns\n         A context manager.\n\n@@ -37,7 +37,7 @@ def test_image_classification():\n     history = model.fit(x_train, y_train, epochs=10, batch_size=16,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert history.history['val_acc'][-1] > 0.75\n+    assert history.history['val_accuracy'][-1] > 0.75\n     config = model.get_config()\n     model = Sequential.from_config(config)\n \n@@ -72,7 +72,7 @@ def test_image_data_generator_training():\n                                   validation_data=img_gen.flow(x_test, y_test,\n                                                                batch_size=16),\n                                   verbose=0)\n-    assert history.history['val_acc'][-1] > 0.75\n+    assert history.history['val_accuracy'][-1] > 0.75\n     model.evaluate_generator(img_gen.flow(x_train, y_train, batch_size=16))\n \n \n\n@@ -38,7 +38,7 @@ def test_temporal_classification():\n     history = model.fit(x_train, y_train, epochs=4, batch_size=10,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert(history.history['acc'][-1] >= 0.8)\n+    assert(history.history['accuracy'][-1] >= 0.8)\n     config = model.get_config()\n     model = Sequential.from_config(config)\n \n@@ -69,7 +69,7 @@ def test_temporal_classification_functional():\n     history = model.fit(x_train, y_train, epochs=4, batch_size=10,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert(history.history['acc'][-1] >= 0.8)\n+    assert(history.history['accuracy'][-1] >= 0.8)\n \n \n def test_temporal_regression():\n\n@@ -41,7 +41,7 @@ def test_tf_optimizer():\n                   metrics=['accuracy'])\n     history = model.fit(x_train, y_train, epochs=8, batch_size=16,\n                         validation_data=(x_test, y_test), verbose=2)\n-    assert history.history['val_acc'][-1] >= target\n+    assert history.history['val_accuracy'][-1] >= target\n \n     # Test saving.\n     _, fname = tempfile.mkstemp('.h5')\n\n@@ -37,7 +37,7 @@ def test_vector_classification():\n     history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n                         validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.8)\n+    assert(history.history['val_accuracy'][-1] > 0.8)\n     config = model.get_config()\n     model = Sequential.from_config(config)\n \n@@ -57,11 +57,11 @@ def test_vector_classification_functional():\n     model = keras.models.Model(inputs, outputs)\n     model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n                   optimizer=keras.optimizers.Adam(1e-3),\n-                  metrics=['acc'])\n+                  metrics=['accuracy'])\n     history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n                         validation_data=(x_train, y_train),\n                         verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.8)\n+    assert(history.history['val_accuracy'][-1] > 0.8)\n \n \n def test_vector_regression():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed07472bc5fc985982db355135d37059a1f887a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/8 | Churn Δ: 51 | Churn Cumulative: 23520 | Contributors (this commit): 146 | Commits (past 90d): 10 | Contributors (cumulative): 195 | DMM Complexity: 0.8108108108108109\n\nDIFF:\n@@ -1488,6 +1488,9 @@ def rnn(step_function, inputs, initial_states,\n             raise ValueError('When specifying `unroll=True`, '\n                              'an `input_length` '\n                              'must be provided to `rnn`.')\n+        if input_length == 1:\n+            raise ValueError('`input_length=1` is not'\n+                             ' supported when `unroll=True`.')\n \n     axes = [1, 0] + list(range(2, ndim))\n     inputs = inputs.dimshuffle(axes)\n\n@@ -610,9 +610,9 @@ class RNN(Layer):\n                              ' initial states.')\n         input_shape = K.int_shape(inputs)\n         timesteps = input_shape[1]\n-        if self.unroll and timesteps in [None, 1]:\n+        if self.unroll and timesteps is None:\n             raise ValueError('Cannot unroll a RNN if the '\n-                             'time dimension is undefined or equal to 1. \\n'\n+                             'time dimension is undefined. \\n'\n                              '- If using a Sequential model, '\n                              'specify the time dimension by passing '\n                              'an `input_shape` or `batch_input_shape` '\n\n@@ -209,8 +209,11 @@ def check_rnn_operation(step_function_k,\n         constants=constants_np,\n         **kwargs)\n     # note that numpy reference implementation is independent of `unroll` argument\n-\n-    for unroll in [True, False]:\n+    if 'unroll' in kwargs:\n+        unroll_options = [kwargs.pop('unroll')]\n+    else:\n+        unroll_options = [True, False]\n+    for unroll in unroll_options:\n         last_output_k, output_k, last_states_k = K.rnn(\n             step_function_k,\n             inputs_k,\n@@ -771,6 +774,42 @@ class TestBackend(object):\n                                 mask_np=kwargs.pop('mask', None),\n                                 **kwargs)\n \n+    @pytest.mark.skipif(K.backend() == 'theano', reason='Not supported')\n+    def test_rnn_unroll_with_len_1(self):\n+        num_samples = 4\n+        input_dim = 5\n+        output_dim = 3\n+\n+        _, x = parse_shape_or_val((num_samples, 1, input_dim))\n+        _, h0 = parse_shape_or_val((num_samples, output_dim))\n+        _, wi = parse_shape_or_val((input_dim, output_dim))\n+        _, wh = parse_shape_or_val((output_dim, output_dim))\n+\n+        wi_k = K.variable(wi)\n+        wh_k = K.variable(wh)\n+\n+        def get_step_function(backend, w_i, w_h):\n+\n+            def simple_rnn(inputs, states):\n+                assert len(states) == 1\n+                h = states[0]\n+                y = backend.dot(inputs, w_i) + backend.dot(h, w_h)\n+                return y, [y]\n+\n+            return simple_rnn\n+\n+        kwargs_list = [\n+            {'go_backwards': False},\n+            {'go_backwards': True},\n+        ]\n+        for kwargs in kwargs_list:\n+            check_rnn_operation(step_function_k=get_step_function(K, wi_k, wh_k),\n+                                step_function_np=get_step_function(KNP, wi, wh),\n+                                inputs_np=x,\n+                                initial_states_np=[h0],\n+                                unroll=True,\n+                                **kwargs)\n+\n     def test_rnn_additional_states(self):\n         # implement a simple RNN with an additional state\n         # whose shape is different from that of the output\n@@ -2073,5 +2112,6 @@ class TestBackend(object):\n             assert cfg.intra_op_parallelism_threads == threads\n             assert cfg.inter_op_parallelism_threads == threads\n \n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efe72ef433852b1d7d54f283efff53085ec4f756", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 930 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -37,9 +37,9 @@ class L1L2(Regularizer):\n     def __call__(self, x):\n         regularization = 0.\n         if self.l1:\n-            regularization += K.sum(self.l1 * K.abs(x))\n+            regularization += self.l1 * K.sum(K.abs(x))\n         if self.l2:\n-            regularization += K.sum(self.l2 * K.square(x))\n+            regularization += self.l2 * K.sum(K.square(x))\n         return regularization\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c10d24959b0ad615a21e671b180a1b2466d77a2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 16 | Methods Changed: 6 | Complexity Δ (Sum/Max): 22/10 | Churn Δ: 103 | Churn Cumulative: 12589 | Contributors (this commit): 89 | Commits (past 90d): 8 | Contributors (cumulative): 107 | DMM Complexity: 0.5079365079365079\n\nDIFF:\n@@ -538,6 +538,7 @@ class RNN(Layer):\n \n         additional_inputs = []\n         additional_specs = []\n+\n         if initial_state is not None:\n             kwargs['initial_state'] = initial_state\n             additional_inputs += initial_state\n@@ -568,6 +569,10 @@ class RNN(Layer):\n             # Perform the call with temporarily replaced input_spec\n             original_input_spec = self.input_spec\n             self.input_spec = full_input_spec\n+            if 'initial_state' in kwargs:\n+                kwargs.pop('initial_state')\n+            if 'constants' in kwargs:\n+                kwargs.pop('constants')\n             output = super(RNN, self).__call__(full_input, **kwargs)\n             self.input_spec = original_input_spec\n             return output\n@@ -580,16 +585,34 @@ class RNN(Layer):\n              training=None,\n              initial_state=None,\n              constants=None):\n+        if not isinstance(initial_state, (list, tuple, type(None))):\n+            initial_state = [initial_state]\n+        if not isinstance(constants, (list, tuple, type(None))):\n+            constants = [constants]\n         # input shape: `(samples, time (padded with zeros), input_dim)`\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec and self.state_spec with complete input shapes.\n         if isinstance(inputs, list):\n+            if len(inputs) == 1:\n+                inputs = inputs[0]\n+            else:\n                 # get initial_state from full input spec\n                 # as they could be copied to multiple GPU.\n                 if self._num_constants is None:\n+                    if initial_state is not None:\n+                        raise ValueError('Layer was passed initial state ' +\n+                                         'via both kwarg and inputs list)')\n                     initial_state = inputs[1:]\n                 else:\n+                    if initial_state is not None and inputs[1:-self._num_constants]:\n+                        raise ValueError('Layer was passed initial state ' +\n+                                         'via both kwarg and inputs list')\n                     initial_state = inputs[1:-self._num_constants]\n+                    if constants is None:\n+                        constants = inputs[-self._num_constants:]\n+                    elif len(inputs) > 1 + len(initial_state):\n+                        raise ValueError('Layer was passed constants ' +\n+                                         'via both kwarg and inputs list)')\n                 if len(initial_state) == 0:\n                     initial_state = None\n                 inputs = inputs[0]\n@@ -608,6 +631,7 @@ class RNN(Layer):\n                              ' states but was passed ' +\n                              str(len(initial_state)) +\n                              ' initial states.')\n+\n         input_shape = K.int_shape(inputs)\n         timesteps = input_shape[1]\n         if self.unroll and timesteps is None:\n\n@@ -477,6 +477,10 @@ class Bidirectional(Wrapper):\n             # Perform the call with temporarily replaced input_spec\n             original_input_spec = self.input_spec\n             self.input_spec = full_input_spec\n+            if 'initial_state' in kwargs:\n+                kwargs.pop('initial_state')\n+            if 'constants' in kwargs:\n+                kwargs.pop('constants')\n             output = super(Bidirectional, self).__call__(full_input, **kwargs)\n             self.input_spec = original_input_spec\n             return output\n@@ -495,31 +499,31 @@ class Bidirectional(Wrapper):\n         if has_arg(self.layer.call, 'mask'):\n             kwargs['mask'] = mask\n         if has_arg(self.layer.call, 'constants'):\n+            if self._num_constants is not None and constants is None:\n+                    constants = inputs[-self._num_constants:]\n+                    inputs = inputs[:-self._num_constants]\n             kwargs['constants'] = constants\n-\n-        if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n-            forward_inputs = [inputs[0]]\n-            backward_inputs = [inputs[0]]\n-            pivot = len(initial_state) // 2 + 1\n-            # add forward initial state\n-            forward_state = inputs[1:pivot]\n-            forward_inputs += forward_state\n-            if self._num_constants is None:\n-                # add backward initial state\n-                backward_state = inputs[pivot:]\n-                backward_inputs += backward_state\n+        if has_arg(self.layer.call, 'initial_state'):\n+            if isinstance(inputs, list) and len(inputs) > 1:\n+                if initial_state is not None:\n+                    raise ValueError('Layer was passed initial state ' +\n+                                     'via both kwarg and inputs list)')\n+                initial_state = inputs[1:]\n+                inputs = [inputs[0]]\n+            if initial_state is None:\n+                forward_state = None\n+                backward_state = None\n             else:\n-                # add backward initial state\n-                backward_state = inputs[pivot:-self._num_constants]\n-                backward_inputs += backward_state\n-                # add constants for forward and backward layers\n-                forward_inputs += inputs[-self._num_constants:]\n-                backward_inputs += inputs[-self._num_constants:]\n-            y = self.forward_layer.call(forward_inputs,\n+                pivot = len(initial_state) // 2\n+                forward_state = initial_state[:pivot]\n+                backward_state = initial_state[pivot:]\n+            y = self.forward_layer.call(inputs,\n                                         initial_state=forward_state, **kwargs)\n-            y_rev = self.backward_layer.call(backward_inputs,\n+            y_rev = self.backward_layer.call(inputs,\n                                              initial_state=backward_state, **kwargs)\n         else:\n+            if isinstance(inputs, list) and len(inputs) > 1 or initial_state:\n+                raise ValueError('Layer does not accept initial_state argument.')\n             y = self.forward_layer.call(inputs, **kwargs)\n             y_rev = self.backward_layer.call(inputs, **kwargs)\n \n\n@@ -3,6 +3,7 @@ import pytest\n import os\n import h5py\n import tempfile\n+import warnings\n from contextlib import contextmanager\n import numpy as np\n from numpy.testing import assert_allclose\n@@ -953,5 +954,39 @@ def test_preprocess_weights_for_loading_gru_incompatible():\n                           'GRU(reset_after=False)')\n \n \n+def test_model_saving_with_rnn_initial_state_and_args():\n+    class CustomRNN(LSTM):\n+        def call(self, inputs, arg=1, mask=None, training=None, initial_state=None):\n+            if isinstance(inputs, list):\n+                inputs = inputs[:]\n+                shape = K.int_shape(inputs[0])\n+                inputs[0] *= arg\n+                inputs[0]._keras_shape = shape  # for theano backend\n+            else:\n+                shape = K.int_shape(inputs)\n+                inputs *= arg\n+                inputs._keras_shape = shape  # for theano backend\n+            return super(CustomRNN, self).call(inputs, mask, training, initial_state)\n+\n+    inp = Input((3, 2))\n+    rnn_out, h, c = CustomRNN(2, return_state=True, return_sequences=True)(inp)\n+    assert hasattr(rnn_out, '_keras_history')\n+    assert hasattr(h, '_keras_history')\n+    assert hasattr(c, '_keras_history')\n+    rnn2_out = CustomRNN(2)(rnn_out, arg=2, initial_state=[h, c])\n+    assert hasattr(rnn2_out, '_keras_history')\n+    model = Model(inputs=inp, outputs=rnn2_out)\n+    x = np.random.random((2, 3, 2))\n+    y1 = model.predict(x)\n+    _, fname = tempfile.mkstemp('.h5')\n+    with warnings.catch_warnings():\n+        warnings.filterwarnings('error')\n+        model.save(fname)\n+    model2 = load_model(fname, custom_objects={'CustomRNN': CustomRNN})\n+    y2 = model2.predict(x)\n+    assert_allclose(y1, y2, atol=1e-5)\n+    os.remove(fname)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa28910799bcd169d28ae1e5b50508c868df7380", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 58/58 | Churn Δ: 34 | Churn Cumulative: 17339 | Contributors (this commit): 154 | Commits (past 90d): 14 | Contributors (cumulative): 174 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.2.4'\r\n+__version__ = '2.2.5'\r\n\n@@ -61,6 +61,18 @@ _MANUAL_VAR_INIT = False\n _LOCAL_DEVICES = None\n \n \n+def _get_default_graph():\n+    try:\n+        return tf.get_default_graph()\n+    except AttributeError:\n+        raise RuntimeError(\n+            'It looks like you are trying to use '\n+            'a version of multi-backend Keras that '\n+            'does not support TensorFlow 2.0. We recommend '\n+            'using `tf.keras`, or alternatively, '\n+            'downgrading to TensorFlow 1.14.')\n+\n+\n def get_uid(prefix=''):\n     \"\"\"Get the uid for the default graph.\n \n@@ -71,7 +83,7 @@ def get_uid(prefix=''):\n         A unique identifier for the graph.\n     \"\"\"\n     global _GRAPH_UID_DICTS\n-    graph = tf.get_default_graph()\n+    graph = _get_default_graph()\n     if graph not in _GRAPH_UID_DICTS:\n         _GRAPH_UID_DICTS[graph] = defaultdict(int)\n     _GRAPH_UID_DICTS[graph][prefix] += 1\n@@ -101,7 +113,7 @@ def clear_session():\n             shape=(),\n             name='keras_learning_phase')\n     _GRAPH_LEARNING_PHASES = {}\n-    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase\n+    _GRAPH_LEARNING_PHASES[_get_default_graph()] = phase\n \n \n def manual_variable_initialization(value):\n@@ -130,7 +142,7 @@ def learning_phase():\n     # Returns\n         Learning phase (scalar integer tensor or Python integer).\n     \"\"\"\n-    graph = tf.get_default_graph()\n+    graph = _get_default_graph()\n     if graph not in _GRAPH_LEARNING_PHASES:\n         with tf.name_scope(''):\n             phase = tf.placeholder_with_default(\n@@ -154,7 +166,7 @@ def set_learning_phase(value):\n     if value not in {0, 1}:\n         raise ValueError('Expected learning phase to be '\n                          '0 or 1.')\n-    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = value\n+    _GRAPH_LEARNING_PHASES[_get_default_graph()] = value\n \n \n def get_session():\n@@ -247,7 +259,7 @@ def _get_current_tf_device():\n         the device (`CPU` or `GPU`). If the scope is not explicitly set, it will\n         return `None`.\n     \"\"\"\n-    g = tf.get_default_graph()\n+    g = _get_default_graph()\n     op = _TfDeviceCaptureOp()\n     g._apply_device_functions(op)\n     return op.device\n\n@@ -24,7 +24,7 @@ and is distributed under the MIT license.\n '''\n \n setup(name='Keras',\n-      version='2.2.4',\n+      version='2.2.5',\n       description='Deep Learning for humans',\n       long_description=long_description,\n       author='Francois Chollet',\n@@ -37,8 +37,8 @@ setup(name='Keras',\n                         'six>=1.9.0',\n                         'pyyaml',\n                         'h5py',\n-                        'keras_applications>=1.0.6',\n-                        'keras_preprocessing>=1.0.5'],\n+                        'keras_applications>=1.0.8',\n+                        'keras_preprocessing>=1.1.0'],\n       extras_require={\n           'visualize': ['pydot>=1.2.4'],\n           'tests': ['pytest',\n\n@@ -42,6 +42,8 @@ def handle_class(name, member):\n \n \n def handle_function(name, member):\n+    if name.startswith('_'):\n+        return\n     if is_accepted(name, member) or member_too_small(member):\n         # We don't need to check this one.\n         return\n\n@@ -480,8 +480,6 @@ def test_recursion():\n         j_tf = tf.placeholder(dtype=K.floatx())\n         k_tf = tf.placeholder(dtype=K.floatx())\n         m_tf, n_tf = tf_model([j_tf, k_tf])\n-        assert m_tf.get_shape().as_list() == [None, 64]\n-        assert n_tf.get_shape().as_list() == [None, 5]\n \n         # test merge\n         layers.concatenate([j_tf, k_tf], axis=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c074416c9a0731c1f55c2f6ba200b4459dbcd4ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 1389 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -320,7 +320,8 @@ class KerasRegressor(BaseWrapper):\n                 Predictions.\n         \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n-        return np.squeeze(self.model.predict(x, **kwargs), axis=-1)\n+        preds = self.model.predict(x, **kwargs)\n+        return np.squeeze(preds, axis=len(preds.shape) - 1)\n \n     def score(self, x, y, **kwargs):\n         \"\"\"Returns the mean loss on the given test data and labels.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb7f49ef5b07f2ceee1d2d6c45f273df6672734c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1395 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -320,8 +320,10 @@ class KerasRegressor(BaseWrapper):\n                 Predictions.\n         \"\"\"\n         kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n-        preds = self.model.predict(x, **kwargs)\n-        return np.squeeze(preds, axis=len(preds.shape) - 1)\n+        preds = np.array(self.model.predict(x, **kwargs))\n+        if preds.shape[-1] == 1:\n+            return np.squeeze(preds, axis=-1)\n+        return preds\n \n     def score(self, x, y, **kwargs):\n         \"\"\"Returns the mean loss on the given test data and labels.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#790c74cbe1a81b3e737f9e46839da29803600236", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 8 | Files Changed: 7 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 29 | Churn Cumulative: 17981 | Contributors (this commit): 166 | Commits (past 90d): 19 | Contributors (cumulative): 199 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.2.4'\r\n+__version__ = '2.3.0'\r\n\n@@ -159,8 +159,13 @@ def constant(value, dtype=None, shape=None, name=None):\n         dtype = floatx()\n     if shape is None:\n         shape = ()\n-    np_value = value * np.ones(shape)\n-    const = T.constant(np_value,\n+    if not is_tensor(value):\n+        value = np.array(value)\n+        if len(value.shape) == 0:\n+            value = value * np.ones(shape)\n+        if shape and value.shape != shape:\n+            value = np.reshape(value, shape)\n+    const = T.constant(value,\n                        dtype=dtype,\n                        name=_prepare_name(name, 'constant'))\n     const._keras_shape = shape\n@@ -220,7 +225,8 @@ def is_keras_tensor(x):\n \n def is_tensor(x):\n     return isinstance(x, (T.TensorVariable,\n-                          T.sharedvar.TensorSharedVariable))\n+                          T.sharedvar.TensorSharedVariable,\n+                          T.TensorConstant))\n \n \n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n\n@@ -95,7 +95,7 @@ class TensorBoard(Callback):\n             import tensorflow as tf\n             from tensorflow.contrib.tensorboard.plugins import projector\n         except ImportError:\n-            raise ImportError('You need the TensorFlow module installed to '\n+            raise ImportError('You need the TensorFlow (v1) module installed to '\n                               'use TensorBoard.')\n \n         if K.backend() != 'tensorflow':\n\n@@ -714,10 +714,10 @@ class Model(Network):\n         \"\"\"Caches metric name and function attributes for every model output.\"\"\"\n         output_shapes = []\n         for output in self.outputs:\n-            if output is None or output.shape.rank is None:\n+            if output is None:\n                 output_shapes.append(None)\n             else:\n-                output_shapes.append(output.shape.as_list())\n+                output_shapes.append(list(output.shape))\n         self._per_output_metrics = training_utils.collect_per_output_metric_info(\n             metrics, self.output_names, output_shapes, self.loss_functions)\n         self._per_output_weighted_metrics = (\n\n@@ -114,8 +114,10 @@ def broadcast_weights(values, sample_weight):\n         for i in range(weights_rank, values_rank):\n             sample_weight = K.expand_dims(sample_weight, axis=i)\n \n+        if weights_shape is not None and values_shape is not None:\n             for i in range(weights_rank):\n-            if (weights_shape[i] is not None and values_shape[i] is not None and\n+                if (weights_shape[i] is not None and\n+                    values_shape[i] is not None and\n                     weights_shape[i] != values_shape[i]):\n                     # Cannot be broadcasted.\n                     if weights_shape[i] != 1:\n\n@@ -924,6 +924,7 @@ def test_LambdaCallback():\n     assert not p.is_alive()\n \n \n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses TensorBoard')\n def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n     import shutil\n     np.random.seed(np.random.randint(1, 1e7))\n\n@@ -21,6 +21,10 @@ train_samples = 20\n test_samples = 20\n \n \n+if K.backend() != 'tensorflow':\n+    pytestmark = pytest.mark.skip\n+\n+\n def data_generator(x, y, batch_size):\n     x = to_list(x)\n     y = to_list(y)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8feac19b8570fd15724c5f3138f08862176f0d89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 12230 | Contributors (this commit): 111 | Commits (past 90d): 20 | Contributors (cumulative): 124 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,6 +122,7 @@ class Model(Network):\n         self._feed_output_names = []\n         self._feed_output_shapes = []\n         self._feed_loss_fns = []\n+        self._metric_updates = []\n \n         # if loss function is None, then this output will be skipped during total\n         # loss calculation and feed targets preparation.\n@@ -308,7 +309,8 @@ class Model(Network):\n                         params=self._collected_trainable_weights,\n                         loss=self.total_loss)\n                 updates = (self.updates +\n-                           training_updates)\n+                           training_updates +\n+                           self._metric_updates)\n \n                 metrics = self._get_training_eval_metrics()\n                 metrics_tensors = [\n@@ -342,7 +344,7 @@ class Model(Network):\n             self.test_function = K.function(\n                 inputs,\n                 [self.total_loss] + metrics_tensors,\n-                updates=self.state_updates,\n+                updates=self.state_updates + self._metric_updates,\n                 name='test_function',\n                 **self._function_kwargs)\n \n@@ -822,8 +824,9 @@ class Model(Network):\n \n         for metric_name, metric_fn in metrics_dict.items():\n             with K.name_scope(metric_name):\n-                metric_result = training_utils.call_metric_function(\n+                metric_result, update_ops = training_utils.call_metric_function(\n                     metric_fn, y_true, y_pred, weights=weights, mask=mask)\n+                self._metric_updates += update_ops\n \n     def _handle_metrics(self,\n                         outputs,\n\n@@ -1030,6 +1030,10 @@ def call_metric_function(metric_fn,\n             weights *= mask\n \n     if y_pred is not None:\n-        return metric_fn(y_true, y_pred, sample_weight=weights)\n+        update_op = metric_fn.update_state(y_true, y_pred, sample_weight=weights)\n+        result = metric_fn.result()\n+    else:\n         # `Mean` metric only takes a single value.\n-    return metric_fn(y_true, sample_weight=weights)\n+        update_op = metric_fn.update_state(y_true, sample_weight=weights)\n+        result = metric_fn.result()\n+    return result, update_op\n\n@@ -75,8 +75,11 @@ class Metric(Layer):\n \n     def __call__(self, *args, **kwargs):\n         \"\"\"Accumulates statistics and then computes metric result value.\"\"\"\n+        if K.backend() != 'tensorflow':\n+            raise RuntimeError(\n+                'Metric calling only supported with TensorFlow backend.')\n         update_op = self.update_state(*args, **kwargs)\n-        with K.control_dependencies([update_op]):\n+        with K.control_dependencies([update_op]):  # For TF\n             return self.result()\n \n     def get_config(self):\n@@ -135,10 +138,12 @@ class Reduce(Metric):\n         \"\"\"\n         super(Reduce, self).__init__(name=name, dtype=dtype)\n         self.reduction = reduction\n-        self.total = self.add_weight('total', initializer='zeros')\n+        self.total = self.add_weight(\n+            'total', initializer='zeros')\n         if reduction in [metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n                          metrics_utils.Reduction.WEIGHTED_MEAN]:\n-            self.count = self.add_weight('count', initializer='zeros')\n+            self.count = self.add_weight(\n+                'count', initializer='zeros')\n \n     def update_state(self, values, sample_weight=None):\n         \"\"\"Accumulates statistics for computing the reduction metric.\n@@ -150,6 +155,9 @@ class Reduce(Metric):\n         # Arguments\n             values: Per-example value.\n             sample_weight: Optional weighting of each example. Defaults to 1.\n+\n+        # Returns\n+            List of update ops.\n         \"\"\"\n         values = K.cast(values, self.dtype)\n         if sample_weight is not None:\n@@ -165,12 +173,11 @@ class Reduce(Metric):\n             values = values * sample_weight\n \n         value_sum = K.sum(values)\n-        with K.control_dependencies([value_sum]):\n         update_total_op = K.update_add(self.total, value_sum)\n \n         # Exit early if the reduction doesn't have a denominator.\n         if self.reduction == metrics_utils.Reduction.SUM:\n-            return update_total_op\n+            return [update_total_op]\n \n         # Update `count` for reductions that require a denominator.\n         if self.reduction == metrics_utils.Reduction.SUM_OVER_BATCH_SIZE:\n@@ -184,8 +191,7 @@ class Reduce(Metric):\n             raise NotImplementedError(\n                 'reduction [%s] not implemented' % self.reduction)\n \n-        with K.control_dependencies([update_total_op]):\n-            return K.update_add(self.count, num_values)\n+        return [update_total_op, K.update_add(self.count, num_values)]\n \n     def result(self):\n         if self.reduction == metrics_utils.Reduction.SUM:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
