{"custom_id": "keras#0e0d77fd5f1544545980e9388bb0264751e01997", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 212 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,27 +17,21 @@\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers import optimizer\n-from keras.saving.object_registration import register_keras_serializable\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@register_keras_serializable()\n-@keras_export(\n-    \"keras.optimizers.experimental.Lion\",\n-    \"keras.optimizers.Lion\",\n-    v1=[],\n-)\n+@keras_export(\"keras.optimizers.Lion\", v1=[])\n class Lion(optimizer.Optimizer):\n-    r\"\"\"Optimizer that implements the Lion algorithm.\n+    \"\"\"Optimizer that implements the Lion algorithm.\n \n     The Lion optimizer is a stochastic-gradient-descent method that uses the\n     sign operator to control the magnitude of the update, unlike other adaptive\n     optimizers such as Adam that also rely on second-order moments. This make\n     Lion more memory-efficient as it only keeps track of the momentum. According\n     to the authors (see reference), its performance gain over Adam grows with\n-    the training batch size.\n+    the batch size.\n \n     Args:\n       learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n@@ -54,7 +48,7 @@ class Lion(optimizer.Optimizer):\n \n     References:\n       - [Chen et al., 2023](http://arxiv.org/abs/2302.06675)\n-      - [Authors implementation](\\\n+      - [Authors' implementation](\n           http://github.com/google/automl/tree/master/lion)\n \n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86f4f03a0cceb8400c300c1b2cd05b34acaf2be2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 79 | Churn Cumulative: 373 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,6 +15,7 @@\n \"\"\"Public API surface for saving APIs.\"\"\"\n \n import os\n+import warnings\n import zipfile\n \n import tensorflow.compat.v2 as tf\n@@ -116,15 +117,32 @@ def save_model(model, filepath, overwrite=True, save_format=None, **kwargs):\n     `tf.keras.saving.load_model`.\n     \"\"\"\n     save_format = get_save_format(filepath, save_format)\n-    if save_format not in (\"keras\", \"tf\", \"h5\", \"keras_v3\"):\n-        raise ValueError(\n-            \"Unknown `save_format` argument. Expected one of \"\n-            \"'keras', 'tf', or 'h5'. \"\n-            f\"Received: save_format{save_format}\"\n+\n+    # Deprecation warnings\n+    if save_format == \"tf\":\n+        warnings.warn(\n+            \"You are saving your model as a TensorFlow SavedModel via \"\n+            \"`model.save()`. This is no longer a recommended workflow.\\n\\n\"\n+            \"* If you intend to be able to reload the exact same model in a \"\n+            \"Python runtime, we recommend using the native Keras format, \"\n+            \"e.g. `model.save('my_model.keras')`.\\n\\n\"\n+            \"* If you intend to export a SavedModel artifact for inference \"\n+            \"(e.g. via TF-Serving), we recommend using \"\n+            \"`model.export('my_export_artifact')`. If you want to further \"\n+            \"customize SavedModel serving endpoints you can also use the \"\n+            \"low-level `keras.export.ExportArchive` class.\",\n+            stacklevel=2,\n         )\n-    if save_format == \"keras_v3\" or (\n-        saving_lib.saving_v3_enabled() and save_format == \"keras\"\n-    ):\n+    if save_format == \"h5\":\n+        warnings.warn(\n+            \"You are saving your model as an HDF5 file via `model.save()`. \"\n+            \"This file format is considered legacy. \"\n+            \"We recommend using instead the native Keras format, \"\n+            \"e.g. `model.save('my_model.keras')`.\",\n+            stacklevel=2,\n+        )\n+\n+    if save_format == \"keras\":\n         # If file exists and should not be overwritten.\n         try:\n             exists = os.path.exists(filepath)\n@@ -248,21 +266,42 @@ def load_weights(model, filepath, skip_mismatch=False, **kwargs):\n \n \n def get_save_format(filepath, save_format):\n+    if save_format:\n+        if save_format == \"keras_v3\":\n+            return \"keras\"\n+        if save_format == \"keras\":\n             if saving_lib.saving_v3_enabled():\n-        default_format = \"keras\"\n-    elif tf.__internal__.tf2.enabled():\n-        default_format = \"tf\"\n+                return \"keras\"\n             else:\n-        default_format = \"h5\"\n+                return \"h5\"\n+        if save_format in (\"h5\", \"hdf5\"):\n+            return \"h5\"\n+        if save_format in (\"tf\", \"tensorflow\"):\n+            return \"tf\"\n \n-    if (h5py is not None and isinstance(filepath, h5py.File)) or str(\n-        filepath\n-    ).endswith((\".h5\", \".hdf5\")):\n-        if save_format and save_format != \"h5\":\n         raise ValueError(\n-                \"Provided `save_format` is inconsistent with `filepath`. \"\n-                f\"Received: save_format='{save_format}', filepath='{filepath}'\"\n+            \"Unknown `save_format` argument. Expected one of \"\n+            \"'keras', 'tf', or 'h5'. \"\n+            f\"Received: save_format{save_format}\"\n         )\n-        save_format = \"h5\"\n \n-    return save_format or default_format\n+    # No save format specified: infer from filepath.\n+\n+    if str(filepath).endswith(\".keras\"):\n+        if saving_lib.saving_v3_enabled():\n+            return \"keras\"\n+        else:\n+            return \"h5\"\n+\n+    if str(filepath).endswith((\".h5\", \".hdf5\")):\n+        return \"h5\"\n+\n+    if h5py is not None and isinstance(filepath, h5py.File):\n+        return \"h5\"\n+\n+    # No recognizable file format: default to TF in TF2 and h5 in TF1.\n+\n+    if tf.__internal__.tf2.enabled():\n+        return \"tf\"\n+    else:\n+        return \"h5\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f648e5772d38d5c05e50c92ac8a72625324ab57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 419 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -85,8 +85,10 @@ class DefunWrapper:\n \n             layer_func = gru.gru_with_backend_selection\n \n-        self.defun_layer = tf.__internal__.function.defun_with_attributes(\n-            layer_func, attributes=supportive_attributes, autograph=False\n+        self.defun_layer = tf.function(\n+            layer_func,\n+            autograph=False,\n+            experimental_attributes=supportive_attributes,\n         )\n \n     def __deepcopy__(self, memo):\n@@ -223,8 +225,8 @@ def generate_defun_backend(\n         _FUNCTION_DEVICE_ATTRIBUTE: preferred_device,\n     }\n     function_attributes.update(supportive_attributes)\n-    return tf.__internal__.function.defun_with_attributes(\n-        func=func, attributes=function_attributes, autograph=False\n+    return tf.function(\n+        func, autograph=False, experimental_attributes=function_attributes\n     )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#994e586961db13586868ed4be2c4abff16ed6b70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 165 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 189 | Churn Cumulative: 669 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -579,37 +579,76 @@ class InverseTimeDecay(LearningRateSchedule):\n     \"keras.optimizers.schedules.CosineDecay\", \"keras.experimental.CosineDecay\"\n )\n class CosineDecay(LearningRateSchedule):\n-    \"\"\"A LearningRateSchedule that uses a cosine decay schedule.\n+    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n \n     See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n     SGDR: Stochastic Gradient Descent with Warm Restarts.\n \n-    When training a model, it is often useful to lower the learning rate as\n-    the training progresses. This schedule applies a cosine decay function\n-    to an optimizer step, given a provided initial learning rate.\n-    It requires a `step` value to compute the decayed learning rate. You can\n+    For the idea of a linear warmup of our learning rate,\n+    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n+\n+    When we begin training a model, we often want an initial increase in our\n+    learning rate followed by a decay. If `warmup_target` is an int, this\n+    schedule applies a linear increase per optimizer step to our learning rate\n+    from `initial_learning_rate` to `warmup_target` for a duration of\n+    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n+    learning rate from `warmup_target` to `alpha` for a duration of\n+    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n+    will take our learning rate from `initial_learning_rate` to `alpha`.\n+    It requires a `step` value to  compute the learning rate. You can\n     just pass a TensorFlow variable that you increment at each training step.\n \n-    The schedule is a 1-arg callable that produces a decayed learning\n-    rate when passed the current optimizer step. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n-    It is computed as:\n+    The schedule is a 1-arg callable that produces a warmup followed by a\n+    decayed learning rate when passed the current optimizer step. This can be\n+    useful for changing the learning rate value across different invocations of\n+    optimizer functions.\n+\n+    Our warmup is computed as:\n \n     ```python\n+    def warmup_learning_rate(step):\n+        completed_fraction = step / warmup_steps\n+        total_delta = target_warmup - initial_learning_rate\n+        return completed_fraction * total_delta\n+    ```\n+\n+    And our decay is computed as:\n+\n+    ```python\n+    if warmup_target is None:\n+        initial_decay_lr = initial_learning_rate\n+    else:\n+        initial_decay_lr = warmup_target\n+\n     def decayed_learning_rate(step):\n         step = min(step, decay_steps)\n         cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n         decayed = (1 - alpha) * cosine_decay + alpha\n-      return initial_learning_rate * decayed\n+        return initial_decay_lr * decayed\n     ```\n \n-    Example usage:\n+    Example usage without warmup:\n+\n     ```python\n     decay_steps = 1000\n+    initial_learning_rate = 0.1\n     lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n         initial_learning_rate, decay_steps)\n     ```\n \n+    Example usage with warmup:\n+\n+    ```python\n+    decay_steps = 1000\n+    initial_learning_rate = 0\n+    warmup_steps = 1000\n+    target_learning_rate = 0.1\n+    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n+        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n+        warmup_steps=warmup_steps\n+    )\n+    ```\n+\n     You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n     as the learning rate. The learning rate schedule is also serializable and\n     deserializable using `tf.keras.optimizers.schedules.serialize` and\n@@ -622,19 +661,34 @@ class CosineDecay(LearningRateSchedule):\n     \"\"\"\n \n     def __init__(\n-        self, initial_learning_rate, decay_steps, alpha=0.0, name=None\n+        self,\n+        initial_learning_rate,\n+        decay_steps,\n+        alpha=0.0,\n+        name=None,\n+        warmup_target=None,\n+        warmup_steps=0,\n     ):\n         \"\"\"Applies cosine decay to the learning rate.\n \n         Args:\n-          initial_learning_rate: A scalar `float32` or `float64` Tensor or a\n-            Python number. The initial learning rate.\n-          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n+          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n+            Python int. The initial learning rate.\n+          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n             Number of steps to decay over.\n-          alpha: A scalar `float32` or `float64` Tensor or a Python number.\n-            Minimum learning rate value as a fraction of initial_learning_rate.\n+          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n+            Minimum learning rate value for decay as a fraction of\n+            `initial_learning_rate`.\n           name: String. Optional name of the operation.  Defaults to\n             'CosineDecay'.\n+          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n+            Python int. The target learning rate for our warmup phase. Will cast\n+            to the `initial_learning_rate` datatype. Setting to None will skip\n+            warmup and begins decay phase from `initial_learning_rate`.\n+            Otherwise scheduler will warmup from `initial_learning_rate` to\n+            `warmup_target`.\n+          warmup_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n+            Number of steps to warmup over.\n         \"\"\"\n         super().__init__()\n \n@@ -642,6 +696,24 @@ class CosineDecay(LearningRateSchedule):\n         self.decay_steps = decay_steps\n         self.alpha = alpha\n         self.name = name\n+        self.warmup_steps = warmup_steps\n+        self.warmup_target = warmup_target\n+\n+    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n+        with tf.name_scope(self.name or \"CosineDecay\"):\n+            completed_fraction = step / decay_steps\n+            tf_pi = tf.constant(math.pi, dtype=dtype)\n+            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n+            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n+            return tf.multiply(decay_from_lr, decayed)\n+\n+    def _warmup_function(\n+        self, step, warmup_steps, warmup_target, initial_learning_rate\n+    ):\n+        with tf.name_scope(self.name or \"CosineDecay\"):\n+            completed_fraction = step / warmup_steps\n+            total_step_delta = warmup_target - initial_learning_rate\n+            return total_step_delta * completed_fraction + initial_learning_rate\n \n     def __call__(self, step):\n         with tf.name_scope(self.name or \"CosineDecay\"):\n@@ -650,17 +722,39 @@ class CosineDecay(LearningRateSchedule):\n             )\n             dtype = initial_learning_rate.dtype\n             decay_steps = tf.cast(self.decay_steps, dtype)\n-\n             global_step_recomp = tf.cast(step, dtype)\n+\n+            if self.warmup_target is None:\n                 global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n-            completed_fraction = global_step_recomp / decay_steps\n-            cosine_decayed = 0.5 * (\n-                1.0\n-                + tf.cos(tf.constant(math.pi, dtype=dtype) * completed_fraction)\n+                return self._decay_function(\n+                    global_step_recomp,\n+                    decay_steps,\n+                    initial_learning_rate,\n+                    dtype,\n                 )\n \n-            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n-            return tf.multiply(initial_learning_rate, decayed)\n+            warmup_target = tf.cast(self.warmup_target, dtype)\n+            warmup_steps = tf.cast(self.warmup_steps, dtype)\n+\n+            global_step_recomp = tf.minimum(\n+                global_step_recomp, decay_steps + warmup_steps\n+            )\n+\n+            return tf.cond(\n+                global_step_recomp < warmup_steps,\n+                lambda: self._warmup_function(\n+                    global_step_recomp,\n+                    warmup_steps,\n+                    warmup_target,\n+                    initial_learning_rate,\n+                ),\n+                lambda: self._decay_function(\n+                    global_step_recomp - warmup_steps,\n+                    decay_steps,\n+                    warmup_target,\n+                    dtype,\n+                ),\n+            )\n \n     def get_config(self):\n         return {\n@@ -668,6 +762,8 @@ class CosineDecay(LearningRateSchedule):\n             \"decay_steps\": self.decay_steps,\n             \"alpha\": self.alpha,\n             \"name\": self.name,\n+            \"warmup_target\": self.warmup_target,\n+            \"warmup_steps\": self.warmup_steps,\n         }\n \n \n\n@@ -361,6 +361,28 @@ class CosineDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n \n+    def linear_warmup(self, step, warmup_steps, initial_lr, target_lr):\n+        completed_fraction = step / warmup_steps\n+        total_delta = target_lr - initial_lr\n+        return completed_fraction * total_delta\n+\n+    def testWarmup(self, serialize):\n+        warmup_steps = 1500\n+        initial_lr = 0.0\n+        target_lr = 10.0\n+        for step in range(0, 1500, 250):\n+            lr = learning_rate_schedule.CosineDecay(\n+                initial_lr,\n+                0,\n+                warmup_target=target_lr,\n+                warmup_steps=warmup_steps,\n+            )\n+            lr = _maybe_serialized(lr, serialize)\n+            expected = self.linear_warmup(\n+                step, warmup_steps, initial_lr, target_lr\n+            )\n+            self.assertAllClose(self.evaluate(lr(step)), expected)\n+\n     def testAlpha(self, serialize):\n         num_training_steps = 1000\n         initial_lr = 1.0\n@@ -384,6 +406,29 @@ class CosineDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n             expected = self.np_cosine_decay(step, num_training_steps)\n             self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n \n+    def testWarmupDecay(self, serialize):\n+        warmup_steps = 2000\n+        decay_steps = 1000\n+        initial_lr = 0.0\n+        target_lr = 10.0\n+        for step in range(0, 3000, 250):\n+            lr = learning_rate_schedule.CosineDecay(\n+                initial_lr,\n+                decay_steps,\n+                warmup_target=target_lr,\n+                warmup_steps=warmup_steps,\n+            )\n+            lr = _maybe_serialized(lr, serialize)\n+            if step < warmup_steps + 1:\n+                expected = self.linear_warmup(\n+                    step, warmup_steps, initial_lr, target_lr\n+                )\n+            else:\n+                expected = target_lr * self.np_cosine_decay(\n+                    step - warmup_steps, decay_steps\n+                )\n+            self.assertAllClose(self.evaluate(lr(step)), expected)\n+\n \n @test_combinations.generate(\n     test_combinations.combine(serialize=[False, True], mode=[\"graph\", \"eager\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#caaf8512e4d69f576e7582257fe9633f424395a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 329 | Lines Deleted: 328 | Files Changed: 1 | Hunks: 4 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 657 | Churn Cumulative: 3408 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -323,22 +323,6 @@ class BatchNormalizationBase(Layer):\n         else:\n             return self.dtype or tf.float32\n \n-    def _support_zero_size_input(self):\n-        if not tf.distribute.has_strategy():\n-            return False\n-        strategy = tf.distribute.get_strategy()\n-        # TODO(b/195085185): remove experimental_enable_get_next_as_optional\n-        # after migrating all users.\n-        return getattr(\n-            strategy.extended,\n-            \"enable_partial_batch_handling\",\n-            getattr(\n-                strategy.extended,\n-                \"experimental_enable_get_next_as_optional\",\n-                False,\n-            ),\n-        )\n-\n     def build(self, input_shape):\n         self.axis = tf_utils.validate_axis(self.axis, input_shape)\n         input_shape = tf.TensorShape(input_shape)\n@@ -560,6 +544,307 @@ class BatchNormalizationBase(Layer):\n                 self._scope.set_partitioner(partitioner)\n         self.built = True\n \n+    def call(self, inputs, training=None, mask=None):\n+        inputs = tf.cast(inputs, self.compute_dtype)\n+        training = self._get_training_value(training)\n+        # Determine a boolean value for `training`: could be True, False, or\n+        # None.\n+        training_value = control_flow_util.constant_value(training)\n+\n+        if self.virtual_batch_size is not None:\n+            # Virtual batches (aka ghost batches) can be simulated by reshaping\n+            # the Tensor and reusing the existing batch norm implementation\n+            original_shape = tf.shape(inputs)\n+            original_shape = tf.concat(\n+                [tf.constant([-1]), original_shape[1:]], axis=0\n+            )\n+\n+            if tf.__internal__.tf2.enabled():\n+                expanded_shape = (\n+                    [self.virtual_batch_size, -1] if training_value else [-1, 1]\n+                )\n+                expanded_shape = tf.concat(\n+                    [\n+                        tf.constant(expanded_shape),\n+                        original_shape[1:],\n+                    ],\n+                    axis=0,\n+                )\n+            else:\n+                # Preserve incorrect legacy behavior for backwards compatibility\n+                expanded_shape = tf.concat(\n+                    [\n+                        tf.constant([self.virtual_batch_size, -1]),\n+                        original_shape[1:],\n+                    ],\n+                    axis=0,\n+                )\n+\n+            # Will cause errors if virtual_batch_size does not divide the batch\n+            # size\n+            inputs = tf.reshape(inputs, expanded_shape)\n+\n+            def undo_virtual_batching(outputs):\n+                outputs = tf.reshape(outputs, original_shape)\n+                return outputs\n+\n+        if self.fused:\n+            outputs = self._fused_batch_norm(\n+                inputs, mask=mask, training=training\n+            )\n+            if self.virtual_batch_size is not None:\n+                # Currently never reaches here since fused_batch_norm does not\n+                # support virtual batching\n+                outputs = undo_virtual_batching(outputs)\n+            return outputs\n+\n+        inputs_dtype = inputs.dtype.base_dtype\n+        if inputs_dtype in (tf.float16, tf.bfloat16):\n+            # Do all math in float32 if given 16-bit inputs for numeric\n+            # stability.  In particular, it's very easy for variance to overflow\n+            # in float16 and for safety we also choose to cast bfloat16 to\n+            # float32.\n+            inputs = tf.cast(inputs, tf.float32)\n+\n+        # Compute the axes along which to reduce the mean / variance\n+        input_shape = inputs.shape\n+        ndims = len(input_shape)\n+        reduction_axes = [i for i in range(ndims) if i not in self.axis]\n+        if self.virtual_batch_size is not None:\n+            del reduction_axes[1]  # Do not reduce along virtual batch dim\n+\n+        # Broadcasting only necessary for single-axis batch norm where the axis\n+        # is not the last dimension\n+        broadcast_shape = [1] * ndims\n+        broadcast_shape[self.axis[0]] = input_shape.dims[self.axis[0]].value\n+\n+        def _broadcast(v):\n+            if (\n+                v is not None\n+                and len(v.shape) != ndims\n+                and reduction_axes != list(range(ndims - 1))\n+            ):\n+                return tf.reshape(v, broadcast_shape)\n+            return v\n+\n+        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n+\n+        def _compose_transforms(scale, offset, then_scale, then_offset):\n+            if then_scale is not None:\n+                scale *= then_scale\n+                offset *= then_scale\n+            if then_offset is not None:\n+                offset += then_offset\n+            return (scale, offset)\n+\n+        if training_value == False:  # noqa: E712\n+            mean, variance = self.moving_mean, self.moving_variance\n+        else:\n+            if self.adjustment:\n+                adj_scale, adj_bias = self.adjustment(tf.shape(inputs))\n+                # Adjust only during training.\n+                adj_scale = control_flow_util.smart_cond(\n+                    training, lambda: adj_scale, lambda: tf.ones_like(adj_scale)\n+                )\n+                adj_bias = control_flow_util.smart_cond(\n+                    training, lambda: adj_bias, lambda: tf.zeros_like(adj_bias)\n+                )\n+                scale, offset = _compose_transforms(\n+                    adj_scale, adj_bias, scale, offset\n+                )\n+\n+            # Some of the computations here are not necessary when\n+            # training==False but not a constant. However, this makes the code\n+            # simpler.\n+            keep_dims = (\n+                self.virtual_batch_size is not None or len(self.axis) > 1\n+            )\n+            mean, variance = self._moments(\n+                tf.cast(inputs, self._param_dtype),\n+                reduction_axes,\n+                keep_dims=keep_dims,\n+                mask=mask,\n+            )\n+\n+            moving_mean = self.moving_mean\n+            moving_variance = self.moving_variance\n+\n+            mean = control_flow_util.smart_cond(\n+                training,\n+                lambda: mean,\n+                lambda: tf.convert_to_tensor(moving_mean),\n+            )\n+            variance = control_flow_util.smart_cond(\n+                training,\n+                lambda: variance,\n+                lambda: tf.convert_to_tensor(moving_variance),\n+            )\n+\n+            if self.virtual_batch_size is not None:\n+                # This isn't strictly correct since in ghost batch norm, you are\n+                # supposed to sequentially update the moving_mean and\n+                # moving_variance with each sub-batch. However, since the moving\n+                # statistics are only used during evaluation, it is more\n+                # efficient to just update in one step and should not make a\n+                # significant difference in the result.\n+                new_mean = tf.reduce_mean(mean, axis=1, keepdims=True)\n+                new_variance = tf.reduce_mean(variance, axis=1, keepdims=True)\n+            else:\n+                new_mean, new_variance = mean, variance\n+\n+            if self._support_zero_size_input():\n+                # Keras assumes that batch dimension is the first dimension for\n+                # Batch Normalization.\n+                input_batch_size = tf.shape(inputs)[0]\n+            else:\n+                input_batch_size = None\n+\n+            if self.renorm:\n+                (\n+                    r,\n+                    d,\n+                    new_mean,\n+                    new_variance,\n+                ) = self._renorm_correction_and_moments(\n+                    new_mean, new_variance, training, input_batch_size\n+                )\n+                # When training, the normalized values (say, x) will be\n+                # transformed as x * gamma + beta without renorm, and (x * r +\n+                # d) * gamma + beta = x * (r * gamma) + (d * gamma + beta) with\n+                # renorm.\n+                r = _broadcast(tf.stop_gradient(r, name=\"renorm_r\"))\n+                d = _broadcast(tf.stop_gradient(d, name=\"renorm_d\"))\n+                scale, offset = _compose_transforms(r, d, scale, offset)\n+\n+            def _do_update(var, value):\n+                \"\"\"Compute the updates for mean and variance.\"\"\"\n+                return self._assign_moving_average(\n+                    var, value, self.momentum, input_batch_size\n+                )\n+\n+            def mean_update():\n+                true_branch = lambda: _do_update(self.moving_mean, new_mean)\n+                false_branch = lambda: self.moving_mean\n+                return control_flow_util.smart_cond(\n+                    training, true_branch, false_branch\n+                )\n+\n+            def variance_update():\n+                \"\"\"Update the moving variance.\"\"\"\n+\n+                def true_branch_renorm():\n+                    # We apply epsilon as part of the moving_stddev to mirror\n+                    # the training code path.\n+                    moving_stddev = _do_update(\n+                        self.moving_stddev, tf.sqrt(new_variance + self.epsilon)\n+                    )\n+                    return self._assign_new_value(\n+                        self.moving_variance,\n+                        # Apply relu in case floating point rounding causes it\n+                        # to go negative.\n+                        backend.relu(\n+                            moving_stddev * moving_stddev - self.epsilon\n+                        ),\n+                    )\n+\n+                if self.renorm:\n+                    true_branch = true_branch_renorm\n+                else:\n+                    true_branch = lambda: _do_update(\n+                        self.moving_variance, new_variance\n+                    )\n+\n+                false_branch = lambda: self.moving_variance\n+                return control_flow_util.smart_cond(\n+                    training, true_branch, false_branch\n+                )\n+\n+            self.add_update(mean_update)\n+            self.add_update(variance_update)\n+\n+        mean = tf.cast(mean, inputs.dtype)\n+        variance = tf.cast(variance, inputs.dtype)\n+        if offset is not None:\n+            offset = tf.cast(offset, inputs.dtype)\n+        if scale is not None:\n+            scale = tf.cast(scale, inputs.dtype)\n+        outputs = tf.nn.batch_normalization(\n+            inputs,\n+            _broadcast(mean),\n+            _broadcast(variance),\n+            offset,\n+            scale,\n+            self.epsilon,\n+        )\n+        if inputs_dtype in (tf.float16, tf.bfloat16):\n+            outputs = tf.cast(outputs, inputs_dtype)\n+\n+        # If some components of the shape got lost due to adjustments, fix that.\n+        outputs.set_shape(input_shape)\n+\n+        if self.virtual_batch_size is not None:\n+            outputs = undo_virtual_batching(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"axis\": self.axis,\n+            \"momentum\": self.momentum,\n+            \"epsilon\": self.epsilon,\n+            \"center\": self.center,\n+            \"scale\": self.scale,\n+            \"beta_initializer\": initializers.serialize(self.beta_initializer),\n+            \"gamma_initializer\": initializers.serialize(self.gamma_initializer),\n+            \"moving_mean_initializer\": initializers.serialize(\n+                self.moving_mean_initializer\n+            ),\n+            \"moving_variance_initializer\": initializers.serialize(\n+                self.moving_variance_initializer\n+            ),\n+            \"beta_regularizer\": regularizers.serialize(self.beta_regularizer),\n+            \"gamma_regularizer\": regularizers.serialize(self.gamma_regularizer),\n+            \"beta_constraint\": constraints.serialize(self.beta_constraint),\n+            \"gamma_constraint\": constraints.serialize(self.gamma_constraint),\n+        }\n+        # Only add TensorFlow-specific parameters if they are set, so as to\n+        # preserve model compatibility with external Keras.\n+        if self.renorm:\n+            config[\"renorm\"] = True\n+            config[\"renorm_clipping\"] = self.renorm_clipping\n+            config[\"renorm_momentum\"] = self.renorm_momentum\n+        if self.virtual_batch_size is not None:\n+            config[\"virtual_batch_size\"] = self.virtual_batch_size\n+        # Note: adjustment is not serializable.\n+        if self.adjustment is not None:\n+            logging.warning(\n+                \"The `adjustment` function of this `BatchNormalization` \"\n+                \"layer cannot be serialized and has been omitted from \"\n+                \"the layer config. It will not be included when \"\n+                \"re-creating the layer from the saved config.\"\n+            )\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    ######################## Start of private methods ##########################\n+    def _support_zero_size_input(self):\n+        if not tf.distribute.has_strategy():\n+            return False\n+        strategy = tf.distribute.get_strategy()\n+        # TODO(b/195085185): remove experimental_enable_get_next_as_optional\n+        # after migrating all users.\n+        return getattr(\n+            strategy.extended,\n+            \"enable_partial_batch_handling\",\n+            getattr(\n+                strategy.extended,\n+                \"experimental_enable_get_next_as_optional\",\n+                False,\n+            ),\n+        )\n+\n     def _assign_moving_average(self, variable, value, momentum, inputs_size):\n         def calculate_update_delta():\n             decay = tf.convert_to_tensor(1.0 - momentum, name=\"decay\")\n@@ -846,318 +1131,6 @@ class BatchNormalizationBase(Layer):\n                 keepdims=keep_dims,\n             )\n \n-    def _moments(self, inputs, reduction_axes, keep_dims, mask=None):\n-        mean, variance = self._calculate_mean_and_var(\n-            inputs, reduction_axes, keep_dims, mask=mask\n-        )\n-        # TODO(b/129279393): Support zero batch input in non\n-        # DistributionStrategy code as well.\n-        if self._support_zero_size_input():\n-            input_batch_size = tf.shape(inputs)[0]\n-            mean = tf.where(\n-                input_batch_size > 0, mean, backend.zeros_like(mean)\n-            )\n-            variance = tf.where(\n-                input_batch_size > 0, variance, backend.zeros_like(variance)\n-            )\n-        return mean, variance\n-\n-    def _get_training_value(self, training=None):\n-        if training is None:\n-            training = backend.learning_phase()\n-        if self._USE_V2_BEHAVIOR:\n-            if isinstance(training, int):\n-                training = bool(training)\n-            if not self.trainable:\n-                # When the layer is not trainable, it overrides the value passed\n-                # from model.\n-                training = False\n-        return training\n-\n-    def call(self, inputs, training=None, mask=None):\n-        inputs = tf.cast(inputs, self.compute_dtype)\n-        training = self._get_training_value(training)\n-        # Determine a boolean value for `training`: could be True, False, or\n-        # None.\n-        training_value = control_flow_util.constant_value(training)\n-\n-        if self.virtual_batch_size is not None:\n-            # Virtual batches (aka ghost batches) can be simulated by reshaping\n-            # the Tensor and reusing the existing batch norm implementation\n-            original_shape = tf.shape(inputs)\n-            original_shape = tf.concat(\n-                [tf.constant([-1]), original_shape[1:]], axis=0\n-            )\n-\n-            if tf.__internal__.tf2.enabled():\n-                expanded_shape = (\n-                    [self.virtual_batch_size, -1] if training_value else [-1, 1]\n-                )\n-                expanded_shape = tf.concat(\n-                    [\n-                        tf.constant(expanded_shape),\n-                        original_shape[1:],\n-                    ],\n-                    axis=0,\n-                )\n-            else:\n-                # Preserve incorrect legacy behavior for backwards compatibility\n-                expanded_shape = tf.concat(\n-                    [\n-                        tf.constant([self.virtual_batch_size, -1]),\n-                        original_shape[1:],\n-                    ],\n-                    axis=0,\n-                )\n-\n-            # Will cause errors if virtual_batch_size does not divide the batch\n-            # size\n-            inputs = tf.reshape(inputs, expanded_shape)\n-\n-            def undo_virtual_batching(outputs):\n-                outputs = tf.reshape(outputs, original_shape)\n-                return outputs\n-\n-        if self.fused:\n-            outputs = self._fused_batch_norm(\n-                inputs, mask=mask, training=training\n-            )\n-            if self.virtual_batch_size is not None:\n-                # Currently never reaches here since fused_batch_norm does not\n-                # support virtual batching\n-                outputs = undo_virtual_batching(outputs)\n-            return outputs\n-\n-        inputs_dtype = inputs.dtype.base_dtype\n-        if inputs_dtype in (tf.float16, tf.bfloat16):\n-            # Do all math in float32 if given 16-bit inputs for numeric\n-            # stability.  In particular, it's very easy for variance to overflow\n-            # in float16 and for safety we also choose to cast bfloat16 to\n-            # float32.\n-            inputs = tf.cast(inputs, tf.float32)\n-\n-        # Compute the axes along which to reduce the mean / variance\n-        input_shape = inputs.shape\n-        ndims = len(input_shape)\n-        reduction_axes = [i for i in range(ndims) if i not in self.axis]\n-        if self.virtual_batch_size is not None:\n-            del reduction_axes[1]  # Do not reduce along virtual batch dim\n-\n-        # Broadcasting only necessary for single-axis batch norm where the axis\n-        # is not the last dimension\n-        broadcast_shape = [1] * ndims\n-        broadcast_shape[self.axis[0]] = input_shape.dims[self.axis[0]].value\n-\n-        def _broadcast(v):\n-            if (\n-                v is not None\n-                and len(v.shape) != ndims\n-                and reduction_axes != list(range(ndims - 1))\n-            ):\n-                return tf.reshape(v, broadcast_shape)\n-            return v\n-\n-        scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n-\n-        def _compose_transforms(scale, offset, then_scale, then_offset):\n-            if then_scale is not None:\n-                scale *= then_scale\n-                offset *= then_scale\n-            if then_offset is not None:\n-                offset += then_offset\n-            return (scale, offset)\n-\n-        if training_value == False:  # noqa: E712\n-            mean, variance = self.moving_mean, self.moving_variance\n-        else:\n-            if self.adjustment:\n-                adj_scale, adj_bias = self.adjustment(tf.shape(inputs))\n-                # Adjust only during training.\n-                adj_scale = control_flow_util.smart_cond(\n-                    training, lambda: adj_scale, lambda: tf.ones_like(adj_scale)\n-                )\n-                adj_bias = control_flow_util.smart_cond(\n-                    training, lambda: adj_bias, lambda: tf.zeros_like(adj_bias)\n-                )\n-                scale, offset = _compose_transforms(\n-                    adj_scale, adj_bias, scale, offset\n-                )\n-\n-            # Some of the computations here are not necessary when\n-            # training==False but not a constant. However, this makes the code\n-            # simpler.\n-            keep_dims = (\n-                self.virtual_batch_size is not None or len(self.axis) > 1\n-            )\n-            mean, variance = self._moments(\n-                tf.cast(inputs, self._param_dtype),\n-                reduction_axes,\n-                keep_dims=keep_dims,\n-                mask=mask,\n-            )\n-\n-            moving_mean = self.moving_mean\n-            moving_variance = self.moving_variance\n-\n-            mean = control_flow_util.smart_cond(\n-                training,\n-                lambda: mean,\n-                lambda: tf.convert_to_tensor(moving_mean),\n-            )\n-            variance = control_flow_util.smart_cond(\n-                training,\n-                lambda: variance,\n-                lambda: tf.convert_to_tensor(moving_variance),\n-            )\n-\n-            if self.virtual_batch_size is not None:\n-                # This isn't strictly correct since in ghost batch norm, you are\n-                # supposed to sequentially update the moving_mean and\n-                # moving_variance with each sub-batch. However, since the moving\n-                # statistics are only used during evaluation, it is more\n-                # efficient to just update in one step and should not make a\n-                # significant difference in the result.\n-                new_mean = tf.reduce_mean(mean, axis=1, keepdims=True)\n-                new_variance = tf.reduce_mean(variance, axis=1, keepdims=True)\n-            else:\n-                new_mean, new_variance = mean, variance\n-\n-            if self._support_zero_size_input():\n-                # Keras assumes that batch dimension is the first dimension for\n-                # Batch Normalization.\n-                input_batch_size = tf.shape(inputs)[0]\n-            else:\n-                input_batch_size = None\n-\n-            if self.renorm:\n-                (\n-                    r,\n-                    d,\n-                    new_mean,\n-                    new_variance,\n-                ) = self._renorm_correction_and_moments(\n-                    new_mean, new_variance, training, input_batch_size\n-                )\n-                # When training, the normalized values (say, x) will be\n-                # transformed as x * gamma + beta without renorm, and (x * r +\n-                # d) * gamma + beta = x * (r * gamma) + (d * gamma + beta) with\n-                # renorm.\n-                r = _broadcast(tf.stop_gradient(r, name=\"renorm_r\"))\n-                d = _broadcast(tf.stop_gradient(d, name=\"renorm_d\"))\n-                scale, offset = _compose_transforms(r, d, scale, offset)\n-\n-            def _do_update(var, value):\n-                \"\"\"Compute the updates for mean and variance.\"\"\"\n-                return self._assign_moving_average(\n-                    var, value, self.momentum, input_batch_size\n-                )\n-\n-            def mean_update():\n-                true_branch = lambda: _do_update(self.moving_mean, new_mean)\n-                false_branch = lambda: self.moving_mean\n-                return control_flow_util.smart_cond(\n-                    training, true_branch, false_branch\n-                )\n-\n-            def variance_update():\n-                \"\"\"Update the moving variance.\"\"\"\n-\n-                def true_branch_renorm():\n-                    # We apply epsilon as part of the moving_stddev to mirror\n-                    # the training code path.\n-                    moving_stddev = _do_update(\n-                        self.moving_stddev, tf.sqrt(new_variance + self.epsilon)\n-                    )\n-                    return self._assign_new_value(\n-                        self.moving_variance,\n-                        # Apply relu in case floating point rounding causes it\n-                        # to go negative.\n-                        backend.relu(\n-                            moving_stddev * moving_stddev - self.epsilon\n-                        ),\n-                    )\n-\n-                if self.renorm:\n-                    true_branch = true_branch_renorm\n-                else:\n-                    true_branch = lambda: _do_update(\n-                        self.moving_variance, new_variance\n-                    )\n-\n-                false_branch = lambda: self.moving_variance\n-                return control_flow_util.smart_cond(\n-                    training, true_branch, false_branch\n-                )\n-\n-            self.add_update(mean_update)\n-            self.add_update(variance_update)\n-\n-        mean = tf.cast(mean, inputs.dtype)\n-        variance = tf.cast(variance, inputs.dtype)\n-        if offset is not None:\n-            offset = tf.cast(offset, inputs.dtype)\n-        if scale is not None:\n-            scale = tf.cast(scale, inputs.dtype)\n-        outputs = tf.nn.batch_normalization(\n-            inputs,\n-            _broadcast(mean),\n-            _broadcast(variance),\n-            offset,\n-            scale,\n-            self.epsilon,\n-        )\n-        if inputs_dtype in (tf.float16, tf.bfloat16):\n-            outputs = tf.cast(outputs, inputs_dtype)\n-\n-        # If some components of the shape got lost due to adjustments, fix that.\n-        outputs.set_shape(input_shape)\n-\n-        if self.virtual_batch_size is not None:\n-            outputs = undo_virtual_batching(outputs)\n-        return outputs\n-\n-    def compute_output_shape(self, input_shape):\n-        return input_shape\n-\n-    def get_config(self):\n-        config = {\n-            \"axis\": self.axis,\n-            \"momentum\": self.momentum,\n-            \"epsilon\": self.epsilon,\n-            \"center\": self.center,\n-            \"scale\": self.scale,\n-            \"beta_initializer\": initializers.serialize(self.beta_initializer),\n-            \"gamma_initializer\": initializers.serialize(self.gamma_initializer),\n-            \"moving_mean_initializer\": initializers.serialize(\n-                self.moving_mean_initializer\n-            ),\n-            \"moving_variance_initializer\": initializers.serialize(\n-                self.moving_variance_initializer\n-            ),\n-            \"beta_regularizer\": regularizers.serialize(self.beta_regularizer),\n-            \"gamma_regularizer\": regularizers.serialize(self.gamma_regularizer),\n-            \"beta_constraint\": constraints.serialize(self.beta_constraint),\n-            \"gamma_constraint\": constraints.serialize(self.gamma_constraint),\n-        }\n-        # Only add TensorFlow-specific parameters if they are set, so as to\n-        # preserve model compatibility with external Keras.\n-        if self.renorm:\n-            config[\"renorm\"] = True\n-            config[\"renorm_clipping\"] = self.renorm_clipping\n-            config[\"renorm_momentum\"] = self.renorm_momentum\n-        if self.virtual_batch_size is not None:\n-            config[\"virtual_batch_size\"] = self.virtual_batch_size\n-        # Note: adjustment is not serializable.\n-        if self.adjustment is not None:\n-            logging.warning(\n-                \"The `adjustment` function of this `BatchNormalization` \"\n-                \"layer cannot be serialized and has been omitted from \"\n-                \"the layer config. It will not be included when \"\n-                \"re-creating the layer from the saved config.\"\n-            )\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n     def _sync_calculate_mean_and_var(self, x, axes, keep_dims, mask=None):\n         with backend.name_scope(\"moments\"):\n             # The dynamic range of fp16 is too limited to support the collection\n@@ -1218,6 +1191,34 @@ class BatchNormalizationBase(Layer):\n             else:\n                 return (mean, variance)\n \n+    def _moments(self, inputs, reduction_axes, keep_dims, mask=None):\n+        mean, variance = self._calculate_mean_and_var(\n+            inputs, reduction_axes, keep_dims, mask=mask\n+        )\n+        # TODO(b/129279393): Support zero batch input in non\n+        # DistributionStrategy code as well.\n+        if self._support_zero_size_input():\n+            input_batch_size = tf.shape(inputs)[0]\n+            mean = tf.where(\n+                input_batch_size > 0, mean, backend.zeros_like(mean)\n+            )\n+            variance = tf.where(\n+                input_batch_size > 0, variance, backend.zeros_like(variance)\n+            )\n+        return mean, variance\n+\n+    def _get_training_value(self, training=None):\n+        if training is None:\n+            training = backend.learning_phase()\n+        if self._USE_V2_BEHAVIOR:\n+            if isinstance(training, int):\n+                training = bool(training)\n+            if not self.trainable:\n+                # When the layer is not trainable, it overrides the value passed\n+                # from model.\n+                training = False\n+        return training\n+\n \n @keras_export(\"keras.layers.BatchNormalization\", v1=[])\n class BatchNormalization(BatchNormalizationBase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
