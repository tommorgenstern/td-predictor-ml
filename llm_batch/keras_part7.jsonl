{"custom_id": "keras#189e4dae6f0d2179bb45d47ffbb10c6916196788", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 42 | Churn Cumulative: 395 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -227,3 +227,45 @@ class TimeDistributedDense(Layer):\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__}\n+\n+\n+\n+\n+class Maxout(Layer):\n+    '''\n+        Max-out layer, n_features is the number of pieces in the piecewise linear approx.\n+        Refer to http://arxiv.org/pdf/1302.4389.pdf\n+    '''\n+    def __init__(self, input_dim, output_dim, n_features=4, init='glorot_uniform', weights=None, \n+        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+\n+        super(Maxout,self).__init__()\n+        self.init = initializations.get(init)\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+        self.n_features = n_features\n+\n+        self.input = T.matrix()\n+        self.W = self.init((self.n_features, self.input_dim, self.output_dim))\n+        self.b = shared_zeros((self.n_features, self.output_dim))\n+\n+        self.params = [self.W, self.b]\n+\n+        self.regularizers = [W_regularizer, b_regularizer]\n+        self.constraints = [W_constraint, b_constraint]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        # -- don't need activation since it's just linear.\n+        output = T.max(T.dot(X, self.W) + self.b, axis=1)\n+        return output\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"input_dim\":self.input_dim,\n+            \"output_dim\":self.output_dim,\n+            \"init\":self.init.__name__,\n+            \"n_features\" : self.n_features}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1c6499c6e844e33064736135a1fabef80374a42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 5 | Churn Cumulative: 792 | Contributors (this commit): 12 | Commits (past 90d): 33 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -169,9 +169,6 @@ class Sequential(object):\n                 history['val_acc'] = []\n \n         index_array = np.arange(len(X))\n-\n-        # -- allow user to terminate training\n-        try:\n         for epoch in range(nb_epoch):\n             if verbose:\n                 print('Epoch', epoch)\n@@ -224,8 +221,6 @@ class Sequential(object):\n                 history['acc'].append(av_acc/seen)\n                 if do_validation:\n                     history['val_acc'].append(float(val_acc))\n-        except KeyboardInterrupt:\n-            print('Terminating on KeyboardInterrupt at Epoch', epoch)\n         return history\n \n     def predict(self, X, batch_size=128, verbose=1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75fe1fd242ff306b854094a3a83d7be394e83834", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 411 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -231,23 +231,23 @@ class TimeDistributedDense(Layer):\n \n \n \n-class Maxout(Layer):\n+class MaxoutDense(Layer):\n     '''\n-        Max-out layer, n_features is the number of pieces in the piecewise linear approx.\n+        Max-out layer, nb_feature is the number of pieces in the piecewise linear approx.\n         Refer to http://arxiv.org/pdf/1302.4389.pdf\n     '''\n-    def __init__(self, input_dim, output_dim, n_features=4, init='glorot_uniform', weights=None, \n+    def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None, \n         W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n \n-        super(Maxout,self).__init__()\n+        super(MaxoutDense,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.n_features = n_features\n+        self.nb_feature = nb_feature\n \n         self.input = T.matrix()\n-        self.W = self.init((self.n_features, self.input_dim, self.output_dim))\n-        self.b = shared_zeros((self.n_features, self.output_dim))\n+        self.W = self.init((self.nb_feature, self.input_dim, self.output_dim))\n+        self.b = shared_zeros((self.nb_feature, self.output_dim))\n \n         self.params = [self.W, self.b]\n \n@@ -268,4 +268,4 @@ class Maxout(Layer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"n_features\" : self.n_features}\n+            \"nb_feature\" : self.nb_feature}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abedfa8a6483a0190a98aa68a5a25f20d983db6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 381 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class Optimizer(object):\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n             norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n-            grads = [clip_norm(g, c, norm) for g in grads]\n+            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n \n         new_grads = []\n         for p, g, r in zip(params, grads, regularizers):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#77fb6a3873ca6bd82a7dc1620eeb706c66177fc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 49 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -73,7 +73,9 @@ class Progbar(object):\n             sys.stdout.write(\"\\b\" * (self.total_width+1))\n             sys.stdout.write(\"\\r\")\n \n-            bar = '%d/%d [' % (current, self.target)\n+            numdigits = int(np.floor(np.log10(self.target))) + 1\n+            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n+            bar = barstr % (current, self.target)\n             prog = float(current)/self.target\n             prog_width = int(self.width*prog)\n             if prog_width > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff3d0624ea66a01f99c1ef6f865b70ebb00f899a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 4 | Churn Cumulative: 796 | Contributors (this commit): 12 | Commits (past 90d): 34 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -256,10 +256,10 @@ class Sequential(Model):\n                 # validation\n                 if do_validation and (batch_index == len(batches) - 1):\n                     if show_accuracy:\n-                        val_loss, val_acc = self.test(X_val, y_val, accuracy=True)\n+                        val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0, show_accuracy=True)\n                         log_values += [('val. loss', val_loss), ('val. acc.', val_acc)]\n                     else:\n-                        val_loss = self.test(X_val, y_val)\n+                        val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n                         log_values += [('val. loss', val_loss)]\n                 \n                 # logging\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1cf2065d31f6d4294d47b6eccb719e2cdf2081d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 4 | Churn Cumulative: 415 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,6 +81,10 @@ class Merge(object):\n                 res.append(o)\n         return res\n \n+    @property\n+    def input(self):\n+        return self.get_input()    \n+\n     def get_weights(self):\n         weights = []\n         for m in self.models:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ad874c9c40c27b3faaad086987e5232fe893048", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 51 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -70,7 +70,7 @@ class Progbar(object):\n         now = time.time()\n         if self.verbose == 1:\n             prev_total_width = self.total_width\n-            sys.stdout.write(\"\\b\" * (self.total_width+1))\n+            sys.stdout.write(\"\\b\" * prev_total_width)\n             sys.stdout.write(\"\\r\")\n \n             numdigits = int(np.floor(np.log10(self.target))) + 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#926883ad108fe23867c29efc8e18687087113b0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 799 | Contributors (this commit): 13 | Commits (past 90d): 35 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,9 +71,6 @@ class Model(object):\n         self.y_train = self.get_output(train=True)\n         self.y_test = self.get_output(train=False)\n \n-        self.y_train = self.get_output(train=True)\n-        self.y_test = self.get_output(train=False)\n-\n         # target of model\n         self.y = T.zeros_like(self.y_train)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc10c4d907d959eb9009ad502c727542a5259613", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 113 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ def skipgrams(sequence, vocabulary_size,\n         if not wi:\n             continue\n         if sampling_table is not None:\n-            if sampling_table[i] < random.random():\n+            if sampling_table[wi] < random.random():\n                 continue\n \n         window_start = max(0, i-window_size)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4df2240c1ce8944ffa88bcaf7ab527e600c4378", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 420 | Contributors (this commit): 6 | Commits (past 90d): 22 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -47,8 +47,6 @@ class Merge(object):\n         ''' Merge the output of a list of models into a single tensor.\n             mode: {'sum', 'concat'}\n         '''\n-        if len(models) < 2:\n-            raise Exception(\"Please specify two or more input models to merge\")\n         self.mode = mode\n         self.models = models\n         self.params = []\n@@ -102,6 +100,9 @@ class Merge(object):\n             \"models\":[m.get_config() for m in self.models],\n             \"mode\":self.mode}\n \n+    def connect(self, node, which=0):\n+        self.models[which].layers[0].previous = node\n+\n \n class Dropout(Layer):\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#12f7f374c38760a265c468f8cb65f8c030a0cddb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 88 | Churn Cumulative: 887 | Contributors (this commit): 14 | Commits (past 90d): 37 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,8 @@ from . import optimizers\n from . import objectives\n from . import regularizers\n from . import constraints\n+from .layers.core import Merge\n+\n import time, copy\n from .utils.generic_utils import Progbar, printv\n from six.moves import range\n@@ -392,3 +394,27 @@ class Sequential(Model):\n             self.layers[k].set_weights(weights)\n         f.close()\n \n+\n+class Autoencoder(Sequential):\n+\n+    def __init__(self, encoder, decoder):\n+        super(Autoencoder,self).__init__()\n+        self.encoder = encoder\n+        self.decoder = decoder\n+\n+        self.add(Merge([self.encoder]))\n+        self.add(Merge([self.decoder]))\n+\n+    def train(self, X, **kwargs):\n+        super(Autoencoder,self).train(X,X,**kwargs)\n+\n+    def test(self, X, **kwargs):\n+        super(Autoencoder,self).test(X,X,**kwargs)\n+\n+    def fit(self, X, **kwargs):\n+        super(Autoencoder,self).fit(X,X,**kwargs)\n+\n+    def freeze_encoder(self):\n+        def zero_grad(g, p):\n+            return 0.\n+        self.encoder.regularizers = [zero_grad for r in self.encoder.regularizers]\n\n@@ -0,0 +1,62 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential, Autoencoder\n+from keras.layers.core import Dense, Dropout, Activation, Layer, Merge\n+from keras.optimizers import RMSprop\n+import theano.tensor as T\n+import numpy as np\n+\n+from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n+srng = RandomStreams()\n+\n+\n+(X_train, y_train), (_, _) = mnist.load_data()\n+\n+X_train = X_train.reshape((X_train.shape[0],-1)).astype(float)/255.\n+\n+encoder = Sequential()\n+decoder = Sequential()\n+\n+n_hidden = 64\n+\n+layer_enc = Dense(X_train.shape[1],n_hidden,W_regularizer=None)\n+layer_dec = Dense(n_hidden,X_train.shape[1])\n+\n+# Tie the weights of encoder and decoder\n+layer_dec.params.remove(layer_dec.W)\n+layer_dec.W = T.transpose(layer_enc.W)\n+\n+encoder.add(layer_enc)\n+encoder.add(Dropout(p=0.3))\n+encoder.add(Activation('relu'))\n+encoder.add(Activation('softmax'))\n+\n+decoder.add(layer_dec)\n+decoder.add(Activation('relu'))\n+\n+autoencoder = Autoencoder(encoder,decoder)\n+\n+rms = RMSprop()\n+autoencoder.compile(loss='mean_squared_error', optimizer=rms)\n+\n+autoencoder.fit(X_train, verbose=1, nb_epoch=10)\n+\n+autoencoder.freeze_encoder()\n+\n+testnet = Sequential()\n+testnet.add(Merge([autoencoder.encoder]))\n+testnet.add(Dense(n_hidden,10))\n+testnet.add(Activation('sigmoid'))\n+\n+W_pretrain = layer_enc.W.get_value()\n+\n+y_train_full = np.zeros((y_train.shape[0],10))\n+for n in range(len(y_train)):\n+    y_train_full[n,y_train[n]] = 1.\n+testnet.compile(loss='mean_squared_error', optimizer=rms)\n+testnet.fit(X_train,y_train_full,nb_epoch=10)\n+\n+W_posttrain = layer_enc.W.get_value()\n+\n+assert(np.abs(W_posttrain-W_pretrain).max() < 1e-9)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74bef0cc90572845e32d1ed6460d62214610dc5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 151 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 151 | Churn Cumulative: 151 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,151 @@\n+from __future__ import absolute_import\n+import numpy as np\n+\n+class KerasClassifier(object):\n+    \"\"\"\n+    Implementation of the scikit-learn classifier API for Keras.\n+\n+    Parameters\n+    ----------\n+    model : object, optional\n+        A pre-compiled Keras model is required to use the scikit-learn wrapper.\n+    \"\"\"\n+    def __init__(self, model=None):\n+        self.model = model\n+        self.classes_ = []\n+        self.config_ = []\n+        self.weights_ = []\n+\n+    def get_params(self, deep=True):\n+        \"\"\"\n+        Get parameters for this estimator.\n+\n+        Parameters\n+        ----------\n+        deep: boolean, optional\n+            If True, will return the parameters for this estimator and\n+            contained subobjects that are estimators.\n+\n+        Returns\n+        -------\n+        params : dict\n+            Dictionary of parameter names mapped to their values.\n+        \"\"\"\n+        return {'model': self.model}\n+\n+    def set_params(self, **params):\n+        \"\"\"\n+        Set the parameters of this estimator.\n+\n+        Parameters\n+        ----------\n+        params: dict\n+            Dictionary of parameter names mapped to their values.\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        for parameter, value in params.items():\n+            self.setattr(parameter, value)\n+        return self\n+\n+    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=0, shuffle=True):\n+        \"\"\"\n+        Fit the model according to the given training data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Training samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n+            True labels for X.\n+        batch_size : int, optional\n+            Number of training samples evaluated at a time.\n+        nb_epochs : int, optional\n+            Number of training epochs.\n+        verbose : int, optional\n+            Verbosity level.\n+        shuffle : boolean, optional\n+            Indicator to shuffle the training data.\n+\n+        Returns\n+        -------\n+        self : object\n+            Returns self.\n+        \"\"\"\n+        if len(y.shape) == 1:\n+            self.classes_ = list(np.unique(y))\n+        else:\n+            self.classes_ = np.arange(0, y.shape[1])\n+        self.model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, shuffle=shuffle)\n+        self.config_ = self.model.get_config()\n+        self.weights_ = self.model.get_weights()\n+        return self\n+\n+    def score(self, X, y, batch_size=128, verbose=0):\n+        \"\"\"\n+        Returns the mean accuracy on the given test data and labels.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Test samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n+            True labels for X.\n+        batch_size : int, optional\n+            Number of test samples evaluated at a time.\n+        verbose : int, optional\n+            Verbosity level.\n+\n+        Returns\n+        -------\n+        score : float\n+            Mean accuracy of self.predict(X) wrt. y.\n+        \"\"\"\n+        loss, accuracy = self.model.evaluate(X, y, batch_size=batch_size, show_accuracy=True, verbose=verbose)\n+        return accuracy\n+\n+    def predict(self, X, batch_size=128, verbose=0):\n+        \"\"\"\n+        Returns the class predictions for the given test data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Test samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        batch_size : int, optional\n+            Number of test samples evaluated at a time.\n+        verbose : int, optional\n+            Verbosity level.\n+\n+        Returns\n+        -------\n+        preds : array-like, shape = (n_samples)\n+            Class predictions.\n+        \"\"\"\n+        return self.model.predict_classes(X, batch_size=batch_size, verbose=verbose)\n+\n+    def predict_proba(self, X, batch_size=128, verbose=0):\n+        \"\"\"\n+        Returns class probability estimates for the given test data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Test samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        batch_size : int, optional\n+            Number of test samples evaluated at a time.\n+        verbose : int, optional\n+            Verbosity level.\n+\n+        Returns\n+        -------\n+        proba : array-like, shape = (n_samples, n_outputs)\n+            Class probability estimates.\n+        \"\"\"\n+        return self.model.predict_proba(X, batch_size=batch_size, verbose=verbose)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7df64ab2e89c4b600c1f671d0a386556dd7a582", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 35 | Churn Cumulative: 167 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,3 +17,6 @@ def nonneg(p):\n \n def identity(g):\n     return g\n+\n+def unitnorm(e):\n+    return e / T.sqrt(T.sum(e**2, axis=-1, keepdims=True))\n\n@@ -4,6 +4,7 @@ import theano.tensor as T\n \n from .. import activations, initializations\n from ..layers.core import Layer\n+from ..constraints import unitnorm\n \n \n class Embedding(Layer):\n@@ -14,7 +15,7 @@ class Embedding(Layer):\n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', weights=None):\n+    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None):\n         super(Embedding,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n@@ -23,6 +24,8 @@ class Embedding(Layer):\n         self.input = T.imatrix()\n         self.W = self.init((self.input_dim, self.output_dim))\n         self.params = [self.W]\n+        self.constraints = [W_constraint]\n+        self.regularizers = [W_regularizer]\n \n         if weights is not None:\n             self.set_weights(weights)\n\n@@ -0,0 +1,27 @@\n+import unittest\n+import numpy as np\n+from keras.models import Sequential\n+from keras.layers.core import Merge, Dense, Activation, Flatten\n+from keras.layers.embeddings import Embedding\n+from theano import function\n+from keras.constraints import unitnorm\n+\n+class TestConcatenation(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.X1 = np.array([[1], [2]], dtype='int32')\n+        self.W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n+\n+    def test_unitnorm_constraint(self):\n+        lookup = Sequential()\n+        lookup.add(Embedding(3, 2, weights=[self.W1], W_constraint=unitnorm))\n+        lookup.add(Flatten())\n+        lookup.add(Dense(2, 1))\n+        lookup.add(Activation('sigmoid'))\n+        lookup.compile(loss='binary_crossentropy', optimizer='sgd', class_mode='binary')\n+        lookup.train(self.X1, np.array([[1], [0]], dtype='int32'))\n+        norm = np.linalg.norm(lookup.params[0].get_value(), axis=1)\n+        self.assertTrue(np.allclose(norm, np.ones_like(norm).astype('float32')))\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1aaab7a228fdd6f3ef53e888e08bc2afd6b4213b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 3 | Churn Cumulative: 828 | Contributors (this commit): 14 | Commits (past 90d): 37 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,9 +71,6 @@ class Model(object):\n         self.y_train = self.get_output(train=True)\n         self.y_test = self.get_output(train=False)\n \n-        self.y_train = self.get_output(train=True)\n-        self.y_test = self.get_output(train=False)\n-\n         # target of model\n         self.y = T.zeros_like(self.y_train)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#755728dbb0e1d66a08fdf498273b1531ac93089d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 115 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ def skipgrams(sequence, vocabulary_size,\n         if not wi:\n             continue\n         if sampling_table is not None:\n-            if sampling_table[i] < random.random():\n+            if sampling_table[wi] < random.random():\n                 continue\n \n         window_start = max(0, i-window_size)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#241b7ed631c051f15e94b1a421d45f04f6099c20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 117 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ def make_sampling_table(size, sampling_factor=1e-5):\n         We assume that the word frequencies follow Zipf's law (s=1) to derive \n         a numerical approximation of frequency(rank):\n            frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))\n-        where gamma is the Euler–Mascheroni constant.\n+        where gamma is the Euler-Mascheroni constant.\n     '''\n     gamma = 0.577\n     rank = np.array(list(range(size)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#138ad116dc0b5d7700b945a1ec8e2d05bd91ef90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d8a8e2d77476f3c199639d30f543682a7d99bca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 39 | Churn Cumulative: 190 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,17 +1,27 @@\n from __future__ import absolute_import\n+import copy\n import numpy as np\n \n+from ..utils.np_utils import to_categorical\n+\n class KerasClassifier(object):\n     \"\"\"\n     Implementation of the scikit-learn classifier API for Keras.\n \n     Parameters\n     ----------\n-    model : object, optional\n-        A pre-compiled Keras model is required to use the scikit-learn wrapper.\n+    model : object\n+        An un-compiled Keras model object is required to use the scikit-learn wrapper.\n+    optimizer : string, optional\n+        Optimization method used by the model during compilation/training.\n+    loss : string, optional\n+        Loss function used by the model during compilation/training.\n     \"\"\"\n-    def __init__(self, model=None):\n+    def __init__(self, model, optimizer='adam', loss='categorical_crossentropy'):\n         self.model = model\n+        self.optimizer = optimizer\n+        self.loss = loss\n+        self.compiled_model_ = None\n         self.classes_ = []\n         self.config_ = []\n         self.weights_ = []\n@@ -31,7 +41,7 @@ class KerasClassifier(object):\n         params : dict\n             Dictionary of parameter names mapped to their values.\n         \"\"\"\n-        return {'model': self.model}\n+        return {'model': self.model, 'optimizer': self.optimizer, 'loss': self.loss}\n \n     def set_params(self, **params):\n         \"\"\"\n@@ -47,13 +57,17 @@ class KerasClassifier(object):\n         self\n         \"\"\"\n         for parameter, value in params.items():\n-            self.setattr(parameter, value)\n+            setattr(self, parameter, value)\n         return self\n \n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=0, shuffle=True):\n         \"\"\"\n         Fit the model according to the given training data.\n \n+        Makes a copy of the un-compiled model definition to use for\n+        compilation and fitting, leaving the original definition\n+        intact.\n+\n         Parameters\n         ----------\n         X : array-like, shape = (n_samples, n_features)\n@@ -77,11 +91,17 @@ class KerasClassifier(object):\n         \"\"\"\n         if len(y.shape) == 1:\n             self.classes_ = list(np.unique(y))\n+            if self.loss == 'categorical_crossentropy':\n+                y = to_categorical(y)\n         else:\n             self.classes_ = np.arange(0, y.shape[1])\n-        self.model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, shuffle=shuffle)\n+\n+        self.compiled_model_ = copy.deepcopy(self.model)\n+        self.compiled_model_.compile(optimizer=self.optimizer, loss=self.loss)\n+        self.compiled_model_.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, shuffle=shuffle)\n         self.config_ = self.model.get_config()\n         self.weights_ = self.model.get_weights()\n+\n         return self\n \n     def score(self, X, y, batch_size=128, verbose=0):\n@@ -105,7 +125,8 @@ class KerasClassifier(object):\n         score : float\n             Mean accuracy of self.predict(X) wrt. y.\n         \"\"\"\n-        loss, accuracy = self.model.evaluate(X, y, batch_size=batch_size, show_accuracy=True, verbose=verbose)\n+        loss, accuracy = self.compiled_model_.evaluate(X, y, batch_size=batch_size,\n+                                                       show_accuracy=True, verbose=verbose)\n         return accuracy\n \n     def predict(self, X, batch_size=128, verbose=0):\n@@ -127,7 +148,7 @@ class KerasClassifier(object):\n         preds : array-like, shape = (n_samples)\n             Class predictions.\n         \"\"\"\n-        return self.model.predict_classes(X, batch_size=batch_size, verbose=verbose)\n+        return self.compiled_model_.predict_classes(X, batch_size=batch_size, verbose=verbose)\n \n     def predict_proba(self, X, batch_size=128, verbose=0):\n         \"\"\"\n@@ -148,4 +169,4 @@ class KerasClassifier(object):\n         proba : array-like, shape = (n_samples, n_outputs)\n             Class probability estimates.\n         \"\"\"\n-        return self.model.predict_proba(X, batch_size=batch_size, verbose=verbose)\n+        return self.compiled_model_.predict_proba(X, batch_size=batch_size, verbose=verbose)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e2b9358d6741840f3b86adbaf4f5ac8a99ea9ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 96 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ class SimpleRNN(Layer):\n             self._step, # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n             sequences=x, # tensors to iterate over, inputs to _step\n             # initialization of the output. Input to _step with default tap=-1.\n-            outputs_info=alloc_zeros_matrix(X.shape[1], self.output_dim), \n+            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n             truncate_gradient=self.truncate_gradient\n         )\n@@ -233,7 +233,7 @@ class GRU(Layer):\n         outputs, updates = theano.scan(\n             self._step, \n             sequences=[x_z, x_r, x_h], \n-            outputs_info=alloc_zeros_matrix(X.shape[1], self.output_dim),\n+            outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=[self.U_z, self.U_r, self.U_h],\n             truncate_gradient=self.truncate_gradient\n         )\n@@ -346,8 +346,8 @@ class LSTM(Layer):\n             self._step, \n             sequences=[xi, xf, xo, xc],\n             outputs_info=[\n-                alloc_zeros_matrix(X.shape[1], self.output_dim), \n-                alloc_zeros_matrix(X.shape[1], self.output_dim)\n+                T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n+                T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n             ], \n             non_sequences=[self.U_i, self.U_f, self.U_o, self.U_c], \n             truncate_gradient=self.truncate_gradient \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#994260e3f0c7251ae77163b46f0ee21a1b85a3c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 62 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,6 @@ setup(name='Keras',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n       license='MIT',\n-      install_requires=['theano', 'h5py'],\n+      install_requires=['theano'],\n       packages=find_packages(),\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31d7374063469679e25698724bd94fefceb2b436", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 77 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,77 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.utils import np_utils\n+from keras.wrappers.scikit_learn import KerasClassifier\n+import numpy as np\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 1\n+\n+max_train_samples = 5000\n+max_test_samples = 1000\n+\n+np.random.seed(1337) # for reproducibility\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000,784)[:max_train_samples]\n+X_test = X_test.reshape(10000,784)[:max_test_samples]\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n+X_train /= 255\n+X_test /= 255\n+\n+Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n+Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n+\n+#############################\n+# scikit-learn wrapper test #\n+#############################\n+print('Beginning scikit-learn wrapper test')\n+\n+print('Defining model')\n+model = Sequential()\n+model.add(Dense(784, 50))\n+model.add(Activation('relu'))\n+model.add(Dense(50, 10))\n+model.add(Activation('softmax'))\n+\n+print('Creating wrapper')\n+classifier = KerasClassifier(model)\n+\n+print('Fitting model')\n+classifier.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+print('Testing score function')\n+score = classifier.score(X_train, Y_train)\n+print('Score: ', score)\n+\n+print('Testing predict function')\n+preds = classifier.predict(X_test)\n+print('Preds.shape: ', preds.shape)\n+\n+print('Testing predict proba function')\n+proba = classifier.predict_proba(X_test)\n+print('Proba.shape: ', proba.shape)\n+\n+print('Testing get params')\n+print(classifier.get_params())\n+\n+print('Testing set params')\n+classifier.set_params(optimizer='sgd', loss='mse')\n+print(classifier.get_params())\n+\n+print('Testing attributes')\n+print('Classes')\n+print(classifier.classes_)\n+print('Config')\n+print(classifier.config_)\n+print('Weights')\n+print(classifier.weights_)\n+\n+print('Test script complete.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7af67dd1d107eeb47ac01e5960de8dad86b6cf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 261 | Lines Deleted: 36 | Files Changed: 3 | Hunks: 17 | Methods Changed: 17 | Complexity Δ (Sum/Max): 46/49 | Churn Δ: 297 | Churn Cumulative: 1125 | Contributors (this commit): 15 | Commits (past 90d): 40 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,144 @@\n+from __future__ import absolute_import\n+import theano\n+import theano.tensor as T\n+import numpy as np\n+import warnings\n+\n+from .utils.generic_utils import Progbar\n+\n+class CallbackList(object):\n+    \"\"\"docstring for Callback\"\"\"\n+    def __init__(self, callbacks):\n+        self.callbacks = callbacks\n+\n+    def append(self, callback):\n+        self.callbacks.append(callback)\n+\n+    def _set_params(self, params):\n+        for callback in self.callbacks:\n+            callback._set_params(params)\n+\n+    def _set_model(self, model):\n+        for callback in self.callbacks:\n+            callback._set_model(model)\n+\n+    def on_epoch_begin(self, epoch):\n+        for callback in self.callbacks:\n+            callback.on_epoch_begin(epoch)\n+\n+    def on_epoch_end(self, epoch):\n+        for callback in self.callbacks:\n+            callback.on_epoch_end(epoch)\n+\n+    def on_batch_begin(self, batch):\n+        for callback in self.callbacks:\n+            callback.on_batch_begin(batch)\n+\n+    def on_batch_end(self, batch, indices, loss, accuracy, val_loss, val_acc):\n+        for callback in self.callbacks:\n+            callback.on_batch_end(batch, indices, loss, accuracy, val_loss, val_acc)\n+\n+    def on_train_begin(self):\n+        for callback in self.callbacks:\n+            callback.on_train_begin()\n+\n+    def on_train_end(self):\n+        for callback in self.callbacks:\n+            callback.on_train_end()\n+\n+\n+class Callback(object):\n+    \"\"\"docstring for Callback\"\"\"\n+    def __init__(self):\n+        pass\n+\n+    def _set_params(self, params):\n+        self.params = params\n+\n+    def _set_model(self, model):\n+        self.model = model\n+\n+    def on_epoch_begin(self, epoch):\n+        pass\n+\n+    def on_epoch_end(self, epoch):\n+        pass\n+\n+    def on_batch_begin(self, batch):\n+        pass\n+\n+    def on_batch_end(self, batch, indices, loss, accuracy, val_loss, val_acc):\n+        pass\n+\n+    def on_train_begin(self):\n+        pass\n+\n+    def on_train_end(self):\n+        pass\n+\n+class History(Callback):\n+\n+    def on_train_begin(self):\n+        self.epochs = []\n+        self.losses = []\n+        if self.params['show_accuracy']:\n+            self.accuracies = []\n+        if self.params['do_validation']:\n+            self.validation_losses = []\n+            if self.params['show_accuracy']:\n+                self.validation_accuracies = []\n+\n+    def on_epoch_begin(self, epoch):\n+        self.seen = 0\n+        self.cum_loss = 0.\n+        self.cum_accuracy = 0.\n+        self.val_loss = 0.\n+        self.val_accuracy = 0.\n+\n+    def on_batch_end(self, batch, indices, loss, accuracy, val_loss, val_acc):\n+        batch_length = len(indices)\n+        self.seen += batch_length\n+        self.cum_loss += loss * batch_length\n+        if self.params['show_accuracy']:\n+            self.cum_accuracy += accuracy * batch_length\n+        if self.params['do_validation']:\n+            self.val_loss = val_loss\n+            if self.params['show_accuracy']:\n+                self.val_accuracy = val_acc\n+\n+    def on_epoch_end(self, epoch):\n+        self.epochs.append(epoch)\n+        self.losses.append(self.cum_loss / self.seen)\n+        if self.params['show_accuracy']:\n+            self.accuracies.append(self.cum_accuracy / self.seen)\n+        if self.params['do_validation']:\n+            self.validation_losses.append(self.val_loss)\n+            if self.params['show_accuracy']:\n+                self.validation_accuracies.append(self.val_accuracy)\n+\n+class Logger(Callback):\n+\n+    def on_train_begin(self):\n+        self.verbose = self.params['verbose']\n+\n+    def on_epoch_begin(self, epoch):\n+        if self.verbose:\n+            print 'Epoch %d' % epoch\n+            self.progbar = Progbar(target=self.params['nb_sample'], \\\n+                verbose=self.verbose)\n+        self.current = 0\n+\n+    def on_batch_begin(self, batch):\n+        self.log_values = []\n+\n+    def on_batch_end(self, batch, indices, loss, accuracy, val_loss, val_acc):\n+        self.log_values.append(('loss', loss))\n+        self.current += len(indices)\n+        if self.params['show_accuracy']:\n+            self.log_values.append(('acc.', acc))\n+        if self.params['do_validation']:\n+            self.log_values.append(('val. loss', val_loss))\n+            if self.params['show_accuracy']:\n+                self.log_values.append(('val. acc.', val_acc))\n+        if self.verbose:\n+            self.progbar.update(self.current, self.log_values)\n\\ No newline at end of file\n\n@@ -9,6 +9,7 @@ from . import optimizers\n from . import objectives\n from . import regularizers\n from . import constraints\n+from . import callbacks as cbks\n import time, copy\n from .utils.generic_utils import Progbar, printv\n from six.moves import range\n@@ -182,7 +183,7 @@ class Sequential(Model):\n             return self._test(*ins)\n \n \n-    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1,\n+    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, show_accuracy=False):\n         \n         X = standardize_X(X)\n@@ -212,66 +213,59 @@ class Sequential(Model):\n                 if verbose:\n                     print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n \n-        history = {'epoch':[], 'loss':[]}\n-        if show_accuracy:\n-            history['acc'] = []\n-        if do_validation:\n-            history['val_loss'] = []\n-            if show_accuracy:\n-                history['val_acc'] = []\n-\n         index_array = np.arange(len(y))\n+\n+        callbacks = cbks.CallbackList(callbacks)\n+        callbacks.append(cbks.Logger())\n+\n+        callbacks._set_model(self)\n+        callbacks._set_params({\n+            'batch_size': batch_size,\n+            'nb_epoch': nb_epoch,\n+            'nb_sample': len(y),\n+            'verbose': verbose,\n+            'do_validation': do_validation,\n+            'show_accuracy': show_accuracy\n+        })\n+\n+        callbacks.on_train_begin()\n+\n         for epoch in range(nb_epoch):\n-            if verbose:\n-                print('Epoch', epoch)\n-                progbar = Progbar(target=len(y), verbose=verbose)\n+            callbacks.on_epoch_begin(epoch)\n             if shuffle:\n                 np.random.shuffle(index_array)\n \n-            av_loss = 0.\n-            av_acc = 0.\n-            seen = 0\n-\n             batches = make_batches(len(y), batch_size)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n+                callbacks.on_batch_begin(batch_index)\n+\n                 batch_ids = index_array[batch_start:batch_end]\n-                seen += len(batch_ids)\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n                 ins = X_batch + [y_batch]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n-                    log_values = [('loss', loss), ('acc.', acc)]\n-                    av_loss += loss * len(batch_ids)\n-                    av_acc += acc * len(batch_ids)\n                 else:\n                     loss = self._train(*ins)\n-                    log_values = [('loss', loss)]\n-                    av_loss += loss * len(batch_ids)\n+                    acc = None\n \n                 # validation\n+                val_loss, val_acc = None, None\n                 if do_validation and (batch_index == len(batches) - 1):\n                     if show_accuracy:\n                         val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0, show_accuracy=True)\n-                        log_values += [('val. loss', val_loss), ('val. acc.', val_acc)]\n                     else:\n                         val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n-                        log_values += [('val. loss', val_loss)]\n \n-                # logging\n-                if verbose:\n-                    progbar.update(batch_end, log_values)\n+                callbacks.on_batch_end(batch_index, batch_ids, loss, acc, val_loss, val_acc)\n \n-            history['epoch'].append(epoch)\n-            history['loss'].append(av_loss/seen)\n-            if do_validation:\n-                history['val_loss'].append(float(val_loss))\n-            if show_accuracy:\n-                history['acc'].append(av_acc/seen)\n-                if do_validation:\n-                    history['val_acc'].append(float(val_acc))\n-        return history\n+            callbacks.on_epoch_end(epoch)\n+\n+        callbacks.on_train_end()\n+\n+        # return history\n+        return True\n \n     def predict(self, X, batch_size=128, verbose=1):\n         X = standardize_X(X)\n\n@@ -0,0 +1,87 @@\n+import numpy as np\n+import random\n+import theano\n+\n+from keras.models import Sequential\n+from keras.callbacks import Callback\n+from keras.layers.core import Dense, Dropout, Activation, Flatten\n+from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.utils import np_utils\n+from keras.datasets import mnist\n+\n+from matplotlib import pyplot as plt\n+from matplotlib import animation\n+\n+nb_classes = 10\n+batch_size = 128\n+nb_epoch = 1\n+\n+max_train_samples = 5000\n+max_test_samples = 1\n+\n+np.random.seed(1337)\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(-1,1,28,28)[:max_train_samples]\n+X_train = X_train.astype(\"float32\")\n+X_train /= 255\n+\n+X_test = X_test.reshape(-1,1,28,28)[:max_test_samples]\n+X_test = X_test.astype(\"float32\")\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n+\n+class DrawWeight(Callback):\n+    def __init__(self):\n+        self.fig = plt.figure()\n+        self.ax = self.fig.add_subplot(1, 1, 1)\n+\n+    def on_train_begin(self):\n+        self.imgs = []\n+        # self.test = theano.function([self.model.get_input()], self.model.layers[1].get_output(train=False))\n+        self.test = theano.function([self.model.get_input()], self.model.layers[10].get_output(train=False))\n+\n+    def on_batch_end(self, batch, indices, loss, accuracy, val_loss, val_acc):\n+        if batch % 1 == 0:\n+            # img = self.ax.imshow(self.test(X_test)[0,0,:,:], interpolation='nearest')\n+            img = self.ax.imshow(self.test(X_test)[0,:].reshape(16,16), interpolation='nearest')\n+            self.imgs.append([img])\n+\n+    def on_train_end(self):\n+        anim = animation.ArtistAnimation(self.fig, self.imgs, interval=10, blit=False, repeat_delay=1000)\n+        plt.show()\n+\n+# model = Sequential()\n+# model.add(Dense(784, 50))\n+# model.add(Activation('relu'))\n+# model.add(Dense(50, 10))\n+# model.add(Activation('softmax'))\n+\n+model = Sequential()\n+model.add(Convolution2D(32, 1, 3, 3, border_mode='full')) \n+model.add(Activation('relu'))\n+model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(Dropout(0.25))\n+\n+model.add(Convolution2D(64, 32, 3, 3, border_mode='full')) \n+model.add(Activation('relu'))\n+model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(Dropout(0.25))\n+\n+model.add(Flatten())\n+model.add(Dense(64*8*8, 256))\n+model.add(Activation('relu'))\n+model.add(Dropout(0.5))\n+\n+model.add(Dense(256, 10))\n+model.add(Activation('softmax'))\n+\n+model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+# Fit the model\n+draw_weights = DrawWeight()\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, callbacks=[draw_weights])\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
