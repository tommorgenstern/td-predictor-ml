{"custom_id": "scrapy#e3f81d8d5f17515b6eba135ac0db7e270ff0a9f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 2726 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -161,7 +161,7 @@ class ExecutionEngine:\n             return None\n \n         d = self._download(request, self.spider)\n-        d.addBoth(self._handle_downloader_output, request, self.spider)\n+        d.addBoth(self._handle_downloader_output, request)\n         d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': self.spider}))\n@@ -176,8 +176,10 @@ class ExecutionEngine:\n         return d\n \n     def _handle_downloader_output(\n-        self, result: Union[Request, Response, Failure], request: Request, spider: Spider\n+        self, result: Union[Request, Response, Failure], request: Request\n     ) -> Optional[Deferred]:\n+        assert self.spider is not None  # typing\n+\n         if not isinstance(result, (Request, Response, Failure)):\n             raise TypeError(f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\")\n \n@@ -186,12 +188,12 @@ class ExecutionEngine:\n             self.crawl(result)\n             return None\n \n-        d = self.scraper.enqueue_scrape(result, request, spider)\n+        d = self.scraper.enqueue_scrape(result, request, self.spider)\n         d.addErrback(\n             lambda f: logger.error(\n                 \"Error while enqueuing downloader output\",\n                 exc_info=failure_to_exc_info(f),\n-                extra={'spider': spider},\n+                extra={'spider': self.spider},\n             )\n         )\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e779ed7d93beec36f565d33a1cc8d3e8fe6068d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 20 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 43 | Churn Cumulative: 258 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: 0.06060606060606061\n\nDIFF:\n@@ -1,35 +1,47 @@\n-import os\n import logging\n+import os\n+from typing import Optional, Set, Type, TypeVar\n \n+from twisted.internet.defer import Deferred\n+\n+from scrapy.http.request import Request\n+from scrapy.settings import BaseSettings\n+from scrapy.spiders import Spider\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import referer_str, request_fingerprint\n \n \n-class BaseDupeFilter:\n+BaseDupeFilterTV = TypeVar(\"BaseDupeFilterTV\", bound=\"BaseDupeFilter\")\n \n+\n+class BaseDupeFilter:\n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_settings(cls: Type[BaseDupeFilterTV], settings: BaseSettings) -> BaseDupeFilterTV:\n         return cls()\n \n-    def request_seen(self, request):\n+    def request_seen(self, request: Request) -> bool:\n         return False\n \n-    def open(self):  # can return deferred\n+    def open(self) -> Optional[Deferred]:\n         pass\n \n-    def close(self, reason):  # can return a deferred\n+    def close(self, reason: str) -> Optional[Deferred]:\n         pass\n \n-    def log(self, request, spider):  # log that a request has been filtered\n+    def log(self, request: Request, spider: Spider) -> None:\n+        \"\"\"Log that a request has been filtered\"\"\"\n         pass\n \n \n+RFPDupeFilterTV = TypeVar(\"RFPDupeFilterTV\", bound=\"RFPDupeFilter\")\n+\n+\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n-    def __init__(self, path=None, debug=False):\n+    def __init__(self, path: Optional[str] = None, debug: bool = False) -> None:\n         self.file = None\n-        self.fingerprints = set()\n+        self.fingerprints: Set[str] = set()\n         self.logdupes = True\n         self.debug = debug\n         self.logger = logging.getLogger(__name__)\n@@ -39,26 +51,27 @@ class RFPDupeFilter(BaseDupeFilter):\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_settings(cls: Type[RFPDupeFilterTV], settings: BaseSettings) -> RFPDupeFilterTV:\n         debug = settings.getbool('DUPEFILTER_DEBUG')\n         return cls(job_dir(settings), debug)\n \n-    def request_seen(self, request):\n+    def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\n             return True\n         self.fingerprints.add(fp)\n         if self.file:\n             self.file.write(fp + '\\n')\n+        return False\n \n-    def request_fingerprint(self, request):\n+    def request_fingerprint(self, request: Request) -> str:\n         return request_fingerprint(request)\n \n-    def close(self, reason):\n+    def close(self, reason: str) -> None:\n         if self.file:\n             self.file.close()\n \n-    def log(self, request, spider):\n+    def log(self, request: Request, spider: Spider) -> None:\n         if self.debug:\n             msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n             args = {'request': request, 'referer': referer_str(request)}\n\n@@ -24,7 +24,7 @@ def request_fingerprint(\n     request: Request,\n     include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n     keep_fragments: bool = False,\n-):\n+) -> str:\n     \"\"\"\n     Return the request fingerprint.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68379197986ae3deb81a545b5fd6920ea3347094", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 442 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 20 | Methods Changed: 33 | Complexity Δ (Sum/Max): 48/23 | Churn Δ: 468 | Churn Cumulative: 1641 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import logging\n \n from scrapy.utils.misc import create_instance\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -17,8 +18,7 @@ def _path_safe(text):\n     >>> _path_safe('some@symbol?').startswith('some_symbol_')\n     True\n     \"\"\"\n-    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n-                             for c in text])\n+    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_' for c in text])\n     # as we replace some letters we can get collision for different slots\n     # add we add unique part\n     unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()\n@@ -35,6 +35,9 @@ class ScrapyPriorityQueue:\n         * close()\n         * __len__()\n \n+    Optionally, the queue could provide a ``peek`` method, that should return the\n+    next object to be returned by ``pop``, but without removing it from the queue.\n+\n     ``__init__`` method of ScrapyPriorityQueue receives a downstream_queue_cls\n     argument, which is a class used to instantiate a new (internal) queue when\n     a new priority is allocated.\n@@ -70,10 +73,12 @@ class ScrapyPriorityQueue:\n         self.curprio = min(startprios)\n \n     def qfactory(self, key):\n-        return create_instance(self.downstream_queue_cls,\n+        return create_instance(\n+            self.downstream_queue_cls,\n             None,\n             self.crawler,\n-                               self.key + '/' + str(key))\n+            self.key + '/' + str(key),\n+        )\n \n     def priority(self, request):\n         return -request.priority\n@@ -99,6 +104,18 @@ class ScrapyPriorityQueue:\n             self.curprio = min(prios) if prios else None\n         return m\n \n+    def peek(self):\n+        \"\"\"Returns the next object to be returned by :meth:`pop`,\n+        but without removing it from the queue.\n+\n+        Raises :exc:`NotImplementedError` if the underlying queue class does\n+        not implement a ``peek`` method, which is optional for queues.\n+        \"\"\"\n+        if self.curprio is None:\n+            return None\n+        queue = self.queues[self.curprio]\n+        return queue.peek()\n+\n     def close(self):\n         active = []\n         for p, q in self.queues.items():\n@@ -116,8 +133,7 @@ class DownloaderInterface:\n         self.downloader = crawler.engine.downloader\n \n     def stats(self, possible_slots):\n-        return [(self._active_downloads(slot), slot)\n-                for slot in possible_slots]\n+        return [(self._active_downloads(slot), slot) for slot in possible_slots]\n \n     def get_slot_key(self, request):\n         return self.downloader._get_slot_key(request, None)\n@@ -162,10 +178,12 @@ class DownloaderAwarePriorityQueue:\n             self.pqueues[slot] = self.pqfactory(slot, startprios)\n \n     def pqfactory(self, slot, startprios=()):\n-        return ScrapyPriorityQueue(self.crawler,\n+        return ScrapyPriorityQueue(\n+            self.crawler,\n             self.downstream_queue_cls,\n             self.key + '/' + _path_safe(slot),\n-                                   startprios)\n+            startprios,\n+        )\n \n     def pop(self):\n         stats = self._downloader_interface.stats(self.pqueues)\n@@ -187,9 +205,22 @@ class DownloaderAwarePriorityQueue:\n         queue = self.pqueues[slot]\n         queue.push(request)\n \n+    def peek(self):\n+        \"\"\"Returns the next object to be returned by :meth:`pop`,\n+        but without removing it from the queue.\n+\n+        Raises :exc:`NotImplementedError` if the underlying queue class does\n+        not implement a ``peek`` method, which is optional for queues.\n+        \"\"\"\n+        stats = self._downloader_interface.stats(self.pqueues)\n+        if not stats:\n+            return None\n+        slot = min(stats)[1]\n+        queue = self.pqueues[slot]\n+        return queue.peek()\n+\n     def close(self):\n-        active = {slot: queue.close()\n-                  for slot, queue in self.pqueues.items()}\n+        active = {slot: queue.close() for slot, queue in self.pqueues.items()}\n         self.pqueues.clear()\n         return active\n \n\n@@ -19,7 +19,6 @@ def _with_mkdir(queue_class):\n             dirname = os.path.dirname(path)\n             if not os.path.exists(dirname):\n                 os.makedirs(dirname, exist_ok=True)\n-\n             super().__init__(path, *args, **kwargs)\n \n     return DirectoriesCreated\n@@ -38,6 +37,20 @@ def _serializable_queue(queue_class, serialize, deserialize):\n             if s:\n                 return deserialize(s)\n \n+        def peek(self):\n+            \"\"\"Returns the next object to be returned by :meth:`pop`,\n+            but without removing it from the queue.\n+\n+            Raises :exc:`NotImplementedError` if the underlying queue class does\n+            not implement a ``peek`` method, which is optional for queues.\n+            \"\"\"\n+            try:\n+                s = super().peek()\n+            except AttributeError as ex:\n+                raise NotImplementedError(\"The underlying queue class does not implement 'peek'\") from ex\n+            if s:\n+                return deserialize(s)\n+\n     return SerializableQueue\n \n \n@@ -59,12 +72,21 @@ def _scrapy_serialization_queue(queue_class):\n \n         def pop(self):\n             request = super().pop()\n-\n             if not request:\n                 return None\n+            return request_from_dict(request, self.spider)\n \n-            request = request_from_dict(request, self.spider)\n-            return request\n+        def peek(self):\n+            \"\"\"Returns the next object to be returned by :meth:`pop`,\n+            but without removing it from the queue.\n+\n+            Raises :exc:`NotImplementedError` if the underlying queue class does\n+            not implement a ``peek`` method, which is optional for queues.\n+            \"\"\"\n+            request = super().peek()\n+            if not request:\n+                return None\n+            return request_from_dict(request, self.spider)\n \n     return ScrapyRequestQueue\n \n@@ -76,6 +98,19 @@ def _scrapy_non_serialization_queue(queue_class):\n         def from_crawler(cls, crawler, *args, **kwargs):\n             return cls()\n \n+        def peek(self):\n+            \"\"\"Returns the next object to be returned by :meth:`pop`,\n+            but without removing it from the queue.\n+\n+            Raises :exc:`NotImplementedError` if the underlying queue class does\n+            not implement a ``peek`` method, which is optional for queues.\n+            \"\"\"\n+            try:\n+                s = super().peek()\n+            except AttributeError as ex:\n+                raise NotImplementedError(\"The underlying queue class does not implement 'peek'\") from ex\n+            return s\n+\n     return ScrapyRequestQueue\n \n \n@@ -109,17 +144,9 @@ MarshalLifoDiskQueueNonRequest = _serializable_queue(\n     marshal.loads\n )\n \n-PickleFifoDiskQueue = _scrapy_serialization_queue(\n-    PickleFifoDiskQueueNonRequest\n-)\n-PickleLifoDiskQueue = _scrapy_serialization_queue(\n-    PickleLifoDiskQueueNonRequest\n-)\n-MarshalFifoDiskQueue = _scrapy_serialization_queue(\n-    MarshalFifoDiskQueueNonRequest\n-)\n-MarshalLifoDiskQueue = _scrapy_serialization_queue(\n-    MarshalLifoDiskQueueNonRequest\n-)\n+PickleFifoDiskQueue = _scrapy_serialization_queue(PickleFifoDiskQueueNonRequest)\n+PickleLifoDiskQueue = _scrapy_serialization_queue(PickleLifoDiskQueueNonRequest)\n+MarshalFifoDiskQueue = _scrapy_serialization_queue(MarshalFifoDiskQueueNonRequest)\n+MarshalLifoDiskQueue = _scrapy_serialization_queue(MarshalLifoDiskQueueNonRequest)\n FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)\n LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)\n\n@@ -0,0 +1,144 @@\n+import tempfile\n+import unittest\n+\n+import queuelib\n+\n+from scrapy.http.request import Request\n+from scrapy.pqueues import ScrapyPriorityQueue, DownloaderAwarePriorityQueue\n+from scrapy.spiders import Spider\n+from scrapy.squeues import FifoMemoryQueue\n+from scrapy.utils.test import get_crawler\n+\n+from tests.test_scheduler import MockDownloader, MockEngine\n+\n+\n+class PriorityQueueTest(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider(\"foo\")\n+\n+    def test_queue_push_pop_one(self):\n+        temp_dir = tempfile.mkdtemp()\n+        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)\n+        self.assertIsNone(queue.pop())\n+        self.assertEqual(len(queue), 0)\n+        req1 = Request(\"https://example.org/1\", priority=1)\n+        queue.push(req1)\n+        self.assertEqual(len(queue), 1)\n+        dequeued = queue.pop()\n+        self.assertEqual(len(queue), 0)\n+        self.assertEqual(dequeued.url, req1.url)\n+        self.assertEqual(dequeued.priority, req1.priority)\n+        self.assertEqual(queue.close(), [])\n+\n+    def test_no_peek_raises(self):\n+        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n+        temp_dir = tempfile.mkdtemp()\n+        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)\n+        queue.push(Request(\"https://example.org\"))\n+        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+            queue.peek()\n+        queue.close()\n+\n+    def test_peek(self):\n+        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n+        temp_dir = tempfile.mkdtemp()\n+        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)\n+        self.assertEqual(len(queue), 0)\n+        self.assertIsNone(queue.peek())\n+        req1 = Request(\"https://example.org/1\")\n+        req2 = Request(\"https://example.org/2\")\n+        req3 = Request(\"https://example.org/3\")\n+        queue.push(req1)\n+        queue.push(req2)\n+        queue.push(req3)\n+        self.assertEqual(len(queue), 3)\n+        self.assertEqual(queue.peek().url, req1.url)\n+        self.assertEqual(queue.pop().url, req1.url)\n+        self.assertEqual(len(queue), 2)\n+        self.assertEqual(queue.peek().url, req2.url)\n+        self.assertEqual(queue.pop().url, req2.url)\n+        self.assertEqual(len(queue), 1)\n+        self.assertEqual(queue.peek().url, req3.url)\n+        self.assertEqual(queue.pop().url, req3.url)\n+        self.assertEqual(queue.close(), [])\n+\n+    def test_queue_push_pop_priorities(self):\n+        temp_dir = tempfile.mkdtemp()\n+        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3])\n+        self.assertIsNone(queue.pop())\n+        self.assertEqual(len(queue), 0)\n+        req1 = Request(\"https://example.org/1\", priority=1)\n+        req2 = Request(\"https://example.org/2\", priority=2)\n+        req3 = Request(\"https://example.org/3\", priority=3)\n+        queue.push(req1)\n+        queue.push(req2)\n+        queue.push(req3)\n+        self.assertEqual(len(queue), 3)\n+        dequeued = queue.pop()\n+        self.assertEqual(len(queue), 2)\n+        self.assertEqual(dequeued.url, req3.url)\n+        self.assertEqual(dequeued.priority, req3.priority)\n+        self.assertEqual(queue.close(), [-1, -2])\n+\n+\n+class DownloaderAwarePriorityQueueTest(unittest.TestCase):\n+    def setUp(self):\n+        crawler = get_crawler(Spider)\n+        crawler.engine = MockEngine(downloader=MockDownloader())\n+        self.queue = DownloaderAwarePriorityQueue.from_crawler(\n+            crawler=crawler,\n+            downstream_queue_cls=FifoMemoryQueue,\n+            key=\"foo/bar\",\n+        )\n+\n+    def tearDown(self):\n+        self.queue.close()\n+\n+    def test_push_pop(self):\n+        self.assertEqual(len(self.queue), 0)\n+        self.assertIsNone(self.queue.pop())\n+        req1 = Request(\"http://www.example.com/1\")\n+        req2 = Request(\"http://www.example.com/2\")\n+        req3 = Request(\"http://www.example.com/3\")\n+        self.queue.push(req1)\n+        self.queue.push(req2)\n+        self.queue.push(req3)\n+        self.assertEqual(len(self.queue), 3)\n+        self.assertEqual(self.queue.pop().url, req1.url)\n+        self.assertEqual(len(self.queue), 2)\n+        self.assertEqual(self.queue.pop().url, req2.url)\n+        self.assertEqual(len(self.queue), 1)\n+        self.assertEqual(self.queue.pop().url, req3.url)\n+        self.assertEqual(len(self.queue), 0)\n+        self.assertIsNone(self.queue.pop())\n+\n+    def test_no_peek_raises(self):\n+        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n+        self.queue.push(Request(\"https://example.org\"))\n+        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+            self.queue.peek()\n+\n+    def test_peek(self):\n+        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n+        self.assertEqual(len(self.queue), 0)\n+        req1 = Request(\"https://example.org/1\")\n+        req2 = Request(\"https://example.org/2\")\n+        req3 = Request(\"https://example.org/3\")\n+        self.queue.push(req1)\n+        self.queue.push(req2)\n+        self.queue.push(req3)\n+        self.assertEqual(len(self.queue), 3)\n+        self.assertEqual(self.queue.peek().url, req1.url)\n+        self.assertEqual(self.queue.pop().url, req1.url)\n+        self.assertEqual(len(self.queue), 2)\n+        self.assertEqual(self.queue.peek().url, req2.url)\n+        self.assertEqual(self.queue.pop().url, req2.url)\n+        self.assertEqual(len(self.queue), 1)\n+        self.assertEqual(self.queue.peek().url, req3.url)\n+        self.assertEqual(self.queue.pop().url, req3.url)\n+        self.assertIsNone(self.queue.peek())\n\n@@ -0,0 +1,214 @@\n+import shutil\n+import tempfile\n+import unittest\n+\n+import queuelib\n+\n+from scrapy.squeues import (\n+    PickleFifoDiskQueue,\n+    PickleLifoDiskQueue,\n+    MarshalFifoDiskQueue,\n+    MarshalLifoDiskQueue,\n+    FifoMemoryQueue,\n+    LifoMemoryQueue,\n+)\n+from scrapy.http import Request\n+from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n+\n+\n+\"\"\"\n+Queues that handle requests\n+\"\"\"\n+\n+\n+class BaseQueueTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tmpdir = tempfile.mkdtemp(prefix=\"scrapy-queue-tests-\")\n+        self.qpath = self.tempfilename()\n+        self.qdir = self.mkdtemp()\n+        self.crawler = get_crawler(Spider)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdir)\n+\n+    def tempfilename(self):\n+        with tempfile.NamedTemporaryFile(dir=self.tmpdir) as nf:\n+            return nf.name\n+\n+    def mkdtemp(self):\n+        return tempfile.mkdtemp(dir=self.tmpdir)\n+\n+\n+class RequestQueueTestMixin:\n+    def queue(self):\n+        raise NotImplementedError()\n+\n+    def test_one_element_with_peek(self):\n+        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.peek())\n+        self.assertIsNone(q.pop())\n+        req = Request(\"http://www.example.com\")\n+        q.push(req)\n+        self.assertEqual(len(q), 1)\n+        self.assertEqual(q.peek().url, req.url)\n+        self.assertEqual(q.pop().url, req.url)\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.peek())\n+        self.assertIsNone(q.pop())\n+        q.close()\n+\n+    def test_one_element_without_peek(self):\n+        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"The queuelib queues define peek\")\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.pop())\n+        req = Request(\"http://www.example.com\")\n+        q.push(req)\n+        self.assertEqual(len(q), 1)\n+        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+            q.peek()\n+        self.assertEqual(q.pop().url, req.url)\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.pop())\n+        q.close()\n+\n+\n+class FifoQueueMixin(RequestQueueTestMixin):\n+    def test_fifo_with_peek(self):\n+        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.peek())\n+        self.assertIsNone(q.pop())\n+        req1 = Request(\"http://www.example.com/1\")\n+        req2 = Request(\"http://www.example.com/2\")\n+        req3 = Request(\"http://www.example.com/3\")\n+        q.push(req1)\n+        q.push(req2)\n+        q.push(req3)\n+        self.assertEqual(len(q), 3)\n+        self.assertEqual(q.peek().url, req1.url)\n+        self.assertEqual(q.pop().url, req1.url)\n+        self.assertEqual(len(q), 2)\n+        self.assertEqual(q.peek().url, req2.url)\n+        self.assertEqual(q.pop().url, req2.url)\n+        self.assertEqual(len(q), 1)\n+        self.assertEqual(q.peek().url, req3.url)\n+        self.assertEqual(q.pop().url, req3.url)\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.peek())\n+        self.assertIsNone(q.pop())\n+        q.close()\n+\n+    def test_fifo_without_peek(self):\n+        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.pop())\n+        req1 = Request(\"http://www.example.com/1\")\n+        req2 = Request(\"http://www.example.com/2\")\n+        req3 = Request(\"http://www.example.com/3\")\n+        q.push(req1)\n+        q.push(req2)\n+        q.push(req3)\n+        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+            q.peek()\n+        self.assertEqual(len(q), 3)\n+        self.assertEqual(q.pop().url, req1.url)\n+        self.assertEqual(len(q), 2)\n+        self.assertEqual(q.pop().url, req2.url)\n+        self.assertEqual(len(q), 1)\n+        self.assertEqual(q.pop().url, req3.url)\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.pop())\n+        q.close()\n+\n+\n+class LifoQueueMixin(RequestQueueTestMixin):\n+    def test_lifo_with_peek(self):\n+        if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.peek())\n+        self.assertIsNone(q.pop())\n+        req1 = Request(\"http://www.example.com/1\")\n+        req2 = Request(\"http://www.example.com/2\")\n+        req3 = Request(\"http://www.example.com/3\")\n+        q.push(req1)\n+        q.push(req2)\n+        q.push(req3)\n+        self.assertEqual(len(q), 3)\n+        self.assertEqual(q.peek().url, req3.url)\n+        self.assertEqual(q.pop().url, req3.url)\n+        self.assertEqual(len(q), 2)\n+        self.assertEqual(q.peek().url, req2.url)\n+        self.assertEqual(q.pop().url, req2.url)\n+        self.assertEqual(len(q), 1)\n+        self.assertEqual(q.peek().url, req1.url)\n+        self.assertEqual(q.pop().url, req1.url)\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.peek())\n+        self.assertIsNone(q.pop())\n+        q.close()\n+\n+    def test_lifo_without_peek(self):\n+        if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n+            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+        q = self.queue()\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.pop())\n+        req1 = Request(\"http://www.example.com/1\")\n+        req2 = Request(\"http://www.example.com/2\")\n+        req3 = Request(\"http://www.example.com/3\")\n+        q.push(req1)\n+        q.push(req2)\n+        q.push(req3)\n+        with self.assertRaises(NotImplementedError, msg=\"The underlying queue class does not implement 'peek'\"):\n+            q.peek()\n+        self.assertEqual(len(q), 3)\n+        self.assertEqual(q.pop().url, req3.url)\n+        self.assertEqual(len(q), 2)\n+        self.assertEqual(q.pop().url, req2.url)\n+        self.assertEqual(len(q), 1)\n+        self.assertEqual(q.pop().url, req1.url)\n+        self.assertEqual(len(q), 0)\n+        self.assertIsNone(q.pop())\n+        q.close()\n+\n+\n+class PickleFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n+    def queue(self):\n+        return PickleFifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/fifo\")\n+\n+\n+class PickleLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n+    def queue(self):\n+        return PickleLifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/lifo\")\n+\n+\n+class MarshalFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n+    def queue(self):\n+        return MarshalFifoDiskQueue.from_crawler(crawler=self.crawler, key=\"marshal/fifo\")\n+\n+\n+class MarshalLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n+    def queue(self):\n+        return MarshalLifoDiskQueue.from_crawler(crawler=self.crawler, key=\"marshal/lifo\")\n+\n+\n+class FifoMemoryQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):\n+    def queue(self):\n+        return FifoMemoryQueue.from_crawler(crawler=self.crawler)\n+\n+\n+class LifoMemoryQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):\n+    def queue(self):\n+        return LifoMemoryQueue.from_crawler(crawler=self.crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddea6b7bfa38bf5402d78350ab61e2c827ca49b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 413 | Lines Deleted: 66 | Files Changed: 4 | Hunks: 39 | Methods Changed: 49 | Complexity Δ (Sum/Max): 43/26 | Churn Δ: 479 | Churn Cumulative: 3802 | Contributors (this commit): 27 | Commits (past 90d): 6 | Contributors (cumulative): 42 | DMM Complexity: 0.97\n\nDIFF:\n@@ -17,6 +17,7 @@ from scrapy import signals\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Response, Request\n+from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n@@ -73,12 +74,22 @@ class ExecutionEngine:\n         self.spider: Optional[Spider] = None\n         self.running = False\n         self.paused = False\n-        self.scheduler_cls = load_object(crawler.settings[\"SCHEDULER\"])\n+        self.scheduler_cls = self._get_scheduler_class(crawler.settings)\n         downloader_cls = load_object(self.settings['DOWNLOADER'])\n         self.downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n         self._spider_closed_callback = spider_closed_callback\n \n+    def _get_scheduler_class(self, settings: BaseSettings) -> type:\n+        from scrapy.core.scheduler import BaseScheduler\n+        scheduler_cls = load_object(settings[\"SCHEDULER\"])\n+        if not issubclass(scheduler_cls, BaseScheduler):\n+            raise TypeError(\n+                f\"The provided scheduler class ({settings['SCHEDULER']})\"\n+                \" does not fully implement the scheduler interface\"\n+            )\n+        return scheduler_cls\n+\n     @inlineCallbacks\n     def start(self) -> Deferred:\n         if self.running:\n@@ -301,6 +312,7 @@ class ExecutionEngine:\n         start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n         self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.spider = spider\n+        if hasattr(scheduler, \"open\"):\n             yield scheduler.open(spider)\n         yield self.scraper.open_spider(spider)\n         self.crawler.stats.open_spider(spider)\n@@ -345,8 +357,9 @@ class ExecutionEngine:\n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n         dfd.addErrback(log_failure('Scraper close failure'))\n \n+        if hasattr(self.slot.scheduler, \"close\"):\n             dfd.addBoth(lambda _: self.slot.scheduler.close(reason))\n-        dfd.addErrback(log_failure('Scheduler close failure'))\n+            dfd.addErrback(log_failure(\"Scheduler close failure\"))\n \n         dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n             signal=signals.spider_closed, spider=spider, reason=reason,\n\n@@ -1,42 +1,179 @@\n-import os\n import json\n import logging\n-from os.path import join, exists\n+import os\n+from abc import abstractmethod\n+from os.path import exists, join\n+from typing import Optional, Type, TypeVar\n \n-from scrapy.utils.misc import load_object, create_instance\n+from twisted.internet.defer import Deferred\n+\n+from scrapy.crawler import Crawler\n+from scrapy.http.request import Request\n+from scrapy.spiders import Spider\n from scrapy.utils.job import job_dir\n+from scrapy.utils.misc import create_instance, load_object\n \n \n logger = logging.getLogger(__name__)\n \n \n-class Scheduler:\n+class BaseSchedulerMeta(type):\n     \"\"\"\n-    Scrapy Scheduler. It allows to enqueue requests and then get\n-    a next request to download. Scheduler is also handling duplication\n-    filtering, via dupefilter.\n-\n-    Prioritization and queueing is not performed by the Scheduler.\n-    User sets ``priority`` field for each Request, and a PriorityQueue\n-    (defined by :setting:`SCHEDULER_PRIORITY_QUEUE`) uses these priorities\n-    to dequeue requests in a desired order.\n-\n-    Scheduler uses two PriorityQueue instances, configured to work in-memory\n-    and on-disk (optional). When on-disk queue is present, it is used by\n-    default, and an in-memory queue is used as a fallback for cases where\n-    a disk queue can't handle a request (can't serialize it).\n-\n-    :setting:`SCHEDULER_MEMORY_QUEUE` and\n-    :setting:`SCHEDULER_DISK_QUEUE` allow to specify lower-level queue classes\n-    which PriorityQueue instances would be instantiated with, to keep requests\n-    on disk and in memory respectively.\n-\n-    Overall, Scheduler is an object which holds several PriorityQueue instances\n-    (in-memory and on-disk) and implements fallback logic for them.\n-    Also, it handles dupefilters.\n+    Metaclass to check scheduler classes against the necessary interface\n     \"\"\"\n-    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,\n-                 logunser=False, stats=None, pqclass=None, crawler=None):\n+    def __instancecheck__(cls, instance):\n+        return cls.__subclasscheck__(type(instance))\n+\n+    def __subclasscheck__(cls, subclass):\n+        return (\n+            hasattr(subclass, \"has_pending_requests\") and callable(subclass.has_pending_requests)\n+            and hasattr(subclass, \"enqueue_request\") and callable(subclass.enqueue_request)\n+            and hasattr(subclass, \"next_request\") and callable(subclass.next_request)\n+        )\n+\n+\n+class BaseScheduler(metaclass=BaseSchedulerMeta):\n+    \"\"\"\n+    The scheduler component is responsible for storing requests received from\n+    the engine, and feeding them back upon request (also to the engine).\n+\n+    The original sources of said requests are:\n+\n+    * Spider: ``start_requests`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n+    * Spider middleware: ``process_spider_output`` and ``process_spider_exception`` methods\n+    * Downloader middleware: ``process_request``, ``process_response`` and ``process_exception`` methods\n+\n+    The order in which the scheduler returns its stored requests (via the ``next_request`` method)\n+    plays a great part in determining the order in which those requests are downloaded.\n+\n+    The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.\n+    \"\"\"\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler):\n+        \"\"\"\n+        Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.\n+        \"\"\"\n+        return cls()\n+\n+    def open(self, spider: Spider) -> Optional[Deferred]:\n+        \"\"\"\n+        Called when the spider is opened by the engine. It receives the spider\n+        instance as argument and it's useful to execute initialization code.\n+\n+        :param spider: the spider object for the current crawl\n+        :type spider: :class:`~scrapy.spiders.Spider`\n+        \"\"\"\n+        pass\n+\n+    def close(self, reason: str) -> Optional[Deferred]:\n+        \"\"\"\n+        Called when the spider is closed by the engine. It receives the reason why the crawl\n+        finished as argument and it's useful to execute cleaning code.\n+\n+        :param reason: a string which describes the reason why the spider was closed\n+        :type reason: :class:`str`\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def has_pending_requests(self) -> bool:\n+        \"\"\"\n+        ``True`` if the scheduler has enqueued requests, ``False`` otherwise\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    @abstractmethod\n+    def enqueue_request(self, request: Request) -> bool:\n+        \"\"\"\n+        Process a request received by the engine.\n+\n+        Return ``True`` if the request is stored correctly, ``False`` otherwise.\n+\n+        If ``False``, the engine will fire a ``request_dropped`` signal, and\n+        will not make further attempts to schedule the request at a later time.\n+        For reference, the default Scrapy scheduler returns ``False`` when the\n+        request is rejected by the dupefilter.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    @abstractmethod\n+    def next_request(self) -> Optional[Request]:\n+        \"\"\"\n+        Return the next :class:`~scrapy.http.Request` to be processed, or ``None``\n+        to indicate that there are no requests to be considered ready at the moment.\n+\n+        Returning ``None`` implies that no request from the scheduler will be sent\n+        to the downloader in the current reactor cycle. The engine will continue\n+        calling ``next_request`` until ``has_pending_requests`` is ``False``.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+SchedulerTV = TypeVar(\"SchedulerTV\", bound=\"Scheduler\")\n+\n+\n+class Scheduler(BaseScheduler):\n+    \"\"\"\n+    Default Scrapy scheduler. This implementation also handles duplication\n+    filtering via the :setting:`dupefilter <DUPEFILTER_CLASS>`.\n+\n+    This scheduler stores requests into several priority queues (defined by the\n+    :setting:`SCHEDULER_PRIORITY_QUEUE` setting). In turn, said priority queues\n+    are backed by either memory or disk based queues (respectively defined by the\n+    :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` settings).\n+\n+    Request prioritization is almost entirely delegated to the priority queue. The only\n+    prioritization performed by this scheduler is using the disk-based queue if present\n+    (i.e. if the :setting:`JOBDIR` setting is defined) and falling back to the memory-based\n+    queue if a serialization error occurs. If the disk queue is not present, the memory one\n+    is used directly.\n+\n+    :param dupefilter: An object responsible for checking and filtering duplicate requests.\n+                       The value for the :setting:`DUPEFILTER_CLASS` setting is used by default.\n+    :type dupefilter: :class:`scrapy.dupefilters.BaseDupeFilter` instance or similar:\n+                      any class that implements the `BaseDupeFilter` interface\n+\n+    :param jobdir: The path of a directory to be used for persisting the crawl's state.\n+                   The value for the :setting:`JOBDIR` setting is used by default.\n+                   See :ref:`topics-jobs`.\n+    :type jobdir: :class:`str` or ``None``\n+\n+    :param dqclass: A class to be used as persistent request queue.\n+                    The value for the :setting:`SCHEDULER_DISK_QUEUE` setting is used by default.\n+    :type dqclass: class\n+\n+    :param mqclass: A class to be used as non-persistent request queue.\n+                    The value for the :setting:`SCHEDULER_MEMORY_QUEUE` setting is used by default.\n+    :type mqclass: class\n+\n+    :param logunser: A boolean that indicates whether or not unserializable requests should be logged.\n+                     The value for the :setting:`SCHEDULER_DEBUG` setting is used by default.\n+    :type logunser: bool\n+\n+    :param stats: A stats collector object to record stats about the request scheduling process.\n+                  The value for the :setting:`STATS_CLASS` setting is used by default.\n+    :type stats: :class:`scrapy.statscollectors.StatsCollector` instance or similar:\n+                 any class that implements the `StatsCollector` interface\n+\n+    :param pqclass: A class to be used as priority queue for requests.\n+                    The value for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting is used by default.\n+    :type pqclass: class\n+\n+    :param crawler: The crawler object corresponding to the current crawl.\n+    :type crawler: :class:`scrapy.crawler.Crawler`\n+    \"\"\"\n+    def __init__(\n+        self,\n+        dupefilter,\n+        jobdir: Optional[str] = None,\n+        dqclass=None,\n+        mqclass=None,\n+        logunser: bool = False,\n+        stats=None,\n+        pqclass=None,\n+        crawler: Optional[Crawler] = None,\n+    ):\n         self.df = dupefilter\n         self.dqdir = self._dqdir(jobdir)\n         self.pqclass = pqclass\n@@ -47,34 +184,57 @@ class Scheduler:\n         self.crawler = crawler\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        settings = crawler.settings\n-        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n-        dupefilter = create_instance(dupefilter_cls, settings, crawler)\n-        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n-        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n-        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n-        logunser = settings.getbool('SCHEDULER_DEBUG')\n-        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n-                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n-                   mqclass=mqclass, crawler=crawler)\n+    def from_crawler(cls: Type[SchedulerTV], crawler) -> SchedulerTV:\n+        \"\"\"\n+        Factory method, initializes the scheduler with arguments taken from the crawl settings\n+        \"\"\"\n+        dupefilter_cls = load_object(crawler.settings['DUPEFILTER_CLASS'])\n+        return cls(\n+            dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),\n+            jobdir=job_dir(crawler.settings),\n+            dqclass=load_object(crawler.settings['SCHEDULER_DISK_QUEUE']),\n+            mqclass=load_object(crawler.settings['SCHEDULER_MEMORY_QUEUE']),\n+            logunser=crawler.settings.getbool('SCHEDULER_DEBUG'),\n+            stats=crawler.stats,\n+            pqclass=load_object(crawler.settings['SCHEDULER_PRIORITY_QUEUE']),\n+            crawler=crawler,\n+        )\n \n-    def has_pending_requests(self):\n+    def has_pending_requests(self) -> bool:\n         return len(self) > 0\n \n-    def open(self, spider):\n+    def open(self, spider: Spider) -> Optional[Deferred]:\n+        \"\"\"\n+        (1) initialize the memory queue\n+        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n+        (3) return the result of the dupefilter's ``open`` method\n+        \"\"\"\n         self.spider = spider\n         self.mqs = self._mq()\n         self.dqs = self._dq() if self.dqdir else None\n         return self.df.open()\n \n-    def close(self, reason):\n-        if self.dqs:\n+    def close(self, reason: str) -> Optional[Deferred]:\n+        \"\"\"\n+        (1) dump pending requests to disk if there is a disk queue\n+        (2) return the result of the dupefilter's ``close`` method\n+        \"\"\"\n+        if self.dqs is not None:\n             state = self.dqs.close()\n+            assert isinstance(self.dqdir, str)\n             self._write_dqs_state(self.dqdir, state)\n         return self.df.close(reason)\n \n-    def enqueue_request(self, request):\n+    def enqueue_request(self, request: Request) -> bool:\n+        \"\"\"\n+        Unless the received request is filtered out by the Dupefilter, attempt to push\n+        it into the disk queue, falling back to pushing it into the memory queue.\n+\n+        Increment the appropriate stats, such as: ``scheduler/enqueued``,\n+        ``scheduler/enqueued/disk``, ``scheduler/enqueued/memory``.\n+\n+        Return ``True`` if the request was stored successfully, ``False`` otherwise.\n+        \"\"\"\n         if not request.dont_filter and self.df.request_seen(request):\n             self.df.log(request, self.spider)\n             return False\n@@ -87,24 +247,35 @@ class Scheduler:\n         self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n         return True\n \n-    def next_request(self):\n+    def next_request(self) -> Optional[Request]:\n+        \"\"\"\n+        Return a :class:`~scrapy.http.Request` object from the memory queue,\n+        falling back to the disk queue if the memory queue is empty.\n+        Return ``None`` if there are no more enqueued requests.\n+\n+        Increment the appropriate stats, such as: ``scheduler/dequeued``,\n+        ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n+        \"\"\"\n         request = self.mqs.pop()\n-        if request:\n+        if request is not None:\n             self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n         else:\n             request = self._dqpop()\n-            if request:\n+            if request is not None:\n                 self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n-        if request:\n+        if request is not None:\n             self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n         return request\n \n-    def __len__(self):\n-        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)\n+    def __len__(self) -> int:\n+        \"\"\"\n+        Return the total amount of enqueued requests\n+        \"\"\"\n+        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)\n \n-    def _dqpush(self, request):\n+    def _dqpush(self, request: Request) -> bool:\n         if self.dqs is None:\n-            return\n+            return False\n         try:\n             self.dqs.push(request)\n         except ValueError as e:  # non serializable request\n@@ -115,18 +286,18 @@ class Scheduler:\n                 logger.warning(msg, {'request': request, 'reason': e},\n                                exc_info=True, extra={'spider': self.spider})\n                 self.logunser = False\n-            self.stats.inc_value('scheduler/unserializable',\n-                                 spider=self.spider)\n-            return\n+            self.stats.inc_value('scheduler/unserializable', spider=self.spider)\n+            return False\n         else:\n             return True\n \n-    def _mqpush(self, request):\n+    def _mqpush(self, request: Request) -> None:\n         self.mqs.push(request)\n \n-    def _dqpop(self):\n-        if self.dqs:\n+    def _dqpop(self) -> Optional[Request]:\n+        if self.dqs is not None:\n             return self.dqs.pop()\n+        return None\n \n     def _mq(self):\n         \"\"\" Create a new priority queue instance, with in-memory storage \"\"\"\n@@ -150,21 +321,22 @@ class Scheduler:\n                         {'queuesize': len(q)}, extra={'spider': self.spider})\n         return q\n \n-    def _dqdir(self, jobdir):\n+    def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n         \"\"\" Return a folder name to keep disk queue state at \"\"\"\n-        if jobdir:\n+        if jobdir is not None:\n             dqdir = join(jobdir, 'requests.queue')\n             if not exists(dqdir):\n                 os.makedirs(dqdir)\n             return dqdir\n+        return None\n \n-    def _read_dqs_state(self, dqdir):\n+    def _read_dqs_state(self, dqdir: str) -> list:\n         path = join(dqdir, 'active.json')\n         if not exists(path):\n-            return ()\n+            return []\n         with open(path) as f:\n             return json.load(f)\n \n-    def _write_dqs_state(self, dqdir, state):\n+    def _write_dqs_state(self, dqdir: str, state: list) -> None:\n         with open(join(dqdir, 'active.json'), 'w') as f:\n             json.dump(state, f)\n\n@@ -1,7 +1,10 @@\n import os\n+from typing import Optional\n+\n+from scrapy.settings import BaseSettings\n \n \n-def job_dir(settings):\n+def job_dir(settings: BaseSettings) -> Optional[str]:\n     path = settings['JOBDIR']\n     if path and not os.path.exists(path):\n         os.makedirs(path)\n\n@@ -0,0 +1,159 @@\n+from typing import Dict, Optional\n+from unittest import TestCase\n+from urllib.parse import urljoin, urlparse\n+\n+from testfixtures import LogCapture\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase as TwistedTestCase\n+\n+from scrapy.core.scheduler import BaseScheduler\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.http import Request\n+from scrapy.spiders import Spider\n+from scrapy.utils.request import request_fingerprint\n+\n+from tests.mockserver import MockServer\n+\n+\n+PATHS = [\"/a\", \"/b\", \"/c\"]\n+URLS = [urljoin(\"https://example.org\", p) for p in PATHS]\n+\n+\n+class MinimalScheduler:\n+    def __init__(self) -> None:\n+        self.requests: Dict[str, Request] = {}\n+\n+    def has_pending_requests(self) -> bool:\n+        return bool(self.requests)\n+\n+    def enqueue_request(self, request: Request) -> bool:\n+        fp = request_fingerprint(request)\n+        if fp not in self.requests:\n+            self.requests[fp] = request\n+            return True\n+        return False\n+\n+    def next_request(self) -> Optional[Request]:\n+        if self.has_pending_requests():\n+            fp, request = self.requests.popitem()\n+            return request\n+        return None\n+\n+\n+class SimpleScheduler(MinimalScheduler):\n+    def open(self, spider: Spider) -> defer.Deferred:\n+        return defer.succeed(\"open\")\n+\n+    def close(self, reason: str) -> defer.Deferred:\n+        return defer.succeed(\"close\")\n+\n+    def __len__(self) -> int:\n+        return len(self.requests)\n+\n+\n+class TestSpider(Spider):\n+    name = \"test\"\n+\n+    def __init__(self, mockserver, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.start_urls = map(mockserver.url, PATHS)\n+\n+    def parse(self, response):\n+        return {\"path\": urlparse(response.url).path}\n+\n+\n+class InterfaceCheckMixin:\n+    def test_scheduler_class(self):\n+        self.assertTrue(isinstance(self.scheduler, BaseScheduler))\n+        self.assertTrue(issubclass(self.scheduler.__class__, BaseScheduler))\n+\n+\n+class BaseSchedulerTest(TestCase, InterfaceCheckMixin):\n+    def setUp(self):\n+        self.scheduler = BaseScheduler()\n+\n+    def test_methods(self):\n+        self.assertIsNone(self.scheduler.open(Spider(\"foo\")))\n+        self.assertIsNone(self.scheduler.close(\"finished\"))\n+        self.assertRaises(NotImplementedError, self.scheduler.has_pending_requests)\n+        self.assertRaises(NotImplementedError, self.scheduler.enqueue_request, Request(\"https://example.org\"))\n+        self.assertRaises(NotImplementedError, self.scheduler.next_request)\n+\n+\n+class MinimalSchedulerTest(TestCase, InterfaceCheckMixin):\n+    def setUp(self):\n+        self.scheduler = MinimalScheduler()\n+\n+    def test_open_close(self):\n+        with self.assertRaises(AttributeError):\n+            self.scheduler.open(Spider(\"foo\"))\n+        with self.assertRaises(AttributeError):\n+            self.scheduler.close(\"finished\")\n+\n+    def test_len(self):\n+        with self.assertRaises(AttributeError):\n+            self.scheduler.__len__()\n+        with self.assertRaises(TypeError):\n+            len(self.scheduler)\n+\n+    def test_enqueue_dequeue(self):\n+        self.assertFalse(self.scheduler.has_pending_requests())\n+        for url in URLS:\n+            self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n+            self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n+        self.assertTrue(self.scheduler.has_pending_requests)\n+\n+        dequeued = []\n+        while self.scheduler.has_pending_requests():\n+            request = self.scheduler.next_request()\n+            dequeued.append(request.url)\n+        self.assertEqual(set(dequeued), set(URLS))\n+        self.assertFalse(self.scheduler.has_pending_requests())\n+\n+\n+class SimpleSchedulerTest(TwistedTestCase, InterfaceCheckMixin):\n+    def setUp(self):\n+        self.scheduler = SimpleScheduler()\n+\n+    @defer.inlineCallbacks\n+    def test_enqueue_dequeue(self):\n+        open_result = yield self.scheduler.open(Spider(\"foo\"))\n+        self.assertEqual(open_result, \"open\")\n+        self.assertFalse(self.scheduler.has_pending_requests())\n+\n+        for url in URLS:\n+            self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n+            self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n+\n+        self.assertTrue(self.scheduler.has_pending_requests())\n+        self.assertEqual(len(self.scheduler), len(URLS))\n+\n+        dequeued = []\n+        while self.scheduler.has_pending_requests():\n+            request = self.scheduler.next_request()\n+            dequeued.append(request.url)\n+        self.assertEqual(set(dequeued), set(URLS))\n+\n+        self.assertFalse(self.scheduler.has_pending_requests())\n+        self.assertEqual(len(self.scheduler), 0)\n+\n+        close_result = yield self.scheduler.close(\"\")\n+        self.assertEqual(close_result, \"close\")\n+\n+\n+class MinimalSchedulerCrawlTest(TwistedTestCase):\n+    scheduler_cls = MinimalScheduler\n+\n+    @defer.inlineCallbacks\n+    def test_crawl(self):\n+        with MockServer() as mockserver:\n+            settings = {\"SCHEDULER\": self.scheduler_cls}\n+            with LogCapture() as log:\n+                yield CrawlerRunner(settings).crawl(TestSpider, mockserver)\n+            for path in PATHS:\n+                self.assertIn(f\"{{'path': '{path}'}}\", str(log))\n+            self.assertIn(f\"'item_scraped_count': {len(PATHS)}\", str(log))\n+\n+\n+class SimpleSchedulerCrawlTest(MinimalSchedulerCrawlTest):\n+    scheduler_cls = SimpleScheduler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
