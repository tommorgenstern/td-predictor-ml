{"custom_id": "scrapy#7afc9157177cb84bd2cdbabe4456a22e902ece09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 36 | Files Changed: 11 | Hunks: 26 | Methods Changed: 13 | Complexity Δ (Sum/Max): 19/11 | Churn Δ: 69 | Churn Cumulative: 652 | Contributors (this commit): 4 | Commits (past 90d): 45 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n from scrapy.command import ScrapyCommand\n from scrapy.fetcher import fetch\n from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.spider import spiders\n from scrapy.utils import display\n from scrapy import log\n@@ -42,7 +42,8 @@ class Command(ScrapyCommand):\n \n             result = callback_fcn(response)\n             links = [i for i in result if isinstance(i, Request)]\n-            items = [self.pipeline_process(i, spider, opts) for i in result if isinstance(i, ScrapedItem)]\n+            items = [self.pipeline_process(i, spider, opts) for i in result if \\\n+                     isinstance(i, BaseItem)]\n             return items, links\n \n         return (), ()\n\n@@ -3,11 +3,12 @@ This module contains some extra base models for scraped items which could be\n useful in some Scrapy implementations\n \"\"\"\n \n+import copy\n import hashlib\n \n from pprint import PrettyPrinter\n \n-from scrapy.item import ScrapedItem, ItemDelta\n+from scrapy.item import ScrapedItem\n from scrapy.item.adaptors import AdaptorPipe\n from scrapy.core.exceptions import DropItem\n from scrapy.utils.python import unique\n@@ -194,6 +195,10 @@ class RobustScrapedItem(ScrapedItem):\n         pipe.add_adaptor(adaptor, position)\n         self.set_attrib_adaptors(attrib, pipe)\n \n+    def copy(self):\n+        \"\"\"Create a new RobustScrapedItem based on the current one\"\"\"\n+        return copy.deepcopy(self)\n+\n     def validate(self):\n         \"\"\"Method used to validate item attributes data\"\"\"\n         raise NotImplemented\n@@ -215,7 +220,7 @@ class RobustScrapedItem(ScrapedItem):\n         return hash_.hexdigest()\n \n \n-class RobustItemDelta(ItemDelta):\n+class RobustItemDelta(object):\n     \"\"\"\n     This class represents the difference between\n     a pair of RobustScrapedItems.\n\n@@ -71,7 +71,7 @@ class CrawlSpider(InitSpider):\n \n     def parse_start_url(self, response):\n         \"\"\"Overrideable callback function for processing start_urls. It must\n-        return a list of ScrapedItems and/or Requests\"\"\"\n+        return a list of BaseItem and/or Requests\"\"\"\n         return []\n \n     def process_results(self, response, results):\n\n@@ -6,7 +6,7 @@ See documentation in docs/ref/spiders.rst\n \"\"\"\n \n from scrapy.contrib.spiders.init import InitSpider\n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n@@ -51,13 +51,13 @@ class XMLFeedSpider(InitSpider):\n         \"\"\"This method is called for the nodes matching the provided tag name\n         (itertag). Receives the response and an XPathSelector for each node.\n         Overriding this method is mandatory. Otherwise, you spider won't work.\n-        This method must return either a ScrapedItem, a Request, or a list\n+        This method must return either a BaseItem, a Request, or a list\n         containing any of them.\n         \"\"\"\n \n         for xSel in nodes:\n             ret = self.parse_item(response, xSel)\n-            if isinstance(ret, (ScrapedItem, Request)):\n+            if isinstance(ret, (BaseItem, Request)):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n                 raise TypeError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n@@ -121,7 +121,7 @@ class CSVFeedSpider(InitSpider):\n \n         for row in csviter(response, self.delimiter, self.headers):\n             ret = self.parse_row(response, row)\n-            if isinstance(ret, (ScrapedItem, Request)):\n+            if isinstance(ret, (BaseItem, Request)):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n                 raise TypeError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n\n@@ -6,7 +6,7 @@ from twisted.internet import defer\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.http import Request\n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.utils.misc import memoize, arg_to_iter\n@@ -63,7 +63,7 @@ def url_to_guid(httprequest):\n         except Exception, ex:\n             return _response(message='Error processing url')\n \n-        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, ScrapedItem)]\n+        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, BaseItem)]\n         return _response(guids=guids)\n \n     deferred = defer.Deferred().addCallbacks(_on_success, _on_error)\n\n@@ -1,6 +1,6 @@\n from UserDict import DictMixin\n \n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.contrib_exp.newitem.fields import BaseField\n \n \n@@ -21,7 +21,7 @@ class _ItemMeta(type):\n         return cls\n \n \n-class Item(DictMixin, ScrapedItem):\n+class Item(DictMixin, BaseItem):\n \n     __metaclass__ = _ItemMeta\n \n\n@@ -9,7 +9,7 @@ from scrapy.utils.misc import arg_to_iter, load_object\n from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n from scrapy.http import Request, Response\n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy import log\n from scrapy.stats import stats\n@@ -151,7 +151,7 @@ class Scraper(object):\n             signals.send_catch_log(signal=signals.request_received, request=output, \\\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n-        elif isinstance(output, ScrapedItem):\n+        elif isinstance(output, BaseItem):\n             log.msg(\"Scraped %s in <%s>\" % (output, request.url), domain=domain)\n             signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n                 item=output, spider=spider, response=response)\n@@ -162,7 +162,7 @@ class Scraper(object):\n         elif output is None:\n             pass\n         else:\n-            log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' in %s\" % \\\n+            log.msg(\"Spider must return Request, BaseItem or None, got '%s' in %s\" % \\\n                 (type(output).__name__, request), log.ERROR, domain=domain)\n \n     def _check_propagated_failure(self, spider_failure, propagated_failure, request, spider):\n\n@@ -1 +1 @@\n-from scrapy.item.models import ScrapedItem, ItemDelta\n+from scrapy.item.models import ScrapedItem\n\n@@ -1,9 +1,9 @@\n-import copy\n+class BaseItem(object):\n+    \"\"\"Base class for all scraped items.\"\"\"\n+    pass\n \n-class ScrapedItem(object):\n-    \"\"\"\n-    This is the base class for all scraped items.\n-    \"\"\"\n+\n+class ScrapedItem(BaseItem):\n \n     def __init__(self, data=None):\n         \"\"\"\n@@ -24,12 +24,3 @@ class ScrapedItem(object):\n         reprdict = dict(items for items in self.__dict__.iteritems() if not items[0].startswith('_'))\n         return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n \n-    def __sub__(self, other):\n-        raise NotImplementedError\n-\n-    def copy(self):\n-        \"\"\"Create a new ScrapedItem based on the current one\"\"\"\n-        return copy.deepcopy(self)\n-\n-class ItemDelta(object):\n-    pass\n\n@@ -1,6 +1,6 @@\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import defer_succeed, mustbe_deferred\n from scrapy.conf import settings\n@@ -39,8 +39,8 @@ class ItemPipelineManager(object):\n             return defer_succeed(item)\n \n         def next_stage(item, stages_left):\n-            assert isinstance(item, ScrapedItem), \\\n-                'Item pipelines must return a ScrapedItem, got %s' % type(item).__name__\n+            assert isinstance(item, BaseItem), \\\n+                'Item pipelines must return a BaseItem, got %s' % type(item).__name__\n             if not stages_left:\n                 return item\n             current_stage = stages_left.pop(0)\n\n@@ -4,7 +4,7 @@ Helper functions for formatting and pretty printing some objects\n import sys\n import pprint as pypprint\n \n-from scrapy.item import ScrapedItem\n+from scrapy.item.models import BaseItem\n from scrapy.http import Request, Response\n \n nocolour = False\n@@ -35,7 +35,7 @@ def pformat(obj, *args, **kwargs):\n \n     if isinstance(obj, (list, tuple)):\n         return \"\".join([pformat(i, *args, **kwargs) for i in obj])\n-    elif isinstance(obj, (ScrapedItem, Request, Response)):\n+    elif isinstance(obj, (BaseItem, Request, Response)):\n         return pformat_dictobj(obj)\n     else:\n         return colorize(pypprint.pformat(obj))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90f1d9e48990e0cd2577cdae04e780c386b19b30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 38 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,7 @@\n \"\"\"\n DuplicatesFilterMiddleware: Filter out already visited urls\n+\n+See documentation in docs/ref/scheduler-middleware.rst\n \"\"\"\n \n from scrapy.core.exceptions import IgnoreRequest, NotConfigured\n@@ -7,7 +9,7 @@ from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n class DuplicatesFilterMiddleware(object):\n-    \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n+\n     def __init__(self):\n         clspath = settings.get('DUPEFILTER_CLASS')\n         if not clspath:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa345e116a6bde6e7e407576066bc1045c59235b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 32 | Files Changed: 7 | Hunks: 19 | Methods Changed: 7 | Complexity Δ (Sum/Max): 19/9 | Churn Δ: 69 | Churn Cumulative: 186 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n-DepthMiddleware is a scrape middleware used for tracking the depth of each\n-Request inside the site being scraped. It can be used to limit the maximum\n-depth to scrape or things like that\n+Depth Spider Middleware\n+\n+See documentation in docs/ref/spider-middleware.rst\n \"\"\"\n \n from scrapy import log\n@@ -18,21 +18,24 @@ class DepthMiddleware(object):\n             stats.set_value('envinfo/request_depth_limit', self.maxdepth)\n \n     def process_spider_output(self, response, result, spider):\n+        domain = spider.domain_name\n         def _filter(request):\n             if isinstance(request, Request):\n                 depth = response.request.meta['depth'] + 1\n                 request.meta['depth'] = depth\n                 if self.maxdepth and depth > self.maxdepth:\n-                    log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), level=log.DEBUG, domain=spider.domain_name)\n+                    log.msg(\"Ignoring link (depth > %d): %s \" % (self.maxdepth, request.url), \\\n+                        level=log.DEBUG, domain=domain)\n                     return False\n                 elif self.stats:\n-                    stats.inc_value('request_depth_count/%s' % depth, domain=spider.domain_name)\n-                    if depth > stats.get_value('request_depth_max', 0, domain=spider.domain_name):\n-                        stats.set_value('request_depth_max', depth, domain=spider.domain_name)\n+                    stats.inc_value('request_depth_count/%s' % depth, domain=domain)\n+                    if depth > stats.get_value('request_depth_max', 0, domain=domain):\n+                        stats.set_value('request_depth_max', depth, domain=domain)\n             return True\n \n-        if self.stats and 'depth' not in response.request.meta: # otherwise we loose stats for depth=0 \n+        # base case (depth=0)\n+        if self.stats and 'depth' not in response.request.meta: \n             response.request.meta['depth'] = 0\n-            stats.inc_value('request_depth_count/0', domain=spider.domain_name)\n+            stats.inc_value('request_depth_count/0', domain=domain)\n \n         return (r for r in result or () if _filter(r))\n\n@@ -1,13 +1,12 @@\n+\"\"\"\n+HttpError Spider Middleware\n \n+See documentation in docs/ref/spider-middleware.rst\n+\"\"\"\n \n class HttpErrorMiddleware(object):\n-    \"\"\"Filter out response outside of a range of valid status codes\n-\n-    This middleware filters out every response with status outside of the range 200<=status<300\n-    Spiders can add more exceptions using `handle_httpstatus_list` spider attribute.\n-    \"\"\"\n \n     def process_spider_input(self, response, spider):\n         if not (200 <= response.status < 300 or \\\n                 response.status in getattr(spider, 'handle_httpstatus_list', [])):\n-            return [] # skip response\n+            return []\n\n@@ -1,6 +1,7 @@\n \"\"\"\n-OffsiteMiddleware: Filters out Requests for URLs outside the domains covered by\n-the spider.\n+Offsite Spider Middleware\n+\n+See documentation in docs/ref/spider-middleware.rst\n \"\"\"\n \n from scrapy.http import Request\n\n@@ -1,11 +1,7 @@\n \"\"\"\n-RequestLimitMiddleware: Limits the scheduler request queue size. When spiders\n-try to schedule more than the allowed amount of requests the new requests\n-(returned by the spider) will be dropped.\n+Request Limit Spider middleware\n \n-The limit can be set using the spider attribue `requests_queue_size` or the\n-setting \"REQUESTS_QUEUE_SIZE\". If not specified (or 0), no limit will be\n-applied. \n+See documentation in docs/ref/spider-middleware.rst\n \"\"\"\n from itertools import imap\n from scrapy.xlib.pydispatch import dispatcher\n\n@@ -1,7 +1,11 @@\n \"\"\"\n-RestrictMiddleware: restricts crawling to fixed set of particular URLs\n+Restrict Spider Middleware\n+\n+See documentation in docs/ref/spider-middleware.rst\n \"\"\"\n \n+from itertools import ifilter\n+\n from scrapy.http import Request\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n@@ -13,9 +17,6 @@ class RestrictMiddleware(object):\n             raise NotConfigured\n \n     def process_spider_output(self, response, result, spider):\n-        def _filter(r):\n-            if isinstance(r, Request) and r.url not in self.allowed_urls:\n-                return False\n-            return True\n-        return (r for r in result or () if _filter(r))\n+        return ifilter(lambda r: isinstance(r, Request) \\\n+            and r.url not in self.allowed_urls, result or ())\n \n\n@@ -1,5 +1,7 @@\n \"\"\"\n-UrlFilterMiddleware: canonicalizes URLs to filter out duplicated ones\n+Url Filter Middleware\n+\n+See documentation in docs/refs/spider-middleware.rst\n \"\"\"\n \n from scrapy.http import Request\n\n@@ -1,5 +1,7 @@\n \"\"\"\n-UrlLengthMiddleware: Filters out requests with URLs longer than URLLENGTH_LIMIT\n+Url Length Spider Middleware\n+\n+See documentation in docs/ref/spider-middleware.rst\n \"\"\"\n \n from scrapy import log\n@@ -16,7 +18,8 @@ class UrlLengthMiddleware(object):\n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request) and len(request.url) > self.maxlength:\n-                log.msg(\"Ignoring link (url length > %d): %s \" % (self.maxlength, request.url), level=log.DEBUG, domain=spider.domain_name)\n+                log.msg(\"Ignoring link (url length > %d): %s \" % (self.maxlength, request.url), \\\n+                    level=log.DEBUG, domain=spider.domain_name)\n                 return False\n             else:\n                 return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb3a1dc16cf8f61e7395db1a1f605699d50857e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 158 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,19 +30,19 @@ class Item(DictMixin, BaseItem):\n     def __init__(self, *args, **kwargs):\n         self._values = {}\n \n-        # load default values\n-        for name, field in self.fields.iteritems():\n-            default = field.get_default()\n-            if default:\n-                self._values[name] = default\n-            \n-        # load init values\n         if args or kwargs: # don't instantiate dict for simple (most common) case\n             for k, v in dict(*args, **kwargs).iteritems():\n                 self[k] = v\n \n     def __getitem__(self, key):\n+        try:\n             return self._values[key]\n+        except KeyError:\n+            default = self.fields[key].get_default()\n+            if default is not None:\n+                return default\n+            else:\n+                raise KeyError(key)\n \n     def __setitem__(self, key, value):\n         self._values[key] = self.fields[key].to_python(value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c4afb23be6fde0f037f2856afd4bde3e60c9edd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 7 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 10 | Churn Cumulative: 955 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1 +0,0 @@\n-from scrapy.contrib_exp.newitem.models import Item\n\n@@ -0,0 +1 @@\n+from scrapy.newitem.models import Item\n\n\n@@ -1,7 +1,7 @@\n from UserDict import DictMixin\n \n from scrapy.item.models import BaseItem\n-from scrapy.contrib_exp.newitem.fields import BaseField\n+from scrapy.newitem.fields import BaseField\n \n \n class _ItemMeta(type):\n\n\n@@ -1,7 +1,7 @@\n import unittest\n import string\n from scrapy.contrib_exp.newitem.adaptors import adaptor, ItemAdaptor\n-from scrapy.contrib_exp.newitem import Item, fields\n+from scrapy.newitem import Item, fields\n \n \n class BaseItem(Item):\n\n@@ -2,8 +2,8 @@ import datetime\n import decimal\n import unittest\n \n-from scrapy.contrib_exp.newitem import Item, fields\n-from scrapy.contrib_exp.newitem.fields import BaseField\n+from scrapy.newitem import Item, fields\n+from scrapy.newitem.fields import BaseField\n \n \n class NewItemTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6eb2609d1eaaf15eec6a593f8bb1b2db773a0d32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 18 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,18 +0,0 @@\n-import simplejson\n-\n-from scrapy.utils.serialization import ScrapyJSONEncoder\n-\n-\n-def item_to_dict(item):\n-    \"\"\"Returns a dict representation of an item\"\"\"\n-    res = {}\n-    for field in item.fields:\n-        res[field] = getattr(item, field)\n-        \n-    return res\n-\n-\n-def item_to_json(item):\n-    \"\"\"Returns a json representation of an item\"\"\"\n-    return simplejson.dumps(item_to_dict(item), cls=ScrapyJSONEncoder)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38c3f7d0b455b07b1904778aa3f03d47ee775ab5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 351 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,7 +152,8 @@ class Scraper(object):\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n-            log.msg(\"Scraped %s in <%s>\" % (output, request.url), domain=domain)\n+            log.msg(\"Scraped %s in <%s>\" % (output, request.url), level=log.DEBUG, \\\n+                domain=domain)\n             signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n                 item=output, spider=spider, response=response)\n             self.sites[domain].itemproc_size += 1\n@@ -187,13 +188,14 @@ class Scraper(object):\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                log.msg(\"Dropped %s - %s\" % (item, str(ex)), log.DEBUG, domain=domain)\n+                log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, domain=domain)\n                 signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.msg('Error processing %s - %s' % (item, output), \\\n                     log.ERROR, domain=domain)\n         else:\n+            log.msg(\"Passed %s\" % item, log.INFO, domain=domain)\n             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, \\\n                 item=item, spider=spider, output=output)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d21a22eab5eeee66518244e05613f67b1a098083", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 102 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ class StatsCollector(object):\n         self._stats = {None: {}} # None is for global stats\n \n         dispatcher.connect(self.open_domain, signal=signals.domain_open)\n-        dispatcher.connect(self.close_domain, signal=signals.domain_closed)\n+        dispatcher.connect(self._start_closing_domain, signal=signals.domain_closed)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n     def get_value(self, key, default=None, domain=None):\n@@ -48,7 +48,6 @@ class StatsCollector(object):\n         signals.send_catch_log(stats_domain_opened, domain=domain)\n \n     def close_domain(self, domain, reason):\n-        signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n         if self._dump:\n             log.msg(\"Dumping stats:\\n\" + pprint.pformat(self.get_stats(domain)), \\\n                 domain=domain)\n@@ -59,6 +58,9 @@ class StatsCollector(object):\n         if self._dump:\n             log.msg(\"Dumping global stats:\\n\" + pprint.pformat(self.get_stats()))\n \n+    def _start_closing_domain(self, domain, reason):\n+        signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n+        self.close_domain(domain, reason)\n \n class MemoryStatsCollector(StatsCollector):\n \n@@ -82,5 +84,5 @@ class DummyStatsCollector(StatsCollector):\n     def set_stats(self, stats, domain=None):\n         pass\n \n-    def inc_value(self, key, count=1, domain=None):\n+    def inc_value(self, key, count=1, start=0, domain=None):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bed3c38014f26595d48231e7ad80618384b29b12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,7 +3,7 @@ DelayedCloseDomain is an extension that keeps open a domain until a\n configurable amount of idle time is reached\n \"\"\"\n \n-from datetime import datetime\n+from time import time\n \n from scrapy.xlib.pydispatch import dispatcher\n from collections import defaultdict\n@@ -20,7 +20,7 @@ class DelayedCloseDomain(object):\n         if not self.delay:\n             raise NotConfigured\n \n-        self.opened_at = defaultdict(datetime.now)\n+        self.opened_at = defaultdict(time)\n         dispatcher.connect(self.domain_idle, signal=signals.domain_idle)\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n \n@@ -32,8 +32,7 @@ class DelayedCloseDomain(object):\n         if not lastseen:\n             lastseen = self.opened_at[domain]\n \n-        delta = datetime.now() - lastseen\n-        if delta.seconds < self.delay:\n+        if time() < lastseen + self.delay:\n             raise DontCloseDomain\n \n     def domain_closed(self, domain):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9da66698f3766a4c7d897b84763a8a311eb60769", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 55 | Files Changed: 10 | Hunks: 20 | Methods Changed: 12 | Complexity Δ (Sum/Max): 40/21 | Churn Δ: 108 | Churn Cumulative: 205 | Contributors (this commit): 2 | Commits (past 90d): 29 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,6 @@\n from scrapy.core.exceptions import NotConfigured\n+from scrapy.utils.request import request_httprepr\n+from scrapy.utils.response import response_httprepr\n from scrapy.stats import stats\n from scrapy.conf import settings\n \n@@ -18,7 +20,7 @@ class DownloaderStats(object):\n         stats.inc_value('downloader/request_count')\n         stats.inc_value('downloader/request_count', domain=domain)\n         stats.inc_value('downloader/request_method_count/%s' % request.method, domain=domain)\n-        reqlen = len(request.httprepr())\n+        reqlen = len(request_httprepr(request))\n         stats.inc_value('downloader/request_bytes', reqlen, domain=domain)\n         stats.inc_value('downloader/request_bytes', reqlen)\n \n@@ -27,7 +29,7 @@ class DownloaderStats(object):\n         stats.inc_value('downloader/response_count')\n         stats.inc_value('downloader/response_count', domain=domain)\n         stats.inc_value('downloader/response_status_count/%s' % response.status, domain=domain)\n-        reslen = len(response.httprepr())\n+        reslen = len(response_httprepr(response))\n         stats.inc_value('downloader/response_bytes', reslen, domain=domain)\n         stats.inc_value('downloader/response_bytes', reslen)\n         return response\n\n@@ -30,6 +30,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.core import signals\n+from scrapy.utils.response import response_httprepr\n from scrapy.stats import stats\n from scrapy.http import Request\n from scrapy import log\n@@ -86,7 +87,7 @@ class ItemSamplerMiddleware(object):\n     def process_spider_input(self, response, spider):\n         if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_domain:\n             return []\n-        elif max_response_size and max_response_size > len(response.httprepr()):  \n+        elif max_response_size and max_response_size > len(response_httprepr(response)):  \n             return []\n \n     def process_spider_output(self, response, result, spider):\n\n@@ -96,18 +96,3 @@ class Request(object):\n                               meta=self.meta if meta is None else meta,\n                               encoding=self.encoding if encoding is None else encoding,\n                               dont_filter=self.dont_filter if dont_filter is None else dont_filter)\n-\n-    def httprepr(self):\n-        \"\"\" Return raw HTTP request representation (as string). This is\n-        provided only for reference since it's not the actual stream of bytes\n-        that will be send when performing the request (that's controlled by\n-        Twisted).\n-        \"\"\"\n-\n-        s  = \"%s %s HTTP/1.1\\r\\n\" % (self.method, self.url)\n-        s += \"Host: %s\\r\\n\" % self.url.hostname\n-        if self.headers:\n-            s += self.headers.to_string() + \"\\r\\n\"\n-        s += \"\\r\\n\"\n-        s += self.body\n-        return s\n\n@@ -7,8 +7,6 @@ See documentation in docs/ref/request-response.rst\n \n import copy\n \n-from twisted.web.http import RESPONSES\n-\n from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n \n@@ -63,17 +61,3 @@ class Response(object):\n                   flags=self.flags if flags is None else flags,\n                   **kwargs)\n         return new\n-\n-    def httprepr(self):\n-        \"\"\"\n-        Return raw HTTP response representation (as string). This is provided\n-        only for reference, since it's not the exact stream of bytes that was\n-        received (that's not exposed by Twisted).\n-        \"\"\"\n-\n-        s  = \"HTTP/1.1 %d %s\\r\\n\" % (self.status, RESPONSES[self.status])\n-        if self.headers:\n-            s += self.headers.to_string() + \"\\r\\n\"\n-        s += \"\\r\\n\"\n-        s += self.body\n-        return s\n\n@@ -166,13 +166,6 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r4.meta, {})\n         assert r4.dont_filter is False\n \n-    def test_httprepr(self):\n-        r1 = Request(\"http://www.example.com\")\n-        self.assertEqual(r1.httprepr(), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n-\n-        r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n-\n \n class FormRequestTest(unittest.TestCase):\n \n\n@@ -104,13 +104,6 @@ class ResponseTest(unittest.TestCase):\n         self.assertEqual(r4.meta, {})\n         self.assertEqual(r4.flags, [])\n \n-    def test_httprepr(self):\n-        r1 = Response(\"http://www.example.com\")\n-        self.assertEqual(r1.httprepr(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n-\n-        r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n-\n     def test_encoding(self):\n         unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n         self.assertRaises(TypeError, Response, 'http://www.example.com', body=u'unicode body')\n\n@@ -1,14 +1,10 @@\n import unittest\n from scrapy.http import Request\n-from scrapy.utils.request import request_fingerprint, request_authenticate\n+from scrapy.utils.request import request_fingerprint, request_authenticate, request_httprepr\n \n class UtilsRequestTest(unittest.TestCase):\n \n     def test_request_fingerprint(self):\n-        url = 'http://www.scrapy.org'\n-        r = Request(url=url)\n-        urlhash = request_fingerprint(r)\n-\n         r1 = Request(\"http://www.example.com/query?id=111&cat=222\")\n         r2 = Request(\"http://www.example.com/query?cat=222&id=111\")\n         self.assertEqual(request_fingerprint(r1), request_fingerprint(r1))\n@@ -59,11 +55,17 @@ class UtilsRequestTest(unittest.TestCase):\n         fp2 = request_fingerprint(r2)\n         self.assertNotEqual(fp1, fp2)\n \n-\n     def test_request_authenticate(self):\n         r = Request(\"http://www.example.com\")\n         request_authenticate(r, 'someuser', 'somepass')\n         self.assertEqual(r.headers['Authorization'], 'Basic c29tZXVzZXI6c29tZXBhc3M=')\n \n+    def test_request_httprepr(self):\n+        r1 = Request(\"http://www.example.com\")\n+        self.assertEqual(request_httprepr(r1), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n+\n+        r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n+        self.assertEqual(request_httprepr(r1), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,6 +1,6 @@\n import unittest\n from scrapy.http import Response, TextResponse\n-from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh\n+from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, response_httprepr\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n@@ -51,5 +51,13 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = Response(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))\n \n+    def test_response_httprepr(self):\n+        r1 = Response(\"http://www.example.com\")\n+        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n+\n+        r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n+        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body')\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -72,3 +72,17 @@ def request_info(request):\n     fp = request_fingerprint(request)\n     return \"<Request: %s %s (%s..)>\" % (request.method, request.url, fp[:8])\n \n+def request_httprepr(request):\n+    \"\"\"Return the raw HTTP representation (as string) of the given request.\n+    This is provided only for reference since it's not the actual stream of\n+    bytes that will be send when performing the request (that's controlled\n+    by Twisted).\n+    \"\"\"\n+\n+    s  = \"%s %s HTTP/1.1\\r\\n\" % (request.method, request.url)\n+    s += \"Host: %s\\r\\n\" % request.url.hostname\n+    if request.headers:\n+        s += request.headers.to_string() + \"\\r\\n\"\n+    s += \"\\r\\n\"\n+    s += request.body\n+    return s\n\n@@ -4,7 +4,10 @@ scrapy.http.Response objects\n \"\"\"\n \n import re\n+\n from twisted.web import http\n+from twisted.web.http import RESPONSES\n+\n from scrapy.http.response import Response\n \n def body_or_str(obj, unicode=True):\n@@ -48,3 +51,16 @@ def response_status_message(status):\n     404 Not Found\n     \"\"\"\n     return '%s %s' % (status, http.responses.get(int(status)))\n+\n+def response_httprepr(response):\n+    \"\"\"Return raw HTTP representation (as string) of the given response. This\n+    is provided only for reference, since it's not the exact stream of bytes\n+    that was received (that's not exposed by Twisted).\n+    \"\"\"\n+\n+    s  = \"HTTP/1.1 %d %s\\r\\n\" % (response.status, RESPONSES[response.status])\n+    if response.headers:\n+        s += response.headers.to_string() + \"\\r\\n\"\n+    s += \"\\r\\n\"\n+    s += response.body\n+    return s\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09ba6927d7b1d7c2cc40886b2a0743dec43a49af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 15 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 47 | Churn Cumulative: 49 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,47 +1,44 @@\n-\"\"\"\n-CloseDomain is an extension that forces spiders to be closed after a given\n-time has expired.\n+\"\"\"CloseDomain is an extension that forces spiders to be closed after certain\n+conditions are met.\n \n+See documentation in docs/ref/extensions.rst\n \"\"\"\n-import datetime\n-import pprint\n \n+from collections import defaultdict\n+\n+from twisted.internet import reactor\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy import log\n from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.mail import MailSender\n-from scrapy.stats import stats\n from scrapy.conf import settings\n \n class CloseDomain(object):\n+\n     def __init__(self):\n         self.timeout = settings.getint('CLOSEDOMAIN_TIMEOUT')\n-        if not self.timeout:\n-            raise NotConfigured\n+        self.itempassed = settings.getint('CLOSEDOMAIN_ITEMPASSED')\n \n+        self.counts = defaultdict(int)\n         self.tasks = {}\n-        self.mail = MailSender()\n-        self.notify = settings.getlist('CLOSEDOMAIN_NOTIFY')\n \n+        if self.timeout:\n             dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n+        if self.itempassed:\n+            dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n \n     def domain_opened(self, domain):\n-        self.tasks[domain] = scrapyengine.addtask(self.close_domain, self.timeout, args=[domain])\n+        self.tasks[domain] = reactor.callLater(self.timeout, scrapyengine.close_domain, \\\n+            domain=domain, reason='closedomain_timeout')\n         \n-    def close_domain(self, domain): \n-        log.msg(\"Domain was opened for more than %d seconds, closing it...\" % self.timeout, domain=domain)\n-        scrapyengine.close_domain(domain)\n-        if self.notify:\n-            body = \"Closed domain %s because it remained opened for more than %s\\n\\n\" % (domain, datetime.timedelta(seconds=self.timeout))\n-            body += \"DOMAIN STATS ------------------------------------------------------\\n\\n\"\n-            body += pprint.pformat(stats.get(domain, None))\n-            subj = \"Closed domain by timeout: %s\" % domain\n-            self.mail.send(self.notify, subj, body)\n+    def item_passed(self, item, spider):\n+        self.counts[spider.domain_name] += 1\n+        if self.counts[spider.domain_name] == self.itempassed:\n+            scrapyengine.close_domain(spider.domain_name, 'closedomain_itempassed')\n \n     def domain_closed(self, domain):\n-        if domain in self.tasks:\n-            scrapyengine.removetask(self.tasks[domain])\n+        self.counts.pop(domain, None)\n+        tsk = self.tasks.pop(domain)\n+        if not tsk.called:\n+            tsk.cancel()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fcc91901eb0c5d05abc0e6695b4e63b89121b1a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 141 | Contributors (this commit): 3 | Commits (past 90d): 25 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ BOT_NAME = 'scrapybot'\n BOT_VERSION = '1.0'\n \n CLOSEDOMAIN_TIMEOUT = 0\n-CLOSEDOMAIN_NOTIFY = []\n+CLOSEDOMAIN_ITEMPASSED = 0\n \n CLUSTER_LOGDIR = ''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64d91555729dbe2ea78fe5d6424677a196b221ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 53 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -39,6 +39,6 @@ class CloseDomain(object):\n \n     def domain_closed(self, domain):\n         self.counts.pop(domain, None)\n-        tsk = self.tasks.pop(domain)\n-        if not tsk.called:\n+        tsk = self.tasks.pop(domain, None)\n+        if tsk and not tsk.called:\n             tsk.cancel()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#643f93721be7e5e6a25e0d91ffe576690a540e92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 155 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -73,7 +73,7 @@ class BaseItemExporter(object):\n class PprintItemExporter(BaseItemExporter):\n \n     def __init__(self, file):\n-        super(PprintItemExporter, self).__init__(self)\n+        super(PprintItemExporter, self).__init__()\n         self.file = file\n \n     def export(self, item):\n@@ -83,7 +83,7 @@ class PprintItemExporter(BaseItemExporter):\n class PickleItemExporter(BaseItemExporter):\n \n     def __init__(self, *args, **kwargs):\n-        super(PickleItemExporter, self).__init__(self)\n+        super(PickleItemExporter, self).__init__()\n         self.pickler = Pickler(*args, **kwargs)\n \n     def export(self, item):\n@@ -95,7 +95,7 @@ class CsvItemExporter(BaseItemExporter):\n     fields_to_export = ()\n \n     def __init__(self, *args, **kwargs):\n-        super(CsvItemExporter, self).__init__(self)\n+        super(CsvItemExporter, self).__init__()\n         self.csv_writer = csv.writer(*args, **kwargs)\n \n     def export(self, item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3d732f28a7b6299fb48f363f045a3144adf1b46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 157 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -77,7 +77,7 @@ class PprintItemExporter(BaseItemExporter):\n         self.file = file\n \n     def export(self, item):\n-        self.file.write(pprint.pprint(dict(item)) + '\\n')\n+        self.file.write(pprint.pformat(dict(item)) + '\\n')\n \n \n class PickleItemExporter(BaseItemExporter):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d57c0100db74e5f096b18566054dba3819780023", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 159 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ class XmlItemExporter(BaseItemExporter):\n     def _export_xml_field(self, field, name, value):\n         if isinstance(field, fields.ListField):\n             for v in value:\n-                self._export_xml_field(field, 'value', v)\n+                self._export_xml_field(field._field, 'value', v)\n         self.xg.startElement(name, {})\n         self.xg.characters(self._serialize_field(field, name, value))\n         self.xg.endElement(name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#230bcef7b611561c6ff34415d45d626740c3c118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 19/18 | Churn Δ: 35 | Churn Cumulative: 62 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ import cProfile\n import scrapy\n from scrapy import log\n from scrapy.spider import spiders\n-from scrapy.conf import settings\n+from scrapy.conf import settings, SETTINGS_MODULE\n \n def find_commands(dir):\n     try:\n@@ -88,6 +88,10 @@ def command_settings(cmdname):\n command_executed = {}\n \n def execute():\n+    if not settings.settings_module:\n+        print \"Scrapy %s\\n\" % scrapy.__version__\n+        print \"Error: Cannot find %r module in python path.\" % SETTINGS_MODULE\n+        sys.exit(1)\n     execute_with_args(sys.argv)\n \n def execute_with_args(argv):\n@@ -113,7 +117,7 @@ def execute_with_args(argv):\n         print \"Scrapy %s\\n\" % scrapy.__version__\n         print \"Unknown command: %s\\n\" % cmdname\n         print 'Type \"%s -h\" for help' % argv[0]\n-        sys.exit()\n+        sys.exit(2)\n \n     (opts, args) = parser.parse_args(args=argv[1:])\n     del args[0]  # args[0] is cmdname\n\n@@ -1,3 +1,9 @@\n+\"\"\"\n+Scrapy settings manager\n+\n+See documentation in docs/topics/settings.rst\n+\"\"\"\n+\n import os\n import cPickle as pickle\n \n@@ -5,42 +11,37 @@ SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')\n SETTINGS_DISABLED = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n \n class Settings(object):\n-    \"\"\"Class to obtain configuration values from settings module\n-       which can be overriden by environment variables prepended by SCRAPY_\"\"\"\n \n     # settings in precedence order\n     overrides = None\n-    settings = None\n+    settings_module = None\n     defaults = None\n     global_defaults = None\n \n     def __init__(self):\n         pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n         self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n-        if not SETTINGS_DISABLED:\n-            self.settings = self._import(SETTINGS_MODULE)\n+        self.settings_module = self._import(SETTINGS_MODULE)\n         self.defaults = {}\n         self.global_defaults = self._import('scrapy.conf.default_settings')\n \n     def _import(self, modulepath):\n+        try:\n             return __import__(modulepath, {}, {}, [''])\n+        except ImportError:\n+            pass\n \n     def __getitem__(self, opt_name):\n         if not SETTINGS_DISABLED:\n             if opt_name in self.overrides:\n                 return self.overrides[opt_name]\n-\n             if 'SCRAPY_' + opt_name in os.environ:\n                 return os.environ['SCRAPY_' + opt_name]\n-\n-            if hasattr(self.settings, opt_name):\n-                return getattr(self.settings, opt_name)\n-\n+            if hasattr(self.settings_module, opt_name):\n+                return getattr(self.settings_module, opt_name)\n             if opt_name in self.defaults:\n                 return self.defaults[opt_name]\n-\n-        if hasattr(self.global_defaults, opt_name):\n-            return getattr(self.global_defaults, opt_name)\n+        return getattr(self.global_defaults, opt_name, None)\n \n     def get(self, name, default=None):\n         return self[name] if self[name] is not None else default\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
