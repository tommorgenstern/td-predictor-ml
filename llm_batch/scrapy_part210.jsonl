{"custom_id": "scrapy#ddc91dda270e44d80dbe08cc79d4249f82cde093", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 134 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,7 @@ class _BaseTest(unittest.TestCase):\n                                headers={'User-Agent': 'test'})\n         self.response = Response('http://www.example.com',\n                                  headers={'Content-Type': 'text/html'},\n-                                 body='test body',\n+                                 body=b'test body',\n                                  status=202)\n         self.crawler.stats.open_spider(self.spider)\n \n@@ -84,9 +84,9 @@ class _BaseTest(unittest.TestCase):\n \n     def assertEqualRequestButWithCacheValidators(self, request1, request2):\n         self.assertEqual(request1.url, request2.url)\n-        assert not 'If-None-Match' in request1.headers\n-        assert not 'If-Modified-Since' in request1.headers\n-        assert any(h in request2.headers for h in ('If-None-Match', 'If-Modified-Since'))\n+        assert not b'If-None-Match' in request1.headers\n+        assert not b'If-Modified-Since' in request1.headers\n+        assert any(h in request2.headers for h in (b'If-None-Match', b'If-Modified-Since'))\n         self.assertEqual(request1.body, request2.body)\n \n     def test_dont_cache(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea0471e33a769f98f853b5ba145b75a9227111a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 127 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.python import to_bytes\n \n \n class DummyPolicy(object):\n@@ -305,7 +306,7 @@ class FilesystemCacheStorage(object):\n             'timestamp': time(),\n         }\n         with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n-            f.write(repr(metadata))\n+            f.write(to_bytes(repr(metadata)))\n         with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n             pickle.dump(metadata, f, protocol=2)\n         with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n@@ -373,14 +374,14 @@ class LeveldbCacheStorage(object):\n             'body': response.body,\n         }\n         batch = self._leveldb.WriteBatch()\n-        batch.Put('%s_data' % key, pickle.dumps(data, protocol=2))\n-        batch.Put('%s_time' % key, str(time()))\n+        batch.Put(key + b'_data', pickle.dumps(data, protocol=2))\n+        batch.Put(key + b'_time', to_bytes(str(time())))\n         self.db.Write(batch)\n \n     def _read_data(self, spider, request):\n         key = self._request_key(request)\n         try:\n-            ts = self.db.Get('%s_time' % key)\n+            ts = self.db.Get(key + b'_time')\n         except KeyError:\n             return  # not found or invalid entry\n \n@@ -388,14 +389,14 @@ class LeveldbCacheStorage(object):\n             return  # expired\n \n         try:\n-            data = self.db.Get('%s_data' % key)\n+            data = self.db.Get(key + b'_data')\n         except KeyError:\n             return  # invalid entry\n         else:\n             return pickle.loads(data)\n \n     def _request_key(self, request):\n-        return request_fingerprint(request)\n+        return to_bytes(request_fingerprint(request))\n \n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87849780bcf77aa47ac42cc1c7b24e1185563c7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 268 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,8 @@ class RFC2616Policy(object):\n     def __init__(self, settings):\n         self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_response_cache_controls = settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')\n+        self.ignore_response_cache_controls = map(\n+            to_bytes, settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS'))\n         self._cc_parsed = WeakKeyDictionary()\n \n     def _parse_cachecontrol(self, r):\n\n@@ -291,7 +291,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n             self.assertEqualResponse(res2, res3)\n             # request with no-cache directive must not return cached response\n             # but it allows new response to be stored\n-            res0b = res0.replace(body='foo')\n+            res0b = res0.replace(body=b'foo')\n             res4 = self._process_requestresponse(mw, req2, res0b)\n             self.assertEqualResponse(res4, res0b)\n             assert 'cached' not in res4.flags\n@@ -435,7 +435,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 assert 'cached' not in res1.flags\n                 # Same request but as cached response is stale a new response must\n                 # be returned\n-                res0b = res0a.replace(body='bar')\n+                res0b = res0a.replace(body=b'bar')\n                 res2 = self._process_requestresponse(mw, req0, res0b)\n                 self.assertEqualResponse(res2, res0b)\n                 assert 'cached' not in res2.flags\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96fcf4cea41a067b4feb4f8adaa5b9ae1d5d38dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 145 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,8 @@ class BaseResponseTest(unittest.TestCase):\n         # Response requires url in the consturctor\n         self.assertRaises(Exception, self.response_class)\n         self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))\n+        if not six.PY2:\n+            self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n         # body can be str or None\n         self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))\n         self.assertTrue(isinstance(self.response_class('http://example.com/', body=b'body'), self.response_class))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4ca1668d894920e4a74c8f4204fa3ee53039a1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 30 | Churn Cumulative: 350 | Contributors (this commit): 8 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -199,14 +199,18 @@ class MockServer():\n         time.sleep(0.2)\n \n \n+def ssl_context_factory():\n+    return ssl.DefaultOpenSSLContextFactory(\n+         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n+         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n+         )\n+\n+\n if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n     httpPort = reactor.listenTCP(8998, factory)\n-    contextFactory = ssl.DefaultOpenSSLContextFactory(\n-         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n-         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n-         )\n+    contextFactory = ssl_context_factory()\n     httpsPort = reactor.listenSSL(8999, factory, contextFactory)\n \n     def print_listening():\n\n@@ -27,7 +27,7 @@ from scrapy.utils.test import get_crawler\n from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n \n-from tests.mockserver import MockServer\n+from tests.mockserver import MockServer, ssl_context_factory\n from tests.spiders import SingleRequestSpider\n \n class DummyDH(object):\n@@ -102,6 +102,7 @@ class FileTestCase(unittest.TestCase):\n \n class HttpTestCase(unittest.TestCase):\n \n+    scheme = 'http'\n     download_handler_cls = HTTPDownloadHandler\n \n     def setUp(self):\n@@ -118,7 +119,12 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"broken\", BrokenDownloadResource())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n-        self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')\n+        self.host = '127.0.0.1'\n+        if self.scheme == 'https':\n+            self.port = reactor.listenSSL(\n+                0, self.wrapper, ssl_context_factory(), interface=self.host)\n+        else:\n+            self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n         self.download_handler = self.download_handler_cls(Settings())\n         self.download_request = self.download_handler.download_request\n@@ -130,7 +136,7 @@ class HttpTestCase(unittest.TestCase):\n             yield self.download_handler.close()\n \n     def getURL(self, path):\n-        return \"http://127.0.0.1:%d/%s\" % (self.portno, path)\n+        return \"%s://%s:%d/%s\" % (self.scheme, self.host, self.portno, path)\n \n     def test_download(self):\n         request = Request(self.getURL('file'))\n@@ -213,6 +219,12 @@ class Http10TestCase(HttpTestCase):\n     download_handler_cls = HTTP10DownloadHandler\n \n \n+class Https10TestCase(Http10TestCase):\n+    scheme = 'https'\n+    def test_timeout_download_from_spider(self):\n+        raise unittest.SkipTest(\"test_timeout_download_from_spider skipped under https\")\n+\n+\n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n     download_handler_cls = HTTP11DownloadHandler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8330776c2193c3954cba2277525401de35672e00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 139 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -257,6 +257,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n     policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'\n \n     def _process_requestresponse(self, mw, request, response):\n+        result = None\n         try:\n             result = mw.process_request(request, self.spider)\n             if result:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0648271d69fc3e5b50c430ad50f1afe44acf0d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 133 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,7 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import to_bytes, to_unicode\n \n \n class DummyPolicy(object):\n@@ -423,6 +423,7 @@ def parse_cachecontrol(header):\n \n def rfc1123_to_epoch(date_str):\n     try:\n+        date_str = to_unicode(date_str, encoding='ascii')\n         return mktime_tz(parsedate_tz(date_str))\n     except Exception:\n         return None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#085fdd628314d6d3268c6ebbd6e9388fbbb2d37f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 137 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -41,8 +41,8 @@ class RFC2616Policy(object):\n     def __init__(self, settings):\n         self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_response_cache_controls = map(\n-            to_bytes, settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS'))\n+        self.ignore_response_cache_controls = [to_bytes(cc) for cc in\n+            settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')]\n         self._cc_parsed = WeakKeyDictionary()\n \n     def _parse_cachecontrol(self, r):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee4fadc00724f02f9098625ac4d72fb29eac4dcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 95 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,29 +4,37 @@ try:\n     from cStringIO import StringIO as BytesIO\n except ImportError:\n     from io import BytesIO\n-\n+from io import UnsupportedOperation\n from gzip import GzipFile\n \n+class ReadOneGzipFile(GzipFile):\n+    def readone(self, size=-1):\n+        try:\n+            return self.read1(size)\n+        except UnsupportedOperation:\n+            return self.read(size)\n \n def gunzip(data):\n     \"\"\"Gunzip the given data and return as much data as possible.\n \n     This is resilient to CRC checksum errors.\n     \"\"\"\n-    f = GzipFile(fileobj=BytesIO(data))\n+    f = ReadOneGzipFile(fileobj=BytesIO(data))\n     output = b''\n     chunk = b'.'\n     while chunk:\n         try:\n-            chunk = f.read(8196)\n+            chunk = f.readone(8196)\n             output += chunk\n         except (IOError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n             # some pages are quite small so output is '' and f.extrabuf\n             # contains the whole page content\n-            if output or f.extrabuf:\n+            if output or getattr(f, 'extrabuf', None):\n+                try:\n                     output += f.extrabuf\n+                finally:\n                     break\n             else:\n                 raise\n\n@@ -34,7 +34,7 @@ class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n         # Trigger Twisted bug #7989\n         import twisted.persisted.styles  # NOQA\n         q = self.queue()\n-        self.assertRaises(ValueError, q.push, lambda x: x)\n+        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)\n \n class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 1\n@@ -114,7 +114,7 @@ class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):\n         # Trigger Twisted bug #7989\n         import twisted.persisted.styles  # NOQA\n         q = self.queue()\n-        self.assertRaises(ValueError, q.push, lambda x: x)\n+        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)\n \n \n class PickleLifoDiskQueueTest(MarshalLifoDiskQueueTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e44766653c294c54a3bb960b3734ee70bc663a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 315 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n     iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n     selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n     for _, node in iterable:\n-        nodetext = etree.tostring(node, encoding=six.text_type)\n+        nodetext = etree.tostring(node, encoding='unicode')\n         node.clear()\n         xs = Selector(text=nodetext, type='xml')\n         if namespace:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd735e377c2a26bc8ebb925bf6031429549faadf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 89 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ if (twisted_version.major, twisted_version.minor, twisted_version.micro) >= (15,\n if six.PY3:\n     for line in open('tests/py3-ignores.txt'):\n         file_path = line.strip()\n-        if len(file_path) > 0 and file_path[0] != '#':\n+        if file_path and file_path[0] != '#':\n             collect_ignore.append(file_path)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3c2b0cf7ec185d18464e85974ee7499acecad79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 308 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -66,7 +66,7 @@ class ScrapyHTTPPageGetter(HTTPClient):\n \n     def handleResponse(self, response):\n         if self.factory.method.upper() == b'HEAD':\n-            self.factory.page('')\n+            self.factory.page(b'')\n         elif self.length is not None and self.length > 0:\n             self.factory.noPage(self._connection_lost_reason)\n         else:\n@@ -131,7 +131,6 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n         respcls = responsetypes.from_args(headers=headers, url=self._url)\n-        body = to_bytes(body)\n         return respcls(url=self._url, status=status, headers=headers, body=body)\n \n     def _set_connection_attributes(self, request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#673df5e4161337136154eeccfc4e327d053dbd12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 31 | Churn Cumulative: 205 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,7 @@ import six\n from six.moves.urllib.parse import urlparse\n \n from twisted.trial import unittest\n-from twisted.web import server, static, error, util\n+from twisted.web import server, static, util, resource\n from twisted.internet import reactor, defer\n from twisted.test.proto_helpers import StringTransport\n from twisted.python.filepath import FilePath\n@@ -18,14 +18,14 @@ from scrapy.http import Request, Headers\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n-def getPage(url, contextFactory=None, *args, **kwargs):\n+def getPage(url, contextFactory=None, r_transform=None, *args, **kwargs):\n     \"\"\"Adapted version of twisted.web.client.getPage\"\"\"\n     def _clientfactory(url, *args, **kwargs):\n         url = to_unicode(url)\n         timeout = kwargs.pop('timeout', 0)\n         f = client.ScrapyHTTPClientFactory(\n             Request(url, *args, **kwargs), timeout=timeout)\n-        f.deferred.addCallback(lambda r: r.body)\n+        f.deferred.addCallback(r_transform or (lambda r: r.body))\n         return f\n \n     from twisted.web.client import _makeGetterFactory\n@@ -213,6 +213,16 @@ from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         ErrorResource, NoLengthResource, HostHeaderResource, \\\n         PayloadResource, BrokenDownloadResource\n \n+\n+class EncodingResource(resource.Resource):\n+    out_encoding = 'cp1251'\n+\n+    def render(self, request):\n+        body = to_unicode(request.content.read())\n+        request.setHeader(b'content-encoding', self.out_encoding)\n+        return body.encode(self.out_encoding)\n+\n+\n class WebClientTestCase(unittest.TestCase):\n     def _listen(self, site):\n         return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n@@ -229,6 +239,7 @@ class WebClientTestCase(unittest.TestCase):\n         r.putChild(b\"host\", HostHeaderResource())\n         r.putChild(b\"payload\", PayloadResource())\n         r.putChild(b\"broken\", BrokenDownloadResource())\n+        r.putChild(b\"encoding\", EncodingResource())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n         self.port = self._listen(self.wrapper)\n@@ -338,3 +349,17 @@ class WebClientTestCase(unittest.TestCase):\n                 b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n                 b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n                 b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n+\n+    def test_Encoding(self):\n+        \"\"\" Test that non-standart body encoding matches\n+        Content-Encoding header \"\"\"\n+        body = b'\\xd0\\x81\\xd1\\x8e\\xd0\\xaf'\n+        return getPage(\n+            self.getURL('encoding'), body=body, r_transform=lambda r: r)\\\n+            .addCallback(self._check_Encoding, body)\n+\n+    def _check_Encoding(self, response, original_body):\n+        content_encoding = to_unicode(response.headers[b'Content-Encoding'])\n+        self.assertEquals(content_encoding, EncodingResource.out_encoding)\n+        self.assertEquals(\n+            response.body.decode(content_encoding), to_unicode(original_body))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b905a9aecb0e0e22339353a983257f3707d98b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 153 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -90,8 +90,8 @@ class StartprojectTemplatesTest(ProjectTest):\n         args = ['--set', 'TEMPLATES_DIR=%s' % self.tmpl]\n         p = self.proc('startproject', self.project_name, *args)\n         out = to_native_str(retry_on_eintr(p.stdout.read))\n-        self.assertIn(\"New Scrapy project %r, using template directory %r, created in:\" % \\\n-                      (self.project_name, join(self.tmpl, 'project')), out)\n+        self.assertIn(\"New Scrapy project %r, using template directory\" % self.project_name, out)\n+        self.assertIn(self.tmpl_proj, out)\n         assert exists(join(self.proj_path, 'root_template'))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2e2ff5e1f2eb1d11e18be248e27bef7fc9b4ef7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 326 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,18 +12,26 @@ from scrapy.responsetypes import responsetypes\n \n \n def _parsed_url_args(parsed):\n+    # Assume parsed is urlparse-d from Request.url,\n+    # which was passed via safe_url_string and is ascii-only.\n+    b = lambda s: to_bytes(s, encoding='ascii')\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n-    path = to_bytes(path)\n-    host = to_bytes(parsed.hostname)\n+    path = b(path)\n+    host = b(parsed.hostname)\n     port = parsed.port\n-    scheme = to_bytes(parsed.scheme, encoding='ascii')\n-    netloc = to_bytes(parsed.netloc)\n+    scheme = b(parsed.scheme)\n+    netloc = b(parsed.netloc)\n     if port is None:\n         port = 443 if scheme == b'https' else 80\n     return scheme, netloc, host, port, path\n \n \n def _parse(url):\n+    \"\"\" Return tuple of (scheme, netloc, host, port, path),\n+    all in bytes except for port which is int.\n+    Assume url is from Request.url, which was passed via safe_url_string\n+    and is ascii-only.\n+    \"\"\"\n     url = url.strip()\n     parsed = urlparse(url)\n     return _parsed_url_args(parsed)\n@@ -95,7 +103,7 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     def __init__(self, request, timeout=180):\n         self._url = urldefrag(request.url)[0]\n         # converting to bytes to comply to Twisted interface\n-        self.url = to_bytes(self._url)\n+        self.url = to_bytes(self._url, encoding='ascii')\n         self.method = to_bytes(request.method, encoding='ascii')\n         self.body = request.body or None\n         self.headers = Headers(request.headers)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#04f69fd18406a9361f3c13df32b5f6ea295ecdbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 290 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -119,7 +119,7 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"broken\", BrokenDownloadResource())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n-        self.host = '127.0.0.1'\n+        self.host = 'localhost'\n         if self.scheme == 'https':\n             self.port = reactor.listenSSL(\n                 0, self.wrapper, ssl_context_factory(), interface=self.host)\n@@ -273,6 +273,10 @@ class Http11TestCase(HttpTestCase):\n         return d\n \n \n+class Https11TestCase(Http11TestCase):\n+    scheme = 'https'\n+\n+\n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n     if twisted_version < (11, 1, 0):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98c060d0b2cc76934e16abc03a033f21850fd565", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 298 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -168,6 +168,9 @@ class HttpTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_download_from_spider(self):\n+        if self.scheme == 'https':\n+            raise unittest.SkipTest(\n+                'test_timeout_download_from_spider skipped under https')\n         spider = Spider('foo')\n         meta = {'download_timeout': 0.2}\n         # client connects but no data is received\n@@ -181,7 +184,8 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_host_header_not_in_request_headers(self):\n         def _test(response):\n-            self.assertEquals(response.body, to_bytes('127.0.0.1:%d' % self.portno))\n+            self.assertEquals(\n+                response.body, to_bytes('%s:%d' % (self.host, self.portno)))\n             self.assertEquals(request.headers, {})\n \n         request = Request(self.getURL('host'))\n@@ -221,8 +225,6 @@ class Http10TestCase(HttpTestCase):\n \n class Https10TestCase(Http10TestCase):\n     scheme = 'https'\n-    def test_timeout_download_from_spider(self):\n-        raise unittest.SkipTest(\"test_timeout_download_from_spider skipped under https\")\n \n \n class Http11TestCase(HttpTestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f527849f2e8eddaf5d756b061699f2eca522a18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 1543 | Contributors (this commit): 15 | Commits (past 90d): 20 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,7 +78,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n     \"\"\"\n \n-    _responseMatcher = re.compile('HTTP/1\\.. 200')\n+    _responseMatcher = re.compile(b'HTTP/1\\.. 200')\n \n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):\n@@ -92,11 +92,15 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n-                                                  self._tunneledPort)\n+        tunnelReq = (\n+            b'CONNECT ' +\n+            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n+            to_bytes(str(self._tunneledPort)) +\n+            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n-            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n-        tunnelReq += '\\r\\n'\n+            tunnelReq += \\\n+                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n+        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse\n\n@@ -388,6 +388,16 @@ class HttpProxyTestCase(unittest.TestCase):\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n+    @defer.inlineCallbacks\n+    def test_download_with_proxy_https_timeout(self):\n+        http_proxy = self.getURL('')\n+        domain = 'https://no-such-domain.nosuch'\n+        request = Request(\n+            domain, meta={'proxy': http_proxy, 'download_timeout': 0.2})\n+        d = self.download_request(request, Spider('foo'))\n+        timeout = yield self.assertFailure(d, error.TimeoutError)\n+        self.assertIn(domain, timeout.osError)\n+\n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7af64e8fd2a90102d85ff0f453cd6ba1dde71caa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1244 | Contributors (this commit): 12 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -206,7 +206,7 @@ class ScrapyAgent(object):\n         agent = self._get_agent(request, timeout)\n \n         # request details\n-        url = to_bytes(urldefrag(request.url)[0])\n+        url = urldefrag(request.url)[0]\n         method = to_bytes(request.method)\n         headers = TxHeaders(request.headers)\n         if isinstance(agent, self._TunnelingAgent):\n@@ -214,7 +214,8 @@ class ScrapyAgent(object):\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n \n         start_time = time()\n-        d = agent.request(method, url, headers, bodyproducer)\n+        d = agent.request(\n+            method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n         # response body is ready to be consumed\n@@ -267,10 +268,8 @@ class ScrapyAgent(object):\n         txresponse, body, flags = result\n         status = int(txresponse.code)\n         headers = Headers(txresponse.headers.getAllRawHeaders())\n-        url = to_unicode(url)\n         respcls = responsetypes.from_args(headers=headers, url=url)\n-        return respcls(\n-            url=url, status=status, headers=headers, body=body, flags=flags)\n+        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n \n \n @implementer(IBodyProducer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b940606b7e3dacbf5d639965c54ccbb87a214c95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 329 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -388,16 +388,6 @@ class HttpProxyTestCase(unittest.TestCase):\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n-    @defer.inlineCallbacks\n-    def test_download_with_proxy_https_timeout(self):\n-        http_proxy = self.getURL('')\n-        domain = 'https://no-such-domain.nosuch'\n-        request = Request(\n-            domain, meta={'proxy': http_proxy, 'download_timeout': 0.2})\n-        d = self.download_request(request, Spider('foo'))\n-        timeout = yield self.assertFailure(d, error.TimeoutError)\n-        self.assertIn(domain, timeout.osError)\n-\n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEquals(response.status, 200)\n@@ -422,6 +412,17 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n     if twisted_version < (11, 1, 0):\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n+    @defer.inlineCallbacks\n+    def test_download_with_proxy_https_timeout(self):\n+        \"\"\" Test TunnelingTCP4ClientEndpoint \"\"\"\n+        http_proxy = self.getURL('')\n+        domain = 'https://no-such-domain.nosuch'\n+        request = Request(\n+            domain, meta={'proxy': http_proxy, 'download_timeout': 0.2})\n+        d = self.download_request(request, Spider('foo'))\n+        timeout = yield self.assertFailure(d, error.TimeoutError)\n+        self.assertIn(domain, timeout.osError)\n+\n \n class HttpDownloadHandlerMock(object):\n     def __init__(self, settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2efd389b09984de15ee0653d169c0bbbfd61a05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 211 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -18,14 +18,14 @@ from scrapy.http import Request, Headers\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n-def getPage(url, contextFactory=None, r_transform=None, *args, **kwargs):\n+def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n     \"\"\"Adapted version of twisted.web.client.getPage\"\"\"\n     def _clientfactory(url, *args, **kwargs):\n         url = to_unicode(url)\n         timeout = kwargs.pop('timeout', 0)\n         f = client.ScrapyHTTPClientFactory(\n             Request(url, *args, **kwargs), timeout=timeout)\n-        f.deferred.addCallback(r_transform or (lambda r: r.body))\n+        f.deferred.addCallback(response_transform or (lambda r: r.body))\n         return f\n \n     from twisted.web.client import _makeGetterFactory\n@@ -355,7 +355,7 @@ class WebClientTestCase(unittest.TestCase):\n         Content-Encoding header \"\"\"\n         body = b'\\xd0\\x81\\xd1\\x8e\\xd0\\xaf'\n         return getPage(\n-            self.getURL('encoding'), body=body, r_transform=lambda r: r)\\\n+            self.getURL('encoding'), body=body, response_transform=lambda r: r)\\\n             .addCallback(self._check_Encoding, body)\n \n     def _check_Encoding(self, response, original_body):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#494643458270311341c509f5476c61737aa27a70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 230 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,16 +78,17 @@ class ParseUrlTestCase(unittest.TestCase):\n         elements of its return tuple, even when passed an URL which has\n         previously been passed to L{urlparse} as a C{unicode} string.\n         \"\"\"\n-        goodInput = u'http://example.com/path'\n-        badInput = goodInput.encode('ascii')\n-        if six.PY2:\n-            goodInput, badInput = badInput, goodInput\n-        urlparse(badInput)\n+        if not six.PY2:\n+            raise unittest.SkipTest(\n+                \"Applies only to Py2, as urls can be ONLY unicode on Py3\")\n+        badInput = u'http://example.com/path'\n+        goodInput = badInput.encode('ascii')\n+        self._parse(badInput)  # cache badInput in urlparse_cached\n         scheme, netloc, host, port, path = self._parse(goodInput)\n-        self.assertTrue(isinstance(scheme, bytes))\n-        self.assertTrue(isinstance(netloc, bytes))\n-        self.assertTrue(isinstance(host, bytes))\n-        self.assertTrue(isinstance(path, bytes))\n+        self.assertTrue(isinstance(scheme, str))\n+        self.assertTrue(isinstance(netloc, str))\n+        self.assertTrue(isinstance(host, str))\n+        self.assertTrue(isinstance(path, str))\n         self.assertTrue(isinstance(port, int))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b9336418ef40ca95052ebbaa02f12953e165115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 28 | Churn Cumulative: 34 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -39,10 +39,10 @@ class HttpCompressionMiddleware(object):\n         return response\n \n     def _decode(self, body, encoding):\n-        if encoding == 'gzip' or encoding == 'x-gzip':\n+        if encoding == b'gzip' or encoding == b'x-gzip':\n             body = gunzip(body)\n \n-        if encoding == 'deflate':\n+        if encoding == b'deflate':\n             try:\n                 body = zlib.decompress(body)\n             except zlib.error:\n\n@@ -50,46 +50,46 @@ class HttpCompressionTest(TestCase):\n         request = Request('http://scrapytest.org')\n         assert 'Accept-Encoding' not in request.headers\n         self.mw.process_request(request, self.spider)\n-        self.assertEqual(request.headers.get('Accept-Encoding'), 'gzip,deflate')\n+        self.assertEqual(request.headers.get('Accept-Encoding'), b'gzip,deflate')\n \n     def test_process_response_gzip(self):\n         response = self._getresponse('gzip')\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], 'gzip')\n+        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        assert newresponse.body.startswith('<!DOCTYPE')\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n \n     def test_process_response_rawdeflate(self):\n         response = self._getresponse('rawdeflate')\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], 'deflate')\n+        self.assertEqual(response.headers['Content-Encoding'], b'deflate')\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        assert newresponse.body.startswith('<!DOCTYPE')\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n \n     def test_process_response_zlibdelate(self):\n         response = self._getresponse('zlibdeflate')\n         request = response.request\n \n-        self.assertEqual(response.headers['Content-Encoding'], 'deflate')\n+        self.assertEqual(response.headers['Content-Encoding'], b'deflate')\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        assert newresponse.body.startswith('<!DOCTYPE')\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n \n     def test_process_response_plain(self):\n-        response = Response('http://scrapytest.org', body='<!DOCTYPE...')\n+        response = Response('http://scrapytest.org', body=b'<!DOCTYPE...')\n         request = Request('http://scrapytest.org')\n \n         assert not response.headers.get('Content-Encoding')\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is response\n-        assert newresponse.body.startswith('<!DOCTYPE')\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n \n     def test_multipleencodings(self):\n         response = self._getresponse('gzip')\n@@ -97,7 +97,7 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is not response\n-        self.assertEqual(newresponse.headers.getlist('Content-Encoding'), ['uuencode'])\n+        self.assertEqual(newresponse.headers.getlist('Content-Encoding'), [b'uuencode'])\n \n     def test_process_response_encoding_inside_body(self):\n         headers = {\n@@ -142,5 +142,5 @@ class HttpCompressionTest(TestCase):\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIs(newresponse, response)\n-        self.assertEqual(response.headers['Content-Encoding'], 'gzip')\n-        self.assertEqual(response.headers['Content-Type'], 'application/gzip')\n+        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n+        self.assertEqual(response.headers['Content-Type'], b'application/gzip')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bcbad2905d3ff375e85ede723d13560d765ee729", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 22 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n         # Trigger Twisted bug #7989\n         import twisted.persisted.styles  # NOQA\n         q = self.queue()\n-        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)\n+        self.assertRaises(ValueError, q.push, lambda x: x)\n \n class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 1\n@@ -114,7 +114,7 @@ class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest):\n         # Trigger Twisted bug #7989\n         import twisted.persisted.styles  # NOQA\n         q = self.queue()\n-        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)\n+        self.assertRaises(ValueError, q.push, lambda x: x)\n \n \n class PickleLifoDiskQueueTest(MarshalLifoDiskQueueTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#120fb4adeb957a348f01bf60e821b2dcab2f9de1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 222 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ class Response(object_ref):\n         return self._url\n \n     def _set_url(self, url):\n-        if isinstance(url, six.string_types):\n+        if isinstance(url, str):\n             self._url = url\n         else:\n             raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fdd3225b293d4951ba079e1141683e7fc55f905", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 335 | Contributors (this commit): 7 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -322,18 +322,18 @@ class Http11MockServerTestCase(unittest.TestCase):\n             # download_maxsize < 100, hence the CancelledError\n             self.assertIsInstance(failure.value, defer.CancelledError)\n \n+            if six.PY2:\n                 request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n                 request = request.replace(url='http://localhost:8998/xpayload')\n                 yield crawler.crawl(seed=request)\n-\n-            if six.PY2:\n                 # download_maxsize = 50 is enough for the gzipped response\n                 # See issue https://twistedmatrix.com/trac/ticket/8175\n-                raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n                 failure = crawler.spider.meta.get('failure')\n                 self.assertTrue(failure == None)\n                 reason = crawler.spider.meta['close_reason']\n                 self.assertTrue(reason, 'finished')\n+            else:\n+                raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n         else:\n             raise unittest.SkipTest(\"xpayload and payload endpoint only enabled for twisted > 12.3.0\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
