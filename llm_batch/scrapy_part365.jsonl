{"custom_id": "scrapy#b4acf5c827b710fac83fa09fc5832ac76041e5c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 53 | Files Changed: 18 | Hunks: 53 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 116 | Churn Cumulative: 18838 | Contributors (this commit): 89 | Commits (past 90d): 29 | Contributors (cumulative): 278 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ import sys\n from functools import wraps\n from inspect import getmembers\n from types import CoroutineType\n-from typing import AsyncGenerator, Dict\n+from typing import AsyncGenerator, Dict, Optional, Type\n from unittest import TestCase\n \n from scrapy.http import Request\n@@ -14,7 +14,7 @@ from scrapy.utils.spider import iterate_spider_output\n class Contract:\n     \"\"\"Abstract class for contracts\"\"\"\n \n-    request_cls = None\n+    request_cls: Optional[Type[Request]] = None\n \n     def __init__(self, method, *args):\n         self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n\n@@ -78,7 +78,7 @@ class ItemLoader(itemloaders.ItemLoader):\n         read-only.\n     \"\"\"\n \n-    default_item_class = Item\n+    default_item_class: type = Item\n     default_selector_class = Selector\n \n     def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n\n@@ -8,6 +8,7 @@ import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n+from typing import Dict, Tuple\n \n from itemadapter import ItemAdapter\n \n@@ -48,7 +49,7 @@ class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n     EXPIRES = 90\n-    THUMBS = {}\n+    THUMBS: Dict[str, Tuple[int, int]] = {}\n     DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n     DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n \n\n@@ -11,7 +11,7 @@ from twisted.python.failure import Failure\n \n \n class ProcessTest:\n-    command = None\n+    command: Optional[str] = None\n     prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n     cwd = os.getcwd()  # trial chdirs to temp dir\n \n\n@@ -1,5 +1,6 @@\n \"\"\"DBM-like dummy module\"\"\"\n import collections\n+from typing import Any, DefaultDict\n \n \n class DummyDB(dict):\n@@ -12,7 +13,7 @@ class DummyDB(dict):\n error = KeyError\n \n \n-_DATABASES = collections.defaultdict(DummyDB)\n+_DATABASES: DefaultDict[Any, DummyDB] = collections.defaultdict(DummyDB)\n \n \n def open(file, flag=\"r\", mode=0o666):\n\n@@ -7,6 +7,7 @@ import tempfile\n import unittest\n from datetime import datetime\n from io import BytesIO\n+from typing import Any\n \n import lxml.etree\n from itemadapter import ItemAdapter\n@@ -53,8 +54,8 @@ class CustomFieldDataclass:\n \n \n class BaseItemExporterTest(unittest.TestCase):\n-    item_class = TestItem\n-    custom_field_item_class = CustomFieldItem\n+    item_class: type = TestItem\n+    custom_field_item_class: type = CustomFieldItem\n \n     def setUp(self):\n         self.i = self.item_class(name=\"John\\xa3\", age=\"22\")\n@@ -517,7 +518,7 @@ class XmlItemExporterDataclassTest(XmlItemExporterTest):\n \n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n-    _expected_nested = {\n+    _expected_nested: Any = {\n         \"name\": \"Jesus\",\n         \"age\": {\"name\": \"Maria\", \"age\": {\"name\": \"Joseph\", \"age\": \"22\"}},\n     }\n@@ -665,7 +666,7 @@ class JsonItemExporterDataclassTest(JsonItemExporterTest):\n \n \n class CustomExporterItemTest(unittest.TestCase):\n-    item_class = TestItem\n+    item_class: type = TestItem\n \n     def setUp(self):\n         if self.item_class is None:\n\n@@ -3,6 +3,7 @@ import re\n import unittest\n import warnings\n import xmlrpc.client\n+from typing import Any, Dict, List\n from unittest import mock\n from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n \n@@ -21,8 +22,8 @@ from scrapy.utils.python import to_bytes, to_unicode\n class RequestTest(unittest.TestCase):\n     request_class = Request\n     default_method = \"GET\"\n-    default_headers = {}\n-    default_meta = {}\n+    default_headers: Dict[bytes, List[bytes]] = {}\n+    default_meta: Dict[str, Any] = {}\n \n     def test_init(self):\n         # Request requires url in the __init__ method\n\n@@ -1,6 +1,7 @@\n import pickle\n import re\n import unittest\n+from typing import Optional\n \n from packaging.version import Version\n from pytest import mark\n@@ -15,7 +16,7 @@ from tests import get_testdata\n # a hack to skip base class tests in pytest\n class Base:\n     class LinkExtractorTestCase(unittest.TestCase):\n-        extractor_cls = None\n+        extractor_cls: Optional[type] = None\n \n         def setUp(self):\n             body = get_testdata(\"link_extractor\", \"linkextractor.html\")\n\n@@ -1,5 +1,6 @@\n import dataclasses\n import unittest\n+from typing import Optional\n \n import attr\n from itemadapter import ItemAdapter\n@@ -87,7 +88,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n \n class InitializationTestMixin:\n-    item_class = None\n+    item_class: Optional[type] = None\n \n     def test_keep_single_value(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n\n@@ -1,5 +1,6 @@\n import shutil\n from pathlib import Path\n+from typing import Optional, Set\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -54,7 +55,7 @@ class FileDownloadCrawlTestCase(TestCase):\n     store_setting_key = \"FILES_STORE\"\n     media_key = \"files\"\n     media_urls_key = \"file_urls\"\n-    expected_checksums = {\n+    expected_checksums: Optional[Set[str]] = {\n         \"5547178b89448faf0015a13f904c936e\",\n         \"c2281c83670e31d8aaab7cb642b824db\",\n         \"ed3f6538dc15d4d9179dae57319edc5f\",\n@@ -193,6 +194,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         )\n \n \n+skip_pillow: Optional[str]\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n\n@@ -7,6 +7,7 @@ from io import BytesIO\n from pathlib import Path\n from shutil import rmtree\n from tempfile import mkdtemp\n+from typing import Dict, List\n from unittest import mock\n from urllib.parse import urlparse\n \n@@ -308,11 +309,11 @@ class FilesPipelineTestCaseFieldsDataClass(\n class FilesPipelineTestAttrsItem:\n     name = attr.ib(default=\"\")\n     # default fields\n-    file_urls = attr.ib(default=lambda: [])\n-    files = attr.ib(default=lambda: [])\n+    file_urls: List[str] = attr.ib(default=lambda: [])\n+    files: List[Dict[str, str]] = attr.ib(default=lambda: [])\n     # overridden fields\n-    custom_file_urls = attr.ib(default=lambda: [])\n-    custom_files = attr.ib(default=lambda: [])\n+    custom_file_urls: List[str] = attr.ib(default=lambda: [])\n+    custom_files: List[Dict[str, str]] = attr.ib(default=lambda: [])\n \n \n class FilesPipelineTestCaseFieldsAttrsItem(\n@@ -690,7 +691,3 @@ def _prepare_request_object(item_url, flags=None):\n         item_url,\n         meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n     )\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -5,6 +5,7 @@ import random\n import warnings\n from shutil import rmtree\n from tempfile import mkdtemp\n+from typing import Dict, List, Optional\n from unittest.mock import patch\n \n import attr\n@@ -18,6 +19,7 @@ from scrapy.pipelines.images import ImageException, ImagesPipeline, NoimagesDrop\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n+skip_pillow: Optional[str]\n try:\n     from PIL import Image\n except ImportError:\n@@ -26,7 +28,7 @@ except ImportError:\n     )\n else:\n     encoders = {\"jpeg_encoder\", \"jpeg_decoder\"}\n-    if not encoders.issubset(set(Image.core.__dict__)):\n+    if not encoders.issubset(set(Image.core.__dict__)):  # type: ignore[attr-defined]\n         skip_pillow = \"Missing JPEG encoders\"\n     else:\n         skip_pillow = None\n@@ -404,11 +406,11 @@ class ImagesPipelineTestCaseFieldsDataClass(\n class ImagesPipelineTestAttrsItem:\n     name = attr.ib(default=\"\")\n     # default fields\n-    image_urls = attr.ib(default=lambda: [])\n-    images = attr.ib(default=lambda: [])\n+    image_urls: List[str] = attr.ib(default=lambda: [])\n+    images: List[Dict[str, str]] = attr.ib(default=lambda: [])\n     # overridden fields\n-    custom_image_urls = attr.ib(default=lambda: [])\n-    custom_images = attr.ib(default=lambda: [])\n+    custom_image_urls: List[str] = attr.ib(default=lambda: [])\n+    custom_images: List[Dict[str, str]] = attr.ib(default=lambda: [])\n \n \n class ImagesPipelineTestCaseFieldsAttrsItem(\n@@ -646,7 +648,3 @@ def _create_image(format, *a, **kw):\n     Image.new(*a, **kw).save(buf, format)\n     buf.seek(0)\n     return Image.open(buf), buf\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,3 +1,5 @@\n+from typing import List\n+\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -62,7 +64,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         },\n     }\n \n-    checks = []\n+    checks: List[bool] = []\n \n     def start_requests(self):\n         data = {\"key\": \"value\", \"number\": 123, \"callback\": \"some_callback\"}\n\n@@ -2,6 +2,7 @@ import collections\n import shutil\n import tempfile\n import unittest\n+from typing import Optional\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -59,7 +60,7 @@ class MockCrawler(Crawler):\n \n \n class SchedulerHandler:\n-    priority_queue_cls = None\n+    priority_queue_cls: Optional[str] = None\n     jobdir = None\n \n     def create_scheduler(self):\n@@ -253,7 +254,7 @@ def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n \n \n class DownloaderAwareSchedulerTestMixin:\n-    priority_queue_cls = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n+    priority_queue_cls: Optional[str] = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n     reopen = False\n \n     def test_logic(self):\n\n@@ -1,4 +1,5 @@\n import logging\n+from typing import Set\n from unittest import TestCase\n \n from testfixtures import LogCapture\n@@ -16,7 +17,7 @@ from tests.spiders import MockServerSpider\n \n class _HttpErrorSpider(MockServerSpider):\n     name = \"httperror\"\n-    bypass_status_codes = set()\n+    bypass_status_codes: Set[int] = set()\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n\n@@ -1,4 +1,5 @@\n import warnings\n+from typing import Any, Dict, List, Optional, Tuple\n from unittest import TestCase\n from urllib.parse import urlparse\n \n@@ -31,10 +32,10 @@ from scrapy.spiders import Spider\n \n \n class TestRefererMiddleware(TestCase):\n-    req_meta = {}\n-    resp_headers = {}\n-    settings = {}\n-    scenarii = [\n+    req_meta: Dict[str, Any] = {}\n+    resp_headers: Dict[str, str] = {}\n+    settings: Dict[str, Any] = {}\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n     ]\n \n@@ -64,7 +65,7 @@ class MixinDefault:\n     with some additional filtering of s3://\n     \"\"\"\n \n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         (\"https://example.com/\", \"https://scrapy.org/\", b\"https://example.com/\"),\n         (\"http://example.com/\", \"http://scrapy.org/\", b\"http://example.com/\"),\n         (\"http://example.com/\", \"https://scrapy.org/\", b\"http://example.com/\"),\n@@ -85,7 +86,7 @@ class MixinDefault:\n \n \n class MixinNoReferrer:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         (\"https://example.com/page.html\", \"https://example.com/\", None),\n         (\"http://www.example.com/\", \"https://scrapy.org/\", None),\n         (\"http://www.example.com/\", \"http://scrapy.org/\", None),\n@@ -95,7 +96,7 @@ class MixinNoReferrer:\n \n \n class MixinNoReferrerWhenDowngrade:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # TLS to TLS: send non-empty referrer\n         (\n             \"https://example.com/page.html\",\n@@ -177,7 +178,7 @@ class MixinNoReferrerWhenDowngrade:\n \n \n class MixinSameOrigin:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -246,7 +247,7 @@ class MixinSameOrigin:\n \n \n class MixinOrigin:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n         (\n             \"https://example.com/page.html\",\n@@ -270,7 +271,7 @@ class MixinOrigin:\n \n \n class MixinStrictOrigin:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n         (\n             \"https://example.com/page.html\",\n@@ -298,7 +299,7 @@ class MixinStrictOrigin:\n \n \n class MixinOriginWhenCrossOrigin:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -405,7 +406,7 @@ class MixinOriginWhenCrossOrigin:\n \n \n class MixinStrictOriginWhenCrossOrigin:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -517,7 +518,7 @@ class MixinStrictOriginWhenCrossOrigin:\n \n \n class MixinUnsafeUrl:\n-    scenarii = [\n+    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n         # TLS to TLS: send referrer\n         (\n             \"https://example.com/sekrit.html\",\n@@ -920,7 +921,9 @@ class TestPolicyHeaderPrecedence004(\n \n class TestReferrerOnRedirect(TestRefererMiddleware):\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n-    scenarii = [\n+    scenarii: List[\n+        Tuple[str, str, Tuple[Tuple[int, str], ...], Optional[bytes], Optional[bytes]]\n+    ] = [  # type: ignore[assignment]\n         (\n             \"http://scrapytest.org/1\",  # parent\n             \"http://scrapytest.org/2\",  # target\n\n@@ -58,7 +58,7 @@ class JsonEncoderTestCase(unittest.TestCase):\n         self.assertIn(r.url, rs)\n         self.assertIn(str(r.status), rs)\n \n-    def test_encode_dataclass_item(self):\n+    def test_encode_dataclass_item(self) -> None:\n         @dataclasses.dataclass\n         class TestDataClass:\n             name: str\n\n@@ -364,7 +364,7 @@ for k, args in enumerate(\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n # TODO: the following tests do not pass with current implementation\n-for k, args in enumerate(\n+for k, skip_args in enumerate(\n     [\n         (\n             r\"C:\\absolute\\path\\to\\a\\file.html\",\n@@ -374,7 +374,7 @@ for k, args in enumerate(\n     ],\n     start=1,\n ):\n-    t_method = create_skipped_scheme_t(args)\n+    t_method = create_skipped_scheme_t(skip_args)\n     t_method.__name__ = f\"test_uri_skipped_{k:03}\"\n     setattr(GuessSchemeTest, t_method.__name__, t_method)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6cee787dd45fabba3f39dbb1752baeef649f5b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 20 | Files Changed: 5 | Hunks: 26 | Methods Changed: 20 | Complexity Δ (Sum/Max): 8/3 | Churn Δ: 101 | Churn Cumulative: 2184 | Contributors (this commit): 37 | Commits (past 90d): 18 | Contributors (cumulative): 79 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import io\n import zlib\n-from typing import TYPE_CHECKING, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n from scrapy import Request, Spider\n from scrapy.crawler import Crawler\n@@ -74,12 +74,12 @@ class HttpCompressionMiddleware:\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n-                kwargs = dict(cls=respcls, body=decoded_body)\n+                kwargs: Dict[str, Any] = dict(body=decoded_body)\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\n                     kwargs[\"encoding\"] = None\n-                response = response.replace(**kwargs)\n+                response = response.replace(cls=respcls, **kwargs)\n                 if not content_encoding:\n                     del response.headers[\"Content-Encoding\"]\n \n\n@@ -27,6 +27,7 @@ def _build_redirect_request(\n     redirect_request = source_request.replace(\n         url=url,\n         **kwargs,\n+        cls=None,\n         cookies=None,\n     )\n     if \"Cookie\" in redirect_request.headers:\n\n@@ -4,8 +4,11 @@ requests in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+from __future__ import annotations\n+\n import inspect\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AnyStr,\n     Callable,\n@@ -19,7 +22,7 @@ from typing import (\n     Type,\n     TypeVar,\n     Union,\n-    cast,\n+    overload,\n )\n \n from w3lib.url import safe_url_string\n@@ -31,6 +34,11 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n \n@@ -173,23 +181,36 @@ class Request(object_ref):\n     def __repr__(self) -> str:\n         return f\"<{self.method} {self.url}>\"\n \n-    def copy(self) -> \"Request\":\n+    def copy(self) -> Self:\n         return self.replace()\n \n-    def replace(self, *args: Any, **kwargs: Any) -> \"Request\":\n+    @overload\n+    def replace(\n+        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n+    ) -> RequestTypeVar:\n+        ...\n+\n+    @overload\n+    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self:\n+        ...\n+\n+    def replace(\n+        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n+    ) -> Request:\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n-        cls = kwargs.pop(\"cls\", self.__class__)\n-        return cast(Request, cls(*args, **kwargs))\n+        if cls is None:\n+            cls = self.__class__\n+        return cls(*args, **kwargs)\n \n     @classmethod\n     def from_curl(\n-        cls: Type[RequestTypeVar],\n+        cls,\n         curl_command: str,\n         ignore_unknown_options: bool = True,\n         **kwargs: Any,\n-    ) -> RequestTypeVar:\n+    ) -> Self:\n         \"\"\"Create a Request object from a string containing a `cURL\n         <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n         URL, the headers, the cookies and the body. It accepts the same\n@@ -221,7 +242,7 @@ class Request(object_ref):\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n \n-    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> Dict[str, Any]:\n+    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> Dict[str, Any]:\n         \"\"\"Return a dictionary containing the Request's data.\n \n         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n@@ -5,12 +5,18 @@ This module implements the JsonRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import copy\n import json\n import warnings\n-from typing import Any, Optional, Tuple\n+from typing import TYPE_CHECKING, Any, Optional, Tuple, Type, overload\n \n-from scrapy.http.request import Request\n+from scrapy.http.request import Request, RequestTypeVar\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class JsonRequest(Request):\n@@ -44,7 +50,19 @@ class JsonRequest(Request):\n     def dumps_kwargs(self) -> dict:\n         return self._dumps_kwargs\n \n-    def replace(self, *args: Any, **kwargs: Any) -> Request:\n+    @overload\n+    def replace(\n+        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n+    ) -> RequestTypeVar:\n+        ...\n+\n+    @overload\n+    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self:\n+        ...\n+\n+    def replace(\n+        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n+    ) -> Request:\n         body_passed = kwargs.get(\"body\", None) is not None\n         data = kwargs.pop(\"data\", None)\n         data_passed = data is not None\n@@ -54,7 +72,7 @@ class JsonRequest(Request):\n         elif not body_passed and data_passed:\n             kwargs[\"body\"] = self._dumps(data)\n \n-        return super().replace(*args, **kwargs)\n+        return super().replace(*args, cls=cls, **kwargs)\n \n     def _dumps(self, data: dict) -> str:\n         \"\"\"Convert to JSON\"\"\"\n\n@@ -19,8 +19,10 @@ from typing import (\n     Mapping,\n     Optional,\n     Tuple,\n+    Type,\n+    TypeVar,\n     Union,\n-    cast,\n+    overload,\n )\n from urllib.parse import urljoin\n \n@@ -33,9 +35,15 @@ from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n \n if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n     from scrapy.selector import SelectorList\n \n \n+ResponseTypeVar = TypeVar(\"ResponseTypeVar\", bound=\"Response\")\n+\n+\n class Response(object_ref):\n     \"\"\"An object that represents an HTTP response, which is usually\n     downloaded (by the Downloader) and fed to the Spiders for processing.\n@@ -132,16 +140,29 @@ class Response(object_ref):\n     def __repr__(self) -> str:\n         return f\"<{self.status} {self.url}>\"\n \n-    def copy(self) -> Response:\n+    def copy(self) -> Self:\n         \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n \n-    def replace(self, *args: Any, **kwargs: Any) -> Response:\n+    @overload\n+    def replace(\n+        self, *args: Any, cls: Type[ResponseTypeVar], **kwargs: Any\n+    ) -> ResponseTypeVar:\n+        ...\n+\n+    @overload\n+    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self:\n+        ...\n+\n+    def replace(\n+        self, *args: Any, cls: Optional[Type[Response]] = None, **kwargs: Any\n+    ) -> Response:\n         \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n-        cls = kwargs.pop(\"cls\", self.__class__)\n-        return cast(Response, cls(*args, **kwargs))\n+        if cls is None:\n+            cls = self.__class__\n+        return cls(*args, **kwargs)\n \n     def urljoin(self, url: str) -> str:\n         \"\"\"Join this Response's url with a possible relative url to form an\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5fccf370b87378fe2db6bdd52b98c1e2a951df3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 421 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -276,7 +276,7 @@ coverage_ignore_pyobjects = [\n \n intersphinx_mapping = {\n     \"attrs\": (\"https://www.attrs.org/en/stable/\", None),\n-    \"coverage\": (\"https://coverage.readthedocs.io/en/stable\", None),\n+    \"coverage\": (\"https://coverage.readthedocs.io/en/latest\", None),\n     \"cryptography\": (\"https://cryptography.io/en/latest/\", None),\n     \"cssselect\": (\"https://cssselect.readthedocs.io/en/latest\", None),\n     \"itemloaders\": (\"https://itemloaders.readthedocs.io/en/latest/\", None),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#080fecd8900b6b1f94e8e143e90338279ba8d6e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 39 | Churn Cumulative: 926 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 34 | DMM Complexity: 0.48484848484848486\n\nDIFF:\n@@ -17,11 +17,17 @@ def _build_redirect_request(source_request, *, url, **kwargs):\n         **kwargs,\n         cookies=None,\n     )\n-    if \"Cookie\" in redirect_request.headers:\n+    has_cookie_header = \"Cookie\" in redirect_request.headers\n+    has_authorization_header = \"Authorization\" in redirect_request.headers\n+    if has_cookie_header or has_authorization_header:\n         source_request_netloc = urlparse_cached(source_request).netloc\n         redirect_request_netloc = urlparse_cached(redirect_request).netloc\n         if source_request_netloc != redirect_request_netloc:\n+            if has_cookie_header:\n                 del redirect_request.headers[\"Cookie\"]\n+            # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name\n+            if has_authorization_header:\n+                del redirect_request.headers[\"Authorization\"]\n     return redirect_request\n \n \n\n@@ -247,6 +247,37 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"\n         self.assertEqual(perc_encoded_utf8_url, req_result.url)\n \n+    def test_cross_domain_header_dropping(self):\n+        safe_headers = {\"A\": \"B\"}\n+        original_request = Request(\n+            \"https://example.com\",\n+            headers={\"Cookie\": \"a=b\", \"Authorization\": \"a\", **safe_headers},\n+        )\n+\n+        internal_response = Response(\n+            \"https://example.com\",\n+            headers={\"Location\": \"https://example.com/a\"},\n+            status=301,\n+        )\n+        internal_redirect_request = self.mw.process_response(\n+            original_request, internal_response, self.spider\n+        )\n+        self.assertIsInstance(internal_redirect_request, Request)\n+        self.assertEqual(original_request.headers, internal_redirect_request.headers)\n+\n+        external_response = Response(\n+            \"https://example.com\",\n+            headers={\"Location\": \"https://example.org/a\"},\n+            status=301,\n+        )\n+        external_redirect_request = self.mw.process_response(\n+            original_request, external_response, self.spider\n+        )\n+        self.assertIsInstance(external_redirect_request, Request)\n+        self.assertEqual(\n+            safe_headers, external_redirect_request.headers.to_unicode_dict()\n+        )\n+\n \n class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59cfdeaa5c83ca1e65be7220296366c135b7676c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 10 | Churn Cumulative: 11733 | Contributors (this commit): 70 | Commits (past 90d): 22 | Contributors (cumulative): 105 | DMM Complexity: None\n\nDIFF:\n@@ -127,7 +127,7 @@ class FileTestCase(unittest.TestCase):\n         return self.download_request(request, Spider(\"foo\")).addCallback(_test)\n \n     def test_non_existent(self):\n-        request = Request(f\"file://{self.mktemp()}\")\n+        request = Request(path_to_file_uri(self.mktemp()))\n         d = self.download_request(request, Spider(\"foo\"))\n         return self.assertFailure(d, OSError)\n \n\n@@ -125,9 +125,6 @@ class FileFeedStorageTest(unittest.TestCase):\n             path.unlink()\n \n \n-@pytest.mark.skipif(\n-    sys.version_info >= (3, 12), reason=\"pyftpdlib doesn't support Python 3.12 yet\"\n-)\n class FTPFeedStorageTest(unittest.TestCase):\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n\n@@ -1,7 +1,6 @@\n import dataclasses\n import os\n import random\n-import sys\n import time\n from datetime import datetime\n from io import BytesIO\n@@ -12,7 +11,6 @@ from unittest import mock\n from urllib.parse import urlparse\n \n import attr\n-import pytest\n from itemadapter import ItemAdapter\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -648,9 +646,6 @@ class TestGCSFilesStore(unittest.TestCase):\n                     store.bucket.get_blob.assert_called_with(expected_blob_path)\n \n \n-@pytest.mark.skipif(\n-    sys.version_info >= (3, 12), reason=\"pyftpdlib doesn't support Python 3.12 yet\"\n-)\n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#538192916f496eb21846d797a6feff5c05f501cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 17 | Files Changed: 7 | Hunks: 21 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 112 | Churn Cumulative: 6093 | Contributors (this commit): 56 | Commits (past 90d): 54 | Contributors (cumulative): 121 | DMM Complexity: 1.0\n\nDIFF:\n@@ -404,8 +404,8 @@ class CrawlerProcess(CrawlerRunner):\n         :param bool stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n \n-        :param bool install_signal_handlers: whether to install the shutdown\n-            handlers (default: True)\n+        :param bool install_signal_handlers: whether to install the OS signal\n+            handlers from Twisted and Scrapy (default: True)\n         \"\"\"\n         from twisted.internet import reactor\n \n@@ -416,15 +416,17 @@ class CrawlerProcess(CrawlerRunner):\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        if install_signal_handlers:\n-            install_shutdown_handlers(self._signal_shutdown)\n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n-        reactor.run(installSignalHandlers=False)  # blocking call\n+        if install_signal_handlers:\n+            reactor.addSystemEventTrigger(\n+                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n+            )\n+        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n     def _graceful_stop_reactor(self) -> Deferred:\n         d = self.stop()\n\n@@ -19,13 +19,10 @@ def install_shutdown_handlers(\n     function: SignalHandlerT, override_sigint: bool = True\n ) -> None:\n     \"\"\"Install the given function as a signal handler for all common shutdown\n-    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n-    SIGINT handler won't be install if there is already a handler in place\n+    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n+    SIGINT handler won't be installed if there is already a handler in place\n     (e.g. Pdb)\n     \"\"\"\n-    from twisted.internet import reactor\n-\n-    reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n         signal.signal(signal.SIGINT, function)\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import os\n import sys\n-from typing import Iterable, Optional, Tuple, cast\n+from typing import Iterable, List, Optional, Tuple, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n@@ -26,14 +26,15 @@ class ProcessTest:\n         env = os.environ.copy()\n         if settings is not None:\n             env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n+        assert self.command\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n-        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n+        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n         return pp.deferred\n \n     def _process_finished(\n-        self, pp: TestProcessProtocol, cmd: str, check_code: bool\n+        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool\n     ) -> Tuple[int, bytes, bytes]:\n         if pp.exitcode and check_code:\n             msg = f\"process {cmd} exit with code {pp.exitcode}\"\n\n@@ -6,8 +6,7 @@ version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n \n \n install_requires = [\n-    # 23.8.0 incompatibility: https://github.com/scrapy/scrapy/issues/6024\n-    \"Twisted>=18.9.0,<23.8.0\",\n+    \"Twisted>=18.9.0\",\n     \"cryptography>=36.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n\n@@ -0,0 +1,24 @@\n+from twisted.internet.defer import Deferred\n+\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.defer import maybe_deferred_to_future\n+\n+\n+class SleepingSpider(scrapy.Spider):\n+    name = \"sleeping\"\n+\n+    start_urls = [\"data:,;\"]\n+\n+    async def parse(self, response):\n+        from twisted.internet import reactor\n+\n+        d = Deferred()\n+        reactor.callLater(3, d.callback, None)\n+        await maybe_deferred_to_future(d)\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(SleepingSpider)\n+process.start()\n\n@@ -1,11 +1,15 @@\n+import sys\n+from io import BytesIO\n from pathlib import Path\n \n+from pexpect.popen_spawn import PopenSpawn\n from twisted.internet import defer\n from twisted.trial import unittest\n \n from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.testsite import SiteTest\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n+from tests.mockserver import MockServer\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n@@ -133,3 +137,25 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n         _, _, err = yield self.execute(args, check_code=True)\n         self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)\n+\n+\n+class InteractiveShellTest(unittest.TestCase):\n+    def test_fetch(self):\n+        args = (\n+            sys.executable,\n+            \"-m\",\n+            \"scrapy.cmdline\",\n+            \"shell\",\n+        )\n+        logfile = BytesIO()\n+        p = PopenSpawn(args, timeout=5)\n+        p.logfile_read = logfile\n+        p.expect_exact(\"Available Scrapy objects\")\n+        with MockServer() as mockserver:\n+            p.sendline(f\"fetch('{mockserver.url('/')}')\")\n+            p.sendline(\"type(response)\")\n+            p.expect_exact(\"HtmlResponse\")\n+        p.sendeof()\n+        p.wait()\n+        logfile.seek(0)\n+        self.assertNotIn(\"Traceback\", logfile.read().decode())\n\n@@ -1,13 +1,16 @@\n import logging\n import os\n import platform\n+import signal\n import subprocess\n import sys\n import warnings\n from pathlib import Path\n+from typing import List\n \n import pytest\n from packaging.version import parse as parse_version\n+from pexpect.popen_spawn import PopenSpawn\n from pytest import mark, raises\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -289,9 +292,12 @@ class ScriptRunnerMixin:\n     script_dir: Path\n     cwd = os.getcwd()\n \n-    def run_script(self, script_name: str, *script_args):\n+    def get_script_args(self, script_name: str, *script_args: str) -> List[str]:\n         script_path = self.script_dir / script_name\n-        args = [sys.executable, str(script_path)] + list(script_args)\n+        return [sys.executable, str(script_path)] + list(script_args)\n+\n+    def run_script(self, script_name: str, *script_args: str) -> str:\n+        args = self.get_script_args(script_name, *script_args)\n         p = subprocess.Popen(\n             args,\n             env=get_mockserver_env(),\n@@ -517,6 +523,29 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"The value of FOO is 42\", log)\n \n+    def test_shutdown_graceful(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(sig)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.expect_exact(\"Spider closed (shutdown)\")\n+        p.wait()\n+\n+    def test_shutdown_forced(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(sig)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.kill(sig)\n+        p.expect_exact(\"forcing unclean shutdown\")\n+        p.wait()\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e4fb0bc5fc066136b171ce488599b1ddd64c83a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 589 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,9 @@ class Contract:\n                     cb_result = cb(response, **cb_kwargs)\n                     if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                         raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(iterate_spider_output(cb_result))\n+                    return list(  # pylint: disable=return-in-finally\n+                        iterate_spider_output(cb_result)\n+                    )\n \n             request.callback = wrapper\n \n@@ -68,7 +70,7 @@ class Contract:\n                 else:\n                     results.addSuccess(self.testcase_post)\n                 finally:\n-                    return output\n+                    return output  # pylint: disable=return-in-finally\n \n             request.callback = wrapper\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1045856a50d379d145e514ec9c7aeeed231aefd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 2679 | Contributors (this commit): 25 | Commits (past 90d): 26 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class SleepingSpider(scrapy.Spider):\n         from twisted.internet import reactor\n \n         d = Deferred()\n-        reactor.callLater(3, d.callback, None)\n+        reactor.callLater(int(self.sleep), d.callback, None)\n         await maybe_deferred_to_future(d)\n \n \n\n@@ -1,3 +1,4 @@\n+import os\n import sys\n from io import BytesIO\n from pathlib import Path\n@@ -147,8 +148,10 @@ class InteractiveShellTest(unittest.TestCase):\n             \"scrapy.cmdline\",\n             \"shell\",\n         )\n+        env = os.environ.copy()\n+        env[\"SCRAPY_PYTHON_SHELL\"] = \"python\"\n         logfile = BytesIO()\n-        p = PopenSpawn(args, timeout=5)\n+        p = PopenSpawn(args, env=env, timeout=5)\n         p.logfile_read = logfile\n         p.expect_exact(\"Available Scrapy objects\")\n         with MockServer() as mockserver:\n\n@@ -525,7 +525,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_shutdown_graceful(self):\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n-        args = self.get_script_args(\"sleeping.py\")\n+        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=3\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n@@ -534,14 +534,21 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.expect_exact(\"Spider closed (shutdown)\")\n         p.wait()\n \n+    @defer.inlineCallbacks\n     def test_shutdown_forced(self):\n+        from twisted.internet import reactor\n+\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n-        args = self.get_script_args(\"sleeping.py\")\n+        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=10\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n         p.kill(sig)\n         p.expect_exact(\"shutting down gracefully\")\n+        # sending the second signal too fast often causes problems\n+        d = defer.Deferred()\n+        reactor.callLater(0.1, d.callback, None)\n+        yield d\n         p.kill(sig)\n         p.expect_exact(\"forcing unclean shutdown\")\n         p.wait()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6969041c5f6891a0298d7e68ece762adee1bb222", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 19 | Methods Changed: 10 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 69 | Churn Cumulative: 1251 | Contributors (this commit): 30 | Commits (past 90d): 11 | Contributors (cumulative): 49 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,9 +2,10 @@ import io\n import warnings\n import zlib\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n@@ -29,24 +30,26 @@ class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n-    def __init__(self, stats=None):\n-        self.stats = stats\n+    def __init__(self, crawler=None):\n+        self.stats = crawler.stats\n+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n             raise NotConfigured\n         try:\n-            return cls(stats=crawler.stats)\n+            return cls(crawler=crawler)\n         except TypeError:\n             warnings.warn(\n                 \"HttpCompressionMiddleware subclasses must either modify \"\n-                \"their '__init__' method to support a 'stats' parameter or \"\n-                \"reimplement the 'from_crawler' method.\",\n+                \"their '__init__' method to support a 'crawler' parameter or \"\n+                \"reimplement their 'from_crawler' method.\",\n                 ScrapyDeprecationWarning,\n             )\n             result = cls()\n             result.stats = crawler.stats\n+            result._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n             return result\n \n     def process_request(self, request, spider):\n@@ -59,7 +62,14 @@ class HttpCompressionMiddleware:\n             content_encoding = response.headers.getlist(\"Content-Encoding\")\n             if content_encoding:\n                 encoding = content_encoding.pop()\n+                try:\n                     decoded_body = self._decode(response.body, encoding.lower())\n+                except _DecompressionMaxSizeExceeded:\n+                    raise IgnoreRequest(\n+                        f\"Ignored response {response} because its body \"\n+                        f\"({len(response.body)}B) exceeded DOWNLOAD_MAXSIZE \"\n+                        f\"({self._max_size}B) during decompression.\"\n+                    )\n                 if self.stats:\n                     self.stats.inc_value(\n                         \"httpcompression/response_bytes\",\n@@ -85,7 +95,7 @@ class HttpCompressionMiddleware:\n \n     def _decode(self, body, encoding):\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\n-            body = gunzip(body)\n+            body = gunzip(body, max_size=self._max_size)\n \n         if encoding == b\"deflate\":\n             try:\n\n@@ -0,0 +1,2 @@\n+class _DecompressionMaxSizeExceeded(ValueError):\n+    pass\n\n@@ -5,8 +5,10 @@ from typing import List\n \n from scrapy.http import Response\n \n+from ._compression import _DecompressionMaxSizeExceeded\n \n-def gunzip(data: bytes) -> bytes:\n+\n+def gunzip(data: bytes, max_size: int = 0) -> bytes:\n     \"\"\"Gunzip the given data and return as much data as possible.\n \n     This is resilient to CRC checksum errors.\n@@ -14,10 +16,10 @@ def gunzip(data: bytes) -> bytes:\n     f = GzipFile(fileobj=BytesIO(data))\n     output_list: List[bytes] = []\n     chunk = b\".\"\n+    decompressed_size = 0\n     while chunk:\n         try:\n             chunk = f.read1(8196)\n-            output_list.append(chunk)\n         except (OSError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n@@ -25,6 +27,14 @@ def gunzip(data: bytes) -> bytes:\n             if output_list:\n                 break\n             raise\n+        decompressed_size += len(chunk)\n+        if max_size and decompressed_size > max_size:\n+            raise _DecompressionMaxSizeExceeded(\n+                f\"The number of bytes decompressed so far \"\n+                f\"({decompressed_size}B) exceed the specified maximum \"\n+                f\"({max_size}B).\"\n+            )\n+        output_list.append(chunk)\n     return b\"\".join(output_list)\n \n \n\n@@ -10,7 +10,7 @@ from scrapy.downloadermiddlewares.httpcompression import (\n     ACCEPTED_ENCODINGS,\n     HttpCompressionMiddleware,\n )\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n@@ -35,12 +35,24 @@ FORMAT = {\n         \"html-zstd-streaming-no-content-size.bin\",\n         \"zstd\",\n     ),\n+    **{\n+        f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)\n+        for format_id in (\n+            # \"br\",\n+            \"gzip\",  # 27 988 → 11 511 612\n+            # \"deflate\",\n+            # \"zstd\",\n+        )\n+    },\n }\n \n \n class HttpCompressionTest(TestCase):\n     def setUp(self):\n-        self.crawler = get_crawler(Spider)\n+        settings = {\n+            \"DOWNLOAD_MAXSIZE\": 10_000_000,  # For compression bomb tests.\n+        }\n+        self.crawler = get_crawler(Spider, settings_dict=settings)\n         self.spider = self.crawler._create_spider(\"scrapytest.org\")\n         self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n         self.crawler.stats.open_spider(self.spider)\n@@ -373,6 +385,19 @@ class HttpCompressionTest(TestCase):\n         self.assertStatsEqual(\"httpcompression/response_count\", None)\n         self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n \n+    def _test_compression_bomb(self, compression_id):\n+        response = self._getresponse(f\"bomb-{compression_id}\")\n+        self.assertRaises(\n+            IgnoreRequest,\n+            self.mw.process_response,\n+            response.request,\n+            response,\n+            self.spider,\n+        )\n+\n+    def test_compression_bomb_gzip(self):\n+        self._test_compression_bomb(\"gzip\")\n+\n \n class HttpCompressionSubclassTest(TestCase):\n     def test_init_missing_stats(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
