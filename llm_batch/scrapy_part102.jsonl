{"custom_id": "scrapy#50621b7ef34ec178887144d5f8f55c071721ab93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ class Command(ScrapyCommand):\n     requires_project = True\n \n     def short_desc(self):\n-        return \"Start the Scrapy manager but don't run any spider (idle mode)\"\n+        return \"Start Scrapy in server mode\"\n \n     def run(self, args, opts):\n         queue_class = load_object(settings['SERVICE_QUEUE'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f80ae9af66efe317dcb4c882479fd6110b241839", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 184 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ class ScrapyService(Service):\n                 self.spawn_process(settings_module, i)\n \n     def spawn_process(self, settings_module, position):\n-        args = [sys.executable, '-m', 'scrapy.cmdline', 'start']\n+        args = [sys.executable, '-m', 'scrapy.cmdline', 'runserver']\n         env = os.environ.copy()\n         botname = self.get_bot_name(settings_module)\n         logfile = self.get_log_file(settings_module, position)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0da61321361974941000312d90332e7f09b9d39a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 231 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,20 +48,15 @@ def _print_usage(inside_project):\n             settings['BOT_NAME'])\n     else:\n         print \"Scrapy %s - no active project\\n\" % scrapy.__version__\n-    print \"Usage\"\n-    print \"=====\\n\"\n-    print \"To run a command:\"\n+    print \"Usage:\"\n     print \"  scrapy <command> [options] [args]\\n\"\n-    print \"To get help:\"\n-    print \"  scrapy <command> -h\\n\"\n-    print \"Available commands\"\n-    print \"==================\\n\"\n+    print \"Available commands:\"\n     cmds = _get_commands_dict()\n     for cmdname, cmdclass in sorted(cmds.iteritems()):\n         if inside_project or not cmdclass.requires_project:\n-            print \"%s %s\" % (cmdname, cmdclass.syntax())\n-            print \"  %s\" % cmdclass.short_desc()\n+            print \"  %-13s %s\" % (cmdname, cmdclass.short_desc())\n     print\n+    print 'Use \"scrapy <command> -h\" for more info about a command'\n \n def check_deprecated_scrapy_ctl(argv):\n     \"\"\"Check if Scrapy was called using the deprecated scrapy-ctl command and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68f9fcffe8030b65b0c01f08e27ba2d8eee1313b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 55 | Files Changed: 2 | Hunks: 31 | Methods Changed: 18 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 97 | Churn Cumulative: 342 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -1,8 +1,7 @@\n-import sys\n import shutil\n import string\n from os import listdir\n-from os.path import join, dirname, abspath, exists\n+from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n from scrapy.spider import spiders\n@@ -10,10 +9,6 @@ from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n-\n-SPIDER_TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'spiders')\n-\n-\n def sanitize_module_name(module_name):\n     \"\"\"Sanitize the given module name, by replacing dashes and points\n     with underscores and prefixing it with a letter if it doesn't start\n@@ -24,10 +19,14 @@ def sanitize_module_name(module_name):\n         module_name = \"a\" + module_name\n     return module_name\n \n+_templates_base_dir = settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], \\\n+    'templates')\n+\n class Command(ScrapyCommand):\n \n     requires_project = True\n     default_settings = {'LOG_ENABLED': False}\n+    templates_dir = join(_templates_base_dir, 'spiders')\n \n     def syntax(self):\n         return \"[options] <name> <domain>\"\n@@ -37,8 +36,10 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--list\", dest=\"list\", action=\"store_true\")\n-        parser.add_option(\"--dump\", dest=\"dump\", action=\"store_true\")\n+        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n+            help=\"List available templates\")\n+        parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n+            help=\"Dump template to standard output\")\n         parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"crawl\",\n             help=\"Uses a custom template.\")\n         parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n@@ -48,33 +49,26 @@ class Command(ScrapyCommand):\n         if opts.list:\n             self._list_templates()\n             return\n-\n         if opts.dump:\n-            template_file = self._find_template(opts.template)\n+            template_file = self._find_template(opts.dump)\n             if template_file:\n-                template = open(template_file, 'r')\n-                print template.read() \n+                print open(template_file, 'r').read()\n             return\n-\n         if len(args) != 2:\n             return False\n \n-        name = args[0]\n-        domain = args[1]\n-\n+        name, domain = args[0:2]\n         module = sanitize_module_name(name)\n-\n-        # if spider already exists and not force option then halt\n         try:\n             spider = spiders.create(name)\n         except KeyError:\n             pass\n         else:\n+            # if spider already exists and not --force then halt\n             if not opts.force:\n-                print \"Spider '%s' already exists in module:\" % name\n+                print \"Spider %r already exists in module:\" % name\n                 print \"  %s\" % spider.__module__\n-                sys.exit(1)\n-\n+                return\n         template_file = self._find_template(opts.template)\n         if template_file:\n             self._genspider(module, name, domain, opts.template, template_file)\n@@ -90,11 +84,9 @@ class Command(ScrapyCommand):\n             'classname': '%sSpider' % ''.join([s.capitalize() \\\n                 for s in module.split('_')])\n         }\n-\n         spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n         spiders_dir = abspath(dirname(spiders_module.__file__))\n         spider_file = \"%s.py\" % join(spiders_dir, module)\n-\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n         print \"Created spider %r using template %r in module:\" % (name, \\\n@@ -102,22 +94,15 @@ class Command(ScrapyCommand):\n         print \"  %s.%s\" % (spiders_module.__name__, module)\n \n     def _find_template(self, template):\n-        template_file = join(settings['TEMPLATES_DIR'], 'spiders', '%s.tmpl' % template)\n-        if not exists(template_file):\n-            template_file = join(SPIDER_TEMPLATES_PATH, '%s.tmpl' % template)\n-            if not exists(template_file):\n-                print \"Unable to find template %r.\" \\\n-                        % template\n-                print \"Use genspider --list to see all available templates.\"\n-                return None\n+        template_file = join(self.templates_dir, '%s.tmpl' % template)\n+        if exists(template_file):\n             return template_file\n+        print \"Unable to find template: %s\\n\" % template\n+        print 'Use \"scrapy genspider --list\" to see all available templates.'\n \n     def _list_templates(self):\n-        files = set(listdir(SPIDER_TEMPLATES_PATH))\n-        if exists(settings['TEMPLATES_DIR']):\n-            files.update(listdir(join(settings['TEMPLATES_DIR'], 'spiders')))\n-\n-        for filename in sorted(files):\n+        print \"Available templates:\"\n+        for filename in sorted(listdir(self.templates_dir)):\n             if filename.endswith('.tmpl'):\n-                print filename\n+                print \"  %s\" % splitext(filename)[0]\n \n\n@@ -32,8 +32,7 @@ class ProjectTest(unittest.TestCase):\n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n             env=self.env, **kwargs)\n \n-    def call_get_proc(self, *new_args, **kwargs):\n-        out = os.tmpfile()\n+    def proc(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         return subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, \\\n             cwd=self.cwd, env=self.env, **kwargs)\n@@ -76,29 +75,32 @@ class GenspiderCommandTest(CommandTest):\n         self.assertEqual(0, self.call('genspider', 'test_name', 'test.com'))\n         assert exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))\n \n-    def test_template_default(self, *args):\n-        self.assertEqual(0, self.call('genspider', 'test_spider', 'test.com', *args))\n-        assert exists(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n-        self.assertEqual(1, self.call('genspider', 'test_spider', 'test.com'))\n+    def test_template(self, tplname='crawl'):\n+        args = ['--template=%s' % tplname] if tplname else []\n+        spname = 'test_spider'\n+        p = self.proc('genspider', spname, 'test.com', *args)\n+        out = p.stdout.read()\n+        self.assert_(\"Created spider %r using template %r in module\" % (spname, tplname) in out)\n+        self.assert_(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n+        p = self.proc('genspider', spname, 'test.com', *args)\n+        out = p.stdout.read()\n+        self.assert_(\"Spider %r already exists in module\" % spname in out)\n \n     def test_template_basic(self):\n-        self.test_template_default('--template=basic')\n+        self.test_template('basic')\n \n     def test_template_csvfeed(self):\n-        self.test_template_default('--template=csvfeed')\n+        self.test_template('csvfeed')\n \n     def test_template_xmlfeed(self):\n-        self.test_template_default('--template=xmlfeed')\n-\n-    def test_template_crawl(self):\n-        self.test_template_default('--template=crawl')\n+        self.test_template('xmlfeed')\n \n     def test_list(self):\n         self.assertEqual(0, self.call('genspider', '--list'))\n \n     def test_dump(self):\n-        self.assertEqual(0, self.call('genspider', '--dump'))\n-        self.assertEqual(0, self.call('genspider', '--dump', '--template=basic'))\n+        self.assertEqual(0, self.call('genspider', '--dump=basic'))\n+        self.assertEqual(0, self.call('genspider', '-d', 'basic'))\n \n \n class MiscCommandsTest(CommandTest):\n@@ -127,7 +129,7 @@ class MySpider(BaseSpider):\n         self.log(\"It Works!\")\n         return []\n \"\"\")\n-        p = self.call_get_proc('runspider', fname)\n+        p = self.proc('runspider', fname)\n         log = p.stderr.read()\n         self.assert_(\"[myspider] DEBUG: It Works!\" in log)\n         self.assert_(\"[myspider] INFO: Spider opened\" in log)\n@@ -143,12 +145,12 @@ class MySpider(BaseSpider):\n from scrapy import log\n from scrapy.spider import BaseSpider\n \"\"\")\n-        p = self.call_get_proc('runspider', fname)\n+        p = self.proc('runspider', fname)\n         log = p.stderr.read()\n         self.assert_(\"ERROR: No spider found in file\" in log)\n \n     def test_runspider_file_not_found(self):\n-        p = self.call_get_proc('runspider', 'some_non_existent_file')\n+        p = self.proc('runspider', 'some_non_existent_file')\n         log = p.stderr.read()\n         self.assert_(\"ERROR: File not found: some_non_existent_file\" in log)\n \n@@ -158,7 +160,7 @@ from scrapy.spider import BaseSpider\n         fname = abspath(join(tmpdir, 'myspider.txt'))\n         with open(fname, 'w') as f:\n             f.write(\"\")\n-        p = self.call_get_proc('runspider', fname)\n+        p = self.proc('runspider', fname)\n         log = p.stderr.read()\n         self.assert_(\"ERROR: Unable to load\" in log)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6d2a3087e85530352a9a152342d3e9f459a2857", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 21 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 50 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +0,0 @@\n-#!/usr/bin/env python\n-\n-import os\n-os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'googledir.settings')\n-\n-from scrapy.cmdline import execute\n-execute()\n\n@@ -1,7 +0,0 @@\n-#!/usr/bin/env python\n-\n-import os\n-os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'imdb.settings')\n-\n-from scrapy.cmdline import execute\n-execute()\n\n@@ -1,7 +0,0 @@\n-#!/usr/bin/env python\n-\n-import os\n-os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'googledir.settings')\n-\n-from scrapy.cmdline import execute\n-execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b8b7b5ad74dd437582ccc8aed9a272e404c721bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 6 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class Command(ScrapyCommand):\n         return \"[options] <name> <domain>\"\n \n     def short_desc(self):\n-        return \"Generate new spider based on template passed with -t or --template\"\n+        return \"Generate new spider using pre-defined templates\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -16,7 +16,7 @@ class Command(ScrapyCommand):\n         return \"[options] <url>\"\n \n     def short_desc(self):\n-        return \"Parse the given URL (using the spider) and print the results\"\n+        return \"Parse URL (using its spider) and print the results\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -29,7 +29,7 @@ class Command(ScrapyCommand):\n         return \"[options] <spider_file>\"\n \n     def short_desc(self):\n-        return \"Run a spider\"\n+        return \"Run a self-contained spider (without creating a project)\"\n \n     def long_desc(self):\n         return \"Run the spider defined in the given file\"\n\n@@ -10,7 +10,7 @@ class Command(ScrapyCommand):\n         return \"[options]\"\n \n     def short_desc(self):\n-        return \"Query Scrapy settings\"\n+        return \"Get settings values\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -29,7 +29,7 @@ class Command(ScrapyCommand):\n         return \"<project_name>\"\n \n     def short_desc(self):\n-        return \"Create new project with an initial project template\"\n+        return \"Create new project\"\n \n     def run(self, args, opts):\n         if len(args) != 1:\n\n@@ -5,7 +5,7 @@ from scrapy.utils.response import open_in_browser\n class Command(fetch.Command):\n \n     def short_desc(self):\n-        return \"Open a URL in browser, as seen by Scrapy\"\n+        return \"Open URL in browser, as seen by Scrapy\"\n \n     def long_desc(self):\n         return \"Fetch a URL using the Scrapy downloader and show its \" \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c276c48c913811edcd9d7ed94509f8c118e2f19d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 427 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,12 +17,13 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n from scrapy.utils.url import any_to_uri\n from scrapy.utils.console import start_python_console\n-from scrapy.conf import settings\n+from scrapy.conf import settings, Settings\n from scrapy.http import Request, Response, TextResponse\n \n class Shell(object):\n \n-    relevant_classes = (BaseSpider, Request, Response, BaseItem, XPathSelector)\n+    relevant_classes = (BaseSpider, Request, Response, BaseItem, \\\n+        XPathSelector, Settings)\n \n     def __init__(self, crawler, update_vars=None, inthread=False):\n         self.crawler = crawler\n@@ -71,6 +72,7 @@ class Shell(object):\n     def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n         self.vars['item'] = item\n+        self.vars['settings'] = settings\n         if url:\n             if isinstance(response, TextResponse):\n                 self.vars['xxs'] = XmlXPathSelector(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#053d45e79f394dc3dcd356505b7ed43fd0e82c0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 20 | Files Changed: 10 | Hunks: 14 | Methods Changed: 4 | Complexity Δ (Sum/Max): 37/27 | Churn Δ: 40 | Churn Cumulative: 887 | Contributors (this commit): 6 | Commits (past 90d): 31 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -234,7 +234,7 @@ SQS_VISIBILITY_TIMEOUT = 7200\n SQS_POLLING_DELAY = 30\n SQS_REGION = 'us-east-1'\n \n-STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'\n+STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n STATS_ENABLED = True\n STATS_DUMP = False\n \n\n@@ -10,7 +10,6 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n from scrapy.stats import stats\n-from scrapy.stats.signals import stats_spider_opened, stats_spider_closing\n from scrapy.conf import settings\n \n class CoreStats(object):\n@@ -22,8 +21,8 @@ class CoreStats(object):\n         stats.set_value('envinfo/logfile', settings['LOG_FILE'])\n         stats.set_value('envinfo/pid', os.getpid())\n \n-        dispatcher.connect(self.stats_spider_opened, signal=stats_spider_opened)\n-        dispatcher.connect(self.stats_spider_closing, signal=stats_spider_closing)\n+        dispatcher.connect(self.stats_spider_opened, signal=signals.stats_spider_opened)\n+        dispatcher.connect(self.stats_spider_closing, signal=signals.stats_spider_closing)\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n\n@@ -6,11 +6,10 @@ Requires the boto library: http://code.google.com/p/boto/\n \n from datetime import datetime\n \n-from boto import connect_sdb\n from twisted.internet import threads\n \n from scrapy.utils.simpledb import to_sdb_value\n-from scrapy.stats.collector import StatsCollector\n+from scrapy.statscol import StatsCollector\n from scrapy import log\n from scrapy.conf import settings\n \n@@ -20,7 +19,9 @@ class SimpledbStatsCollector(StatsCollector):\n         super(SimpledbStatsCollector, self).__init__()\n         self._sdbdomain = settings['STATS_SDB_DOMAIN']\n         self._async = settings.getbool('STATS_SDB_ASYNC')\n-        connect_sdb().create_domain(self._sdbdomain)\n+        import boto\n+        self.connect_sdb = boto.connect_sdb\n+        self.connect_sdb().create_domain(self._sdbdomain)\n \n     def _persist_stats(self, stats, spider=None):\n         if spider is None: # only store spider-specific stats\n@@ -40,7 +41,7 @@ class SimpledbStatsCollector(StatsCollector):\n         sdb_item = dict((k, self._to_sdb_value(v, k)) for k, v in stats.iteritems())\n         sdb_item['spider'] = spider.name\n         sdb_item['timestamp'] = self._to_sdb_value(ts)\n-        connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n+        self.connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n \n     def _get_timestamp(self, spider):\n         return datetime.utcnow()\n\n@@ -6,7 +6,8 @@ Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.stats import stats, signals\n+from scrapy.stats import stats\n+from scrapy import signals\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n from scrapy.exceptions import NotConfigured\n\n@@ -17,3 +17,6 @@ response_downloaded = object()\n item_scraped = object()\n item_passed = object()\n item_dropped = object()\n+stats_spider_opened = object()\n+stats_spider_closing = object()\n+stats_spider_closed = object()\n\n@@ -1,4 +1,4 @@\n-from scrapy.stats.collector import DummyStatsCollector\n+from scrapy.statscol import DummyStatsCollector\n from scrapy.conf import settings\n from scrapy.utils.misc import load_object\n \n\n@@ -1,3 +0,0 @@\n-stats_spider_opened = object()\n-stats_spider_closing = object()\n-stats_spider_closed = object()\n\n@@ -5,7 +5,7 @@ import pprint\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \\\n+from scrapy.signals import stats_spider_opened, stats_spider_closing, \\\n     stats_spider_closed\n from scrapy.utils.signal import send_catch_log\n from scrapy import signals\n\n@@ -105,10 +105,9 @@ class CrawlingSession(object):\n             # stats because of living signals. This whole test_engine.py should\n             # be rewritten from scratch actually.\n             from scrapy.utils.signal import disconnect_all\n-            from scrapy.stats import signals as stats_signals\n-            disconnect_all(stats_signals.stats_spider_opened)\n-            disconnect_all(stats_signals.stats_spider_closing)\n-            disconnect_all(stats_signals.stats_spider_closed)\n+            disconnect_all(signals.stats_spider_opened)\n+            disconnect_all(signals.stats_spider_closing)\n+            disconnect_all(signals.stats_spider_closed)\n \n     def geturl(self, path):\n         return \"http://localhost:%s%s\" % (self.portno, path)\n\n@@ -2,8 +2,8 @@ import unittest\n \n from scrapy.spider import BaseSpider\n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.stats.collector import StatsCollector, DummyStatsCollector\n-from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \\\n+from scrapy.statscol import StatsCollector, DummyStatsCollector\n+from scrapy.signals import stats_spider_opened, stats_spider_closing, \\\n     stats_spider_closed\n \n class StatsCollectorTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#faf7a7da835a4c43294f92b67c37d4b899bbd905", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 160 | Lines Deleted: 154 | Files Changed: 24 | Hunks: 45 | Methods Changed: 43 | Complexity Δ (Sum/Max): 11/25 | Churn Δ: 314 | Churn Cumulative: 4328 | Contributors (this commit): 7 | Commits (past 90d): 102 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -113,8 +113,8 @@ def execute(argv=None):\n \n     settings.defaults.update(cmd.default_settings)\n     del args[0]  # remove command name from args\n-    from scrapy.core.manager import scrapymanager\n-    scrapymanager.configure(control_reactor=True)\n+    from scrapy.project import crawler\n+    crawler.configure(control_reactor=True)\n     ret = _run_command(cmd, args, opts)\n     if ret is False:\n         parser.print_help()\n\n@@ -1,7 +1,7 @@\n from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.core.queue import ExecutionQueue\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.conf import settings\n from scrapy.http import Request\n from scrapy.spider import spiders\n@@ -50,8 +50,8 @@ class Command(ScrapyCommand):\n                 for url in urls:\n                     q.append_url(url, spider)\n \n-        scrapymanager.queue = q\n-        scrapymanager.start()\n+        crawler.queue = q\n+        crawler.start()\n \n     def _group_urls_by_spider(self, urls):\n         spider_urls = defaultdict(list)\n\n@@ -2,7 +2,7 @@ import pprint\n \n from scrapy import log\n from scrapy.command import ScrapyCommand\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.http import Request\n from scrapy.spider import BaseSpider, spiders\n from scrapy.utils.url import is_url\n@@ -47,8 +47,8 @@ class Command(ScrapyCommand):\n             except KeyError:\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n \n-        scrapymanager.configure()\n-        scrapymanager.queue.append_request(request, spider, \\\n+        crawler.configure()\n+        crawler.queue.append_request(request, spider, \\\n             default_spider=BaseSpider('default'))\n-        scrapymanager.start()\n+        crawler.start()\n \n\n@@ -1,5 +1,5 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.spider import spiders\n@@ -88,9 +88,9 @@ class Command(ScrapyCommand):\n                 log.msg('Unable to find spider for URL: %s' % args[0], log.ERROR)\n                 return\n \n-        scrapymanager.configure()\n-        scrapymanager.queue.append_request(request, spider)\n-        scrapymanager.start()\n+        crawler.configure()\n+        crawler.queue.append_request(request, spider)\n+        crawler.start()\n \n         if not responses:\n             log.msg('No response returned', log.ERROR, spider=spider)\n\n@@ -1,5 +1,5 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n \n@@ -12,5 +12,5 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         queue_class = load_object(settings['SERVICE_QUEUE'])\n-        scrapymanager.queue = queue_class()\n-        scrapymanager.start()\n+        crawler.queue = queue_class()\n+        crawler.start()\n\n@@ -4,7 +4,7 @@ import os\n from scrapy import log\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n \n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n@@ -52,5 +52,5 @@ class Command(ScrapyCommand):\n             return\n         spider = spclasses.pop()()\n         # schedule spider and start engine\n-        scrapymanager.queue.append_spider(spider)\n-        scrapymanager.start()\n+        crawler.queue.append_spider(spider)\n+        crawler.start()\n\n@@ -4,7 +4,7 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n@@ -30,7 +30,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n-        shell = Shell(scrapymanager, update_vars=self.update_vars, inthread=True)\n-        shell.start(url=url).addBoth(lambda _: scrapymanager.stop())\n-        scrapymanager.queue = KeepAliveExecutionQueue()\n-        scrapymanager.start()\n+        shell = Shell(crawler, update_vars=self.update_vars, inthread=True)\n+        shell.start(url=url).addBoth(lambda _: crawler.stop())\n+        crawler.queue = KeepAliveExecutionQueue()\n+        crawler.start()\n\n@@ -10,7 +10,7 @@ from twisted.internet import reactor\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.conf import settings\n \n class CloseSpider(object):\n@@ -30,13 +30,13 @@ class CloseSpider(object):\n \n     def spider_opened(self, spider):\n         self.tasks[spider] = reactor.callLater(self.timeout, \\\n-            scrapymanager.engine.close_spider, spider=spider, \\\n+            crawler.engine.close_spider, spider=spider, \\\n             reason='closespider_timeout')\n         \n     def item_passed(self, item, spider):\n         self.counts[spider] += 1\n         if self.counts[spider] == self.itempassed:\n-            scrapymanager.engine.close_spider(spider, 'closespider_itempassed')\n+            crawler.engine.close_spider(spider, 'closespider_itempassed')\n \n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n\n@@ -9,7 +9,7 @@ import robotparser\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n@@ -42,7 +42,7 @@ class RobotsTxtMiddleware(object):\n             self._parsers[netloc] = None\n             robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n             robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n-            dfd = scrapymanager.engine.download(robotsreq, spider)\n+            dfd = crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n             self._spider_netlocs[spider].add(netloc)\n         return self._parsers[netloc]\n\n@@ -13,7 +13,7 @@ from twisted.internet import task\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy import log\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.conf import settings\n@@ -74,7 +74,7 @@ class MemoryUsage(object):\n                         (settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n                 stats.set_value('memusage/limit_notified', 1)\n-            scrapymanager.stop()\n+            crawler.stop()\n \n     def _check_warning(self):\n         if self.warned: # warn only once\n\n@@ -2,7 +2,7 @@ from twisted.internet.defer import Deferred, DeferredList\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n@@ -87,7 +87,7 @@ class MediaPipeline(object):\n     def download(self, request, info):\n         \"\"\"Defines how to download the media request\"\"\"\n         request.priority = self.DOWNLOAD_PRIORITY\n-        return scrapymanager.engine.download(request, info.spider)\n+        return crawler.engine.download(request, info.spider)\n \n     def media_to_download(self, request, info):\n         \"\"\"Check request before starting download\"\"\"\n\n@@ -9,7 +9,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from collections import defaultdict\n \n from scrapy import signals\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.exceptions import NotConfigured, DontCloseSpider\n from scrapy.conf import settings\n \n@@ -26,7 +26,7 @@ class SpiderCloseDelay(object):\n \n     def spider_idle(self, spider):\n         try:\n-            lastseen = scrapymanager.engine.downloader.sites[spider].lastseen\n+            lastseen = crawler.engine.downloader.sites[spider].lastseen\n         except KeyError:\n             lastseen = None\n         if not lastseen:\n\n@@ -7,7 +7,7 @@ from itertools import imap\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.http import Request\n@@ -61,5 +61,5 @@ class RequestLimitMiddleware(object):\n             return request_or_other\n \n     def _pending_count(self, spider):\n-        pending = scrapymanager.engine.scheduler.pending_requests.get(spider, [])\n+        pending = crawler.engine.scheduler.pending_requests.get(spider, [])\n         return len(pending)\n\n@@ -1,12 +1,12 @@\n from scrapy.webservice import JsonResource\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.utils.engine import get_engine_status\n \n class EngineStatusResource(JsonResource):\n \n     ws_name = 'enginestatus'\n \n-    def __init__(self, spider_name=None, _manager=scrapymanager):\n+    def __init__(self, spider_name=None, _manager=crawler):\n         JsonResource.__init__(self)\n         self._spider_name = spider_name\n         self.isLeaf = spider_name is not None\n\n@@ -1,10 +1,10 @@\n from scrapy.webservice import JsonRpcResource\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n \n class ManagerResource(JsonRpcResource):\n \n     ws_name = 'manager'\n \n-    def __init__(self, _manager=scrapymanager):\n+    def __init__(self, _manager=crawler):\n         JsonRpcResource.__init__(self)\n         self._target = _manager\n\n@@ -24,7 +24,8 @@ from scrapy.utils.defer import mustbe_deferred\n \n class ExecutionEngine(object):\n \n-    def __init__(self):\n+    def __init__(self, crawler):\n+        self.crawler = crawler\n         self.configured = False\n         self.closing = {} # dict (spider -> reason) of spiders being closed\n         self.closing_dfds = {} # dict (spider -> deferred) of spiders being closed\n\n@@ -1,98 +1,6 @@\n-import signal\n+import warnings\n+warnings.warn(\"scrapy.core.manager.scrapymanager is deprecated and will be removed in Scrapy 0.11, use scrapy.project.crawler instead\", \\\n+    DeprecationWarning, stacklevel=2)\n \n-from twisted.internet import reactor, defer\n-\n-from scrapy.core.engine import ExecutionEngine\n-from scrapy.core.queue import ExecutionQueue\n-from scrapy.extension import extensions\n-from scrapy import log\n-from scrapy.spider import spiders\n-from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n-\n-\n-class ExecutionManager(object):\n-\n-    def __init__(self):\n-        self.configured = False\n-        self.control_reactor = True\n-        self.engine = ExecutionEngine()\n-\n-    def configure(self, control_reactor=True, queue=None):\n-        self.control_reactor = control_reactor\n-        if control_reactor:\n-            install_shutdown_handlers(self._signal_shutdown)\n-\n-        if not log.started:\n-            log.start()\n-        if not extensions.loaded:\n-            extensions.load()\n-        if not spiders.loaded:\n-            spiders.load()\n-        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n-            level=log.DEBUG)\n-\n-        self.queue = queue or ExecutionQueue()\n-        self.engine.configure(self._spider_closed)\n-        self.configured = True\n-\n-    @defer.inlineCallbacks\n-    def _start_next_spider(self):\n-        spider, requests = yield defer.maybeDeferred(self.queue.get_next)\n-        if spider:\n-            self._start_spider(spider, requests)\n-        if self.engine.has_capacity() and not self._nextcall.active():\n-            self._nextcall = reactor.callLater(self.queue.polling_delay, \\\n-                self._spider_closed)\n-\n-    @defer.inlineCallbacks\n-    def _start_spider(self, spider, requests):\n-        \"\"\"Don't call this method. Use self.queue to start new spiders\"\"\"\n-        yield defer.maybeDeferred(self.engine.open_spider, spider)\n-        for request in requests:\n-            self.engine.crawl(request, spider)\n-\n-    @defer.inlineCallbacks\n-    def _spider_closed(self, spider=None):\n-        if not self.engine.open_spiders:\n-            is_finished = yield defer.maybeDeferred(self.queue.is_finished)\n-            if is_finished:\n-                self.stop()\n-                return\n-        if self.engine.has_capacity():\n-            self._start_next_spider()\n-\n-    @defer.inlineCallbacks\n-    def start(self):\n-        yield defer.maybeDeferred(self.engine.start)\n-        self._nextcall = reactor.callLater(0, self._start_next_spider)\n-        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n-        if self.control_reactor:\n-            reactor.run(installSignalHandlers=False)\n-\n-    @defer.inlineCallbacks\n-    def stop(self):\n-        if self._nextcall.active():\n-            self._nextcall.cancel()\n-        if self.engine.running:\n-            yield defer.maybeDeferred(self.engine.stop)\n-        try:\n-            reactor.stop()\n-        except RuntimeError: # raised if already stopped or in shutdown stage\n-            pass\n-\n-    def _signal_shutdown(self, signum, _):\n-        signame = signal_names[signum]\n-        log.msg(\"Received %s, shutting down gracefully. Send again to force \" \\\n-            \"unclean shutdown\" % signame, level=log.INFO)\n-        reactor.callFromThread(self.stop)\n-        install_shutdown_handlers(self._signal_kill)\n-\n-    def _signal_kill(self, signum, _):\n-        signame = signal_names[signum]\n-        log.msg('Received %s twice, forcing unclean shutdown' % signame, \\\n-            level=log.INFO)\n-        log.log_level = log.SILENT # disable logging of confusing tracebacks\n-        reactor.callFromThread(self.engine.kill)\n-        install_shutdown_handlers(signal.SIG_IGN)\n-\n-scrapymanager = ExecutionManager()\n+from scrapy.project import crawler\n+scrapymanager = crawler\n\n@@ -0,0 +1,96 @@\n+import signal\n+\n+from twisted.internet import reactor, defer\n+\n+from scrapy.core.engine import ExecutionEngine\n+from scrapy.core.queue import ExecutionQueue\n+from scrapy.extension import extensions\n+from scrapy import log\n+from scrapy.spider import spiders\n+from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n+\n+\n+class Crawler(object):\n+\n+    def __init__(self):\n+        self.configured = False\n+        self.control_reactor = True\n+        self.engine = ExecutionEngine(self)\n+\n+    def configure(self, control_reactor=True, queue=None):\n+        self.control_reactor = control_reactor\n+        if control_reactor:\n+            install_shutdown_handlers(self._signal_shutdown)\n+\n+        if not log.started:\n+            log.start()\n+        if not extensions.loaded:\n+            extensions.load()\n+        if not spiders.loaded:\n+            spiders.load()\n+        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n+            level=log.DEBUG)\n+\n+        self.queue = queue or ExecutionQueue()\n+        self.engine.configure(self._spider_closed)\n+        self.configured = True\n+\n+    @defer.inlineCallbacks\n+    def _start_next_spider(self):\n+        spider, requests = yield defer.maybeDeferred(self.queue.get_next)\n+        if spider:\n+            self._start_spider(spider, requests)\n+        if self.engine.has_capacity() and not self._nextcall.active():\n+            self._nextcall = reactor.callLater(self.queue.polling_delay, \\\n+                self._spider_closed)\n+\n+    @defer.inlineCallbacks\n+    def _start_spider(self, spider, requests):\n+        \"\"\"Don't call this method. Use self.queue to start new spiders\"\"\"\n+        yield defer.maybeDeferred(self.engine.open_spider, spider)\n+        for request in requests:\n+            self.engine.crawl(request, spider)\n+\n+    @defer.inlineCallbacks\n+    def _spider_closed(self, spider=None):\n+        if not self.engine.open_spiders:\n+            is_finished = yield defer.maybeDeferred(self.queue.is_finished)\n+            if is_finished:\n+                self.stop()\n+                return\n+        if self.engine.has_capacity():\n+            self._start_next_spider()\n+\n+    @defer.inlineCallbacks\n+    def start(self):\n+        yield defer.maybeDeferred(self.engine.start)\n+        self._nextcall = reactor.callLater(0, self._start_next_spider)\n+        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n+        if self.control_reactor:\n+            reactor.run(installSignalHandlers=False)\n+\n+    @defer.inlineCallbacks\n+    def stop(self):\n+        if self._nextcall.active():\n+            self._nextcall.cancel()\n+        if self.engine.running:\n+            yield defer.maybeDeferred(self.engine.stop)\n+        try:\n+            reactor.stop()\n+        except RuntimeError: # raised if already stopped or in shutdown stage\n+            pass\n+\n+    def _signal_shutdown(self, signum, _):\n+        signame = signal_names[signum]\n+        log.msg(\"Received %s, shutting down gracefully. Send again to force \" \\\n+            \"unclean shutdown\" % signame, level=log.INFO)\n+        reactor.callFromThread(self.stop)\n+        install_shutdown_handlers(self._signal_kill)\n+\n+    def _signal_kill(self, signum, _):\n+        signame = signal_names[signum]\n+        log.msg('Received %s twice, forcing unclean shutdown' % signame, \\\n+            level=log.INFO)\n+        log.log_level = log.SILENT # disable logging of confusing tracebacks\n+        reactor.callFromThread(self.engine.kill)\n+        install_shutdown_handlers(signal.SIG_IGN)\n\n@@ -0,0 +1,3 @@\n+from scrapy.crawler import Crawler\n+\n+crawler = Crawler()\n\n@@ -107,5 +107,5 @@ class Shell(object):\n \n def inspect_response(response, spider=None):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n-    from scrapy.core.manager import scrapymanager\n-    Shell(scrapymanager).start(response=response, spider=spider)\n+    from scrapy.project import crawler\n+    Shell(crawler).start(response=response, spider=spider)\n\n@@ -12,7 +12,7 @@ from twisted.internet import reactor, protocol\n \n from scrapy.extension import extensions\n from scrapy.exceptions import NotConfigured\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.spider import spiders\n from scrapy.stats import stats\n from scrapy.utils.signal import send_catch_log\n@@ -48,8 +48,8 @@ class TelnetConsole(protocol.ServerFactory):\n     def _get_telnet_vars(self):\n         # Note: if you add entries here also update topics/telnetconsole.rst\n         telnet_vars = {\n-            'engine': scrapymanager.engine,\n-            'manager': scrapymanager,\n+            'engine': crawler.engine,\n+            'manager': crawler,\n             'extensions': extensions,\n             'stats': stats,\n             'spiders': spiders,\n\n@@ -8,7 +8,7 @@ from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n \n from scrapy import signals\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.tests import tests_datadir\n from scrapy.spider import BaseSpider\n@@ -96,9 +96,9 @@ class CrawlingSession(object):\n             dispatcher.connect(self.request_received, signals.request_received)\n             dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n-            scrapymanager.configure()\n-            scrapymanager.queue.append_spider(self.spider)\n-            scrapymanager.start()\n+            crawler.configure()\n+            crawler.queue.append_spider(self.spider)\n+            crawler.start()\n             self.port.stopListening()\n             self.wasrun = True\n             # FIXME: extremly ugly hack to avoid propagating errors to other\n\n@@ -1,13 +1,11 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n-from time import time\n-\n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n \n def get_engine_status(engine=None):\n     \"\"\"Return a report of the current engine status\"\"\"\n     if engine is None:\n-        engine = scrapymanager.engine\n+        engine = crawler.engine\n \n     global_tests = [\n         \"time()-engine.start_time\",\n\n@@ -4,7 +4,7 @@ import decimal\n \n from twisted.internet import defer\n \n-from scrapy.core.manager import scrapymanager\n+from scrapy.project import crawler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n from scrapy.utils.py26 import json\n@@ -21,7 +21,7 @@ class SpiderReferencer(object):\n     spider_ref_re = re.compile('^spider:([0-9a-f]+)?:?(.+)?$')\n \n     def __init__(self, manager=None):\n-        self.manager = manager or scrapymanager\n+        self.manager = manager or crawler\n \n     def get_reference_from_spider(self, spider):\n         return 'spider:%x:%s' % (id(spider), spider.name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#52c1e137e56d65b4bcc7a590ad6ae3c95db0f646", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 34 | Files Changed: 14 | Hunks: 31 | Methods Changed: 18 | Complexity Δ (Sum/Max): 16/14 | Churn Δ: 82 | Churn Cumulative: 2459 | Contributors (this commit): 6 | Commits (past 90d): 58 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ from scrapy.core.queue import ExecutionQueue\n from scrapy.project import crawler\n from scrapy.conf import settings\n from scrapy.http import Request\n-from scrapy.spider import spiders\n+from scrapy.project import crawler\n from scrapy.utils.url import is_url\n \n from collections import defaultdict\n@@ -39,14 +39,14 @@ class Command(ScrapyCommand):\n \n         if opts.spider:\n             try:\n-                spider = spiders.create(opts.spider)\n+                spider = crawler.spiders.create(opts.spider)\n                 for url in urls:\n                     q.append_url(url, spider)\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n             for name, urls in self._group_urls_by_spider(urls):\n-                spider = spiders.create(name)\n+                spider = crawler.spiders.create(name)\n                 for url in urls:\n                     q.append_url(url, spider)\n \n@@ -56,7 +56,7 @@ class Command(ScrapyCommand):\n     def _group_urls_by_spider(self, urls):\n         spider_urls = defaultdict(list)\n         for url in urls:\n-            spider_names = spiders.find_by_request(Request(url))\n+            spider_names = crawler.spiders.find_by_request(Request(url))\n             if not spider_names:\n                 log.msg('Could not find spider that handles url: %s' % url,\n                         log.ERROR)\n\n@@ -4,7 +4,8 @@ from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.project import crawler\n from scrapy.http import Request\n-from scrapy.spider import BaseSpider, spiders\n+from scrapy.spider import BaseSpider\n+from scrapy.project import crawler\n from scrapy.utils.url import is_url\n \n class Command(ScrapyCommand):\n@@ -43,7 +44,7 @@ class Command(ScrapyCommand):\n         spider = None\n         if opts.spider:\n             try:\n-                spider = spiders.create(opts.spider)\n+                spider = crawler.spiders.create(opts.spider)\n             except KeyError:\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n \n\n@@ -4,7 +4,7 @@ from os import listdir\n from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n-from scrapy.spider import spiders\n+from scrapy.project import crawler\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.utils.template import render_templatefile, string_camelcase\n@@ -60,7 +60,7 @@ class Command(ScrapyCommand):\n         name, domain = args[0:2]\n         module = sanitize_module_name(name)\n         try:\n-            spider = spiders.create(name)\n+            spider = crawler.spiders.create(name)\n         except KeyError:\n             pass\n         else:\n\n@@ -1,5 +1,5 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.spider import spiders\n+from scrapy.project import crawler\n \n class Command(ScrapyCommand):\n \n@@ -10,4 +10,4 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        print \"\\n\".join(spiders.list())\n+        print \"\\n\".join(crawler.spiders.list())\n\n@@ -2,7 +2,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.project import crawler\n from scrapy.http import Request\n from scrapy.item import BaseItem\n-from scrapy.spider import spiders\n+from scrapy.project import crawler\n from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.url import is_url\n@@ -78,12 +78,12 @@ class Command(ScrapyCommand):\n \n         if opts.spider:\n             try:\n-                spider = spiders.create(opts.spider)\n+                spider = crawler.spiders.create(opts.spider)\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n                 return\n         else:\n-            spider = spiders.create_for_request(request)\n+            spider = crawler.spiders.create_for_request(request)\n             if spider is None:\n                 log.msg('Unable to find spider for URL: %s' % args[0], log.ERROR)\n                 return\n\n@@ -1,10 +1,12 @@\n from scrapy.webservice import JsonRpcResource\n-from scrapy.spider import spiders\n+from scrapy.project import crawler\n \n class SpidersResource(JsonRpcResource):\n \n     ws_name = 'spiders'\n \n-    def __init__(self, _spiders=spiders):\n+    def __init__(self, _spiders=None):\n+        if _spiders is None:\n+            _spiders = crawler.spiders\n         JsonRpcResource.__init__(self)\n         self._target = _spiders\n\n@@ -17,7 +17,6 @@ from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import IgnoreRequest, DontCloseSpider\n from scrapy.http import Response, Request\n-from scrapy.spider import spiders\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.utils.defer import mustbe_deferred\n@@ -278,7 +277,7 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))\n         dfd.addErrback(log.err, \"Unhandled error in stats.close_spider()\",\n             spider=spider)\n-        dfd.addBoth(lambda _: spiders.close_spider(spider))\n+        dfd.addBoth(lambda _: self.crawler.spiders.close_spider(spider))\n         dfd.addErrback(log.err, \"Unhandled error in spiders.close_spider()\",\n             spider=spider)\n         dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n\n@@ -3,14 +3,16 @@ from twisted.internet import defer\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n from scrapy import log\n-from scrapy.spider import spiders\n \n \n class ExecutionQueue(object):\n \n     polling_delay = 5\n \n-    def __init__(self, _spiders=spiders):\n+    def __init__(self, _spiders=None):\n+        if _spiders is None:\n+            from scrapy.project import crawler\n+            _spiders = crawler.spiders\n         self.spider_requests = []\n         self._spiders = _spiders\n \n\n@@ -6,15 +6,15 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.core.queue import ExecutionQueue\n from scrapy.extension import extensions\n from scrapy import log\n-from scrapy.spider import spiders\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n class Crawler(object):\n \n-    def __init__(self):\n+    def __init__(self, spiders):\n         self.configured = False\n         self.control_reactor = True\n+        self.spiders = spiders\n         self.engine = ExecutionEngine(self)\n \n     def configure(self, control_reactor=True, queue=None):\n@@ -26,8 +26,8 @@ class Crawler(object):\n             log.start()\n         if not extensions.loaded:\n             extensions.load()\n-        if not spiders.loaded:\n-            spiders.load()\n+        if not self.spiders.loaded:\n+            self.spiders.load()\n         log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n             level=log.DEBUG)\n \n\n@@ -1,3 +1,6 @@\n+from scrapy.conf import settings\n+from scrapy.utils.misc import load_object\n from scrapy.crawler import Crawler\n \n-crawler = Crawler()\n+_spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n+crawler = Crawler(_spiders)\n\n@@ -11,7 +11,7 @@ from twisted.python.failure import Failure\n \n from scrapy import log\n from scrapy.item import BaseItem\n-from scrapy.spider import BaseSpider, spiders\n+from scrapy.spider import BaseSpider\n from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n@@ -58,8 +58,8 @@ class Shell(object):\n             url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True)\n         if spider is None:\n-            spider = spiders.create_for_request(request, BaseSpider('default'), \\\n-                log_multiple=True)\n+            spider = self.crawler.spiders.create_for_request(request, \\\n+                BaseSpider('default'), log_multiple=True)\n         self.crawler.engine.open_spider(spider)\n         response = None\n         try:\n\n@@ -55,3 +55,16 @@ class BaseSpider(object_ref):\n         return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))\n \n     __repr__ = __str__\n+\n+\n+class ObsoleteClass(object):\n+    def __init__(self, message):\n+        self.message = message\n+\n+    def __getattr__(self, name):\n+        raise AttributeError(self.message)\n+\n+spiders = ObsoleteClass(\"\"\"\n+\"from scrapy.spider import spiders\" no longer works - use \"from scrapy.project import crawler\" and then access crawler.spiders attribute\"\n+\"\"\")\n+\n\n@@ -1,5 +0,0 @@\n-from scrapy.spider.models import BaseSpider\n-from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n-\n-spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n\n@@ -13,7 +13,6 @@ from twisted.internet import reactor, protocol\n from scrapy.extension import extensions\n from scrapy.exceptions import NotConfigured\n from scrapy.project import crawler\n-from scrapy.spider import spiders\n from scrapy.stats import stats\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.trackref import print_live_refs\n@@ -52,7 +51,7 @@ class TelnetConsole(protocol.ServerFactory):\n             'manager': crawler,\n             'extensions': extensions,\n             'stats': stats,\n-            'spiders': spiders,\n+            'spiders': crawler.spiders,\n             'settings': settings,\n             'est': print_engine_status,\n             'p': pprint.pprint,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
