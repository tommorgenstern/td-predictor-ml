{"custom_id": "keras#b4c7aded69549b73d4e39775b34846afa9975c72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\nBinary files a/tests/manual/check_activity_reg.py and b/tests/manual/check_activity_reg.py differ\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11685f68d52266188abc117687493c80a5580718", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 12 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 32 | Churn Cumulative: 643 | Contributors (this commit): 6 | Commits (past 90d): 28 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -369,7 +369,7 @@ class LSTM(Layer):\n \n \n \n-class MutatedRNN_1(Layer):\n+class JZS1(Layer):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n         of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n@@ -394,15 +394,12 @@ class MutatedRNN_1(Layer):\n         activation='tanh', inner_activation='sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n \n-        super(MutatedRNN_1,self).__init__()\n+        super(JZS1,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n \n-        if self.input_dim != self.output_dim:\n-            raise Exception('input_dim must equal output_dim for this architecture')\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n@@ -419,6 +416,11 @@ class MutatedRNN_1(Layer):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n+        # P_h used to project X onto different dimension\n+        P = np.ones((self.input_dim, self.output_dim), dtype=theano.config.floatX)\n+        P = P / np.linalg.norm(P, axis=0)\n+        self.Pmat = theano.shared(P, name=None)\n+\n         self.params = [\n             self.W_z, self.b_z,\n             self.W_r, self.U_r, self.b_r,\n@@ -444,7 +446,7 @@ class MutatedRNN_1(Layer):\n \n         x_z = T.dot(X, self.W_z) + self.b_z\n         x_r = T.dot(X, self.W_r) + self.b_r\n-        x_h = T.tanh(X) + self.b_h\n+        x_h = T.tanh(T.dot(X, self.Pmat)) + self.b_h\n         outputs, updates = theano.scan(\n             self._step, \n             sequences=[x_z, x_r, x_h], \n@@ -469,7 +471,7 @@ class MutatedRNN_1(Layer):\n \n \n \n-class MutatedRNN_2(Layer):\n+class JZS2(Layer):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n         of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n@@ -494,15 +496,12 @@ class MutatedRNN_2(Layer):\n         activation='tanh', inner_activation='sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n \n-        super(MutatedRNN_2,self).__init__()\n+        super(JZS2,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n \n-        if self.input_dim != self.output_dim:\n-            raise Exception('input_dim must equal output_dim for this architecture')\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n@@ -520,6 +519,11 @@ class MutatedRNN_2(Layer):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n+        # P_h used to project X onto different dimension\n+        P = np.ones((self.input_dim, self.output_dim), dtype=theano.config.floatX)\n+        P = P / np.linalg.norm(P, axis=0)\n+        self.Pmat = theano.shared(P, name=None)\n+\n         self.params = [\n             self.W_z, self.U_z, self.b_z,\n             self.U_r, self.b_r,\n@@ -544,7 +548,7 @@ class MutatedRNN_2(Layer):\n         X = X.dimshuffle((1,0,2)) \n \n         x_z = T.dot(X, self.W_z) + self.b_z\n-        x_r = X + self.b_r\n+        x_r = T.dot(X, self.Pmat) + self.b_r\n         x_h = T.dot(X, self.W_h) + self.b_h\n         outputs, updates = theano.scan(\n             self._step, \n@@ -570,7 +574,7 @@ class MutatedRNN_2(Layer):\n \n \n \n-class MutatedRNN_3(Layer):\n+class JZS3(Layer):\n     '''\n         Evolved recurrent neural network architectures from the evaluation of thousands\n         of models, serving as alternatives to LSTMs and GRUs. See Jozefowicz et al. 2015.\n@@ -595,7 +599,7 @@ class MutatedRNN_3(Layer):\n         activation='tanh', inner_activation='sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n \n-        super(MutatedRNN_3,self).__init__()\n+        super(JZS3,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62392a4b5e92ad5a6aefbc2d1379c7423437ff07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 118 | Files Changed: 6 | Hunks: 67 | Methods Changed: 30 | Complexity Δ (Sum/Max): -8/4 | Churn Δ: 208 | Churn Cumulative: 2082 | Contributors (this commit): 16 | Commits (past 90d): 109 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,6 @@ from __future__ import absolute_import\n import theano\n import theano.tensor as T\n import numpy as np\n-from .utils.theano_utils import default_mask_val, get_mask\n import types\n \n def softmax(x):\n\n@@ -6,7 +6,7 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_zeros, floatX, shared_scalar, get_mask, default_mask_val\n+from ..utils.theano_utils import shared_zeros, floatX, shared_scalar\n from ..utils.generic_utils import make_tuple\n from .. import regularizers\n from .. import constraints\n@@ -20,6 +20,8 @@ class Layer(object):\n         self.params = []\n \n     def connect(self, layer):\n+        if layer.get_output_mask() is not None and not self.supports_mask():\n+            raise Exception(\"Attached non-masking layer to layer with masked output\")\n         self.previous = layer\n \n     def get_output(self, train):\n@@ -31,6 +33,12 @@ class Layer(object):\n         else:\n             return self.input\n \n+    def supports_mask(self):\n+        return False\n+\n+    def get_output_mask(self, train=None):\n+        return None\n+\n     def set_weights(self, weights):\n         for p, w in zip(self.params, weights):\n             if p.eval().shape != w.shape:\n@@ -74,6 +82,16 @@ class Layer(object):\n \n         return self.params, regs, consts\n \n+class MaskedLayer(Layer):\n+    def supports_mask(self):\n+        return True\n+\n+    def get_output_mask(self, train=None):\n+        return self.previous.get_output_mask(train)\n+\n+    def get_input_mask(self, train=None):\n+        return self.previous.get_output_mask(train)\n+\n \n class Merge(object): \n     def __init__(self, models, mode='sum'):\n@@ -139,54 +157,49 @@ class Merge(object):\n             \"mode\":self.mode}\n \n \n-class Dropout(Layer):\n+class Dropout(MaskedLayer):\n     '''\n         Hinton's dropout.\n     '''\n-    def __init__(self, p, mask_val=default_mask_val):\n+    def __init__(self, p):\n         super(Dropout,self).__init__()\n         self.p = p\n-        self.mask_val = shared_scalar(mask_val)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        mask = get_mask(X, self.mask_val)\n+        mask = self.get_output_mask(train)\n         if self.p > 0.:\n             retain_prob = 1. - self.p\n             if train:\n                 X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n             else:\n                 X *= retain_prob\n-        return mask * X + (1 - mask) * self.mask_val\n+        return X\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"p\":self.p,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"p\":self.p}\n \n \n-class Activation(Layer):\n+class Activation(MaskedLayer):\n     '''\n         Apply an activation function to an output.\n     '''\n-    def __init__(self, activation, target=0, beta=0.1, mask_val=default_mask_val):\n+    def __init__(self, activation, target=0, beta=0.1):\n         super(Activation,self).__init__()\n         self.activation = activations.get(activation)\n         self.target = target\n         self.beta = beta\n-        self.mask_val = shared_scalar(mask_val)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        mask = get_mask(X, self.mask_val)\n-        return mask * self.activation(X) + (1 - mask) * self.mask_val\n+        return self.activation(X)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n             \"activation\":self.activation.__name__,\n             \"target\":self.target,\n-            \"beta\":self.beta,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"beta\":self.beta}\n \n \n class Reshape(Layer):\n@@ -284,7 +297,7 @@ class Dense(Layer):\n             \"activation\":self.activation.__name__}\n \n \n-class TimeDistributedDense(Layer):\n+class TimeDistributedDense(MaskedLayer):\n     '''\n        Apply a same DenseLayer for each dimension[1] (shared_dimension) input\n        Especially useful after a recurrent network with 'return_sequence=True'\n@@ -293,7 +306,7 @@ class TimeDistributedDense(Layer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, mask_val=default_mask_val):\n+            W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(TimeDistributedDense,self).__init__()\n         self.init = initializations.get(init)\n@@ -309,7 +322,6 @@ class TimeDistributedDense(Layer):\n \n         self.regularizers = [W_regularizer, b_regularizer]\n         self.constraints = [W_constraint, b_constraint]\n-        self.mask_val = shared_scalar(mask_val)\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -317,13 +329,12 @@ class TimeDistributedDense(Layer):\n     def get_output(self, train):\n         X = self.get_input(train)\n         X = X.dimshuffle(1,0,2)\n-        mask = get_mask(X, self.mask_val)\n \n-        def act_func(X, mask):\n-            return mask * self.activation(T.dot(X, self.W) + self.b) + (1 - mask) * self.mask_val\n+        def act_func(X):\n+            return self.activation(T.dot(X, self.W) + self.b)\n \n         output, _ = theano.scan(fn = act_func,\n-                                sequences = [X, mask],\n+                                sequences = X,\n                                 outputs_info=None)\n         return output.dimshuffle(1,0,2)\n \n@@ -332,8 +343,7 @@ class TimeDistributedDense(Layer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"activation\":self.activation.__name__,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"activation\":self.activation.__name__}\n \n class AutoEncoder(Layer):\n     '''\n\n@@ -3,13 +3,13 @@ import theano\n import theano.tensor as T\n \n from .. import activations, initializations\n-from ..layers.core import Layer, default_mask_val\n+from ..layers.core import Layer, MaskedLayer\n from ..utils.theano_utils import sharedX\n \n from ..constraints import unitnorm\n \n \n-class Embedding(Layer):\n+class Embedding(MaskedLayer):\n     '''\n         Turn positive integers (indexes) into denses vectors of fixed size. \n         eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n@@ -17,7 +17,7 @@ class Embedding(Layer):\n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None, zero_is_mask=False, mask_val=default_mask_val):\n+    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None, mask_zero=False):\n         super(Embedding,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n@@ -25,11 +25,7 @@ class Embedding(Layer):\n \n         self.input = T.imatrix()\n         self.W = self.init((self.input_dim, self.output_dim))\n-        self.zero_is_mask = zero_is_mask\n-\n-        if zero_is_mask:\n-            # This doesn't seem particularly elegant\n-            self.W = sharedX(T.set_subtensor(self.W[0, :], mask_val).eval())\n+        self.mask_zero = mask_zero\n \n         self.params = [self.W]\n         self.constraints = [W_constraint]\n@@ -38,6 +34,13 @@ class Embedding(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n+    def get_output_mask(self, train=None):\n+        X = self.get_input(train)\n+        if not self.mask_zero:\n+            return T.ones_like(X)\n+        else:\n+            return T.ones_like(X) * (1 - T.eq(X,0))\n+\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         out = self.W[X]\n\n@@ -5,11 +5,34 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix, get_mask, default_mask_val\n-from ..layers.core import Layer\n+from ..utils.theano_utils import shared_scalar, shared_zeros, alloc_zeros_matrix\n+from ..layers.core import Layer, MaskedLayer\n from six.moves import range\n \n-class SimpleRNN(Layer):\n+class BaseRecurrent(MaskedLayer):\n+    def get_output_mask(self, train=None):\n+        if self.return_sequences:\n+            return super(BaseRecurrent, self).get_output_mask(train)\n+        else:\n+            return None\n+\n+    def get_padded_shuffled_mask(self, train, X, pad = 0):\n+        mask = self.get_input_mask(train)\n+        if mask is None:\n+            mask = T.ones_like(X.sum(axis=-1)) # is there a better way to do this without a sum?\n+        # mask is (nb_samples, time)\n+        mask = T.shape_padright(mask) # (nb_samples, time, 1)\n+        mask = T.addbroadcast(mask, -1) # (time, nb_samples, 1) matrix.\n+        mask = mask.dimshuffle(1,0,2) # (time, nb_samples, 1)\n+\n+        if pad > 0:\n+            # left-pad in time with 0\n+            padding = alloc_zeros_matrix(pad, mask.shape[1], 1).astype('uint8')\n+            return T.concatenate([padding, mask], axis=0)\n+        return mask\n+\n+\n+class SimpleRNN(BaseRecurrent):\n     '''\n         Fully connected RNN where output is to fed back to input.\n \n@@ -19,7 +42,7 @@ class SimpleRNN(Layer):\n     '''\n     def __init__(self, input_dim, output_dim, \n         init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n-        truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n+        truncate_gradient=-1, return_sequences=False):\n         super(SimpleRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -28,7 +51,6 @@ class SimpleRNN(Layer):\n         self.truncate_gradient = truncate_gradient\n         self.activation = activations.get(activation)\n         self.return_sequences = return_sequences\n-        self.mask_val = shared_scalar(mask_val)\n         self.input = T.tensor3()\n \n         self.W = self.init((self.input_dim, self.output_dim))\n@@ -50,9 +72,9 @@ class SimpleRNN(Layer):\n     def get_output(self, train):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n         # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1,0,2)) \n \n-        mask, padded_mask = get_mask(X, self.mask_val, steps_back=1)\n \n         x = T.dot(X, self.W) + self.b\n         \n@@ -67,8 +89,7 @@ class SimpleRNN(Layer):\n             non_sequences=self.U, # static inputs to _step\n             truncate_gradient=self.truncate_gradient\n         )\n-        # Apply the mask:\n-        outputs = mask * outputs + (1 - mask) * self.mask_val\n+\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n@@ -81,11 +102,10 @@ class SimpleRNN(Layer):\n             \"inner_init\":self.inner_init.__name__,\n             \"activation\":self.activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"return_sequences\":self.return_sequences}\n \n \n-class SimpleDeepRNN(Layer):\n+class SimpleDeepRNN(BaseRecurrent):\n     '''\n         Fully connected RNN where the output of multiple timesteps \n         (up to \"depth\" steps in the past) is fed back to the input:\n@@ -98,7 +118,7 @@ class SimpleDeepRNN(Layer):\n     def __init__(self, input_dim, output_dim, depth=3,\n         init='glorot_uniform', inner_init='orthogonal', \n         activation='sigmoid', inner_activation='hard_sigmoid',\n-        weights=None, truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n         super(SimpleDeepRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -110,7 +130,6 @@ class SimpleDeepRNN(Layer):\n         self.depth = depth\n         self.return_sequences = return_sequences\n         self.input = T.tensor3()\n-        self.mask_val = shared_scalar(mask_val)\n \n         self.W = self.init((self.input_dim, self.output_dim))\n         self.Us = [self.inner_init((self.output_dim, self.output_dim)) for _ in range(self.depth)]\n@@ -131,10 +150,9 @@ class SimpleDeepRNN(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=self.depth)\n         X = X.dimshuffle((1,0,2)) \n \n-        mask, padded_mask = get_mask(X, self.mask_val, steps_back=self.depth)\n-\n         x = T.dot(X, self.W) + self.b\n         \n         if self.depth == 1:\n@@ -155,7 +173,7 @@ class SimpleDeepRNN(Layer):\n             non_sequences=self.Us,\n             truncate_gradient=self.truncate_gradient\n         )\n-        outputs = mask * outputs + (1 - mask) * self.mask_val\n+\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n@@ -170,12 +188,10 @@ class SimpleDeepRNN(Layer):\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"return_sequences\":self.return_sequences}\n \n \n-\n-class GRU(Layer):\n+class GRU(BaseRecurrent):\n     '''\n         Gated Recurrent Unit - Cho et al. 2014\n \n@@ -200,7 +216,7 @@ class GRU(Layer):\n     def __init__(self, input_dim, output_dim=128, \n         init='glorot_uniform', inner_init='orthogonal',\n         activation='sigmoid', inner_activation='hard_sigmoid',\n-        weights=None, truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n \n         super(GRU,self).__init__()\n         self.input_dim = input_dim\n@@ -213,7 +229,6 @@ class GRU(Layer):\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.input = T.tensor3()\n-        self.mask_val = shared_scalar(default_mask_val)\n \n         self.W_z = self.init((self.input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n@@ -250,8 +265,8 @@ class GRU(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train) \n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1,0,2)) \n-        mask, padded_mask = get_mask(X, self.mask_val, steps_back=1)\n \n         x_z = T.dot(X, self.W_z) + self.b_z\n         x_r = T.dot(X, self.W_r) + self.b_r\n@@ -264,8 +279,6 @@ class GRU(Layer):\n             truncate_gradient=self.truncate_gradient\n         )\n \n-        outputs = mask * outputs + (1 - mask) * self.mask_val\n-\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n@@ -279,12 +292,11 @@ class GRU(Layer):\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"return_sequences\":self.return_sequences}\n \n \n \n-class LSTM(Layer):\n+class LSTM(BaseRecurrent):\n     '''\n         Acts as a spatiotemporal projection,\n         turning a sequence of vectors into a single vector.\n@@ -312,14 +324,13 @@ class LSTM(Layer):\n     def __init__(self, input_dim, output_dim=128, \n         init='glorot_uniform', inner_init='orthogonal', \n         activation='tanh', inner_activation='hard_sigmoid',\n-        weights=None, truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n     \n         super(LSTM,self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n         self.return_sequences = return_sequences\n-        self.mask_val = shared_scalar(mask_val)\n \n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -369,8 +380,8 @@ class LSTM(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train) \n+        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1,0,2))\n-        mask, padded_mask = get_mask(X, self.mask_val, steps_back=1)\n \n         xi = T.dot(X, self.W_i) + self.b_i\n         xf = T.dot(X, self.W_f) + self.b_f\n@@ -388,7 +399,6 @@ class LSTM(Layer):\n             truncate_gradient=self.truncate_gradient \n         )\n \n-        outputs = mask * outputs + (1 - mask) * self.mask_val\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n@@ -402,7 +412,6 @@ class LSTM(Layer):\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n-            \"return_sequences\":self.return_sequences,\n-            \"mask_val\":self.mask_val.eval()}\n+            \"return_sequences\":self.return_sequences}\n         \n \n\n@@ -3,8 +3,6 @@ import numpy as np\n import theano\n import theano.tensor as T\n \n-default_mask_val = -999\n-\n def floatX(X):\n     return np.asarray(X, dtype=theano.config.floatX)\n \n@@ -23,28 +21,3 @@ def shared_ones(shape, dtype=theano.config.floatX, name=None):\n def alloc_zeros_matrix(*dims):\n     return T.alloc(np.cast[theano.config.floatX](0.), *dims)\n \n-\n-\n-def get_mask(X, mask_val, steps_back=0):\n-    '''\n-        Given X, a tensor or matrix, return mask tension of same number of dimensions thus:\n-                    X.shape                     mask.shape\n-            -------------------------------------------------\n-            (time, samples, dimensions)  -> (time, samples, 1)\n-            (samples, dimensions)        -> (samples, 1)\n-        \n-        and, if steps_back>0, a padded_mask tensor that is left-padded with `steps_back` zeros\n-        along the time dimension.\n-        \n-        The mask has a 1 for every entry except for those corresponding to a vector in X that has every entry equal to mask_val.\n-    '''\n-    mask = T.neq(X, mask_val).sum(axis=-1) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n-    mask = T.shape_padright(mask)\n-    mask = T.addbroadcast(mask, -1) # (time, nb_samples, 1) matrix.\n-\n-    if steps_back > 0:\n-        # left-pad in time with 0\n-        pad = alloc_zeros_matrix(steps_back, mask.shape[1], 1).astype('uint8')\n-        return mask, T.concatenate([pad, mask], axis=0)\n-    return mask\n-\n\n@@ -7,7 +7,6 @@ from keras.models import Sequential\n from keras.layers.core import Dense, Activation, Merge, Dropout, TimeDistributedDense\n from keras.layers.embeddings import Embedding\n from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN, LSTM, GRU\n-from keras.layers.core import default_mask_val\n import theano\n \n theano.config.exception_verbosity='high' \n@@ -17,7 +16,7 @@ X = np.random.random_integers(1, 4, size=(500000,15))\n \n print(\"About to compile the first model\")\n model = Sequential()\n-model.add(Embedding(5, 4, zero_is_mask=True))\n+model.add(Embedding(5, 4, mask_zero=True))\n model.add(TimeDistributedDense(4,4)) # obviously this is redundant. Just testing.\n model.add(SimpleRNN(4,4, activation='relu', return_sequences=True))\n model.add(Dropout(0.5))\n@@ -28,17 +27,8 @@ model.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n print(\"Compiled model\")\n \n-if model.layers[1].get_config()['mask_val'] != default_mask_val:\n-    raise Exception(\"mask_val improperly set in config. Set to %s not %f\" % (model.layers[1].get_config()['mask_val']))\n-\n-W0 = model.layers[0].W.get_value()[0,:]\n-if (W0 != default_mask_val).any():\n-    raise Exception(\"Did not set the mask val properly into the Embedding W matrix, got: \",\n-            W0)\n-\n W = model.get_weights() # We'll save these so we can reset it later\n \n-\n X[:,:10] = 0\n Xmask0 = X.copy()\n Xmask0[:,10] = 0\n@@ -65,11 +55,6 @@ score = model.evaluate(X, X0_onehot, batch_size=batch_size)\n if score > uniform_score*0.9:\n     raise Exception('Failed to learn to copy timestep 0, score %f' % score)\n     \n-W0 = model.layers[0].W.get_value()[0,:]\n-if (W0 != default_mask_val).any():\n-    raise Exception(\"After training, the W0 of the Embedding's mask value changed to: \",\n-            W0)\n-    \n \n model.set_weights(W)\n \n@@ -95,19 +80,12 @@ score = model.evaluate(Xmask0, X1_onehot, batch_size=batch_size)\n if score > uniform_score*0.9:\n     raise Exception('Failed to learn to copy timestep 1 in masked model, score %f' % score)\n \n-\n-\n model.set_weights(W)\n \n # Finally, make sure the mask is actually blocking input, mask out timesteps 1 and 2, and see if\n # it can learn timestep 0 (should fail)\n model.fit(Xmask12, X0_onehot, nb_epoch=1, batch_size=batch_size)\n \n-W0 = model.layers[0].W.get_value()[0,:]\n-if (W0 != default_mask_val).any():\n-    raise Exception(\"After masked training, the W0 of the Embedding's mask value changed to: \",\n-            W0)\n-\n score = model.evaluate(Xmask12, X0_onehot, batch_size=batch_size)\n if score < uniform_score*0.9:\n     raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n@@ -116,7 +94,7 @@ if score < uniform_score*0.9:\n # their output\n print(\"About to compile the second model\")\n model2 = Sequential()\n-model2.add(Embedding(5, 4, zero_is_mask=True))\n+model2.add(Embedding(5, 4, mask_zero=True))\n model2.add(TimeDistributedDense(4,4))\n model2.add(Activation('time_distributed_softmax'))\n model2.add(LSTM(4,4, return_sequences=True))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1564343c6e40803ffd2b1ce5f55e7329336bac64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1517 | Contributors (this commit): 17 | Commits (past 90d): 69 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -170,6 +170,8 @@ class Model(object):\n                 (y, y_val) = (y[:split_at], y[split_at:])\n                 if weights is not None:\n                     (weights, weight_val) = (weights[:split_at], weights[split_at:])\n+                else:\n+                    weight_val = None\n \n                 if verbose:\n                     print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1a39b80a966fb5a154fc9b5e455cd7dc36bed35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1064 | Contributors (this commit): 12 | Commits (past 90d): 57 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,5 @@\n from __future__ import absolute_import\n-import theano\n import theano.tensor as T\n-import numpy as np\n-import types\n \n def softmax(x):\n     return T.nnet.softmax(x.reshape((-1, x.shape[-1]))).reshape(x.shape)\n\n@@ -167,7 +167,6 @@ class Dropout(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        mask = self.get_output_mask(train)\n         if self.p > 0.:\n             retain_prob = 1. - self.p\n             if train:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31a303c38f42649972755fdacf6f27de62e79000", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1032 | Contributors (this commit): 11 | Commits (past 90d): 49 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import theano.tensor as T\n import numpy as np\n \n from .. import activations, initializations\n-from ..utils.theano_utils import shared_zeros, floatX, shared_scalar\n+from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n from .. import regularizers\n from .. import constraints\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#867253e5d862365d91a6645b4cc1fe03ad501470", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 43 | Files Changed: 5 | Hunks: 29 | Methods Changed: 15 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 86 | Churn Cumulative: 3310 | Contributors (this commit): 21 | Commits (past 90d): 151 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ class Sequential(Layer):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n-        self.cost_updates = []\n+        self.loss_updates = []\n \n         for layer in layers:\n             self.add(layer)\n@@ -39,7 +39,7 @@ class Sequential(Layer):\n         self.constraints += constraints\n \n         if hasattr(layer, 'cost_update'):\n-            self.cost_updates.append(layer.cost_update)\n+            self.loss_updates.append(layer.loss_update)\n \n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n\n@@ -286,7 +286,7 @@ class ActivityRegularization(Layer):\n         super(ActivityRegularization, self).__init__()\n         if activity_regularizer is not None:\n             activity_regularizer.set_layer(self)\n-            self.cost_update = activity_regularizer\n+            self.loss_update = activity_regularizer\n \n     def get_output(self, train):\n         return self.get_input(train)\n\n@@ -90,8 +90,8 @@ class Model(object):\n         self.class_mode = class_mode\n \n         if hasattr(self, 'cost_updates'):\n-            for u in self.cost_updates:\n-                if u.modifies_cost:\n+            for u in self.loss_updates:\n+                if u.modifies_loss:\n                     train_loss = u(train_loss)\n \n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n@@ -325,7 +325,7 @@ class Sequential(Model, containers.Sequential):\n         self.params = [] # learnable\n         self.regularizers = [] # same size as params\n         self.constraints = [] # same size as params\n-        self.cost_updates = [] # size can vary, no 1-to-1 mapping to params\n+        self.loss_updates = [] # size can vary, no 1-to-1 mapping to params\n \n \n     def get_config(self, verbose=0):\n\n@@ -16,12 +16,12 @@ def kl_divergence(p, p_hat):\n \n class Optimizer(object):\n     \n-    def get_updates(self, params, regularizers, constraints,  cost):\n+    def get_updates(self, params, regularizers, constraints,  loss):\n         raise NotImplementedError\n \n-    def get_gradients(self, cost, params, regularizers):\n+    def get_gradients(self, loss, params, regularizers):\n \n-        grads = T.grad(cost, params)\n+        grads = T.grad(loss, params)\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n             norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n@@ -42,8 +42,8 @@ class SGD(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, loss):\n+        grads = self.get_gradients(loss, params, regularizers)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n@@ -67,8 +67,8 @@ class RMSprop(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, loss):\n+        grads = self.get_gradients(loss, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -88,8 +88,8 @@ class Adagrad(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, loss):\n+        grads = self.get_gradients(loss, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -110,8 +110,8 @@ class Adadelta(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, loss):\n+        grads = self.get_gradients(loss, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n@@ -145,8 +145,8 @@ class Adam(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, loss):\n+        grads = self.get_gradients(loss, params, regularizers)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n\n@@ -5,16 +5,16 @@ import theano.tensor as T\n class Regularizer(object):\n     def __init__(self):\n         self.modifies_gradient = False\n-        self.modifies_cost = False\n+        self.modifies_loss = False\n \n     def update_gradient(self, gradient, params):\n         raise NotImplementedError\n \n-    def update_cost(self, cost):\n+    def update_loss(self, loss):\n         raise NotImplementedError\n \n \n-class RegularizerWeightsL1(Regularizer):\n+class WeightsL1(Regularizer):\n     def __init__(self, l=0.01):\n         super(Regularizer, self).__init__()\n \n@@ -29,7 +29,7 @@ class RegularizerWeightsL1(Regularizer):\n         return self.update_gradient(gradient, params)\n \n \n-class RegularizerWeightsL2(Regularizer):\n+class WeightsL2(Regularizer):\n     def __init__(self, l=0.01):\n         super(Regularizer, self).__init__()\n \n@@ -44,7 +44,7 @@ class RegularizerWeightsL2(Regularizer):\n         return self.update_gradient(gradient, params)\n \n \n-class RegularizerWeightsL1L2(Regularizer):\n+class WeightsL1L2(Regularizer):\n     def __init__(self, l1=0.01, l2=0.01):\n         super(Regularizer, self).__init__()\n \n@@ -60,7 +60,7 @@ class RegularizerWeightsL1L2(Regularizer):\n     def __call__(self, gradient, params):\n         return self.update_gradient(gradient, params)\n \n-class RegularizerIdentity(Regularizer):\n+class Identity(Regularizer):\n     def __init__(self):\n         super(Regularizer, self).__init__()\n \n@@ -73,45 +73,45 @@ class RegularizerIdentity(Regularizer):\n         return self.update_gradient(gradient, params)\n \n \n-class RegularizerActivityL1(Regularizer):\n+class ActivityL1(Regularizer):\n     def __init__(self, l = 0.01):\n         super(Regularizer, self).__init__()\n \n         self.l = l\n         self.layer = None\n-        self.modifies_cost = True\n+        self.modifies_loss = True\n \n     def set_layer(self, layer):\n         self.layer = layer\n \n-    def update_cost(self, cost):\n-        return cost + self.l * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n+    def update_loss(self, loss):\n+        return loss + self.l * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n \n-    def __call__(self, cost):\n-        return self.update_cost(cost)\n+    def __call__(self, loss):\n+        return self.update_loss(loss)\n \n-class RegularizerActivityL2(Regularizer):\n+class ActivityL2(Regularizer):\n     def __init__(self, l = 0.01):\n         super(Regularizer, self).__init__()\n \n         self.l = l\n         self.layer = None\n-        self.modifies_cost = True\n+        self.modifies_loss = True\n \n     def set_layer(self, layer):\n         self.layer = layer\n \n-    def update_cost(self, cost):\n-        return cost + self.l * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n+    def update_loss(self, loss):\n+        return loss + self.l * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n \n-    def __call__(self, cost):\n-        return self.update_cost(cost)\n+    def __call__(self, loss):\n+        return self.update_loss(loss)\n \n #old style variables for backwards compatibility\n-l1 = RegularizerWeightsL1\n-l2 = RegularizerWeightsL2\n-l1l2 = RegularizerWeightsL1L2\n-identity = RegularizerIdentity()\n+l1 = weights_l1 = WeightsL1\n+l2 = weights_l2 = WeightsL2\n+l1l2 = weights_l1l2 = WeightsL1L2\n+identity = Identity()\n \n-activity_l1 = RegularizerActivityL1\n-activity_l2 = RegularizerActivityL2\n+activity_l1 = ActivityL1\n+activity_l2 = ActivityL2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93ff2240f329431e77f3e613a8dbfaab911c82e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 18 | Churn Cumulative: 738 | Contributors (this commit): 6 | Commits (past 90d): 30 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -416,9 +416,12 @@ class JZS1(Layer):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n-        # P_h used to project X onto different dimension\n-        P = np.ones((self.input_dim, self.output_dim), dtype=theano.config.floatX)\n-        P = P / np.linalg.norm(P, axis=0)\n+        # P_h used to project X onto different dimension, using sparse random projections\n+        if self.input_dim == self.output_dim:\n+            self.Pmat = theano.shared(np.identity(self.output_dim), name=None)\n+        else:\n+            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)) * 2 - 1\n+            P = 1 / np.sqrt(self.input_dim) * P\n             self.Pmat = theano.shared(P, name=None)\n \n         self.params = [\n@@ -519,9 +522,12 @@ class JZS2(Layer):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n-        # P_h used to project X onto different dimension\n-        P = np.ones((self.input_dim, self.output_dim), dtype=theano.config.floatX)\n-        P = P / np.linalg.norm(P, axis=0)\n+        # P_h used to project X onto different dimension, using sparse random projections\n+        if self.input_dim == self.output_dim:\n+            self.Pmat = theano.shared(np.identity(self.output_dim), name=None)\n+        else:\n+            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)) * 2 - 1\n+            P = 1 / np.sqrt(self.input_dim) * P\n             self.Pmat = theano.shared(P, name=None)\n \n         self.params = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e78c9e7588fb10ed26349cd036eb3ea2f6bc5f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 746 | Contributors (this commit): 6 | Commits (past 90d): 31 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -418,9 +418,9 @@ class JZS1(Layer):\n \n         # P_h used to project X onto different dimension, using sparse random projections\n         if self.input_dim == self.output_dim:\n-            self.Pmat = theano.shared(np.identity(self.output_dim), name=None)\n+            self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n         else:\n-            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)) * 2 - 1\n+            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n             P = 1 / np.sqrt(self.input_dim) * P\n             self.Pmat = theano.shared(P, name=None)\n \n@@ -524,9 +524,9 @@ class JZS2(Layer):\n \n         # P_h used to project X onto different dimension, using sparse random projections\n         if self.input_dim == self.output_dim:\n-            self.Pmat = theano.shared(np.identity(self.output_dim), name=None)\n+            self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n         else:\n-            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)) * 2 - 1\n+            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n             P = 1 / np.sqrt(self.input_dim) * P\n             self.Pmat = theano.shared(P, name=None)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9083dd088df93663bba51459222adf610a011847", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -56,3 +56,7 @@ class TestRegularizers(unittest.TestCase):\n             model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n             model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n             model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+\n+if __name__ == '__main__':\n+    print('Test weight and activity regularizers')\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4eb8346df5bf0c6b41306dc0df78ae113df4000a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 19 | Churn Cumulative: 1206 | Contributors (this commit): 12 | Commits (past 90d): 64 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,7 @@ class Layer(object):\n         self.params = []\n \n     def connect(self, layer):\n-        if layer.get_output_mask() is not None and not self.supports_mask():\n+        if layer.get_output_mask() is not None and not self.supports_masked_input():\n             raise Exception(\"Attached non-masking layer to layer with masked output\")\n         self.previous = layer\n \n@@ -33,7 +33,7 @@ class Layer(object):\n         else:\n             return self.input\n \n-    def supports_mask(self):\n+    def supports_masked_input(self):\n         return False\n \n     def get_output_mask(self, train=None):\n@@ -83,14 +83,19 @@ class Layer(object):\n         return self.params, regs, consts\n \n class MaskedLayer(Layer):\n-    def supports_mask(self):\n+    def supports_masked_input(self):\n         return True\n \n-    def get_output_mask(self, train=None):\n-        return self.previous.get_output_mask(train)\n-\n     def get_input_mask(self, train=None):\n+        if hasattr(self, 'previous'):\n             return self.previous.get_output_mask(train)\n+        else:\n+            return None\n+\n+    def get_output_mask(self, train=None):\n+        ''' The default output mask is just the input mask unchanged. Override this in your own\n+        implementations if, for instance, you are reshaping the input'''\n+        return self.get_input_mask(train)\n \n \n class Merge(object): \n\n@@ -9,7 +9,7 @@ from ..utils.theano_utils import sharedX\n from ..constraints import unitnorm\n \n \n-class Embedding(MaskedLayer):\n+class Embedding(Layer):\n     '''\n         Turn positive integers (indexes) into denses vectors of fixed size. \n         eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37e3ffed85881f20878657156e31d4b208357f81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 79 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 113 | Churn Cumulative: 113 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,34 @@\n+import keras\n+import theano\n+import theano.tensor as T\n+\n+import numpy\n+\n+def list_assert_equal(a, b, round_to=7):\n+    pairs = zip(a, b)\n+    for i, j in pairs:\n+        assert round(i, round_to) == round(j, round_to)\n+\n+\n+def test_softmax():\n+\n+    from keras.activations import softmax as s\n+\n+    # Test using a reference implementation of softmax\n+    def softmax(values):\n+        m = max(values)\n+        values = numpy.array(values)\n+        e = numpy.exp(values - m)\n+        dist = list(e / numpy.sum(e))\n+\n+        return [dist]\n+\n+    x = T.vector()\n+    exp = s(x)\n+    f = theano.function([x], exp)\n+    test_values=[0,0.1,0.5,0.9,1.0]\n+\n+    result = f(test_values)\n+    expected = softmax(test_values)\n+\n+    list_assert_equal(result[0], expected[0])\n\n@@ -1,79 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n-from keras.utils import np_utils\n-import numpy as np\n-import unittest\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 5\n-weighted_class = 9\n-standard_weight = 1\n-high_weight = 5\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-np.random.seed(1337) # for reproducibility\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\") / 255\n-X_test = X_test.astype(\"float32\") / 255\n-\n-# convert class vectors to binary class matrices\n-y_train = y_train[:max_train_samples]\n-y_test = y_test[:max_test_samples]\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n-test_ids = np.where(y_test == np.array(weighted_class))[0]\n-\n-def create_model():\n-    model = Sequential()\n-    model.add(Dense(784, 50))\n-    model.add(Activation('relu'))\n-    model.add(Dense(50, 10))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-def test_weights(model, class_weight=None, sample_weight=None):\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, \\\n-        class_weight=class_weight, sample_weight=sample_weight)\n-    score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-    return score\n-\n-class TestConcatenation(unittest.TestCase):\n-\n-    def test_loss_weighting(self):\n-        class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n-        class_weight[weighted_class] = high_weight\n-\n-        sample_weight = np.ones((y_train.shape[0])) * standard_weight\n-        sample_weight[y_train == weighted_class] = high_weight\n-\n-        for loss in ['mae', 'mse', 'categorical_crossentropy']:\n-            print('loss:', loss)\n-            # no weights: reference point\n-            model = create_model()\n-            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            standard_score = test_weights(model)\n-            # test class_weight\n-            model = create_model()\n-            model.compile(loss=loss, optimizer='rmsprop')\n-            score = test_weights(model, class_weight=class_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n-            # test sample_weight\n-            model = create_model()\n-            model.compile(loss=loss, optimizer='rmsprop')\n-            score = test_weights(model, sample_weight=sample_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n-\n-if __name__ == '__main__':\n-    print('Test class_weight and sample_weight')\n-    unittest.main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
