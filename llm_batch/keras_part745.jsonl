{"custom_id": "keras#32d69a42b6811502d0763abec135e68270419319", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,7 +35,8 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n         # This is mainly due to the interaction with tf.distribute.Strategy,\n         # which requires tf.Variable as the inputs for most of its APIs.\n         raise ValueError(\n-            \"stateless_apply is not supported by the TF based optimizer.\"\n+            \"stateless_apply is not supported with the TensorFlow backend \"\n+            \"(as it is incompatible with tf.distribute).\"\n         )\n \n     def _var_key(self, variable):\n\n@@ -11,10 +11,12 @@ from keras.optimizers.sgd import SGD\n class LossScaleOptimizerTest(testing.TestCase, parameterized.TestCase):\n     def _skip_test_for_stateless(self, stateless):\n         if not stateless and backend.backend() == \"jax\":\n-            self.skipTest(\"LossScaleOptimizer must use stateless_apply on jax.\")\n+            self.skipTest(\n+                \"LossScaleOptimizer must use stateless_apply with JAX.\"\n+            )\n         if stateless and backend.backend() == \"tensorflow\":\n             self.skipTest(\n-                \"stateless_apply is not supported by the tf based optimizer\"\n+                \"stateless_apply is not supported with the TF backend.\"\n             )\n \n     def test_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d043c7c56796b9f37e7daa74aeebace78206ae78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1350 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,7 @@ def create_legacy_directory():\n     tf_keras_dirpath = os.path.join(tf_keras_dirpath_parent, \"keras\")\n     os.makedirs(tf_keras_dirpath)\n     with open(os.path.join(tf_keras_dirpath_parent, \"__init__.py\"), \"w\") as f:\n-        f.write(\"\")\n+        f.write(\"from keras._tf_keras import keras\\n\")\n     with open(os.path.join(package, \"__init__.py\")) as f:\n         init_file = f.read()\n         init_file = init_file.replace(\n@@ -86,7 +86,7 @@ def create_legacy_directory():\n                 ignore=ignore_files,\n             )\n \n-    # Copy keras/_legacy/ file contents to keras/_tf_keras/\n+    # Copy keras/_legacy/ file contents to keras/_tf_keras/keras\n     legacy_submodules = [\n         path[:-3]\n         for path in os.listdir(os.path.join(package, \"src\", \"legacy\"))\n@@ -102,7 +102,7 @@ def create_legacy_directory():\n         for fname in fnames:\n             if fname.endswith(\".py\"):\n                 legacy_fpath = os.path.join(root, fname)\n-                tf_keras_root = root.replace(\"/_legacy\", \"/_tf_keras\")\n+                tf_keras_root = root.replace(\"/_legacy\", \"/_tf_keras/keras\")\n                 core_api_fpath = os.path.join(\n                     root.replace(\"/_legacy\", \"\"), fname\n                 )\n@@ -112,7 +112,7 @@ def create_legacy_directory():\n                 with open(legacy_fpath) as f:\n                     legacy_contents = f.read()\n                     legacy_contents = legacy_contents.replace(\n-                        \"keras._legacy\", \"keras._tf_keras\"\n+                        \"keras._legacy\", \"keras._tf_keras.keras\"\n                     )\n                 if os.path.exists(core_api_fpath):\n                     with open(core_api_fpath) as f:\n@@ -127,7 +127,7 @@ def create_legacy_directory():\n                         )\n                         core_api_contents = core_api_contents.replace(\n                             f\"keras.{legacy_submodule}\",\n-                            f\"keras._tf_keras.{legacy_submodule}\",\n+                            f\"keras._tf_keras.keras.{legacy_submodule}\",\n                         )\n                     legacy_contents = core_api_contents + \"\\n\" + legacy_contents\n                 with open(tf_keras_fpath, \"w\") as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#879a1942e5aa364cf31e8d65cdf4e3720209b03f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -18,9 +18,9 @@ try:\n except ImportError:\n     h5py = None\n \n-TRAIN_SAMPLES = 10\n-TEST_SAMPLES = 10\n-NUM_CLASSES = 2\n+TRAIN_SAMPLES = 30\n+TEST_SAMPLES = 30\n+NUM_CLASSES = 3\n INPUT_DIM = 3\n NUM_HIDDEN = 5\n BATCH_SIZE = 5\n@@ -203,7 +203,7 @@ class ModelCheckpointTest(testing.TestCase):\n         model.fit(\n             x_train,\n             y_train,\n-            batch_size=2,  # 5 batches / epoch, so should backup every 3 epochs\n+            batch_size=6,  # 5 batches / epoch, so should backup every 3 epochs\n             validation_data=(x_test, y_test),\n             callbacks=cbks,\n             epochs=10,\n@@ -236,7 +236,7 @@ class ModelCheckpointTest(testing.TestCase):\n         model.fit(\n             x_train,\n             y_train,\n-            batch_size=2,\n+            batch_size=6,\n             validation_data=(x_test, y_test),\n             callbacks=cbks,\n             epochs=10,\n@@ -288,7 +288,7 @@ class ModelCheckpointTest(testing.TestCase):\n         model.fit(\n             x_train,\n             y_train,\n-            batch_size=5,\n+            batch_size=15,\n             validation_data=(x_test, y_test),\n             callbacks=cbks,\n             epochs=5,\n@@ -318,7 +318,7 @@ class ModelCheckpointTest(testing.TestCase):\n         model.fit(\n             x_train,\n             y_train,\n-            batch_size=5,\n+            batch_size=15,\n             validation_data=(x_test, y_test),\n             callbacks=cbks,\n             epochs=5,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ab9642de3a7961bd0173ba257b26c5a4dfce374", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 126 | Files Changed: 6 | Hunks: 24 | Methods Changed: 18 | Complexity Δ (Sum/Max): 11/28 | Churn Δ: 287 | Churn Cumulative: 345 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,8 @@ from keras import optimizers as optimizers_module\n from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.torch.core import is_tensor\n+from keras.trainers import data_adapters\n+from keras.trainers import epoch_iterator\n from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n@@ -259,7 +261,7 @@ class TorchTrainer(base_trainer.Trainer):\n         )\n         if needs_building:\n             # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            for _, data in epoch_iterator.enumerate_epoch():\n                 data_batch = data[0]\n                 self._symbolic_build(data_batch)\n                 break\n@@ -288,7 +290,7 @@ class TorchTrainer(base_trainer.Trainer):\n             # do training behavior in case the user did not use `self.training`\n             # when implementing a custom layer with torch layers.\n             self.train()\n-            for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            for step, data in epoch_iterator.enumerate_epoch():\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n@@ -384,7 +386,7 @@ class TorchTrainer(base_trainer.Trainer):\n \n         if not all(layer.built for layer in self._flatten_layers()):\n             # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            for _, data in epoch_iterator.enumerate_epoch():\n                 data_batch = data[0]\n                 self._symbolic_build(data_batch)\n                 break\n@@ -408,7 +410,7 @@ class TorchTrainer(base_trainer.Trainer):\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n-        for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+        for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n@@ -424,7 +426,7 @@ class TorchTrainer(base_trainer.Trainer):\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n         # Create an iterator that yields batches of input data.\n-        epoch_iterator = EpochIterator(\n+        epoch_iterator = TorchEpochIterator(\n             x=x,\n             batch_size=batch_size,\n             steps_per_epoch=steps,\n@@ -465,7 +467,7 @@ class TorchTrainer(base_trainer.Trainer):\n         self.make_predict_function()\n         callbacks.on_predict_begin()\n         outputs = None\n-        for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+        for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_predict_batch_begin(step)\n             batch_outputs = self.predict_function(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n@@ -535,3 +537,13 @@ class TorchTrainer(base_trainer.Trainer):\n             backend.convert_to_numpy, batch_outputs\n         )\n         return batch_outputs\n+\n+\n+class TorchEpochIterator(epoch_iterator.EpochIterator):\n+    def _get_iterator(self, return_type=\"auto\"):\n+        if return_type == \"auto\" and isinstance(\n+            self.data_adapter, data_adapters.TorchDataLoaderAdapter\n+        ):\n+            return self.data_adapter.get_torch_dataloader()\n+\n+        return super()._get_iterator(return_type)\n\n@@ -0,0 +1,125 @@\n+import types\n+\n+from keras.trainers.data_adapters import array_data_adapter\n+from keras.trainers.data_adapters import py_dataset_adapter\n+from keras.trainers.data_adapters.array_data_adapter import ArrayDataAdapter\n+from keras.trainers.data_adapters.generator_data_adapter import (\n+    GeneratorDataAdapter,\n+)\n+from keras.trainers.data_adapters.py_dataset_adapter import PyDatasetAdapter\n+from keras.trainers.data_adapters.tf_dataset_adapter import TFDatasetAdapter\n+from keras.trainers.data_adapters.torch_data_loader_adapter import (\n+    TorchDataLoaderAdapter,\n+)\n+\n+\n+def get_data_adapter(\n+    x,\n+    y=None,\n+    sample_weight=None,\n+    batch_size=None,\n+    steps_per_epoch=None,\n+    shuffle=False,\n+    class_weight=None,\n+):\n+    if array_data_adapter.can_convert_arrays((x, y, sample_weight)):\n+        return ArrayDataAdapter(\n+            x,\n+            y,\n+            sample_weight=sample_weight,\n+            class_weight=class_weight,\n+            shuffle=shuffle,\n+            batch_size=batch_size,\n+            steps=steps_per_epoch,\n+        )\n+    elif is_tf_dataset(x):\n+        # Unsupported args: y, sample_weight, shuffle\n+        if y is not None:\n+            raise_unsupported_arg(\"y\", \"the targets\", \"tf.data.Dataset\")\n+        if sample_weight is not None:\n+            raise_unsupported_arg(\n+                \"sample_weights\", \"the sample weights\", \"tf.data.Dataset\"\n+            )\n+        return TFDatasetAdapter(x, class_weight=class_weight)\n+        # TODO: should we warn or not?\n+        # warnings.warn(\n+        #     \"`shuffle=True` was passed, but will be ignored since the \"\n+        #     \"data `x` was provided as a tf.data.Dataset. The Dataset is \"\n+        #     \"expected to already be shuffled \"\n+        #     \"(via `.shuffle(tf.data.AUTOTUNE)`)\"\n+        # )\n+    elif isinstance(x, py_dataset_adapter.PyDataset):\n+        if y is not None:\n+            raise_unsupported_arg(\"y\", \"the targets\", \"PyDataset\")\n+        if sample_weight is not None:\n+            raise_unsupported_arg(\n+                \"sample_weights\", \"the sample weights\", \"PyDataset\"\n+            )\n+        return PyDatasetAdapter(x, class_weight=class_weight, shuffle=shuffle)\n+    elif is_torch_dataloader(x):\n+        if y is not None:\n+            raise_unsupported_arg(\"y\", \"the targets\", \"torch DataLoader\")\n+        if sample_weight is not None:\n+            raise_unsupported_arg(\n+                \"sample_weights\", \"the sample weights\", \"torch DataLoader\"\n+            )\n+        if class_weight is not None:\n+            raise ValueError(\n+                \"Argument `class_weight` is not supported for torch \"\n+                f\"DataLoader inputs. Received: class_weight={class_weight}\"\n+            )\n+        return TorchDataLoaderAdapter(x)\n+        # TODO: should we warn or not?\n+        # warnings.warn(\n+        #     \"`shuffle=True` was passed, but will be ignored since the \"\n+        #     \"data `x` was provided as a torch DataLoader. The DataLoader \"\n+        #     \"is expected to already be shuffled.\"\n+        # )\n+    elif isinstance(x, types.GeneratorType):\n+        if y is not None:\n+            raise_unsupported_arg(\"y\", \"the targets\", \"PyDataset\")\n+        if sample_weight is not None:\n+            raise_unsupported_arg(\n+                \"sample_weights\", \"the sample weights\", \"PyDataset\"\n+            )\n+        if class_weight is not None:\n+            raise ValueError(\n+                \"Argument `class_weight` is not supported for Python \"\n+                f\"generator inputs. Received: class_weight={class_weight}\"\n+            )\n+        if shuffle:\n+            raise ValueError(\n+                \"Argument `shuffle` is not supported for Python generator \"\n+                f\"inputs. Received: shuffle={shuffle}\"\n+            )\n+        return GeneratorDataAdapter(x)\n+    else:\n+        raise ValueError(f\"Unrecognized data type: x={x} (of type {type(x)})\")\n+\n+\n+def raise_unsupported_arg(arg_name, arg_description, input_type):\n+    raise ValueError(\n+        f\"When providing `x` as a {input_type}, `{arg_name}` \"\n+        f\"should not be passed. Instead, {arg_description} should \"\n+        f\"be included as part of the {input_type}.\"\n+    )\n+\n+\n+def is_tf_dataset(x):\n+    if hasattr(x, \"__class__\"):\n+        for parent in x.__class__.__mro__:\n+            if parent.__name__ == \"DatasetV2\" and str(\n+                parent.__module__\n+            ).startswith(\"tensorflow.python.types.data\"):\n+                return True\n+    return False\n+\n+\n+def is_torch_dataloader(x):\n+    if hasattr(x, \"__class__\"):\n+        for parent in x.__class__.__mro__:\n+            if parent.__name__ == \"DataLoader\" and str(\n+                parent.__module__\n+            ).startswith(\"torch.utils.data\"):\n+                return True\n+    return False\n\n\n@@ -4,7 +4,7 @@ import tensorflow as tf\n \n from keras import backend\n from keras import testing\n-from keras.trainers.data_adapters.torch_data_adapter import (\n+from keras.trainers.data_adapters.torch_data_loader_adapter import (\n     TorchDataLoaderAdapter,\n )\n \n\n@@ -38,14 +38,9 @@ EpochIterator steps:\n or until there is no data\n \n \"\"\"\n-import types\n import warnings\n \n-from keras.trainers.data_adapters import array_data_adapter\n-from keras.trainers.data_adapters import generator_data_adapter\n-from keras.trainers.data_adapters import py_dataset_adapter\n-from keras.trainers.data_adapters import tf_dataset_adapter\n-from keras.trainers.data_adapters import torch_data_adapter\n+from keras.trainers import data_adapters\n \n \n class EpochIterator:\n@@ -65,100 +60,30 @@ class EpochIterator:\n         if steps_per_epoch:\n             self._current_iterator = None\n             self._insufficient_data = False\n-        if array_data_adapter.can_convert_arrays((x, y, sample_weight)):\n-            self.data_adapter = array_data_adapter.ArrayDataAdapter(\n-                x,\n-                y,\n+        self.data_adapter = data_adapters.get_data_adapter(\n+            x=x,\n+            y=y,\n             sample_weight=sample_weight,\n-                class_weight=class_weight,\n-                shuffle=shuffle,\n             batch_size=batch_size,\n-                steps=steps_per_epoch,\n-            )\n-        elif is_tf_dataset(x):\n-            self.data_adapter = tf_dataset_adapter.TFDatasetAdapter(\n-                x, class_weight=class_weight\n-            )\n-            # Unsupported args: y, sample_weight, shuffle\n-            if y is not None:\n-                raise_unsupported_arg(\"y\", \"the targets\", \"tf.data.Dataset\")\n-            if sample_weight is not None:\n-                raise_unsupported_arg(\n-                    \"sample_weights\", \"the sample weights\", \"tf.data.Dataset\"\n-                )\n-            # TODO: should we warn or not?\n-            # warnings.warn(\n-            #     \"`shuffle=True` was passed, but will be ignored since the \"\n-            #     \"data `x` was provided as a tf.data.Dataset. The Dataset is \"\n-            #     \"expected to already be shuffled \"\n-            #     \"(via `.shuffle(tf.data.AUTOTUNE)`)\"\n-            # )\n-        elif isinstance(x, py_dataset_adapter.PyDataset):\n-            self.data_adapter = py_dataset_adapter.PyDatasetAdapter(\n-                x, class_weight=class_weight, shuffle=shuffle\n-            )\n-            if y is not None:\n-                raise_unsupported_arg(\"y\", \"the targets\", \"PyDataset\")\n-            if sample_weight is not None:\n-                raise_unsupported_arg(\n-                    \"sample_weights\", \"the sample weights\", \"PyDataset\"\n-                )\n-        elif is_torch_dataloader(x):\n-            self.data_adapter = torch_data_adapter.TorchDataLoaderAdapter(x)\n-            if y is not None:\n-                raise_unsupported_arg(\"y\", \"the targets\", \"torch DataLoader\")\n-            if sample_weight is not None:\n-                raise_unsupported_arg(\n-                    \"sample_weights\", \"the sample weights\", \"torch DataLoader\"\n-                )\n-            if class_weight is not None:\n-                raise ValueError(\n-                    \"Argument `class_weight` is not supported for torch \"\n-                    f\"DataLoader inputs. Received: class_weight={class_weight}\"\n-                )\n-            # TODO: should we warn or not?\n-            # warnings.warn(\n-            #     \"`shuffle=True` was passed, but will be ignored since the \"\n-            #     \"data `x` was provided as a torch DataLoader. The DataLoader \"\n-            #     \"is expected to already be shuffled.\"\n-            # )\n-        elif isinstance(x, types.GeneratorType):\n-            self.data_adapter = generator_data_adapter.GeneratorDataAdapter(x)\n-            if y is not None:\n-                raise_unsupported_arg(\"y\", \"the targets\", \"PyDataset\")\n-            if sample_weight is not None:\n-                raise_unsupported_arg(\n-                    \"sample_weights\", \"the sample weights\", \"PyDataset\"\n-                )\n-            if class_weight is not None:\n-                raise ValueError(\n-                    \"Argument `class_weight` is not supported for Python \"\n-                    f\"generator inputs. Received: class_weight={class_weight}\"\n-                )\n-            if shuffle:\n-                raise ValueError(\n-                    \"Argument `shuffle` is not supported for Python generator \"\n-                    f\"inputs. Received: shuffle={shuffle}\"\n-                )\n-        else:\n-            raise ValueError(\n-                f\"Unrecognized data type: x={x} (of type {type(x)})\"\n+            steps_per_epoch=steps_per_epoch,\n+            shuffle=shuffle,\n+            class_weight=class_weight,\n         )\n         self._num_batches = self.data_adapter.num_batches\n \n-    def _get_iterator(self, return_type):\n-        if return_type not in (\"np\", \"tf\"):\n+    def _get_iterator(self, return_type=\"auto\"):\n+        if return_type not in (\"np\", \"tf\", \"auto\"):\n             raise ValueError(\n-                \"Argument `return_type` must be one of `{'np', 'tf'}`. \"\n+                \"Argument `return_type` must be one of `{'np', 'tf', 'auto'}`. \"\n                 f\"Received instead: return_type={return_type}\"\n             )\n-        if return_type == \"np\":\n-            iterator = self.data_adapter.get_numpy_iterator()\n-        else:\n+        if return_type == \"tf\":\n             iterator = self.data_adapter.get_tf_dataset()\n+        else:\n+            iterator = self.data_adapter.get_numpy_iterator()\n         return iterator\n \n-    def enumerate_epoch(self, return_type=\"np\"):\n+    def enumerate_epoch(self, return_type=\"auto\"):\n         buffer = []\n         if self.steps_per_epoch:\n             if not self._current_iterator:\n@@ -209,31 +134,3 @@ class EpochIterator:\n         # Either copied from the data_adapter, or\n         # inferred at the end of an iteration.\n         return self._num_batches\n-\n-\n-def raise_unsupported_arg(arg_name, arg_description, input_type):\n-    raise ValueError(\n-        f\"When providing `x` as a {input_type}, `{arg_name}` \"\n-        f\"should not be passed. Instead, {arg_description} should \"\n-        f\"be included as part of the {input_type}.\"\n-    )\n-\n-\n-def is_tf_dataset(x):\n-    if hasattr(x, \"__class__\"):\n-        for parent in x.__class__.__mro__:\n-            if parent.__name__ == \"DatasetV2\" and str(\n-                parent.__module__\n-            ).startswith(\"tensorflow.python.types.data\"):\n-                return True\n-    return False\n-\n-\n-def is_torch_dataloader(x):\n-    if hasattr(x, \"__class__\"):\n-        for parent in x.__class__.__mro__:\n-            if parent.__name__ == \"DataLoader\" and str(\n-                parent.__module__\n-            ).startswith(\"torch.utils.data\"):\n-                return True\n-    return False\n\n@@ -4,6 +4,7 @@ import tensorflow as tf\n \n from keras import backend\n from keras import testing\n+from keras.trainers import data_adapters\n from keras.trainers import epoch_iterator\n \n \n@@ -172,7 +173,7 @@ class TestEpochIterator(testing.TestCase):\n         epoch_iter = epoch_iterator.EpochIterator(x=x)\n         self.assertIsInstance(\n             epoch_iter.data_adapter,\n-            epoch_iterator.generator_data_adapter.GeneratorDataAdapter,\n+            data_adapters.GeneratorDataAdapter,\n         )\n \n     def test_unrecognized_data_type(self):\n@@ -187,6 +188,6 @@ class TestEpochIterator(testing.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Argument `return_type` must be one of `{'np', 'tf'}`\",\n+            \"Argument `return_type` must be one of `{'np', 'tf', 'auto'}`\",\n         ):\n             _ = epoch_iter._get_iterator(\"unsupported\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#299419aa47deaf1526ebf6c347169c1f0dc9d4ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 1303 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -99,14 +99,16 @@ class Dense(Layer):\n                 initializer=self.bias_initializer,\n                 regularizer=self.bias_regularizer,\n             )\n+        else:\n+            self.bias = None\n         self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n         self.built = True\n \n     def call(self, inputs):\n         x = ops.matmul(inputs, self.kernel)\n-        if self.use_bias:\n+        if self.bias is not None:\n             x = x + self.bias\n-        if self.activation:\n+        if self.activation is not None:\n             x = self.activation(x)\n         return x\n \n\n@@ -47,6 +47,7 @@ class DenseTest(testing.TestCase):\n         )\n \n     def test_dense_correctness(self):\n+        # With bias and activation.\n         layer = layers.Dense(units=2, activation=\"relu\")\n         layer.build((1, 2))\n         layer.set_weights(\n@@ -60,6 +61,20 @@ class DenseTest(testing.TestCase):\n         )\n         self.assertAllClose(layer(inputs), [[10.0, 0.0]])\n \n+        # Just a kernel matmul.\n+        layer = layers.Dense(units=2, use_bias=False)\n+        layer.build((1, 2))\n+        layer.set_weights(\n+            [\n+                np.array([[1.0, -2.0], [3.0, -4.0]]),\n+            ]\n+        )\n+        inputs = np.array(\n+            [[-1.0, 2.0]],\n+        )\n+        self.assertEqual(layer.bias, None)\n+        self.assertAllClose(layer(inputs), [[5.0, -6.0]])\n+\n     def test_dense_errors(self):\n         with self.assertRaisesRegex(ValueError, \"incompatible with the layer\"):\n             layer = layers.Dense(units=2, activation=\"relu\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1537470e174ef10b47e8abcc00c4cfb0be4d9345", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 35 | Churn Cumulative: 297 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class KerasVariable:\n         self._dtype = dtype\n         self._shape = None\n         self._initializer = None\n-        self.trainable = trainable\n+        self._trainable = trainable\n         if callable(initializer):\n             if shape is None:\n                 raise ValueError(\n@@ -151,6 +151,14 @@ class KerasVariable:\n     def ndim(self):\n         return self._ndim\n \n+    @property\n+    def trainable(self):\n+        return self._trainable\n+\n+    @trainable.setter\n+    def trainable(self, value):\n+        self._trainable = value\n+\n     def __repr__(self):\n         return (\n             f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n\n@@ -35,6 +35,21 @@ class VariablesTest(test_case.TestCase):\n         out = scope.get_current_value(v)\n         self.assertAllClose(out, np.ones((2, 2)))\n \n+    def test_trainable_setter(self):\n+        v = backend.Variable(\n+            initializer=initializers.RandomNormal(),\n+            shape=(2, 2),\n+        )\n+        self.assertTrue(v.trainable)\n+        v.trainable = False\n+        self.assertFalse(v.trainable)\n+\n+        if backend.backend() == \"torch\":\n+            v.trainable = True\n+            self.assertTrue(v._value.requires_grad)\n+            v.trainable = False\n+            self.assertFalse(v._value.requires_grad)\n+\n     def test_autocasting(self):\n         v = backend.Variable(\n             initializer=initializers.RandomNormal(),\n\n@@ -113,6 +113,16 @@ class Variable(KerasVariable):\n             )\n         return value\n \n+    @property\n+    def trainable(self):\n+        return self._trainable\n+\n+    @trainable.setter\n+    def trainable(self, value):\n+        self._trainable = value\n+        if self._value is not None:\n+            self._value.requires_grad = value\n+\n     def __eq__(self, other):\n         try:\n             return super().__eq__(other)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fab88ec1975a0b35cec88c477fc783ed09c39d03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 125 | Files Changed: 4 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): -2/3 | Churn Δ: 159 | Churn Cumulative: 340 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,31 +0,0 @@\n-import torch\n-\n-from keras import layers\n-from keras.backend.common import KerasVariable\n-\n-\n-class Net(torch.nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-        self.fc1 = layers.Dense(1)\n-\n-    def forward(self, x):\n-        x = self.fc1(x)\n-        return x\n-\n-\n-net = Net()\n-\n-# Test using Keras layer in a nn.Module.\n-# Test forward pass\n-assert list(net(torch.empty(100, 10)).shape) == [100, 1]\n-# Test KerasVariables are added as nn.Parameter.\n-assert len(list(net.parameters())) == 2\n-optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n-\n-\n-# Test using KerasVariable as a torch tensor for torch ops.\n-kernel = net.fc1.kernel\n-transposed_kernel = torch.transpose(kernel, 0, 1)\n-assert isinstance(kernel, KerasVariable)\n-assert isinstance(torch.mul(kernel, transposed_kernel), torch.Tensor)\n\n\n@@ -1,94 +0,0 @@\n-import numpy as np\n-\n-import keras\n-from keras import layers\n-from keras import ops\n-\n-keras.utils.set_random_seed(1337)\n-x = np.random.rand(100, 32, 32, 3)\n-y = np.random.randint(0, 2, size=(100, 1))\n-\n-# Test sequential model.\n-model = keras.Sequential(\n-    [\n-        layers.Conv2D(filters=10, kernel_size=3),\n-        layers.GlobalAveragePooling2D(),\n-        layers.Dense(1, activation=\"sigmoid\"),\n-    ]\n-)\n-model.compile(\n-    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n-)\n-history = model.fit(\n-    x=x,\n-    y=y,\n-    epochs=10,\n-    validation_data=(x, y),\n-    verbose=0,\n-)\n-model.evaluate(x, y, verbose=0)\n-model.predict(x, verbose=0)\n-\n-# Test on batch functions\n-model.train_on_batch(x, y)\n-model.test_on_batch(x, y)\n-model.predict_on_batch(x)\n-\n-# Test functional model.\n-inputs = keras.Input(shape=(32, 32, 3))\n-outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n-outputs = layers.GlobalAveragePooling2D()(outputs)\n-outputs = layers.Dense(1, activation=\"sigmoid\")(outputs)\n-model = keras.Model(inputs, outputs)\n-model.compile(\n-    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n-)\n-history = model.fit(\n-    x=x,\n-    y=y,\n-    epochs=10,\n-    validation_data=(x, y),\n-    verbose=0,\n-)\n-model.evaluate(x, y, verbose=0)\n-model.predict(x, verbose=0)\n-\n-\n-# Test custom layer\n-class Linear(layers.Layer):\n-    def __init__(self, units=32):\n-        super().__init__()\n-        self.units = units\n-\n-    def build(self, input_shape):\n-        self.w = self.add_weight(\n-            shape=(input_shape[-1], self.units),\n-            initializer=\"random_normal\",\n-            trainable=True,\n-        )\n-        self.b = self.add_weight(\n-            shape=(self.units,), initializer=\"random_normal\", trainable=True\n-        )\n-\n-    def call(self, inputs):\n-        return ops.matmul(inputs, self.w) + self.b\n-\n-\n-inputs = keras.Input(shape=(32, 32, 3))\n-outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n-outputs = layers.GlobalAveragePooling2D()(outputs)\n-outputs = Linear(1)(outputs)\n-outputs = layers.Activation(\"sigmoid\")(outputs)\n-model = keras.Model(inputs, outputs)\n-model.compile(\n-    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n-)\n-history = model.fit(\n-    x=x,\n-    y=y,\n-    epochs=10,\n-    validation_data=(x, y),\n-    verbose=0,\n-)\n-model.evaluate(x, y, verbose=0)\n-model.predict(x, verbose=0)\n\n@@ -0,0 +1,34 @@\n+import torch\n+\n+from keras import layers\n+from keras import testing\n+from keras.backend.common import KerasVariable\n+\n+\n+class Net(torch.nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+        self.fc1 = layers.Dense(1)\n+\n+    def forward(self, x):\n+        x = self.fc1(x)\n+        return x\n+\n+\n+class TorchWorkflowTest(testing.TestCase):\n+    def test_keras_layer_in_nn_module(self):\n+        net = Net()\n+\n+        # Test using Keras layer in a nn.Module.\n+        # Test forward pass\n+        self.assertAllEqual(list(net(torch.empty(100, 10)).shape), [100, 1])\n+        # Test KerasVariables are added as nn.Parameter.\n+        self.assertLen(list(net.parameters()), 2)\n+\n+        # Test using KerasVariable as a torch tensor for torch ops.\n+        kernel = net.fc1.kernel\n+        transposed_kernel = torch.transpose(kernel, 0, 1)\n+        self.assertIsInstance(kernel, KerasVariable)\n+        self.assertIsInstance(\n+            torch.mul(kernel, transposed_kernel), torch.Tensor\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8633ffa87f5d113c17dcb91f9e3ab541d8a82ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 31 | Files Changed: 5 | Hunks: 31 | Methods Changed: 19 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 103 | Churn Cumulative: 12942 | Contributors (this commit): 36 | Commits (past 90d): 11 | Contributors (cumulative): 70 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,9 +81,14 @@ MODEL_LIST = [\n     (resnet_v2.ResNet101V2, 2048, resnet_v2),\n     (resnet_v2.ResNet152V2, 2048, resnet_v2),\n ]\n-# Add names for `named_parameters`.\n-MODEL_LIST = [(e[0].__name__, *e) for e in MODEL_LIST]\n+MODELS_UNSUPPORTED_CHANNELS_FIRST = [\"ConvNeXt\", \"DenseNet\", \"NASNet\"]\n \n+# Add names for `named_parameters`, and add each data format for each model\n+test_parameters = [\n+    ('{}_{}'.format(model[0].__name__, image_data_format), *model, image_data_format)\n+    for image_data_format in [\"channels_first\", \"channels_last\"]\n+    for model in MODEL_LIST\n+]\n \n def _get_elephant(target_size):\n     # For models that don't include a Flatten step,\n@@ -109,44 +114,73 @@ def _get_elephant(target_size):\n )\n @pytest.mark.requires_trainable_backend\n class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.named_parameters(MODEL_LIST)\n-    def test_application_notop_variable_input_channels(self, app, last_dim, _):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.original_image_data_format = backend.image_data_format()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        backend.set_image_data_format(cls.original_image_data_format)\n+\n+    def skip_if_invalid_image_data_format_for_model(self, app, image_data_format):\n+        does_not_support_channels_first = any(\n+            [unsupported_name.lower() in app.__name__.lower() for unsupported_name in\n+             MODELS_UNSUPPORTED_CHANNELS_FIRST])\n+        if image_data_format == \"channels_first\" and does_not_support_channels_first:\n+            self.skipTest(\n+                \"{} does not support channels first\".format(app.__name__)\n+            )\n+\n+    @parameterized.named_parameters(test_parameters)\n+    def test_application_notop_variable_input_channels(self, app, last_dim, _, image_data_format):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n             )\n+        self.skip_if_invalid_image_data_format_for_model(app, image_data_format)\n+        backend.set_image_data_format(image_data_format)\n \n         # Test compatibility with 1 channel\n-        if backend.image_data_format() == \"channels_first\":\n+        if image_data_format == \"channels_first\":\n             input_shape = (1, None, None)\n+            correct_output_shape = [None, last_dim, None, None]\n         else:\n             input_shape = (None, None, 1)\n+            correct_output_shape = [None, None, None, last_dim]\n+\n         model = app(weights=None, include_top=False, input_shape=input_shape)\n         output_shape = list(model.outputs[0].shape)\n-        self.assertEqual(output_shape, [None, None, None, last_dim])\n+        self.assertEqual(output_shape, correct_output_shape)\n \n         # Test compatibility with 4 channels\n-        if backend.image_data_format() == \"channels_first\":\n+        if image_data_format == \"channels_first\":\n             input_shape = (4, None, None)\n         else:\n             input_shape = (None, None, 4)\n         model = app(weights=None, include_top=False, input_shape=input_shape)\n         output_shape = list(model.outputs[0].shape)\n-        self.assertEqual(output_shape, [None, None, None, last_dim])\n+        self.assertEqual(output_shape, correct_output_shape)\n \n-    @parameterized.named_parameters(MODEL_LIST)\n+    @parameterized.named_parameters(test_parameters)\n     @pytest.mark.skipif(PIL is None, reason=\"Requires PIL.\")\n-    def test_application_base(self, app, _, app_module):\n+    def test_application_base(self, app, _, app_module, image_data_format):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n             )\n+        self.skip_if_invalid_image_data_format_for_model(app, image_data_format)\n+        backend.set_image_data_format(image_data_format)\n \n         # Can be instantiated with default arguments\n         model = app(weights=\"imagenet\")\n \n         # Can run a correct inference on a test image\n-        x = _get_elephant(model.input_shape[1:3])\n+        if image_data_format == \"channels_first\":\n+            shape = model.input_shape[2:4]\n+        else:\n+            shape = model.input_shape[1:3]\n+        x = _get_elephant(shape)\n+\n         x = app_module.preprocess_input(x)\n         preds = model.predict(x)\n         names = [p[1] for p in app_module.decode_predictions(preds)[0]]\n@@ -158,29 +192,39 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n         reconstructed_model = serialization_lib.deserialize_keras_object(config)\n         self.assertEqual(len(model.weights), len(reconstructed_model.weights))\n \n-    @parameterized.named_parameters(MODEL_LIST)\n-    def test_application_notop_custom_input_shape(self, app, last_dim, _):\n+    @parameterized.named_parameters(test_parameters)\n+    def test_application_notop_custom_input_shape(self, app, last_dim, _, image_data_format):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n             )\n+        self.skip_if_invalid_image_data_format_for_model(app, image_data_format)\n+        backend.set_image_data_format(image_data_format)\n \n-        model = app(weights=None, include_top=False, input_shape=(123, 123, 3))\n+        if image_data_format == \"channels_first\":\n+            input_shape = (3, 123, 123)\n+            last_dim_axis = 1\n+        else:\n+            input_shape = (123, 123, 3)\n+            last_dim_axis = -1\n+        model = app(weights=None, include_top=False, input_shape=input_shape)\n         output_shape = list(model.outputs[0].shape)\n-        self.assertEqual(output_shape[-1], last_dim)\n+        self.assertEqual(output_shape[last_dim_axis], last_dim)\n \n-    @parameterized.named_parameters(MODEL_LIST)\n-    def test_application_pooling(self, app, last_dim, _):\n+    @parameterized.named_parameters(test_parameters)\n+    def test_application_pooling(self, app, last_dim, _, image_data_format):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n             )\n+        self.skip_if_invalid_image_data_format_for_model(app, image_data_format)\n+        backend.set_image_data_format(image_data_format)\n \n         model = app(weights=None, include_top=False, pooling=\"max\")\n         output_shape = list(model.outputs[0].shape)\n         self.assertEqual(output_shape, [None, last_dim])\n \n-    @parameterized.named_parameters(MODEL_LIST)\n+    @parameterized.named_parameters(test_parameters)\n     def test_application_classifier_activation(self, app, *_):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n\n@@ -629,7 +629,7 @@ def MBConvBlock(\n                 strides=1,\n                 kernel_initializer=CONV_KERNEL_INITIALIZER,\n                 padding=\"same\",\n-                data_format=\"channels_last\",\n+                data_format=backend.image_data_format(),\n                 use_bias=False,\n                 name=name + \"expand_conv\",\n             )(inputs)\n@@ -650,7 +650,7 @@ def MBConvBlock(\n             strides=strides,\n             depthwise_initializer=CONV_KERNEL_INITIALIZER,\n             padding=\"same\",\n-            data_format=\"channels_last\",\n+            data_format=backend.image_data_format(),\n             use_bias=False,\n             name=name + \"dwconv2\",\n         )(x)\n@@ -695,7 +695,7 @@ def MBConvBlock(\n             strides=1,\n             kernel_initializer=CONV_KERNEL_INITIALIZER,\n             padding=\"same\",\n-            data_format=\"channels_last\",\n+            data_format=backend.image_data_format(),\n             use_bias=False,\n             name=name + \"project_conv\",\n         )(x)\n@@ -743,7 +743,7 @@ def FusedMBConvBlock(\n                 kernel_size=kernel_size,\n                 strides=strides,\n                 kernel_initializer=CONV_KERNEL_INITIALIZER,\n-                data_format=\"channels_last\",\n+                data_format=backend.image_data_format(),\n                 padding=\"same\",\n                 use_bias=False,\n                 name=name + \"expand_conv\",\n@@ -1020,7 +1020,7 @@ def EfficientNetV2(\n         strides=1,\n         kernel_initializer=CONV_KERNEL_INITIALIZER,\n         padding=\"same\",\n-        data_format=\"channels_last\",\n+        data_format=backend.image_data_format(),\n         use_bias=False,\n         name=\"top_conv\",\n     )(x)\n\n@@ -320,7 +320,8 @@ def obtain_input_shape(\n     \"\"\"\n     if weights != \"imagenet\" and input_shape and len(input_shape) == 3:\n         if data_format == \"channels_first\":\n-            if input_shape[0] not in {1, 3}:\n+            correct_channel_axis = 1 if len(input_shape) == 4 else 0\n+            if input_shape[correct_channel_axis] not in {1, 3}:\n                 warnings.warn(\n                     \"This model usually expects 1 or 3 input channels. \"\n                     \"However, it was passed an input_shape \"\n\n@@ -247,7 +247,10 @@ def MobileNetV3(\n                 input_shape = (cols, rows, 3)\n     # If input_shape is None and input_tensor is None using standard shape\n     if input_shape is None and input_tensor is None:\n+        if backend.image_data_format() == \"channels_last\":\n             input_shape = (None, None, 3)\n+        else:\n+            input_shape = (3, None, None)\n \n     if backend.image_data_format() == \"channels_last\":\n         row_axis, col_axis = (0, 1)\n\n@@ -161,10 +161,6 @@ def NASNet(\n             'to follow the \"channels_last\" data format.',\n             stacklevel=2,\n         )\n-        backend.set_image_data_format(\"channels_last\")\n-        old_data_format = \"channels_first\"\n-    else:\n-        old_data_format = None\n \n     if input_tensor is None:\n         img_input = layers.Input(shape=input_shape)\n@@ -305,9 +301,6 @@ def NASNet(\n     elif weights is not None:\n         model.load_weights(weights)\n \n-    if old_data_format:\n-        backend.set_image_data_format(old_data_format)\n-\n     return model\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c4d452a162398f8c6f5cef102796cb0bd333c08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 866 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -85,11 +85,16 @@ MODELS_UNSUPPORTED_CHANNELS_FIRST = [\"ConvNeXt\", \"DenseNet\", \"NASNet\"]\n \n # Add names for `named_parameters`, and add each data format for each model\n test_parameters = [\n-    ('{}_{}'.format(model[0].__name__, image_data_format), *model, image_data_format)\n+    (\n+        \"{}_{}\".format(model[0].__name__, image_data_format),\n+        *model,\n+        image_data_format,\n+    )\n     for image_data_format in [\"channels_first\", \"channels_last\"]\n     for model in MODEL_LIST\n ]\n \n+\n def _get_elephant(target_size):\n     # For models that don't include a Flatten step,\n     # the default is to accept variable-size inputs\n@@ -122,17 +127,27 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n     def tearDownClass(cls):\n         backend.set_image_data_format(cls.original_image_data_format)\n \n-    def skip_if_invalid_image_data_format_for_model(self, app, image_data_format):\n+    def skip_if_invalid_image_data_format_for_model(\n+        self, app, image_data_format\n+    ):\n         does_not_support_channels_first = any(\n-            [unsupported_name.lower() in app.__name__.lower() for unsupported_name in\n-             MODELS_UNSUPPORTED_CHANNELS_FIRST])\n-        if image_data_format == \"channels_first\" and does_not_support_channels_first:\n+            [\n+                unsupported_name.lower() in app.__name__.lower()\n+                for unsupported_name in MODELS_UNSUPPORTED_CHANNELS_FIRST\n+            ]\n+        )\n+        if (\n+            image_data_format == \"channels_first\"\n+            and does_not_support_channels_first\n+        ):\n             self.skipTest(\n                 \"{} does not support channels first\".format(app.__name__)\n             )\n \n     @parameterized.named_parameters(test_parameters)\n-    def test_application_notop_variable_input_channels(self, app, last_dim, _, image_data_format):\n+    def test_application_notop_variable_input_channels(\n+        self, app, last_dim, _, image_data_format\n+    ):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n@@ -193,7 +208,9 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(len(model.weights), len(reconstructed_model.weights))\n \n     @parameterized.named_parameters(test_parameters)\n-    def test_application_notop_custom_input_shape(self, app, last_dim, _, image_data_format):\n+    def test_application_notop_custom_input_shape(\n+        self, app, last_dim, _, image_data_format\n+    ):\n         if app == nasnet.NASNetMobile and backend.backend() == \"torch\":\n             self.skipTest(\n                 \"NASNetMobile pretrained incorrect with torch backend.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#440ae857b8f6bb90c9eeb3fe2206c87f9c69b541", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 18 | Churn Cumulative: 10370 | Contributors (this commit): 24 | Commits (past 90d): 8 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -389,6 +389,12 @@ def ConvNeXt(\n     Returns:\n         A model instance.\n     \"\"\"\n+    if backend.image_data_format() == \"channels_first\":\n+        raise ValueError(\n+            \"ConvNeXt does not support the `channels_first` image data \"\n+            \"format. Switch to `channels_last` by editing your local \"\n+            \"config file at ~/.keras/keras.json\"\n+        )\n     if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n\n@@ -180,6 +180,12 @@ def DenseNet(\n     Returns:\n         A model instance.\n     \"\"\"\n+    if backend.image_data_format() == \"channels_first\":\n+        raise ValueError(\n+            \"DenseNet does not support the `channels_first` image data \"\n+            \"format. Switch to `channels_last` by editing your local \"\n+            \"config file at ~/.keras/keras.json\"\n+        )\n     if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n\n@@ -109,6 +109,12 @@ def NASNet(\n     Returns:\n         A model instance.\n     \"\"\"\n+    if backend.image_data_format() == \"channels_first\":\n+        raise ValueError(\n+            \"NASNet does not support the `channels_first` image data \"\n+            \"format. Switch to `channels_last` by editing your local \"\n+            \"config file at ~/.keras/keras.json\"\n+        )\n     if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
