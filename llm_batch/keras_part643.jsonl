{"custom_id": "keras#b04bba2b00b998c58c59d9401508302c7ad0175b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 864 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 28 | Complexity Δ (Sum/Max): 43/43 | Churn Δ: 864 | Churn Cumulative: 864 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.8471502590673575\n\nDIFF:\n@@ -0,0 +1,864 @@\n+\"\"\"\n+Title: Denoising Diffusion Probabilistic Model\n+Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n+Date created: 2022/11/30\n+Last modified: 2022/12/07\n+Description: Generating images of flowers with denoising diffusion probabilistic models.\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+Generative modeling experienced tremendous growth in the last five years. Models like\n+VAEs, GANs, and flow-based models proved to be a great success in generating\n+high-quality content, especially images. Diffusion models are a new type of generative\n+model that has proven to be better than previous approaches.\n+\n+Diffusion models are inspired by non-equilibrium thermodynamics, and they learn to\n+generate by denoising. Learning by denoising consists of two processes,\n+each of which is a Markov Chain. These are:\n+\n+1. The forward process: In the forward process, we slowly add random noise to the data\n+in a series of time steps `(t1, t2, ..., tn )`. Samples at the current time step are\n+drawn from a Gaussian distribution where the mean of the distribution is conditioned\n+on the sample at the previous time step, and the variance of the distribution follows\n+a fixed schedule. At the end of the forward process, the samples end up with a pure\n+noise distribution.\n+\n+2. The reverse process: During the reverse process, we try to undo the added noise at\n+every time step. We start with the pure noise distribution (the last step of the\n+forward process) and try to denoise the samples in the backward direction\n+`(tn, tn-1, ..., t1)`.\n+\n+We implement the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n+paper or DDPMs for short in this code example. It was the first paper demonstrating\n+the use of diffusion models for generating high-quality images. The authors proved\n+that a certain parameterization of diffusion models reveals an equivalence with\n+denoising score matching over multiple noise levels during training and with annealed\n+Langevin dynamics during sampling that generates the best quality results.\n+\n+This paper replicates both the Markov chains (forward process and reverse process)\n+involved in the diffusion process but for images. The forward process is fixed and\n+gradually adds Gaussian noise to the images according to a fixed variance schedule\n+denoted by beta in the paper. This is what the diffusion process looks like in case\n+of images: (image -> noise::noise -> image)\n+\n+![diffusion process gif](https://imgur.com/Yn7tho9.gif)\n+\n+\n+The paper describes two algorithms, one for training the model, and the other for\n+sampling from the trained model. Training is performed by optimizing the usual\n+variational bound on negative log-likelihood. The objective function is further\n+simplified, and the network is treated as a noise prediction network. Once optimized,\n+we can sample from the network to generate new images from noise samples. Here is an\n+overview of both algorithms as presented in the paper:\n+\n+![ddpms](https://i.imgur.com/S7KH5hZ.png)\n+\n+\n+**Note:** DDPM is just one way of implementing a diffusion model. Also, the sampling\n+algorithm in the DDPM replicates the complete Markov chain. Hence it's slow in\n+generating new samples compared to other generative models like GANs. Lots of research\n+efforts have been made to address this issue. One such example is Denoising Diffusion\n+Implicit Models, or DDIM for short, where the authors replaced the Markov chain with a\n+non-Markovian process to sample faster. You can find the code example for DDIM\n+[here](https://keras.io/examples/generative/ddim/)\n+\n+Implementing a DDPM model is simple. We define a model that takes\n+two inputs: Images and the randomly sampled time steps. At each training step, we\n+perform the following operations to train our model:\n+\n+1. Sample random noise to be added to the inputs.\n+2. Apply the forward process to diffuse the inputs with the sampled noise.\n+3. Your model takes these noisy samples as inputs and outputs the noise\n+prediction for each time step.\n+4. Given true noise and predicted noise, we calculate the loss values\n+5. We then calculate the gradients and update the model weights.\n+\n+Given that our model knows how to denoise a noisy sample at a given time step,\n+we can leverage this idea to generate new samples, starting from a pure noise\n+distribution.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import math\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+# Requires TensorFlow >=2.11 for the GroupNormalization layer.\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+import tensorflow_datasets as tfds\n+\n+\"\"\"\n+## Hyperparameters\n+\"\"\"\n+\n+batch_size = 32\n+num_epochs = 1  # Just for the sake of demonstration\n+total_timesteps = 1000\n+norm_groups = 8  # Number of groups used in GroupNormalization layer\n+learning_rate = 2e-4\n+\n+img_size = 64\n+img_channels = 3\n+clip_min = -1.0\n+clip_max = 1.0\n+\n+first_conv_channels = 64\n+channel_multiplier = [1, 2, 4, 8]\n+widths = [first_conv_channels * mult for mult in channel_multiplier]\n+has_attention = [False, False, True, True]\n+num_res_blocks = 2  # Number of residual blocks\n+\n+dataset_name = \"oxford_flowers102\"\n+splits = [\"train\"]\n+\n+\n+\"\"\"\n+## Dataset\n+\n+We use the [Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102)\n+dataset for generating images of flowers. In terms of preprocessing, we use center\n+cropping for resizing the images to the desired image size, and we rescale the pixel\n+values in the range `[-1.0, 1.0]`. This is in line with the range of the pixel values that\n+was applied by the authors of the [DDPMs paper](https://arxiv.org/abs/2006.11239). For\n+augmenting training data, we randomly flip the images left/right.\n+\"\"\"\n+\n+\n+# Load the dataset\n+(ds,) = tfds.load(\n+    dataset_name, split=splits, with_info=False, shuffle_files=True\n+)\n+\n+\n+def augment(img):\n+    \"\"\"Flips an image left/right randomly.\"\"\"\n+    return tf.image.random_flip_left_right(img)\n+\n+\n+def resize_and_rescale(img, size):\n+    \"\"\"Resize the image to the desired size first and then\n+    rescale the pixel values in the range [-1.0, 1.0].\n+\n+    Args:\n+        img: Image tensor\n+        size: Desired image size for resizing\n+    Returns:\n+        Resized and rescaled image tensor\n+    \"\"\"\n+\n+    height = tf.shape(img)[0]\n+    width = tf.shape(img)[1]\n+    crop_size = tf.minimum(height, width)\n+\n+    img = tf.image.crop_to_bounding_box(\n+        img,\n+        (height - crop_size) // 2,\n+        (width - crop_size) // 2,\n+        crop_size,\n+        crop_size,\n+    )\n+\n+    # Resize\n+    img = tf.cast(img, dtype=tf.float32)\n+    img = tf.image.resize(img, size=size, antialias=True)\n+\n+    # Rescale the pixel values\n+    img = img / 127.5 - 1.0\n+    img = tf.clip_by_value(img, clip_min, clip_max)\n+    return img\n+\n+\n+def train_preprocessing(x):\n+    img = x[\"image\"]\n+    img = resize_and_rescale(img, size=(img_size, img_size))\n+    img = augment(img)\n+    return img\n+\n+\n+train_ds = (\n+    ds.map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n+    .batch(batch_size, drop_remainder=True)\n+    .shuffle(batch_size * 2)\n+    .prefetch(tf.data.AUTOTUNE)\n+)\n+\n+\n+\"\"\"\n+## Gaussian diffusion utilities\n+\n+We define the forward process and the reverse process\n+as a separate utility. Most of the code in this utility has been borrowed\n+from the original implementation with some slight modifications.\n+\"\"\"\n+\n+\n+class GaussianDiffusion:\n+    \"\"\"Gaussian diffusion utility.\n+\n+    Args:\n+        beta_start: Start value of the scheduled variance\n+        beta_end: End value of the scheduled variance\n+        timesteps: Number of time steps in the forward process\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        beta_start=1e-4,\n+        beta_end=0.02,\n+        timesteps=1000,\n+        clip_min=-1.0,\n+        clip_max=1.0,\n+    ):\n+        self.beta_start = beta_start\n+        self.beta_end = beta_end\n+        self.timesteps = timesteps\n+        self.clip_min = clip_min\n+        self.clip_max = clip_max\n+\n+        # Define the linear variance schedule\n+        self.betas = betas = np.linspace(\n+            beta_start,\n+            beta_end,\n+            timesteps,\n+            dtype=np.float64,  # Using float64 for better precision\n+        )\n+        self.num_timesteps = int(timesteps)\n+\n+        alphas = 1.0 - betas\n+        alphas_cumprod = np.cumprod(alphas, axis=0)\n+        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n+\n+        self.betas = tf.constant(betas, dtype=tf.float32)\n+        self.alphas_cumprod = tf.constant(alphas_cumprod, dtype=tf.float32)\n+        self.alphas_cumprod_prev = tf.constant(\n+            alphas_cumprod_prev, dtype=tf.float32\n+        )\n+\n+        # Calculations for diffusion q(x_t | x_{t-1}) and others\n+        self.sqrt_alphas_cumprod = tf.constant(\n+            np.sqrt(alphas_cumprod), dtype=tf.float32\n+        )\n+\n+        self.sqrt_one_minus_alphas_cumprod = tf.constant(\n+            np.sqrt(1.0 - alphas_cumprod), dtype=tf.float32\n+        )\n+\n+        self.log_one_minus_alphas_cumprod = tf.constant(\n+            np.log(1.0 - alphas_cumprod), dtype=tf.float32\n+        )\n+\n+        self.sqrt_recip_alphas_cumprod = tf.constant(\n+            np.sqrt(1.0 / alphas_cumprod), dtype=tf.float32\n+        )\n+        self.sqrt_recipm1_alphas_cumprod = tf.constant(\n+            np.sqrt(1.0 / alphas_cumprod - 1), dtype=tf.float32\n+        )\n+\n+        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n+        posterior_variance = (\n+            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n+        )\n+        self.posterior_variance = tf.constant(\n+            posterior_variance, dtype=tf.float32\n+        )\n+\n+        # Log calculation clipped because the posterior variance is 0 at the beginning\n+        # of the diffusion chain\n+        self.posterior_log_variance_clipped = tf.constant(\n+            np.log(np.maximum(posterior_variance, 1e-20)), dtype=tf.float32\n+        )\n+\n+        self.posterior_mean_coef1 = tf.constant(\n+            betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),\n+            dtype=tf.float32,\n+        )\n+\n+        self.posterior_mean_coef2 = tf.constant(\n+            (1.0 - alphas_cumprod_prev)\n+            * np.sqrt(alphas)\n+            / (1.0 - alphas_cumprod),\n+            dtype=tf.float32,\n+        )\n+\n+    def _extract(self, a, t, x_shape):\n+        \"\"\"Extract some coefficients at specified timesteps,\n+        then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n+\n+        Args:\n+            a: Tensor to extract from\n+            t: Timestep for which the coefficients are to be extracted\n+            x_shape: Shape of the current batched samples\n+        \"\"\"\n+        batch_size = x_shape[0]\n+        out = tf.gather(a, t)\n+        return tf.reshape(out, [batch_size, 1, 1, 1])\n+\n+    def q_mean_variance(self, x_start, t):\n+        \"\"\"Extracts the mean, and the variance at current timestep.\n+\n+        Args:\n+            x_start: Initial sample (before the first diffusion step)\n+            t: Current timestep\n+        \"\"\"\n+        x_start_shape = tf.shape(x_start)\n+        mean = (\n+            self._extract(self.sqrt_alphas_cumprod, t, x_start_shape) * x_start\n+        )\n+        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start_shape)\n+        log_variance = self._extract(\n+            self.log_one_minus_alphas_cumprod, t, x_start_shape\n+        )\n+        return mean, variance, log_variance\n+\n+    def q_sample(self, x_start, t, noise):\n+        \"\"\"Diffuse the data.\n+\n+        Args:\n+            x_start: Initial sample (before the first diffusion step)\n+            t: Current timestep\n+            noise: Gaussian noise to be added at the current timestep\n+        Returns:\n+            Diffused samples at timestep `t`\n+        \"\"\"\n+        x_start_shape = tf.shape(x_start)\n+        return (\n+            self._extract(self.sqrt_alphas_cumprod, t, tf.shape(x_start))\n+            * x_start\n+            + self._extract(\n+                self.sqrt_one_minus_alphas_cumprod, t, x_start_shape\n+            )\n+            * noise\n+        )\n+\n+    def predict_start_from_noise(self, x_t, t, noise):\n+        x_t_shape = tf.shape(x_t)\n+        return (\n+            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t_shape) * x_t\n+            - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t_shape)\n+            * noise\n+        )\n+\n+    def q_posterior(self, x_start, x_t, t):\n+        \"\"\"Compute the mean and variance of the diffusion\n+        posterior q(x_{t-1} | x_t, x_0).\n+\n+        Args:\n+            x_start: Stating point(sample) for the posterior computation\n+            x_t: Sample at timestep `t`\n+            t: Current timestep\n+        Returns:\n+            Posterior mean and variance at current timestep\n+        \"\"\"\n+\n+        x_t_shape = tf.shape(x_t)\n+        posterior_mean = (\n+            self._extract(self.posterior_mean_coef1, t, x_t_shape) * x_start\n+            + self._extract(self.posterior_mean_coef2, t, x_t_shape) * x_t\n+        )\n+        posterior_variance = self._extract(\n+            self.posterior_variance, t, x_t_shape\n+        )\n+        posterior_log_variance_clipped = self._extract(\n+            self.posterior_log_variance_clipped, t, x_t_shape\n+        )\n+        return (\n+            posterior_mean,\n+            posterior_variance,\n+            posterior_log_variance_clipped,\n+        )\n+\n+    def p_mean_variance(self, pred_noise, x, t, clip_denoised=True):\n+        x_recon = self.predict_start_from_noise(x, t=t, noise=pred_noise)\n+        if clip_denoised:\n+            x_recon = tf.clip_by_value(x_recon, self.clip_min, self.clip_max)\n+\n+        (\n+            model_mean,\n+            posterior_variance,\n+            posterior_log_variance,\n+        ) = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n+        return model_mean, posterior_variance, posterior_log_variance\n+\n+    def p_sample(self, pred_noise, x, t, clip_denoised=True):\n+        \"\"\"Sample from the diffuison model.\n+\n+        Args:\n+            pred_noise: Noise predicted by the diffusion model\n+            x: Samples at a given timestep for which the noise was predicted\n+            t: Current timestep\n+            clip_denoised (bool): Whether to clip the predicted noise\n+                within the specified range or not.\n+        \"\"\"\n+        model_mean, _, model_log_variance = self.p_mean_variance(\n+            pred_noise, x=x, t=t, clip_denoised=clip_denoised\n+        )\n+        noise = tf.random.normal(shape=x.shape, dtype=x.dtype)\n+        # No noise when t == 0\n+        nonzero_mask = tf.reshape(\n+            1 - tf.cast(tf.equal(t, 0), tf.float32), [tf.shape(x)[0], 1, 1, 1]\n+        )\n+        return (\n+            model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise\n+        )\n+\n+\n+\"\"\"\n+## Network architecture\n+\n+U-Net, originally developed for semantic segmentation, is an architecture that is\n+widely used for implementing diffusion models but with some slight modifications:\n+\n+1. The network accepts two inputs: Image and time step\n+2. Self-attention between the convolution blocks once we reach a specific resolution\n+(16x16 in the paper)\n+3. Group Normalization instead of weight normalization\n+\n+We implement most of the things as used in the original paper. We use the\n+`swish` activation function throughout the network. We use the variance scaling\n+kernel initializer.\n+\n+The only difference here is the number of groups used for the\n+`GroupNormalization` layer. For the flowers dataset,\n+we found that a value of `groups=8` produces better results\n+compared to the default value of `groups=32`. Dropout is optional and should be\n+used where chances of over fitting is high. In the paper, the authors used dropout\n+only when training on CIFAR10.\n+\"\"\"\n+\n+\n+# Kernel initializer to use\n+def kernel_init(scale):\n+    scale = max(scale, 1e-10)\n+    return keras.initializers.VarianceScaling(\n+        scale, mode=\"fan_avg\", distribution=\"uniform\"\n+    )\n+\n+\n+class AttentionBlock(layers.Layer):\n+    \"\"\"Applies self-attention.\n+\n+    Args:\n+        units: Number of units in the dense layers\n+        groups: Number of groups to be used for GroupNormalization layer\n+    \"\"\"\n+\n+    def __init__(self, units, groups=8, **kwargs):\n+        self.units = units\n+        self.groups = groups\n+        super().__init__(**kwargs)\n+\n+        self.norm = layers.GroupNormalization(groups=groups)\n+        self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n+        self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n+        self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n+        self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0))\n+\n+    def call(self, inputs):\n+        batch_size = tf.shape(inputs)[0]\n+        height = tf.shape(inputs)[1]\n+        width = tf.shape(inputs)[2]\n+        scale = tf.cast(self.units, tf.float32) ** (-0.5)\n+\n+        inputs = self.norm(inputs)\n+        q = self.query(inputs)\n+        k = self.key(inputs)\n+        v = self.value(inputs)\n+\n+        attn_score = tf.einsum(\"bhwc, bHWc->bhwHW\", q, k) * scale\n+        attn_score = tf.reshape(\n+            attn_score, [batch_size, height, width, height * width]\n+        )\n+\n+        attn_score = tf.nn.softmax(attn_score, -1)\n+        attn_score = tf.reshape(\n+            attn_score, [batch_size, height, width, height, width]\n+        )\n+\n+        proj = tf.einsum(\"bhwHW,bHWc->bhwc\", attn_score, v)\n+        proj = self.proj(proj)\n+        return inputs + proj\n+\n+\n+class TimeEmbedding(layers.Layer):\n+    def __init__(self, dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.dim = dim\n+        self.half_dim = dim // 2\n+        self.emb = math.log(10000) / (self.half_dim - 1)\n+        self.emb = tf.exp(tf.range(self.half_dim, dtype=tf.float32) * -self.emb)\n+\n+    def call(self, inputs):\n+        inputs = tf.cast(inputs, dtype=tf.float32)\n+        emb = inputs[:, None] * self.emb[None, :]\n+        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n+        return emb\n+\n+\n+def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish):\n+    def apply(inputs):\n+        x, t = inputs\n+        input_width = x.shape[3]\n+\n+        if input_width == width:\n+            residual = x\n+        else:\n+            residual = layers.Conv2D(\n+                width, kernel_size=1, kernel_initializer=kernel_init(1.0)\n+            )(x)\n+\n+        temb = activation_fn(t)\n+        temb = layers.Dense(width, kernel_initializer=kernel_init(1.0))(temb)[\n+            :, None, None, :\n+        ]\n+\n+        x = layers.GroupNormalization(groups=groups)(x)\n+        x = activation_fn(x)\n+        x = layers.Conv2D(\n+            width,\n+            kernel_size=3,\n+            padding=\"same\",\n+            kernel_initializer=kernel_init(1.0),\n+        )(x)\n+\n+        x = layers.Add()([x, temb])\n+        x = layers.GroupNormalization(groups=groups)(x)\n+        x = activation_fn(x)\n+\n+        x = layers.Conv2D(\n+            width,\n+            kernel_size=3,\n+            padding=\"same\",\n+            kernel_initializer=kernel_init(0.0),\n+        )(x)\n+        x = layers.Add()([x, residual])\n+        return x\n+\n+    return apply\n+\n+\n+def DownSample(width):\n+    def apply(x):\n+        x = layers.Conv2D(\n+            width,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"same\",\n+            kernel_initializer=kernel_init(1.0),\n+        )(x)\n+        return x\n+\n+    return apply\n+\n+\n+def UpSample(width, interpolation=\"nearest\"):\n+    def apply(x):\n+        x = layers.UpSampling2D(size=2, interpolation=interpolation)(x)\n+        x = layers.Conv2D(\n+            width,\n+            kernel_size=3,\n+            padding=\"same\",\n+            kernel_initializer=kernel_init(1.0),\n+        )(x)\n+        return x\n+\n+    return apply\n+\n+\n+def TimeMLP(units, activation_fn=keras.activations.swish):\n+    def apply(inputs):\n+        temb = layers.Dense(\n+            units, activation=activation_fn, kernel_initializer=kernel_init(1.0)\n+        )(inputs)\n+        temb = layers.Dense(units, kernel_initializer=kernel_init(1.0))(temb)\n+        return temb\n+\n+    return apply\n+\n+\n+def build_model(\n+    img_size,\n+    img_channels,\n+    widths,\n+    has_attention,\n+    num_res_blocks=2,\n+    norm_groups=8,\n+    interpolation=\"nearest\",\n+    activation_fn=keras.activations.swish,\n+):\n+    image_input = layers.Input(\n+        shape=(img_size, img_size, img_channels), name=\"image_input\"\n+    )\n+    time_input = keras.Input(shape=(), dtype=tf.int64, name=\"time_input\")\n+\n+    x = layers.Conv2D(\n+        first_conv_channels,\n+        kernel_size=(3, 3),\n+        padding=\"same\",\n+        kernel_initializer=kernel_init(1.0),\n+    )(image_input)\n+\n+    temb = TimeEmbedding(dim=first_conv_channels * 4)(time_input)\n+    temb = TimeMLP(units=first_conv_channels * 4, activation_fn=activation_fn)(\n+        temb\n+    )\n+\n+    skips = [x]\n+\n+    # DownBlock\n+    for i in range(len(widths)):\n+        for _ in range(num_res_blocks):\n+            x = ResidualBlock(\n+                widths[i], groups=norm_groups, activation_fn=activation_fn\n+            )([x, temb])\n+            if has_attention[i]:\n+                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n+            skips.append(x)\n+\n+        if widths[i] != widths[-1]:\n+            x = DownSample(widths[i])(x)\n+            skips.append(x)\n+\n+    # MiddleBlock\n+    x = ResidualBlock(\n+        widths[-1], groups=norm_groups, activation_fn=activation_fn\n+    )([x, temb])\n+    x = AttentionBlock(widths[-1], groups=norm_groups)(x)\n+    x = ResidualBlock(\n+        widths[-1], groups=norm_groups, activation_fn=activation_fn\n+    )([x, temb])\n+\n+    # UpBlock\n+    for i in reversed(range(len(widths))):\n+        for _ in range(num_res_blocks + 1):\n+            x = layers.Concatenate(axis=-1)([x, skips.pop()])\n+            x = ResidualBlock(\n+                widths[i], groups=norm_groups, activation_fn=activation_fn\n+            )([x, temb])\n+            if has_attention[i]:\n+                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n+\n+        if i != 0:\n+            x = UpSample(widths[i], interpolation=interpolation)(x)\n+\n+    # End block\n+    x = layers.GroupNormalization(groups=norm_groups)(x)\n+    x = activation_fn(x)\n+    x = layers.Conv2D(\n+        3, (3, 3), padding=\"same\", kernel_initializer=kernel_init(0.0)\n+    )(x)\n+    return keras.Model([image_input, time_input], x, name=\"unet\")\n+\n+\n+\"\"\"\n+## Training\n+\n+We follow the same setup for training the diffusion model as described\n+in the paper. We use `Adam` optimizer with a learning rate of `2e-4`.\n+We use EMA on model parameters with a decay factor of 0.999. We\n+treat our model as noise prediction network i.e. at every training step, we\n+input a batch of images and corresponding time steps to our UNet,\n+and the network outputs the noise as predictions.\n+\n+The only difference is that we aren't using the Kernel Inception Distance (KID)\n+or Frechet Inception Distance (FID) for evaluating the quality of generated\n+samples during training. This is because both these metrics are compute heavy\n+and are skipped for the brevity of implementation.\n+\n+**Note: ** We are using mean squared error as the loss function which is aligned with\n+the paper, and theoretically makes sense. In practice, though, it is also common to\n+use mean absolute error or Huber loss as the loss function.\n+\"\"\"\n+\n+\n+class DiffusionModel(keras.Model):\n+    def __init__(self, network, ema_network, timesteps, gdf_util, ema=0.999):\n+        super().__init__()\n+        self.network = network\n+        self.ema_network = ema_network\n+        self.timesteps = timesteps\n+        self.gdf_util = gdf_util\n+        self.ema = ema\n+\n+    def train_step(self, images):\n+        # 1. Get the batch size\n+        batch_size = tf.shape(images)[0]\n+\n+        # 2. Sample timesteps uniformly\n+        t = tf.random.uniform(\n+            minval=0, maxval=self.timesteps, shape=(batch_size,), dtype=tf.int64\n+        )\n+\n+        with tf.GradientTape() as tape:\n+            # 3. Sample random noise to be added to the images in the batch\n+            noise = tf.random.normal(shape=tf.shape(images), dtype=images.dtype)\n+\n+            # 4. Diffuse the images with noise\n+            images_t = self.gdf_util.q_sample(images, t, noise)\n+\n+            # 5. Pass the diffused images and time steps to the network\n+            pred_noise = self.network([images_t, t], training=True)\n+\n+            # 6. Calculate the loss\n+            loss = self.loss(noise, pred_noise)\n+\n+        # 7. Get the gradients\n+        gradients = tape.gradient(loss, self.network.trainable_weights)\n+\n+        # 8. Update the weights of the network\n+        self.optimizer.apply_gradients(\n+            zip(gradients, self.network.trainable_weights)\n+        )\n+\n+        # 9. Updates the weight values for the network with EMA weights\n+        for weight, ema_weight in zip(\n+            self.network.weights, self.ema_network.weights\n+        ):\n+            ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)\n+\n+        # 10. Return loss values\n+        return {\"loss\": loss}\n+\n+    def generate_images(self, num_images=16):\n+        # 1. Randomly sample noise (starting point for reverse process)\n+        samples = tf.random.normal(\n+            shape=(num_images, img_size, img_size, img_channels),\n+            dtype=tf.float32,\n+        )\n+        # 2. Sample from the model iteratively\n+        for t in reversed(range(0, self.timesteps)):\n+            tt = tf.cast(tf.fill(num_images, t), dtype=tf.int64)\n+            pred_noise = self.ema_network.predict(\n+                [samples, tt], verbose=0, batch_size=num_images\n+            )\n+            samples = self.gdf_util.p_sample(\n+                pred_noise, samples, tt, clip_denoised=True\n+            )\n+        # 3. Return generated samples\n+        return samples\n+\n+    def plot_images(\n+        self, epoch=None, logs=None, num_rows=2, num_cols=8, figsize=(12, 5)\n+    ):\n+        \"\"\"Utility to plot images using the diffusion model during training.\"\"\"\n+        generated_samples = self.generate_images(num_images=num_rows * num_cols)\n+        generated_samples = (\n+            tf.clip_by_value(generated_samples * 127.5 + 127.5, 0.0, 255.0)\n+            .numpy()\n+            .astype(np.uint8)\n+        )\n+\n+        _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n+        for i, image in enumerate(generated_samples):\n+            if num_rows == 1:\n+                ax[i].imshow(image)\n+                ax[i].axis(\"off\")\n+            else:\n+                ax[i // num_cols, i % num_cols].imshow(image)\n+                ax[i // num_cols, i % num_cols].axis(\"off\")\n+\n+        plt.tight_layout()\n+        plt.show()\n+\n+\n+# Build the unet model\n+network = build_model(\n+    img_size=img_size,\n+    img_channels=img_channels,\n+    widths=widths,\n+    has_attention=has_attention,\n+    num_res_blocks=num_res_blocks,\n+    norm_groups=norm_groups,\n+    activation_fn=keras.activations.swish,\n+)\n+ema_network = build_model(\n+    img_size=img_size,\n+    img_channels=img_channels,\n+    widths=widths,\n+    has_attention=has_attention,\n+    num_res_blocks=num_res_blocks,\n+    norm_groups=norm_groups,\n+    activation_fn=keras.activations.swish,\n+)\n+ema_network.set_weights(\n+    network.get_weights()\n+)  # Initially the weights are the same\n+\n+# Get an instance of the Gaussian Diffusion utilities\n+gdf_util = GaussianDiffusion(timesteps=total_timesteps)\n+\n+# Get the model\n+model = DiffusionModel(\n+    network=network,\n+    ema_network=ema_network,\n+    gdf_util=gdf_util,\n+    timesteps=total_timesteps,\n+)\n+\n+# Compile the model\n+model.compile(\n+    loss=keras.losses.MeanSquaredError(),\n+    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n+    jit_compile=False,\n+)\n+\n+# Train the model\n+model.fit(\n+    train_ds,\n+    epochs=num_epochs,\n+    batch_size=batch_size,\n+    callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images)],\n+)\n+\n+\"\"\"\n+## Results\n+\n+We trained this model for 800 epochs on a V100 GPU,\n+and each epoch took almost 8 seconds to finish. We load those weights\n+here, and we generate a few samples starting from pure noise.\n+\"\"\"\n+\n+\"\"\"shell\n+curl -LO https://github.com/AakashKumarNain/ddpms/releases/download/v3.0.0/checkpoints.zip\n+unzip -qq checkpoints.zip\n+\"\"\"\n+\n+# Load the model weights\n+model.ema_network.load_weights(\"checkpoints/diffusion_model_checkpoint\")\n+\n+# Generate and plot some samples\n+model.plot_images(num_rows=4, num_cols=8)\n+\n+\n+\"\"\"\n+## Conclusion\n+\n+We successfully implemented and trained a diffusion model exactly in the same\n+fashion as implemented by the authors of the DDPMs paper. You can find the\n+original implementation [here](https://github.com/hojonathanho/diffusion).\n+\n+There are a few things that you can try to improve the model:\n+\n+1. Increasing the width of each block. A bigger model can learn to denoise\n+in fewer epochs, though you may have to take care of overfitting.\n+\n+2. We implemented the linear schedule for variance scheduling. You can implement\n+other schemes like cosine scheduling and compare the performance.\n+\"\"\"\n+\n+\"\"\"\n+## References\n+\n+1. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n+2. [Author's implementation](https://github.com/hojonathanho/diffusion)\n+3. [A deep dive into DDPMs](https://magic-with-latents.github.io/latent/posts/ddpms/part3/)\n+4. [Denoising Diffusion Implicit Models](https://keras.io/examples/generative/ddim/)\n+5. [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n+6. [AIAIART](https://www.youtube.com/watch?v=XTs7M6TSK9I&t=14s)\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8227bffcc505426323f00b8a9082284e20ecd8fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 140 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 143 | Churn Cumulative: 845 | Contributors (this commit): 8 | Commits (past 90d): 26 | Contributors (cumulative): 10 | DMM Complexity: 0.7857142857142857\n\nDIFF:\n@@ -0,0 +1,123 @@\n+\"\"\"\n+Title: Customizing the convolution operation of a Conv2D layer\n+Author: [lukewood](https://lukewood.xyz)\n+Date created: 11/03/2021\n+Last modified: 11/03/2021\n+Description: This example shows how to implement custom convolution layers using the `Conv.convolution_op()` API.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+You may sometimes need to implement custom versions of convolution layers like `Conv1D` and `Conv2D`.\n+Keras enables you do this without implementing the entire layer from scratch: you can reuse\n+most of the base convolution layer and just customize the convolution op itself via the\n+`convolution_op()` method.\n+\n+This method was introduced in Keras 2.7. So before using the\n+`convolution_op()` API, ensure that you are running Keras version 2.7.0 or greater.\n+\"\"\"\n+\n+\"\"\"\n+## A Simple `StandardizedConv2D` implementation\n+\n+There are two ways to use the `Conv.convolution_op()` API. The first way\n+is to override the `convolution_op()` method on a convolution layer subclass.\n+Using this approach, we can quickly implement a\n+[StandardizedConv2D](https://arxiv.org/abs/1903.10520) as shown below.\n+\"\"\"\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+import numpy as np\n+\n+\n+class StandardizedConv2DWithOverride(layers.Conv2D):\n+    def convolution_op(self, inputs, kernel):\n+        mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n+        return tf.nn.conv2d(\n+            inputs,\n+            (kernel - mean) / tf.sqrt(var + 1e-10),\n+            padding=\"valid\",\n+            strides=list(self.strides),\n+            name=self.__class__.__name__,\n+        )\n+\n+\n+\"\"\"\n+The other way to use the `Conv.convolution_op()` API is to directly call the\n+`convolution_op()` method from the `call()` method of a convolution layer subclass.\n+A comparable class implemented using this approach is shown below.\n+\"\"\"\n+\n+\n+class StandardizedConv2DWithCall(layers.Conv2D):\n+    def call(self, inputs):\n+        mean, var = tf.nn.moments(self.kernel, axes=[0, 1, 2], keepdims=True)\n+        result = self.convolution_op(\n+            inputs, (self.kernel - mean) / tf.sqrt(var + 1e-10)\n+        )\n+        if self.use_bias:\n+            result = result + self.bias\n+        return result\n+\n+\n+\"\"\"\n+## Example Usage\n+\n+Both of these layers work as drop-in replacements for `Conv2D`. The following\n+demonstration performs classification on the MNIST dataset.\n+\"\"\"\n+\n+# Model / data parameters\n+num_classes = 10\n+input_shape = (28, 28, 1)\n+\n+# the data, split between train and test sets\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+# Scale images to the [0, 1] range\n+x_train = x_train.astype(\"float32\") / 255\n+x_test = x_test.astype(\"float32\") / 255\n+# Make sure images have shape (28, 28, 1)\n+x_train = np.expand_dims(x_train, -1)\n+x_test = np.expand_dims(x_test, -1)\n+print(\"x_train shape:\", x_train.shape)\n+print(x_train.shape[0], \"train samples\")\n+print(x_test.shape[0], \"test samples\")\n+\n+# convert class vectors to binary class matrices\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+\n+model = keras.Sequential(\n+    [\n+        keras.layers.Input(shape=input_shape),\n+        StandardizedConv2DWithCall(32, kernel_size=(3, 3), activation=\"relu\"),\n+        layers.MaxPooling2D(pool_size=(2, 2)),\n+        StandardizedConv2DWithOverride(64, kernel_size=(3, 3), activation=\"relu\"),\n+        layers.MaxPooling2D(pool_size=(2, 2)),\n+        layers.Flatten(),\n+        layers.Dropout(0.5),\n+        layers.Dense(num_classes, activation=\"softmax\"),\n+    ]\n+)\n+\n+model.summary()\n+\"\"\"\n+\n+\"\"\"\n+batch_size = 128\n+epochs = 5\n+\n+model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+\n+model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_split=0.1)\n+\n+\"\"\"\n+## Conclusion\n+\n+The `Conv.convolution_op()` API provides an easy and readable way to implement custom\n+convolution layers. A `StandardizedConvolution` implementation using the API is quite\n+terse, consisting of only four lines of code.\n+\"\"\"\n\n@@ -204,16 +204,21 @@ class BaseConv(Layer):\n             self.bias = None\n         self.built = True\n \n-    def call(self, inputs):\n-        outputs = ops.conv(\n+    def convolution_op(self, inputs, kernel):\n+        return ops.conv(\n             inputs,\n-            self.kernel,\n+            kernel,\n             strides=list(self.strides),\n             padding=self.padding,\n             dilation_rate=self.dilation_rate,\n             data_format=self.data_format,\n         )\n \n+    def call(self, inputs):\n+        outputs = self.convolution_op(\n+            inputs,\n+            self.kernel,\n+        )\n         if self.use_bias:\n             if self.data_format == \"channels_last\":\n                 bias_shape = (1,) * (self.rank + 1) + (self.filters,)\n\n@@ -1,3 +1,5 @@\n+import warnings\n+\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n@@ -14,9 +16,16 @@ class InputLayer(Layer):\n         batch_shape=None,\n         input_tensor=None,\n         name=None,\n+        **kwargs,\n     ):\n         # TODO: support for sparse, ragged.\n         super().__init__(name=name)\n+        if \"input_shape\" in kwargs:\n+            warnings.warn(\n+                \"Argument `input_shape` is deprecated. Use `shape` instead.\"\n+            )\n+            shape = kwargs.pop(\"input_shape\")\n+\n         if shape is not None and batch_shape is not None:\n             raise ValueError(\n                 \"You cannot pass both `shape` and `batch_shape` at the \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#55452ad24ccec28eda26860c439cc161255cdb82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 412 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 412 | Churn Cumulative: 412 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,236 @@\n+\"\"\"\n+Title: Collaborative Filtering for Movie Recommendations\n+Author: [Siddhartha Banerjee](https://twitter.com/sidd2006)\n+Date created: 2020/05/24\n+Last modified: 2020/05/24\n+Description: Recommending movies using a model trained on Movielens dataset.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates\n+[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n+using the [Movielens dataset](https://www.kaggle.com/c/movielens-100k)\n+to recommend movies to users.\n+The MovieLens ratings dataset lists the ratings given by a set of users to a set of movies.\n+Our goal is to be able to predict ratings for movies a user has not yet watched.\n+The movies with the highest predicted ratings can then be recommended to the user.\n+\n+The steps in the model are as follows:\n+\n+1. Map user ID to a \"user vector\" via an embedding matrix\n+2. Map movie ID to a \"movie vector\" via an embedding matrix\n+3. Compute the dot product between the user vector and movie vector, to obtain\n+the a match score between the user and the movie (predicted rating).\n+4. Train the embeddings via gradient descent using all known user-movie pairs.\n+\n+**References:**\n+\n+- [Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/371920.372071)\n+- [Neural Collaborative Filtering](https://dl.acm.org/doi/pdf/10.1145/3038912.3052569)\n+\"\"\"\n+\n+import pandas as pd\n+from pathlib import Path\n+import matplotlib.pyplot as plt\n+import numpy as np\n+from zipfile import ZipFile\n+\n+import keras_core as keras\n+from keras_core import layers\n+import keras_core.operations as ops\n+\"\"\"\n+## First, load the data and apply preprocessing\n+\"\"\"\n+\n+# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n+# Use the ratings.csv file\n+movielens_data_file_url = (\n+    \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n+)\n+movielens_zipped_file = keras.utils.get_file(\n+    \"ml-latest-small.zip\", movielens_data_file_url, extract=False\n+)\n+keras_datasets_path = Path(movielens_zipped_file).parents[0]\n+movielens_dir = keras_datasets_path / \"ml-latest-small\"\n+\n+# Only extract the data the first time the script is run.\n+if not movielens_dir.exists():\n+    with ZipFile(movielens_zipped_file, \"r\") as zip:\n+        # Extract files\n+        print(\"Extracting all the files now...\")\n+        zip.extractall(path=keras_datasets_path)\n+        print(\"Done!\")\n+\n+ratings_file = movielens_dir / \"ratings.csv\"\n+df = pd.read_csv(ratings_file)\n+\n+\"\"\"\n+First, need to perform some preprocessing to encode users and movies as integer indices.\n+\"\"\"\n+user_ids = df[\"userId\"].unique().tolist()\n+user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n+userencoded2user = {i: x for i, x in enumerate(user_ids)}\n+movie_ids = df[\"movieId\"].unique().tolist()\n+movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n+movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n+df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n+df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n+\n+num_users = len(user2user_encoded)\n+num_movies = len(movie_encoded2movie)\n+df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n+# min and max ratings will be used to normalize the ratings later\n+min_rating = min(df[\"rating\"])\n+max_rating = max(df[\"rating\"])\n+\n+print(\n+    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n+        num_users, num_movies, min_rating, max_rating\n+    )\n+)\n+\n+\"\"\"\n+## Prepare training and validation data\n+\"\"\"\n+df = df.sample(frac=1, random_state=42)\n+x = df[[\"user\", \"movie\"]].values\n+# Normalize the targets between 0 and 1. Makes it easy to train.\n+y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n+# Assuming training on 90% of the data and validating on 10%.\n+train_indices = int(0.9 * df.shape[0])\n+x_train, x_val, y_train, y_val = (\n+    x[:train_indices],\n+    x[train_indices:],\n+    y[:train_indices],\n+    y[train_indices:],\n+)\n+\n+\"\"\"\n+## Create the model\n+\n+We embed both users and movies in to 50-dimensional vectors.\n+\n+The model computes a match score between user and movie embeddings via a dot product,\n+and adds a per-movie and per-user bias. The match score is scaled to the `[0, 1]`\n+interval via a sigmoid (since our ratings are normalized to this range).\n+\"\"\"\n+EMBEDDING_SIZE = 50\n+\n+\n+class RecommenderNet(keras.Model):\n+    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n+        super().__init__(**kwargs)\n+        self.num_users = num_users\n+        self.num_movies = num_movies\n+        self.embedding_size = embedding_size\n+        self.user_embedding = layers.Embedding(\n+            num_users,\n+            embedding_size,\n+            embeddings_initializer=\"he_normal\",\n+            embeddings_regularizer=keras.regularizers.l2(1e-6),\n+        )\n+        self.user_bias = layers.Embedding(num_users, 1)\n+        self.movie_embedding = layers.Embedding(\n+            num_movies,\n+            embedding_size,\n+            embeddings_initializer=\"he_normal\",\n+            embeddings_regularizer=keras.regularizers.l2(1e-6),\n+        )\n+        self.movie_bias = layers.Embedding(num_movies, 1)\n+\n+    def call(self, inputs):\n+        user_vector = self.user_embedding(inputs[:, 0])\n+        user_bias = self.user_bias(inputs[:, 0])\n+        movie_vector = self.movie_embedding(inputs[:, 1])\n+        movie_bias = self.movie_bias(inputs[:, 1])\n+        dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)\n+        # Add all the components (including bias)\n+        x = dot_user_movie + user_bias + movie_bias\n+        # The sigmoid activation forces the rating to between 0 and 1\n+        return ops.nn.sigmoid(x)\n+\n+\n+model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n+model.compile(\n+    loss=keras.losses.BinaryCrossentropy(),\n+    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n+)\n+\n+\"\"\"\n+## Train the model based on the data split\n+\"\"\"\n+history = model.fit(\n+    x=x_train,\n+    y=y_train,\n+    batch_size=64,\n+    epochs=5,\n+    verbose=1,\n+    validation_data=(x_val, y_val),\n+)\n+\n+\"\"\"\n+## Plot training and validation loss\n+\"\"\"\n+plt.plot(history.history[\"loss\"])\n+plt.plot(history.history[\"val_loss\"])\n+plt.title(\"model loss\")\n+plt.ylabel(\"loss\")\n+plt.xlabel(\"epoch\")\n+plt.legend([\"train\", \"test\"], loc=\"upper left\")\n+plt.show()\n+\n+\"\"\"\n+## Show top 10 movie recommendations to a user\n+\"\"\"\n+\n+movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n+\n+# Let us get a user and see the top recommendations.\n+user_id = df.userId.sample(1).iloc[0]\n+movies_watched_by_user = df[df.userId == user_id]\n+movies_not_watched = movie_df[\n+    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n+][\"movieId\"]\n+movies_not_watched = list(\n+    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n+)\n+movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n+user_encoder = user2user_encoded.get(user_id)\n+user_movie_array = np.hstack(\n+    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n+)\n+ratings = model.predict(user_movie_array).flatten()\n+top_ratings_indices = ratings.argsort()[-10:][::-1]\n+recommended_movie_ids = [\n+    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n+]\n+\n+print(\"Showing recommendations for user: {}\".format(user_id))\n+print(\"====\" * 9)\n+print(\"Movies with high ratings from user\")\n+print(\"----\" * 8)\n+top_movies_user = (\n+    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n+    .head(5)\n+    .movieId.values\n+)\n+movie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\n+for row in movie_df_rows.itertuples():\n+    print(row.title, \":\", row.genres)\n+\n+print(\"----\" * 8)\n+print(\"Top 10 movie recommendations\")\n+print(\"----\" * 8)\n+recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\n+for row in recommended_movies.itertuples():\n+    print(row.title, \":\", row.genres)\n+\n+\"\"\"\n+**Example available on HuggingFace**\n+\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Collaborative%20Filtering-black.svg)](https://huggingface.co/keras-io/collaborative-filtering-movielens) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Collaborative%20Filtering-black.svg)](https://huggingface.co/spaces/keras-io/collaborative-filtering-movielens) |\n+\"\"\"\n\n@@ -0,0 +1,176 @@\n+\"\"\"\n+Title: Timeseries classification with a Transformer model\n+Author: [Theodoros Ntakouris](https://github.com/ntakouris)\n+Date created: 2021/06/25\n+Last modified: 2021/08/05\n+Description: This notebook demonstrates how to do timeseries classification using a Transformer model.\n+Accelerator: GPU\n+\"\"\"\n+\n+\n+\"\"\"\n+## Introduction\n+\n+This is the Transformer architecture from\n+[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n+applied to timeseries instead of natural language.\n+\n+This example requires TensorFlow 2.4 or higher.\n+\n+## Load the dataset\n+\n+We are going to use the same dataset and preprocessing as the\n+[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n+example.\n+\"\"\"\n+\n+import numpy as np\n+\n+\n+def readucr(filename):\n+    data = np.loadtxt(filename, delimiter=\"\\t\")\n+    y = data[:, 0]\n+    x = data[:, 1:]\n+    return x, y.astype(int)\n+\n+\n+root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n+\n+x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n+x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n+\n+x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n+x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n+\n+n_classes = len(np.unique(y_train))\n+\n+idx = np.random.permutation(len(x_train))\n+x_train = x_train[idx]\n+y_train = y_train[idx]\n+\n+y_train[y_train == -1] = 0\n+y_test[y_test == -1] = 0\n+\n+\"\"\"\n+## Build the model\n+\n+Our model processes a tensor of shape `(batch size, sequence length, features)`,\n+where `sequence length` is the number of time steps and `features` is each input\n+timeseries.\n+\n+You can replace your classification RNN layers with this one: the\n+inputs are fully compatible!\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+We include residual connections, layer normalization, and dropout.\n+The resulting layer can be stacked multiple times.\n+\n+The projection layers are implemented through `keras.layers.Conv1D`.\n+\"\"\"\n+\n+\n+def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n+    # Attention and Normalization\n+    x = layers.MultiHeadAttention(\n+        key_dim=head_size, num_heads=num_heads, dropout=dropout\n+    )(inputs, inputs)\n+    x = layers.Dropout(dropout)(x)\n+    x = layers.LayerNormalization(epsilon=1e-6)(x)\n+    res = x + inputs\n+\n+    # Feed Forward Part\n+    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n+    x = layers.Dropout(dropout)(x)\n+    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n+    x = layers.LayerNormalization(epsilon=1e-6)(x)\n+    return x + res\n+\n+\n+\"\"\"\n+The main part of our model is now complete. We can stack multiple of those\n+`transformer_encoder` blocks and we can also proceed to add the final\n+Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n+layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n+our model down to a vector of features for each data point in the current\n+batch. A common way to achieve this is to use a pooling layer. For\n+this example, a `GlobalAveragePooling1D` layer is sufficient.\n+\"\"\"\n+\n+\n+def build_model(\n+    input_shape,\n+    head_size,\n+    num_heads,\n+    ff_dim,\n+    num_transformer_blocks,\n+    mlp_units,\n+    dropout=0,\n+    mlp_dropout=0,\n+):\n+    inputs = keras.Input(shape=input_shape)\n+    x = inputs\n+    for _ in range(num_transformer_blocks):\n+        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n+\n+    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n+    for dim in mlp_units:\n+        x = layers.Dense(dim, activation=\"relu\")(x)\n+        x = layers.Dropout(mlp_dropout)(x)\n+    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n+    return keras.Model(inputs, outputs)\n+\n+\n+\"\"\"\n+## Train and evaluate\n+\"\"\"\n+\n+input_shape = x_train.shape[1:]\n+\n+model = build_model(\n+    input_shape,\n+    head_size=256,\n+    num_heads=4,\n+    ff_dim=4,\n+    num_transformer_blocks=4,\n+    mlp_units=[128],\n+    mlp_dropout=0.4,\n+    dropout=0.25,\n+)\n+\n+model.compile(\n+    loss=\"sparse_categorical_crossentropy\",\n+    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n+    metrics=[\"sparse_categorical_accuracy\"],\n+)\n+model.summary()\n+\n+callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n+\n+model.fit(\n+    x_train,\n+    y_train,\n+    validation_split=0.2,\n+    epochs=2,\n+    batch_size=64,\n+    callbacks=callbacks,\n+)\n+\n+model.evaluate(x_test, y_test, verbose=1)\n+\n+\"\"\"\n+## Conclusions\n+\n+In about 110-120 epochs (25s each on Colab), the model reaches a training\n+accuracy of ~0.95, validation accuracy of ~84 and a testing\n+accuracy of ~85, without hyperparameter tuning. And that is for a model\n+with less than 100k parameters. Of course, parameter count and accuracy could be\n+improved by a hyperparameter search and a more sophisticated learning rate\n+schedule, or a different optimizer.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
