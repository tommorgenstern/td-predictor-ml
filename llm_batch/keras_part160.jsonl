{"custom_id": "keras#5bb86288e9d145b2a3cd4b708ef1659d8f24233c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 22 | Methods Changed: 16 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 62 | Churn Cumulative: 531 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -167,8 +167,7 @@ class VarianceScaling(Initializer):\n     def __init__(self, scale=1.0,\n                  mode='fan_in',\n                  distribution='normal',\n-                 seed=None,\n-                 data_format=None):\n+                 seed=None):\n         if scale <= 0.:\n             raise ValueError('`scale` must be a positive float. Got:', scale)\n         mode = mode.lower()\n@@ -181,22 +180,13 @@ class VarianceScaling(Initializer):\n             raise ValueError('Invalid `distribution` argument: '\n                              'expected one of {\"normal\", \"uniform\"} '\n                              'but got', distribution)\n-        if data_format is None:\n-            data_format = K.image_data_format()\n-        else:\n-            data_format = data_format.lower()\n-        if data_format not in {'channels_first', 'channels_last'}:\n-            raise ValueError('Invalid `data_format` argument: '\n-                             'expected one of {\"channels_first\", '\n-                             '\"channels_last\"} but got', data_format)\n         self.scale = scale\n         self.mode = mode\n         self.distribution = distribution\n         self.seed = seed\n-        self.data_format = data_format\n \n     def __call__(self, shape, dtype=None):\n-        fan_in, fan_out = _compute_fans(shape, self.data_format)\n+        fan_in, fan_out = _compute_fans(shape)\n         scale = self.scale\n         if self.mode == 'fan_in':\n             scale /= max(1., fan_in)\n@@ -218,8 +208,7 @@ class VarianceScaling(Initializer):\n             'scale': self.scale,\n             'mode': self.mode,\n             'distribution': self.distribution,\n-            'seed': self.seed,\n-            'data_format': self.data_format\n+            'seed': self.seed\n         }\n \n \n@@ -281,7 +270,7 @@ class Identity(Initializer):\n         }\n \n \n-def lecun_uniform(seed=None, data_format=None):\n+def lecun_uniform(seed=None):\n     \"\"\"LeCun uniform initializer.\n \n     It draws samples from a uniform distribution within [-limit, limit]\n@@ -290,7 +279,6 @@ def lecun_uniform(seed=None, data_format=None):\n \n     # Arguments\n         seed: A Python integer. Used to seed the random generator.\n-        data_format: Image data format to use for convolution kernels.\n \n     # Returns\n         An initializer.\n@@ -299,10 +287,13 @@ def lecun_uniform(seed=None, data_format=None):\n         LeCun 98, Efficient Backprop,\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n     \"\"\"\n-    return VarianceScaling(scale=1., mode='fan_in', distribution='uniform')\n+    return VarianceScaling(scale=1.,\n+                           mode='fan_in',\n+                           distribution='uniform',\n+                           seed=seed)\n \n \n-def glorot_normal(seed=None, data_format=None):\n+def glorot_normal(seed=None):\n     \"\"\"Glorot normal initializer, also called Xavier normal initializer.\n \n     It draws samples from a truncated normal distribution centered on 0\n@@ -312,7 +303,6 @@ def glorot_normal(seed=None, data_format=None):\n \n     # Arguments\n         seed: A Python integer. Used to seed the random generator.\n-        data_format: Image data format to use for convolution kernels.\n \n     # Returns\n         An initializer.\n@@ -321,10 +311,13 @@ def glorot_normal(seed=None, data_format=None):\n         Glorot & Bengio, AISTATS 2010\n         http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n     \"\"\"\n-    return VarianceScaling(scale=1., mode='fan_avg', distribution='normal')\n+    return VarianceScaling(scale=1.,\n+                           mode='fan_avg',\n+                           distribution='normal',\n+                           seed=seed)\n \n \n-def glorot_uniform(seed=None, data_format=None):\n+def glorot_uniform(seed=None):\n     \"\"\"Glorot uniform initializer, also called Xavier uniform initializer.\n \n     It draws samples from a uniform distribution within [-limit, limit]\n@@ -334,7 +327,6 @@ def glorot_uniform(seed=None, data_format=None):\n \n     # Arguments\n         seed: A Python integer. Used to seed the random generator.\n-        data_format: Image data format to use for convolution kernels.\n \n     # Returns\n         An initializer.\n@@ -343,10 +335,13 @@ def glorot_uniform(seed=None, data_format=None):\n         Glorot & Bengio, AISTATS 2010\n         http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n     \"\"\"\n-    return VarianceScaling(scale=1., mode='fan_avg', distribution='uniform')\n+    return VarianceScaling(scale=1.,\n+                           mode='fan_avg',\n+                           distribution='uniform',\n+                           seed=seed)\n \n \n-def he_normal(seed=None, data_format=None):\n+def he_normal(seed=None):\n     \"\"\"He normal initializer.\n \n     It draws samples from a truncated normal distribution centered on 0\n@@ -355,7 +350,6 @@ def he_normal(seed=None, data_format=None):\n \n     # Arguments\n         seed: A Python integer. Used to seed the random generator.\n-        data_format: Image data format to use for convolution kernels.\n \n     # Returns\n         An initializer.\n@@ -363,10 +357,13 @@ def he_normal(seed=None, data_format=None):\n     # References\n         He et al., http://arxiv.org/abs/1502.01852\n     \"\"\"\n-    return VarianceScaling(scale=2., mode='fan_in', distribution='normal')\n+    return VarianceScaling(scale=2.,\n+                           mode='fan_in',\n+                           distribution='normal',\n+                           seed=seed)\n \n \n-def he_uniform(seed=None, data_format=None):\n+def he_uniform(seed=None):\n     \"\"\"He uniform variance scaling initializer.\n \n     It draws samples from a uniform distribution within [-limit, limit]\n@@ -375,7 +372,6 @@ def he_uniform(seed=None, data_format=None):\n \n     # Arguments\n         seed: A Python integer. Used to seed the random generator.\n-        data_format: Image data format to use for convolution kernels.\n \n     # Returns\n         An initializer.\n@@ -383,7 +379,10 @@ def he_uniform(seed=None, data_format=None):\n     # References\n         He et al., http://arxiv.org/abs/1502.01852\n     \"\"\"\n-    return VarianceScaling(scale=2., mode='fan_in', distribution='uniform')\n+    return VarianceScaling(scale=2.,\n+                           mode='fan_in',\n+                           distribution='uniform',\n+                           seed=seed)\n \n \n # Compatibility aliases\n@@ -400,12 +399,15 @@ orthogonal = Orthogonal\n # Utility functions\n \n \n-def _compute_fans(shape, data_format='channels_first'):\n+def _compute_fans(shape, data_format='channels_last'):\n     \"\"\"Computes the number of input and output units for a weight shape.\n \n     # Arguments\n         shape: Integer shape tuple.\n         data_format: Image data format to use for convolution kernels.\n+            Note that all kernels in Keras are standardized on the\n+            `channels_last` ordering (even when inputs are set\n+            to `channels_first`).\n \n     # Returns\n         A tuple of scalars, `(fan_in, fan_out)`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0564b940ec8f5008018c12c24a1431e91407d8c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 25 | Churn Cumulative: 964 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -151,9 +151,10 @@ def serialize(constraint):\n     return serialize_keras_object(constraint)\n \n \n-def deserialize(config):\n+def deserialize(config, custom_objects=None):\n     return deserialize_keras_object(config,\n                                     module_objects=globals(),\n+                                    custom_objects=custom_objects,\n                                     printable_module_name='constraint')\n \n \n\n@@ -446,7 +446,7 @@ def serialize(initializer):\n def deserialize(config, custom_objects=None):\n     return deserialize_keras_object(config,\n                                     module_objects=globals(),\n-                                    custom_objects=None,\n+                                    custom_objects=custom_objects,\n                                     printable_module_name='initializer')\n \n \n\n@@ -52,41 +52,41 @@ def test_constant(tensor_shape):\n \n @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n def test_lecun_uniform(tensor_shape):\n-    fan_in, _ = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    fan_in, _ = initializers._compute_fans(tensor_shape)\n     scale = np.sqrt(3. / fan_in)\n-    _runner(initializers.lecun_uniform(data_format='channels_last'), tensor_shape,\n+    _runner(initializers.lecun_uniform(), tensor_shape,\n             target_mean=0., target_max=scale, target_min=-scale)\n \n \n @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n def test_glorot_uniform(tensor_shape):\n-    fan_in, fan_out = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    fan_in, fan_out = initializers._compute_fans(tensor_shape)\n     scale = np.sqrt(6. / (fan_in + fan_out))\n-    _runner(initializers.glorot_uniform(data_format='channels_last'), tensor_shape,\n+    _runner(initializers.glorot_uniform(), tensor_shape,\n             target_mean=0., target_max=scale, target_min=-scale)\n \n \n @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n def test_he_uniform(tensor_shape):\n-    fan_in, _ = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    fan_in, _ = initializers._compute_fans(tensor_shape)\n     scale = np.sqrt(6. / fan_in)\n-    _runner(initializers.he_uniform(data_format='channels_last'), tensor_shape,\n+    _runner(initializers.he_uniform(), tensor_shape,\n             target_mean=0., target_max=scale, target_min=-scale)\n \n \n @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n def test_glorot_normal(tensor_shape):\n-    fan_in, fan_out = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    fan_in, fan_out = initializers._compute_fans(tensor_shape)\n     scale = np.sqrt(2. / (fan_in + fan_out))\n-    _runner(initializers.glorot_normal(data_format='channels_last'), tensor_shape,\n+    _runner(initializers.glorot_normal(), tensor_shape,\n             target_mean=0., target_std=None, target_max=2 * scale)\n \n \n @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n def test_he_normal(tensor_shape):\n-    fan_in, _ = initializers._compute_fans(tensor_shape, data_format='channels_last')\n+    fan_in, _ = initializers._compute_fans(tensor_shape)\n     scale = np.sqrt(2. / fan_in)\n-    _runner(initializers.he_normal(data_format='channels_last'), tensor_shape,\n+    _runner(initializers.he_normal(), tensor_shape,\n             target_mean=0., target_std=None, target_max=2 * scale)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1bfa665eedb02aeece1731394ece001c46a3a894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 148 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 151 | Churn Cumulative: 1723 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 0.7959183673469388\n\nDIFF:\n@@ -81,7 +81,7 @@ class Average(_Merge):\n     def _merge_function(self, inputs):\n         output = inputs[0]\n         for i in range(1, len(inputs)):\n-            output *= inputs[i]\n+            output += inputs[i]\n         return output / len(inputs)\n \n \n@@ -243,8 +243,15 @@ class Dot(_Merge):\n                              'on a list of 2 inputs.')\n         shape1 = list(input_shape[0])\n         shape2 = list(input_shape[1])\n-        shape1.pop(self.axes[0])\n-        shape2.pop(self.axes[1])\n+        if isinstance(self.axes, int):\n+            if self.axes < 0:\n+                axes = [self.axes % len(shape1), self.axes % len(shape2)]\n+            else:\n+                axes = [self.axes] * 2\n+        else:\n+            axes = self.axes\n+        shape1.pop(axes[0])\n+        shape2.pop(axes[1])\n         shape2.pop(0)\n         output_shape = shape1 + shape2\n         if len(output_shape) == 1:\n\n@@ -116,6 +116,26 @@ def test_learning_phase():\n     assert fn_outputs_no_dp[1].sum() != fn_outputs_dp[1].sum()\n \n \n+@keras_test\n+def test_layer_call_arguments():\n+    # Test the ability to pass and serialize arguments to `call`.\n+    inp = layers.Input(shape=(2,))\n+    x = layers.Dense(3)(inp)\n+    x = layers.Dropout(0.5)(x, training=True)\n+    model = Model(inp, x)\n+    assert not model.uses_learning_phase\n+\n+    # Test that argument is kept when applying the model\n+    inp2 = layers.Input(shape=(2,))\n+    out2 = model(inp2)\n+    assert not out2._uses_learning_phase\n+\n+    # Test that argument is kept after loading a model\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    assert not model.uses_learning_phase\n+\n+\n @keras_test\n def test_node_construction():\n     ####################################################\n\n@@ -0,0 +1,118 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+from keras import layers\n+from keras import models\n+from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import keras_test\n+from keras.layers import merge\n+\n+\n+@keras_test\n+def test_merge_sum():\n+    i1 = layers.Input(shape=(4, 5))\n+    i2 = layers.Input(shape=(4, 5))\n+    i3 = layers.Input(shape=(4, 5))\n+    o = layers.sum([i1, i2, i3])\n+    assert o._keras_shape == (None, 4, 5)\n+    model = models.Model([i1, i2, i3], o)\n+\n+    x1 = np.random.random((2, 4, 5))\n+    x2 = np.random.random((2, 4, 5))\n+    x3 = np.random.random((2, 4, 5))\n+    out = model.predict([x1, x2, x3])\n+    assert out.shape == (2, 4, 5)\n+    assert_allclose(out, x1 + x2 + x3, atol=1e-4)\n+\n+\n+@keras_test\n+def test_merge_multiply():\n+    i1 = layers.Input(shape=(4, 5))\n+    i2 = layers.Input(shape=(4, 5))\n+    i3 = layers.Input(shape=(4, 5))\n+    o = layers.multiply([i1, i2, i3])\n+    assert o._keras_shape == (None, 4, 5)\n+    model = models.Model([i1, i2, i3], o)\n+\n+    x1 = np.random.random((2, 4, 5))\n+    x2 = np.random.random((2, 4, 5))\n+    x3 = np.random.random((2, 4, 5))\n+    out = model.predict([x1, x2, x3])\n+    assert out.shape == (2, 4, 5)\n+    assert_allclose(out, x1 * x2 * x3, atol=1e-4)\n+\n+\n+@keras_test\n+def test_merge_average():\n+    i1 = layers.Input(shape=(4, 5))\n+    i2 = layers.Input(shape=(4, 5))\n+    o = layers.average([i1, i2])\n+    assert o._keras_shape == (None, 4, 5)\n+    model = models.Model([i1, i2], o)\n+\n+    x1 = np.random.random((2, 4, 5))\n+    x2 = np.random.random((2, 4, 5))\n+    out = model.predict([x1, x2])\n+    assert out.shape == (2, 4, 5)\n+    assert_allclose(out, 0.5 * (x1 + x2), atol=1e-4)\n+\n+\n+@keras_test\n+def test_merge_maximum():\n+    i1 = layers.Input(shape=(4, 5))\n+    i2 = layers.Input(shape=(4, 5))\n+    o = layers.maximum([i1, i2])\n+    assert o._keras_shape == (None, 4, 5)\n+    model = models.Model([i1, i2], o)\n+\n+    x1 = np.random.random((2, 4, 5))\n+    x2 = np.random.random((2, 4, 5))\n+    out = model.predict([x1, x2])\n+    assert out.shape == (2, 4, 5)\n+    assert_allclose(out, np.maximum(x1, x2), atol=1e-4)\n+\n+\n+@keras_test\n+def test_merge_concatenate():\n+    i1 = layers.Input(shape=(4, 5))\n+    i2 = layers.Input(shape=(4, 5))\n+    o = layers.concatenate([i1, i2], axis=1)\n+    assert o._keras_shape == (None, 8, 5)\n+    model = models.Model([i1, i2], o)\n+\n+    x1 = np.random.random((2, 4, 5))\n+    x2 = np.random.random((2, 4, 5))\n+    out = model.predict([x1, x2])\n+    assert out.shape == (2, 8, 5)\n+    assert_allclose(out, np.concatenate([x1, x2], axis=1), atol=1e-4)\n+\n+\n+@keras_test\n+def test_merge_dot():\n+    i1 = layers.Input(shape=(4,))\n+    i2 = layers.Input(shape=(4,))\n+    o = layers.dot([i1, i2], axes=1)\n+    assert o._keras_shape == (None, 1)\n+    model = models.Model([i1, i2], o)\n+\n+    x1 = np.random.random((2, 4))\n+    x2 = np.random.random((2, 4))\n+    out = model.predict([x1, x2])\n+    assert out.shape == (2, 1)\n+    expected = np.zeros((2, 1))\n+    expected[0, 0] = np.dot(x1[0], x2[0])\n+    expected[1, 0] = np.dot(x1[1], x2[1])\n+    assert_allclose(out, expected, atol=1e-4)\n+\n+    # Test with negative tuple of axes.\n+    o = layers.dot([i1, i2], axes=(-1, -1))\n+    assert o._keras_shape == (None, 1)\n+    model = models.Model([i1, i2], o)\n+    out = model.predict([x1, x2])\n+    assert out.shape == (2, 1)\n+    assert_allclose(out, expected, atol=1e-4)\n+\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f46ccff056525cef9ea2cb7885d9cc1b38db7a6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 563 | Lines Deleted: 324 | Files Changed: 5 | Hunks: 84 | Methods Changed: 22 | Complexity Δ (Sum/Max): 14/25 | Churn Δ: 887 | Churn Cumulative: 9050 | Contributors (this commit): 28 | Commits (past 90d): 44 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2013,10 +2013,13 @@ class Container(Layer):\n                             json.dumps(node.arguments)\n                             kwargs = node.arguments\n                         except TypeError:\n-                            raise TypeError(\n+                            warnings.warn(\n                                 'Layer ' + layer.name +\n                                 ' was passed non-serializable keyword arguments: ' +\n-                                str(node.arguments) + '. Cannot serialize model.')\n+                                str(node.arguments) + '. They will not be included '\n+                                'in the serialized model (and thus will be missing '\n+                                'at deserialization time).')\n+                            kwargs = {}\n                     else:\n                         kwargs = {}\n                     if node.inbound_layers:\n\n@@ -16,6 +16,7 @@ from .noise import *\n from .advanced_activations import *\n from .wrappers import *\n from .convolutional_recurrent import *\n+from ..legacy.layers import *\n \n \n def deserialize(config, custom_objects=None):\n\n@@ -1,9 +1,14 @@\n import inspect\n import types as python_types\n+import warnings\n \n-from ..engine.topology import Layer\n+from ..engine.topology import Layer, InputSpec\n from .. import backend as K\n from ..utils.generic_utils import func_dump, func_load\n+from .. import regularizers\n+from .. import constraints\n+from .. import activations\n+from .. import initializers\n \n \n class Merge(Layer):\n@@ -58,6 +63,10 @@ class Merge(Layer):\n                  dot_axes=-1, output_shape=None, output_mask=None,\n                  arguments=None, node_indices=None, tensor_indices=None,\n                  name=None):\n+        warnings.warn('The `Merge` layer is deprecated '\n+                      'and will be removed after 08/2017. '\n+                      'Use instead layers from `keras.layers.merge`, '\n+                      'e.g. `sum`, `concatenate`, etc.')\n         self.layers = layers\n         self.mode = mode\n         self.concat_axis = concat_axis\n@@ -66,6 +75,7 @@ class Merge(Layer):\n         self.node_indices = node_indices\n         self._output_mask = output_mask\n         self.arguments = arguments if arguments else {}\n+        self._initial_weights = None\n \n         # Layer parameters.\n         self.inbound_nodes = []\n@@ -227,7 +237,7 @@ class Merge(Layer):\n         else:\n             raise ValueError('Unknown merge mode.')\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         # Must have multiple input shape tuples.\n         assert isinstance(input_shape, list)\n         # Case: callable self._output_shape.\n@@ -382,3 +392,338 @@ class Merge(Layer):\n         config['output_shape'] = output_shape\n         config['output_mask'] = output_mask\n         return super(Merge, cls).from_config(config)\n+\n+\n+def merge(inputs, mode='sum', concat_axis=-1,\n+          dot_axes=-1, output_shape=None, output_mask=None,\n+          arguments=None, name=None):\n+    \"\"\"Functional merge, to apply to Keras tensors (NOT layers).\n+    Returns a Keras tensor.\n+    # Example\n+    ```python\n+    tensor_a = Input(shape=(32,))\n+    tensor_b = Input(shape=(32,))\n+    merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)\n+    ```\n+    # Arguments\n+        mode: String or lambda/function. If string, must be one\n+            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+            If lambda/function, it should take as input a list of tensors\n+            and return a single tensor.\n+        concat_axis: Integer, axis to use in mode `concat`.\n+        dot_axes: Integer or tuple of integers,\n+            axes to use in mode `dot` or `cos`.\n+        output_shape: Shape tuple (tuple of integers), or lambda/function\n+            to compute output_shape (only if merge mode is a lambda/function).\n+            If the latter case, it should take as input a list of shape tuples\n+            (1:1 mapping to input tensors) and return a single shape tuple,\n+            including the batch size\n+            (same convention as the `get_output_shape_for` method of layers).\n+        node_indices: Optional list of integers containing\n+            the output node index for each input layer\n+            (in case some input layers have multiple output nodes).\n+            will default to an array of 0s if not provided.\n+        tensor_indices: Optional list of indices of output tensors\n+            to consider for merging\n+            (in case some input layer node returns multiple tensors).\n+    \"\"\"\n+    warnings.warn('The `merge` function is deprecated '\n+                  'and will be removed after 08/2017. '\n+                  'Use instead layers from `keras.layers.merge`, '\n+                  'e.g. `sum`, `concatenate`, etc.')\n+    all_keras_tensors = True\n+    for x in inputs:\n+        if not hasattr(x, '_keras_history'):\n+            all_keras_tensors = False\n+            break\n+    if all_keras_tensors:\n+        input_layers = []\n+        node_indices = []\n+        tensor_indices = []\n+        for x in inputs:\n+            input_layer, node_index, tensor_index = x._keras_history\n+            input_layers.append(input_layer)\n+            node_indices.append(node_index)\n+            tensor_indices.append(tensor_index)\n+        merge_layer = Merge(input_layers, mode=mode,\n+                            concat_axis=concat_axis,\n+                            dot_axes=dot_axes,\n+                            output_shape=output_shape,\n+                            output_mask=output_mask,\n+                            arguments=arguments,\n+                            node_indices=node_indices,\n+                            tensor_indices=tensor_indices,\n+                            name=name)\n+        return merge_layer.inbound_nodes[0].output_tensors[0]\n+    else:\n+        merge_layer = Merge(mode=mode,\n+                            concat_axis=concat_axis,\n+                            dot_axes=dot_axes,\n+                            output_shape=output_shape,\n+                            output_mask=output_mask,\n+                            arguments=arguments,\n+                            name=name)\n+        return merge_layer(inputs)\n+\n+\n+class MaxoutDense(Layer):\n+    \"\"\"A dense maxout layer.\n+    A `MaxoutDense` layer takes the element-wise maximum of\n+    `nb_feature` `Dense(input_dim, output_dim)` linear layers.\n+    This allows the layer to learn a convex,\n+    piecewise linear activation function over the inputs.\n+    Note that this is a *linear* layer;\n+    if you wish to apply activation function\n+    (you shouldn't need to --they are universal function approximators),\n+    an `Activation` layer must be added after.\n+    # Arguments\n+        output_dim: int > 0.\n+        nb_feature: number of Dense layers to use internally.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights\n+            initialization. This parameter is only relevant\n+            if you don't pass a `weights` argument.\n+        weights: list of Numpy arrays to set as initial weights.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`\n+            and (output_dim,) for weights and biases respectively.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+        input_dim: dimensionality of the input (integer). This argument\n+            (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n+    # Input shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+    # Output shape\n+        2D tensor with shape: `(nb_samples, output_dim)`.\n+    # References\n+        - [Maxout Networks](http://arxiv.org/abs/1302.4389)\n+    \"\"\"\n+\n+    def __init__(self, output_dim,\n+                 nb_feature=4,\n+                 init='glorot_uniform',\n+                 weights=None,\n+                 W_regularizer=None,\n+                 b_regularizer=None,\n+                 activity_regularizer=None,\n+                 W_constraint=None,\n+                 b_constraint=None,\n+                 bias=True,\n+                 input_dim=None,\n+                 **kwargs):\n+        warnings.warn('The `MaxoutDense` layer is deprecated '\n+                      'and will be removed after 06/2017.')\n+        self.output_dim = output_dim\n+        self.nb_feature = nb_feature\n+        self.init = initializers.get(init)\n+\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+\n+        self.bias = bias\n+        self.initial_weights = weights\n+        self.input_spec = InputSpec(ndim=2)\n+\n+        self.input_dim = input_dim\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_dim,)\n+        super(MaxoutDense, self).__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        input_dim = input_shape[1]\n+        self.input_spec = InputSpec(dtype=K.floatx(),\n+                                    shape=(None, input_dim))\n+\n+        self.W = self.add_weight((self.nb_feature, input_dim, self.output_dim),\n+                                 initializer=self.init,\n+                                 name='W',\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n+        if self.bias:\n+            self.b = self.add_weight((self.nb_feature, self.output_dim,),\n+                                     initializer='zero',\n+                                     name='b',\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n+        else:\n+            self.b = None\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+        self.built = True\n+\n+    def compute_output_shape(self, input_shape):\n+        assert input_shape and len(input_shape) == 2\n+        return (input_shape[0], self.output_dim)\n+\n+    def call(self, x):\n+        # no activation, this layer is only linear.\n+        output = K.dot(x, self.W)\n+        if self.bias:\n+            output += self.b\n+        output = K.max(output, axis=1)\n+        return output\n+\n+    def get_config(self):\n+        config = {'output_dim': self.output_dim,\n+                  'init': initializers.serialize(self.init),\n+                  'nb_feature': self.nb_feature,\n+                  'W_regularizer': regularizers.serialize(self.W_regularizer),\n+                  'b_regularizer': regularizers.serialize(self.b_regularizer),\n+                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'W_constraint': constraints.serialize(self.W_constraint),\n+                  'b_constraint': constraints.serialize(self.b_constraint),\n+                  'bias': self.bias,\n+                  'input_dim': self.input_dim}\n+        base_config = super(MaxoutDense, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class Highway(Layer):\n+    \"\"\"Densely connected highway network.\n+    Highway layers are a natural extension of LSTMs to feedforward networks.\n+    # Arguments\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights\n+            initialization. This parameter is only relevant\n+            if you don't pass a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of Numpy arrays to set as initial weights.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`\n+            and (output_dim,) for weights and biases respectively.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+        input_dim: dimensionality of the input (integer). This argument\n+            (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n+    # Input shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+    # Output shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+    # References\n+        - [Highway Networks](http://arxiv.org/abs/1505.00387v2)\n+    \"\"\"\n+\n+    def __init__(self,\n+                 init='glorot_uniform',\n+                 activation=None,\n+                 weights=None,\n+                 W_regularizer=None,\n+                 b_regularizer=None,\n+                 activity_regularizer=None,\n+                 W_constraint=None,\n+                 b_constraint=None,\n+                 bias=True,\n+                 input_dim=None,\n+                 **kwargs):\n+        warnings.warn('The `Highway` layer is deprecated '\n+                      'and will be removed after 06/2017.')\n+        if 'transform_bias' in kwargs:\n+            kwargs.pop('transform_bias')\n+            warnings.warn('`transform_bias` argument is deprecated and '\n+                          'has been removed.')\n+        self.init = initializers.get(init)\n+        self.activation = activations.get(activation)\n+\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+\n+        self.bias = bias\n+        self.initial_weights = weights\n+        self.input_spec = InputSpec(ndim=2)\n+\n+        self.input_dim = input_dim\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_dim,)\n+        super(Highway, self).__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        input_dim = input_shape[1]\n+        self.input_spec = InputSpec(dtype=K.floatx(),\n+                                    shape=(None, input_dim))\n+\n+        self.W = self.add_weight((input_dim, input_dim),\n+                                 initializer=self.init,\n+                                 name='W',\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n+        self.W_carry = self.add_weight((input_dim, input_dim),\n+                                       initializer=self.init,\n+                                       name='W_carry')\n+        if self.bias:\n+            self.b = self.add_weight((input_dim,),\n+                                     initializer='zero',\n+                                     name='b',\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n+            self.b_carry = self.add_weight((input_dim,),\n+                                           initializer='one',\n+                                           name='b_carry')\n+        else:\n+            self.b_carry = None\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+        self.built = True\n+\n+    def call(self, x):\n+        y = K.dot(x, self.W_carry)\n+        if self.bias:\n+            y += self.b_carry\n+        transform_weight = activations.sigmoid(y)\n+        y = K.dot(x, self.W)\n+        if self.bias:\n+            y += self.b\n+        act = self.activation(y)\n+        act *= transform_weight\n+        output = act + (1 - transform_weight) * x\n+        return output\n+\n+    def get_config(self):\n+        config = {'init': initializers.serialize(self.init),\n+                  'activation': activations.serialize(self.activation),\n+                  'W_regularizer': regularizers.serialize(self.W_regularizer),\n+                  'b_regularizer': regularizers.serialize(self.b_regularizer),\n+                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'W_constraint': constraints.serialize(self.W_constraint),\n+                  'b_constraint': constraints.serialize(self.b_constraint),\n+                  'bias': self.bias,\n+                  'input_dim': self.input_dim}\n+        base_config = super(Highway, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1,118 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras.models import Sequential\n-from keras.layers import convolutional_recurrent\n-from keras.utils.test_utils import layer_test\n-from keras import regularizers\n-\n-\n-def test_convolutional_recurrent():\n-    num_row = 3\n-    num_col = 3\n-    filters = 5\n-    num_samples = 2\n-    input_channel = 2\n-    input_num_row = 5\n-    input_num_col = 5\n-    sequence_len = 2\n-    for data_format in ['channels_first', 'channels_last']:\n-\n-        if data_format == 'channels_first':\n-            inputs = np.random.rand(num_samples, sequence_len,\n-                                    input_channel,\n-                                    input_num_row, input_num_col)\n-        else:\n-            inputs = np.random.rand(num_samples, sequence_len,\n-                                    input_num_row, input_num_col,\n-                                    input_channel)\n-\n-        for return_sequences in [True, False]:\n-            # test for output shape:\n-            output = layer_test(convolutional_recurrent.ConvLSTM2D,\n-                                kwargs={'data_format': data_format,\n-                                        'return_sequences': return_sequences,\n-                                        'filters': filters,\n-                                        'kernel_size': (num_row, num_col),\n-                                        'padding': 'valid'},\n-                                input_shape=inputs.shape)\n-\n-            # No need to check following tests for both data formats\n-            if data_format == 'channels_first' or return_sequences:\n-                continue\n-\n-            # Tests for statefulness\n-            model = Sequential()\n-            kwargs = {'data_format': data_format,\n-                      'return_sequences': return_sequences,\n-                      'filters': filters,\n-                      'kernel_size': (num_row, num_col),\n-                      'stateful': True,\n-                      'batch_input_shape': inputs.shape,\n-                      'padding': 'same'}\n-            layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n-\n-            model.add(layer)\n-            model.compile(optimizer='sgd', loss='mse')\n-            out1 = model.predict(np.ones_like(inputs))\n-\n-            # train once so that the states change\n-            model.train_on_batch(np.ones_like(inputs),\n-                                 np.random.random(out1.shape))\n-            out2 = model.predict(np.ones_like(inputs))\n-\n-            # if the state is not reset, output should be different\n-            assert(out1.max() != out2.max())\n-\n-            # check that output changes after states are reset\n-            # (even though the model itself didn't change)\n-            layer.reset_states()\n-            out3 = model.predict(np.ones_like(inputs))\n-            assert(out2.max() != out3.max())\n-\n-            # check that container-level reset_states() works\n-            model.reset_states()\n-            out4 = model.predict(np.ones_like(inputs))\n-            assert_allclose(out3, out4, atol=1e-5)\n-\n-            # check that the call to `predict` updated the states\n-            out5 = model.predict(np.ones_like(inputs))\n-            assert(out4.max() != out5.max())\n-\n-            # check regularizers\n-            kwargs = {'data_format': data_format,\n-                      'return_sequences': return_sequences,\n-                      'kernel_size': (num_row, num_col),\n-                      'stateful': True,\n-                      'filters': filters,\n-                      'batch_input_shape': inputs.shape,\n-                      'kernel_regularizer': regularizers.L1L2(l1=0.01),\n-                      'recurrent_regularizer': regularizers.L1L2(l1=0.01),\n-                      'bias_regularizer': 'l2',\n-                      'kernel_constraint': 'max_norm',\n-                      'recurrent_constraint': 'max_norm',\n-                      'bias_constraint': 'max_norm',\n-                      'padding': 'same'}\n-\n-            layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n-            layer.build(inputs.shape)\n-            assert len(layer.losses) == 3\n-            output = layer(K.variable(np.ones(inputs.shape)))\n-            assert len(layer.losses) == 3\n-            K.eval(output)\n-\n-            # check dropout\n-            layer_test(convolutional_recurrent.ConvLSTM2D,\n-                       kwargs={'data_format': data_format,\n-                               'return_sequences': return_sequences,\n-                               'filters': filters,\n-                               'kernel_size': (num_row, num_col),\n-                               'padding': 'same',\n-                               'dropout': 0.1,\n-                               'recurrent_dropout': 0.1},\n-                       input_shape=inputs.shape)\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,5 +1,8 @@\n+import pytest\n+\n from keras.utils.test_utils import keras_test\n from keras.utils.test_utils import layer_test\n+from keras.legacy import layers as legacy_layers\n from keras import layers\n from keras import models\n from keras import regularizers\n@@ -10,11 +13,11 @@ import numpy as np\n \n @keras_test\n def test_highway():\n-    layer_test(layers.Highway,\n+    layer_test(legacy_layers.Highway,\n                kwargs={},\n                input_shape=(3, 2))\n \n-    layer_test(layers.Highway,\n+    layer_test(legacy_layers.Highway,\n                kwargs={'W_regularizer': regularizers.l2(0.01),\n                        'b_regularizer': regularizers.l1(0.01),\n                        'activity_regularizer': regularizers.l2(0.01),\n@@ -25,11 +28,11 @@ def test_highway():\n \n @keras_test\n def test_maxout_dense():\n-    layer_test(layers.MaxoutDense,\n+    layer_test(legacy_layers.MaxoutDense,\n                kwargs={'output_dim': 3},\n                input_shape=(3, 2))\n \n-    layer_test(layers.MaxoutDense,\n+    layer_test(legacy_layers.MaxoutDense,\n                kwargs={'output_dim': 3,\n                        'W_regularizer': regularizers.l2(0.01),\n                        'b_regularizer': regularizers.l1(0.01),\n@@ -50,8 +53,8 @@ def test_merge():\n         print(mode)\n         input_a = layers.Input(shape=input_shapes[0][1:])\n         input_b = layers.Input(shape=input_shapes[1][1:])\n-        merged = layers.merge([input_a, input_b], mode=mode)\n-        model = layers.Model([input_a, input_b], merged)\n+        merged = legacy_layers.merge([input_a, input_b], mode=mode)\n+        model = models.Model([input_a, input_b], merged)\n         model.compile('rmsprop', 'mse')\n \n         expected_output_shape = model.compute_output_shape(input_shapes)\n@@ -63,7 +66,7 @@ def test_merge():\n         model.compile('rmsprop', 'mse')\n \n         # test Merge (#2460)\n-        merged = layers.Merge(mode=mode)([input_a, input_b])\n+        merged = legacy_layers.Merge(mode=mode)([input_a, input_b])\n         model = models.Model([input_a, input_b], merged)\n         model.compile('rmsprop', 'mse')\n \n@@ -74,7 +77,8 @@ def test_merge():\n     # test lambda with output_shape lambda\n     input_a = layers.Input(shape=input_shapes[0][1:])\n     input_b = layers.Input(shape=input_shapes[1][1:])\n-    merged = layers.merge([input_a, input_b],\n+    merged = legacy_layers.merge(\n+        [input_a, input_b],\n         mode=lambda tup: K.concatenate([tup[0], tup[1]]),\n         output_shape=lambda tup: tup[0][:-1] + (tup[0][-1] + tup[1][-1],))\n     model = models.Model([input_a, input_b], merged)\n@@ -97,7 +101,7 @@ def test_merge():\n \n     input_a = layers.Input(shape=input_shapes[0][1:])\n     input_b = layers.Input(shape=input_shapes[1][1:])\n-    merged = layers.merge([input_a, input_b],\n+    merged = legacy_layers.merge([input_a, input_b],\n                                  mode=fn_mode,\n                                  output_shape=fn_output_shape)\n     model = models.Model([input_a, input_b], merged)\n@@ -122,7 +126,7 @@ def test_merge():\n     input_b = layers.Input(shape=input_shapes[1][1:])\n     a = layers.Masking()(input_a)\n     b = layers.Masking()(input_b)\n-    merged = layers.merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n+    merged = legacy_layers.merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n     model = models.Model([input_a, input_b], merged)\n     expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n@@ -143,7 +147,8 @@ def test_merge():\n     input_b = layers.Input(shape=input_shapes[1][1:])\n     a = layers.Masking()(input_a)\n     b = layers.Masking()(input_b)\n-    merged = layers.merge([a, b], mode=lambda tup: K.concatenate([tup[0], tup[1]], axis=1),\n+    merged = legacy_layers.merge(\n+        [a, b], mode=lambda tup: K.concatenate([tup[0], tup[1]], axis=1),\n         output_shape=lambda tup: (tup[0][0], tup[0][1] + tup[1][1]) + tup[0][2:],\n         output_mask=lambda tup: K.concatenate([tup[0], tup[1]]))\n     model = models.Model([input_a, input_b], merged)\n@@ -168,7 +173,7 @@ def test_merge():\n \n     input_a = layers.Input(shape=input_shapes[0][1:])\n     input_b = layers.Input(shape=input_shapes[1][1:])\n-    merged = layers.merge([input_a, input_b], mode=fn_mode, output_shape=lambda s: s[0], arguments={'a': 0.7, 'b': 0.3})\n+    merged = legacy_layers.merge([input_a, input_b], mode=fn_mode, output_shape=lambda s: s[0], arguments={'a': 0.7, 'b': 0.3})\n     model = models.Model([input_a, input_b], merged)\n     output = model.predict(inputs)\n \n@@ -191,9 +196,9 @@ def test_merge_mask_2d():\n     masked_b = layers.Masking(mask_value=0)(input_b)\n \n     # three different types of merging\n-    merged_sum = layers.merge([masked_a, masked_b], mode='sum')\n-    merged_concat = layers.merge([masked_a, masked_b], mode='concat', concat_axis=1)\n-    merged_concat_mixed = layers.merge([masked_a, input_b], mode='concat', concat_axis=1)\n+    merged_sum = legacy_layers.merge([masked_a, masked_b], mode='sum')\n+    merged_concat = legacy_layers.merge([masked_a, masked_b], mode='concat', concat_axis=1)\n+    merged_concat_mixed = legacy_layers.merge([masked_a, input_b], mode='concat', concat_axis=1)\n \n     # test sum\n     model_sum = models.Model([input_a, input_b], [merged_sum])\n@@ -228,7 +233,7 @@ def test_merge_mask_3d():\n     rnn_b = rnn(embedding_b)\n \n     # concatenation\n-    merged_concat = layers.merge([rnn_a, rnn_b], mode='concat', concat_axis=-1)\n+    merged_concat = legacy_layers.merge([rnn_a, rnn_b], mode='concat', concat_axis=-1)\n     model = models.Model([input_a, input_b], [merged_concat])\n     model.compile(loss='mse', optimizer='sgd')\n     model.fit([rand(2, 3), rand(2, 3)], [rand(2, 3, 6)])\n@@ -239,7 +244,7 @@ def test_sequential_regression():\n     # start with a basic example of using a Sequential model\n     # inside the functional API\n     seq = models.Sequential()\n-    seq.add(layers.Dense(input_dim=10, output_dim=10))\n+    seq.add(layers.Dense(10, input_shape=(10,)))\n \n     x = layers.Input(shape=(10,))\n     y = seq(x)\n@@ -268,17 +273,16 @@ def test_sequential_regression():\n     branch_3 = models.Sequential(name='branch_3')\n     branch_3.add(layers.Dense(32, input_shape=(6,), name='dense_3'))\n \n-    branch_1_2 = models.Sequential([layers.Merge([branch_1, branch_2], mode='concat')], name='branch_1_2')\n+    branch_1_2 = models.Sequential([legacy_layers.Merge([branch_1, branch_2], mode='concat')], name='branch_1_2')\n     branch_1_2.add(layers.Dense(16, name='dense_1_2-0'))\n     # test whether impromtu input_shape breaks the model\n     branch_1_2.add(layers.Dense(16, input_shape=(16,), name='dense_1_2-1'))\n \n-    model = models.Sequential([layers.Merge([branch_1_2, branch_3], mode='concat')], name='final')\n+    model = models.Sequential([legacy_layers.Merge([branch_1_2, branch_3], mode='concat')], name='final')\n     model.add(layers.Dense(16, name='dense_final'))\n     model.compile(optimizer='rmsprop',\n                   loss='categorical_crossentropy',\n                   metrics=['accuracy'])\n-\n     model.summary()\n \n     x = (100 * np.random.random((100, 2))).astype('int32')\n@@ -311,241 +315,245 @@ def test_sequential_regression():\n \n \n \n-@keras_test\n-def test_merge_sum():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = models.Sequential()\n-    left.add(layers.Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(layers.Activation('relu'))\n+# @keras_test\n+# def test_merge_sum():\n+#     (x_train, y_train), (x_test, y_test) = _get_test_data()\n+#     left = models.Sequential()\n+#     left.add(layers.Dense(num_hidden, input_shape=(input_dim,)))\n+#     left.add(layers.Activation('relu'))\n \n-    right = models.Sequential()\n-    right.add(layers.Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(layers.Activation('relu'))\n+#     right = models.Sequential()\n+#     right.add(layers.Dense(num_hidden, input_shape=(input_dim,)))\n+#     right.add(layers.Activation('relu'))\n \n-    model = models.Sequential()\n-    model.add(layers.Merge([left, right], mode='sum'))\n-    model.add(layers.Dense(num_class))\n-    model.add(layers.Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     model = models.Sequential()\n+#     model.add(legacy_layers.Merge([left, right], mode='sum'))\n+#     model.add(layers.Dense(num_class))\n+#     model.add(layers.Activation('softmax'))\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n-    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+#     loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n \n-    model.predict([x_test, x_test], verbose=0)\n-    model.predict_classes([x_test, x_test], verbose=0)\n-    model.predict_proba([x_test, x_test], verbose=0)\n+#     model.predict([x_test, x_test], verbose=0)\n+#     model.predict_classes([x_test, x_test], verbose=0)\n+#     model.predict_proba([x_test, x_test], verbose=0)\n \n-    # test weight saving\n-    fname = 'test_merge_sum_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n-    right = Sequential()\n-    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n-    model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.load_weights(fname)\n-    os.remove(fname)\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     # test weight saving\n+#     fname = 'test_merge_sum_temp.h5'\n+#     model.save_weights(fname, overwrite=True)\n+#     left = Sequential()\n+#     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+#     left.add(Activation('relu'))\n+#     right = Sequential()\n+#     right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+#     right.add(Activation('relu'))\n+#     model = Sequential()\n+#     model.add(Merge([left, right], mode='sum'))\n+#     model.add(Dense(num_class))\n+#     model.add(Activation('softmax'))\n+#     model.load_weights(fname)\n+#     os.remove(fname)\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n-    assert(loss == nloss)\n+#     nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+#     assert(loss == nloss)\n \n-    # test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n+#     # test serialization\n+#     config = model.get_config()\n+#     Sequential.from_config(config)\n \n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n+#     model.summary()\n+#     json_str = model.to_json()\n+#     model_from_json(json_str)\n \n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n+#     yaml_str = model.to_yaml()\n+#     model_from_yaml(yaml_str)\n \n \n-@keras_test\n-def test_merge_dot():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+# @keras_test\n+# def test_merge_dot():\n+#     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n-    left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    left.add(Activation('relu'))\n+#     left = Sequential()\n+#     left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+#     left.add(Activation('relu'))\n \n-    right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    right.add(Activation('relu'))\n+#     right = Sequential()\n+#     right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+#     right.add(Activation('relu'))\n \n-    model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=1))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n+#     model = Sequential()\n+#     model.add(Merge([left, right], mode='dot', dot_axes=1))\n+#     model.add(Dense(num_class))\n+#     model.add(Activation('softmax'))\n \n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    left.add(Activation('relu'))\n+#     left = Sequential()\n+#     left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+#     left.add(Activation('relu'))\n \n-    right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n-    right.add(Activation('relu'))\n+#     right = Sequential()\n+#     right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n+#     right.add(Activation('relu'))\n \n-    model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n+#     model = Sequential()\n+#     model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n+#     model.add(Dense(num_class))\n+#     model.add(Activation('softmax'))\n \n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n \n-@keras_test\n-def test_merge_concat():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n+# @keras_test\n+# def test_merge_concat():\n+#     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n-    left = Sequential(name='branch_1')\n-    left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n-    left.add(Activation('relu', name='relu_1'))\n+#     left = Sequential(name='branch_1')\n+#     left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n+#     left.add(Activation('relu', name='relu_1'))\n \n-    right = Sequential(name='branch_2')\n-    right.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_2'))\n-    right.add(Activation('relu', name='relu_2'))\n+#     right = Sequential(name='branch_2')\n+#     right.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_2'))\n+#     right.add(Activation('relu', name='relu_2'))\n \n-    model = Sequential(name='merged_branches')\n-    model.add(Merge([left, right], mode='concat', name='merge'))\n-    model.add(Dense(num_class, name='final_dense'))\n-    model.add(Activation('softmax', name='softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     model = Sequential(name='merged_branches')\n+#     model.add(Merge([left, right], mode='concat', name='merge'))\n+#     model.add(Dense(num_class, name='final_dense'))\n+#     model.add(Activation('softmax', name='softmax'))\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n-    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+#     loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n \n-    model.predict([x_test, x_test], verbose=0)\n-    model.predict_classes([x_test, x_test], verbose=0)\n-    model.predict_proba([x_test, x_test], verbose=0)\n-    model.get_config()\n+#     model.predict([x_test, x_test], verbose=0)\n+#     model.predict_classes([x_test, x_test], verbose=0)\n+#     model.predict_proba([x_test, x_test], verbose=0)\n+#     model.get_config()\n \n-    fname = 'test_merge_concat_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.load_weights(fname)\n-    os.remove(fname)\n+#     fname = 'test_merge_concat_temp.h5'\n+#     model.save_weights(fname, overwrite=True)\n+#     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+#     model.load_weights(fname)\n+#     os.remove(fname)\n \n-    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n-    assert(loss == nloss)\n+#     nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n+#     assert(loss == nloss)\n \n \n-@keras_test\n-def test_merge_recursivity():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n+# @keras_test\n+# def test_merge_recursivity():\n+#     (x_train, y_train), (x_test, y_test) = _get_test_data()\n+#     left = Sequential()\n+#     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+#     left.add(Activation('relu'))\n \n-    right = Sequential()\n-    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n+#     right = Sequential()\n+#     right.add(Dense(num_hidden, input_shape=(input_dim,)))\n+#     right.add(Activation('relu'))\n \n-    righter = Sequential()\n-    righter.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    righter.add(Activation('relu'))\n+#     righter = Sequential()\n+#     righter.add(Dense(num_hidden, input_shape=(input_dim,)))\n+#     righter.add(Activation('relu'))\n \n-    intermediate = Sequential()\n-    intermediate.add(Merge([left, right], mode='sum'))\n-    intermediate.add(Dense(num_hidden))\n-    intermediate.add(Activation('relu'))\n+#     intermediate = Sequential()\n+#     intermediate.add(Merge([left, right], mode='sum'))\n+#     intermediate.add(Dense(num_hidden))\n+#     intermediate.add(Activation('relu'))\n \n-    model = Sequential()\n-    model.add(Merge([intermediate, righter], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     model = Sequential()\n+#     model.add(Merge([intermediate, righter], mode='sum'))\n+#     model.add(Dense(num_class))\n+#     model.add(Activation('softmax'))\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test, x_test], y_test))\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+#     model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test, x_test], y_test))\n+#     model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+#     model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+#     model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n-    loss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n+#     loss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n \n-    model.predict([x_test, x_test, x_test], verbose=0)\n-    model.predict_classes([x_test, x_test, x_test], verbose=0)\n-    model.predict_proba([x_test, x_test, x_test], verbose=0)\n+#     model.predict([x_test, x_test, x_test], verbose=0)\n+#     model.predict_classes([x_test, x_test, x_test], verbose=0)\n+#     model.predict_proba([x_test, x_test, x_test], verbose=0)\n \n-    fname = 'test_merge_recursivity_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model.load_weights(fname)\n-    os.remove(fname)\n+#     fname = 'test_merge_recursivity_temp.h5'\n+#     model.save_weights(fname, overwrite=True)\n+#     model.load_weights(fname)\n+#     os.remove(fname)\n \n-    nloss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n-    assert(loss == nloss)\n+#     nloss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n+#     assert(loss == nloss)\n \n-    # test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n+#     # test serialization\n+#     config = model.get_config()\n+#     Sequential.from_config(config)\n \n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n+#     model.summary()\n+#     json_str = model.to_json()\n+#     model_from_json(json_str)\n \n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n+#     yaml_str = model.to_yaml()\n+#     model_from_yaml(yaml_str)\n \n \n-@keras_test\n-def test_merge_overlap():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n+# @keras_test\n+# def test_merge_overlap():\n+#     (x_train, y_train), (x_test, y_test) = _get_test_data()\n+#     left = Sequential()\n+#     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n+#     left.add(Activation('relu'))\n \n-    model = Sequential()\n-    model.add(Merge([left, left], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+#     model = Sequential()\n+#     model.add(Merge([left, left], mode='sum'))\n+#     model.add(Dense(num_class))\n+#     model.add(Activation('softmax'))\n+#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n+#     model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n+#     model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+#     model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+#     model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n-    model.train_on_batch(x_train[:32], y_train[:32])\n+#     model.train_on_batch(x_train[:32], y_train[:32])\n \n-    loss = model.evaluate(x_test, y_test, verbose=0)\n-    model.predict(x_test, verbose=0)\n-    model.predict_classes(x_test, verbose=0)\n-    model.predict_proba(x_test, verbose=0)\n+#     loss = model.evaluate(x_test, y_test, verbose=0)\n+#     model.predict(x_test, verbose=0)\n+#     model.predict_classes(x_test, verbose=0)\n+#     model.predict_proba(x_test, verbose=0)\n \n-    fname = 'test_merge_overlap_temp.h5'\n-    print(model.layers)\n-    model.save_weights(fname, overwrite=True)\n-    print(model.trainable_weights)\n+#     fname = 'test_merge_overlap_temp.h5'\n+#     print(model.layers)\n+#     model.save_weights(fname, overwrite=True)\n+#     print(model.trainable_weights)\n \n-    model.load_weights(fname)\n-    os.remove(fname)\n+#     model.load_weights(fname)\n+#     os.remove(fname)\n \n-    nloss = model.evaluate(x_test, y_test, verbose=0)\n-    assert(loss == nloss)\n+#     nloss = model.evaluate(x_test, y_test, verbose=0)\n+#     assert(loss == nloss)\n \n-    # test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n+#     # test serialization\n+#     config = model.get_config()\n+#     Sequential.from_config(config)\n \n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n+#     model.summary()\n+#     json_str = model.to_json()\n+#     model_from_json(json_str)\n \n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n\\ No newline at end of file\n+#     yaml_str = model.to_yaml()\n+#     model_from_yaml(yaml_str)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
