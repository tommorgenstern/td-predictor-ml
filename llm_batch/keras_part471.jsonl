{"custom_id": "keras#564b8d9287a35879e041347de6273316bf5bcc88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 19 | Churn Cumulative: 3250 | Contributors (this commit): 12 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -385,8 +385,17 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 self.assertEqual(self.evaluate(opt.loss_scale), 8)\n \n                 # Test Inf gradients are still skipped instead of being clipped\n+<<<<<<< HEAD\n                 loss = lambda: var * float(\"Inf\")\n                 run_fn = lambda: opt.minimize(loss, var_list=[var])\n+=======\n+                def run_fn():\n+                    def loss():\n+                        return var * float(\"Inf\")\n+\n+                    return opt.minimize(loss, var_list=[var])\n+\n+>>>>>>> 0bb24689 (fix F811)\n                 run_op = strategy.experimental_run(run_fn)\n                 self._run_if_in_graph_mode(run_op)\n                 self.assertAllClose(\n@@ -417,8 +426,17 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertEqual(4.0, self.evaluate(opt.loss_scale))\n \n             # Test optimizer with NaN gradients\n+<<<<<<< HEAD\n             loss = lambda: var * float(\"NaN\")\n             run_fn = lambda: opt.minimize(loss, var_list=[var])\n+=======\n+            def run_fn():\n+                def loss():\n+                    return var * float(\"NaN\")\n+\n+                return opt.minimize(loss, var_list=[var])\n+\n+>>>>>>> 0bb24689 (fix F811)\n             run_op = strategy.experimental_run(run_fn)\n             self._run_if_in_graph_mode(run_op)\n             # Variable should not change from before, due to NaN gradients.\n\n@@ -22,7 +22,6 @@ import os\n import sys\n \n import numpy as np\n-import tensorflow as tf\n import tensorflow.compat.v2 as tf\n from absl import flags\n from absl.testing import absltest\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cf72f4934f3104ac2378c8b9b3638afea38ba1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 70 | Files Changed: 23 | Hunks: 45 | Methods Changed: 26 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 112 | Churn Cumulative: 85007 | Contributors (this commit): 264 | Commits (past 90d): 205 | Contributors (cumulative): 500 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -16,6 +16,7 @@ from __future__ import absolute_import as _absolute_import\n from __future__ import division as _division\n from __future__ import print_function as _print_function\n \n+import os\n import time\n import uuid\n \n\n@@ -1564,7 +1564,7 @@ class ModelCheckpoint(Callback):\n                         )\n \n                 self._maybe_remove_file()\n-            except IsADirectoryError as e:  # h5py 3.x\n+            except IsADirectoryError:  # h5py 3.x\n                 raise IOError(\n                     \"Please specify a non-directory filepath for \"\n                     \"ModelCheckpoint. Filepath used is an existing \"\n\n@@ -33,8 +33,8 @@ from absl.testing import parameterized\n \n import keras\n from keras.callbacks import BackupAndRestore\n-from keras.callbacks import Callback\n from keras.callbacks import BackupAndRestoreExperimental\n+from keras.callbacks import Callback\n from keras.engine import sequential\n from keras.layers import Activation\n from keras.layers import Dense\n@@ -387,7 +387,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                 if epoch == 5 or epoch == 12:\n                     raise RuntimeError(\"Interruption\")\n \n-        log_dir = self.get_temp_dir()\n+        self.get_temp_dir()\n \n         # The following asserts that the train counter is fault tolerant.\n         self.assertEqual(model._train_counter.numpy(), 0)\n@@ -462,7 +462,8 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             )\n \n         class InterruptingCallback(keras.callbacks.Callback):\n-            \"\"\"A callback to intentionally introduce interruption to training.\"\"\"\n+            \"\"\"A callback to intentionally introduce interruption to\n+            training.\"\"\"\n \n             batch_count = 0\n \n\n@@ -62,8 +62,8 @@ class WorkerTrainingState:\n         backend.set_value(\n             self._ckpt_saved_batch, self.CKPT_SAVED_BATCH_UNUSED_VALUE\n         )\n-        # _ckpt_saved_epoch  and _ckpt_saved_batch gets tracked and is included in\n-        # the checkpoint file when backing up.\n+        # _ckpt_saved_epoch  and _ckpt_saved_batch gets tracked and is included\n+        # in the checkpoint file when backing up.\n         checkpoint = tf.train.Checkpoint(\n             model=self._model,\n             ckpt_saved_epoch=self._ckpt_saved_epoch,\n@@ -155,8 +155,8 @@ class WorkerTrainingState:\n         Returns:\n           If the training is recovering from previous failure under multi-worker\n           training setting, return the (epoch, step) the training is supposed to\n-          continue at. Otherwise, return the `initial_epoch, initial_step` the user\n-          passes in.\n+          continue at. Otherwise, return the `initial_epoch, initial_step` the\n+          user passes in.\n         \"\"\"\n \n         initial_step = 0\n@@ -165,19 +165,20 @@ class WorkerTrainingState:\n         if mode == mode_keys.ModeKeys.TRAIN:\n             if self._save_freq == \"epoch\":\n                 if epoch >= 0:\n-                    # The most recently saved epoch is one epoch prior to the epoch it\n-                    # failed at, so return the value of 'self._ckpt_saved_epoch' plus one.\n+                    # The most recently saved epoch is one epoch prior to the\n+                    # epoch it failed at, so return the value of\n+                    # 'self._ckpt_saved_epoch' plus one.\n                     initial_epoch = epoch + 1\n             else:\n                 if batch >= 0 and epoch >= 0:\n-                    # If the checkpoint was last saved at last batch of the epoch, return\n-                    # the next epoch number and batch=0\n+                    # If the checkpoint was last saved at last batch of the\n+                    # epoch, return the next epoch number and batch=0\n                     if batch == steps_per_epoch - 1:\n                         initial_epoch = epoch + 1\n                         initial_step = 0\n                     else:\n-                        # If the checkpoint was not last saved at last batch of the epoch,\n-                        # return the same epoch and next batch number\n+                        # If the checkpoint was not last saved at last batch of\n+                        # the epoch, return the same epoch and next batch number\n                         initial_epoch = epoch\n                         initial_step = batch + 1\n         return (initial_epoch, initial_step)\n\n@@ -181,9 +181,7 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n     # TODO(scottzhu): This method and create_and_initialize might be removed if\n     # we decide to just use the tf.Variable to replace this class.\n     def initialize(self):\n-        with ops.name_scope(\n-            self._name, \"Variable\", skip_on_eager=False\n-        ) as name:\n+        with ops.name_scope(self._name, \"Variable\", skip_on_eager=False):\n             with ops.colocate_with(self._handle), ops.name_scope(\"Initializer\"):\n                 if callable(self._initial_value):\n                     initial_value = self._initial_value()\n\n@@ -685,6 +685,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             and dtype.is_floating\n         ):\n             old_getter = getter\n+\n             # Wrap variable constructor to return an AutoCastVariable.\n             def getter(*args, **kwargs):  # pylint: disable=function-redefined\n                 variable = old_getter(*args, **kwargs)\n@@ -3082,9 +3083,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         if (\n             name == \"_self_setattr_tracking\"\n             or not getattr(self, \"_self_setattr_tracking\", True)\n-            or\n             # Exclude @property.setters from tracking\n-            hasattr(self.__class__, name)\n+            or hasattr(self.__class__, name)\n         ):\n             try:\n                 super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n\n@@ -1279,10 +1279,9 @@ class Layer(base_layer.Layer):\n         if (\n             tf.distribute.has_strategy()\n             and tf.distribute.in_cross_replica_context()\n-            and\n             # When saving the model, the distribution strategy context should be\n             # ignored, following the default path for adding updates.\n-            not call_context.saving\n+            and not call_context.saving\n         ):\n             # Updates don't need to be run in a cross-replica context.\n             return\n@@ -2330,9 +2329,8 @@ class Layer(base_layer.Layer):\n         if (\n             name == \"_self_setattr_tracking\"\n             or not getattr(self, \"_self_setattr_tracking\", True)\n-            or\n             # Exclude @property.setters from tracking\n-            hasattr(self.__class__, name)\n+            or hasattr(self.__class__, name)\n         ):\n             try:\n                 super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n\n@@ -1237,9 +1237,8 @@ def _should_skip_first_node(layer):\n     if layer._self_tracked_trackables:\n         return (\n             isinstance(layer, Functional)\n-            and\n             # Filter out Sequential models without an input shape.\n-            isinstance(\n+            and isinstance(\n                 layer._self_tracked_trackables[0], input_layer_module.InputLayer\n             )\n         )\n\n@@ -18,4 +18,4 @@\n Everything has been moved to keras/saving/. This file will be deleted soon.\n \"\"\"\n \n-from keras.saving import *  # noqa: F401\n+from keras.saving import *  # noqa: F401,F403\n\n@@ -333,7 +333,7 @@ class Sequential(functional.Functional):\n                             # Create Functional API connection by calling the\n                             # current layer\n                             layer_output = layer(layer_input)\n-                        except:  # pylint:disable=bare-except\n+                        except:  # noqa: E722\n                             # Functional API calls may fail for a number of\n                             # reasons: 1) The layer may be buggy. In this case\n                             # it will be easier for the user to debug if we fail\n@@ -367,7 +367,7 @@ class Sequential(functional.Functional):\n                         # not be supporting such layers.\n                         self._init_graph_network(inputs, outputs)\n                         self._graph_initialized = True\n-                    except:  # pylint:disable=bare-except\n+                    except:  # noqa: E722\n                         self._use_legacy_deferred_behavior = True\n                 self._inferred_input_shape = new_shape\n \n\n@@ -1547,7 +1547,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             (\n                 data_handler._initial_epoch,\n                 data_handler._initial_step,\n-            ) = self._maybe_load_initial_counters_from_ckpt(  # pylint: disable=protected-access\n+            ) = self._maybe_load_initial_counters_from_ckpt(\n                 steps_per_epoch_inferred, initial_epoch\n             )\n             logs = None\n@@ -3523,8 +3523,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Returns:\n           If the training is recovering from previous failure under multi-worker\n           training setting, return the (epoch, step) the training is supposed to\n-          continue at. Otherwise, return the `initial_epoch, initial_step` the user\n-          passes in.\n+          continue at. Otherwise, return the `initial_epoch, initial_step` the\n+          user passes in.\n         \"\"\"\n         initial_step = 0\n         if self._training_state is not None:\n\n@@ -1723,7 +1723,7 @@ class TrainingTest(test_combinations.TestCase):\n             \"mse\",\n             run_eagerly=test_utils.should_run_eagerly(),\n         )\n-        history = model.fit(x, y, epochs=2)\n+        model.fit(x, y, epochs=2)\n         policy.set_global_policy(\"float32\")\n \n     @test_combinations.run_all_keras_modes\n@@ -2368,10 +2368,8 @@ class LossWeightingTest(test_combinations.TestCase):\n             y_train[:batch_size],\n             class_weight=class_weight,\n         )\n-        ref_score = model.evaluate(\n-            x_test, y_test, verbose=0\n-        )  # pylint: disable=unused-variable\n-        score = model.evaluate(  # pylint: disable=unused-variable\n+        ref_score = model.evaluate(x_test, y_test, verbose=0)  # noqa: F841\n+        score = model.evaluate(  # noqa: F841\n             x_test[test_ids, :], y_test[test_ids, :], verbose=0\n         )\n         # TODO(b/152990697): Fix the class weights test here.\n\n@@ -70,7 +70,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n     def skip_fetch_failure_exception(self):\n         try:\n             yield\n-        except zipfile.BadZipfile as e:\n+        except zipfile.BadZipfile:\n             # There can be a race when multiple processes are downloading the\n             # data.  Skip the test if that results in loading errors.\n             self.skipTest(\n\n@@ -898,9 +898,7 @@ class BatchNormalizationBase(Layer):\n         # Determine a boolean value for `training`: could be True, False, or\n         # None.\n         training_value = control_flow_util.constant_value(training)\n-        if (\n-            training_value == False\n-        ):  # pylint: disable=singleton-comparison,g-explicit-bool-comparison\n+        if training_value == False:  # noqa: E712\n             mean, variance = self.moving_mean, self.moving_variance\n         else:\n             if self.adjustment:\n\n@@ -209,9 +209,8 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n             a_prime = tf.random.uniform(shape=(3, 1))\n             a_prime = a_prime * 3\n             error_string = \"An exception should have been raised before this\"\n-            error_raised = \"An exception should have been raised before this\"\n             try:\n-                c = tf.random.uniform(shape=(3, 1))\n+                tf.random.uniform(shape=(3, 1))\n                 raise RuntimeError(error_string)\n \n             except ValueError as err:\n\n@@ -385,17 +385,8 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 self.assertEqual(self.evaluate(opt.loss_scale), 8)\n \n                 # Test Inf gradients are still skipped instead of being clipped\n-<<<<<<< HEAD\n                 loss = lambda: var * float(\"Inf\")\n                 run_fn = lambda: opt.minimize(loss, var_list=[var])\n-=======\n-                def run_fn():\n-                    def loss():\n-                        return var * float(\"Inf\")\n-\n-                    return opt.minimize(loss, var_list=[var])\n-\n->>>>>>> 0bb24689 (fix F811)\n                 run_op = strategy.experimental_run(run_fn)\n                 self._run_if_in_graph_mode(run_op)\n                 self.assertAllClose(\n@@ -426,17 +417,8 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertEqual(4.0, self.evaluate(opt.loss_scale))\n \n             # Test optimizer with NaN gradients\n-<<<<<<< HEAD\n             loss = lambda: var * float(\"NaN\")\n             run_fn = lambda: opt.minimize(loss, var_list=[var])\n-=======\n-            def run_fn():\n-                def loss():\n-                    return var * float(\"NaN\")\n-\n-                return opt.minimize(loss, var_list=[var])\n-\n->>>>>>> 0bb24689 (fix F811)\n             run_op = strategy.experimental_run(run_fn)\n             self._run_if_in_graph_mode(run_op)\n             # Variable should not change from before, due to NaN gradients.\n\n@@ -713,9 +713,8 @@ class KerasObjectLoader:\n         for node_id, (node, _) in self.loaded_nodes.items():\n             if (\n                 not isinstance(node, base_layer.Layer)\n-                or\n                 # Don't finalize models until all layers have finished loading.\n-                node_id in self.model_layer_dependencies\n+                or node_id in self.model_layer_dependencies\n             ):\n                 continue\n \n\n@@ -1125,7 +1125,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n         class Model(keras.models.Model):\n             def __init__(self):\n                 super().__init__()\n-                self.layer = CustomLayer()\n+                self.layer = CustomLayer()  # noqa: F821\n \n             @tf.function(input_signature=[tf.TensorSpec([None, 1])])\n             def call(self, inputs):\n\n@@ -365,7 +365,7 @@ def try_build_compiled_arguments(model):\n                 model.compiled_loss.build(model.outputs)\n             if not model.compiled_metrics.built:\n                 model.compiled_metrics.build(model.outputs, model.outputs)\n-        except:  # pylint: disable=bare-except\n+        except:  # noqa: E722\n             logging.warning(\n                 \"Compiled the loaded model, but the compiled metrics have \"\n                 \"yet to be built. `model.compile_metrics` will be empty \"\n\n@@ -18,8 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-# pylint: disable=wildcard-import\n-from keras.saving.utils_v1.export_output import *\n+from keras.saving.utils_v1.export_output import *  # noqa: F403\n from keras.saving.utils_v1.export_utils import EXPORT_TAG_MAP\n from keras.saving.utils_v1.export_utils import SIGNATURE_KEY_MAP\n from keras.saving.utils_v1.export_utils import build_all_signature_defs\n@@ -28,5 +27,4 @@ from keras.saving.utils_v1.export_utils import get_export_outputs\n from keras.saving.utils_v1.export_utils import get_temp_export_dir\n from keras.saving.utils_v1.export_utils import get_timestamped_export_dir\n \n-# pylint: enable=wildcard-import\n # LINT.ThenChange(//tensorflow/python/saved_model/model_utils/__init__.py)\n\n@@ -32,7 +32,7 @@ tf.compat.v1.enable_v2_behavior()\n \n # We put doctest after absltest so that it picks up the unittest monkeypatch.\n # Otherwise doctest tests aren't runnable at all.\n-import doctest  # pylint: disable=g-import-not-at-top,g-bad-import-order\n+import doctest  # noqa: E402\n \n FLAGS = flags.FLAGS\n \n\n@@ -298,7 +298,7 @@ def get_file(\n                 raise Exception(error_msg.format(origin, e.code, e.msg))\n             except urllib.error.URLError as e:\n                 raise Exception(error_msg.format(origin, e.errno, e.reason))\n-        except (Exception, KeyboardInterrupt) as e:\n+        except (Exception, KeyboardInterrupt):\n             if os.path.exists(fpath):\n                 os.remove(fpath)\n             raise\n\n@@ -15,6 +15,6 @@\n \"\"\"Keras model mode constants.\"\"\"\n \n # isort: off\n-from tensorflow.python.saved_model.model_utils.mode_keys import (  # noqa: E501\n+from tensorflow.python.saved_model.model_utils.mode_keys import (  # noqa: F401,E501\n     KerasModeKeys as ModeKeys,\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3613c3defc39c236fb1592c4f7ba1a9cc887343a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 839 | Lines Deleted: 1538 | Files Changed: 359 | Hunks: 951 | Methods Changed: 532 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 2377 | Churn Cumulative: 380193 | Contributors (this commit): 498 | Commits (past 90d): 2366 | Contributors (cumulative): 2907 | DMM Complexity: 0.30869565217391304\n\nDIFF:\n@@ -24,7 +24,7 @@ from keras.engine.sequential import Sequential\n from keras.engine.training import Model\n \n # isort: off\n-# pylint: disable=unused-import\n+\n from tensorflow.python import tf2\n from tensorflow.python.util.tf_export import keras_export\n \n\n@@ -94,7 +94,7 @@ def softmax(x, axis=-1):\n         )\n \n     # Cache the logits to use for crossentropy loss.\n-    output._keras_logits = x  # pylint: disable=protected-access\n+    output._keras_logits = x\n     return output\n \n \n@@ -410,7 +410,7 @@ def sigmoid(x):\n     \"\"\"\n     output = tf.sigmoid(x)\n     # Cache the logits to use for crossentropy loss.\n-    output._keras_logits = x  # pylint: disable=protected-access\n+    output._keras_logits = x\n     return output\n \n \n\n@@ -226,9 +226,7 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n                     )\n                 )\n             else:\n-                from scipy.stats import (\n-                    norm,  # pylint: disable=g-import-not-at-top\n-                )\n+                from scipy.stats import norm\n \n                 return x * norm.cdf(x)\n \n\n@@ -244,7 +244,7 @@ class ApiCompatibilityTest(tf.test.TestCase):\n                 verbose_diff_message = diff_message\n             else:\n                 # Do not truncate diff\n-                self.maxDiff = None  # pylint: disable=invalid-name\n+                self.maxDiff = None\n                 # Now we can run an actual proto diff.\n                 try:\n                     self.assertProtoEquals(expected_dict[key], actual_dict[key])\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n from keras.applications.convnext import ConvNeXtBase\n from keras.applications.convnext import ConvNeXtLarge\n\n@@ -183,7 +183,7 @@ class ApplicationsLoadWeightTest(tf.test.TestCase, parameterized.TestCase):\n         for app in apps:\n             try:\n                 model = app(weights=\"imagenet\")\n-            except Exception:  # pylint: disable=broad-except\n+            except Exception:\n                 self.skipTest(\"TODO(b/227700184): Re-enable.\")\n             self.assertShapeEqual(model.output_shape, (None, _IMAGENET_CLASSES))\n             x = _get_elephant(model.input_shape[1:3])\n\n@@ -12,10 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=missing-docstring\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-direct-tensorflow-import\n+\n+\n \"\"\"ConvNeXt models for Keras.\n \n References:\n@@ -734,7 +732,7 @@ ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n \n \n @keras_export(\"keras.applications.convnext.preprocess_input\")\n-def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the convnext model\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"DenseNet models for Keras.\n \n Reference:\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=missing-docstring\n+\n+\n \"\"\"EfficientNet models for Keras.\n \n Reference:\n@@ -840,7 +840,7 @@ EfficientNetB7.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB7\")\n \n \n @keras_export(\"keras.applications.efficientnet.preprocess_input\")\n-def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the efficientnet model\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=missing-docstring\n+\n+\n \"\"\"EfficientNet V2 models for Keras.\n \n Reference:\n@@ -1331,7 +1331,7 @@ EfficientNetV2L.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2L\")\n \n \n @keras_export(\"keras.applications.efficientnet_v2.preprocess_input\")\n-def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the EfficientNetV2 model\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"Inception-ResNet V2 model for Keras.\n \n Reference:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"Inception V3 model for Keras.\n \n Reference:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"MobileNet v1 models for Keras.\n \n MobileNet is a general architecture and can be used for multiple use cases.\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"MobileNet v2 models for Keras.\n \n MobileNetV2 is a general architecture and can be used for multiple use cases.\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=missing-function-docstring\n+\n+\n \"\"\"MobileNet v3 models for Keras.\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -670,7 +670,7 @@ def _inverted_res_block(\n \n \n @keras_export(\"keras.applications.mobilenet_v3.preprocess_input\")\n-def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the mobilenet_v3 model\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"NASNet-A models for Keras.\n \n NASNet refers to Neural Architecture Search Network, a family of models\n\n@@ -12,9 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=missing-docstring\n-# pylint: disable=g-classes-have-attributes\n+\n \n \"\"\"RegNet models for Keras.\n \n@@ -1811,7 +1809,7 @@ RegNetY320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY320\")\n \n \n @keras_export(\"keras.applications.regnet.preprocess_input\")\n-def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the regnet model\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"ResNet models for Keras.\n \n Reference:\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=missing-function-docstring\n+\n+\n \"\"\"ResNet-RS models for Keras.\n \n Reference:\n@@ -539,7 +539,6 @@ def ResNetRS(\n     weights=\"imagenet\",\n     input_tensor=None,\n     classes=1000,\n-    # pylint: disable=g-bare-generic\n     classifier_activation: Union[str, Callable] = \"softmax\",\n     include_preprocessing=True,\n ):\n@@ -947,7 +946,6 @@ def ResNetRS420(\n     )\n \n \n-# pylint: disable=unused-argument\n @keras_export(\"keras.applications.resnet_rs.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"ResNet v2 models for Keras.\n \n Reference:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"VGG16 model for Keras.\n \n Reference:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"VGG19 model for Keras.\n \n Reference:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n+\n \"\"\"Xception V1 model for Keras.\n \n On ImageNet, this model gets to a top-1 validation accuracy of 0.790\n\n@@ -12,12 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=redefined-outer-name\n-# pylint: disable=redefined-builtin\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-bad-import-order\n-# pylint: disable=missing-function-docstring\n+\n+\n \"\"\"Keras backend API.\"\"\"\n \n import collections\n@@ -708,7 +704,7 @@ def _current_graph(op_input_list, graph=None):\n             op_input, (tf.Operation, tf.Tensor, tf.__internal__.CompositeTensor)\n         ) and (\n             (not isinstance(op_input, tf.Tensor)) or type(op_input) == tf.Tensor\n-        ):  # pylint: disable=unidiomatic-typecheck\n+        ):\n             graph_element = op_input\n         else:\n             graph_element = _as_graph_element(op_input)\n@@ -1451,7 +1447,7 @@ def placeholder(\n         # when the placeholder is built in a top-level eager context\n         # (intended to be used with keras.backend.function)\n         from keras.engine import (\n-            input_layer,  # pylint: disable=g-import-not-at-top\n+            input_layer,\n         )\n \n         x = input_layer.Input(tensor=x)\n@@ -1472,7 +1468,7 @@ def is_placeholder(x):\n     try:\n         if tf.compat.v1.executing_eagerly_outside_functions():\n             return hasattr(x, \"_is_backend_placeholder\")\n-        from keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\n+        from keras.utils import tf_utils\n \n         if tf_utils.is_extension_type(x):\n             flat_components = tf.nest.flatten(x, expand_composites=True)\n@@ -1977,7 +1973,7 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n             self._generator = None\n         elif self._rng_type == self.RNG_STATEFUL:\n             from keras.utils import (\n-                tf_utils,  # pylint: disable=g-import-not-at-top\n+                tf_utils,\n             )\n \n             with tf_utils.maybe_init_scope(self):\n@@ -4242,7 +4238,7 @@ def batch_get_value(tensors):\n     \"\"\"\n     if tf.executing_eagerly():\n         return [x.numpy() for x in tensors]\n-    elif tf.inside_function():  # pylint: disable=protected-access\n+    elif tf.inside_function():\n         raise RuntimeError(\"Cannot get value inside Tensorflow graph function.\")\n     if tensors:\n         return get_session(tensors).run(tensors)\n@@ -4526,7 +4522,7 @@ class GraphExecutionFunction:\n         # the CompositeTensors. E.g., if output_structure contains a\n         # SparseTensor, then this ensures that we return its value as a\n         # SparseTensorValue rather than a SparseTensor.\n-        from keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\n+        from keras.utils import tf_utils\n \n         if tf_utils.is_extension_type(tensor):\n             return self._session.run(tensor)\n@@ -4623,8 +4619,8 @@ def function(inputs, outputs, updates=None, name=None, **kwargs):\n                 \"`updates` argument is not supported during \"\n                 \"eager execution. You passed: %s\" % (updates,)\n             )\n-        from keras import models  # pylint: disable=g-import-not-at-top\n-        from keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\n+        from keras import models\n+        from keras.utils import tf_utils\n \n         model = models.Model(inputs=inputs, outputs=outputs)\n \n@@ -5266,7 +5262,7 @@ def in_train_phase(x, alt, training=None):\n         the `training` flag defaults to `K.learning_phase()`.\n     \"\"\"\n     from keras.engine import (\n-        base_layer_utils,  # pylint: disable=g-import-not-at-top\n+        base_layer_utils,\n     )\n \n     if training is None:\n@@ -5497,7 +5493,7 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     # Use logits whenever they are available. `softmax` and `sigmoid`\n     # activations cache logits on the `output` Tensor.\n     if hasattr(output, \"_keras_logits\"):\n-        output = output._keras_logits  # pylint: disable=protected-access\n+        output = output._keras_logits\n         if from_logits:\n             warnings.warn(\n                 '\"`categorical_crossentropy` received `from_logits=True`, but '\n@@ -5564,7 +5560,7 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     # Use logits whenever they are available. `softmax` and `sigmoid`\n     # activations cache logits on the `output` Tensor.\n     if hasattr(output, \"_keras_logits\"):\n-        output = output._keras_logits  # pylint: disable=protected-access\n+        output = output._keras_logits\n         if from_logits:\n             warnings.warn(\n                 '\"`sparse_categorical_crossentropy` received '\n@@ -5665,7 +5661,7 @@ def binary_crossentropy(target, output, from_logits=False):\n     # Use logits whenever they are available. `softmax` and `sigmoid`\n     # activations cache logits on the `output` Tensor.\n     if hasattr(output, \"_keras_logits\"):\n-        output = output._keras_logits  # pylint: disable=protected-access\n+        output = output._keras_logits\n         if from_logits:\n             warnings.warn(\n                 '\"`binary_crossentropy` received `from_logits=True`, '\n@@ -7222,7 +7218,7 @@ def configure_and_create_distributed_session(distribution_strategy):\n             distribution_strategy.configure(session_config)\n             master = (\n                 distribution_strategy.extended._tpu_cluster_resolver.master()\n-            )  # pylint: disable=protected-access\n+            )\n             session = tf.compat.v1.Session(config=session_config, target=master)\n         else:\n             worker_context = dc.get_current_worker_context()\n@@ -7416,9 +7412,7 @@ class ContextValueCache(weakref.WeakKeyDictionary):\n \n         value = self._get_recursive(key)\n         if value is None:\n-            value = self[\n-                key\n-            ] = self.default_factory()  # pylint:disable=not-callable\n+            value = self[key] = self.default_factory()\n         return value\n \n     def setdefault(self, key=None, default=None, kwargs=None):\n\n@@ -102,7 +102,7 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n         x = tf.convert_to_tensor([[1.0]])\n \n         def fn():\n-            layer(x)  # pylint: disable=not-callable\n+            layer(x)\n \n         self._run(fn, 10000)\n \n@@ -116,7 +116,7 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n         model = tf.keras.Model(inputs=model_input, outputs=model_output)\n \n         def fn():\n-            model(x)  # pylint: disable=not-callable\n+            model(x)\n \n         fn()\n         self._run(fn, 100)\n@@ -145,7 +145,7 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n         self._run(fn, 10000)\n \n \n-class KerasLayerCallOverheadBenchmarks(  # pylint: disable=undefined-variable\n+class KerasLayerCallOverheadBenchmarks(\n     MicroBenchmarksBase, metaclass=tf.__internal__.test.ParameterizedBenchmark\n ):\n \n\n@@ -24,7 +24,7 @@ _LOSS = \"binary_crossentropy\"\n _OPTIMIZER = \"rmsprop\"\n \n \n-class KerasModelCPUBenchmark(  # pylint: disable=undefined-variable\n+class KerasModelCPUBenchmark(\n     tf.test.Benchmark, metaclass=tf.__internal__.test.ParameterizedBenchmark\n ):\n     \"\"\"Required Arguments for measure_performance.\n\n@@ -169,7 +169,7 @@ class Antirectifier(tf.keras.layers.Layer):\n             trainable=True,\n         )\n \n-    def call(self, inputs):  # pylint: disable=arguments-differ\n+    def call(self, inputs):\n         inputs -= tf.reduce_mean(inputs, axis=-1, keepdims=True)\n         pos = tf.nn.relu(inputs)\n         neg = tf.nn.relu(-inputs)\n\n@@ -48,9 +48,9 @@ class TextWithTransformerBenchmark(tf.test.Benchmark):\n         embedding_layer = TokenAndPositionEmbedding(\n             self.max_len, self.max_feature, embed_dim\n         )\n-        x = embedding_layer(inputs)  # pylint: disable=not-callable\n+        x = embedding_layer(inputs)\n         transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n-        x = transformer_block(x)  # pylint: disable=not-callable\n+        x = transformer_block(x)\n         x = tf.keras.layers.GlobalAvgPool1D()(x)\n         x = tf.keras.layers.Dropout(0.1)(x)\n         x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n@@ -189,7 +189,7 @@ class MultiHeadSelfAttention(tf.keras.layers.Layer):\n         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n         return tf.transpose(x, perm=[0, 2, 1, 3])\n \n-    def call(self, inputs):  # pylint: disable=arguments-differ\n+    def call(self, inputs):\n         # x.shape = [batch_size, seq_len, embedding_dim]\n         batch_size = tf.shape(inputs)[0]\n         query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n@@ -234,8 +234,8 @@ class TransformerBlock(tf.keras.layers.Layer):\n         self.dropout1 = tf.keras.layers.Dropout(rate)\n         self.dropout2 = tf.keras.layers.Dropout(rate)\n \n-    def call(self, inputs, training):  # pylint: disable=arguments-differ\n-        attn_output = self.att(inputs)  # pylint: disable=not-callable\n+    def call(self, inputs, training):\n+        attn_output = self.att(inputs)\n         attn_output = self.dropout1(attn_output, training=training)\n         out1 = self.layernorm1(inputs + attn_output)\n         ffn_output = self.ffn(out1)\n@@ -255,7 +255,7 @@ class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n             input_dim=maxlen, output_dim=embed_dim\n         )\n \n-    def call(self, x):  # pylint: disable=arguments-differ\n+    def call(self, x):\n         maxlen = tf.shape(x)[-1]\n         positions = tf.range(start=0, limit=maxlen, delta=1)\n         positions = self.pos_emb(positions)\n\n@@ -427,7 +427,7 @@ POOLING_LAYERS = [\n ]\n \n \n-class KerasLayerBenchmarks(  # pylint: disable=undefined-variable\n+class KerasLayerBenchmarks(\n     layer_benchmarks_test_base.LayerBenchmarksBase,\n     metaclass=tf.__internal__.test.ParameterizedBenchmark,\n ):\n\n@@ -18,7 +18,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n try:\n-    import memory_profiler  # pylint:disable=g-import-not-at-top\n+    import memory_profiler\n except ImportError:\n     memory_profiler = None\n \n\n@@ -145,7 +145,7 @@ class KerasComponentsBenchmarks(tf.test.Benchmark):\n         model = SubclassedKerasModel()\n         data = tf.random.uniform((10, 10))\n \n-        func = lambda: model(data)  # pylint: disable=not-callable\n+        func = lambda: model(data)\n         # First call is more expensive (creates variables etc.), discount that.\n         func()\n \n@@ -159,12 +159,10 @@ class KerasComponentsBenchmarks(tf.test.Benchmark):\n     def benchmark_keras_model_functional(self):\n         model = make_keras_model()\n         data = tf.random.uniform((10, 10))\n-        func = lambda: model(data)  # pylint: disable=not-callable\n+        func = lambda: model(data)\n         # Symmetry with benchmark_keras_model_subclassed\n         func()\n-        assert np.equal(\n-            func(), SubclassedKerasModel()(data)\n-        ).all()  # pylint: disable=not-callable\n+        assert np.equal(func(), SubclassedKerasModel()(data)).all()\n         self._run(func, 30000)\n \n     def benchmark_keras_model_sequential(self):\n\n@@ -27,7 +27,7 @@ from absl import flags\n from absl import logging\n \n try:\n-    import memory_profiler  # pylint:disable=g-import-not-at-top\n+    import memory_profiler\n except ImportError:\n     memory_profiler = None\n \n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-import-not-at-top\n-# pylint: disable=g-classes-have-attributes\n+\n+\n \"\"\"Callbacks: utilities called at certain points during model training.\"\"\"\n \n import collections\n@@ -99,9 +99,7 @@ def configure_callbacks(\n     callback_list = CallbackList(callbacks)\n \n     # Set callback model\n-    callback_model = (\n-        model._get_callback_model()\n-    )  # pylint: disable=protected-access\n+    callback_model = model._get_callback_model()\n     callback_list.set_model(callback_model)\n \n     set_callback_parameters(\n@@ -229,7 +227,7 @@ class CallbackList:\n             self.set_params(params)\n \n         # Performance optimization: determines if batch hooks need to be called.\n-        # pylint: disable=protected-access\n+\n         self._supports_tf_logs = all(\n             getattr(cb, \"_supports_tf_logs\", False) for cb in self.callbacks\n         )\n@@ -250,7 +248,6 @@ class CallbackList:\n         self._should_call_predict_batch_hooks = any(\n             cb._implements_predict_batch_hooks() for cb in self.callbacks\n         )\n-        # pylint: enable=protected-access\n \n         self._disallow_batch_hooks_in_ps_strategy()\n \n@@ -587,7 +584,7 @@ class CallbackList:\n \n     def _disallow_batch_hooks_in_ps_strategy(self):\n         \"\"\"Error out if batch-level callbacks are passed with PSStrategy.\"\"\"\n-        # pylint: disable=protected-access\n+\n         strategy = tf.distribute.get_strategy()\n         if strategy._should_use_with_coordinator:\n             unsupported_callbacks = []\n@@ -607,7 +604,6 @@ class CallbackList:\n                     \"`ParameterServerStrategy`. Found unsupported \"\n                     f\"callbacks: {unsupported_callbacks}\"\n                 )\n-        # pylint: enable=protected-access\n \n \n @keras_export(\"keras.callbacks.Callback\")\n@@ -672,7 +668,7 @@ class Callback:\n     \"\"\"\n \n     def __init__(self):\n-        self.validation_data = None  # pylint: disable=g-missing-from-attributes\n+        self.validation_data = None\n         self.model = None\n         # Whether this Callback should only run on the chief worker in a\n         # Multi-Worker setting.\n@@ -1056,15 +1052,9 @@ class ProgbarLogger(Callback):\n         self._call_batch_hooks = self.verbose == 1\n         if self.target is None:\n             try:\n-                self._train_step = (\n-                    self.model._train_counter\n-                )  # pylint: disable=protected-access\n-                self._test_step = (\n-                    self.model._test_counter\n-                )  # pylint: disable=protected-access\n-                self._predict_step = (\n-                    self.model._predict_counter\n-                )  # pylint: disable=protected-access\n+                self._train_step = self.model._train_counter\n+                self._test_step = self.model._test_counter\n+                self._predict_step = self.model._predict_counter\n             except AttributeError:\n                 self._call_batch_hooks = True\n \n@@ -1136,9 +1126,7 @@ class ProgbarLogger(Callback):\n                 unit_name=\"step\" if self.use_steps else \"sample\",\n             )\n \n-        self.progbar._update_stateful_metrics(\n-            self.stateful_metrics\n-        )  # pylint: disable=protected-access\n+        self.progbar._update_stateful_metrics(self.stateful_metrics)\n \n     def _implements_train_batch_hooks(self):\n         return self._call_batch_hooks\n@@ -1470,7 +1458,7 @@ class ModelCheckpoint(Callback):\n \n     def on_epoch_end(self, epoch, logs=None):\n         self.epochs_since_last_save += 1\n-        # pylint: disable=protected-access\n+\n         if self.save_freq == \"epoch\":\n             self._save_model(epoch=epoch, batch=None, logs=logs)\n \n@@ -1584,7 +1572,7 @@ class ModelCheckpoint(Callback):\n \n     def _get_file_path(self, epoch, batch, logs):\n         \"\"\"Returns the file path for checkpoint.\"\"\"\n-        # pylint: disable=protected-access\n+\n         try:\n             # `filepath` may contain placeholders such as\n             # `{epoch:02d}`,`{batch:02d}` and `{mape:.2f}`. A mismatch between\n@@ -1832,7 +1820,6 @@ class BackupAndRestore(Callback):\n     def on_train_begin(self, logs=None):\n         # TrainingState is used to manage the training state needed for\n         # failure-recovery of a worker in training.\n-        # pylint: disable=protected-access\n \n         if self.model._distribution_strategy and not isinstance(\n             self.model.distribute_strategy, self._supported_strategies\n@@ -1862,7 +1849,7 @@ class BackupAndRestore(Callback):\n         return self._save_freq != \"epoch\"\n \n     def on_train_end(self, logs=None):\n-        # pylint: disable=protected-access\n+\n         # On exit of training, delete the training state backup file that was\n         # saved for the purpose of worker recovery.\n         self._training_state.delete_backup()\n@@ -2243,7 +2230,7 @@ def keras_model_summary(name, data, step=None):\n \n     try:\n         json_string = data.to_json()\n-    except Exception as exc:  # pylint: disable=broad-except\n+    except Exception as exc:\n         # An exception should not break a model code.\n         logging.warning(\n             \"Model failed to serialize as JSON. Ignoring... %s\", exc\n@@ -2262,7 +2249,7 @@ def keras_model_summary(name, data, step=None):\n \n @keras_export(\"keras.callbacks.TensorBoard\", v1=[])\n class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n-    # pylint: disable=line-too-long\n+\n     \"\"\"Enable visualizations for TensorBoard.\n \n     TensorBoard is a visualization tool provided with TensorFlow.\n@@ -2389,8 +2376,6 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     ```\n     \"\"\"\n \n-    # pylint: enable=line-too-long\n-\n     def __init__(\n         self,\n         log_dir=\"logs\",\n@@ -2477,14 +2462,10 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         self._log_write_dir = self._get_log_write_dir()\n \n         self._train_dir = os.path.join(self._log_write_dir, \"train\")\n-        self._train_step = (\n-            self.model._train_counter\n-        )  # pylint: disable=protected-access\n+        self._train_step = self.model._train_counter\n \n         self._val_dir = os.path.join(self._log_write_dir, \"validation\")\n-        self._val_step = (\n-            self.model._test_counter\n-        )  # pylint: disable=protected-access\n+        self._val_step = self.model._test_counter\n \n         self._writers = {}  # Resets writers.\n \n@@ -2529,9 +2510,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                 # If the train_function is a `tf.function`, we can write out a\n                 # graph\n                 if hasattr(train_fn, \"function_spec\"):\n-                    tf.summary.graph(\n-                        train_fn._concrete_stateful_fn.graph\n-                    )  # pylint: disable=protected-access\n+                    tf.summary.graph(train_fn._concrete_stateful_fn.graph)\n \n     def _write_keras_model_summary(self):\n         \"\"\"Writes Keras graph network summary to TensorBoard.\"\"\"\n@@ -2540,7 +2519,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                 summary_writable = (\n                     self.model._is_graph_network\n                     or self.model.__class__.__name__ == \"Sequential\"\n-                )  # pylint: disable=protected-access\n+                )\n                 if summary_writable:\n                     keras_model_summary(\"keras\", self.model, step=0)\n \n\n@@ -49,12 +49,12 @@ from keras.utils import np_utils\n from tensorflow.python.platform import tf_logging as logging\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n try:\n-    import requests  # pylint:disable=g-import-not-at-top\n+    import requests\n except ImportError:\n     requests = None\n \n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-import-not-at-top\n-# pylint: disable=g-classes-have-attributes\n+\n+\n \"\"\"Callbacks: utilities called at certain points during model training.\"\"\"\n \n import os\n@@ -31,7 +31,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n @keras_export(v1=[\"keras.callbacks.TensorBoard\"])\n class TensorBoard(callbacks.TensorBoard):\n-    # pylint: disable=line-too-long\n+\n     \"\"\"Enable visualizations for TensorBoard.\n \n     TensorBoard is a visualization tool provided with TensorFlow.\n@@ -104,8 +104,6 @@ class TensorBoard(callbacks.TensorBoard):\n     @end_compatibility\n     \"\"\"\n \n-    # pylint: enable=line-too-long\n-\n     def __init__(\n         self,\n         log_dir=\"./logs\",\n@@ -259,7 +257,7 @@ class TensorBoard(callbacks.TensorBoard):\n         if self.embeddings_freq and self.embeddings_data is not None:\n             # Avoid circular dependency.\n             from keras.engine import (\n-                training_utils_v1,  # pylint: disable=g-import-not-at-top\n+                training_utils_v1,\n             )\n \n             self.embeddings_data = training_utils_v1.standardize_input_data(\n@@ -422,7 +420,7 @@ class TensorBoard(callbacks.TensorBoard):\n \n         # check if histogram summary should be run for this epoch\n         if self.histogram_freq and epoch % self.histogram_freq == 0:\n-            # pylint: disable=protected-access\n+\n             # add the histogram summary op if it should run this epoch\n             self.model._make_test_function()\n             if self.merged not in self.model.test_function.fetches:\n@@ -430,7 +428,6 @@ class TensorBoard(callbacks.TensorBoard):\n                 self.model.test_function.fetch_callbacks[\n                     self.merged\n                 ] = self._fetch_callback\n-            # pylint: enable=protected-access\n \n     def on_epoch_end(self, epoch, logs=None):\n         \"\"\"Checks if summary ops should run next epoch, logs scalar\n@@ -451,12 +448,11 @@ class TensorBoard(callbacks.TensorBoard):\n \n         # pop the histogram summary op after each epoch\n         if self.histogram_freq:\n-            # pylint: disable=protected-access\n+\n             if self.merged in self.model.test_function.fetches:\n                 self.model.test_function.fetches.remove(self.merged)\n             if self.merged in self.model.test_function.fetch_callbacks:\n                 self.model.test_function.fetch_callbacks.pop(self.merged)\n-            # pylint: enable=protected-access\n \n         if self.embeddings_data is None and self.embeddings_freq:\n             raise ValueError(\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=g-classes-have-attributes\n+\n+\n \"\"\"Constraints: functions that impose constraints on weight values.\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -297,9 +297,7 @@ class RadialConstraint(Constraint):\n             backend.cast(tf.math.floormod(kernel_shape, 2), \"bool\"),\n             lambda: kernel[start - 1 : start, start - 1 : start],\n             lambda: kernel[start - 1 : start, start - 1 : start]\n-            + backend.zeros(  # pylint: disable=g-long-lambda\n-                (2, 2), dtype=kernel.dtype\n-            ),\n+            + backend.zeros((2, 2), dtype=kernel.dtype),\n         )\n         index = backend.switch(\n             backend.cast(tf.math.floormod(kernel_shape, 2), \"bool\"),\n\n@@ -63,9 +63,7 @@ def load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113):\n         origin=origin_folder + \"boston_housing.npz\",\n         file_hash=\"f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5\",  # noqa: E501\n     )\n-    with np.load(\n-        path, allow_pickle=True\n-    ) as f:  # pylint: disable=unexpected-keyword-arg\n+    with np.load(path, allow_pickle=True) as f:\n         x = f[\"x\"]\n         y = f[\"y\"]\n \n\n@@ -113,9 +113,7 @@ def load_data(\n         origin=origin_folder + \"imdb.npz\",\n         file_hash=\"69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f\",  # noqa: E501\n     )\n-    with np.load(\n-        path, allow_pickle=True\n-    ) as f:  # pylint: disable=unexpected-keyword-arg\n+    with np.load(path, allow_pickle=True) as f:\n         x_train, labels_train = f[\"x_train\"], f[\"y_train\"]\n         x_test, labels_test = f[\"x_test\"], f[\"y_test\"]\n \n\n@@ -77,9 +77,7 @@ def load_data(path=\"mnist.npz\"):\n         origin=origin_folder + \"mnist.npz\",\n         file_hash=\"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\",  # noqa: E501\n     )\n-    with np.load(\n-        path, allow_pickle=True\n-    ) as f:  # pylint: disable=unexpected-keyword-arg\n+    with np.load(path, allow_pickle=True) as f:\n         x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n         x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n \n\n@@ -119,9 +119,7 @@ def load_data(\n         origin=origin_folder + \"reuters.npz\",\n         file_hash=\"d6586e694ee56d7a4e65172e12b3e987c03096cb01eab99753921ef915959916\",  # noqa: E501\n     )\n-    with np.load(\n-        path, allow_pickle=True\n-    ) as f:  # pylint: disable=unexpected-keyword-arg\n+    with np.load(path, allow_pickle=True) as f:\n         xs, labels = f[\"x\"], f[\"y\"]\n \n     rng = np.random.RandomState(seed)\n\n@@ -14,5 +14,5 @@\n # ==============================================================================\n \"\"\"Keras' Distribution Strategy library.\"\"\"\n \n-# pylint: disable=unused-import\n+\n from keras.distribute import sidecar_evaluator\n\n@@ -131,13 +131,13 @@ class _WorkerContext:\n                 \"You cannot run distribute coordinator in a `worker_fn`.\\t\"\n                 + self._debug_message()\n             )\n-        # pylint: disable=protected-access\n+\n         _worker_context.current = self\n \n     def __exit__(\n         self, unused_exception_type, unused_exception_value, unused_traceback\n     ):\n-        # pylint: disable=protected-access\n+\n         _worker_context.current = None\n \n     def _get_master_target(self):\n@@ -465,7 +465,7 @@ def _run_std_server(\n def _configure_session_config_for_std_servers(\n     strategy, eval_strategy, session_config, cluster_spec, task_type, task_id\n ):\n-    # pylint: disable=g-doc-args\n+\n     \"\"\"Call strategy's `configure` to mutate the session_config.\n \n     The session_config is currently needed as default config for a TensorFlow\n@@ -631,9 +631,7 @@ def run_distribute_coordinator(\n         # TODO(yuefengz): validate cluster_spec.\n         cluster_spec = normalize_cluster_spec(cluster_spec)\n     elif hasattr(strategy.extended, \"_cluster_resolver\"):\n-        cluster_resolver = (\n-            strategy.extended._cluster_resolver\n-        )  # pylint: disable=protected-access\n+        cluster_resolver = strategy.extended._cluster_resolver\n         task_type = cluster_resolver.task_type\n         task_id = cluster_resolver.task_id\n         rpc_layer = cluster_resolver.rpc_layer or rpc_layer\n\n@@ -50,7 +50,7 @@ import tensorflow.compat.v2 as tf\n \n \n def _get_base_dirpath(strategy):\n-    task_id = strategy.extended._task_id  # pylint: disable=protected-access\n+    task_id = strategy.extended._task_id\n     return \"workertemp_\" + str(task_id)\n \n \n@@ -86,9 +86,7 @@ def write_dirpath(dirpath, strategy):\n         # If strategy is still not available, this is not in distributed\n         # training.  Fallback to original dirpath.\n         return dirpath\n-    if (\n-        not strategy.extended._in_multi_worker_mode()\n-    ):  # pylint: disable=protected-access\n+    if not strategy.extended._in_multi_worker_mode():\n         return dirpath\n     if strategy.extended.should_checkpoint:\n         return dirpath\n\n@@ -26,9 +26,7 @@ FLAGS = flags.FLAGS\n # core MirroredStrategy only. Remove this check when contrib MirroredStrategy is\n # no longer needed.\n def global_batch_size_supported(distribution_strategy):\n-    return (\n-        distribution_strategy.extended._global_batch_size\n-    )  # pylint: disable=protected-access\n+    return distribution_strategy.extended._global_batch_size\n \n \n def call_replica_local_fn(fn, *args, **kwargs):\n\n@@ -33,8 +33,6 @@ from keras.utils.mode_keys import ModeKeys\n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n \n-# pylint:disable=protected-access\n-\n \n def set_weights(distribution_strategy, dist_model, weights):\n     \"\"\"Sets the weights of the replicated models.\n@@ -237,7 +235,7 @@ def flatten_per_replica_values(distribution_strategy, per_replica_values):\n       List of values of all the PerReplica objects.\n \n     \"\"\"\n-    # pylint: disable=g-complex-comprehension\n+\n     # This function takes a PerReplica object or a list of PerReplica objects\n     # and returns all the values associated with it.\n     return [\n@@ -404,9 +402,7 @@ def validate_all_tensor_shapes(x, x_values):\n \n def _wait_for_variable_initialization(session):\n     \"\"\"Utility to wait for variables to be initialized.\"\"\"\n-    all_variables = backend._get_variables(\n-        backend.get_graph()\n-    )  # pylint: disable=protected-access\n+    all_variables = backend._get_variables(backend.get_graph())\n     candidate_vars = []\n     for v in all_variables:\n         if not getattr(v, \"_keras_initialized\", False):\n@@ -423,7 +419,7 @@ def _wait_for_variable_initialization(session):\n         for flag, v in zip(is_initialized, candidate_vars):\n             if not flag:\n                 uninitialized_vars.append(v)\n-            v._keras_initialized = True  # pylint: disable=protected-access\n+            v._keras_initialized = True\n         if not uninitialized_vars:\n             break\n \n@@ -431,9 +427,7 @@ def _wait_for_variable_initialization(session):\n def init_restore_or_wait_for_variables():\n     \"\"\"Initialize or restore variables or wait for variables to be\n     initialized.\"\"\"\n-    backend._initialize_variables(\n-        backend._get_session()\n-    )  # pylint: disable=protected-access\n+    backend._initialize_variables(backend._get_session())\n \n \n def validate_inputs(x, y):\n@@ -768,8 +762,8 @@ def _build_network_on_replica(model, mode, inputs=None, targets=None):\n       A new model with shared layers with the old model.\n     \"\"\"\n     # Need to do imports here since we run into a circular dependency error.\n-    from keras import models  # pylint: disable=g-import-not-at-top\n-    from keras.engine import sequential  # pylint: disable=g-import-not-at-top\n+    from keras import models\n+    from keras.engine import sequential\n \n     # We rely on the internal methods to avoid having share_weights weights in\n     # the public API.\n@@ -833,7 +827,7 @@ def _clone_and_build_model(model, mode, inputs=None, targets=None):\n     \"\"\"Clone and build the given keras_model.\"\"\"\n     # We need to set the import here since we run into a circular dependency\n     # error.\n-    from keras import models  # pylint: disable=g-import-not-at-top\n+    from keras import models\n \n     cloned_model = models.clone_model(model, input_tensors=inputs)\n \n@@ -1236,7 +1230,7 @@ def filter_distributed_callbacks(callbacks_list, model):\n         callback\n         for callback in callbacks_list\n         if not callback._chief_worker_only\n-    ]  # pylint: disable=protected-access\n+    ]\n \n \n def _update_sample_weight_modes(model, mode, sample_weights):\n\n@@ -75,7 +75,7 @@ class MirroredStrategyDefunTest(tf.test.TestCase, parameterized.TestCase):\n             optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.25)\n             update_ops = optimizer._distributed_apply(\n                 distribution, grads_and_vars\n-            )  # pylint: disable=protected-access\n+            )\n \n             if not tf.executing_eagerly():\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n\n@@ -78,10 +78,8 @@ class MirroredVariableCreationTest(tf.test.TestCase):\n \n     def _is_mirrored(self, val):\n         if distributed_training_utils.is_distributed_variable(val):\n-            if val._policy:  # pylint: disable=protected-access\n-                return (\n-                    val._policy._is_mirrored()\n-                )  # pylint: disable=protected-access\n+            if val._policy:\n+                return val._policy._is_mirrored()\n         # Since `Mirrored` is a private symbol in tf.distribute, we're checking\n         # with `DistributedValues` as an approximation.\n         return isinstance(val, tf.distribute.DistributedValues)\n\n@@ -33,11 +33,11 @@ from tensorflow.python.training.server_lib import (\n \n _portpicker_import_error = None\n try:\n-    import portpicker  # pylint: disable=g-import-not-at-top\n+    import portpicker\n except (\n     ImportError,\n     ModuleNotFoundError,\n-) as _error:  # pylint: disable=invalid-name\n+) as _error:\n     _portpicker_import_error = _error\n     portpicker = None\n \n@@ -105,7 +105,7 @@ def make_parameter_server_cluster(num_workers, num_ps):\n def pick_unused_port():\n     \"\"\"Returns an unused and unassigned local port.\"\"\"\n     if _portpicker_import_error:\n-        raise _portpicker_import_error  # pylint: disable=raising-bad-type\n+        raise _portpicker_import_error\n \n     global ASSIGNED_PORTS\n     with lock:\n@@ -138,7 +138,7 @@ def _create_cluster(\n ):\n     \"\"\"Creates and starts local servers and returns the cluster_spec dict.\"\"\"\n     if _portpicker_import_error:\n-        raise _portpicker_import_error  # pylint: disable=raising-bad-type\n+        raise _portpicker_import_error\n     worker_ports = [pick_unused_port() for _ in range(num_workers)]\n     ps_ports = [pick_unused_port() for _ in range(num_ps)]\n \n\n@@ -32,8 +32,6 @@ from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import layout_map as layout_map_lib\n from keras.utils import np_utils\n \n-# pylint: disable=missing-function-docstring\n-\n NUM_CLASS = 10  # MNIST has 10 digits\n \n \n\n@@ -27,7 +27,6 @@ from keras.engine import base_layer\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=missing-class-docstring\n \n # We will skip the path for certain attributes when mapping the layout, e.g.\n # model._self_tracked_trackables, or layer._trainable_weights/\n@@ -257,7 +256,7 @@ def _map_subclass_model_variable(model, layout_map):\n     # Note that the model._flatten is a method from tf.Module, and it returns\n     # duplicated items (since some of the items have different paths).\n     for path, variable in model._flatten(\n-        predicate=_is_lazy_init_variable,  # pylint: disable=protected-access\n+        predicate=_is_lazy_init_variable,\n         with_path=True,\n     ):\n         # Note that path is a tuple that contains string and ints, eg:\n@@ -271,7 +270,7 @@ def _map_subclass_model_variable(model, layout_map):\n         _set_object_by_path(model, path, new_variable)\n         lazy_init_variable_to_tf_variable_map[id(variable)] = new_variable\n \n-    for layer in model._flatten(  # pylint: disable=protected-access\n+    for layer in model._flatten(\n         predicate=lambda o: isinstance(o, base_layer.Layer)\n     ):\n         _config_dvariable_regularization(\n@@ -280,7 +279,7 @@ def _map_subclass_model_variable(model, layout_map):\n     # After we replaced all the variables, we want to make sure all the cached\n     # attributes are having the new variable, rather than old LazyInitVariable.\n     for path, variable in model._flatten(\n-        predicate=_is_lazy_init_variable,  # pylint: disable=protected-access\n+        predicate=_is_lazy_init_variable,\n         with_path=True,\n     ):\n         tf_variable = lazy_init_variable_to_tf_variable_map[id(variable)]\n@@ -349,7 +348,7 @@ def _init_state_variable_for_rng(model, layout_map):\n         BaseRandomLayers.\n       layout_map: used to get the default mesh information to create DVariable.\n     \"\"\"\n-    # pylint: disable=protected-access\n+\n     for l in model._flatten(\n         predicate=lambda o: isinstance(o, base_layer.BaseRandomLayer)\n     ):\n@@ -393,7 +392,7 @@ def _config_dvariable_regularization(\n       lazy_init_variable_to_tf_variable_map: the dict between LazyInitVariable\n         ID and newly created DVariable.\n     \"\"\"\n-    # pylint: disable=protected-access\n+\n     for (name, variable, regualarizer) in layer._captured_weight_regularizer:\n         if not _is_lazy_init_variable(variable):\n             raise ValueError(\n@@ -432,7 +431,7 @@ def _create_dvariable(layout_map, object_path, variable):\n         layout = dtensor.Layout.replicated(\n             mesh=layout_map.get_default_mesh(), rank=variable_rank\n         )\n-    init_val = variable._initial_value  # pylint: disable=protected-access\n+    init_val = variable._initial_value\n     if callable(init_val):\n         with lazy_variable.disable_init_variable_creator():\n             init_val = utils.call_with_layout(init_val, layout)\n\n@@ -47,9 +47,7 @@ def _infer_shape_dtype_and_create_handle(initial_value, shape, dtype, name):\n                 s=[compat.as_bytes(\"loc:@%s\" % handle_name)]\n             )\n         )\n-        with ops.get_default_graph()._attr_scope(\n-            {\"_class\": attr}\n-        ):  # pylint: disable=protected-access\n+        with ops.get_default_graph()._attr_scope({\"_class\": attr}):\n             with ops.name_scope(\"Initializer\"), device_context_manager(None):\n                 if not callable(initial_value):\n                     if isinstance(\n@@ -100,7 +98,7 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n         initial_value=None,\n         trainable=None,\n         collections=None,\n-        validate_shape=True,  # pylint: disable=unused-argument\n+        validate_shape=True,\n         caching_device=None,\n         name=None,\n         dtype=None,\n\n@@ -30,7 +30,6 @@ from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n \n-# pylint: disable=protected-access,missing-class-docstring\n class Optimizer(optimizer_lib._BaseOptimizer):\n     \"\"\"DTensor specific optimizers.\n \n\n@@ -42,7 +42,7 @@ class DTensorBaseTest(tf.test.TestCase, parameterized.TestCase):\n         reset_dtensor()\n \n     @staticmethod\n-    def configTestMesh(device_type_mesh_map):  # pylint: disable=invalid-name\n+    def configTestMesh(device_type_mesh_map):\n         \"\"\"Configs corresponding mesh given test context.\n \n         If runs on a CPU mesh, set virtual device on CPU.\n@@ -84,7 +84,7 @@ def create_device_array(shape, device_type):\n     device_count = np.prod(shape)\n     return np.asarray(\n         [\n-            tf.DeviceSpec(  # pylint: disable=g-complex-comprehension\n+            tf.DeviceSpec(\n                 job=\"localhost/replica:0/task:0\",\n                 device_type=device_type,\n                 device_index=i,\n@@ -105,7 +105,7 @@ def create_device_ids_array(shape):\n \n \n def reset_context():\n-    context._reset_context()  # pylint: disable=protected-access\n+    context._reset_context()\n \n \n def reset_logical_devices(device_type, count):\n@@ -147,4 +147,4 @@ def reset_logical_devices(device_type, count):\n \n \n def reset_dtensor():\n-    dtensor_api._reset()  # pylint: disable=protected-access\n+    dtensor_api._reset()\n\n@@ -140,7 +140,7 @@ def inject_mesh(init_method):\n         # of __init__, since the class might need the mesh to create weights in\n         # the __init__.\n         if mesh is not None:\n-            instance._mesh = mesh  # pylint: disable=protected-access\n+            instance._mesh = mesh\n         init_method(instance, *args, **kwargs)\n \n     return tf.__internal__.decorator.make_decorator(\n\n@@ -12,9 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-bad-import-order\n+\n+\n \"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\n \n import collections\n@@ -63,11 +62,11 @@ from tensorflow.python.util.tf_export import (\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n-# pylint: disable=g-inconsistent-quotes\n+\n metrics_mod = generic_utils.LazyLoader(\n     \"metrics_mod\", globals(), \"keras.metrics\"\n )\n-# pylint: enable=g-inconsistent-quotes\n+\n \n # Prefix that is added to the TF op layer names.\n _TF_OP_LAYER_NAME_PREFIX = \"tf_op_layer_\"\n@@ -496,7 +495,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         self.built = True\n \n     @doc_controls.for_subclass_implementers\n-    def call(self, inputs, *args, **kwargs):  # pylint: disable=unused-argument\n+    def call(self, inputs, *args, **kwargs):\n         \"\"\"This is where the layer's logic lives.\n \n         The `call()` method may not create state (except in its first\n@@ -687,7 +686,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             old_getter = getter\n \n             # Wrap variable constructor to return an AutoCastVariable.\n-            def getter(*args, **kwargs):  # pylint: disable=function-redefined\n+            def getter(*args, **kwargs):\n                 variable = old_getter(*args, **kwargs)\n                 return autocast_variable.create_autocast_variable(variable)\n \n@@ -928,9 +927,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         )\n \n     @generic_utils.default\n-    def compute_mask(\n-        self, inputs, mask=None\n-    ):  # pylint: disable=unused-argument\n+    def compute_mask(self, inputs, mask=None):\n         \"\"\"Computes an output mask tensor.\n \n         Args:\n@@ -1127,9 +1124,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 if current_name_scope == \"/\":\n                     current_name_scope = self._name_scope_on_declaration\n                 with tf.name_scope(current_name_scope):\n-                    name_scope = (\n-                        self._name_scope()\n-                    )  # Avoid autoincrementing.  # pylint: disable=not-callable\n+                    name_scope = self._name_scope()  # Avoid autoincrementing.\n         else:\n             name_scope = self._name_scope()\n \n@@ -1458,7 +1453,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 return None\n             if not tf.is_tensor(loss):\n                 loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n-            loss._unconditional_loss = True  # pylint: disable=protected-access\n+            loss._unconditional_loss = True\n             return loss\n \n         losses = tf.nest.flatten(losses)\n@@ -1693,7 +1688,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         if not call_context.frozen:\n             for update in tf.nest.flatten(updates):\n                 if callable(update):\n-                    update()  # pylint: disable=not-callable\n+                    update()\n \n     def set_weights(self, weights):\n         \"\"\"Sets the weights of the layer, from NumPy arrays.\n@@ -2396,9 +2391,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 keras_tensor.keras_tensor_to_placeholder, input_masks\n             )\n \n-            with backend.name_scope(\n-                self._name_scope()\n-            ):  # pylint: disable=not-callable\n+            with backend.name_scope(self._name_scope()):\n                 with autocast_variable.enable_auto_cast_variables(\n                     self._compute_dtype_object\n                 ):\n@@ -2717,7 +2710,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         value = tf.as_dtype(value).name\n         self._set_dtype_policy(policy.Policy(value))\n \n-    def _name_scope(self):  # pylint: disable=method-hidden\n+    def _name_scope(self):\n         if not tf.__internal__.tf2.enabled():\n             return self.name\n         name_scope = self.name\n@@ -2953,7 +2946,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 # `init_scope` to avoid creating symbolic Tensors that will\n                 # later pollute any eager operations.\n                 with tf_utils.maybe_init_scope(self):\n-                    self.build(input_shapes)  # pylint:disable=not-callable\n+                    self.build(input_shapes)\n             # We must set also ensure that the layer is marked as built, and the\n             # build shape is stored since user defined build functions may not\n             # be calling `super.build()`\n@@ -3028,7 +3021,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         if existing_value not in reference_counts:\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n                 name\n-            )  # pylint: disable=bad-super-call\n+            )\n             return\n \n         reference_count = reference_counts[existing_value]\n@@ -3038,22 +3031,18 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             reference_counts[existing_value] = reference_count - 1\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n                 name\n-            )  # pylint: disable=bad-super-call\n+            )\n             return\n         else:\n             # This is the last remaining reference.\n             del reference_counts[existing_value]\n \n-        super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n-            name\n-        )  # pylint: disable=bad-super-call\n+        super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(name)\n \n         if isinstance(existing_value, Layer) or base_layer_utils.has_weights(\n             existing_value\n         ):\n-            super(\n-                tf.__internal__.tracking.AutoTrackable, self\n-            ).__setattr__(  # pylint: disable=bad-super-call\n+            super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                 \"_self_tracked_trackables\",\n                 [\n                     l\n@@ -3062,15 +3051,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 ],\n             )\n         if isinstance(existing_value, tf.Variable):\n-            super(\n-                tf.__internal__.tracking.AutoTrackable, self\n-            ).__setattr__(  # pylint: disable=bad-super-call\n+            super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                 \"_trainable_weights\",\n                 [w for w in self._trainable_weights if w is not existing_value],\n             )\n-            super(\n-                tf.__internal__.tracking.AutoTrackable, self\n-            ).__setattr__(  # pylint: disable=bad-super-call\n+            super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                 \"_non_trainable_weights\",\n                 [\n                     w\n@@ -3089,7 +3074,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             try:\n                 super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                     name, value\n-                )  # pylint: disable=bad-super-call\n+                )\n             except AttributeError:\n                 raise AttributeError(\n                     (\n@@ -3164,7 +3149,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # status quo. See the comment at __delattr__.\n         super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n             name, value\n-        )  # pylint: disable=bad-super-call\n+        )\n \n     def _gather_children_attribute(self, attribute):\n         assert attribute in {\n@@ -3575,9 +3560,7 @@ class AddMetric(Layer):\n         return config\n \n \n-def _in_functional_construction_mode(\n-    layer, inputs, args, kwargs, input_list\n-):  # pylint: disable=unused-argument\n+def _in_functional_construction_mode(layer, inputs, args, kwargs, input_list):\n     \"\"\"Check the arguments to see if we are constructing a functional model.\"\"\"\n     # We are constructing a functional model if any of the inputs\n     # are KerasTensors\n\n@@ -17,8 +17,6 @@ import copy\n import os\n \n import numpy as np\n-\n-# pylint: disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n@@ -129,9 +127,7 @@ class BaseLayerTest(test_combinations.TestCase):\n \n     def test_manual_compute_output_shape(self):\n         class BuildCounter(base_layer.Layer):\n-            def __init__(\n-                self, *args, **kwargs\n-            ):  # pylint: disable=redefined-outer-name\n+            def __init__(self, *args, **kwargs):\n                 super().__init__(*args, **kwargs)\n                 self.build_counter = 0\n \n@@ -679,9 +675,7 @@ class BaseLayerTest(test_combinations.TestCase):\n         )\n \n         class MyLayerNew2(base_layer.Layer):\n-            def __init__(\n-                self, name=\"MyLayerName\", dtype=None, **kwargs\n-            ):  # pylint:disable=redefined-outer-name\n+            def __init__(self, name=\"MyLayerName\", dtype=None, **kwargs):\n                 super().__init__(name=name, dtype=dtype, **kwargs)\n \n         # Check that if the kwargs in `__init__` are base layer constructor\n@@ -922,13 +916,11 @@ class BaseLayerTest(test_combinations.TestCase):\n \n     def _test_custom_layer_training_arg(\n         self,\n-        # pylint: disable=invalid-name\n         CustomLayerNoTrainingArg,\n         CustomLayerDefaultTrainingMissing,\n         CustomLayerDefaultTrainingNone,\n         CustomLayerDefaultTrainingFalse,\n         CustomLayerDefaultTrainingTrue,\n-        # pylint: enable=invalid-name\n     ):\n         x = tf.ones(shape=(1, 1))\n \n@@ -1133,9 +1125,7 @@ class SymbolicSupportTest(test_combinations.TestCase):\n         try:\n             _ = TypeErrorLayer()(inputs)\n         except TypeError as e:\n-            self.assertIn(\n-                \"easily_identifiable_name\", str(e)\n-            )  # pylint: disable=g-assert-in-except\n+            self.assertIn(\"easily_identifiable_name\", str(e))\n \n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n\n@@ -54,7 +54,7 @@ def make_variable(\n     collections=None,\n     synchronization=tf.VariableSynchronization.AUTO,\n     aggregation=tf.VariableAggregation.NONE,\n-    partitioner=None,  # pylint: disable=unused-argument\n+    partitioner=None,\n     layout=None,\n ):\n     \"\"\"Temporary util to create a variable (relies on `variable_scope.variable`).\n@@ -449,7 +449,7 @@ def mark_checked(tensors):\n     \"\"\"\n \n     def _mark_checked(tensor):\n-        tensor._keras_history_checked = True  # pylint: disable=protected-access\n+        tensor._keras_history_checked = True\n \n     tf.nest.map_structure(_mark_checked, tensors)\n \n@@ -751,7 +751,6 @@ def mark_as_return(outputs, acd):\n         if not tf.is_tensor(tensor):\n             return tensor\n \n-        # pylint: disable=protected-access\n         return_tensor = acd.mark_as_return(tensor)\n         if getattr(tensor, \"_keras_mask\", None) is not None:\n             return_tensor._keras_mask = acd.mark_as_return(tensor._keras_mask)\n@@ -764,7 +763,6 @@ def mark_as_return(outputs, acd):\n             return_tensor._tfp_distribution = tensor._tfp_distribution\n \n         return return_tensor\n-        # pylint: enable=protected-access\n \n     return tf.nest.map_structure(_mark_as_return, outputs)\n \n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-bad-import-order\n+\n+\n \"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\n \n import functools\n@@ -49,7 +49,6 @@ from tensorflow.python.platform import tf_logging\n from tensorflow.tools.docs import doc_controls\n \n \n-# pylint: disable=g-classes-have-attributes\n class Layer(base_layer.Layer):\n     \"\"\"Base layer class.\n \n@@ -273,7 +272,7 @@ class Layer(base_layer.Layer):\n         self.built = True\n \n     @doc_controls.for_subclass_implementers\n-    def call(self, inputs, **kwargs):  # pylint: disable=unused-argument\n+    def call(self, inputs, **kwargs):\n         \"\"\"This is where the layer's logic lives.\n \n         Args:\n@@ -443,7 +442,7 @@ class Layer(base_layer.Layer):\n             # Wrap 'getter' with a version that returns an AutoCastVariable.\n             old_getter = getter\n \n-            def getter(*args, **kwargs):  # pylint: disable=function-redefined\n+            def getter(*args, **kwargs):\n                 variable = old_getter(*args, **kwargs)\n                 return autocast_variable.create_autocast_variable(variable)\n \n@@ -649,9 +648,7 @@ class Layer(base_layer.Layer):\n         )\n \n     @generic_utils.default\n-    def compute_mask(\n-        self, inputs, mask=None\n-    ):  # pylint: disable=unused-argument\n+    def compute_mask(self, inputs, mask=None):\n         \"\"\"Computes an output mask tensor.\n \n         Args:\n@@ -811,9 +808,7 @@ class Layer(base_layer.Layer):\n                     self.input_spec, inputs, self.name\n                 )\n                 graph = backend.get_graph()\n-                with graph.as_default(), backend.name_scope(\n-                    self._name_scope()\n-                ):  # pylint: disable=not-callable\n+                with graph.as_default(), backend.name_scope(self._name_scope()):\n                     # Build layer if applicable (if the `build` method has been\n                     # overridden).\n                     self._maybe_build(inputs)\n@@ -894,9 +889,7 @@ class Layer(base_layer.Layer):\n                         self._set_inputs(inputs, outputs)\n             else:\n                 # Eager execution on data tensors.\n-                with backend.name_scope(\n-                    self._name_scope()\n-                ):  # pylint: disable=not-callable\n+                with backend.name_scope(self._name_scope()):\n                     self._maybe_build(inputs)\n                     cast_inputs = self._maybe_cast_inputs(inputs)\n                     with autocast_variable.enable_auto_cast_variables(\n@@ -1123,9 +1116,7 @@ class Layer(base_layer.Layer):\n                 return None\n             if not tf.is_tensor(loss):\n                 loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n-            loss._unconditional_loss = (\n-                inputs is None\n-            )  # pylint: disable=protected-access\n+            loss._unconditional_loss = inputs is None\n             return loss\n \n         losses = tf.nest.flatten(losses)\n@@ -1960,7 +1951,7 @@ class Layer(base_layer.Layer):\n         value = tf.as_dtype(value).name\n         self._set_dtype_policy(policy.Policy(value))\n \n-    def _name_scope(self):  # pylint: disable=method-hidden\n+    def _name_scope(self):\n         return self.name\n \n     def _init_set_name(self, name, zero_based=True):\n@@ -2274,7 +2265,7 @@ class Layer(base_layer.Layer):\n         if existing_value not in reference_counts:\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n                 name\n-            )  # pylint: disable=bad-super-call\n+            )\n             return\n \n         reference_count = reference_counts[existing_value]\n@@ -2284,22 +2275,18 @@ class Layer(base_layer.Layer):\n             reference_counts[existing_value] = reference_count - 1\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n                 name\n-            )  # pylint: disable=bad-super-call\n+            )\n             return\n         else:\n             # This is the last remaining reference.\n             del reference_counts[existing_value]\n \n-        super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n-            name\n-        )  # pylint: disable=bad-super-call\n+        super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(name)\n \n         if isinstance(existing_value, Layer) or base_layer_utils.has_weights(\n             existing_value\n         ):\n-            super(\n-                tf.__internal__.tracking.AutoTrackable, self\n-            ).__setattr__(  # pylint: disable=bad-super-call\n+            super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                 \"_self_tracked_trackables\",\n                 [\n                     l\n@@ -2308,15 +2295,11 @@ class Layer(base_layer.Layer):\n                 ],\n             )\n         if isinstance(existing_value, tf.Variable):\n-            super(\n-                tf.__internal__.tracking.AutoTrackable, self\n-            ).__setattr__(  # pylint: disable=bad-super-call\n+            super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                 \"_trainable_weights\",\n                 [w for w in self._trainable_weights if w is not existing_value],\n             )\n-            super(\n-                tf.__internal__.tracking.AutoTrackable, self\n-            ).__setattr__(  # pylint: disable=bad-super-call\n+            super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                 \"_non_trainable_weights\",\n                 [\n                     w\n@@ -2335,7 +2318,7 @@ class Layer(base_layer.Layer):\n             try:\n                 super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n                     name, value\n-                )  # pylint: disable=bad-super-call\n+                )\n             except AttributeError:\n                 raise AttributeError(\n                     (\n@@ -2415,7 +2398,7 @@ class Layer(base_layer.Layer):\n         # status quo. See the comment at __delattr__.\n         super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n             name, value\n-        )  # pylint: disable=bad-super-call\n+        )\n \n     # This is a hack so that the is_layer (within\n     # training/trackable/layer_utils.py) check doesn't get the weights attr.\n\n@@ -79,7 +79,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n         raise NotImplementedError\n \n     @doc_controls.do_not_generate_docs\n-    def reset_state(self):  # pylint: disable=method-hidden\n+    def reset_state(self):\n         \"\"\"Resets the statistics of the preprocessing layer.\"\"\"\n         raise NotImplementedError\n \n@@ -238,9 +238,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n         \"\"\"\n         _disallow_inside_tf_function(\"adapt\")\n         if not version_utils.should_use_v2():\n-            raise RuntimeError(\n-                \"`adapt` is only supported in tensorflow v2.\"\n-            )  # pylint: disable=g-doc-exception\n+            raise RuntimeError(\"`adapt` is only supported in tensorflow v2.\")\n         if not self._is_compiled:\n             self.compile()  # Compile with defaults.\n         if self.built:\n\n@@ -35,7 +35,7 @@ class AddingPreprocessingLayer(base_preprocessing_layer.PreprocessingLayer):\n     def update_state(self, data):\n         self.sum.assign_add(tf.reduce_sum(tf.cast(data, tf.float32)))\n \n-    def reset_state(self):  # pylint: disable=method-hidden\n+    def reset_state(self):\n         self.sum.assign(0.0)\n \n     def set_total(self, sum_value):\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Utilities for `Model.compile`.\"\"\"\n \n \n@@ -356,9 +356,7 @@ class LossesContainer(Container):\n             if loss_name is None:\n                 raise ValueError(f\"Loss should be a callable, received: {loss}\")\n             loss = losses_mod.LossFunctionWrapper(loss, name=loss_name)\n-        loss._allow_sum_over_batch_size = (\n-            True  # pylint: disable=protected-access\n-        )\n+        loss._allow_sum_over_batch_size = True\n         return loss\n \n     def _should_broadcast(self, obj):\n@@ -518,7 +516,7 @@ class MetricsContainer(Container):\n         # For multi-output models, prepend the output name to the metric name.\n         # For weighted metrics, prepend \"weighted_\" if the name would be\n         # non-unique.\n-        # pylint: disable=protected-access\n+\n         metric_names = set()\n         is_multi_output = len(self._output_names) > 1\n         zip_args = (self._output_names, self._metrics, self._weighted_metrics)\n@@ -556,7 +554,6 @@ class MetricsContainer(Container):\n                         \"to have unique names.\"\n                     )\n                 metric_names.add(wm._name)\n-        # pylint: enable=protected-access\n \n     def _create_ordered_metrics(self):\n         \"\"\"Cache the flat order needed when return metrics, for backcompat.\"\"\"\n@@ -678,9 +675,7 @@ class MetricsContainer(Container):\n                     metric_obj = metrics_mod.categorical_crossentropy\n \n         if isinstance(metric_obj, losses_mod.Loss):\n-            metric_obj._allow_sum_over_batch_size = (\n-                True  # pylint: disable=protected-access\n-            )\n+            metric_obj._allow_sum_over_batch_size = True\n \n         if not isinstance(metric_obj, metrics_mod.Metric):\n             if isinstance(metric, str):\n\n@@ -40,7 +40,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n try:\n-    import pandas as pd  # pylint: disable=g-import-not-at-top\n+    import pandas as pd\n except ImportError:\n     pd = None\n \n@@ -888,9 +888,7 @@ class GeneratorDataAdapter(DataAdapter):\n \n         def _get_tensor_spec(t):\n             # TODO(b/226395276): Remove _with_tensor_ranks_only usage.\n-            return type_spec.type_spec_from_value(\n-                t\n-            )._with_tensor_ranks_only()  # pylint: disable=protected-access\n+            return type_spec.type_spec_from_value(t)._with_tensor_ranks_only()\n \n         output_signature = tf.nest.map_structure(_get_tensor_spec, peek)\n \n@@ -1857,7 +1855,7 @@ def _get_tensor_types():\n \n def _is_scipy_sparse(x):\n     try:\n-        from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n+        from scipy.sparse import issparse\n \n         return issparse(x)\n     except ImportError:\n\n@@ -272,7 +272,7 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n \n     def test_can_handle_pandas(self):\n         try:\n-            import pandas as pd  # pylint: disable=g-import-not-at-top\n+            import pandas as pd\n         except ImportError:\n             self.skipTest(\"Skipping test because pandas is not installed.\")\n         self.assertTrue(\n@@ -291,7 +291,7 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_training_pandas(self):\n         try:\n-            import pandas as pd  # pylint: disable=g-import-not-at-top\n+            import pandas as pd\n         except ImportError:\n             self.skipTest(\"Skipping test because pandas is not installed.\")\n         input_a = keras.Input(shape=(3,), name=\"input_a\")\n\n@@ -25,7 +25,7 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\"\"\"\n \n \n@@ -44,7 +44,6 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.tools.docs import doc_controls\n \n \n-# pylint: disable=g-classes-have-attributes\n class Functional(training_lib.Model):\n     \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.\n \n@@ -243,7 +242,7 @@ class Functional(training_lib.Model):\n                 layer,\n                 node_index,\n                 tensor_index,\n-            ) = x._keras_history  # pylint: disable=protected-access\n+            ) = x._keras_history\n             self._output_layers.append(layer)\n             self._output_coordinates.append((layer, node_index, tensor_index))\n \n@@ -253,7 +252,7 @@ class Functional(training_lib.Model):\n                 layer,\n                 node_index,\n                 tensor_index,\n-            ) = x._keras_history  # pylint: disable=protected-access\n+            ) = x._keras_history\n             # It's supposed to be an input layer, so only one node\n             # and one tensor output.\n             assert node_index == 0\n@@ -586,9 +585,7 @@ class Functional(training_lib.Model):\n                         layer_output_shapes, to_tuples=False\n                     )\n \n-                    node_index = layer._inbound_nodes.index(\n-                        node\n-                    )  # pylint: disable=protected-access\n+                    node_index = layer._inbound_nodes.index(node)\n                     for j, shape in enumerate(\n                         tf.nest.flatten(layer_output_shapes)\n                     ):\n@@ -802,7 +799,7 @@ class Functional(training_lib.Model):\n                     f\"Received inputs={x} (missing previous layer metadata).\"\n                 )\n             # Check that x is an input tensor.\n-            # pylint: disable=protected-access\n+\n             layer = x._keras_history.layer\n             if len(layer._inbound_nodes) > 1 or (\n                 layer._inbound_nodes and not layer._inbound_nodes[0].is_input\n@@ -1178,8 +1175,8 @@ def _build_map_helper(\n         layer,\n         node_index,\n         _,\n-    ) = tensor._keras_history  # pylint: disable=protected-access\n-    node = layer._inbound_nodes[node_index]  # pylint: disable=protected-access\n+    ) = tensor._keras_history\n+    node = layer._inbound_nodes[node_index]\n \n     # Don't repeat work for shared subgraphs\n     if node in finished_nodes:\n\n@@ -228,7 +228,7 @@ class NetworkConstructionTest(test_combinations.TestCase):\n \n         x = input_layer_lib.Input(shape=(32,))\n         test_layer = PowersLayer()\n-        p1, p2 = test_layer(x)  # pylint: disable=not-callable\n+        p1, p2 = test_layer(x)\n \n         self.assertIs(test_layer.input, x)\n         self._assertAllIs(test_layer.output, [p1, p2])\n@@ -247,7 +247,7 @@ class NetworkConstructionTest(test_combinations.TestCase):\n         a = input_layer_lib.Input(shape=(32,))\n         b = input_layer_lib.Input(shape=(32,))\n         test_layer = AddLayer()\n-        y = test_layer([a, b])  # pylint: disable=not-callable\n+        y = test_layer([a, b])\n \n         self._assertAllIs(test_layer.input, [a, b])\n         self.assertIs(test_layer.output, y)\n@@ -806,7 +806,7 @@ class NetworkConstructionTest(test_combinations.TestCase):\n             self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))\n         else:\n             x = input_layer_lib.Input(shape=(32,))\n-            y = MaskedLayer()(x)  # pylint: disable=not-callable\n+            y = MaskedLayer()(x)\n             network = functional.Functional(x, y)\n \n             # test callability on Input\n@@ -1591,7 +1591,7 @@ class DeferredModeTest(test_combinations.TestCase):\n             def call(self, inputs):\n                 return inputs[0] + inputs[1]\n \n-        c = AddLayer()([a, input_b])  # pylint: disable=not-callable\n+        c = AddLayer()([a, input_b])\n         c = layers.Dense(2)(c)\n \n         network = functional.Functional([input_a, input_b], [a, c])\n@@ -1730,7 +1730,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n                 self.block = BasicBlock()\n \n             def call(self, x):\n-                x = self.block(x)  # pylint: disable=not-callable\n+                x = self.block(x)\n                 return x\n \n         model = CompoundModel()\n@@ -1741,7 +1741,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n             \"Model should have its weights created as it \" \"has been built\",\n         )\n         sample_input = tf.ones((1, 10, 10, 1))\n-        output = model(sample_input)  # pylint: disable=not-callable\n+        output = model(sample_input)\n         self.assertEqual(output.shape, (1, 3))\n \n     @test_combinations.generate(\n\n@@ -183,9 +183,7 @@ def clone_graph_nodes(inputs, outputs):\n         # It is used in the Node constructor to check if the tensor\n         # \"is_keras_tensor()\" The history will be override by the Node\n         # constructor anyway for the corresponding layer output anyway.\n-        cpy._keras_history = (\n-            kt_output._keras_history\n-        )  # pylint: disable=protected-access\n+        cpy._keras_history = kt_output._keras_history\n         cloned_outputs.append(cpy)\n         kt_id_mapping[id(kt_output)] = cpy\n     cloned_outputs = tf.nest.pack_sequence_as(outputs, cloned_outputs)\n@@ -235,9 +233,7 @@ def clone_keras_tensors(args, keras_tensor_mapping):\n             else:\n                 # Create copy of keras_tensor if we haven't done it before\n                 cpy = _clone_keras_tensor(obj)\n-                cpy._keras_history = (\n-                    obj._keras_history\n-                )  # pylint: disable=protected-access\n+                cpy._keras_history = obj._keras_history\n                 keras_tensor_mapping[id(obj)] = cpy\n             result.append(cpy)\n         else:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Input layer code (`Input` and `InputLayer`).\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -259,9 +259,7 @@ class InputLayer(base_layer.Layer):\n         if isinstance(input_tensor, keras_tensor.KerasTensor) or (\n             tf_utils.is_extension_type(input_tensor)\n         ):\n-            self._type_spec = (\n-                input_tensor._type_spec\n-            )  # pylint: disable=protected-access\n+            self._type_spec = input_tensor._type_spec\n         else:\n             self._type_spec = tf.TensorSpec(\n                 shape=input_tensor.shape,\n@@ -289,7 +287,7 @@ class InputLayer(base_layer.Layer):\n \n @keras_export(\"keras.Input\", \"keras.layers.Input\")\n @traceback_utils.filter_traceback\n-def Input(  # pylint: disable=invalid-name\n+def Input(\n     shape=None,\n     batch_size=None,\n     name=None,\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-classes-have-attributes\n+\n+\n \"\"\"Contains the InputSpec class.\"\"\"\n \n import tensorflow.compat.v2 as tf\n\n@@ -21,8 +21,6 @@ from keras.utils import object_identity\n # isort: off\n from tensorflow.python.data.util import structure\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n # Tensorflow tensors have a maximum rank of 254\n # (See `MaxDimensions()` in //tensorflow/core/framework/tensor_shape.h )\n@@ -422,9 +420,7 @@ class KerasTensor:\n         return self._name\n \n     @classmethod\n-    def _overload_all_operators(\n-        cls, tensor_class\n-    ):  # pylint: disable=invalid-name\n+    def _overload_all_operators(cls, tensor_class):\n         \"\"\"Register overloads for all operators.\"\"\"\n         for operator in tf.Tensor.OVERLOADABLE_OPERATORS:\n             cls._overload_operator(tensor_class, operator)\n@@ -435,9 +431,7 @@ class KerasTensor:\n             cls._overload_operator(tensor_class, \"experimental_ref\")\n \n     @classmethod\n-    def _overload_operator(\n-        cls, tensor_class, operator\n-    ):  # pylint: disable=invalid-name\n+    def _overload_operator(cls, tensor_class, operator):\n         \"\"\"Overload an operator with the same implementation as a base Tensor class.\n \n         We pull the operator out of the class dynamically to avoid ordering\n@@ -457,9 +451,7 @@ class KerasTensor:\n         setattr(cls, operator, tensor_oper)\n \n \n-KerasTensor._overload_all_operators(\n-    tf.Tensor\n-)  # pylint: disable=protected-access\n+KerasTensor._overload_all_operators(tf.Tensor)\n \n \n class SparseKerasTensor(KerasTensor):\n@@ -540,23 +532,13 @@ class RaggedKerasTensor(KerasTensor):\n \n \n # Overload slicing\n-RaggedKerasTensor._overload_operator(\n-    tf.RaggedTensor, \"__getitem__\"\n-)  # pylint: disable=protected-access\n+RaggedKerasTensor._overload_operator(tf.RaggedTensor, \"__getitem__\")\n \n # Overload math ops\n-RaggedKerasTensor._overload_operator(\n-    tf.RaggedTensor, \"__add__\"\n-)  # pylint: disable=protected-access\n-RaggedKerasTensor._overload_operator(\n-    tf.RaggedTensor, \"__radd__\"\n-)  # pylint: disable=protected-access\n-RaggedKerasTensor._overload_operator(\n-    tf.RaggedTensor, \"__mul__\"\n-)  # pylint: disable=protected-access\n-RaggedKerasTensor._overload_operator(\n-    tf.RaggedTensor, \"__rmul__\"\n-)  # pylint: disable=protected-access\n+RaggedKerasTensor._overload_operator(tf.RaggedTensor, \"__add__\")\n+RaggedKerasTensor._overload_operator(tf.RaggedTensor, \"__radd__\")\n+RaggedKerasTensor._overload_operator(tf.RaggedTensor, \"__mul__\")\n+RaggedKerasTensor._overload_operator(tf.RaggedTensor, \"__rmul__\")\n \n \n # TODO(b/161487382):\n@@ -666,7 +648,7 @@ def register_keras_tensor_specialization(cls, keras_tensor_subclass):\n def keras_tensor_to_placeholder(x):\n     \"\"\"Construct a graph placeholder to represent a KerasTensor when tracing.\"\"\"\n     if isinstance(x, KerasTensor):\n-        return x._to_placeholder()  # pylint: disable=protected-access\n+        return x._to_placeholder()\n     else:\n         return x\n \n@@ -684,9 +666,7 @@ def keras_tensor_from_tensor(tensor):\n     out = keras_tensor_cls.from_tensor(tensor)\n \n     if hasattr(tensor, \"_keras_mask\"):\n-        out._keras_mask = keras_tensor_from_tensor(\n-            tensor._keras_mask\n-        )  # pylint: disable=protected-access\n+        out._keras_mask = keras_tensor_from_tensor(tensor._keras_mask)\n     return out\n \n \n@@ -707,7 +687,7 @@ def keras_tensor_from_type_spec(type_spec, name=None):\n def type_spec_with_shape(spec, shape):\n     \"\"\"Returns a copy of TypeSpec `spec` with its shape set to `shape`.\"\"\"\n     if isinstance(spec, tf.TensorSpec):\n-        # pylint: disable=protected-access\n+\n         # TODO(b/203201161) Figure out why mutation is needed here, and remove\n         # it. (TensorSpec objects should be immutable; and we should not be\n         # modifying private fields.)\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"InputSpec tests.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n@@ -235,7 +235,7 @@ class KerasTensorTest(test_combinations.TestCase):\n             AttributeError,\n             \"KerasTensor wraps TypeSpec .* which does not have a dtype.\",\n         ):\n-            kt.dtype  # pylint: disable=pointless-statement\n+            kt.dtype\n \n     def test_wrong_dtype_type_error(self):\n         spec = CustomTypeSpec(None, tf.int32)\n@@ -245,7 +245,7 @@ class KerasTensorTest(test_combinations.TestCase):\n             TypeError,\n             \"KerasTensor requires that wrapped TypeSpec's dtype is a DType; .*\",\n         ):\n-            kt.dtype  # pylint: disable=pointless-statement\n+            kt.dtype\n \n \n if __name__ == \"__main__\":\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-classes-have-attributes\n+\n+\n \"\"\"Contains the `Node` class.\"\"\"\n \n import collections\n\n@@ -19,8 +19,6 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n \n-# pylint: disable=protected-access\n-\n \n class PartialBatchPaddingHandler:\n     \"\"\"A container that holds info about partial batches for `predict()`.\"\"\"\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Model saving utilities.\n \n Everything has been moved to keras/saving/. This file will be deleted soon.\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Home of the `Sequential` model.\"\"\"\n \n import copy\n@@ -111,11 +111,7 @@ class Sequential(functional.Functional):\n         \"\"\"\n         # Skip the init in FunctionalModel since model doesn't have input/output\n         # yet\n-        super(\n-            functional.Functional, self\n-        ).__init__(  # pylint: disable=bad-super-call\n-            name=name, autocast=False\n-        )\n+        super(functional.Functional, self).__init__(name=name, autocast=False)\n         base_layer.keras_api_gauge.get_cell(\"Sequential\").set(True)\n         self.supports_masking = True\n         self._compute_output_and_mask_jointly = True\n@@ -385,9 +381,7 @@ class Sequential(functional.Functional):\n                 super().build(input_shape)\n         self.built = True\n \n-    def call(\n-        self, inputs, training=None, mask=None\n-    ):  # pylint: disable=redefined-outer-name\n+    def call(self, inputs, training=None, mask=None):\n         # If applicable, update the static input shape of the model.\n         if not self._has_explicit_input_shape:\n             if not tf.is_tensor(inputs) and not isinstance(inputs, tf.Tensor):\n@@ -447,9 +441,7 @@ class Sequential(functional.Functional):\n         # TODO(omalleyt): b/123540974 This function is not really safe to call\n         # by itself because it will duplicate any updates and losses in graph\n         # mode by `call`ing the Layers again.\n-        outputs = self.call(\n-            inputs, mask=mask\n-        )  # pylint: disable=unexpected-keyword-arg\n+        outputs = self.call(inputs, mask=mask)\n         return getattr(outputs, \"_keras_mask\", None)\n \n     def get_config(self):\n@@ -516,9 +508,7 @@ class Sequential(functional.Functional):\n             return\n         # When the graph has not been initialized, use the Model's\n         # implementation to to check if the weights has been created.\n-        super(\n-            functional.Functional, self\n-        )._assert_weights_created()  # pylint: disable=bad-super-call\n+        super(functional.Functional, self)._assert_weights_created()\n \n \n def _get_shape_tuple(t):\n\n@@ -181,7 +181,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             ),\n             base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES,\n         )\n-    )  # pylint: disable=protected-access\n+    )\n     _SCALAR_UPRANKING_ON = False\n \n     def __new__(cls, *args, **kwargs):\n@@ -836,7 +836,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 metrics += self.compiled_metrics.metrics\n \n         for l in self._flatten_layers():\n-            metrics.extend(l._metrics)  # pylint: disable=protected-access\n+            metrics.extend(l._metrics)\n         return metrics\n \n     @property\n@@ -898,9 +898,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Returns:\n           Boolean, whether the model should run eagerly.\n         \"\"\"\n-        if (\n-            self.dynamic and self._run_eagerly == False\n-        ):  # pylint:disable=g-bool-id-comparison\n+        if self.dynamic and self._run_eagerly == False:\n             # TODO(fchollet): consider using py_func to enable this.\n             raise ValueError(\n                 \"Your model contains layers that can only be \"\n@@ -1137,9 +1135,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 outputs = model.train_step(data)\n                 # Ensure counter is updated only if `train_step` succeeds.\n                 with tf.control_dependencies(_minimum_control_deps(outputs)):\n-                    model._train_counter.assign_add(\n-                        1\n-                    )  # pylint: disable=protected-access\n+                    model._train_counter.assign_add(1)\n                 return outputs\n \n             if self._jit_compile:\n@@ -1491,9 +1487,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 val_sample_weight,\n             ) = data_adapter.unpack_x_y_sample_weight(validation_data)\n \n-        if (\n-            self.distribute_strategy._should_use_with_coordinator\n-        ):  # pylint: disable=protected-access\n+        if self.distribute_strategy._should_use_with_coordinator:\n             self._cluster_coordinator = (\n                 tf.distribute.experimental.coordinator.ClusterCoordinator(\n                     self.distribute_strategy\n@@ -1557,7 +1551,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 with data_handler.catch_stop_iteration():\n                     data_handler._initial_step = data_handler._initial_step or (\n                         self._maybe_load_initial_step_from_ckpt()\n-                    )  # pylint: disable=protected-access\n+                    )\n                     for step in data_handler.steps():\n                         with tf.profiler.experimental.Trace(\n                             \"train\",\n@@ -1707,9 +1701,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 outputs = model.test_step(data)\n                 # Ensure counter is updated only if `test_step` succeeds.\n                 with tf.control_dependencies(_minimum_control_deps(outputs)):\n-                    model._test_counter.assign_add(\n-                        1\n-                    )  # pylint: disable=protected-access\n+                    model._test_counter.assign_add(1)\n                 return outputs\n \n             if self._jit_compile:\n@@ -1895,9 +1887,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if kwargs:\n             raise TypeError(f\"Invalid keyword arguments: {list(kwargs.keys())}\")\n \n-        if (\n-            self.distribute_strategy._should_use_with_coordinator\n-        ):  # pylint: disable=protected-access\n+        if self.distribute_strategy._should_use_with_coordinator:\n             self._cluster_coordinator = (\n                 tf.distribute.experimental.coordinator.ClusterCoordinator(\n                     self.distribute_strategy\n@@ -2025,9 +2015,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 outputs = model.predict_step(data)\n                 # Ensure counter is updated only if `test_step` succeeds.\n                 with tf.control_dependencies(_minimum_control_deps(outputs)):\n-                    model._predict_counter.assign_add(\n-                        1\n-                    )  # pylint: disable=protected-access\n+                    model._predict_counter.assign_add(1)\n                 return outputs\n \n             if self._jit_compile:\n@@ -2194,9 +2182,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # prediction.  If running under PSS, then swap it with OneDeviceStrategy\n         # so that execution will run on the coordinator.\n         original_pss_strategy = None\n-        if (\n-            self.distribute_strategy._should_use_with_coordinator\n-        ):  # pylint: disable=protected-access\n+        if self.distribute_strategy._should_use_with_coordinator:\n             original_pss_strategy = self.distribute_strategy\n             self._distribution_strategy = None\n \n@@ -2665,7 +2651,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         options=None,\n         save_traces=True,\n     ):\n-        # pylint: disable=line-too-long\n+\n         \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\n \n         Please see `tf.keras.models.save_model` or the\n@@ -2708,7 +2694,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         model = load_model('my_model.h5')\n         ```\n         \"\"\"\n-        # pylint: enable=line-too-long\n+\n         save.save_model(\n             self,\n             filepath,\n@@ -3003,7 +2989,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # as a result.\n         config = {}\n \n-        if saving_lib._ENABLED:  # pylint: disable=protected-access\n+        if saving_lib._ENABLED:\n             if self.optimizer:\n                 config[\"optimizer\"] = saving_lib.serialize_keras_object(\n                     self.optimizer\n@@ -3073,7 +3059,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     f\"Error encountered during deserialization:\\n{e}\"\n                 )\n \n-            if saving_lib._ENABLED:  # pylint: disable=protected-access\n+            if saving_lib._ENABLED:\n \n                 if optimizer or loss:\n                     model.compile(optimizer=optimizer, loss=loss)\n@@ -3560,7 +3546,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             and len(x.element_spec) == 3\n         )\n \n-        # pylint: disable=protected-access\n         if (\n             sample_weight_present\n             and self.compiled_metrics._user_weighted_metrics is None\n@@ -3632,7 +3617,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           Dictionary of arguments that were used when compiling the model.\n         \"\"\"\n         self._assert_compile_was_called()\n-        # pylint: disable=protected-access\n \n         saved_metrics = self.compiled_metrics._user_metrics\n         saved_weighted_metrics = self.compiled_metrics._user_weighted_metrics\n@@ -3650,16 +3634,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             \"weighted_metrics\": saved_weighted_metrics,\n             \"loss_weights\": self.compiled_loss._user_loss_weights,\n         }\n-        # pylint: enable=protected-access\n+\n         return compile_args\n \n     def _get_callback_model(self):\n         return self\n \n     def _in_multi_worker_mode(self):\n-        return (\n-            self.distribute_strategy.extended._in_multi_worker_mode()\n-        )  # pylint: disable=protected-access\n+        return self.distribute_strategy.extended._in_multi_worker_mode()\n \n     @property\n     def _compile_was_called(self):\n@@ -3757,9 +3739,7 @@ def potentially_ragged_concat(tensors):\n \n def _get_verbosity(verbose, distribute_strategy):\n     \"\"\"Find the right verbosity value for 'auto'.\"\"\"\n-    if (\n-        verbose == 1 and distribute_strategy._should_use_with_coordinator\n-    ):  # pylint: disable=protected-access\n+    if verbose == 1 and distribute_strategy._should_use_with_coordinator:\n         raise ValueError(\n             \"`verbose=1` is not allowed with `ParameterServerStrategy` for \"\n             f\"performance reasons. Received: verbose={verbose}\"\n@@ -3931,7 +3911,7 @@ def disable_multi_worker(method):\n     \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"\n \n     def _method_wrapper(self, *args, **kwargs):\n-        if self._in_multi_worker_mode():  # pylint: disable=protected-access\n+        if self._in_multi_worker_mode():\n             raise ValueError(\n                 f\"{method.__name__} is not supported in multi-worker \"\n                 \"mode. Please use a non-multi-worker \"\n\n@@ -31,11 +31,9 @@ from keras.utils.mode_keys import ModeKeys\n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n \n-# pylint: disable=protected-access\n-\n \n try:\n-    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n+    from scipy.sparse import issparse\n except ImportError:\n     issparse = None\n \n\n@@ -31,8 +31,6 @@ from keras.utils.mode_keys import ModeKeys\n from tensorflow.python.distribute import input_lib\n from tensorflow.python.platform import tf_logging as logging\n \n-# pylint: disable=protected-access\n-\n \n def _per_replica_execution_function(model, mode):\n     exec_func = model._make_execution_function(mode)\n\n@@ -27,8 +27,6 @@ from keras.utils import losses_utils\n from tensorflow.python.eager.backprop import GradientTape\n from tensorflow.python.platform import tf_logging as logging\n \n-# pylint: disable=protected-access\n-\n \n def _eager_loss_fn(outputs, targets, loss_fn, output_name):\n     with backend.name_scope(output_name + \"_loss\"):\n\n@@ -32,8 +32,6 @@ from keras.utils.mode_keys import ModeKeys\n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n \n-# pylint: disable=protected-access\n-\n \n def model_iteration(\n     model,\n@@ -590,9 +588,7 @@ def _make_execution_function(model, mode, class_weight=None):\n     else:\n         # Match signature of other modes to allow\n         # 1, 2, or 3-tuples from generator\n-        def predict_on_batch(\n-            x, y=None, sample_weights=None\n-        ):  # pylint: disable=unused-argument\n+        def predict_on_batch(x, y=None, sample_weights=None):\n             return model.predict_on_batch(x)\n \n         f = predict_on_batch\n\n@@ -94,7 +94,7 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n                 labels_channels_first = [\n                     np.array(\n                         [[[[0, 1, 3], [2, 1, 0], [2, 2, 1]]]], dtype=np.float32\n-                    ),  # pylint: disable=line-too-long\n+                    ),\n                     np.array(\n                         [\n                             [\n@@ -105,7 +105,7 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n                             ]\n                         ],\n                         dtype=np.float32,\n-                    ),  # pylint: disable=line-too-long\n+                    ),\n                     np.array(\n                         [\n                             [\n@@ -115,7 +115,7 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n                         ],\n                         dtype=np.float32,\n                     ),\n-                ]  # pylint: disable=line-too-long\n+                ]\n                 # Compute one loss for each loss function in the list\n                 # `losses_to_test`:\n                 loss_channels_last = [0.0, 0.0, 0.0]\n\n@@ -143,9 +143,7 @@ def _gather_test_cases():\n         arg_dict,\n         filter_fn,\n     ) in _LAYERS_TO_TEST:\n-        arg_combinations = [\n-            [(k, i) for i in v] for k, v in arg_dict.items()\n-        ]  # pylint: disable=g-complex-comprehension\n+        arg_combinations = [[(k, i) for i in v] for k, v in arg_dict.items()]\n         for arguments in itertools.product(*arg_combinations):\n             layer_kwargs = {k: v for k, v in arguments}\n             if filter_fn is not None and not filter_fn(**layer_kwargs):\n\n@@ -54,7 +54,7 @@ from tensorflow.python.training.rmsprop import (\n )\n \n try:\n-    import scipy.sparse as scipy_sparse  # pylint: disable=g-import-not-at-top\n+    import scipy.sparse as scipy_sparse\n except ImportError:\n     scipy_sparse = None\n \n@@ -1762,9 +1762,7 @@ class TrainingTest(test_combinations.TestCase):\n \n             _HAS_AGGREGATE_GRAD = False\n \n-            def apply_gradients(\n-                self, grads_and_vars, name=None\n-            ):  # pylint: disable=useless-super-delegation\n+            def apply_gradients(self, grads_and_vars, name=None):\n                 return super().apply_gradients(grads_and_vars, name)\n \n         mock_optimizer = _OptimizerOverrideApplyGradients()\n\n@@ -152,12 +152,8 @@ class RespectCompiledTrainableState:\n         self._should_set_trainable = False\n \n     def __enter__(self):\n-        self._current_trainable_state = (\n-            self._model._get_trainable_state()\n-        )  # pylint: disable=protected-access\n-        self._compiled_trainable_state = (\n-            self._model._compiled_trainable_state\n-        )  # pylint: disable=protected-access\n+        self._current_trainable_state = self._model._get_trainable_state()\n+        self._compiled_trainable_state = self._model._compiled_trainable_state\n \n         # Check to see if any layer's trainable state has changed since\n         # `compile`.\n@@ -171,22 +167,19 @@ class RespectCompiledTrainableState:\n \n         # If so, restore the model to its compiled state.\n         if self._should_set_trainable:\n-            self._model._set_trainable_state(\n-                self._compiled_trainable_state\n-            )  # pylint: disable=protected-access\n+            self._model._set_trainable_state(self._compiled_trainable_state)\n \n     def __exit__(self, type_arg, value_arg, traceback_arg):\n         # If we set the values to their compiled state in __enter__, we need to\n         # restore the original values before leaving the scope.\n         if self._should_set_trainable:\n-            self._model._set_trainable_state(\n-                self._current_trainable_state\n-            )  # pylint: disable=protected-access\n+            self._model._set_trainable_state(self._current_trainable_state)\n         return False  # False values do not suppress exceptions\n \n \n # Allow use of methods not exposed to the user.\n-# pylint: disable=protected-access\n+\n+\n def get_input_shape_and_dtype(layer):\n     \"\"\"Retrieves input shape and input dtype of layer if applicable.\n \n@@ -219,9 +212,6 @@ def get_input_shape_and_dtype(layer):\n     return None, None\n \n \n-# pylint: enable=protected-access\n-\n-\n def get_static_batch_size(layer):\n     \"\"\"Gets the static batch size of a Layer.\n \n\n@@ -416,7 +416,7 @@ class SliceAggregator(Aggregator):\n         try:\n             self.results[batch_start:batch_end] = batch_element\n \n-        except Exception as e:  # pylint: disable=broad-except\n+        except Exception as e:\n             # `_slice_assign` should only be called in threads and exceptions\n             # raised in threads do not carry over to the main thread. So instead\n             # we perform a a broad catch in the thread and then store the\n@@ -760,7 +760,7 @@ def standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n     \"\"\"\n     if x_weight is None or (\n         isinstance(x_weight, (list, tuple)) and len(x_weight) == 0\n-    ):  # pylint: disable=g-explicit-length-test\n+    ):\n         return [None for _ in output_names]\n     if len(output_names) == 1:\n         if isinstance(x_weight, (list, tuple)) and len(x_weight) == 1:\n@@ -1038,9 +1038,7 @@ def collect_per_output_metric_info(\n             metric_fn = get_metric_function(\n                 metric, output_shape=output_shapes[i], loss_fn=loss_fns[i]\n             )\n-            metric_fn._from_serialized = (\n-                from_serialized  # pylint: disable=protected-access\n-            )\n+            metric_fn._from_serialized = from_serialized\n \n             # If the metric function is not stateful, we create a stateful\n             # version.\n@@ -1051,9 +1049,7 @@ def collect_per_output_metric_info(\n                 # If the metric is being revived from something stateless, such\n                 # as a string (e.g. \"accuracy\"), we may need to later reapply\n                 # transformations such as renaming.\n-                metric_fn._from_serialized = (\n-                    False  # pylint: disable=protected-access\n-                )\n+                metric_fn._from_serialized = False\n             metrics_dict[metric_name] = metric_fn\n         per_output_metrics.append(metrics_dict)\n \n@@ -1771,7 +1767,6 @@ def is_eager_dataset_or_iterator(data):\n     )\n \n \n-# pylint: disable=protected-access\n def get_dataset_graph_def(dataset):\n     if tf.executing_eagerly():\n         graph_def_str = dataset._as_serialized_graph().numpy()\n@@ -2042,10 +2037,6 @@ class ModelInputs:\n \n \n # Allow use of methods not exposed to the user.\n-# pylint: disable=protected-access\n-\n-\n-# pylint: enable=protected-access\n \n \n def generic_output_names(outputs_list):\n@@ -2146,7 +2137,7 @@ def unpack_validation_data(validation_data, raise_if_ambiguous=True):\n             (\n                 val_x,\n                 val_y,\n-            ) = validation_data  # pylint: disable=unpacking-non-sequence\n+            ) = validation_data\n             val_sample_weight = None\n         except ValueError:\n             val_x, val_y, val_sample_weight = validation_data, None, None\n@@ -2156,7 +2147,7 @@ def unpack_validation_data(validation_data, raise_if_ambiguous=True):\n                 val_x,\n                 val_y,\n                 val_sample_weight,\n-            ) = validation_data  # pylint: disable=unpacking-non-sequence\n+            ) = validation_data\n         except ValueError:\n             val_x, val_y, val_sample_weight = validation_data, None, None\n     else:\n\n@@ -96,7 +96,6 @@ class ModelInputsTest(tf.test.TestCase):\n \n class DatasetUtilsTest(tf.test.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(\n-        # pylint: disable=g-long-lambda\n         (\"Batch\", lambda: tf.data.Dataset.range(5).batch(2)),\n         (\"Cache\", lambda: tf.data.Dataset.range(5).cache()),\n         (\n@@ -172,7 +171,6 @@ class DatasetUtilsTest(tf.test.TestCase, parameterized.TestCase):\n         (\"TFRecordDataset\", lambda: tf.data.TFRecordDataset([])),\n         (\"Window\", lambda: tf.data.Dataset.range(5).window(2)),\n         (\"Zip\", lambda: tf.data.Dataset.zip(tf.data.Dataset.range(5))),\n-        # pylint: enable=g-long-lambda\n     )\n     def test_verify_dataset_shuffled(self, dataset_fn, expect_shuffled=False):\n         dataset = dataset_fn()\n@@ -246,7 +244,7 @@ class MonitoredPool(multiprocessing.pool.ThreadPool):\n     def apply_async(self, func, *args, **kwargs):\n         self._apply_counter += 1\n         if self._func_wrapper:\n-            func = self._func_wrapper(func)  # pylint: disable=not-callable\n+            func = self._func_wrapper(func)\n         return super().apply_async(func, *args, **kwargs)\n \n \n@@ -261,9 +259,7 @@ def add_sleep(f):\n \n def cause_error(f):\n     @functools.wraps(f)\n-    def wrapped(\n-        batch_element, batch_start, batch_end, is_finished\n-    ):  # pylint: disable=unused-argument\n+    def wrapped(batch_element, batch_start, batch_end, is_finished):\n         # Induce a TypeError during assignment.\n         return f(None, None, None, is_finished)\n \n\n@@ -17,8 +17,6 @@ import collections\n import warnings\n \n import numpy as np\n-\n-# pylint: disable=g-classes-have-attributes\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n@@ -51,7 +49,7 @@ from keras.utils.mode_keys import ModeKeys\n from tensorflow.python.platform import tf_logging as logging\n \n try:\n-    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n+    from scipy.sparse import issparse\n except ImportError:\n     issparse = None\n \n@@ -216,7 +214,7 @@ class Model(training_lib.Model):\n         if backend.is_tpu_strategy(self._distribution_strategy):\n             if self._distribution_strategy.extended.steps_per_run > 1 and (\n                 not saving_utils.is_hdf5_filepath(filepath)\n-            ):  # pylint: disable=protected-access\n+            ):\n                 raise ValueError(\n                     \"Load weights is not yet supported with TPUStrategy \"\n                     \"with steps_per_run greater than 1.\"\n@@ -1077,9 +1075,7 @@ class Model(training_lib.Model):\n \n         # Reset metrics on all the distributed (cloned) models.\n         if self._distribution_strategy:\n-            distributed_training_utils_v1._reset_metrics(\n-                self\n-            )  # pylint: disable=protected-access\n+            distributed_training_utils_v1._reset_metrics(self)\n \n     def train_on_batch(\n         self,\n@@ -1171,9 +1167,7 @@ class Model(training_lib.Model):\n                 + output_dict[\"output_losses\"]\n                 + output_dict[\"metrics\"]\n             )\n-            outputs = [\n-                _non_none_constant_value(v) for v in outputs\n-            ]  # pylint: disable=protected-access\n+            outputs = [_non_none_constant_value(v) for v in outputs]\n         else:\n             x = training_utils_v1.ModelInputs(x).as_list()\n             ins = x + list(y or []) + list(sample_weights or [])\n@@ -1183,7 +1177,7 @@ class Model(training_lib.Model):\n \n             self._update_sample_weight_modes(sample_weights=sample_weights)\n             self._make_train_function()\n-            outputs = self.train_function(ins)  # pylint: disable=not-callable\n+            outputs = self.train_function(ins)\n \n         if reset_metrics:\n             self.reset_metrics()\n@@ -1262,16 +1256,14 @@ class Model(training_lib.Model):\n                 + output_dict[\"output_losses\"]\n                 + output_dict[\"metrics\"]\n             )\n-            outputs = [\n-                _non_none_constant_value(v) for v in outputs\n-            ]  # pylint: disable=protected-access\n+            outputs = [_non_none_constant_value(v) for v in outputs]\n         else:\n             x = training_utils_v1.ModelInputs(x).as_list()\n             inputs = x + list(y or []) + list(sample_weights or [])\n \n             self._update_sample_weight_modes(sample_weights=sample_weights)\n             self._make_test_function()\n-            outputs = self.test_function(inputs)  # pylint: disable=not-callable\n+            outputs = self.test_function(inputs)\n \n         if reset_metrics:\n             self.reset_metrics()\n@@ -1322,7 +1314,7 @@ class Model(training_lib.Model):\n                 if len(inputs) == 1:\n                     inputs = inputs[0]\n \n-            return self(inputs)  # pylint: disable=not-callable\n+            return self(inputs)\n \n         self._make_predict_function()\n         outputs = self.predict_function(inputs)\n@@ -1563,7 +1555,7 @@ class Model(training_lib.Model):\n \n         if target_tensors is not None and not (\n             isinstance(target_tensors, list) and target_tensors == []\n-        ):  # pylint: disable=g-explicit-bool-comparison\n+        ):\n             if isinstance(target_tensors, list):\n                 if len(target_tensors) != len(self.outputs):\n                     raise ValueError(\n@@ -2099,7 +2091,7 @@ class Model(training_lib.Model):\n \n             # Update the name on the metric class to be the unique generated\n             # name.\n-            metric_fn._name = metric_name  # pylint: disable=protected-access\n+            metric_fn._name = metric_name\n             updated_metrics_dict[metric_name] = metric_fn\n             # Keep track of metric name and function.\n             self._compile_metric_functions.append(metric_fn)\n@@ -2299,9 +2291,7 @@ class Model(training_lib.Model):\n                 metrics_tensors = [\n                     m._call_result\n                     for m in metrics\n-                    if hasattr(\n-                        m, \"_call_result\"\n-                    )  # pylint: disable=protected-access\n+                    if hasattr(m, \"_call_result\")\n                 ]\n \n             with backend.name_scope(\"training\"):\n@@ -2335,9 +2325,7 @@ class Model(training_lib.Model):\n                 metrics_tensors = [\n                     m._call_result\n                     for m in metrics\n-                    if hasattr(\n-                        m, \"_call_result\"\n-                    )  # pylint: disable=protected-access\n+                    if hasattr(m, \"_call_result\")\n                 ]\n \n             with backend.name_scope(\"evaluation\"):\n@@ -2734,7 +2722,7 @@ class Model(training_lib.Model):\n             def _type_spec_from_value(value):\n                 \"\"\"Grab type_spec without converting array-likes to tensors.\"\"\"\n                 if tf_utils.is_extension_type(value):\n-                    return value._type_spec  # pylint: disable=protected-access\n+                    return value._type_spec\n                 # Get a TensorSpec for array-like data without\n                 # converting the data to a Tensor\n                 if hasattr(value, \"shape\") and hasattr(value, \"dtype\"):\n@@ -3213,9 +3201,7 @@ class Model(training_lib.Model):\n         # Otherwise, use the strategy whose scope this is in.\n         if not strategy and tf.distribute.has_strategy():\n             strategy = tf.distribute.get_strategy()\n-        return (\n-            strategy and strategy.extended._in_multi_worker_mode()\n-        )  # pylint: disable=protected-access\n+        return strategy and strategy.extended._in_multi_worker_mode()\n \n     @property\n     def _trackable_saved_model_saver(self):\n@@ -3270,7 +3256,7 @@ class DistributedCallbackModel(Model):\n         orig_model_weights = self._original_model.get_weights()\n         distributed_training_utils_v1.set_weights(\n             self._original_model._distribution_strategy,\n-            self,  # pylint: disable=protected-access\n+            self,\n             orig_model_weights,\n         )\n \n@@ -3637,7 +3623,7 @@ def _get_metrics_from_layers(layers):\n             # We cannot call 'metrics' on the model because we do not want to\n             # include the metrics that were added in compile API of a nested\n             # model.\n-            metrics.extend(layer._metrics)  # pylint: disable=protected-access\n+            metrics.extend(layer._metrics)\n             metrics.extend(_get_metrics_from_layers(layer.layers))\n         else:\n             metrics.extend(layer.metrics)\n\n@@ -168,7 +168,7 @@ def model_to_estimator(\n     try:\n         # isort: off\n         from tensorflow_estimator.python.estimator import (\n-            keras_lib,  # pylint: disable=g-import-not-at-top\n+            keras_lib,\n         )\n     except ImportError:\n         raise NotImplementedError(\n@@ -176,8 +176,7 @@ def model_to_estimator(\n             \"your installation.\"\n         )\n     _model_to_estimator_usage_gauge.get_cell(\"v1\").set(True)\n-    return (\n-        keras_lib.model_to_estimator(  # pylint:disable=unexpected-keyword-arg\n+    return keras_lib.model_to_estimator(\n         keras_model=keras_model,\n         keras_model_path=keras_model_path,\n         custom_objects=custom_objects,\n@@ -188,7 +187,6 @@ def model_to_estimator(\n         metric_names_map=metric_names_map,\n         export_outputs=export_outputs,\n     )\n-    )\n \n \n @keras_export(\"keras.estimator.model_to_estimator\", v1=[])\n@@ -367,7 +365,7 @@ def model_to_estimator_v2(\n     try:\n         # isort: off\n         from tensorflow_estimator.python.estimator import (\n-            keras_lib,  # pylint: disable=g-import-not-at-top\n+            keras_lib,\n         )\n     except ImportError:\n         raise NotImplementedError(\n@@ -375,8 +373,7 @@ def model_to_estimator_v2(\n             \"your installation.\"\n         )\n     _model_to_estimator_usage_gauge.get_cell(\"v2\").set(True)\n-    return (\n-        keras_lib.model_to_estimator(  # pylint:disable=unexpected-keyword-arg\n+    return keras_lib.model_to_estimator(\n         keras_model=keras_model,\n         keras_model_path=keras_model_path,\n         custom_objects=custom_objects,\n@@ -387,7 +384,6 @@ def model_to_estimator_v2(\n         metric_names_map=metric_names_map,\n         export_outputs=export_outputs,\n     )\n-    )\n \n \n # LINT.ThenChange(//tensorflow_estimator/python/estimator/keras_lib.py)\n\n@@ -31,7 +31,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export(v1=[\"keras.layers.DenseFeatures\"])\n-class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n+class DenseFeatures(kfc._BaseFeaturesLayer):\n     \"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.\n \n     Generally a single example in training data is described with\n\n@@ -403,7 +403,7 @@ class DenseFeaturesTest(test_combinations.TestCase):\n             with self.assertRaisesRegex(\n                 ValueError,\n                 r\"Batch size \\(first dimension\\) of each feature must be same.\",\n-            ):  # pylint: disable=anomalous-backslash-in-string\n+            ):\n                 df.DenseFeatures([price1, price2])(features)\n \n     def test_subset_of_static_batch_size_mismatch(self):\n@@ -421,7 +421,7 @@ class DenseFeaturesTest(test_combinations.TestCase):\n             with self.assertRaisesRegex(\n                 ValueError,\n                 r\"Batch size \\(first dimension\\) of each feature must be same.\",\n-            ):  # pylint: disable=anomalous-backslash-in-string\n+            ):\n                 df.DenseFeatures([price1, price2, price3])(features)\n \n     def test_runtime_batch_size_mismatch(self):\n\n@@ -94,15 +94,11 @@ class DenseFeatures(dense_features.DenseFeatures):\n             with tf.name_scope(column.name):\n                 column.create_state(self._state_manager)\n         # We would like to call Layer.build and not _DenseFeaturesHelper.build.\n-        # pylint: disable=protected-access\n-        super(kfc._BaseFeaturesLayer, self).build(\n-            None\n-        )  # pylint: disable=bad-super-call\n+\n+        super(kfc._BaseFeaturesLayer, self).build(None)\n \n \n-class _StateManagerImplV2(\n-    tf.__internal__.feature_column.StateManager\n-):  # pylint: disable=protected-access\n+class _StateManagerImplV2(tf.__internal__.feature_column.StateManager):\n     \"\"\"Manages the state of DenseFeatures.\"\"\"\n \n     def create_variable(\n@@ -130,9 +126,7 @@ class _StateManagerImplV2(\n                 use_resource=use_resource,\n             )\n         if isinstance(var, tf.__internal__.tracking.Trackable):\n-            self._layer._track_trackable(\n-                var, feature_column.name + \"/\" + name\n-            )  # pylint: disable=protected-access\n+            self._layer._track_trackable(var, feature_column.name + \"/\" + name)\n         self._cols_to_vars_map[feature_column][name] = var\n         return var\n \n@@ -161,7 +155,7 @@ def no_manual_dependency_tracking_scope(obj):\n     Yields:\n       a scope in which the object doesn't track dependencies manually.\n     \"\"\"\n-    # pylint: disable=protected-access\n+\n     previous_value = getattr(obj, \"_manual_tracking\", True)\n     obj._manual_tracking = False\n     try:\n\n@@ -396,7 +396,7 @@ class DenseFeaturesTest(test_combinations.TestCase):\n             with self.assertRaisesRegex(\n                 ValueError,\n                 r\"Batch size \\(first dimension\\) of each feature must be same.\",\n-            ):  # pylint: disable=anomalous-backslash-in-string\n+            ):\n                 df.DenseFeatures([price1, price2])(features)\n \n     def test_subset_of_static_batch_size_mismatch(self):\n@@ -414,7 +414,7 @@ class DenseFeaturesTest(test_combinations.TestCase):\n             with self.assertRaisesRegex(\n                 ValueError,\n                 r\"Batch size \\(first dimension\\) of each feature must be same.\",\n-            ):  # pylint: disable=anomalous-backslash-in-string\n+            ):\n                 df.DenseFeatures([price1, price2, price3])(features)\n \n     def test_runtime_batch_size_mismatch(self):\n\n@@ -29,8 +29,6 @@ from keras.feature_column import base_feature_layer as kfc\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=protected-access\n-\n \n @keras_export(\"keras.experimental.SequenceFeatures\")\n class SequenceFeatures(kfc._BaseFeaturesLayer):\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras initializers for TF 1.\"\"\"\n-# pylint:disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -1146,9 +1146,7 @@ def _ensure_keras_seeded():\n     initialized with same seed for tf.random.Generator, so that the value\n     created are in sync among all the clients.\n     \"\"\"\n-    if not getattr(\n-        backend._SEED_GENERATOR, \"generator\", None\n-    ):  # pylint:disable=protected-access\n+    if not getattr(backend._SEED_GENERATOR, \"generator\", None):\n         raise ValueError(\n             \"When using DTensor APIs, you need to set the global seed \"\n             \"before using any Keras initializers. Please make sure \"\n\n@@ -256,9 +256,7 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n                 output, [input_value] + layer.trainable_variables\n             )\n             jac_forward = _jacfwd(\n-                lambda *args: layer(\n-                    args[0], training=training\n-                ),  # pylint:disable=cell-var-from-loop\n+                lambda *args: layer(args[0], training=training),\n                 [input_value] + layer.trainable_variables,\n             )\n             for backward, forward in zip(jac_back, jac_forward):\n@@ -322,16 +320,14 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n                 return self.proj(self.embed(x))\n \n         model = M()\n-        model(tf.zeros([3, 3], dtype=tf.int32))  # pylint: disable=not-callable\n+        model(tf.zeros([3, 3], dtype=tf.int32))\n         parameters = model.embed.variables\n         tangents = [tf.ones_like(v) for v in parameters]\n         with tf.autodiff.ForwardAccumulator(parameters, tangents):\n             # Note that forwardprop runs alongside the original computation.\n             # This test is just checking that it doesn't crash; correctness is\n             # tested in core TF.\n-            model(\n-                tf.zeros([3, 3], dtype=tf.int32)\n-            )  # pylint: disable=not-callable\n+            model(tf.zeros([3, 3], dtype=tf.int32))\n \n \n class HessianTests(tf.test.TestCase, parameterized.TestCase):\n\n@@ -89,7 +89,7 @@ class FunctionTest(tf.test.TestCase):\n         model.call = tf.function(model.call)\n \n         x = tf.ones([1, 2])\n-        y = model(x)  # pylint:disable=not-callable\n+        y = model(x)\n \n         self.assertAllEqual([[3.0]], self.evaluate(y))\n \n@@ -147,7 +147,7 @@ class FunctionTest(tf.test.TestCase):\n \n     def testDecoratedMethodVariableCleanup(self):\n         m = DefunnedMiniModel()\n-        m(tf.ones([1, 2]))  # pylint:disable=not-callable\n+        m(tf.ones([1, 2]))\n         variable_refs = list({v.ref() for v in m.variables})\n         self.assertLen(variable_refs, 2)\n         del m\n@@ -222,7 +222,7 @@ class FunctionTest(tf.test.TestCase):\n         x = tf.constant([[3.0, 4.0]])\n         y = tf.constant([2.0])\n         model = ModelWithOptimizer()\n-        model(x, y)  # pylint:disable=not-callable\n+        model(x, y)\n \n \n class AutomaticControlDependenciesTest(tf.test.TestCase):\n\n@@ -58,7 +58,7 @@ class GradientsTest(tf.test.TestCase):\n         test_model = TestKerasModelClass(10)\n         test_input = tf.constant(tf.zeros((10, 10), dtype=np.float32))\n         # Ensures keras model is initialized.\n-        test_model(test_input)  # pylint: disable=not-callable\n+        test_model(test_input)\n         grads_re, grads = self._TestVariablesGradient(\n             test_input, test_model, test_input\n         )\n@@ -92,7 +92,7 @@ class GradientsTest(tf.test.TestCase):\n         def jacobian(x):\n             with tf.GradientTape() as tape:\n                 tape.watch(x)\n-                y = m(x)  # pylint: disable=not-callable\n+                y = m(x)\n             return tape.batch_jacobian(y, x)\n \n         inp = tf.nn.l2_normalize(tf.ones([1, 2, 3]), axis=[1, 2])\n\n@@ -368,10 +368,9 @@ class LegacyRNNTest(tf.test.TestCase):\n \n         z = tf.zeros((2, 3))\n \n-        kn1(z)  # pylint:disable=not-callable\n-        kn2(z)  # pylint:disable=not-callable\n+        kn1(z)\n+        kn2(z)\n \n-        # pylint: disable=protected-access\n         self.assertTrue(all(\"kn1\" in v.name for v in kn1._cell.variables))\n         self.assertTrue(all(\"kn2\" in v.name for v in kn2._cell.variables))\n \n@@ -379,10 +378,10 @@ class LegacyRNNTest(tf.test.TestCase):\n             kn1_new = KerasNetworkTFRNNs(name=\"kn1_new\")\n             kn2_new = KerasNetworkKerasRNNs(name=\"kn2_new\")\n \n-        kn2_new(z)  # pylint:disable=not-callable\n+        kn2_new(z)\n         # Most importantly, this doesn't fail due to variable scope reuse\n         # issues.\n-        kn1_new(z)  # pylint:disable=not-callable\n+        kn1_new(z)\n \n         self.assertTrue(\n             all(\"kn1_new\" in v.name for v in kn1_new._cell.variables)\n\n@@ -76,7 +76,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n             self.skipTest(\n                 \"Data loading error: Bad magic number for file header.\"\n             )\n-        except Exception as e:  # pylint: disable=broad-except\n+        except Exception as e:\n             if \"URL fetch failure\" in str(e):\n                 self.skipTest(\n                     \"URL fetch error not considered failure of the test.\"\n@@ -269,9 +269,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n \n         for worker_id in range(NUM_WORKERS):\n             accu_result = tf.nest.map_structure(\n-                lambda x: extract_accuracy(\n-                    worker_id, x\n-                ),  # pylint: disable=cell-var-from-loop\n+                lambda x: extract_accuracy(worker_id, x),\n                 mpr_result.stdout,\n             )\n             self.assertTrue(\n@@ -299,7 +297,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n                 multi_worker_dataset = (\n                     strategy.distribute_datasets_from_function(\n                         lambda input_context: self.dataset_fn(\n-                            global_batch_size,  # pylint: disable=g-long-lambda\n+                            global_batch_size,\n                             input_context,\n                         )\n                     )\n\n@@ -180,7 +180,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n                 )\n \n                 train_dataset = raw_dataset.map(\n-                    lambda x: (  # pylint: disable=g-long-lambda\n+                    lambda x: (\n                         {\"features\": feature_ps(x[\"features\"])},\n                         label_ps(x[\"label\"]),\n                     )\n\n@@ -223,7 +223,7 @@ class KerasLoadTest(tf.test.TestCase, parameterized.TestCase):\n             def call(self, x):\n                 return x + 1.0, x + 2.0\n \n-        out = _MultiOutput(name=\"out\")(inp)  # pylint: disable=not-callable\n+        out = _MultiOutput(name=\"out\")(inp)\n         model = tf.keras.Model(inp, out)\n         loaded = cycle(model, cycles)\n         self.assertAllClose(\n\n@@ -168,7 +168,7 @@ class TpuStrategyTest(tf.test.TestCase):\n                 )\n \n                 train_dataset = raw_dataset.map(\n-                    lambda x: (  # pylint: disable=g-long-lambda\n+                    lambda x: (\n                         {\"features\": feature_mapper(x[\"features\"])},\n                         label_mapper(x[\"label\"]),\n                     )\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Layers that act as activation functions.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n from keras.layers.activation.elu import ELU\n from keras.layers.activation.leaky_relu import LeakyReLU\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Exponential Linear Unit activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.engine.base_layer import Layer\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Leaky version of a Rectified Linear Unit activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.engine.base_layer import Layer\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Parametric Rectified Linear Unit activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras import constraints\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Rectified Linear Unit activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.engine.base_layer import Layer\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Softmax activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Thresholded Rectified Linear Unit activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras attention layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n from keras.layers.attention.additive_attention import AdditiveAttention\n from keras.layers.attention.attention import Attention\n\n@@ -17,7 +17,7 @@\n This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -17,7 +17,7 @@\n This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -17,7 +17,7 @@\n This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras-based multi-head attention layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import collections\n import math\n@@ -313,9 +313,7 @@ class MultiHeadAttention(Layer):\n                 str(cls),\n             )\n         else:\n-            layer._build_from_signature(\n-                query_shape, value_shape, key_shape\n-            )  # pylint: disable=protected-access\n+            layer._build_from_signature(query_shape, value_shape, key_shape)\n         return layer\n \n     def _build_from_signature(self, query, value, key=None):\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras convolution layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n # Convolution layer aliases.\n # Convolution layers.\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras base class for convolution layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -185,13 +185,12 @@ class Conv(Layer):\n             )\n \n         if self.padding == \"causal\":\n-            # pylint: disable=g-import-not-at-top\n+\n             from keras.layers.convolutional.conv1d import Conv1D\n             from keras.layers.convolutional.separable_conv1d import (\n                 SeparableConv1D,\n             )\n \n-            # pylint: enable=g-import-not-at-top\n             if not isinstance(self, (Conv1D, SeparableConv1D)):\n                 raise ValueError(\n                     \"Causal padding is only supported for `Conv1D`\"\n@@ -354,7 +353,7 @@ class Conv(Layer):\n                 f\"dimension.\"\n             )\n \n-    def _recreate_conv_op(self, inputs):  # pylint: disable=unused-argument\n+    def _recreate_conv_op(self, inputs):\n         return False\n \n     def get_config(self):\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras abstract base for depthwise convolutions.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras abstract base layer for separable nD convolution.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras 1D convolution layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import activations\n from keras import constraints\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras 1D transposed convolution layer (sometimes called deconvolution).\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras 2D convolution layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import activations\n from keras import constraints\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras 2D transposed convolution layer (sometimes called deconvolution).\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras 3D convolution layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import activations\n from keras import constraints\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras 3D transposed convolution layer (sometimes called deconvolution).\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -141,7 +141,7 @@ class Conv2DTransposeTest(test_combinations.TestCase):\n         )\n \n         input_data = np.arange(48).reshape((1, 4, 4, 3)).astype(np.float32)\n-        # pylint: disable=too-many-function-args\n+\n         expected_output = np.float32(\n             [\n                 [192, 228, 192, 228],\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras depthwise 1D convolution.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras depthwise 2D convolution.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.layers.convolutional.base_depthwise_conv import DepthwiseConv\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras depthwise separable 1D convolution.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras depthwise separable 2D convolution.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Activation layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import activations\n from keras.engine.base_layer import Layer\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Dense layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras-based einsum dense layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import re\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Embedding layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -107,11 +107,11 @@ class EmbeddingTest(test_combinations.TestCase):\n         inputs = keras.layers.Input(\n             shape=(None,), dtype=tf.float32, ragged=True\n         )\n-        # pylint: disable=unnecessary-lambda\n+\n         outputs = keras.layers.Lambda(\n             lambda args: keras.backend.identity(args)\n         )(inputs)\n-        # pylint: enable=unnecessary-lambda\n+\n         outputs = layer(outputs)\n \n         model = keras.Model(inputs, outputs)\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Lambda layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n import sys\n import textwrap\n import types as python_types\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Masking layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -79,9 +79,7 @@ class Masking(Layer):\n         )\n         outputs = inputs * tf.cast(boolean_mask, inputs.dtype)\n         # Compute the mask and outputs simultaneously.\n-        outputs._keras_mask = tf.squeeze(\n-            boolean_mask, axis=-1\n-        )  # pylint: disable=protected-access\n+        outputs._keras_mask = tf.squeeze(boolean_mask, axis=-1)\n         return outputs\n \n     def compute_output_shape(self, input_shape):\n\n@@ -28,8 +28,6 @@ from tensorflow.python.util.tf_export import (\n     get_symbol_from_name,\n )\n \n-# pylint: enable=g-bad-import-order\n-\n \n class ClassMethod(Layer):\n     \"\"\"Wraps a TF API Class's class method  in a `Layer` object.\n@@ -359,9 +357,7 @@ class TFOpLambda(Layer):\n         return cls(**config)\n \n \n-def _delegate_property(\n-    keras_tensor_cls, property_name\n-):  # pylint: disable=invalid-name\n+def _delegate_property(keras_tensor_cls, property_name):\n     \"\"\"Register property on a KerasTensor class.\n \n     Calling this multiple times with the same arguments should be a no-op.\n@@ -380,13 +376,11 @@ def _delegate_property(\n     # due to dynamic layer class versioning.\n     property_access = property(\n         lambda self: InstanceProperty(property_name)(self)\n-    )  # pylint: disable=unnecessary-lambda\n+    )\n     setattr(keras_tensor_cls, property_name, property_access)\n \n \n-def _delegate_method(\n-    keras_tensor_cls, method_name\n-):  # pylint: disable=invalid-name\n+def _delegate_method(keras_tensor_cls, method_name):\n     \"\"\"Register method on a KerasTensor class.\n \n     Calling this function times with the same arguments should be a no-op.\n@@ -583,7 +577,7 @@ class TFSlicingOpDispatcher(tf.__internal__.dispatch.OpDispatcher):\n \n \n for slicing_op in [\n-    tf.__operators__.getitem,  # pylint: disable=protected-access\n+    tf.__operators__.getitem,\n     tf.compat.v1.boolean_mask,\n     tf.boolean_mask,\n     tf.__operators__.ragged_getitem,\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Keras layers that implement explicit (approximate) kernel feature maps.\"\"\"\n \n import numpy as np\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Tests for layers.__init__.\"\"\"\n \n import tensorflow.compat.v2 as tf\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \"\"\"Locally-connected layer for 1D input.\"\"\"\n \n from keras import activations\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \"\"\"Locally-connected layer for 2D input.\"\"\"\n \n from keras import activations\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras merging layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n # Merging functions.\n # Merging layers.\n\n@@ -58,9 +58,7 @@ class _Merge(Layer):\n         if None in [shape1, shape2]:\n             return None\n         elif len(shape1) < len(shape2):\n-            return self._compute_elemwise_op_output_shape(\n-                shape2, shape1\n-            )  # pylint: disable=arguments-out-of-order\n+            return self._compute_elemwise_op_output_shape(shape2, shape1)\n         elif not shape2:\n             return shape1\n         output_shape = list(shape1[: -len(shape2)])\n@@ -240,5 +238,5 @@ class _Merge(Layer):\n             backend.concatenate(masks, axis=0), axis=0, keepdims=False\n         )\n \n-    def get_config(self):  # pylint: disable=useless-super-delegation\n+    def get_config(self):\n         return super().get_config()\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Layers that operate regularization via the addition of noise.\"\"\"\n-# pylint: disable=g-bad-import-order,unused-import\n+\n \n from keras.layers.regularization.alpha_dropout import AlphaDropout  # noqa: F401\n \n\n@@ -567,9 +567,7 @@ class BatchNormalizationBase(Layer):\n             if tf.compat.v1.executing_eagerly_outside_functions():\n                 return variable.assign_sub(calculate_update_delta(), name=scope)\n             else:\n-                with tf.compat.v1.colocate_with(\n-                    variable\n-                ):  # pylint: disable=protected-access\n+                with tf.compat.v1.colocate_with(variable):\n                     return tf.compat.v1.assign_sub(\n                         variable, calculate_update_delta(), name=scope\n                     )\n@@ -579,9 +577,7 @@ class BatchNormalizationBase(Layer):\n             if tf.compat.v1.executing_eagerly_outside_functions():\n                 return variable.assign(value, name=scope)\n             else:\n-                with tf.compat.v1.colocate_with(\n-                    variable\n-                ):  # pylint: disable=protected-access\n+                with tf.compat.v1.colocate_with(variable):\n                     return tf.compat.v1.assign(variable, value, name=scope)\n \n     def _fused_batch_norm(self, inputs, training):\n@@ -1089,7 +1085,6 @@ class BatchNormalizationBase(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.layers.experimental.SyncBatchNormalization\", v1=[])\n class SyncBatchNormalization(BatchNormalizationBase):\n     r\"\"\"Normalize and scale inputs or activations synchronously across replicas.\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Batch Normalization V1 layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n from keras.layers.normalization import batch_normalization\n \n@@ -21,7 +21,6 @@ from keras.layers.normalization import batch_normalization\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=missing-docstring\n @keras_export(v1=[\"keras.layers.BatchNormalization\"])\n class BatchNormalization(batch_normalization.BatchNormalizationBase):\n     _USE_V2_BEHAVIOR = False\n\n@@ -26,8 +26,6 @@ from keras.utils import tf_utils\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @keras_export(\"keras.layers.LayerNormalization\")\n class LayerNormalization(Layer):\n\n@@ -349,7 +349,6 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n                 )\n                 norm.build(x.shape)\n \n-                # pylint: disable=cell-var-from-loop\n                 def forward_fn(x, beta, gamma):\n                     # We must monkey-patch the attributes of `norm` with the\n                     # function arguments, so that the gradient checker will\n@@ -364,7 +363,6 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n                         ):\n                             return norm(x)\n \n-                # pylint: enable=cell-var-from-loop\n                 results = tf.test.compute_gradient(\n                     forward_fn,\n                     [keras.backend.cast(x, dtype), norm.beta, norm.gamma],\n\n@@ -13,9 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Unit Normalization layer.\"\"\"\n-# pylint: disable=g-bad-import-order\n \n-# pylint: disable=g-classes-have-attributes\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for Unit Normalization layer.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras Pooling layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n # Pooling layer aliases.\n # Pooling layers.\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Average pooling 1D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import functools\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Average pooling 2D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Average pooling 3D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Private base class for global pooling 1D layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Private base class for global pooling 2D layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Private base class for global pooling 3D layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Private base class for pooling 1D layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Private base class for pooling 2D layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Private base class for pooling 3D layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Global average pooling 1D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Global average pooling 2D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.layers.pooling.base_global_pooling2d import GlobalPooling2D\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Global average pooling 3D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.layers.pooling.base_global_pooling3d import GlobalPooling3D\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Global max pooling 1D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.layers.pooling.base_global_pooling1d import GlobalPooling1D\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Global max pooling 2D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.layers.pooling.base_global_pooling2d import GlobalPooling2D\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Global max pooling 3D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import backend\n from keras.layers.pooling.base_global_pooling3d import GlobalPooling3D\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Max pooling 1D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import functools\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Max pooling 2D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Max pooling 3D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -43,7 +43,7 @@ def tensor_gen(batch, num_elements):\n def get_vocab():\n     vocab = list(\n         set([a + b for a in string.ascii_letters for b in string.ascii_letters])\n-    )  # pylint:disable=g-complex-comprehension\n+    )\n     vocab.sort()\n     return vocab\n \n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras CategoryEncoding preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras discretization preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -308,7 +306,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n             initializer=lambda shape, dtype: [\n                 [],\n                 [],\n-            ],  # pylint: disable=unused-arguments\n+            ],\n             trainable=False,\n         )\n \n@@ -389,7 +387,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n             get_bin_boundaries(self.summary, self.num_bins)\n         )\n \n-    def reset_state(self):  # pylint: disable=method-hidden\n+    def reset_state(self):\n         if self.input_bin_boundaries is not None or not self.built:\n             return\n \n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras hashed crossing preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras hashing preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras image preprocessing layers.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -457,7 +455,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n         bounding_box = inputs.get(BOUNDING_BOXES, None)\n         transformation = self.get_random_transformation(\n             image=image, label=label, bounding_box=bounding_box\n-        )  # pylint: disable=assignment-from-none\n+        )\n         image = self.augment_image(image, transformation=transformation)\n         result = {IMAGES: image}\n         if label is not None:\n@@ -1480,9 +1478,7 @@ class RandomZoom(BaseImageAugmentationLayer):\n                 self.width_lower = width_factor[0]\n                 self.width_upper = width_factor[1]\n             else:\n-                self.width_lower = (\n-                    -width_factor\n-                )  # pylint: disable=invalid-unary-operand-type\n+                self.width_lower = -width_factor\n                 self.width_upper = width_factor\n \n             if self.width_lower < -1.0 or self.width_upper < -1.0:\n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras index lookup preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import collections\n \n@@ -85,9 +83,7 @@ class VocabWeightHandler(base_layer_utils.TrackableWeightHandler):\n \n     def set_weights(self, weights):\n         tokens = tf.convert_to_tensor(weights[0], self._dtype)\n-        self._layer.lookup_table = self._layer._lookup_table_from_tokens(\n-            tokens\n-        )  # pylint: disable=protected-access\n+        self._layer.lookup_table = self._layer._lookup_table_from_tokens(tokens)\n \n     def get_tensors(self):\n         # Just save the non-config part of the vocab (no special tokens).\n@@ -717,7 +713,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         # tables.\n         self.reset_state()\n \n-    def reset_state(self):  # pylint: disable=method-hidden\n+    def reset_state(self):\n         if self._has_input_vocabulary:\n             return\n \n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras string lookup preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Normalization preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -331,7 +329,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         self.adapt_variance.assign(total_variance)\n         self.count.assign(total_count)\n \n-    def reset_state(self):  # pylint: disable=method-hidden\n+    def reset_state(self):\n         if self.input_mean is not None or not self.built:\n             return\n \n\n@@ -22,8 +22,6 @@ from keras.engine import functional\n from keras.engine import sequential\n from keras.utils import tf_utils\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n # Sequential methods should take precedence.\n class PreprocessingStage(\n@@ -84,13 +82,9 @@ class PreprocessingStage(\n                   Batch of inputs to be processed by layer\n                     `self.layers[current_layer_index]`\n                 \"\"\"\n-                if (\n-                    current_layer_index == 0\n-                ):  # pylint: disable=cell-var-from-loop\n+                if current_layer_index == 0:\n                     return x\n-                for i in range(\n-                    current_layer_index\n-                ):  # pylint: disable=cell-var-from-loop\n+                for i in range(current_layer_index):\n                     x = self.layers[i](x)\n                 return x\n \n\n@@ -30,8 +30,6 @@ from keras.layers.preprocessing import preprocessing_stage\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.testing_infra import test_combinations\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n class PL(base_preprocessing_layer.PreprocessingLayer):\n     def __init__(self, **kwargs):\n\n@@ -24,8 +24,6 @@ from keras.layers.preprocessing import preprocessing_stage\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.testing_infra import test_combinations\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class PreprocessingStageTest(\n\n@@ -23,8 +23,6 @@ from keras.layers.preprocessing import index_lookup\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @keras_export(\n     \"keras.layers.StringLookup\",\n\n@@ -14,8 +14,6 @@\n # ==============================================================================\n \"\"\"Keras text vectorization preprocessing layer.\"\"\"\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -474,7 +472,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     def finalize_state(self):\n         self._lookup_layer.finalize_state()\n \n-    def reset_state(self):  # pylint: disable=method-hidden\n+    def reset_state(self):\n         self._lookup_layer.reset_state()\n \n     def get_vocabulary(self, include_special_tokens=True):\n\n@@ -1788,9 +1788,9 @@ class TextVectorizationOutputTest(\n         )\n \n         # pyformat: disable\n-        # pylint: disable=bad-whitespace\n+\n         expected_output = [[0, 0.8, 0.25, 0.75, 0, 0], [1, 0.4, 0, 0, 0.6, 0]]\n-        # pylint: enable=bad-whitespace\n+\n         # pyformat: enable\n         max_tokens = 6\n         expected_output_shape = [None, max_tokens]\n@@ -1831,9 +1831,9 @@ class TextVectorizationOutputTest(\n         )\n \n         # pyformat: disable\n-        # pylint: disable=bad-whitespace\n+\n         expected_output = [[0, 0.8, 0.25, 0.75, 0], [1, 0.4, 0, 0, 0.6]]\n-        # pylint: enable=bad-whitespace\n+\n         # pyformat: enable\n         max_tokens = 5\n         expected_output_shape = [None, max_tokens]\n@@ -1873,9 +1873,9 @@ class TextVectorizationOutputTest(\n         )\n \n         # pyformat: disable\n-        # pylint: disable=bad-whitespace\n+\n         expected_output = [[0, 0.8, 0.25, 0.75, 0], [0.2, 0.4, 0, 0, 0.6]]\n-        # pylint: enable=bad-whitespace\n+\n         # pyformat: enable\n         max_tokens = 5\n         expected_output_shape = [None, max_tokens]\n@@ -2349,10 +2349,10 @@ class TextVectorizationSavingTest(\n         )\n \n         # pyformat: disable\n-        # pylint: disable=bad-whitespace\n+\n         expected_output = [[0, 0.8, 0.25, 0.75, 0], [1, 0.4, 0, 0, 0.6]]\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-        # pylint: enable=bad-whitespace\n+\n         # pyformat: enable\n \n         # Build and validate a golden model.\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras regularization layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n from keras.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the ActivityRegularization layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import regularizers\n from keras.engine.base_layer import Layer\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the AlphaDropout layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -69,9 +69,7 @@ class AlphaDropout(base_layer.BaseRandomLayer):\n         if 0.0 < self.rate < 1.0:\n             noise_shape = self._get_noise_shape(inputs)\n \n-            def dropped_inputs(\n-                inputs=inputs, rate=self.rate\n-            ):  # pylint: disable=missing-docstring\n+            def dropped_inputs(inputs=inputs, rate=self.rate):\n                 alpha = 1.6732632423543772848170429916717\n                 scale = 1.0507009873554804934193349852946\n                 alpha_p = -alpha * scale\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Dropout layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -90,7 +90,7 @@ class Dropout(base_layer.BaseRandomLayer):\n         self.supports_masking = True\n \n     def build(self, input_shape):\n-        self._random_generator._maybe_init()  # pylint: disable=protected-access\n+        self._random_generator._maybe_init()\n \n     def _get_noise_shape(self, inputs):\n         # Subclasses of `Dropout` may implement `_get_noise_shape(self,\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the GaussianDropout layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the GaussianNoise layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the SpatialDropout1D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the SpatialDropout2D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the SpatialDropout3D layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras cropping layer for 1D input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras cropping layer for 2D input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -107,7 +107,7 @@ class Cropping2D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         input_shape = tf.TensorShape(input_shape).as_list()\n-        # pylint: disable=invalid-unary-operand-type\n+\n         if self.data_format == \"channels_first\":\n             return tf.TensorShape(\n                 [\n@@ -134,10 +134,9 @@ class Cropping2D(Layer):\n                     input_shape[3],\n                 ]\n             )\n-        # pylint: enable=invalid-unary-operand-type\n \n     def call(self, inputs):\n-        # pylint: disable=invalid-unary-operand-type\n+\n         if self.data_format == \"channels_first\":\n             if (\n                 inputs.shape[2] is not None\n@@ -211,8 +210,7 @@ class Cropping2D(Layer):\n                 self.cropping[0][0] : -self.cropping[0][1],\n                 self.cropping[1][0] : -self.cropping[1][1],\n                 :,\n-            ]  # pylint: disable=invalid-unary-operand-type\n-        # pylint: enable=invalid-unary-operand-type\n+            ]\n \n     def get_config(self):\n         config = {\"cropping\": self.cropping, \"data_format\": self.data_format}\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras cropping layer for 3D input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -120,7 +120,7 @@ class Cropping3D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         input_shape = tf.TensorShape(input_shape).as_list()\n-        # pylint: disable=invalid-unary-operand-type\n+\n         if self.data_format == \"channels_first\":\n             if input_shape[2] is not None:\n                 dim1 = (\n@@ -165,10 +165,9 @@ class Cropping3D(Layer):\n             return tf.TensorShape(\n                 [input_shape[0], dim1, dim2, dim3, input_shape[4]]\n             )\n-        # pylint: enable=invalid-unary-operand-type\n \n     def call(self, inputs):\n-        # pylint: disable=invalid-unary-operand-type\n+\n         if self.data_format == \"channels_first\":\n             if (\n                 self.cropping[0][1]\n@@ -306,8 +305,7 @@ class Cropping3D(Layer):\n                 self.cropping[1][0] : -self.cropping[1][1],\n                 self.cropping[2][0] : -self.cropping[2][1],\n                 :,\n-            ]  # pylint: disable=invalid-unary-operand-type\n-        # pylint: enable=invalid-unary-operand-type\n+            ]\n \n     def get_config(self):\n         config = {\"cropping\": self.cropping, \"data_format\": self.data_format}\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the flatten layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import functools\n import operator\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Permute layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import copy\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the RepeatVector layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the Reshape layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras upsampling layer for 1D inputs.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras upsampling layer for 2D inputs.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras upsampling layer for 3D inputs.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras zero-padding layer for 1D input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras zero-padding layer for 2D input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras zero-padding layer for 3D input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Base class for RNN cells.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras.engine import base_layer\n from keras.layers.rnn import rnn_utils\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Base class for N-D convolutional LSTM layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Base class for convolutional-recurrent layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -210,9 +210,7 @@ class ConvRNN(RNN):\n         # Note input_shape will be list of shapes of initial states and\n         # constants if these are passed in __call__.\n         if self._num_constants is not None:\n-            constants_shape = input_shape[\n-                -self._num_constants :\n-            ]  # pylint: disable=invalid-unary-operand-type\n+            constants_shape = input_shape[-self._num_constants :]\n         else:\n             constants_shape = None\n \n@@ -315,12 +313,8 @@ class ConvRNN(RNN):\n                 )\n \n             def step(inputs, states):\n-                constants = states[\n-                    -self._num_constants :\n-                ]  # pylint: disable=invalid-unary-operand-type\n-                states = states[\n-                    : -self._num_constants\n-                ]  # pylint: disable=invalid-unary-operand-type\n+                constants = states[-self._num_constants :]\n+                states = states[: -self._num_constants]\n                 return self.cell.call(\n                     inputs, states, constants=constants, **kwargs\n                 )\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Base class for recurrent layers backed by cuDNN.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -52,7 +52,7 @@ class _CuDNNRNN(RNN):\n     ):\n         # We invoke the base layer's initializer directly here because we do not\n         # want to create RNN cell instance.\n-        super(RNN, self).__init__(**kwargs)  # pylint: disable=bad-super-call\n+        super(RNN, self).__init__(**kwargs)\n         self.return_sequences = return_sequences\n         self.return_state = return_state\n         self.go_backwards = go_backwards\n@@ -123,9 +123,7 @@ class _CuDNNRNN(RNN):\n             \"stateful\": self.stateful,\n             \"time_major\": self.time_major,\n         }\n-        base_config = super(  # pylint: disable=bad-super-call\n-            RNN, self\n-        ).get_config()\n+        base_config = super(RNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n     @classmethod\n@@ -146,9 +144,7 @@ class _CuDNNRNN(RNN):\n \n     @property\n     def losses(self):\n-        return super(RNN, self).losses  # pylint: disable=bad-super-call\n+        return super(RNN, self).losses\n \n     def get_losses_for(self, inputs=None):\n-        return super(  # pylint: disable=bad-super-call\n-            RNN, self\n-        ).get_losses_for(inputs=inputs)\n+        return super(RNN, self).get_losses_for(inputs=inputs)\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Base class for recurrent layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import collections\n \n@@ -692,12 +692,8 @@ class RNN(base_layer.Layer):\n                 )\n \n             def step(inputs, states):\n-                constants = states[\n-                    -self._num_constants :\n-                ]  # pylint: disable=invalid-unary-operand-type\n-                states = states[\n-                    : -self._num_constants\n-                ]  # pylint: disable=invalid-unary-operand-type\n+                constants = states[-self._num_constants :]\n+                states = states[: -self._num_constants]\n \n                 states = (\n                     states[0] if len(states) == 1 and is_tf_rnn_cell else states\n@@ -972,7 +968,7 @@ class RNN(base_layer.Layer):\n         )\n         num_constants = config.pop(\"num_constants\", 0)\n         layer = cls(cell, **config)\n-        layer._num_constants = num_constants  # pylint: disable=protected-access\n+        layer._num_constants = num_constants\n         return layer\n \n     @property\n\n@@ -16,7 +16,7 @@\n \n Wrappers are layers that augment the functionality of another layer.\n \"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import copy\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Bidirectional wrapper for RNNs.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import copy\n \n@@ -180,9 +180,7 @@ class Bidirectional(Wrapper):\n \n     @property\n     def _use_input_spec_as_call_signature(self):\n-        return (\n-            self.layer._use_input_spec_as_call_signature\n-        )  # pylint: disable=protected-access\n+        return self.layer._use_input_spec_as_call_signature\n \n     def _verify_layer_config(self):\n         \"\"\"Ensure the forward and backward layers have valid common property.\"\"\"\n@@ -514,5 +512,5 @@ class Bidirectional(Wrapper):\n             config[\"backward_layer\"] = backward_layer\n         # Instantiate the wrapper, adjust it and return it.\n         layer = cls(**config)\n-        layer._num_constants = num_constants  # pylint: disable=protected-access\n+        layer._num_constants = num_constants\n         return layer\n\n@@ -943,7 +943,6 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         x = tf.cast(x, \"float32\")\n \n-        # pylint: disable=g-long-lambda\n         with self.cached_session():\n             if merge_mode == \"ave\":\n                 merge_func = lambda y, y_rev: (y + y_rev) / 2\n@@ -951,7 +950,6 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n                 merge_func = lambda y, y_rev: tf.concat((y, y_rev), axis=-1)\n             elif merge_mode == \"mul\":\n                 merge_func = lambda y, y_rev: (y * y_rev)\n-                # pylint: enable=g-long-lambda\n \n             inputs = keras.Input(\n                 shape=(None, 3), batch_size=4, dtype=\"float32\", ragged=True\n\n@@ -454,9 +454,7 @@ class DropoutWrapper(_RNNCellWrapper):\n             \"input_size\": self._input_size,\n             \"seed\": self._seed,\n         }\n-        if (\n-            self._dropout_state_filter != _default_dropout_state_filter_visitor\n-        ):  # pylint: disable=comparison-with-callable\n+        if self._dropout_state_filter != _default_dropout_state_filter_visitor:\n             (\n                 function,\n                 function_type,\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"1D Convolutional LSTM layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,disable=g-direct-tensorflow-import\n+\n \n from keras.layers.rnn.base_conv_lstm import ConvLSTM\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"2D Convolutional LSTM layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,disable=g-direct-tensorflow-import\n+\n \n from keras.layers.rnn.base_conv_lstm import ConvLSTM\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"3D Convolutional LSTM layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,disable=g-direct-tensorflow-import\n+\n \n from keras.layers.rnn.base_conv_lstm import ConvLSTM\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Fast GRU layer backed by cuDNN.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import collections\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Fast LSTM layer backed by cuDNN.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import collections\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Gated Recurrent Unit layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import uuid\n \n\n@@ -74,13 +74,13 @@ class DefunWrapper:\n         }\n         if self.layer_name == \"lstm\":\n             from keras.layers.rnn import (\n-                lstm,  # pylint: disable=g-import-not-at-top\n+                lstm,\n             )\n \n             layer_func = lstm.lstm_with_backend_selection\n         else:\n             from keras.layers.rnn import (\n-                gru,  # pylint: disable=g-import-not-at-top\n+                gru,\n             )\n \n             layer_func = gru.gru_with_backend_selection\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Gated Recurrent Unit V1 layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import activations\n from keras import constraints\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Module implementing the V1 version of RNN cell wrappers.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from __future__ import absolute_import\n from __future__ import division\n@@ -497,9 +497,7 @@ class DropoutWrapper(_RNNCellWrapperV1):\n             \"input_size\": self._input_size,\n             \"seed\": self._seed,\n         }\n-        if (\n-            self._dropout_state_filter != _default_dropout_state_filter_visitor\n-        ):  # pylint: disable=comparison-with-callable\n+        if self._dropout_state_filter != _default_dropout_state_filter_visitor:\n             (\n                 function,\n                 function_type,\n@@ -659,7 +657,7 @@ class DeviceWrapper(_RNNCellWrapperV1):\n \n def _default_dropout_state_filter_visitor(substate):\n     from keras.layers.rnn.legacy_cells import (\n-        LSTMStateTuple,  # pylint: disable=g-import-not-at-top\n+        LSTMStateTuple,\n     )\n \n     if isinstance(substate, LSTMStateTuple):\n\n@@ -20,7 +20,7 @@ operators that allow adding dropouts, projections, or embeddings for inputs.\n Constructing multi-layer cells is supported by the class `MultiRNNCell`, or by\n calling the `rnn` ops several times.\n \"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from __future__ import absolute_import\n from __future__ import division\n@@ -327,7 +327,7 @@ class RNNCell(base_layer.Layer):\n         return output\n \n     # TODO(b/134773139): Remove when contrib RNN cells implement `get_config`\n-    def get_config(self):  # pylint: disable=useless-super-delegation\n+    def get_config(self):\n         return super().get_config()\n \n     @property\n@@ -1147,9 +1147,9 @@ class LSTMCell(LayerRNNCell):\n             ) * self._activation(j)\n \n         if self._cell_clip is not None:\n-            # pylint: disable=invalid-unary-operand-type\n+\n             c = tf.clip_by_value(c, -self._cell_clip, self._cell_clip)\n-            # pylint: enable=invalid-unary-operand-type\n+\n         if self._use_peepholes:\n             m = sigmoid(o + self._w_o_diag * c) * self._activation(c)\n         else:\n@@ -1159,9 +1159,8 @@ class LSTMCell(LayerRNNCell):\n             m = tf.matmul(m, self._proj_kernel)\n \n             if self._proj_clip is not None:\n-                # pylint: disable=invalid-unary-operand-type\n+\n                 m = tf.clip_by_value(m, -self._proj_clip, self._proj_clip)\n-                # pylint: enable=invalid-unary-operand-type\n \n         new_state = (\n             LSTMStateTuple(c, m)\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Long Short-Term Memory layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import uuid\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Long Short-Term Memory V1 layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n from keras import activations\n from keras import constraints\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Utilities for RNN cells and layers.\"\"\"\n-# pylint: disable=protected-access\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Fully connected RNN layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import functools\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Wrapper layer to apply every temporal slice of an input.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -193,7 +193,7 @@ class TimeDistributed(Wrapper):\n                 mask=mask,\n                 unroll=False,\n             )\n-            # pylint: disable=g-long-lambda\n+\n             y = tf.nest.map_structure(\n                 lambda output: backend.maybe_convert_to_ragged(\n                     is_ragged_input, output, row_lengths\n@@ -253,7 +253,7 @@ class TimeDistributed(Wrapper):\n \n                 # Shape: (num_samples, timesteps, ...)\n                 output_shape = self.compute_output_shape(input_shape)\n-                # pylint: disable=g-long-lambda\n+\n                 output_shape = tf.nest.map_structure(\n                     lambda tensor, int_shape: self._get_shape_tuple(\n                         (-1, input_length), tensor, 1, int_shape[2:]\n\n@@ -139,15 +139,15 @@ def populate_deserializable_objects():\n     ] = batch_normalization.BatchNormalization\n \n     # Prevent circular dependencies.\n-    from keras import models  # pylint: disable=g-import-not-at-top\n+    from keras import models\n     from keras.feature_column.sequence_feature_column import (\n-        SequenceFeatures,  # pylint: disable=g-import-not-at-top\n+        SequenceFeatures,\n     )\n     from keras.premade_models.linear import (\n-        LinearModel,  # pylint: disable=g-import-not-at-top\n+        LinearModel,\n     )\n     from keras.premade_models.wide_deep import (\n-        WideDeepModel,  # pylint: disable=g-import-not-at-top\n+        WideDeepModel,\n     )\n \n     LOCAL.ALL_OBJECTS[\"Input\"] = input_layer.Input\n@@ -161,13 +161,13 @@ def populate_deserializable_objects():\n \n     if tf.__internal__.tf2.enabled():\n         from keras.feature_column.dense_features_v2 import (\n-            DenseFeatures,  # pylint: disable=g-import-not-at-top\n+            DenseFeatures,\n         )\n \n         LOCAL.ALL_OBJECTS[\"DenseFeatures\"] = DenseFeatures\n     else:\n         from keras.feature_column.dense_features import (\n-            DenseFeatures,  # pylint: disable=g-import-not-at-top\n+            DenseFeatures,\n         )\n \n         LOCAL.ALL_OBJECTS[\"DenseFeatures\"] = DenseFeatures\n\n@@ -1,5 +1,3 @@\n \"\"\"Init file.\"\"\"\n \n-from keras.legacy_tf_layers import (\n-    migration_utils,  # pylint: disable=unused-import\n-)\n+from keras.legacy_tf_layers import migration_utils\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # =============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n@@ -320,7 +320,7 @@ class Layer(base_layer.Layer):\n                 new_losses, tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES\n             )\n \n-    def _name_scope(self):  # pylint: disable=method-hidden\n+    def _name_scope(self):\n         \"\"\"Determines op naming for the Layer.\"\"\"\n         if self._keras_style:\n             return super()._name_scope()\n@@ -477,9 +477,7 @@ class Layer(base_layer.Layer):\n             self._scope, reuse=reuse, auxiliary_name_scope=False\n         ) as scope:\n             self._current_scope = scope\n-            with backend.name_scope(\n-                self._name_scope()\n-            ):  # pylint: disable=not-callable\n+            with backend.name_scope(self._name_scope()):\n                 use_resource = (\n                     use_resource\n                     or self._use_resource_variables\n@@ -510,9 +508,7 @@ class Layer(base_layer.Layer):\n                         self._handle_weight_regularization(\n                             name, variable, regularizer\n                         )\n-                        var_store = (\n-                            vs._get_default_variable_store()\n-                        )  # pylint: disable=protected-access\n+                        var_store = vs._get_default_variable_store()\n                         # When the shim to get variable scope working in TF2 is\n                         # used, We need to explicitly make the shim track the\n                         # regularization losses as the collections will not be\n@@ -585,9 +581,7 @@ class Layer(base_layer.Layer):\n                 # Some classes which inherit from Layer do not use its\n                 # constructor, so rather than initializing to None we check for\n                 # an AttributeError.\n-                scope_context_manager = (\n-                    self._always_reuse_variable_scope\n-                )  # pylint: disable=access-member-before-definition\n+                scope_context_manager = self._always_reuse_variable_scope\n             except AttributeError:\n                 scope_context_manager = None\n \n@@ -654,9 +648,7 @@ class Layer(base_layer.Layer):\n     def __setattr__(self, value, name):\n         # By-pass the automatic dependency tracking performed by the parent\n         # Layer.\n-        super(tf.__internal__.tracking.Trackable, self).__setattr__(\n-            value, name\n-        )  # pylint: disable=bad-super-call\n+        super(tf.__internal__.tracking.Trackable, self).__setattr__(value, name)\n \n     @property\n     def _is_legacy_layer(self):\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # =============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Contains the convolutional layer classes and their functional aliases.\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # =============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Contains the core layers: Dense, Dropout.\n \n Also contains their functional aliases.\n\n@@ -402,7 +402,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     def testComputeOutputShape(self):\n         dense = core_layers.Dense(2, activation=tf.nn.relu, name=\"dense1\")\n         ts = tf.TensorShape\n-        # pylint: disable=protected-access\n+\n         with self.assertRaises(ValueError):\n             dense.compute_output_shape(ts(None))\n         with self.assertRaises(ValueError):\n@@ -418,7 +418,6 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(\n             [None, 4, 2], dense.compute_output_shape(ts([None, 4, 3])).as_list()\n         )\n-        # pylint: enable=protected-access\n \n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n@@ -436,9 +435,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n \n \n def _get_variable_dict_from_varstore():\n-    var_dict = (\n-        variable_scope._get_default_variable_store()._vars\n-    )  # pylint: disable=protected-access\n+    var_dict = variable_scope._get_default_variable_store()._vars\n     sorted_var_dict = collections.OrderedDict(\n         sorted(var_dict.items(), key=lambda t: t[0])\n     )\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # =============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Contains the normalization layer classes and their functional aliases.\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # =============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Contains the pooling layer classes and their functional aliases.\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # =============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Contains a shim to allow using TF1 get_variable code in TF2.\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n@@ -326,7 +326,7 @@ class _EagerVariableStore(tf.Module):\n         # it to custom_getter.\n         # Note: the parameters of _true_getter, and their documentation, match\n         # *exactly* item-for-item with the docstring of this method.\n-        def _true_getter(  # pylint: disable=missing-docstring\n+        def _true_getter(\n             name,\n             shape=None,\n             dtype=tf.float32,\n@@ -334,11 +334,11 @@ class _EagerVariableStore(tf.Module):\n             regularizer=None,\n             reuse=None,\n             trainable=None,\n-            collections=None,  # pylint: disable=unused-argument\n+            collections=None,\n             caching_device=None,\n             partitioner=None,\n             validate_shape=True,\n-            use_resource=None,  # pylint: disable=unused-argument\n+            use_resource=None,\n             constraint=None,\n             synchronization=tf.VariableSynchronization.AUTO,\n             aggregation=tf.compat.v1.VariableAggregation.NONE,\n@@ -502,7 +502,7 @@ class _EagerVariableStore(tf.Module):\n             return found_var\n \n         # The code below handles only the case of creating a new variable.\n-        if reuse is True:  # pylint: disable=g-bool-id-comparison\n+        if reuse is True:\n             raise ValueError(\n                 \"Variable %s does not exist, or was not created with \"\n                 \"tf.get_variable(). Did you mean to set \"\n@@ -827,13 +827,9 @@ def track_tf1_style_variables(method):\n                     \"does not extend Module, Layer, or Model.\".format(self)\n                 )\n             var_store = _EagerVariableStore()\n-            self._tf1_style_var_store = (\n-                var_store  # pylint: disable=protected-access\n-            )\n+            self._tf1_style_var_store = var_store\n \n-        existing_regularized_variables = set(\n-            var_store._regularizers.keys()\n-        )  # pylint: disable=protected-access\n+        existing_regularized_variables = set(var_store._regularizers.keys())\n         with var_store.scope():\n             out = method(self, *args, **kwargs)\n \n@@ -843,9 +839,7 @@ def track_tf1_style_variables(method):\n             for (\n                 var_name,\n                 regularizer,\n-            ) in (\n-                var_store._regularizers.items()\n-            ):  # pylint: disable=protected-access\n+            ) in var_store._regularizers.items():\n                 if var_name not in existing_regularized_variables:\n                     self.add_loss(regularizer)\n \n@@ -1078,7 +1072,7 @@ def get_or_create_layer(name, create_layer_method):\n     Returns:\n       The created layer.\n     \"\"\"\n-    store = vs._get_default_variable_store()  # pylint: disable=protected-access\n+    store = vs._get_default_variable_store()\n     if not isinstance(store, _EagerVariableStore):\n         if not tf.compat.v1.executing_eagerly_outside_functions():\n             # tf1 case; just create and return layer\n\n@@ -961,7 +961,7 @@ class VariableScopeModule(tf.Module):\n         return {\n             name: regularizer()\n             for name, regularizer in self._tf1_style_var_store._regularizers.items()  # noqa: E501\n-        }  # pylint: disable=protected-access\n+        }\n \n \n @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n@@ -1151,7 +1151,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n                 return {\n                     name: regularizer()\n                     for name, regularizer in self._variable_store._regularizers.items()  # noqa: E501\n-                }  # pylint: disable=protected-access\n+                }\n \n             def __call__(self, inputs, training=None):\n                 with self._variable_store.scope():\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Built-in loss functions.\"\"\"\n \n \n@@ -273,7 +273,7 @@ class LossFunctionWrapper(Loss):\n                 backend.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n             )\n \n-        if saving_lib._ENABLED:  # pylint: disable=protected-access\n+        if saving_lib._ENABLED:\n             config[\"fn\"] = generic_utils.get_registered_name(self.fn)\n \n         base_config = super().get_config()\n@@ -289,7 +289,7 @@ class LossFunctionWrapper(Loss):\n         Returns:\n             A `keras.losses.Loss` instance.\n         \"\"\"\n-        if saving_lib._ENABLED:  # pylint: disable=protected-access\n+        if saving_lib._ENABLED:\n             fn_name = config.pop(\"fn\", None)\n             if fn_name and cls is LossFunctionWrapper:\n                 config[\"fn\"] = get(fn_name)\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"All Keras metrics.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n # Utilities\n # Base classes\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-doc-return-or-yield\n+\n+\n \"\"\"Base Metric classes.\"\"\"\n \n import abc\n@@ -187,14 +187,12 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n             ):\n                 update_op = None\n             else:\n-                update_op = self.update_state(\n-                    *args, **kwargs\n-                )  # pylint: disable=not-callable\n+                update_op = self.update_state(*args, **kwargs)\n             update_ops = []\n             if update_op is not None:\n                 update_ops.append(update_op)\n             with tf.control_dependencies(update_ops):\n-                result_t = self.result()  # pylint: disable=not-callable\n+                result_t = self.result()\n \n                 # We are adding the metric object as metadata on the result\n                 # tensor.  This is required when we want to use a metric with\n@@ -205,11 +203,11 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n                 #   model = Model()\n                 #   mean = Mean()\n                 #   model.add_metric(mean(values), name='mean')\n-                result_t._metric_obj = self  # pylint: disable=protected-access\n+                result_t._metric_obj = self\n                 return result_t\n \n         from keras.distribute import (\n-            distributed_training_utils,  # pylint:disable=g-import-not-at-top\n+            distributed_training_utils,\n         )\n \n         return distributed_training_utils.call_replica_local_fn(\n@@ -705,9 +703,7 @@ class MeanMetricWrapper(Mean):\n     def get_config(self):\n         config = {}\n \n-        if (\n-            type(self) is MeanMetricWrapper\n-        ):  # pylint: disable=unidiomatic-typecheck\n+        if type(self) is MeanMetricWrapper:\n             # Only include function argument when the object is a\n             # MeanMetricWrapper and not a subclass.\n             config[\"fn\"] = self._fn\n@@ -719,7 +715,7 @@ class MeanMetricWrapper(Mean):\n \n     @classmethod\n     def from_config(cls, config):\n-        from keras.metrics import get  # pylint: disable=g-import-not-at-top\n+        from keras.metrics import get\n \n         # Note that while MeanMetricWrapper itself isn't public, objects of this\n         # class may be created and added to the model by calling model.compile.\n@@ -788,9 +784,7 @@ class MeanTensor(Metric):\n         )\n         with tf.init_scope():\n             if not tf.executing_eagerly():\n-                backend._initialize_variables(\n-                    backend._get_session()\n-                )  # pylint: disable=protected-access\n+                backend._initialize_variables(backend._get_session())\n         self._built = True\n \n     @property\n\n@@ -625,9 +625,7 @@ class CustomMetricsTest(tf.test.TestCase):\n             ]\n         )\n \n-        update_op = btp_obj.update_state(\n-            y_true, y_pred\n-        )  # pylint: disable=assignment-from-no-return\n+        update_op = btp_obj.update_state(y_true, y_pred)\n         self.evaluate(update_op)\n         result = btp_obj.result()\n         self.assertEqual(7, self.evaluate(result))\n@@ -777,14 +775,10 @@ class CustomMetricsTest(tf.test.TestCase):\n         y = layers.Dense(3)(x)\n         model = training_module.Model(x, y)\n \n-        def bad_metric(\n-            y_true, y_pred, sample_weight=None\n-        ):  # pylint: disable=unused-argument\n+        def bad_metric(y_true, y_pred, sample_weight=None):\n             return None\n \n-        def dict_metric(\n-            y_true, y_pred, sample_weight=None\n-        ):  # pylint: disable=unused-argument\n+        def dict_metric(y_true, y_pred, sample_weight=None):\n             return {\"value\": 0.0}\n \n         with self.assertRaisesRegex(\n\n@@ -1615,7 +1615,7 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n \n     def test_extra_dims(self):\n         try:\n-            from scipy import special  # pylint: disable=g-import-not-at-top\n+            from scipy import special\n \n             self.setup()\n             logits = special.expit(\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-doc-return-or-yield\n+\n+\n \"\"\"Built-in metrics.\"\"\"\n \n import abc\n@@ -1866,9 +1866,7 @@ class AUC(base_metric.Metric):\n                 # AUC should be initialized outside of any tf.functions, and\n                 # therefore in eager mode.\n                 if not tf.executing_eagerly():\n-                    backend._initialize_variables(\n-                        backend._get_session()\n-                    )  # pylint: disable=protected-access\n+                    backend._initialize_variables(backend._get_session())\n \n         self._built = True\n \n\n@@ -28,9 +28,9 @@ _autocast_dtype = threading.local()\n def numpy_text(tensor, is_repr=False):\n     \"\"\"Human readable representation of a tensor's numpy value.\"\"\"\n     if tensor.dtype.is_numpy_compatible:\n-        # pylint: disable=protected-access\n+\n         text = repr(tensor._numpy()) if is_repr else str(tensor._numpy())\n-        # pylint: enable=protected-access\n+\n     else:\n         text = \"<unprintable>\"\n     if \"\\n\" in text:\n@@ -231,7 +231,7 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n                 # 'op' attribute is defined. This matches the behavior of\n                 # tf.Variable.assign.\n                 var = create_autocast_variable(self._variable)\n-                var._op = assign_op  # pylint:disable=protected-access\n+                var._op = assign_op\n                 return var\n             return assign_op\n \n@@ -330,7 +330,7 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n \n     @property\n     def _shared_name(self):\n-        return self._variable._shared_name  # pylint:disable=protected-access\n+        return self._variable._shared_name\n \n     @property\n     def initializer(self):\n@@ -347,9 +347,7 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         return self._op\n \n     def _as_graph_element(self):\n-        graph_element = (\n-            self._variable._as_graph_element()\n-        )  # pylint:disable=protected-access\n+        graph_element = self._variable._as_graph_element()\n         if graph_element is None:\n             return self._op\n         return graph_element\n@@ -370,16 +368,12 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         # AutoCastVariables are identical to checkpoints with normal variables.\n         # Therefore models checkpointed with AutoCastVariables can be restored\n         # on models with normal variables, and vice versa.\n-        return (\n-            self._variable._gather_saveables_for_checkpoint()\n-        )  # pylint:disable=protected-access\n+        return self._variable._gather_saveables_for_checkpoint()\n \n     def _map_resources(self, save_options):\n         # By delegating this method to the wrapped variable, SavedModel with\n         # AutoCastVariables are identical to SavedModel with normal variables.\n-        obj_map, resource_map = self._variable._map_resources(\n-            save_options\n-        )  # pylint:disable=protected-access\n+        obj_map, resource_map = self._variable._map_resources(save_options)\n         obj_map[self] = obj_map[self._variable]\n         return obj_map, resource_map\n \n@@ -401,25 +395,19 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n     # private attributes is hacky and difficult to maintain.\n     @property\n     def _handle_name(self):\n-        return self._variable._handle_name  # pylint: disable=protected-access\n+        return self._variable._handle_name\n \n     @_handle_name.setter\n     def _handle_name(self, handle_name):\n-        self._variable._handle_name = (\n-            handle_name  # pylint: disable=protected-access\n-        )\n+        self._variable._handle_name = handle_name\n \n     @property\n     def _initializer_op(self):\n-        return (\n-            self._variable._initializer_op\n-        )  # pylint: disable=protected-access\n+        return self._variable._initializer_op\n \n     @_initializer_op.setter\n     def _initializer_op(self, initializer_op):\n-        self._variable._initializer_op = (\n-            initializer_op  # pylint: disable=protected-access\n-        )\n+        self._variable._initializer_op = initializer_op\n \n     # Operator overloads:\n     # Note we only overload operators that support floating-point types, as\n@@ -485,7 +473,7 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         return pow(o, self.read_value())\n \n     def __neg__(self):\n-        return -self.read_value()  # pylint: disable=invalid-unary-operand-type\n+        return -self.read_value()\n \n     def __abs__(self):\n         return abs(self.read_value())\n@@ -522,12 +510,10 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n             # https://docs.python.org/3/library/constants.html#NotImplemented\n             return NotImplemented\n \n-    # pylint: enable=multiple-statements\n-\n \n tf.register_tensor_conversion_function(\n     AutoCastVariable, AutoCastVariable._dense_var_to_tensor\n-)  # pylint:disable=protected-access\n+)\n \n \n def create_autocast_variable(variable):\n@@ -558,18 +544,16 @@ def create_autocast_variable(variable):\n \n         def __repr__(self):\n \n-            # pylint: disable=missing-format-attribute\n             return (\n                 \"<AutoCastDistributedVariable dtype={v.dtype.name} \"\n                 \"dtype_to_cast_to={v._cast_dtype.name} \"\n                 \"inner_variable={v._variable}>\"\n             ).format(v=self)\n-            # pylint: enable=missing-format-attribute\n \n     return AutoCastDistributedVariable(variable)\n \n \n-class enable_auto_cast_variables:  # pylint:disable=invalid-name\n+class enable_auto_cast_variables:\n     \"\"\"Context manager which enables the autocasting of `AutoCastVariable`s.\n \n     Under this context manager, `AutoCastVariable`s will be cast to `dtype` if\n\n@@ -165,9 +165,7 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n                         self.assertIsInstance(\n                             var, autocast_variable.AutoCastVariable\n                         )\n-                        self.assertEqual(\n-                            tf.identity(var).dtype, read_dtype\n-                        )  # pylint: disable=cell-var-from-loop\n+                        self.assertEqual(tf.identity(var).dtype, read_dtype)\n                         return self.evaluate(var)\n \n                 x = get_var(7.0, tf.float32)\n@@ -444,7 +442,7 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n             # AutoCastVariable.\n             if tf.executing_eagerly():\n                 with self.assertRaises(AttributeError):\n-                    x.op  # pylint: disable=pointless-statement\n+                    x.op\n                 self.assertIsNone(x.assign(1.0).op)\n                 self.assertIsNone(x.assign_add(1.0).op)\n                 self.assertIsNone(x.assign_sub(1.0).op)\n\n@@ -183,7 +183,7 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n             graph_key = None\n         else:\n             graph = tf.compat.v1.get_default_graph()\n-            graph_key = graph._graph_key  # pylint: disable=protected-access\n+            graph_key = graph._graph_key\n \n         key = (name, graph_key)\n         self._weights[key] = variable\n@@ -197,7 +197,7 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n             graph_key = None\n         else:\n             graph = tf.compat.v1.get_default_graph()\n-            graph_key = graph._graph_key  # pylint: disable=protected-access\n+            graph_key = graph._graph_key\n         weights = {}\n         for (name, g), v in sorted(\n             self._weights.items(), key=lambda i: i[0][0]\n@@ -216,7 +216,7 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n             graph_key = None\n         else:\n             graph = tf.compat.v1.get_default_graph()\n-            graph_key = graph._graph_key  # pylint: disable=protected-access\n+            graph_key = graph._graph_key\n         return self._weights.get((name, graph_key), None)\n \n     @property\n@@ -356,7 +356,8 @@ class LossScaleOptimizerMetaclass(type):\n \n \n # TODO(b/215389169): Delete this class after `OptimizerV2` is deprecated.\n-# pylint: disable=g-classes-have-attributes\n+\n+\n @keras_export(\"keras.mixed_precision.LossScaleOptimizer\")\n class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n     \"\"\"An optimizer that applies loss scaling to prevent numeric underflow.\n@@ -585,7 +586,6 @@ class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n         raise NotImplementedError\n \n \n-# pylint: disable=g-classes-have-attributes\n class LossScaleOptimizer(\n     tf.__internal__.tracking.DelegatingTrackableMixin,\n     optimizer_v2.OptimizerV2,\n@@ -774,9 +774,7 @@ class LossScaleOptimizer(\n         return self.get_unscaled_gradients(grads)\n \n     def _create_all_weights(self, var_list):\n-        self._optimizer._create_all_weights(\n-            var_list\n-        )  # pylint: disable=protected-access\n+        self._optimizer._create_all_weights(var_list)\n \n     def apply_gradients(\n         self, grads_and_vars, name=None, experimental_aggregate_gradients=True\n@@ -806,7 +804,6 @@ class LossScaleOptimizer(\n             grads_and_vars = self._optimizer._aggregate_gradients(\n                 grads_and_vars\n             )\n-            # pylint: enable=protected-access\n \n         grads_and_vars = tuple(grads_and_vars)\n         grads = [g for g, _ in grads_and_vars]\n@@ -911,11 +908,7 @@ class LossScaleOptimizer(\n                 loss_scale, tf.compat.v1.mixed_precision.FixedLossScale\n             ):\n                 config[\"dynamic\"] = False\n-                config[\n-                    \"initial_scale\"\n-                ] = (\n-                    loss_scale._loss_scale_value\n-                )  # pylint: disable=protected-access\n+                config[\"initial_scale\"] = loss_scale._loss_scale_value\n             elif isinstance(\n                 loss_scale, tf.compat.v1.mixed_precision.DynamicLossScale\n             ):\n@@ -993,14 +986,12 @@ class LossScaleOptimizer(\n         self._optimizer.clipvalue = val\n \n     def _aggregate_gradients(self, grads_and_vars):\n-        return self._optimizer._aggregate_gradients(\n-            grads_and_vars\n-        )  # pylint: disable=protected-access\n+        return self._optimizer._aggregate_gradients(grads_and_vars)\n \n     def _restore_slot_variable(self, slot_name, variable, slot_variable):\n         return self._optimizer._restore_slot_variable(\n             slot_name,\n-            variable,  # pylint: disable=protected-access\n+            variable,\n             slot_variable,\n         )\n \n@@ -1478,9 +1469,7 @@ def _create_loss_scale_optimizer_from_v1_loss_scale(optimizer, loss_scale):\n             optimizer, dynamic=False, initial_scale=loss_scale\n         )\n     elif isinstance(loss_scale, tf.compat.v1.mixed_precision.FixedLossScale):\n-        ls_val = (\n-            loss_scale._loss_scale_value\n-        )  # pylint: disable=protected-access\n+        ls_val = loss_scale._loss_scale_value\n         return LossScaleOptimizer(\n             optimizer, dynamic=False, initial_scale=ls_val\n         )\n\n@@ -340,7 +340,6 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n         self.assertEqual(self.evaluate(opt.loss_scale), 2**15)\n \n-    # pylint: disable=cell-var-from-loop\n     @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n     def testClipping(self, opt_cls, strategy_fn, use_tf_function):\n         strategy = strategy_fn()\n@@ -394,8 +393,6 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 )  # Var does not change\n                 self.assertEqual(self.evaluate(opt.loss_scale), 4)\n \n-    # pylint: enable=cell-var-from-loop\n-\n     @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n     def testDynamicUpdate(self, opt_cls, strategy_fn, use_tf_function):\n         with strategy_fn().scope() as strategy:\n@@ -639,7 +636,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             opt = adam.Adam(learning_rate=1.0, beta_1=0.5, beta_2=0.9)\n             lso = loss_scale_optimizer.LossScaleOptimizer(opt)\n             # Force hyperparameters to be created\n-            opt.lr  # pylint: disable=pointless-statement\n+            opt.lr\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n \n             self.assertEqual(self.evaluate(lso.beta_1), 0.5)\n@@ -684,7 +681,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             AttributeError,\n             \"'LossScaleOptimizer(V3)?' object has no attribute 'nesterov'\",\n         ):\n-            lso.nesterov  # pylint: disable=pointless-statement\n+            lso.nesterov\n \n         lso.nesterov = True\n         self.assertTrue(lso.nesterov)\n@@ -791,7 +788,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             opt = create_lso(opt)\n \n             # Force hyperparameters to be created\n-            opt.learning_rate  # pylint: disable=pointless-statement\n+            opt.learning_rate\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n \n             self.assertEqual(self.evaluate(opt.learning_rate), 1.0)\n@@ -1002,7 +999,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             opt = loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n \n         # Force hyperparameters to be created\n-        opt.learning_rate  # pylint: disable=pointless-statement\n+        opt.learning_rate\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n \n         # Test attributes on the optimizer\n@@ -1073,7 +1070,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             opt = loss_scale_optimizer.LossScaleOptimizer.from_config(config)\n \n         # Force hyperparameters to be created\n-        opt.learning_rate  # pylint: disable=pointless-statement\n+        opt.learning_rate\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n \n         # Test attributes on the optimizer\n@@ -1153,7 +1150,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             config = optimizers.serialize(opt)\n         opt = optimizers.deserialize(config)\n         # Force hyperparameters to be created\n-        opt.learning_rate  # pylint: disable=pointless-statement\n+        opt.learning_rate\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n \n         self.assertEqual(self.evaluate(opt.learning_rate), 2.0)\n@@ -1195,7 +1192,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         custom_objects = {\"MySGD\": MySGD}\n         opt = optimizers.deserialize(config, custom_objects=custom_objects)\n         # Force hyperparameters to be created\n-        opt.learning_rate  # pylint: disable=pointless-statement\n+        opt.learning_rate\n         self.evaluate(tf.compat.v1.global_variables_initializer())\n \n         self.assertEqual(self.evaluate(opt.learning_rate), 2.0)\n\n@@ -27,7 +27,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.mixed_precision.Policy\", v1=[])\n class Policy:\n     \"\"\"A dtype policy for a Keras layer.\n@@ -491,7 +490,7 @@ def _policy_equivalent_to_dtype(policy):\n     \"\"\"\n     # We use type() instead of isinstance because a subclass of Policy is never\n     # equivalent to a dtype.\n-    return type(policy) == Policy and (  # pylint: disable=unidiomatic-typecheck\n+    return type(policy) == Policy and (\n         policy.name == \"_infer\" or _is_convertible_to_dtype(policy.name)\n     )\n \n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras models API.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n from keras.engine.functional import Functional\n from keras.engine.sequential import Sequential\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Code for model cloning, plus model-related API entries.\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -37,8 +37,8 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n # API entries importable from `keras.models`:\n-Model = training.Model  # pylint: disable=invalid-name\n-Sequential = sequential.Sequential  # pylint: disable=invalid-name\n+Model = training.Model\n+Sequential = sequential.Sequential\n \n \n # Callable used to clone a layer with weights preserved.\n@@ -645,7 +645,7 @@ def _reset_build_compile_trackers(model):\n     model.inputs = None\n     model.outputs = None\n     # Reset compile state\n-    model._is_compiled = False  # pylint:disable=protected-access\n+    model._is_compiled = False\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n         model._v1_compile_was_called = False\n     model.optimizer = None\n@@ -750,9 +750,7 @@ def clone_and_build_model(\n         )\n \n     if compile_clone:\n-        compile_args = (\n-            model._get_compile_args()\n-        )  # pylint: disable=protected-access\n+        compile_args = model._get_compile_args()\n         # Allows this method to be robust to switching graph and eager classes.\n         model._get_compile_args = lambda: compile_args\n \n\n@@ -26,8 +26,6 @@ from keras.utils import generic_utils\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.models.experimental.SharpnessAwareMinimization\", v1=[])\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=g-bad-import-order\n+\n+\n \"\"\"Built-in optimizer classes.\n \n For more examples see the base class `tf.keras.optimizers.Optimizer`.\n@@ -120,7 +120,7 @@ def deserialize(config, custom_objects=None):\n     # loss_scale_optimizer has a direct dependency of optimizer, import here\n     # rather than top to avoid the cyclic dependency.\n     from keras.mixed_precision import (\n-        loss_scale_optimizer,  # pylint: disable=g-import-not-at-top\n+        loss_scale_optimizer,\n     )\n \n     all_classes = {\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adadelta\", v1=[])\n class Adadelta(optimizer.Optimizer):\n\n@@ -24,7 +24,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adagrad\", v1=[])\n class Adagrad(optimizer.Optimizer):\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adam\", v1=[])\n class Adam(optimizer.Optimizer):\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Adamax\", v1=[])\n class Adamax(optimizer.Optimizer):\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.AdamW\", v1=[])\n class AdamW(optimizer.Optimizer):\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Ftrl\", v1=[])\n class Ftrl(optimizer.Optimizer):\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.Nadam\", v1=[])\n class Nadam(optimizer.Optimizer):\n\n@@ -120,7 +120,7 @@ class _BaseOptimizer(tf.Module):\n         # Get the distributed variable if it exists.\n         # TODO(b/199214315): replace _unique_id with ref() after fixing ref()\n         # issues on AggregatingVariable.\n-        return variable._unique_id  # pylint: disable=protected-access\n+        return variable._unique_id\n \n     @abc.abstractmethod\n     def update_step(self, gradient, variable):\n@@ -625,7 +625,6 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n       **kwargs: keyword arguments only used for backward compatibility.\"\"\"\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.experimental.Optimizer\", v1=[])\n class Optimizer(_BaseOptimizer):\n     \"\"\"Abstract optimizer base class.\n@@ -837,7 +836,7 @@ class Optimizer(_BaseOptimizer):\n \n     def _var_key(self, variable):\n         \"\"\"Get a unique identifier of the given variable.\"\"\"\n-        # pylint: disable=protected-access\n+\n         # Get the distributed variable if it exists.\n         # TODO(b/197554203): replace _distributed_container() with a public api.\n         if hasattr(variable, \"_distributed_container\"):\n\n@@ -25,7 +25,7 @@ STRATEGIES = [\n \n adadelta_fn = tf.__internal__.test.combinations.NamedObject(\n     \"adadelta\",\n-    lambda: adadelta.Adadelta(  # pylint: disable=g-long-lambda\n+    lambda: adadelta.Adadelta(\n         0.002, use_ema=True, ema_overwrite_frequency=None\n     ),\n )\n@@ -52,9 +52,7 @@ rmsprop_fn = tf.__internal__.test.combinations.NamedObject(\n )\n sgd_fn = tf.__internal__.test.combinations.NamedObject(\n     \"sgdaverage\",\n-    lambda: sgd.SGD(  # pylint: disable=g-long-lambda\n-        0.002, use_ema=True, ema_overwrite_frequency=1\n-    ),\n+    lambda: sgd.SGD(0.002, use_ema=True, ema_overwrite_frequency=1),\n )\n \n OPTIMIZER_FN = [\n\n@@ -46,7 +46,7 @@ STRATEGIES = [\n \n adadelta_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladadelta\",\n-    lambda: adadelta_new.Adadelta(  # pylint: disable=g-long-lambda\n+    lambda: adadelta_new.Adadelta(\n         0.002, use_ema=True, ema_overwrite_frequency=None\n     ),\n )\n@@ -73,9 +73,7 @@ rmsprop_new_fn = tf.__internal__.test.combinations.NamedObject(\n )\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalsgdaverage\",\n-    lambda: sgd_new.SGD(  # pylint: disable=g-long-lambda\n-        0.002, use_ema=True, ema_overwrite_frequency=1\n-    ),\n+    lambda: sgd_new.SGD(0.002, use_ema=True, ema_overwrite_frequency=1),\n )\n \n OPTIMIZER_FN = [\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.RMSprop\", v1=[])\n class RMSprop(optimizer.Optimizer):\n\n@@ -23,7 +23,6 @@ from keras.utils import generic_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export(\"keras.optimizers.experimental.SGD\", v1=[])\n class SGD(optimizer.Optimizer):\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=g-classes-have-attributes\n+\n+\n \"\"\"Legacy v1 optimizer classes.\n \n For more examples see the base class `tf.compat.v1.keras.optimizers.Optimizer`.\n@@ -836,9 +836,7 @@ class Nadam(Optimizer):\n class TFOptimizer(Optimizer, tf.__internal__.tracking.Trackable):\n     \"\"\"Wrapper class for native TensorFlow optimizers.\"\"\"\n \n-    def __init__(\n-        self, optimizer, iterations=None\n-    ):  # pylint: disable=super-init-not-called\n+    def __init__(self, optimizer, iterations=None):\n         self.optimizer = optimizer\n         self._track_trackable(optimizer, name=\"optimizer\")\n         if iterations is None:\n\n@@ -23,10 +23,7 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n \n-\n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.Adadelta\")\n class Adadelta(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n\n@@ -52,7 +52,7 @@ class AdadeltaOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                             learning_rate=lambda: lr,\n                             rho=lambda: rho,\n                             epsilon=epsilon,\n-                        )  # pylint: disable=cell-var-from-loop\n+                        )\n                     else:\n                         adadelta_opt = adadelta.Adadelta(\n                             learning_rate=lr, rho=rho, epsilon=epsilon\n@@ -178,7 +178,7 @@ class AdadeltaOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 def loss():\n                     pred = tf.matmul(\n                         tf.compat.v1.nn.embedding_lookup([var0], [0]), x\n-                    )  # pylint: disable=cell-var-from-loop\n+                    )\n                     return pred * pred\n \n                 sgd_op = adadelta.Adadelta(1.0, 1.0, 1.0).minimize(\n\n@@ -23,10 +23,7 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n \n-\n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.Adagrad\")\n class Adagrad(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adagrad algorithm.\n\n@@ -257,7 +257,7 @@ class AdagradOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 def loss():\n                     pred = tf.matmul(\n                         tf.compat.v1.nn.embedding_lookup([var0], [0]), x\n-                    )  # pylint: disable=cell-var-from-loop\n+                    )\n                     return pred * pred\n \n                 sgd_op = adagrad.Adagrad(1.0).minimize(loss, var_list=[var0])\n@@ -463,18 +463,13 @@ class AdagradOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         with tf.Graph().as_default():\n             for dtype in _DATA_TYPES:\n                 var_repeated = tf.Variable([1.0, 2.0], dtype=dtype)\n-                loss_repeated = (\n-                    lambda: tf.reduce_sum(  # pylint: disable=g-long-lambda\n+                loss_repeated = lambda: tf.reduce_sum(\n                     tf.compat.v1.nn.embedding_lookup(var_repeated, [0, 0])\n                 )\n-                )  # pylint: disable=cell-var-from-loop\n                 var_aggregated = tf.Variable([1.0, 2.0], dtype=dtype)\n-                loss_aggregated = (\n-                    lambda: 2\n-                    * tf.reduce_sum(  # pylint: disable=g-long-lambda\n+                loss_aggregated = lambda: 2 * tf.reduce_sum(\n                     tf.compat.v1.nn.embedding_lookup(var_aggregated, [0])\n                 )\n-                )  # pylint: disable=cell-var-from-loop\n                 update_op_repeated = adagrad.Adagrad(2.0).minimize(\n                     loss_repeated, var_list=[var_repeated]\n                 )\n\n@@ -23,7 +23,6 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.Adam\")\n class Adam(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adam algorithm.\n\n@@ -167,9 +167,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 # placed on it (i.e. they have GPU kernels).\n                 var = tf.Variable([[1.0], [2.0]])\n                 indices = tf.constant([0, 1], dtype=index_dtype)\n-                g_sum = lambda: tf.reduce_sum(\n-                    tf.gather(var, indices)\n-                )  # pylint: disable=cell-var-from-loop\n+                g_sum = lambda: tf.reduce_sum(tf.gather(var, indices))\n                 optimizer = adam.Adam(3.0)\n                 minimize_op = optimizer.minimize(g_sum, var_list=[var])\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n@@ -738,9 +736,7 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 # placed on it (i.e. they have GPU kernels).\n                 var = tf.Variable([[1.0], [2.0]])\n                 indices = tf.constant([0, 1], dtype=index_dtype)\n-                g_sum = lambda: tf.reduce_sum(\n-                    tf.gather(var, indices)\n-                )  # pylint: disable=cell-var-from-loop\n+                g_sum = lambda: tf.reduce_sum(tf.gather(var, indices))\n                 optimizer = adam.NonFusedAdam(3.0)\n                 minimize_op = optimizer.minimize(g_sum, var_list=[var])\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n\n@@ -23,7 +23,6 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.Adamax\")\n class Adamax(optimizer_v2.OptimizerV2):\n     \"\"\"Optimizer that implements the Adamax algorithm.\n\n@@ -68,9 +68,7 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         for dtype in [tf.half, tf.float32, tf.float64]:\n             with tf.Graph().as_default(), self.cached_session():\n                 # Initialize variables for numpy implementation.\n-                zero_slots = lambda: np.zeros(\n-                    (3), dtype=dtype.as_numpy_dtype\n-                )  # pylint: disable=cell-var-from-loop\n+                zero_slots = lambda: np.zeros((3), dtype=dtype.as_numpy_dtype)\n                 m0, v0, m1, v1 = (\n                     zero_slots(),\n                     zero_slots(),\n@@ -137,9 +135,7 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 # placed on it (i.e. they have GPU kernels).\n                 var = tf.Variable([[1.0], [2.0]])\n                 indices = tf.constant([0, 1], dtype=index_dtype)\n-                g_sum = lambda: tf.reduce_sum(\n-                    tf.gather(var, indices)\n-                )  # pylint: disable=cell-var-from-loop\n+                g_sum = lambda: tf.reduce_sum(tf.gather(var, indices))\n                 optimizer = adamax.Adamax(3.0)\n                 minimize_op = optimizer.minimize(g_sum, var_list=[var])\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n\n@@ -13,8 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Ftrl-proximal optimizer implementation.\"\"\"\n-# pylint: disable=g-bad-import-order\n-# pylint: disable=g-classes-have-attributes\n+\n \n import tensorflow.compat.v2 as tf\n \n@@ -24,7 +23,6 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.Ftrl\")\n class Ftrl(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n\n@@ -111,7 +111,7 @@ class FtrlOptimizerTest(tf.test.TestCase):\n                 def loss():\n                     pred = tf.matmul(\n                         tf.compat.v1.nn.embedding_lookup([var0], [0]), x\n-                    )  # pylint: disable=cell-var-from-loop\n+                    )\n                     return pred * pred\n \n                 sgd_op = ftrl.Ftrl(1.0).minimize(loss, var_list=[var0])\n\n@@ -13,8 +13,8 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"SGD optimizer implementation.\"\"\"\n-# pylint: disable=g-bad-import-order\n-# pylint: disable=g-classes-have-attributes\n+\n+\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_v2 import optimizer_v2\n@@ -23,7 +23,6 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.SGD\")\n class SGD(optimizer_v2.OptimizerV2):\n     r\"\"\"Gradient descent (with momentum) optimizer.\n\n@@ -145,9 +145,7 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n             var1 = tf.Variable([3.0], dtype=dtype)\n             x = tf.constant([[4.0], [5.0]], dtype=dtype)\n-            loss = (\n-                lambda: tf.matmul(var0, x) + var1\n-            )  # pylint: disable=cell-var-from-loop\n+            loss = lambda: tf.matmul(var0, x) + var1\n             sgd = gradient_descent.SGD(1.0)\n             sgd_op = sgd.minimize(loss, [var0, var1])\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n@@ -170,8 +168,8 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 def loss():\n                     pred = tf.matmul(\n                         tf.compat.v1.nn.embedding_lookup([var0], [0]), x\n-                    )  # pylint: disable=cell-var-from-loop\n-                    pred += var1  # pylint: disable=cell-var-from-loop\n+                    )\n+                    pred += var1\n                     return pred * pred\n \n                 sgd_op = gradient_descent.SGD(1.0).minimize(loss, [var0, var1])\n@@ -217,9 +215,7 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 opt = gradient_descent.SGD(3.0)\n                 values = [1.0, 3.0]\n                 vars_ = [tf.Variable([v], dtype=dtype) for v in values]\n-                loss = (\n-                    lambda: vars_[0] + vars_[1]\n-                )  # pylint: disable=cell-var-from-loop\n+                loss = lambda: vars_[0] + vars_[1]\n                 grads_and_vars = opt._compute_gradients(loss, vars_)\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n                 for grad, _ in grads_and_vars:\n@@ -435,9 +431,7 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n                 accum0_np = np.array([0.0, 0.0], dtype=dtype.as_numpy_dtype)\n                 accum1_np = np.array([0.0, 0.0], dtype=dtype.as_numpy_dtype)\n-                loss = (\n-                    lambda: 5 * var0 * var0 + 3 * var1\n-                )  # pylint: disable=cell-var-from-loop\n+                loss = lambda: 5 * var0 * var0 + 3 * var1\n                 mom_op = gradient_descent.SGD(\n                     learning_rate=2.0, momentum=0.9, nesterov=True\n                 )\n@@ -507,7 +501,6 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             for dtype in [tf.half, tf.float32, tf.float64]:\n                 var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n \n-                # pylint: disable=cell-var-from-loop\n                 def loss():\n                     x = tf.constant([[4.0], [5.0]], dtype=dtype)\n                     pred = tf.matmul(\n@@ -515,8 +508,6 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                     )\n                     return pred * pred\n \n-                # pylint: enable=cell-var-from-loop\n-\n                 opt = gradient_descent.SGD(learning_rate=1.0, momentum=0.9)\n                 sgd_op = opt.minimize(loss, [var0])\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n\n@@ -24,7 +24,6 @@ from keras.optimizers.schedules import learning_rate_schedule\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.Nadam\")\n class Nadam(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the NAdam algorithm.\n@@ -139,7 +138,7 @@ class Nadam(optimizer_v2.OptimizerV2):\n \n         apply_state[(var_device, var_dtype)] = dict(\n             lr_t=lr_t,\n-            neg_lr_t=-lr_t,  # pylint: disable=invalid-unary-operand-type\n+            neg_lr_t=-lr_t,\n             epsilon=tf.convert_to_tensor(self.epsilon, var_dtype),\n             beta_1_t=beta_1_t,\n             beta_2_t=beta_2_t,\n\n@@ -13,7 +13,6 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Version 2 of class Optimizer.\"\"\"\n-# pylint: disable=g-bad-name\n \n \n import abc\n@@ -820,9 +819,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 # If the current context is graph mode or any of the update ops\n                 # are symbolic then the step update should be carried out under\n                 # a graph context. (eager updates execute immediately)\n-                with backend._current_graph(\n-                    update_ops\n-                ).as_default():  # pylint: disable=protected-access\n+                with backend._current_graph(update_ops).as_default():\n                     with tf.control_dependencies([tf.group(update_ops)]):\n                         return self.iterations.assign_add(1, read_value=False)\n \n@@ -935,9 +932,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         sharded_vars = set()\n         for var in var_list:\n             if getattr(var, \"_sharded_container\", False):\n-                sharded_vars.add(\n-                    var._sharded_container()\n-                )  # pylint: disable=protected-access\n+                sharded_vars.add(var._sharded_container())\n \n         for sharded_var in sharded_vars:\n             sharded_key = _var_key(sharded_var)\n@@ -1058,7 +1053,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                         % (\n                             var._shared_name,\n                             slot_name,\n-                        ),  # pylint: disable=protected-access\n+                        ),\n                         dtype=var.dtype,\n                         trainable=False,\n                         initial_value=initial_value,\n@@ -1093,7 +1088,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         keys = set()\n         for var in var_list:\n             if isinstance(var, tf.distribute.DistributedValues):\n-                var_devices = var._devices  # pylint: disable=protected-access\n+                var_devices = var._devices\n             else:\n                 var_devices = [var.device]\n             var_dtype = var.dtype.base_dtype\n@@ -1652,7 +1647,6 @@ def _var_key(var):\n       the unique name of the variable.\n     \"\"\"\n \n-    # pylint: disable=protected-access\n     # Get the distributed variable if it exists.\n     if hasattr(var, \"_distributed_container\"):\n         var = var._distributed_container()\n\n@@ -64,9 +64,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with test_utils.use_gpu():\n                 var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n                 var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n-                loss = (\n-                    lambda: 5 * var0 + 3 * var1\n-                )  # pylint: disable=cell-var-from-loop\n+                loss = lambda: 5 * var0 + 3 * var1\n                 sgd = gradient_descent.SGD(3.0)\n \n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n@@ -91,9 +89,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n \n                 def loss():\n-                    return (\n-                        5 * var0 + 3 * var1\n-                    )  # pylint: disable=cell-var-from-loop\n+                    return 5 * var0 + 3 * var1\n \n                 sgd = gradient_descent.SGD(1.0)\n \n@@ -138,9 +134,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with test_utils.use_gpu():\n                 var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n                 var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n-                loss = (\n-                    lambda: 5 * var0 + 3 * var1\n-                )  # pylint: disable=cell-var-from-loop\n+                loss = lambda: 5 * var0 + 3 * var1\n                 grad_loss = tf.constant([42, -42], dtype=dtype)\n                 sgd = gradient_descent.SGD(3.0)\n \n@@ -172,7 +166,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with test_utils.use_gpu():\n                 var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n                 var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n-                loss = lambda: 5 * var0  # pylint: disable=cell-var-from-loop\n+                loss = lambda: 5 * var0\n                 sgd_op = gradient_descent.SGD(3.0)\n                 with self.assertRaisesRegex(ValueError, \"No gradients\"):\n                     # var1 has no gradient\n@@ -216,9 +210,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with test_utils.use_gpu():\n                 var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n                 var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n-                loss = (\n-                    lambda: 5 * var0 + 3 * var1\n-                )  # pylint: disable=cell-var-from-loop\n+                loss = lambda: 5 * var0 + 3 * var1\n \n                 sgd = gradient_descent.SGD(3.0)\n                 grads_and_vars = sgd._compute_gradients(loss, [var0, var1])\n@@ -811,9 +803,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             # Simulate an all-reduce where the other replica has zeros for\n             # gradients, by dividing each gradient by 2.\n             grads = [g for g, _ in grads_and_vars]\n-            vars = [\n-                v for _, v in grads_and_vars\n-            ]  # pylint: disable=redefined-builtin\n+            vars = [v for _, v in grads_and_vars]\n             all_reduced_grads = [g / 2 for g in grads]\n             return list(zip(all_reduced_grads, vars))\n \n@@ -834,9 +824,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 # Simulate an all-reduce where the other replica has zeros for\n                 # gradients, by dividing each gradient by 2.\n                 grads = [g for g, _ in grads_and_vars]\n-                vars = [\n-                    v for _, v in grads_and_vars\n-                ]  # pylint: disable=redefined-builtin\n+                vars = [v for _, v in grads_and_vars]\n                 all_reduced_grads = [g / 2 for g in grads]\n                 return list(zip(all_reduced_grads, vars))\n \n@@ -1451,14 +1439,10 @@ class OptimizerCoefficientTest(test_combinations.TestCase):\n         \"\"\"Ensure that subclassed optimizers without apply_state still work.\"\"\"\n \n         class SubclassedOptimizer(optimizer_class):\n-            def _resource_apply_dense(\n-                self, grad, var\n-            ):  # pylint: disable=useless-super-delegation\n+            def _resource_apply_dense(self, grad, var):\n                 return super()._resource_apply_dense(grad, var)\n \n-            def _resource_apply_sparse(\n-                self, grad, var, indices\n-            ):  # pylint: disable=useless-super-delegation\n+            def _resource_apply_sparse(self, grad, var, indices):\n                 return super()._resource_apply_sparse(grad, var, indices)\n \n         init_kwargs = init_kwargs or {}\n\n@@ -23,10 +23,7 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n \n-\n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.RMSprop\")\n class RMSprop(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the RMSprop algorithm.\n\n@@ -440,7 +440,7 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 def loss():\n                     pred = tf.matmul(\n                         tf.compat.v1.nn.embedding_lookup([var0], [0]), x\n-                    )  # pylint: disable=cell-var-from-loop\n+                    )\n                     return pred * pred\n \n                 sgd_op = rmsprop.RMSprop(\n@@ -472,10 +472,10 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 def loss():\n                     pred = tf.matmul(\n                         tf.compat.v1.nn.embedding_lookup([var0], [0]), x\n-                    )  # pylint: disable=cell-var-from-loop\n+                    )\n                     return pred * pred\n \n-                # loss = lambda: pred * pred  # pylint:\n+                # loss = lambda: pred * pred\n                 # disable=cell-var-from-loop\n                 sgd_op = rmsprop.RMSprop(\n                     learning_rate=1.0,\n\n@@ -192,7 +192,7 @@ class LinearModel(training.Model):\n         if self.use_bias:\n             result = tf.nn.bias_add(result, self.bias)\n         if self.activation is not None:\n-            return self.activation(result)  # pylint: disable=not-callable\n+            return self.activation(result)\n         return result\n \n     def get_config(self):\n\n@@ -101,7 +101,7 @@ class WideDeepModel(keras_training.Model):\n         else:\n             linear_inputs, dnn_inputs = inputs\n         linear_output = self.linear_model(linear_inputs)\n-        # pylint: disable=protected-access\n+\n         if self.dnn_model._expects_training_arg:\n             if training is None:\n                 training = backend.learning_phase()\n@@ -193,9 +193,7 @@ class WideDeepModel(keras_training.Model):\n                 metrics_tensors = [\n                     m._call_result\n                     for m in metrics\n-                    if hasattr(\n-                        m, \"_call_result\"\n-                    )  # pylint: disable=protected-access\n+                    if hasattr(m, \"_call_result\")\n                 ]\n \n             with backend.name_scope(\"training\"):\n\n@@ -12,9 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=invalid-name\n-# pylint: disable=g-import-not-at-top\n-# pylint: disable=g-classes-have-attributes\n+\n \n \"\"\"Utilies for image preprocessing and augmentation.\n \n@@ -338,9 +336,7 @@ class BatchFromFilesMixin:\n         self.save_format = save_format\n         self.interpolation = interpolation\n         if subset is not None:\n-            validation_split = (\n-                self.image_data_generator._validation_split\n-            )  # pylint: disable=protected-access\n+            validation_split = self.image_data_generator._validation_split\n             if subset == \"validation\":\n                 split = (0, validation_split)\n             elif subset == \"training\":\n\n@@ -32,7 +32,7 @@ from keras.testing_infra import test_utils\n from keras.utils import image_utils\n \n try:\n-    import PIL  # pylint:disable=g-import-not-at-top\n+    import PIL\n except ImportError:\n     PIL = None\n \n\n@@ -20,8 +20,6 @@ the `tf.data` APIs which provide a much more flexible mechanisms for dealing\n with sequences. See the [tf.data guide](https://www.tensorflow.org/guide/data)\n for more details.\n \"\"\"\n-# pylint: disable=invalid-name\n-# pylint: disable=g-classes-have-attributes\n \n \n import json\n\n@@ -181,7 +181,7 @@ class TestSequence(tf.test.TestCase):\n \n             self.assertEqual(expected, actual)\n \n-            if len(g) > 0:  # pylint: disable=g-explicit-length-test\n+            if len(g) > 0:\n                 # All elements in range(length, 10) should be used as current\n                 # step\n                 expected = np.arange(length, 10).reshape(-1, 1)\n\n@@ -23,8 +23,6 @@ the [text loading tutorial]\n and [preprocessing layer guide]\n (https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \"\"\"\n-# pylint: disable=invalid-name\n-# pylint: disable=g-classes-have-attributes\n \n \n import collections\n\n@@ -13,8 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Built-in regularizers.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=invalid-name\n+\n \n import math\n \n@@ -133,7 +132,7 @@ class Regularizer:\n \n     >>> @tf.keras.utils.register_keras_serializable(package='Custom', name='l2')\n     ... class L2Regularizer(tf.keras.regularizers.Regularizer):\n-    ...   def __init__(self, l2=0.):  # pylint: disable=redefined-outer-name\n+    ...   def __init__(self, l2=0.):\n     ...     self.l2 = l2\n     ...\n     ...   def __call__(self, x):\n@@ -230,7 +229,7 @@ class L1L2(Regularizer):\n         l2: Float; L2 regularization factor.\n     \"\"\"\n \n-    def __init__(self, l1=0.0, l2=0.0):  # pylint: disable=redefined-outer-name\n+    def __init__(self, l1=0.0, l2=0.0):\n         # The default value for l1 and l2 are different from the value in l1_l2\n         # for backward compatibility reason. Eg, L1L2(l2=0.1) will only have l2\n         # and no l1 penalty.\n@@ -272,9 +271,7 @@ class L1(Regularizer):\n         l1: Float; L1 regularization factor.\n     \"\"\"\n \n-    def __init__(\n-        self, l1=0.01, **kwargs\n-    ):  # pylint: disable=redefined-outer-name\n+    def __init__(self, l1=0.01, **kwargs):\n         l1 = kwargs.pop(\"l\", l1)  # Backwards compatibility\n         if kwargs:\n             raise TypeError(f\"Argument(s) not recognized: {kwargs}\")\n@@ -308,9 +305,7 @@ class L2(Regularizer):\n         l2: Float; L2 regularization factor.\n     \"\"\"\n \n-    def __init__(\n-        self, l2=0.01, **kwargs\n-    ):  # pylint: disable=redefined-outer-name\n+    def __init__(self, l2=0.01, **kwargs):\n         l2 = kwargs.pop(\"l\", l2)  # Backwards compatibility\n         if kwargs:\n             raise TypeError(f\"Argument(s) not recognized: {kwargs}\")\n@@ -396,7 +391,7 @@ class OrthogonalRegularizer(Regularizer):\n \n \n @keras_export(\"keras.regularizers.l1_l2\")\n-def l1_l2(l1=0.01, l2=0.01):  # pylint: disable=redefined-outer-name\n+def l1_l2(l1=0.01, l2=0.01):\n     r\"\"\"Create a regularizer that applies both L1 and L2 penalties.\n \n     The L1 regularization penalty is computed as:\n\n@@ -112,9 +112,7 @@ class NewSavingTest(tf.test.TestCase):\n         @keras.utils.generic_utils.register_keras_serializable(\n             package=\"my_custom_package\"\n         )\n-        def my_mean_squared_error(\n-            y_true, y_pred\n-        ):  # pylint: disable=redefined-outer-name\n+        def my_mean_squared_error(y_true, y_pred):\n             \"\"\"Function-local `mean_squared_error`.\"\"\"\n             return backend.mean(\n                 tf.math.squared_difference(y_pred, y_true), axis=-1\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Functions for saving and loading a Keras Model from HDF5 format.\"\"\"\n \n import json\n@@ -44,11 +44,10 @@ except ImportError:\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n-# pylint:disable=g-inconsistent-quotes\n+\n sequential_lib = LazyLoader(\n     \"sequential_lib\", globals(), \"keras.engine.sequential\"\n )\n-# pylint:enable=g-inconsistent-quotes\n \n \n def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n@@ -147,9 +146,7 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n             f.close()\n \n \n-def load_model_from_hdf5(\n-    filepath, custom_objects=None, compile=True\n-):  # pylint: disable=redefined-builtin\n+def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n     \"\"\"Loads a model saved via `save_model_to_hdf5`.\n \n     Args:\n\n@@ -31,7 +31,7 @@ from keras.utils import generic_utils\n from keras.utils import losses_utils\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n\n@@ -30,7 +30,7 @@ from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Functions that save the model's config into different formats.\"\"\"\n \n # isort: off\n@@ -50,7 +50,7 @@ def model_from_config(config, custom_objects=None):\n             f\"Received: config={config}. Did you meant to use \"\n             \"`Sequential.from_config(config)`?\"\n         )\n-    from keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n+    from keras.layers import deserialize\n \n     return deserialize(config, custom_objects=custom_objects)\n \n@@ -103,7 +103,7 @@ def model_from_json(json_string, custom_objects=None):\n         A Keras model instance (uncompiled).\n     \"\"\"\n     from keras.layers import (\n-        deserialize_from_json,  # pylint: disable=g-import-not-at-top\n+        deserialize_from_json,\n     )\n \n     return deserialize_from_json(json_string, custom_objects=custom_objects)\n\n@@ -19,8 +19,6 @@ import tarfile\n import uuid\n \n import numpy\n-\n-# pylint: disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n from keras.saving import save as save_module\n\n@@ -17,8 +17,6 @@ import copy\n import pickle\n \n import numpy as np\n-\n-# pylint: disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n from keras.testing_infra import test_combinations\n@@ -38,7 +36,7 @@ class TestPickleProtocol(test_combinations.TestCase):\n                 lambda model: pickle.loads(\n                     pickle.dumps(model, protocol=protocol)\n                 ),\n-            )  # pylint: disable=cell-var-from-loop\n+            )\n             for protocol in range(pickle.HIGHEST_PROTOCOL + 1)\n         ),\n     )\n\n@@ -28,12 +28,11 @@ from keras.utils.io_utils import path_to_string\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-import-not-at-top\n+\n try:\n     import h5py\n except ImportError:\n     h5py = None\n-# pylint: enable=g-import-not-at-top\n \n \n @keras_export(\"keras.models.save_model\")\n@@ -48,7 +47,7 @@ def save_model(\n     options=None,\n     save_traces=True,\n ):\n-    # pylint: disable=line-too-long\n+\n     \"\"\"Saves a model as a TensorFlow SavedModel or HDF5 file.\n \n     See the [Serialization and Saving\n@@ -129,8 +128,8 @@ def save_model(\n     Raises:\n         ImportError: If save format is hdf5, and h5py is not available.\n     \"\"\"\n-    # pylint: enable=line-too-long\n-    from keras.engine import sequential  # pylint: disable=g-import-not-at-top\n+\n+    from keras.engine import sequential\n \n     default_format = \"tf\" if tf.__internal__.tf2.enabled() else \"h5\"\n     save_format = save_format or default_format\n@@ -148,11 +147,8 @@ def save_model(\n         or saving_utils.is_hdf5_filepath(filepath)\n     ):\n         # TODO(b/130258301): add utility method for detecting model type.\n-        if (\n-            not model._is_graph_network\n-            and not isinstance(  # pylint:disable=protected-access\n+        if not model._is_graph_network and not isinstance(\n             model, sequential.Sequential\n-            )\n         ):\n             raise NotImplementedError(\n                 \"Saving the model to HDF5 format requires the model to be a \"\n@@ -180,9 +176,7 @@ def save_model(\n \n @keras_export(\"keras.models.load_model\")\n @traceback_utils.filter_traceback\n-def load_model(\n-    filepath, custom_objects=None, compile=True, options=None\n-):  # pylint: disable=redefined-builtin\n+def load_model(filepath, custom_objects=None, compile=True, options=None):\n     \"\"\"Loads a model saved via `model.save()`.\n \n     Usage:\n\n@@ -42,7 +42,7 @@ from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n@@ -1444,7 +1444,7 @@ def _make_sequential_input_shape(input_size, output_size):\n     )\n \n \n-class _make_subclassed(keras.Model):  # pylint: disable=invalid-name\n+class _make_subclassed(keras.Model):\n     def __init__(self, input_size, output_size):\n         super().__init__()\n         self._config = {\"input_size\": input_size, \"output_size\": output_size}\n@@ -1465,7 +1465,7 @@ class _make_subclassed(keras.Model):  # pylint: disable=invalid-name\n         return cls(**config)\n \n \n-class _make_subclassed_built(_make_subclassed):  # pylint: disable=invalid-name\n+class _make_subclassed_built(_make_subclassed):\n     def __init__(self, input_size, output_size):\n         super().__init__(input_size, output_size)\n         self.build((None, input_size))\n\n@@ -30,7 +30,7 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n@@ -454,7 +454,7 @@ class TestWeightSavingAndLoadingTFFormat(\n \n             x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n             executing_eagerly = tf.executing_eagerly()\n-            model(x)  # pylint: disable=not-callable\n+            model(x)\n             if not executing_eagerly:\n                 session.run([v.initializer for v in model.variables])\n             model.save_weights(prefix, save_format=\"tensorflow\")\n@@ -498,7 +498,7 @@ class TestWeightSavingAndLoadingTFFormat(\n                 prefix = os.path.join(temp_dir, \"ckpt\")\n \n                 x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n-                model(x)  # pylint: disable=not-callable\n+                model(x)\n                 session.run([v.initializer for v in model.variables])\n                 model.save_weights(prefix, save_format=\"tensorflow\")\n                 op_count = len(graph.get_operations())\n\n@@ -41,7 +41,7 @@ _EXTENSION_TYPE_SPEC = \"_EXTENSION_TYPE_SPEC\"\n class Encoder(json.JSONEncoder):\n     \"\"\"JSON encoder and decoder that handles TensorShapes and tuples.\"\"\"\n \n-    def default(self, obj):  # pylint: disable=method-hidden\n+    def default(self, obj):\n         \"\"\"Encodes objects for types that aren't handled by the default\n         encoder.\"\"\"\n         if isinstance(obj, tf.TensorShape):\n@@ -108,9 +108,7 @@ def _decode_helper(\n         if obj[\"class_name\"] == \"TensorShape\":\n             return tf.TensorShape(obj[\"items\"])\n         elif obj[\"class_name\"] == \"TypeSpec\":\n-            return type_spec.lookup(\n-                obj[\"type_spec\"]\n-            )._deserialize(  # pylint: disable=protected-access\n+            return type_spec.lookup(obj[\"type_spec\"])._deserialize(\n                 _decode_helper(obj[\"serialized\"])\n             )\n         elif obj[\"class_name\"] == \"CompositeTensor\":\n@@ -200,7 +198,7 @@ def get_json_type(obj):\n                 \"class_name\": \"TypeSpec\",\n                 \"type_spec\": type_spec_name,\n                 \"serialized\": obj._serialize(),\n-            }  # pylint: disable=protected-access\n+            }\n         except ValueError:\n             raise ValueError(\n                 f\"Unable to serialize {obj} to JSON, because the TypeSpec \"\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Tests the JSON encoder and decoder.\"\"\"\n \n import enum\n\n@@ -45,9 +45,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n             name=self.obj.name,\n             trainable=self.obj.trainable,\n             expects_training_arg=self.obj._expects_training_arg,\n-            dtype=policy.serialize(\n-                self.obj._dtype_policy\n-            ),  # pylint: disable=protected-access\n+            dtype=policy.serialize(self.obj._dtype_policy),\n             batch_input_shape=getattr(self.obj, \"_batch_input_shape\", None),\n             stateful=self.obj.stateful,\n             must_restore_from_config=self.obj._must_restore_from_config,\n@@ -71,12 +69,8 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n             ] = generic_utils.serialize_keras_object(\n                 self.obj.activity_regularizer\n             )\n-        if (\n-            self.obj._build_input_shape is not None\n-        ):  # pylint: disable=protected-access\n-            metadata[\n-                \"build_input_shape\"\n-            ] = self.obj._build_input_shape  # pylint: disable=protected-access\n+        if self.obj._build_input_shape is not None:\n+            metadata[\"build_input_shape\"] = self.obj._build_input_shape\n         return metadata\n \n     def objects_to_serialize(self, serialization_cache):\n@@ -104,7 +98,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n         if (\n             save_impl.should_skip_serialization(self.obj)\n             or self.obj._must_restore_from_config\n-        ):  # pylint: disable=protected-access\n+        ):\n             return serialized_attr\n \n         object_dict, function_dict = self._get_serialized_attributes_internal(\n@@ -211,5 +205,5 @@ class VocabularySavedModelSaver(LayerSavedModelSaver):\n         # construction.\n         metadata[\"config\"][\n             \"has_input_vocabulary\"\n-        ] = self.obj._has_input_vocabulary  # pylint: disable=protected-access\n+        ] = self.obj._has_input_vocabulary\n         return metadata\n\n@@ -44,7 +44,7 @@ from keras.utils.generic_utils import LazyLoader\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n-# pylint:disable=g-inconsistent-quotes\n+\n models_lib = LazyLoader(\"models_lib\", globals(), \"keras.models\")\n base_layer = LazyLoader(\"base_layer\", globals(), \"keras.engine.base_layer\")\n layers_module = LazyLoader(\"layers_module\", globals(), \"keras.layers\")\n@@ -58,7 +58,7 @@ training_lib_v1 = LazyLoader(\n )\n metrics = LazyLoader(\"metrics\", globals(), \"keras.metrics\")\n base_rnn = LazyLoader(\"base_rnn\", globals(), \"keras.layers.rnn.base_rnn\")\n-# pylint:enable=g-inconsistent-quotes\n+\n \n PUBLIC_ATTRIBUTES = CommonEndpoints.all_functions.union(\n     CommonEndpoints.all_checkpointable_objects\n@@ -66,7 +66,7 @@ PUBLIC_ATTRIBUTES = CommonEndpoints.all_functions.union(\n PUBLIC_ATTRIBUTES.add(constants.KERAS_ATTR)\n \n \n-def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n+def load(path, compile=True, options=None):\n     \"\"\"Loads Keras objects from a SavedModel.\n \n     Any Keras layer or model saved to the SavedModel will be loaded back\n@@ -148,7 +148,6 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n \n     model = loaded[\"root\"]\n \n-    # pylint: disable=protected-access\n     if isinstance(model, training_lib.Model) and compile:\n         # TODO(kathywu): Use compiled objects from SavedModel, instead of\n         # creating new objects from the training config.\n@@ -176,7 +175,6 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n                 \"No training configuration found in save file, so the \"\n                 \"model was *not* compiled. Compile it manually.\"\n             )\n-    # pylint: enable=protected-access\n \n     # Force variables and resources to initialize.\n     if not tf.executing_eagerly():\n@@ -261,7 +259,7 @@ def _generate_object_paths(object_graph_def):\n \n def _is_graph_network(layer):\n     \"\"\"Determines whether the layer is a graph network.\"\"\"\n-    # pylint: disable=protected-access\n+\n     if isinstance(layer, RevivedNetwork):\n         return False\n     elif isinstance(layer, functional_lib.Functional):\n@@ -331,28 +329,24 @@ class KerasObjectLoader:\n                 # loading layers from the config, such as variables.\n                 continue\n             for name in PUBLIC_ATTRIBUTES:\n-                node._delete_tracking(name)  # pylint: disable=protected-access\n+                node._delete_tracking(name)\n \n             if isinstance(node, functional_lib.Functional):\n                 # Delete the temporary layer dependencies, which were used to\n                 # restore the checkpointed values. When the model is live, the\n                 # user can delete or add layers to the model at any time, so\n                 # these layer dependencies may be obsolete.\n-                dependencies = list(\n-                    node._self_unconditional_dependency_names\n-                )  # pylint: disable=protected-access\n+                dependencies = list(node._self_unconditional_dependency_names)\n                 for name in dependencies:\n                     if (\n                         re.match(r\"^layer(_with_weights)?-[\\d+]\", name)\n                         is not None\n                     ):\n-                        node._delete_tracking(\n-                            name\n-                        )  # pylint: disable=protected-access\n+                        node._delete_tracking(name)\n \n     def _add_children_recreated_from_config(self, obj, proto, node_id):\n         \"\"\"Recursively records objects recreated from config.\"\"\"\n-        # pylint: disable=protected-access\n+\n         if node_id in self._traversed_nodes_from_config:\n             return\n \n@@ -409,7 +403,6 @@ class KerasObjectLoader:\n                 setter = _revive_setter\n             else:\n                 setter = setattr\n-                # pylint: enable=protected-access\n \n             if child_id in self.loaded_nodes:\n                 if self.loaded_nodes[child_id][0] is not obj_child:\n@@ -428,9 +421,7 @@ class KerasObjectLoader:\n                 child_proto.WhichOneof(\"kind\") == \"variable\"\n                 and child_proto.variable.name\n             ):\n-                obj_child._handle_name = (\n-                    child_proto.variable.name + \":0\"\n-                )  # pylint: disable=protected-access\n+                obj_child._handle_name = child_proto.variable.name + \":0\"\n \n             if isinstance(\n                 obj_child, tf.__internal__.tracking.TrackableDataStructure\n@@ -444,7 +435,7 @@ class KerasObjectLoader:\n             )\n             self.loaded_nodes[child_id] = obj_child, setter\n \n-    def load_layers(self, compile=True):  # pylint: disable=redefined-builtin\n+    def load_layers(self, compile=True):\n         \"\"\"Load all layer nodes from the metadata.\"\"\"\n         # Load metrics after models and layers, since it's likely that models\n         # and layers will create the metric when initialized (this avoids\n@@ -625,7 +616,7 @@ class KerasObjectLoader:\n         # Use the dtype, name, and trainable status. Often times these are not\n         # specified in custom configs, so retrieve their values from the\n         # metadata.\n-        # pylint: disable=protected-access\n+\n         obj._name = metadata[\"name\"]\n         if metadata.get(\"trainable\") is not None:\n             obj.trainable = metadata[\"trainable\"]\n@@ -641,7 +632,6 @@ class KerasObjectLoader:\n                 args_spec, kwargs_spec = full_save_spec\n                 inputs_spec = args_spec.pop(0)\n                 obj._set_save_spec(inputs_spec, args_spec, kwargs_spec)\n-        # pylint: enable=protected-access\n \n         build_input_shape = metadata.get(\"build_input_shape\")\n         built = self._try_build_layer(obj, node_id, build_input_shape)\n@@ -670,7 +660,7 @@ class KerasObjectLoader:\n \n         build_input_shape = metadata.get(\"build_input_shape\")\n         if build_input_shape is not None and hasattr(obj, \"_build\"):\n-            obj._build(build_input_shape)  # pylint: disable=protected-access\n+            obj._build(build_input_shape)\n \n         return obj\n \n@@ -811,9 +801,7 @@ class KerasObjectLoader:\n                 input_shapes = self._infer_inputs(\n                     first_layer, convert_to_shapes=True\n                 )\n-                model._set_inputs(\n-                    input_specs\n-                )  # pylint: disable=protected-access\n+                model._set_inputs(input_specs)\n                 if not model.built and not isinstance(input_specs, dict):\n                     model.build(input_shapes)\n         else:  # Reconstruct functional model\n@@ -919,9 +907,7 @@ class KerasObjectLoader:\n \n         def setattr_wrapper(obj, name, value):\n             # Avoid overwriting attributes of objects recreated from the config.\n-            if (\n-                obj._lookup_dependency(name) is None\n-            ):  # pylint: disable=protected-access\n+            if obj._lookup_dependency(name) is None:\n                 setter(obj, name, value)\n \n         return setattr_wrapper\n@@ -929,7 +915,7 @@ class KerasObjectLoader:\n \n def _finalize_saved_model_layers(layers):\n     \"\"\"Runs the final steps of loading Keras Layers from SavedModel.\"\"\"\n-    # pylint: disable=protected-access\n+\n     # 1. Set up call functions for all layers initialized from the SavedModel (\n     # and not the config)\n     for layer in layers:\n@@ -982,9 +968,7 @@ def _finalize_saved_model_layers(layers):\n                     args = list(args)\n                     inputs = args.pop(0)\n                     kwargs = None\n-                layer._set_save_spec(\n-                    inputs, args, kwargs\n-                )  # pylint: disable=protected-access\n+                layer._set_save_spec(inputs, args, kwargs)\n \n                 # V1 models require calling _set_inputs to set the `.inputs`\n                 # attr.  Skip this step when there are multiple tensor inputs\n@@ -1002,8 +986,6 @@ def _finalize_saved_model_layers(layers):\n         # 4. Restore metrics list\n         _restore_layer_metrics(layer)\n \n-    # pylint: enable=protected-access\n-\n \n def _unable_to_call_layer_due_to_serialization_issue(\n     layer, *unused_args, **unused_kwargs\n@@ -1096,9 +1078,7 @@ def _restore_layer_unconditional_losses(layer):\n         # Some earlier SavedModels may not have layer_regularization_losses\n         # serialized separately. Fall back to using the regularization_losses\n         # list if it does not exist.\n-        losses = layer._serialized_attributes.get(\n-            \"regularization_losses\", []\n-        )  # pylint: disable=protected-access\n+        losses = layer._serialized_attributes.get(\"regularization_losses\", [])\n     for loss in losses:\n         layer.add_loss(loss)\n \n@@ -1150,9 +1130,7 @@ def revive_custom_object(identifier, metadata):\n         revived_cls = type(\n             tf.compat.as_str(metadata[\"class_name\"]), parent_classes, {}\n         )\n-        return revived_cls._init_from_metadata(\n-            metadata\n-        )  # pylint: disable=protected-access\n+        return revived_cls._init_from_metadata(metadata)\n     else:\n         raise ValueError(\n             f\"Unable to restore custom object of type {identifier}. \"\n@@ -1164,14 +1142,12 @@ def revive_custom_object(identifier, metadata):\n \n def _restore_layer_metrics(layer):\n     metrics_list = getattr(_get_keras_attr(layer), \"layer_metrics\", {})\n-    layer_metrics = {\n-        m.name: m for m in layer._metrics\n-    }  # pylint: disable=protected-access\n+    layer_metrics = {m.name: m for m in layer._metrics}\n     for name, metric in metrics_list.items():\n         if name not in layer_metrics:\n             # Metrics may be added during initialization/building of custom\n             # layers.\n-            layer._metrics.append(metric)  # pylint: disable=protected-access\n+            layer._metrics.append(metric)\n \n \n # TODO(kathywu): Centrally define keys and functions for both  serialization and\n@@ -1191,7 +1167,7 @@ class RevivedLayer:\n         revived_obj = cls(**init_args)\n \n         with utils.no_automatic_dependency_tracking_scope(revived_obj):\n-            # pylint:disable=protected-access\n+\n             revived_obj._call_spec.expects_training_arg = metadata[\n                 \"expects_training_arg\"\n             ]\n@@ -1211,7 +1187,6 @@ class RevivedLayer:\n                 revived_obj._is_feature_layer = metadata[\"_is_feature_layer\"]\n             if metadata.get(\"stateful\") is not None:\n                 revived_obj.stateful = metadata[\"stateful\"]\n-            # pylint:enable=protected-access\n \n         return revived_obj, _revive_setter\n \n@@ -1231,11 +1206,11 @@ def _revive_setter(layer, name, value):\n     # Many attributes in the SavedModel conflict with properties defined in\n     # Layer and Model. Save these attributes to a separate dictionary.\n     if name in PUBLIC_ATTRIBUTES:\n-        # pylint: disable=protected-access\n+\n         if isinstance(value, tf.__internal__.tracking.Trackable):\n             layer._track_trackable(value, name=name)\n         layer._serialized_attributes[name] = value\n-        # pylint: enable=protected-access\n+\n     elif (\n         isinstance(layer, functional_lib.Functional)\n         and re.match(r\"^layer(_with_weights)?-[\\d+]\", name) is not None\n@@ -1250,9 +1225,7 @@ def _revive_setter(layer, name, value):\n         # different layer-n. This may cause variable values to not be loaded\n         # properly in the original layer-n, but we already warn the users about\n         # this (ctrl-f \"shared between different layers/models\").\n-        layer._track_trackable(\n-            value, name, overwrite=True\n-        )  # pylint: disable=protected-access\n+        layer._track_trackable(value, name, overwrite=True)\n     elif getattr(layer, name, None) is not None:\n         # Don't overwrite already defined attributes.\n         pass\n@@ -1275,9 +1248,7 @@ class RevivedInputLayer:\n         )\n         revived_obj = cls(**init_args)\n         with utils.no_automatic_dependency_tracking_scope(revived_obj):\n-            revived_obj._config = metadata[\n-                \"config\"\n-            ]  # pylint:disable=protected-access\n+            revived_obj._config = metadata[\"config\"]\n \n         return revived_obj, setattr\n \n@@ -1327,7 +1298,7 @@ def infer_inputs_from_restored_call_function(fn):\n             # Doesn't particularly matter what is returned in this case because\n             # the result will be filtered out in _set_input_shape.\n             return x\n-        # pylint:disable=protected-access\n+\n         result = x._without_tensor_names().most_specific_common_supertype(\n             [y._without_tensor_names()]\n         )\n@@ -1356,7 +1327,7 @@ class RevivedNetwork(RevivedLayer):\n         # dictionary. The attributes are the ones listed in CommonEndpoints or\n         # \"keras_api\" for keras-specific attributes.\n         with utils.no_automatic_dependency_tracking_scope(revived_obj):\n-            # pylint:disable=protected-access\n+\n             revived_obj._call_spec.expects_training_arg = metadata[\n                 \"expects_training_arg\"\n             ]\n@@ -1368,20 +1339,18 @@ class RevivedNetwork(RevivedLayer):\n                 revived_obj.activity_regularizer = regularizers.deserialize(\n                     metadata[\"activity_regularizer\"]\n                 )\n-            # pylint:enable=protected-access\n \n-        return revived_obj, _revive_setter  # pylint:disable=protected-access\n+        return revived_obj, _revive_setter\n \n \n def _set_network_attributes_from_metadata(revived_obj):\n     \"\"\"Sets attributes recorded in the metadata.\"\"\"\n     with utils.no_automatic_dependency_tracking_scope(revived_obj):\n-        # pylint:disable=protected-access\n+\n         metadata = revived_obj._serialized_attributes[\"metadata\"]\n         if metadata.get(\"dtype\") is not None:\n             revived_obj._set_dtype_policy(metadata[\"dtype\"])\n         revived_obj._trainable = metadata[\"trainable\"]\n-        # pylint:enable=protected-access\n \n \n def _maybe_add_serialized_attributes(layer, metadata):\n@@ -1390,9 +1359,7 @@ def _maybe_add_serialized_attributes(layer, metadata):\n     # \"keras_api\" for keras-specific attributes.\n     if not hasattr(layer, \"_serialized_attributes\"):\n         with utils.no_automatic_dependency_tracking_scope(layer):\n-            layer._serialized_attributes = {\n-                \"metadata\": metadata\n-            }  # pylint: disable=protected-access\n+            layer._serialized_attributes = {\"metadata\": metadata}\n \n \n def _get_keras_attr(layer):\n\n@@ -35,12 +35,8 @@ class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n             dtype=self.obj.dtype,\n         )\n         metadata.update(layer_serialization.get_serialized(self.obj))\n-        if (\n-            self.obj._build_input_shape is not None\n-        ):  # pylint: disable=protected-access\n-            metadata[\n-                \"build_input_shape\"\n-            ] = self.obj._build_input_shape  # pylint: disable=protected-access\n+        if self.obj._build_input_shape is not None:\n+            metadata[\"build_input_shape\"] = self.obj._build_input_shape\n         return metadata\n \n     def _get_serialized_attributes_internal(self, unused_serialization_cache):\n\n@@ -31,9 +31,7 @@ class ModelSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n         metadata = super()._python_properties_internal()\n         # Network stateful property is dependent on the child layers.\n         metadata.pop(\"stateful\")\n-        metadata[\n-            \"is_graph_network\"\n-        ] = self.obj._is_graph_network  # pylint: disable=protected-access\n+        metadata[\"is_graph_network\"] = self.obj._is_graph_network\n         spec = self.obj.save_spec(dynamic_batch=False)\n         metadata[\"full_save_spec\"] = spec\n         # save_spec is saved for forward compatibility on older TF versions.\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Tests reviving models from config and SavedModel.\n \n These tests ensure that a model revived from a combination of config and\n\n@@ -86,7 +86,7 @@ def save(\n         model.optimizer = None\n         # TODO(b/180760306) Change to del model.optimizer if Layer's __delattr__\n         # calls AutoTrackable's __delattr__.\n-        model._delete_tracking(\"optimizer\")  # pylint: disable=protected-access\n+        model._delete_tracking(\"optimizer\")\n \n     # Trace all functions and signatures with `training=0` instead of using an\n     # already-set learning phase placeholder.\n@@ -132,7 +132,7 @@ def generate_keras_metadata(saved_nodes, node_paths):\n                 ),\n                 identifier=node._object_identifier,\n                 metadata=node._tracking_metadata,\n-            )  # pylint: disable=protected-access\n+            )\n \n             # Log warning if the node's class name conflicts with a Keras\n             # built-in object.\n\n@@ -45,7 +45,7 @@ from keras.utils.generic_utils import LazyLoader\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n-# pylint:disable=g-inconsistent-quotes\n+\n base_layer = LazyLoader(\"base_layer\", globals(), \"keras.engine.base_layer\")\n metrics = LazyLoader(\"metrics\", globals(), \"keras.metrics\")\n input_layer = LazyLoader(\"input_layer\", globals(), \"keras.engine.input_layer\")\n@@ -53,7 +53,6 @@ training_lib = LazyLoader(\"training_lib\", globals(), \"keras.engine.training\")\n sequential_lib = LazyLoader(\n     \"sequential_lib\", globals(), \"keras.engine.sequential\"\n )\n-# pylint:enable=g-inconsistent-quotes\n \n \n def should_skip_serialization(layer):\n@@ -62,7 +61,7 @@ def should_skip_serialization(layer):\n     saved_model_input_spec_set = (\n         isinstance(layer, training_lib.Model)\n         and layer._saved_model_inputs_spec is not None\n-    )  # pylint: disable=protected-access\n+    )\n     if not layer.built and not saved_model_input_spec_set:\n         logging.warning(\n             \"Skipping full serialization of Keras layer {}, because \"\n@@ -92,11 +91,9 @@ def wrap_layer_objects(layer, serialization_cache):\n     # Wrap all regularization losses as tf.functions.\n     # First, generate list of all regularization losses in this layer and\n     # sublayers.\n-    all_losses = layer._callable_losses[:]  # pylint: disable=protected-access\n+    all_losses = layer._callable_losses[:]\n     for child_layer in utils.list_all_layers(layer):\n-        all_losses.extend(\n-            child_layer._callable_losses\n-        )  # pylint: disable=protected-access\n+        all_losses.extend(child_layer._callable_losses)\n     # Next, wrap all loss functions as tf.functions. Use the serialization cache\n     # to store already-wrapped functions.\n     keras_loss_cache = serialization_cache.setdefault(\"keras_losses\", {})\n@@ -111,13 +108,12 @@ def wrap_layer_objects(layer, serialization_cache):\n             keras_loss_cache[loss_fn] = wrapped_loss\n             wrapped_loss_functions.append(wrapped_loss)\n     wrapped_layer_losses = [\n-        keras_loss_cache[fn]\n-        for fn in layer._callable_losses[:]  # pylint: disable=protected-access\n+        keras_loss_cache[fn] for fn in layer._callable_losses[:]\n     ]\n \n     layer_metrics = tf.__internal__.tracking.wrap(\n         {m.name: m for m in layer._metrics}\n-    )  # pylint: disable=protected-access\n+    )\n \n     # Avoid duplicate creation of shard Variables on loading.\n     # `layer.variables` will return the shard Variables rather than the\n@@ -143,7 +139,6 @@ def wrap_layer_objects(layer, serialization_cache):\n         ),\n         layer_metrics=layer_metrics,\n     )\n-    # pylint: disable=protected-access\n \n \n def wrap_layer_functions(layer, serialization_cache):\n@@ -200,9 +195,7 @@ def wrap_layer_functions(layer, serialization_cache):\n         \"__call__\": call_fn,\n     }\n \n-    if (\n-        layer._activity_regularizer is not None\n-    ):  # pylint: disable=protected-access\n+    if layer._activity_regularizer is not None:\n         fns[\"activity_regularizer_fn\"] = _wrap_activity_regularizer(layer)\n         fns[\n             \"call_and_return_all_conditional_losses\"\n@@ -269,7 +262,7 @@ def _replace_child_layer_functions(layer, serialization_cache):\n           Child layer 2: ...\n         }\n     \"\"\"\n-    # pylint: disable=protected-access\n+\n     original_fns = {}\n \n     def replace_layer_functions(child_layer, serialized_fns):\n@@ -333,7 +326,6 @@ def _replace_child_layer_functions(layer, serialization_cache):\n             replace_layer_functions(child_layer, serialized_functions)\n \n     return original_fns\n-    # pylint: enable=protected-access\n \n \n def _restore_child_layer_functions(original_fns):\n@@ -342,16 +334,13 @@ def _restore_child_layer_functions(original_fns):\n         with utils.no_automatic_dependency_tracking_scope(child_layer):\n             for fn_name, fn in fns.items():\n                 try:\n-                    setattr(\n-                        child_layer, fn_name, fn\n-                    )  # pylint: disable=protected-access\n+                    setattr(child_layer, fn_name, fn)\n                 except AttributeError:\n                     # In the case of _activity_regularizer, setting the\n                     # attribute may be disallowed.\n                     pass\n \n \n-# pylint: disable=protected-access\n def _reset_layer_losses(parent_layer):\n     \"\"\"Resets losses of layer and its sublayers, and returns original losses.\"\"\"\n     losses_dict = {}\n@@ -373,9 +362,6 @@ def _restore_layer_losses(losses_dict):\n             layer._eager_losses = losses_dict[layer][\"eager_losses\"]\n \n \n-# pylint: enable=protected-access\n-\n-\n class LayerTracingContext(threading.local):\n     def __init__(self):\n         super().__init__()\n@@ -437,15 +423,13 @@ class LayerCallCollection:\n \n         self.layer_call_method = _get_layer_call_method(layer)\n         self._expects_training_arg = utils.layer_uses_training_bool(layer)\n-        self._call_spec = layer._call_spec  # pylint: disable=protected-access\n+        self._call_spec = layer._call_spec\n \n         # Create new call spec if the layer itself does not accept a training\n         # arg, but one of its child layers does. When this layer's call\n         # functions are traced, they will be traced with an added `training`\n         # keyword argument.\n-        if (\n-            not self.layer._expects_training_arg and self._expects_training_arg\n-        ):  # pylint: disable=protected-access\n+        if not self.layer._expects_training_arg and self._expects_training_arg:\n             arg_spec = utils.set_training_arg_spec(\n                 self._call_spec.full_argspec, False\n             )\n@@ -482,12 +466,10 @@ class LayerCallCollection:\n         elif (\n             layer.input_spec is not None\n             and layer._use_input_spec_as_call_signature\n-        ):  # pylint: disable=protected-access\n+        ):\n \n             def to_tensor_spec_or_none(x):\n-                spec = input_spec.to_tensor_spec(\n-                    x, layer._compute_dtype\n-                )  # pylint: disable=protected-access\n+                spec = input_spec.to_tensor_spec(x, layer._compute_dtype)\n                 # If the shape is too general (e.g. multiple dimensions are\n                 # allowed), return None so that separate functions can be\n                 # generated for each inferred input signature.\n@@ -533,11 +515,9 @@ class LayerCallCollection:\n                 add_trace_to_queue(fn, args, kwargs)\n \n     def training_arg_was_passed(self, args, kwargs):\n-        return (\n-            self._call_spec.arg_was_passed(  # pylint: disable=protected-access\n+        return self._call_spec.arg_was_passed(\n             \"training\", args, kwargs, inputs_in_args=True\n         )\n-        )\n \n     def get_training_arg_value(self, args, kwargs):\n         try:\n@@ -548,17 +528,13 @@ class LayerCallCollection:\n             return None\n \n     def get_input_arg_value(self, args, kwargs):\n-        return (\n-            self._call_spec.get_arg_value(  # pylint: disable=protected-access\n+        return self._call_spec.get_arg_value(\n             self._input_arg_name, args, kwargs, inputs_in_args=True\n         )\n-        )\n \n     def _maybe_wrap_with_training_arg(self, call_fn, match_layer_training_arg):\n         \"\"\"Wraps call function with added training argument if necessary.\"\"\"\n-        if (\n-            not self.layer._expects_training_arg and self._expects_training_arg\n-        ):  # pylint: disable=protected-access\n+        if not self.layer._expects_training_arg and self._expects_training_arg:\n             # Add training arg to wrapper function.\n             def wrap_with_training_arg(*args, **kwargs):\n                 if match_layer_training_arg:\n@@ -638,12 +614,12 @@ def layer_call_wrapper(call_collection, method, name):\n         layer = call_collection.layer\n         training = None\n         inputs = _filtered_inputs([args, kwargs])\n-        # pylint: disable=protected-access\n+\n         if (args or kwargs) and call_collection.training_arg_was_passed(\n             args, kwargs\n         ):\n             training = call_collection.get_training_arg_value(args, kwargs)\n-        # pylint: enable=protected-access\n+\n         original_losses = _reset_layer_losses(layer)\n         with base_layer_utils.call_context().enter(\n             layer,\n@@ -654,7 +630,7 @@ def layer_call_wrapper(call_collection, method, name):\n         ):\n             with autocast_variable.enable_auto_cast_variables(\n                 layer._compute_dtype_object\n-            ):  # pylint: disable=protected-access\n+            ):\n                 ret = method(*args, **kwargs)\n         _restore_layer_losses(original_losses)\n         return ret\n@@ -738,7 +714,7 @@ def _wrap_call_and_conditional_losses(layer):\n def _extract_outputs_from_fn(layer, call_and_return_conditional_losses):\n     \"\"\"Returns a function that returns only call function outputs.\"\"\"\n     if isinstance(layer, keras_load.RevivedLayer):\n-        return layer.keras_api.__call__  # pylint: disable=protected-access\n+        return layer.keras_api.__call__\n \n     def call(inputs, *args, **kwargs):\n         return call_and_return_conditional_losses(inputs, *args, **kwargs)[0]\n@@ -763,9 +739,9 @@ def _append_activity_regularizer_loss(\n def _create_call_fn_decorator(layer, wrapped_call):\n     call_fn = _get_layer_call_method(layer)\n     fn, arg_spec = utils.maybe_add_training_arg(\n-        layer._call_spec,  # pylint: disable=protected-access\n+        layer._call_spec,\n         wrapped_call,\n-        layer._expects_training_arg,  # pylint: disable=protected-access\n+        layer._expects_training_arg,\n         default_training_value=False,\n     )\n     return tf.__internal__.decorator.make_decorator(\n@@ -787,7 +763,7 @@ def _wrap_unconditional_loss(loss_fn, index):\n \n def _wrap_activity_regularizer(layer):\n     \"\"\"Wraps the activity regularizer.\"\"\"\n-    # pylint: disable=protected-access\n+\n     if isinstance(\n         layer._activity_regularizer, tf.__internal__.function.Function\n     ):\n@@ -799,7 +775,6 @@ def _wrap_activity_regularizer(layer):\n             tf.TensorSpec(None, layer._compute_dtype or backend.floatx())\n         ],\n     )\n-    # pylint: enable=protected-access\n \n \n def _get_layer_call_method(layer):\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Tests for saving and loading Keras models and layers from SavedModel.\n \n These should ensure that all layer properties are correctly assigned after\n@@ -61,9 +61,7 @@ class LayerWithLearningPhase(keras.engine.base_layer.Layer):\n             training, lambda: x * 0, lambda: tf.identity(x)\n         )\n         if not tf.executing_eagerly():\n-            output._uses_learning_phase = (\n-                True  # pylint: disable=protected-access\n-            )\n+            output._uses_learning_phase = True\n         return output\n \n     def compute_output_shape(self, input_shape):\n@@ -1328,7 +1326,7 @@ class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n \n @generic_utils.register_keras_serializable(\"Testing\")\n class CustomMeanMetric(keras.metrics.Mean):\n-    def update_state(self, *args):  # pylint: disable=useless-super-delegation\n+    def update_state(self, *args):\n         # Sometimes built-in metrics return an op in update_state. Custom\n         # metrics don't support returning ops, so wrap the update_state method\n         # while returning nothing.\n@@ -1431,9 +1429,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n     )\n     def test_custom_metric(self, base_cls, num_tensor_args, requires_build):\n         class CustomMetric(base_cls):\n-            def update_state(\n-                self, *args\n-            ):  # pylint: disable=useless-super-delegation\n+            def update_state(self, *args):\n                 # Sometimes built-in metrics return an op in update_state.\n                 # Custom metrics don't support returning ops, so wrap the\n                 # update_state method while returning nothing.\n@@ -1444,9 +1440,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n             save_dir = self._save_model_dir(\"first_save\")\n \n             if requires_build:\n-                metric(\n-                    *self.generate_inputs(num_tensor_args)\n-                )  # pylint: disable=not-callable\n+                metric(*self.generate_inputs(num_tensor_args))\n \n             self.evaluate([v.initializer for v in metric.variables])\n \n@@ -1549,7 +1543,7 @@ class TestUpdateMetadata(tf.test.TestCase):\n             ),\n             identifier=\"_tf_keras_model\",\n             metadata=node_metadata,\n-        )  # pylint: disable=protected-access\n+        )\n \n         new_metadata = keras_load._update_to_current_version(metadata)\n         node_metadata = json_utils.decode(new_metadata.nodes[0].metadata)\n\n@@ -24,12 +24,11 @@ from keras.utils.generic_utils import LazyLoader\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n-# pylint:disable=g-inconsistent-quotes\n+\n base_layer = LazyLoader(\"base_layer\", globals(), \"keras.engine.base_layer\")\n training_lib = LazyLoader(\"training_lib\", globals(), \"keras.engine.training\")\n metrics = LazyLoader(\"metrics\", globals(), \"keras.metrics\")\n base_rnn = LazyLoader(\"base_rnn\", globals(), \"keras.layers.rnn.base_rnn\")\n-# pylint:enable=g-inconsistent-quotes\n \n \n class SerializedAttributes:\n\n@@ -29,9 +29,7 @@ from keras.utils import layer_utils\n from keras.utils import tf_contextlib\n from keras.utils.generic_utils import LazyLoader\n \n-# pylint:disable=g-inconsistent-quotes\n training_lib = LazyLoader(\"training_lib\", globals(), \"keras.engine.training\")\n-# pylint:enable=g-inconsistent-quotes\n \n \n def use_wrapped_call(\n@@ -74,14 +72,14 @@ def use_wrapped_call(\n         # tensors). To fix this, whenever eager losses are added to one layer,\n         # add eager losses to all child layers. This causes `.losses` to only\n         # return eager losses.\n-        # pylint: disable=protected-access\n+\n         if tf.executing_eagerly():\n             for i in layer._flatten_layers():\n                 if i is not layer:\n                     i._eager_losses = [\n                         base_layer_utils.REVIVED_LOSS_PLACEHOLDER\n                     ]\n-        # pylint: enable=protected-access\n+\n         return outputs\n \n     decorated = tf.__internal__.decorator.make_decorator(\n@@ -99,7 +97,7 @@ def use_wrapped_call(\n def layer_uses_training_bool(layer):\n     \"\"\"Returns whether this layer or any of its children uses the training\n     arg.\"\"\"\n-    if layer._expects_training_arg:  # pylint: disable=protected-access\n+    if layer._expects_training_arg:\n         return True\n     visited = {layer}\n     to_visit = list_all_layers(layer)\n@@ -120,9 +118,7 @@ def list_all_layers(obj):\n         # the `Input` layer.\n         return obj.layers\n     else:\n-        return list(\n-            obj._flatten_layers(include_self=False, recursive=False)\n-        )  # pylint: disable=protected-access\n+        return list(obj._flatten_layers(include_self=False, recursive=False))\n \n \n def list_all_layers_and_sublayers(obj):\n@@ -278,10 +274,8 @@ def no_automatic_dependency_tracking_scope(obj):\n       a scope in which the object doesn't track dependencies.\n     \"\"\"\n     previous_value = getattr(obj, \"_setattr_tracking\", True)\n-    obj._setattr_tracking = False  # pylint: disable=protected-access\n+    obj._setattr_tracking = False\n     try:\n         yield\n     finally:\n-        obj._setattr_tracking = (\n-            previous_value  # pylint: disable=protected-access\n-        )\n+        obj._setattr_tracking = previous_value\n\n@@ -36,11 +36,10 @@ from tensorflow.python.util.tf_export import keras_export\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n-# pylint:disable=g-inconsistent-quotes\n+\n metrics_lib = LazyLoader(\"metrics_lib\", globals(), \"keras.metrics\")\n models_lib = LazyLoader(\"models_lib\", globals(), \"keras.models\")\n sequential = LazyLoader(\"sequential\", globals(), \"keras.engine.sequential\")\n-# pylint:enable=g-inconsistent-quotes\n \n \n # File name for json format of SavedModel.\n@@ -165,7 +164,7 @@ def _export_model_variables(model, saved_model_path):\n \n def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n     \"\"\"Exports model to v1 SavedModel format.\"\"\"\n-    if not model._is_graph_network:  # pylint: disable=protected-access\n+    if not model._is_graph_network:\n         if isinstance(model, sequential.Sequential):\n             # If input shape is not directly set in the model, the exported\n             # model will infer the expected shapes of the input from the model.\n@@ -185,9 +184,7 @@ def _save_v1_format(model, path, custom_objects, as_text, input_signature):\n                 \"set argument serving_only=True.\"\n             )\n \n-    builder = tf.__internal__.saved_model.SavedModelBuilder(\n-        path\n-    )  # pylint: disable=protected-access\n+    builder = tf.__internal__.saved_model.SavedModelBuilder(path)\n \n     # Manually save variables to export them in an object-based checkpoint. This\n     # skips the `builder.add_meta_graph_and_variables()` step, which saves a\n@@ -315,12 +312,12 @@ def _export_mode(\n         # Extract update and train ops from train/test/predict functions.\n         train_op = None\n         if mode == mode_keys.ModeKeys.TRAIN:\n-            clone._make_train_function()  # pylint: disable=protected-access\n+            clone._make_train_function()\n             train_op = clone.train_function.updates_op\n         elif mode == mode_keys.ModeKeys.TEST:\n-            clone._make_test_function()  # pylint: disable=protected-access\n+            clone._make_test_function()\n         else:\n-            clone._make_predict_function()  # pylint: disable=protected-access\n+            clone._make_predict_function()\n         g.get_collection_ref(tf.compat.v1.GraphKeys.UPDATE_OPS).extend(\n             clone.state_updates\n         )\n@@ -348,9 +345,7 @@ def _export_mode(\n                 clone.save_weights(\n                     checkpoint_path, save_format=\"tf\", overwrite=True\n                 )\n-                builder._has_saved_variables = (\n-                    True  # pylint: disable=protected-access\n-                )\n+                builder._has_saved_variables = True\n \n             # Add graph to the SavedModel builder.\n             builder.add_meta_graph(\n@@ -374,7 +369,7 @@ def _create_signature_def_map(model, mode):\n     if model.optimizer:\n         targets_dict = {\n             x.name.split(\":\")[0]: x for x in model._targets if x is not None\n-        }  # pylint: disable=protected-access\n+        }\n         inputs_dict.update(targets_dict)\n     outputs_dict = {\n         name: x for name, x in zip(model.output_names, model.outputs)\n@@ -414,9 +409,7 @@ def _create_signature_def_map(model, mode):\n     )\n \n \n-def _assert_same_non_optimizer_objects(\n-    model, model_graph, clone, clone_graph\n-):  # pylint: disable=unused-argument\n+def _assert_same_non_optimizer_objects(model, model_graph, clone, clone_graph):\n     \"\"\"Asserts model and clone contain the same trackable objects.\"\"\"\n \n     # TODO(fchollet, kathywu): make sure this works in eager mode.\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Tests for saving/loading function for keras Model.\"\"\"\n \n import os\n@@ -213,9 +213,7 @@ class LayerWithLearningPhase(keras.engine.base_layer.Layer):\n             training, lambda: x * 0, lambda: tf.identity(x)\n         )\n         if not tf.executing_eagerly():\n-            output._uses_learning_phase = (\n-                True  # pylint: disable=protected-access\n-            )\n+            output._uses_learning_phase = True\n         return output\n \n     def compute_output_shape(self, input_shape):\n\n@@ -17,7 +17,6 @@\n import copy\n import os\n \n-# pylint: disable=g-bad-import-order, g-direct-tensorflow-import\n import tensorflow.compat.v2 as tf\n \n import keras\n@@ -33,8 +32,6 @@ from keras.utils.io_utils import ask_to_proceed_with_overwrite\n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n \n-# pylint: enable=g-bad-import-order, g-direct-tensorflow-import\n-\n \n def extract_model_metrics(model):\n     \"\"\"Convert metrics from a Keras model `compile` API to dictionary.\n@@ -52,9 +49,7 @@ def extract_model_metrics(model):\n         # TODO(psv/kathywu): use this implementation in model to estimator flow.\n         # We are not using model.metrics here because we want to exclude the\n         # metrics added using `add_metric` API.\n-        return {\n-            m.name: m for m in model._compile_metric_functions\n-        }  # pylint: disable=protected-access\n+        return {m.name: m for m in model._compile_metric_functions}\n     return None\n \n \n@@ -142,10 +137,7 @@ def trace_model_call(model, input_signature=None):\n     @tf.function\n     def _wrapped_model(*args, **kwargs):\n         \"\"\"A concrete tf.function that wraps the model's call function.\"\"\"\n-        (\n-            args,\n-            kwargs,\n-        ) = model._call_spec.set_arg_value(  # pylint: disable=protected-access\n+        (args, kwargs,) = model._call_spec.set_arg_value(\n             \"training\", False, args, kwargs, inputs_in_args=True\n         )\n \n@@ -196,10 +188,8 @@ def model_metadata(model, include_optimizer=True, require_config=True):\n                 \"Prefer using a Keras optimizer instead \"\n                 \"(see keras.io/optimizers).\"\n             )\n-        elif model._compile_was_called:  # pylint: disable=protected-access\n-            training_config = model._get_compile_args(\n-                user_metrics=False\n-            )  # pylint: disable=protected-access\n+        elif model._compile_was_called:\n+            training_config = model._get_compile_args(user_metrics=False)\n             training_config.pop(\"optimizer\", None)  # Handled separately.\n             metadata[\"training_config\"] = _serialize_nested_config(\n                 training_config\n@@ -342,7 +332,7 @@ def _enforce_names_consistency(specs):\n     def _clear_name(spec):\n         spec = copy.deepcopy(spec)\n         if hasattr(spec, \"name\"):\n-            spec._name = None  # pylint:disable=protected-access\n+            spec._name = None\n         return spec\n \n     flat_specs = tf.nest.flatten(specs)\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Utilities for unit-testing Keras.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n import collections\n import functools\n@@ -27,7 +27,7 @@ import keras\n from keras.testing_infra import test_utils\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n\n@@ -1050,7 +1050,7 @@ def for_all_test_methods(decorator, *args, **kwargs):\n \n \n # The description is just for documentation purposes.\n-def run_without_tensor_float_32(description):  # pylint: disable=unused-argument\n+def run_without_tensor_float_32(description):\n     \"\"\"Execute test with TensorFloat-32 disabled.\n \n     While almost every real-world deep learning model runs fine with\n@@ -1084,7 +1084,7 @@ def run_without_tensor_float_32(description):  # pylint: disable=unused-argument\n # The description is just for documentation purposes.\n def run_all_without_tensor_float_32(\n     description,\n-):  # pylint: disable=unused-argument\n+):\n     \"\"\"Execute all tests in a class with TensorFloat-32 disabled.\"\"\"\n     return for_all_test_methods(run_without_tensor_float_32, description)\n \n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Sample `get_config` results for testing backwards compatibility.\"\"\"\n \n # inputs = tf.keras.Input(10)\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Tests for saving/loading function for keras Model.\"\"\"\n \n import os\n\n@@ -25,7 +25,7 @@ from keras.testing_infra import test_utils\n from keras.tests import model_subclassing_test_util as model_util\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n\n@@ -35,7 +35,7 @@ from tensorflow.python.training.tracking import (\n )\n \n try:\n-    import h5py  # pylint:disable=g-import-not-at-top\n+    import h5py\n except ImportError:\n     h5py = None\n \n\n@@ -18,7 +18,6 @@ import keras\n from keras.testing_infra import test_utils\n \n \n-# pylint: disable=missing-docstring,not-callable\n class SimpleConvTestModel(keras.Model):\n     def __init__(self, num_classes=10):\n         super().__init__(name=\"test_model\")\n@@ -47,11 +46,9 @@ def get_multi_io_subclass_model(use_bn=False, use_dp=False, num_classes=(2, 3)):\n         branch_b.append(keras.layers.BatchNormalization())\n     branch_b.append(keras.layers.Dense(num_classes[1], activation=\"softmax\"))\n \n-    model = (\n-        test_utils._MultiIOSubclassModel(  # pylint: disable=protected-access\n+    model = test_utils._MultiIOSubclassModel(\n         branch_a, branch_b, name=\"test_model\"\n     )\n-    )\n     return model\n \n \n\n@@ -39,7 +39,6 @@ from tensorflow.python.training.tracking import (\n )\n \n \n-# pylint: disable=not-callable\n class MyModel(training.Model):\n     \"\"\"A concrete Model for testing.\"\"\"\n \n@@ -433,7 +432,6 @@ class CheckpointingTests(test_combinations.TestCase):\n         self.assertNotIn(\"(root).v1'\", messages)\n         self.assertIn(\"expect_partial()\", messages)\n \n-    # pylint: disable=cell-var-from-loop\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -487,8 +485,6 @@ class CheckpointingTests(test_combinations.TestCase):\n                         self.evaluate(root.save_counter),\n                     )\n \n-    # pylint: enable=cell-var-from-loop\n-\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def testAnonymousVarsInInit(self):\n         class Model(training.Model):\n\n@@ -42,7 +42,6 @@ class NonLayerTrackable(tf.Module):\n         )\n \n \n-# pylint: disable=not-callable\n class MyModel(training.Model):\n     \"\"\"A concrete Model for testing.\"\"\"\n \n@@ -312,9 +311,7 @@ class CheckpointingTests(test_combinations.TestCase):\n                 # TODO(allenl): Use a Dataset and serialize/checkpoint it.\n                 input_value = tf.constant([[3.0]])\n                 optimizer.minimize(\n-                    lambda: model(\n-                        input_value\n-                    ),  # pylint: disable=cell-var-from-loop\n+                    lambda: model(input_value),\n                     global_step=root.optimizer_step,\n                 )\n             root.save(file_prefix=checkpoint_prefix)\n@@ -493,7 +490,6 @@ class CheckpointingTests(test_combinations.TestCase):\n                         self.evaluate(root.save_counter),\n                     )\n \n-    # pylint: disable=cell-var-from-loop\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -552,8 +548,6 @@ class CheckpointingTests(test_combinations.TestCase):\n                         self.evaluate(root.save_counter),\n                     )\n \n-    # pylint: enable=cell-var-from-loop\n-\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def testAnonymousVarsInInit(self):\n         class Model(training.Model):\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Public Keras utilities.\"\"\"\n-# pylint: disable=g-bad-import-order\n+\n \n # Audio related\n from keras.utils.audio_dataset import audio_dataset_from_directory\n\n@@ -22,8 +22,6 @@ from keras.utils import dataset_utils\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n try:\n     import tensorflow_io as tfio\n\n@@ -21,7 +21,7 @@ import tensorflow.compat.v2 as tf\n \n \n def InXlaContext(graph):\n-    ctxt = graph._get_control_flow_context()  # pylint: disable=protected-access\n+    ctxt = graph._get_control_flow_context()\n     return GetContainingXLAContext(ctxt) is not None\n \n \n@@ -36,7 +36,7 @@ def GraphOrParentsInXlaContext(graph):\n \n \n def IsInWhileLoop(op):\n-    ctxt = op._get_control_flow_context()  # pylint: disable=protected-access\n+    ctxt = op._get_control_flow_context()\n     return GetContainingWhileContext(ctxt) is not None\n \n \n@@ -84,9 +84,7 @@ def GetContainingXLAContext(ctxt):\n     return None\n \n \n-def smart_cond(\n-    pred, true_fn=None, false_fn=None, name=None\n-):  # pylint: disable=invalid-name\n+def smart_cond(pred, true_fn=None, false_fn=None, name=None):\n     \"\"\"Return either `true_fn()` if predicate `pred` is true else `false_fn()`.\n \n     If `pred` is a bool or has a constant value, we return either `true_fn()`\n@@ -112,7 +110,7 @@ def smart_cond(\n     )\n \n \n-def constant_value(pred):  # pylint: disable=invalid-name\n+def constant_value(pred):\n     \"\"\"Return the bool value for `pred`, or None if `pred` had a dynamic value.\n \n     Args:\n\n@@ -274,7 +274,7 @@ class TestConvUtils(tf.test.TestCase, parameterized.TestCase):\n         output_shape = _get_const_output_shape(input_shape, dim=1)\n \n         mask = np.zeros(input_shape + output_shape, np.bool)\n-        if all(d > 0 for d in mask.shape):  # pylint: disable=not-an-iterable\n+        if all(d > 0 for d in mask.shape):\n             mask[(0,) * len(output_shape)] = True\n \n         self.assertAllEqual(\n@@ -292,7 +292,7 @@ class TestConvUtils(tf.test.TestCase, parameterized.TestCase):\n         output_shape = _get_const_output_shape(input_shape, dim=2)\n \n         mask = np.zeros(input_shape + output_shape, np.bool)\n-        if all(d > 0 for d in mask.shape):  # pylint: disable=not-an-iterable\n+        if all(d > 0 for d in mask.shape):\n             for in_position in itertools.product(\n                 *[[0, d - 1] for d in input_shape]\n             ):\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-import-not-at-top\n+\n \"\"\"Utilities for file download and caching.\"\"\"\n \n import functools\n@@ -87,7 +87,7 @@ if True:  # This gets transformed to `if sys.version_info[0] == 2:` in OSS.\n                 fd.write(chunk)\n \n else:\n-    from urllib.request import urlretrieve  # pylint: disable=g-importing-member\n+    from urllib.request import urlretrieve\n \n \n def is_generator_or_sequence(x):\n@@ -327,9 +327,7 @@ def get_file(\n \n \n def _makedirs_exist_ok(datadir):\n-    os.makedirs(\n-        datadir, exist_ok=True\n-    )  # pylint: disable=unexpected-keyword-arg\n+    os.makedirs(datadir, exist_ok=True)\n \n \n def _resolve_hasher(algorithm, file_hash=None):\n@@ -423,7 +421,7 @@ class ThreadsafeIter:\n     def __next__(self):\n         with self.lock:\n             if self._exception:\n-                raise self._exception  # pylint: disable=raising-bad-type\n+                raise self._exception\n \n             try:\n                 return next(self.it)\n@@ -816,7 +814,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n             except queue.Empty:\n                 pass\n-            except Exception as e:  # pylint: disable=broad-except\n+            except Exception as e:\n                 self.stop()\n                 raise e\n \n@@ -946,7 +944,7 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n             for inputs in last_ones:\n                 if inputs is not None:\n                     yield inputs\n-        except Exception as e:  # pylint: disable=broad-except\n+        except Exception as e:\n             self.stop()\n             if \"generator already executing\" in str(e):\n                 raise RuntimeError(\n@@ -1067,10 +1065,10 @@ def pad_sequences(\n \n     x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n     for idx, s in enumerate(sequences):\n-        if not len(s):  # pylint: disable=g-explicit-length-test\n+        if not len(s):\n             continue  # empty list/array was found\n         if truncating == \"pre\":\n-            trunc = s[-maxlen:]  # pylint: disable=invalid-unary-operand-type\n+            trunc = s[-maxlen:]\n         elif truncating == \"post\":\n             trunc = s[:maxlen]\n         else:\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+\n \"\"\"Input dataset creator for `model.fit`.\"\"\"\n \n import tensorflow.compat.v2 as tf\n\n@@ -26,8 +26,6 @@ import tensorflow.compat.v2 as tf\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @keras_export(\"keras.utils.split_dataset\", v1=[])\n def split_dataset(\n@@ -475,7 +473,7 @@ def is_batched(tf_dataset):\n def get_batch_size(tf_dataset):\n     \"\"\"Get the batch size of the dataset.\"\"\"\n     if is_batched(tf_dataset):\n-        return tf_dataset._batch_size  # pylint: disable=protected-access\n+        return tf_dataset._batch_size\n     else:\n         return None\n \n\n@@ -6,8 +6,6 @@ import tensorflow.compat.v2 as tf\n from keras.testing_infra import test_utils\n from keras.utils import dataset_utils\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @test_utils.run_v2_only\n class SplitDatasetTest(tf.test.TestCase):\n\n@@ -488,14 +488,10 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n     return None\n \n \n-# pylint: disable=g-bad-exception-name\n class CustomMaskWarning(Warning):\n     pass\n \n \n-# pylint: enable=g-bad-exception-name\n-\n-\n @keras_export(\"keras.utils.serialize_keras_object\")\n def serialize_keras_object(instance):\n     \"\"\"Serialize a Keras object into a JSON-compatible representation.\n@@ -515,7 +511,6 @@ def serialize_keras_object(instance):\n     if instance is None:\n         return None\n \n-    # pylint: disable=protected-access\n     #\n     # For v1 layers, checking supports_masking is not enough. We have to also\n     # check whether compute_mask has been overridden.\n@@ -531,7 +526,6 @@ def serialize_keras_object(instance):\n             category=CustomMaskWarning,\n             stacklevel=2,\n         )\n-    # pylint: enable=protected-access\n \n     if hasattr(instance, \"get_config\"):\n         name = get_registered_name(instance.__class__)\n@@ -722,9 +716,7 @@ def deserialize_keras_object(\n         # If this object has already been loaded (i.e. it's shared between\n         # multiple objects), return the already-loaded object.\n         shared_object_id = config.get(SHARED_OBJECT_KEY)\n-        shared_object = _shared_object_loading_scope().get(\n-            shared_object_id\n-        )  # pylint: disable=assignment-from-none\n+        shared_object = _shared_object_loading_scope().get(shared_object_id)\n         if shared_object is not None:\n             return shared_object\n \n@@ -839,7 +831,7 @@ def func_load(code, defaults=None, closure=None, globs=None):\n         \"\"\"\n \n         def dummy_fn():\n-            # pylint: disable=pointless-statement\n+\n             value  # just access it so it gets captured in .__closure__\n \n         cell_value = dummy_fn.__closure__[0]\n@@ -1277,7 +1269,7 @@ def validate_config(config):\n \n def default(method):\n     \"\"\"Decorates a method to detect overrides in subclasses.\"\"\"\n-    method._is_default = True  # pylint: disable=protected-access\n+    method._is_default = True\n     return method\n \n \n@@ -1320,4 +1312,4 @@ class LazyLoader(python_types.ModuleType):\n \n # Aliases\n \n-custom_object_scope = CustomObjectScope  # pylint: disable=invalid-name\n+custom_object_scope = CustomObjectScope\n\n@@ -132,7 +132,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n         ):\n \n             @keras.utils.generic_utils.register_keras_serializable()\n-            class TestClass:  # pylint: disable=function-redefined\n+            class TestClass:\n                 def __init__(self, value):\n                     self._value = value\n \n\n@@ -23,8 +23,6 @@ from keras.utils import image_utils\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n ALLOWLIST_FORMATS = (\".bmp\", \".gif\", \".jpeg\", \".jpg\", \".png\")\n \n\n@@ -26,7 +26,7 @@ from keras.utils import image_dataset\n from keras.utils import image_utils\n \n try:\n-    import PIL  # pylint:disable=g-import-not-at-top\n+    import PIL\n except ImportError:\n     PIL = None\n \n\n@@ -14,7 +14,6 @@\n # ==============================================================================\n \"\"\"Utilities related to image handling.\"\"\"\n \n-# pylint: disable=g-import-not-at-top\n \n import io\n import pathlib\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-import-not-at-top\n+\n \"\"\"Utilities related to disk I/O.\"\"\"\n \n import os\n\n@@ -114,7 +114,7 @@ class DistributeKplTestUtils(tf.test.TestCase):\n         )\n \n         train_dataset = raw_dataset.map(\n-            lambda x: (  # pylint: disable=g-long-lambda\n+            lambda x: (\n                 {\"features\": feature_mapper(x[\"features\"])},\n                 label_mapper(x[\"label\"]),\n             )\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Utilities related to layer/model functionality.\"\"\"\n \n import copy\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Utilities related to loss functions.\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -203,11 +203,9 @@ def squeeze_or_expand_dimensions(y_pred, y_true=None, sample_weight=None):\n             rank_diff = tf.rank(y_pred) - tf.rank(y_true)\n             squeeze_dims = lambda: remove_squeezable_dimensions(y_true, y_pred)\n             is_last_dim_1 = tf.equal(1, tf.shape(y_pred)[-1])\n-            maybe_squeeze_dims = (\n-                lambda: tf.cond(  # pylint: disable=g-long-lambda\n+            maybe_squeeze_dims = lambda: tf.cond(\n                 is_last_dim_1, squeeze_dims, lambda: (y_true, y_pred)\n             )\n-            )\n             y_true, y_pred = tf.cond(\n                 tf.equal(1, rank_diff), maybe_squeeze_dims, squeeze_dims\n             )\n@@ -324,9 +322,7 @@ def compute_weighted_loss(\n     with backend.name_scope(name or \"weighted_loss\"):\n         # Save the `reduction` argument for loss normalization when distributing\n         # to multiple replicas. Used only for estimator + v1 optimizer flow.\n-        tf.compat.v1.get_default_graph()._last_loss_reduction = (\n-            reduction  # pylint: disable=protected-access\n-        )\n+        tf.compat.v1.get_default_graph()._last_loss_reduction = reduction\n \n         if not isinstance(losses, (keras_tensor.KerasTensor, tf.RaggedTensor)):\n             losses = tf.convert_to_tensor(losses)\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Utils related to keras metrics.\"\"\"\n \n import functools\n@@ -909,9 +909,7 @@ def _assert_splits_match(nested_splits_lists):\n         if len(splits_list) != len(nested_splits_lists[0]):\n             raise ValueError(error_msg)\n     return [\n-        tf.debugging.assert_equal(\n-            s1, s2, message=error_msg\n-        )  # pylint: disable=g-complex-comprehension\n+        tf.debugging.assert_equal(s1, s2, message=error_msg)\n         for splits_list in nested_splits_lists[1:]\n         for (s1, s2) in zip(nested_splits_lists[0], splits_list)\n     ]\n\n@@ -169,9 +169,7 @@ class RaggedSizeOpTest(tf.test.TestCase, parameterized.TestCase):\n     def test_failing_different_ragged_and_dense_ranks(self, x_list, y_list):\n         x = tf.ragged.constant(x_list)\n         y = tf.ragged.constant(y_list)\n-        with self.assertRaises(\n-            ValueError\n-        ):  # pylint: disable=g-error-prone-assert-raises\n+        with self.assertRaises(ValueError):\n             [\n                 x,\n                 y,\n@@ -188,9 +186,7 @@ class RaggedSizeOpTest(tf.test.TestCase, parameterized.TestCase):\n         x = tf.ragged.constant(x_list)\n         y = tf.ragged.constant(y_list)\n         mask = tf.ragged.constant(mask_list)\n-        with self.assertRaises(\n-            ValueError\n-        ):  # pylint: disable=g-error-prone-assert-raises\n+        with self.assertRaises(ValueError):\n             [\n                 x,\n                 y,\n@@ -206,9 +202,7 @@ class RaggedSizeOpTest(tf.test.TestCase, parameterized.TestCase):\n         # adding a ragged dimension\n         x = tf.RaggedTensor.from_row_splits(dt, row_splits=[0, 1])\n         y = tf.ragged.constant([[[[1, 2]]]])\n-        with self.assertRaises(\n-            ValueError\n-        ):  # pylint: disable=g-error-prone-assert-raises\n+        with self.assertRaises(ValueError):\n             [\n                 x,\n                 y,\n\n@@ -46,23 +46,17 @@ class _ObjectIdentityWrapper:\n \n     def __lt__(self, other):\n         self._assert_type(other)\n-        return id(self._wrapped) < id(\n-            other._wrapped\n-        )  # pylint: disable=protected-access\n+        return id(self._wrapped) < id(other._wrapped)\n \n     def __gt__(self, other):\n         self._assert_type(other)\n-        return id(self._wrapped) > id(\n-            other._wrapped\n-        )  # pylint: disable=protected-access\n+        return id(self._wrapped) > id(other._wrapped)\n \n     def __eq__(self, other):\n         if other is None:\n             return False\n         self._assert_type(other)\n-        return (\n-            self._wrapped is other._wrapped\n-        )  # pylint: disable=protected-access\n+        return self._wrapped is other._wrapped\n \n     def __ne__(self, other):\n         return not self.__eq__(other)\n@@ -194,7 +188,7 @@ class ObjectIdentitySet(collections.abc.MutableSet):\n     @staticmethod\n     def _from_storage(storage):\n         result = ObjectIdentitySet()\n-        result._storage = storage  # pylint: disable=protected-access\n+        result._storage = storage\n         return result\n \n     def _wrap_key(self, key):\n\n@@ -17,14 +17,13 @@ import collections\n import functools\n import inspect as _inspect\n \n-# pylint: disable=g-classes-have-attributes\n import tensorflow.compat.v2 as tf\n \n ArgSpec = _inspect.ArgSpec\n \n \n if hasattr(_inspect, \"FullArgSpec\"):\n-    FullArgSpec = _inspect.FullArgSpec  # pylint: disable=invalid-name\n+    FullArgSpec = _inspect.FullArgSpec\n else:\n     FullArgSpec = collections.namedtuple(\n         \"FullArgSpec\",\n@@ -55,7 +54,7 @@ def _convert_maybe_argspec_to_fullargspec(argspec):\n \n \n if hasattr(_inspect, \"getfullargspec\"):\n-    _getfullargspec = _inspect.getfullargspec  # pylint: disable=invalid-name\n+    _getfullargspec = _inspect.getfullargspec\n \n     def _getargspec(target):\n         \"\"\"A python3 version of getargspec.\n\n@@ -63,9 +63,7 @@ def set_random_seed(seed):\n     random.seed(seed)\n     np.random.seed(seed)\n     tf.random.set_seed(seed)\n-    backend._SEED_GENERATOR.generator = random.Random(\n-        seed\n-    )  # pylint:disable=protected-access\n+    backend._SEED_GENERATOR.generator = random.Random(seed)\n \n \n def is_tensor_or_tensor_list(v):\n@@ -107,7 +105,7 @@ def get_reachable_from_inputs(inputs, targets=None):\n \n         if isinstance(x, tf.Operation):\n             outputs = x.outputs[:] or []\n-            outputs += x._control_outputs  # pylint: disable=protected-access\n+            outputs += x._control_outputs\n         elif isinstance(x, tf.Variable):\n             try:\n                 outputs = [x.op]\n@@ -136,7 +134,8 @@ def get_reachable_from_inputs(inputs, targets=None):\n \n \n # This function needs access to private functions of `nest`.\n-#  pylint: disable=protected-access\n+\n+\n def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n     \"\"\"Maps the atomic elements of a nested structure.\n \n@@ -181,9 +180,6 @@ def get_shapes(tensors):\n     )\n \n \n-#  pylint: enable=protected-access\n-\n-\n def convert_shapes(input_shape, to_tuples=True):\n     \"\"\"Converts nested shape representations to desired format.\n \n@@ -464,7 +460,7 @@ def register_symbolic_tensor_type(cls):\n def type_spec_from_value(value):\n     \"\"\"Grab type_spec without converting array-likes to tensors.\"\"\"\n     if is_extension_type(value):\n-        return value._type_spec  # pylint: disable=protected-access\n+        return value._type_spec\n     # Get a TensorSpec for array-like data without\n     # converting the data to a Tensor\n     if hasattr(value, \"shape\") and hasattr(value, \"dtype\"):\n@@ -568,7 +564,7 @@ def dataset_is_infinite(dataset):\n \n def get_tensor_spec(t, dynamic_batch=False, name=None):\n     \"\"\"Returns a `TensorSpec` given a single `Tensor` or `TensorSpec`.\"\"\"\n-    # pylint: disable=protected-access\n+\n     if isinstance(t, tf.TypeSpec):\n         spec = t\n     elif is_extension_type(t):\n@@ -584,7 +580,6 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n         spec = tf.TensorSpec(shape=t.shape, dtype=t.dtype, name=name)\n     else:\n         return None  # Allow non-Tensors to pass through.\n-    # pylint: enable=protected-access\n \n     if not dynamic_batch:\n         return spec\n\n@@ -23,7 +23,7 @@ from keras.testing_infra import test_combinations\n from keras.utils import tf_utils\n \n try:\n-    import attr  # pylint:disable=g-import-not-at-top\n+    import attr\n except ImportError:\n     attr = None\n \n\n@@ -20,8 +20,6 @@ import tensorflow.compat.v2 as tf\n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-classes-have-attributes\n-\n \n @keras_export(\n     \"keras.utils.timeseries_dataset_from_array\",\n@@ -236,7 +234,7 @@ def timeseries_dataset_from_array(\n     indices = tf.data.Dataset.zip(\n         (tf.data.Dataset.range(len(start_positions)), positions_ds)\n     ).map(\n-        lambda i, positions: tf.range(  # pylint: disable=g-long-lambda\n+        lambda i, positions: tf.range(\n             positions[i],\n             positions[i] + sequence_length * sampling_rate,\n             sampling_rate,\n@@ -271,9 +269,7 @@ def timeseries_dataset_from_array(\n def sequences_from_indices(array, indices_ds, start_index, end_index):\n     dataset = tf.data.Dataset.from_tensors(array[start_index:end_index])\n     dataset = tf.data.Dataset.zip((dataset.repeat(), indices_ds)).map(\n-        lambda steps, inds: tf.gather(\n-            steps, inds\n-        ),  # pylint: disable=unnecessary-lambda\n+        lambda steps, inds: tf.gather(steps, inds),\n         num_parallel_calls=tf.data.AUTOTUNE,\n     )\n     return dataset\n\n@@ -63,7 +63,7 @@ def filter_traceback(fn):\n         filtered_tb = None\n         try:\n             return fn(*args, **kwargs)\n-        except Exception as e:  # pylint: disable=broad-except\n+        except Exception as e:\n             filtered_tb = _process_traceback_frames(e.__traceback__)\n             # To get the full stack trace, call:\n             # `tf.debugging.disable_traceback_filtering()`\n@@ -94,7 +94,7 @@ def inject_argument_info_in_traceback(fn, object_name=None):\n         bound_signature = None\n         try:\n             return fn(*args, **kwargs)\n-        except Exception as e:  # pylint: disable=broad-except\n+        except Exception as e:\n             if hasattr(e, \"_keras_call_info_injected\"):\n                 # Only inject info for the innermost failing call\n                 raise e\n@@ -149,9 +149,7 @@ def inject_argument_info_in_traceback(fn, object_name=None):\n                         # For any custom error that doesn't have a standard\n                         # signature.\n                         new_e = RuntimeError(message)\n-                new_e._keras_call_info_injected = (\n-                    True  # pylint: disable=protected-access\n-                )\n+                new_e._keras_call_info_injected = True\n             else:\n                 new_e = e\n             raise new_e.with_traceback(e.__traceback__) from None\n\n@@ -93,11 +93,9 @@ class LayerCallInfoInjectionTest(tf.test.TestCase):\n         tf.debugging.enable_traceback_filtering()\n         try:\n             fn()\n-        except Exception as e:  # pylint: disable=broad-except\n+        except Exception as e:\n             # Info should be injected exactly once.\n-            self.assertEqual(\n-                str(e).count(\"Call arguments received\"), 1\n-            )  # pylint: disable=g-assert-in-except\n+            self.assertEqual(str(e).count(\"Call arguments received\"), 1)\n \n     def test_custom_layer_call_nested(self):\n         class InnerLayer(layers.Layer):\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n+\n \"\"\"Utilities for Keras classes with v1 and v2 versions.\"\"\"\n \n import tensorflow.compat.v2 as tf\n@@ -21,7 +21,7 @@ from keras.utils.generic_utils import LazyLoader\n \n # TODO(b/134426265): Switch back to single-quotes once the issue\n # with copybara is fixed.\n-# pylint: disable=g-inconsistent-quotes\n+\n training = LazyLoader(\"training\", globals(), \"keras.engine.training\")\n training_v1 = LazyLoader(\"training_v1\", globals(), \"keras.engine.training_v1\")\n base_layer = LazyLoader(\"base_layer\", globals(), \"keras.engine.base_layer\")\n@@ -32,35 +32,28 @@ callbacks = LazyLoader(\"callbacks\", globals(), \"keras.callbacks\")\n callbacks_v1 = LazyLoader(\"callbacks_v1\", globals(), \"keras.callbacks_v1\")\n \n \n-# pylint: enable=g-inconsistent-quotes\n-\n-\n class ModelVersionSelector:\n     \"\"\"Chooses between Keras v1 and v2 Model class.\"\"\"\n \n-    def __new__(cls, *args, **kwargs):  # pylint: disable=unused-argument\n+    def __new__(cls, *args, **kwargs):\n         use_v2 = should_use_v2()\n-        cls = swap_class(\n-            cls, training.Model, training_v1.Model, use_v2\n-        )  # pylint: disable=self-cls-assignment\n+        cls = swap_class(cls, training.Model, training_v1.Model, use_v2)\n         return super(ModelVersionSelector, cls).__new__(cls)\n \n \n class LayerVersionSelector:\n     \"\"\"Chooses between Keras v1 and v2 Layer class.\"\"\"\n \n-    def __new__(cls, *args, **kwargs):  # pylint: disable=unused-argument\n+    def __new__(cls, *args, **kwargs):\n         use_v2 = should_use_v2()\n-        cls = swap_class(\n-            cls, base_layer.Layer, base_layer_v1.Layer, use_v2\n-        )  # pylint: disable=self-cls-assignment\n+        cls = swap_class(cls, base_layer.Layer, base_layer_v1.Layer, use_v2)\n         return super(LayerVersionSelector, cls).__new__(cls)\n \n \n class TensorBoardVersionSelector:\n     \"\"\"Chooses between Keras v1 and v2 TensorBoard callback class.\"\"\"\n \n-    def __new__(cls, *args, **kwargs):  # pylint: disable=unused-argument\n+    def __new__(cls, *args, **kwargs):\n         use_v2 = should_use_v2()\n         start_cls = cls\n         cls = swap_class(\n\n@@ -133,7 +133,7 @@ class SplitUtilsTest(test_combinations.TestCase):\n                 return 2 * inputs\n \n         with self.assertRaisesRegex(TypeError, \"instantiate abstract class\"):\n-            AbstractModel()  # pylint: disable=abstract-class-instantiated\n+            AbstractModel()\n \n         model = MyModel()\n         model_class = model.__class__.__bases__[0].__bases__[0]\n\n@@ -12,8 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=protected-access\n-# pylint: disable=g-import-not-at-top\n+\n+\n \"\"\"Utilities related to model visualization.\"\"\"\n \n import os\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Wrapper for using the Scikit-Learn API with Keras models.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+\n \n import copy\n import types\n@@ -110,7 +110,7 @@ class BaseWrapper:\n                         \"{} is not a legal parameter\".format(params_name)\n                     )\n \n-    def get_params(self, **params):  # pylint: disable=unused-argument\n+    def get_params(self, **params):\n         \"\"\"Gets parameters for this estimator.\n \n         Args:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
