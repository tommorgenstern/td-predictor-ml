{"custom_id": "scrapy#911c8082b071a7d779fb569cffe09d4a349e7921", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 154 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ class Command(ScrapyCommand):\n         return \"[options] <spider>\"\n \n     def short_desc(self):\n-        return \"Start crawling from a spider\"\n+        return \"Run a spider\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b35166de61e8ff998b6cd84d7f6104fbbfc4f95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 13 | Files Changed: 7 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/4 | Churn Δ: 30 | Churn Cumulative: 1512 | Contributors (this commit): 12 | Commits (past 90d): 13 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,7 +81,7 @@ class CookiesMiddleware(object):\n         else:\n             cookie_list = request.cookies\n \n-        cookies = map(self._format_cookie, cookie_list)\n+        cookies = [self._format_cookie(x) for x in cookie_list]\n         headers = {'Set-Cookie': cookies}\n         response = Response(request.url, headers=headers)\n \n\n@@ -15,7 +15,7 @@ class DummyPolicy(object):\n \n     def __init__(self, settings):\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n-        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))\n+        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]\n \n     def should_cache_request(self, request):\n         return urlparse_cached(request).scheme not in self.ignore_schemes\n\n@@ -34,7 +34,7 @@ class TelnetConsole(protocol.ServerFactory):\n             raise NotConfigured\n         self.crawler = crawler\n         self.noisy = False\n-        self.portrange = map(int, crawler.settings.getlist('TELNETCONSOLE_PORT'))\n+        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n         self.host = crawler.settings['TELNETCONSOLE_HOST']\n         self.crawler.signals.connect(self.start_listening, signals.engine_started)\n         self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n\n@@ -91,7 +91,7 @@ class ContractsManagerTest(unittest.TestCase):\n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request)\n         self.assertEqual(len(contracts), 2)\n-        self.assertEqual(frozenset(map(type, contracts)),\n+        self.assertEqual(frozenset(type(x) for x in contracts),\n             frozenset([UrlContract, ReturnsContract]))\n \n         # returns request for valid method\n@@ -109,13 +109,13 @@ class ContractsManagerTest(unittest.TestCase):\n         # returns_item\n         request = self.conman.from_method(spider.returns_item, self.results)\n         output = request.callback(response)\n-        self.assertEqual(map(type, output), [TestItem])\n+        self.assertEqual([type(x) for x in output], [TestItem])\n         self.should_succeed()\n \n         # returns_request\n         request = self.conman.from_method(spider.returns_request, self.results)\n         output = request.callback(response)\n-        self.assertEqual(map(type, output), [Request])\n+        self.assertEqual([type(x) for x in output], [Request])\n         self.should_succeed()\n \n         # returns_fail\n@@ -130,7 +130,7 @@ class ContractsManagerTest(unittest.TestCase):\n         # scrapes_item_ok\n         request = self.conman.from_method(spider.scrapes_item_ok, self.results)\n         output = request.callback(response)\n-        self.assertEqual(map(type, output), [TestItem])\n+        self.assertEqual([type(x) for x in output], [TestItem])\n         self.should_succeed()\n \n         # scrapes_item_fail\n\n@@ -8,7 +8,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n         if hasattr(module, '__version__'):\n-            installed_version = map(int, module.__version__.split('.')[:2])\n+            installed_version = [int(x) for x in module.__version__.split('.')[:2]]\n             assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n \n if __name__ == \"__main__\":\n\n@@ -63,12 +63,14 @@ class FilesPipelineTestCase(unittest.TestCase):\n             mock.patch.object(FilesPipeline, 'get_media_requests',\n                               return_value=[_prepare_request_object(item_url)])\n         ]\n-        map(lambda p: p.start(), patchers)\n+        for p in patchers:\n+            p.start()\n \n         result = yield self.pipeline.process_item(item, None)\n         self.assertEqual(result['files'][0]['checksum'], 'abc')\n \n-        map(lambda p: p.stop(), patchers)\n+        for p in patchers:\n+            p.stop()\n \n     @defer.inlineCallbacks\n     def test_file_expired(self):\n@@ -82,12 +84,14 @@ class FilesPipelineTestCase(unittest.TestCase):\n                               return_value=[_prepare_request_object(item_url)]),\n             mock.patch.object(FilesPipeline, 'inc_stats', return_value=True)\n         ]\n-        map(lambda p: p.start(), patchers)\n+        for p in patchers:\n+            p.start()\n \n         result = yield self.pipeline.process_item(item, None)\n         self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n \n-        map(lambda p: p.stop(), patchers)\n+        for p in patchers:\n+            p.stop()\n \n class FilesPipelineTestCaseFields(unittest.TestCase):\n \n\n@@ -69,7 +69,7 @@ class WebService(server.Site):\n             raise NotConfigured\n         self.crawler = crawler\n         logfile = crawler.settings['WEBSERVICE_LOGFILE']\n-        self.portrange = map(int, crawler.settings.getlist('WEBSERVICE_PORT'))\n+        self.portrange = [int(x) for x in crawler.settings.getlist('WEBSERVICE_PORT')]\n         self.host = crawler.settings['WEBSERVICE_HOST']\n         root = RootResource(crawler)\n         reslist = build_component_list(crawler.settings['WEBSERVICE_RESOURCES_BASE'], \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a958cb2c5df3bde708278130f5f6e375a0396d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 18 | Files Changed: 7 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 4125 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -108,7 +108,7 @@ class ExecutionEngine(object):\n \n         if slot.start_requests and not self._needs_backout(spider):\n             try:\n-                request = slot.start_requests.next()\n+                request = next(slot.start_requests)\n             except StopIteration:\n                 slot.start_requests = None\n             except Exception, exc:\n\n@@ -61,7 +61,7 @@ class XmliterTestCase(unittest.TestCase):\n         response = XmlResponse(url='http://mydummycompany.com', body=body)\n         my_iter = self.xmliter(response, 'item')\n \n-        node = my_iter.next()\n+        node = next(my_iter)\n         node.register_namespace('g', 'http://base.google.com/ns/1.0')\n         self.assertEqual(node.xpath('title/text()').extract(), ['Item 1'])\n         self.assertEqual(node.xpath('description/text()').extract(), ['This is item 1'])\n@@ -77,10 +77,10 @@ class XmliterTestCase(unittest.TestCase):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n \n         iter = self.xmliter(body, 'product')\n-        iter.next()\n-        iter.next()\n+        next(iter)\n+        next(iter)\n \n-        self.assertRaises(StopIteration, iter.next)\n+        self.assertRaises(StopIteration, next, iter)\n \n     def test_xmliter_encoding(self):\n         body = '<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n<xml>\\n    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n</xml>\\n\\n'\n@@ -122,9 +122,9 @@ class LxmlXmliterTestCase(XmliterTestCase):\n         self.assertEqual(len(list(no_namespace_iter)), 0)\n \n         namespace_iter = self.xmliter(response, 'image_link', 'http://base.google.com/ns/1.0')\n-        node = namespace_iter.next()\n+        node = next(namespace_iter)\n         self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])\n-        node = namespace_iter.next()\n+        node = next(namespace_iter)\n         self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item2.jpg'])\n \n \n@@ -205,12 +205,12 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n         iter = csviter(response)\n-        iter.next()\n-        iter.next()\n-        iter.next()\n-        iter.next()\n+        next(iter)\n+        next(iter)\n+        next(iter)\n+        next(iter)\n \n-        self.assertRaises(StopIteration, iter.next)\n+        self.assertRaises(StopIteration, next, iter)\n \n     def test_csviter_encoding(self):\n         body1 = get_testdata('feeds', 'feed-sample4.csv')\n\n@@ -121,7 +121,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n     def test_weakkeycache(self):\n         class _Weakme(object): pass\n         _values = count()\n-        wk = WeakKeyCache(lambda k: _values.next())\n+        wk = WeakKeyCache(lambda k: next(_values))\n         k = _Weakme()\n         v = wk[k]\n         self.assertEqual(v, wk[k])\n\n@@ -93,7 +93,7 @@ def iter_errback(iterable, errback, *a, **kw):\n     it = iter(iterable)\n     while 1:\n         try:\n-            yield it.next()\n+            yield next(it)\n         except StopIteration:\n             break\n         except:\n\n@@ -47,7 +47,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     \"\"\"\n     encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n     def _getrow(csv_r):\n-        return [str_to_unicode(field, encoding) for field in csv_r.next()]\n+        return [str_to_unicode(field, encoding) for field in next(csv_r)]\n \n     lines = StringIO(body_or_str(obj, unicode=False))\n     if delimiter:\n\n@@ -70,9 +70,9 @@ class OrderedDict(dict, DictMixin):\n         if not self:\n             raise KeyError('dictionary is empty')\n         if last:\n-            key = reversed(self).next()\r\n+            key = next(reversed(self))\n         else:\n-            key = iter(self).next()\r\n+            key = next(iter(self))\n         value = self.pop(key)\n         return key, value\n \n\n@@ -810,7 +810,7 @@ def _tokenize(description):\n             current = ''\n             ops = nextOps[n]\n         elif n == '\\\\':\n-            current += description.next()\n+            current += next(description)\n         else:\n             current += n\n     yield _STRING, current\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b598476eb287099f9b2ef8e1cff38cbd69e6d0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 32 | Churn Cumulative: 44 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@\n # Jp Calderone & Itamar Shtull-Trauring\n # Johan Dahlin\n \n+from __future__ import print_function\n import optparse\n import os\n import sys\n@@ -27,7 +28,7 @@ class KCacheGrind(object):\n \n     def output(self, out_file):\n         self.out_file = out_file\n-        print >> out_file, 'events: Ticks'\n+        print('events: Ticks', file=out_file)\n         self._print_summary()\n         for entry in self.data:\n             self._entry(entry)\n@@ -37,7 +38,7 @@ class KCacheGrind(object):\n         for entry in self.data:\n             totaltime = int(entry.totaltime * 1000)\n             max_cost = max(max_cost, totaltime)\n-        print >> self.out_file, 'summary: %d' % (max_cost,)\n+        print('summary: %d' % (max_cost,), file=self.out_file)\n \n     def _entry(self, entry):\n         out_file = self.out_file\n@@ -45,16 +46,16 @@ class KCacheGrind(object):\n         code = entry.code\n         #print >> out_file, 'ob=%s' % (code.co_filename,)\n         if isinstance(code, str):\n-            print >> out_file, 'fi=~'\n+            print('fi=~', file=out_file)\n         else:\n-            print >> out_file, 'fi=%s' % (code.co_filename,)\n-        print >> out_file, 'fn=%s' % (label(code),)\n+            print('fi=%s' % (code.co_filename,), file=out_file)\n+        print('fn=%s' % (label(code),), file=out_file)\n \n         inlinetime = int(entry.inlinetime * 1000)\n         if isinstance(code, str):\n-            print >> out_file, '0 ', inlinetime\n+            print('0 ', inlinetime, file=out_file)\n         else:\n-            print >> out_file, '%d %d' % (code.co_firstlineno, inlinetime)\n+            print('%d %d' % (code.co_firstlineno, inlinetime), file=out_file)\n \n         # recursive calls are counted in entry.calls\n         if entry.calls:\n@@ -69,23 +70,23 @@ class KCacheGrind(object):\n \n         for subentry in calls:\n             self._subentry(lineno, subentry)\n-        print >> out_file\n+        print(file=out_file)\n \n     def _subentry(self, lineno, subentry):\n         out_file = self.out_file\n         code = subentry.code\n         #print >> out_file, 'cob=%s' % (code.co_filename,)\n-        print >> out_file, 'cfn=%s' % (label(code),)\n+        print('cfn=%s' % (label(code),), file=out_file)\n         if isinstance(code, str):\n-            print >> out_file, 'cfi=~'\n-            print >> out_file, 'calls=%d 0' % (subentry.callcount,)\n+            print('cfi=~', file=out_file)\n+            print('calls=%d 0' % (subentry.callcount,), file=out_file)\n         else:\n-            print >> out_file, 'cfi=%s' % (code.co_filename,)\n-            print >> out_file, 'calls=%d %d' % (\n-                subentry.callcount, code.co_firstlineno)\n+            print('cfi=%s' % (code.co_filename,), file=out_file)\n+            print('calls=%d %d' % (\n+                subentry.callcount, code.co_firstlineno), file=out_file)\n \n         totaltime = int(subentry.totaltime * 1000)\n-        print >> out_file, '%d %d' % (lineno, totaltime)\n+        print('%d %d' % (lineno, totaltime), file=out_file)\n \n def main(args):\n     usage = \"%s [-o output_file_path] scriptfile [arg] ...\"\n\n@@ -1,4 +1,5 @@\n \"\"\"Refactored \"safe reference\" from dispatcher.py\"\"\"\n+from __future__ import print_function\n import weakref, traceback\n \n def safeRef(target, onDelete = None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#343f997ed6a20241480316c6d24490f071a3b524", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 1390 | Contributors (this commit): 13 | Commits (past 90d): 16 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,8 @@ from __future__ import print_function\n import os\n import shutil\n import string\n+\n+from importlib import import_module\n from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n@@ -90,7 +92,7 @@ class Command(ScrapyCommand):\n             'classname': '%sSpider' % ''.join([s.capitalize() \\\n                 for s in module.split('_')])\n         }\n-        spiders_module = __import__(self.settings['NEWSPIDER_MODULE'], {}, {}, [''])\n+        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n         spiders_dir = abspath(dirname(spiders_module.__file__))\n         spider_file = \"%s.py\" % join(spiders_dir, module)\n         shutil.copyfile(template_file, spider_file)\n\n@@ -1,5 +1,6 @@\n import sys\n import os\n+from importlib import import_module\n \n from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n@@ -15,7 +16,7 @@ def _import_file(filepath):\n     if dirname:\n         sys.path = [dirname] + sys.path\n     try:\n-        module = __import__(fname, {}, {}, [''])\n+        module = import_module(fname)\n     finally:\n         if dirname:\n             sys.path.pop(0)\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n import os\n import cPickle as pickle\n+from importlib import import_module\n from time import time\n from weakref import WeakKeyDictionary\n from email.utils import mktime_tz, parsedate_tz\n@@ -164,7 +165,7 @@ class DbmCacheStorage(object):\n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'], {}, {}, [''])\n+        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n         self.db = None\n \n     def open_spider(self, spider):\n\n@@ -3,7 +3,7 @@ from twisted.trial import unittest\n class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n-            module = __import__('OpenSSL', {}, {}, [''])\n+            module = __import__('OpenSSL')\n         except ImportError as ex:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n\n@@ -2,6 +2,8 @@\n \n import re\n import hashlib\n+\n+from importlib import import_module\n from pkgutil import iter_modules\n \n from w3lib.html import remove_entities\n@@ -35,7 +37,7 @@ def load_object(path):\n \n     module, name = path[:dot], path[dot+1:]\n     try:\n-        mod = __import__(module, {}, {}, [''])\n+        mod = import_module(module)\n     except ImportError as e:\n         raise ImportError(\"Error loading object '%s': %s\" % (path, e))\n \n@@ -55,7 +57,7 @@ def walk_modules(path, load=False):\n     \"\"\"\n \n     mods = []\n-    mod = __import__(path, {}, {}, [''])\n+    mod = import_module(path)\n     mods.append(mod)\n     if hasattr(mod, '__path__'):\n         for _, subpath, ispkg in iter_modules(mod.__path__):\n@@ -63,7 +65,7 @@ def walk_modules(path, load=False):\n             if ispkg:\n                 mods += walk_modules(fullpath)\n             else:\n-                submod = __import__(fullpath, {}, {}, [''])\n+                submod = import_module(fullpath)\n                 mods.append(submod)\n     return mods\n \n\n@@ -1,8 +1,10 @@\n import os\n-from os.path import join, dirname, abspath, isabs, exists\n import cPickle as pickle\n import warnings\n \n+from importlib import import_module\n+from os.path import join, dirname, abspath, isabs, exists\n+\n from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n from scrapy.settings import CrawlerSettings\n from scrapy.exceptions import NotConfigured\n@@ -14,7 +16,7 @@ def inside_project():\n     scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n     if scrapy_module is not None:\n         try:\n-            __import__(scrapy_module)\n+            import_module(scrapy_module)\n         except ImportError as exc:\n             warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n         else:\n@@ -53,7 +55,7 @@ def get_project_settings():\n         init_env(project)\n     settings_module_path = os.environ.get(ENVVAR)\n     if settings_module_path:\n-        settings_module = __import__(settings_module_path, {}, {}, [''])\n+        settings_module = import_module(settings_module_path)\n     else:\n         settings_module = None\n     settings = CrawlerSettings(settings_module)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10e22aa5fba928f4092b4aec597f4539566dca59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 579 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/extensions.rst\n \n import socket\n from pprint import pformat\n+from importlib import import_module\n \n from twisted.internet import task\n \n@@ -20,7 +21,8 @@ class MemoryUsage(object):\n         if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n         try:\n-            self.resource = __import__('resource')\n+            # stdlib's resource module is only availabe on unix platforms.\n+            self.resource = import_module('resource')\n         except ImportError:\n             raise NotConfigured\n \n\n@@ -15,6 +15,7 @@ Scrapy developers, if you add a setting here remember to:\n \n import os\n import sys\n+from importlib import import_module\n from os.path import join, abspath, dirname\n \n BOT_NAME = 'scrapybot'\n@@ -229,7 +230,7 @@ TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n \n URLLENGTH_LIMIT = 2083\n \n-USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__\n+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % import_module('scrapy').__version__\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n\n@@ -1,9 +1,10 @@\n+from importlib import import_module\n from twisted.trial import unittest\n \n class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n-            module = __import__('OpenSSL')\n+            module = import_module('OpenSSL')\n         except ImportError as ex:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n\n@@ -4,6 +4,7 @@ This module contains some assorted functions used in tests\n \n import os\n \n+from importlib import import_module\n from twisted.trial.unittest import SkipTest\n \n \n@@ -39,7 +40,7 @@ def get_crawler(settings_dict=None):\n def get_pythonpath():\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n     installation of Scrapy\"\"\"\n-    scrapy_path = __import__('scrapy').__path__[0]\n+    scrapy_path = import_module('scrapy').__path__[0]\n     return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')\n \n def get_testenv():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2df8156431fc052bc38aae53f8d51b2113daa711", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 155 | Files Changed: 7 | Hunks: 7 | Methods Changed: 15 | Complexity Δ (Sum/Max): -30/0 | Churn Δ: 158 | Churn Cumulative: 1298 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,8 +8,8 @@ version_info = tuple(__version__.split('.')[:3])\n \n import sys, os, warnings\n \n-if sys.version_info < (2, 6):\n-    print(\"Scrapy %s requires Python 2.6 or above\" % __version__)\n+if sys.version_info < (2, 7):\n+    print(\"Scrapy %s requires Python 2.7\" % __version__)\n     sys.exit(1)\n \n # ignore noisy twisted deprecation warnings\n\n@@ -63,8 +63,6 @@ class TestDefaultHeadersMiddleware(TestCase):\n \n \n     def test_no_proxy(self):\n-        if sys.version_info < (2, 6):\n-            raise SkipTest('no_proxy is not supported in python < 2.6')\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n \n\n@@ -6,8 +6,7 @@ This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n import copy\n-\n-from scrapy.utils.py27 import OrderedDict\n+from collections import OrderedDict\n \n \n class MultiValueDictKeyError(KeyError):\n\n@@ -1,10 +0,0 @@\n-import json\n-from shutil import copytree, ignore_patterns\n-from multiprocessing import cpu_count\n-from pkgutil import get_data\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-import warnings\n-warnings.warn(\"Module `scrapy.utils.py26` is deprecated and will be removed in Scrapy 0.17\",\n-    ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -1,11 +0,0 @@\n-\"\"\"\n-This module provides functions added in Python 2.7, which weren't yet available\n-in Python 2.6. The Python 2.7 function is used when available.\n-\"\"\"\n-\n-__all__ = ['OrderedDict']\n-\n-try:\n-    from collections import OrderedDict\n-except ImportError:\n-    from scrapy.xlib.ordereddict import OrderedDict\n\n@@ -1,127 +0,0 @@\n-# Copyright (c) 2009 Raymond Hettinger\n-#\n-# Permission is hereby granted, free of charge, to any person\n-# obtaining a copy of this software and associated documentation files\n-# (the \"Software\"), to deal in the Software without restriction,\n-# including without limitation the rights to use, copy, modify, merge,\n-# publish, distribute, sublicense, and/or sell copies of the Software,\n-# and to permit persons to whom the Software is furnished to do so,\n-# subject to the following conditions:\n-#\n-#     The above copyright notice and this permission notice shall be\n-#     included in all copies or substantial portions of the Software.\n-#\n-#     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n-#     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n-#     OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n-#     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n-#     HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n-#     WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n-#     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n-#     OTHER DEALINGS IN THE SOFTWARE.\n-\n-from UserDict import DictMixin\n-\n-class OrderedDict(dict, DictMixin):\n-\n-    def __init__(self, *args, **kwds):\n-        if len(args) > 1:\n-            raise TypeError('expected at most 1 arguments, got %d' % len(args))\n-        try:\n-            self.__end\n-        except AttributeError:\n-            self.clear()\n-        self.update(*args, **kwds)\n-\n-    def clear(self):\n-        self.__end = end = []\n-        end += [None, end, end]         # sentinel node for doubly linked list\n-        self.__map = {}                 # key --> [key, prev, next]\n-        dict.clear(self)\n-\n-    def __setitem__(self, key, value):\n-        if key not in self:\n-            end = self.__end\n-            curr = end[1]\n-            curr[2] = end[1] = self.__map[key] = [key, curr, end]\n-        dict.__setitem__(self, key, value)\n-\n-    def __delitem__(self, key):\n-        dict.__delitem__(self, key)\n-        key, prev, next = self.__map.pop(key)\n-        prev[2] = next\n-        next[1] = prev\n-\n-    def __iter__(self):\n-        end = self.__end\n-        curr = end[2]\n-        while curr is not end:\n-            yield curr[0]\n-            curr = curr[2]\n-\n-    def __reversed__(self):\n-        end = self.__end\n-        curr = end[1]\n-        while curr is not end:\n-            yield curr[0]\n-            curr = curr[1]\n-\n-    def popitem(self, last=True):\n-        if not self:\n-            raise KeyError('dictionary is empty')\n-        if last:\n-            key = next(reversed(self))\n-        else:\n-            key = next(iter(self))\n-        value = self.pop(key)\n-        return key, value\n-\n-    def __reduce__(self):\n-        items = [[k, self[k]] for k in self]\n-        tmp = self.__map, self.__end\n-        del self.__map, self.__end\n-        inst_dict = vars(self).copy()\n-        self.__map, self.__end = tmp\n-        if inst_dict:\n-            return (self.__class__, (items,), inst_dict)\n-        return self.__class__, (items,)\n-\n-    def keys(self):\n-        return list(self)\n-\n-    setdefault = DictMixin.setdefault\n-    update = DictMixin.update\n-    pop = DictMixin.pop\n-    values = DictMixin.values\n-    items = DictMixin.items\n-    iterkeys = DictMixin.iterkeys\n-    itervalues = DictMixin.itervalues\n-    iteritems = DictMixin.iteritems\n-\n-    def __repr__(self):\n-        if not self:\n-            return '%s()' % (self.__class__.__name__,)\n-        return '%s(%r)' % (self.__class__.__name__, self.items())\n-\n-    def copy(self):\n-        return self.__class__(self)\n-\n-    @classmethod\n-    def fromkeys(cls, iterable, value=None):\n-        d = cls()\n-        for key in iterable:\n-            d[key] = value\n-        return d\n-\n-    def __eq__(self, other):\n-        if isinstance(other, OrderedDict):\n-            if len(self) != len(other):\n-                return False\n-            for p, q in  zip(self.items(), other.items()):\n-                if p != q:\n-                    return False\n-            return True\n-        return dict.__eq__(self, other)\n-\n-    def __ne__(self, other):\n-        return not self == other\n\n@@ -104,7 +104,6 @@ setup_args = {\n     'scripts': scripts,\n     'classifiers': [\n         'Programming Language :: Python',\n-        'Programming Language :: Python :: 2.6',\n         'Programming Language :: Python :: 2.7',\n         'License :: OSI Approved :: BSD License',\n         'Operating System :: OS Independent',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c7292a08e6c13ff9f6bd1ad6732a4651d035955", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 285 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,8 @@ Scrapy - a screen scraping framework written in Python\n from __future__ import print_function\n import pkgutil\n __version__ = pkgutil.get_data(__package__, 'VERSION').strip()\n+if not isinstance(__version__, str):\n+    __version__ = __version__.decode('ascii')\n version_info = tuple(__version__.split('.')[:3])\n \n import sys, os, warnings\n\n@@ -14,7 +14,8 @@ import weakref, os\n from collections import defaultdict\n from time import time\n from operator import itemgetter\n-from types import NoneType\n+\n+NoneType = type(None)\n \n live_refs = defaultdict(weakref.WeakKeyDictionary)\n \n\n@@ -1,3 +1,6 @@\n+import sys\n+\n+if sys.version_info[0] == 2:\n     from urlparse import urlparse\n \n     # workaround for http://bugs.python.org/issue7904 - Python < 2.7\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2318c56f14cd887a358f80768cf660646d6246d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 798 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,6 +80,7 @@ class CrawlerProcess(object):\n         self.settings = settings\n         self.crawlers = {}\n         self.stopping = False\n+        self._started = None\n \n     def create_crawler(self, name=None):\n         if name not in self.crawlers:\n@@ -94,8 +95,8 @@ class CrawlerProcess(object):\n     @defer.inlineCallbacks\n     def stop(self):\n         self.stopping = True\n-        for crawler in self.crawlers.itervalues():\n-            yield crawler.stop()\n+        if self._active_crawler:\n+            yield self._active_crawler.stop()\n \n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n@@ -129,19 +130,19 @@ class CrawlerProcess(object):\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n     def _start_crawler(self):\n-        if self.crawlers and not self.stopping:\n-            name, crawler = self.crawlers.popitem()\n+        if not self.crawlers or self.stopping:\n+            return\n \n+        name, crawler = self.crawlers.popitem()\n+        self._active_crawler = crawler\n         sflo = log.start_from_crawler(crawler)\n         crawler.configure()\n         crawler.install()\n         crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n         if sflo:\n             crawler.signals.connect(sflo.stop, signals.engine_stopped)\n-\n         crawler.signals.connect(self._check_done, signals.engine_stopped)\n         crawler.start()\n-\n         return name, crawler\n \n     def _check_done(self, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6bed44c2b76bba687e240b42445a1f703e939df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 174 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -75,13 +75,7 @@ class DjangoItemTest(unittest.TestCase):\n         long_name = 'z' * 300\n         i = BasePersonItem(name=long_name)\n         self.assertFalse(i.is_valid())\n-        self.assertEqual(\n-            {\n-                'age': [u'This field cannot be null.'],\n-                'name': [u'Ensure this value has at most 255 characters (it has 300).']\n-            },\n-            i.errors)\n-\n+        self.assertEqual(set(i.errors), set(['age', 'name']))\n         i = BasePersonItem(name='John')\n         self.assertTrue(i.is_valid(exclude=['age']))\n         self.assertEqual({}, i.errors)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b1760d7a1d8949f14a43aa7ce00726e084b2abe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 1421 | Contributors (this commit): 13 | Commits (past 90d): 23 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,8 @@ from __future__ import print_function\n import os\n import shutil\n import string\n+\n+from importlib import import_module\n from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n@@ -90,7 +92,7 @@ class Command(ScrapyCommand):\n             'classname': '%sSpider' % ''.join([s.capitalize() \\\n                 for s in module.split('_')])\n         }\n-        spiders_module = __import__(self.settings['NEWSPIDER_MODULE'], {}, {}, [''])\n+        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n         spiders_dir = abspath(dirname(spiders_module.__file__))\n         spider_file = \"%s.py\" % join(spiders_dir, module)\n         shutil.copyfile(template_file, spider_file)\n\n@@ -1,5 +1,6 @@\n import sys\n import os\n+from importlib import import_module\n \n from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n@@ -15,7 +16,7 @@ def _import_file(filepath):\n     if dirname:\n         sys.path = [dirname] + sys.path\n     try:\n-        module = __import__(fname, {}, {}, [''])\n+        module = import_module(fname)\n     finally:\n         if dirname:\n             sys.path.pop(0)\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n import os\n import cPickle as pickle\n+from importlib import import_module\n from time import time\n from weakref import WeakKeyDictionary\n from email.utils import mktime_tz, parsedate_tz\n@@ -164,7 +165,7 @@ class DbmCacheStorage(object):\n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'], {}, {}, [''])\n+        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n         self.db = None\n \n     def open_spider(self, spider):\n\n@@ -3,7 +3,7 @@ from twisted.trial import unittest\n class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n-            module = __import__('OpenSSL', {}, {}, [''])\n+            module = __import__('OpenSSL')\n         except ImportError as ex:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n\n@@ -2,6 +2,8 @@\n \n import re\n import hashlib\n+\n+from importlib import import_module\n from pkgutil import iter_modules\n \n from w3lib.html import remove_entities\n@@ -35,7 +37,7 @@ def load_object(path):\n \n     module, name = path[:dot], path[dot+1:]\n     try:\n-        mod = __import__(module, {}, {}, [''])\n+        mod = import_module(module)\n     except ImportError as e:\n         raise ImportError(\"Error loading object '%s': %s\" % (path, e))\n \n@@ -55,7 +57,7 @@ def walk_modules(path, load=False):\n     \"\"\"\n \n     mods = []\n-    mod = __import__(path, {}, {}, [''])\n+    mod = import_module(path)\n     mods.append(mod)\n     if hasattr(mod, '__path__'):\n         for _, subpath, ispkg in iter_modules(mod.__path__):\n@@ -63,7 +65,7 @@ def walk_modules(path, load=False):\n             if ispkg:\n                 mods += walk_modules(fullpath)\n             else:\n-                submod = __import__(fullpath, {}, {}, [''])\n+                submod = import_module(fullpath)\n                 mods.append(submod)\n     return mods\n \n\n@@ -1,8 +1,10 @@\n import os\n-from os.path import join, dirname, abspath, isabs, exists\n import cPickle as pickle\n import warnings\n \n+from importlib import import_module\n+from os.path import join, dirname, abspath, isabs, exists\n+\n from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n from scrapy.settings import CrawlerSettings\n from scrapy.exceptions import NotConfigured\n@@ -14,7 +16,7 @@ def inside_project():\n     scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n     if scrapy_module is not None:\n         try:\n-            __import__(scrapy_module)\n+            import_module(scrapy_module)\n         except ImportError as exc:\n             warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n         else:\n@@ -53,7 +55,7 @@ def get_project_settings():\n         init_env(project)\n     settings_module_path = os.environ.get(ENVVAR)\n     if settings_module_path:\n-        settings_module = __import__(settings_module_path, {}, {}, [''])\n+        settings_module = import_module(settings_module_path)\n     else:\n         settings_module = None\n     settings = CrawlerSettings(settings_module)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f5423aebd1d985712200593cca2f2eb134e144a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 594 | Contributors (this commit): 12 | Commits (past 90d): 16 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/extensions.rst\n \n import socket\n from pprint import pformat\n+from importlib import import_module\n \n from twisted.internet import task\n \n@@ -20,7 +21,8 @@ class MemoryUsage(object):\n         if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n         try:\n-            self.resource = __import__('resource')\n+            # stdlib's resource module is only availabe on unix platforms.\n+            self.resource = import_module('resource')\n         except ImportError:\n             raise NotConfigured\n \n\n@@ -15,6 +15,7 @@ Scrapy developers, if you add a setting here remember to:\n \n import os\n import sys\n+from importlib import import_module\n from os.path import join, abspath, dirname\n \n BOT_NAME = 'scrapybot'\n@@ -229,7 +230,7 @@ TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n \n URLLENGTH_LIMIT = 2083\n \n-USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__\n+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % import_module('scrapy').__version__\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n\n@@ -1,9 +1,10 @@\n+from importlib import import_module\n from twisted.trial import unittest\n \n class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n-            module = __import__('OpenSSL')\n+            module = import_module('OpenSSL')\n         except ImportError as ex:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n\n@@ -4,6 +4,7 @@ This module contains some assorted functions used in tests\n \n import os\n \n+from importlib import import_module\n from twisted.trial.unittest import SkipTest\n \n \n@@ -39,7 +40,7 @@ def get_crawler(settings_dict=None):\n def get_pythonpath():\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n     installation of Scrapy\"\"\"\n-    scrapy_path = __import__('scrapy').__path__[0]\n+    scrapy_path = import_module('scrapy').__path__[0]\n     return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')\n \n def get_testenv():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc7fa61136cfe51792c093416e4f18e7c7def86c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 182 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -75,13 +75,7 @@ class DjangoItemTest(unittest.TestCase):\n         long_name = 'z' * 300\n         i = BasePersonItem(name=long_name)\n         self.assertFalse(i.is_valid())\n-        self.assertEqual(\n-            {\n-                'age': [u'This field cannot be null.'],\n-                'name': [u'Ensure this value has at most 255 characters (it has 300).']\n-            },\n-            i.errors)\n-\n+        self.assertEqual(set(i.errors), set(['age', 'name']))\n         i = BasePersonItem(name='John')\n         self.assertTrue(i.is_valid(exclude=['age']))\n         self.assertEqual({}, i.errors)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c74903f9da8d85909c7225c6a107a109f98056d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 163 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -82,8 +82,8 @@ def process_parallel(callbacks, input, *a, **kw):\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n-    d = defer.gatherResults(dfds)\n-    d.addErrback(lambda _: _.value.subFailure)\n+    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n+    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n     return d\n \n def iter_errback(iterable, errback, *a, **kw):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#755b9ba5a416bd8f042759b96651d6137c091f3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 149 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -59,13 +59,10 @@ class TelnetConsole(protocol.ServerFactory):\n \n     def _get_telnet_vars(self):\n         # Note: if you add entries here also update topics/telnetconsole.rst\n-        slots = self.crawler.engine.slots\n-        if len(slots) == 1:\n-            spider, slot = slots.items()[0]\n         telnet_vars = {\n             'engine': self.crawler.engine,\n-            'spider': spider,\n-            'slot': slot,\n+            'spider': self.crawler.engine.spider,\n+            'slot': self.crawler.engine.slot,\n             'crawler': self.crawler,\n             'extensions': self.crawler.extensions,\n             'stats': self.crawler.stats,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec7833a910536a6c876a9a3f06979e5ba9bab8d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/6 | Churn Δ: 47 | Churn Cumulative: 479 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,22 +5,28 @@ from twisted.internet import defer, threads\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n+\n def deprecated(use_instead=None):\n     \"\"\"This is a decorator which can be used to mark functions\n     as deprecated. It will result in a warning being emitted\n     when the function is used.\"\"\"\n \n-    def wrapped(func):\n+    def deco(func):\n         @wraps(func)\n-        def new_func(*args, **kwargs):\n+        def wrapped(*args, **kwargs):\n             message = \"Call to deprecated function %s.\" % func.__name__\n             if use_instead:\n                 message += \" Use %s instead.\" % use_instead\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             return func(*args, **kwargs)\n-        return new_func\n         return wrapped\n \n+    if callable(use_instead):\n+        deco = deco(use_instead)\n+        use_instead = None\n+    return deco\n+\n+\n def defers(func):\n     \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n     @wraps(func)\n\n@@ -1,11 +1,10 @@\n import re, csv\n from cStringIO import StringIO\n \n-from scrapy.http import TextResponse\n+from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n from scrapy import log\n from scrapy.utils.python import re_rsearch, str_to_unicode\n-from scrapy.utils.response import body_or_str\n \n \n def xmliter(obj, nodename):\n@@ -19,7 +18,7 @@ def xmliter(obj, nodename):\n     \"\"\"\n     HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename, re.S)\n     HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename, re.S)\n-    text = body_or_str(obj)\n+    text = _body_or_str(obj)\n \n     header_start = re.search(HEADER_START_RE, text)\n     header_start = header_start.group(1).strip() if header_start else ''\n@@ -49,7 +48,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     def _getrow(csv_r):\n         return [str_to_unicode(field, encoding) for field in next(csv_r)]\n \n-    lines = StringIO(body_or_str(obj, unicode=False))\n+    lines = StringIO(_body_or_str(obj, unicode=False))\n     if delimiter:\n         csv_r = csv.reader(lines, delimiter=delimiter)\n     else:\n@@ -67,3 +66,13 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n         else:\n             yield dict(zip(headers, row))\n \n+\n+def _body_or_str(obj, unicode=True):\n+    assert isinstance(obj, (Response, basestring)), \\\n+        \"obj must be Response or basestring, not %s\" % type(obj).__name__\n+    if isinstance(obj, Response):\n+        return obj.body_as_unicode() if unicode else obj.body\n+    elif isinstance(obj, str):\n+        return obj.decode('utf-8') if unicode else obj\n+    else:\n+        return obj if unicode else obj.encode('utf-8')\n\n@@ -13,17 +13,15 @@ from twisted.web import http\n from twisted.web.http import RESPONSES\n from w3lib import html\n \n-from scrapy.http import Response, HtmlResponse, TextResponse\n+from scrapy.http import HtmlResponse, TextResponse\n+from scrapy.utils.decorator import deprecated\n+\n+\n+@deprecated\n+def body_or_str(*a, **kw):\n+    from scrapy.utils.iterators import _body_or_str\n+    return _body_or_str(*a, **kw)\n \n-def body_or_str(obj, unicode=True):\n-    assert isinstance(obj, (Response, basestring)), \\\n-        \"obj must be Response or basestring, not %s\" % type(obj).__name__\n-    if isinstance(obj, Response):\n-        return obj.body_as_unicode() if unicode else obj.body\n-    elif isinstance(obj, str):\n-        return obj.decode('utf-8') if unicode else obj\n-    else:\n-        return obj if unicode else obj.encode('utf-8')\n \n _baseurl_cache = weakref.WeakKeyDictionary()\n def get_base_url(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
