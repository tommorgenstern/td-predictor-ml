{"custom_id": "scrapy#c2baf4d0dad5f656e51dce6e00118fbc0419d0db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7546 | Contributors (this commit): 46 | Commits (past 90d): 4 | Contributors (cumulative): 46 | DMM Complexity: None\n\nDIFF:\n@@ -2299,7 +2299,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     content[feed[\"format\"]].append(file.read_bytes())\n         finally:\n             self.tearDown()\n-        defer.returnValue(content)\n+        return content\n \n     @defer.inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa0c598096de6e26a7b22e7d53cf8c073f96f3a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 419 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 15 | Methods Changed: 45 | Complexity Δ (Sum/Max): 54/42 | Churn Δ: 430 | Churn Cumulative: 4611 | Contributors (this commit): 38 | Commits (past 90d): 10 | Contributors (cumulative): 55 | DMM Complexity: 1.0\n\nDIFF:\n@@ -178,6 +178,48 @@ class Crawler:\n             assert self.engine\n             yield maybeDeferred(self.engine.stop)\n \n+    @staticmethod\n+    def _get_component(component_class, components):\n+        for component in components:\n+            if isinstance(component, component_class):\n+                return component\n+        return None\n+\n+    def get_addon(self, cls):\n+        return self._get_component(cls, self.addons.addons)\n+\n+    def get_downloader_middleware(self, cls):\n+        if not self.engine:\n+            raise RuntimeError(\n+                \"Crawler.get_downloader_middleware() can only be called after \"\n+                \"the crawl engine has been created.\"\n+            )\n+        return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n+\n+    def get_extension(self, cls):\n+        if not self.extensions:\n+            raise RuntimeError(\n+                \"Crawler.get_extension() can only be called after the \"\n+                \"extension manager has been created.\"\n+            )\n+        return self._get_component(cls, self.extensions.middlewares)\n+\n+    def get_item_pipeline(self, cls):\n+        if not self.engine:\n+            raise RuntimeError(\n+                \"Crawler.get_item_pipeline() can only be called after the \"\n+                \"crawl engine has been created.\"\n+            )\n+        return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n+\n+    def get_spider_middleware(self, cls):\n+        if not self.engine:\n+            raise RuntimeError(\n+                \"Crawler.get_spider_middleware() can only be called after the \"\n+                \"crawl engine has been created.\"\n+            )\n+        return self._get_component(cls, self.engine.scraper.spidermw.middlewares)\n+\n \n class CrawlerRunner:\n     \"\"\"\n\n@@ -12,12 +12,13 @@ import pytest\n from packaging.version import parse as parse_version\n from pexpect.popen_spawn import PopenSpawn\n from pytest import mark, raises\n-from twisted.internet import defer\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.trial import unittest\n from w3lib import __version__ as w3lib_version\n from zope.interface.exceptions import MultipleInvalid\n \n import scrapy\n+from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extensions import telnet\n@@ -29,6 +30,19 @@ from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer, get_mockserver_env\n \n+# To prevent warnings.\n+BASE_SETTINGS = {\n+    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+}\n+\n+\n+def get_raw_crawler(spidercls=None, settings_dict=None):\n+    \"\"\"get_crawler alternative that only calls the __init__ method of the\n+    crawler.\"\"\"\n+    settings = Settings()\n+    settings.setdict(settings_dict or {})\n+    return Crawler(spidercls or DefaultSpider, settings)\n+\n \n class BaseCrawlerTest(unittest.TestCase):\n     def assertOptionIsDefault(self, settings, key):\n@@ -39,7 +53,7 @@ class BaseCrawlerTest(unittest.TestCase):\n class CrawlerTestCase(BaseCrawlerTest):\n     def test_populate_spidercls_settings(self):\n         spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n-        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n+        project_settings = {**BASE_SETTINGS, \"TEST1\": \"project\", \"TEST3\": \"project\"}\n \n         class CustomSettingsSpider(DefaultSpider):\n             custom_settings = spider_settings\n@@ -71,9 +85,9 @@ class CrawlerTestCase(BaseCrawlerTest):\n         with raises(ValueError):\n             Crawler(DefaultSpider())\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_crawl_twice_deprecated(self):\n-        crawler = Crawler(NoRequestsSpider)\n+        crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n         yield crawler.crawl()\n         with pytest.warns(\n             ScrapyDeprecationWarning,\n@@ -81,6 +95,358 @@ class CrawlerTestCase(BaseCrawlerTest):\n         ):\n             yield crawler.crawl()\n \n+    def test_get_addon(self):\n+        class ParentAddon:\n+            pass\n+\n+        class TrackingAddon(ParentAddon):\n+            instances = []\n+\n+            def __init__(self):\n+                TrackingAddon.instances.append(self)\n+\n+            def update_settings(self, settings):\n+                pass\n+\n+        settings = {\n+            **BASE_SETTINGS,\n+            \"ADDONS\": {\n+                TrackingAddon: 0,\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        self.assertEqual(len(TrackingAddon.instances), 1)\n+        expected = TrackingAddon.instances[-1]\n+\n+        addon = crawler.get_addon(TrackingAddon)\n+        self.assertEqual(addon, expected)\n+\n+        addon = crawler.get_addon(DefaultSpider)\n+        self.assertIsNone(addon)\n+\n+        addon = crawler.get_addon(ParentAddon)\n+        self.assertEqual(addon, expected)\n+\n+        class ChildAddon(TrackingAddon):\n+            pass\n+\n+        addon = crawler.get_addon(ChildAddon)\n+        self.assertIsNone(addon)\n+\n+    @inlineCallbacks\n+    def test_get_downloader_middleware(self):\n+        class ParentDownloaderMiddleware:\n+            pass\n+\n+        class TrackingDownloaderMiddleware(ParentDownloaderMiddleware):\n+            instances = []\n+\n+            def __init__(self):\n+                TrackingDownloaderMiddleware.instances.append(self)\n+\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                return cls(crawler=crawler)\n+\n+            def __init__(self, crawler):\n+                self.crawler = crawler\n+\n+            def start_requests(self):\n+                MySpider.result = crawler.get_downloader_middleware(MySpider.cls)\n+                return\n+                yield\n+\n+        settings = {\n+            **BASE_SETTINGS,\n+            \"DOWNLOADER_MIDDLEWARES\": {\n+                TrackingDownloaderMiddleware: 0,\n+            },\n+        }\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = TrackingDownloaderMiddleware\n+        yield crawler.crawl()\n+        self.assertEqual(len(TrackingDownloaderMiddleware.instances), 1)\n+        self.assertEqual(MySpider.result, TrackingDownloaderMiddleware.instances[-1])\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = DefaultSpider\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ParentDownloaderMiddleware\n+        yield crawler.crawl()\n+        self.assertEqual(MySpider.result, TrackingDownloaderMiddleware.instances[-1])\n+\n+        class ChildDownloaderMiddleware(TrackingDownloaderMiddleware):\n+            pass\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ChildDownloaderMiddleware\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+    def test_get_downloader_middleware_not_crawling(self):\n+        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n+        self.assertRaises(\n+            RuntimeError, crawler.get_downloader_middleware, DefaultSpider\n+        )\n+\n+    @inlineCallbacks\n+    def test_get_downloader_middleware_no_engine(self):\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                try:\n+                    crawler.get_downloader_middleware(DefaultSpider)\n+                except Exception as e:\n+                    MySpider.result = e\n+                    raise\n+\n+        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n+        with raises(RuntimeError):\n+            yield crawler.crawl()\n+\n+    @inlineCallbacks\n+    def test_get_extension(self):\n+        class ParentExtension:\n+            pass\n+\n+        class TrackingExtension(ParentExtension):\n+            instances = []\n+\n+            def __init__(self):\n+                TrackingExtension.instances.append(self)\n+\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                return cls(crawler=crawler)\n+\n+            def __init__(self, crawler):\n+                self.crawler = crawler\n+\n+            def start_requests(self):\n+                MySpider.result = crawler.get_extension(MySpider.cls)\n+                return\n+                yield\n+\n+        settings = {\n+            **BASE_SETTINGS,\n+            \"EXTENSIONS\": {\n+                TrackingExtension: 0,\n+            },\n+        }\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = TrackingExtension\n+        yield crawler.crawl()\n+        self.assertEqual(len(TrackingExtension.instances), 1)\n+        self.assertEqual(MySpider.result, TrackingExtension.instances[-1])\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = DefaultSpider\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ParentExtension\n+        yield crawler.crawl()\n+        self.assertEqual(MySpider.result, TrackingExtension.instances[-1])\n+\n+        class ChildExtension(TrackingExtension):\n+            pass\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ChildExtension\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+    def test_get_extension_not_crawling(self):\n+        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n+        self.assertRaises(RuntimeError, crawler.get_extension, DefaultSpider)\n+\n+    @inlineCallbacks\n+    def test_get_extension_no_engine(self):\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                try:\n+                    crawler.get_extension(DefaultSpider)\n+                except Exception as e:\n+                    MySpider.result = e\n+                    raise\n+\n+        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n+        with raises(RuntimeError):\n+            yield crawler.crawl()\n+\n+    @inlineCallbacks\n+    def test_get_item_pipeline(self):\n+        class ParentItemPipeline:\n+            pass\n+\n+        class TrackingItemPipeline(ParentItemPipeline):\n+            instances = []\n+\n+            def __init__(self):\n+                TrackingItemPipeline.instances.append(self)\n+\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                return cls(crawler=crawler)\n+\n+            def __init__(self, crawler):\n+                self.crawler = crawler\n+\n+            def start_requests(self):\n+                MySpider.result = crawler.get_item_pipeline(MySpider.cls)\n+                return\n+                yield\n+\n+        settings = {\n+            **BASE_SETTINGS,\n+            \"ITEM_PIPELINES\": {\n+                TrackingItemPipeline: 0,\n+            },\n+        }\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = TrackingItemPipeline\n+        yield crawler.crawl()\n+        self.assertEqual(len(TrackingItemPipeline.instances), 1)\n+        self.assertEqual(MySpider.result, TrackingItemPipeline.instances[-1])\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = DefaultSpider\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ParentItemPipeline\n+        yield crawler.crawl()\n+        self.assertEqual(MySpider.result, TrackingItemPipeline.instances[-1])\n+\n+        class ChildItemPipeline(TrackingItemPipeline):\n+            pass\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ChildItemPipeline\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+    def test_get_item_pipeline_not_crawling(self):\n+        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n+        self.assertRaises(RuntimeError, crawler.get_item_pipeline, DefaultSpider)\n+\n+    @inlineCallbacks\n+    def test_get_item_pipeline_no_engine(self):\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                try:\n+                    crawler.get_item_pipeline(DefaultSpider)\n+                except Exception as e:\n+                    MySpider.result = e\n+                    raise\n+\n+        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n+        with raises(RuntimeError):\n+            yield crawler.crawl()\n+\n+    @inlineCallbacks\n+    def test_get_spider_middleware(self):\n+        class ParentSpiderMiddleware:\n+            pass\n+\n+        class TrackingSpiderMiddleware(ParentSpiderMiddleware):\n+            instances = []\n+\n+            def __init__(self):\n+                TrackingSpiderMiddleware.instances.append(self)\n+\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                return cls(crawler=crawler)\n+\n+            def __init__(self, crawler):\n+                self.crawler = crawler\n+\n+            def start_requests(self):\n+                MySpider.result = crawler.get_spider_middleware(MySpider.cls)\n+                return\n+                yield\n+\n+        settings = {\n+            **BASE_SETTINGS,\n+            \"SPIDER_MIDDLEWARES\": {\n+                TrackingSpiderMiddleware: 0,\n+            },\n+        }\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = TrackingSpiderMiddleware\n+        yield crawler.crawl()\n+        self.assertEqual(len(TrackingSpiderMiddleware.instances), 1)\n+        self.assertEqual(MySpider.result, TrackingSpiderMiddleware.instances[-1])\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = DefaultSpider\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ParentSpiderMiddleware\n+        yield crawler.crawl()\n+        self.assertEqual(MySpider.result, TrackingSpiderMiddleware.instances[-1])\n+\n+        class ChildSpiderMiddleware(TrackingSpiderMiddleware):\n+            pass\n+\n+        crawler = get_raw_crawler(MySpider, settings)\n+        MySpider.cls = ChildSpiderMiddleware\n+        yield crawler.crawl()\n+        self.assertIsNone(MySpider.result)\n+\n+    def test_get_spider_middleware_not_crawling(self):\n+        crawler = get_raw_crawler(settings_dict=BASE_SETTINGS)\n+        self.assertRaises(RuntimeError, crawler.get_spider_middleware, DefaultSpider)\n+\n+    @inlineCallbacks\n+    def test_get_spider_middleware_no_engine(self):\n+        class MySpider(Spider):\n+            name = \"myspider\"\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                try:\n+                    crawler.get_spider_middleware(DefaultSpider)\n+                except Exception as e:\n+                    MySpider.result = e\n+                    raise\n+\n+        crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n+        with raises(RuntimeError):\n+            yield crawler.crawl()\n+\n \n class SpiderSettingsTestCase(unittest.TestCase):\n     def test_spider_custom_settings(self):\n@@ -223,20 +589,20 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n     def _runner(self):\n         return CrawlerRunner({\"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\"})\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n         runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n         self.assertFalse(runner.bootstrap_failed)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_runner_bootstrap_successful_for_several(self):\n         runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n         yield runner.crawl(NoRequestsSpider)\n         self.assertFalse(runner.bootstrap_failed)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_runner_bootstrap_failed(self):\n         runner = self._runner()\n \n@@ -249,7 +615,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n         self.assertTrue(runner.bootstrap_failed)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_runner_bootstrap_failed_for_several(self):\n         runner = self._runner()\n \n@@ -264,7 +630,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n         self.assertTrue(runner.bootstrap_failed)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_runner_asyncio_enabled_true(self):\n         if self.reactor_pytest == \"asyncio\":\n             CrawlerRunner(\n@@ -531,7 +897,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.expect_exact(\"Spider closed (shutdown)\")\n         p.wait()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_shutdown_forced(self):\n         from twisted.internet import reactor\n \n@@ -543,7 +909,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.kill(sig)\n         p.expect_exact(\"shutting down gracefully\")\n         # sending the second signal too fast often causes problems\n-        d = defer.Deferred()\n+        d = Deferred()\n         reactor.callLater(0.1, d.callback, None)\n         yield d\n         p.kill(sig)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8dadb959219afea1d3a3f67ce03ac3c7a51520c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 61 | Churn Cumulative: 1926 | Contributors (this commit): 29 | Commits (past 90d): 3 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import functools\n import inspect\n import json\n import logging\n@@ -251,26 +252,20 @@ class Command(BaseRunSpiderCommand):\n \n         return scraped_data\n \n-    def prepare_request(self, spider, request, opts):\n-        def callback(response, **cb_kwargs):\n-            # memorize first request\n-            if not self.first_response:\n-                self.first_response = response\n-\n-            # determine real callback\n+    def _get_callback(self, *, spider, opts, response=None):\n+        cb = None\n+        if response:\n             cb = response.meta[\"_callback\"]\n         if not cb:\n             if opts.callback:\n                 cb = opts.callback\n-                elif opts.rules and self.first_response == response:\n+            elif response and opts.rules and self.first_response == response:\n                 cb = self.get_callback_from_rules(spider, response)\n-\n                 if not cb:\n-                        logger.error(\n-                            \"Cannot find a rule that matches %(url)r in spider: %(spider)s\",\n-                            {\"url\": response.url, \"spider\": spider.name},\n+                    raise ValueError(\n+                        f\"Cannot find a rule that matches {response.url!r} in spider: \"\n+                        f\"{spider.name}\"\n                     )\n-                        return\n             else:\n                 cb = \"parse\"\n \n@@ -279,11 +274,18 @@ class Command(BaseRunSpiderCommand):\n             if callable(cb_method):\n                 cb = cb_method\n             else:\n-                    logger.error(\n-                        \"Cannot find callback %(callback)r in spider: %(spider)s\",\n-                        {\"callback\": cb, \"spider\": spider.name},\n+                raise ValueError(\n+                    f\"Cannot find callback {cb!r} in spider: {spider.name}\"\n                 )\n-                    return\n+        return cb\n+\n+    def prepare_request(self, spider, request, opts):\n+        def callback(response, **cb_kwargs):\n+            # memorize first request\n+            if not self.first_response:\n+                self.first_response = response\n+\n+            cb = self._get_callback(spider=spider, opts=opts, response=response)\n \n             # parse items and requests\n             depth = response.meta[\"_depth\"]\n@@ -303,6 +305,9 @@ class Command(BaseRunSpiderCommand):\n \n         request.meta[\"_depth\"] = 1\n         request.meta[\"_callback\"] = request.callback\n+        if not request.callback and not opts.rules:\n+            cb = self._get_callback(spider=spider, opts=opts)\n+            functools.update_wrapper(callback, cb)\n         request.callback = callback\n         return request\n \n\n@@ -16,11 +16,11 @@ import scrapy\n \n class CheckSpider(scrapy.Spider):\n     name = '{self.spider_name}'\n-    start_urls = ['http://toscrape.com']\n+    start_urls = ['data:,']\n \n     def parse(self, response, **cb_kwargs):\n         \\\"\\\"\\\"\n-        @url http://toscrape.com\n+        @url data:,\n         {contracts}\n         \\\"\\\"\\\"\n         {parse_def}\n\n@@ -78,9 +78,21 @@ class AsyncDefAsyncioGenExcSpider(scrapy.Spider):\n             if i > 5:\n                 raise ValueError(\"Stopping the processing\")\n \n+class CallbackSignatureDownloaderMiddleware:\n+    def process_request(self, request, spider):\n+        from inspect import signature\n+        spider.logger.debug(f\"request.callback signature: {{signature(request.callback)}}\")\n+\n+\n class MySpider(scrapy.Spider):\n     name = '{self.spider_name}'\n \n+    custom_settings = {{\n+        \"DOWNLOADER_MIDDLEWARES\": {{\n+            CallbackSignatureDownloaderMiddleware: 0,\n+        }}\n+    }}\n+\n     def parse(self, response):\n         if getattr(self, 'test_arg', None):\n             self.logger.debug('It Works!')\n@@ -220,7 +232,11 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                 self.url(\"/html\"),\n             ]\n         )\n-        self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n+        log = _textmode(stderr)\n+        self.assertIn(\"DEBUG: It Works!\", log)\n+        self.assertIn(\n+            \"DEBUG: request.callback signature: (response, foo=None, key=None)\", log\n+        )\n \n     @defer.inlineCallbacks\n     def test_request_without_meta(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09a7efef7c75558c9ea198a00fc11ab26fb16ce5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 2 | Churn Cumulative: 7548 | Contributors (this commit): 46 | Commits (past 90d): 5 | Contributors (cumulative): 46 | DMM Complexity: None\n\nDIFF:\n@@ -2300,7 +2300,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                     content[feed[\"format\"]].append(file.read_bytes())\n         finally:\n             self.tearDown()\n-        defer.returnValue(content)\n+        return content\n \n     @defer.inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2487e3cc035e490777b921badc84c11b9fb77b20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 400 | Files Changed: 3 | Hunks: 19 | Methods Changed: 18 | Complexity Δ (Sum/Max): -49/0 | Churn Δ: 405 | Churn Cumulative: 3250 | Contributors (this commit): 77 | Commits (past 90d): 4 | Contributors (cumulative): 96 | DMM Complexity: 0.4827586206896552\n\nDIFF:\n@@ -260,7 +260,7 @@ REFERER_ENABLED = True\n REFERRER_POLICY = \"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"\n \n REQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\n-REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.6\"\n+REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n\n@@ -5,7 +5,6 @@ scrapy.http.Request objects\n \n import hashlib\n import json\n-import warnings\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -26,7 +25,6 @@ from w3lib.http import basic_auth_header\n from w3lib.url import canonicalize_url\n \n from scrapy import Request, Spider\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n@@ -34,9 +32,6 @@ from scrapy.utils.python import to_bytes, to_unicode\n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n-_deprecated_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\"\n-_deprecated_fingerprint_cache = WeakKeyDictionary()\n-\n \n def _serialize_headers(\n     headers: Iterable[bytes], request: Request\n@@ -47,120 +42,6 @@ def _serialize_headers(\n             yield from request.headers.getlist(header)\n \n \n-def request_fingerprint(\n-    request: Request,\n-    include_headers: Optional[Iterable[Union[bytes, str]]] = None,\n-    keep_fragments: bool = False,\n-) -> str:\n-    \"\"\"\n-    Return the request fingerprint as an hexadecimal string.\n-\n-    The request fingerprint is a hash that uniquely identifies the resource the\n-    request points to. For example, take the following two urls:\n-\n-    http://www.example.com/query?id=111&cat=222\n-    http://www.example.com/query?cat=222&id=111\n-\n-    Even though those are two different URLs both point to the same resource\n-    and are equivalent (i.e. they should return the same response).\n-\n-    Another example are cookies used to store session ids. Suppose the\n-    following page is only accessible to authenticated users:\n-\n-    http://www.example.com/members/offers.html\n-\n-    Lots of sites use a cookie to store the session id, which adds a random\n-    component to the HTTP Request and thus should be ignored when calculating\n-    the fingerprint.\n-\n-    For this reason, request headers are ignored by default when calculating\n-    the fingerprint. If you want to include specific headers use the\n-    include_headers argument, which is a list of Request headers to include.\n-\n-    Also, servers usually ignore fragments in urls when handling requests,\n-    so they are also ignored by default when calculating the fingerprint.\n-    If you want to include them, set the keep_fragments argument to True\n-    (for instance when handling requests with a headless browser).\n-    \"\"\"\n-    if include_headers or keep_fragments:\n-        message = (\n-            \"Call to deprecated function \"\n-            \"scrapy.utils.request.request_fingerprint().\\n\"\n-            \"\\n\"\n-            \"If you are using this function in a Scrapy component because you \"\n-            \"need a non-default fingerprinting algorithm, and you are OK \"\n-            \"with that non-default fingerprinting algorithm being used by \"\n-            \"all Scrapy components and not just the one calling this \"\n-            \"function, use crawler.request_fingerprinter.fingerprint() \"\n-            \"instead in your Scrapy component (you can get the crawler \"\n-            \"object from the 'from_crawler' class method), and use the \"\n-            \"'REQUEST_FINGERPRINTER_CLASS' setting to configure your \"\n-            \"non-default fingerprinting algorithm.\\n\"\n-            \"\\n\"\n-            \"Otherwise, consider using the \"\n-            \"scrapy.utils.request.fingerprint() function instead.\\n\"\n-            \"\\n\"\n-            \"If you switch to 'fingerprint()', or assign the \"\n-            \"'REQUEST_FINGERPRINTER_CLASS' setting a class that uses \"\n-            \"'fingerprint()', the generated fingerprints will not only be \"\n-            \"bytes instead of a string, but they will also be different from \"\n-            \"those generated by 'request_fingerprint()'. Before you switch, \"\n-            \"make sure that you understand the consequences of this (e.g. \"\n-            \"cache invalidation) and are OK with them; otherwise, consider \"\n-            \"implementing your own function which returns the same \"\n-            \"fingerprints as the deprecated 'request_fingerprint()' function.\"\n-        )\n-    else:\n-        message = (\n-            \"Call to deprecated function \"\n-            \"scrapy.utils.request.request_fingerprint().\\n\"\n-            \"\\n\"\n-            \"If you are using this function in a Scrapy component, and you \"\n-            \"are OK with users of your component changing the fingerprinting \"\n-            \"algorithm through settings, use \"\n-            \"crawler.request_fingerprinter.fingerprint() instead in your \"\n-            \"Scrapy component (you can get the crawler object from the \"\n-            \"'from_crawler' class method).\\n\"\n-            \"\\n\"\n-            \"Otherwise, consider using the \"\n-            \"scrapy.utils.request.fingerprint() function instead.\\n\"\n-            \"\\n\"\n-            \"Either way, the resulting fingerprints will be returned as \"\n-            \"bytes, not as a string, and they will also be different from \"\n-            \"those generated by 'request_fingerprint()'. Before you switch, \"\n-            \"make sure that you understand the consequences of this (e.g. \"\n-            \"cache invalidation) and are OK with them; otherwise, consider \"\n-            \"implementing your own function which returns the same \"\n-            \"fingerprints as the deprecated 'request_fingerprint()' function.\"\n-        )\n-    warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n-    processed_include_headers: Optional[Tuple[bytes, ...]] = None\n-    if include_headers:\n-        processed_include_headers = tuple(\n-            to_bytes(h.lower()) for h in sorted(include_headers)\n-        )\n-    cache = _deprecated_fingerprint_cache.setdefault(request, {})\n-    cache_key = (processed_include_headers, keep_fragments)\n-    if cache_key not in cache:\n-        fp = hashlib.sha1()\n-        fp.update(to_bytes(request.method))\n-        fp.update(\n-            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))\n-        )\n-        fp.update(request.body or b\"\")\n-        if processed_include_headers:\n-            for part in _serialize_headers(processed_include_headers, request):\n-                fp.update(part)\n-        cache[cache_key] = fp.hexdigest()\n-    return cache[cache_key]\n-\n-\n-def _request_fingerprint_as_bytes(*args: Any, **kwargs: Any) -> bytes:\n-    with warnings.catch_warnings():\n-        warnings.simplefilter(\"ignore\")\n-        return bytes.fromhex(request_fingerprint(*args, **kwargs))\n-\n-\n _fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\"\n _fingerprint_cache = WeakKeyDictionary()\n \n@@ -258,32 +139,14 @@ class RequestFingerprinter:\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n             )\n         else:\n-            implementation = \"2.6\"\n-        if implementation == \"2.6\":\n-            message = (\n-                \"'2.6' is a deprecated value for the \"\n-                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\\n\"\n-                \"\\n\"\n-                \"It is also the default value. In other words, it is normal \"\n-                \"to get this warning if you have not defined a value for the \"\n-                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so \"\n-                \"for backward compatibility reasons, but it will change in a \"\n-                \"future version of Scrapy.\\n\"\n-                \"\\n\"\n-                \"See the documentation of the \"\n-                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for \"\n-                \"information on how to handle this deprecation.\"\n-            )\n-            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n-            self._fingerprint = _request_fingerprint_as_bytes\n-        elif implementation == \"2.7\":\n+            implementation = \"2.7\"\n+        if implementation == \"2.7\":\n             self._fingerprint = fingerprint\n         else:\n             raise ValueError(\n                 f\"Got an invalid value on setting \"\n                 f\"'REQUEST_FINGERPRINTER_IMPLEMENTATION': \"\n-                f\"{implementation!r}. Valid values are '2.6' (deprecated) \"\n-                f\"and '2.7'.\"\n+                f\"{implementation!r}. Valid value is '2.7'.\"\n             )\n \n     def fingerprint(self, request: Request) -> bytes:\n\n@@ -1,23 +1,15 @@\n import json\n import unittest\n-import warnings\n from hashlib import sha1\n-from typing import Dict, Mapping, Optional, Tuple, Union\n+from typing import Dict, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n \n-import pytest\n-from w3lib.url import canonicalize_url\n-\n from scrapy.http import Request\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n-    _deprecated_fingerprint_cache,\n     _fingerprint_cache,\n-    _request_fingerprint_as_bytes,\n     fingerprint,\n     request_authenticate,\n-    request_fingerprint,\n     request_httprepr,\n     request_to_curl,\n )\n@@ -233,168 +225,6 @@ class FingerprintTest(unittest.TestCase):\n         self.assertEqual(actual, expected)\n \n \n-class RequestFingerprintTest(FingerprintTest):\n-    function = staticmethod(request_fingerprint)\n-    cache = _deprecated_fingerprint_cache\n-    known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n-        (\n-            Request(\"http://example.org\"),\n-            \"b2e5245ef826fd9576c93bd6e392fce3133fab62\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org\"),\n-            \"bd10a0a89ea32cdee77917320f1309b0da87e892\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org?a\"),\n-            \"2fb7d48ae02f04b749f40caa969c0bc3c43204ce\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org?a=b\"),\n-            \"42e5fe149b147476e3f67ad0670c57b4cc57856a\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org?a=b&a\"),\n-            \"d23a9787cb56c6375c2cae4453c5a8c634526942\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org?a=b&a=c\"),\n-            \"9a18a7a8552a9182b7f1e05d33876409e421e5c5\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org\", method=\"POST\"),\n-            \"ba20a80cb5c5ca460021ceefb3c2467b2bfd1bc6\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org\", body=b\"a\"),\n-            \"4bb136e54e715a4ea7a9dd1101831765d33f2d60\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org\", method=\"POST\", body=b\"a\"),\n-            \"6c6595374a304b293be762f7b7be3f54e9947c65\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n-            \"bd10a0a89ea32cdee77917320f1309b0da87e892\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n-            \"515b633cb3ca502a33a9d8c890e889ec1e425e65\",\n-            {\"include_headers\": [\"A\"]},\n-        ),\n-        (\n-            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n-            \"505c96e7da675920dfef58725e8c957dfdb38f47\",\n-            {\"keep_fragments\": True},\n-        ),\n-        (\n-            Request(\"https://example.org#a\", headers={\"A\": b\"B\"}),\n-            \"d6f673cdcb661b7970c2b9a00ee63e87d1e2e5da\",\n-            {\"include_headers\": [\"A\"], \"keep_fragments\": True},\n-        ),\n-        (\n-            Request(\"https://example.org/ab\"),\n-            \"4e2870fee58582d6f81755e9b8fdefe3cba0c951\",\n-            {},\n-        ),\n-        (\n-            Request(\"https://example.org/a\", body=b\"b\"),\n-            \"4e2870fee58582d6f81755e9b8fdefe3cba0c951\",\n-            {},\n-        ),\n-    )\n-\n-    def setUp(self) -> None:\n-        warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n-\n-    def tearDown(self) -> None:\n-        warnings.simplefilter(\"default\", ScrapyDeprecationWarning)\n-\n-    @pytest.mark.xfail(reason=\"known bug kept for backward compatibility\", strict=True)\n-    def test_part_separation(self):\n-        super().test_part_separation()\n-\n-\n-class RequestFingerprintDeprecationTest(unittest.TestCase):\n-    def test_deprecation_default_parameters(self):\n-        with pytest.warns(ScrapyDeprecationWarning) as warnings:\n-            request_fingerprint(Request(\"http://www.example.com\"))\n-        messages = [str(warning.message) for warning in warnings]\n-        self.assertTrue(\n-            any(\"Call to deprecated function\" in message for message in messages)\n-        )\n-        self.assertFalse(any(\"non-default\" in message for message in messages))\n-\n-    def test_deprecation_non_default_parameters(self):\n-        with pytest.warns(ScrapyDeprecationWarning) as warnings:\n-            request_fingerprint(Request(\"http://www.example.com\"), keep_fragments=True)\n-        messages = [str(warning.message) for warning in warnings]\n-        self.assertTrue(\n-            any(\"Call to deprecated function\" in message for message in messages)\n-        )\n-        self.assertTrue(any(\"non-default\" in message for message in messages))\n-\n-\n-class RequestFingerprintAsBytesTest(FingerprintTest):\n-    function = staticmethod(_request_fingerprint_as_bytes)\n-    cache = _deprecated_fingerprint_cache\n-    known_hashes = RequestFingerprintTest.known_hashes\n-\n-    def test_caching(self):\n-        r1 = Request(\"http://www.example.com/hnnoticiaj1.aspx?78160,199\")\n-        self.assertEqual(\n-            self.function(r1), bytes.fromhex(self.cache[r1][self.default_cache_key])\n-        )\n-\n-    @pytest.mark.xfail(reason=\"known bug kept for backward compatibility\", strict=True)\n-    def test_part_separation(self):\n-        super().test_part_separation()\n-\n-    def test_hashes(self):\n-        actual = [\n-            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes\n-        ]\n-        expected = [\n-            bytes.fromhex(_fingerprint) for _, _fingerprint, _ in self.known_hashes\n-        ]\n-        self.assertEqual(actual, expected)\n-\n-\n-_fingerprint_cache_2_6: Mapping[Request, Tuple[None, bool]] = WeakKeyDictionary()\n-\n-\n-def request_fingerprint_2_6(request, include_headers=None, keep_fragments=False):\n-    if include_headers:\n-        include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))\n-    cache = _fingerprint_cache_2_6.setdefault(request, {})\n-    cache_key = (include_headers, keep_fragments)\n-    if cache_key not in cache:\n-        fp = sha1()\n-        fp.update(to_bytes(request.method))\n-        fp.update(\n-            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))\n-        )\n-        fp.update(request.body or b\"\")\n-        if include_headers:\n-            for hdr in include_headers:\n-                if hdr in request.headers:\n-                    fp.update(hdr)\n-                    for v in request.headers.getlist(hdr):\n-                        fp.update(v)\n-        cache[cache_key] = fp.hexdigest()\n-    return cache[cache_key]\n-\n-\n REQUEST_OBJECTS_TO_TEST = (\n     Request(\"http://www.example.com/\"),\n     Request(\"http://www.example.com/query?id=111&cat=222\"),\n@@ -424,105 +254,17 @@ REQUEST_OBJECTS_TO_TEST = (\n )\n \n \n-class BackwardCompatibilityTestCase(unittest.TestCase):\n-    def test_function_backward_compatibility(self):\n-        include_headers_to_test = (\n-            None,\n-            [\"Accept-Language\"],\n-            [\"accept-language\", \"sessionid\"],\n-            [\"SESSIONID\", \"Accept-Language\"],\n-        )\n-        for request_object in REQUEST_OBJECTS_TO_TEST:\n-            for include_headers in include_headers_to_test:\n-                for keep_fragments in (False, True):\n-                    with warnings.catch_warnings():\n-                        warnings.simplefilter(\"ignore\")\n-                        fp = request_fingerprint(\n-                            request_object,\n-                            include_headers=include_headers,\n-                            keep_fragments=keep_fragments,\n-                        )\n-                    old_fp = request_fingerprint_2_6(\n-                        request_object,\n-                        include_headers=include_headers,\n-                        keep_fragments=keep_fragments,\n-                    )\n-                    self.assertEqual(fp, old_fp)\n-\n-    def test_component_backward_compatibility(self):\n-        for request_object in REQUEST_OBJECTS_TO_TEST:\n-            with warnings.catch_warnings():\n-                warnings.simplefilter(\"ignore\")\n-                crawler = get_crawler(prevent_warnings=False)\n-                fp = crawler.request_fingerprinter.fingerprint(request_object)\n-            old_fp = request_fingerprint_2_6(request_object)\n-            self.assertEqual(fp.hex(), old_fp)\n-\n-    def test_custom_component_backward_compatibility(self):\n-        \"\"\"Tests that the backward-compatible request fingerprinting class featured\n-        in the documentation is indeed backward compatible and does not cause a\n-        warning to be logged.\"\"\"\n-\n-        class RequestFingerprinter:\n-            cache = WeakKeyDictionary()\n-\n-            def fingerprint(self, request):\n-                if request not in self.cache:\n-                    fp = sha1()\n-                    fp.update(to_bytes(request.method))\n-                    fp.update(to_bytes(canonicalize_url(request.url)))\n-                    fp.update(request.body or b\"\")\n-                    self.cache[request] = fp.digest()\n-                return self.cache[request]\n-\n-        for request_object in REQUEST_OBJECTS_TO_TEST:\n-            with warnings.catch_warnings() as logged_warnings:\n-                settings = {\n-                    \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n-                }\n-                crawler = get_crawler(settings_dict=settings)\n-                fp = crawler.request_fingerprinter.fingerprint(request_object)\n-            old_fp = request_fingerprint_2_6(request_object)\n-            self.assertEqual(fp.hex(), old_fp)\n-            self.assertFalse(logged_warnings)\n-\n-\n class RequestFingerprinterTestCase(unittest.TestCase):\n     def test_default_implementation(self):\n-        with warnings.catch_warnings(record=True) as logged_warnings:\n-            crawler = get_crawler(prevent_warnings=False)\n-        request = Request(\"https://example.com\")\n-        self.assertEqual(\n-            crawler.request_fingerprinter.fingerprint(request),\n-            _request_fingerprint_as_bytes(request),\n-        )\n-        self.assertTrue(logged_warnings)\n-\n-    def test_deprecated_implementation(self):\n-        settings = {\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.6\",\n-        }\n-        with warnings.catch_warnings(record=True) as logged_warnings:\n-            crawler = get_crawler(settings_dict=settings)\n-        request = Request(\"https://example.com\")\n-        self.assertEqual(\n-            crawler.request_fingerprinter.fingerprint(request),\n-            _request_fingerprint_as_bytes(request),\n-        )\n-        self.assertTrue(logged_warnings)\n-\n-    def test_recommended_implementation(self):\n         settings = {\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n-        with warnings.catch_warnings(record=True) as logged_warnings:\n         crawler = get_crawler(settings_dict=settings)\n         request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n             fingerprint(request),\n         )\n-        self.assertFalse(logged_warnings)\n \n     def test_unknown_implementation(self):\n         settings = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#019443dd5761afd5b4f7bba5948508554f1cb5f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1528 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -256,10 +256,7 @@ REQUEST_OBJECTS_TO_TEST = (\n \n class RequestFingerprinterTestCase(unittest.TestCase):\n     def test_default_implementation(self):\n-        settings = {\n-            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n-        }\n-        crawler = get_crawler(settings_dict=settings)\n+        crawler = get_crawler()\n         request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24634f1bb236f72a1a7b73900f8e3b524f7717b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 35 | Churn Cumulative: 3792 | Contributors (this commit): 78 | Commits (past 90d): 9 | Contributors (cumulative): 116 | DMM Complexity: 1.0\n\nDIFF:\n@@ -260,7 +260,7 @@ REFERER_ENABLED = True\n REFERRER_POLICY = \"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"\n \n REQUEST_FINGERPRINTER_CLASS = \"scrapy.utils.request.RequestFingerprinter\"\n-REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n+REQUEST_FINGERPRINTER_IMPLEMENTATION = \"SENTINEL\"\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n\n@@ -5,6 +5,7 @@ scrapy.http.Request objects\n \n import hashlib\n import json\n+import warnings\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -25,6 +26,7 @@ from w3lib.http import basic_auth_header\n from w3lib.url import canonicalize_url\n \n from scrapy import Request, Spider\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n@@ -139,14 +141,22 @@ class RequestFingerprinter:\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\"\n             )\n         else:\n-            implementation = \"2.7\"\n-        if implementation == \"2.7\":\n+            implementation = \"SENTINEL\"\n+\n+        if implementation == \"SENTINEL\":\n+            self._fingerprint = fingerprint\n+        elif implementation == \"2.7\":\n+            message = (\n+                \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n+                \"And it will be removed in future version of Scrapy.\"\n+            )\n+            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             self._fingerprint = fingerprint\n         else:\n             raise ValueError(\n                 f\"Got an invalid value on setting \"\n                 f\"'REQUEST_FINGERPRINTER_IMPLEMENTATION': \"\n-                f\"{implementation!r}. Valid value is '2.7'.\"\n+                f\"{implementation!r}. Valid values are '2.7' and 'SENTINEL'.\"\n             )\n \n     def fingerprint(self, request: Request) -> bytes:\n\n@@ -76,7 +76,6 @@ class TestSpider(Spider):\n def get_crawler(\n     spidercls: Optional[Type[Spider]] = None,\n     settings_dict: Optional[Dict[str, Any]] = None,\n-    prevent_warnings: bool = True,\n ) -> Crawler:\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n@@ -86,8 +85,6 @@ def get_crawler(\n \n     # Set by default settings that prevent deprecation warnings.\n     settings: Dict[str, Any] = {}\n-    if prevent_warnings:\n-        settings[\"REQUEST_FINGERPRINTER_IMPLEMENTATION\"] = \"2.7\"\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     crawler = runner.create_crawler(spidercls or TestSpider)\n\n@@ -1,5 +1,6 @@\n import json\n import unittest\n+import warnings\n from hashlib import sha1\n from typing import Dict, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n@@ -263,6 +264,19 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n             fingerprint(request),\n         )\n \n+    def test_deprecated_implementation(self):\n+        settings = {\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n+        with warnings.catch_warnings(record=True) as logged_warnings:\n+            crawler = get_crawler(settings_dict=settings)\n+        request = Request(\"https://example.com\")\n+        self.assertEqual(\n+            crawler.request_fingerprinter.fingerprint(request),\n+            fingerprint(request),\n+        )\n+        self.assertTrue(logged_warnings)\n+\n     def test_unknown_implementation(self):\n         settings = {\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.5\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
