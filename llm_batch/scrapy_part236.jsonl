{"custom_id": "scrapy#0d57b5cd43343a335fcf2e923b29e76d09dd0b51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 47 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,11 +63,13 @@ class RetryMiddleware(object):\n     def _retry(self, request, reason, spider):\n         retries = request.meta.get('retry_times', 0) + 1\n \n+        retry_times = self.max_retry_times\n+\n         if 'max_retry_times' in request.meta:\n-            self.max_retry_times = request.meta['max_retry_times']\n+            retry_times = request.meta['max_retry_times']\n \n         stats = spider.crawler.stats\n-        if retries <= self.max_retry_times:\n+        if retries <= retry_times:\n             logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                          {'request': request, 'retries': retries, 'reason': reason},\n                          extra={'spider': spider})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cd9185aa12430708b17eb8c02a6bdc3709ed5f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 251 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,7 @@ class FSFilesStore(object):\n         absolute_path = self._get_filesystem_path(path)\n         try:\n             last_modified = os.path.getmtime(absolute_path)\n-        except:  # FIXME: catching everything!\n+        except os.error:\n             return {}\n \n         with open(absolute_path, 'rb') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbb411a805724fec50b786f369be79dc221c798e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 47 | Churn Cumulative: 350 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ This module must not depend on any module outside the Standard Library.\n import copy\n import six\n import warnings\n-from collections import OrderedDict\n+from collections import OrderedDict, Mapping\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -224,7 +224,7 @@ class CaselessDict(dict):\n         return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))\n \n     def update(self, seq):\n-        seq = seq.items() if isinstance(seq, dict) else seq\n+        seq = seq.items() if isinstance(seq, Mapping) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n         super(CaselessDict, self).update(iseq)\n \n\n@@ -1,5 +1,6 @@\n import copy\n import unittest\n+from collections import Mapping, MutableMapping\n \n from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n \n@@ -18,6 +19,48 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n+        class MyMapping(Mapping):\n+            def __init__(self, **kwargs):\n+                self._d = kwargs\n+\n+            def __getitem__(self, key):\n+                return self._d[key]\n+\n+            def __iter__(self):\n+                return iter(self._d)\n+\n+            def __len__(self):\n+                return len(self._d)\n+\n+        seq = MyMapping(red=1, black=3)\n+        d = CaselessDict(seq)\n+        self.assertEqual(d['red'], 1)\n+        self.assertEqual(d['black'], 3)\n+\n+        class MyMutableMapping(MutableMapping):\n+            def __init__(self, **kwargs):\n+                self._d = kwargs\n+\n+            def __getitem__(self, key):\n+                return self._d[key]\n+\n+            def __setitem__(self, key, value):\n+                self._d[key] = value\n+\n+            def __delitem__(self, key):\n+                del self._d[key]\n+\n+            def __iter__(self):\n+                return iter(self._d)\n+\n+            def __len__(self):\n+                return len(self._d)\n+\n+        seq = MyMutableMapping(red=1, black=3)\n+        d = CaselessDict(seq)\n+        self.assertEqual(d['red'], 1)\n+        self.assertEqual(d['black'], 3)\n+\n     def test_caseless(self):\n         d = CaselessDict()\n         d['key_Lower'] = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f2a5cdb8edb3d4d6ce542f13b5d7e5858905bae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 198 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,6 +43,7 @@ class LogFormatter(object):\n                 'request_flags' : request_flags,\n                 'referer': referer_str(request),\n                 'response_flags': response_flags,\n+                'flags': response_flags\n             }\n         }\n \n\n@@ -64,5 +64,32 @@ class LoggingContribTest(unittest.TestCase):\n         assert all(isinstance(x, six.text_type) for x in lines)\n         self.assertEqual(lines, [u\"Scraped from <200 http://www.example.com>\", u'name: \\xa3'])\n \n+\n+class LogFormatterSubclass(LogFormatter):\n+    # Formatter with format spec that is same as in Scrapy before 1.3 version.\n+    def crawled(self, request, response, spider):\n+        kwargs = super(LogFormatterSubclass, self).crawled(\n+        request, response, spider)\n+        CRAWLEDMSG = (\n+            u\"Crawled (%(status)s) %(request)s (referer: \"\n+            u\"%(referer)s)%(flags)s\"\n+        )\n+        return {\n+            'level': kwargs['level'],\n+            'msg': CRAWLEDMSG,\n+            'args': kwargs['args']\n+        }\n+\n+\n+class LogformatterSubclassTest(LoggingContribTest):\n+    # Test if old crawledmsg format string still works fine\n+    def setUp(self):\n+        self.formatter = LogFormatterSubclass()\n+        self.spider = Spider('default')\n+\n+    def test_flags_in_request(self):\n+        pass\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#694c6d3d7460ab80ace979c4563af8f310614d37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 52 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,10 +63,7 @@ class RetryMiddleware(object):\n     def _retry(self, request, reason, spider):\n         retries = request.meta.get('retry_times', 0) + 1\n \n-        retry_times = self.max_retry_times\n-\n-        if 'max_retry_times' in request.meta:\n-            retry_times = request.meta['max_retry_times']\n+        retry_times = request.meta.get('max_retry_times') or self.max_retry_times\n \n         stats = spider.crawler.stats\n         if retries <= retry_times:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#966bd49c421fdf40a8b21b49c76cd54ded06fe50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 28 | Churn Cumulative: 78 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,6 +103,34 @@ class RetryTest(unittest.TestCase):\n         req = self.mw.process_exception(req, exception, self.spider)\n         self.assertEqual(req, None)\n \n+    def test_different_retry(self):\n+        \n+        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 1})\n+        self._test_retry(req, DNSLookupError('foo'))\n+        req2 = Request('http://www.scrapytest.org/invalid_url')\n+        self._test_retry(req2, DNSLookupError('foo'))\n+\n+        stats = self.crawler.stats\n+        assert stats.get_value('retry/max_reached') == 2\n+        assert stats.get_value('retry/count') == 3\n+\n+    def _test_retry(self, req, exception):\n+        \n+        req = self.mw.process_exception(req, exception, self.spider)\n+        assert isinstance(req, Request)\n+\n+        retry_times = req.meta.get('max_retry_times') or self.mw.max_retry_times\n+\n+        while req.meta['retry_times'] != retry_times:\n+            req = self.mw.process_exception(req, exception, self.spider)\n+            assert isinstance(req, Request)\n+        \n+        self.assertEqual(req.meta['retry_times'], retry_times)\n+\n+        # discard it\n+        req = self.mw.process_exception(req, exception, self.spider)\n+        self.assertEqual(req, None)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e321ac9931007360a38dbd6a5933794af415274b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 115 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -105,27 +105,34 @@ class RetryTest(unittest.TestCase):\n \n     def test_different_retry(self):\n         \n-        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 1})\n-        self._test_retry(req, DNSLookupError('foo'))\n+        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})\n+\n+        # SETINGS: meta(max_retry_times) = 3, RETRY_TIMES = 2\n+        self._test_retry(req, DNSLookupError('foo'), 3)\n+\n         req2 = Request('http://www.scrapytest.org/invalid_url')\n-        self._test_retry(req2, DNSLookupError('foo'))\n         \n-        stats = self.crawler.stats\n-        assert stats.get_value('retry/max_reached') == 2\n-        assert stats.get_value('retry/count') == 3\n+        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n+        self._test_retry(req2, DNSLookupError('foo'), 2)\n \n-    def _test_retry(self, req, exception):\n+        # SETINGS: RETRY_TIMES = 0\n+        self.mw.max_retry_times = 0\n+        self._test_retry(req2, DNSLookupError('foo'), 0)\n         \n+        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n+        self.mw.max_retry_times = 4\n+        self._test_retry(req2, DNSLookupError('foo'), 4)\n+\n+        # RESET RETRY_TIMES SETTINGS\n+        self.mw.max_retry_times = 2\n+\n+    def _test_retry(self, req, exception, max_retry_times):\n+        \n+        while max_retry_times > 0:\n             req = self.mw.process_exception(req, exception, self.spider)\n             assert isinstance(req, Request)\n-\n-        retry_times = req.meta.get('max_retry_times') or self.mw.max_retry_times\n-\n-        while req.meta['retry_times'] != retry_times:\n-            req = self.mw.process_exception(req, exception, self.spider)\n-            assert isinstance(req, Request)\n-        \n-        self.assertEqual(req.meta['retry_times'], retry_times)\n+            if req.meta['retry_times'] == max_retry_times:\n+                break\n \n         # discard it\n         req = self.mw.process_exception(req, exception, self.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ba4b0a21b0bb2efe0523bec0c1db07771816347", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 164 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,15 @@ def _embed_bpython_shell(namespace={}, banner=''):\n         bpython.embed(locals_=namespace, banner=banner)\n     return wrapper\n \n+def _embed_ptpython_shell(namespace={}, banner=''):\n+    \"\"\"Start a ptpython shell\"\"\"\n+    import ptpython.repl\n+    @wraps(_embed_ptpython_shell)\n+    def wrapper(namespace=namespace, banner=''):\n+        print(banner)\n+        ptpython.repl.embed(locals=namespace)\n+    return wrapper\n+\n def _embed_standard_shell(namespace={}, banner=''):\n     \"\"\"Start a standard python shell\"\"\"\n     import code\n@@ -49,6 +58,7 @@ def _embed_standard_shell(namespace={}, banner=''):\n DEFAULT_PYTHON_SHELLS = OrderedDict([\n     ('ipython', _embed_ipython_shell),\n     ('bpython', _embed_bpython_shell),\n+    ('ptpython', _embed_ptpython_shell),\n     ('python', _embed_standard_shell),\n ])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a84652e775fda1135fe959465b27cf6ed2c25e1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 5 | Churn Cumulative: 111 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,17 +8,19 @@ __doctests__ = ['scrapy.utils.datatypes']\n \n class CaselessDictTest(unittest.TestCase):\n \n-    def test_init(self):\n+    def test_init_dict(self):\n         seq = {'red': 1, 'black': 3}\n         d = CaselessDict(seq)\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n+    def test_init_pair_sequence(self):\n         seq = (('red', 1), ('black', 3))\n         d = CaselessDict(seq)\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n+    def test_init_mapping(self):\n         class MyMapping(Mapping):\n             def __init__(self, **kwargs):\n                 self._d = kwargs\n@@ -37,6 +39,7 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(d['red'], 1)\n         self.assertEqual(d['black'], 3)\n \n+    def test_init_mutable_mapping(self):\n         class MyMutableMapping(MutableMapping):\n             def __init__(self, **kwargs):\n                 self._d = kwargs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4345eaf1b640bfecb2340a0e27649b1ab1079da6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 201 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -43,6 +43,7 @@ class LogFormatter(object):\n                 'request_flags' : request_flags,\n                 'referer': referer_str(request),\n                 'response_flags': response_flags,\n+                # backward compatibility with Scrapy logformatter below 1.4 version\n                 'flags': response_flags\n             }\n         }\n\n@@ -66,7 +66,6 @@ class LoggingContribTest(unittest.TestCase):\n \n \n class LogFormatterSubclass(LogFormatter):\n-    # Formatter with format spec that is same as in Scrapy before 1.3 version.\n     def crawled(self, request, response, spider):\n         kwargs = super(LogFormatterSubclass, self).crawled(\n         request, response, spider)\n@@ -82,7 +81,6 @@ class LogFormatterSubclass(LogFormatter):\n \n \n class LogformatterSubclassTest(LoggingContribTest):\n-    # Test if old crawledmsg format string still works fine\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider('default')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49c5afc5ff6c810c78678ea1c86beb19ca3487ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 57 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,7 +63,10 @@ class RetryMiddleware(object):\n     def _retry(self, request, reason, spider):\n         retries = request.meta.get('retry_times', 0) + 1\n \n-        retry_times = request.meta.get('max_retry_times') or self.max_retry_times\n+        retry_times = self.max_retry_times\n+\n+        if 'max_retry_times' in request.meta:\n+            retry_times = request.meta['max_retry_times']\n \n         stats = spider.crawler.stats\n         if retries <= retry_times:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d9ebd6e1ed58654ea1996d5a236b4d0240590df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 72 | Churn Cumulative: 187 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,29 +103,75 @@ class RetryTest(unittest.TestCase):\n         req = self.mw.process_exception(req, exception, self.spider)\n         self.assertEqual(req, None)\n \n-    def test_different_retry(self):\n \n-        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})\n+class MaxRetryTimesTest(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mw = RetryMiddleware.from_crawler(self.crawler)\n+        self.mw.max_retry_times = 2\n \n-        # SETINGS: meta(max_retry_times) = 3, RETRY_TIMES = 2\n-        self._test_retry(req, DNSLookupError('foo'), 3)\n+    def test_without_metakey(self):\n         \n-        req2 = Request('http://www.scrapytest.org/invalid_url')\n+        req = Request('http://www.scrapytest.org/invalid_url')\n \n-        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n-        self._test_retry(req2, DNSLookupError('foo'), 2)\n+        # SETTINGS: RETRY_TIMES is NON-ZERO\n+        self.mw.max_retry_times = 5\n+        self._test_retry(req, DNSLookupError('foo'), 5)\n \n-        # SETINGS: RETRY_TIMES = 0\n+        # SETTINGS: RETRY_TIMES = 0\n         self.mw.max_retry_times = 0\n-        self._test_retry(req2, DNSLookupError('foo'), 0)\n-        \n-        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n-        self.mw.max_retry_times = 4\n-        self._test_retry(req2, DNSLookupError('foo'), 4)\n+        self._test_retry(req, DNSLookupError('foo'), 0)\n \n         # RESET RETRY_TIMES SETTINGS\n         self.mw.max_retry_times = 2\n \n+    def test_with_metakey_preceding(self):\n+        # request with meta(max_retry_times) is called first\n+        \n+        req1 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})\n+        req2 = Request('http://www.scrapytest.org/invalid_url')\n+        req3 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 4})\n+\n+        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n+        self.mw.max_retry_times = 2\n+        self._test_retry(req1, DNSLookupError('foo'), 3)\n+        self._test_retry(req2, DNSLookupError('foo'), 2)\n+\n+        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n+        self.mw.max_retry_times = 5\n+        self._test_retry(req3, DNSLookupError('foo'), 4)\n+        self._test_retry(req2, DNSLookupError('foo'), 5)\n+\n+        # RESET RETRY_TIMES SETTINGS\n+        self.mw.max_retry_times = 2\n+\n+    def test_with_metakey_succeeding(self):\n+        # request with meta(max_retry_times) is called second\n+        \n+        req1 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})\n+        req2 = Request('http://www.scrapytest.org/invalid_url')\n+        req3 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 4})\n+\n+        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n+        self.mw.max_retry_times = 2\n+        self._test_retry(req2, DNSLookupError('foo'), 2)\n+        self._test_retry(req1, DNSLookupError('foo'), 3)\n+\n+        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n+        self.mw.max_retry_times = 5\n+        self._test_retry(req2, DNSLookupError('foo'), 5)\n+        self._test_retry(req3, DNSLookupError('foo'), 4)\n+\n+        # RESET RETRY_TIMES SETTINGS\n+        self.mw.max_retry_times = 2\n+\n+    def test_with_metakey_zero(self):\n+        \n+        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 0})\n+        self._test_retry(req, DNSLookupError('foo'), 0)\n+\n+\n     def _test_retry(self, req, exception, max_retry_times):\n         \n         while max_retry_times > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b040df5ac09cf133cd07b505d20469a56409129c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 39 | Files Changed: 1 | Hunks: 15 | Methods Changed: 14 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 73 | Churn Cumulative: 467 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -255,61 +255,62 @@ class ThirdMiddleware(object):\n \n class TestSpiderMiddleware(TestCase):\n \n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n-    def test_process_spider_exception_from_parse_method(self):\n-        # non-generator return value\n-        crawler = get_crawler(NotAGeneratorSpider)\n+    def crawl_log(self, spider):\n+        crawler = get_crawler(spider)\n         with LogCapture() as log:\n             yield crawler.crawl()\n+        raise defer.returnValue(log)\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_from_parse_method_non_generator(self):\n+        # non-generator return value\n+        log = yield self.crawl_log(NotAGeneratorSpider)\n         self.assertIn(\"AssertionError exception caught\", str(log))\n         self.assertIn(\"spider_exceptions/AssertionError\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_from_parse_method_generator_no_items(self):\n         # generator return value, no items before the error\n-        crawler = get_crawler(GeneratorErrorBeforeItemsSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n+        log = yield self.crawl_log(GeneratorErrorBeforeItemsSpider)\n         self.assertIn(\"ValueError exception caught\", str(log))\n         self.assertIn(\"spider_exceptions/ValueError\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_exception_from_parse_method_generator_with_items(self):\n         # generator return value, 3 items before the error\n-        crawler = get_crawler(GeneratorErrorAfterItemsSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n+        log = yield self.crawl_log(GeneratorErrorAfterItemsSpider)\n         self.assertIn(\"'item_scraped_count': 3\", str(log))\n         self.assertIn(\"FloatingPointError exception caught\", str(log))\n         self.assertIn(\"spider_exceptions/FloatingPointError\", str(log))\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_from_previous_middleware_input(self):\n-        crawler = get_crawler(FromPreviousMiddlewareInputSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n+        log = yield self.crawl_log(FromPreviousMiddlewareInputSpider)\n         self.assertIn(\"LookupError exception caught\", str(log))\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_from_previous_middleware_output(self):\n         # non-generator output value\n-        crawler = get_crawler(NotAGeneratorFromPreviousMiddlewareOutputSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n+        log = yield self.crawl_log(NotAGeneratorFromPreviousMiddlewareOutputSpider)\n         self.assertNotIn(\"UnicodeError exception caught\", str(log))\n         # generator output value\n-        crawler = get_crawler(GeneratorFromPreviousMiddlewareOutputSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n+        log = yield self.crawl_log(GeneratorFromPreviousMiddlewareOutputSpider)\n         self.assertIn(\"'item_scraped_count': 3\", str(log))\n         self.assertIn(\"NameError exception caught\", str(log))\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_do_something(self):\n-        crawler = get_crawler(DoSomethingSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n+        log = yield self.crawl_log(DoSomethingSpider)\n         self.assertIn(\"ImportError exception caught\", str(log))\n         self.assertIn(\"{'value': 10}\", str(log))\n         self.assertIn(\"{'value': 'asdf'}\", str(log))\n@@ -320,25 +321,19 @@ class TestSpiderMiddleware(TestCase):\n     def test_process_spider_exception_invalid_return_value_previous_middleware(self):\n         \"\"\" don't catch _InvalidOutput from middleware \"\"\"\n         # on middleware's input\n-        crawler1 = get_crawler(InvalidReturnValueFromPreviousMiddlewareInputSpider)\n-        with LogCapture() as log1:\n-            yield crawler1.crawl()\n+        log1 = yield self.crawl_log(InvalidReturnValueFromPreviousMiddlewareInputSpider)\n         self.assertNotIn(\"_InvalidOutput exception caught\", str(log1))\n         self.assertIn(\"'spider_exceptions/_InvalidOutput'\", str(log1))\n         # on middleware's output\n-        crawler2 = get_crawler(InvalidReturnValueFromPreviousMiddlewareOutputSpider)\n-        with LogCapture() as log2:\n-            yield crawler2.crawl()\n+        log2 = yield self.crawl_log(InvalidReturnValueFromPreviousMiddlewareOutputSpider)\n         self.assertNotIn(\"_InvalidOutput exception caught\", str(log2))\n         self.assertIn(\"'spider_exceptions/_InvalidOutput'\", str(log2))\n \n     @defer.inlineCallbacks\n     def test_process_spider_exception_execution_chain(self):\n         # on middleware's input\n-        crawler1 = get_crawler(ExecutionChainSpider)\n-        with LogCapture() as log1:\n-            yield crawler1.crawl()\n-        self.assertNotIn(\"handled_by_first_middleware\", str(log1))\n-        self.assertNotIn(\"handled_by_second_middleware\", str(log1))\n-        self.assertIn(\"MemoryError exception caught\", str(log1))\n-        self.assertIn(\"handled_by_third_middleware\", str(log1))\n+        log = yield self.crawl_log(ExecutionChainSpider)\n+        self.assertNotIn(\"handled_by_first_middleware\", str(log))\n+        self.assertNotIn(\"handled_by_second_middleware\", str(log))\n+        self.assertIn(\"MemoryError exception caught\", str(log))\n+        self.assertIn(\"handled_by_third_middleware\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21d794d35ae1347918e6bf5b2ffe13515ea28795", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 663 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -132,6 +132,11 @@ class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n+        elif image.mode == 'P':\n+            image = image.convert(\"RGBA\")\n+            background = Image.new('RGBA', image.size, (255, 255, 255))\n+            background.paste(image, image)\n+            image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')\n \n\n@@ -96,6 +96,14 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEquals(converted.mode, 'RGB')\n         self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n \n+        # transparency case with palette: P and PNG\n+        COLOUR = (0, 127, 255, 50)\n+        im = _create_image('PNG', 'RGBA', SIZE, COLOUR)\n+        im = im.convert('P')\n+        converted, _ = self.pipeline.convert_image(im)\n+        self.assertEquals(converted.mode, 'RGB')\n+        self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n+\n \n class DeprecatedImagesPipeline(ImagesPipeline):\n     def file_key(self, url):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99e3c0d653e23d1af3e4236b88a747317e1c8a8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1498 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -280,19 +280,7 @@ class ScrapyAgent(object):\n         if request.body:\n             bodyproducer = _RequestBodyProducer(request.body)\n         else:\n-            bodyproducer = None\n-            # Setting Content-Length: 0 even for POST requests is not a\n-            # MUST per HTTP RFCs, but it's common behavior, and some\n-            # servers require this, otherwise returning HTTP 411 Length required\n-            #\n-            # RFC 7230#section-3.3.2:\n-            # \"a Content-Length header field is normally sent in a POST\n-            # request even when the value is 0 (indicating an empty payload body).\"\n-            #\n-            # Twisted Agent will not add \"Content-Length: 0\" by itself\n-            if method == b'POST':\n-                headers.addRawHeader(b'Content-Length', b'0')\n-\n+            bodyproducer = _RequestBodyProducer(b'') if method == b'POST' else None\n         start_time = time()\n         d = agent.request(\n             method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38e6857c957ef023533128f054688152be223c87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 105 | Churn Cumulative: 292 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -110,75 +110,72 @@ class MaxRetryTimesTest(unittest.TestCase):\n         self.spider = self.crawler._create_spider('foo')\n         self.mw = RetryMiddleware.from_crawler(self.crawler)\n         self.mw.max_retry_times = 2\n+        self.invalid_url = 'http://www.scrapytest.org/invalid_url'\n \n-    def test_without_metakey(self):\n-        \n-        req = Request('http://www.scrapytest.org/invalid_url')\n-\n-        # SETTINGS: RETRY_TIMES is NON-ZERO\n-        self.mw.max_retry_times = 5\n-        self._test_retry(req, DNSLookupError('foo'), 5)\n+    def test_with_settings_zero(self):\n \n         # SETTINGS: RETRY_TIMES = 0\n         self.mw.max_retry_times = 0\n-        self._test_retry(req, DNSLookupError('foo'), 0)\n \n-        # RESET RETRY_TIMES SETTINGS\n-        self.mw.max_retry_times = 2\n-\n-    def test_with_metakey_preceding(self):\n-        # request with meta(max_retry_times) is called first\n-        \n-        req1 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})\n-        req2 = Request('http://www.scrapytest.org/invalid_url')\n-        req3 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 4})\n-\n-        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n-        self.mw.max_retry_times = 2\n-        self._test_retry(req1, DNSLookupError('foo'), 3)\n-        self._test_retry(req2, DNSLookupError('foo'), 2)\n-\n-        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n-        self.mw.max_retry_times = 5\n-        self._test_retry(req3, DNSLookupError('foo'), 4)\n-        self._test_retry(req2, DNSLookupError('foo'), 5)\n-\n-        # RESET RETRY_TIMES SETTINGS\n-        self.mw.max_retry_times = 2\n-\n-    def test_with_metakey_succeeding(self):\n-        # request with meta(max_retry_times) is called second\n-        \n-        req1 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})\n-        req2 = Request('http://www.scrapytest.org/invalid_url')\n-        req3 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 4})\n-\n-        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n-        self.mw.max_retry_times = 2\n-        self._test_retry(req2, DNSLookupError('foo'), 2)\n-        self._test_retry(req1, DNSLookupError('foo'), 3)\n-\n-        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n-        self.mw.max_retry_times = 5\n-        self._test_retry(req2, DNSLookupError('foo'), 5)\n-        self._test_retry(req3, DNSLookupError('foo'), 4)\n-\n-        # RESET RETRY_TIMES SETTINGS\n-        self.mw.max_retry_times = 2\n+        req = Request(self.invalid_url)\n+        self._test_retry(req, DNSLookupError('foo'), self.mw.max_retry_times)\n \n     def test_with_metakey_zero(self):\n \n-        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 0})\n+        # SETTINGS: meta(max_retry_times) = 0\n+        meta_max_retry_times = 0\n+        \n+        req = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n+        self._test_retry(req, DNSLookupError('foo'), meta_max_retry_times)\n+\n+    def test_without_metakey(self):\n+\n+        # SETTINGS: RETRY_TIMES is NON-ZERO\n+        self.mw.max_retry_times = 5\n+\n+        req = Request(self.invalid_url)\n+        self._test_retry(req, DNSLookupError('foo'), self.mw.max_retry_times)\n+\n+    def test_with_metakey_greater(self):\n+        \n+        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n+        self.mw.max_retry_times = 2\n+        meta_max_retry_times = 3\n+\n+        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n+        req2 = Request(self.invalid_url)\n+\n+        self._test_retry(req1, DNSLookupError('foo'), meta_max_retry_times)\n+        self._test_retry(req2, DNSLookupError('foo'), self.mw.max_retry_times)\n+\n+    def test_with_metakey_lesser(self):\n+        \n+        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n+        self.mw.max_retry_times = 5\n+        meta_max_retry_times = 4\n+\n+        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n+        req2 = Request(self.invalid_url)\n+\n+        self._test_retry(req1, DNSLookupError('foo'), meta_max_retry_times)\n+        self._test_retry(req2, DNSLookupError('foo'), self.mw.max_retry_times)\n+\n+    def test_with_dont_retry(self):\n+\n+        # SETTINGS: meta(max_retry_times) = 4\n+        meta_max_retry_times = 4\n+\n+        req = Request(self.invalid_url, meta= \\\n+            {'max_retry_times': meta_max_retry_times, 'dont_retry': True})\n+\n         self._test_retry(req, DNSLookupError('foo'), 0)\n \n \n     def _test_retry(self, req, exception, max_retry_times):\n         \n-        while max_retry_times > 0:\n+        for i in range(0, max_retry_times):\n             req = self.mw.process_exception(req, exception, self.spider)\n             assert isinstance(req, Request)\n-            if req.meta['retry_times'] == max_retry_times:\n-                break\n \n         # discard it\n         req = self.mw.process_exception(req, exception, self.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6352c2e9b2028473acdbd58175bbc5638258e29d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 605 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,7 @@ def _get_commands_from_module(module, inproject):\n     d = {}\n     for cmd in _iter_command_classes(module):\n         if inproject or not cmd.requires_project:\n+            if not cmd.__module__ == module:\n                 cmdname = cmd.__module__.split('.')[-1]\n                 d[cmdname] = cmd()\n     return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05ce1296c6a60f23e81af7ec38ac1855e78be79f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 609 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,14 +20,14 @@ def _iter_command_classes(module_name):\n         for obj in vars(module).values():\n             if inspect.isclass(obj) and \\\n                     issubclass(obj, ScrapyCommand) and \\\n-                    obj.__module__ == module.__name__:\n+                    obj.__module__ == module.__name__ and \\\n+                    not obj == ScrapyCommand:\n                 yield obj\n \n def _get_commands_from_module(module, inproject):\n     d = {}\n     for cmd in _iter_command_classes(module):\n         if inproject or not cmd.requires_project:\n-            if not cmd.__module__ == module:\n             cmdname = cmd.__module__.split('.')[-1]\n             d[cmdname] = cmd()\n     return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97fc68fa1699b1c782c6e6d888e21d995bdf071d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1502 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -279,8 +279,10 @@ class ScrapyAgent(object):\n             headers.removeHeader(b'Proxy-Authorization')\n         if request.body:\n             bodyproducer = _RequestBodyProducer(request.body)\n+        elif method == b'POST':\n+            bodyproducer = _RequestBodyProducer(b'')\n         else:\n-            bodyproducer = _RequestBodyProducer(b'') if method == b'POST' else None\n+            bodyproducer = None\n         start_time = time()\n         d = agent.request(\n             method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1a0a6e25810353f46d314cea1fd34bd37109b0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 186 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,6 @@ from twisted.internet.task import deferLater\n \n \n from scrapy.utils.python import to_bytes, to_unicode\n-from tests import tests_datadir\n \n \n def getarg(request, name, default=None, type=None):\n@@ -122,6 +121,7 @@ class Echo(LeafResource):\n             'body': to_unicode(request.content.read()),\n         }\n         return to_bytes(json.dumps(output))\n+    render_POST = render_GET\n \n \n class RedirectTo(LeafResource):\n@@ -174,7 +174,11 @@ class Root(Resource):\n         self.putChild(b\"echo\", Echo())\n         self.putChild(b\"payload\", PayloadResource())\n         self.putChild(b\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n+        try:\n+            from tests import tests_datadir\n             self.putChild(b\"files\", File(os.path.join(tests_datadir, 'test_site/files/')))\n+        except:\n+            pass\n         self.putChild(b\"redirect-to\", RedirectTo())\n \n     def getChild(self, name, request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a63d9f502f50fbd948154fc65c8a94ffd4722d11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1514 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -280,6 +280,18 @@ class ScrapyAgent(object):\n         if request.body:\n             bodyproducer = _RequestBodyProducer(request.body)\n         elif method == b'POST':\n+            # Setting Content-Length: 0 even for POST requests is not a\n+            # MUST per HTTP RFCs, but it's common behavior, and some\n+            # servers require this, otherwise returning HTTP 411 Length required\n+            #\n+            # RFC 7230#section-3.3.2:\n+            # \"a Content-Length header field is normally sent in a POST\n+            # request even when the value is 0 (indicating an empty payload body).\"\n+            #\n+            # Twisted < 17 will not add \"Content-Length: 0\" by itself;\n+            # Twisted >= 17 fixes this;\n+            # Using a producer with an empty-string sends `0` as Content-Length\n+            # for all versions of Twisted.\n             bodyproducer = _RequestBodyProducer(b'')\n         else:\n             bodyproducer = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3d0f9b6c10b68e436054ce9421d2ddadfc47087", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 16 | Churn Cumulative: 928 | Contributors (this commit): 12 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,7 @@ from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n \n from scrapy.spiders import Spider\n-from scrapy.http import Request\n+from scrapy.http import Headers, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.settings import Settings\n@@ -37,7 +37,7 @@ from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n \n-from tests.mockserver import MockServer, ssl_context_factory\n+from tests.mockserver import MockServer, ssl_context_factory, Echo\n from tests.spiders import SingleRequestSpider\n \n class DummyDH(object):\n@@ -202,6 +202,7 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"broken-chunked\", BrokenChunkedResource())\n         r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n         r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n+        r.putChild(b\"echo\", Echo())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n         self.host = 'localhost'\n@@ -310,6 +311,17 @@ class HttpTestCase(unittest.TestCase):\n         request = Request(self.getURL('contentlength'), method='POST', headers={'Host': 'example.com'})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n+    def test_content_length_zero_bodyless_post_only_one(self):\n+        def _test(response):\n+            import json\n+            headers = Headers(json.loads(response.text)['headers'])\n+            contentlengths = headers.getlist('Content-Length')\n+            self.assertEquals(len(contentlengths), 1)\n+            self.assertEquals(contentlengths, [b\"0\"])\n+\n+        request = Request(self.getURL('echo'), method='POST')\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n     def test_payload(self):\n         body = b'1'*100 # PayloadResource requires body length to be 100\n         request = Request(self.getURL('payload'), method='POST', body=body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b34c6edffcf14d81b5f11369648f4661e067ccc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 41 | Churn Cumulative: 2483 | Contributors (this commit): 19 | Commits (past 90d): 18 | Contributors (cumulative): 28 | DMM Complexity: 0.875\n\nDIFF:\n@@ -348,7 +348,8 @@ class ScrapyAgent(object):\n                            {'size': expected_size, 'warnsize': warnsize})\n \n         def _cancel(_):\n-            txresponse._transport._producer.loseConnection()\n+            # Abort connection inmediately.\n+            txresponse._transport._producer.abortConnection()\n \n         d = defer.Deferred(_cancel)\n         txresponse.deliverBody(_ResponseReader(\n@@ -401,6 +402,11 @@ class _ResponseReader(protocol.Protocol):\n         self._bytes_received = 0\n \n     def dataReceived(self, bodyBytes):\n+        # This maybe called several times after cancel was called with buffered\n+        # data.\n+        if self._finished.called:\n+            return\n+\n         self._bodybuf.write(bodyBytes)\n         self._bytes_received += len(bodyBytes)\n \n@@ -409,6 +415,9 @@ class _ResponseReader(protocol.Protocol):\n                          \"max size (%(maxsize)s).\",\n                          {'bytes': self._bytes_received,\n                           'maxsize': self._maxsize})\n+            # Clear buffer earlier to avoid keeping data in memory for a long\n+            # time.\n+            self._bodybuf.truncate(0)\n             self._finished.cancel()\n \n         if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:\n\n@@ -177,6 +177,16 @@ class EmptyContentTypeHeaderResource(resource.Resource):\n         return request.content.read()\n \n \n+class LargeChunkedFileResource(resource.Resource):\n+    def render(self, request):\n+        def response():\n+            for i in range(1024):\n+                request.write(b\"x\" * 1024)\n+            request.finish()\n+        reactor.callLater(0, response)\n+        return server.NOT_DONE_YET\n+\n+\n class HttpTestCase(unittest.TestCase):\n \n     scheme = 'http'\n@@ -202,6 +212,7 @@ class HttpTestCase(unittest.TestCase):\n         r.putChild(b\"broken-chunked\", BrokenChunkedResource())\n         r.putChild(b\"contentlength\", ContentLengthHeaderResource())\n         r.putChild(b\"nocontenttype\", EmptyContentTypeHeaderResource())\n+        r.putChild(b\"largechunkedfile\", LargeChunkedFileResource())\n         r.putChild(b\"echo\", Echo())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n@@ -384,6 +395,25 @@ class Http11TestCase(HttpTestCase):\n         d = self.download_request(request, Spider('foo', download_maxsize=9))\n         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n \n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize_very_large_file(self):\n+        with mock.patch('scrapy.core.downloader.handlers.http11.logger') as logger:\n+            request = Request(self.getURL('largechunkedfile'))\n+\n+            def check(logger):\n+                logger.error.assert_called_once_with(mock.ANY, mock.ANY)\n+\n+            d = self.download_request(request, Spider('foo', download_maxsize=1500))\n+            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+            # As the error message is logged in the dataReceived callback, we\n+            # have to give a bit of time to the reactor to process the queue\n+            # after closing the connection.\n+            d = defer.Deferred()\n+            d.addCallback(check)\n+            reactor.callLater(.1, d.callback, logger)\n+            yield d\n+\n     @defer.inlineCallbacks\n     def test_download_with_maxsize_per_req(self):\n         meta = {'download_maxsize': 2}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
