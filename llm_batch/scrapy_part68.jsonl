{"custom_id": "scrapy#1dc592882b3eb62b3c95140e00da7a81ceaec8c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 134 | Lines Deleted: 134 | Files Changed: 5 | Hunks: 79 | Methods Changed: 56 | Complexity Δ (Sum/Max): 63/30 | Churn Δ: 268 | Churn Cumulative: 281 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,13 +0,0 @@\n-\"\"\"Common functions used in Item Parsers code\"\"\"\n-\n-from functools import partial\n-from scrapy.utils.python import get_func_args\n-\n-def wrap_parser_context(function, context):\n-    \"\"\"Wrap functions that receive parser_context to contain those parser\n-    arguments pre-loaded and expose a interface that receives only one argument\n-    \"\"\"\n-    if 'parser_context' in get_func_args(function):\n-        return partial(function, parser_context=context)\n-    else:\n-        return function\n\n@@ -1,7 +1,7 @@\n \"\"\"\n-Item Parser\n+Item Loader\n \n-See documentation in docs/topics/itemparser.rst\n+See documentation in docs/topics/loaders.rst\n \"\"\"\n \n from collections import defaultdict\n@@ -9,14 +9,14 @@ from collections import defaultdict\n from scrapy.newitem import Item\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.utils.misc import arg_to_iter\n-from .common import wrap_parser_context\n-from .parsers import Identity\n+from .common import wrap_loader_context\n+from .processor import Identity\n \n-class ItemParser(object):\n+class ItemLoader(object):\n \n     default_item_class = Item\n-    default_input_parser = Identity()\n-    default_output_parser = Identity()\n+    default_input_processor = Identity()\n+    default_output_processor = Identity()\n \n     def __init__(self, item=None, **context):\n         if item is None:\n@@ -40,34 +40,34 @@ class ItemParser(object):\n         return item\n \n     def get_output_value(self, field_name):\n-        parser = self.get_output_parser(field_name)\n-        parser = wrap_parser_context(parser, self.context)\n-        return parser(self._values[field_name])\n+        proc = self.get_output_processor(field_name)\n+        proc = wrap_loader_context(proc, self.context)\n+        return proc(self._values[field_name])\n \n     def get_collected_values(self, field_name):\n         return self._values[field_name]\n \n-    def get_input_parser(self, field_name):\n-        parser = getattr(self, '%s_in' % field_name, None)\n-        if not parser:\n-            parser = self.item.fields[field_name].get('input_parser', \\\n-                self.default_input_parser)\n-        return parser\n+    def get_input_processor(self, field_name):\n+        proc = getattr(self, '%s_in' % field_name, None)\n+        if not proc:\n+            proc = self.item.fields[field_name].get('input_processor', \\\n+                self.default_input_processor)\n+        return proc\n \n-    def get_output_parser(self, field_name):\n-        parser = getattr(self, '%s_out' % field_name, None)\n-        if not parser:\n-            parser = self.item.fields[field_name].get('output_parser', \\\n-                self.default_output_parser)\n-        return parser\n+    def get_output_processor(self, field_name):\n+        proc = getattr(self, '%s_out' % field_name, None)\n+        if not proc:\n+            proc = self.item.fields[field_name].get('output_processor', \\\n+                self.default_output_processor)\n+        return proc\n \n     def _parse_input_value(self, field_name, value):\n-        parser = self.get_input_parser(field_name)\n-        parser = wrap_parser_context(parser, self.context)\n-        return parser(value)\n+        proc = self.get_input_processor(field_name)\n+        proc = wrap_loader_context(proc, self.context)\n+        return proc(value)\n \n \n-class XPathItemParser(ItemParser):\n+class XPathItemLoader(ItemLoader):\n \n     default_selector_class = HtmlXPathSelector\n \n@@ -79,7 +79,7 @@ class XPathItemParser(ItemParser):\n             selector = self.default_selector_class(response)\n         self.selector = selector\n         context.update(selector=selector, response=response)\n-        super(XPathItemParser, self).__init__(item, **context)\n+        super(XPathItemLoader, self).__init__(item, **context)\n \n     def add_xpath(self, field_name, xpath, re=None):\n         self.add_value(field_name, self._get_values(field_name, xpath, re))\n\n@@ -0,0 +1,13 @@\n+\"\"\"Common functions used in Item Loaders code\"\"\"\n+\n+from functools import partial\n+from scrapy.utils.python import get_func_args\n+\n+def wrap_loader_context(function, context):\n+    \"\"\"Wrap functions that receive loader_context to contain the context\n+    \"pre-loaded\" and expose a interface that receives only one argument\n+    \"\"\"\n+    if 'loader_context' in get_func_args(function):\n+        return partial(function, loader_context=context)\n+    else:\n+        return function\n\n@@ -1,26 +1,26 @@\n \"\"\"\n-This module provides some commonly used parser functions for Item Parsers.\n+This module provides some commonly used processors for Item Loaders.\n \n-See documentation in docs/topics/itemparser.rst\n+See documentation in docs/topics/loaders.rst\n \"\"\"\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.datatypes import MergeDict\n-from .common import wrap_parser_context\n+from .common import wrap_loader_context\n \n class ApplyConcat(object):\n \n-    def __init__(self, *functions, **default_parser_context):\n+    def __init__(self, *functions, **default_loader_context):\n         self.functions = functions\n-        self.default_parser_context = default_parser_context\n+        self.default_loader_context = default_loader_context\n         \n-    def __call__(self, value, parser_context=None):\n+    def __call__(self, value, loader_context=None):\n         values = arg_to_iter(value)\n-        if parser_context:\n-            context = MergeDict(parser_context, self.default_parser_context)\n+        if loader_context:\n+            context = MergeDict(loader_context, self.default_loader_context)\n         else:\n-            context = self.default_parser_context\n-        wrapped_funcs = [wrap_parser_context(f, context) for f in self.functions]\n+            context = self.default_loader_context\n+        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n         for func in wrapped_funcs:\n             next_values = []\n             for v in values:\n\n@@ -1,7 +1,7 @@\n import unittest\n \n-from scrapy.contrib.itemparser import ItemParser, XPathItemParser\n-from scrapy.contrib.itemparser.parsers import ApplyConcat, Join, Identity\n+from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n+from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity\n from scrapy.newitem import Item, Field\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n@@ -15,30 +15,30 @@ class TestItem(NameItem):\n     url = Field()\n     summary = Field()\n \n-# test item parsers\n+# test item loaders\n \n-class NameItemParser(ItemParser):\n+class NameItemLoader(ItemLoader):\n     default_item_class = TestItem\n \n-class TestItemParser(NameItemParser):\n+class TestItemLoader(NameItemLoader):\n     name_in = ApplyConcat(lambda v: v.title())\n \n-class DefaultedItemParser(NameItemParser):\n-    default_input_parser = ApplyConcat(lambda v: v[:-1])\n+class DefaultedItemLoader(NameItemLoader):\n+    default_input_processor = ApplyConcat(lambda v: v[:-1])\n \n-# test parsers\n+# test processors\n \n-def parser_with_args(value, other=None, parser_context=None):\n-    if 'key' in parser_context:\n-        return parser_context['key']\n+def processor_with_args(value, other=None, loader_context=None):\n+    if 'key' in loader_context:\n+        return loader_context['key']\n     return value\n \n-class ItemParserTest(unittest.TestCase):\n+class ItemLoaderTest(unittest.TestCase):\n \n     def test_populate_item_using_default_loader(self):\n         i = TestItem()\n         i['summary'] = u'lala'\n-        ip = ItemParser(item=i)\n+        ip = ItemLoader(item=i)\n         ip.add_value('name', u'marta')\n         item = ip.populate_item()\n         assert item is i\n@@ -46,13 +46,13 @@ class ItemParserTest(unittest.TestCase):\n         self.assertEqual(item['name'], [u'marta'])\n \n     def test_populate_item_using_custom_loader(self):\n-        ip = TestItemParser()\n+        ip = TestItemLoader()\n         ip.add_value('name', u'marta')\n         item = ip.populate_item()\n         self.assertEqual(item['name'], [u'Marta'])\n \n     def test_add_value(self):\n-        ip = TestItemParser()\n+        ip = TestItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_collected_values('name'), [u'Marta'])\n         self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n@@ -61,7 +61,7 @@ class ItemParserTest(unittest.TestCase):\n         self.assertEqual(ip.get_output_value('name'), [u'Marta', u'Pepe'])\n \n     def test_replace_value(self):\n-        ip = TestItemParser()\n+        ip = TestItemLoader()\n         ip.replace_value('name', u'marta')\n         self.assertEqual(ip.get_collected_values('name'), [u'Marta'])\n         self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n@@ -69,208 +69,208 @@ class ItemParserTest(unittest.TestCase):\n         self.assertEqual(ip.get_collected_values('name'), [u'Pepe'])\n         self.assertEqual(ip.get_output_value('name'), [u'Pepe'])\n \n-    def test_map_concat_filter(self):\n+    def test_apply_concat_filter(self):\n         def filter_world(x):\n             return None if x == 'world' else x\n \n-        parser = ApplyConcat(filter_world, str.upper)\n-        self.assertEqual(parser(['hello', 'world', 'this', 'is', 'scrapy']),\n+        proc = ApplyConcat(filter_world, str.upper)\n+        self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),\n                          ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n \n     def test_map_concat_filter_multiple_functions(self):\n-        class TestItemParser(NameItemParser):\n+        class TestItemLoader(NameItemLoader):\n             name_in = ApplyConcat(lambda v: v.title(), lambda v: v[:-1])\n \n-        ip = TestItemParser()\n+        ip = TestItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'Mart'])\n         item = ip.populate_item()\n         self.assertEqual(item['name'], [u'Mart'])\n \n-    def test_default_input_parser(self):\n-        ip = DefaultedItemParser()\n+    def test_default_input_processor(self):\n+        ip = DefaultedItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'mart'])\n \n-    def test_inherited_default_input_parser(self):\n-        class InheritDefaultedItemParser(DefaultedItemParser):\n+    def test_inherited_default_input_processor(self):\n+        class InheritDefaultedItemLoader(DefaultedItemLoader):\n             pass\n \n-        ip = InheritDefaultedItemParser()\n+        ip = InheritDefaultedItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'mart'])\n \n-    def test_input_parser_inheritance(self):\n-        class ChildItemParser(TestItemParser):\n+    def test_input_processor_inheritance(self):\n+        class ChildItemLoader(TestItemLoader):\n             url_in = ApplyConcat(lambda v: v.lower())\n \n-        ip = ChildItemParser()\n+        ip = ChildItemLoader()\n         ip.add_value('url', u'HTTP://scrapy.ORG')\n         self.assertEqual(ip.get_output_value('url'), [u'http://scrapy.org'])\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n \n-        class ChildChildItemParser(ChildItemParser):\n+        class ChildChildItemLoader(ChildItemLoader):\n             url_in = ApplyConcat(lambda v: v.upper())\n             summary_in = ApplyConcat(lambda v: v)\n \n-        ip = ChildChildItemParser()\n+        ip = ChildChildItemLoader()\n         ip.add_value('url', u'http://scrapy.org')\n         self.assertEqual(ip.get_output_value('url'), [u'HTTP://SCRAPY.ORG'])\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n \n     def test_empty_map_concat(self):\n-        class IdentityDefaultedItemParser(DefaultedItemParser):\n+        class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = ApplyConcat()\n \n-        ip = IdentityDefaultedItemParser()\n+        ip = IdentityDefaultedItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'marta'])\n \n-    def test_identity_input_parser(self):\n-        class IdentityDefaultedItemParser(DefaultedItemParser):\n+    def test_identity_input_processor(self):\n+        class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = Identity()\n \n-        ip = IdentityDefaultedItemParser()\n+        ip = IdentityDefaultedItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'marta'])\n \n-    def test_extend_custom_input_parsers(self):\n-        class ChildItemParser(TestItemParser):\n-            name_in = ApplyConcat(TestItemParser.name_in, unicode.swapcase)\n+    def test_extend_custom_input_processors(self):\n+        class ChildItemLoader(TestItemLoader):\n+            name_in = ApplyConcat(TestItemLoader.name_in, unicode.swapcase)\n \n-        ip = ChildItemParser()\n+        ip = ChildItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'mARTA'])\n \n-    def test_extend_default_input_parsers(self):\n-        class ChildDefaultedItemParser(DefaultedItemParser):\n-            name_in = ApplyConcat(DefaultedItemParser.default_input_parser, unicode.swapcase)\n+    def test_extend_default_input_processors(self):\n+        class ChildDefaultedItemLoader(DefaultedItemLoader):\n+            name_in = ApplyConcat(DefaultedItemLoader.default_input_processor, unicode.swapcase)\n \n-        ip = ChildDefaultedItemParser()\n+        ip = ChildDefaultedItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'MART'])\n \n-    def test_output_parser_using_function(self):\n-        ip = TestItemParser()\n+    def test_output_processor_using_function(self):\n+        ip = TestItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n \n-        class TakeFirstItemParser(TestItemParser):\n+        class TakeFirstItemLoader(TestItemLoader):\n             name_out = u\" \".join\n \n-        ip = TakeFirstItemParser()\n+        ip = TakeFirstItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), u'Mar Ta')\n \n-    def test_output_parser_using_classes(self):\n-        ip = TestItemParser()\n+    def test_output_processor_using_classes(self):\n+        ip = TestItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n \n-        class TakeFirstItemParser(TestItemParser):\n+        class TakeFirstItemLoader(TestItemLoader):\n             name_out = Join()\n \n-        ip = TakeFirstItemParser()\n+        ip = TakeFirstItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), u'Mar Ta')\n \n-        class TakeFirstItemParser(TestItemParser):\n+        class TakeFirstItemLoader(TestItemLoader):\n             name_out = Join(\"<br>\")\n \n-        ip = TakeFirstItemParser()\n+        ip = TakeFirstItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), u'Mar<br>Ta')\n \n-    def test_default_output_parser(self):\n-        ip = TestItemParser()\n+    def test_default_output_processor(self):\n+        ip = TestItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n \n-        class LalaItemParser(TestItemParser):\n-            default_output_parser = Identity()\n+        class LalaItemLoader(TestItemLoader):\n+            default_output_processor = Identity()\n \n-        ip = LalaItemParser()\n+        ip = LalaItemLoader()\n         ip.add_value('name', [u'mar', u'ta'])\n         self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])\n \n-    def test_parser_context_on_declaration(self):\n-        class ChildItemParser(TestItemParser):\n-            url_in = ApplyConcat(parser_with_args, key=u'val')\n+    def test_loader_context_on_declaration(self):\n+        class ChildItemLoader(TestItemLoader):\n+            url_in = ApplyConcat(processor_with_args, key=u'val')\n \n-        ip = ChildItemParser()\n+        ip = ChildItemLoader()\n         ip.add_value('url', u'text')\n         self.assertEqual(ip.get_output_value('url'), ['val'])\n         ip.replace_value('url', u'text2')\n         self.assertEqual(ip.get_output_value('url'), ['val'])\n \n-    def test_parser_context_on_instantiation(self):\n-        class ChildItemParser(TestItemParser):\n-            url_in = ApplyConcat(parser_with_args)\n+    def test_loader_context_on_instantiation(self):\n+        class ChildItemLoader(TestItemLoader):\n+            url_in = ApplyConcat(processor_with_args)\n \n-        ip = ChildItemParser(key=u'val')\n+        ip = ChildItemLoader(key=u'val')\n         ip.add_value('url', u'text')\n         self.assertEqual(ip.get_output_value('url'), ['val'])\n         ip.replace_value('url', u'text2')\n         self.assertEqual(ip.get_output_value('url'), ['val'])\n \n-    def test_parser_context_on_assign(self):\n-        class ChildItemParser(TestItemParser):\n-            url_in = ApplyConcat(parser_with_args)\n+    def test_loader_context_on_assign(self):\n+        class ChildItemLoader(TestItemLoader):\n+            url_in = ApplyConcat(processor_with_args)\n \n-        ip = ChildItemParser()\n+        ip = ChildItemLoader()\n         ip.context['key'] = u'val'\n         ip.add_value('url', u'text')\n         self.assertEqual(ip.get_output_value('url'), ['val'])\n         ip.replace_value('url', u'text2')\n         self.assertEqual(ip.get_output_value('url'), ['val'])\n \n-    def test_item_passed_to_input_parser_functions(self):\n-        def parser(value, parser_context):\n-            return parser_context['item']['name']\n+    def test_item_passed_to_input_processor_functions(self):\n+        def processor(value, loader_context):\n+            return loader_context['item']['name']\n \n-        class ChildItemParser(TestItemParser):\n-            url_in = ApplyConcat(parser)\n+        class ChildItemLoader(TestItemLoader):\n+            url_in = ApplyConcat(processor)\n \n         it = TestItem(name='marta')\n-        ip = ChildItemParser(item=it)\n+        ip = ChildItemLoader(item=it)\n         ip.add_value('url', u'text')\n         self.assertEqual(ip.get_output_value('url'), ['marta'])\n         ip.replace_value('url', u'text2')\n         self.assertEqual(ip.get_output_value('url'), ['marta'])\n \n     def test_add_value_on_unknown_field(self):\n-        ip = TestItemParser()\n+        ip = TestItemLoader()\n         self.assertRaises(KeyError, ip.add_value, 'wrong_field', [u'lala', u'lolo'])\n \n \n-class TestXPathItemParser(XPathItemParser):\n+class TestXPathItemLoader(XPathItemLoader):\n     default_item_class = TestItem\n     name_in = ApplyConcat(lambda v: v.title())\n \n-class XPathItemParserTest(unittest.TestCase):\n+class XPathItemLoaderTest(unittest.TestCase):\n \n     def test_constructor_errors(self):\n-        self.assertRaises(RuntimeError, XPathItemParser)\n+        self.assertRaises(RuntimeError, XPathItemLoader)\n \n     def test_constructor_with_selector(self):\n         sel = HtmlXPathSelector(text=u\"<html><body><div>marta</div></body></html>\")\n-        l = TestXPathItemParser(selector=sel)\n+        l = TestXPathItemLoader(selector=sel)\n         self.assert_(l.selector is sel)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n     def test_constructor_with_response(self):\n         response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n-        l = TestXPathItemParser(response=response)\n+        l = TestXPathItemLoader(response=response)\n         self.assert_(l.selector)\n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n \n     def test_add_xpath_re(self):\n         response = HtmlResponse(url=\"\", body=\"<html><body><div>marta</div></body></html>\")\n-        l = TestXPathItemParser(response=response)\n+        l = TestXPathItemLoader(response=response)\n         l.add_xpath('name', '//div/text()', re='ma')\n         self.assertEqual(l.get_output_value('name'), [u'Ma'])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf566d6238681a8dcfa7ade53801230cb36f44a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,6 +54,14 @@ class ResponseUtilsTest(unittest.TestCase):\n         response = Response(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))\n \n+        # meta refresh in multiple lines\n+        body = \"\"\"<html><head>\n+               <META\n+               HTTP-EQUIV=\"Refresh\"\n+               CONTENT=\"1; URL=http://example.org/newpage\">\"\"\"\n+        response = Response(url='http://example.org', body=body)\n+        self.assertEqual(get_meta_refresh(response), ('1', 'http://example.org/newpage'))\n+\n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n@@ -31,7 +31,7 @@ def get_base_url(response):\n         response.cache['base_url'] = match.group(1) if match else response.url\n     return response.cache['base_url']\n \n-META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);\\s*url=([^\"\\']+)', re.IGNORECASE)\n+META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);\\s*url=([^\"\\']+)', re.DOTALL | re.IGNORECASE)\n def get_meta_refresh(response):\n     \"\"\" Return a tuple of two strings containing the interval and url included\n     in the http-equiv parameter of the HTML meta element. If no url is included\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77670a6b13cb5a00c837cced9b1434f7afbcfabd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 29 | Churn Cumulative: 217 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,23 @@ class ApplyConcat(object):\n         return list(values)\n \n \n+class Pipe(object):\n+\n+    def __init__(self, *functions, **default_parser_context):\n+        self.functions = functions\n+        self.default_parser_context = default_parser_context\n+    \n+    def __call__(self, value, parser_context=None):\n+        if parser_context:\n+            context = MergeDict(parser_context, self.default_parser_context)\n+        else:\n+            context = self.default_parser_context\n+        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n+        for func in wrapped_funcs:\n+            value = func(value)\n+        return value\n+\n+\n class TakeFirst(object):\n \n     def __call__(self, values):\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n-from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity\n+from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Pipe\n from scrapy.newitem import Item, Field\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n@@ -244,6 +244,16 @@ class ItemLoaderTest(unittest.TestCase):\n         ip = TestItemLoader()\n         self.assertRaises(KeyError, ip.add_value, 'wrong_field', [u'lala', u'lolo'])\n \n+    def test_pipe_pprocwssor(self):\n+        class TestItemLoader(NameItemLoader):\n+            name_out = Pipe(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])\n+\n+        il = TestItemLoader()\n+        il.add_value('name', [u'marta', u'other'])\n+        self.assertEqual(il.get_output_value('name'), u'Mart')\n+        item = il.populate_item()\n+        self.assertEqual(item['name'], u'Mart')\n+\n \n class TestXPathItemLoader(XPathItemLoader):\n     default_item_class = TestItem\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d94b6335653f5cfad3910a7c7643c3dc33385834", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 248 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -33,7 +33,7 @@ class ItemLoader(object):\n         parsed_value = self._parse_input_value(field_name, value)\n         self._values[field_name] = arg_to_iter(parsed_value)\n \n-    def populate_item(self):\n+    def load_item(self):\n         item = self.item\n         for field_name in self._values:\n             item[field_name] = self.get_output_value(field_name)\n\n@@ -35,20 +35,20 @@ def processor_with_args(value, other=None, loader_context=None):\n \n class ItemLoaderTest(unittest.TestCase):\n \n-    def test_populate_item_using_default_loader(self):\n+    def test_load_item_using_default_loader(self):\n         i = TestItem()\n         i['summary'] = u'lala'\n         ip = ItemLoader(item=i)\n         ip.add_value('name', u'marta')\n-        item = ip.populate_item()\n+        item = ip.load_item()\n         assert item is i\n         self.assertEqual(item['summary'], u'lala')\n         self.assertEqual(item['name'], [u'marta'])\n \n-    def test_populate_item_using_custom_loader(self):\n+    def test_load_item_using_custom_loader(self):\n         ip = TestItemLoader()\n         ip.add_value('name', u'marta')\n-        item = ip.populate_item()\n+        item = ip.load_item()\n         self.assertEqual(item['name'], [u'Marta'])\n \n     def test_add_value(self):\n@@ -84,7 +84,7 @@ class ItemLoaderTest(unittest.TestCase):\n         ip = TestItemLoader()\n         ip.add_value('name', u'marta')\n         self.assertEqual(ip.get_output_value('name'), [u'Mart'])\n-        item = ip.populate_item()\n+        item = ip.load_item()\n         self.assertEqual(item['name'], [u'Mart'])\n \n     def test_default_input_processor(self):\n@@ -251,7 +251,7 @@ class ItemLoaderTest(unittest.TestCase):\n         il = TestItemLoader()\n         il.add_value('name', [u'marta', u'other'])\n         self.assertEqual(il.get_output_value('name'), u'Mart')\n-        item = il.populate_item()\n+        item = il.load_item()\n         self.assertEqual(item['name'], u'Mart')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e74f32401999bf993d2f18d635d0c90541a00ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 249 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -29,17 +29,17 @@ class ApplyConcat(object):\n         return list(values)\n \n \n-class Pipe(object):\n+class Compose(object):\n \n-    def __init__(self, *functions, **default_parser_context):\n+    def __init__(self, *functions, **default_loader_context):\n         self.functions = functions\n-        self.default_parser_context = default_parser_context\n+        self.default_loader_context = default_loader_context\n     \n-    def __call__(self, value, parser_context=None):\n-        if parser_context:\n-            context = MergeDict(parser_context, self.default_parser_context)\n+    def __call__(self, value, loader_context=None):\n+        if loader_context:\n+            context = MergeDict(loader_context, self.default_loader_context)\n         else:\n-            context = self.default_parser_context\n+            context = self.default_loader_context\n         wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n         for func in wrapped_funcs:\n             value = func(value)\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n-from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Pipe\n+from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Compose\n from scrapy.newitem import Item, Field\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n@@ -244,9 +244,9 @@ class ItemLoaderTest(unittest.TestCase):\n         ip = TestItemLoader()\n         self.assertRaises(KeyError, ip.add_value, 'wrong_field', [u'lala', u'lolo'])\n \n-    def test_pipe_pprocwssor(self):\n+    def test_compose_processor(self):\n         class TestItemLoader(NameItemLoader):\n-            name_out = Pipe(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])\n+            name_out = Compose(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])\n \n         il = TestItemLoader()\n         il.add_value('name', [u'marta', u'other'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6dd4d09556eaa2a8abdd5eb6918b99133b9265d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 17 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 285 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.datatypes import MergeDict\n from .common import wrap_loader_context\n \n-class ApplyConcat(object):\n+class MapCompose(object):\n \n     def __init__(self, *functions, **default_loader_context):\n         self.functions = functions\n@@ -26,7 +26,7 @@ class ApplyConcat(object):\n             for v in values:\n                 next_values += arg_to_iter(func(v))\n             values = next_values\n-        return list(values)\n+        return values\n \n \n class Compose(object):\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n-from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Compose\n+from scrapy.contrib.loader.processor import Join, Identity, Compose, MapCompose\n from scrapy.newitem import Item, Field\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n@@ -21,10 +21,10 @@ class NameItemLoader(ItemLoader):\n     default_item_class = TestItem\n \n class TestItemLoader(NameItemLoader):\n-    name_in = ApplyConcat(lambda v: v.title())\n+    name_in = MapCompose(lambda v: v.title())\n \n class DefaultedItemLoader(NameItemLoader):\n-    default_input_processor = ApplyConcat(lambda v: v[:-1])\n+    default_input_processor = MapCompose(lambda v: v[:-1])\n \n # test processors\n \n@@ -73,13 +73,13 @@ class ItemLoaderTest(unittest.TestCase):\n         def filter_world(x):\n             return None if x == 'world' else x\n \n-        proc = ApplyConcat(filter_world, str.upper)\n+        proc = MapCompose(filter_world, str.upper)\n         self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),\n                          ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n \n     def test_map_concat_filter_multiple_functions(self):\n         class TestItemLoader(NameItemLoader):\n-            name_in = ApplyConcat(lambda v: v.title(), lambda v: v[:-1])\n+            name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])\n \n         ip = TestItemLoader()\n         ip.add_value('name', u'marta')\n@@ -102,7 +102,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_input_processor_inheritance(self):\n         class ChildItemLoader(TestItemLoader):\n-            url_in = ApplyConcat(lambda v: v.lower())\n+            url_in = MapCompose(lambda v: v.lower())\n \n         ip = ChildItemLoader()\n         ip.add_value('url', u'HTTP://scrapy.ORG')\n@@ -111,8 +111,8 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(ip.get_output_value('name'), [u'Marta'])\n \n         class ChildChildItemLoader(ChildItemLoader):\n-            url_in = ApplyConcat(lambda v: v.upper())\n-            summary_in = ApplyConcat(lambda v: v)\n+            url_in = MapCompose(lambda v: v.upper())\n+            summary_in = MapCompose(lambda v: v)\n \n         ip = ChildChildItemLoader()\n         ip.add_value('url', u'http://scrapy.org')\n@@ -122,7 +122,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_empty_map_concat(self):\n         class IdentityDefaultedItemLoader(DefaultedItemLoader):\n-            name_in = ApplyConcat()\n+            name_in = MapCompose()\n \n         ip = IdentityDefaultedItemLoader()\n         ip.add_value('name', u'marta')\n@@ -138,7 +138,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_extend_custom_input_processors(self):\n         class ChildItemLoader(TestItemLoader):\n-            name_in = ApplyConcat(TestItemLoader.name_in, unicode.swapcase)\n+            name_in = MapCompose(TestItemLoader.name_in, unicode.swapcase)\n \n         ip = ChildItemLoader()\n         ip.add_value('name', u'marta')\n@@ -146,7 +146,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_extend_default_input_processors(self):\n         class ChildDefaultedItemLoader(DefaultedItemLoader):\n-            name_in = ApplyConcat(DefaultedItemLoader.default_input_processor, unicode.swapcase)\n+            name_in = MapCompose(DefaultedItemLoader.default_input_processor, unicode.swapcase)\n \n         ip = ChildDefaultedItemLoader()\n         ip.add_value('name', u'marta')\n@@ -197,7 +197,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_loader_context_on_declaration(self):\n         class ChildItemLoader(TestItemLoader):\n-            url_in = ApplyConcat(processor_with_args, key=u'val')\n+            url_in = MapCompose(processor_with_args, key=u'val')\n \n         ip = ChildItemLoader()\n         ip.add_value('url', u'text')\n@@ -207,7 +207,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_loader_context_on_instantiation(self):\n         class ChildItemLoader(TestItemLoader):\n-            url_in = ApplyConcat(processor_with_args)\n+            url_in = MapCompose(processor_with_args)\n \n         ip = ChildItemLoader(key=u'val')\n         ip.add_value('url', u'text')\n@@ -217,7 +217,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n     def test_loader_context_on_assign(self):\n         class ChildItemLoader(TestItemLoader):\n-            url_in = ApplyConcat(processor_with_args)\n+            url_in = MapCompose(processor_with_args)\n \n         ip = ChildItemLoader()\n         ip.context['key'] = u'val'\n@@ -231,7 +231,7 @@ class ItemLoaderTest(unittest.TestCase):\n             return loader_context['item']['name']\n \n         class ChildItemLoader(TestItemLoader):\n-            url_in = ApplyConcat(processor)\n+            url_in = MapCompose(processor)\n \n         it = TestItem(name='marta')\n         ip = ChildItemLoader(item=it)\n@@ -257,7 +257,7 @@ class ItemLoaderTest(unittest.TestCase):\n \n class TestXPathItemLoader(XPathItemLoader):\n     default_item_class = TestItem\n-    name_in = ApplyConcat(lambda v: v.title())\n+    name_in = MapCompose(lambda v: v.title())\n \n class XPathItemLoaderTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6452e1934c78d2a64f259154cfd00e93bd15dfd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 596 | Files Changed: 9 | Hunks: 9 | Methods Changed: 52 | Complexity Δ (Sum/Max): -96/0 | Churn Δ: 596 | Churn Cumulative: 718 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 22 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -1,5 +0,0 @@\n-from scrapy.contrib_exp.adaptors.extraction import extract, ExtractImageLinks\n-from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, replace_escape, unquote\n-from scrapy.contrib_exp.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, delist, Regex\n-from scrapy.contrib_exp.adaptors.date import to_date\n-\n\n@@ -1,14 +0,0 @@\n-import time\n-\n-def to_date(format):\n-    \"\"\"\n-    Converts a date string to a YYYY-MM-DD one suitable for DateField\n-\n-    format is the format string passed to time.strptime\n-    \n-    Input: string/unicode\n-    Output: string\n-    \"\"\"\n-    def _to_date(value):\n-        return time.strftime('%Y-%m-%d', time.strptime(value, format))\n-    return _to_date\n\n@@ -1,74 +0,0 @@\n-\"\"\"\n-Adaptors related with extraction of data\n-\"\"\"\n-\n-from scrapy.utils.url import urljoin_rfc\n-from scrapy.utils.response import get_base_url\n-from scrapy.utils.python import unicode_to_str\n-from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n-from scrapy.contrib.linkextractors.image import HTMLImageLinkExtractor\n-\n-def extract(location, adaptor_args=None):\n-    \"\"\"\n-    This adaptor tries to extract data from the given locations.\n-    Any XPathSelector in it will be extracted, and any other data\n-    will be added as-is to the result.\n-\n-    If an XPathSelector is a text/cdata node, and `use_unquote`\n-    is True, that selector will be extracted using the `extract_unquoted`\n-    method; otherwise, the `extract` method will be used.\n-\n-    Input: anything\n-    Output: tuple of extracted selectors plus anything else in the input\n-    \"\"\"\n-    use_unquote = adaptor_args.get('use_unquote', True) if adaptor_args else True\n-\n-    if isinstance(location, XPathSelectorList):\n-        ret = location.extract()\n-    elif isinstance(location, XPathSelector):\n-        if location.xmlNode.type in ('text', 'cdata') and use_unquote:\n-            ret = location.extract_unquoted()\n-        else:\n-            ret = location.extract()\n-        ret = ret if hasattr(ret, '__iter__') else [ret]\n-    elif isinstance(location, list):\n-        ret = tuple(location)\n-    else:\n-        ret = tuple([location])\n-\n-    return tuple(x for x in ret if x is not None)\n-\n-class ExtractImageLinks(object):\n-    \"\"\"\n-    This adaptor may receive either XPathSelectors pointing to\n-    the desired locations for finding image urls, or just a list of\n-    XPath expressions (which will be turned into selectors anyway).\n-\n-    Input: XPathSelector, XPathSelectorList, iterable\n-    Output: tuple of urls (strings)\n-    \"\"\"\n-    def __init__(self, response=None, canonicalize=True):\n-        self.response = response\n-        self.canonicalize = canonicalize\n-\n-    def __call__(self, locations):\n-        if not locations:\n-            return tuple()\n-        elif not hasattr(locations, '__iter__'):\n-            locations = [locations]\n-\n-        selectors, raw_links = [], []\n-        for location in locations:\n-            if isinstance(location, (XPathSelector, XPathSelectorList)):\n-                selectors.append(location)\n-            else:\n-                raw_links.append(location)\n-\n-        if raw_links:\n-            base_url = get_base_url(self.response)\n-            raw_links = [urljoin_rfc(base_url, unicode_to_str(rel_url), self.response.encoding) for rel_url in raw_links]\n-\n-        lx = HTMLImageLinkExtractor(locations=selectors, canonicalize=self.canonicalize)\n-        urls = map(lambda link: link.url, lx.extract_links(self.response))\n-        return tuple(urls + raw_links)\n-\n\n@@ -1,64 +0,0 @@\n-import re\n-from scrapy.utils.markup import replace_tags, replace_escape_chars, unquote_markup\n-from scrapy.utils.python import str_to_unicode\n-\n-def remove_tags(tags=()):\n-    \"\"\"\n-    Factory that returns an adaptor for removing each\n-    tag in the `tags` parameter found in the given value.\n-    If no `tags` are specified, all of them are removed.\n-\n-    Input: string/unicode\n-    Output: unicode\n-    \"\"\"\n-    def _remove_tags(value):\n-        return replace_tags(value, tags)\n-    return _remove_tags\n-\n-_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-def remove_root(value):\n-    \"\"\"\n-    This adaptor removes the root tag of the given string/unicode,\n-    if it's found.\n-\n-    Input: string/unicode\n-    Output: unicode\n-    \"\"\"\n-    m = _remove_root_re.search(value)\n-    if m:\n-        value = m.group(1)\n-    return str_to_unicode(value)\n-\n-def replace_escape(which_ones=('\\n', '\\t', '\\r'), replace_by=u''):\n-    \"\"\"\n-    Factory that returns an adaptor for removing/replacing each escape\n-    character in the `wich_ones` parameter found in the given value.\n-\n-    If `replace_by` is given, escape characters are replaced by that\n-    text, else they're removed.\n-\n-    Input: string/unicode\n-    Output: unicode\n-    \n-    \"\"\"\n-    def _replace_escape(value):\n-        return replace_escape_chars(value, which_ones, replace_by)\n-    return _replace_escape\n-\n-def unquote(keep=None):\n-    \"\"\"\n-    This factory returns an adaptor that\n-    receives a string or unicode, removes all of the\n-    CDATAs and entities (except the ones in CDATAs, and the ones\n-    you specify in the `keep` parameter) and then, returns a new\n-    string or unicode.\n-\n-    Input: string/unicode\n-    Output: string/unicode\n-    \"\"\"\n-    default_keep = [] if keep is None else keep\n-\n-    def unquote(value, adaptor_args):\n-        keep = adaptor_args.get('keep_entities', default_keep)\n-        return unquote_markup(value, keep=keep)\n-    return unquote\n\n@@ -1,96 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\n-import re\n-from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n-from scrapy.utils.misc import extract_regex\n-from scrapy.utils.python import flatten, str_to_unicode, unicode_to_str\n-from scrapy.contrib.item.adaptors import adaptize\n-\n-def to_unicode(value, adaptor_args):\n-    \"\"\"\n-    Receives a string and converts it to unicode\n-    using the given encoding (if specified, else utf-8 is used)\n-    and returns a new unicode object.\n-    E.g:\n-      >> to_unicode('it costs 20\\xe2\\x82\\xac, or 30\\xc2\\xa3')\n-        [u'it costs 20\\u20ac, or 30\\xa3']\n-\n-    Input: string\n-    Output: unicode\n-    \"\"\"\n-    if not isinstance(value, basestring):\n-        value = str(value)\n-    return str_to_unicode(value, adaptor_args.get('encoding', 'utf-8'))\n-\n-_clean_spaces_re = re.compile(\"\\s+\", re.U)\n-def clean_spaces(value):\n-    \"\"\"\n-    Converts multispaces into single spaces for the given string.\n-    E.g:\n-      >> clean_spaces(u'Hello   sir')\n-      u'Hello sir'\n-\n-    Input: string/unicode\n-    Output: unicode\n-    \"\"\"\n-    return _clean_spaces_re.sub(' ', str_to_unicode(value))\n-\n-def strip(value):\n-    \"\"\"\n-    Removes any spaces at both the start and the ending\n-    of the provided string.\n-    E.g:\n-      >> strip(' hi buddies    ')\n-      u'hi buddies'\n-\n-    Input: string/unicode\n-    Output: string/unicode\n-    \"\"\"\n-    return value.strip()\n-\n-def drop_empty(value):\n-    \"\"\"\n-    Removes any index that evaluates to None\n-    from the provided iterable.\n-    E.g:\n-      >> drop_empty([0, 'this', None, 'is', False, 'an example'])\n-      ['this', 'is', 'an example'] \n-\n-    Input: iterable\n-    Output: list\n-    \"\"\"\n-    return [ v for v in value if v ]\n-\n-def delist(delimiter=''):\n-    \"\"\"\n-    This factory returns and adaptor that joins\n-    an iterable with the specified delimiter.\n-\n-    Input: iterable of strings/unicodes\n-    Output: string/unicode\n-    \"\"\"\n-    def delist(value, adaptor_args):\n-        delim = adaptor_args.get('join_delimiter', delimiter)\n-        return delim.join(value)\n-    return delist\n-\n-class Regex(object):\n-    \"\"\"\n-    This adaptor must receive either a list of strings or an XPathSelector\n-    and return a new list with the matches of the given strings with the given regular\n-    expression (which is passed by a keyword argument, and is mandatory for this adaptor).\n-\n-    Input: XPathSelector, XPathSelectorList, iterable\n-    Output: list of unicodes\n-    \"\"\"\n-    def __init__(self, regex=r''):\n-        self.regex = regex\n-\n-    def __call__(self, value):\n-        if self.regex:\n-            if isinstance(value, (XPathSelector, XPathSelectorList)):\n-                return value.re(self.regex)\n-            elif hasattr(value, '__iter__'):\n-                return flatten([extract_regex(self.regex, string, 'utf-8') for string in value])\n-        return value\n-\n\n\n@@ -1,111 +0,0 @@\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import get_func_args\n-\n-\n-def adaptize(func):\n-    func_args = get_func_args(func)\n-    if 'adaptor_args' in func_args:\n-        return func\n-\n-    def _adaptor(value, adaptor_args):\n-        return func(value)\n-    return _adaptor\n-\n-\n-def IDENTITY(v):\n-    return v\n-\n-\n-class ItemAdaptorMeta(type):\n-    def __new__(meta, class_name, bases, attrs):\n-        da = attrs.get('default_adaptor')\n-        if da:\n-            attrs['default_adaptor'] = staticmethod(adaptize(da))\n-\n-        cls = type.__new__(meta, class_name, bases, attrs)\n-\n-        cls._field_adaptors = cls._field_adaptors.copy()\n-    \n-        if cls.item_class:\n-            for item_field in cls.item_class.fields:\n-                if item_field in attrs:\n-                    adaptor = adaptize(attrs[item_field])\n-                    cls._field_adaptors[item_field] = adaptor\n-                    setattr(cls, item_field, staticmethod(adaptor))\n-        return cls\n-\n-    def __getattr__(cls, name):\n-        if name in cls.item_class.fields:\n-            return cls.default_adaptor\n-        raise AttributeError(name)\n-\n-\n-class ItemAdaptor(object):\n-    __metaclass__ = ItemAdaptorMeta\n-\n-    item_class = None\n-    default_adaptor = IDENTITY\n-\n-    _field_adaptors = {}\n-\n-    def __init__(self, response=None, item=None):\n-        self.item_instance = item if item else self.item_class()\n-        self._response = response\n-\n-    def __setattr__(self, name, value):\n-        if (name.startswith('_') or name == 'item_instance' \\\n-                or name == 'default_adaptor'):\n-            return object.__setattr__(self, name, value)\n-\n-        fa = self._field_adaptors.get(name, self.default_adaptor)\n-\n-        adaptor_args = {'response': self._response, 'item': self.item_instance}\n-        ovalue = fa(value, adaptor_args=adaptor_args)\n-        self.item_instance[name] = ovalue\n-\n-    def __getattribute__(self, name):\n-        if (name.startswith('_') or name.startswith('item_') \\\n-                or name == 'default_adaptor'):\n-            return object.__getattribute__(self, name)\n-\n-        return self.item_instance[name]\n-\n-\n-def adaptor(*funcs, **default_adaptor_args):\n-    \"\"\"A pipe adaptor implementing the tree adaption logic\n-\n-    It takes multiples unnamed arguments used as functions of the pipe, and\n-    keywords used as adaptor_args to be passed to functions that supports it\n-\n-    If an adaptor function returns a list of values, each value is used as\n-    input for next adaptor function\n-\n-    Always returns a list of values\n-    \"\"\"\n-\n-    _funcs = []\n-    for func in funcs:\n-        accepts_args = 'adaptor_args' in get_func_args(func)\n-        _funcs.append((func, accepts_args))\n-\n-    def _adaptor(value, adaptor_args=None):\n-        values = arg_to_iter(value)\n-        aargs = default_adaptor_args\n-        if adaptor_args:\n-            aargs = aargs.copy()\n-            aargs.update(adaptor_args)\n-        pipe_kwargs = {'adaptor_args': aargs}\n-        for func, accepts_args in _funcs:\n-            next = []\n-            kwargs = pipe_kwargs if accepts_args else {}\n-            for val in values:\n-                val = func(val, **kwargs)\n-                if hasattr(val, '__iter__'):\n-                    next.extend(val)\n-                elif val is not None:\n-                    next.append(val)\n-            values = next\n-        return list(values)\n-\n-    return _adaptor\n-\n\n@@ -1,196 +0,0 @@\n-# -*- coding: utf8 -*-\n-import unittest\n-import re\n-\n-from scrapy.contrib.item.adaptors import AdaptorPipe\n-from scrapy.contrib_exp import adaptors\n-from scrapy.http import HtmlResponse, Headers\n-from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n-from scrapy.tests import get_testdata\n-\n-class AdaptorPipeTestCase(unittest.TestCase):\n-    def test_pipe_init(self):\n-        self.assertRaises(TypeError, AdaptorPipe, [adaptors.extract, 'a string'])\n-\n-    def test_adaptor_args(self):\n-        def sample_adaptor(value, adaptor_args):\n-            '''Dummy adaptor that joins the received value with the given string'''\n-            sample_text = adaptor_args.get('sample_arg', 'sample text 1')\n-            return '%s \"%s\"' % (value, sample_text)\n-\n-        sample_value = 'hi, this is my text:'\n-        sample_pipe = AdaptorPipe([sample_adaptor])\n-        self.assertEqual(sample_pipe(sample_value), ('hi, this is my text: \"sample text 1\"', ))\n-        self.assertEqual(sample_pipe(sample_value, {'sample_arg': 'foobarfoobar'}),\n-            ('hi, this is my text: \"foobarfoobar\"', ))\n-\n-    def test_add(self):\n-        pipe1 = AdaptorPipe([adaptors.extract])\n-        pipe2 = [adaptors.remove_tags]\n-        pipe3 = (adaptors.remove_root, )\n-        sample_callable = dir\n-\n-        self.assertTrue(isinstance(pipe1 + pipe1, AdaptorPipe))\n-        self.assertTrue(isinstance(pipe1 + pipe2, AdaptorPipe))\n-        self.assertTrue(isinstance(pipe1 + pipe3, AdaptorPipe))\n-        self.assertTrue(isinstance(pipe1 + sample_callable, AdaptorPipe))\n-\n-class AdaptorsTestCase(unittest.TestCase):\n-\n-    def get_selector(self, domain, url, sample_filename, headers=None, selector=HtmlXPathSelector):\n-        body = get_testdata('adaptors', sample_filename)\n-        response = HtmlResponse(url=url, headers=Headers(headers), status=200, body=body)\n-        return selector(response)\n-\n-    def test_extract(self):\n-        def check_extractor(x, pound=True, euro=True):\n-            poundre = re.compile(r'<span class=\"pound\" .*?>(.*?)</span>')\n-            eurore = re.compile(r'<span class=\"euro\" .*?>(.*?)</span>')\n-\n-            if pound:\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/text()\")), (u'\\xa3', ))\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='pound']/@value\")), (u'\\xa3', ))\n-                self.assertEqual(adaptors.extract(x.re(poundre)), (u'\\xa3', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/text()\")), (u'\\xa3', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundent']/@value\")), (u'\\xa3', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/text()\")), (u'\\xa3', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='poundnum']/@value\")), (u'\\xa3', ))\n-            if euro:\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/text()\")), (u'\\u20ac', ))\n-                self.assertEqual(adaptors.extract(x.x(\"//span[@class='euro']/@value\")), (u'\\u20ac', ))\n-                self.assertEqual(adaptors.extract(x.re(eurore)), (u'\\u20ac', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/text()\")), (u'\\u20ac', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euroent']/@value\")), (u'\\u20ac', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/text()\")), (u'\\u20ac', ))\n-            self.assertEqual(adaptors.extract(x.x(\"//span[@class='euronum']/@value\")), (u'\\u20ac', ))\n-\n-        x = self.get_selector('example.com',\n-                         'http://www.example.com/test/utf8',\n-                         'enc-utf8.html',\n-                         {})\n-        check_extractor(x)\n-\n-        x = self.get_selector('example.com',\n-                         'http://www.example.com/test/latin1',\n-                         'enc-latin1.html',\n-                         {})\n-        check_extractor(x, euro=False)\n-\n-        x = self.get_selector('example.com',\n-                         'http://www.example.com/test/cp1252',\n-                         'enc-cp1252.html',\n-                         {})\n-        check_extractor(x)\n-\n-        # HTTP utf-8 | Meta latin1 | Content ascii | using entities\n-        x = self.get_selector('example.com',\n-                         'http://www.example.com/test/ascii',\n-                         'enc-ascii.html',\n-                         {'Content-Type': ['text/html; charset=utf-8']})\n-        check_extractor(x, pound=False, euro=False)\n-\n-        # Test for inconsistencies between HTTP header encoding and \n-        # META header encoding. It must prefer HTTP header like browsers do\n-        x = self.get_selector('example.com',\n-                         'http://www.example.com/test/utf8-meta-latin1',\n-                         'enc-utf8-meta-latin1.html',\n-                         {'Content-Type': ['text/html; charset=utf-8']})\n-        check_extractor(x)\n-\n-\n-    def test_extract_unquoted(self):\n-        x = self.get_selector('example.com', 'http://www.example.com/test_unquoted', 'extr_unquoted.xml', selector=XmlXPathSelector)\n-\n-        # test unquoting\n-        self.assertEqual(adaptors.extract(x.x('//tag1/text()[2]')[0]), (u'more test text &amp; &gt;', ))\n-        self.assertEqual(adaptors.extract(x.x('//tag2/text()[1]')[0]), (u'blaheawfds<', ))\n-\n-        # test without unquoting\n-        self.assertEqual(adaptors.extract(x.x('//tag1/text()'), {'use_unquote': False}),\n-            (u'test text &amp; &amp;', u'<![CDATA[more test text &amp; &gt;]]>', u'blah&amp;blah'))\n-        self.assertEqual(adaptors.extract(x.x('//tag2/text()'), {'use_unquote': False}), (u'blaheawfds&lt;', ))\n-\n-    def test_extract_image_links(self):\n-        test_data = \"\"\"<html><body>\n-                         <div>\n-                           <a href=\"lala1/lala1.html\">lala1</a>\n-                           <a href=\"/lala2.html\">lala2</a>\n-                           <a href=\"http://foobar.com/pepepe/papapa/lala3.html\">lala3</a>\n-                           <a href=\"lala4.html\"><img src=\"/imgs/lala4.jpg\" /></a>\n-                           <a onclick=\"javascript: opensomething('/my_html1.html');\">something1</a>\n-                           <a onclick=\"javascript: opensomething('dummy/my_html2.html');\">something2</a>\n-                         </div>\n-                       </body></html>\"\"\"\n-        sample_response = HtmlResponse('http://foobar.com/dummy', body=test_data)\n-        sample_selector = HtmlXPathSelector(sample_response)\n-        sample_adaptor = adaptors.ExtractImageLinks(response=sample_response)\n-\n-        self.assertEqual(sample_adaptor(None), tuple())\n-        self.assertEqual(sample_adaptor([]), tuple())\n-        self.assertEqual(sample_adaptor([sample_selector.x('//@href'), 'http://foobar.com/my_image.gif']), (\n-            'http://foobar.com/lala1/lala1.html',\n-            'http://foobar.com/lala2.html',\n-            'http://foobar.com/pepepe/papapa/lala3.html',\n-            'http://foobar.com/lala4.html',\n-            'http://foobar.com/my_image.gif',\n-        ))\n-        self.assertEqual(sample_adaptor(sample_selector.x('//a')), (\n-            'http://foobar.com/lala1/lala1.html',\n-            'http://foobar.com/lala2.html',\n-            'http://foobar.com/pepepe/papapa/lala3.html',\n-            'http://foobar.com/imgs/lala4.jpg',\n-        ))\n-        self.assertEqual(sample_adaptor(sample_selector.x('//a[@onclick]').re(r'opensomething\\(\\'(.*?)\\'\\)')), (\n-            'http://foobar.com/my_html1.html',\n-            'http://foobar.com/dummy/my_html2.html'\n-        ))\n-\n-    def test_to_unicode(self):\n-        self.assertEqual(adaptors.to_unicode('lelelulu\\xc3\\xb1', {}), u'lelelulu\\xf1')\n-        self.assertEqual(adaptors.to_unicode(1, {}), u'1')\n-        self.assertEqual(adaptors.to_unicode('\\xc3\\xa1\\xc3\\xa9', {'encoding': 'utf-8'}), u'\\xe1\\xe9')\n-        self.assertEqual(adaptors.to_unicode('\\xf1', {'encoding': 'latin1'}), u'\\xf1')\n-\n-    def test_regex(self):\n-        adaptor = adaptors.Regex(regex=r'href=\"(.*?)\"')\n-        self.assertEqual(adaptor(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n-                                  '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n-                                  ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n-\n-    def test_unquote_all(self):\n-        unquote = adaptors.unquote()\n-        self.assertEqual(unquote(u'hello&copy;&amp;welcome&lt;br /&gt;&amp;', {}),\n-            u'hello\\xa9&welcome<br />&')\n-\n-    def test_unquote(self):\n-        unquote = adaptors.unquote(keep=['amp', 'lt'])\n-        self.assertEqual(unquote(u'hello&copy;&amp;welcome&lt;br /&gt;&amp;', {}),\n-            u'hello\\xa9&amp;welcome&lt;br />&amp;')\n-\n-    def test_remove_tags(self):\n-        self.assertEqual(adaptors.remove_tags()('<a href=\"lala\">adsaas<br /></a>'), 'adsaas')\n-        self.assertEqual(adaptors.remove_tags()('<div id=\"1\"><table>dsadasf</table></div>'), 'dsadasf')\n-\n-    def test_remove_root(self):\n-        self.assertEqual(adaptors.remove_root('<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>'),\n-            'lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />')\n-\n-    def test_remove_multispaces(self):\n-        self.assertEqual(adaptors.clean_spaces('  hello,  whats     up?'), ' hello, whats up?')\n-\n-    def test_strip(self):\n-        self.assertEqual(adaptors.strip('      hello there, this is my test     '),\n-            'hello there, this is my test')\n-\n-    def test_drop_empty_elements(self):\n-        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, 0, 6, False, 'hi']),\n-            [1, 2, 5, 6, 'hi'])\n-\n-    def test_delist(self):\n-        delist = adaptors.delist(' ')\n-        self.assertEqual(delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.'], {}),\n-            'hi there fellas. this is my test.')\n-        self.assertEqual(delist(['hi', 'there', 'fellas,', 'this', 'is', 'my', 'test.'], {'join_delimiter': '.'}),\n-            'hi.there.fellas,.this.is.my.test.')\n-\n-\n\n@@ -2,8 +2,6 @@\n import unittest\n \n from scrapy.contrib.item import RobustScrapedItem\n-from scrapy.contrib.item.adaptors import AdaptorPipe\n-from scrapy.contrib_exp import adaptors\n \n class MyItem(RobustScrapedItem):\n     ATTRIBUTES = {\n@@ -55,37 +53,3 @@ class RobustScrapedItemTestCase(unittest.TestCase):\n         self.item.attribute('children', 'Johnny', 'Rodrigo', add=True)\n         self.assertEqual(self.item.children, ['Ken', 'Tom', 'Jimmy', 'Johnny', 'Rodrigo'])\n \n-    def test_set_adaptors(self):\n-        self.assertEqual(self.item._adaptors_dict, {})\n-\n-        delist = adaptors.delist()\n-        self.item.set_adaptors({'name': [adaptors.extract, delist]})\n-        self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n-        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n-        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"delist\")\n-\n-        self.item.set_adaptors({'description': [adaptors.extract]})\n-        self.assertEqual(self.item._adaptors_dict['description'][0].name, \"extract\")\n-\n-    def test_set_attrib_adaptors(self):\n-        self.assertEqual(self.item._adaptors_dict, {})\n-\n-        self.item.set_attrib_adaptors('name', [adaptors.extract, adaptors.strip])\n-        self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n-        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n-        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"strip\")\n-\n-        unquote = adaptors.unquote()\n-        self.item.set_attrib_adaptors('name', [adaptors.extract, unquote])\n-        self.assertTrue(isinstance(self.item._adaptors_dict['name'], AdaptorPipe))\n-        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n-        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"unquote\")\n-\n-    def test_add_adaptor(self):\n-        self.assertEqual(self.item._adaptors_dict, {})\n-\n-        self.item.add_adaptor('name', adaptors.strip)\n-        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"strip\")\n-        self.item.add_adaptor('name', adaptors.extract, position=0)\n-        self.assertEqual(self.item._adaptors_dict['name'][0].name, \"extract\")\n-        self.assertEqual(self.item._adaptors_dict['name'][1].name, \"strip\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
