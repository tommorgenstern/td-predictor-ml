{"custom_id": "scrapy#2b11e7da939993668c798f1118e2fe010d783587", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 965 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -103,6 +103,9 @@ class ImagesPipeline(FilesPipeline):\n         media_guid = hashlib.sha1(url).hexdigest()\n         return 'full/%s.jpg' % (media_guid)\n \n+    # backwards compatibility\n+    image_key = file_key\n+\n     def item_completed(self, results, item, info):\n         if 'images' in item.fields:\n             item['images'] = [x for ok, x in results if ok]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83bd151c579c5b02abc9816cd10794223e916303", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 1379 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -229,7 +229,7 @@ class FilesPipeline(MediaPipeline):\n \n         try:\n             key = self.file_key(request.url)\n-            checksum = self.process_downloaded_media(response, request, info)\n+            checksum = self.file_downloaded(response, request, info)\n         except FileException as exc:\n             whyfmt = 'File (error): Error processing image from %(request)s referred in <%(referer)s>: %(errormsg)s'\n             log.msg(format=whyfmt, level=log.WARNING, spider=info.spider,\n@@ -255,7 +255,7 @@ class FilesPipeline(MediaPipeline):\n         media_ext = os.path.splitext(url)[1]\n         return 'full/%s%s' % (media_guid, media_ext)\n \n-    def process_downloaded_media(self, response, request, info):\n+    def file_downloaded(self, response, request, info):\n         key = self.file_key(request.url)\n         buf = StringIO(response.body)\n         self.store.persist_file(key, buf, info)\n\n@@ -46,7 +46,13 @@ class ImagesPipeline(FilesPipeline):\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri)\n \n-    def process_downloaded_media(self, response, request, info):\n+    def file_key(self, url):\n+        return self.image_key(url)\n+\n+    def file_downloaded(self, response, request, info):\n+        return self.image_downloaded(response, request, info)\n+\n+    def image_downloaded(self, response, request, info):\n         checksum = None\n         for key, image, buf in self.get_images(response, request, info):\n             if checksum is None:\n@@ -99,13 +105,11 @@ class ImagesPipeline(FilesPipeline):\n     def get_media_requests(self, item, info):\n         return [Request(x) for x in item.get('image_urls', [])]\n \n-    def file_key(self, url):\n+    # backwards compatibility\n+    def image_key(self, url):\n         media_guid = hashlib.sha1(url).hexdigest()\n         return 'full/%s.jpg' % (media_guid)\n \n-    # backwards compatibility\n-    image_key = file_key\n-\n     def item_completed(self, results, item, info):\n         if 'images' in item.fields:\n             item['images'] = [x for ok, x in results if ok]\n\n@@ -56,7 +56,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         item = _create_item_with_files(item_url)\n         patchers = [\n             mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),\n-            mock.patch.object(FSFilesStore, 'stat_image', return_value={\n+            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n                 'checksum': 'abc', 'last_modified': time.time()}),\n             mock.patch.object(FilesPipeline, 'get_media_requests',\n                               return_value=[_prepare_request_object(item_url)])\n@@ -73,7 +73,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         item_url = \"http://example.com/file2.pdf\"\n         item = _create_item_with_files(item_url)\n         patchers = [\n-            mock.patch.object(FSFilesStore, 'stat_image', return_value={\n+            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n                 'checksum': 'abc',\n                 'last_modified': time.time() - (FilesPipeline.EXPIRES * 60 * 60 * 24 * 2)}),\n             mock.patch.object(FilesPipeline, 'get_media_requests',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19ff9ac4f925790b58b35536dea433797863df23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 13 | Files Changed: 5 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 32 | Churn Cumulative: 1953 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,6 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-def deprecated_setter(setter, attrname):\n+def obsolete_setter(setter, attrname):\n     def newsetter(self, value):\n         c = self.__class__.__name__\n-        warnings.warn(\"Don't modify %s.%s attribute, use %s.replace() instead\" % \\\n-            (c, attrname, c), ScrapyDeprecationWarning, stacklevel=2)\n-        return setter(self, value)\n+        msg = \"%s.%s is not modifiable, use %s.replace() instead\" % (c, attrname, c)\n+        raise AttributeError(msg)\n     return newsetter\n\n@@ -13,7 +13,7 @@ from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.decorator import deprecated\n from scrapy.utils.url import escape_ajax\n-from scrapy.http.common import deprecated_setter\n+from scrapy.http.common import obsolete_setter\n \n class Request(object_ref):\n \n@@ -60,7 +60,7 @@ class Request(object_ref):\n         if ':' not in self._url:\n             raise ValueError('Missing scheme in request url: %s' % self._url)\n \n-    url = property(_get_url, deprecated_setter(_set_url, 'url'))\n+    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n \n     def _get_body(self):\n         return self._body\n@@ -78,7 +78,7 @@ class Request(object_ref):\n         else:\n             raise TypeError(\"Request body must either str or unicode. Got: '%s'\" % type(body).__name__)\n \n-    body = property(_get_body, deprecated_setter(_set_body, 'body'))\n+    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n \n     @property\n     def encoding(self):\n\n@@ -9,7 +9,7 @@ import copy\n \n from scrapy.http.headers import Headers\n from scrapy.utils.trackref import object_ref\n-from scrapy.http.common import deprecated_setter\n+from scrapy.http.common import obsolete_setter\n \n class Response(object_ref):\n \n@@ -39,7 +39,7 @@ class Response(object_ref):\n             raise TypeError('%s url must be str, got %s:' % (type(self).__name__, \\\n                 type(url).__name__))\n \n-    url = property(_get_url, deprecated_setter(_set_url, 'url'))\n+    url = property(_get_url, obsolete_setter(_set_url, 'url'))\n \n     def _get_body(self):\n         return self._body\n@@ -56,7 +56,7 @@ class Response(object_ref):\n             raise TypeError(\"Response body must either str or unicode. Got: '%s'\" \\\n                 % type(body).__name__)\n \n-    body = property(_get_body, deprecated_setter(_set_body, 'body'))\n+    body = property(_get_body, obsolete_setter(_set_body, 'body'))\n \n     def __str__(self):\n         return \"<%d %s>\" % (self.status, self.url)\n\n@@ -175,6 +175,11 @@ class RequestTest(unittest.TestCase):\n         r = self.request_class(\"http://www.example.com\", method=u\"POST\")\n         assert isinstance(r.method, str)\n \n+    def test_immutable_attributes(self):\n+        r = self.request_class(\"http://example.com\")\n+        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n+        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n+\n \n class FormRequestTest(RequestTest):\n \n\n@@ -107,6 +107,11 @@ class BaseResponseTest(unittest.TestCase):\n     def _assert_response_encoding(self, response, encoding):\n         self.assertEqual(response.encoding, resolve_encoding(encoding))\n \n+    def test_immutable_attributes(self):\n+        r = self.response_class(\"http://example.com\")\n+        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n+        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n+\n class ResponseText(BaseResponseTest):\n \n     def test_no_unicode_url(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da1f6d31440dfacd7ef36c6213286a1b2a181b0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 10 | Files Changed: 7 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 2466 | Contributors (this commit): 16 | Commits (past 90d): 19 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,6 @@ from threading import Thread\n \n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n-from scrapy import log\n \n class Command(ScrapyCommand):\n \n\n@@ -10,7 +10,6 @@ from cStringIO import StringIO\n from tempfile import mktemp\n \n from scrapy import log\n-from scrapy.http import Response\n from scrapy.responsetypes import responsetypes\n \n \n\n@@ -8,10 +8,9 @@ from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer\n-from twisted.web.http import PotentialDataLoss\n from twisted.internet.error import TimeoutError\n from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \\\n-    HTTPConnectionPool, TCP4ClientEndpoint, ResponseFailed\n+    HTTPConnectionPool, TCP4ClientEndpoint\n \n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n\n@@ -1,11 +1,9 @@\n from time import time\n from urlparse import urlparse, urlunparse, urldefrag\n \n-from twisted.internet.ssl import ClientContextFactory\n from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n from twisted.internet import defer\n-from OpenSSL import SSL\n \n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -1,5 +1,3 @@\n-import socket\n-\n from twisted.internet import defer\n from twisted.internet.base import ThreadedResolver\n \n\n@@ -6,7 +6,6 @@ See documentation in docs/topics/spiders.rst\n \n from scrapy import log\n from scrapy.http import Request\n-from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n\n@@ -1,7 +1,6 @@\n import cgi\n import unittest\n import xmlrpclib\n-from cStringIO import StringIO\n from urlparse import urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, HtmlResponse\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6720acc1afdb10112a75c883da47ede36ced9731", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 775 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -147,5 +147,7 @@ class _ResponseReader(protocol.Protocol):\n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n             self._finished.callback((self._txresponse, body, None))\n+        elif reason.check(PotentialDataLoss):\n+            self._finished.callback((self._txresponse, body, ['partial']))\n         else:\n             self._finished.errback(reason)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#caa0f90263af76119cfda23d468f902a0e877488", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 48 | Churn Cumulative: 1401 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer\n from twisted.internet.error import TimeoutError\n+from twisted.web.http import PotentialDataLoss\n from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \\\n     HTTPConnectionPool, TCP4ClientEndpoint\n \n\n@@ -20,6 +20,7 @@ class DeferMixin(object):\n             # silence CancelledError\n             d.addErrback(lambda _: None)\n             d.cancel()\n+\n         d = deferLater(reactor, delay, f, *a, **kw)\n         request.notifyFinish().addErrback(_cancelrequest)\n         return d\n@@ -84,6 +85,23 @@ class Status(Resource):\n         return \"\"\n \n \n+class Raw(DeferMixin, Resource):\n+\n+    isLeaf = True\n+\n+    def render_GET(self, request):\n+        request.startedWriting = 1\n+        self.deferRequest(request, 0, self._delayedRender, request)\n+        return NOT_DONE_YET\n+\n+    def _delayedRender(self, request):\n+        raw = getarg(request, 'raw', 'HTTP 1.1 200 OK\\n')\n+        request.startedWriting = 1\n+        request.write(raw)\n+        request.channel.transport.loseConnection()\n+        request.finish()\n+\n+\n class Partial(DeferMixin, Resource):\n \n     isLeaf = True\n@@ -92,6 +110,7 @@ class Partial(DeferMixin, Resource):\n         request.setHeader(\"Content-Length\", \"1024\")\n         self.deferRequest(request, 0, self._delayedRender, request)\n         return NOT_DONE_YET\n+    render_POST = render_GET\n \n     def _delayedRender(self, request):\n         request.write(\"partial content\\n\")\n@@ -119,6 +138,7 @@ class Root(Resource):\n         self.putChild(\"delay\", Delay())\n         self.putChild(\"partial\", Partial())\n         self.putChild(\"drop\", Drop())\n+        self.putChild(\"raw\", Raw())\n \n     def getChild(self, name, request):\n         return self\n\n@@ -90,6 +90,33 @@ class CrawlTestCase(TestCase):\n         yield docrawl(spider)\n         self._assert_retried()\n \n+    @defer.inlineCallbacks\n+    def test_unbounded_response(self):\n+        # Completeness of responses without Content-Length or Transfer-Encoding\n+        # can not be determined, we treat them as valid but flagged as \"partial\"\n+        from urllib import urlencode\n+        query = urlencode({'raw': '''\\\n+HTTP/1.1 200 OK\n+Server: Apache-Coyote/1.1\n+X-Powered-By: Servlet 2.4; JBoss-4.2.3.GA (build: SVNTag=JBoss_4_2_3_GA date=200807181417)/JBossWeb-2.0\n+Set-Cookie: JSESSIONID=08515F572832D0E659FD2B0D8031D75F; Path=/\n+Pragma: no-cache\n+Expires: Thu, 01 Jan 1970 00:00:00 GMT\n+Cache-Control: no-cache\n+Cache-Control: no-store\n+Content-Type: text/html;charset=UTF-8\n+Content-Language: en\n+Date: Tue, 27 Aug 2013 13:05:05 GMT\n+Connection: close\n+\n+foo body\n+with multiples lines\n+'''})\n+        spider = SimpleSpider(\"http://localhost:8998/raw?{}\".format(query))\n+        yield docrawl(spider)\n+        log = get_testlog()\n+        self.assertEqual(log.count(\"Got response 200\"), 1)\n+\n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92826586d5f2740598e7f8a71047decd44c531c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 313 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,7 @@ Connection: close\n foo body\n with multiples lines\n '''})\n-        spider = SimpleSpider(\"http://localhost:8998/raw?{}\".format(query))\n+        spider = SimpleSpider(\"http://localhost:8998/raw?{0}\".format(query))\n         yield docrawl(spider)\n         log = get_testlog()\n         self.assertEqual(log.count(\"Got response 200\"), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#588a262b73e73f319edede130882f2520350d968", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 62 | Churn Cumulative: 614 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,12 +58,15 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         self.factory.gotHeaders(self.headers)\n \n     def connectionLost(self, reason):\n+        self._connection_lost_reason = reason\n         HTTPClient.connectionLost(self, reason)\n         self.factory.noPage(reason)\n \n     def handleResponse(self, response):\n         if self.factory.method.upper() == 'HEAD':\n             self.factory.page('')\n+        elif self.length is not None and self.length > 0:\n+            self.factory.noPage(self._connection_lost_reason)\n         else:\n             self.factory.page(response)\n         self.transport.loseConnection()\n\n@@ -2,7 +2,28 @@ import sys, time, random, urllib\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n-from twisted.internet import reactor\n+from twisted.internet import reactor, defer\n+from scrapy import twisted_version\n+\n+\n+if twisted_version < (11, 0, 0):\n+    def deferLater(clock, delay, func, *args, **kw):\n+        def _cancel_method():\n+            _cancel_cb(None)\n+            d.errback(Exception())\n+\n+        def _cancel_cb(result):\n+            if cl.active():\n+                cl.cancel()\n+            return result\n+\n+        d = defer.Deferred()\n+        d.cancel = _cancel_method\n+        d.addCallback(lambda ignored: func(*args, **kw))\n+        d.addBoth(_cancel_cb)\n+        cl = clock.callLater(delay, d.callback, None)\n+        return d\n+else:\n     from twisted.internet.task import deferLater\n \n \n@@ -13,7 +34,9 @@ def getarg(request, name, default=None, type=str):\n         return default\n \n \n-class DeferMixin(object):\n+class LeafResource(Resource):\n+\n+    isLeaf = True\n \n     def deferRequest(self, request, delay, f, *a, **kw):\n         def _cancelrequest(_):\n@@ -26,9 +49,7 @@ class DeferMixin(object):\n         return d\n \n \n-class Follow(DeferMixin, Resource):\n-\n-    isLeaf = True\n+class Follow(LeafResource):\n \n     def render(self, request):\n         total = getarg(request, \"total\", 100, type=int)\n@@ -57,9 +78,7 @@ class Follow(DeferMixin, Resource):\n         request.finish()\n \n \n-class Delay(DeferMixin, Resource):\n-\n-    isLeaf = True\n+class Delay(LeafResource):\n \n     def render_GET(self, request):\n         n = getarg(request, \"n\", 1, type=float)\n@@ -75,9 +94,7 @@ class Delay(DeferMixin, Resource):\n         request.finish()\n \n \n-class Status(Resource):\n-\n-    isLeaf = True\n+class Status(LeafResource):\n \n     def render_GET(self, request):\n         n = getarg(request, \"n\", 200, type=int)\n@@ -85,14 +102,13 @@ class Status(Resource):\n         return \"\"\n \n \n-class Raw(DeferMixin, Resource):\n-\n-    isLeaf = True\n+class Raw(LeafResource):\n \n     def render_GET(self, request):\n         request.startedWriting = 1\n         self.deferRequest(request, 0, self._delayedRender, request)\n         return NOT_DONE_YET\n+    render_POST = render_GET\n \n     def _delayedRender(self, request):\n         raw = getarg(request, 'raw', 'HTTP 1.1 200 OK\\n')\n@@ -102,15 +118,12 @@ class Raw(DeferMixin, Resource):\n         request.finish()\n \n \n-class Partial(DeferMixin, Resource):\n-\n-    isLeaf = True\n+class Partial(LeafResource):\n \n     def render_GET(self, request):\n         request.setHeader(\"Content-Length\", \"1024\")\n         self.deferRequest(request, 0, self._delayedRender, request)\n         return NOT_DONE_YET\n-    render_POST = render_GET\n \n     def _delayedRender(self, request):\n         request.write(\"partial content\\n\")\n@@ -122,10 +135,13 @@ class Drop(Partial):\n     def _delayedRender(self, request):\n         abort = getarg(request, \"abort\", 0, type=int)\n         request.write(\"this connection will be dropped\\n\")\n-        if abort:\n-            request.channel.transport.abortConnection()\n+        tr = request.channel.transport\n+        try:\n+            if abort and hasattr(tr, 'abortConnection'):\n+                tr.abortConnection()\n             else:\n-            request.channel.transport.loseConnection()\n+                tr.loseConnection()\n+        finally:\n             request.finish()\n \n \n@@ -165,6 +181,7 @@ if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n     port = reactor.listenTCP(8998, factory)\n+\n     def print_listening():\n         h = port.getHost()\n         print \"Mock server running at http://%s:%d\" % (h.host, h.port)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a8bf2c9e4720d4276add52cc46fc601f9ce228a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 290 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -111,7 +111,7 @@ class S3FilesStore(object):\n         if headers:\n             h.update(headers)\n         buf.seek(0)\n-        return threads.deferToThread(k.set_contents_from_file, buf,\n+        return threads.deferToThread(k.set_contents_from_string, buf.getvalue(),\n                                      headers=h, policy=self.POLICY)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50b653e5ae1048be1ec0feb733f36cc06b8f7ea2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 630 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -90,7 +90,7 @@ class ProcessMixin(object):\n         install_shutdown_handlers(self._signal_shutdown)\n \n     def start(self):\n-        self.start_crawling()\n+        if self.start_crawling():\n             self.start_reactor()\n \n     def start_reactor(self):\n@@ -144,6 +144,7 @@ class CrawlerProcess(ProcessMixin):\n         return self.crawlers[name]\n \n     def start_crawler(self):\n+        if self.crawlers and not self.stopping:\n             name, crawler = self.crawlers.popitem()\n \n             sflo = log.start_from_crawler(crawler)\n@@ -159,14 +160,12 @@ class CrawlerProcess(ProcessMixin):\n             return name, crawler\n \n     def check_done(self, **kwargs):\n-        if self.crawlers and not self.stopping:\n-            self.start_crawler()\n-        else:\n+        if not self.start_crawler():\n             self.stop_reactor()\n \n     def start_crawling(self):\n         log.scrapy_info(self.settings)\n-        self.start_crawler()\n+        return self.start_crawler() is not None\n \n     @defer.inlineCallbacks\n     def stop(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2be3f7132a8e82fa00b5bd74b05ce727f9dbd36f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1200 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,7 @@ class BaseItemExporter(object):\n class JsonLinesItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n-        self._configure(kwargs)\n+        self._configure(kwargs, dont_fail=True)\n         self.file = file\n         self.encoder = ScrapyJSONEncoder(**kwargs)\n \n@@ -91,7 +91,7 @@ class JsonLinesItemExporter(BaseItemExporter):\n class JsonItemExporter(JsonLinesItemExporter):\n \n     def __init__(self, file, **kwargs):\n-        self._configure(kwargs)\n+        self._configure(kwargs, dont_fail=True)\n         self.file = file\n         self.encoder = ScrapyJSONEncoder(**kwargs)\n         self.first_item = True\n\n@@ -266,6 +266,13 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         exported = json.loads(self.output.getvalue())\n         self.assertEqual(exported, self._expected_nested)\n \n+    def test_extra_keywords(self):\n+        self.ie = self._get_exporter(sort_keys=True)\n+        self.test_export_item()\n+        self._check_output()\n+        self.assertRaises(TypeError, self._get_exporter, foo_unknown_keyword_bar=True)\n+\n+\n class JsonItemExporterTest(JsonLinesItemExporterTest):\n \n     _expected_nested = [JsonLinesItemExporterTest._expected_nested]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#801564c2c9163c7769648c48bb81e9fccea79730", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 466 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -6,16 +6,16 @@ import csv\n import sys\n import pprint\n import marshal\n-import json\n import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.item import BaseItem\n \n-__all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n-    'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', \\\n+__all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n+           'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter',\n            'JsonItemExporter', 'MarshalItemExporter']\n \n+\n class BaseItemExporter(object):\n \n     def __init__(self, **kwargs):\n@@ -64,7 +64,7 @@ class BaseItemExporter(object):\n                 field_iter = self.fields_to_export\n             else:\n                 nonempty_fields = set(item.keys())\n-                field_iter = (x for x in self.fields_to_export if x in \\\n+                field_iter = (x for x in self.fields_to_export if x in\n                               nonempty_fields)\n         for field_name in field_iter:\n             if field_name in item:\n@@ -182,7 +182,7 @@ class CsvItemExporter(BaseItemExporter):\n             self._headers_not_written = False\n             self._write_headers_and_set_fields_to_export(item)\n \n-        fields = self._get_serialized_fields(item, default_value='', \\\n+        fields = self._get_serialized_fields(item, default_value='',\n                                              include_empty=True)\n         values = [x[1] for x in fields]\n         self.csv_writer.writerow(values)\n@@ -215,6 +215,7 @@ class MarshalItemExporter(BaseItemExporter):\n     def export_item(self, item):\n         marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n \n+\n class PprintItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n@@ -225,6 +226,7 @@ class PprintItemExporter(BaseItemExporter):\n         itemdict = dict(self._get_serialized_fields(item))\n         self.file.write(pprint.pformat(itemdict) + '\\n')\n \n+\n class PythonItemExporter(BaseItemExporter):\n     \"\"\"The idea behind this exporter is to have a mechanism to serialize items\n     to built-in python types so any serialization library (like\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65135db70cde02294598bfd4dc088936e703dc7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 427 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ class ProjectTest(unittest.TestCase):\n         while p.poll() is None:\n             sleep(interval)\n             waited += interval\n-            if waited > 5:\n+            if waited > 15:\n                 p.kill()\n                 assert False, 'Command took too much time to complete'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ed2d0cda16cbe1c94373dacf85a632a25df221a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 12 | Churn Cumulative: 392 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,10 +13,6 @@ class SitemapSpider(BaseSpider):\n     sitemap_follow = ['']\n \n     def __init__(self, *a, **kw):\n-        self._alternate = False\n-        if 'alternate' in kw and kw.pop('alternate') == True:\n-            self._alternate = True\n-\n         super(SitemapSpider, self).__init__(*a, **kw)\n         self._cbs = []\n         for r, c in self.sitemap_rules:\n@@ -41,7 +37,7 @@ class SitemapSpider(BaseSpider):\n \n             s = Sitemap(body)\n             if s.type == 'sitemapindex':\n-                for loc in iterloc(s, self._alternate):\n+                for loc in iterloc(s, self.use_alternate_links):\n                     if any(x.search(loc) for x in self._follow):\n                         yield Request(loc, callback=self._parse_sitemap)\n             elif s.type == 'urlset':\n@@ -74,6 +70,6 @@ def iterloc(it, alt=False):\n         yield d['loc']\n \n         # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n-        if alt == True and 'alternate' in d:\n+        if alt and 'alternate' in d:\n             for l in d['alternate']:\n                 yield l\n\n@@ -171,6 +171,7 @@ Disallow: /forum/active/\n                 href=\"http://www.example.com/schweiz-deutsch/\"/>\n             <xhtml:link rel=\"alternate\" hreflang=\"en\"\n                 href=\"http://www.example.com/english/\"/>\n+            <xhtml:link rel=\"alternate\" hreflang=\"en\"/><!-- wrong tag without href -->\n         </url>\n     </urlset>\"\"\")\n         \n\n@@ -12,7 +12,7 @@ class Sitemap(object):\n     (type=sitemapindex) files\"\"\"\n \n     def __init__(self, xmltext):\n-        xmlp = lxml.etree.XMLParser(recover=True)\n+        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True)\n         self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n         rt = self._root.tag\n         self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt\n@@ -25,6 +25,7 @@ class Sitemap(object):\n                 name = tag.split('}', 1)[1] if '}' in tag else tag\n \n                 if name == 'link':\n+                    if 'href' in el.attrib:\n                         d.setdefault('alternate', []).append(el.get('href'))\n                 else:\n                     d[name] = el.text.strip() if el.text else ''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b326b87154239f1b79e302408c39f01ac322422a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 1800 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,7 +91,7 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n \n     inputs = form.xpath('descendant::textarea'\n                         '|descendant::select'\n-                        '|descendant::input[@type!=\"submit\" and @type!=\"image\" '\n+                        '|descendant::input[@type!=\"submit\" and @type!=\"image\" and @type!=\"reset\"'\n                         'and ((@type!=\"checkbox\" and @type!=\"radio\") or @checked)]')\n     values = [(k, u'' if v is None else v) \\\n               for k, v in (_value(e) for e in inputs) \\\n\n@@ -324,6 +324,18 @@ class FormRequestTest(RequestTest):\n         fs = _qs(req)\n         self.assertEqual(fs, {'i1': ['i1v']})\n \n+    def test_from_response_dont_submit_reset_as_input(self):\n+        response = _buildresponse(\n+            \"\"\"<form>\n+            <input type=\"hidden\" name=\"i1\" value=\"i1v\">\n+            <input type=\"text\" name=\"i2\" value=\"i2v\">\n+            <input type=\"reset\" name=\"resetme\">\n+            <input type=\"submit\" name=\"i3\" value=\"i3v\">\n+            </form>\"\"\")\n+        req = self.request_class.from_response(response, dont_click=True)\n+        fs = _qs(req)\n+        self.assertEqual(fs, {'i1': ['i1v'], 'i2': ['i2v']})\n+\n     def test_from_response_multiple_clickdata(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"get.php\" method=\"GET\">\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#684cfc0b68b7207c5935773f5f5c092178c9dbe6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 853 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -180,15 +180,18 @@ class Scraper(object):\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here)\n         \"\"\"\n-        if spider_failure is download_failure:\n-            errmsg = spider_failure.getErrorMessage()\n-            if spider_failure.frames:\n-                log.err(spider_failure, 'Error downloading %s' % request,\n-                        level=log.ERROR, spider=spider)\n-            elif errmsg:\n+        if isinstance(download_failure, Failure):\n+            if download_failure.frames:\n+                log.err(download_failure, 'Error downloading %s' % request,\n+                        spider=spider)\n+            else:\n+                errmsg = download_failure.getErrorMessage()\n+                if errmsg:\n                     log.msg(format='Error downloading %(request)s: %(errmsg)s',\n-                        level=log.ERROR, spider=spider, request=request, errmsg=errmsg)\n-            return\n+                            level=log.ERROR, spider=spider, request=request,\n+                            errmsg=errmsg)\n+\n+        if spider_failure is not download_failure:\n             return spider_failure\n \n     def _itemproc_finished(self, output, item, response, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8be35dce5561e81d278251e8932c86fc15b9c5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 858 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n-from scrapy.exceptions import CloseSpider, DropItem\n+from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n@@ -180,7 +180,8 @@ class Scraper(object):\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here)\n         \"\"\"\n-        if isinstance(download_failure, Failure):\n+        if isinstance(download_failure, Failure) \\\n+                and not download_failure.check(IgnoreRequest):\n             if download_failure.frames:\n                 log.err(download_failure, 'Error downloading %s' % request,\n                         spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6994959181941529aecd8dd57369b99212555f30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 142 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -11,6 +11,7 @@ class SitemapSpider(BaseSpider):\n     sitemap_urls = ()\n     sitemap_rules = [('', 'parse')]\n     sitemap_follow = ['']\n+    sitemap_alternate_links = False\n \n     def __init__(self, *a, **kw):\n         super(SitemapSpider, self).__init__(*a, **kw)\n@@ -37,7 +38,7 @@ class SitemapSpider(BaseSpider):\n \n             s = Sitemap(body)\n             if s.type == 'sitemapindex':\n-                for loc in iterloc(s, self.use_alternate_links):\n+                for loc in iterloc(s, self.sitemap_alternate_links):\n                     if any(x.search(loc) for x in self._follow):\n                         yield Request(loc, callback=self._parse_sitemap)\n             elif s.type == 'urlset':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#855041ebfd58a3e0e4190455de305d60bfa104aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 204 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -159,6 +159,20 @@ Disallow: /forum/active/\n             {'lastmod': '2013-07-15', 'loc': 'http://www.example.com/sitemap3.xml'},\n         ])\n \n+    def test_comment(self):\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+    <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n+        xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n+        <url>\n+            <loc>http://www.example.com/</loc>\n+            <!-- this is a comment on which the parser might raise an exception if implemented incorrectly -->\n+        </url>\n+    </urlset>\"\"\")\n+\n+        self.assertEqual(list(s), [\n+            {'loc': 'http://www.example.com/'}\n+        ])\n+\n     def test_alternate(self):\n         s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n     <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc388f4636b91ed488e0141a65475c888c67b872", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 835 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in docs/item-pipeline.rst\n \"\"\"\n \n from scrapy.middleware import MiddlewareManager\n+from scrapy.utils.conf import build_component_list\n \n class ItemPipelineManager(MiddlewareManager):\n \n@@ -12,7 +13,16 @@ class ItemPipelineManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return settings.getlist('ITEM_PIPELINES')\n+        item_pipelines = settings['ITEM_PIPELINES']\n+        if isinstance(item_pipelines, (tuple, list)):\n+            from scrapy.exceptions import ScrapyDeprecationWarning\n+            import warnings\n+            warnings.warn('ITEM_PIPELINES defined as a list is deprecated, switch to a dict',\n+                category=ScrapyDeprecationWarning, stacklevel=1)\n+            # convert old ITEM_PIPELINE list to a dict with order 500\n+            item_pipelines = dict(zip(item_pipelines, range(500, 500+len(item_pipelines))))\n+        return build_component_list(settings['ITEM_PIPELINES_BASE'],\n+            settings['ITEM_PIPELINES'])\n \n     def _add_middleware(self, pipe):\n         super(ItemPipelineManager, self)._add_middleware(pipe)\n\n@@ -150,8 +150,8 @@ HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.DummyPolicy'\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n-# Item pipelines are typically set in specific commands settings\n-ITEM_PIPELINES = []\n+ITEM_PIPELINES = {}\n+ITEM_PIPELINES_BASE = {}\n \n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n\n@@ -212,8 +212,8 @@ class MyPipeline(object):\n         fname = abspath(join(self.proj_mod_path, 'settings.py'))\n         with open(fname, 'a') as f:\n             f.write(\"\"\"\n-ITEM_PIPELINES = ['{0}.pipelines.MyPipeline']\n-\"\"\".format(self.project_name))\n+ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n+\"\"\" % self.project_name)\n \n     def test_spider_arguments(self):\n         p = self.proc('parse', '--spider', self.spider_name, '-a', 'test_arg=1',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b74e5aa5518399e45cb4c55d1744b131e4fa1055", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 198 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,15 +14,14 @@ class ItemPipelineManager(MiddlewareManager):\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n         item_pipelines = settings['ITEM_PIPELINES']\n-        if isinstance(item_pipelines, (tuple, list)):\n+        if isinstance(item_pipelines, (tuple, list, set, frozenset)):\n             from scrapy.exceptions import ScrapyDeprecationWarning\n             import warnings\n-            warnings.warn('ITEM_PIPELINES defined as a list is deprecated, switch to a dict',\n+            warnings.warn('ITEM_PIPELINES defined as a list or a set is deprecated, switch to a dict',\n                 category=ScrapyDeprecationWarning, stacklevel=1)\n             # convert old ITEM_PIPELINE list to a dict with order 500\n             item_pipelines = dict(zip(item_pipelines, range(500, 500+len(item_pipelines))))\n-        return build_component_list(settings['ITEM_PIPELINES_BASE'],\n-            settings['ITEM_PIPELINES'])\n+        return build_component_list(settings['ITEM_PIPELINES_BASE'], item_pipelines)\n \n     def _add_middleware(self, pipe):\n         super(ItemPipelineManager, self)._add_middleware(pipe)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc8a93f185617ada3fa2813df09c0149338bbdb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 187 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ Scrapy web services extension\n See docs/topics/webservice.rst\n \"\"\"\n \n-from twisted.web import server, error\n+from twisted.web import server, resource\n \n from scrapy.exceptions import NotConfigured\n from scrapy import log, signals\n@@ -45,7 +45,7 @@ class JsonRpcResource(JsonResource):\n             newtarget = getattr(target, name)\n             return JsonRpcResource(self.crawler, newtarget)\n         except AttributeError:\n-            return error.NoResource(\"No such child resource.\")\n+            return resource.ErrorPage(404, \"No Such Resource\", \"No such child resource.\")\n \n     def get_target(self):\n         return self._target\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a9c351989733599df25a67724a0a49889ac265f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 214 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -122,6 +122,6 @@ try:\n except ImportError:\n     from distutils.core import setup\n else:\n-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']\n+    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']\n \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e81120e68356cb0d880ec839fd42705b6bf1f4e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 91 | Churn Cumulative: 1616 | Contributors (this commit): 8 | Commits (past 90d): 17 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -141,6 +141,8 @@ class FilesPipeline(MediaPipeline):\n         'file': FSFilesStore,\n         's3': S3FilesStore,\n     }\n+    DEFAULT_FILES_URLS_FIELD = 'file_urls'\n+    DEFAULT_FILES_RESULT_FIELD = 'files'\n \n     def __init__(self, store_uri, download_func=None):\n         if not store_uri:\n@@ -154,6 +156,8 @@ class FilesPipeline(MediaPipeline):\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n \n+        cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD', cls.DEFAULT_FILES_URLS_FIELD)\n+        cls.FILES_RESULT_FIELD = settings.get('FILES_RESULT_FIELD', cls.DEFAULT_FILES_RESULT_FIELD)\n         cls.EXPIRES = settings.getint('FILES_EXPIRES', 90)\n         store_uri = settings['FILES_STORE']\n         return cls(store_uri)\n@@ -248,7 +252,7 @@ class FilesPipeline(MediaPipeline):\n \n     ### Overridable Interface\n     def get_media_requests(self, item, info):\n-        return [Request(x) for x in item.get('file_urls', [])]\n+        return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]\n \n     def file_key(self, url):\n         media_guid = hashlib.sha1(url).hexdigest()\n@@ -263,6 +267,6 @@ class FilesPipeline(MediaPipeline):\n         return checksum\n \n     def item_completed(self, results, item, info):\n-        if 'files' in item.fields:\n-            item['files'] = [x for ok, x in results if ok]\n+        if self.FILES_RESULT_FIELD in item.fields:\n+            item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]\n         return item\n\n@@ -33,6 +33,8 @@ class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n     THUMBS = {}\n+    DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n+    DEFAULT_IMAGES_RESULT_FIELD = 'images'\n \n     @classmethod\n     def from_settings(cls, settings):\n@@ -43,6 +45,9 @@ class ImagesPipeline(FilesPipeline):\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n+\n+        cls.IMAGES_URLS_FIELD = settings.get('IMAGES_URLS_FIELD', cls.DEFAULT_IMAGES_URLS_FIELD)\n+        cls.IMAGES_RESULT_FIELD = settings.get('IMAGES_RESULT_FIELD', cls.DEFAULT_IMAGES_RESULT_FIELD)\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri)\n \n@@ -103,7 +108,7 @@ class ImagesPipeline(FilesPipeline):\n         return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n \n     def get_media_requests(self, item, info):\n-        return [Request(x) for x in item.get('image_urls', [])]\n+        return [Request(x) for x in item.get(self.IMAGES_URLS_FIELD, [])]\n \n     # backwards compatibility\n     def image_key(self, url):\n@@ -111,6 +116,6 @@ class ImagesPipeline(FilesPipeline):\n         return 'full/%s.jpg' % (media_guid)\n \n     def item_completed(self, results, item, info):\n-        if 'images' in item.fields:\n-            item['images'] = [x for ok, x in results if ok]\n+        if self.IMAGES_RESULT_FIELD in item.fields:\n+            item[self.IMAGES_RESULT_FIELD] = [x for ok, x in results if ok]\n         return item\n\n@@ -10,6 +10,7 @@ from twisted.internet import defer\n from scrapy.contrib.pipeline.files import FilesPipeline, FSFilesStore\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n+from scrapy.settings import Settings\n \n \n def _mocked_download_func(request, info):\n@@ -21,7 +22,8 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = FilesPipeline(self.tempdir, download_func=_mocked_download_func)\n+        self.pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir}))\n+        self.pipeline.download_func = _mocked_download_func\n         self.pipeline.open_spider(None)\n \n     def tearDown(self):\n@@ -87,6 +89,38 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n         map(lambda p: p.stop(), patchers)\n \n+class FilesPipelineTestCaseFields(unittest.TestCase):\n+\n+    def test_item_fields_default(self):\n+        from scrapy.contrib.pipeline.files import FilesPipeline\n+        class TestItem(Item):\n+            name = Field()\n+            file_urls = Field()\n+            files = Field()\n+        url = 'http://www.example.com/files/1.txt'\n+        item = TestItem({'name': 'item1', 'file_urls': [url]})\n+        pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/'}))\n+        requests = list(pipeline.get_media_requests(item, None))\n+        self.assertEqual(requests[0].url, url)\n+        results = [(True, {'url': url})]\n+        pipeline.item_completed(results, item, None)\n+        self.assertEqual(item['files'], [results[0][1]])\n+        \n+    def test_item_fields_override_settings(self):\n+        from scrapy.contrib.pipeline.files import FilesPipeline\n+        class TestItem(Item):\n+            name = Field()\n+            files = Field()\n+            stored_file = Field()\n+        url = 'http://www.example.com/files/1.txt'\n+        item = TestItem({'name': 'item1', 'files': [url]})\n+        pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/',\n+                'FILES_URLS_FIELD': 'files', 'FILES_RESULT_FIELD': 'stored_file'}))\n+        requests = list(pipeline.get_media_requests(item, None))\n+        self.assertEqual(requests[0].url, url)\n+        results = [(True, {'url': url})]\n+        pipeline.item_completed(results, item, None)\n+        self.assertEqual(item['stored_file'], [results[0][1]])\n  \n class ItemWithFiles(Item):\n     file_urls = Field()\n\n@@ -5,6 +5,8 @@ from shutil import rmtree\n \n from twisted.trial import unittest\n \n+from scrapy.item import Item, Field\n+from scrapy.settings import Settings\n \n skip = False\n try:\n@@ -82,6 +84,38 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEquals(converted.mode, 'RGB')\n         self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])\n \n+class ImagesPipelineTestCaseFields(unittest.TestCase):\n+\n+    def test_item_fields_default(self):\n+        from scrapy.contrib.pipeline.images import ImagesPipeline\n+        class TestItem(Item):\n+            name = Field()\n+            image_urls = Field()\n+            images = Field()\n+        url = 'http://www.example.com/images/1.jpg'\n+        item = TestItem({'name': 'item1', 'image_urls': [url]})\n+        pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/'}))\n+        requests = list(pipeline.get_media_requests(item, None))\n+        self.assertEqual(requests[0].url, url)\n+        results = [(True, {'url': url})]\n+        pipeline.item_completed(results, item, None)\n+        self.assertEqual(item['images'], [results[0][1]])\n+        \n+    def test_item_fields_override_settings(self):\n+        from scrapy.contrib.pipeline.images import ImagesPipeline\n+        class TestItem(Item):\n+            name = Field()\n+            image = Field()\n+            stored_image = Field()\n+        url = 'http://www.example.com/images/1.jpg'\n+        item = TestItem({'name': 'item1', 'image': [url]})\n+        pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/',\n+                'IMAGES_URLS_FIELD': 'image', 'IMAGES_RESULT_FIELD': 'stored_image'}))\n+        requests = list(pipeline.get_media_requests(item, None))\n+        self.assertEqual(requests[0].url, url)\n+        results = [(True, {'url': url})]\n+        pipeline.item_completed(results, item, None)\n+        self.assertEqual(item['stored_image'], [results[0][1]])\n  \n def _create_image(format, *a, **kw):\n     buf = StringIO()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
