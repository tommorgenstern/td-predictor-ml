{"custom_id": "keras#777ed83e4f85f0e24af7ee3d14fb1b1eb098b28d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 12 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 18155 | Contributors (this commit): 29 | Commits (past 90d): 66 | Contributors (cumulative): 71 | DMM Complexity: None\n\nDIFF:\n@@ -224,7 +224,7 @@ def MobileNetV3(stack_fn,\n         rows = backend.int_shape(input_tensor)[1]\n         cols = backend.int_shape(input_tensor)[2]\n         input_shape = (cols, rows, 3)\n-  # If input_shape is None and input_tensor is None using standart shape\n+  # If input_shape is None and input_tensor is None using standard shape\n   if input_shape is None and input_tensor is None:\n     input_shape = (None, None, 3)\n \n\n@@ -631,7 +631,7 @@ class TestDistributionStrategyWithNumpyArrays(tf.test.TestCase,\n           batch_size=2,\n           sample_weight=sample_weights,\n           verbose=1)\n-      # The per sample loss is multipled by the corresponding sample weight. The\n+      # The per sample loss is multiplied by the corresponding sample weight. The\n       # average of these weighted losses is the return value of the `evaluate`\n       # call. For example, in the test above the average weighted loss is\n       # calculated in the following manner:\n@@ -1545,7 +1545,7 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n       ds = tf.data.Dataset.from_tensor_slices(\n           (inputs, targets, sample_weights)).batch(2)\n       result = model.evaluate(ds, verbose=1)\n-      # The per sample loss is multipled by the corresponding sample weight. The\n+      # The per sample loss is multiplied by the corresponding sample weight. The\n       # average of these weighted losses is the return value of the `evaluate`\n       # call. For example, in the test above the average weighted loss is\n       # calculated in the following manner:\n\n@@ -1620,7 +1620,7 @@ def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n     # For single x-input, we do no tuple wrapping since in this case\n     # there is no ambiguity. This also makes NumPy and Dataset\n     # consistent in that the user does not have to wrap their Dataset\n-    # data in an unecessary tuple\n+    # data in an unnecessary tuple\n     if not tf.nest.is_nested(x):\n       return x\n     else:\n\n@@ -350,7 +350,7 @@ class Sequential(functional.Functional):\n     if not self._has_explicit_input_shape:\n       if not tf.is_tensor(inputs) and not isinstance(\n           inputs, tf.Tensor):\n-        # This is a Sequential with mutiple inputs. This is technically an\n+        # This is a Sequential with multiple inputs. This is technically an\n         # invalid use case of Sequential, but we tolerate it for backwards\n         # compatibility.\n         self._use_legacy_deferred_behavior = True\n\n@@ -237,7 +237,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     ds = tf.data.Dataset.from_tensor_slices(\n         (inputs, targets, sample_weights)).batch(2)\n     result = model.evaluate(ds, verbose=1)\n-    # The per sample loss is multipled by the corresponding sample weight. The\n+    # The per sample loss is multiplied by the corresponding sample weight. The\n     # average of these weighted losses is the return value of the `evaluate`\n     # call. For example, in the test above the average weighted loss is\n     # calculated in the following manner:\n\n@@ -1666,7 +1666,7 @@ def infer_steps_for_dataset(model,\n       (dataset.options().experimental_distribute.auto_shard_policy !=\n        tf.data.experimental.AutoShardPolicy.OFF)):\n     # If the dataset would be auto-sharded, we should not infer a local\n-    # steps_per_epoch due to the possible inbalanced sharding between workers.\n+    # steps_per_epoch due to the possible imbalanced sharding between workers.\n     return None\n \n   size = backend.get_value(tf.data.experimental.cardinality(dataset))\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Common utilities for our Keras preprocessing intergration tests.\"\"\"\n+\"\"\"Common utilities for our Keras preprocessing integration tests.\"\"\"\n \n import os\n \n\n@@ -32,7 +32,7 @@ class _Merge(Layer):\n   \"\"\"\n \n   def __init__(self, **kwargs):\n-    \"\"\"Intializes a Merge layer.\n+    \"\"\"Initializes a Merge layer.\n \n     Args:\n       **kwargs: standard layer keyword arguments.\n\n@@ -207,7 +207,7 @@ class MultiHeadAttention(Layer):\n       where `T` is for target sequence shapes and `E` is the query input last\n       dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n       are project to the shape specified by `output_shape`.\n-    attention_scores: [Optional] multi-head attention coeffients over\n+    attention_scores: [Optional] multi-head attention coefficients over\n       attention axes.\n   \"\"\"\n \n\n@@ -332,7 +332,7 @@ class StringLookup(index_lookup.IndexLookup):\n     base_config = super(StringLookup, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n-  # Overriden methods from IndexLookup.\n+  # Overridden methods from IndexLookup.\n   def _tensor_vocab_to_numpy(self, vocabulary):\n     vocabulary = vocabulary.numpy()\n     return np.array([tf.compat.as_text(x, self.encoding) for x in vocabulary])\n\n@@ -79,7 +79,7 @@ def _get_end_to_end_test_cases():\n       {\n           \"testcase_name\":\n               \"test_special_tokens_int_mode\",\n-          # Mask tokens in the vocab data should be ingored, and mapped to 0 in\n+          # Mask tokens in the vocab data should be ignored, and mapped to 0 in\n           # from the input data.\n           \"vocab_data\":\n               np.array([[\"fire\"], [\"earth\"], [\"earth\"], [\"earth\"], [\"earth\"],\n\n@@ -42,7 +42,7 @@ def index_directory(directory,\n         to the alphanumeric order of the image file paths\n         (obtained via `os.walk(directory)` in Python).\n     formats: Allowlist of file extensions to index (e.g. \".jpg\", \".txt\").\n-    class_names: Only valid if \"labels\" is \"inferred\". This is the explict\n+    class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n         list of class names (must match names of subdirectories). Used\n         to control the order of the classes\n         (otherwise alphanumerical order is used).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b8772891b6334f9df8ef50c7acdb0b9ffd4278f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 102 | Lines Deleted: 66 | Files Changed: 4 | Hunks: 31 | Methods Changed: 20 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 168 | Churn Cumulative: 10992 | Contributors (this commit): 18 | Commits (past 90d): 21 | Contributors (cumulative): 30 | DMM Complexity: 0.7560975609756098\n\nDIFF:\n@@ -330,6 +330,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # Mutable properties\n     # Indicates whether the layer's weights are updated during training\n     # and whether the layer's updates are run during training.\n+    if not (isinstance(trainable, bool) or\n+            (isinstance(trainable, (tf.Tensor, tf.Variable)) and\n+             trainable.dtype is tf.bool)):\n+      raise TypeError(\n+          f'Expected trainable argument to be a boolean, but got: {trainable}')\n     self._trainable = trainable\n     # A stateful layer is a layer whose updates are run during inference too,\n     # for instance stateful RNNs.\n\n@@ -466,6 +466,14 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(layer.weights[0]), 1.)\n     self.assertEqual(self.evaluate(layer.weights[1]), 2.)\n \n+  def test_exception_if_trainable_not_boolean(self):\n+    base_layer.Layer(trainable=True)\n+    base_layer.Layer(trainable=tf.constant(True))\n+    base_layer.Layer(trainable=tf.Variable(tf.constant(True)))\n+    with self.assertRaisesRegex(TypeError,\n+                                'Expected trainable argument to be a boolean'):\n+      base_layer.Layer(trainable=0)\n+\n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_layer_names(self):\n     inputs = input_layer.Input(shape=[2])\n@@ -916,7 +924,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self, **kwargs):\n-        super(MyLayer, self).__init__(self, **kwargs)\n+        super(MyLayer, self).__init__(**kwargs)\n         self.my_modules = {}\n         self.my_modules['a'] = MyModule()\n \n\n@@ -48,9 +48,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=['eager']))\n   def test_reuses_variables(self):\n     sparse_input = tf.SparseTensor(\n-        indices=((0, 0), (1, 0), (2, 0)),\n-        values=(0, 1, 2),\n-        dense_shape=(3, 3))\n+        indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))\n \n     # Create feature columns (categorical and embedding).\n     categorical_column = tf.feature_column.categorical_column_with_identity(\n@@ -122,8 +120,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n         initializer=_embedding_column_initializer)\n \n     dense_features = df.DenseFeatures(\n-        [embedding_column],\n-        partitioner=tf.compat.v1.fixed_size_partitioner(2))\n+        [embedding_column], partitioner=tf.compat.v1.fixed_size_partitioner(2))\n     features = {'a': sparse_input}\n \n     inputs = dense_features(features)\n@@ -145,9 +142,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=['eager']))\n   def test_feature_column_dense_features_gradient(self):\n     sparse_input = tf.SparseTensor(\n-        indices=((0, 0), (1, 0), (2, 0)),\n-        values=(0, 1, 2),\n-        dense_shape=(3, 3))\n+        indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))\n \n     # Create feature columns (categorical and embedding).\n     categorical_column = tf.feature_column.categorical_column_with_identity(\n@@ -205,7 +200,8 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n   def test_does_not_support_dict_columns(self):\n     with self.assertRaisesRegex(\n         ValueError, 'Expected feature_columns to be iterable, found dict.'):\n-      df.DenseFeatures(feature_columns={'a': tf.feature_column.numeric_column('a')})(\n+      df.DenseFeatures(\n+          feature_columns={'a': tf.feature_column.numeric_column('a')})(\n               features={\n                   'a': [[0]]\n               })\n@@ -234,9 +230,10 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n   def test_raises_if_duplicate_name(self):\n     with self.assertRaisesRegex(\n         ValueError, 'Duplicate feature column name found for columns'):\n-      df.DenseFeatures(\n-          feature_columns=[tf.feature_column.numeric_column('a'),\n-                           tf.feature_column.numeric_column('a')])(\n+      df.DenseFeatures(feature_columns=[\n+          tf.feature_column.numeric_column('a'),\n+          tf.feature_column.numeric_column('a')\n+      ])(\n           features={\n               'a': [[0]]\n           })\n@@ -348,7 +345,8 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n       self.assertAllClose([[1., 3.]], self.evaluate(net2))\n \n   def test_fails_for_categorical_column(self):\n-    animal = tf.feature_column.categorical_column_with_identity('animal', num_buckets=4)\n+    animal = tf.feature_column.categorical_column_with_identity(\n+        'animal', num_buckets=4)\n     with tf.Graph().as_default():\n       features = {\n           'animal':\n@@ -431,15 +429,19 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n       df.DenseFeatures(all_cols)(features)\n       df.DenseFeatures(all_cols)(features)\n       # Make sure that 2 variables get created in this case.\n-      self.assertEqual(2,\n-                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n+      self.assertEqual(\n+          2,\n+          len(\n+              tf.compat.v1.get_collection(\n+                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n       expected_var_names = [\n           'dense_features/sparse_feature_embedding/embedding_weights:0',\n           'dense_features_1/sparse_feature_embedding/embedding_weights:0'\n       ]\n-      self.assertItemsEqual(\n-          expected_var_names,\n-          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])\n+      self.assertCountEqual(expected_var_names, [\n+          v.name for v in tf.compat.v1.get_collection(\n+              tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n+      ])\n \n   @test_util.run_deprecated_v1\n   def test_multiple_layers_with_same_shared_embedding_column(self):\n@@ -469,11 +471,15 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n       df.DenseFeatures(all_cols)(features)\n       df.DenseFeatures(all_cols)(features)\n       # Make sure that only 1 variable gets created in this case.\n-      self.assertEqual(1,\n-                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n-      self.assertItemsEqual(\n-          ['aaa_bbb_shared_embedding:0'],\n-          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])\n+      self.assertEqual(\n+          1,\n+          len(\n+              tf.compat.v1.get_collection(\n+                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n+      self.assertCountEqual(['aaa_bbb_shared_embedding:0'], [\n+          v.name for v in tf.compat.v1.get_collection(\n+              tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n+      ])\n \n   @test_util.run_deprecated_v1\n   def test_multiple_layers_with_same_shared_embedding_column_diff_graphs(self):\n@@ -502,8 +508,11 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n       }\n       df.DenseFeatures(all_cols)(features)\n       # Make sure that only 1 variable gets created in this case.\n-      self.assertEqual(1,\n-                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n+      self.assertEqual(\n+          1,\n+          len(\n+              tf.compat.v1.get_collection(\n+                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n \n     with tf.Graph().as_default():\n       features1 = {\n@@ -521,11 +530,15 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n \n       df.DenseFeatures(all_cols)(features1)\n       # Make sure that only 1 variable gets created in this case.\n-      self.assertEqual(1,\n-                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n-      self.assertItemsEqual(\n-          ['aaa_bbb_shared_embedding:0'],\n-          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])\n+      self.assertEqual(\n+          1,\n+          len(\n+              tf.compat.v1.get_collection(\n+                  tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))\n+      self.assertCountEqual(['aaa_bbb_shared_embedding:0'], [\n+          v.name for v in tf.compat.v1.get_collection(\n+              tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n+      ])\n \n   @test_util.run_deprecated_v1\n   def test_with_1d_sparse_tensor(self):\n@@ -672,7 +685,8 @@ class IndicatorColumnTest(tf.test.TestCase):\n   @test_util.run_deprecated_v1\n   def test_dense_features(self):\n     animal = tf.feature_column.indicator_column(\n-        tf.feature_column.categorical_column_with_identity('animal', num_buckets=4))\n+        tf.feature_column.categorical_column_with_identity(\n+            'animal', num_buckets=4))\n     with tf.Graph().as_default():\n       features = {\n           'animal':\n@@ -771,7 +785,8 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n       dense_features = l({'aaa': sparse_input})\n \n     # Assert expected embedding variable and lookups.\n-    global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n+    global_vars = tf.compat.v1.get_collection(\n+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n     if partition_variables:\n       self.assertCountEqual(\n           ('vars/dense_features/aaa_embedding/embedding_weights/part_0:0',\n@@ -783,7 +798,8 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n           tuple([v.name for v in global_vars]))\n     for v in global_vars:\n       self.assertIsInstance(v, tf.Variable)\n-    trainable_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)\n+    trainable_vars = tf.compat.v1.get_collection(\n+        tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)\n     if partition_variables:\n       self.assertCountEqual(\n           ('vars/dense_features/aaa_embedding/embedding_weights/part_0:0',\n@@ -801,7 +817,8 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(expected_lookups, self.evaluate(dense_features))\n \n     if use_safe_embedding_lookup:\n-      self.assertIn('SparseFillEmptyRows',\n+      self.assertIn(\n+          'SparseFillEmptyRows',\n           [x.type for x in tf.compat.v1.get_default_graph().get_operations()])\n     else:\n       self.assertNotIn(\n@@ -862,11 +879,13 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n     })\n \n     # Assert expected embedding variable and lookups.\n-    global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n+    global_vars = tf.compat.v1.get_collection(\n+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n     self.assertCountEqual(('dense_features/aaa_embedding/embedding_weights:0',),\n                           tuple([v.name for v in global_vars]))\n     self.assertCountEqual([],\n-                          tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES))\n+                          tf.compat.v1.get_collection(\n+                              tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES))\n \n     self.evaluate(tf.compat.v1.global_variables_initializer())\n     self.evaluate(tf.compat.v1.tables_initializer())\n@@ -970,13 +989,15 @@ class SharedEmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n                              features)\n \n     # Assert expected embedding variable and lookups.\n-    global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n+    global_vars = tf.compat.v1.get_collection(\n+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n     self.assertCountEqual(\n         ['aaa_bbb_shared_embedding:0', 'ccc_ddd_shared_embedding:0'],\n         tuple([v.name for v in global_vars]))\n     for v in global_vars:\n       self.assertIsInstance(v, tf.Variable)\n-    trainable_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)\n+    trainable_vars = tf.compat.v1.get_collection(\n+        tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)\n     if trainable:\n       self.assertCountEqual(\n           ['aaa_bbb_shared_embedding:0', 'ccc_ddd_shared_embedding:0'],\n@@ -1004,39 +1025,42 @@ class SharedEmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class DenseFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @parameterized.named_parameters(\n-      ('default', None, None),\n-      ('trainable', True, 'trainable'),\n+  @parameterized.named_parameters(('trainable', True, 'trainable'),\n                                   ('not_trainable', False, 'frozen'))\n   def test_get_config(self, trainable, name):\n-    cols = [tf.feature_column.numeric_column('a'),\n-            tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity(\n-                key='b', num_buckets=3), dimension=2)]\n-    orig_layer = df.DenseFeatures(\n-        cols, trainable=trainable, name=name)\n+    cols = [\n+        tf.feature_column.numeric_column('a'),\n+        tf.feature_column.embedding_column(\n+            tf.feature_column.categorical_column_with_identity(\n+                key='b', num_buckets=3),\n+            dimension=2)\n+    ]\n+    orig_layer = df.DenseFeatures(cols, trainable=trainable, name=name)\n     config = orig_layer.get_config()\n \n     self.assertEqual(config['name'], orig_layer.name)\n     self.assertEqual(config['trainable'], trainable)\n     self.assertLen(config['feature_columns'], 2)\n-    self.assertEqual(\n-        config['feature_columns'][0]['class_name'], 'NumericColumn')\n+    self.assertEqual(config['feature_columns'][0]['class_name'],\n+                     'NumericColumn')\n     self.assertEqual(config['feature_columns'][0]['config']['shape'], (1,))\n-    self.assertEqual(\n-        config['feature_columns'][1]['class_name'], 'EmbeddingColumn')\n+    self.assertEqual(config['feature_columns'][1]['class_name'],\n+                     'EmbeddingColumn')\n \n-  @parameterized.named_parameters(\n-      ('default', None, None),\n-      ('trainable', True, 'trainable'),\n+  @parameterized.named_parameters(('trainable', True, 'trainable'),\n                                   ('not_trainable', False, 'frozen'))\n   def test_from_config(self, trainable, name):\n-    cols = [tf.feature_column.numeric_column('a'),\n-            tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_vocabulary_list(\n-                'b', vocabulary_list=['1', '2', '3']), dimension=2),\n-            tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_hash_bucket(\n-                key='c', hash_bucket_size=3))]\n-    orig_layer = df.DenseFeatures(\n-        cols, trainable=trainable, name=name)\n+    cols = [\n+        tf.feature_column.numeric_column('a'),\n+        tf.feature_column.embedding_column(\n+            tf.feature_column.categorical_column_with_vocabulary_list(\n+                'b', vocabulary_list=['1', '2', '3']),\n+            dimension=2),\n+        tf.feature_column.indicator_column(\n+            tf.feature_column.categorical_column_with_hash_bucket(\n+                key='c', hash_bucket_size=3))\n+    ]\n+    orig_layer = df.DenseFeatures(cols, trainable=trainable, name=name)\n     config = orig_layer.get_config()\n \n     new_layer = df.DenseFeatures.from_config(config)\n@@ -1106,7 +1130,8 @@ class SequenceFeatureColumnsTest(tf.test.TestCase):\n \n     categorical_column_a = tf.feature_column.sequence_categorical_column_with_identity(\n         key='aaa', num_buckets=vocabulary_size)\n-    indicator_column_a = tf.feature_column.indicator_column(categorical_column_a)\n+    indicator_column_a = tf.feature_column.indicator_column(\n+        categorical_column_a)\n \n     input_layer = df.DenseFeatures([indicator_column_a])\n     with self.assertRaisesRegex(\n\n@@ -563,8 +563,7 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class SequenceFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @parameterized.named_parameters(('default', None, None),\n-                                  ('trainable', True, 'trainable'),\n+  @parameterized.named_parameters(('trainable', True, 'trainable'),\n                                   ('not_trainable', False, 'frozen'))\n   def test_get_config(self, trainable, name):\n     cols = [tf.feature_column.sequence_numeric_column('a')]\n@@ -578,8 +577,7 @@ class SequenceFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase\n                      'SequenceNumericColumn')\n     self.assertEqual(config['feature_columns'][0]['config']['shape'], (1,))\n \n-  @parameterized.named_parameters(('default', None, None),\n-                                  ('trainable', True, 'trainable'),\n+  @parameterized.named_parameters(('trainable', True, 'trainable'),\n                                   ('not_trainable', False, 'frozen'))\n   def test_from_config(self, trainable, name):\n     cols = [tf.feature_column.sequence_numeric_column('a')]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d807a464a8cda688ddde5459a50ae2c945061a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 129 | Lines Deleted: 43 | Files Changed: 4 | Hunks: 29 | Methods Changed: 19 | Complexity Δ (Sum/Max): 16/7 | Churn Δ: 172 | Churn Cumulative: 21543 | Contributors (this commit): 106 | Commits (past 90d): 41 | Contributors (cumulative): 118 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -2243,7 +2243,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         return\n     if save_format == 'h5':\n       with h5py.File(filepath, 'w') as f:\n-        hdf5_format.save_weights_to_hdf5_group(f, self.layers)\n+        hdf5_format.save_weights_to_hdf5_group(f, self)\n     else:\n       if tf.executing_eagerly():\n         session = None\n@@ -2349,10 +2349,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if 'layer_names' not in f.attrs and 'model_weights' in f:\n           f = f['model_weights']\n         if by_name:\n-          hdf5_format.load_weights_from_hdf5_group_by_name(\n-              f, self.layers, skip_mismatch=skip_mismatch)\n+          hdf5_format.load_weights_from_hdf5_group_by_name(f, self, skip_mismatch)\n         else:\n-          hdf5_format.load_weights_from_hdf5_group(f, self.layers)\n+          hdf5_format.load_weights_from_hdf5_group(f, self)\n       \n     # Perform any layer defined finalization of the layer state.\n     for layer in self.layers:\n\n@@ -18,6 +18,7 @@ import tensorflow.compat.v2 as tf\n \n import collections\n import io\n+import tempfile\n import sys\n \n from absl.testing import parameterized\n@@ -3866,6 +3867,27 @@ class TestBuildCustomModel(keras_parameterized.TestCase):\n     model.build({'x': [None, 16]})\n     self.assertEqual(model.l1.kernel.shape.as_list(), [16, 1])\n \n+  @keras_parameterized.run_all_keras_modes\n+  def test_save_model_weights_h5(self):\n+\n+    class MyModel(training_module.Model):\n+\n+      def __init__(self):\n+        super(MyModel, self).__init__()\n+        self.class_token = self.add_weight(shape=[2, 3])\n+\n+      def call(self, inputs):\n+        pass\n+\n+    h5_file = tempfile.mktemp('.h5')\n+    m1 = MyModel()\n+    m1.build([None])\n+    m1.save_weights(h5_file)\n+\n+    m2 = MyModel()\n+    m2.build([None])\n+    m2.load_weights(h5_file)\n+    self.assertAllEqual(m1.get_weights(), m2.get_weights())\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -116,8 +116,7 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n         f.attrs[k] = v\n \n     model_weights_group = f.create_group('model_weights')\n-    model_layers = model.layers\n-    save_weights_to_hdf5_group(model_weights_group, model_layers)\n+    save_weights_to_hdf5_group(model_weights_group, model)\n \n     # TODO(b/128683857): Add integration tests between tf.keras and external\n     # Keras, to avoid breaking TF.js users.\n@@ -181,7 +180,7 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):  # pylint\n                                                custom_objects=custom_objects)\n \n     # set weights\n-    load_weights_from_hdf5_group(f['model_weights'], model.layers)\n+    load_weights_from_hdf5_group(f['model_weights'], model)\n \n     if compile:\n       # instantiate optimizer\n@@ -312,7 +311,8 @@ def preprocess_weights_for_loading(layer,\n         trainable_weights = trainable_weights[num_trainable_weights:]\n         non_trainable_weights = non_trainable_weights[\n             num_non_trainable_weights:]\n-\n+    new_trainable_weights += layer._trainable_weights\n+    new_non_trainable_weights += layer._non_trainable_weights\n     return new_trainable_weights + new_non_trainable_weights\n \n   # Convert layers nested in Bidirectional/Model/Sequential.\n@@ -615,30 +615,18 @@ def load_optimizer_weights_from_hdf5_group(hdf5_group):\n   return [weights_group[weight_name] for weight_name in optimizer_weight_names]\n \n \n-def save_weights_to_hdf5_group(f, layers):\n-  \"\"\"Saves the weights of a list of layers to a HDF5 group.\n+def save_subset_weights_to_hdf5_group(f, weights):\n+  \"\"\"Save top-level weights of a model to a HDF5 group.\n \n   Args:\n       f: HDF5 group.\n-      layers: List of layer instances.\n+      model: model instance.\n   \"\"\"\n-  from keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n-\n-  save_attributes_to_hdf5_group(\n-      f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n-  f.attrs['backend'] = backend.backend().encode('utf8')\n-  f.attrs['keras_version'] = str(keras_version).encode('utf8')\n-\n-  # Sort model layers by layer name to ensure that group names are strictly\n-  # growing to avoid prefix issues.\n-  for layer in sorted(layers, key=lambda x: x.name):\n-    g = f.create_group(layer.name)\n-    weights = _legacy_weights(layer)\n   weight_values = backend.batch_get_value(weights)\n   weight_names = [w.name.encode('utf8') for w in weights]\n-    save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n+  save_attributes_to_hdf5_group(f, 'weight_names', weight_names)\n   for name, val in zip(weight_names, weight_values):\n-      param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\n+    param_dset = f.create_dataset(name, val.shape, dtype=val.dtype)\n     if not val.shape:\n       # scalar\n       param_dset[()] = val\n@@ -646,7 +634,45 @@ def save_weights_to_hdf5_group(f, layers):\n       param_dset[:] = val\n \n \n-def load_weights_from_hdf5_group(f, layers):\n+def save_weights_to_hdf5_group(f, model):\n+  \"\"\"Saves the weights of a list of layers to a HDF5 group.\n+\n+  Args:\n+      f: HDF5 group.\n+      layers: List of layer instances.\n+  \"\"\"\n+  from keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n+  save_attributes_to_hdf5_group(\n+      f, 'layer_names', [layer.name.encode('utf8') for layer in model.layers])\n+  f.attrs['backend'] = backend.backend().encode('utf8')\n+  f.attrs['keras_version'] = str(keras_version).encode('utf8')\n+\n+  # Sort model layers by layer name to ensure that group names are strictly\n+  # growing to avoid prefix issues.\n+  for layer in sorted(model.layers, key=lambda x: x.name):\n+    g = f.create_group(layer.name)\n+    weights = _legacy_weights(layer)\n+    save_subset_weights_to_hdf5_group(g, weights)\n+  weights = model._trainable_weights + model._non_trainable_weights\n+  g = f.create_group('top_level_model_weights')\n+  save_subset_weights_to_hdf5_group(g, weights)\n+\n+\n+def load_subset_weights_from_hdf5_group(f):\n+  \"\"\"Load layer weights of a model from hdf5.\n+\n+  Args:\n+      f: A pointer to a HDF5 group.\n+\n+  Raises:\n+      ValueError: in case of mismatch between provided model\n+          and weights file.\n+  \"\"\"\n+  weight_names = load_attributes_from_hdf5_group(f, 'weight_names')\n+  return [np.asarray(f[weight_name]) for weight_name in weight_names]\n+\n+\n+def load_weights_from_hdf5_group(f, model):\n   \"\"\"Implements topological (order-based) weight loading.\n \n   Args:\n@@ -671,7 +697,7 @@ def load_weights_from_hdf5_group(f, layers):\n     original_backend = None\n \n   filtered_layers = []\n-  for layer in layers:\n+  for layer in model.layers:\n     weights = _legacy_weights(layer)\n     if weights:\n       filtered_layers.append(layer)\n@@ -695,12 +721,12 @@ def load_weights_from_hdf5_group(f, layers):\n   weight_value_tuples = []\n   for k, name in enumerate(layer_names):\n     g = f[name]\n-    weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n-    weight_values = [np.asarray(g[weight_name]) for weight_name in weight_names]\n     layer = filtered_layers[k]\n     symbolic_weights = _legacy_weights(layer)\n-    weight_values = preprocess_weights_for_loading(\n-        layer, weight_values, original_keras_version, original_backend)\n+    weight_values = load_subset_weights_from_hdf5_group(g)\n+    weight_values = preprocess_weights_for_loading(layer, weight_values,\n+                                                   original_keras_version,\n+                                                   original_backend)\n     if len(weight_values) != len(symbolic_weights):\n       raise ValueError('Layer #' + str(k) + ' (named \"' + layer.name +\n                        '\" in the current model) was found to '\n@@ -710,11 +736,19 @@ def load_weights_from_hdf5_group(f, layers):\n                        ' weights, but the saved weights have ' +\n                        str(len(weight_values)) + ' elements.')\n     weight_value_tuples += zip(symbolic_weights, weight_values)\n+  symbolic_weights = model._trainable_weights + model._non_trainable_weights\n+  weight_values = load_subset_weights_from_hdf5_group(f['top_level_model_weights'])\n+  if len(weight_values) != len(symbolic_weights):\n+    raise ValueError('The model ' + model.name + ' expects ' +\n+                      str(len(symbolic_weights)) +\n+                      ' weights, but the saved weights have ' +\n+                      str(len(weight_values)) + ' elements.')\n+  weight_value_tuples += zip(symbolic_weights, weight_values)\n   backend.batch_set_value(weight_value_tuples)\n \n \n def load_weights_from_hdf5_group_by_name(\n-    f, layers, skip_mismatch=False):\n+    f, model, skip_mismatch=False):\n   \"\"\"Implements name-based weight loading.\n \n   (instead of topological weight loading).\n@@ -723,7 +757,7 @@ def load_weights_from_hdf5_group_by_name(\n \n   Args:\n       f: A pointer to a HDF5 group.\n-      layers: a list of target layers.\n+      model: a model instance.\n       skip_mismatch: Boolean, whether to skip loading of layers\n           where there is a mismatch in the number of weights,\n           or a mismatch in the shape of the weights.\n@@ -750,7 +784,7 @@ def load_weights_from_hdf5_group_by_name(\n \n   # Reverse index of layer name to list of layers with name.\n   index = {}\n-  for layer in layers:\n+  for layer in model.layers:\n     if layer.name:\n       index.setdefault(layer.name, []).append(layer)\n \n@@ -759,9 +793,7 @@ def load_weights_from_hdf5_group_by_name(\n   weight_value_tuples = []\n   for k, name in enumerate(layer_names):\n     g = f[name]\n-    weight_names = load_attributes_from_hdf5_group(g, 'weight_names')\n-    weight_values = [np.asarray(g[weight_name]) for weight_name in weight_names]\n-\n+    weight_values = load_subset_weights_from_hdf5_group(g)\n     for layer in index.get(name, []):\n       symbolic_weights = _legacy_weights(layer)\n       weight_values = preprocess_weights_for_loading(\n@@ -796,6 +828,39 @@ def load_weights_from_hdf5_group_by_name(\n \n         else:\n           weight_value_tuples.append((symbolic_weights[i], weight_values[i]))\n+  symbolic_weights = model._trainable_weights + model._non_trainable_weights\n+  weight_values = load_subset_weights_from_hdf5_group(f['top_level_model_weights'])\n+  if len(weight_values) != len(symbolic_weights):\n+    if skip_mismatch:\n+      logging.warning('Skipping loading of weights for '\n+                      'model {}'.format(model.name) + ' due to mismatch '\n+                      'in number of weights ({} vs {}).'.format(\n+                          len(symbolic_weights), len(weight_values)))\n+      backend.batch_set_value(weight_value_tuples)\n+      return\n+    raise ValueError(' (named \"' + model.name +\n+                      '\") expects ' + str(len(symbolic_weights)) +\n+                      ' weight(s), but the saved weights' + ' have ' +\n+                      str(len(weight_values)) + ' element(s).')\n+  # Set values.\n+  for i in range(len(weight_values)):\n+    if backend.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n+      if skip_mismatch:\n+        logging.warning('Skipping loading of weights for '\n+                        'model {}'.format(model.name) + ' due to '\n+                        'mismatch in shape ({} vs {}).'.format(\n+                            symbolic_weights[i].shape,\n+                            weight_values[i].shape))\n+        continue\n+      raise ValueError('(named \"' + model.name +\n+                        '\"), weight ' + str(symbolic_weights[i]) +\n+                        ' has shape {}'.format(backend.int_shape(\n+                            symbolic_weights[i])) +\n+                        ', but the saved weight has shape ' +\n+                        str(weight_values[i].shape) + '.')\n+    else:\n+      weight_value_tuples.append((symbolic_weights[i], weight_values[i]))\n+\n   backend.batch_set_value(weight_value_tuples)\n \n \n\n@@ -284,7 +284,7 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n                         metrics=[keras.metrics.categorical_accuracy])\n \n       f_ref_model = h5py.File(h5_path, 'w')\n-      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model.layers)\n+      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model)\n \n       f_model = h5py.File(h5_path, 'r')\n       model = keras.models.Sequential()\n@@ -298,10 +298,10 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n           ValueError, r'Layer #0 \\(named \\\"d1\\\"\\) expects 1 '\n           r'weight\\(s\\), but the saved weights have 2 '\n           r'element\\(s\\)\\.'):\n-        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model.layers)\n+        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n \n       hdf5_format.load_weights_from_hdf5_group_by_name(\n-          f_model, model.layers, skip_mismatch=True)\n+          f_model, model, skip_mismatch=True)\n       self.assertAllClose(keras.backend.get_value(ref_model.layers[1].kernel),\n                           keras.backend.get_value(model.layers[1].kernel))\n \n@@ -325,7 +325,7 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n \n       f_ref_model = h5py.File(h5_path, 'w')\n       keras.backend.set_value(ref_model.layers[1].bias, [3.5] * num_classes)\n-      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model.layers)\n+      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model)\n \n       f_model = h5py.File(h5_path, 'r')\n       model = keras.models.Sequential()\n@@ -341,10 +341,10 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n           r'shape=\\(3, 10\\) dtype=float32> has '\n           r'shape \\(3, 10\\), but the saved weight has '\n           r'shape \\(3, 5\\)\\.'):\n-        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model.layers)\n+        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n \n       hdf5_format.load_weights_from_hdf5_group_by_name(\n-          f_model, model.layers, skip_mismatch=True)\n+          f_model, model, skip_mismatch=True)\n       self.assertAllClose([3.5] * num_classes,\n                           keras.backend.get_value(model.layers[1].bias))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3050b57d28b86cac507b4a85a8c53931744a5eaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 432 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -189,4 +189,4 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n \n \n if __name__ == \"__main__\":\n-  tf.test.main()\n+  tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -43,6 +43,8 @@ def strategy_combinations_eager_data_fn():\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n           tf.__internal__.distribute.combinations\n+          .parameter_server_strategy_1worker_2ps_cpu,\n+          tf.__internal__.distribute.combinations\n           .parameter_server_strategy_1worker_2ps_1gpu,\n           # NOTE: TPUStrategy not tested because the models in this test are\n           # sparse and do not work with TPUs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0dc8e73a6224f7e65df19252c7d6d5e9921720ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 135 | Files Changed: 56 | Hunks: 135 | Methods Changed: 61 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 270 | Churn Cumulative: 76795 | Contributors (this commit): 182 | Commits (past 90d): 163 | Contributors (cumulative): 419 | DMM Complexity: None\n\nDIFF:\n@@ -2143,7 +2143,7 @@ class BackendGraphTests(tf.test.TestCase, parameterized.TestCase):\n \n   def test_function_fetch_callbacks(self):\n \n-    class CallbackStub(object):\n+    class CallbackStub:\n \n       def __init__(self):\n         self.times_called = 0\n\n@@ -176,7 +176,7 @@ def get_strategy_scope(strategy):\n   return strategy_scope\n \n \n-class DummyContextManager(object):\n+class DummyContextManager:\n \n   def __enter__(self):\n     pass\n\n@@ -1215,7 +1215,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n   def test_EarlyStopping_final_weights_when_restoring_model_weights(self):\n \n-    class DummyModel(object):\n+    class DummyModel:\n \n       def __init__(self):\n         self.stop_training = False\n@@ -1415,12 +1415,12 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n   def test_ReduceLROnPlateau_patience(self):\n \n-    class DummyOptimizer(object):\n+    class DummyOptimizer:\n \n       def __init__(self):\n         self.lr = keras.backend.variable(1.0)\n \n-    class DummyModel(object):\n+    class DummyModel:\n \n       def __init__(self):\n         self.optimizer = DummyOptimizer()\n@@ -2015,7 +2015,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n _ObservedSummary = collections.namedtuple('_ObservedSummary', ('logdir', 'tag'))\n \n \n-class _SummaryFile(object):\n+class _SummaryFile:\n   \"\"\"A record of summary tags and the files to which they were written.\n \n   Fields `scalars`, `images`, `histograms`, and `tensors` are sets\n\n@@ -226,7 +226,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n \n   def test_Tensorboard_histogram_summaries_in_test_function(self):\n \n-    class FileWriterStub(object):\n+    class FileWriterStub:\n \n       def __init__(self, logdir, graph=None):\n         self.logdir = logdir\n@@ -387,7 +387,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n \n   def test_Tensorboard_batch_logging(self):\n \n-    class FileWriterStub(object):\n+    class FileWriterStub:\n \n       def __init__(self, logdir, graph=None):\n         self.logdir = logdir\n@@ -422,7 +422,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n \n   def test_Tensorboard_epoch_and_batch_logging(self):\n \n-    class FileWriterStub(object):\n+    class FileWriterStub:\n \n       def __init__(self, logdir, graph=None):\n         self.logdir = logdir\n@@ -496,7 +496,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n \n   def test_TensorBoard_update_freq(self):\n \n-    class FileWriterStub(object):\n+    class FileWriterStub:\n \n       def __init__(self, logdir, graph=None):\n         self.logdir = logdir\n\n@@ -30,7 +30,7 @@ _NUM_EPOCHS = 2\n _STEPS_PER_EPOCH = 2\n \n \n-class MaybeStrategyScope(object):\n+class MaybeStrategyScope:\n   \"\"\"Provides a context allowing no distribution strategy.\"\"\"\n \n   def __init__(self, strategy):\n\n@@ -47,7 +47,7 @@ def get_current_worker_context():\n     return None\n \n \n-class _TaskType(object):\n+class _TaskType:\n   PS = \"ps\"\n   WORKER = \"worker\"\n   CHIEF = \"chief\"\n@@ -63,7 +63,7 @@ def _get_num_workers(cluster_spec):\n       cluster_spec.as_dict().get(_TaskType.CHIEF, []))\n \n \n-class _WorkerContext(object):\n+class _WorkerContext:\n   \"\"\"The worker context class.\n \n   This context object provides configuration information for each task. One\n@@ -377,7 +377,7 @@ def _run_std_server(cluster_spec=None,\n   if rpc_layer:\n     target = rpc_layer + \"://\" + target\n \n-  class _FakeServer(object):\n+  class _FakeServer:\n     \"\"\"A fake server that runs a master session.\"\"\"\n \n     def start(self):\n\n@@ -23,17 +23,17 @@ from keras.distribute import distributed_file_utils\n \n class DistributedFileUtilsTest(tf.test.TestCase):\n \n-  class MockedExtended(object):\n+  class MockedExtended:\n     pass\n \n-  class MockedChiefStrategy(object):\n+  class MockedChiefStrategy:\n \n     def __init__(self):\n       self.extended = DistributedFileUtilsTest.MockedExtended()\n       self.extended._in_multi_worker_mode = lambda: True\n       self.extended.should_checkpoint = True\n \n-  class MockedWorkerStrategy(object):\n+  class MockedWorkerStrategy:\n \n     def __init__(self):\n       self.extended = DistributedFileUtilsTest.MockedExtended()\n@@ -41,7 +41,7 @@ class DistributedFileUtilsTest(tf.test.TestCase):\n       self.extended.should_checkpoint = False\n       self.extended._task_id = 3\n \n-  class MockedSingleWorkerStrategy(object):\n+  class MockedSingleWorkerStrategy:\n \n     def __init__(self):\n       self.extended = DistributedFileUtilsTest.MockedExtended()\n\n@@ -114,7 +114,7 @@ def multi_worker_mirrored_eager_and_graph():\n       eager_mode_test_configuration() + graph_mode_test_configuration())\n \n \n-class MaybeDistributionScope(object):\n+class MaybeDistributionScope:\n   \"\"\"Provides a context allowing no distribution strategy.\"\"\"\n \n   def __init__(self, distribution):\n\n@@ -44,7 +44,7 @@ def list_checkpoint_attributes(ckpt_dir_or_file):\n \n \n @keras_export('keras.experimental.SidecarEvaluator', v1=[])\n-class SidecarEvaluator(object):\n+class SidecarEvaluator:\n   \"\"\"A class designed for a dedicated evaluator task.\n \n   `SidecarEvaluator` is expected to be run in a process on a separate machine\n\n@@ -28,7 +28,7 @@ CKPT_SAVED_EPOCH = '_ckpt_saved_epoch'\n CKPT_SAVED_EPOCH_UNUSED_VALUE = -1\n \n \n-class WorkerTrainingState(object):\n+class WorkerTrainingState:\n   \"\"\"Training state management class.\n \n   This class provides apis for backing up and restoring the training state.\n\n@@ -423,7 +423,7 @@ def call_context():\n tf.__internal__.register_call_context_function(call_context)\n \n \n-class CallContext(object):\n+class CallContext:\n   \"\"\"Keeps track of properties currently inside a Layer/Model's `call`.\n \n   Attributes:\n@@ -512,7 +512,7 @@ class CallContext(object):\n             getattr(backend.get_graph(), 'name', None) == 'keras_graph')\n \n \n-class CallContextManager(object):\n+class CallContextManager:\n   \"\"\"Context manager for `CallContext`.\"\"\"\n \n   def __init__(self, call_ctx, state):\n@@ -769,7 +769,7 @@ def v2_dtype_behavior_enabled():\n   return V2_DTYPE_BEHAVIOR\n \n \n-class TrackableWeightHandler(object):\n+class TrackableWeightHandler:\n   \"\"\"Keras wrapper for handling tracking.Trackable object saving and restoring.\n \n   This class handles Trackables in both V1 and V2 modes, ensuring that they can\n\n@@ -24,7 +24,7 @@ from keras.utils import losses_utils\n from keras.utils import tf_utils\n \n \n-class Container(object):\n+class Container:\n   \"\"\"Base Container class.\"\"\"\n \n   def __init__(self, output_names=None):\n\n@@ -346,7 +346,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     def custom_loss_fn(y_true, y_pred):\n       return tf.reduce_sum(y_true - y_pred)\n \n-    class CustomLossClass(object):\n+    class CustomLossClass:\n \n       def __call__(self, y_true, y_pred):\n         return tf.reduce_sum(y_true - y_pred)\n@@ -771,7 +771,7 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     def custom_metric_fn(y_true, y_pred):\n       return tf.reduce_sum(y_true - y_pred)\n \n-    class CustomMetricClass(object):\n+    class CustomMetricClass:\n \n       def __call__(self, y_true, y_pred):\n         return tf.reduce_sum(y_true - y_pred)\n\n@@ -1086,7 +1086,7 @@ def broadcast_sample_weight_modes(target_structure, sample_weight_modes):\n   return sample_weight_modes\n \n \n-class DataHandler(object):\n+class DataHandler:\n   \"\"\"Handles iterating over epoch-level `tf.data.Iterator` objects.\"\"\"\n \n   def __init__(self,\n\n@@ -28,7 +28,7 @@ from keras.engine import data_adapter\n from keras.utils import data_utils\n \n \n-class DummyArrayLike(object):\n+class DummyArrayLike:\n   \"\"\"Dummy array-like object.\"\"\"\n \n   def __init__(self, data):\n\n@@ -1083,7 +1083,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       def call(self, x1, kwarg=None):\n         return x1 + x1\n \n-    class NonSerializable(object):\n+    class NonSerializable:\n \n       def __init__(self, foo=None):\n         self.foo = foo\n@@ -2443,7 +2443,7 @@ class FunctionalSubclassModel(training_lib.Model):\n     super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)\n \n \n-class MixinClass(object):\n+class MixinClass:\n \n   def __init__(self, foo, **kwargs):\n     self._foo = foo\n\n@@ -26,7 +26,7 @@ from tensorflow.python.util.tf_export import tf_export\n               v1=['keras.layers.InputSpec',\n                   'keras.__internal__.legacy.layers.InputSpec'])\n @tf_export(v1=['layers.InputSpec'])\n-class InputSpec(object):\n+class InputSpec:\n   \"\"\"Specifies the rank, dtype and shape of every input to a layer.\n \n   Layers can expose (if appropriate) an `input_spec` attribute:\n\n@@ -27,7 +27,7 @@ from keras.utils import object_identity\n _MAX_TENSOR_RANK = 254\n \n \n-class KerasTensor(object):\n+class KerasTensor:\n   \"\"\"A representation of a Keras in/output during Functional API construction.\n \n   `KerasTensor`s are tensor-like objects that represent the symbolic inputs\n@@ -535,7 +535,7 @@ class UserRegisteredTypeKerasTensor(KerasTensor):\n     return self._user_registered_symbolic_object\n \n \n-class _KerasTensorIterator(object):\n+class _KerasTensorIterator:\n   \"\"\"Iterates over the leading dim of a KerasTensor. Performs 0 error checks.\"\"\"\n \n   def __init__(self, tensor, dim0):\n\n@@ -21,7 +21,7 @@ import numpy as np\n from keras import backend\n \n \n-class PartialBatchPaddingHandler(object):\n+class PartialBatchPaddingHandler:\n   \"\"\"A container that holds info about partial batches for `predict()`.\"\"\"\n \n   def __init__(self, output_shape):\n\n@@ -393,7 +393,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def test_finite_dataset_unknown_cardinality_no_step_with_train_and_val(self):\n \n-    class CaptureStdout(object):\n+    class CaptureStdout:\n \n       def __enter__(self):\n         self._stdout = sys.stdout\n\n@@ -115,7 +115,7 @@ def handle_partial_sample_weights(outputs, sample_weights, sample_weight_modes,\n           any_sample_weight, partial_sample_weight)\n \n \n-class RespectCompiledTrainableState(object):\n+class RespectCompiledTrainableState:\n   \"\"\"Set and restore trainable state if it has changed since compile.\n \n   The keras API guarantees that the value of each Layer's `trainable` property\n\n@@ -1694,7 +1694,7 @@ def infer_steps_for_dataset(model,\n   return steps\n \n \n-class ModelInputs(object):\n+class ModelInputs:\n   \"\"\"Encapsulates model inputs.\n \n   Allows for transforming model inputs while keeping the same structure.\n@@ -1885,7 +1885,7 @@ def unpack_validation_data(validation_data, raise_if_ambiguous=True):\n   return val_x, val_y, val_sample_weight\n \n \n-class TrainingLoop(object):\n+class TrainingLoop:\n   \"\"\"TrainingLoop is a wrapper class around the training logic.\n \n   This class is trying to encapsulate the different logic of fit/eval/predict\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"V1 Training-related part of the Keras engine.\"\"\"\n-\n+# pylint: disable=g-classes-have-attributes\n import tensorflow.compat.v2 as tf\n \n import collections\n@@ -2867,7 +2867,7 @@ class DistributedCallbackModel(Model):\n     return super(DistributedCallbackModel, self).__getattr__(item)\n \n \n-class _TrainingEndpoint(object):\n+class _TrainingEndpoint:\n   \"\"\"A container for the training output/target and related entities.\n \n   In the case of model with multiple outputs, there is a one-to-one mapping\n@@ -3099,7 +3099,7 @@ class _TrainingEndpoint(object):\n           name=self.output_name + '_sample_weights')\n \n \n-class _TrainingTarget(object):\n+class _TrainingTarget:\n   \"\"\"Container for a target tensor (y_true) and its metadata (shape, loss...).\n \n   Args:\n\n@@ -27,7 +27,7 @@ _PARTITION_OFFSET = 'partition_offset'\n \n \n @keras_export('keras.initializers.Initializer')\n-class Initializer(object):\n+class Initializer:\n   \"\"\"Initializer base class: all Keras initializers inherit from this class.\n \n   Initializers should implement a `__call__` method with the following\n@@ -943,7 +943,7 @@ def _assert_float_dtype(dtype):\n   return dtype\n \n \n-class _RandomGenerator(object):\n+class _RandomGenerator:\n   \"\"\"Random generator that selects appropriate random ops.\"\"\"\n \n   def __init__(self, seed=None):\n\n@@ -253,7 +253,7 @@ from keras.layers.serialization import deserialize\n from keras.layers.serialization import serialize\n \n \n-class VersionAwareLayers(object):\n+class VersionAwareLayers:\n   \"\"\"Utility to be used internally to access layers in a V1/V2-aware fashion.\n \n   When using layers within the Keras codebase, under the constraint that\n\n@@ -27,7 +27,7 @@ import warnings\n from keras.utils import generic_utils\n \n \n-class DropoutWrapperBase(object):\n+class DropoutWrapperBase:\n   \"\"\"Operator adding dropout to inputs and outputs of the given cell.\"\"\"\n \n   def __init__(self,\n@@ -312,7 +312,7 @@ class DropoutWrapperBase(object):\n         config, custom_objects=custom_objects)\n \n \n-class ResidualWrapperBase(object):\n+class ResidualWrapperBase:\n   \"\"\"RNNCell wrapper that ensures cell inputs are added to the outputs.\"\"\"\n \n   def __init__(self, cell, residual_fn=None, **kwargs):\n@@ -400,7 +400,7 @@ class ResidualWrapperBase(object):\n         config, custom_objects=custom_objects)\n \n \n-class DeviceWrapperBase(object):\n+class DeviceWrapperBase:\n   \"\"\"Operator that ensures an RNNCell runs on a particular device.\"\"\"\n \n   def __init__(self, cell, device, **kwargs):\n\n@@ -1107,7 +1107,7 @@ class AbstractRNNCell(Layer):\n \n \n @doc_controls.do_not_generate_docs\n-class DropoutRNNCellMixin(object):\n+class DropoutRNNCellMixin:\n   \"\"\"Object that hold dropout related fields for RNN Cell.\n \n   This class is not a standalone RNN cell. It suppose to be used with a RNN cell\n\n@@ -44,7 +44,7 @@ class RNNTest(keras_parameterized.TestCase):\n \n   def test_minimal_rnn_cell_non_layer(self):\n \n-    class MinimalRNNCell(object):\n+    class MinimalRNNCell:\n \n       def __init__(self, units, input_dim):\n         self.units = units\n@@ -84,7 +84,7 @@ class RNNTest(keras_parameterized.TestCase):\n \n   def test_minimal_rnn_cell_non_layer_multiple_states(self):\n \n-    class MinimalRNNCell(object):\n+    class MinimalRNNCell:\n \n       def __init__(self, units, input_dim):\n         self.units = units\n\n@@ -55,7 +55,7 @@ def _use_new_code():\n \n # TODO(b/169707691): The wrapper can be removed if TFLite doesn't need to rely\n # on supportive attributes from LSTM/GRU.\n-class _DefunWrapper(object):\n+class _DefunWrapper:\n   \"\"\"A wrapper with no deep copy of the Defun in LSTM/GRU layer.\"\"\"\n \n   def __init__(self, time_major, go_backwards, layer_name):\n\n@@ -526,7 +526,7 @@ def create_autocast_variable(variable):\n   return AutoCastDistributedVariable(variable)\n \n \n-class enable_auto_cast_variables(object):  # pylint:disable=invalid-name\n+class enable_auto_cast_variables:  # pylint:disable=invalid-name\n   \"\"\"Context manager which enables the autocasting of `AutoCastVariable`s.\n \n   Under this context manager, `AutoCastVariable`s will be cast to `dtype` if\n\n@@ -27,7 +27,7 @@ from tensorflow.python.platform import tf_logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class _UnwrapPreventer(object):\n+class _UnwrapPreventer:\n   \"\"\"Wrapper that DistributionStrategy will not unwrap.\n \n   Typically, DistributionStrategy will unwrap values when going from a cross-\n@@ -45,7 +45,7 @@ class _UnwrapPreventer(object):\n     self.value = value\n \n \n-class _DelegatingTrackableMixin(object):\n+class _DelegatingTrackableMixin:\n   \"\"\"A mixin that delegates all Trackable methods to another trackable object.\n \n   This class must be used with multiple inheritance. A class that subclasses\n\n@@ -28,7 +28,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n # pylint: disable=g-classes-have-attributes\n @keras_export('keras.mixed_precision.Policy', v1=[])\n-class Policy(object):\n+class Policy:\n   \"\"\"A dtype policy for a Keras layer.\n \n   A dtype policy determines a layer's computation and variable dtypes. Each\n\n@@ -23,7 +23,7 @@ import tensorflow.compat.v2 as tf\n from keras import backend\n \n \n-class Optimizer(object):\n+class Optimizer:\n   \"\"\"Abstract optimizer base class.\n \n   Note: this is the parent class of all optimizers, not an actual optimizer\n\n@@ -23,7 +23,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export(\"keras.optimizers.schedules.LearningRateSchedule\")\n-class LearningRateSchedule(object):\n+class LearningRateSchedule:\n   \"\"\"The learning rate schedule base class.\n \n   You can use a learning rate schedule to modulate how the learning rate\n\n@@ -62,7 +62,7 @@ def _deduplicate_indexed_slices(values, indices):\n   return (summed_values, unique_indices)\n \n \n-class NullContextmanager(object):\n+class NullContextmanager:\n \n   def __init__(self, *args, **kwargs):\n     pass\n\n@@ -43,7 +43,7 @@ def _none_to_default(inputs, default):\n \n \n @keras_export('keras.regularizers.Regularizer')\n-class Regularizer(object):\n+class Regularizer:\n   \"\"\"Regularizer base class.\n \n   Regularizers allow you to apply penalties on layer parameters or layer\n\n@@ -244,7 +244,7 @@ def _is_graph_network(layer):\n   return False\n \n \n-class KerasObjectLoader(object):\n+class KerasObjectLoader:\n   \"\"\"Loader that recreates Keras objects (e.g. layers, models).\n \n   Layers and models are revived from either the config or SavedModel following\n@@ -1008,7 +1008,7 @@ def _restore_layer_metrics(layer):\n \n # TODO(kathywu): Centrally define keys and functions for both  serialization and\n # deserialization.\n-class RevivedLayer(object):\n+class RevivedLayer:\n   \"\"\"Keras layer loaded from a SavedModel.\"\"\"\n \n   @classmethod\n@@ -1086,7 +1086,7 @@ def _revive_setter(layer, name, value):\n     setattr(layer, name, value)\n \n \n-class RevivedInputLayer(object):\n+class RevivedInputLayer:\n   \"\"\"InputLayer loaded from a SavedModel.\"\"\"\n \n   @classmethod\n\n@@ -374,7 +374,7 @@ def tracing_enabled():\n   return _thread_local_data.enable_call_tracing\n \n \n-class LayerCallCollection(object):\n+class LayerCallCollection:\n   \"\"\"Groups wrapped layer call functions.\n \n   This is used to ensure that all layer call functions are traced with the same\n@@ -582,7 +582,7 @@ def layer_call_wrapper(call_collection, method, name):\n   return fn\n \n \n-class LayerCall(object):\n+class LayerCall:\n   \"\"\"Function that triggers traces of other functions in the same collection.\"\"\"\n \n   def __init__(self, call_collection, call_fn, name):\n\n@@ -38,7 +38,7 @@ recurrent = LazyLoader(\n # pylint:enable=g-inconsistent-quotes\n \n \n-class SerializedAttributes(object):\n+class SerializedAttributes:\n   \"\"\"Class that tracks and validates all serialization attributes.\n \n   Keras models contain many Python-defined components. For example, the\n\n@@ -21,7 +21,7 @@ import abc\n from keras.saving.utils_v1 import signature_def_utils as unexported_signature_utils\n \n \n-class ExportOutput(object):\n+class ExportOutput:\n   \"\"\"Represents an output of a model that can be served.\n \n   These typically correspond to model heads.\n\n@@ -317,7 +317,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n \n   def test_summary(self):\n \n-    class ToString(object):\n+    class ToString:\n \n       def __init__(self):\n         self.contents = ''\n\n@@ -255,7 +255,7 @@ def get_file(fname=None,\n   if download:\n     print('Downloading data from', origin)\n \n-    class ProgressTracker(object):\n+    class ProgressTracker:\n       # Maintain progbar for the lifetime of download.\n       # This design was chosen for Python 2.7 compatibility.\n       progbar = None\n@@ -362,7 +362,7 @@ def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n     return False\n \n \n-class ThreadsafeIter(object):\n+class ThreadsafeIter:\n   \"\"\"Wrap an iterator with a lock and propagate exceptions to all threads.\"\"\"\n \n   def __init__(self, it):\n@@ -406,7 +406,7 @@ def threadsafe_generator(f):\n \n \n @keras_export('keras.utils.Sequence')\n-class Sequence(object):\n+class Sequence:\n   \"\"\"Base object for fitting to a sequence of data, such as a dataset.\n \n   Every `Sequence` must implement the `__getitem__` and the `__len__` methods.\n@@ -564,7 +564,7 @@ def get_index(uid, i):\n \n \n @keras_export('keras.utils.SequenceEnqueuer')\n-class SequenceEnqueuer(object):\n+class SequenceEnqueuer:\n   \"\"\"Base class to enqueue inputs.\n \n   The task of an Enqueuer is to use parallelism to speed up preprocessing.\n\n@@ -20,7 +20,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export('keras.utils.experimental.DatasetCreator', v1=[])\n-class DatasetCreator(object):\n+class DatasetCreator:\n   \"\"\"Object that returns a `tf.data.Dataset` upon invoking.\n \n   `tf.keras.utils.experimental.DatasetCreator` is designated as a supported type\n\n@@ -49,7 +49,7 @@ _LAYER_UNDEFINED_CONFIG_KEY = 'layer was saved without config'\n \n @keras_export('keras.utils.custom_object_scope',  # pylint: disable=g-classes-have-attributes\n               'keras.utils.CustomObjectScope')\n-class CustomObjectScope(object):\n+class CustomObjectScope:\n   \"\"\"Exposes custom classes/functions to Keras deserialization internals.\n \n   Under a scope `with custom_object_scope(objects_dict)`, Keras methods such\n@@ -142,7 +142,7 @@ def _shared_object_saving_scope():\n   return getattr(SHARED_OBJECT_SAVING, 'scope', None)\n \n \n-class DisableSharedObjectScope(object):\n+class DisableSharedObjectScope:\n   \"\"\"A context manager for disabling handling of shared objects.\n \n   Disables shared object handling for both saving and loading.\n@@ -162,7 +162,7 @@ class DisableSharedObjectScope(object):\n     SHARED_OBJECT_SAVING.scope = self._orig_saving_scope\n \n \n-class NoopLoadingScope(object):\n+class NoopLoadingScope:\n   \"\"\"The default shared object loading scope. It does nothing.\n \n   Created to simplify serialization code that doesn't care about shared objects\n@@ -176,7 +176,7 @@ class NoopLoadingScope(object):\n     pass\n \n \n-class SharedObjectLoadingScope(object):\n+class SharedObjectLoadingScope:\n   \"\"\"A context manager for keeping track of loaded objects.\n \n   During the deserialization process, we may come across objects that are\n@@ -253,7 +253,7 @@ class SharedObjectConfig(dict):\n     self.ref_count += 1\n \n \n-class SharedObjectSavingScope(object):\n+class SharedObjectSavingScope:\n   \"\"\"Keeps track of shared object configs when serializing.\"\"\"\n \n   def __enter__(self):\n@@ -812,7 +812,7 @@ def has_arg(fn, name, accept_all=False):\n \n \n @keras_export('keras.utils.Progbar')\n-class Progbar(object):\n+class Progbar:\n   \"\"\"Displays a progress bar.\n \n   Args:\n\n@@ -77,7 +77,7 @@ class TestCustomObjectScope(tf.test.TestCase):\n     def custom_fn():\n       pass\n \n-    class CustomClass(object):\n+    class CustomClass:\n       pass\n \n     with keras.utils.generic_utils.custom_object_scope(\n@@ -100,7 +100,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n   def test_serialize_custom_class_with_default_name(self):\n \n     @keras.utils.generic_utils.register_keras_serializable()\n-    class TestClass(object):\n+    class TestClass:\n \n       def __init__(self, value):\n         self._value = value\n@@ -122,7 +122,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n     # Make sure registering a new class with same name will fail.\n     with self.assertRaisesRegex(ValueError, '.*has already been registered.*'):\n       @keras.utils.generic_utils.register_keras_serializable()  # pylint: disable=function-redefined\n-      class TestClass(object):  # pylint: disable=function-redefined\n+      class TestClass:  # pylint: disable=function-redefined\n \n         def __init__(self, value):\n           self._value = value\n@@ -134,7 +134,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n \n     @keras.utils.generic_utils.register_keras_serializable(\n         'TestPackage', 'CustomName')\n-    class OtherTestClass(object):\n+    class OtherTestClass:\n \n       def __init__(self, val):\n         self._val = val\n@@ -188,7 +188,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n \n       @keras.utils.generic_utils.register_keras_serializable(  # pylint: disable=unused-variable\n           'TestPackage', 'TestClass')\n-      class TestClass(object):\n+      class TestClass:\n \n         def __init__(self, value):\n           self._value = value\n@@ -403,7 +403,7 @@ class SliceArraysTest(tf.test.TestCase):\n \n # object() alone isn't compatible with WeakKeyDictionary, which we use to\n # track shared configs.\n-class MaybeSharedObject(object):\n+class MaybeSharedObject:\n   pass\n \n \n\n@@ -43,7 +43,7 @@ class TestIOUtils(keras_parameterized.TestCase):\n \n   def test_path_to_string(self):\n \n-    class PathLikeDummy(object):\n+    class PathLikeDummy:\n \n       def __fspath__(self):\n         return 'dummypath'\n\n@@ -354,7 +354,7 @@ def cached_per_instance(f):\n   Consider the following class:\n \n   ```\n-  class MyClass(object):\n+  class MyClass:\n     def __setattr__(self, key, value):\n       # Some expensive class specific code\n       # ...\n\n@@ -22,7 +22,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export('keras.losses.Reduction', v1=[])\n-class ReductionV2(object):\n+class ReductionV2:\n   \"\"\"Types of loss reduction.\n \n   Contains the following values:\n\n@@ -19,7 +19,7 @@ import weakref\n \n \n # LINT.IfChange\n-class _ObjectIdentityWrapper(object):\n+class _ObjectIdentityWrapper:\n   \"\"\"Wraps an object, mapping __eq__ on wrapper to \"is\" on wrapped.\n \n   Since __eq__ is based on object identity, it's safe to also define __hash__\n\n@@ -13,7 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"TFDecorator-aware replacements for the inspect module.\"\"\"\n-\n+# pylint: disable=g-classes-have-attributes\n import tensorflow.compat.v2 as tf\n \n import collections\n@@ -293,11 +293,11 @@ def getframeinfo(*args, **kwargs):\n   return _inspect.getframeinfo(*args, **kwargs)\n \n \n-def getdoc(object):  # pylint: disable=redefined-builtin\n+def getdoc(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.getdoc.\n \n   Args:\n-    object: An object, possibly decorated.\n+    obj: An object, possibly decorated.\n \n   Returns:\n     The docstring associated with the object.\n@@ -305,12 +305,12 @@ def getdoc(object):  # pylint: disable=redefined-builtin\n   The outermost-decorated object is intended to have the most complete\n   documentation, so the decorated parameter is not unwrapped.\n   \"\"\"\n-  return _inspect.getdoc(object)\n+  return _inspect.getdoc(obj)\n \n \n-def getfile(object):  # pylint: disable=redefined-builtin\n+def getfile(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.getfile.\"\"\"\n-  unwrapped_object = tf.__internal__.decorator.unwrap(object)[1]\n+  unwrapped_object = tf.__internal__.decorator.unwrap(obj)[1]\n \n   # Work around for the case when object is a stack frame\n   # and only .pyc files are used. In this case, getfile\n@@ -322,14 +322,14 @@ def getfile(object):  # pylint: disable=redefined-builtin\n   return _inspect.getfile(unwrapped_object)\n \n \n-def getmembers(object, predicate=None):  # pylint: disable=redefined-builtin\n+def getmembers(obj, predicate=None):\n   \"\"\"TFDecorator-aware replacement for inspect.getmembers.\"\"\"\n-  return _inspect.getmembers(object, predicate)\n+  return _inspect.getmembers(obj, predicate)\n \n \n-def getmodule(object):  # pylint: disable=redefined-builtin\n+def getmodule(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.getmodule.\"\"\"\n-  return _inspect.getmodule(object)\n+  return _inspect.getmodule(obj)\n \n \n def getmro(cls):\n@@ -337,64 +337,64 @@ def getmro(cls):\n   return _inspect.getmro(cls)\n \n \n-def getsource(object):  # pylint: disable=redefined-builtin\n+def getsource(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.getsource.\"\"\"\n-  return _inspect.getsource(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.getsource(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def getsourcefile(object):  # pylint: disable=redefined-builtin\n+def getsourcefile(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.getsourcefile.\"\"\"\n-  return _inspect.getsourcefile(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.getsourcefile(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def getsourcelines(object):  # pylint: disable=redefined-builtin\n+def getsourcelines(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.getsourcelines.\"\"\"\n-  return _inspect.getsourcelines(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.getsourcelines(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isbuiltin(object):  # pylint: disable=redefined-builtin\n+def isbuiltin(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.isbuiltin.\"\"\"\n-  return _inspect.isbuiltin(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isbuiltin(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isclass(object):  # pylint: disable=redefined-builtin\n+def isclass(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.isclass.\"\"\"\n-  return _inspect.isclass(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isclass(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isfunction(object):  # pylint: disable=redefined-builtin\n+def isfunction(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.isfunction.\"\"\"\n-  return _inspect.isfunction(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isfunction(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isframe(object):  # pylint: disable=redefined-builtin\n+def isframe(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.ismodule.\"\"\"\n-  return _inspect.isframe(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isframe(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isgenerator(object):  # pylint: disable=redefined-builtin\n+def isgenerator(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.isgenerator.\"\"\"\n-  return _inspect.isgenerator(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isgenerator(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isgeneratorfunction(object):  # pylint: disable=redefined-builtin\n+def isgeneratorfunction(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.isgeneratorfunction.\"\"\"\n-  return _inspect.isgeneratorfunction(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isgeneratorfunction(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def ismethod(object):  # pylint: disable=redefined-builtin\n+def ismethod(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.ismethod.\"\"\"\n-  return _inspect.ismethod(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.ismethod(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def ismodule(object):  # pylint: disable=redefined-builtin\n+def ismodule(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.ismodule.\"\"\"\n-  return _inspect.ismodule(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.ismodule(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n-def isroutine(object):  # pylint: disable=redefined-builtin\n+def isroutine(obj):\n   \"\"\"TFDecorator-aware replacement for inspect.isroutine.\"\"\"\n-  return _inspect.isroutine(tf.__internal__.decorator.unwrap(object)[1])\n+  return _inspect.isroutine(tf.__internal__.decorator.unwrap(obj)[1])\n \n \n def stack(context=1):\n\n@@ -184,7 +184,7 @@ def convert_shapes(input_shape, to_tuples=True):\n                                    input_shape)\n \n \n-class ListWrapper(object):\n+class ListWrapper:\n   \"\"\"A wrapper for lists to be treated as elements for `nest`.\"\"\"\n \n   def __init__(self, list_to_wrap):\n@@ -333,7 +333,7 @@ def register_symbolic_tensor_type(cls):\n \n   ```python\n   # One-time setup.\n-  class Foo(object):\n+  class Foo:\n     def __init__(self, input_):\n       self._input = input_\n     def value(self):\n\n@@ -53,7 +53,7 @@ class TestIsSymbolicTensor(tf.test.TestCase, parameterized.TestCase):\n \n   def test_works_with_registered(self):\n \n-    class CustomClass(object):\n+    class CustomClass:\n \n       def value(self):\n         return tf.convert_to_tensor(42.)\n@@ -89,7 +89,7 @@ class TestIsSymbolicTensor(tf.test.TestCase, parameterized.TestCase):\n       self.skipTest('`compile` functionality changed.')\n     # Setup.\n \n-    class Foo(object):\n+    class Foo:\n \n       def __init__(self, input_):\n         self._input = input_\n@@ -169,7 +169,7 @@ class AttrsTest(tf.test.TestCase):\n       self.skipTest('attr module is unavailable.')\n \n     @attr.s(frozen=True)\n-    class Foo(object):\n+    class Foo:\n \n       bar = attr.ib()\n \n\n@@ -44,7 +44,7 @@ callbacks_v1 = LazyLoader(\n # pylint: enable=g-inconsistent-quotes\n \n \n-class ModelVersionSelector(object):\n+class ModelVersionSelector:\n   \"\"\"Chooses between Keras v1 and v2 Model class.\"\"\"\n \n   def __new__(cls, *args, **kwargs):  # pylint: disable=unused-argument\n@@ -53,7 +53,7 @@ class ModelVersionSelector(object):\n     return super(ModelVersionSelector, cls).__new__(cls)\n \n \n-class LayerVersionSelector(object):\n+class LayerVersionSelector:\n   \"\"\"Chooses between Keras v1 and v2 Layer class.\"\"\"\n \n   def __new__(cls, *args, **kwargs):  # pylint: disable=unused-argument\n@@ -62,7 +62,7 @@ class LayerVersionSelector(object):\n     return super(LayerVersionSelector, cls).__new__(cls)\n \n \n-class TensorBoardVersionSelector(object):\n+class TensorBoardVersionSelector:\n   \"\"\"Chooses between Keras v1 and v2 TensorBoard callback class.\"\"\"\n \n   def __new__(cls, *args, **kwargs):  # pylint: disable=unused-argument\n\n@@ -149,7 +149,7 @@ class SplitUtilsTest(keras_parameterized.TestCase):\n \n   def test_multiple_inheritance(self):\n \n-    class Return2(object):\n+    class Return2:\n \n       def return_2(self):\n         return 2\n\n@@ -27,7 +27,7 @@ from keras.utils.np_utils import to_categorical\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class BaseWrapper(object):\n+class BaseWrapper:\n   \"\"\"Base class for the Keras scikit-learn wrapper.\n \n   Warning: This class should not be used directly.\n\n@@ -111,7 +111,7 @@ class ScikitLearnAPIWrapperTest(tf.test.TestCase):\n \n   def test_classify_class_build_fn(self):\n \n-    class ClassBuildFnClf(object):\n+    class ClassBuildFnClf:\n \n       def __call__(self, hidden_dim):\n         return build_fn_clf(hidden_dim)\n@@ -153,7 +153,7 @@ class ScikitLearnAPIWrapperTest(tf.test.TestCase):\n \n   def test_regression_class_build_fn(self):\n \n-    class ClassBuildFnReg(object):\n+    class ClassBuildFnReg:\n \n       def __call__(self, hidden_dim):\n         return build_fn_reg(hidden_dim)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
