{"custom_id": "keras#6fd736cead0e1bb829fe9b26e814c05ab7b4b2be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 0 | Files Changed: 11 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 20/5 | Churn Δ: 125 | Churn Cumulative: 11058 | Contributors (this commit): 48 | Commits (past 90d): 133 | Contributors (cumulative): 147 | DMM Complexity: 1.0\n\nDIFF:\n@@ -155,6 +155,7 @@ from keras.src.ops.numpy import concatenate as concatenate\n from keras.src.ops.numpy import conj as conj\n from keras.src.ops.numpy import conjugate as conjugate\n from keras.src.ops.numpy import copy as copy\n+from keras.src.ops.numpy import corrcoef as corrcoef\n from keras.src.ops.numpy import correlate as correlate\n from keras.src.ops.numpy import cos as cos\n from keras.src.ops.numpy import cosh as cosh\n\n@@ -44,6 +44,7 @@ from keras.src.ops.numpy import concatenate as concatenate\n from keras.src.ops.numpy import conj as conj\n from keras.src.ops.numpy import conjugate as conjugate\n from keras.src.ops.numpy import copy as copy\n+from keras.src.ops.numpy import corrcoef as corrcoef\n from keras.src.ops.numpy import correlate as correlate\n from keras.src.ops.numpy import cos as cos\n from keras.src.ops.numpy import cosh as cosh\n\n@@ -155,6 +155,7 @@ from keras.src.ops.numpy import concatenate as concatenate\n from keras.src.ops.numpy import conj as conj\n from keras.src.ops.numpy import conjugate as conjugate\n from keras.src.ops.numpy import copy as copy\n+from keras.src.ops.numpy import corrcoef as corrcoef\n from keras.src.ops.numpy import correlate as correlate\n from keras.src.ops.numpy import cos as cos\n from keras.src.ops.numpy import cosh as cosh\n\n@@ -44,6 +44,7 @@ from keras.src.ops.numpy import concatenate as concatenate\n from keras.src.ops.numpy import conj as conj\n from keras.src.ops.numpy import conjugate as conjugate\n from keras.src.ops.numpy import copy as copy\n+from keras.src.ops.numpy import corrcoef as corrcoef\n from keras.src.ops.numpy import correlate as correlate\n from keras.src.ops.numpy import cos as cos\n from keras.src.ops.numpy import cosh as cosh\n\n@@ -1338,6 +1338,11 @@ def logical_xor(x1, x2):\n     return jnp.logical_xor(x1, x2)\n \n \n+def corrcoef(x):\n+    x = convert_to_tensor(x)\n+    return jnp.corrcoef(x)\n+\n+\n def correlate(x1, x2, mode=\"valid\"):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n\n@@ -1256,6 +1256,19 @@ def logical_xor(x1, x2):\n     return np.logical_xor(x1, x2)\n \n \n+def corrcoef(x):\n+    if x.dtype in [\"int64\", \"float64\"]:\n+        dtype = \"float64\"\n+    elif x.dtype in [\"bfloat16\", \"float16\"]:\n+        dtype = x.dtype\n+    else:\n+        dtype = config.floatx()\n+\n+    x = convert_to_tensor(x)\n+\n+    return np.corrcoef(x).astype(dtype)\n+\n+\n def correlate(x1, x2, mode=\"valid\"):\n     dtype = dtypes.result_type(\n         getattr(x1, \"dtype\", type(x1)),\n\n@@ -1666,6 +1666,12 @@ def logical_xor(x1, x2):\n     return OpenVINOKerasTensor(ov_opset.logical_xor(x1, x2).output(0))\n \n \n+def corrcoef(x):\n+    raise NotImplementedError(\n+        \"`corrcoef` is not supported with openvino backend\"\n+    )\n+\n+\n def correlate(x1, x2, mode=\"valid\"):\n     raise NotImplementedError(\n         \"`correlate` is not supported with openvino backend\"\n\n@@ -2786,6 +2786,38 @@ def logical_xor(x1, x2):\n     return tf.math.logical_xor(x1, x2)\n \n \n+def corrcoef(x):\n+    dtype = x.dtype\n+    if dtype in [\"bool\", \"int8\", \"int16\", \"int32\", \"uint8\", \"uint16\", \"uint32\"]:\n+        dtype = config.floatx()\n+    x = convert_to_tensor(x, dtype)\n+\n+    if tf.rank(x) == 0:\n+        return tf.constant(float(\"nan\"), dtype=config.floatx())\n+\n+    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n+    x_centered = x - mean\n+\n+    num_samples = tf.cast(tf.shape(x)[-1], x.dtype)\n+    cov_matrix = tf.matmul(x_centered, x_centered, adjoint_b=True) / (\n+        num_samples - 1\n+    )\n+\n+    diag = tf.linalg.diag_part(cov_matrix)\n+    stddev = tf.sqrt(tf.math.real(diag))\n+\n+    outer_std = tf.tensordot(stddev, stddev, axes=0)\n+    outer_std = tf.cast(outer_std, cov_matrix.dtype)\n+    correlation = cov_matrix / outer_std\n+\n+    correlation_clipped = tf.clip_by_value(tf.math.real(correlation), -1.0, 1.0)\n+    if correlation.dtype.is_complex:\n+        imag_clipped = tf.clip_by_value(tf.math.imag(correlation), -1.0, 1.0)\n+        return tf.complex(correlation_clipped, imag_clipped)\n+    else:\n+        return correlation_clipped\n+\n+\n def correlate(x1, x2, mode=\"valid\"):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n\n@@ -1742,6 +1742,17 @@ def logical_xor(x1, x2):\n     return torch.logical_xor(x1, x2)\n \n \n+def corrcoef(x):\n+    x = convert_to_tensor(x)\n+\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        x = cast(x, config.floatx())\n+    elif standardize_dtype(x.dtype) == \"int64\":\n+        x = cast(x, \"float64\")\n+\n+    return torch.corrcoef(x)\n+\n+\n def correlate(x1, x2, mode=\"valid\"):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n\n@@ -6866,6 +6866,35 @@ def logical_xor(x1, x2):\n     return backend.numpy.logical_xor(x1, x2)\n \n \n+class Corrcoef(Operation):\n+    def call(self, x):\n+        return backend.numpy.corrcoef(x)\n+\n+    def compute_output_spec(self, x):\n+        dtype = backend.standardize_dtype(getattr(x, \"dtype\", backend.floatx()))\n+        if dtype == \"int64\":\n+            dtype = \"float64\"\n+        else:\n+            dtype = dtypes.result_type(dtype, float)\n+        return KerasTensor(x.shape, dtype=dtype)\n+\n+\n+@keras_export([\"keras.ops.corrcoef\", \"keras.ops.numpy.corrcoef\"])\n+def corrcoef(x):\n+    \"\"\"Compute the Pearson correlation coefficient matrix.\n+\n+    Args:\n+        x: A 2D tensor of shape `(N, D)`, where N is the number of variables\n+           and D is the number of observations.\n+\n+    Returns:\n+        A tensor of shape `(N, N)` representing the correlation matrix.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Corrcoef().symbolic_call(x)\n+    return backend.numpy.corrcoef(x)\n+\n+\n class Correlate(Operation):\n     def __init__(self, mode=\"valid\", *, name=None):\n         super().__init__(name=name)\n\n@@ -1313,6 +1313,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.copy(x).shape, (None, 3))\n \n+    def test_corrcoef(self):\n+        x = KerasTensor((3, None))\n+        self.assertEqual(knp.corrcoef(x).shape, (3, None))\n+\n     def test_cos(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.cos(x).shape, (None, 3))\n@@ -3838,6 +3842,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.copy(x), np.copy(x))\n         self.assertAllClose(knp.Copy()(x), np.copy(x))\n \n+    def test_corrcoef(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(knp.corrcoef(x), np.corrcoef(x))\n+        self.assertAllClose(knp.Corrcoef()(x), np.corrcoef(x))\n+\n     def test_cos(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.cos(x), np.cos(x))\n@@ -6568,6 +6577,22 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_corrcoef(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((2, 4), dtype=dtype)\n+        x_jax = jnp.ones((2, 4), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.corrcoef(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.corrcoef(x).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Corrcoef().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#764ed95651c1c7dfa71ca523287f6eeb514ccf77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 36 | Churn Cumulative: 1627 | Contributors (this commit): 19 | Commits (past 90d): 9 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import warnings\n+\n from keras.src import constraints\n from keras.src import initializers\n from keras.src import ops\n@@ -82,12 +84,6 @@ class LayerNormalization(Layer):\n             When the next layer is linear (also e.g. `nn.relu`), this can be\n             disabled since the scaling will be done by the next layer.\n             Defaults to `True`.\n-        rms_scaling: If True, `center` and `scale` are ignored, and the\n-            inputs are scaled by `gamma` and the inverse square root\n-            of the square of all inputs. This is an approximate and faster\n-            approach that avoids ever computing the mean of the input. Note that\n-            this *isn't* equivalent to the computation that the\n-            `keras.layers.RMSNormalization` layer performs.\n         beta_initializer: Initializer for the beta weight. Defaults to zeros.\n         gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n         beta_regularizer: Optional regularizer for the beta weight.\n@@ -112,7 +108,6 @@ class LayerNormalization(Layer):\n         epsilon=1e-3,\n         center=True,\n         scale=True,\n-        rms_scaling=False,\n         beta_initializer=\"zeros\",\n         gamma_initializer=\"ones\",\n         beta_regularizer=None,\n@@ -121,6 +116,15 @@ class LayerNormalization(Layer):\n         gamma_constraint=None,\n         **kwargs,\n     ):\n+        rms_scaling = kwargs.pop(\"rms_scaling\", False)\n+        if rms_scaling:\n+            warnings.warn(\n+                \"You passed `rms_scaling=True`, which is deprecated. This \"\n+                \"argument incorrectly scales the input by the variance, not \"\n+                \"the root mean square. To correctly use RMS Normalization, \"\n+                \"please use `keras.layers.RMSNormalization` instead.\"\n+            )\n+\n         super().__init__(**kwargs)\n         if isinstance(axis, (list, tuple)):\n             self.axis = list(axis)\n@@ -185,7 +189,7 @@ class LayerNormalization(Layer):\n             self.beta,\n             self.axis,\n             self.epsilon,\n-            self.rms_scaling,\n+            rms_scaling=self.rms_scaling,\n         )\n         return ops.cast(outputs, self.compute_dtype)\n \n\n@@ -2875,7 +2875,7 @@ class LayerNorm(Operation):\n     ]\n )\n def layer_normalization(\n-    x, gamma=None, beta=None, axis=-1, epsilon=None, rms_scaling=False\n+    x, gamma=None, beta=None, axis=-1, epsilon=None, **kwargs\n ):\n     \"\"\"Layer normalization layer (Ba et al., 2016).\n \n@@ -2889,9 +2889,6 @@ def layer_normalization(\n             Default to -1.\n         gamma: Optional scaling factor for the normalization.\n         beta: Optional add offset for the normalized tensor.\n-        rms_scaling:This is an approximate and faster\n-            approach that avoids ever computing the mean of the input. Note that\n-            this *isn't* equivalent to the computation that rms_normalization\n         epsilon: A lower bound value for the norm.\n             Defaults to `backend.epsilon()`.\n \n@@ -2902,6 +2899,16 @@ def layer_normalization(\n     >>> print(x_norm)\n     array([-1.4142135 , -0.70710677,  0.,  0.7071067 ,  1.4142135 ])\n     \"\"\"\n+    rms_scaling = kwargs.pop(\"rms_scaling\", False)\n+    if rms_scaling:\n+        warnings.warn(\n+            \"You passed `rms_scaling=True`, which is deprecated. This argument \"\n+            \"incorrectly scales the input by the variance, not the root mean \"\n+            \"square. To correctly use RMS Normalization, please use \"\n+            \"`keras.ops.rms_normalization` / `keras.ops.nn.rms_normalization` \"\n+            \"instead.\"\n+        )\n+\n     if any_symbolic_tensors((x,)):\n         return LayerNorm(\n             gamma=gamma,\n@@ -2953,7 +2960,6 @@ def _layer_normalization(\n         # Calculate the variance along self.axis (layer activations).\n         variance = backend.numpy.var(x, axis=axis, keepdims=True)\n         inv = backend.math.rsqrt(variance + epsilon)\n-\n         outputs = x * inv * backend.cast(_broadcast(gamma), x.dtype)\n     elif backend.config.backend() == \"torch\" and is_continuous_axis(axis):\n         # when using torch backend,use kernel to improve performance\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
