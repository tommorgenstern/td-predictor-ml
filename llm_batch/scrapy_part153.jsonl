{"custom_id": "scrapy#910effd145e29660a7bfc1935ffb4a727bf9f7e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 452 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n import sys, os, glob, shutil\n from subprocess import check_call\n+from scrapy import version_info\n \n def build(suffix):\n     for ifn in glob.glob(\"debian/scrapy.*\"):\n@@ -29,7 +30,7 @@ def clean(suffix):\n \n def main():\n     cmd = sys.argv[1]\n-    suffix = '%s.%s' % __import__('scrapy').version_info[:2]\n+    suffix = '%s.%s' % version_info[:2]\n     if cmd == 'build':\n         build(suffix)\n     elif cmd == 'clean':\n\n@@ -1,9 +1,9 @@\n \"\"\"\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n-\n-version_info = (0, 17, 0)\n-__version__ = \"0.17.0\"\n+import pkgutil\n+__version__ = pkgutil.get_data(__package__, 'VERSION').strip()\n+version_info = tuple(__version__.split('.')[:3])\n \n import sys, os, warnings\n \n\n@@ -82,9 +82,11 @@ if os.name == 'nt':\n \n if os.environ.get('SCRAPY_VERSION_FROM_GIT'):\n     v = Popen(\"git describe\", shell=True, stdout=PIPE).communicate()[0]\n-    with open('scrapy/__init__.py', 'a') as f:\n-        f.write(\"\\n__version__ = '%s'\" % v.strip())\n-version = __import__('scrapy').__version__\n+    with open('scrapy/VERSION', 'w+') as f:\n+        f.write(v.strip())\n+with open(os.path.join(os.path.dirname(__file__), 'scrapy/VERSION')) as f:\n+    version = f.read().strip()\n+\n \n setup_args = {\n     'name': 'Scrapy',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d4b764f29fc3c5fbaa64506289d3c15eabbe44f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 44 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -27,11 +27,11 @@ class BaseSpider(object_ref):\n         if not hasattr(self, 'start_urls'):\n             self.start_urls = []\n \n-    def log(self, message, level=log.DEBUG):\n+    def log(self, message, level=log.DEBUG, **kw):\n         \"\"\"Log the given messages at the given log level. Always use this\n         method to send log messages from your spider\n         \"\"\"\n-        log.msg(message, spider=self, level=level)\n+        log.msg(message, spider=self, level=level, **kw)\n \n     def set_crawler(self, crawler):\n         assert not hasattr(self, '_crawler'), \"Spider already bounded to %s\" % crawler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27ca25472fd3fb5b540d83ea95a9eada5ae13c61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 743 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import unittest\n from scrapy.spider import BaseSpider\n-from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url\n+from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url, url_has_any_extension\n+from scrapy.linkextractor import IGNORED_EXTENSIONS\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -157,6 +158,13 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC/\"),\n                          \"http://foo.com/AC%2FDC/\")\n \n+    def test_url_has_any_extension(self):\n+        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.doc\", IGNORED_EXTENSIONS))\n+        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.pdf\", IGNORED_EXTENSIONS))\n+        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.htm\", IGNORED_EXTENSIONS))\n+        self.assertFalse(url_has_any_extension(\"http://www.example.com/\", IGNORED_EXTENSIONS))\n+        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.doc.html\", IGNORED_EXTENSIONS))\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -28,7 +28,7 @@ def url_is_from_spider(url, spider):\n         getattr(spider, 'allowed_domains', []))\n \n def url_has_any_extension(url, extensions):\n-    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n+    return posixpath.splitext(parse_url(url).path)[1].replace('.', '').lower() in extensions\n \n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n         encoding=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9043ffa6ea8ba1d698ceb2c9b5f60135417afb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 752 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -158,13 +158,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC/\"),\n                          \"http://foo.com/AC%2FDC/\")\n \n-    def test_url_has_any_extension(self):\n-        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.doc\", IGNORED_EXTENSIONS))\n-        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.pdf\", IGNORED_EXTENSIONS))\n-        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.htm\", IGNORED_EXTENSIONS))\n-        self.assertFalse(url_has_any_extension(\"http://www.example.com/\", IGNORED_EXTENSIONS))\n-        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.doc.html\", IGNORED_EXTENSIONS))\n-\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -28,7 +28,7 @@ def url_is_from_spider(url, spider):\n         getattr(spider, 'allowed_domains', []))\n \n def url_has_any_extension(url, extensions):\n-    return posixpath.splitext(parse_url(url).path)[1].replace('.', '').lower() in extensions\n+    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n \n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n         encoding=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5971333a9371e0c8049a0cf4d90061718a3aff3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 110 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -4,6 +4,7 @@ Base class for Scrapy commands\n \n import os\n from optparse import OptionGroup\n+from twisted.python import failure\n \n from scrapy import log\n from scrapy.utils.conf import arglist_to_dict\n@@ -80,6 +81,7 @@ class ScrapyCommand(object):\n             help=\"write process ID to FILE\")\n         group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set/override setting (may be repeated)\")\n+        group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n         parser.add_option_group(group)\n         \n     def process_options(self, args, opts):\n@@ -103,6 +105,9 @@ class ScrapyCommand(object):\n             with open(opts.pidfile, \"w\") as f:\n                 f.write(str(os.getpid()) + os.linesep)\n \n+        if opts.pdb:\n+            failure.startDebugMode()\n+\n     def run(self, args, opts):\n         \"\"\"\n         Entry point for running commands\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2efd85952591de5dd8ff0666a82533bcdb7d0d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 9 | Churn Cumulative: 446 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.5\n\nDIFF:\n@@ -56,6 +56,11 @@ class Command(ScrapyCommand):\n \n         name, domain = args[0:2]\n         module = sanitize_module_name(name)\n+\n+        if self.settings.get('BOT_NAME') == module:\n+            print \"Cannot create a spider with the same name as your project\"\n+            return\n+\n         try:\n             spider = self.crawler.spiders.create(name)\n         except KeyError:\n\n@@ -112,6 +112,10 @@ class GenspiderCommandTest(CommandTest):\n         self.assertEqual(0, self.call('genspider', '--dump=basic'))\n         self.assertEqual(0, self.call('genspider', '-d', 'basic'))\n \n+    def test_same_name_as_project(self):\n+        self.assertEqual(2, self.call('genspider', self.project_name))\n+        assert not exists(join(self.proj_mod_path, 'spiders', '%s.py' % self.project_name))\n+\n \n class MiscCommandsTest(CommandTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3f50b97b225515623957436bd6ac085b283a1a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 154 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,7 @@ from UserDict import DictMixin\n \n from scrapy.utils.trackref import object_ref\n \n+\n class BaseItem(object_ref):\n     \"\"\"Base class for all scraped items.\"\"\"\n     pass\n@@ -52,7 +53,7 @@ class DictItem(DictMixin, BaseItem):\n         if key in self.fields:\n             self._values[key] = value\n         else:\n-            raise KeyError(\"%s does not support field: %s\" % \\\n+            raise KeyError(\"%s does not support field: %s\" %\n                 (self.__class__.__name__, key))\n \n     def __delitem__(self, key):\n@@ -65,7 +66,7 @@ class DictItem(DictMixin, BaseItem):\n \n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n-            raise AttributeError(\"Use item[%r] = %r to set field value\" % \\\n+            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n                 (name, value))\n         super(DictItem, self).__setattr__(name, value)\n \n@@ -79,4 +80,3 @@ class DictItem(DictMixin, BaseItem):\n class Item(DictItem):\n \n     __metaclass__ = ItemMeta\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7400ceb1ed972e66e5d625a91259f2d24c54138f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 192 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -192,7 +192,7 @@ REFERER_ENABLED = True\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = [500, 503, 504, 400, 408]\n+RETRY_HTTP_CODES = [500, 502, 503, 504, 400, 408]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2bbd92742b796e1a565d4914a77889c884dd01ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 329 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import os\n import unittest\n from cStringIO import StringIO\n \n+from scrapy.item import Item, Field\n from scrapy.utils.misc import load_object, arg_to_iter, walk_modules\n \n __doctests__ = ['scrapy.utils.misc']\n@@ -56,6 +57,10 @@ class UtilsMiscTestCase(unittest.TestCase):\n             sys.path.remove(egg)\n \n     def test_arg_to_iter(self):\n+\n+        class TestItem(Item):\n+            name = Field()\n+\n         assert hasattr(arg_to_iter(None), '__iter__')\n         assert hasattr(arg_to_iter(100), '__iter__')\n         assert hasattr(arg_to_iter('lala'), '__iter__')\n@@ -68,6 +73,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])\n         self.assertEqual(list(arg_to_iter([1,2,3])), [1,2,3])\n         self.assertEqual(list(arg_to_iter({'a':1})), [{'a': 1}])\n+        self.assertEqual(list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -6,6 +6,7 @@ from pkgutil import iter_modules\n \n from w3lib.html import remove_entities\n from scrapy.utils.python import flatten\n+from scrapy.item import BaseItem\n \n def arg_to_iter(arg):\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n@@ -15,7 +16,7 @@ def arg_to_iter(arg):\n     \"\"\"\n     if arg is None:\n         return []\n-    elif not isinstance(arg, dict) and hasattr(arg, '__iter__'):\n+    elif not isinstance(arg, dict) and not isinstance(arg, BaseItem) and hasattr(arg, '__iter__'):\n         return arg\n     else:\n         return [arg]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a6b80259d246b52472fddc1b4fb80155317ce78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 2272 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -233,6 +233,7 @@ class ExecutionEngine(object):\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n+        print \">>> spider_idle!\"\n         res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n             spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n\n@@ -3,7 +3,7 @@ from scrapy.utils import signal\n \n class SignalManager(object):\n \n-    def __init__(self, sender=dispatcher.Anonymous):\n+    def __init__(self, sender=dispatcher.Any):\n         self.sender = sender\n \n     def connect(self, *a, **kw):\n\n@@ -16,7 +16,7 @@ def arg_to_iter(arg):\n     \"\"\"\n     if arg is None:\n         return []\n-    elif not isinstance(arg, dict) and not isinstance(arg, BaseItem) and hasattr(arg, '__iter__'):\n+    elif not isinstance(arg, (dict, BaseItem)) and hasattr(arg, '__iter__'):\n         return arg\n     else:\n         return [arg]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f3a509d44de6a991552e2b1e214d05b69766488", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2025 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -233,7 +233,6 @@ class ExecutionEngine(object):\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n         \"\"\"\n-        print \">>> spider_idle!\"\n         res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n             spider=spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n\n@@ -3,7 +3,7 @@ from scrapy.utils import signal\n \n class SignalManager(object):\n \n-    def __init__(self, sender=dispatcher.Any):\n+    def __init__(self, sender=dispatcher.Anonymous):\n         self.sender = sender\n \n     def connect(self, *a, **kw):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5d944fa44f55159a11716c566d96f8c9dabb283", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 17 | Churn Cumulative: 547 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from scrapy.extension import ExtensionManager\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n+from scrapy.settings import overridden_settings\n from scrapy import log, signals\n \n \n@@ -33,6 +34,8 @@ class Crawler(object):\n         if self.configured:\n             return\n         self.configured = True\n+        d = dict(overridden_settings(self.settings))\n+        log.msg(format=\"Overridden settings: %(settings)r\", settings=d, level=log.DEBUG)\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n\n@@ -57,3 +57,17 @@ class CrawlerSettings(Settings):\n \n     def __str__(self):\n         return \"<CrawlerSettings module=%r>\" % self.settings_module\n+\n+\n+def iter_default_settings():\n+    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n+    for name in dir(default_settings):\n+        if name.isupper():\n+            yield name, getattr(default_settings, name)\n+\n+def overridden_settings(settings):\n+    \"\"\"Return a dict of the settings that have been overridden\"\"\"\n+    for name, defvalue in iter_default_settings():\n+        value = settings[name]\n+        if not isinstance(defvalue, dict) and value != defvalue:\n+            yield name, value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81111dd39a0e1bc3b40997450777356e262a7144", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 575 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,6 @@ Scrapy Shell\n \n See documentation in docs/topics/shell.rst\n \"\"\"\n-\n import signal\n \n from twisted.internet import reactor, threads\n@@ -20,10 +19,12 @@ from scrapy.utils.response import open_in_browser\n from scrapy.utils.console import start_python_console\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, HtmlResponse, XmlResponse\n+from scrapy.exceptions import IgnoreRequest\n+\n \n class Shell(object):\n \n-    relevant_classes = (BaseSpider, Request, Response, BaseItem, \\\n+    relevant_classes = (BaseSpider, Request, Response, BaseItem,\n         XPathSelector, Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n@@ -63,7 +64,7 @@ class Shell(object):\n         if self.spider:\n             return self.spider\n         if spider is None:\n-            spider = create_spider_for_request(self.crawler.spiders, request, \\\n+            spider = create_spider_for_request(self.crawler.spiders, request,\n                 BaseSpider('default'), log_multiple=True)\n         spider.set_crawler(self.crawler)\n         self.crawler.engine.open_spider(spider, close_if_idle=False)\n@@ -79,8 +80,11 @@ class Shell(object):\n             request = Request(url, dont_filter=True)\n             request.meta['handle_httpstatus_all'] = True\n         response = None\n-        response, spider = threads.blockingCallFromThread(reactor, \\\n-            self._schedule, request, spider)\n+        try:\n+            response, spider = threads.blockingCallFromThread(\n+                reactor, self._schedule, request, spider)\n+        except IgnoreRequest:\n+            pass\n         self.populate_vars(response, request, spider)\n \n     def populate_vars(self, response=None, request=None, spider=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9261f6c54a56167f1998d3c8cab3875b6c7c7c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 518 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import sys\n import optparse\n import cProfile\n import inspect\n+import pkg_resources\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n@@ -30,8 +31,19 @@ def _get_commands_from_module(module, inproject):\n             d[cmdname] = cmd()\n     return d\n \n+def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n+    cmds = {}\n+    for entry_point in pkg_resources.iter_entry_points(group):\n+        obj = entry_point.load()\n+        if inspect.isclass(obj):\n+            cmds[entry_point.name] = obj()\n+        else:\n+            raise Exception(\"Invalid entry point %s\" % entry_point.name)\n+    return cmds\n+\n def _get_commands_dict(settings, inproject):\n     cmds = _get_commands_from_module('scrapy.commands', inproject)\n+    cmds.update(_get_commands_from_entry_points(inproject))\n     cmds_module = settings['COMMANDS_MODULE']\n     if cmds_module:\n         cmds.update(_get_commands_from_module(cmds_module, inproject))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b48ec1dce4c02c36ad6037dc34355f0c7705c807", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 367 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,6 @@ library.\n Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n-\n import urlparse\n import urllib\n import cgi\n@@ -13,6 +12,7 @@ import cgi\n from w3lib.url import *\n from scrapy.utils.python import unicode_to_str\n \n+\n def url_is_from_any_domain(url, domains):\n     \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n     host = parse_url(url).netloc\n@@ -22,15 +22,18 @@ def url_is_from_any_domain(url, domains):\n     else:\n         return False\n \n+\n def url_is_from_spider(url, spider):\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    return url_is_from_any_domain(url, [spider.name] + \\\n-        getattr(spider, 'allowed_domains', []))\n+    return url_is_from_any_domain(url,\n+        [spider.name] + list(getattr(spider, 'allowed_domains', [])))\n+\n \n def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n \n-def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n+\n+def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n         encoding=None):\n     \"\"\"Canonicalize the given url by applying the following procedures:\n \n@@ -70,6 +73,7 @@ def parse_url(url, encoding=None):\n     return url if isinstance(url, urlparse.ParseResult) else \\\n         urlparse.urlparse(unicode_to_str(url, encoding))\n \n+\n def escape_ajax(url):\n     \"\"\"\n     Return the crawleable url according to:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a027da0a2c3981eac38bb61a90274afd8cee6235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 28 | Churn Cumulative: 352 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.7857142857142857\n\nDIFF:\n@@ -53,6 +53,8 @@ class Command(ScrapyCommand):\n             help=\"the version to deploy. Defaults to current timestamp\")\n         parser.add_option(\"-l\", \"--list-targets\", action=\"store_true\", \\\n             help=\"list available targets\")\n+        parser.add_option(\"-d\", \"--debug\", action=\"store_true\",\n+            help=\"debug mode (do not remove build dir)\")\n         parser.add_option(\"-L\", \"--list-projects\", metavar=\"TARGET\", \\\n             help=\"list available projects on TARGET\")\n         parser.add_option(\"--egg\", metavar=\"FILE\",\n@@ -93,11 +95,14 @@ class Command(ScrapyCommand):\n                 _log(\"Using egg: %s\" % opts.egg)\n                 egg = opts.egg\n             else:\n-                _log(\"Building egg of %s-%s\" % (project, version))\n+                _log(\"Packing version %s\" % version)\n                 egg, tmpdir = _build_egg()\n             _upload_egg(target, egg, project, version)\n \n         if tmpdir:\n+            if opts.debug:\n+                _log(\"Output dir not removed: %s\" % tmpdir)\n+            else:\n                 shutil.rmtree(tmpdir)\n \n def _log(message):\n@@ -148,10 +153,16 @@ def _get_version(target, opts):\n     version = opts.version or target.get('version')\n     if version == 'HG':\n         p = Popen(['hg', 'tip', '--template', '{rev}'], stdout=PIPE)\n-        return 'r%s' % p.communicate()[0]\n+        d = 'r%s' % p.communicate()[0]\n+        p = Popen(['hg', 'branch'], stdout=PIPE)\n+        b = p.communicate()[0].strip('\\n')\n+        return '%s-%s' % (d, b)\n     elif version == 'GIT':\n         p = Popen(['git', 'describe', '--always'], stdout=PIPE)\n-        return '%s' % p.communicate()[0].strip('\\n')\n+        d = p.communicate()[0].strip('\\n')\n+        p = Popen(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], stdout=PIPE)\n+        b = p.communicate()[0].strip('\\n')\n+        return '%s-%s' % (d, b)\n     elif version:\n         return version\n     else:\n@@ -173,7 +184,7 @@ def _upload_egg(target, eggpath, project, version):\n     }\n     req = urllib2.Request(url, body, headers)\n     _add_auth_header(req, target)\n-    _log(\"Deploying %s-%s to %s\" % (project, version, url))\n+    _log('Deploying to project \"%s\" in %s' % (project, url))\n     _http_post(req)\n \n def _add_auth_header(request, target):\n@@ -205,9 +216,12 @@ def _build_egg():\n     if not os.path.exists('setup.py'):\n         settings = get_config().get('settings', 'default')\n         _create_default_setup_py(settings=settings)\n-    d = tempfile.mkdtemp()\n-    f = tempfile.TemporaryFile(dir=d)\n-    retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)\n+    d = tempfile.mkdtemp(prefix=\"scrapydeploy-\")\n+    o = open(os.path.join(d, \"stdout\"), \"wb\")\n+    e = open(os.path.join(d, \"stderr\"), \"wb\")\n+    retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=o, stderr=e)\n+    o.close()\n+    e.close()\n     egg = glob.glob(os.path.join(d, '*.egg'))[0]\n     return egg, d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a613e15154970dde12455ede88bee19f3b437d88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 417 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,6 +48,16 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))\n \n+        spider = BaseSpider(name='example.com', allowed_domains=set(('example.com', 'example.net')))\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n+\n+        spider = BaseSpider(name='example.com', allowed_domains=('example.com', 'example.net'))\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n+        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))\n+\n+        spider = BaseSpider(name='example.com', allowed_domains={'example.org': None})\n+        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', spider))\n+\n     def test_url_is_from_spider_with_allowed_domains_class_attributes(self):\n         class MySpider(BaseSpider):\n             name = 'example.com'\n@@ -59,6 +69,16 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n+        class MySetSpider(BaseSpider):\n+            name = 'example.com'\n+            allowed_domains = set(('example.org', 'example.net'))\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySetSpider))\n+        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySetSpider))\n+        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', MySetSpider))\n+        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', MySetSpider))\n+        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySetSpider))\n+        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySetSpider))\n+\n     def test_canonicalize_url(self):\n         # simplest case\n         self.assertEqual(canonicalize_url(\"http://www.example.com/\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1514b3b5db1183c97c2379cfc1b2a1710ba0f598", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 423 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,11 @@\n import unittest\n+\n from scrapy.spider import BaseSpider\n-from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url, url_has_any_extension\n-from scrapy.linkextractor import IGNORED_EXTENSIONS\n+from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url\n \n __doctests__ = ['scrapy.utils.url']\n \n+\n class UrlUtilsTest(unittest.TestCase):\n \n     def test_url_is_from_any_domain(self):\n@@ -181,4 +182,3 @@ class UrlUtilsTest(unittest.TestCase):\n \n if __name__ == \"__main__\":\n     unittest.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5828179c5f76db0b98a44db3702a3465122e1159", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 439 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -54,15 +54,11 @@ class UrlUtilsTest(unittest.TestCase):\n \n         spider = BaseSpider(name='example.com', allowed_domains=('example.com', 'example.net'))\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n-        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))\n-\n-        spider = BaseSpider(name='example.com', allowed_domains={'example.org': None})\n-        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', spider))\n \n     def test_url_is_from_spider_with_allowed_domains_class_attributes(self):\n         class MySpider(BaseSpider):\n             name = 'example.com'\n-            allowed_domains = ['example.org', 'example.net']\n+            allowed_domains = ('example.org', 'example.net')\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n         self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))\n         self.assertTrue(url_is_from_spider('http://example.com/some/page.html', MySpider))\n@@ -70,16 +66,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n-        class MySetSpider(BaseSpider):\n-            name = 'example.com'\n-            allowed_domains = set(('example.org', 'example.net'))\n-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySetSpider))\n-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySetSpider))\n-        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', MySetSpider))\n-        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', MySetSpider))\n-        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySetSpider))\n-        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySetSpider))\n-\n     def test_canonicalize_url(self):\n         # simplest case\n         self.assertEqual(canonicalize_url(\"http://www.example.com/\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#296db1dc09de0d6909e27f3450da9948ec2798a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 395 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,6 +46,7 @@ class Scheduler(object):\n \n     def enqueue_request(self, request):\n         if not request.dont_filter and self.df.request_seen(request):\n+            self.df.log(request, self.spider)\n             return\n         dqok = self._dqpush(request)\n         if dqok:\n\n@@ -1,6 +1,7 @@\n import os\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.job import job_dir\n+from scrapy import log\n \n \n class BaseDupeFilter(object):\n@@ -18,6 +19,8 @@ class BaseDupeFilter(object):\n     def close(self, reason): # can return a deferred\n         pass\n \n+    def log(self, request, spider): # log that a request has been filtered\n+        pass\n \n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n@@ -25,6 +28,7 @@ class RFPDupeFilter(BaseDupeFilter):\n     def __init__(self, path=None):\n         self.file = None\n         self.fingerprints = set()\n+        self.logdupes = True\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n@@ -44,3 +48,9 @@ class RFPDupeFilter(BaseDupeFilter):\n     def close(self, reason):\n         if self.file:\n             self.file.close()\n+\n+    def log(self, request, spider):\n+        if self.logdupes:\n+            fmt = \"Filtered duplicate request: %(request)s - no more duplicates will be shown (see DUPEFILTER_CLASS)\"\n+            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n+            self.logdupes = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2e9d031f90e2976340be41b515c0f83a4fca745", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 1150 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -555,17 +555,6 @@ class FormRequestTest(RequestTest):\n         fs = _qs(req)\n         self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})\n \n-    def test_from_response_input_hidden(self):\n-        res = _buildresponse(\n-            '''<form>\n-            <input type=\"hidden\" name=\"i1\" value=\"i1v1\">\n-            <input type=\"hidden\" name=\"i2\">\n-            <input type=\"hidden\">\n-            </form>''')\n-        req = self.request_class.from_response(res)\n-        fs = _qs(req)\n-        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})\n-\n     def test_from_response_input_textarea(self):\n         res = _buildresponse(\n             '''<form>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f043bba04948d514041f37f4fb99f5fd3b3eff2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 1780 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,7 +36,8 @@ class FormRequest(Request):\n         form = _get_form(response, formname, formnumber, formxpath)\n         formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n         url = form.action or form.base_url\n-        return cls(url, method=form.method, formdata=formdata, **kwargs)\n+        method = kwargs.pop('method', form.method)\n+        return cls(url, method=method, formdata=formdata, **kwargs)\n \n \n def _urlencode(seq, enc):\n\n@@ -268,6 +268,16 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs['one'], ['1'])\n         self.assertEqual(fs['two'], ['2'])\n \n+    def test_from_response_override_method(self):\n+        response = _buildresponse(\n+                '''<html><body>\n+                <form action=\"/app\"></form>\n+                </body></html>''')\n+        request = FormRequest.from_response(response)\n+        self.assertEqual(request.method, 'GET')\n+        request = FormRequest.from_response(response, method='POST')\n+        self.assertEqual(request.method, 'POST')\n+\n     def test_from_response_submit_first_clickable(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"get.php\" method=\"GET\">\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e630126b827cf6769064df2e7bcddea79bfe5122", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 358 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.5\n\nDIFF:\n@@ -97,7 +97,8 @@ class Command(ScrapyCommand):\n             else:\n                 _log(\"Packing version %s\" % version)\n                 egg, tmpdir = _build_egg()\n-            _upload_egg(target, egg, project, version)\n+            if not _upload_egg(target, egg, project, version):\n+                self.exitcode = 1\n \n         if tmpdir:\n             if opts.debug:\n@@ -185,7 +186,7 @@ def _upload_egg(target, eggpath, project, version):\n     req = urllib2.Request(url, body, headers)\n     _add_auth_header(req, target)\n     _log('Deploying to project \"%s\" in %s' % (project, url))\n-    _http_post(req)\n+    return _http_post(req)\n \n def _add_auth_header(request, target):\n     if 'username' in target:\n@@ -204,6 +205,7 @@ def _http_post(request):\n         f = urllib2.urlopen(request)\n         _log(\"Server response (%s):\" % f.code)\n         print f.read()\n+        return True\n     except urllib2.HTTPError, e:\n         _log(\"Deploy failed (%s):\" % e.code)\n         print e.read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#58975abeabaecec7d4895a1b4d1a52efa2182433", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 34 | Churn Cumulative: 3055 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -123,7 +123,7 @@ class ExecutionEngine(object):\n         return not self.running \\\n             or slot.closing \\\n             or self.downloader.needs_backout() \\\n-            or self.scraper.slots[spider].needs_backout()\n+            or self.scraper.slot.needs_backout()\n \n     def _next_request_from_scheduler(self, spider):\n         slot = self.slots[spider]\n@@ -151,8 +151,7 @@ class ExecutionEngine(object):\n         return d\n \n     def spider_is_idle(self, spider):\n-        scraper_idle = spider in self.scraper.slots \\\n-            and self.scraper.slots[spider].is_idle()\n+        scraper_idle = self.scraper.slot.is_idle()\n         pending = self.slots[spider].scheduler.has_pending_requests()\n         downloading = bool(self.downloader.active)\n         idle = scraper_idle and not (pending or downloading)\n\n@@ -60,7 +60,7 @@ class Slot(object):\n class Scraper(object):\n \n     def __init__(self, crawler):\n-        self.slots = {}\n+        self.slot = None\n         self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n         itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n         self.itemproc = itemproc_cls.from_crawler(crawler)\n@@ -72,14 +72,12 @@ class Scraper(object):\n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n-        assert spider not in self.slots, \"Spider already opened: %s\" % spider\n-        self.slots[spider] = Slot()\n+        self.slot = Slot()\n         yield self.itemproc.open_spider(spider)\n \n     def close_spider(self, spider):\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n-        assert spider in self.slots, \"Spider not opened: %s\" % spider\n-        slot = self.slots[spider]\n+        slot = self.slot\n         slot.closing = defer.Deferred()\n         slot.closing.addCallback(self.itemproc.close_spider)\n         self._check_if_closing(spider, slot)\n@@ -87,15 +85,14 @@ class Scraper(object):\n \n     def is_idle(self):\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n-        return not self.slots\n+        return not self.slot\n \n     def _check_if_closing(self, spider, slot):\n         if slot.closing and slot.is_idle():\n-            del self.slots[spider]\n             slot.closing.callback(spider)\n \n     def enqueue_scrape(self, response, request, spider):\n-        slot = self.slots[spider]\n+        slot = self.slot\n         dfd = slot.add_response_request(response, request)\n         def finish_scraping(_):\n             slot.finish_response(response, request)\n@@ -169,7 +166,7 @@ class Scraper(object):\n                 spider=spider)\n             self.crawler.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n-            self.slots[spider].itemproc_size += 1\n+            self.slot.itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n             dfd.addBoth(self._itemproc_finished, output, response, spider)\n             return dfd\n@@ -196,7 +193,7 @@ class Scraper(object):\n     def _itemproc_finished(self, output, item, response, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n-        self.slots[spider].itemproc_size -= 1\n+        self.slot.itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n\n@@ -11,7 +11,7 @@ def get_engine_status(engine):\n         \"len(engine.downloader.slots)\",\n         \"len(engine.downloader.active)\",\n         \"engine.scraper.is_idle()\",\n-        \"len(engine.scraper.slots)\",\n+        \"1\",\n     ]\n     spider_tests = [\n         \"engine.spider_is_idle(spider)\",\n@@ -19,11 +19,11 @@ def get_engine_status(engine):\n         \"len(engine.slots[spider].inprogress)\",\n         \"len(engine.slots[spider].scheduler.dqs or [])\",\n         \"len(engine.slots[spider].scheduler.mqs)\",\n-        \"len(engine.scraper.slots[spider].queue)\",\n-        \"len(engine.scraper.slots[spider].active)\",\n-        \"engine.scraper.slots[spider].active_size\",\n-        \"engine.scraper.slots[spider].itemproc_size\",\n-        \"engine.scraper.slots[spider].needs_backout()\",\n+        \"len(engine.scraper.slot.queue)\",\n+        \"len(engine.scraper.slot.active)\",\n+        \"engine.scraper.slot.active_size\",\n+        \"engine.scraper.slot.itemproc_size\",\n+        \"engine.scraper.slot.needs_backout()\",\n     ]\n \n     status = {'global': [], 'spiders': {}}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b347c14b5fc06ae450619b09ad08718421dd97d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 230 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,6 @@ def get_engine_status(engine):\n         \"len(engine.downloader.slots)\",\n         \"len(engine.downloader.active)\",\n         \"engine.scraper.is_idle()\",\n-        \"1\",\n     ]\n     spider_tests = [\n         \"engine.spider_is_idle(spider)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
