{"custom_id": "keras#8b8ffe0ea4b9739e8b4ad397cc5e232beb24cad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 932 | Contributors (this commit): 6 | Commits (past 90d): 29 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -324,12 +324,14 @@ legacy_deconv2d_support = generate_legacy_interface(\n \n \n def conv3d_args_preprocessor(args, kwargs):\n+    converted = []\n     if len(args) > 5:\n         raise TypeError('Layer can receive at most 4 positional arguments.')\n     if len(args) == 5:\n         if isinstance(args[2], int) and isinstance(args[3], int) and isinstance(args[4], int):\n             kernel_size = (args[2], args[3], args[4])\n             args = [args[0], args[1], kernel_size]\n+            converted.append(('kernel_size', 'kernel_dim*'))\n     elif len(args) == 4 and isinstance(args[3], int):\n         if isinstance(args[2], int) and isinstance(args[3], int):\n             new_keywords = ['padding', 'strides', 'data_format']\n@@ -344,19 +346,22 @@ def conv3d_args_preprocessor(args, kwargs):\n         if 'kernel_dim3' in kwargs:\n             kernel_size = (args[2], args[3], kwargs.pop('kernel_dim3'))\n             args = [args[0], args[1], kernel_size]\n+            converted.append(('kernel_size', 'kernel_dim*'))\n     elif len(args) == 3:\n         if 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n             kernel_size = (args[2],\n                            kwargs.pop('kernel_dim2'),\n                            kwargs.pop('kernel_dim3'))\n             args = [args[0], args[1], kernel_size]\n+            converted.append(('kernel_size', 'kernel_dim*'))\n     elif len(args) == 2:\n         if 'kernel_dim1' in kwargs and 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n             kernel_size = (kwargs.pop('kernel_dim1'),\n                            kwargs.pop('kernel_dim2'),\n                            kwargs.pop('kernel_dim3'))\n             args = [args[0], args[1], kernel_size]\n-    return args, kwargs, [('kernel_size', 'kernel_dim*')]\n+            converted.append(('kernel_size', 'kernel_dim*'))\n+    return args, kwargs, converted\n \n legacy_conv3d_support = generate_legacy_interface(\n     allowed_positional_args=['filters', 'kernel_size'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa4aba7a9be78c0cffd80e6e60c344517b46da5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 28/28 | Churn Δ: 128 | Churn Cumulative: 7538 | Contributors (this commit): 27 | Commits (past 90d): 44 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,6 +17,7 @@ from .. import backend as K\n from .. import initializers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n from ..utils.layer_utils import print_summary as print_layer_summary\n+from ..utils import conv_utils\n \n try:\n     import h5py\n@@ -2710,6 +2711,106 @@ def save_weights_to_hdf5_group(f, layers):\n                 param_dset[:] = val\n \n \n+def preprocess_weights_for_loading(layer, weights,\n+                                   original_keras_version=None,\n+                                   original_backend=None):\n+    if original_keras_version == '1':\n+        if layer.__class__.__name__ == 'Conv1D':\n+            weights[0] = weights[0][:, 0, :, :]\n+            shape = weights[0].shape\n+            # Handle Keras 1.1 format\n+            if shape[:2] != (layer.filter_length, 1) or shape[3] != layer.filters:\n+                # Legacy shape:\n+                # (filters, input_dim, filter_length, 1)\n+                assert shape[0] == layer.filters and shape[2:] == (layer.kernel_size[0], 1)\n+                weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n+\n+        if layer.__class__.__name__ == 'Conv2D':\n+            if layer.data_format == 'channels_first':\n+                # old: (filters, stack_size, kernel_rows, kernel_cols)\n+                # new: (kernel_rows, kernel_cols, stack_size, filters)\n+                weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n+\n+        if layer.__class__.__name__ == 'Conv2DTranspose':\n+            if layer.data_format == 'channels_last':\n+                # old: (kernel_rows, kernel_cols, stack_size, filters)\n+                # new: (kernel_rows, kernel_cols, filters, stack_size)\n+                weights[0] = np.transpose(weights[0], (0, 1, 3, 2))\n+            if layer.data_format == 'channels_first':\n+                # old: (filters, stack_size, kernel_rows, kernel_cols)\n+                # new: (kernel_rows, kernel_cols, filters, stack_size)\n+                weights[0] = np.transpose(weights[0], (2, 3, 0, 1))\n+\n+        if layer.__class__.__name__ == 'Conv3D':\n+            if layer.data_format == 'channels_first':\n+                # old: (filters, stack_size, ...)\n+                # new: (..., stack_size, filters)\n+                weights[0] = np.transpose(weights[0], (2, 3, 4, 1, 0))\n+\n+        if layer.__class__.__name__ == 'GRU':\n+            if len(weights) == 9:\n+                kernel = np.concatenate([weights[0],\n+                                         weights[3],\n+                                         weights[6]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[4],\n+                                                   weights[7]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[5],\n+                                       weights[8]], axis=-1)\n+                weights = [kernel, recurrent_kernel, bias]\n+\n+        if layer.__class__.__name__ == 'LSTM':\n+            if len(weights) == 12:\n+                kernel = np.concatenate([weights[0],\n+                                         weights[3],\n+                                         weights[6],\n+                                         weights[9]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[4],\n+                                                   weights[7],\n+                                                   weights[10]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[5],\n+                                       weights[8],\n+                                       weights[11]], axis=-1)\n+                weights = [kernel, recurrent_kernel, bias]\n+\n+        if layer.__class__.__name__ == 'ConvLSTM2D':\n+            if len(weights) == 12:\n+                kernel = np.concatenate([weights[0],\n+                                         weights[3],\n+                                         weights[6],\n+                                         weights[9]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[4],\n+                                                   weights[7],\n+                                                   weights[10]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[5],\n+                                       weights[8],\n+                                       weights[11]], axis=-1)\n+                if layer.data_format == 'channels_first':\n+                    # old: (filters, stack_size, kernel_rows, kernel_cols)\n+                    # new: (kernel_rows, kernel_cols, stack_size, filters)\n+                    kernel = np.transpose(kernel, (2, 3, 1, 0))\n+                    recurrent_kernel = np.transpose(recurrent_kernel,\n+                                                    (2, 3, 1, 0))\n+                weights = [kernel, recurrent_kernel, bias]\n+\n+    if original_backend and K.backend() != original_backend:\n+        conv_layers = ['Conv1D',\n+                       'Conv2D',\n+                       'Conv3D',\n+                       'Conv2DTranspose']\n+        if layer.__class__.__name__ in conv_layers:\n+            weights[0] = conv_utils.convert_kernel(weights[0])\n+        if layer.__class__.__name__ == 'ConvLSTM2D':\n+            weights[0] = conv_utils.convert_kernel(weights[0])\n+            weights[1] = conv_utils.convert_kernel(weights[1])\n+    return weights\n+\n+\n def load_weights_from_hdf5_group(f, layers):\n     \"\"\"Implements topological (order-based) weight loading.\n \n@@ -2721,6 +2822,15 @@ def load_weights_from_hdf5_group(f, layers):\n         ValueError: in case of mismatch between provided layers\n             and weights file.\n     \"\"\"\n+    if 'keras_version' in f.attrs:\n+        original_keras_version = f.attrs['keras_version']\n+    else:\n+        original_keras_version = '1'\n+    if 'backend' in f.attrs:\n+        original_backend = f.attrs['backend']\n+    else:\n+        original_backend = None\n+\n     filtered_layers = []\n     for layer in layers:\n         weights = layer.weights\n@@ -2750,6 +2860,10 @@ def load_weights_from_hdf5_group(f, layers):\n         weight_values = [g[weight_name] for weight_name in weight_names]\n         layer = filtered_layers[k]\n         symbolic_weights = layer.weights\n+        weight_values = preprocess_weights_for_loading(layer,\n+                                                       weight_values,\n+                                                       original_keras_version,\n+                                                       original_backend)\n         if len(weight_values) != len(symbolic_weights):\n             raise ValueError('Layer #' + str(k) +\n                              ' (named \"' + layer.name +\n@@ -2780,6 +2894,15 @@ def load_weights_from_hdf5_group_by_name(f, layers):\n         ValueError: in case of mismatch between provided layers\n             and weights file.\n     \"\"\"\n+    if 'keras_version' in f.attrs:\n+        original_keras_version = f.attrs['keras_version']\n+    else:\n+        original_keras_version = '1'\n+    if 'backend' in f.attrs:\n+        original_backend = f.attrs['backend']\n+    else:\n+        original_backend = None\n+\n     # New file format.\n     layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n \n@@ -2799,6 +2922,11 @@ def load_weights_from_hdf5_group_by_name(f, layers):\n \n         for layer in index.get(name, []):\n             symbolic_weights = layer.weights\n+            weight_values = preprocess_weights_for_loading(\n+                layer,\n+                weight_values,\n+                original_keras_version,\n+                original_backend)\n             if len(weight_values) != len(symbolic_weights):\n                 raise ValueError('Layer #' + str(k) +\n                                  ' (named \"' + layer.name +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e71cbccc2a91c338db010802528bd9ce8f8f14bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7542 | Contributors (this commit): 27 | Commits (past 90d): 45 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -2716,14 +2716,14 @@ def preprocess_weights_for_loading(layer, weights,\n                                    original_backend=None):\n     if original_keras_version == '1':\n         if layer.__class__.__name__ == 'Conv1D':\n-            weights[0] = weights[0][:, 0, :, :]\n             shape = weights[0].shape\n             # Handle Keras 1.1 format\n-            if shape[:2] != (layer.filter_length, 1) or shape[3] != layer.filters:\n+            if shape[:2] != (layer.kernel_size[0], 1) or shape[3] != layer.filters:\n                 # Legacy shape:\n                 # (filters, input_dim, filter_length, 1)\n                 assert shape[0] == layer.filters and shape[2:] == (layer.kernel_size[0], 1)\n                 weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n+            weights[0] = weights[0][:, 0, :, :]\n \n         if layer.__class__.__name__ == 'Conv2D':\n             if layer.data_format == 'channels_first':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5a0737a01b4fd2ea24ff2fa5051f1e1fce7910f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1144 | Contributors (this commit): 20 | Commits (past 90d): 14 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -150,7 +150,7 @@ class BatchNormalization(Layer):\n                                                      self.momentum)],\n                             inputs)\n \n-            def normalize_in_training():\n+            def normalize_inference():\n                 if needs_broadcasting:\n                     # In this case we must explictly broadcast all parameters.\n                     broadcast_moving_mean = K.reshape(self.moving_mean,\n@@ -183,8 +183,8 @@ class BatchNormalization(Layer):\n                         epsilon=self.epsilon)\n \n         # Pick the normalized form corresponding to the training phase.\n-        return K.in_train_phase(normalize_in_training,\n-                                normed,\n+        return K.in_train_phase(normed,\n+                                normalize_inference,\n                                 training=training)\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d04d05442c1596c4ad0547a0854755be6dd24fb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1822 | Contributors (this commit): 6 | Commits (past 90d): 54 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -145,7 +145,11 @@ legacy_recurrent_support = generate_legacy_interface(\n                  ('b_regularizer', 'bias_regularizer'),\n                  ('U_regularizer', 'recurrent_regularizer'),\n                  ('dropout_W', 'dropout'),\n-                 ('dropout_U', 'recurrent_dropout')],\n+                 ('dropout_U', 'recurrent_dropout'),\n+                 ('consume_less', 'implementation')],\n+    value_conversions={'consume_less': {'cpu': 0,\n+                                        'mem': 1,\n+                                        'gpu': 2}},\n     preprocessor=lstm_args_preprocessor)\n \n legacy_gaussiandropout_support = generate_legacy_interface(\n\n@@ -108,6 +108,14 @@ def test_lstm_legacy_interface():\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n+    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d', consume_less='mem')\n+    new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=1)\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n+    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d', consume_less='gpu')\n+    new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=2)\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n+\n     old_layer = keras.layers.LSTM(2, init='normal',\n                                   inner_init='glorot_uniform',\n                                   forget_bias_init='one',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a00eef21c90bee7a2dbaa62d06d41fc883c85d8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 13 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 7556 | Contributors (this commit): 27 | Commits (past 90d): 46 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -2762,33 +2762,35 @@ def preprocess_weights_for_loading(layer, weights,\n \n         if layer.__class__.__name__ == 'LSTM':\n             if len(weights) == 12:\n+                # old: i, c, f, o\n+                # new: i, f, c, o\n                 kernel = np.concatenate([weights[0],\n-                                         weights[3],\n                                          weights[6],\n+                                         weights[3],\n                                          weights[9]], axis=-1)\n                 recurrent_kernel = np.concatenate([weights[1],\n-                                                   weights[4],\n                                                    weights[7],\n+                                                   weights[4],\n                                                    weights[10]], axis=-1)\n                 bias = np.concatenate([weights[2],\n-                                       weights[5],\n                                        weights[8],\n+                                       weights[5],\n                                        weights[11]], axis=-1)\n                 weights = [kernel, recurrent_kernel, bias]\n \n         if layer.__class__.__name__ == 'ConvLSTM2D':\n             if len(weights) == 12:\n                 kernel = np.concatenate([weights[0],\n-                                         weights[3],\n                                          weights[6],\n+                                         weights[3],\n                                          weights[9]], axis=-1)\n                 recurrent_kernel = np.concatenate([weights[1],\n-                                                   weights[4],\n                                                    weights[7],\n+                                                   weights[4],\n                                                    weights[10]], axis=-1)\n                 bias = np.concatenate([weights[2],\n-                                       weights[5],\n                                        weights[8],\n+                                       weights[5],\n                                        weights[11]], axis=-1)\n                 if layer.data_format == 'channels_first':\n                     # old: (filters, stack_size, kernel_rows, kernel_cols)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2082bafd183eb6fea4778701f4c2c047970014f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 6424 | Contributors (this commit): 34 | Commits (past 90d): 25 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -183,7 +183,7 @@ class Recurrent(Layer):\n         self.go_backwards = go_backwards\n         self.stateful = stateful\n         self.unroll = unroll\n-        self.implementation = 0\n+        self.implementation = implementation\n         self.supports_masking = True\n         self.input_spec = InputSpec(ndim=3)\n         self.state_spec = None\n@@ -811,8 +811,8 @@ class GRU(Recurrent):\n                 x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n                 if self.use_bias:\n                     x_z = K.bias_add(x_z, self.bias_z)\n-                    x_r = K.bias_add(self.bias_r)\n-                    x_h = K.bias_add(self.bias_h)\n+                    x_r = K.bias_add(x_r, self.bias_r)\n+                    x_h = K.bias_add(x_r, self.bias_h)\n             else:\n                 raise ValueError('Unknown `implementation` mode.')\n             z = self.recurrent_activation(x_z + K.dot(h_tm1 * rec_dp_mask[0],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28c208deab339a12c4133f9859145afb5b37f151", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6426 | Contributors (this commit): 34 | Commits (past 90d): 26 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -166,7 +166,7 @@ class Recurrent(Layer):\n         a specific layer, or on your entire model.\n \n     # Note on specifying initial states in RNNs\n-        You can specify the initial state of RNN layers by calling theme with\n+        You can specify the initial state of RNN layers by calling them with\n         the keyword argument `initial_state`. The value of `initial_state`\n         should be a tensor or list of tensors representing the initial state\n         of the RNN layer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11107c5f2c0eb8eccd6818352cc4124885ffe7e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 58 | Churn Cumulative: 14038 | Contributors (this commit): 64 | Commits (past 90d): 88 | Contributors (cumulative): 73 | DMM Complexity: 1.0\n\nDIFF:\n@@ -142,6 +142,7 @@ class SpatialDropout1D(Dropout):\n         - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n+    @interfaces.legacy_spatialdropout1d_support\n     def __init__(self, rate, **kwargs):\n         super(SpatialDropout1D, self).__init__(rate, **kwargs)\n         self.input_spec = InputSpec(ndim=3)\n@@ -186,6 +187,7 @@ class SpatialDropout2D(Dropout):\n         - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n+    @interfaces.legacy_spatialdropoutNd_support\n     def __init__(self, rate, data_format=None, **kwargs):\n         super(SpatialDropout2D, self).__init__(rate, **kwargs)\n         if data_format is None:\n@@ -240,6 +242,7 @@ class SpatialDropout3D(Dropout):\n         - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n+    @interfaces.legacy_spatialdropoutNd_support\n     def __init__(self, rate, data_format=None, **kwargs):\n         super(SpatialDropout3D, self).__init__(rate, **kwargs)\n         if data_format is None:\n@@ -592,6 +595,7 @@ class Lambda(Layer):\n         (or auto-inferred when using TensorFlow).\n     \"\"\"\n \n+    @interfaces.legacy_lambda_support\n     def __init__(self, function, output_shape=None,\n                  mask=None, arguments=None, **kwargs):\n         super(Lambda, self).__init__(**kwargs)\n\n@@ -496,3 +496,18 @@ legacy_cropping3d_support = generate_legacy_interface(\n     value_conversions={'dim_ordering': {'tf': 'channels_last',\n                                         'th': 'channels_first',\n                                         'default': None}})\n+\n+legacy_spatialdropout1d_support = generate_legacy_interface(\n+    allowed_positional_args=['rate'],\n+    conversions=[('p', 'rate')])\n+\n+legacy_spatialdropoutNd_support = generate_legacy_interface(\n+    allowed_positional_args=['rate'],\n+    conversions=[('p', 'rate'),\n+                 ('dim_ordering', 'data_format')],\n+    value_conversions={'dim_ordering': {'tf': 'channels_last',\n+                                        'th': 'channels_first',\n+                                        'default': None}})\n+\n+legacy_lambda_support = generate_legacy_interface(\n+    allowed_positional_args=['function', 'output_shape'])\n\n@@ -757,5 +757,44 @@ def test_cropping3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n+@keras_test\n+def test_spatialdropout1d_legacy_interface():\n+    old_layer = keras.layers.SpatialDropout1D(p=0.6, name='sd1d')\n+    new_layer_1 = keras.layers.SpatialDropout1D(rate=0.6, name='sd1d')\n+    new_layer_2 = keras.layers.SpatialDropout1D(0.6, name='sd1d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n+\n+\n+@keras_test\n+def test_spatialdropout2d_legacy_interface():\n+    old_layer = keras.layers.SpatialDropout2D(p=0.5,\n+                                              dim_ordering='tf',\n+                                              name='sd2d')\n+    new_layer_1 = keras.layers.SpatialDropout2D(rate=0.5,\n+                                                data_format='channels_last',\n+                                                name='sd2d')\n+    new_layer_2 = keras.layers.SpatialDropout2D(0.5,\n+                                                data_format='channels_last',\n+                                                name='sd2d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n+\n+\n+@keras_test\n+def test_spatialdropout3d_legacy_interface():\n+    old_layer = keras.layers.SpatialDropout3D(p=0.5,\n+                                              dim_ordering='tf',\n+                                              name='sd3d')\n+    new_layer_1 = keras.layers.SpatialDropout3D(rate=0.5,\n+                                                data_format='channels_last',\n+                                                name='sd3d')\n+    new_layer_2 = keras.layers.SpatialDropout3D(0.5,\n+                                                data_format='channels_last',\n+                                                name='sd3d')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98af4eb39faa55a6f0d89e6fe375da9b985e34a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 14022 | Contributors (this commit): 54 | Commits (past 90d): 74 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2764,18 +2764,19 @@ def preprocess_weights_for_loading(layer, weights,\n             if len(weights) == 12:\n                 # old: i, c, f, o\n                 # new: i, f, c, o\n-                kernel = np.concatenate([weights[0],\n-                                         weights[6],\n-                                         weights[3],\n-                                         weights[9]], axis=-1)\n-                recurrent_kernel = np.concatenate([weights[1],\n-                                                   weights[7],\n-                                                   weights[4],\n-                                                   weights[10]], axis=-1)\n-                bias = np.concatenate([weights[2],\n-                                       weights[8],\n-                                       weights[5],\n-                                       weights[11]], axis=-1)\n+                wi, ui, bi, wc, uc, bc, wf, uf, bf, wo, uo, bo = weights\n+                kernel = np.concatenate([wi,\n+                                         wf,\n+                                         wc,\n+                                         wo], axis=-1)\n+                recurrent_kernel = np.concatenate([ui,\n+                                                   uf,\n+                                                   uc,\n+                                                   uo], axis=-1)\n+                bias = np.concatenate([bi,\n+                                       bf,\n+                                       bc,\n+                                       bo], axis=-1)\n                 weights = [kernel, recurrent_kernel, bias]\n \n         if layer.__class__.__name__ == 'ConvLSTM2D':\n\n@@ -781,7 +781,7 @@ class GRU(Recurrent):\n         dp_mask = states[1]  # dropout matrices for recurrent units\n         rec_dp_mask = states[2]\n \n-        if self.implementation == 1:\n+        if self.implementation == 2:\n             matrix_x = K.dot(inputs * dp_mask[0], self.kernel)\n             if self.use_bias:\n                 matrix_x = K.bias_add(matrix_x, self.bias)\n@@ -805,7 +805,7 @@ class GRU(Recurrent):\n                 x_z = inputs[:, :self.units]\n                 x_r = inputs[:, self.units: 2 * self.units]\n                 x_h = inputs[:, 2 * self.units:]\n-            elif self.implementation == 2:\n+            elif self.implementation == 1:\n                 x_z = K.dot(inputs * dp_mask[0], self.kernel_z)\n                 x_r = K.dot(inputs * dp_mask[1], self.kernel_r)\n                 x_h = K.dot(inputs * dp_mask[2], self.kernel_h)\n@@ -991,12 +991,13 @@ class LSTM(Recurrent):\n             self.bias = None\n \n         self.kernel_i = self.kernel[:, :self.units]\n-        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n         self.kernel_f = self.kernel[:, self.units: self.units * 2]\n-        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n         self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n-        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n         self.kernel_o = self.kernel[:, self.units * 3:]\n+\n+        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n+        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n+        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n         self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n \n         if self.use_bias:\n@@ -1071,7 +1072,7 @@ class LSTM(Recurrent):\n         dp_mask = states[2]\n         rec_dp_mask = states[3]\n \n-        if self.implementation == 1:\n+        if self.implementation == 2:\n             z = K.dot(inputs * dp_mask[0], self.kernel)\n             z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n             if self.use_bias:\n@@ -1092,7 +1093,7 @@ class LSTM(Recurrent):\n                 x_f = inputs[:, self.units: 2 * self.units]\n                 x_c = inputs[:, 2 * self.units: 3 * self.units]\n                 x_o = inputs[:, 3 * self.units:]\n-            elif self.implementation == 2:\n+            elif self.implementation == 1:\n                 x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n                 x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n                 x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7374442a1c1507051fb1f9ee1ef774c9a9654d8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 14055 | Contributors (this commit): 54 | Commits (past 90d): 76 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2764,19 +2764,18 @@ def preprocess_weights_for_loading(layer, weights,\n             if len(weights) == 12:\n                 # old: i, c, f, o\n                 # new: i, f, c, o\n-                wi, ui, bi, wc, uc, bc, wf, uf, bf, wo, uo, bo = weights\n-                kernel = np.concatenate([wi,\n-                                         wf,\n-                                         wc,\n-                                         wo], axis=-1)\n-                recurrent_kernel = np.concatenate([ui,\n-                                                   uf,\n-                                                   uc,\n-                                                   uo], axis=-1)\n-                bias = np.concatenate([bi,\n-                                       bf,\n-                                       bc,\n-                                       bo], axis=-1)\n+                kernel = np.concatenate([weights[0],\n+                                         weights[6],\n+                                         weights[3],\n+                                         weights[9]], axis=-1)\n+                recurrent_kernel = np.concatenate([weights[1],\n+                                                   weights[7],\n+                                                   weights[4],\n+                                                   weights[10]], axis=-1)\n+                bias = np.concatenate([weights[2],\n+                                       weights[8],\n+                                       weights[5],\n+                                       weights[11]], axis=-1)\n                 weights = [kernel, recurrent_kernel, bias]\n \n         if layer.__class__.__name__ == 'ConvLSTM2D':\n\n@@ -875,7 +875,7 @@ class LSTM(Recurrent):\n             (see [initializers](../initializers.md)).\n         unit_forget_bias: Boolean.\n             If True, add 1 to the bias of the forget gate at initialization.\n-            Use in combination with `bias_initializer=\"zeros\"`.\n+            Setting it to true will also force `bias_initializer=\"zeros\"`.\n             This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n         kernel_regularizer: Regularizer function applied to\n             the `kernel` weights matrix\n@@ -984,9 +984,9 @@ class LSTM(Recurrent):\n                                         regularizer=self.bias_regularizer,\n                                         constraint=self.bias_constraint)\n             if self.unit_forget_bias:\n-                self.bias += K.concatenate([K.zeros((self.units,)),\n-                                           K.ones((self.units,)),\n-                                           K.zeros((self.units * 2,))])\n+                bias_value = np.zeros((self.units * 4,))\n+                bias_value[self.units: self.units * 2] = 1.\n+                K.set_value(self.bias, bias_value)\n         else:\n             self.bias = None\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e289d71866b4782dbe752adce1f26ea895f5dd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1010 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -369,9 +369,9 @@ class ConvLSTM2D(ConvRecurrent2D):\n                                         regularizer=self.bias_regularizer,\n                                         constraint=self.bias_constraint)\n             if self.unit_forget_bias:\n-                self.bias += K.concatenate([K.zeros((self.filters,)),\n-                                           K.ones((self.filters,)),\n-                                           K.zeros((self.filters * 2,))])\n+                bias_value = np.zeros((self.filters * 4,))\n+                bias_value[self.filters: self.filters * 2] = 1.\n+                K.set_value(self.bias, bias_value)\n         else:\n             self.bias = None\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10d7e21efcf04bdb3438a809863a2fe728efe614", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 9/7 | Churn Δ: 57 | Churn Cumulative: 6744 | Contributors (this commit): 34 | Commits (past 90d): 35 | Contributors (cumulative): 36 | DMM Complexity: 0.7333333333333333\n\nDIFF:\n@@ -92,6 +92,8 @@ class Recurrent(Layer):\n             `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n         return_sequences: Boolean. Whether to return the last output\n             in the output sequence, or the full sequence.\n+        return_state: Boolean. Whether to return the last state\n+            in addition to the output.\n         go_backwards: Boolean (default False).\n             If True, process the input sequence backwards.\n         stateful: Boolean (default False). If True, the last state\n@@ -134,6 +136,9 @@ class Recurrent(Layer):\n         (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n \n     # Output shape\n+        - if `return_state`: a list of tensors. The first tensor is\n+            the ouput. The remaining tensors are the state, with shape\n+            `(batch_size, units)`.\n         - if `return_sequences`: 3D tensor with shape\n             `(batch_size, timesteps, units)`.\n         - else, 2D tensor with shape `(batch_size, units)`.\n@@ -173,6 +178,7 @@ class Recurrent(Layer):\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n+                 return_state=False,\n                  go_backwards=False,\n                  stateful=False,\n                  unroll=False,\n@@ -180,6 +186,7 @@ class Recurrent(Layer):\n                  **kwargs):\n         super(Recurrent, self).__init__(**kwargs)\n         self.return_sequences = return_sequences\n+        self.return_state = return_state\n         self.go_backwards = go_backwards\n         self.stateful = stateful\n         self.unroll = unroll\n@@ -193,16 +200,27 @@ class Recurrent(Layer):\n     def compute_output_shape(self, input_shape):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n+\n         if self.return_sequences:\n-            return (input_shape[0], input_shape[1], self.units)\n+            output_shape = (input_shape[0], input_shape[1], self.units)\n         else:\n-            return (input_shape[0], self.units)\n+            output_shape = (input_shape[0], self.units)\n+\n+        if self.return_state:\n+            state_shape = [(input_shape[0], self.units) for _ in self.states]\n+            return [output_shape] + state_shape\n+        else:\n+            return output_shape\n \n     def compute_mask(self, inputs, mask):\n-        if self.return_sequences:\n-            return mask\n+        if isinstance(mask, list):\n+            mask = mask[0]\n+        output_mask = mask if self.return_sequences else None\n+        if self.return_state:\n+            state_mask = [None for _ in self.states]\n+            return [output_mask] + state_mask\n         else:\n-            return None\n+            return output_mask\n \n     def step(self, inputs, states):\n         raise NotImplementedError\n@@ -308,9 +326,18 @@ class Recurrent(Layer):\n             outputs._uses_learning_phase = True\n \n         if self.return_sequences:\n-            return outputs\n+            output = outputs\n         else:\n-            return last_output\n+            output = last_output\n+\n+        if self.return_state:\n+            if not isinstance(states, (list, tuple)):\n+                states = [states]\n+            else:\n+                states =  list(states)\n+            return [output] + states\n+        else:\n+            return output\n \n     def reset_states(self, states_value=None):\n         if not self.stateful:\n\n@@ -218,5 +218,21 @@ def test_reset_states_with_values(layer_class):\n                                np.ones(K.int_shape(layer.states[0])),\n                                atol=1e-4)\n \n+\n+@rnn_test\n+def test_return_state(layer_class):\n+    num_states = 2 if layer_class is recurrent.LSTM else 1\n+\n+    inputs = Input(batch_shape=(num_samples, timesteps, embedding_dim))\n+    layer = layer_class(units, return_state=True, stateful=True)\n+    outputs = layer(inputs)\n+    output, state = outputs[0], outputs[1:]\n+    assert len(state) == num_states\n+    model = Model(inputs, state[0])\n+\n+    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n+    state = model.predict(inputs)\n+    np.testing.assert_allclose(K.eval(layer.states[0]), state, atol=1e-4)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#12907534f8506a666cdb0b3c095941b3b47d8589", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 6491 | Contributors (this commit): 34 | Commits (past 90d): 30 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -385,6 +385,7 @@ class Recurrent(Layer):\n \n     def get_config(self):\n         config = {'return_sequences': self.return_sequences,\n+                  'return_state': self.return_state,\n                   'go_backwards': self.go_backwards,\n                   'stateful': self.stateful,\n                   'unroll': self.unroll,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e071cd7df38a9ace26cf44a8be64fc6b3aec394", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 68/68 | Churn Δ: 1 | Churn Cumulative: 7607 | Contributors (this commit): 27 | Commits (past 90d): 49 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1587,6 +1587,7 @@ class Merge(Layer):\n \n     @classmethod\n     def from_config(cls, config):\n+        config = config.copy()\n         mode_type = config.pop('mode_type')\n         if mode_type == 'function':\n             mode = globals()[config['mode']]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b43caf7b49746fcfa9738c5f9ea6fd46de9b5cd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7611 | Contributors (this commit): 28 | Commits (past 90d): 50 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -1512,7 +1512,7 @@ class Merge(Layer):\n \n         assert hasattr(mask, '__len__') and len(mask) == len(inputs)\n \n-        if self.mode in ['sum', 'mul', 'ave']:\n+        if self.mode in ['sum', 'mul', 'ave', 'max']:\n             masks = [K.expand_dims(m, 0) for m in mask if m is not None]\n             return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)\n         elif self.mode == 'concat':\n@@ -1636,7 +1636,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n \n     # Arguments\n         mode: String or lambda/function. If string, must be one\n-            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n         concat_axis: Integer, axis to use in mode `concat`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f420d898646d520b94c5ec5a4494de9afd02ed98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 29/22 | Churn Δ: 120 | Churn Cumulative: 6156 | Contributors (this commit): 39 | Commits (past 90d): 93 | Contributors (cumulative): 47 | DMM Complexity: 0.48484848484848486\n\nDIFF:\n@@ -22,6 +22,7 @@ from .. import losses\n from .. import metrics as metrics_module\n from ..utils.generic_utils import Progbar\n from .. import callbacks as cbks\n+from ..legacy import interfaces\n \n \n def _standardize_input_data(data, names, shapes=None,\n@@ -1675,6 +1676,7 @@ class Model(Container):\n             return outputs[0]\n         return outputs\n \n+    @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                       steps_per_epoch,\n                       epochs=1,\n@@ -1911,6 +1913,7 @@ class Model(Container):\n         callbacks.on_train_end()\n         return self.history\n \n+    @interfaces.legacy_generator_methods_support\n     def evaluate_generator(self, generator, steps,\n                            max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Evaluates the model on a data generator.\n@@ -2007,6 +2010,7 @@ class Model(Container):\n                                            weights=batch_sizes))\n             return averages\n \n+    @interfaces.legacy_generator_methods_support\n     def predict_generator(self, generator, steps,\n                           max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Generates predictions for the input samples from a data generator.\n\n@@ -66,6 +66,71 @@ def generate_legacy_interface(allowed_positional_args=None,\n     return legacy_support\n \n \n+def generate_legacy_method_interface(allowed_positional_args=None,\n+                                     conversions=None,\n+                                     preprocessor=None,\n+                                     value_conversions=None):\n+    allowed_positional_args = allowed_positional_args or []\n+    conversions = conversions or []\n+    value_conversions = value_conversions or []\n+\n+    def legacy_support(func):\n+        @six.wraps(func)\n+        def wrapper(*args, **kwargs):\n+            method_name = func.__name__\n+            if preprocessor:\n+                args, kwargs, converted = preprocessor(args, kwargs)\n+            else:\n+                converted = []\n+            if len(args) > len(allowed_positional_args) + 1:\n+                raise TypeError('The function `' + method_name +\n+                                '` can accept only ' +\n+                                str(len(allowed_positional_args)) +\n+                                ' positional arguments (' +\n+                                str(allowed_positional_args) + '), but '\n+                                'you passed the following '\n+                                'positional arguments: ' +\n+                                str(args[1:]))\n+            for key in value_conversions:\n+                if key in kwargs:\n+                    old_value = kwargs[key]\n+                    if old_value in value_conversions[key]:\n+                        kwargs[key] = value_conversions[key][old_value]\n+            for old_name, new_name in conversions:\n+                if old_name in kwargs:\n+                    value = kwargs.pop(old_name)\n+                    if new_name in kwargs:\n+                        raise_duplicate_arg_error(old_name, new_name)\n+                    kwargs[new_name] = value\n+                    converted.append((new_name, old_name))\n+            if converted:\n+                signature = '`' + method_name + '('\n+                for value in args[1:]:\n+                    if isinstance(value, six.string_types):\n+                        signature += '\"' + value + '\"'\n+                    elif hasattr(value, '__name__'):\n+                        signature += value.__name__ + '()'\n+                    else:\n+                        signature += str(value)\n+                    signature += ', '\n+                for i, (name, value) in enumerate(kwargs.items()):\n+                    signature += name + '='\n+                    if isinstance(value, six.string_types):\n+                        signature += '\"' + value + '\"'\n+                    elif hasattr(value, '__name__'):\n+                        signature += value.__name__ + '()'\n+                    else:\n+                        signature += str(value)\n+                    if i < len(kwargs) - 1:\n+                        signature += ', '\n+                signature += ')`'\n+                warnings.warn('Update your `' + method_name +\n+                              '` function call to the Keras 2 API: ' + signature)\n+            return func(*args, **kwargs)\n+        return wrapper\n+    return legacy_support\n+\n+\n def raise_duplicate_arg_error(old_arg, new_arg):\n     raise TypeError('For the `' + new_arg + '` argument, '\n                     'the layer received both '\n@@ -497,6 +562,18 @@ legacy_cropping3d_support = generate_legacy_interface(\n                                         'th': 'channels_first',\n                                         'default': None}})\n \n+legacy_generator_methods_support = generate_legacy_method_interface(\n+    allowed_positional_args=['generator', 'steps_per_epoch',\n+                             'epochs', 'verbose', 'callbacks',\n+                             'validation_data', 'validation_steps',\n+                             'class_weight', 'max_q_size', 'workers',\n+                             'pickle_safe', 'initial_epoch'],\n+    conversions=[('samples_per_epoch', 'steps_per_epoch'),\n+                 ('val_samples', 'steps'),\n+                 ('nb_epoch', 'epochs'),\n+                 ('nb_val_samples', 'validation_steps'),\n+                 ('nb_worker', 'workers')])\n+\n legacy_spatialdropout1d_support = generate_legacy_interface(\n     allowed_positional_args=['rate'],\n     conversions=[('p', 'rate')])\n\n@@ -2,6 +2,7 @@ import pytest\n import json\n from keras.utils.test_utils import keras_test\n import keras\n+import numpy as np\n \n \n @keras_test\n@@ -758,6 +759,44 @@ def test_cropping3d_legacy_interface():\n \n \n @keras_test\n+def test_generator_methods_interface():\n+    def train_generator():\n+        x = np.random.randn(2, 2)\n+        y = np.random.randint(0, 2, size=[2, 1])\n+        while True:\n+            yield (x, y)\n+\n+    def val_generator():\n+        x = np.random.randn(2, 2)\n+        y = np.random.randint(0, 2, size=[2, 1])\n+        while True:\n+            yield (x, y)\n+\n+    def pred_generator():\n+        x = np.random.randn(1, 2)\n+        while True:\n+            yield x\n+\n+    x = keras.layers.Input(shape=(2, ))\n+    y = keras.layers.Dense(2)(x)\n+\n+    model = keras.models.Model(inputs=x, outputs=y)\n+    model.compile(optimizer='rmsprop',\n+                  loss='sparse_categorical_crossentropy',\n+                  metrics=['accuracy'])\n+    model.fit_generator(generator=train_generator(),\n+                        samples_per_epoch=1,\n+                        validation_data=val_generator(),\n+                        nb_val_samples=1,\n+                        nb_worker=1)\n+    model.evaluate_generator(generator=train_generator(),\n+                             val_samples=2,\n+                             nb_worker=1)\n+    model.predict_generator(generator=pred_generator(),\n+                            val_samples=2,\n+                            nb_worker=1)\n+\n+\n def test_spatialdropout1d_legacy_interface():\n     old_layer = keras.layers.SpatialDropout1D(p=0.6, name='sd1d')\n     new_layer_1 = keras.layers.SpatialDropout1D(rate=0.6, name='sd1d')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
