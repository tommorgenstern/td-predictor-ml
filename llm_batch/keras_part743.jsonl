{"custom_id": "keras#f45b99f3b1f5075d4168cdc944b661f696982e7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 15 | Methods Changed: 1 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 45 | Churn Cumulative: 374 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,10 @@\n \"\"\"\n Title: Image classification with ConvMixer\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras Core: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2021/10/12\n Last modified: 2021/10/12\n Description: An all-convolutional network applied to patches of images.\n+Accelerator: GPU\n \"\"\"\n \"\"\"\n ## Introduction\n@@ -19,8 +19,7 @@ learning just from the training data with as minimal inductive priors as possibl\n yield great downstream performance when trained with proper regularization, data\n augmentation, and relatively large datasets.\n \n-In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note:\n-at\n+In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note: at\n the time of writing, it is a submission to the ICLR 2022 conference), the authors extend\n the idea of using patches to train an all-convolutional network and demonstrate\n competitive results. Their architecture namely **ConvMixer** uses recipes from the recent\n@@ -31,24 +30,23 @@ and so on.\n \n In this example, we will implement the ConvMixer model and demonstrate its performance on\n the CIFAR-10 dataset.\n-\"\"\"\n \n-\"\"\"shell\n-pip install -qU keras_core\n+To use the AdamW optimizer, we need to install TensorFlow Addons:\n+\n+```shell\n+pip install -U -q tensorflow-addons\n+```\n \"\"\"\n \n \"\"\"\n ## Imports\n \"\"\"\n \n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"jax\"\n-\n-import keras_core as keras\n-from keras_core import layers\n+from tensorflow.keras import layers\n+from tensorflow import keras\n \n import matplotlib.pyplot as plt\n+import tensorflow_addons as tfa\n import tensorflow as tf\n import numpy as np\n \n@@ -87,17 +85,15 @@ print(f\"Test data samples: {len(x_test)}\")\n \n Our data augmentation pipeline is different from what the authors used for the CIFAR-10\n dataset, which is fine for the purpose of the example.\n-Note that, it's ok to use **TF APIs for data I/O and preprocessing** with other backends\n-(jax, torch) as it is feature-complete framework when it comes to data preprocessing.\n \"\"\"\n \n image_size = 32\n auto = tf.data.AUTOTUNE\n \n-data_augmentation = tf.keras.Sequential(\n+data_augmentation = keras.Sequential(\n     [\n-        tf.keras.layers.RandomCrop(image_size, image_size),\n-        tf.keras.layers.RandomFlip(\"horizontal\"),\n+        layers.RandomCrop(image_size, image_size),\n+        layers.RandomFlip(\"horizontal\"),\n     ],\n     name=\"data_augmentation\",\n )\n@@ -200,7 +196,7 @@ parameters.\n \n \n def run_experiment(model):\n-    optimizer = keras.optimizers.AdamW(\n+    optimizer = tfa.optimizers.AdamW(\n         learning_rate=learning_rate, weight_decay=weight_decay\n     )\n \n@@ -210,12 +206,12 @@ def run_experiment(model):\n         metrics=[\"accuracy\"],\n     )\n \n-    checkpoint_filepath = \"/tmp/checkpoint.keras\"\n+    checkpoint_filepath = \"/tmp/checkpoint\"\n     checkpoint_callback = keras.callbacks.ModelCheckpoint(\n         checkpoint_filepath,\n         monitor=\"val_accuracy\",\n         save_best_only=True,\n-        save_weights_only=False,\n+        save_weights_only=True,\n     )\n \n     history = model.fit(\n@@ -304,8 +300,7 @@ kernel = np.expand_dims(kernel.squeeze(), axis=2)\n visualization_plot(kernel)\n \n \"\"\"\n-We see that different filters in the kernel have different locality spans, and this\n-pattern\n+We see that different filters in the kernel have different locality spans, and this pattern\n is likely to evolve with more training.\n \"\"\"\n \n@@ -321,9 +316,5 @@ like self-attention. Following works are along this line of research:\n \n | Trained Model | Demo |\n | :--: | :--: |\n-| [![Generic\n-badge](https://img.shields.io/badge/ðŸ¤—%20Model-ConvMixer-black.svg)](https://huggingface.co\n-/keras-io/conv_Mixer) | [![Generic\n-badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-ConvMixer-black.svg)](https://huggingface.c\n-o/spaces/keras-io/conv_Mixer) |\n+| [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Model-ConvMixer-black.svg)](https://huggingface.co/keras-io/conv_Mixer) | [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-ConvMixer-black.svg)](https://huggingface.co/spaces/keras-io/conv_Mixer) |\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#637bc84ac0e1082d84278b614b4b861a52bd5ce7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 15 | Methods Changed: 1 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 45 | Churn Cumulative: 419 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,10 @@\n \"\"\"\n Title: Image classification with ConvMixer\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Converted to Keras Core: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2021/10/12\n Last modified: 2021/10/12\n Description: An all-convolutional network applied to patches of images.\n-Accelerator: GPU\n \"\"\"\n \"\"\"\n ## Introduction\n@@ -19,7 +19,8 @@ learning just from the training data with as minimal inductive priors as possibl\n yield great downstream performance when trained with proper regularization, data\n augmentation, and relatively large datasets.\n \n-In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note: at\n+In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note:\n+at\n the time of writing, it is a submission to the ICLR 2022 conference), the authors extend\n the idea of using patches to train an all-convolutional network and demonstrate\n competitive results. Their architecture namely **ConvMixer** uses recipes from the recent\n@@ -30,23 +31,24 @@ and so on.\n \n In this example, we will implement the ConvMixer model and demonstrate its performance on\n the CIFAR-10 dataset.\n+\"\"\"\n \n-To use the AdamW optimizer, we need to install TensorFlow Addons:\n-\n-```shell\n-pip install -U -q tensorflow-addons\n-```\n+\"\"\"shell\n+pip install -qU keras_core\n \"\"\"\n \n \"\"\"\n ## Imports\n \"\"\"\n \n-from tensorflow.keras import layers\n-from tensorflow import keras\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n+import keras_core as keras\n+from keras_core import layers\n \n import matplotlib.pyplot as plt\n-import tensorflow_addons as tfa\n import tensorflow as tf\n import numpy as np\n \n@@ -85,15 +87,17 @@ print(f\"Test data samples: {len(x_test)}\")\n \n Our data augmentation pipeline is different from what the authors used for the CIFAR-10\n dataset, which is fine for the purpose of the example.\n+Note that, it's ok to use **TF APIs for data I/O and preprocessing** with other backends\n+(jax, torch) as it is feature-complete framework when it comes to data preprocessing.\n \"\"\"\n \n image_size = 32\n auto = tf.data.AUTOTUNE\n \n-data_augmentation = keras.Sequential(\n+data_augmentation = tf.keras.Sequential(\n     [\n-        layers.RandomCrop(image_size, image_size),\n-        layers.RandomFlip(\"horizontal\"),\n+        tf.keras.layers.RandomCrop(image_size, image_size),\n+        tf.keras.layers.RandomFlip(\"horizontal\"),\n     ],\n     name=\"data_augmentation\",\n )\n@@ -196,7 +200,7 @@ parameters.\n \n \n def run_experiment(model):\n-    optimizer = tfa.optimizers.AdamW(\n+    optimizer = keras.optimizers.AdamW(\n         learning_rate=learning_rate, weight_decay=weight_decay\n     )\n \n@@ -206,12 +210,12 @@ def run_experiment(model):\n         metrics=[\"accuracy\"],\n     )\n \n-    checkpoint_filepath = \"/tmp/checkpoint\"\n+    checkpoint_filepath = \"/tmp/checkpoint.keras\"\n     checkpoint_callback = keras.callbacks.ModelCheckpoint(\n         checkpoint_filepath,\n         monitor=\"val_accuracy\",\n         save_best_only=True,\n-        save_weights_only=True,\n+        save_weights_only=False,\n     )\n \n     history = model.fit(\n@@ -300,7 +304,8 @@ kernel = np.expand_dims(kernel.squeeze(), axis=2)\n visualization_plot(kernel)\n \n \"\"\"\n-We see that different filters in the kernel have different locality spans, and this pattern\n+We see that different filters in the kernel have different locality spans, and this\n+pattern\n is likely to evolve with more training.\n \"\"\"\n \n@@ -316,5 +321,9 @@ like self-attention. Following works are along this line of research:\n \n | Trained Model | Demo |\n | :--: | :--: |\n-| [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Model-ConvMixer-black.svg)](https://huggingface.co/keras-io/conv_Mixer) | [![Generic badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-ConvMixer-black.svg)](https://huggingface.co/spaces/keras-io/conv_Mixer) |\n+| [![Generic\n+badge](https://img.shields.io/badge/ðŸ¤—%20Model-ConvMixer-black.svg)](https://huggingface.co\n+/keras-io/conv_Mixer) | [![Generic\n+badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-ConvMixer-black.svg)](https://huggingface.c\n+o/spaces/keras-io/conv_Mixer) |\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d2d963557b901f2df0a57067e83ff9a3c7bf116", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 10 | Churn Cumulative: 429 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Image classification with ConvMixer\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras Core: [Md Awsafur Rahman](https://awsaf49.github.io)\n+Converted to Keras 3: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2021/10/12\n Last modified: 2021/10/12\n Description: An all-convolutional network applied to patches of images.\n@@ -33,10 +33,6 @@ In this example, we will implement the ConvMixer model and demonstrate its perfo\n the CIFAR-10 dataset.\n \"\"\"\n \n-\"\"\"shell\n-pip install -qU keras_core\n-\"\"\"\n-\n \"\"\"\n ## Imports\n \"\"\"\n@@ -45,8 +41,8 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"jax\"\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras\n+from keras import layers\n \n import matplotlib.pyplot as plt\n import tensorflow as tf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bdde80beb8b51d6b82eeaac2bbc6964cb0dc0016", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Î” (Sum/Max): 2/2 | Churn Î”: 19 | Churn Cumulative: 448 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -90,13 +90,16 @@ Note that, it's ok to use **TF APIs for data I/O and preprocessing** with other\n image_size = 32\n auto = tf.data.AUTOTUNE\n \n-data_augmentation = tf.keras.Sequential(\n-    [\n-        tf.keras.layers.RandomCrop(image_size, image_size),\n-        tf.keras.layers.RandomFlip(\"horizontal\"),\n-    ],\n-    name=\"data_augmentation\",\n-)\n+augmentation_layers = [\n+    keras.layers.RandomCrop(image_size, image_size),\n+    keras.layers.RandomFlip(\"horizontal\")\n+]\n+\n+\n+def augment_images(images):\n+    for layer in augmentation_layers:\n+        images = layer(images, training=True)\n+    return images\n \n \n def make_datasets(images, labels, is_train=False):\n@@ -106,7 +109,7 @@ def make_datasets(images, labels, is_train=False):\n     dataset = dataset.batch(batch_size)\n     if is_train:\n         dataset = dataset.map(\n-            lambda x, y: (data_augmentation(x), y), num_parallel_calls=auto\n+            lambda x, y: (augment_images(x), y), num_parallel_calls=auto\n         )\n     return dataset.prefetch(auto)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0162ac2cd826cbf17b42ee518ee48a4e383e965", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 8 | Churn Cumulative: 456 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -317,12 +317,4 @@ like self-attention. Following works are along this line of research:\n * ConViT ([d'Ascoli et al.](https://arxiv.org/abs/2103.10697))\n * CCT ([Hassani et al.](https://arxiv.org/abs/2104.05704))\n * CoAtNet ([Dai et al.](https://arxiv.org/abs/2106.04803))\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![Generic\n-badge](https://img.shields.io/badge/ðŸ¤—%20Model-ConvMixer-black.svg)](https://huggingface.co\n-/keras-io/conv_Mixer) | [![Generic\n-badge](https://img.shields.io/badge/ðŸ¤—%20Spaces-ConvMixer-black.svg)](https://huggingface.c\n-o/spaces/keras-io/conv_Mixer) |\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2989b6f8fc2aca8bf8d0ffa32e58ceeaa3ef35ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 2 | Churn Cumulative: 458 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Image classification with ConvMixer\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras 3: [Md Awsafur Rahman](https://awsaf49.github.io)\n+Converted to Keras 3 by: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2021/10/12\n Last modified: 2021/10/12\n Description: An all-convolutional network applied to patches of images.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a45d3abf65059ecdba6334fa6731e112fab71c9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 293 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Î” (Sum/Max): 6/6 | Churn Î”: 293 | Churn Cumulative: 293 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,293 @@\n+\"\"\"\n+Title: Knowledge Distillation\n+Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n+Converted to Keras 3: [Md Awsafur Rahman](https://awsaf49.github.io)\n+Date created: 2020/09/01\n+Last modified: 2020/09/01\n+Description: Implementation of classical Knowledge Distillation.\n+\"\"\"\n+\"\"\"\n+## Introduction to Knowledge Distillation\n+\n+Knowledge Distillation is a procedure for model\n+compression, in which a small (student) model is trained to match a large pre-trained\n+(teacher) model. Knowledge is transferred from the teacher model to the student\n+by minimizing a loss function, aimed at matching softened teacher logits as well as\n+ground-truth labels.\n+\n+The logits are softened by applying a \"temperature\" scaling function in the softmax,\n+effectively smoothing out the probability distribution and revealing\n+inter-class relationships learned by the teacher.\n+\n+**Reference:**\n+\n+- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import keras\n+from keras import layers\n+from keras import ops\n+import tensorflow as tf\n+import numpy as np\n+\n+\"\"\"\n+## Construct `Distiller()` class\n+\n+The custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step`,\n+and `compile()`. In order to use the distiller, we need:\n+\n+- A trained teacher model\n+- A student model to train\n+- A student loss function on the difference between student predictions and ground-truth\n+- A distillation loss function, along with a `temperature`, on the difference between the\n+soft student predictions and the soft teacher labels\n+- An `alpha` factor to weight the student and distillation loss\n+- An optimizer for the student and (optional) metrics to evaluate performance\n+\n+In the `train_step` method, we perform a forward pass of both the teacher and student,\n+calculate the loss with weighting of the `student_loss` and `distillation_loss` by\n+`alpha` and\n+`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights\n+are updated,\n+and therefore we only calculate the gradients for the student weights.\n+\n+In the `test_step` method, we evaluate the student model on the provided dataset.\n+\"\"\"\n+\n+\n+class Distiller(keras.Model):\n+    def __init__(self, student, teacher):\n+        super().__init__()\n+        self.teacher = teacher\n+        self.student = student\n+\n+    def compile(\n+        self,\n+        optimizer,\n+        metrics,\n+        student_loss_fn,\n+        distillation_loss_fn,\n+        alpha=0.1,\n+        temperature=3,\n+    ):\n+        \"\"\"Configure the distiller.\n+\n+        Args:\n+            optimizer: Keras optimizer for the student weights\n+            metrics: Keras metrics for evaluation\n+            student_loss_fn: Loss function of difference between student\n+                predictions and ground-truth\n+            distillation_loss_fn: Loss function of difference between soft\n+                student predictions and soft teacher predictions\n+            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n+            temperature: Temperature for softening probability distributions.\n+                Larger temperature gives softer distributions.\n+        \"\"\"\n+        super().compile(optimizer=optimizer, metrics=metrics)\n+        self.student_loss_fn = student_loss_fn\n+        self.distillation_loss_fn = distillation_loss_fn\n+        self.alpha = alpha\n+        self.temperature = temperature\n+\n+    def train_step(self, data):\n+        # Unpack data\n+        x, y = data\n+\n+        # Forward pass of teacher\n+        teacher_predictions = self.teacher(x, training=False)\n+\n+        with tf.GradientTape() as tape:\n+            # Forward pass of student\n+            student_predictions = self.student(x, training=True)\n+\n+            # Compute losses\n+            student_loss = self.student_loss_fn(y, student_predictions)\n+\n+            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n+            # The magnitudes of the gradients produced by the soft targets scale\n+            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n+            distillation_loss = (\n+                self.distillation_loss_fn(\n+                    ops.softmax(teacher_predictions / self.temperature, axis=1),\n+                    ops.softmax(student_predictions / self.temperature, axis=1),\n+                )\n+                * self.temperature**2\n+            )\n+\n+            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n+\n+        # Compute gradients\n+        trainable_vars = self.student.trainable_variables\n+        gradients = tape.gradient(loss, trainable_vars)\n+\n+        # Update weights\n+        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n+\n+        # Update the metrics configured in `compile()`.\n+        self.compiled_metrics.update_state(y, student_predictions)\n+\n+        # Return a dict of performance\n+        results = {m.name: m.result() for m in self.metrics}\n+        results.update(\n+            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n+        )\n+        return results\n+\n+    def test_step(self, data):\n+        # Unpack the data\n+        x, y = data\n+\n+        # Compute predictions\n+        y_prediction = self.student(x, training=False)\n+\n+        # Calculate the loss\n+        student_loss = self.student_loss_fn(y, y_prediction)\n+\n+        # Update the metrics.\n+        self.compiled_metrics.update_state(y, y_prediction)\n+\n+        # Return a dict of performance\n+        results = {m.name: m.result() for m in self.metrics}\n+        results.update({\"student_loss\": student_loss})\n+        return results\n+\n+\n+\"\"\"\n+## Create student and teacher models\n+\n+Initialy, we create a teacher model and a smaller student model. Both models are\n+convolutional neural networks and created using `Sequential()`,\n+but could be any Keras model.\n+\"\"\"\n+\n+# Create the teacher\n+teacher = keras.Sequential(\n+    [\n+        keras.Input(shape=(28, 28, 1)),\n+        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n+        layers.LeakyReLU(negative_slope=0.2),\n+        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n+        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n+        layers.Flatten(),\n+        layers.Dense(10),\n+    ],\n+    name=\"teacher\",\n+)\n+\n+# Create the student\n+student = keras.Sequential(\n+    [\n+        keras.Input(shape=(28, 28, 1)),\n+        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n+        layers.LeakyReLU(negative_slope=0.2),\n+        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n+        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n+        layers.Flatten(),\n+        layers.Dense(10),\n+    ],\n+    name=\"student\",\n+)\n+\n+# Clone student for later comparison\n+student_scratch = keras.models.clone_model(student)\n+\n+\"\"\"\n+## Prepare the dataset\n+\n+The dataset used for training the teacher and distilling the teacher is\n+[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for\n+any other\n+dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n+of models. Both the student and teacher are trained on the training set and evaluated on\n+the test set.\n+\"\"\"\n+\n+# Prepare the train and test dataset.\n+batch_size = 64\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+# Normalize data\n+x_train = x_train.astype(\"float32\") / 255.0\n+x_train = np.reshape(x_train, (-1, 28, 28, 1))\n+\n+x_test = x_test.astype(\"float32\") / 255.0\n+x_test = np.reshape(x_test, (-1, 28, 28, 1))\n+\n+\n+\"\"\"\n+## Train the teacher\n+\n+In knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\n+by training the teacher model on the training set in the usual way.\n+\"\"\"\n+\n+# Train teacher as usual\n+teacher.compile(\n+    optimizer=keras.optimizers.Adam(),\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+)\n+\n+# Train and evaluate teacher on data.\n+teacher.fit(x_train, y_train, epochs=5)\n+teacher.evaluate(x_test, y_test)\n+\n+\"\"\"\n+## Distill teacher to student\n+\n+We have already trained the teacher model, and we only need to initialize a\n+`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n+hyperparameters and optimizer, and distill the teacher to the student.\n+\"\"\"\n+\n+# Initialize and compile distiller\n+distiller = Distiller(student=student, teacher=teacher)\n+distiller.compile(\n+    optimizer=keras.optimizers.Adam(),\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    distillation_loss_fn=keras.losses.KLDivergence(),\n+    alpha=0.1,\n+    temperature=10,\n+)\n+\n+# Distill teacher to student\n+distiller.fit(x_train, y_train, epochs=3)\n+\n+# Evaluate student on test dataset\n+distiller.evaluate(x_test, y_test)\n+\n+\"\"\"\n+## Train student from scratch for comparison\n+\n+We can also train an equivalent student model from scratch without the teacher, in order\n+to evaluate the performance gain obtained by knowledge distillation.\n+\"\"\"\n+\n+# Train student as doen usually\n+student_scratch.compile(\n+    optimizer=keras.optimizers.Adam(),\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+)\n+\n+# Train and evaluate student trained from scratch.\n+student_scratch.fit(x_train, y_train, epochs=3)\n+student_scratch.evaluate(x_test, y_test)\n+\n+\"\"\"\n+If the teacher is trained for 5 full epochs and the student is distilled on this teacher\n+for 3 full epochs, you should in this example experience a performance boost compared to\n+training the same student model from scratch, and even compared to the teacher itself.\n+You should expect the teacher to have accuracy around 97.6%, the student trained from\n+scratch should be around 97.6%, and the distilled student should be around 98.1%. Remove\n+or try out different seeds to use different weight initializations.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f5f44c5c0197d35a2d912f592e37135a03342a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 11 | Methods Changed: 1 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 30 | Churn Cumulative: 323 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,12 @@\n \"\"\"\n Title: Knowledge Distillation\n Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n-Converted to Keras 3: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2020/09/01\n Last modified: 2020/09/01\n Description: Implementation of classical Knowledge Distillation.\n+Accelerator: GPU\n \"\"\"\n+\n \"\"\"\n ## Introduction to Knowledge Distillation\n \n@@ -28,16 +29,12 @@ inter-class relationships learned by the teacher.\n ## Setup\n \"\"\"\n \n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n-import keras\n-from keras import layers\n-from keras import ops\n import tensorflow as tf\n+from tensorflow import keras\n+from tensorflow.keras import layers\n import numpy as np\n \n+\n \"\"\"\n ## Construct `Distiller()` class\n \n@@ -53,10 +50,8 @@ soft student predictions and the soft teacher labels\n - An optimizer for the student and (optional) metrics to evaluate performance\n \n In the `train_step` method, we perform a forward pass of both the teacher and student,\n-calculate the loss with weighting of the `student_loss` and `distillation_loss` by\n-`alpha` and\n-`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights\n-are updated,\n+calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and\n+`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights are updated,\n and therefore we only calculate the gradients for the student weights.\n \n In the `test_step` method, we evaluate the student model on the provided dataset.\n@@ -116,8 +111,8 @@ class Distiller(keras.Model):\n             # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n             distillation_loss = (\n                 self.distillation_loss_fn(\n-                    ops.softmax(teacher_predictions / self.temperature, axis=1),\n-                    ops.softmax(student_predictions / self.temperature, axis=1),\n+                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n+                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                 )\n                 * self.temperature**2\n             )\n@@ -173,7 +168,7 @@ teacher = keras.Sequential(\n     [\n         keras.Input(shape=(28, 28, 1)),\n         layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.LeakyReLU(negative_slope=0.2),\n+        layers.LeakyReLU(alpha=0.2),\n         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n         layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n         layers.Flatten(),\n@@ -187,7 +182,7 @@ student = keras.Sequential(\n     [\n         keras.Input(shape=(28, 28, 1)),\n         layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.LeakyReLU(negative_slope=0.2),\n+        layers.LeakyReLU(alpha=0.2),\n         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n         layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n         layers.Flatten(),\n@@ -203,8 +198,7 @@ student_scratch = keras.models.clone_model(student)\n ## Prepare the dataset\n \n The dataset used for training the teacher and distilling the teacher is\n-[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for\n-any other\n+[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for any other\n dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n of models. Both the student and teacher are trained on the training set and evaluated on\n the test set.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6da243a0d46c3f81996407939c5620c4b3e27742", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 10 | Methods Changed: 1 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 32 | Churn Cumulative: 355 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,12 +1,11 @@\n \"\"\"\n Title: Knowledge Distillation\n Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n+Converted to Keras 3: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2020/09/01\n Last modified: 2020/09/01\n Description: Implementation of classical Knowledge Distillation.\n-Accelerator: GPU\n \"\"\"\n-\n \"\"\"\n ## Introduction to Knowledge Distillation\n \n@@ -29,11 +28,15 @@ inter-class relationships learned by the teacher.\n ## Setup\n \"\"\"\n \n-import tensorflow as tf\n-from tensorflow import keras\n-from tensorflow.keras import layers\n-import numpy as np\n+import os\n \n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import keras\n+from keras import layers\n+from keras import ops\n+import tensorflow as tf\n+import numpy as np\n \n \"\"\"\n ## Construct `Distiller()` class\n@@ -50,8 +53,10 @@ soft student predictions and the soft teacher labels\n - An optimizer for the student and (optional) metrics to evaluate performance\n \n In the `train_step` method, we perform a forward pass of both the teacher and student,\n-calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and\n-`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights are updated,\n+calculate the loss with weighting of the `student_loss` and `distillation_loss` by\n+`alpha` and\n+`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights\n+are updated,\n and therefore we only calculate the gradients for the student weights.\n \n In the `test_step` method, we evaluate the student model on the provided dataset.\n@@ -111,8 +116,8 @@ class Distiller(keras.Model):\n             # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n             distillation_loss = (\n                 self.distillation_loss_fn(\n-                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n-                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n+                    ops.softmax(teacher_predictions / self.temperature, axis=1),\n+                    ops.softmax(student_predictions / self.temperature, axis=1),\n                 )\n                 * self.temperature**2\n             )\n@@ -168,7 +173,7 @@ teacher = keras.Sequential(\n     [\n         keras.Input(shape=(28, 28, 1)),\n         layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n+        layers.LeakyReLU(negative_slope=0.2),\n         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n         layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n         layers.Flatten(),\n@@ -182,7 +187,7 @@ student = keras.Sequential(\n     [\n         keras.Input(shape=(28, 28, 1)),\n         layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n-        layers.LeakyReLU(alpha=0.2),\n+        layers.LeakyReLU(negative_slope=0.2),\n         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n         layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n         layers.Flatten(),\n@@ -198,7 +203,8 @@ student_scratch = keras.models.clone_model(student)\n ## Prepare the dataset\n \n The dataset used for training the teacher and distilling the teacher is\n-[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for any other\n+[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for\n+any other\n dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n of models. Both the student and teacher are trained on the training set and evaluated on\n the test set.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bd0c2e969830308e9bd2be6154c0878c213a8261", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 3 | Churn Cumulative: 358 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,11 @@\n \"\"\"\n Title: Knowledge Distillation\n Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n-Converted to Keras 3: [Md Awsafur Rahman](https://awsaf49.github.io)\n+Converted to Keras 3 by: [Md Awsafur Rahman](https://awsaf49.github.io)\n Date created: 2020/09/01\n Last modified: 2020/09/01\n Description: Implementation of classical Knowledge Distillation.\n+Accelerator: GPU\n \"\"\"\n \"\"\"\n ## Introduction to Knowledge Distillation\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa1c5f674a7af26303fdd496a18d7d810a566e86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 4 | Churn Cumulative: 362 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -254,7 +254,9 @@ distiller = Distiller(student=student, teacher=teacher)\n distiller.compile(\n     optimizer=keras.optimizers.Adam(),\n     metrics=[keras.metrics.SparseCategoricalAccuracy()],\n-    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(\n+        from_logits=True\n+    ),\n     distillation_loss_fn=keras.losses.KLDivergence(),\n     alpha=0.1,\n     temperature=10,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#634cb58dc786dc36d0f28b408c425a82678e9d2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 67 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Î” (Sum/Max): -2/0 | Churn Î”: 84 | Churn Cumulative: 446 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,8 +42,8 @@ import numpy as np\n \"\"\"\n ## Construct `Distiller()` class\n \n-The custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step`,\n-and `compile()`. In order to use the distiller, we need:\n+The custom `Distiller()` class, overrides the `Model` methods `compile`, `compute_loss`,\n+and `call`. In order to use the distiller, we need:\n \n - A trained teacher model\n - A student model to train\n@@ -53,14 +53,9 @@ soft student predictions and the soft teacher labels\n - An `alpha` factor to weight the student and distillation loss\n - An optimizer for the student and (optional) metrics to evaluate performance\n \n-In the `train_step` method, we perform a forward pass of both the teacher and student,\n-calculate the loss with weighting of the `student_loss` and `distillation_loss` by\n-`alpha` and\n-`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights\n-are updated,\n-and therefore we only calculate the gradients for the student weights.\n-\n-In the `test_step` method, we evaluate the student model on the provided dataset.\n+In the `compute_loss` method, we perform a forward pass of both the teacher and student,\n+calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha`\n+and `1 - alpha`, respectively. Note: only the student weights are updated.\n \"\"\"\n \n \n@@ -98,67 +93,22 @@ class Distiller(keras.Model):\n         self.alpha = alpha\n         self.temperature = temperature\n \n-    def train_step(self, data):\n-        # Unpack data\n-        x, y = data\n+    def compute_loss(\n+        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n+    ):\n+        teacher_pred = self.teacher(x, training=False)\n+        student_loss = self.student_loss_fn(y, y_pred)\n \n-        # Forward pass of teacher\n-        teacher_predictions = self.teacher(x, training=False)\n-\n-        with tf.GradientTape() as tape:\n-            # Forward pass of student\n-            student_predictions = self.student(x, training=True)\n-\n-            # Compute losses\n-            student_loss = self.student_loss_fn(y, student_predictions)\n-\n-            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n-            # The magnitudes of the gradients produced by the soft targets scale\n-            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n-            distillation_loss = (\n-                self.distillation_loss_fn(\n-                    ops.softmax(teacher_predictions / self.temperature, axis=1),\n-                    ops.softmax(student_predictions / self.temperature, axis=1),\n-                )\n-                * self.temperature**2\n-            )\n+        distillation_loss = self.distillation_loss_fn(\n+            ops.softmax(teacher_pred / self.temperature, axis=1),\n+            ops.softmax(y_pred / self.temperature, axis=1),\n+        ) * (self.temperature**2)\n \n         loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n+        return loss\n \n-        # Compute gradients\n-        trainable_vars = self.student.trainable_variables\n-        gradients = tape.gradient(loss, trainable_vars)\n-\n-        # Update weights\n-        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n-\n-        # Update the metrics configured in `compile()`.\n-        self.compiled_metrics.update_state(y, student_predictions)\n-\n-        # Return a dict of performance\n-        results = {m.name: m.result() for m in self.metrics}\n-        results.update(\n-            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n-        )\n-        return results\n-\n-    def test_step(self, data):\n-        # Unpack the data\n-        x, y = data\n-\n-        # Compute predictions\n-        y_prediction = self.student(x, training=False)\n-\n-        # Calculate the loss\n-        student_loss = self.student_loss_fn(y, y_prediction)\n-\n-        # Update the metrics.\n-        self.compiled_metrics.update_state(y, y_prediction)\n-\n-        # Return a dict of performance\n-        results = {m.name: m.result() for m in self.metrics}\n-        results.update({\"student_loss\": student_loss})\n-        return results\n+    def call(self, x):\n+        return self.student(x)\n \n \n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0484165fa7cba3c095d95da19b1c45945b47c60b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6086ecdce9e19109936961666929d7c85e65d315", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 169 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 11 | Complexity Î” (Sum/Max): 14/11 | Churn Î”: 169 | Churn Cumulative: 169 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,51 @@\n+import numpy as np\n+import pytest\n+\n+import keras\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import optimizers\n+from keras import testing\n+\n+\n+class MyModel(keras.Model):\n+    def __init__(self, hidden_dim, output_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.hidden_dim = hidden_dim\n+        self.output_dim = output_dim\n+        self.dense1 = layers.Dense(hidden_dim, activation=\"relu\")\n+        self.dense2 = layers.Dense(hidden_dim, activation=\"relu\")\n+        self.dense3 = layers.Dense(output_dim)\n+\n+    def call(self, x):\n+        x = self.dense1(x)\n+        x = self.dense2(x)\n+        return self.dense3(x)\n+\n+\n+@pytest.mark.requires_trainable_backend\n+class BasicFlowTest(testing.TestCase):\n+    def test_basic_fit(self):\n+        model = MyModel(hidden_dim=256, output_dim=16)\n+\n+        x = np.random.random((50000, 128))\n+        y = np.random.random((50000, 16))\n+        batch_size = 32\n+        epochs = 3\n+\n+        model.compile(\n+            optimizer=optimizers.SGD(learning_rate=0.001),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+        output_before_fit = model(x)\n+        history = model.fit(\n+            x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+        )\n+        output_after_fit = model(x)\n+\n+        print(\"History:\")\n+        print(history.history)\n+\n+        self.assertNotAllClose(output_before_fit, output_after_fit)\n\n@@ -0,0 +1,118 @@\n+import os\n+import re\n+import subprocess\n+\n+from keras_core import backend\n+\n+BACKEND_REQ = {\n+    \"tensorflow\": \"tensorflow\",\n+    \"torch\": \"torch torchvision\",\n+    \"jax\": \"jax jaxlib\",\n+}\n+\n+\n+def setup_package():\n+    subprocess.run(\"rm -rf tmp_build_dir\", shell=True)\n+    build_process = subprocess.run(\n+        \"python3 pip_build.py\",\n+        capture_output=True,\n+        text=True,\n+        shell=True,\n+    )\n+    print(build_process.stdout)\n+    match = re.search(\n+        r\"\\s[^\\s]*\\.whl\",\n+        build_process.stdout,\n+    )\n+    if not match:\n+        raise ValueError(\"Installing Keras Core package unsuccessful. \")\n+        print(build_process.stderr)\n+    whl_path = match.group()\n+    return whl_path\n+\n+\n+def create_virtualenv():\n+    env_setup = [\n+        # Create and activate virtual environment\n+        \"python3 -m venv test_env\",\n+        # \"source ./test_env/bin/activate\",\n+    ]\n+    os.environ[\"PATH\"] = (\n+        \"/test_env/bin/\" + os.pathsep + os.environ.get(\"PATH\", \"\")\n+    )\n+    run_commands_local(env_setup)\n+\n+\n+def manage_venv_installs(whl_path):\n+    other_backends = list(set(BACKEND_REQ.keys()) - {backend.backend()})\n+    install_setup = [\n+        # Installs the backend's package and common requirements\n+        \"pip install \" + BACKEND_REQ[backend.backend()],\n+        \"pip install -r requirements-common.txt\",\n+        \"pip install pytest\",\n+        # Ensure other backends are uninstalled\n+        \"pip uninstall -y \"\n+        + BACKEND_REQ[other_backends[0]]\n+        + \" \"\n+        + BACKEND_REQ[other_backends[1]],\n+        # Install `.whl` package\n+        \"pip install \" + whl_path + \" --force-reinstall --no-dependencies\",\n+    ]\n+    run_commands_venv(install_setup)\n+\n+\n+def run_keras_core_flow():\n+    test_script = [\n+        # Runs the example script\n+        \"python -m pytest integration_tests/basic_full_flow.py\",\n+    ]\n+    run_commands_venv(test_script)\n+\n+\n+def cleanup():\n+    cleanup_script = [\n+        # Exits virtual environment, deletes files, and any\n+        # miscellaneous install logs\n+        \"exit\",\n+        \"rm -rf test_env\",\n+        \"rm -rf tmp_build_dir\",\n+        \"rm -f *+cpu\",\n+    ]\n+    run_commands_local(cleanup_script)\n+\n+\n+def run_commands_local(commands):\n+    for command in commands:\n+        subprocess.run(command, shell=True)\n+\n+\n+def run_commands_venv(commands):\n+    for command in commands:\n+        cmd_with_args = command.split(\" \")\n+        cmd_with_args[0] = \"test_env/bin/\" + cmd_with_args[0]\n+        p = subprocess.Popen(cmd_with_args)\n+        p.wait()\n+\n+\n+def test_keras_core_imports():\n+    # Ensures packages from all backends are installed.\n+    # Builds Keras core package and returns package file path.\n+    whl_path = setup_package()\n+\n+    # Creates and activates a virtual environment.\n+    create_virtualenv()\n+\n+    # Ensures the backend's package is installed\n+    # and the other backends are uninstalled.\n+    manage_venv_installs(whl_path)\n+\n+    # Runs test of basic flow in Keras Core.\n+    # Tests for backend-specific imports and `model.fit()`.\n+    run_keras_core_flow()\n+\n+    # Removes virtual environment and associated files\n+    cleanup()\n+\n+\n+if __name__ == \"__main__\":\n+    test_keras_core_imports()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f5dd2fc98a79d843c991635712ee88d0da56c1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b38644fc662c1b91068612452b344ed13c7bf42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Î” (Sum/Max): 11/11 | Churn Î”: 12 | Churn Cumulative: 12 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@ import os\n import re\n import subprocess\n \n-from keras_core import backend\n+from keras import backend\n \n BACKEND_REQ = {\n     \"tensorflow\": \"tensorflow\",\n@@ -25,7 +25,7 @@ def setup_package():\n         build_process.stdout,\n     )\n     if not match:\n-        raise ValueError(\"Installing Keras Core package unsuccessful. \")\n+        raise ValueError(\"Installing Keras package unsuccessful. \")\n         print(build_process.stderr)\n     whl_path = match.group()\n     return whl_path\n@@ -61,7 +61,7 @@ def manage_venv_installs(whl_path):\n     run_commands_venv(install_setup)\n \n \n-def run_keras_core_flow():\n+def run_keras_flow():\n     test_script = [\n         # Runs the example script\n         \"python -m pytest integration_tests/basic_full_flow.py\",\n@@ -94,7 +94,7 @@ def run_commands_venv(commands):\n         p.wait()\n \n \n-def test_keras_core_imports():\n+def test_keras_imports():\n     # Ensures packages from all backends are installed.\n     # Builds Keras core package and returns package file path.\n     whl_path = setup_package()\n@@ -108,11 +108,11 @@ def test_keras_core_imports():\n \n     # Runs test of basic flow in Keras Core.\n     # Tests for backend-specific imports and `model.fit()`.\n-    run_keras_core_flow()\n+    run_keras_flow()\n \n     # Removes virtual environment and associated files\n     cleanup()\n \n \n if __name__ == \"__main__\":\n-    test_keras_core_imports()\n\\ No newline at end of file\n+    test_keras_imports()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
