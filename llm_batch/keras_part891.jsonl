{"custom_id": "keras#7ed8edbedd8866fd143df90bb49e53aa60c38d17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 157 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -63,8 +63,7 @@ def to_categorical(x, num_classes=None):\n     >>> b = np.array([.9, .04, .03, .03,\n     ...               .3, .45, .15, .13,\n     ...               .04, .01, .94, .05,\n-    ...               .12, .21, .5, .17],\n-    ...               shape=[4, 4])\n+    ...               .12, .21, .5, .17]).reshape(4,4)\n     >>> loss = keras.ops.categorical_crossentropy(a, b)\n     >>> print(np.around(loss, 5))\n     [0.10536 0.82807 0.1011  1.77196]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c5d9b3128242dff2612b2481569b1bf311b97af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 227 | Lines Deleted: 23 | Files Changed: 8 | Hunks: 40 | Methods Changed: 31 | Complexity Δ (Sum/Max): 12/2 | Churn Δ: 250 | Churn Cumulative: 1518 | Contributors (this commit): 8 | Commits (past 90d): 12 | Contributors (cumulative): 33 | DMM Complexity: 0.6724137931034483\n\nDIFF:\n@@ -80,6 +80,11 @@ class BaseConv(Layer):\n             computation cost of fine-tuning large dense layers.\n             You can also enable LoRA on an existing layer by calling\n             `layer.enable_lora(rank)`.\n+        lora_alpha: Optional integer. If set, this parameter scales the\n+            low-rank adaptation delta (computed as the product of two lower-rank\n+            trainable matrices) during the forward pass. The delta is scaled by\n+            `lora_alpha / lora_rank`, allowing you to fine-tune the strength of\n+            the LoRA adjustment independently of `lora_rank`.\n     \"\"\"\n \n     def __init__(\n@@ -102,6 +107,7 @@ class BaseConv(Layer):\n         kernel_constraint=None,\n         bias_constraint=None,\n         lora_rank=None,\n+        lora_alpha=None,\n         **kwargs,\n     ):\n         super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n@@ -124,6 +130,7 @@ class BaseConv(Layer):\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n         self.lora_rank = lora_rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else lora_rank\n         self.lora_enabled = False\n         self.input_spec = InputSpec(min_ndim=self.rank + 2)\n         self.data_format = self.data_format\n@@ -215,7 +222,7 @@ class BaseConv(Layer):\n             self.bias = None\n         self.built = True\n         if self.lora_rank:\n-            self.enable_lora(self.lora_rank)\n+            self.enable_lora(self.lora_rank, lora_alpha=self.lora_alpha)\n \n     @property\n     def kernel(self):\n@@ -224,9 +231,9 @@ class BaseConv(Layer):\n                 \"You must build the layer before accessing `kernel`.\"\n             )\n         if self.lora_enabled:\n-            return self._kernel + ops.matmul(\n-                self.lora_kernel_a, self.lora_kernel_b\n-            )\n+            return self._kernel + (\n+                self.lora_alpha / self.lora_rank\n+            ) * ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n         return self._kernel\n \n     def convolution_op(self, inputs, kernel):\n@@ -268,7 +275,11 @@ class BaseConv(Layer):\n         )\n \n     def enable_lora(\n-        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+        self,\n+        rank,\n+        lora_alpha=None,\n+        a_initializer=\"he_uniform\",\n+        b_initializer=\"zeros\",\n     ):\n         if self.kernel_constraint:\n             raise ValueError(\n@@ -301,6 +312,7 @@ class BaseConv(Layer):\n         self._tracker.lock()\n         self.lora_enabled = True\n         self.lora_rank = rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else rank\n \n     def save_own_variables(self, store):\n         # Do nothing if the layer isn't yet built\n@@ -363,6 +375,7 @@ class BaseConv(Layer):\n         )\n         if self.lora_rank:\n             config[\"lora_rank\"] = self.lora_rank\n+            config[\"lora_alpha\"] = self.lora_alpha\n         return config\n \n     def _check_load_own_variables(self, store):\n\n@@ -9,6 +9,7 @@ from keras.src import backend\n from keras.src import constraints\n from keras.src import layers\n from keras.src import models\n+from keras.src import ops\n from keras.src import saving\n from keras.src import testing\n \n@@ -735,6 +736,51 @@ class ConvBasicTest(testing.TestCase):\n             model.conv2d.lora_kernel_a.path, \"mymodel/conv2d/lora_kernel_a\"\n         )\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora_with_alpha(self):\n+        # Create a `Conv2D` layer with a small kernel for simplicity.\n+        layer = layers.Conv2D(filters=3, kernel_size=(2, 2), padding=\"valid\")\n+        # Use a fixed input shape: batch size 1, height=4, width=4, channels=3.\n+        input_shape = (1, 4, 4, 3)\n+        layer.build(input_shape)\n+\n+        # Set the base kernel to known, deterministic values.\n+        base_kernel = np.linspace(\n+            0, 1, num=np.prod(layer.kernel.shape), dtype=np.float32\n+        )\n+        base_kernel = base_kernel.reshape(layer.kernel.shape)\n+        layer.kernel.assign(base_kernel)\n+\n+        # Enable LoRA with `rank`=2 and a custom `lora_alpha` value (e.g. 3.0).\n+        layer.enable_lora(rank=2, lora_alpha=3.0)\n+        self.assertEqual(layer.lora_rank, 2)\n+        self.assertEqual(layer.lora_alpha, 3.0)\n+\n+        # For `Conv2D`, assume the LoRA weights have shapes:\n+        #   `lora_kernel_a`: (kernel_height, kernel_width, in_channels, rank)\n+        #   `lora_kernel_b`: (rank, out_channels)\n+        lora_a_shape = layer.lora_kernel_a.shape\n+        lora_b_shape = layer.lora_kernel_b.shape\n+\n+        # Assign known constant values to LoRA weights.\n+        lora_a = np.full(lora_a_shape, 0.1, dtype=np.float32)\n+        lora_b = np.full(lora_b_shape, 0.2, dtype=np.float32)\n+        layer.lora_kernel_a.assign(lora_a)\n+        layer.lora_kernel_b.assign(lora_b)\n+\n+        # Compute the expected delta.\n+        # Flatten `lora_kernel_a` to shape (-1, `rank`),\n+        # multiply with `lora_kernel_b`,\n+        # then reshape to the kernel's shape.\n+        scaling = 3.0 / 2  # `lora_alpha / lora_rank`\n+        delta = np.matmul(lora_a.reshape(-1, 2), lora_b)\n+        delta = delta.reshape(base_kernel.shape)\n+        expected_effective_kernel = base_kernel + scaling * delta\n+\n+        # Compare the effective kernel computed via the property.\n+        actual_effective_kernel = ops.convert_to_numpy(layer.kernel)\n+        self.assertAllClose(actual_effective_kernel, expected_effective_kernel)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_lora_rank_argument(self):\n         self.run_layer_test(\n\n@@ -57,6 +57,11 @@ class Dense(Layer):\n             computation cost of fine-tuning large dense layers.\n             You can also enable LoRA on an existing\n             `Dense` layer by calling `layer.enable_lora(rank)`.\n+        lora_alpha: Optional integer. If set, this parameter scales the\n+            low-rank adaptation delta (computed as the product of two lower-rank\n+            trainable matrices) during the forward pass. The delta is scaled by\n+            `lora_alpha / lora_rank`, allowing you to fine-tune the strength of\n+            the LoRA adjustment independently of `lora_rank`.\n \n     Input shape:\n         N-D tensor with shape: `(batch_size, ..., input_dim)`.\n@@ -82,6 +87,7 @@ class Dense(Layer):\n         kernel_constraint=None,\n         bias_constraint=None,\n         lora_rank=None,\n+        lora_alpha=None,\n         **kwargs,\n     ):\n         super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n@@ -95,6 +101,7 @@ class Dense(Layer):\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n         self.lora_rank = lora_rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else lora_rank\n         self.lora_enabled = False\n         self.input_spec = InputSpec(min_ndim=2)\n         self.supports_masking = True\n@@ -135,9 +142,9 @@ class Dense(Layer):\n                 \"You must build the layer before accessing `kernel`.\"\n             )\n         if self.lora_enabled:\n-            return self._kernel + ops.matmul(\n-                self.lora_kernel_a, self.lora_kernel_b\n-            )\n+            return self._kernel + (\n+                self.lora_alpha / self.lora_rank\n+            ) * ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n         return self._kernel\n \n     def call(self, inputs, training=None):\n@@ -154,7 +161,11 @@ class Dense(Layer):\n         return tuple(output_shape)\n \n     def enable_lora(\n-        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+        self,\n+        rank,\n+        lora_alpha=None,\n+        a_initializer=\"he_uniform\",\n+        b_initializer=\"zeros\",\n     ):\n         if self.kernel_constraint:\n             raise ValueError(\n@@ -187,6 +198,7 @@ class Dense(Layer):\n         self._tracker.lock()\n         self.lora_enabled = True\n         self.lora_rank = rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else rank\n \n     def save_own_variables(self, store):\n         # Do nothing if the layer isn't yet built\n@@ -261,6 +273,7 @@ class Dense(Layer):\n         }\n         if self.lora_rank:\n             config[\"lora_rank\"] = self.lora_rank\n+            config[\"lora_alpha\"] = self.lora_alpha\n         return {**base_config, **config}\n \n     def _check_load_own_variables(self, store):\n@@ -410,7 +423,7 @@ class Dense(Layer):\n         if self.lora_enabled:\n             lora_x = ops.matmul(inputs, self.lora_kernel_a)\n             lora_x = ops.matmul(lora_x, self.lora_kernel_b)\n-            x = ops.add(x, lora_x)\n+            x = ops.add(x, (self.lora_alpha / self.lora_rank) * lora_x)\n         if self.bias is not None:\n             x = ops.add(x, self.bias)\n         if self.activation is not None:\n@@ -544,7 +557,8 @@ class Dense(Layer):\n                 kernel_value = ops.divide(kernel_value, kernel_scale)\n                 kernel_value = ops.add(\n                     kernel_value,\n-                    ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n+                    (self.lora_alpha / self.lora_rank)\n+                    * ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n                 )\n                 kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                     kernel_value, axis=0, to_numpy=True\n\n@@ -271,6 +271,32 @@ class DenseTest(testing.TestCase):\n         model.load_weights(temp_filepath)\n         self.assertAllClose(model.predict(x), new_model.predict(x))\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora_with_alpha(self):\n+        # Create a `Dense` layer and build it.\n+        layer = layers.Dense(units=8)\n+        layer.build((None, 4))\n+\n+        # Enable LoRA with `rank`=2 and `lora_alpha`=3.0.\n+        layer.enable_lora(2, lora_alpha=3.0)\n+        self.assertEqual(layer.lora_rank, 2)\n+        self.assertEqual(layer.lora_alpha, 3.0)\n+\n+        # Manually compute the expected effective kernel:\n+        # `effective_kernel_expected` = `base_kernel` +\n+        # `lora_alpha / lora_rank` * `lora_kernel_a @ lora_kernel_b`\n+        base_kernel = ops.convert_to_numpy(layer._kernel)\n+        lora_update = np.matmul(\n+            ops.convert_to_numpy(layer.lora_kernel_a),\n+            ops.convert_to_numpy(layer.lora_kernel_b),\n+        )\n+        effective_kernel_expected = base_kernel + (3.0 / 2) * lora_update\n+\n+        # Verify that the effective kernel matches expectation.\n+        self.assertAllClose(\n+            ops.convert_to_numpy(layer.kernel), effective_kernel_expected\n+        )\n+\n     @pytest.mark.requires_trainable_backend\n     def test_lora_weight_name(self):\n         class MyModel(models.Model):\n\n@@ -58,6 +58,11 @@ class EinsumDense(Layer):\n             computation cost of fine-tuning large dense layers.\n             You can also enable LoRA on an existing\n             `EinsumDense` layer by calling `layer.enable_lora(rank)`.\n+         lora_alpha: Optional integer. If set, this parameter scales the\n+            low-rank adaptation delta (computed as the product of two lower-rank\n+            trainable matrices) during the forward pass. The delta is scaled by\n+            `lora_alpha / lora_rank`, allowing you to fine-tune the strength of\n+            the LoRA adjustment independently of `lora_rank`.\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Examples:\n@@ -125,6 +130,7 @@ class EinsumDense(Layer):\n         kernel_constraint=None,\n         bias_constraint=None,\n         lora_rank=None,\n+        lora_alpha=None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -142,6 +148,7 @@ class EinsumDense(Layer):\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n         self.lora_rank = lora_rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else lora_rank\n         self.lora_enabled = False\n \n     def build(self, input_shape):\n@@ -184,7 +191,7 @@ class EinsumDense(Layer):\n             self.bias = None\n         self.built = True\n         if self.lora_rank:\n-            self.enable_lora(self.lora_rank)\n+            self.enable_lora(self.lora_rank, lora_alpha=self.lora_alpha)\n \n     @property\n     def kernel(self):\n@@ -193,9 +200,9 @@ class EinsumDense(Layer):\n                 \"You must build the layer before accessing `kernel`.\"\n             )\n         if self.lora_enabled:\n-            return self._kernel + ops.matmul(\n-                self.lora_kernel_a, self.lora_kernel_b\n-            )\n+            return self._kernel + (\n+                self.lora_alpha / self.lora_rank\n+            ) * ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n         return self._kernel\n \n     def compute_output_shape(self, _):\n@@ -210,7 +217,11 @@ class EinsumDense(Layer):\n         return x\n \n     def enable_lora(\n-        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+        self,\n+        rank,\n+        lora_alpha=None,\n+        a_initializer=\"he_uniform\",\n+        b_initializer=\"zeros\",\n     ):\n         if self.kernel_constraint:\n             raise ValueError(\n@@ -243,6 +254,7 @@ class EinsumDense(Layer):\n         self._tracker.lock()\n         self.lora_enabled = True\n         self.lora_rank = rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else rank\n \n     def save_own_variables(self, store):\n         # Do nothing if the layer isn't yet built\n@@ -321,6 +333,7 @@ class EinsumDense(Layer):\n         }\n         if self.lora_rank:\n             config[\"lora_rank\"] = self.lora_rank\n+            config[\"lora_alpha\"] = self.lora_alpha\n         return {**base_config, **config}\n \n     def _check_load_own_variables(self, store):\n@@ -525,7 +538,7 @@ class EinsumDense(Layer):\n         if self.lora_enabled:\n             lora_x = ops.einsum(self.equation, inputs, self.lora_kernel_a)\n             lora_x = ops.matmul(lora_x, self.lora_kernel_b)\n-            x = ops.add(x, lora_x)\n+            x = ops.add(x, (self.lora_alpha / self.lora_rank) * lora_x)\n         if self.bias is not None:\n             x += self.bias\n         if self.activation is not None:\n@@ -700,7 +713,8 @@ class EinsumDense(Layer):\n                 kernel_value = ops.divide(kernel_value, kernel_scale)\n                 kernel_value = ops.add(\n                     kernel_value,\n-                    ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n+                    (self.lora_alpha / self.lora_rank)\n+                    * ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n                 )\n                 kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                     kernel_value, axis=self._kernel_reduced_axes, to_numpy=True\n\n@@ -361,6 +361,49 @@ class EinsumDenseTest(testing.TestCase):\n         model.load_weights(temp_filepath)\n         self.assertAllClose(model.predict(x), new_model.predict(x))\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora_with_alpha(self):\n+        # Use a simple equation that mimics a `Dense` layer behavior.\n+        equation = \"ab,bc->ac\"\n+        output_shape = 3  # This means the kernel shape will be (input_dim, 3).\n+        bias_axes = None\n+\n+        # Create and build the `EinsumDense` layer\n+        # with an input shape (None, 2).\n+        layer = layers.EinsumDense(\n+            equation=equation, output_shape=output_shape, bias_axes=bias_axes\n+        )\n+        # Build the layer with an input shape of (batch, 2).\n+        layer.build((None, 2))\n+\n+        # Set the base kernel weights to a known value.\n+        base_kernel = np.array(\n+            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float32\n+        )\n+        layer._kernel.assign(base_kernel)\n+\n+        # Enable LoRA with `rank`=2 and a custom `lora_alpha`=3.0.\n+        layer.enable_lora(rank=2, lora_alpha=3.0)\n+        self.assertEqual(layer.lora_rank, 2)\n+        self.assertEqual(layer.lora_alpha, 3.0)\n+\n+        # The expected shapes are:\n+        #   `base_kernel`: (2, 3)\n+        #   `lora_kernel_a`: (2, 2) and `lora_kernel_b`: (2, 3)\n+        a_val = np.array([[0.1, 0.2], [0.3, 0.4]], dtype=np.float32)\n+        b_val = np.array([[0.5, 0.6, 0.7], [0.8, 0.9, 1.0]], dtype=np.float32)\n+        layer.lora_kernel_a.assign(a_val)\n+        layer.lora_kernel_b.assign(b_val)\n+\n+        # Compute expected effective kernel.\n+        # Scaling factor is `lora_alpha / lora_rank` = 3.0 / 2 = 1.5\n+        expected_delta = 1.5 * np.matmul(a_val, b_val)\n+        expected_kernel = base_kernel + expected_delta\n+\n+        # Verify that the effective kernel property returns the expected value.\n+        actual_kernel = ops.convert_to_numpy(layer.kernel)\n+        self.assertAllClose(actual_kernel, expected_kernel)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_lora_rank_argument(self):\n         self.run_layer_test(\n\n@@ -65,6 +65,11 @@ class Embedding(Layer):\n             computation cost of fine-tuning large embedding layers.\n             You can also enable LoRA on an existing\n             `Embedding` layer by calling `layer.enable_lora(rank)`.\n+        lora_alpha: Optional integer. If set, this parameter scales the\n+            low-rank adaptation delta (computed as the product of two lower-rank\n+            trainable matrices) during the forward pass. The delta is scaled by\n+            `lora_alpha / lora_rank`, allowing you to fine-tune the strength of\n+            the LoRA adjustment independently of `lora_rank`.\n \n     Input shape:\n         2D tensor with shape: `(batch_size, input_length)`.\n@@ -83,6 +88,7 @@ class Embedding(Layer):\n         mask_zero=False,\n         weights=None,\n         lora_rank=None,\n+        lora_alpha=None,\n         **kwargs,\n     ):\n         input_length = kwargs.pop(\"input_length\", None)\n@@ -100,6 +106,7 @@ class Embedding(Layer):\n         self.supports_masking = mask_zero\n         self.autocast = False\n         self.lora_rank = lora_rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else lora_rank\n         self.lora_enabled = False\n \n         if weights is not None:\n@@ -129,9 +136,10 @@ class Embedding(Layer):\n     @property\n     def embeddings(self):\n         if self.lora_enabled:\n-            return self._embeddings + ops.matmul(\n-                self.lora_embeddings_a, self.lora_embeddings_b\n-            )\n+            return self._embeddings + (\n+                self.lora_alpha / self.lora_rank\n+            ) * ops.matmul(self.lora_embeddings_a, self.lora_embeddings_b)\n+\n         return self._embeddings\n \n     def call(self, inputs):\n@@ -149,7 +157,11 @@ class Embedding(Layer):\n         return (*input_shape, self.output_dim)\n \n     def enable_lora(\n-        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+        self,\n+        rank,\n+        lora_alpha=None,\n+        a_initializer=\"he_uniform\",\n+        b_initializer=\"zeros\",\n     ):\n         if self.embeddings_constraint:\n             raise ValueError(\n@@ -182,6 +194,7 @@ class Embedding(Layer):\n         self._tracker.lock()\n         self.lora_enabled = True\n         self.lora_rank = rank\n+        self.lora_alpha = lora_alpha if lora_alpha is not None else rank\n \n     def save_own_variables(self, store):\n         # Do nothing if the layer isn't yet built\n@@ -246,6 +259,7 @@ class Embedding(Layer):\n         }\n         if self.lora_rank:\n             config[\"lora_rank\"] = self.lora_rank\n+            config[\"lora_alpha\"] = self.lora_alpha\n         return {**base_config, **config}\n \n     def _check_load_own_variables(self, store):\n@@ -339,7 +353,9 @@ class Embedding(Layer):\n         if self.lora_enabled:\n             lora_outputs = ops.take(self.lora_embeddings_a, inputs, axis=0)\n             lora_outputs = ops.matmul(lora_outputs, self.lora_embeddings_b)\n-            outputs = ops.add(outputs, lora_outputs)\n+            outputs = ops.add(\n+                outputs, (self.lora_alpha / self.lora_rank) * lora_outputs\n+            )\n         return outputs\n \n     def quantize(self, mode, type_check=True):\n\n@@ -181,6 +181,38 @@ class EmbeddingTest(test_case.TestCase):\n         model.load_weights(temp_filepath)\n         self.assertAllClose(model.predict(x), new_model.predict(x))\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora_with_alpha(self):\n+        # Create an `Embedding` layer without specifying `lora_rank`\n+        layer = layers.Embedding(input_dim=3, output_dim=2)\n+        layer.build((None,))  # Build the layer\n+\n+        # Set the base embeddings to known values.\n+        base_emb = np.array(\n+            [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype=np.float32\n+        )\n+        layer.embeddings.assign(base_emb)\n+\n+        # Enable LoRA with a custom alpha: `rank`=2, `lora_alpha`=3.0.\n+        layer.enable_lora(2, lora_alpha=3.0)\n+        self.assertEqual(layer.lora_rank, 2)\n+        self.assertEqual(layer.lora_alpha, 3.0)\n+\n+        # Manually assign known values to lora weights.\n+        a_val = np.array([[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]], dtype=np.float32)\n+        b_val = np.array([[0.5, 0.5], [0.6, 0.6]], dtype=np.float32)\n+        layer.lora_embeddings_a.assign(a_val)\n+        layer.lora_embeddings_b.assign(b_val)\n+\n+        # Compute the expected delta.\n+        # Scaling factor: (3.0 / 2) = 1.5\n+        effective_delta = 1.5 * np.matmul(a_val, b_val)\n+        expected_embeddings = base_emb + effective_delta\n+\n+        # Verify that the effective embeddings match expectation.\n+        actual_embeddings = ops.convert_to_numpy(layer.embeddings)\n+        self.assertAllClose(actual_embeddings, expected_embeddings)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_lora_rank_argument(self):\n         self.run_layer_test(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea84f00658e1990b7ada5bf06eff0b2ee23fc4c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 60 | Churn Cumulative: 1585 | Contributors (this commit): 15 | Commits (past 90d): 23 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -328,11 +328,67 @@ def arctanh(x):\n \n \n def argmax(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`argmax` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    if rank == 0:\n+        return OpenVINOKerasTensor(ov_opset.constant([0], Type.i32).output(0))\n+    if axis is None:\n+        flatten_shape = ov_opset.constant(\n+            [-1] + [1] * (rank - 1), Type.i32\n+        ).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+        k = ov_opset.constant(1, Type.i32).output(0)\n+    else:\n+        if axis < 0:\n+            axis = rank + axis\n+        k = ov_opset.constant(1, Type.i32).output(0)\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k,\n+        axis=axis,\n+        mode=\"max\",\n+        sort=\"value\",\n+        stable=True,\n+        index_element_type=Type.i32,\n+    )\n+    topk_indices = topk_outputs.output(1)\n+    if not keepdims:\n+        topk_indices = ov_opset.squeeze(topk_indices, [axis]).output(0)\n+    return OpenVINOKerasTensor(topk_indices)\n \n \n def argmin(x, axis=None, keepdims=False):\n-    raise NotImplementedError(\"`argmin` is not supported with openvino backend\")\n+    x = get_ov_output(x)\n+    x_shape = x.get_partial_shape()\n+    rank = x_shape.rank.get_length()\n+    if rank == 0:\n+        return OpenVINOKerasTensor(ov_opset.constant([0], Type.i32).output(0))\n+    if axis is None:\n+        flatten_shape = ov_opset.constant(\n+            [-1] + [1] * (rank - 1), Type.i32\n+        ).output(0)\n+        x = ov_opset.reshape(x, flatten_shape, False).output(0)\n+        axis = 0\n+        k = ov_opset.constant(1, Type.i32).output(0)\n+    else:\n+        if axis < 0:\n+            axis = rank + axis\n+        k = ov_opset.constant(1, Type.i32).output(0)\n+    topk_outputs = ov_opset.topk(\n+        x,\n+        k=k,\n+        axis=axis,\n+        mode=\"min\",\n+        sort=\"value\",\n+        stable=True,\n+        index_element_type=Type.i32,\n+    )\n+    topk_indices = topk_outputs.output(1)\n+    if not keepdims:\n+        topk_indices = ov_opset.squeeze(topk_indices, [axis]).output(0)\n+    return OpenVINOKerasTensor(topk_indices)\n \n \n def argsort(x, axis=-1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78ca6a46b690fbad86dff20ac826e821be281a03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 81/81 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -131,7 +131,7 @@ def indexed_slices_union_indices_and_values(x1, x2_indices, x2_values=None):\n         )\n         to_union_indices = tf.gather(indices_indices, union_indices)\n         values_with_leading_zeros = tf.concat(\n-            [tf.zeros((1,) + values.shape[1:], values.dtype), values], axis=0\n+            [tf.zeros_like(values[0:1]), values], axis=0\n         )\n         return tf.gather(values_with_leading_zeros, to_union_indices)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f8bbca0dfa4d93dd856b628fcbb4ad951d6a962", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 1604 | Contributors (this commit): 16 | Commits (past 90d): 24 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -850,9 +850,22 @@ def imag(x):\n \n \n def isclose(x1, x2, rtol=1e-5, atol=1e-8, equal_nan=False):\n-    raise NotImplementedError(\n-        \"`isclose` is not supported with openvino backend\"\n-    )\n+    dtype = OPENVINO_DTYPES[config.floatx()]\n+\n+    x1 = ov_opset.convert(get_ov_output(x1), dtype)\n+    x2 = ov_opset.convert(get_ov_output(x2), dtype)\n+    rtol = ov_opset.convert(get_ov_output(rtol), dtype)\n+    atol = ov_opset.convert(get_ov_output(atol), dtype)\n+\n+    abs_diff = ov_opset.abs(x1 - x2)\n+    abs_x2 = ov_opset.abs(x2)\n+    total_tolerance = atol + rtol * abs_x2\n+    is_close = ov_opset.less_equal(abs_diff, total_tolerance)\n+    if equal_nan:\n+        both_nan = ov_opset.logical_and(ov_opset.isnan(x1), ov_opset.isnan(x2))\n+        is_close = ov_opset.logical_or(is_close, both_nan)\n+\n+    return OpenVINOKerasTensor(is_close.output(0))\n \n \n def isfinite(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2111fbca07bd01a98c443f22d4117514fe82a367", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 53 | Churn Cumulative: 76 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,15 +58,25 @@ class Softmax(Layer):\n             inputs += adder\n         if isinstance(self.axis, (tuple, list)):\n             if len(self.axis) > 1:\n-                return backend.numpy.exp(\n+                outputs = backend.numpy.exp(\n                     inputs\n                     - backend.math.logsumexp(\n                         inputs, axis=self.axis, keepdims=True\n                     )\n                 )\n             else:\n-                return activations.softmax(inputs, axis=self.axis[0])\n-        return activations.softmax(inputs, axis=self.axis)\n+                outputs = activations.softmax(inputs, axis=self.axis[0])\n+        else:\n+            outputs = activations.softmax(inputs, axis=self.axis)\n+\n+        if mask is not None:\n+            # Apply the mask to the softmax output to ensure that masked\n+            # values are set to 0 in case the entire axis is masked.\n+            outputs = backend.numpy.multiply(\n+                outputs, backend.cast(mask, outputs.dtype)\n+            )\n+\n+        return outputs\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -49,3 +49,40 @@ class SoftmaxTest(testing.TestCase):\n         )\n         result = softmax_layer(input)\n         self.assertAllClose(result, expected_output)\n+\n+    def test_softmax_masked_values_are_zero_including_fully_masked(self):\n+        \"\"\"\n+        Tests softmax with mask on default axis (-1).\n+        Ensures output is 0 where mask is False.\n+        Includes a row where all elements are masked.\n+        \"\"\"\n+        softmax_layer = softmax.Softmax()  # Default axis = -1\n+\n+        input = np.array(\n+            [\n+                [1.0, 2.0, 5.0, 1.0],\n+                [1.0, 1.0, 1.0, 1.0],\n+                [3.0, 1.0, 2.0, 4.0],\n+            ],\n+            dtype=np.float32,\n+        )\n+        mask = np.array(\n+            [\n+                [True, True, False, False],  # Partially masked\n+                [False, False, False, False],  # Fully masked\n+                [True, True, True, True],  # Not masked\n+            ],\n+            dtype=bool,\n+        )\n+\n+        expected_output = np.array(\n+            [\n+                [0.268941, 0.731059, 0.0, 0.0],  # last two masked\n+                [0.0, 0.0, 0.0, 0.0],  # Fully masked row should be all zeros\n+                [0.236883, 0.032059, 0.087144, 0.643914],\n+            ]\n+        )\n+\n+        result = softmax_layer(input, mask=mask)\n+\n+        self.assertAllClose(result, expected_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c90a3a58862c8fc2890597da3564eb6384c62278", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 458 | Contributors (this commit): 14 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -449,10 +449,6 @@ class JAXTrainer(base_trainer.Trainer):\n \n                 # Override with model metrics instead of last step logs if\n                 # needed.\n-                # The jax spmd_mode is need for multi-process context, since the\n-                # metrics values are replicated, and we don't want to do a all\n-                # gather, and only need the local copy of the value.\n-                with jax.spmd_mode(\"allow_all\"):\n                 epoch_logs = dict(self._get_metrics_result_or_logs(logs))\n \n                 # Run validation.\n@@ -605,10 +601,6 @@ class JAXTrainer(base_trainer.Trainer):\n         # Reattach state back to model (if not already done by a callback).\n         self.jax_state_sync()\n \n-        # The jax spmd_mode is need for multi-process context, since the\n-        # metrics values are replicated, and we don't want to do a all\n-        # gather, and only need the local copy of the value.\n-        with jax.spmd_mode(\"allow_all\"):\n         logs = self._get_metrics_result_or_logs(logs)\n         callbacks.on_test_end(logs)\n         self._jax_state = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d52164c4b077e7995c5231d7d3a1ddcd3fb03ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 26 | Churn Cumulative: 123 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,8 @@ class TorchModuleWrapper(Layer):\n             instance, then its parameters must be initialized before\n             passing the instance to `TorchModuleWrapper` (e.g. by calling\n             it once).\n+        output_shape :The shape of the output of this layer. It helps Keras\n+            perform automatic shape inference.\n         name: The name of the layer (string).\n \n     Example:\n@@ -80,7 +82,7 @@ class TorchModuleWrapper(Layer):\n     ```\n     \"\"\"\n \n-    def __init__(self, module, name=None, **kwargs):\n+    def __init__(self, module, name=None, output_shape=None, **kwargs):\n         super().__init__(name=name, **kwargs)\n         import torch.nn as nn\n \n@@ -98,6 +100,7 @@ class TorchModuleWrapper(Layer):\n \n         self.module = module.to(get_device())\n         self._track_module_parameters()\n+        self.output_shape = output_shape\n \n     def parameters(self, recurse=True):\n         return self.module.parameters(recurse=recurse)\n@@ -138,13 +141,21 @@ class TorchModuleWrapper(Layer):\n             state_dict[key] = convert_to_tensor(store[key])\n         self.module.load_state_dict(state_dict)\n \n+    def compute_output_shape(self, input_shape):\n+        if self.output_shape is None:\n+            return super().compute_output_shape(input_shape)\n+        return self.output_shape\n+\n     def get_config(self):\n         base_config = super().get_config()\n         import torch\n \n         buffer = io.BytesIO()\n         torch.save(self.module, buffer)\n-        config = {\"module\": buffer.getvalue()}\n+        config = {\n+            \"module\": buffer.getvalue(),\n+            \"output_shape\": self.output_shape,\n+        }\n         return {**base_config, **config}\n \n     @classmethod\n\n@@ -5,6 +5,7 @@ import pytest\n import torch\n from absl.testing import parameterized\n \n+import keras\n from keras.src import backend\n from keras.src import layers\n from keras.src import models\n@@ -235,3 +236,13 @@ class TorchUtilsTest(testing.TestCase):\n         new_mw = TorchModuleWrapper.from_config(config)\n         for ref_w, new_w in zip(mw.get_weights(), new_mw.get_weights()):\n             self.assertAllClose(ref_w, new_w, atol=1e-5)\n+\n+    def test_build_model(self):\n+        x = keras.Input([4])\n+        z = TorchModuleWrapper(torch.nn.Linear(4, 8), output_shape=[None, 8])(x)\n+        y = TorchModuleWrapper(torch.nn.Linear(8, 16), output_shape=[None, 16])(\n+            z\n+        )\n+        model = keras.Model(x, y)\n+        self.assertEqual(model.predict(np.zeros([5, 4])).shape, (5, 16))\n+        self.assertEqual(model(np.zeros([5, 4])).shape, (5, 16))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44a655bdb28037046ab279a49d4cd679fea7ca50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 100 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 120 | Churn Cumulative: 317 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -355,110 +355,30 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n \n         def run_output_asserts(layer, output, eager=False):\n             if expected_output_shape is not None:\n-                if isinstance(expected_output_shape, tuple) and is_shape_tuple(\n-                    expected_output_shape[0]\n-                ):\n-                    self.assertIsInstance(output, tuple)\n-                    self.assertEqual(\n-                        len(output),\n-                        len(expected_output_shape),\n-                        msg=\"Unexpected number of outputs\",\n+\n+                def verify_shape(expected_shape, x):\n+                    return expected_shape == x.shape\n+\n+                shapes_match = tree.map_structure_up_to(\n+                    output, verify_shape, expected_output_shape, output\n                 )\n-                    output_shape = tuple(v.shape for v in output)\n-                    self.assertEqual(\n-                        expected_output_shape,\n-                        output_shape,\n-                        msg=\"Unexpected output shape\",\n-                    )\n-                elif isinstance(expected_output_shape, tuple):\n-                    self.assertEqual(\n-                        expected_output_shape,\n-                        output.shape,\n-                        msg=\"Unexpected output shape\",\n-                    )\n-                elif isinstance(expected_output_shape, dict):\n-                    self.assertIsInstance(output, dict)\n-                    self.assertEqual(\n-                        set(output.keys()),\n-                        set(expected_output_shape.keys()),\n-                        msg=\"Unexpected output dict keys\",\n-                    )\n-                    output_shape = {k: v.shape for k, v in output.items()}\n-                    self.assertEqual(\n-                        expected_output_shape,\n-                        output_shape,\n-                        msg=\"Unexpected output shape\",\n-                    )\n-                elif isinstance(expected_output_shape, list):\n-                    self.assertIsInstance(output, list)\n-                    self.assertEqual(\n-                        len(output),\n-                        len(expected_output_shape),\n-                        msg=\"Unexpected number of outputs\",\n-                    )\n-                    output_shape = [v.shape for v in output]\n-                    self.assertEqual(\n-                        expected_output_shape,\n-                        output_shape,\n-                        msg=\"Unexpected output shape\",\n-                    )\n-                else:\n-                    raise ValueError(\n-                        \"The type of expected_output_shape is not supported\"\n+                self.assertTrue(\n+                    all(tree.flatten(shapes_match)),\n+                    msg=f\"Expected output shapes {expected_output_shape} but \"\n+                    f\"received {tree.map_structure(lambda x: x.shape, output)}\",\n                 )\n             if expected_output_dtype is not None:\n-                if isinstance(expected_output_dtype, tuple):\n-                    self.assertIsInstance(output, tuple)\n-                    self.assertEqual(\n-                        len(output),\n-                        len(expected_output_dtype),\n-                        msg=\"Unexpected number of outputs\",\n+\n+                def verify_dtype(expected_dtype, x):\n+                    return expected_dtype == backend.standardize_dtype(x.dtype)\n+\n+                dtypes_match = tree.map_structure(\n+                    verify_dtype, expected_output_dtype, output\n                 )\n-                    output_dtype = tuple(\n-                        backend.standardize_dtype(v.dtype) for v in output\n-                    )\n-                    self.assertEqual(\n-                        expected_output_dtype,\n-                        output_dtype,\n-                        msg=\"Unexpected output dtype\",\n-                    )\n-                elif isinstance(expected_output_dtype, dict):\n-                    self.assertIsInstance(output, dict)\n-                    self.assertEqual(\n-                        set(output.keys()),\n-                        set(expected_output_dtype.keys()),\n-                        msg=\"Unexpected output dict keys\",\n-                    )\n-                    output_dtype = {\n-                        k: backend.standardize_dtype(v.dtype)\n-                        for k, v in output.items()\n-                    }\n-                    self.assertEqual(\n-                        expected_output_dtype,\n-                        output_dtype,\n-                        msg=\"Unexpected output dtype\",\n-                    )\n-                elif isinstance(expected_output_dtype, list):\n-                    self.assertIsInstance(output, list)\n-                    self.assertEqual(\n-                        len(output),\n-                        len(expected_output_dtype),\n-                        msg=\"Unexpected number of outputs\",\n-                    )\n-                    output_dtype = [\n-                        backend.standardize_dtype(v.dtype) for v in output\n-                    ]\n-                    self.assertEqual(\n-                        expected_output_dtype,\n-                        output_dtype,\n-                        msg=\"Unexpected output dtype\",\n-                    )\n-                else:\n-                    output_dtype = tree.flatten(output)[0].dtype\n-                    self.assertEqual(\n-                        expected_output_dtype,\n-                        backend.standardize_dtype(output_dtype),\n-                        msg=\"Unexpected output dtype\",\n+                self.assertTrue(\n+                    all(tree.flatten(dtypes_match)),\n+                    msg=f\"Expected output dtypes {expected_output_dtype} but \"\n+                    f\"received {tree.map_structure(lambda x: x.dtype, output)}\",\n                 )\n             if expected_output_sparse:\n                 for x in tree.flatten(output):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e61510d0a951f9f789a7c7db61e4ef30b069cc65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 39 | Churn Cumulative: 169 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.8148148148148148\n\nDIFF:\n@@ -168,7 +168,12 @@ def get_tensor_spec(batches):\n \n         dtype = backend.standardize_dtype(x.dtype)\n         if isinstance(x, tf.RaggedTensor):\n-            return tf.RaggedTensorSpec(shape=shape, dtype=dtype)\n+            return tf.RaggedTensorSpec(\n+                shape=shape,\n+                dtype=dtype,\n+                ragged_rank=x.ragged_rank,\n+                row_splits_dtype=x.row_splits.dtype,\n+            )\n         if (\n             isinstance(x, tf.SparseTensor)\n             or is_scipy_sparse(x)\n\n@@ -166,7 +166,7 @@ class GeneratorDataAdapterTest(testing.TestCase):\n         not backend.SUPPORTS_SPARSE_TENSORS,\n         reason=\"Backend does not support sparse tensors\",\n     )\n-    def test_scipy_sparse_tensors(self, generator_type):\n+    def test_sparse_tensors(self, generator_type):\n         if generator_type == \"tf\":\n             x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 4))\n             y = tf.SparseTensor([[0, 0], [1, 1]], [3.0, 4.0], (2, 2))\n@@ -197,3 +197,33 @@ class GeneratorDataAdapterTest(testing.TestCase):\n             self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.shape, (2, 4))\n             self.assertEqual(by.shape, (2, 2))\n+\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_RAGGED_TENSORS,\n+        reason=\"Backend does not support ragged tensors\",\n+    )\n+    def test_ragged_tensors(self):\n+        x = tf.ragged.constant(\n+            [[[0.0, 1.0]], [[2.0, 3.0], [4.0, 5.0]]], ragged_rank=1\n+        )\n+        y = tf.ragged.constant(\n+            [[[0.0, 1.0]], [[0.0, 1.0], [0.0, 1.0]]], ragged_rank=1\n+        )\n+\n+        def generate():\n+            for _ in range(4):\n+                yield x, y\n+\n+        adapter = generator_data_adapter.GeneratorDataAdapter(generate())\n+\n+        if backend.backend() == \"tensorflow\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.RaggedTensor\n+\n+        for batch in it:\n+            self.assertEqual(len(batch), 2)\n+            bx, by = batch\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n+            self.assertEqual(bx.shape, (2, None, 2))\n+            self.assertEqual(by.shape, (2, None, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#128e28018753c5bd8638b4bb60c100b8749134b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 356 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 15 | Methods Changed: 16 | Complexity Δ (Sum/Max): 40/40 | Churn Δ: 369 | Churn Cumulative: 373 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.36363636363636365\n\nDIFF:\n@@ -1,7 +1,9 @@\n+import numpy as np\n import torch\n \n from keras.src import tree\n from keras.src.backend.torch.core import convert_to_tensor\n+from keras.src.backend.torch.core import get_device\n \n \n def rnn(\n@@ -46,11 +48,13 @@ def rnn(\n     def _expand_mask(mask_t, input_t, fixed_dim=1):\n         if tree.is_nested(mask_t):\n             raise ValueError(\n-                f\"mask_t is expected to be tensor, but got {mask_t}\"\n+                f\"mask_t is expected to be tensor,\\\n+                  but got {mask_t}\"\n             )\n         if tree.is_nested(input_t):\n             raise ValueError(\n-                f\"input_t is expected to be tensor, but got {input_t}\"\n+                f\"input_t is expected to be tensor,\\\n+                  but got {input_t}\"\n             )\n         rank_diff = len(input_t.shape) - len(mask_t.shape)\n         for _ in range(rank_diff):\n@@ -79,7 +83,7 @@ def rnn(\n         if tree.is_nested(inputs):\n             processed_input = tree.map_structure(\n                 _process_single_input_t, inputs\n-            )\n+            )  # noqa: E501\n         else:\n             processed_input = (_process_single_input_t(inputs),)\n \n@@ -111,12 +115,12 @@ def rnn(\n                 flat_new_states = tree.flatten(new_states)\n                 tiled_mask_t = tuple(\n                     _expand_mask(mask_t, s) for s in flat_states\n-                )\n+                )  # noqa: E501\n                 flat_final_states = tuple(\n                     torch.where(m, s, ps)\n                     for m, s, ps in zip(\n                         tiled_mask_t, flat_new_states, flat_states\n-                    )\n+                    )  # noqa: E501\n                 )\n                 states = tree.pack_sequence_as(states, flat_final_states)\n \n@@ -147,7 +151,7 @@ def rnn(\n                 inp = _get_input_tensor(i)\n                 output, states = step_function(\n                     inp, tuple(states) + tuple(constants)\n-                )\n+                )  # noqa: E501\n                 if return_all_outputs:\n                     successive_outputs.append(output)\n                     successive_states.append(states)\n@@ -237,7 +241,7 @@ def rnn(\n             def compute_masked_output(mask_t, flat_out, flat_mask):\n                 return tuple(\n                     torch.where(mask_t, o, zo)\n-                    for (o, zo) in zip(flat_out, flat_mask)\n+                    for (o, zo) in zip(flat_out, flat_mask)  # noqa: E501\n                 )\n \n         else:\n@@ -286,7 +290,7 @@ def rnn(\n                 flat_final_state = compute_masked_output(\n                     mask_t, flat_new_state, flat_state\n                 )\n-                new_states = tree.pack_sequence_as(new_states, flat_final_state)\n+                new_states = tree.pack_sequence_as(new_states, flat_final_state)  # noqa: E501\n \n                 ta_index_to_write = time if return_all_outputs else 0\n                 for ta, out in zip(output_ta_t, flat_new_output):\n@@ -305,7 +309,7 @@ def rnn(\n             while time < time_steps_t and it < max_iterations:\n                 final_outputs = _step(\n                     time, output_ta_t, prev_output, *new_states\n-                )\n+                )  # noqa: E501\n                 time, output_ta_t, prev_output = final_outputs[:3]\n                 new_states = final_outputs[3:]\n                 it += 1\n@@ -337,7 +341,7 @@ def rnn(\n \n                 new_states = tree.pack_sequence_as(\n                     initial_states, flat_new_state\n-                )\n+                )  # noqa: E501\n                 return (time + 1, output_ta_t) + tuple(new_states)\n \n             it = 0\n@@ -371,13 +375,352 @@ def rnn(\n     return last_output, outputs, new_states\n \n \n-def cudnn_ok(*args, **kwargs):\n-    return False\n+def _is_sequence_right_padded(mask):\n+    \"\"\"Check the mask tensor and see if it right padded.\n+\n+    cuDNN uses the sequence length param to skip the tailing\n+    timestep. If the data is left padded, or not a strict right padding (has\n+    masked value in the middle of the sequence), then cuDNN won't work\n+    properly in those cases.\n+\n+    Left padded data: [[False, False, True, True, True]].\n+    Right padded data: [[True, True, True, False, False]].\n+    Mixture of mask/unmasked data: [[True, False, True, False, False]].\n+\n+    Note that for the mixed data example above, the actually data RNN should see\n+    are those 2 Trues (index 0 and 2), the index 1 False should be ignored and\n+    not pollute the internal states.\n+\n+    Args:\n+        mask: the Boolean tensor with shape [batch, timestep]\n+\n+    Returns:\n+        boolean scalar tensor, whether the mask is strictly right padded.\n+    \"\"\"\n+    # Get max sequence length\n+    max_seq_length = mask.shape[1]\n+    # Count True values in each sequence\n+    count_of_true = torch.sum(mask, dim=1)\n+    # Create right padded mask\n+    batch_size = mask.shape[0]\n+    indices = torch.arange(max_seq_length, device=mask.device).repeat(\n+        batch_size, 1\n+    )  # noqa: E501\n+    right_padded_mask = indices < count_of_true.unsqueeze(1)\n+    return torch.all(mask == right_padded_mask)\n \n \n-def lstm(*args, **kwargs):\n+def _has_fully_masked_sequence(mask):\n+    # Cudnn kernel will error out if the input sequence contains any\n+    # fully masked data. We walk around this issue by rerouting the computation\n+    # to standard kernel, until the issue on cudnn side has been fixed.  For a\n+    # fully masked sequence, it will contain all Falses. To make it easy to\n+    # check, we inverse the boolean, check if any of the sequence has all True.\n+    return torch.any(torch.all(~mask, dim=1))\n+\n+\n+def _assert_valid_mask(mask):\n+    # Check if mask is valid for cuDNN\n+    no_fully_masked = ~_has_fully_masked_sequence(mask)\n+    is_right_padded = _is_sequence_right_padded(mask)\n+    valid = no_fully_masked & is_right_padded\n+\n+    if not valid.item():\n+        error_message = (\n+            \"You are passing a RNN mask that does not correspond to \"\n+            \"right-padded sequences, while using cuDNN, which is not \"\n+            \"supported. With cuDNN, RNN masks can only be used for \"\n+            \"right-padding, e.g. `[[True, True, False, False]]` would \"\n+            \"be a valid mask, but any mask that isn't just contiguous \"\n+            \"`True`'s on the left and contiguous `False`'s on the right \"\n+            \"would be invalid. You can pass `use_cudnn=False` to your \"\n+            \"RNN layer to stop using cuDNN (this may be slower).\"\n+        )\n+        raise ValueError(error_message)\n+\n+\n+def _compute_sequence_length_from_mask(mask, batch_first):\n+    \"\"\"Calculate the sequence length tensor (1-D) based on the masking tensor.\n+\n+    The masking tensor is a 2D boolean tensor with shape [batch, timestep]. For\n+    any timestep that should be masked, the corresponding field will be False.\n+    Consider the following example:\n+      a = [[True, True, False, False]\n+           [True, True, True, False]]\n+    It is a (2, 4) tensor, and the corresponding sequence length result should\n+    be 1D tensor with value [2, 3]. Note that the masking tensor must be right\n+    padded that could be checked by, e.g., `is_sequence_right_padded()`.\n+\n+    Args:\n+        mask: Boolean tensor with shape [batch, timestep] or [timestep, batch]\n+            if time_major=True.\n+        time_major: Boolean, which indicates whether the mask is time major or\n+            batch major.\n+\n+    Returns:\n+        sequence_length: 1D int32 tensor.\n+    \"\"\"\n+    timestep_index = 0 if not batch_first else 1\n+    return torch.sum(mask.int(), dim=timestep_index)\n+\n+\n+def prepare_lstm_weights(lstm, kernel, recurrent_kernel, bias, device):\n+    \"\"\"Copies kernel and recurrent kernel weights in the Pytorch format\n+    We split the kernel and recurrent kernel weights, create associated\n+    torch tensors adapted to be in line with the Cudnn optimization.\n+    After we have copied the weights, we ensure the paramters are on\n+    the same device and memory layout is optimized for Cudnn.\n+\n+    \"\"\"\n+\n+    lstm = lstm.to(device)\n+    hidden_size = lstm.hidden_size\n+\n+    # Convert gates from Keras [i,f,c,o] to PyTorch [i,f,g,o]\n+    i_k, f_k, c_k, o_k = np.split(kernel, 4, axis=1)\n+    weight_ih_data = np.concatenate([i_k, f_k, c_k, o_k], axis=1).T\n+\n+    i_r, f_r, c_r, o_r = np.split(recurrent_kernel, 4, axis=1)\n+    weight_hh_data = np.concatenate([i_r, f_r, c_r, o_r], axis=1).T\n+\n+    if bias is not None:\n+        # Split Keras combined bias into input and hidden biases\n+        bias_ih_data = convert_to_tensor(bias, dtype=\"float32\")\n+        bias_hh_data = torch.zeros_like(bias_ih_data)\n+\n+    else:\n+        bias_ih_data = torch.zeros(4 * hidden_size, device=device)\n+        bias_hh_data = torch.zeros(4 * hidden_size, device=device)\n+\n+    # Create PyTorch tensors for weights\n+    weight_ih = convert_to_tensor(weight_ih_data, dtype=\"float32\").contiguous()\n+    weight_hh = convert_to_tensor(weight_hh_data, dtype=\"float32\").contiguous()\n+    bias_ih = convert_to_tensor(bias_ih_data, dtype=\"float32\").contiguous()\n+    bias_hh = convert_to_tensor(bias_hh_data, dtype=\"float32\").contiguous()\n+\n+    # Ensure the weights are all on the same device\n+    weight_ih = weight_ih.to(device)\n+    weight_hh = weight_hh.to(device)\n+    bias_ih = bias_ih.to(device)\n+    bias_hh = bias_hh.to(device)\n+\n+    # Copy Keras weights into Torch's flat weights\n+    with torch.no_grad():\n+        lstm.weight_ih_l0.copy_(weight_ih)\n+        lstm.weight_hh_l0.copy_(weight_hh)\n+        lstm.bias_ih_l0.copy_(bias_ih)\n+        lstm.bias_hh_l0.copy_(bias_hh)\n+\n+    # Optimize the layout\n+    lstm.flatten_parameters()\n+\n+    # After prepare_lstm_weights:\n+    # Force all LSTM parameters to be on the correct device\n+    for param in lstm.parameters():\n+        if param.device != device:\n+            param.data = param.data.to(device)\n+\n+\n+def _is_cuda_cudnn_available():\n+    # We check if the cuda device and drivers are available\n+    return torch.cuda.is_available() and torch.backends.cudnn.is_available()\n+\n+\n+def cudnn_ok(\n+    activation,\n+    recurrent_activation,\n+    unroll,\n+    use_bias=True,\n+):\n+    from keras.src import activations\n+    from keras.src import ops\n+\n+    return (\n+        activation in (activations.tanh, torch.tanh, ops.tanh)\n+        and recurrent_activation\n+        in (activations.sigmoid, torch.sigmoid, ops.sigmoid)  # noqa: E501\n+        and not unroll\n+        and use_bias\n+        and _is_cuda_cudnn_available()\n+    )\n+\n+\n+def lstm(\n+    inputs,\n+    initial_state_h,\n+    initial_state_c,\n+    mask,\n+    kernel,\n+    recurrent_kernel,\n+    bias,\n+    activation,\n+    recurrent_activation,\n+    return_sequences=False,\n+    go_backwards=False,\n+    unroll=False,\n+    batch_first=True,\n+):\n+    cudnn_supported = cudnn_ok(\n+        activation,\n+        recurrent_activation,\n+        unroll,\n+        use_bias=bias is not None,\n+    )\n+\n+    if not cudnn_supported:\n         raise NotImplementedError\n \n+    # Get device from inputs\n+    device = get_device()\n+\n+    from keras.src.backend.torch import Variable\n+\n+    if isinstance(kernel, Variable):\n+        kernel = kernel.value\n+    if isinstance(recurrent_kernel, Variable):\n+        recurrent_kernel = recurrent_kernel.value\n+    if isinstance(bias, Variable):\n+        bias = bias.value\n+\n+    # Convert to torch tensors\n+    inputs = convert_to_tensor(inputs, dtype=\"float32\")\n+    initial_state_h = convert_to_tensor(initial_state_h, dtype=\"float32\")\n+    initial_state_c = convert_to_tensor(initial_state_c, dtype=\"float32\")\n+    if mask is not None:\n+        mask = convert_to_tensor(mask, dtype=\"bool\")\n+\n+    # Preprocess for go_backwards by flipping the sequence\n+    if go_backwards:\n+        seq_dim = 1 if batch_first else 0\n+        inputs = torch.flip(inputs, dims=[seq_dim])\n+        if mask is not None:\n+            mask = torch.flip(mask, dims=[seq_dim])\n+\n+    # Move all tensors to the same device\n+    inputs = inputs.to(device)\n+    initial_state_h = initial_state_h.to(device)\n+    initial_state_c = initial_state_c.to(device)\n+    if mask is not None:\n+        mask = mask.to(device)\n+\n+    try:\n+        return _cudnn_lstm(\n+            inputs,\n+            initial_state_h,\n+            initial_state_c,\n+            kernel,\n+            recurrent_kernel,\n+            bias,\n+            mask,\n+            batch_first,\n+            go_backwards,\n+            return_sequences,\n+            device,\n+        )\n+    except Exception:\n+        raise NotImplementedError\n+\n+\n+def _cudnn_lstm(\n+    inputs,\n+    initial_state_h,\n+    initial_state_c,\n+    kernel,\n+    recurrent_kernel,\n+    bias,\n+    mask,\n+    batch_first,\n+    go_backwards,\n+    return_sequences,\n+    device,\n+):\n+    if mask is not None:\n+        _assert_valid_mask(mask)\n+        sequence_lengths = _compute_sequence_length_from_mask(mask, batch_first)\n+\n+    # Ensure inputs are in batch_first format for consistency\n+    if not batch_first:\n+        inputs = inputs.permute(1, 0, 2)\n+\n+    seq_axis, batch_axis = (0, 1) if not batch_first else (1, 0)\n+\n+    # If shape is [batch, hidden]; Make [1, batch, hidden]\n+    if initial_state_h.dim() == 2:\n+        initial_state_h = initial_state_h.unsqueeze(0)\n+        initial_state_c = initial_state_c.unsqueeze(0)\n+    # If shape is [batch, 1, hidden]\n+    elif initial_state_h.dim() == 3 and initial_state_h.shape[1] == 1:\n+        initial_state_h = initial_state_h.permute(1, 0, 2)\n+        initial_state_c = initial_state_c.permute(1, 0, 2)\n+\n+    input_size = kernel.shape[0]\n+    hidden_size = recurrent_kernel.shape[0]\n+\n+    # Configure LSTM with the provided parameters\n+    lstm = torch.nn.LSTM(\n+        input_size=input_size,\n+        hidden_size=hidden_size,\n+        num_layers=1,\n+        batch_first=batch_first,\n+        bidirectional=False,\n+    )\n+\n+    prepare_lstm_weights(lstm, kernel, recurrent_kernel, bias, device)\n+\n+    if mask is not None:\n+        # Sort and pack\n+        sorted_lengths, sorted_indices = torch.sort(\n+            sequence_lengths, descending=True\n+        )  # noqa: E501\n+        sorted_inputs = inputs[sorted_indices]\n+        sorted_initial_h = initial_state_h[:, sorted_indices]\n+        sorted_initial_c = initial_state_c[:, sorted_indices]\n+\n+        # Create the packed sequence\n+        packed_inputs = torch.nn.utils.rnn.pack_padded_sequence(\n+            sorted_inputs, sorted_lengths.cpu(), batch_first\n+        )\n+\n+        # Process with LSTM (which handles the packed sequence correctly)\n+        packed_outputs, (h_n, c_n) = lstm(\n+            packed_inputs, (sorted_initial_h, sorted_initial_c)\n+        )\n+\n+        # Unpack back to padded tensor\n+        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(\n+            packed_outputs, batch_first\n+        )  # noqa: E501\n+\n+    else:\n+        # Run LSTM without packing for fixed-length sequences\n+        outputs, (h_n, c_n) = lstm(inputs, (initial_state_h, initial_state_c))\n+\n+    outputs = outputs.detach().clone().cpu()\n+    h_n = h_n.detach().clone().cpu()\n+    c_n = c_n.detach().clone().cpu()\n+    # Reshape hidden states for return\n+    h_n = h_n.squeeze(batch_axis)\n+    c_n = c_n.squeeze(batch_axis)\n+\n+    # Return appropriate outputs based on return_sequences flag\n+\n+    if mask is not None:\n+        last_output = h_n\n+    else:\n+        last_output = outputs[:, -1] if batch_first else outputs[-1]\n+\n+    if not return_sequences:\n+        outputs = (\n+            last_output.unsqueeze(1)\n+            if batch_first\n+            else last_output.unsqueeze(0)\n+        )  # noqa: E501\n+\n+    if go_backwards and return_sequences:\n+        outputs = torch.flip(outputs, dims=[seq_axis])\n+\n+    return last_output, outputs, [h_n, c_n]\n+\n \n def gru(*args, **kwargs):\n     raise NotImplementedError\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
