{"custom_id": "keras#10deb8f267e1d982415a23a8f5c8cc44da4f4671", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 86 | Files Changed: 1 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 111 | Churn Cumulative: 150 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n import pytest\n import numpy as np\n-from keras.layers.core import Dense, Activation\n from numpy.testing import assert_allclose\n \n+from keras.layers.core import Dense, Activation\n+from keras.utils.test_utils import layer_test\n from keras.layers import normalization\n from keras.models import Sequential, Graph\n from keras import backend as K\n@@ -14,8 +15,16 @@ input_3 = np.ones((10))\n input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n \n+def basic_batchnorm_test():\n+    layer_test(normalization.BatchNormalization,\n+               kwargs={'mode': 1},\n+               input_shape=(3, 4, 2))\n+    layer_test(normalization.BatchNormalization,\n+               kwargs={'mode': 0},\n+               input_shape=(3, 4, 2))\n+\n+\n def test_batchnorm_mode_0():\n-    np.random.seed(1337)\n     model = Sequential()\n     norm_m0 = normalization.BatchNormalization(mode=0, input_shape=(10,))\n     model.add(norm_m0)\n@@ -24,11 +33,13 @@ def test_batchnorm_mode_0():\n     # centered on 5.0, variance 10.0\n     X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n     model.fit(X, X, nb_epoch=5, verbose=0)\n-    norm_m0.input = K.variable(X)\n-    out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n+    out = norm_m0.call(K.variable(X))\n+    out -= norm_m0.beta\n+    out /= norm_m0.gamma\n+    np_out = K.function([K.learning_phase()], [out])([1.])[0]\n \n-    assert_allclose(K.eval(K.mean(out)), 0.0, atol=1e-1)\n-    assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n+    assert_allclose(np_out.mean(), 0.0, atol=1e-1)\n+    assert_allclose(np_out.std(), 1.0, atol=1e-1)\n \n \n def test_batchnorm_mode_0_convnet():\n@@ -40,20 +51,21 @@ def test_batchnorm_mode_0_convnet():\n     # centered on 5.0, variance 10.0\n     X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n     model.fit(X, X, nb_epoch=5, verbose=0)\n-    norm_m0.input = K.variable(X)\n-    out = (norm_m0.get_output(train=True) - K.reshape(norm_m0.beta, (1, 3, 1, 1))) / K.reshape(norm_m0.gamma, (1, 3, 1, 1))\n+    out = norm_m0.call(K.variable(X))\n+    out -= K.reshape(norm_m0.beta, (1, 3, 1, 1))\n+    out /= K.reshape(norm_m0.gamma, (1, 3, 1, 1))\n+    np_out = K.function([K.learning_phase()], [out])([1.])[0]\n \n-    assert_allclose(K.eval(K.mean(out, axis=(0, 2, 3))), 0.0, atol=1e-1)\n-    assert_allclose(K.eval(K.std(out, axis=(0, 2, 3))), 1.0, atol=1e-1)\n+    assert_allclose(np.mean(np_out, axis=(0, 2, 3)), 0.0, atol=1e-1)\n+    assert_allclose(np.std(np_out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n \n \n def test_batchnorm_mode_1():\n-    np.random.seed(1337)\n     norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n+    norm_m1.build(input_shape=(None, 10))\n \n     for inp in [input_1, input_2, input_3]:\n-        norm_m1.input = K.variable(inp)\n-        out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n+        out = (norm_m1.call(K.variable(inp)) - norm_m1.beta) / norm_m1.gamma\n         assert_allclose(K.eval(K.mean(out)), 0.0, atol=1e-1)\n         if inp.std() > 0.:\n             assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n@@ -61,78 +73,5 @@ def test_batchnorm_mode_1():\n             assert_allclose(K.eval(K.std(out)), 0.0, atol=1e-1)\n \n \n-def test_batchnorm_shapes():\n-    \"\"\"\n-    Test batch normalization with various input shapes\n-    \"\"\"\n-    for inp in input_shapes:\n-        norm_m0 = normalization.BatchNormalization(batch_input_shape=inp.shape, mode=0)\n-        norm_m0.input = K.variable(inp)\n-        out = norm_m0.get_output(train=True)\n-        K.eval(out)\n-\n-        norm_m1 = normalization.BatchNormalization(batch_input_shape=inp.shape, mode=1)\n-        norm_m1.input = K.variable(inp)\n-        out = norm_m1.get_output(train=True)\n-        K.eval(out)\n-\n-\n-def test_batchnorm_weight_init():\n-    \"\"\"\n-    Test weight initialization\n-    \"\"\"\n-    np.random.seed(1337)\n-    norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1,\n-                                               weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n-\n-    for inp in [input_1, input_2, input_3]:\n-        norm_m1.input = K.variable(inp)\n-        out = (norm_m1.get_output(train=True) - np.ones(10)) / 1.\n-        assert_allclose(K.eval(K.mean(out)), 0.0, atol=1e-1)\n-        if inp.std() > 0.:\n-            assert_allclose(K.eval(K.std(out)), 1.0, atol=1e-1)\n-        else:\n-            assert_allclose(K.eval(K.std(out)), 0.0, atol=1e-1)\n-\n-    assert_allclose(K.eval(norm_m1.gamma), np.ones(10), atol=1e-1)\n-    assert_allclose(K.eval(norm_m1.beta), np.ones(10), atol=1e-1)\n-\n-\n-def test_batchnorm_config():\n-    norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n-                                            epsilon=0.1, momentum=0.9)\n-    norm.get_config()\n-\n-\n-def test_batchnorm_save_weights():\n-    norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n-                                            epsilon=0.1)\n-    weights = norm.get_weights()\n-    assert(len(weights) == 4)\n-    norm.set_weights(weights)\n-\n-\n-def test_batchnorm_nested():\n-    # regression test for issue #1386\n-    g = Graph()\n-    g.add_input(\"input\", input_shape=[20])\n-    g.add_node(Dense(10), \"dense\", \"input\")\n-    g.add_node(normalization.BatchNormalization(), \"bn\", \"dense\")\n-    g.add_node(Activation('relu'), \"activ\", \"bn\")\n-    g.add_output(\"output\", \"activ\")\n-\n-    g2 = Graph()\n-    g2.add_input(\"input\", input_shape=[10])\n-    g2.add_node(Dense(15), \"dense\", \"input\")\n-    g2.add_node(normalization.BatchNormalization(), \"bn\", \"dense\")\n-    g2.add_node(Activation('relu'), \"activ\", \"bn\")\n-    g2.add_output(\"output\", \"activ\")\n-\n-    model = Sequential()\n-    model.add(g)\n-    model.add(g2)\n-    model.compile(loss=\"mse\", optimizer=\"adadelta\")\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8e2df16f1169c5b6198419d44244d2cbc163c92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1169 | Contributors (this commit): 13 | Commits (past 90d): 38 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -109,8 +109,8 @@ def dot(x, y):\n \n def batch_dot(x, y, axes=None):\n     if axes:\n-        adj_x = None if axes[0][0] == x.ndim-1 else True\n-        adj_y = True if axes[1][0] == y.ndim-1 else None\n+        adj_x = None if axes[0][0] == ndim(x) - 1 else True\n+        adj_y = True if axes[1][0] == ndim(y) - 1 else None\n     else:\n         adj_x = None\n         adj_y = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#337c0c66cf8d077d3c2ec9cee552cd7911f2e62b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 223 | Files Changed: 1 | Hunks: 1 | Methods Changed: 18 | Complexity Δ (Sum/Max): -50/0 | Churn Δ: 223 | Churn Cumulative: 538 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.461038961038961\n\nDIFF:\n@@ -1,223 +0,0 @@\n-import pytest\n-import numpy as np\n-\n-from keras import backend as K\n-from keras.layers.core import *\n-from keras.layers.convolutional import *\n-from keras.layers.recurrent import SimpleRNN\n-\n-\n-def check_layer_output_shape(layer, input_data):\n-    ndim = len(input_data.shape)\n-    layer.set_input(K.placeholder(ndim=ndim), input_data.shape)\n-    expected_output_shape = layer.get_output_shape_for(input_data.shape)\n-\n-    function = K.function([layer.input], [layer.output])\n-    output = function([input_data])[0]\n-\n-    assert output.shape == expected_output_shape, str(output.shape) + ' != ' + str(expected_output_shape)\n-\n-\n-########\n-# Core #\n-########\n-def test_Reshape():\n-    input_data = np.random.random((2, 6))\n-\n-    layer = Reshape((2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-    layer = Reshape((-1,))\n-    check_layer_output_shape(layer, input_data)\n-\n-    layer = Reshape((-1, 2))\n-    check_layer_output_shape(layer, input_data)\n-\n-    layer = Reshape((2, -1))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_Permute():\n-    layer = Permute(dims=(1, 3, 2))\n-    input_data = np.random.random((2, 2, 4, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_Flatten():\n-    layer = Flatten()\n-    input_data = np.random.random((2, 2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_RepeatVector():\n-    layer = RepeatVector(2)\n-    input_data = np.random.random((2, 2))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_Dense():\n-    layer = Dense(3)\n-    input_data = np.random.random((2, 2))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_TimeDistributedDense():\n-    layer = TimeDistributedDense(2)\n-    input_data = np.random.random((2, 2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-#################\n-# Convolutional #\n-#################\n-def test_Convolution1D():\n-    for border_mode in ['same', 'valid']:\n-        for filter_length in [2, 3]:\n-            for subsample_length in [1]:\n-                if subsample_length > 1 and border_mode == 'same':\n-                    continue\n-                for input_data_shape in [(2, 3, 4), (2, 4, 4)]:\n-                    layer = Convolution1D(nb_filter=1,\n-                                          filter_length=filter_length,\n-                                          border_mode=border_mode,\n-                                          subsample_length=subsample_length)\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_Convolution2D():\n-    for border_mode in ['same', 'valid']:\n-        for nb_row, nb_col in [(3, 3), (4, 4), (3, 4)]:\n-            for subsample in [(1, 1), (2, 2), (3, 3)]:\n-                if (subsample[0] > nb_row or subsample[1] > nb_col) and border_mode == 'same':\n-                    continue\n-                for input_data_shape in [(2, 1, 5, 5), (2, 1, 6, 6)]:\n-                    layer = Convolution2D(nb_filter=1, nb_row=nb_row,\n-                                          nb_col=nb_row,\n-                                          border_mode=border_mode,\n-                                          subsample=subsample,\n-                                          dim_ordering='th')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-                for input_data_shape in [(2, 5, 5, 1)]:\n-                    layer = Convolution2D(nb_filter=1, nb_row=nb_row,\n-                                          nb_col=nb_row,\n-                                          border_mode=border_mode,\n-                                          subsample=subsample,\n-                                          dim_ordering='tf')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_MaxPooling1D():\n-    for ignore_border in [True, False]:\n-        for pool_length in [1, 2]:\n-            for stride in [1]:\n-                for input_data_shape in [(2, 3, 4), (2, 4, 4)]:\n-                    layer = MaxPooling1D(pool_length=pool_length,\n-                                         stride=stride,\n-                                         border_mode='valid')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_AveragePooling1D():\n-    for ignore_border in [True, False]:\n-        for pool_length in [1, 2]:\n-            for stride in [1]:\n-                for input_data_shape in [(2, 3, 4), (2, 4, 4)]:\n-                    layer = AveragePooling1D(pool_length=pool_length,\n-                                             stride=stride,\n-                                             border_mode='valid')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_MaxPooling2D():\n-    for ignore_border in [True, False]:\n-        for strides in [(1, 1), (2, 2)]:\n-            for pool_size in [(2, 2), (3, 3), (4, 4)]:\n-                for input_data_shape in [(2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n-                    layer = MaxPooling2D(pool_size=pool_size,\n-                                         strides=strides,\n-                                         border_mode='valid',\n-                                         dim_ordering='th')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-                for input_data_shape in [(2, 4, 4, 1)]:\n-                    layer = MaxPooling2D(pool_size=pool_size,\n-                                         strides=strides,\n-                                         border_mode='valid',\n-                                         dim_ordering='tf')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_AveragePooling2D():\n-    for ignore_border in [True, False]:\n-        for strides in [(1, 1), (2, 2)]:\n-            for pool_size in [(2, 2), (3, 3), (4, 4)]:\n-                for input_data_shape in [(2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n-                    layer = AveragePooling2D(pool_size=pool_size,\n-                                             strides=strides,\n-                                             border_mode='valid',\n-                                             dim_ordering='th')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-                for input_data_shape in [(2, 4, 4, 1)]:\n-                    layer = AveragePooling2D(pool_size=pool_size,\n-                                             strides=strides,\n-                                             border_mode='valid',\n-                                             dim_ordering='tf')\n-                    input_data = np.random.random(input_data_shape)\n-                    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_UpSampling1D():\n-    layer = UpSampling1D(length=2)\n-    input_data = np.random.random((2, 2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_UpSampling2D():\n-    layer = UpSampling2D(size=(2, 2), dim_ordering='th')\n-    input_data = np.random.random((2, 1, 2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-    layer = UpSampling2D(size=(2, 2), dim_ordering='tf')\n-    input_data = np.random.random((2, 2, 3, 1))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_ZeroPadding1D():\n-    layer = ZeroPadding1D(1)\n-    input_data = np.random.random((2, 2, 1))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-def test_ZeroPadding2D():\n-    layer = ZeroPadding2D((1, 2), dim_ordering='th')\n-    input_data = np.random.random((2, 1, 2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-    layer = ZeroPadding2D((1, 2), dim_ordering='tf')\n-    input_data = np.random.random((2, 2, 3, 1))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-# #############\n-# # Recurrent #\n-# #############\n-def test_SimpleRNN():\n-    # all recurrent layers inherit output_shape\n-    # from the same base recurrent layer\n-    layer = SimpleRNN(2)\n-    input_data = np.random.random((2, 2, 3))\n-    check_layer_output_shape(layer, input_data)\n-\n-\n-if __name__ == \"__main__\":\n-    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75bef59016a8a230823a04836e1ab6e5bf0079dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 21 | Churn Cumulative: 1190 | Contributors (this commit): 13 | Commits (past 90d): 39 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,7 +61,10 @@ def int_shape(x):\n \n \n def ndim(x):\n-    return len(x.get_shape())\n+    dims = x.get_shape()._dims\n+    if dims is not None:\n+        return len(dims)\n+    return None\n \n \n def dtype(x):\n@@ -104,7 +107,21 @@ def cast(x, dtype):\n # LINEAR ALGEBRA\n \n def dot(x, y):\n-    return tf.matmul(x, y)\n+    '''Multiplies 2 tensors.\n+    When attempting to multiply a 2D tensor\n+    with a 3D tensor, reproduces the Theano behavior\n+    (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n+    '''\n+    if ndim(x) == 2 and ndim(y) == 3:\n+        slices = []\n+        for i in range(int_shape(y)[0]):\n+            slice_i = tf.matmul(x, y[i, :, :])\n+            slice_i = expand_dims(slice_i, 1)\n+            slices.append(slice_i)\n+        out = tf.concat(1, slices)\n+        return out\n+    out = tf.matmul(x, y)\n+    return out\n \n \n def batch_dot(x, y, axes=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0836e47dfce8ecde847f3f5b9ccd559a9e0ca745", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1563 | Contributors (this commit): 17 | Commits (past 90d): 45 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -465,7 +465,9 @@ class Function(object):\n \n     def __init__(self, inputs, outputs, updates=[], **kwargs):\n         self.function = theano.function(inputs, outputs, updates=updates,\n-                                        allow_input_downcast=True, **kwargs)\n+                                        allow_input_downcast=True,\n+                                        on_unused_input='warn',\n+                                        **kwargs)\n \n     def __call__(self, inputs):\n         assert type(inputs) in {list, tuple}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52dbeb1f26bac4272e35a8bb578f16cc584c3721", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 75 | Churn Cumulative: 719 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -223,50 +223,49 @@ def test_siamese_1():\n     new_graph = model_from_yaml(yaml_str)\n \n \n-# def test_siamese_2():\n-#     # Note: not working. TODO: fix it.\n-#     graph = Graph()\n-#     graph.add_input(name='input1', input_shape=(32,))\n-#     graph.add_input(name='input2', input_shape=(32,))\n+def test_siamese_2():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n \n-#     graph.add_shared_node(Dense(4), name='shared',\n-#                           inputs=['input1', 'input2'],\n-#                           outputs=['shared_output1', 'shared_output2'])\n-#     graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n-#     graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n+    graph.add_shared_node(Dense(4), name='shared',\n+                          inputs=['input1', 'input2'],\n+                          outputs=['shared_output1', 'shared_output2'])\n+    graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n+    graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n \n-#     graph.add_output(name='output1', inputs=['dense1', 'dense2'],\n-#                      merge_mode='sum')\n-#     graph.compile('rmsprop', {'output1': 'mse'})\n+    graph.add_output(name='output1', inputs=['dense1', 'dense2'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n \n-#     graph.fit({'input1': X_train_graph,\n-#                'input2': X2_train_graph,\n-#                'output1': y_train_graph},\n-#               nb_epoch=10)\n-#     out = graph.predict({'input1': X_test_graph,\n-#                          'input2': X2_test_graph})\n-#     assert(type(out == dict))\n-#     assert(len(out) == 1)\n+    graph.fit({'input1': X_train_graph,\n+               'input2': X2_train_graph,\n+               'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph,\n+                         'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n \n-#     loss = graph.test_on_batch({'input1': X_test_graph,\n-#                                 'input2': X2_test_graph,\n-#                                 'output1': y_test_graph})\n-#     loss = graph.train_on_batch({'input1': X_test_graph,\n-#                                  'input2': X2_test_graph,\n-#                                  'output1': y_test_graph})\n-#     loss = graph.evaluate({'input1': X_test_graph,\n-#                            'input2': X2_test_graph,\n-#                            'output1': y_test_graph})\n-#     # test serialization\n-#     config = graph.get_config()\n-#     new_graph = Graph.from_config(config)\n+    loss = graph.test_on_batch({'input1': X_test_graph,\n+                                'input2': X2_test_graph,\n+                                'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph,\n+                                 'input2': X2_test_graph,\n+                                 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph,\n+                           'input2': X2_test_graph,\n+                           'output1': y_test_graph})\n+    # test serialization\n+    config = graph.get_config()\n+    new_graph = Graph.from_config(config)\n \n-#     graph.summary()\n-#     json_str = graph.to_json()\n-#     new_graph = model_from_json(json_str)\n+    graph.summary()\n+    json_str = graph.to_json()\n+    new_graph = model_from_json(json_str)\n \n-#     yaml_str = graph.to_yaml()\n-#     new_graph = model_from_yaml(yaml_str)\n+    yaml_str = graph.to_yaml()\n+    new_graph = model_from_yaml(yaml_str)\n \n \n def test_2o_1i_save_weights():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91a819fb343d27e05a5cdf44ac84b43892bdaa56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 23 | Churn Cumulative: 2377 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1530,11 +1530,23 @@ class Container(Layer):\n         nodes_depths = {}  # map {node: depth value}\n         layers_depths = {}  # map {layer: depth value}\n \n-        def build_map_of_graph(tensor, seen_nodes=set(), depth=0):\n+        def build_map_of_graph(tensor, seen_nodes=set(), depth=0,\n+                               layer=None, node_index=None, tensor_index=None):\n             '''This recursively updates the maps nodes_depths,\n             layers_depths and the set container_nodes.\n             Does not try to detect cycles in graph (TODO?)\n+\n+            # Arguments\n+                tensor: some tensor in a graph\n+                seen_nodes: set of node ids (\"{layer.name}_ib-{node_index}\")\n+                    of nodes seen so far. Useful to prevent infinite loops.\n+                depth: current depth in the graph (0 = last output).\n+                layer: layer from which `tensor` comes from. If not provided,\n+                    will be obtained from `tensor._keras_history`.\n+                node_index: node index from which `tensor` comes from.\n+                tensor_index: tensor_index from which `tensor` comes from.\n             '''\n+            if not layer or node_index is None or tensor_index is None:\n                 layer, node_index, tensor_index = tensor._keras_history\n             node = layer.inbound_nodes[node_index]\n \n@@ -1562,8 +1574,13 @@ class Container(Layer):\n                 layers_depths[layer] = max(depth, layers_depths[layer])\n \n             # propagate to all previous tensors connected to this node\n-            for x in node.input_tensors:\n-                build_map_of_graph(x, copy.copy(seen_nodes), depth + 1)\n+            for i in range(len(node.inbound_layers)):\n+                x = node.input_tensors[i]\n+                layer = node.inbound_layers[i]\n+                node_index = node.node_indices[i]\n+                tensor_index = node.tensor_indices[i]\n+                build_map_of_graph(x, copy.copy(seen_nodes), depth + 1,\n+                                   layer, node_index, tensor_index)\n \n         for x in self.outputs:\n             build_map_of_graph(x, seen_nodes=set(), depth=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a4f6b942db5da321c627ebc1f0ca448bf491556", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 1534 | Contributors (this commit): 13 | Commits (past 90d): 20 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,6 +40,8 @@ class TestBackend(object):\n \n     def test_linear_operations(self):\n         check_two_tensor_operation('dot', (4, 2), (2, 4))\n+        check_two_tensor_operation('dot', (4, 2), (5, 2, 3))\n+\n         check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n                                    axes=((2,), (2,)))\n         check_single_tensor_operation('transpose', (4, 2))\n\n@@ -114,9 +114,23 @@ def test_dense():\n \n \n def test_activity_regularization():\n-    layer_test(core.ActivityRegularization,\n-               kwargs={'l1': 0.01, 'l2': 0.01},\n-               input_shape=(3, 2, 3))\n+    from keras.engine import Input, Model\n+\n+    layer = core.ActivityRegularization(l1=0.01, l2=0.01)\n+\n+    # test in functional API\n+    x = Input(shape=(3,))\n+    z = core.Dense(2)(x)\n+    y = layer(z)\n+    model = Model(input=x, output=y)\n+    model.compile('rmsprop', 'mse', mode='FAST_COMPILE')\n+\n+    model.predict(np.random.random((2, 3)))\n+\n+    # test serialization\n+    model_config = model.get_config()\n+    model = Model.from_config(model_config)\n+    model.compile('rmsprop', 'mse')\n \n \n def test_maxout_dense():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ad686595202c6aabc44af3d16c0440e016e3fae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 14345 | Contributors (this commit): 54 | Commits (past 90d): 68 | Contributors (cumulative): 67 | DMM Complexity: 0.5\n\nDIFF:\n@@ -430,6 +430,9 @@ class Convolution3D(Layer):\n         super(Convolution3D, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n+        assert len(input_shape) == 5\n+        self.input_spec = [InputSpec(shape=input_shape)]\n+\n         if self.dim_ordering == 'th':\n             stack_size = input_shape[1]\n             self.W_shape = (self.nb_filter, stack_size,\n@@ -495,10 +498,11 @@ class Convolution3D(Layer):\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def call(self, x, mask=None):\n+        input_shape = self.input_spec[0].shape\n         conv_out = K.conv3d(x, self.W, strides=self.subsample,\n                             border_mode=self.border_mode,\n                             dim_ordering=self.dim_ordering,\n-                            volume_shape=self.input_shape,\n+                            volume_shape=input_shape,\n                             filter_shape=self.W_shape)\n \n         if self.dim_ordering == 'th':\n\n@@ -372,7 +372,7 @@ class Lambda(Layer):\n                 else:\n                     return K.int_shape(x)\n             # otherwise, we default to the input shape\n-            return self.input_shape\n+            return input_shape\n         elif type(self._output_shape) in {tuple, list}:\n             nb_samples = input_shape[0] if input_shape else None\n             return (nb_samples,) + tuple(self._output_shape)\n\n@@ -63,7 +63,7 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     x = Input(shape=input_shape[1:], dtype=input_dtype)\n     y = layer(x)\n     model = Model(input=x, output=y)\n-    model.compile('rmsprop', 'mse', mode='FAST_COMPILE')\n+    model.compile('rmsprop', 'mse')\n \n     expected_output_shape = layer.get_output_shape_for(input_shape)\n     actual_output = model.predict(input_data)\n\n@@ -133,7 +133,8 @@ def test_convolution_3d():\n                                'kernel_dim3': kernel_dim3,\n                                'border_mode': border_mode,\n                                'subsample': subsample},\n-                       input_shape=(nb_samples, stack_size, nb_row, nb_col))\n+                       input_shape=(nb_samples, stack_size,\n+                                    input_len_dim1, input_len_dim2, input_len_dim3))\n \n             layer_test(convolutional.Convolution3D,\n                        kwargs={'nb_filter': nb_filter,\n@@ -320,4 +321,4 @@ def test_upsampling_3d():\n \n if __name__ == '__main__':\n     # pytest.main([__file__])\n-    test_convolution_1d()\n+    test_convolution_3d()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc3c1488bb5a75af798cee7a81a46e454e17d85d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 71 | Churn Cumulative: 3423 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 8 | DMM Complexity: 0.8979591836734694\n\nDIFF:\n@@ -1151,10 +1151,10 @@ class Merge(Layer):\n \n         if self.mode == 'sum' or self.mode == 'ave':\n             s = inputs[0]\n-            for i in range(1, len(self.layers)):\n+            for i in range(1, len(inputs)):\n                 s += inputs[i]\n             if self.mode == 'ave':\n-                s /= len(self.layers)\n+                s /= len(inputs)\n             return s\n \n         elif self.mode == 'concat':\n@@ -1162,7 +1162,7 @@ class Merge(Layer):\n \n         elif self.mode == 'mul':\n             s = inputs[0]\n-            for i in range(1, len(self.layers)):\n+            for i in range(1, len(inputs)):\n                 s *= inputs[i]\n             return s\n \n@@ -1170,9 +1170,6 @@ class Merge(Layer):\n             l1 = inputs[0]\n             l2 = inputs[1]\n             output = K.batch_dot(l1, l2, self.dot_axes)\n-            # output_shape = list(self.get_output_shape_for(inputs[0]._keras_shape))\n-            # output_shape[0] = l1.shape[0]\n-            # output = output.reshape(tuple(output_shape))\n             return output\n \n         elif self.mode == 'cos':\n@@ -1844,7 +1841,10 @@ class Container(Layer):\n \n         cache_key = ','.join([str(x) for x in input_shapes])\n         if cache_key in self._output_shape_cache:\n-            return self._output_shape_cache[cache_key]\n+            output_shapes = self._output_shape_cache[cache_key]\n+            if type(output_shapes) is list and len(output_shapes) == 1:\n+                return output_shapes[0]\n+            return output_shapes\n         else:\n             # bad luck, have to run the graph manually\n             layers_to_output_shapes = {}\n@@ -1906,6 +1906,8 @@ class Container(Layer):\n                 output_shapes.append(layers_to_output_shapes[key])\n             # store in cache\n             self._output_shape_cache[cache_key] = output_shapes\n+            if type(output_shapes) is list and len(output_shapes) == 1:\n+                return output_shapes[0]\n             return output_shapes\n \n     def run_internal_graph(self, inputs, masks=None):\n\n@@ -14,10 +14,63 @@ def test_masking():\n \n \n def test_merge():\n+    from keras.layers import Input, merge\n+    from keras.models import Model\n+\n     # test modes: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+    input_shapes = [(3, 2), (3, 2)]\n+    inputs = [np.random.random(shape) for shape in input_shapes]\n+\n+    # test graph API\n+    for mode in ['sum', 'mul', 'concat', 'ave', 'cos', 'dot']:\n+        print(mode)\n+        input_a = Input(shape=input_shapes[0][1:])\n+        input_b = Input(shape=input_shapes[1][1:])\n+        merged = merge([input_a, input_b], mode='sum')\n+        model = Model([input_a, input_b], merged)\n+        model.compile('rmsprop', 'mse')\n+\n+        expected_output_shape = model.get_output_shape_for(input_shapes)\n+        actual_output_shape = model.predict(inputs).shape\n+        assert expected_output_shape == actual_output_shape\n+\n+        config = model.get_config()\n+        model = Model.from_config(config)\n+        model.compile('rmsprop', 'mse')\n+\n     # test lambda with output_shape lambda\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    merged = merge([input_a, input_b],\n+                   mode=lambda (x, y): K.concatenate([x, y]),\n+                   output_shape=lambda (s1, s2): (s1[:-1],) + (s1[-1] + s2[-1],))\n+    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n     # test function with output_shape function\n-    pass\n+    def fn_mode((x, y)):\n+        return K.concatenate([x, y])\n+\n+    def fn_output_shape((s1, s2)):\n+        return (s1[:-1],) + (s1[-1] + s2[-1],)\n+\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    merged = merge([input_a, input_b],\n+                   mode=fn_mode,\n+                   output_shape=fn_output_shape)\n+    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n \n \n def test_dropout():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dacf017d3884a4b84342d891c1504ee3c8bf6a02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 2414 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2303,12 +2303,19 @@ class Container(Layer):\n         print_summary(flattened_layers, getattr(self, 'container_nodes', None))\n \n \n-def get_source_inputs(tensor):\n+def get_source_inputs(tensor, layer=None, node_index=None, tensor_index=None):\n     '''Returns the list of input tensors\n     necessary to compute `tensor`.\n \n     Output will always be a list of tensors\n     (potentially with 1 element).\n+\n+    # Arguments\n+        tensor: the tensor to start from.\n+        layer: origin layer of the tensor. Will be\n+            determined via tensor._keras_history if not provided.\n+        node_index: origin node index of the tensor.\n+        tensor_index: origin tensor index of the tensor.\n     '''\n     if not hasattr(tensor, '_keras_history'):\n         raise Exception('Tensor must be a Keras tensor. Found: ' + str(tensor))\n@@ -2323,9 +2330,15 @@ def get_source_inputs(tensor):\n             return node.input_tensors\n         else:\n             source_tensors = []\n-            for x in node.input_tensors:\n-                # TODO: what about graph cycles?\n-                previous_sources = get_source_inputs(x)\n+            for i in range(len(node.inbound_layers)):\n+                x = node.input_tensors[i]\n+                layer = node.inbound_layers[i]\n+                node_index = node.node_indices[i]\n+                tensor_index = node.tensor_indices[i]\n+                previous_sources = get_source_inputs(x,\n+                                                     layer,\n+                                                     node_index,\n+                                                     tensor_index)\n                 # avoid input redundancy\n                 for x in previous_sources:\n                     if x not in source_tensors:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c429e651c1d4fe83c84bc6a4405d5645faac6a2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 496 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -34,9 +34,9 @@ class WeightRegularizer(Regularizer):\n                             'ActivityRegularizer '\n                             '(i.e. activity_regularizer=\"l2\" instead '\n                             'of activity_regularizer=\"activity_l2\".')\n-        loss += K.sum(K.abs(self.p)) * self.l1\n-        loss += K.sum(K.square(self.p)) * self.l2\n-        return loss\n+        regularized_loss = loss + K.sum(K.abs(self.p)) * self.l1\n+        regularized_loss += K.sum(K.square(self.p)) * self.l2\n+        return K.in_train_phase(regularized_loss, loss)\n \n     def get_config(self):\n         return {'name': self.__class__.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62f905333056b0fef3593025d5d444389b84c6c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 307 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -179,11 +179,12 @@ print('Build model...')\n \n sentrnn = Sequential()\n sentrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n-                      input_length=story_maxlen, mask_zero=True))\n+                      input_length=story_maxlen))\n sentrnn.add(Dropout(0.3))\n \n qrnn = Sequential()\n-qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE, input_length=query_maxlen))\n+qrnn.add(Embedding(vocab_size, EMBED_HIDDEN_SIZE,\n+                   input_length=query_maxlen))\n qrnn.add(Dropout(0.3))\n qrnn.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n qrnn.add(RepeatVector(story_maxlen))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8c7dbb758cbf94d1d3e03b1d44f6f6e8defd7de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 144 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ _config_path = os.path.expanduser(os.path.join(_keras_dir, 'keras.json'))\n if os.path.exists(_config_path):\n     _config = json.load(open(_config_path))\n     _floatx = _config.get('floatx', floatx())\n-    assert _floatx in {'float32', 'float64'}\n+    assert _floatx in {'float16', 'float32', 'float64'}\n     _epsilon = _config.get('epsilon', epsilon())\n     assert type(_epsilon) == float\n     _backend = _config.get('backend', _BACKEND)\n\n@@ -17,14 +17,14 @@ def set_epsilon(e):\n \n def floatx():\n     '''Returns the default float type, as a string\n-    (e.g. 'float32', 'float64').\n+    (e.g. 'float16', 'float32', 'float64').\n     '''\n     return _FLOATX\n \n \n def set_floatx(floatx):\n     global _FLOATX\n-    if floatx not in {'float32', 'float64'}:\n+    if floatx not in {'float16', 'float32', 'float64'}:\n         raise Exception('Unknown floatx type: ' + str(floatx))\n     floatx = str(floatx)\n     _FLOATX = floatx\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ebfde534c04d10d1fd414cdafac0697721f0c000", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 1265 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -558,7 +558,15 @@ class Model(Container):\n \n         # prepare gradient updates and state updates\n         self.optimizer = optimizers.get(optimizer)\n-        train_updates = self.optimizer.get_updates(self.trainable_weights,\n+        # dedupe trainable weights\n+        trainable_weights_set = set()\n+        trainable_weights = []\n+        for w in self.trainable_weights:\n+            if w not in trainable_weights_set:\n+                trainable_weights_set.add(w)\n+                trainable_weights.append(w)\n+        # compute weight updates\n+        train_updates = self.optimizer.get_updates(trainable_weights,\n                                                    self.constraints,\n                                                    total_loss)\n         train_updates += self.updates\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ca775144526712656725c3ebfb2d0072b34c328", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 269 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,8 @@ Five digits inverted:\n '''\n \n from __future__ import print_function\n-from keras.models import Sequential, slice_X\n+from keras.models import Sequential\n+from keras.engine.training import slice_X\n from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n from keras.layers import recurrent\n import numpy as np\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b3543fca9d811c638bb72d78601c8564f5465fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 18 | Files Changed: 6 | Hunks: 15 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 94 | Churn Cumulative: 7065 | Contributors (this commit): 24 | Commits (past 90d): 111 | Contributors (cumulative): 52 | DMM Complexity: 0.0\n\nDIFF:\n@@ -167,7 +167,7 @@ question_encoder.add(Dropout(0.3))\n match = Sequential()\n match.add(Merge([input_encoder_m, question_encoder],\n                 mode='dot',\n-                dot_axes=[(2,), (2,)]))\n+                dot_axes=[2, 2]))\n # output: (samples, story_maxlen, query_maxlen)\n # embed the input into a single vector with size = story_maxlen:\n input_encoder_c = Sequential()\n\n@@ -125,13 +125,38 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n-    if axes:\n-        adj_x = None if axes[0][0] == ndim(x) - 1 else True\n-        adj_y = True if axes[1][0] == ndim(y) - 1 else None\n+    '''batchwise dot product\n+    batch_dot results in a tensor with less dimensions than the input.\n+    If the number of dimensions is reduced to 1, we use `expand_dims` to\n+    make sure that ndim is at least 2.\n+\n+    # Example\n+        Assume x = [[1, 2]   and y = [[5, 6]\n+                    [3, 4]]           [7, 8]]\n+        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n+        of x.dot(y.T), although we never have to calculate the off-diagonal\n+        elements.\n+\n+\n+    # Arguments\n+        x, y: tensors with ndim >= 2\n+        axes: list (or single) int with target dimensions\n+\n+    # Returns\n+        Tensor with ndim >= 2\n+    '''\n+    if type(axes) == int:\n+        axes = (axes, axes)\n+    if axes is not None:\n+        adj_x = None if axes[0] == ndim(x) - 1 else True\n+        adj_y = True if axes[1] == ndim(y) - 1 else None\n     else:\n         adj_x = None\n         adj_y = None\n-    return tf.batch_matmul(x, y, adj_x=adj_x, adj_y=adj_y)\n+    out = tf.batch_matmul(x, y, adj_x=adj_x, adj_y=adj_y)\n+    if ndim(out) == 1:\n+        out = expand_dims(out, 1)\n+    return out\n \n \n def transpose(x):\n@@ -256,6 +281,10 @@ def round(x):\n     return tf.round(x)\n \n \n+def sign(x):\n+    return tf.sign(x)\n+\n+\n def pow(x, a):\n     return tf.pow(x, a)\n \n\n@@ -127,10 +127,35 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n+    '''batchwise dot product\n+    batch_dot results in a tensor with less dimensions than the input.\n+    If the number of dimensions is reduced to 1, we use `expand_dims` to\n+    make sure that ndim is at least 2.\n+\n+    # Example\n+        Assume x = [[1, 2]   and y = [[5, 6]\n+                    [3, 4]]           [7, 8]]\n+        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n+        of x.dot(y.T), although we never have to calculate the off-diagonal\n+        elements.\n+\n+\n+    # Arguments\n+        x, y: tensors with ndim >= 2\n+        axes: list (or single) int with target dimensions\n+\n+    # Returns\n+        Tensor with ndim >= 2\n+    '''\n+    if type(axes) == int:\n+        axes = (axes, axes)\n     if axes is None:\n         # behaves like tf.batch_matmul as default\n-        axes = [(x.ndim - 1,), (y.ndim - 2,)]\n-    return T.batched_tensordot(x, y, axes=axes)\n+        axes = [x.ndim - 1, y.ndim - 2]\n+    out = T.batched_tensordot(x, y, axes=axes)\n+    if ndim(out) == 1:\n+        out = expand_dims(out, 1)\n+    return out\n \n \n def transpose(x):\n@@ -219,6 +244,10 @@ def round(x):\n     return T.round(x)\n \n \n+def sign(x):\n+    return T.sgn(x)\n+\n+\n def pow(x, a):\n     return T.pow(x, a)\n \n\n@@ -1038,6 +1038,8 @@ class Merge(Layer):\n         self.mode = mode\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n+        if type(self.dot_axes) == int:\n+            self.dot_axes = [self.dot_axes, ] * 2\n         self._output_shape = output_shape\n         self.node_indices = node_indices\n \n@@ -1113,19 +1115,18 @@ class Merge(Layer):\n             if mode == 'dot':\n                 if type(dot_axes) == int:\n                     if dot_axes < 0:\n-                        dot_axes = [range(dot_axes % n1, n1), range(dot_axes % n2, n2)]\n+                        dot_axes = [dot_axes % n1, dot_axes % n2]\n                     else:\n-                        dot_axes = [range(n1 - dot_axes, n2), range(1, dot_axes + 1)]\n+                        dot_axes = [n1 - dot_axes, n2-dot_axes]\n                 if type(dot_axes) not in [list, tuple]:\n                     raise Exception('Invalid type for dot_axes - should be a list.')\n                 if len(dot_axes) != 2:\n                     raise Exception('Invalid format for dot_axes - should contain two elements.')\n-                if type(dot_axes[0]) not in [list, tuple, range] or type(dot_axes[1]) not in [list, tuple, range]:\n-                    raise Exception('Invalid format for dot_axes - list elements should have type \"list\" or \"tuple\".')\n-                for i in range(len(dot_axes[0])):\n-                    if shape1[dot_axes[0][i]] != shape2[dot_axes[1][i]]:\n+                if type(dot_axes[0]) is not int or type(dot_axes[1]) is not int:\n+                    raise Exception('Invalid format for dot_axes - list elements should be \"int\".')\n+                if shape1[dot_axes[0]] != shape2[dot_axes[1]]:\n                     raise Exception('Dimension incompatibility using dot mode: ' +\n-                                        '%s != %s. ' % (shape1[dot_axes[0][i]], shape2[dot_axes[1][i]]) +\n+                                    '%s != %s. ' % (shape1[dot_axes[0]], shape2[dot_axes[1][i]]) +\n                                     'Layer shapes: %s, %s' % (shape1, shape2))\n         elif mode == 'concat':\n             reduced_inputs_shapes = [list(shape) for shape in input_shapes]\n@@ -1242,9 +1243,7 @@ class Merge(Layer):\n         elif self.mode == 'dot':\n             shape1 = list(input_shapes[0])\n             shape2 = list(input_shapes[1])\n-            dot_axes = []\n-            for axes in self.dot_axes:\n-                dot_axes.append([index-1 for index in axes])\n+            dot_axes = [a-1 for a in self.dot_axes]\n             tensordot_output = np.tensordot(np.zeros(tuple(shape1[1:])),\n                                             np.zeros(tuple(shape2[1:])),\n                                             axes=dot_axes)\n\n@@ -149,6 +149,7 @@ class TestBackend(object):\n         check_single_tensor_operation('exp', (4, 2))\n         check_single_tensor_operation('log', (4, 2))\n         check_single_tensor_operation('round', (4, 2))\n+        check_single_tensor_operation('sign', (4, 2))\n         check_single_tensor_operation('pow', (4, 2), a=3)\n         check_single_tensor_operation('clip', (4, 2), min_value=0.4,\n                                       max_value=0.6)\n\n@@ -281,7 +281,7 @@ def test_merge_dot():\n     right.add(Activation('relu'))\n \n     model = Sequential()\n-    model.add(Merge([left, right], mode='dot', dot_axes=([1], [1])))\n+    model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n     model.add(Dense(nb_class))\n     model.add(Activation('softmax'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
