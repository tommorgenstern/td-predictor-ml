{"custom_id": "keras#f5cbb0e5898541ca5acc3638ee3f8140fa4cf7e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1807 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -34,13 +34,13 @@ def split_dataset(\n     \"\"\"Split a dataset into a left half and a right half (e.g. train / test).\n \n     Args:\n-        dataset: A `tf.data.Dataset` object or a list/tuple of arrays with the\n+        dataset: A `tf.data.Dataset` object, or a list/tuple of arrays with the\n           same length.\n-        left_size: If float, it should be in range `[0, 1]` range and signifies\n+        left_size: If float (in the range `[0, 1]`), it signifies\n           the fraction of the data to pack in the left dataset. If integer, it\n           signifies the number of samples to pack in the left dataset. If\n           `None`, it defaults to the complement to `right_size`.\n-        right_size: If float, it should be in range `[0, 1]` range and signifies\n+        right_size: If float (in the range `[0, 1]`), it signifies\n           the fraction of the data to pack in the right dataset. If integer, it\n           signifies the number of samples to pack in the right dataset. If\n           `None`, it defaults to the complement to `left_size`.\n@@ -49,6 +49,16 @@ def split_dataset(\n \n     Returns:\n         A tuple of two `tf.data.Dataset` objects: the left and right splits.\n+\n+    Example:\n+\n+    >>> data = np.random.random(size=(1000, 4))\n+    >>> left_ds, right_ds = tf.keras.utils.split_dataset(data, left_size=0.8)\n+    >>> int(left_ds.cardinality())\n+    800\n+    >>> int(right_ds.cardinality())\n+    200\n+\n     \"\"\"\n     dataset_type_spec = _get_type_spec(dataset)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb78a0caaea0503728784c0fa5cf547c623633b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 98 | Churn Cumulative: 21422 | Contributors (this commit): 44 | Commits (past 90d): 18 | Contributors (cumulative): 76 | DMM Complexity: 0.0\n\nDIFF:\n@@ -294,19 +294,74 @@ class LossesContainerTest(test_combinations.TestCase):\n         self.assertIsInstance(total_loss, tf.Tensor)\n         self.assertEqual(total_loss.dtype, tf.float64)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(\n+            input_type=[\"dense\", \"masked\", \"ragged\"],\n+            reduction=[\"auto\", \"sum\"],\n+            use_sample_weights=[True, False],\n+        ),\n+    )\n+    def test_loss_consistency(self, input_type, reduction, use_sample_weights):\n+        y_p = tf.ragged.constant(\n+            [[[1], [1], [1]], [[1], [1]]], dtype=tf.float32\n+        )\n+        y_t = tf.ragged.constant(\n+            [[[1], [0], [0]], [[1], [1]]], dtype=tf.float32\n+        )\n+\n+        if input_type == \"masked\":\n+            mask = tf.ones_like(y_p).to_tensor()\n+            y_p = y_p.to_tensor()\n+            y_t = y_t.to_tensor()\n+            y_p._keras_mask = mask\n+        elif input_type == \"dense\":\n+            y_p = y_p.to_tensor()\n+            y_t = y_t.to_tensor()\n+\n+        if input_type == \"dense\":\n+            count = 6\n+        else:\n+            count = 5\n+\n+        if use_sample_weights:\n+            wrong = 4\n+            maybe_sample_weight = {\n+                \"sample_weight\": tf.constant([[2], [1]], dtype=tf.float32)\n+            }\n+        else:\n+            wrong = 2\n+            maybe_sample_weight = {}\n+\n+        expected = wrong\n+        if reduction != \"sum\":\n+            expected /= count\n+\n+        loss_obj = losses_mod.MeanAbsoluteError(reduction=reduction)\n+\n+        result = loss_obj(y_t, y_p, **maybe_sample_weight)\n+        self.assertAlmostEqual(result.numpy(), expected)\n+\n+        container = compile_utils.LossesContainer(loss_obj)\n+        container_result = container(y_t, y_p, **maybe_sample_weight)\n+        self.assertAlmostEqual(container_result.numpy(), expected)\n+\n     def test_loss_masking(self):\n         loss_container = compile_utils.LossesContainer(\"mae\")\n         y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)\n         y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)\n+        # Reduction is \"sum_over_batch_size\" that's not the literal batch size,\n+        # but the number of elements being summed: The number of valid\n+        # emlements. So since the mask has two valid items, the number of\n+        # elements is 2.\n         y_p._keras_mask = tf.constant([[1, 0], [1, 0]], dtype=tf.float32)\n \n         total_loss = loss_container(y_t, y_p)\n-        self.assertAlmostEqual(total_loss.numpy(), 0.25)  # sum over batch size\n+        self.assertAlmostEqual(total_loss.numpy(), 0.5)  # sum over num valid\n \n         self.assertLen(loss_container.metrics, 1)\n         loss_metric = loss_container.metrics[0]\n         self.assertEqual(loss_metric.name, \"loss\")\n-        self.assertAlmostEqual(loss_metric.result().numpy(), 0.25)\n+        self.assertAlmostEqual(loss_metric.result().numpy(), 0.5)\n \n     def test_loss_sample_weight(self):\n         loss_container = compile_utils.LossesContainer(\"mae\")\n@@ -331,13 +386,13 @@ class LossesContainerTest(test_combinations.TestCase):\n         y_p._keras_mask = tf.constant([[1, 0], [1, 0]], dtype=tf.float32)\n \n         total_loss = loss_container(y_t, y_p, sample_weight=sw)\n-        # (0 * .2 + 1 * .5) / 4\n-        self.assertAlmostEqual(total_loss.numpy(), 0.125)  # sum over batch size\n+        # (0 * .2 + 1 * .5) / 2\n+        self.assertAlmostEqual(total_loss.numpy(), 0.25)  # sum over num valid\n \n         self.assertLen(loss_container.metrics, 1)\n         loss_metric = loss_container.metrics[0]\n         self.assertEqual(loss_metric.name, \"loss\")\n-        self.assertAlmostEqual(loss_metric.result().numpy(), 0.125)\n+        self.assertAlmostEqual(loss_metric.result().numpy(), 0.25)\n \n     def test_custom_loss_callables(self):\n         def custom_loss_fn(y_true, y_pred):\n\n@@ -3732,7 +3732,9 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n         model.add(layers_module.Masking(mask_value=0, input_shape=(2, 1)))\n         model.add(\n             layers_module.TimeDistributed(\n-                layers_module.Dense(1, kernel_initializer=\"ones\")\n+                layers_module.Dense(\n+                    1, kernel_initializer=\"ones\", trainable=False\n+                )\n             )\n         )\n         model.compile(\n@@ -3743,7 +3745,10 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n         )\n \n         # verify that masking is applied.\n-        x = np.array([[[1], [1]], [[1], [1]], [[0], [0]]])\n+        x = np.array(\n+            # third row is masked\n+            [[[1], [1]], [[1], [1]], [[0], [0]]]\n+        )\n         y = np.array([[[1], [1]], [[0], [1]], [[1], [1]]])\n         scores = model.train_on_batch(x, y)\n         self.assertArrayNear(scores, [0.25, 0.75], 0.1)\n@@ -3751,7 +3756,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n         # verify that masking is combined with sample weights.\n         w = np.array([3, 2, 4])\n         scores = model.train_on_batch(x, y, sample_weight=w)\n-        self.assertArrayNear(scores, [0.3328, 0.8], 0.001)\n+        self.assertArrayNear(scores, [0.5, 0.8], 0.001)\n \n     @test_combinations.run_all_keras_modes\n     def test_add_metric_with_tensor_on_model(self):\n\n@@ -1756,10 +1756,15 @@ class Model(training_lib.Model):\n                             ) = losses_utils.squeeze_or_expand_dimensions(\n                                 mask, sample_weight=sample_weight\n                             )\n-                            sample_weight *= mask\n \n                     if hasattr(loss_fn, \"reduction\"):\n                         per_sample_losses = loss_fn.call(y_true, y_pred)\n+                        sample_weight = losses_utils.apply_valid_mask(\n+                            per_sample_losses,\n+                            sample_weight,\n+                            mask,\n+                            loss_fn.reduction,\n+                        )\n                         weighted_losses = losses_utils.compute_weighted_loss(\n                             per_sample_losses,\n                             sample_weight=sample_weight,\n\n@@ -148,8 +148,21 @@ class Loss:\n                 call_fn = tf.__internal__.autograph.tf_convert(\n                     self.call, tf.__internal__.autograph.control_status_ctx()\n                 )\n+\n             losses = call_fn(y_true, y_pred)\n-            mask = losses_utils.get_mask(losses)\n+\n+            in_mask = losses_utils.get_mask(y_pred)\n+            out_mask = losses_utils.get_mask(losses)\n+\n+            if in_mask is not None and out_mask is not None:\n+                mask = in_mask & out_mask\n+            elif in_mask is not None:\n+                mask = in_mask\n+            elif out_mask is not None:\n+                mask = out_mask\n+            else:\n+                mask = None\n+\n             reduction = self._get_reduction()\n             sample_weight = losses_utils.apply_valid_mask(\n                 losses, sample_weight, mask, reduction\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eedaf5aba9c7c99995a09c1e8e4ebd681ca96ba9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1843 | Contributors (this commit): 26 | Commits (past 90d): 2 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -209,7 +209,7 @@ class Tokenizer(object):\n \n     By default, all punctuation is removed, turning the texts into\n     space-separated sequences of words\n-    (words maybe include the `'` character). These sequences are then\n+    (words may include the `'` character). These sequences are then\n     split into lists of tokens. They will then be indexed or vectorized.\n \n     `0` is a reserved index that won't be assigned to any word.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fbfb8dfd27501d8f017446cf1ae490790b5fce21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 493 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,6 @@ from keras.engine.training import Model\n from tensorflow.python import tf2\n from tensorflow.python.util.tf_export import keras_export\n \n-__version__ = \"2.11.0\"\n+__version__ = \"2.12.0\"\n \n keras_export(\"keras.__version__\").export_constant(__name__, \"__version__\")\n\n@@ -31,7 +31,7 @@ DOCLINES = __doc__.split(\"\\n\")\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = \"2.11.0\"\n+_VERSION = \"2.12.0\"\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b393d4049afd187ed7d24ea70177a72cf4a3ce2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 35 | Methods Changed: 12 | Complexity Δ (Sum/Max): 28/28 | Churn Δ: 189 | Churn Cumulative: 1912 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 0.5\n\nDIFF:\n@@ -21,6 +21,7 @@ import threading\n import warnings\n import zipfile\n \n+import numpy as np\n import tensorflow.compat.v2 as tf\n \n import keras\n@@ -39,10 +40,9 @@ except ImportError:\n \n # isort: off\n \n-_SELF_DIRNAME = \"self\"\n _CONFIG_FILENAME = \"config.json\"\n _METADATA_FILENAME = \"metadata.json\"\n-_VARS_FNAME = \"variables.h5\"\n+_VARS_FNAME = \"variables\"\n _ASSETS_DIRNAME = \"assets\"\n \n # A temporary flag to enable the new idempotent saving framework.\n@@ -72,24 +72,25 @@ ATTR_SKIPLIST = frozenset(\n )\n \n \n-def save_model(model, filepath):\n-    \"\"\"Save a zip-archive representing a Keras model to the given filepath.\n+def save_model(model, filepath, weights_format=\"h5\", use_zip=True):\n+    \"\"\"Save an archive representing a Keras model to the given filepath.\n \n     The zip-based archive contains the following structure:\n \n-    - JSON-based configuration file (config.json): Records of model, layer, and\n-        other trackables' configuration.\n-    - NPZ-based trackable state files, found in respective directories, such as\n-        model/states.npz, model/dense_layer/states.npz, etc.\n-    - Metadata file (this is a TODO).\n+    - JSON configuration file (`config.json`): Records of model, layer, and\n+        other object configurations.\n+    - Npz or h5 model variables file (`variables.npz` or `variables.h5`).\n+    - Assets files (if any) found in the `assets/` directory structure,\n+        which mirrors the model's inner structure.\n+    - JSON metadata file (`metdata.json`).\n \n     The states of Keras trackables (layers, optimizers, loss, and metrics) are\n     automatically saved as long as they can be discovered through the attributes\n-    returned by `dir(Model)`. Typically, the state includes the variables\n+    returned by `dir(model)`. Typically, the state includes the variables\n     associated with the trackable, but some specially purposed layers may\n     contain more such as the vocabularies stored in the hashmaps. The trackables\n-    define how their states are saved by exposing `save_state()` and\n-    `load_state()` APIs.\n+    define how their asset state is saved by exposing `save_assets()` and\n+    `load_assets()` APIs.\n \n     For the case of layer states, the variables will be visited as long as\n     they are either 1) referenced via layer attributes, or 2) referenced via a\n@@ -101,8 +102,11 @@ def save_model(model, filepath):\n             \"Invalid filename: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n-    if h5py is None:\n-        raise ImportError(\"h5py must be installed in order to save a model.\")\n+    if weights_format == \"h5\" and h5py is None:\n+        raise ImportError(\n+            \"h5py must be installed in order to save a model in hdf5 format.\"\n+        )\n+\n     if not model.built:\n         warnings.warn(\n             \"You are saving a model that has not yet been built. \"\n@@ -123,93 +127,131 @@ def save_model(model, filepath):\n             \"date_saved\": datetime.datetime.now().strftime(\"%Y-%m-%d@%H:%M:%S\"),\n         }\n     )\n-\n+    if use_zip:\n         # Use a temporary directory for the storing files prior to zipping.\n-    temp_path = _get_temp_dir()\n+        write_path = _get_temp_dir()\n+    else:\n+        tf.io.gfile.makedirs(filepath)\n+        write_path = filepath\n     try:\n         # Write files locally before zipping.\n-        with open(tf.io.gfile.join(temp_path, _METADATA_FILENAME), \"w\") as f:\n+        with open(tf.io.gfile.join(write_path, _METADATA_FILENAME), \"w\") as f:\n             f.write(metadata_json)\n-        with open(tf.io.gfile.join(temp_path, _CONFIG_FILENAME), \"w\") as f:\n+        with open(tf.io.gfile.join(write_path, _CONFIG_FILENAME), \"w\") as f:\n             f.write(config_json)\n \n-        h5_file = h5py.File(tf.io.gfile.join(temp_path, _VARS_FNAME), \"w\")\n-        assets_dir = tf.io.gfile.join(temp_path, _ASSETS_DIRNAME)\n+        weights_path = tf.io.gfile.join(write_path, _VARS_FNAME)\n+        assets_path = tf.io.gfile.join(write_path, _ASSETS_DIRNAME)\n+\n+        if weights_format == \"h5\":\n+            weights_store = H5IOStore(weights_path, mode=\"w\")\n+        elif weights_format == \"npz\":\n+            weights_store = NpzIOStore(weights_path, mode=\"w\")\n+        else:\n+            raise ValueError(\n+                \"Unknown `weights_format`. Expected 'h5' or 'npz'.  \"\n+                f\"Received: {weights_format}\"\n+            )\n         _save_state(\n             model,\n-            weights_handler=H5IOHandler(h5_file),\n-            assets_handler=DiskIOHandler(assets_dir),\n+            weights_handler=weights_store,\n+            assets_handler=DiskIOStore(assets_path),\n             inner_path=\"\",\n             visited_trackables=set(),\n         )\n-        _print_h5_file(h5_file, action=\"saving\")\n-        h5_file.close()\n+        weights_store.close()\n \n+        if use_zip:\n             # Zip local files into an archive.\n             with zipfile.ZipFile(filepath, \"w\") as zipfile_to_save:\n-            _write_recursively(zipfile_to_save, temp_path, \"\")\n-        _print_zip_file(zipfile_to_save, \"saving\")\n+                _write_to_zip_recursively(zipfile_to_save, write_path, \"\")\n     except Exception as e:\n         raise e\n     finally:\n         _SAVING_V3_ENABLED.value = saving_v3_enabled_value\n+        if use_zip and tf.io.gfile.exists(write_path):\n             # Remove the directory temporarily used.\n-        tf.io.gfile.rmtree(temp_path)\n+            tf.io.gfile.rmtree(write_path)\n \n \n def load_model(filepath, custom_objects=None):\n-    \"\"\"Load a zip archive representing a Keras model.\"\"\"\n+    \"\"\"Load an archive representing a Keras model.\"\"\"\n     if not filepath.endswith(\".keras\"):\n         raise ValueError(\n             \"Invalid filename: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n-    if h5py is None:\n-        raise ImportError(\"h5py must be installed in order to load a model.\")\n+    use_zip = not tf.io.gfile.isdir(filepath)\n+\n     saving_v3_enabled_value = getattr(_SAVING_V3_ENABLED, \"value\", False)\n     _SAVING_V3_ENABLED.value = True\n-    temp_path = _get_temp_dir()\n-    try:\n-        with zipfile.ZipFile(filepath, \"r\") as zipfile_to_load:\n-            _print_zip_file(zipfile_to_load, \"loading\")\n-            zipfile_to_load.extractall(temp_path)\n \n-        with open(tf.io.gfile.join(temp_path, _CONFIG_FILENAME), \"r\") as f:\n+    if use_zip:\n+        read_path = _get_temp_dir()\n+    else:\n+        read_path = filepath\n+    try:\n+        if use_zip:\n+            with zipfile.ZipFile(filepath, \"r\") as zipfile_to_load:\n+                zipfile_to_load.extractall(read_path)\n+\n+        with open(tf.io.gfile.join(read_path, _CONFIG_FILENAME), \"r\") as f:\n             config_json = f.read()\n         # Note: we should NOT use a custom JSON decoder. Anything that\n         # needs custom decoding must be handled in deserialize_keras_object.\n         config_dict = json.loads(config_json)\n         # Construct the model from the configuration file in the archive.\n         model = deserialize_keras_object(config_dict, custom_objects)\n-        h5_file = h5py.File(tf.io.gfile.join(temp_path, _VARS_FNAME), \"r\")\n-        _print_h5_file(h5_file, action=\"loading\")\n-        assets_dir = tf.io.gfile.join(temp_path, _ASSETS_DIRNAME)\n+\n+        weights_path = tf.io.gfile.join(read_path, _VARS_FNAME)\n+        if tf.io.gfile.exists(weights_path + \".h5\"):\n+            weights_format = \"h5\"\n+            if h5py is None:\n+                raise ImportError(\n+                    \"h5py must be installed in order to save \"\n+                    \"a model in hdf5 format.\"\n+                )\n+        elif tf.io.gfile.exists(weights_path + \".npz\"):\n+            weights_format = \"npz\"\n+\n+        if weights_format == \"h5\":\n+            weights_store = H5IOStore(weights_path, mode=\"r\")\n+        elif weights_format == \"npz\":\n+            weights_store = NpzIOStore(weights_path, mode=\"r\")\n+        else:\n+            raise ValueError(\n+                f\"Expected a {weights_path}.h5 or {weights_path}.npz file.\"\n+            )\n+\n+        assets_path = tf.io.gfile.join(read_path, _ASSETS_DIRNAME)\n         _load_state(\n             model,\n-            weights_handler=H5IOHandler(h5_file),\n-            assets_handler=DiskIOHandler(assets_dir),\n+            weights_handler=weights_store,\n+            assets_handler=DiskIOStore(assets_path),\n             inner_path=\"\",\n             visited_trackables=set(),\n         )\n-        h5_file.close()\n+        weights_store.close()\n     except Exception as e:\n         raise e\n     else:\n         return model\n     finally:\n         _SAVING_V3_ENABLED.value = saving_v3_enabled_value\n-        if tf.io.gfile.exists(temp_path):\n-            tf.io.gfile.rmtree(temp_path)\n+        if use_zip and tf.io.gfile.exists(read_path):\n+            tf.io.gfile.rmtree(read_path)\n \n \n-def _write_recursively(zipfile_to_save, system_path, zip_path):\n+def _write_to_zip_recursively(zipfile_to_save, system_path, zip_path):\n     if not tf.io.gfile.isdir(system_path):\n         zipfile_to_save.write(system_path, zip_path)\n     else:\n         for file_name in tf.io.gfile.listdir(system_path):\n             system_file_path = tf.io.gfile.join(system_path, file_name)\n             zip_file_path = tf.io.gfile.join(zip_path, file_name)\n-            _write_recursively(zipfile_to_save, system_file_path, zip_file_path)\n+            _write_to_zip_recursively(\n+                zipfile_to_save, system_file_path, zip_file_path\n+            )\n \n \n def _save_state(\n@@ -339,9 +381,9 @@ def _load_container_state(\n             )\n \n \n-class DiskIOHandler:\n-    def __init__(self, base_directory):\n-        self.base_directory = base_directory\n+class DiskIOStore:\n+    def __init__(self, root_path, mode=None):\n+        self.base_directory = root_path\n \n     def make(self, path):\n         if not path:\n@@ -359,10 +401,13 @@ class DiskIOHandler:\n             return path\n         return None\n \n+    def close(self):\n+        pass\n \n-class H5IOHandler:\n-    def __init__(self, h5_file):\n-        self.h5_file = h5_file\n+\n+class H5IOStore:\n+    def __init__(self, root_path, mode=\"r\"):\n+        self.h5_file = h5py.File(root_path + \".h5\", mode=mode)\n \n     def make(self, path):\n         if not path:\n@@ -372,11 +417,47 @@ class H5IOHandler:\n     def get(self, path):\n         if not path:\n             return self.h5_file[\"vars\"]\n-        if path in self.h5_file:\n+        if path in self.h5_file and \"vars\" in self.h5_file[path]:\n             return self.h5_file[path][\"vars\"]\n-        print(f\"Warning: asset missing from file: {path}\")\n         return {}\n \n+    def close(self):\n+        self.h5_file.close()\n+\n+\n+class NpzIOStore:\n+    def __init__(self, root_path, mode=\"r\"):\n+        self.root_path = root_path\n+        self.mode = mode\n+        if mode == \"w\":\n+            self.contents = {}\n+        else:\n+            f = open(root_path + \".npz\", mode=\"rb\")\n+            self.contents = np.load(f)\n+            f.close()\n+\n+    def make(self, path):\n+        if not path:\n+            self.contents[\"vars\"] = {}\n+            return self.contents[\"vars\"]\n+        self.contents[path] = {\"vars\": {}}\n+        return self.contents[path][\"vars\"]\n+\n+    def get(self, path):\n+        if not path:\n+            if \"vars\" in self.contents:\n+                return self.contents[\"vars\"]\n+            return {}\n+        if path in self.contents and \"vars\" in self.contents[path]:\n+            return self.contents[path][\"vars\"]\n+        return {}\n+\n+    def close(self):\n+        if self.mode == \"w\":\n+            f = open(self.root_path + \".npz\", mode=\"wb\")\n+            np.savez(f, **self.contents)\n+            f.close()\n+\n \n def _get_temp_dir():\n     temp_dir = tempfile.mkdtemp()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f329850e830b3c7d4f7f2bed3627f88ade8cf6b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1318 | Contributors (this commit): 11 | Commits (past 90d): 34 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -127,7 +127,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             \"errors. Please update the optimizer referenced in your code \"\n             \"to be an instance of \"\n             \"`tf.keras.optimizers.legacy.Optimizer`, e.g.: \"\n-            f\"`tf.keras.optimizer.legacy.{self.__class__.__name__}`.\"\n+            f\"`tf.keras.optimizers.legacy.{self.__class__.__name__}`.\"\n         )\n \n     def _var_key(self, variable):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cce064403f0ac3fdea12007d31159533719c4f98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 4178 | Contributors (this commit): 20 | Commits (past 90d): 26 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -1125,14 +1125,14 @@ class LossScaleOptimizerV3(\n                 # Give better error message if the OptimizerV2 class is passed\n                 # instead of the new experimental optimizer.\n                 raise TypeError(\n-                    \"You passed a `tf.keras.optimizer.Optimizer` instance to \"\n+                    \"You passed a `tf.keras.optimizers.Optimizer` instance to \"\n                     \"LossScaleOptimizerV3, but only the new experimental \"\n                     \"optimizer defined in \"\n                     \"keras/optimizer_expeirmental/optimizer.py can be \"\n                     \"passed. Please use \"\n                     \"`tf.keras.mixed_precision.LossScaleOptimizer` \"\n                     \"instead of LossScaleOptimizerV3, as the former supports \"\n-                    \"`tf.keras.optimizer.Optimizer`s. Got optimizer: \"\n+                    \"`tf.keras.optimizers.Optimizer`s. Got optimizer: \"\n                     f\"{inner_optimizer}\"\n                 )\n             raise TypeError(\n\n@@ -196,7 +196,7 @@ def convert_to_legacy_optimizer(optimizer):\n \n     This function takes in a `tf.keras.optimizers.experimental.Optimizer`\n     instance and converts it to the corresponding\n-    `tf.keras.optimizer.legacy.Optimizer` instance.\n+    `tf.keras.optimizers.legacy.Optimizer` instance.\n     For example, `tf.keras.optimizers.experimental.Adam(...)` to\n     `tf.keras.optimizers.legacy.Adam(...)`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bece2b30bc97534e42e742cc8242d3a9faebaaa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 11 | Complexity Δ (Sum/Max): 12/10 | Churn Δ: 95 | Churn Cumulative: 28873 | Contributors (this commit): 142 | Commits (past 90d): 43 | Contributors (cumulative): 158 | DMM Complexity: 0.7761194029850746\n\nDIFF:\n@@ -50,6 +50,7 @@ from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n+from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n from keras.utils import version_utils\n@@ -3057,6 +3058,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         }\n         return model_config\n \n+    @generic_utils.default\n     def get_config(self):\n         \"\"\"Returns the config of the `Model`.\n \n@@ -3072,18 +3074,24 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Developers of subclassed `Model` are advised to override this method,\n         and continue to update the dict from `super(MyModel, self).get_config()`\n         to provide the proper configuration of this `Model`. The default config\n-        is an empty dict. Optionally, raise `NotImplementedError` to allow Keras\n-        to attempt a default serialization.\n+        will return config dict for init parameters if they are basic types.\n+        Raises `NotImplementedError` when in cases where a custom\n+        `get_config()` implementation is required for the subclassed model.\n \n         Returns:\n             Python dictionary containing the configuration of this `Model`.\n         \"\"\"\n-        # Return an empty dict here because otherwise Model\n-        # subclass developers may see\n-        # their model's `__init__()` fed with unexpected keyword arguments,\n-        # if their `__init__()` takes no argument for example, and they\n-        # don't override `from_config()`, which would use `cls(**config)`\n-        # as a result.\n+        # If sublcass doesn't implement `get_config()` parse from init args\n+        # otherwise default to empty dict\n+        if generic_utils.is_default(self.get_config):\n+            config = super().get_config()\n+            # `super.get_config` adds additional keys, keep them if they\n+            # are explicitly specified in `__init__`\n+            init_args = tf_inspect.getfullargspec(self.__init__).args[1:]\n+            xtra_args = set([\"name\", \"trainable\", \"dtype\", \"batch_input_shape\"])\n+            for key in xtra_args - xtra_args.intersection(init_args):\n+                config.pop(key, None)\n+        else:\n             config = {}\n         if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n             if self._is_compiled and hasattr(self, \"_compile_config\"):\n\n@@ -942,6 +942,77 @@ class TrainingTest(test_combinations.TestCase):\n         model = MyModel()\n         self.assertIn('{\"a\": {}}', model.to_json())\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_get_config_default(self):\n+        class MyModel(training_module.Model):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.units = units\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+        # Test default config with named args\n+        model = MyModel(units=10)\n+        config = model.get_config()\n+        self.assertLen(config, 1)\n+        self.assertEqual(config[\"units\"], 10)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n+        # Test default config with positinal args\n+        model = MyModel(10)\n+        config = model.get_config()\n+        self.assertLen(config, 1)\n+        self.assertEqual(config[\"units\"], 10)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n+        # Test non-serializable\n+        model = MyModel(units=np.int32(10))\n+        with self.assertRaises(NotImplementedError):\n+            model.get_config()\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_get_config_kwargs(self):\n+        class MyModel(training_module.Model):\n+            def __init__(self, units, **kwargs):\n+                super().__init__()\n+                self.units = units\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+        model = MyModel(10, extra=1)\n+        config = model.get_config()\n+        self.assertLen(config, 2)\n+        self.assertEqual(config[\"units\"], 10)\n+        self.assertEqual(config[\"extra\"], 1)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_get_config_override(self):\n+        class MyModel(training_module.Model):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.units = units\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+            def get_config(self):\n+                config = {\"units\": int(self.units)}\n+                config.update(super().get_config())\n+                return config\n+\n+        model = MyModel(units=np.int32(10))\n+        config = model.get_config()\n+        self.assertLen(config, 1)\n+        self.assertEqual(config[\"units\"], 10)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n     def test_training_on_sparse_data_with_dense_placeholders_v1(self):\n         with tf.Graph().as_default():\n             if scipy_sparse is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c54533a8c82c81ea9c04b2f598b7f989e8b3f3e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 102 | Files Changed: 4 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): -11/1 | Churn Δ: 125 | Churn Cumulative: 30999 | Contributors (this commit): 145 | Commits (past 90d): 103 | Contributors (cumulative): 175 | DMM Complexity: 0.234375\n\nDIFF:\n@@ -50,7 +50,6 @@ from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n-from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n from keras.utils import version_utils\n@@ -3058,7 +3057,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         }\n         return model_config\n \n-    @generic_utils.default\n     def get_config(self):\n         \"\"\"Returns the config of the `Model`.\n \n@@ -3074,24 +3072,18 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Developers of subclassed `Model` are advised to override this method,\n         and continue to update the dict from `super(MyModel, self).get_config()`\n         to provide the proper configuration of this `Model`. The default config\n-        will return config dict for init parameters if they are basic types.\n-        Raises `NotImplementedError` when in cases where a custom\n-        `get_config()` implementation is required for the subclassed model.\n+        is an empty dict. Optionally, raise `NotImplementedError` to allow Keras\n+        to attempt a default serialization.\n \n         Returns:\n             Python dictionary containing the configuration of this `Model`.\n         \"\"\"\n-        # If sublcass doesn't implement `get_config()` parse from init args\n-        # otherwise default to empty dict\n-        if generic_utils.is_default(self.get_config):\n-            config = super().get_config()\n-            # `super.get_config` adds additional keys, keep them if they\n-            # are explicitly specified in `__init__`\n-            init_args = tf_inspect.getfullargspec(self.__init__).args[1:]\n-            xtra_args = set([\"name\", \"trainable\", \"dtype\", \"batch_input_shape\"])\n-            for key in xtra_args - xtra_args.intersection(init_args):\n-                config.pop(key, None)\n-        else:\n+        # Return an empty dict here because otherwise Model\n+        # subclass developers may see\n+        # their model's `__init__()` fed with unexpected keyword arguments,\n+        # if their `__init__()` takes no argument for example, and they\n+        # don't override `from_config()`, which would use `cls(**config)`\n+        # as a result.\n         config = {}\n         if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n             if self._is_compiled and hasattr(self, \"_compile_config\"):\n\n@@ -942,77 +942,6 @@ class TrainingTest(test_combinations.TestCase):\n         model = MyModel()\n         self.assertIn('{\"a\": {}}', model.to_json())\n \n-    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_get_config_default(self):\n-        class MyModel(training_module.Model):\n-            def __init__(self, units):\n-                super().__init__()\n-                self.units = units\n-\n-            def call(self, inputs):\n-                return inputs\n-\n-        # Test default config with named args\n-        model = MyModel(units=10)\n-        config = model.get_config()\n-        self.assertLen(config, 1)\n-        self.assertEqual(config[\"units\"], 10)\n-        model = model.from_config(config)\n-        self.assertDictEqual(model.get_config(), config)\n-\n-        # Test default config with positinal args\n-        model = MyModel(10)\n-        config = model.get_config()\n-        self.assertLen(config, 1)\n-        self.assertEqual(config[\"units\"], 10)\n-        model = model.from_config(config)\n-        self.assertDictEqual(model.get_config(), config)\n-\n-        # Test non-serializable\n-        model = MyModel(units=np.int32(10))\n-        with self.assertRaises(NotImplementedError):\n-            model.get_config()\n-\n-    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_get_config_kwargs(self):\n-        class MyModel(training_module.Model):\n-            def __init__(self, units, **kwargs):\n-                super().__init__()\n-                self.units = units\n-\n-            def call(self, inputs):\n-                return inputs\n-\n-        model = MyModel(10, extra=1)\n-        config = model.get_config()\n-        self.assertLen(config, 2)\n-        self.assertEqual(config[\"units\"], 10)\n-        self.assertEqual(config[\"extra\"], 1)\n-        model = model.from_config(config)\n-        self.assertDictEqual(model.get_config(), config)\n-\n-    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_get_config_override(self):\n-        class MyModel(training_module.Model):\n-            def __init__(self, units):\n-                super().__init__()\n-                self.units = units\n-\n-            def call(self, inputs):\n-                return inputs\n-\n-            def get_config(self):\n-                config = {\"units\": int(self.units)}\n-                config.update(super().get_config())\n-                return config\n-\n-        model = MyModel(units=np.int32(10))\n-        config = model.get_config()\n-        self.assertLen(config, 1)\n-        self.assertEqual(config[\"units\"], 10)\n-        model = model.from_config(config)\n-        self.assertDictEqual(model.get_config(), config)\n-\n     def test_training_on_sparse_data_with_dense_placeholders_v1(self):\n         with tf.Graph().as_default():\n             if scipy_sparse is None:\n\n@@ -101,18 +101,25 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     def _process_kwargs(self, kwargs):\n         # Remove the `is_legacy_optimizer` arg, which is for serialization only.\n         kwargs.pop(\"is_legacy_optimizer\", None)\n+        lr = kwargs.pop(\"lr\", None)\n+        if lr:\n+            logging.warning(\n+                \"`lr` is deprecated in Keras optimizer, please use \"\n+                \"`learning_rate` or use the legacy optimizer, e.g.,\"\n+                f\"tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\n+            )\n         legacy_kwargs = {\n-            \"lr\",\n             \"decay\",\n-            \"gradient_transformers\",\n             \"gradient_aggregator\",\n+            \"gradient_transformers\",\n         }\n         for k in kwargs:\n             if k in legacy_kwargs:\n-                logging.warning(\n-                    \"%s is deprecated in `optimizer_experimental.Optimizer`\"\n-                    \", please check the docstring for valid arguments.\",\n-                    k,\n+                raise ValueError(\n+                    f\"{k} is deprecated in the new Keras optimizer, please\"\n+                    \"check the docstring for valid arguments, or use the \"\n+                    \"legacy optimizer, e.g., \"\n+                    f\"tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\n                 )\n             else:\n                 raise TypeError(\n\n@@ -4,11 +4,9 @@ More context in go/new-keras-optimizer\n \"\"\"\n \n import os\n-import re\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n-from absl import logging\n from absl.testing import parameterized\n \n import keras\n@@ -209,14 +207,9 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         clipped_grad = optimizer._clip_gradients(grad)\n         self.assertAllClose(clipped_grad[0], [0.5, 0.5])\n \n-    def testPassingLegacyArgsRaiseWarning(self):\n-        with self.assertLogs(level=\"WARNING\") as log_output:\n-            logging.set_verbosity(logging.WARNING)\n+    def testPassingLegacyArgsRaiseError(self):\n+        with self.assertRaisesRegex(ValueError, \"decay is deprecated*\"):\n             _ = adam_new.Adam(clipnorm=1, decay=0.5)\n-            expected_log = \"decay is deprecated in\"\n-            output = log_output[0][0].message\n-\n-            self.assertTrue(re.search(expected_log, output))\n \n     def testPassingLegacyClipnorm(self):\n         optimizer = adam_new.Adam(clipnorm=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c873c58c2a4a32d8fca4775022b210c74d4279aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 5831 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -42,13 +42,13 @@ BOUNDING_BOXES = \"bounding_boxes\"\n def check_fill_mode_and_interpolation(fill_mode, interpolation):\n     if fill_mode not in {\"reflect\", \"wrap\", \"constant\", \"nearest\"}:\n         raise NotImplementedError(\n-            \"Unknown `fill_mode` {}. Only `reflect`, `wrap`, \"\n-            \"`constant` and `nearest` are supported.\".format(fill_mode)\n+            f\"Unknown `fill_mode` {fill_mode}. Only `reflect`, `wrap`, \"\n+            \"`constant` and `nearest` are supported.\"\n         )\n     if interpolation not in {\"nearest\", \"bilinear\"}:\n         raise NotImplementedError(\n-            \"Unknown `interpolation` {}. Only `nearest` and \"\n-            \"`bilinear` are supported.\".format(interpolation)\n+            f\"Unknown `interpolation` {interpolation}. Only `nearest` and \"\n+            \"`bilinear` are supported.\"\n         )\n \n \n@@ -744,8 +744,8 @@ class RandomFlip(BaseImageAugmentationLayer):\n             self.vertical = True\n         else:\n             raise ValueError(\n-                \"RandomFlip layer {name} received an unknown mode \"\n-                \"argument {arg}\".format(name=self.name, arg=mode)\n+                f\"RandomFlip layer {self.name} received an unknown mode \"\n+                f\"argument {mode}\"\n             )\n         self.auto_vectorize = False\n \n@@ -871,12 +871,12 @@ class RandomTranslation(BaseImageAugmentationLayer):\n         if self.height_upper < self.height_lower:\n             raise ValueError(\n                 \"`height_factor` cannot have upper bound less than \"\n-                \"lower bound, got {}\".format(height_factor)\n+                f\"lower bound, got {height_factor}\"\n             )\n         if abs(self.height_lower) > 1.0 or abs(self.height_upper) > 1.0:\n             raise ValueError(\n                 \"`height_factor` must have values between [-1, 1], \"\n-                \"got {}\".format(height_factor)\n+                f\"got {height_factor}\"\n             )\n \n         self.width_factor = width_factor\n@@ -889,12 +889,12 @@ class RandomTranslation(BaseImageAugmentationLayer):\n         if self.width_upper < self.width_lower:\n             raise ValueError(\n                 \"`width_factor` cannot have upper bound less than \"\n-                \"lower bound, got {}\".format(width_factor)\n+                f\"lower bound, got {width_factor}\"\n             )\n         if abs(self.width_lower) > 1.0 or abs(self.width_upper) > 1.0:\n             raise ValueError(\n                 \"`width_factor` must have values between [-1, 1], \"\n-                \"got {}\".format(width_factor)\n+                f\"got {width_factor}\"\n             )\n \n         check_fill_mode_and_interpolation(fill_mode, interpolation)\n@@ -1096,7 +1096,7 @@ def transform(\n             raise ValueError(\n                 \"output_shape must be a 1-D Tensor of 2 elements: \"\n                 \"new_height, new_width, instead got \"\n-                \"{}\".format(output_shape)\n+                f\"{output_shape}\"\n             )\n \n         fill_value = tf.convert_to_tensor(\n@@ -1388,7 +1388,7 @@ class RandomZoom(BaseImageAugmentationLayer):\n         if abs(self.height_lower) > 1.0 or abs(self.height_upper) > 1.0:\n             raise ValueError(\n                 \"`height_factor` must have values between [-1, 1], \"\n-                \"got {}\".format(height_factor)\n+                f\"got {height_factor}\"\n             )\n \n         self.width_factor = width_factor\n@@ -1403,7 +1403,7 @@ class RandomZoom(BaseImageAugmentationLayer):\n             if self.width_lower < -1.0 or self.width_upper < -1.0:\n                 raise ValueError(\n                     \"`width_factor` must have values larger than -1, \"\n-                    \"got {}\".format(width_factor)\n+                    f\"got {width_factor}\"\n                 )\n \n         check_fill_mode_and_interpolation(fill_mode, interpolation)\n@@ -1573,7 +1573,7 @@ class RandomContrast(BaseImageAugmentationLayer):\n         if self.lower < 0.0 or self.upper < 0.0 or self.lower > 1.0:\n             raise ValueError(\n                 \"Factor cannot have negative values or greater than 1.0,\"\n-                \" got {}\".format(factor)\n+                f\" got {factor}\"\n             )\n         self.seed = seed\n \n@@ -1824,7 +1824,7 @@ class RandomHeight(BaseImageAugmentationLayer):\n         if self.height_upper < self.height_lower:\n             raise ValueError(\n                 \"`factor` cannot have upper bound less than \"\n-                \"lower bound, got {}\".format(factor)\n+                f\"lower bound, got {factor}\"\n             )\n         if self.height_lower < -1.0 or self.height_upper < -1.0:\n             raise ValueError(\n@@ -1947,7 +1947,7 @@ class RandomWidth(BaseImageAugmentationLayer):\n         if self.width_upper < self.width_lower:\n             raise ValueError(\n                 \"`factor` cannot have upper bound less than \"\n-                \"lower bound, got {}\".format(factor)\n+                f\"lower bound, got {factor}\"\n             )\n         if self.width_lower < -1.0 or self.width_upper < -1.0:\n             raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c401c032b3021f89609eac79bd1c881b9bbc84f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 205 | Lines Deleted: 186 | Files Changed: 5 | Hunks: 19 | Methods Changed: 16 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 391 | Churn Cumulative: 4070 | Contributors (this commit): 14 | Commits (past 90d): 10 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,7 +65,7 @@ def get_model(sync_batchnorm=False):\n         )\n     )\n     if sync_batchnorm:\n-        model.add(keras.layers.SyncBatchNormalization())\n+        model.add(keras.layers.BatchNormalization(synchronized=True))\n     else:\n         model.add(keras.layers.BatchNormalization())\n     model.add(keras.layers.Dense(10, activation=\"relu\"))\n\n@@ -28,6 +28,7 @@ from keras.testing_infra import test_utils\n     \"float64, the test sometimes fails with TensorFloat-32 enabled for unknown \"\n     \"reasons\"\n )\n+@test_utils.run_v2_only()\n class DistributionStrategyCnnCorrectnessTest(\n     keras_correctness_test_base.TestDistributionStrategyCorrectnessBase\n ):\n@@ -48,8 +49,12 @@ class DistributionStrategyCnnCorrectnessTest(\n                 c1 = keras.layers.BatchNormalization(name=\"bn1\")(c1)\n             elif self.with_batch_norm == \"sync\":\n                 # Test with parallel batch norms to verify all-reduce works OK.\n-                bn1 = keras.layers.SyncBatchNormalization(name=\"bn1\")(c1)\n-                bn2 = keras.layers.SyncBatchNormalization(name=\"bn2\")(c1)\n+                bn1 = keras.layers.BatchNormalization(\n+                    name=\"bn1\", synchronized=True\n+                )(c1)\n+                bn2 = keras.layers.BatchNormalization(\n+                    name=\"bn2\", synchronized=True\n+                )(c1)\n                 c1 = keras.layers.Add()([bn1, bn2])\n             c1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(c1)\n             logits = keras.layers.Dense(10, activation=\"softmax\", name=\"pred\")(\n@@ -133,8 +138,9 @@ class DistributionStrategyCnnCorrectnessTest(\n         self, distribution, use_numpy, use_validation_data\n     ):\n         if not tf.executing_eagerly():\n-            self.skipTest(\"SyncBatchNorm is not enabled in graph mode.\")\n-\n+            self.skipTest(\n+                \"BatchNorm with `synchronized` is not enabled in graph mode.\"\n+            )\n         self.run_correctness_test(\n             distribution, use_numpy, use_validation_data, with_batch_norm=\"sync\"\n         )\n\n@@ -31,6 +31,7 @@ from tensorflow.python.ops.control_flow_ops import (\n     get_enclosing_xla_context,\n )\n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util import deprecation\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -1100,181 +1101,6 @@ class BatchNormalizationBase(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export(\"keras.layers.experimental.SyncBatchNormalization\", v1=[])\n-class SyncBatchNormalization(BatchNormalizationBase):\n-    r\"\"\"Normalize and scale inputs or activations synchronously across replicas.\n-\n-    Applies batch normalization to activations of the previous layer at each\n-    batch by synchronizing the global batch statistics across all devices that\n-    are training the model. For specific details about batch normalization\n-    please refer to the `tf.keras.layers.BatchNormalization` layer docs.\n-\n-    If this layer is used when using tf.distribute strategy to train models\n-    across devices/workers, there will be an allreduce call to aggregate batch\n-    statistics across all replicas at every training step. Without tf.distribute\n-    strategy, this layer behaves as a regular\n-    `tf.keras.layers.BatchNormalization` layer.\n-\n-    Example usage:\n-\n-    ```python\n-    strategy = tf.distribute.MirroredStrategy()\n-\n-    with strategy.scope():\n-      model = tf.keras.Sequential()\n-      model.add(tf.keras.layers.Dense(16))\n-      model.add(tf.keras.layers.experimental.SyncBatchNormalization())\n-    ```\n-\n-    Args:\n-      axis: Integer, the axis that should be normalized\n-        (typically the features axis).\n-        For instance, after a `Conv2D` layer with\n-        `data_format=\"channels_first\"`,\n-        set `axis=1` in `BatchNormalization`.\n-      momentum: Momentum for the moving average.\n-      epsilon: Small float added to variance to avoid dividing by zero.\n-      center: If True, add offset of `beta` to normalized tensor.\n-        If False, `beta` is ignored.\n-      scale: If True, multiply by `gamma`.\n-        If False, `gamma` is not used.\n-        When the next layer is linear (also e.g. `nn.relu`),\n-        this can be disabled since the scaling\n-        will be done by the next layer.\n-      beta_initializer: Initializer for the beta weight.\n-      gamma_initializer: Initializer for the gamma weight.\n-      moving_mean_initializer: Initializer for the moving mean.\n-      moving_variance_initializer: Initializer for the moving variance.\n-      beta_regularizer: Optional regularizer for the beta weight.\n-      gamma_regularizer: Optional regularizer for the gamma weight.\n-      beta_constraint: Optional constraint for the beta weight.\n-      gamma_constraint: Optional constraint for the gamma weight.\n-\n-    Call arguments:\n-      inputs: Input tensor (of any rank).\n-      training: Python boolean indicating whether the layer should behave in\n-        training mode or in inference mode.\n-        - `training=True`: The layer will normalize its inputs using the\n-          mean and variance of the current batch of inputs.\n-        - `training=False`: The layer will normalize its inputs using the\n-          mean and variance of its moving statistics, learned during training.\n-\n-    Input shape:\n-      Arbitrary. Use the keyword argument `input_shape`\n-      (tuple of integers, does not include the samples axis)\n-      when using this layer as the first layer in a model.\n-\n-    Output shape:\n-      Same shape as input.\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        axis=-1,\n-        momentum=0.99,\n-        epsilon=1e-3,\n-        center=True,\n-        scale=True,\n-        beta_initializer=\"zeros\",\n-        gamma_initializer=\"ones\",\n-        moving_mean_initializer=\"zeros\",\n-        moving_variance_initializer=\"ones\",\n-        beta_regularizer=None,\n-        gamma_regularizer=None,\n-        beta_constraint=None,\n-        gamma_constraint=None,\n-        **kwargs,\n-    ):\n-        if kwargs.pop(\"fused\", None):\n-            raise ValueError(\n-                \"`fused` argument cannot be True for SyncBatchNormalization.\"\n-            )\n-\n-        # Currently we only support aggregating over the global batch size.\n-        super().__init__(\n-            axis=axis,\n-            momentum=momentum,\n-            epsilon=epsilon,\n-            center=center,\n-            scale=scale,\n-            beta_initializer=beta_initializer,\n-            gamma_initializer=gamma_initializer,\n-            moving_mean_initializer=moving_mean_initializer,\n-            moving_variance_initializer=moving_variance_initializer,\n-            beta_regularizer=beta_regularizer,\n-            gamma_regularizer=gamma_regularizer,\n-            beta_constraint=beta_constraint,\n-            gamma_constraint=gamma_constraint,\n-            fused=False,\n-            **kwargs,\n-        )\n-\n-    def _calculate_mean_and_var(self, x, axes, keep_dims):\n-\n-        with backend.name_scope(\"moments\"):\n-            # The dynamic range of fp16 is too limited to support the collection\n-            # of sufficient statistics. As a workaround we simply perform the\n-            # operations on 32-bit floats before converting the mean and\n-            # variance back to fp16\n-            y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n-            replica_ctx = tf.distribute.get_replica_context()\n-            if replica_ctx:\n-                local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n-                local_squared_sum = tf.reduce_sum(\n-                    tf.square(y), axis=axes, keepdims=True\n-                )\n-                batch_size = tf.cast(tf.shape(y)[axes[0]], tf.float32)\n-                # TODO(b/163099951): batch the all-reduces once we sort out the\n-                # ordering issue for NCCL. We don't have a mechanism to launch\n-                # NCCL in the same order in each replica nowadays, so we limit\n-                # NCCL to batch all-reduces.\n-                y_sum = replica_ctx.all_reduce(\n-                    tf.distribute.ReduceOp.SUM, local_sum\n-                )\n-                y_squared_sum = replica_ctx.all_reduce(\n-                    tf.distribute.ReduceOp.SUM, local_squared_sum\n-                )\n-                global_batch_size = replica_ctx.all_reduce(\n-                    tf.distribute.ReduceOp.SUM, batch_size\n-                )\n-\n-                axes_vals = [\n-                    (tf.shape(y))[axes[i]] for i in range(1, len(axes))\n-                ]\n-                multiplier = tf.cast(tf.reduce_prod(axes_vals), tf.float32)\n-                multiplier = multiplier * global_batch_size\n-\n-                mean = y_sum / multiplier\n-                y_squared_mean = y_squared_sum / multiplier\n-                # var = E(x^2) - E(x)^2\n-                variance = y_squared_mean - tf.square(mean)\n-            else:\n-                # Compute true mean while keeping the dims for proper\n-                # broadcasting.\n-                mean = tf.reduce_mean(y, axes, keepdims=True, name=\"mean\")\n-                # sample variance, not unbiased variance\n-                # Note: stop_gradient does not change the gradient that gets\n-                # backpropagated to the mean from the variance calculation,\n-                # because that gradient is zero\n-                variance = tf.reduce_mean(\n-                    tf.math.squared_difference(y, tf.stop_gradient(mean)),\n-                    axes,\n-                    keepdims=True,\n-                    name=\"variance\",\n-                )\n-            if not keep_dims:\n-                mean = tf.squeeze(mean, axes)\n-                variance = tf.squeeze(variance, axes)\n-            if x.dtype == tf.float16:\n-                return (\n-                    tf.cast(mean, tf.float16),\n-                    tf.cast(variance, tf.float16),\n-                )\n-            else:\n-                return (mean, variance)\n-\n-\n @keras_export(\"keras.layers.BatchNormalization\", v1=[])\n class BatchNormalization(BatchNormalizationBase):\n     \"\"\"Layer that normalizes its inputs.\n@@ -1315,6 +1141,23 @@ class BatchNormalization(BatchNormalizationBase):\n     *after having been trained on data that has similar statistics as the\n     inference data*.\n \n+    When `synchronized=True` is set and if this layer is used within a\n+    `tf.distribute` strategy, there will be an `allreduce` call\n+    to aggregate batch statistics across all replicas at every\n+    training step. Setting `synchronized` has no impact when the model is\n+    trained without specifying any distribution strategy.\n+\n+    Example usage:\n+\n+    ```python\n+    strategy = tf.distribute.MirroredStrategy()\n+\n+    with strategy.scope():\n+      model = tf.keras.Sequential()\n+      model.add(tf.keras.layers.Dense(16))\n+      model.add(tf.keras.layers.BatchNormalization(synchronized=True))\n+    ```\n+\n     Args:\n       axis: Integer, the axis that should be normalized (typically the features\n         axis). For instance, after a `Conv2D` layer with\n@@ -1334,6 +1177,11 @@ class BatchNormalization(BatchNormalizationBase):\n       gamma_regularizer: Optional regularizer for the gamma weight.\n       beta_constraint: Optional constraint for the beta weight.\n       gamma_constraint: Optional constraint for the gamma weight.\n+      synchronized: If True, synchronizes the global batch statistics (mean and\n+        variance) for the layer across all devices at each training step in a\n+        distributed training strategy. If False, each replica uses its own\n+        local batch statistics. Only relevant when used inside a\n+        `tf.distribute` strategy.\n \n     Call arguments:\n       inputs: Input tensor (of any rank).\n@@ -1404,8 +1252,19 @@ class BatchNormalization(BatchNormalizationBase):\n         gamma_regularizer=None,\n         beta_constraint=None,\n         gamma_constraint=None,\n+        synchronized=False,\n         **kwargs,\n     ):\n+        fused = kwargs.get(\"fused\", None)\n+        if synchronized and fused:\n+            raise ValueError(\n+                \"`fused=True` is not supported when `synchronized=True`.\"\n+            )\n+        self.synchronized = synchronized\n+        if self.synchronized:\n+            kwargs[\"fused\"] = False\n+\n+        # Currently we only support aggregating over the global batch size.\n         super().__init__(\n             axis=axis,\n             momentum=momentum,\n@@ -1422,3 +1281,128 @@ class BatchNormalization(BatchNormalizationBase):\n             gamma_constraint=gamma_constraint,\n             **kwargs,\n         )\n+\n+    def _calculate_mean_and_var(self, x, axes, keep_dims):\n+        \"\"\"Override mean and var calculation when used with `synchronized`.\"\"\"\n+        if self.synchronized:\n+            return self._sync_calculate_mean_and_var(x, axes, keep_dims)\n+        else:\n+            return super()._calculate_mean_and_var(x, axes, keep_dims)\n+\n+    def _sync_calculate_mean_and_var(self, x, axes, keep_dims):\n+        with backend.name_scope(\"moments\"):\n+            # The dynamic range of fp16 is too limited to support the collection\n+            # of sufficient statistics. As a workaround we simply perform the\n+            # operations on 32-bit floats before converting the mean and\n+            # variance back to fp16\n+            y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n+            replica_ctx = tf.distribute.get_replica_context()\n+            if replica_ctx:\n+                local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n+                local_squared_sum = tf.reduce_sum(\n+                    tf.square(y), axis=axes, keepdims=True\n+                )\n+                batch_size = tf.cast(tf.shape(y)[axes[0]], tf.float32)\n+                # TODO(b/163099951): batch the all-reduces once we sort out the\n+                # ordering issue for NCCL. We don't have a mechanism to launch\n+                # NCCL in the same order in each replica nowadays, so we limit\n+                # NCCL to batch all-reduces.\n+                y_sum = replica_ctx.all_reduce(\n+                    tf.distribute.ReduceOp.SUM, local_sum\n+                )\n+                y_squared_sum = replica_ctx.all_reduce(\n+                    tf.distribute.ReduceOp.SUM, local_squared_sum\n+                )\n+                global_batch_size = replica_ctx.all_reduce(\n+                    tf.distribute.ReduceOp.SUM, batch_size\n+                )\n+\n+                axes_vals = [\n+                    (tf.shape(y))[axes[i]] for i in range(1, len(axes))\n+                ]\n+                multiplier = tf.cast(tf.reduce_prod(axes_vals), tf.float32)\n+                multiplier = multiplier * global_batch_size\n+\n+                mean = y_sum / multiplier\n+                y_squared_mean = y_squared_sum / multiplier\n+                # var = E(x^2) - E(x)^2\n+                variance = y_squared_mean - tf.square(mean)\n+            else:\n+                # Compute true mean while keeping the dims for proper\n+                # broadcasting.\n+                mean = tf.reduce_mean(y, axes, keepdims=True, name=\"mean\")\n+                # sample variance, not unbiased variance\n+                # Note: stop_gradient does not change the gradient that gets\n+                # backpropagated to the mean from the variance calculation,\n+                # because that gradient is zero\n+                variance = tf.reduce_mean(\n+                    tf.math.squared_difference(y, tf.stop_gradient(mean)),\n+                    axes,\n+                    keepdims=True,\n+                    name=\"variance\",\n+                )\n+            if not keep_dims:\n+                mean = tf.squeeze(mean, axes)\n+                variance = tf.squeeze(variance, axes)\n+            if x.dtype == tf.float16:\n+                return (\n+                    tf.cast(mean, tf.float16),\n+                    tf.cast(variance, tf.float16),\n+                )\n+            else:\n+                return (mean, variance)\n+\n+\n+@keras_export(\"keras.layers.experimental.SyncBatchNormalization\", v1=[])\n+@deprecation.deprecated_endpoints(\n+    \"keras.layers.experimental.SyncBatchNormalization\"\n+)\n+class SyncBatchNormalization(BatchNormalization):\n+    \"\"\"Deprecated. Please use `tf.keras.layers.BatchNormalization` instead.\n+\n+    Caution: `tf.keras.layers.experimental.SyncBatchNormalization` endpoint is\n+      deprecated and will be removed in a future release. Please use\n+      `tf.keras.layers.BatchNormalization` with parameter `synchronized`\n+      set to True\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        axis=-1,\n+        momentum=0.99,\n+        epsilon=1e-3,\n+        center=True,\n+        scale=True,\n+        beta_initializer=\"zeros\",\n+        gamma_initializer=\"ones\",\n+        moving_mean_initializer=\"zeros\",\n+        moving_variance_initializer=\"ones\",\n+        beta_regularizer=None,\n+        gamma_regularizer=None,\n+        beta_constraint=None,\n+        gamma_constraint=None,\n+        **kwargs,\n+    ):\n+        logging.warning(\n+            \"`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is \"\n+            \"deprecated and will be removed in a future release. Please use \"\n+            \"`tf.keras.layers.BatchNormalization` with parameter \"\n+            \"`synchronized` set to True.\"\n+        )\n+        super().__init__(\n+            axis=axis,\n+            momentum=momentum,\n+            epsilon=epsilon,\n+            center=center,\n+            scale=scale,\n+            beta_initializer=beta_initializer,\n+            gamma_initializer=gamma_initializer,\n+            moving_mean_initializer=moving_mean_initializer,\n+            moving_variance_initializer=moving_variance_initializer,\n+            beta_regularizer=beta_regularizer,\n+            gamma_regularizer=gamma_regularizer,\n+            beta_constraint=beta_constraint,\n+            gamma_constraint=gamma_constraint,\n+            synchronized=True,\n+            **kwargs,\n+        )\n\n@@ -95,13 +95,22 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         self.assertEqual(layer.gamma.constraint, max_norm)\n         self.assertEqual(layer.beta.constraint, max_norm)\n \n-    @test_combinations.run_all_keras_modes\n-    def test_batchnorm_convnet(self):\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_batchnorm_sync_fused_error(self):\n+        with self.assertRaises(ValueError):\n+            _ = batch_normalization.BatchNormalization(\n+                synchronized=True, fused=True\n+            )\n+\n+    def _test_batchnorm_convnet(self, synchronized=False):\n         if tf.test.is_gpu_available(cuda_only=True):\n             with self.session():\n                 model = keras.models.Sequential()\n                 norm = keras.layers.BatchNormalization(\n-                    axis=1, input_shape=(3, 4, 4), momentum=0.8\n+                    axis=1,\n+                    input_shape=(3, 4, 4),\n+                    momentum=0.8,\n+                    synchronized=synchronized,\n                 )\n                 model.add(norm)\n                 model.compile(\n@@ -124,6 +133,14 @@ class BatchNormalizationTest(test_combinations.TestCase):\n                     np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1\n                 )\n \n+    @test_combinations.run_all_keras_modes\n+    def test_batchnorm_convnet(self):\n+        self._test_batchnorm_convnet(synchronized=False)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_batchnorm_convnet_synchronized(self):\n+        self._test_batchnorm_convnet(synchronized=True)\n+\n     @test_combinations.run_all_keras_modes\n     def test_batchnorm_convnet_channel_last(self):\n         model = keras.models.Sequential()\n@@ -155,6 +172,11 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         _run_batchnorm_correctness_test(\n             batch_normalization.BatchNormalization, dtype=\"float32\"\n         )\n+        _run_batchnorm_correctness_test(\n+            batch_normalization.BatchNormalization,\n+            dtype=\"float32\",\n+            synchronized=True,\n+        )\n \n     @test_combinations.run_all_keras_modes\n     def test_batchnorm_float16(self):\n@@ -451,10 +473,12 @@ class BatchNormalizationV2Test(test_combinations.TestCase):\n         self.assertAllEqual(layer.beta, tape_vars[1])\n \n \n-def _run_batchnorm_correctness_test(layer, dtype=\"float32\", fused=False):\n+def _run_batchnorm_correctness_test(\n+    layer, dtype=\"float32\", fused=False, synchronized=False\n+):\n     model = keras.models.Sequential()\n     model.add(keras.Input(shape=(2, 2, 2), dtype=dtype))\n-    norm = layer(momentum=0.8, fused=fused)\n+    norm = layer(momentum=0.8, fused=fused, synchronized=synchronized)\n     model.add(norm)\n     if dtype == \"float16\":\n         # Keras models require float32 losses.\n@@ -558,7 +582,7 @@ class NormalizationLayersGraphModeOnlyTest(\n             self.assertAllClose(x1, x2, atol=1e-7)\n \n     def test_batchnorm_trainable(self, layer):\n-        \"\"\"Tests that batchnorm layer is trainable when learning phase is enabled.\n+        \"\"\"Tests that batchnorm layer is trainable when learning phase is set.\n \n         Computes mean and std for current inputs then\n         applies batch normalization using them.\n\n@@ -24,3 +24,8 @@ from tensorflow.python.util.tf_export import keras_export\n @keras_export(v1=[\"keras.layers.BatchNormalization\"])\n class BatchNormalization(batch_normalization.BatchNormalizationBase):\n     _USE_V2_BEHAVIOR = False\n+\n+    def __init__(self, *args, **kwargs):\n+        # synchronized not implemented in V1\n+        kwargs.pop(\"synchronized\", None)\n+        super().__init__(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
