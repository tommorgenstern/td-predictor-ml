{"custom_id": "scrapy#13181ba7882f2aef3ea103a9c2391fd8f87fbb44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 217 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,7 +32,7 @@ class HttpErrorMiddleware:\n         if 200 <= response.status < 300:  # common case\n             return\n         meta = response.meta\n-        if 'handle_httpstatus_all' in meta:\n+        if meta.get('handle_httpstatus_all', False):\n             return\n         if 'handle_httpstatus_list' in meta:\n             allowed_statuses = meta['handle_httpstatus_list']\n\n@@ -139,6 +139,19 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n         self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)\n \n+    def test_httperror_allow_all_false(self):\n+        crawler = get_crawler(_HttpErrorSpider)\n+        mw = HttpErrorMiddleware.from_crawler(crawler)\n+        request_httpstatus_false = Request('http://scrapytest.org', meta={'handle_httpstatus_all': False})\n+        request_httpstatus_true = Request('http://scrapytest.org', meta={'handle_httpstatus_all': True})\n+        res404 = self.res404.copy()\n+        res404.request = request_httpstatus_false\n+        res402 = self.res402.copy()\n+        res402.request = request_httpstatus_true\n+\n+        self.assertRaises(HttpError, mw.process_spider_input, res404, self.spider)\n+        self.assertIsNone(mw.process_spider_input(res402, self.spider))\n+\n \n class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0c3019d90f187482cd84b946b83c496411ec34b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 16 | Methods Changed: 18 | Complexity Δ (Sum/Max): 21/15 | Churn Δ: 83 | Churn Cumulative: 3594 | Contributors (this commit): 23 | Commits (past 90d): 50 | Contributors (cumulative): 26 | DMM Complexity: 0.3055555555555556\n\nDIFF:\n@@ -6,7 +6,7 @@ from urllib.parse import urldefrag\n from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n-from twisted.web.client import URI\n+from twisted.web.client import URI, BrowserLikePolicyForHTTPS\n \n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n@@ -45,19 +45,24 @@ class ScrapyProxyH2Agent(H2Agent):\n     def __init__(\n         self, reactor: ReactorBase,\n         proxy_uri: URI, pool: H2ConnectionPool,\n+        context_factory=BrowserLikePolicyForHTTPS(),\n         connect_timeout: Optional[float] = None, bind_address: Optional[bytes] = None\n     ) -> None:\n         super(ScrapyProxyH2Agent, self).__init__(\n             reactor=reactor,\n             pool=pool,\n+            context_factory=context_factory,\n             connect_timeout=connect_timeout,\n             bind_address=bind_address\n         )\n         self._proxy_uri = proxy_uri\n \n-    @staticmethod\n-    def get_key(uri: URI) -> Tuple:\n-        return \"http-proxy\", uri.host, uri.port\n+    def get_endpoint(self, uri: URI):\n+        return self.endpoint_factory.endpointForURI(self._proxy_uri)\n+\n+    def get_key(self, uri: URI) -> Tuple:\n+        \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n+        return \"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n \n \n class ScrapyH2Agent:\n@@ -100,6 +105,7 @@ class ScrapyH2Agent:\n             else:\n                 return self._ProxyAgent(\n                     reactor=reactor,\n+                    context_factory=self._context_factory,\n                     proxy_uri=URI.fromBytes(bytes(proxy, encoding='ascii')),\n                     connect_timeout=timeout,\n                     bind_address=bind_address,\n\n@@ -118,22 +118,25 @@ class H2Agent:\n         self._reactor = reactor\n         self._pool = pool\n         self._context_factory = AcceptableProtocolsContextFactory(context_factory, acceptable_protocols=[b'h2'])\n-        self._endpoint_factory = _StandardEndpointFactory(\n+        self.endpoint_factory = _StandardEndpointFactory(\n             self._reactor, self._context_factory,\n             connect_timeout, bind_address\n         )\n \n-    def _get_endpoint(self, uri: URI):\n-        return self._endpoint_factory.endpointForURI(uri)\n+    def get_endpoint(self, uri: URI):\n+        return self.endpoint_factory.endpointForURI(uri)\n \n-    @staticmethod\n-    def get_key(uri: URI) -> Tuple:\n+    def get_key(self, uri: URI) -> Tuple:\n+        \"\"\"\n+        Arguments:\n+            uri - URI obtained directly from request URL\n+        \"\"\"\n         return uri.scheme, uri.host, uri.port\n \n     def request(self, request: Request, spider: Spider) -> Deferred:\n         uri = URI.fromBytes(bytes(request.url, encoding='utf-8'))\n         try:\n-            endpoint = self._get_endpoint(uri)\n+            endpoint = self.get_endpoint(uri)\n         except SchemeNotSupported:\n             return defer.fail(Failure())\n \n\n@@ -185,10 +185,28 @@ class Stream:\n         if url.query:\n             path += '?' + url.query\n \n+        # This pseudo-header field MUST NOT be empty for \"http\" or \"https\"\n+        # URIs; \"http\" or \"https\" URIs that do not contain a path component\n+        # MUST include a value of '/'. The exception to this rule is an\n+        # OPTIONS request for an \"http\" or \"https\" URI that does not include\n+        # a path component; these MUST include a \":path\" pseudo-header field\n+        # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n+        if not path:\n+            if self._request.method == 'OPTIONS':\n+                path = path or '*'\n+            else:\n+                path = path or '/'\n+\n         # Make sure pseudo-headers comes before all the other headers\n         headers = [\n             (':method', self._request.method),\n             (':authority', url.netloc),\n+        ]\n+\n+        # The \":scheme\" and \":path\" pseudo-header fields MUST\n+        # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)\n+        if self._request.method != 'CONNECT':\n+            headers += [\n                 (':scheme', self._protocol.metadata['uri'].scheme),\n                 (':path', path),\n             ]\n\n@@ -786,7 +786,10 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'http://example.com')\n+            self.assertTrue(\n+                response.body == b'http://example.com'  # HTTP/1.x\n+                or response.body == b'/'  # HTTP/2\n+            )\n \n         http_proxy = self.getURL('')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n@@ -796,10 +799,13 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'https://example.com')\n+            self.assertTrue(\n+                response.body == b'http://example.com'  # HTTP/1.x\n+                or response.body == b'/'  # HTTP/2\n+            )\n \n         http_proxy = '%s?noconnect' % self.getURL('')\n-        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        request = Request('http://example.com', meta={'proxy': http_proxy})\n         with self.assertWarnsRegex(ScrapyDeprecationWarning,\n                                    r'Using HTTPS proxies in the noconnect mode is deprecated'):\n             return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -836,10 +842,30 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n         self.assertIn(domain, timeout.osError)\n \n \n-# TODO:\n-class Http2ProxyTestCase(Http11ProxyTestCase):\n+class Https2ProxyTestCase(Http11ProxyTestCase):\n+    # only used for HTTPS tests\n+    keyfile = 'keys/localhost.key'\n+    certfile = 'keys/localhost.crt'\n+\n+    scheme = 'https'\n+    host = u'127.0.0.1'\n+\n     download_handler_cls = H2DownloadHandler\n \n+    def setUp(self):\n+        site = server.Site(UriResource(), timeout=None)\n+        self.port = reactor.listenSSL(\n+            0, site,\n+            ssl_context_factory(self.keyfile, self.certfile),\n+            interface=self.host\n+        )\n+        self.portno = self.port.getHost().port\n+        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n+        self.download_request = self.download_handler.download_request\n+\n+    def getURL(self, path):\n+        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n+\n \n class HttpDownloadHandlerMock:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c67d6dea318d6f0915ae86d46ce367b6c1e8ee51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 36 | Files Changed: 4 | Hunks: 18 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 75 | Churn Cumulative: 3811 | Contributors (this commit): 24 | Commits (past 90d): 49 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,13 @@\n-from OpenSSL import SSL\n import warnings\n \n+from OpenSSL import SSL\n from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust, AcceptableCiphers\n from twisted.web.client import BrowserLikePolicyForHTTPS\n from twisted.web.iweb import IPolicyForHTTPS\n from zope.interface.declarations import implementer\n-from scrapy.core.downloader.tls import openssl_methods\n-from scrapy.utils.misc import create_instance, load_object\n \n-from scrapy.core.downloader.tls import ScrapyClientTLSOptions, DEFAULT_CIPHERS\n+from scrapy.core.downloader.tls import DEFAULT_CIPHERS, openssl_methods, ScrapyClientTLSOptions\n+from scrapy.utils.misc import create_instance, load_object\n \n \n @implementer(IPolicyForHTTPS)\n\n@@ -6,15 +6,15 @@ from urllib.parse import urldefrag\n from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n-from twisted.web.client import URI, BrowserLikePolicyForHTTPS\n+from twisted.web.client import BrowserLikePolicyForHTTPS, URI\n \n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n from scrapy.core.http2.agent import H2Agent, H2ConnectionPool\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n+from scrapy.utils.python import to_bytes\n \n \n class H2DownloadHandler:\n@@ -88,25 +88,21 @@ class ScrapyH2Agent:\n         if proxy:\n             _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n             scheme = _parse(request.url)[0]\n-            proxy_host = str(proxy_host, 'utf-8')\n-            omit_connect_timeout = b'noconnect' in proxy_params\n-            if omit_connect_timeout:\n-                warnings.warn(\"Using HTTPS proxies in the noconnect mode is deprecated. \"\n-                              \"If you use Crawlera, it doesn't require this mode anymore, \"\n-                              \"so you should update scrapy-crawlera to 1.3.0+ \"\n-                              \"and remove '?noconnect' from the Crawlera URL.\",\n-                              ScrapyDeprecationWarning)\n+            proxy_host = proxy_host.decode()\n+            omit_connect_tunnel = b'noconnect' in proxy_params\n+            if omit_connect_tunnel:\n+                warnings.warn(\"Using HTTPS proxies in the noconnect mode is not supported by the \"\n+                              \"downloader handler. If you use Crawlera, it doesn't require this \"\n+                              \"mode anymore, so you should update scrapy-crawlera to 1.3.0+ \"\n+                              \"and remove '?noconnect' from the Crawlera URL.\")\n \n-            if scheme == b'https' and not omit_connect_timeout:\n-                proxy_auth = request.headers.get(b'Proxy-Authorization', None)\n-                proxy_conf = (proxy_host, proxy_port, proxy_auth)\n-\n-                # TODO: Return TunnelingAgent instance\n-            else:\n+            if scheme == b'https' and not omit_connect_tunnel:\n+                # ToDo\n+                raise NotImplementedError('Tunneling via CONNECT method using HTTP/2.0 is not yet supported')\n             return self._ProxyAgent(\n                 reactor=reactor,\n                 context_factory=self._context_factory,\n-                    proxy_uri=URI.fromBytes(bytes(proxy, encoding='ascii')),\n+                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding='ascii')),\n                 connect_timeout=timeout,\n                 bind_address=bind_address,\n                 pool=self._pool\n\n@@ -192,10 +192,7 @@ class Stream:\n         # a path component; these MUST include a \":path\" pseudo-header field\n         # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)\n         if not path:\n-            if self._request.method == 'OPTIONS':\n-                path = path or '*'\n-            else:\n-                path = path or '/'\n+            path = '*' if self._request.method == 'OPTIONS' else '/'\n \n         # Make sure pseudo-headers comes before all the other headers\n         headers = [\n\n@@ -315,8 +315,8 @@ class HttpTestCase(unittest.TestCase):\n         host = self.host + ':' + str(self.portno)\n \n         def _test(response):\n-            self.assertEqual(response.body, bytes(host, 'utf-8'))\n-            self.assertEqual(request.headers.get('Host'), bytes(host, 'utf-8'))\n+            self.assertEqual(response.body, to_bytes(host))\n+            self.assertEqual(request.headers.get('Host'), to_bytes(host))\n \n         request = Request(self.getURL('host'), headers={'Host': host})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -764,6 +764,7 @@ class UriResource(resource.Resource):\n \n class HttpProxyTestCase(unittest.TestCase):\n     download_handler_cls = HTTPDownloadHandler\n+    expected_http_proxy_request_body = b'http://example.com'\n \n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n@@ -786,10 +787,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.url, request.url)\n-            self.assertTrue(\n-                response.body == b'http://example.com'  # HTTP/1.x\n-                or response.body == b'/'  # HTTP/2\n-            )\n+            self.assertEqual(response.body, self.expected_http_proxy_request_body)\n \n         http_proxy = self.getURL('')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n@@ -799,13 +797,10 @@ class HttpProxyTestCase(unittest.TestCase):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.url, request.url)\n-            self.assertTrue(\n-                response.body == b'http://example.com'  # HTTP/1.x\n-                or response.body == b'/'  # HTTP/2\n-            )\n+            self.assertEqual(response.body, b'https://example.com')\n \n         http_proxy = '%s?noconnect' % self.getURL('')\n-        request = Request('http://example.com', meta={'proxy': http_proxy})\n+        request = Request('https://example.com', meta={'proxy': http_proxy})\n         with self.assertWarnsRegex(ScrapyDeprecationWarning,\n                                    r'Using HTTPS proxies in the noconnect mode is deprecated'):\n             return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -851,6 +846,7 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n     host = u'127.0.0.1'\n \n     download_handler_cls = H2DownloadHandler\n+    expected_http_proxy_request_body = b'/'\n \n     def setUp(self):\n         site = server.Site(UriResource(), timeout=None)\n@@ -866,6 +862,21 @@ class Https2ProxyTestCase(Http11ProxyTestCase):\n     def getURL(self, path):\n         return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n \n+    def test_download_with_proxy_https_noconnect(self):\n+        def _test(response):\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'/')\n+\n+        http_proxy = '%s?noconnect' % self.getURL('')\n+        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        with self.assertWarnsRegex(\n+            Warning,\n+            r'Using HTTPS proxies in the noconnect mode is not supported by the '\n+            r'downloader handler.'\n+        ):\n+            return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n \n class HttpDownloadHandlerMock:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c4b4cc6b046e121be33a57031a0900ca1347e16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 209 | Lines Deleted: 26 | Files Changed: 5 | Hunks: 31 | Methods Changed: 52 | Complexity Δ (Sum/Max): 31/20 | Churn Δ: 235 | Churn Cumulative: 2523 | Contributors (this commit): 30 | Commits (past 90d): 11 | Contributors (cumulative): 72 | DMM Complexity: 1.0\n\nDIFF:\n@@ -409,7 +409,7 @@ class FilesPipeline(MediaPipeline):\n         store_cls = self.STORE_SCHEMES[scheme]\n         return store_cls(uri)\n \n-    def media_to_download(self, request, info):\n+    def media_to_download(self, request, info, *, item=None):\n         def _onsuccess(result):\n             if not result:\n                 return  # returning None force download\n@@ -436,7 +436,7 @@ class FilesPipeline(MediaPipeline):\n             checksum = result.get('checksum', None)\n             return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}\n \n-        path = self.file_path(request, info=info)\n+        path = self.file_path(request, info=info, item=item)\n         dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n         dfd.addCallbacks(_onsuccess, lambda _: None)\n         dfd.addErrback(\n@@ -460,7 +460,7 @@ class FilesPipeline(MediaPipeline):\n \n         raise FileException\n \n-    def media_downloaded(self, response, request, info):\n+    def media_downloaded(self, response, request, info, *, item=None):\n         referer = referer_str(request)\n \n         if response.status != 200:\n@@ -492,8 +492,8 @@ class FilesPipeline(MediaPipeline):\n         self.inc_stats(info.spider, status)\n \n         try:\n-            path = self.file_path(request, response=response, info=info)\n-            checksum = self.file_downloaded(response, request, info)\n+            path = self.file_path(request, response=response, info=info, item=item)\n+            checksum = self.file_downloaded(response, request, info, item=item)\n         except FileException as exc:\n             logger.warning(\n                 'File (error): Error processing file from %(request)s '\n@@ -522,8 +522,8 @@ class FilesPipeline(MediaPipeline):\n         urls = ItemAdapter(item).get(self.files_urls_field, [])\n         return [Request(u) for u in urls]\n \n-    def file_downloaded(self, response, request, info):\n-        path = self.file_path(request, response=response, info=info)\n+    def file_downloaded(self, response, request, info, *, item=None):\n+        path = self.file_path(request, response=response, info=info, item=item)\n         buf = BytesIO(response.body)\n         checksum = md5sum(buf)\n         buf.seek(0)\n@@ -535,7 +535,7 @@ class FilesPipeline(MediaPipeline):\n             ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n         return item\n \n-    def file_path(self, request, response=None, info=None):\n+    def file_path(self, request, response=None, info=None, *, item=None):\n         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         media_ext = os.path.splitext(request.url)[1]\n         # Handles empty and wild extensions by trying to guess the\n\n@@ -103,12 +103,12 @@ class ImagesPipeline(FilesPipeline):\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n \n-    def file_downloaded(self, response, request, info):\n-        return self.image_downloaded(response, request, info)\n+    def file_downloaded(self, response, request, info, *, item=None):\n+        return self.image_downloaded(response, request, info, item=item)\n \n-    def image_downloaded(self, response, request, info):\n+    def image_downloaded(self, response, request, info, *, item=None):\n         checksum = None\n-        for path, image, buf in self.get_images(response, request, info):\n+        for path, image, buf in self.get_images(response, request, info, item=item):\n             if checksum is None:\n                 buf.seek(0)\n                 checksum = md5sum(buf)\n@@ -119,8 +119,8 @@ class ImagesPipeline(FilesPipeline):\n                 headers={'Content-Type': 'image/jpeg'})\n         return checksum\n \n-    def get_images(self, response, request, info):\n-        path = self.file_path(request, response=response, info=info)\n+    def get_images(self, response, request, info, *, item=None):\n+        path = self.file_path(request, response=response, info=info, item=item)\n         orig_image = Image.open(BytesIO(response.body))\n \n         width, height = orig_image.size\n@@ -166,7 +166,7 @@ class ImagesPipeline(FilesPipeline):\n             ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n         return item\n \n-    def file_path(self, request, response=None, info=None):\n+    def file_path(self, request, response=None, info=None, *, item=None):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return 'full/%s.jpg' % (image_guid)\n \n\n@@ -1,12 +1,16 @@\n import functools\n import logging\n from collections import defaultdict\n+from inspect import signature\n+from warnings import warn\n+\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import mustbe_deferred, defer_result\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.log import failure_to_exc_info\n@@ -27,6 +31,7 @@ class MediaPipeline:\n \n     def __init__(self, download_func=None, settings=None):\n         self.download_func = download_func\n+        self._expects_item = {}\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n@@ -38,6 +43,9 @@ class MediaPipeline:\n         )\n         self._handle_statuses(self.allow_redirects)\n \n+        # Check if deprecated methods are being used and make them compatible\n+        self._make_compatible()\n+\n     def _handle_statuses(self, allow_redirects):\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n@@ -77,11 +85,11 @@ class MediaPipeline:\n     def process_item(self, item, spider):\n         info = self.spiderinfo\n         requests = arg_to_iter(self.get_media_requests(item, info))\n-        dlist = [self._process_request(r, info) for r in requests]\n+        dlist = [self._process_request(r, info, item) for r in requests]\n         dfd = DeferredList(dlist, consumeErrors=1)\n         return dfd.addCallback(self.item_completed, item, info)\n \n-    def _process_request(self, request, info):\n+    def _process_request(self, request, info, item):\n         fp = request_fingerprint(request)\n         cb = request.callback or (lambda _: _)\n         eb = request.errback\n@@ -102,34 +110,73 @@ class MediaPipeline:\n \n         # Download request checking media_to_download hook output first\n         info.downloading.add(fp)\n-        dfd = mustbe_deferred(self.media_to_download, request, info)\n-        dfd.addCallback(self._check_media_to_download, request, info)\n+        dfd = mustbe_deferred(self.media_to_download, request, info, item=item)\n+        dfd.addCallback(self._check_media_to_download, request, info, item=item)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n             f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n+    def _make_compatible(self):\n+        \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n+        methods = [\n+            \"file_path\", \"media_to_download\", \"media_downloaded\",\n+            \"file_downloaded\", \"image_downloaded\", \"get_images\"\n+        ]\n+\n+        for method_name in methods:\n+            method = getattr(self, method_name, None)\n+            if callable(method):\n+                setattr(self, method_name, self._compatible(method))\n+\n+    def _compatible(self, func):\n+        \"\"\"Wrapper for overridable methods to allow backwards compatibility\"\"\"\n+        self._check_signature(func)\n+\n+        @functools.wraps(func)\n+        def wrapper(*args, **kwargs):\n+            if self._expects_item[func.__name__]:\n+                return func(*args, **kwargs)\n+\n+            kwargs.pop('item', None)\n+            return func(*args, **kwargs)\n+\n+        return wrapper\n+\n+    def _check_signature(self, func):\n+        sig = signature(func)\n+        self._expects_item[func.__name__] = True\n+\n+        if 'item' not in sig.parameters:\n+            old_params = str(sig)[1:-1]\n+            new_params = old_params + \", *, item=None\"\n+            warn('%s(self, %s) is deprecated, '\n+                 'please use %s(self, %s)'\n+                 % (func.__name__, old_params, func.__name__, new_params),\n+                 ScrapyDeprecationWarning, stacklevel=2)\n+            self._expects_item[func.__name__] = False\n+\n     def _modify_media_request(self, request):\n         if self.handle_httpstatus_list:\n             request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n         else:\n             request.meta['handle_httpstatus_all'] = True\n \n-    def _check_media_to_download(self, result, request, info):\n+    def _check_media_to_download(self, result, request, info, item):\n         if result is not None:\n             return result\n         if self.download_func:\n             # this ugly code was left only to support tests. TODO: remove\n             dfd = mustbe_deferred(self.download_func, request, info.spider)\n             dfd.addCallbacks(\n-                callback=self.media_downloaded, callbackArgs=(request, info),\n+                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                 errback=self.media_failed, errbackArgs=(request, info))\n         else:\n             self._modify_media_request(request)\n             dfd = self.crawler.engine.download(request, info.spider)\n             dfd.addCallbacks(\n-                callback=self.media_downloaded, callbackArgs=(request, info),\n+                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                 errback=self.media_failed, errbackArgs=(request, info))\n         return dfd\n \n@@ -171,7 +218,7 @@ class MediaPipeline:\n             defer_result(result).chainDeferred(wad)\n \n     # Overridable Interface\n-    def media_to_download(self, request, info):\n+    def media_to_download(self, request, info, *, item=None):\n         \"\"\"Check request before starting download\"\"\"\n         pass\n \n@@ -179,7 +226,7 @@ class MediaPipeline:\n         \"\"\"Returns the media requests to download\"\"\"\n         pass\n \n-    def media_downloaded(self, response, request, info):\n+    def media_downloaded(self, response, request, info, *, item=None):\n         \"\"\"Handler for success downloads\"\"\"\n         return response\n \n@@ -199,3 +246,7 @@ class MediaPipeline:\n                         extra={'spider': info.spider}\n                     )\n         return item\n+\n+    def file_path(self, request, response=None, info=None, *, item=None):\n+        \"\"\"Returns the path where downloaded media should be stored\"\"\"\n+        pass\n\n@@ -161,6 +161,19 @@ class FilesPipelineTestCase(unittest.TestCase):\n         for p in patchers:\n             p.stop()\n \n+    def test_file_path_from_item(self):\n+        \"\"\"\n+        Custom file path based on item data, overriding default implementation\n+        \"\"\"\n+        class CustomFilesPipeline(FilesPipeline):\n+            def file_path(self, request, response=None, info=None, item=None):\n+                return 'full/%s' % item.get('path')\n+\n+        file_path = CustomFilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir})).file_path\n+        item = dict(path='path-to-store-file')\n+        request = Request(\"http://example.com\")\n+        self.assertEqual(file_path(request, item=item), 'full/path-to-store-file')\n+\n \n class FilesPipelineTestCaseFieldsMixin:\n \n\n@@ -7,7 +7,9 @@ from twisted.internet.defer import Deferred, inlineCallbacks\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.request import request_fingerprint\n+from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.pipelines.files import FileException\n from scrapy.utils.log import failure_to_exc_info\n@@ -169,7 +171,7 @@ class MockedMediaPipeline(MediaPipeline):\n         self._mockcalled.append('download')\n         return super().download(request, info)\n \n-    def media_to_download(self, request, info):\n+    def media_to_download(self, request, info, *, item=None):\n         self._mockcalled.append('media_to_download')\n         if 'result' in request.meta:\n             return request.meta.get('result')\n@@ -179,7 +181,7 @@ class MockedMediaPipeline(MediaPipeline):\n         self._mockcalled.append('get_media_requests')\n         return item.get('requests')\n \n-    def media_downloaded(self, response, request, info):\n+    def media_downloaded(self, response, request, info, *, item=None):\n         self._mockcalled.append('media_downloaded')\n         return super().media_downloaded(response, request, info)\n \n@@ -335,6 +337,123 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n             ['get_media_requests', 'media_to_download', 'item_completed'])\n \n \n+class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n+\n+    def __init__(self, *args, **kwargs):\n+        super(MockedMediaPipelineDeprecatedMethods, self).__init__(*args, **kwargs)\n+        self._mockcalled = []\n+\n+    def get_media_requests(self, item, info):\n+        item_url = item['image_urls'][0]\n+        return Request(\n+            item_url,\n+            meta={'response': Response(item_url, status=200, body=b'data')}\n+        )\n+\n+    def inc_stats(self, *args, **kwargs):\n+        return True\n+\n+    def media_to_download(self, request, info):\n+        self._mockcalled.append('media_to_download')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).media_to_download(request, info)\n+\n+    def media_downloaded(self, response, request, info):\n+        self._mockcalled.append('media_downloaded')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).media_downloaded(response, request, info)\n+\n+    def file_downloaded(self, response, request, info):\n+        self._mockcalled.append('file_downloaded')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).file_downloaded(response, request, info)\n+\n+    def file_path(self, request, response=None, info=None):\n+        self._mockcalled.append('file_path')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).file_path(request, response, info)\n+\n+    def get_images(self, response, request, info):\n+        self._mockcalled.append('get_images')\n+        return []\n+\n+    def image_downloaded(self, response, request, info):\n+        self._mockcalled.append('image_downloaded')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).image_downloaded(response, request, info)\n+\n+\n+class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.pipe = MockedMediaPipelineDeprecatedMethods(store_uri='store-uri', download_func=_mocked_download_func)\n+        self.pipe.open_spider(None)\n+        self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])\n+\n+    def _assert_method_called_with_warnings(self, method, message, warnings):\n+        self.assertIn(method, self.pipe._mockcalled)\n+        warningShown = False\n+        for warning in warnings:\n+            if warning['message'] == message and warning['category'] == ScrapyDeprecationWarning:\n+                warningShown = True\n+        self.assertTrue(warningShown)\n+\n+    @inlineCallbacks\n+    def test_media_to_download_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'media_to_download(self, request, info) is deprecated, '\n+            'please use media_to_download(self, request, info, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('media_to_download', message, warnings)\n+\n+    @inlineCallbacks\n+    def test_media_downloaded_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'media_downloaded(self, response, request, info) is deprecated, '\n+            'please use media_downloaded(self, response, request, info, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('media_downloaded', message, warnings)\n+\n+    @inlineCallbacks\n+    def test_file_downloaded_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'file_downloaded(self, response, request, info) is deprecated, '\n+            'please use file_downloaded(self, response, request, info, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('file_downloaded', message, warnings)\n+\n+    @inlineCallbacks\n+    def test_file_path_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'file_path(self, request, response=None, info=None) is deprecated, '\n+            'please use file_path(self, request, response=None, info=None, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('file_path', message, warnings)\n+\n+    @inlineCallbacks\n+    def test_get_images_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'get_images(self, response, request, info) is deprecated, '\n+            'please use get_images(self, response, request, info, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('get_images', message, warnings)\n+\n+    @inlineCallbacks\n+    def test_image_downloaded_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'image_downloaded(self, response, request, info) is deprecated, '\n+            'please use image_downloaded(self, response, request, info, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('image_downloaded', message, warnings)\n+\n+\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n \n     def _assert_request_no3xx(self, pipeline_class, settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#125a058340cf77a70fe605b0317b3c517f637c81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 63 | Churn Cumulative: 1121 | Contributors (this commit): 18 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: 0.9767441860465116\n\nDIFF:\n@@ -12,10 +12,12 @@ def render_templatefile(path, **kwargs):\n     content = string.Template(raw).substitute(**kwargs)\n \n     render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n+\n+    if path.endswith('.tmpl'):\n+        os.rename(path, render_path)\n+\n     with open(render_path, 'wb') as fp:\n         fp.write(content.encode('utf8'))\n-    if path.endswith('.tmpl'):\n-        os.remove(path)\n \n \n CAMELCASE_INVALID_CHARS = re.compile(r'[^a-zA-Z\\d]')\n\n@@ -126,9 +126,13 @@ class StartprojectTest(ProjectTest):\n \n \n def get_permissions_dict(path, renamings=None, ignore=None):\n+\n+    def get_permissions(path):\n+        return oct(os.stat(path).st_mode)\n+\n     renamings = renamings or tuple()\n     permissions_dict = {\n-        '.': os.stat(path).st_mode,\n+        '.': get_permissions(path),\n     }\n     for root, dirs, files in os.walk(path):\n         nodes = list(chain(dirs, files))\n@@ -143,13 +147,15 @@ def get_permissions_dict(path, renamings=None, ignore=None):\n                     search_string,\n                     replacement\n                 )\n-            permissions = os.stat(absolute_path).st_mode\n+            permissions = get_permissions(absolute_path)\n             permissions_dict[relative_path] = permissions\n     return permissions_dict\n \n \n class StartprojectTemplatesTest(ProjectTest):\n \n+    maxDiff = None\n+\n     def setUp(self):\n         super().setUp()\n         self.tmpl = join(self.temp_path, 'templates')\n@@ -311,6 +317,53 @@ class StartprojectTemplatesTest(ProjectTest):\n \n         self.assertEqual(actual_permissions, expected_permissions)\n \n+    def test_startproject_permissions_umask_022(self):\n+        \"\"\"Check that generated files have the right permissions when the\n+        system uses a umask value that causes new files to have different\n+        permissions than those from the template folder.\"\"\"\n+        @contextmanager\n+        def umask(new_mask):\n+            cur_mask = os.umask(new_mask)\n+            yield\n+            os.umask(cur_mask)\n+\n+        scrapy_path = scrapy.__path__[0]\n+        project_template = os.path.join(\n+            scrapy_path,\n+            'templates',\n+            'project'\n+        )\n+        project_name = 'umaskproject'\n+        renamings = (\n+            ('module', project_name),\n+            ('.tmpl', ''),\n+        )\n+        expected_permissions = get_permissions_dict(\n+            project_template,\n+            renamings,\n+            IGNORE,\n+        )\n+\n+        with umask(0o002):\n+            destination = mkdtemp()\n+            process = subprocess.Popen(\n+                (\n+                    sys.executable,\n+                    '-m',\n+                    'scrapy.cmdline',\n+                    'startproject',\n+                    project_name,\n+                ),\n+                cwd=destination,\n+                env=self.env,\n+            )\n+            process.wait()\n+\n+            project_dir = os.path.join(destination, project_name)\n+            actual_permissions = get_permissions_dict(project_dir)\n+\n+            self.assertEqual(actual_permissions, expected_permissions)\n+\n \n class CommandTest(ProjectTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c0afb606c59e6c8eabe4bf97cdb0494cf26dec2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1067 | Contributors (this commit): 16 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -297,7 +297,7 @@ class StartprojectTemplatesTest(ProjectTest):\n                 path.mkdir(mode=permissions)\n             else:\n                 path.touch(mode=permissions)\n-            expected_permissions[node] = path.stat().st_mode\n+            expected_permissions[node] = oct(path.stat().st_mode)\n \n         process = subprocess.Popen(\n             (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f4df622a17b1d8f9c3b9b693cb2f7119ffec27f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 578 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -28,5 +28,5 @@ tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n def get_testdata(*paths):\n     \"\"\"Return test data\"\"\"\n     path = os.path.join(tests_datadir, *paths)\n-    with open(path, 'rb') as f:\n+    with open(path, 'rb', newline='') as f:\n         return f.read()\n\n@@ -7,9 +7,6 @@ from scrapy.http import XmlResponse, TextResponse, Response\n from tests import get_testdata\n \n \n-FOOBAR_NL = \"foo{}bar\".format(os.linesep)\n-\n-\n class XmliterTestCase(unittest.TestCase):\n \n     xmliter = staticmethod(xmliter)\n@@ -267,7 +264,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual(result,\n                          [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n                           {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': FOOBAR_NL},\n+                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n                           {'id': '4', 'name': 'empty', 'value': ''}])\n \n         # explicit type check cuz' we no like stinkin' autocasting! yarrr\n@@ -283,7 +280,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv],\n                          [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n                           {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': FOOBAR_NL},\n+                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n                           {'id': '4', 'name': 'empty', 'value': ''}])\n \n     def test_csviter_quotechar(self):\n@@ -296,7 +293,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv1],\n                          [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n                           {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': FOOBAR_NL},\n+                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n                           {'id': '4', 'name': 'empty', 'value': ''}])\n \n         response2 = TextResponse(url=\"http://example.com/\", body=body2)\n@@ -305,7 +302,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv2],\n                          [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n                           {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': FOOBAR_NL},\n+                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n                           {'id': '4', 'name': 'empty', 'value': ''}])\n \n     def test_csviter_wrong_quotechar(self):\n@@ -327,7 +324,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv],\n                          [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n                           {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': FOOBAR_NL},\n+                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n                           {'id': '4', 'name': 'empty', 'value': ''}])\n \n     def test_csviter_headers(self):\n@@ -353,7 +350,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv],\n                          [{'id': '1', 'name': 'alpha', 'value': 'foobar'},\n                           {'id': '2', 'name': 'unicode', 'value': '\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {'id': '3', 'name': 'multi', 'value': FOOBAR_NL},\n+                          {'id': '3', 'name': 'multi', 'value': \"foo\\nbar\"},\n                           {'id': '4', 'name': 'empty', 'value': ''}])\n \n     def test_csviter_exception(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24ba5a71aca9b368eee21bd7d9043a7d26dba403", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 102 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -28,5 +28,5 @@ tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n def get_testdata(*paths):\n     \"\"\"Return test data\"\"\"\n     path = os.path.join(tests_datadir, *paths)\n-    with open(path, 'rb', newline='') as f:\n+    with open(path, 'rb') as f:\n         return f.read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af73f141b23aabbef208a12fe95ae63c27105fda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 160 | Lines Deleted: 139 | Files Changed: 2 | Hunks: 10 | Methods Changed: 27 | Complexity Δ (Sum/Max): 1/16 | Churn Δ: 299 | Churn Cumulative: 2085 | Contributors (this commit): 22 | Commits (past 90d): 10 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,158 @@\n+from unittest import mock\n+\n+from twisted.internet import defer, error, reactor\n+from twisted.trial import unittest\n+from twisted.web import server\n+\n+from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n+from scrapy.http import Request\n+from scrapy.spiders import Spider\n+from scrapy.utils.misc import create_instance\n+from scrapy.utils.test import get_crawler\n+from tests.mockserver import ssl_context_factory\n+from tests.test_downloader_handlers import (\n+    Https11TestCase, Https11CustomCiphers,\n+    Http11MockServerTestCase, Http11ProxyTestCase,\n+    UriResource\n+)\n+\n+\n+class Https2TestCase(Https11TestCase):\n+    scheme = 'https'\n+    download_handler_cls = H2DownloadHandler\n+    HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_maxsize_very_large_file(self):\n+        with mock.patch('scrapy.core.http2.stream.logger') as logger:\n+            request = Request(self.getURL('largechunkedfile'))\n+\n+            def check(logger):\n+                logger.error.assert_called_once_with(mock.ANY)\n+\n+            d = self.download_request(request, Spider('foo', download_maxsize=1500))\n+            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n+\n+            # As the error message is logged in the dataReceived callback, we\n+            # have to give a bit of time to the reactor to process the queue\n+            # after closing the connection.\n+            d = defer.Deferred()\n+            d.addCallback(check)\n+            reactor.callLater(.1, d.callback, logger)\n+            yield d\n+\n+    def test_download_broken_content_cause_data_loss(self, url='broken'):\n+        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n+\n+    def test_download_broken_chunked_content_cause_data_loss(self):\n+        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n+\n+    def test_download_broken_content_allow_data_loss(self, url='broken'):\n+        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n+\n+    def test_download_broken_chunked_content_allow_data_loss(self):\n+        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n+\n+    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n+        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n+\n+    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n+        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n+\n+\n+class Https2WrongHostnameTestCase(Https2TestCase):\n+    tls_log_message = (\n+        'SSL connection certificate: issuer \"/C=XW/ST=XW/L=The '\n+        'Internet/O=Scrapy/CN=www.example.com/emailAddress=test@example.com\", '\n+        'subject \"/C=XW/ST=XW/L=The '\n+        'Internet/O=Scrapy/CN=www.example.com/emailAddress=test@example.com\"'\n+    )\n+\n+    # above tests use a server certificate for \"localhost\",\n+    # client connection to \"localhost\" too.\n+    # here we test that even if the server certificate is for another domain,\n+    # \"www.example.com\" in this case,\n+    # the tests still pass\n+    keyfile = 'keys/example-com.key.pem'\n+    certfile = 'keys/example-com.cert.pem'\n+\n+\n+class Https2InvalidDNSId(Https2TestCase):\n+    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n+\n+    def setUp(self):\n+        super(Https2InvalidDNSId, self).setUp()\n+        self.host = '127.0.0.1'\n+\n+\n+class Https2InvalidDNSPattern(Https2TestCase):\n+    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n+\n+    keyfile = 'keys/localhost.ip.key'\n+    certfile = 'keys/localhost.ip.crt'\n+\n+    def setUp(self):\n+        try:\n+            from service_identity.exceptions import CertificateError  # noqa: F401\n+        except ImportError:\n+            raise unittest.SkipTest(\"cryptography lib is too old\")\n+        self.tls_log_message = (\n+            'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", '\n+            'subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n+        )\n+        super(Https2InvalidDNSPattern, self).setUp()\n+\n+\n+class Https2CustomCiphers(Https11CustomCiphers):\n+    scheme = 'https'\n+    download_handler_cls = H2DownloadHandler\n+\n+\n+class Http2MockServerTestCase(Http11MockServerTestCase):\n+    \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n+\n+\n+class Https2ProxyTestCase(Http11ProxyTestCase):\n+    # only used for HTTPS tests\n+    keyfile = 'keys/localhost.key'\n+    certfile = 'keys/localhost.crt'\n+\n+    scheme = 'https'\n+    host = u'127.0.0.1'\n+\n+    download_handler_cls = H2DownloadHandler\n+    expected_http_proxy_request_body = b'/'\n+\n+    def setUp(self):\n+        site = server.Site(UriResource(), timeout=None)\n+        self.port = reactor.listenSSL(\n+            0, site,\n+            ssl_context_factory(self.keyfile, self.certfile),\n+            interface=self.host\n+        )\n+        self.portno = self.port.getHost().port\n+        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n+        self.download_request = self.download_handler.download_request\n+\n+    def getURL(self, path):\n+        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n+\n+    def test_download_with_proxy_https_noconnect(self):\n+        def _test(response):\n+            self.assertEqual(response.status, 200)\n+            self.assertEqual(response.url, request.url)\n+            self.assertEqual(response.body, b'/')\n+\n+        http_proxy = '%s?noconnect' % self.getURL('')\n+        request = Request('https://example.com', meta={'proxy': http_proxy})\n+        with self.assertWarnsRegex(\n+            Warning,\n+            r'Using HTTPS proxies in the noconnect mode is not supported by the '\n+            r'downloader handler.'\n+        ):\n+            return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n+    @defer.inlineCallbacks\n+    def test_download_with_proxy_https_timeout(self):\n+        with self.assertRaises(NotImplementedError):\n+            yield super(Https2ProxyTestCase, self).test_download_with_proxy_https_timeout()\n\n@@ -23,7 +23,6 @@ from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n-from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Headers, Request\n@@ -315,8 +314,8 @@ class HttpTestCase(unittest.TestCase):\n         host = self.host + ':' + str(self.portno)\n \n         def _test(response):\n-            self.assertEqual(response.body, to_bytes(host))\n-            self.assertEqual(request.headers.get('Host'), to_bytes(host))\n+            self.assertEqual(response.body, host.encode())\n+            self.assertEqual(request.headers.get('Host'), host.encode())\n \n         request = Request(self.getURL('host'), headers={'Host': host})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n@@ -522,49 +521,6 @@ class Https11TestCase(Http11TestCase):\n             yield download_handler.close()\n \n \n-class Https2TestCase(Https11TestCase):\n-    scheme = 'https'\n-    download_handler_cls = H2DownloadHandler\n-    HTTP2_DATALOSS_SKIP_REASON = \"Content-Length mismatch raises InvalidBodyLengthError\"\n-\n-    @defer.inlineCallbacks\n-    def test_download_with_maxsize_very_large_file(self):\n-        with mock.patch('scrapy.core.http2.stream.logger') as logger:\n-            request = Request(self.getURL('largechunkedfile'))\n-\n-            def check(logger):\n-                logger.error.assert_called_once_with(mock.ANY)\n-\n-            d = self.download_request(request, Spider('foo', download_maxsize=1500))\n-            yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)\n-\n-            # As the error message is logged in the dataReceived callback, we\n-            # have to give a bit of time to the reactor to process the queue\n-            # after closing the connection.\n-            d = defer.Deferred()\n-            d.addCallback(check)\n-            reactor.callLater(.1, d.callback, logger)\n-            yield d\n-\n-    def test_download_broken_content_cause_data_loss(self, url='broken'):\n-        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n-\n-    def test_download_broken_chunked_content_cause_data_loss(self):\n-        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n-\n-    def test_download_broken_content_allow_data_loss(self, url='broken'):\n-        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n-\n-    def test_download_broken_chunked_content_allow_data_loss(self):\n-        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n-\n-    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n-        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n-\n-    def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n-        raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n-\n-\n class Https11WrongHostnameTestCase(Http11TestCase):\n     scheme = 'https'\n \n@@ -577,23 +533,6 @@ class Https11WrongHostnameTestCase(Http11TestCase):\n     certfile = 'keys/example-com.cert.pem'\n \n \n-class Https2WrongHostnameTestCase(Https2TestCase):\n-    tls_log_message = (\n-        'SSL connection certificate: issuer \"/C=XW/ST=XW/L=The '\n-        'Internet/O=Scrapy/CN=www.example.com/emailAddress=test@example.com\", '\n-        'subject \"/C=XW/ST=XW/L=The '\n-        'Internet/O=Scrapy/CN=www.example.com/emailAddress=test@example.com\"'\n-    )\n-\n-    # above tests use a server certificate for \"localhost\",\n-    # client connection to \"localhost\" too.\n-    # here we test that even if the server certificate is for another domain,\n-    # \"www.example.com\" in this case,\n-    # the tests still pass\n-    keyfile = 'keys/example-com.key.pem'\n-    certfile = 'keys/example-com.cert.pem'\n-\n-\n class Https11InvalidDNSId(Https11TestCase):\n     \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n \n@@ -602,14 +541,6 @@ class Https11InvalidDNSId(Https11TestCase):\n         self.host = '127.0.0.1'\n \n \n-class Https2InvalidDNSId(Https2TestCase):\n-    \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n-\n-    def setUp(self):\n-        super(Https2InvalidDNSId, self).setUp()\n-        self.host = '127.0.0.1'\n-\n-\n class Https11InvalidDNSPattern(Https11TestCase):\n     \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n \n@@ -628,24 +559,6 @@ class Https11InvalidDNSPattern(Https11TestCase):\n         super(Https11InvalidDNSPattern, self).setUp()\n \n \n-class Https2InvalidDNSPattern(Https2TestCase):\n-    \"\"\"Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain.\"\"\"\n-\n-    keyfile = 'keys/localhost.ip.key'\n-    certfile = 'keys/localhost.ip.crt'\n-\n-    def setUp(self):\n-        try:\n-            from service_identity.exceptions import CertificateError  # noqa: F401\n-        except ImportError:\n-            raise unittest.SkipTest(\"cryptography lib is too old\")\n-        self.tls_log_message = (\n-            'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", '\n-            'subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n-        )\n-        super(Https2InvalidDNSPattern, self).setUp()\n-\n-\n class Https11CustomCiphers(unittest.TestCase):\n     scheme = 'https'\n     download_handler_cls = HTTP11DownloadHandler\n@@ -686,11 +599,6 @@ class Https11CustomCiphers(unittest.TestCase):\n         return d\n \n \n-class Https2CustomCiphers(Https11CustomCiphers):\n-    scheme = 'https'\n-    download_handler_cls = H2DownloadHandler\n-\n-\n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n \n@@ -742,10 +650,6 @@ class Http11MockServerTestCase(unittest.TestCase):\n         self.assertTrue(reason, 'finished')\n \n \n-class Http2MockServerTestCase(Http11MockServerTestCase):\n-    \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n-\n-\n class UriResource(resource.Resource):\n     \"\"\"Return the full uri that was requested\"\"\"\n \n@@ -837,47 +741,6 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n         self.assertIn(domain, timeout.osError)\n \n \n-class Https2ProxyTestCase(Http11ProxyTestCase):\n-    # only used for HTTPS tests\n-    keyfile = 'keys/localhost.key'\n-    certfile = 'keys/localhost.crt'\n-\n-    scheme = 'https'\n-    host = u'127.0.0.1'\n-\n-    download_handler_cls = H2DownloadHandler\n-    expected_http_proxy_request_body = b'/'\n-\n-    def setUp(self):\n-        site = server.Site(UriResource(), timeout=None)\n-        self.port = reactor.listenSSL(\n-            0, site,\n-            ssl_context_factory(self.keyfile, self.certfile),\n-            interface=self.host\n-        )\n-        self.portno = self.port.getHost().port\n-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())\n-        self.download_request = self.download_handler.download_request\n-\n-    def getURL(self, path):\n-        return f\"{self.scheme}://{self.host}:{self.portno}/{path}\"\n-\n-    def test_download_with_proxy_https_noconnect(self):\n-        def _test(response):\n-            self.assertEqual(response.status, 200)\n-            self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'/')\n-\n-        http_proxy = '%s?noconnect' % self.getURL('')\n-        request = Request('https://example.com', meta={'proxy': http_proxy})\n-        with self.assertWarnsRegex(\n-            Warning,\n-            r'Using HTTPS proxies in the noconnect mode is not supported by the '\n-            r'downloader handler.'\n-        ):\n-            return self.download_request(request, Spider('foo')).addCallback(_test)\n-\n-\n class HttpDownloadHandlerMock:\n \n     def __init__(self, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
