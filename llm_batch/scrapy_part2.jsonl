{"custom_id": "scrapy#8a81fe82c7b7a0d177cc201441f7b139a5139273", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 363 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,7 +34,6 @@ class Node:\n         if not status:\n             self.available = False\n         else:\n-            print \">----------<\"\n             self.available = True\n             self.running = status['running']\n             self.closing = status['closing']\n@@ -43,10 +42,8 @@ class Node:\n             self.timestamp = status['timestamp']\n             self.loadavg = status['loadavg']\n             self.logdir = status['logdir']\n-            print \"Running: %s\" % self.running\n             free_slots = self.maxproc - len(self.running)\n             while free_slots > 0 and self.master.pending:\n-                print \"Free slots %s\" % free_slots\n                 pending = self.master.pending.pop(0)\n                 self.run(pending)\n                 free_slots -= 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f7f0d09945cfb3013f8bbe07f89e16199e63a12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 157 | Files Changed: 5 | Hunks: 7 | Methods Changed: 15 | Complexity Δ (Sum/Max): -27/3 | Churn Δ: 217 | Churn Cumulative: 586 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,30 @@\n+import unittest\n+\n+from scrapy.utils.xml import xpathselector_iternodes\n+from scrapy.http import Response\n+\n+class UtilsXmlTestCase(unittest.TestCase):\n+\n+    def test_iterator(self):\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n+  <product id=\"001\">\n+    <type>Type 1</type>\n+    <name>Name 1</name>\n+  </product>\n+  <product id=\"002\">\n+    <type>Type 2</type>\n+    <name>Name 2</name>\n+  </product>\n+</products>\n+        \"\"\"\n+        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        attrs = []\n+        for x in xpathselector_iternodes(response, 'product'):\n+            attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n+\n+        self.assertEqual(attrs, \n+                         [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n+\n+if __name__ == \"__main__\":\n+    unittest.main()   \n\n@@ -6,8 +6,6 @@ import libxml2\n \n from scrapy.http import Response\n from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n-#from scrapy.xpath.constructors import xmlDoc_from_xml, xmlDoc_from_html\n-from scrapy.xpath.iterator import XMLNodeIterator\n \n class XPathTestCase(unittest.TestCase):\n \n@@ -173,26 +171,5 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(xxs.extract(),\n                          u'<root>lala</root>')\n \n-    def test_iterator(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n-  <product id=\"001\">\n-    <type>Type 1</type>\n-    <name>Name 1</name>\n-  </product>\n-  <product id=\"002\">\n-    <type>Type 2</type>\n-    <name>Name 2</name>\n-  </product>\n-</products>\n-        \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        attrs = []\n-        for x in XMLNodeIterator(response, 'product'):\n-            attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n-\n-        self.assertEqual(attrs, \n-                         [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n-\n if __name__ == \"__main__\":\n     unittest.main()   \n\n@@ -0,0 +1,30 @@\n+import re\n+\n+from scrapy.xpath import XmlXPathSelector\n+from scrapy.http import Response\n+\n+def xpathselector_iternodes(obj, nodename):\n+    \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n+       given tha name of the node to iterate. Useful for parsing XML feeds.\n+\n+    obj can be:\n+    - a Response object\n+    - a unicode string\n+    - a string encoded as utf-8\n+    \"\"\"\n+    \n+    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n+\n+    if isinstance(obj, Response):\n+        text = obj.body.to_string()\n+        enc = obj.body.get_real_encoding()\n+    else:\n+        text = obj\n+        enc = 'utf-8'\n+\n+\n+    r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n+\n+    for match in r.finditer(text):\n+        nodetext = match.group().decode(enc)\n+        yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n\n@@ -7,9 +7,7 @@ spiders, for convenience.\n \n * XPath - a simple class to represent a XPath expression\n * XPathSelector - to extract data using XPaths (parses the entire response)\n-* XMLNodeIterator - to iterate over XML nodes without parsing the entire response in memory\n \"\"\"\n \n from scrapy.xpath.types import XPath\n from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n-from scrapy.xpath.iterator import XMLNodeIterator\n\n@@ -1,132 +0,0 @@\n-import re\n-from xml.sax.saxutils import escape\n-from cStringIO import StringIO\n-\n-import libxml2\n-\n-from scrapy.xpath.constructors import xml_parser_options\n-from scrapy.xpath.selector import XmlXPathSelector\n-\n-class XMLNodeIterator(object):\n-    \"\"\"XMLNodeIterator provides a way to iterate over all nodes of the same\n-    name (passed in the constructor) in a XML Response without parsing the\n-    entire response in memory. The iterator returns XPathSelector objects.\n-\n-    Usage example:\n-\n-    for x in XMLNodeIterator(response, \"product\"):\n-        i = ScrapedItem()\n-        i.assign(\"id\", x.x(\"@id\"))\n-        i.assign(\"name\", x.x(\"./name/text()\")\n-    \"\"\"\n-\n-    def __init__(self, response, node, chunk_size=2048):\n-        self.response = response\n-        self.node = node\n-        self.chunk_size = 2048\n-\n-    def __iter__(self):\n-        sax_parser = XMLNodeSAXParser(self.node, self.response)\n-        contents = self.response.body.to_string()\n-        ctxt = libxml2.createPushParser(sax_parser, '', 0, None)\n-        ctxt.ctxtUseOptions(xml_parser_options)\n-        for i in xrange(0, len(contents), self.chunk_size):\n-            chunk = contents[i:i + self.chunk_size]\n-            ctxt.parseChunk(chunk, len(chunk), 0)\n-            while sax_parser.selectors:\n-                yield sax_parser.selectors.pop(0)\n-        ctxt.parseChunk('', 0, 1)\n-\n-class XMLNodeSAXParser():\n-\n-    xmldeclr_re = re.compile(r'<\\?xml.*?\\?>')\n-\n-    def __init__(self, requested_nodename, response):\n-        self.requested_nodename = requested_nodename\n-        self.inside_requested_node = False\n-        self.buffer = StringIO()\n-        self.xml_declaration = self._extract_xmldecl(response.body.to_string()[0:4096])\n-        self.selectors = []\n-\n-    def startElement(self, name, attributes):\n-        if name == self.requested_nodename:\n-            self.inside_requested_node = True\n-            self.buffer.close()\n-            self.buffer = StringIO()\n-        attributes = attributes or {}\n-        attribute_strings = [\"%s='%s'\" % tuple(ka) for ka in attributes.items()]\n-        self.buffer.write('<' + ' '.join([name] + attribute_strings) + '>')\n-\n-    def endElement(self, name):\n-        self.buffer.write('</%s>' % name)\n-\n-        if name == self.requested_nodename:\n-            self.inside_requested_node = False\n-            string = ''.join([self.xml_declaration, self.buffer.getvalue()])\n-            selector = XmlXPathSelector(text=string).x('/' + self.requested_nodename)[0]\n-            self.selectors.append(selector)\n-\n-    def characters(self, data):\n-        if self.inside_requested_node:\n-            self.buffer.write(escape(data))\n-\n-    def cdataBlock(self, data):\n-        #self.characters('<![CDATA[' + data + ']]>')\n-        if self.inside_requested_node:\n-            self.buffer.write('<![CDATA[' + data + ']]>')\n-\n-    def _extract_xmldecl(self, string):\n-        m = self.xmldeclr_re.search(string)\n-        return m.group() if m else ''\n-\n-\n-# TESTING #\n-from xml.parsers.expat import ParserCreate\n-\n-class expat_XMLNodeIterator():\n-    def __init__(self, response, req_nodename, chunk_size=2048):\n-        self._response = response\n-        self._req_nodename = req_nodename\n-        self._chunk_size = chunk_size\n-\n-        self._byte_offset_buffer = []\n-\n-        self._parser = ParserCreate()\n-        self._parser.StartElementHandler = self._StartElementHandler\n-        self._parser.EndElementHandler = self._EndElementHandler\n-\n-    def _StartElementHandler(self, name, attrs):\n-        if name == self._req_nodename and not self._inside_req_node:\n-            self._start_pos = self._parser.CurrentByteIndex\n-            self._inside_req_node = True\n-\n-    def _EndElementHandler(self, name):\n-        if name == self._req_nodename and self._inside_req_node:\n-            self._byte_offset_buffer.append((self._start_pos, self._parser.CurrentByteIndex))\n-            self._inside_req_node = False\n-\n-    def __iter__(self):\n-        response_body = self._response.body.to_string()\n-        self._inside_req_node = False\n-        for i in xrange(0, len(response_body), self._chunk_size):\n-            self._parser.Parse(response_body[i:i + self._chunk_size])\n-            while self._byte_offset_buffer:\n-                start, end = self._byte_offset_buffer.pop(0)\n-                yield response_body[start:end]\n-        self._parser.Parse('', 1)\n-\n-\n-# TESTING (pablo) #\n-# Yet another node iterator: this one is based entirely on regular expressions,\n-# which means it should be faster but needs some profiling to confirm.\n-\n-class re_XMLNodeIterator():\n-\n-    def __init__(self, response, node):\n-        self.response = response\n-        self.node = node\n-        self.re = re.compile(r\"<%s[\\s>].*?</%s>\" % (node, node), re.DOTALL)\n-\n-    def __iter__(self):\n-        for match in self.re.finditer(self.response.body.to_string()):\n-            yield XmlXPathSelector(text=match.group()).x('/' + self.node)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d993f493b5bead8e3a0dd75adeba0d28beca7c90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 40 | Churn Cumulative: 418 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ class XPathTestCase(unittest.TestCase):\n         xl = xpath.x('//input')\n         self.assertEqual(2, len(xl))\n         for x in xl:\n-            assert isinstance(x, XPathSelector)\n+            assert isinstance(x, HtmlXPathSelector)\n \n         self.assertEqual(xpath.x('//input').extract(),\n                          [x.extract() for x in xpath.x('//input')])\n@@ -41,6 +41,26 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual([x.extract() for x in xpath.x(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n                          [u'12'])\n \n+    def test_selector_same_type(self):\n+        \"\"\"Test XPathSelector returning the same type in x() method\"\"\"\n+        text = '<p>test<p>'\n+        assert isinstance(XmlXPathSelector(text=text).x(\"//p\")[0],\n+                          XmlXPathSelector)\n+        assert isinstance(HtmlXPathSelector(text=text).x(\"//p\")[0], \n+                          HtmlXPathSelector)\n+\n+    def test_selector_xml_html(self):\n+        \"\"\"Test that XML and HTML XPathSelector's behave differently\"\"\"\n+\n+        # some text which is parsed differently by XML and HTML flavors\n+        text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n+\n+        self.assertEqual(XmlXPathSelector(text=text).x(\"//div\").extract(),\n+                         [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n+\n+        self.assertEqual(HtmlXPathSelector(text=text).x(\"//div\").extract(),\n+                         [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n+\n     def test_selector_nested(self):\n         \"\"\"Nested selector tests\"\"\"\n         body = \"\"\"<body>\n\n@@ -39,10 +39,11 @@ class XPathSelector(object):\n         if hasattr(self.xmlNode, 'xpathEval'):\n             self.doc.xpathContext.setContextNode(self.xmlNode)\n             xpath_result = self.doc.xpathContext.xpathEval(xpath)\n+            cls = type(self)\n             if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([XPathSelector(node=node, parent=self, expr=xpath) for node in xpath_result])\n+                return XPathSelectorList([cls(node=node, parent=self, expr=xpath) for node in xpath_result])\n             else:\n-                return XPathSelectorList([XPathSelector(node=xpath_result, parent=self, expr=xpath)])\n+                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath)])\n         else:\n             return XPathSelectorList([])\n \n@@ -77,7 +78,7 @@ class XPathSelector(object):\n         self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n \n     def __str__(self):\n-        return \"<XPathSelector (%s) xpath=%s>\" % (getattr(self.xmlNode, 'name'), self.expr)\n+        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, 'name'), self.expr)\n \n     __repr__ = __str__\n \n@@ -100,12 +101,15 @@ class XPathSelectorList(list):\n         XPathSelector of the list\"\"\"\n         return [x.extract() if isinstance(x, XPathSelector) else x for x in self]\n \n+\n class XmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for XML content\"\"\"\n-    def __init__(self, response=None, text=None):\n-        XPathSelector.__init__(self, response=response, text=text, constructor=xmlDoc_from_xml)\n+    def __init__(self, *args, **kwargs):\n+        kwargs['constructor'] = xmlDoc_from_xml\n+        XPathSelector.__init__(self, *args, **kwargs)\n \n class HtmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for HTML content\"\"\"\n-    def __init__(self, response=None, text=None):\n-        XPathSelector.__init__(self, response=response, text=text, constructor=xmlDoc_from_html)\n+    def __init__(self, *args, **kwargs):\n+        kwargs['constructor'] = xmlDoc_from_html\n+        XPathSelector.__init__(self, *args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2684dc17bf55cd0e04e9b07c82a55109fee4c40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 75 | Churn Cumulative: 886 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,12 +31,12 @@ class Node:\n         self.master = master\n \n     def _set_status(self, status):\n+        self.status_as_dict = status\n         if not status:\n             self.available = False\n         else:\n             self.available = True\n             self.running = status['running']\n-            self.closing = status['closing']\n             self.maxproc = status['maxproc']\n             self.starttime = status['starttime']\n             self.timestamp = status['timestamp']\n\n@@ -4,7 +4,8 @@ from pydispatch import dispatcher\n \n from scrapy.spider import spiders \n from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.contrib.pbcluster.master.manager import ClusterMaster, priorities\n+from scrapy.contrib.pbcluster.master.manager import *\n+from scrapy.utils.serialization import serialize\n \n class ClusterMasterWeb(ClusterMaster):\n     webconsole_id = 'cluster_master'\n@@ -21,6 +22,8 @@ class ClusterMasterWeb(ClusterMaster):\n             return self.render_nodes(wc_request)\n         elif wc_request.path == '/cluster_master/domains/':\n             return self.render_domains(wc_request)\n+        elif wc_request.path == '/cluster_master/ws/':\n+            return self.webconsole_control(wc_request, ws=True)\n         elif wc_request.args:\n             changes = self.webconsole_control(wc_request)\n \n@@ -44,21 +47,55 @@ class ClusterMasterWeb(ClusterMaster):\n \n         return str(s)\n \n-    def webconsole_control(self, wc_request):\n+    def webconsole_control(self, wc_request, ws=False):\n         args = wc_request.args\n-\n         if \"updatenodes\" in args:\n             self.update_nodes()\n+            if ws:\n+                return self.ws_status(wc_request)\n \n         if \"schedule\" in args:\n-            self.schedule(args[\"schedule\"], priority=eval(args[\"priority\"][0]))\n+            if ws:\n+                sep = \",\"\n+                domains = args[\"schedule\"][0].split(sep)\n+            else:\n+                sep = \"\\r\"\n+                domains = args[\"schedule\"]\n+            priority = eval(args.get(\"priority\", [\"PRIORITY_NORMAL\"])[0])\n+            slist = args.get(\"settings\", [\"\"])[0].split(sep)\n+            spider_settings = {}\n+            for s in slist:\n+                try:\n+                    k, v = s.strip().split(\"=\")\n+                except ValueError:\n+                    pass\n+                else:\n+                    spider_settings[k] = v\n+            self.schedule(domains, spider_settings, priority)\n+            if ws:\n+                return self.ws_status(wc_request)\n \n         if \"stop\" in args:\n-            self.stop(args[\"stop\"])\n+            if ws:\n+                domains = args[\"stop\"][0].split(\",\")\n+            else:\n+                domains=args[\"stop\"]\n+            self.stop(domains)\n+            if ws:\n+                return self.ws_status(wc_request)\n \n         if \"remove\" in args:\n-            self.remove(args[\"remove\"])\n+            if ws:\n+                domains = args[\"remove\"][0].split(\",\")\n+            else:\n+                domains=args[\"remove\"]\n+            self.remove(domains)\n+            if ws:\n+                return self.ws_status(wc_request)\n \n+        if ws:\n+            return self.ws_status(wc_request)\n+        else:\n             return \"\"\n \n     def render_nodes(self, wc_request):\n@@ -110,7 +147,7 @@ class ClusterMasterWeb(ClusterMaster):\n         for domain in sorted(inactive_domains):\n             s += \"<option>%s</option>\\n\" % domain\n         s += \"</select>\\n\"\n-        s += \"</br>\\n\"\n+        s += \"<br />\\n\"\n         s += \"Priority:<br />\\n\"\n         s += \"<select name='priority'>\\n\"\n         for p, pname in priorities.items():\n@@ -119,6 +156,12 @@ class ClusterMasterWeb(ClusterMaster):\n             else:\n                 s += \"<option value='%s'>%s</option>\" % (p, pname)\n         s += \"</select>\\n\"\n+        s += \"<br />\\n\"\n+        s += \"Spider settings:<br />\\n\"\n+        s += \"<textarea name='settings' rows='6'>\\n\"\n+        s += \"UNAVAILABLES_NOTIFY=2\\n\"\n+        s += \"UNAVAILABLES_DAYS_BACK=3\\n\"\n+        s += \"</textarea>\\n\"\n         s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n         s += \"</form>\\n\"\n \n@@ -161,3 +204,18 @@ class ClusterMasterWeb(ClusterMaster):\n \n     def webconsole_discover_module(self):\n         return self\n+\n+    def ws_status(self, wc_request):\n+        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n+        wc_request.setHeader('content-type', 'text/plain')\n+        status = {}\n+        nodes_status = {}\n+        running = []\n+        for d, n in self.nodes.iteritems():\n+            nodes_status[d] = n.status_as_dict\n+            running.extend(n.running)\n+        status[\"nodes\"] = nodes_status\n+        status[\"pending\"] = self.pending\n+        status[\"running\"] = running\n+        content = serialize(status, format)\n+        return content\n\\ No newline at end of file\n\n@@ -88,7 +88,6 @@ class ClusterWorker(pb.Root):\n             if not os.path.exists(os.path.dirname(logfile)):\n                 os.makedirs(os.path.dirname(logfile))\n             scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n-    \n             args = [sys.executable, sys.argv[0], 'crawl', domain]\n             proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n             self.running[domain] = scrapy_proc\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74ef3ab5b0f47de1e105a6e1b5c3642c2d0d9c9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 16 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 58 | Churn Cumulative: 944 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,12 +71,12 @@ class Node:\n         def _run_callback(status):\n             if status['callresponse'][0] == 1:\n                 #slots are complete. Reschedule in master. This is a security issue because could happen that the slots were completed since last status update by another cluster (thinking at future with full-distributed worker-master clusters)\n-                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])\n+                self.master.schedule(pending['domain'], pending['settings'], pending['priority'], pending[\"env\"])\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n             self._set_status(status)\n \n         try:\n-            deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"])\n+            deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"], pending[\"env\"])\n         except pb.DeadReferenceError:\n             self._set_status(None)\n             log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n@@ -132,7 +132,7 @@ class ClusterMaster(object):\n     def remove_node(self, nodename):\n         raise NotImplemented\n \n-    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):\n+    def schedule(self, domains, spider_settings=None, env=None, priority=PRIORITY_NORMAL):\n         i = 0\n         for p in self.pending:\n             if p['priority'] <= priority:\n@@ -140,7 +140,7 @@ class ClusterMaster(object):\n             else:\n                 break\n         for domain in domains:\n-            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'priority': priority})\n+            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'env': env, 'priority': priority})\n         self.update_nodes()\n \n     def stop(self, domains):\n\n@@ -62,6 +62,8 @@ class ClusterMasterWeb(ClusterMaster):\n                 sep = \"\\r\"\n                 domains = args[\"schedule\"]\n             priority = eval(args.get(\"priority\", [\"PRIORITY_NORMAL\"])[0])\n+            \n+            #spider settings\n             slist = args.get(\"settings\", [\"\"])[0].split(sep)\n             spider_settings = {}\n             for s in slist:\n@@ -71,7 +73,19 @@ class ClusterMasterWeb(ClusterMaster):\n                     pass\n                 else:\n                     spider_settings[k] = v\n-            self.schedule(domains, spider_settings, priority)\n+            \n+            #other environment settings\n+            envlist = args.get(\"env\", [\"\"])[0].split(sep)\n+            env = {}\n+            for e in envlist:\n+                try:\n+                    k, v = e.strip().split(\"=\")\n+                except ValueError:\n+                    pass\n+                else:\n+                    env[k] = v\n+                    \n+            self.schedule(domains, spider_settings, env, priority)\n             if ws:\n                 return self.ws_status(wc_request)\n \n@@ -157,11 +171,20 @@ class ClusterMasterWeb(ClusterMaster):\n                 s += \"<option value='%s'>%s</option>\" % (p, pname)\n         s += \"</select>\\n\"\n         s += \"<br />\\n\"\n+        \n+        #spider settings\n         s += \"Spider settings:<br />\\n\"\n         s += \"<textarea name='settings' rows='6'>\\n\"\n         s += \"UNAVAILABLES_NOTIFY=2\\n\"\n         s += \"UNAVAILABLES_DAYS_BACK=3\\n\"\n         s += \"</textarea>\\n\"\n+        s += \"<br />\\n\"\n+        \n+        #other environment settings\n+        s += \"Other environment settings:<br />\\n\"\n+        s += \"<textarea name='env' rows='3'>\\n\"\n+        s += \"</textarea>\\n\"\n+        \n         s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n         s += \"</form>\\n\"\n \n@@ -210,12 +233,7 @@ class ClusterMasterWeb(ClusterMaster):\n         wc_request.setHeader('content-type', 'text/plain')\n         status = {}\n         nodes_status = {}\n-        running = []\n-        for d, n in self.nodes.iteritems():\n-            nodes_status[d] = n.status_as_dict\n-            running.extend(n.running)\n         status[\"nodes\"] = nodes_status\n         status[\"pending\"] = self.pending\n-        status[\"running\"] = running\n         content = serialize(status, format)\n         return content\n\\ No newline at end of file\n\n@@ -12,26 +12,26 @@ from scrapy.conf import settings\n from scrapy.core.engine import scrapyengine\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n-    def __init__(self, procman, domain, logfile=None, spider_settings=None):\n+    def __init__(self, procman, domain, logfile=None, spider_settings=None, env=None):\n         self.procman = procman\n         self.domain = domain\n         self.logfile = logfile\n         self.start_time = datetime.datetime.utcnow()\n         self.status = \"starting\"\n         self.pid = -1\n-\n-        env = {'SCRAPY_LOGFILE': self.logfile, 'SCRAPY_CLUSTER_WORKER_ENABLED': '0', 'SCRAPY_WEBCONSOLE_ENABLED': '0'}\n+        self.env_original = env or {}\n+        self.env = self.env_original.copy()\n         #We conserve original setting format for info purposes (avoid lots of unnecesary \"SCRAPY_\")\n-        self.settings = spider_settings or {}\n-        for k in self.settings:\n-            env[\"SCRAPY_%s\" % k] = self.settings[k]\n-        self.env = env\n+        self.scrapy_settings = spider_settings or {}\n+        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n+        for k in self.scrapy_settings:\n+            self.env[\"SCRAPY_%s\" % k] = self.scrapy_settings[k]\n \n     def __str__(self):\n         return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n \n     def as_dict(self):\n-        return {\"domain\": self.domain, \"pid\": self.pid, \"status\": self.status, \"settings\": self.settings, \"logfile\": self.logfile, \"starttime\": self.start_time}\n+        return {\"domain\": self.domain, \"pid\": self.pid, \"status\": self.status, \"settings\": self.scrapy_settings, \"logfile\": self.logfile, \"starttime\": self.start_time, \"env\": self.env_original}\n \n     def connectionMade(self):\n         self.pid = self.transport.pid\n@@ -81,13 +81,13 @@ class ClusterWorker(pb.Root):\n         status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n         return status\n \n-    def remote_run(self, domain, spider_settings=None):\n+    def remote_run(self, domain, spider_settings=None, env=None):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n             logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n             if not os.path.exists(os.path.dirname(logfile)):\n                 os.makedirs(os.path.dirname(logfile))\n-            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n+            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings, env)\n             args = [sys.executable, sys.argv[0], 'crawl', domain]\n             proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n             self.running[domain] = scrapy_proc\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c713529e412e13272cb16f33f7deacdd4e27cd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 325 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -233,6 +233,8 @@ class ClusterMasterWeb(ClusterMaster):\n         wc_request.setHeader('content-type', 'text/plain')\n         status = {}\n         nodes_status = {}\n+        for d, n in self.nodes.iteritems():\n+            nodes_status[d] = n.status_as_dict\n         status[\"nodes\"] = nodes_status\n         status[\"pending\"] = self.pending\n         content = serialize(status, format)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8344819a927da9130d5a2b57325d1d8e1aa8c1da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 383 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import urlparse\n import urllib\n import bisect\n import sys\n+import pickle, socket\n \n from pydispatch import dispatcher\n \n@@ -86,11 +87,16 @@ class Node:\n class ClusterMaster(object):\n \n     def __init__(self):\n+                    \n         if not settings.getbool('CLUSTER_MASTER_ENABLED'):\n             raise NotConfigured\n-        self.nodes = {}\n+        try:\n+            self.pending = pickle.load( open(\"pending_cache_%s\" % socket.gethostname(), \"r\") )\n+        except IOError:\n             self.pending = []\n+        self.nodes = {}\n         dispatcher.connect(self._engine_started, signal=signals.engine_started)\n+        dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n         \n     def load_nodes(self):\n \n@@ -191,3 +197,5 @@ class ClusterMaster(object):\n     def _engine_started(self):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n+    def _engine_stopped(self):\n+        pickle.dump( self.pending, open(\"pending_cache_%s\" % socket.gethostname(), \"w\") )\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23b3408403bae6448343d9e73ac1211f7d759b4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 33 | Files Changed: 3 | Hunks: 20 | Methods Changed: 13 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 72 | Churn Cumulative: 1028 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,13 @@ priorities = { 20:'NORMAL',\n for val, attr in priorities.items():\n     setattr(sys.modules[__name__], \"PRIORITY_%s\" % attr, val )\n \n+def my_import(name):\n+    mod = __import__(name)\n+    components = name.split('.')\n+    for comp in components[1:]:\n+        mod = getattr(mod, comp)\n+    return mod\n+\n class Node:\n     def __init__(self, remote, status, name, master):\n         self.__remote = remote\n@@ -72,12 +79,12 @@ class Node:\n         def _run_callback(status):\n             if status['callresponse'][0] == 1:\n                 #slots are complete. Reschedule in master. This is a security issue because could happen that the slots were completed since last status update by another cluster (thinking at future with full-distributed worker-master clusters)\n-                self.master.schedule(pending['domain'], pending['settings'], pending['priority'], pending[\"env\"])\n+                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])\n                 log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n             self._set_status(status)\n \n         try:\n-            deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"], pending[\"env\"])\n+            deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"])\n         except pb.DeadReferenceError:\n             self._set_status(None)\n             log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n@@ -88,12 +95,21 @@ class ClusterMaster(object):\n \n     def __init__(self):\n \n-        if not settings.getbool('CLUSTER_MASTER_ENABLED'):\n+        if not (settings.getbool('CLUSTER_MASTER_ENABLED')):\n             raise NotConfigured\n+\n+        #import groups settings\n+        if settings.getbool('GROUPSETTINGS_ENABLED'):\n+            self.get_spider_groupsettings = my_import(settings[\"GROUPSETTINGS_MODULE\"]).get_spider_groupsettings\n+        else:\n+            self.get_spider_groupsettings = lambda x: {}\n+\n+        #load pending domains\n         try:\n             self.pending = pickle.load( open(\"pending_cache_%s\" % socket.gethostname(), \"r\") )\n         except IOError:\n             self.pending = []\n+\n         self.nodes = {}\n         dispatcher.connect(self._engine_started, signal=signals.engine_started)\n         dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n@@ -138,7 +154,7 @@ class ClusterMaster(object):\n     def remove_node(self, nodename):\n         raise NotImplemented\n \n-    def schedule(self, domains, spider_settings=None, env=None, priority=PRIORITY_NORMAL):\n+    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):\n         i = 0\n         for p in self.pending:\n             if p['priority'] <= priority:\n@@ -146,7 +162,9 @@ class ClusterMaster(object):\n             else:\n                 break\n         for domain in domains:\n-            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'env': env, 'priority': priority})\n+            final_spider_settings = self.get_spider_groupsettings(domain)\n+            final_spider_settings.update(spider_settings or {})\n+            self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n         self.update_nodes()\n \n     def stop(self, domains):\n\n@@ -74,18 +74,7 @@ class ClusterMasterWeb(ClusterMaster):\n                 else:\n                     spider_settings[k] = v\n \n-            #other environment settings\n-            envlist = args.get(\"env\", [\"\"])[0].split(sep)\n-            env = {}\n-            for e in envlist:\n-                try:\n-                    k, v = e.strip().split(\"=\")\n-                except ValueError:\n-                    pass\n-                else:\n-                    env[k] = v\n-                    \n-            self.schedule(domains, spider_settings, env, priority)\n+            self.schedule(domains, spider_settings, priority)\n             if ws:\n                 return self.ws_status(wc_request)\n \n@@ -173,18 +162,12 @@ class ClusterMasterWeb(ClusterMaster):\n         s += \"<br />\\n\"\n         \n         #spider settings\n-        s += \"Spider settings:<br />\\n\"\n-        s += \"<textarea name='settings' rows='6'>\\n\"\n+        s += \"Overrided spider settings:<br />\\n\"\n+        s += \"<textarea name='settings' rows='4'>\\n\"\n         s += \"UNAVAILABLES_NOTIFY=2\\n\"\n-        s += \"UNAVAILABLES_DAYS_BACK=3\\n\"\n         s += \"</textarea>\\n\"\n         s += \"<br />\\n\"\n         \n-        #other environment settings\n-        s += \"Other environment settings:<br />\\n\"\n-        s += \"<textarea name='env' rows='3'>\\n\"\n-        s += \"</textarea>\\n\"\n-        \n         s += \"<p><input type='submit' value='Schedule selected domains'></p>\\n\"\n         s += \"</form>\\n\"\n \n\n@@ -12,26 +12,25 @@ from scrapy.conf import settings\n from scrapy.core.engine import scrapyengine\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n-    def __init__(self, procman, domain, logfile=None, spider_settings=None, env=None):\n+    def __init__(self, procman, domain, logfile=None, spider_settings=None):\n         self.procman = procman\n         self.domain = domain\n         self.logfile = logfile\n         self.start_time = datetime.datetime.utcnow()\n         self.status = \"starting\"\n         self.pid = -1\n-        self.env_original = env or {}\n-        self.env = self.env_original.copy()\n+        self.env = {}\n         #We conserve original setting format for info purposes (avoid lots of unnecesary \"SCRAPY_\")\n         self.scrapy_settings = spider_settings or {}\n         self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n         for k in self.scrapy_settings:\n-            self.env[\"SCRAPY_%s\" % k] = self.scrapy_settings[k]\n+            self.env[\"SCRAPY_%s\" % k] = str(self.scrapy_settings[k])\n \n     def __str__(self):\n         return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n \n     def as_dict(self):\n-        return {\"domain\": self.domain, \"pid\": self.pid, \"status\": self.status, \"settings\": self.scrapy_settings, \"logfile\": self.logfile, \"starttime\": self.start_time, \"env\": self.env_original}\n+        return {\"domain\": self.domain, \"pid\": self.pid, \"status\": self.status, \"settings\": self.scrapy_settings, \"logfile\": self.logfile, \"starttime\": self.start_time}\n \n     def connectionMade(self):\n         self.pid = self.transport.pid\n@@ -81,16 +80,22 @@ class ClusterWorker(pb.Root):\n         status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n         return status\n \n-    def remote_run(self, domain, spider_settings=None, env=None):\n+    def remote_run(self, domain, spider_settings=None):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n+            try:\n+                import pysvn\n+                c=pysvn.Client()\n+                r = c.update(settings[\"SVN_WORKDIR\"] or \".\")\n+                log.msg(\"Updated to revision %s.\" %r[0].number )\n+            except:\n+                pass\n             logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n             if not os.path.exists(os.path.dirname(logfile)):\n                 os.makedirs(os.path.dirname(logfile))\n-            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings, env)\n+            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n             args = [sys.executable, sys.argv[0], 'crawl', domain]\n             proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n             self.running[domain] = scrapy_proc\n             return self.status(0, \"Started process %s.\" % scrapy_proc)\n         return self.status(1, \"No free slot to run another process.\")\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e11d824ed0bb27ec2e2932cbdf72c816e4d79ddc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"\n Base class for Scrapy commands\n \"\"\"\n+import os\n from scrapy.conf import settings\n \n class ScrapyCommand(object):\n@@ -41,6 +42,7 @@ class ScrapyCommand(object):\n         parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n         parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n         parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n+        parser.add_option(\"--pidfile\", dest=\"pidfile\", help=\"Write process pid to file FILE\", metavar=\"FILE\")\n         \n     def process_options(self, args, opts):\n         if opts.logfile:\n@@ -58,6 +60,10 @@ class ScrapyCommand(object):\n             from scrapy.spider import spiders\n             spiders.default_domain = opts.spider\n \n+        if opts.pidfile:\n+            pid = os.getpid()\n+            open(opts.pidfile, \"w\").write(str(pid))\n+\n     def run(self, args, opts):\n         \"\"\"\n         Entry point for running commands\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6bbbb0c1f6edf538d6de7ff040f985728de722a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 413 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,6 +39,7 @@ class Node:\n         self.master = master\n \n     def _set_status(self, status):\n+        log.msg(\"Response from worker: %s\" % status)\n         self.status_as_dict = status\n         if not status:\n             self.available = False\n@@ -103,7 +104,6 @@ class ClusterMaster(object):\n             self.get_spider_groupsettings = my_import(settings[\"GROUPSETTINGS_MODULE\"]).get_spider_groupsettings\n         else:\n             self.get_spider_groupsettings = lambda x: {}\n-\n         #load pending domains\n         try:\n             self.pending = pickle.load( open(\"pending_cache_%s\" % socket.gethostname(), \"r\") )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d876db80b59dda45e606eb26d15e5064e5e33f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 282 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n         for k in self.scrapy_settings:\n             self.env[\"SCRAPY_%s\" % k] = str(self.scrapy_settings[k])\n+        self.env[\"PYTHONPATH\"]=\":\".join(sys.path)\n \n     def __str__(self):\n         return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n@@ -83,6 +84,12 @@ class ClusterWorker(pb.Root):\n     def remote_run(self, domain, spider_settings=None):\n         \"\"\"Spawn process to run the given domain.\"\"\"\n         if len(self.running) < self.maxproc:\n+            logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n+            if not os.path.exists(os.path.dirname(logfile)):\n+                os.makedirs(os.path.dirname(logfile))\n+            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n+            args = [sys.executable, sys.argv[0], 'crawl', domain]\n+            self.running[domain] = scrapy_proc\n             try:\n                 import pysvn\n                 c=pysvn.Client()\n@@ -90,12 +97,6 @@ class ClusterWorker(pb.Root):\n                 log.msg(\"Updated to revision %s.\" %r[0].number )\n             except:\n                 pass\n-            logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n-            if not os.path.exists(os.path.dirname(logfile)):\n-                os.makedirs(os.path.dirname(logfile))\n-            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)\n-            args = [sys.executable, sys.argv[0], 'crawl', domain]\n             proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n-            self.running[domain] = scrapy_proc\n             return self.status(0, \"Started process %s.\" % scrapy_proc)\n         return self.status(1, \"No free slot to run another process.\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#648b312bcaa775319f3d6e173528526c35e96232", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 543 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -121,6 +121,7 @@ class ExecutionEngine(object):\n                 reactor.listenTCP(*args, **kwargs)\n             self.running = True\n             reactor.run() # blocking call\n+            self.stop()\n \n     def stop(self):\n         \"\"\"Stop the execution engine\"\"\"\n@@ -134,6 +135,7 @@ class ExecutionEngine(object):\n             self.tasks = []\n             for p in [p for p in self.ports if not isinstance(p, tuple)]:\n                 p.stopListening()\n+            if reactor.running:\n                 reactor.stop()\n             signals.send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e78f7a93bd2a016db7a37a86c40fc92f25b8545d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 692 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -121,7 +121,6 @@ class ExecutionEngine(object):\n                 reactor.listenTCP(*args, **kwargs)\n             self.running = True\n             reactor.run() # blocking call\n-            self.stop()\n \n     def stop(self):\n         \"\"\"Stop the execution engine\"\"\"\n\n@@ -57,7 +57,8 @@ class ExecutionManager(object):\n         \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n         self.configure(**opts)\n         scrapyengine.keep_alive = True\n-        scrapyengine.start()\n+        scrapyengine.start()# blocking call\n+        self.stop()\n \n     def stop(self):\n         \"\"\"Stop the scrapy server, shutting down the execution engine\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#863d94adec48752502ccb185e8574623939c2b33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 703 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -106,7 +106,7 @@ class ClusterMaster(object):\n             self.get_spider_groupsettings = lambda x: {}\n         #load pending domains\n         try:\n-            self.pending = pickle.load( open(\"pending_cache_%s\" % socket.gethostname(), \"r\") )\n+            self.pending = pickle.load( open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"r\") )\n         except IOError:\n             self.pending = []\n \n@@ -216,4 +216,4 @@ class ClusterMaster(object):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n     def _engine_stopped(self):\n-        pickle.dump( self.pending, open(\"pending_cache_%s\" % socket.gethostname(), \"w\") )\n\\ No newline at end of file\n+        pickle.dump( self.pending, open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"w\") )\n\n@@ -25,7 +25,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n         for k in self.scrapy_settings:\n             self.env[\"SCRAPY_%s\" % k] = str(self.scrapy_settings[k])\n-        self.env[\"PYTHONPATH\"] = \":\".join(sys.path)\n+        self.env[\"PYTHONPATH\"] = \":\".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.\n \n     def __str__(self):\n         return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n@@ -93,7 +93,7 @@ class ClusterWorker(pb.Root):\n             try:\n                 import pysvn\n                 c=pysvn.Client()\n-                r = c.update(settings[\"SVN_WORKDIR\"] or \".\")\n+                r = c.update(settings[\"CLUSTER_WORKER_SVNWORKDIR\"] or \".\")\n                 log.msg(\"Updated to revision %s.\" %r[0].number )\n             except:\n                 pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
