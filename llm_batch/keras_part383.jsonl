{"custom_id": "keras#01321c530ea84be2fd796aed4b8da8e5e095be80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2058 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -612,7 +612,7 @@ def round_repeats(repeats, depth_coefficient):\n   return int(math.ceil(depth_coefficient * repeats))\n \n \n-def Block0(\n+def MBConvBlock(\n   input_filters: int,\n   output_filters: int,\n   expand_ratio=1,\n@@ -725,7 +725,7 @@ def Block0(\n   return apply\n \n \n-def Block1(\n+def FusedMBConvBlock(\n   input_filters: int,\n   output_filters: int,\n   expand_ratio=1,\n@@ -986,7 +986,7 @@ def EfficientNetV2(\n     )\n \n     # Determine which conv type to use:\n-    block = {0: Block0, 1: Block1}[args.pop(\"conv_type\")]\n+    block = {0: MBConvBlock, 1: FusedMBConvBlock}[args.pop(\"conv_type\")]\n     repeats = round_repeats(\n       repeats=args.pop(\"num_repeat\"), depth_coefficient=depth_coefficient\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4342dcdf93371e0c783e560c918e8c60b4760a6d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 2067 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -938,6 +938,15 @@ def EfficientNetV2(\n   x = img_input\n \n   if include_preprocessing:\n+    # V1 preprocessing for Bx variants\n+    if model_name.split('-')[-1].startswith('b'):\n+      x = layers.Rescaling(scale=1./255)(x)\n+      x = layers.Normalization(\n+        mean=[0.485, 0.456, 0.406],\n+        variance=[0.229 ** 2, 0.224 ** 2, 0.225 ** 2],\n+        axis=bn_axis\n+      )(x)\n+    else:\n       x = layers.Rescaling(scale=1./128.0, offset=-1)(x)\n \n   # Build stem\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5171d521acbf2ebbb6414352d5792163c41479f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 13 | Churn Cumulative: 3367 | Contributors (this commit): 6 | Commits (past 90d): 19 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -215,7 +215,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       del kwargs[\"bins\"]\n \n     # By default, output int64 when output_mode='int' and floats otherwise.\n-    if \"dtype\" not in kwargs:\n+    if \"dtype\" not in kwargs or kwargs[\"dtype\"] is None:\n       kwargs[\"dtype\"] = tf.int64 if output_mode == INT else backend.floatx()\n     elif output_mode == \"int\" and not tf.as_dtype(kwargs[\"dtype\"]).is_integer:\n       # Compat for when dtype was alwyas floating and ingored by the layer.\n\n@@ -193,6 +193,11 @@ class DiscretizationTest(keras_parameterized.TestCase,\n                                           dtype=\"float32\")\n     outputs = layer(inputs)\n     self.assertAllEqual(outputs.dtype, tf.int64)\n+    # In TF1 we sometimes face an explicit dtype=None in the config.\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5],\n+                                          dtype=None)\n+    outputs = layer(inputs)\n+    self.assertAllEqual(outputs.dtype, tf.int64)\n \n   @parameterized.named_parameters(\n       (\"float32\", tf.float32),\n\n@@ -165,7 +165,7 @@ class Hashing(base_layer.Layer):\n           f'values. Received: num_bins={num_bins}.')\n \n     # By default, output int64 when output_mode='int' and floats otherwise.\n-    if 'dtype' not in kwargs:\n+    if 'dtype' not in kwargs or kwargs['dtype'] is None:\n       kwargs['dtype'] = tf.int64 if output_mode == INT else backend.floatx()\n     elif output_mode == 'int' and not tf.as_dtype(kwargs['dtype']).is_integer:\n       # Compat for when dtype was alwyas floating and ingored by the layer.\n\n@@ -318,6 +318,10 @@ class HashingTest(keras_parameterized.TestCase):\n     layer = hashing.Hashing(num_bins=3, dtype='float32')\n     outputs = layer(inputs)\n     self.assertAllEqual(outputs.dtype, tf.int64)\n+    # In TF1 we sometimes face an explicit dtype=None in the config.\n+    layer = hashing.Hashing(num_bins=3, dtype=None)\n+    outputs = layer(inputs)\n+    self.assertAllEqual(outputs.dtype, tf.int64)\n \n   @parameterized.named_parameters(\n       ('float32', tf.float32),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c56113365d2aa1036980c9b9a408e2400c2b7fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 10 | Churn Cumulative: 1323 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -142,12 +142,6 @@ class Zeros(Initializer):\n       raise ValueError(f'Expected numeric or boolean dtype, got {dtype}.')\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n-    return self._call(shape, dtype, **kwargs)\n-\n-  def _call(self, shape, dtype, **kwargs):\n-    # TODO(scottzhu): remove this extra level of _call when layout is part of\n-    # the public API.\n-    del kwargs\n     return tf.zeros(shape, dtype)\n \n \n@@ -185,10 +179,6 @@ class Ones(Initializer):\n       raise ValueError(f'Expected numeric or boolean dtype, got {dtype}.')\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n-    return self._call(shape, dtype, **kwargs)\n-\n-  def _call(self, shape, dtype, **kwargs):\n-    del kwargs\n     return tf.ones(shape, dtype)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b774d4db361649a58cdb1e70b6e78fd3752de5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 7325 | Contributors (this commit): 20 | Commits (past 90d): 14 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -490,11 +490,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   def call(self, inputs, *args, **kwargs):  # pylint: disable=unused-argument\n     \"\"\"This is where the layer's logic lives.\n \n-    Note here that `call()` method in `tf.keras` is little bit different\n-    from `keras` API. In `keras` API, you can pass support masking for\n-    layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n-    method to support masking.\n-\n     The `call()` method may not create state (except in its first invocation,\n     wrapping the creation of variables or other resources in `tf.init_scope()`).\n     It is recommended to create state in `__init__()`, or the `build()` method\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#985b80ea05c4c7dd1780b7ccf7f86a043722bd10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 68 | Churn Cumulative: 1252 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -14,14 +14,12 @@\n # ==============================================================================\n \"\"\"Custom Training Loop correctness test.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras.distribute import optimizer_combinations\n from keras.distribute import strategy_combinations\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n _NUM_SAMPLES = 66\n _BATCH_SIZE = 32\n@@ -235,7 +233,8 @@ class TestDistributionStrategyDnnCorrectness(tf.test.TestCase,\n               distribution=strategy_combinations.multiworker_strategies,\n               optimizer_fn=[\n                   optimizer_combinations.gradient_descent_optimizer_keras_v2_fn,\n-                  optimizer_combinations.adagrad_optimizer_keras_v2_fn\n+                  optimizer_combinations.adagrad_optimizer_keras_v2_fn,\n+                  optimizer_combinations.adam_experimental_fn,\n               ],\n               mode=['eager'],\n               iteration_type=['iterator', 'dataset'],\n\n@@ -14,7 +14,7 @@\n # ==============================================================================\n \"\"\"Strategy and optimizer combinations for combinations.combine().\"\"\"\n \n-import tensorflow.compat.v2 as tf\n+from keras.optimizer_experimental import adam as adam_experimental\n from keras.optimizer_v2 import adadelta as adadelta_keras_v2\n from keras.optimizer_v2 import adagrad as adagrad_keras_v2\n from keras.optimizer_v2 import adam as adam_keras_v2\n@@ -23,6 +23,7 @@ from keras.optimizer_v2 import ftrl as ftrl_keras_v2\n from keras.optimizer_v2 import gradient_descent as gradient_descent_keras_v2\n from keras.optimizer_v2 import nadam as nadam_keras_v2\n from keras.optimizer_v2 import rmsprop as rmsprop_keras_v2\n+import tensorflow.compat.v2 as tf\n \n \n gradient_descent_optimizer_v1_fn = tf.__internal__.test.combinations.NamedObject(\n@@ -49,6 +50,8 @@ adagrad_optimizer_keras_v2_fn = tf.__internal__.test.combinations.NamedObject(\n     \"AdagradKerasV2\", lambda: adagrad_keras_v2.Adagrad(0.001))\n adam_optimizer_keras_v2_fn = tf.__internal__.test.combinations.NamedObject(\n     \"AdamKerasV2\", lambda: adam_keras_v2.Adam(0.001, epsilon=1.0))\n+adam_experimental_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"AdamExperimental\", lambda: adam_experimental.Adam(0.001))\n adamax_optimizer_keras_v2_fn = tf.__internal__.test.combinations.NamedObject(\n     \"AdamaxKerasV2\", lambda: adamax_keras_v2.Adamax(0.001, epsilon=1.0))\n nadam_optimizer_keras_v2_fn = tf.__internal__.test.combinations.NamedObject(\n\n@@ -48,8 +48,8 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     self._global_clipnorm = global_clipnorm\n     if self._clipnorm is not None and self._global_clipnorm is not None:\n       raise ValueError(f\"At most one of `clipnorm` and `global_clipnorm` can \"\n-                       f\"be set. Received: clipnorm={self.clipnorm}, \"\n-                       f\"global_clipnorm={self.global_clipnorm}.\")\n+                       f\"be set. Received: clipnorm={self._clipnorm}, \"\n+                       f\"global_clipnorm={self._global_clipnorm}.\")\n     self._clipvalue = clipvalue\n     with tf.init_scope():\n       # Lift the variable creation to init scope to avoid environment issue.\n@@ -99,14 +99,18 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n       return clipped_grads\n \n     if self._global_clipnorm is not None and self._global_clipnorm > 0:\n-      return tf.clip_by_global_norm(grads, self._global_clipnorm)\n+      return tf.clip_by_global_norm(grads, self._global_clipnorm)[0]\n \n     if self._clipvalue is not None and self._clipvalue > 0:\n       for g in grads:\n         if g is None:\n           clipped_grads.append(g)\n         else:\n-          clipped_grads.append(tf.clip_by_value(g, self._clipvalue))\n+          clipped_grads.append(\n+              tf.clip_by_value(\n+                  g,\n+                  clip_value_min=-self._clipvalue,  # pylint: disable=invalid-unary-operand-type\n+                  clip_value_max=self._clipvalue))\n       return clipped_grads\n \n     return grads\n@@ -464,7 +468,7 @@ class RestoredOptimizer(Optimizer):\n \n # Register the optimizer for loading from saved_model purpose.\n tf.__internal__.saved_model.load.register_revived_type(\n-    \"optimizerV3\",\n+    \"experimentalOptimizer\",\n     lambda obj: isinstance(obj, Optimizer),\n     versions=[\n         tf.__internal__.saved_model.load.VersionedTypeRegistration(\n\n@@ -52,6 +52,44 @@ class OptimizerFuntionalityTest(tf.test.TestCase):\n     optimizer._build_index_dict(var_list)\n     self.assertEqual(optimizer._index_dict[optimizer._var_key(var_list[7])], 7)\n \n+  def testClipNorm(self):\n+    optimizer = adam_new.Adam(clipnorm=1)\n+    grad = [tf.convert_to_tensor([100.0, 100.0])]\n+    clipped_grad = optimizer._clip_gradients(grad)\n+    self.assertAllClose(clipped_grad[0], [2**0.5/2, 2**0.5/2])\n+\n+  def testClipValue(self):\n+    optimizer = adam_new.Adam(clipvalue=1)\n+    grad = [tf.convert_to_tensor([100.0, 100.0])]\n+    clipped_grad = optimizer._clip_gradients(grad)\n+    self.assertAllEqual(clipped_grad[0], [1.0, 1.0])\n+\n+  def testClipGlobalNorm(self):\n+    optimizer = adam_new.Adam(global_clipnorm=1)\n+    grad = [tf.cast([100.0, 100.0], dtype=tf.float32),\n+            tf.cast([100.0, 100.0], dtype=tf.float32)]\n+    clipped_grad = optimizer._clip_gradients(grad)\n+    self.assertAllClose(clipped_grad[0], [0.5, 0.5])\n+\n+    with self.assertRaisesRegex(ValueError, \"At most one of*\"):\n+      _ = adam_new.Adam(\n+          learning_rate=1, epsilon=0, global_clipnorm=1, clipnorm=1)\n+\n+  def testSetLearningRate(self):\n+    optimizer = adam_new.Adam(learning_rate=1.0)\n+    self.assertIsInstance(optimizer._learning_rate, tf.Variable)\n+    self.assertEqual(self.evaluate(optimizer.learning_rate), 1.0)\n+    optimizer.learning_rate = 2.0\n+    self.assertEqual(self.evaluate(optimizer.learning_rate), 2.0)\n+\n+    lr_schedule = learning_rate_schedule.ExponentialDecay(\n+        initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.9)\n+    optimizer = adam_new.Adam(learning_rate=lr_schedule)\n+    self.assertIsInstance(optimizer._learning_rate,\n+                          learning_rate_schedule.ExponentialDecay)\n+    with self.assertRaisesRegex(TypeError, \"This optimizer was created with*\"):\n+      optimizer.learning_rate = 2.0\n+\n   def testGetConfig(self):\n     optimizer = adam_new.Adam(\n         learning_rate=np.float64(0.05),\n@@ -100,7 +138,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase):\n         self.evaluate(optimizer_1._iterations),\n         self.evaluate(optimizer_2._iterations))\n \n-  def disabletestSaveAndLoadOptimizerWithModel(self):\n+  def testSaveAndLoadOptimizerWithModel(self):\n     model = keras.Sequential(\n         [keras.layers.Input(shape=(1,)),\n          keras.layers.Dense(1)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53ea41ba8ed79fecb0a0c021315c10c457337d30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 34 | Churn Cumulative: 1022 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,6 @@\n # ==============================================================================\n \"\"\"Keras Input Tensor used to track functional API Topology.\"\"\"\n \n-import copy\n from keras.utils import object_identity\n import tensorflow.compat.v2 as tf\n \n@@ -283,11 +282,6 @@ class KerasTensor:\n       raise ValueError(\n           f\"Keras symbolic input/output's shape {self.shape} is not \"\n           f\"compatible with supplied shape {shape}.\")\n-    else:\n-      if isinstance(self._type_spec, tf.TensorSpec):\n-        # Note: this mutates self._type_spec in place -- if any other code has\n-        # a reference to self._type_spec, then they will also see this change.\n-        self._type_spec._shape = shape  # pylint: disable=protected-access\n     else:\n       self._type_spec = type_spec_with_shape(self._type_spec, shape)\n \n@@ -641,11 +635,14 @@ def keras_tensor_from_type_spec(type_spec, name=None):\n \n def type_spec_with_shape(spec, shape):\n   \"\"\"Returns a copy of TypeSpec `spec` with its shape set to `shape`.\"\"\"\n-  if isinstance(spec,\n-                (tf.TensorSpec, tf.RaggedTensorSpec, tf.SparseTensorSpec)):\n-    result = copy.deepcopy(spec)\n-    result._shape = shape  # pylint: disable=protected-access\n-    return result\n+  if isinstance(spec, tf.TensorSpec):\n+    return tf.TensorSpec(shape, spec.dtype, spec.name)\n+  elif isinstance(spec, tf.RaggedTensorSpec):\n+    return tf.RaggedTensorSpec(shape, spec.dtype, spec.ragged_rank,\n+                               spec.row_splits_dtype,\n+                               spec.flat_values_spec)\n+  elif isinstance(spec, tf.SparseTensorSpec):\n+    return tf.SparseTensorSpec(shape, spec.dtype)\n   elif hasattr(spec, 'with_shape'):\n     # TODO(edloper): Consider adding .with_shape method to TensorSpec,\n     # RaggedTensorSpec, and SparseTensorSpec.\n\n@@ -166,6 +166,21 @@ class KerasTensorTest(keras_parameterized.TestCase):\n         ValueError, \"Keras requires TypeSpec to have a `with_shape` method\"):\n       kt.set_shape([3, 3])\n \n+  def test_set_shape_equals_expected_shape(self):\n+    # Tests b/203201161: DenseSpec has both a _shape and a _shape_tuple field,\n+    # and we need to be sure both get updated.\n+    kt = keras_tensor.KerasTensor(tf.TensorSpec([8, None], tf.int32))\n+    kt.set_shape([8, 3])\n+    self.assertEqual(kt.type_spec, tf.TensorSpec([8, 3], tf.int32))\n+\n+  def test_type_spec_with_shape_equals_expected_shape(self):\n+    # Tests b/203201161: DenseSpec has both a _shape and a _shape_tuple field,\n+    # and we need to be sure both get updated.\n+    spec1 = tf.TensorSpec([8, None], tf.int32)\n+    spec2 = keras_tensor.type_spec_with_shape(spec1, [8, 3])\n+    expected = tf.TensorSpec([8, 3], tf.int32)\n+    self.assertEqual(spec2, expected)\n+\n   def test_missing_shape_error(self):\n     spec = CustomTypeSpec(None, tf.int32)\n     del spec.shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ad416818f7348bb23e5b0201c4f8f54bd931dc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 30 | Churn Cumulative: 1064 | Contributors (this commit): 8 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -272,17 +272,12 @@ class KerasTensor:\n     \"\"\"Updates the shape of this KerasTensor. Mimics `tf.Tensor.set_shape()`.\"\"\"\n     if not isinstance(shape, tf.TensorShape):\n       shape = tf.TensorShape(shape)\n-    if shape.dims is not None:\n-      dim_list = [dim.value for dim in shape.dims]\n-      for dim in range(len(dim_list)):\n-        if dim_list[dim] is None and self.shape.dims is not None:\n-          dim_list[dim] = self.shape.dims[dim]\n-      shape = tf.TensorShape(dim_list)\n     if not self.shape.is_compatible_with(shape):\n       raise ValueError(\n           f\"Keras symbolic input/output's shape {self.shape} is not \"\n           f\"compatible with supplied shape {shape}.\")\n     else:\n+      shape = self.shape.merge_with(shape)\n       self._type_spec = type_spec_with_shape(self._type_spec, shape)\n \n   def __str__(self):\n\n@@ -141,22 +141,23 @@ class KerasTensorTest(keras_parameterized.TestCase):\n     self.assertEqual(kt.shape.as_list(), expected_shape)\n \n   @parameterized.parameters([\n-      (tf.TensorSpec([8, 3], tf.int32), [8, 3]),\n-      (tf.TensorSpec([None, 3], tf.int32), [8, 3]),\n-      (tf.TensorSpec(None, tf.int32), [8, 3]),\n-      (tf.TensorSpec(None, tf.int32), [8, None]),\n-      (tf.TensorSpec(None, tf.int32), None),\n-      (tf.RaggedTensorSpec([2, None, None]), [2, None, 5]),\n-      (tf.SparseTensorSpec([8]), [8]),\n-      (CustomTypeSpec2([3, None], tf.int32), [3, 8]),\n+      (tf.TensorSpec([8, 3], tf.int32), [8, 3], [8, 3]),\n+      (tf.TensorSpec([None, 3], tf.int32), [8, 3], [8, 3]),\n+      (tf.TensorSpec([8, 3], tf.int32), [None, 3], [8, 3]),\n+      (tf.TensorSpec(None, tf.int32), [8, 3], [8, 3]),\n+      (tf.TensorSpec(None, tf.int32), [8, None], [8, None]),\n+      (tf.TensorSpec(None, tf.int32), None, None),\n+      (tf.RaggedTensorSpec([2, None, None]), [2, None, 5], [2, None, 5]),\n+      (tf.SparseTensorSpec([8]), [8], [8]),\n+      (CustomTypeSpec2([3, None], tf.int32), [3, 8], [3, 8]),\n   ])\n-  def test_set_shape(self, spec, new_shape):\n+  def test_set_shape(self, spec, new_shape, expected_shape):\n     kt = keras_tensor.KerasTensor(spec)\n     kt.set_shape(new_shape)\n-    if new_shape is None:\n+    if expected_shape is None:\n       self.assertIsNone(kt.type_spec.shape.rank)\n     else:\n-      self.assertEqual(kt.type_spec.shape.as_list(), new_shape)\n+      self.assertEqual(kt.type_spec.shape.as_list(), expected_shape)\n     self.assertTrue(kt.type_spec.is_compatible_with(spec))\n \n   def test_set_shape_error(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#411ab6b26d360f4820a52ac307c502ef951b870c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 44 | Files Changed: 5 | Hunks: 19 | Methods Changed: 11 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 64 | Churn Cumulative: 5661 | Contributors (this commit): 15 | Commits (past 90d): 20 | Contributors (cumulative): 29 | DMM Complexity: 0.4666666666666667\n\nDIFF:\n@@ -619,7 +619,7 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n       else:\n         return (tf.no_op(), True)\n \n-    if optimizer_utils.strategy_supports_no_merge_call():\n+    if tf.__internal__.distribute.strategy_supports_no_merge_call():\n       loss_scale_update_op, should_apply_grads = _if_should_apply_grads(grads)\n       def apply_fn():\n         return self._apply_gradients(grads, wrapped_vars, name)\n\n@@ -18,7 +18,6 @@ This is under development, and subject to interface/implementation changes.\n \"\"\"\n \n import abc\n-import functools\n \n from keras import backend\n from keras import initializers\n@@ -433,16 +432,12 @@ class Optimizer(_BaseOptimizer):\n     super().apply_gradients(grads_and_vars)\n \n   def _internal_apply_gradients(self, grads_and_vars):\n-    # TODO(b/202332404): create a tf.distribute util to handle the if-else.\n-    if optimizer_utils.strategy_supports_no_merge_call():\n-      self._distributed_apply_gradients(self._distribution_strategy,\n+    tf.__internal__.distribute.interim.maybe_merge_call(\n+        self._distributed_apply_gradients_fn, self._distribution_strategy,\n         grads_and_vars)\n-    else:\n-      tf.distribute.get_replica_context().merge_call(\n-          functools.partial(self._distributed_apply_gradients),\n-          args=(grads_and_vars,))\n \n-  def _distributed_apply_gradients(self, distribution, grads_and_vars):\n+  def _distributed_apply_gradients_fn(self, distribution, grads_and_vars,\n+                                      **kwargs):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n \n     def apply_grad_to_update_var(var, grad):\n\n@@ -13,10 +13,9 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Version 2 of class Optimizer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n # pylint: disable=g-bad-name\n \n+\n import abc\n import contextlib\n import functools\n@@ -30,6 +29,7 @@ from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -668,18 +668,13 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         grads_and_vars = self._aggregate_gradients(grads_and_vars)\n       grads_and_vars = self._transform_gradients(grads_and_vars)\n \n-      if optimizer_utils.strategy_supports_no_merge_call():\n-        return self._distributed_apply(strategy, grads_and_vars, name,\n-                                       apply_state)\n-      else:\n-        return tf.distribute.get_replica_context().merge_call(\n+      return tf.__internal__.distribute.interim.maybe_merge_call(\n           functools.partial(self._distributed_apply, apply_state=apply_state),\n-            args=(grads_and_vars,),\n-            kwargs={\n-                \"name\": name,\n-            })\n+          strategy,\n+          grads_and_vars,\n+          name=name)\n \n-  def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):\n+  def _distributed_apply(self, distribution, grads_and_vars, apply_state, name):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n \n     def apply_grad_to_update_var(var, grad):\n\n@@ -30,7 +30,7 @@ def all_reduce_sum_gradients(grads_and_vars):\n   grads_and_vars = list(grads_and_vars)\n   filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n   if filtered_grads_and_vars:\n-    if strategy_supports_no_merge_call():\n+    if tf.__internal__.distribute.strategy_supports_no_merge_call():\n       grads = [pair[0] for pair in filtered_grads_and_vars]\n       reduced = tf.distribute.get_replica_context().all_reduce(\n           tf.distribute.ReduceOp.SUM, grads)\n@@ -150,11 +150,3 @@ def make_gradient_clipvalue_fn(clipvalue):\n def _all_reduce_sum_fn(distribution, grads_and_vars):\n   return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\n                                                grads_and_vars)\n-\n-\n-def strategy_supports_no_merge_call():\n-  \"\"\"Returns if the current Strategy can operate in pure replica context.\"\"\"\n-  if not tf.distribute.has_strategy():\n-    return True\n-  strategy = tf.distribute.get_strategy()\n-  return not strategy.extended._use_merge_call()  # pylint: disable=protected-access\n\n@@ -15,18 +15,15 @@\n # pylint: disable=protected-access\n \"\"\"Utils related to keras metrics.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n+from enum import Enum\n import functools\n import weakref\n-\n-from enum import Enum\n-\n-import numpy as np\n from keras import backend\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n from keras.utils.generic_utils import to_list\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n NEG_INF = -1e10\n \n@@ -99,7 +96,6 @@ def result_wrapper(result_fn):\n \n   def decorated(metric_obj, *args):\n     \"\"\"Decorated function with merge_call.\"\"\"\n-    has_strategy = tf.distribute.has_strategy()\n     replica_context = tf.distribute.get_replica_context()\n \n     # The purpose of using `merge_call` to call `result()` is to trigger cross\n@@ -120,14 +116,12 @@ def result_wrapper(result_fn):\n     #    if NCCL is used to do the aggregation, the program will hang because\n     #    NCCL ops are only launched on the non-pruned first replica.\n     #\n-    # We condition on strategy.extended._use_merge_call() since we know if it is\n-    # false, the program uses `jit_compile` to compile replica fn, meaning it is\n+    # We condition on strategy_supports_no_merge_call() since we know if it is\n+    # True, the program uses `jit_compile` to compile replica fn, meaning it is\n     # not V1 training (hence #1 is okay), and no pruning will happen as\n     # compiled functions are not inlined (hence #2 is okay).\n-\n-    if (not has_strategy or replica_context is None or\n-        not tf.distribute.get_strategy(\n-        ).extended._use_merge_call()):\n+    if (replica_context is None or\n+        tf.__internal__.distribute.strategy_supports_no_merge_call()):\n       with tf.__internal__.distribute.variable_sync_on_read_context():\n         raw_result = result_fn(*args)\n         # Results need to be wrapped in a `tf.identity` op to ensure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b78faedf5eb5aeb63450c13dec437f79811983a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 23 | Churn Cumulative: 295 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -198,20 +198,23 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):  # py\n   with generic_utils.SharedObjectLoadingScope():\n     with generic_utils.CustomObjectScope(custom_objects or {}):\n       with load_context.load_context(options):\n-        if (h5py is not None and\n-            (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n-          return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n-                                                  compile)\n+        filepath_str = path_to_string(filepath)\n+        if isinstance(filepath_str, str):\n+          if not tf.io.gfile.exists(filepath_str):\n+            raise IOError(f'No file or directory found at {filepath_str}')\n \n-        filepath = path_to_string(filepath)\n-        if isinstance(filepath, str):\n-          if not tf.io.gfile.exists(filepath):\n-            raise IOError(f'No file or directory found at {filepath}')\n-          if saving_utils.is_hdf5_filepath(filepath) and h5py is None:\n+          if tf.io.gfile.isdir(filepath_str):\n+            return saved_model_load.load(filepath_str, compile, options)\n+          else:\n+            if h5py is None:\n               raise ImportError(\n                   'Filepath looks like a hdf5 file but h5py is not available.'\n                   f' filepath={filepath}')\n-          return saved_model_load.load(filepath, compile, options)\n+            return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n+                                                    compile)\n+        elif h5py is not None and isinstance(filepath, h5py.File):\n+          return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n+                                                  compile)\n \n   raise IOError(\n       'Unable to load model. Filepath is not an hdf5 file (or h5py is not '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dba23625735eace3d1c7f64f46959f22c971eafb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 398 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -62,9 +62,12 @@ ARG_TO_MODEL = {\n                       efficientnet.EfficientNetB4, efficientnet.EfficientNetB5,\n                       efficientnet.EfficientNetB6, efficientnet.EfficientNetB7]),\n     'efficientnet_v2': (efficientnet_v2,\n-                        [efficientnet_v2.EfficientNetV2B0, efficientnet_v2.EfficientNetV2B1,\n-                         efficientnet_v2.EfficientNetV2B2, efficientnet_v2.EfficientNetV2B3,\n-                         efficientnet_v2.EfficientNetV2S, efficientnet_v2.EfficientNetV2M,\n+                        [efficientnet_v2.EfficientNetV2B0,\n+                         efficientnet_v2.EfficientNetV2B1,\n+                         efficientnet_v2.EfficientNetV2B2,\n+                         efficientnet_v2.EfficientNetV2B3,\n+                         efficientnet_v2.EfficientNetV2S,\n+                         efficientnet_v2.EfficientNetV2M,\n                          efficientnet_v2.EfficientNetV2L])\n }\n \n\n@@ -62,13 +62,13 @@ MODEL_LIST_NO_NASNET = [\n     (efficientnet.EfficientNetB5, 2048),\n     (efficientnet.EfficientNetB6, 2304),\n     (efficientnet.EfficientNetB7, 2560),\n-    (efficientnet_v2.EfficientNetV2_B0, 1280),\n-    (efficientnet_v2.EfficientNetV2_B1, 1280),\n-    (efficientnet_v2.EfficientNetV2_B2, 1408),\n-    (efficientnet_v2.EfficientNetV2_B3, 1536),\n-    (efficientnet_v2.EfficientNetV2_S, 1280),\n-    (efficientnet_v2.EfficientNetV2_M, 1280),\n-    (efficientnet_v2.EfficientNetV2_L, 1280),\n+    (efficientnet_v2.EfficientNetV2B0, 1280),\n+    (efficientnet_v2.EfficientNetV2B1, 1280),\n+    (efficientnet_v2.EfficientNetV2B2, 1408),\n+    (efficientnet_v2.EfficientNetV2B3, 1536),\n+    (efficientnet_v2.EfficientNetV2S, 1280),\n+    (efficientnet_v2.EfficientNetV2M, 1280),\n+    (efficientnet_v2.EfficientNetV2L, 1280),\n ]\n \n NASNET_LIST = [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0ec44e0841302de4427caba0127196da45147d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 2095 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -37,20 +37,20 @@ from tensorflow.python.util.tf_export import keras_export\n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/sebastian-sz-weights/efficientnet-v2/\"\n \n WEIGHTS_HASHES = {\n-  \"b0\": (\"42016207e946245a808dc807e21e2334\",\n-         \"953eb10b448fc338a0206deb1a0228c8\"),\n-  \"b1\": (\"fda7eaf4fe84f9ab1aab43ab0b7ffd26\",\n-         \"fa1764f91f33620586b628e18f265ed4\"),\n-  \"b2\": (\"c991196606d604057d3b713ed41087d9\",\n-         \"d3ff2c5f2ae01882452d0d8f1b38c3d7\"),\n-  \"b3\": (\"0e02aea454e5735908f321f3946daf0a\",\n-         \"0e8a2ed23f6c14cf1a4dddfcd2682cc9\"),\n-  \"-s\": (\"5c0a6e0d26188ab9a1dbe81fcf6a8a2b\",\n-         \"36ec622752444b5b96942d5c6d8845dc\"),\n-  \"-m\": (\"4063c4fbcf08a22a49949f1314abbe67\",\n-         \"ee4aa57b3e0ece509d8e17e5c50f06a9\"),\n-  \"-l\": (\"938d235ec40bfd60b8b1e075ded6ffd0\",\n-           \"3a032f50516a9eaaf1b5ebf2a2304657\"),\n+  \"b0\": (\"21ecbf6da12460d5c40bb2f29ceb2188\",\n+         \"893217f2bb855e2983157299931e43ff\"),\n+  \"b1\": (\"069f0534ff22adf035c89e2d9547a9dc\",\n+         \"0e80663031ca32d657f9caa404b6ec37\"),\n+  \"b2\": (\"424e49f28180edbde1e94797771950a7\",\n+         \"1dfe2e7a5d45b6632553a8961ea609eb\"),\n+  \"b3\": (\"1f1fc43bd98a6e4fd8fdfd551e02c7a0\",\n+         \"f6abf7b5849ac99a89b50dd3fd532856\"),\n+  \"-s\": (\"e1d88a8495beba45748fedd0cecbe016\",\n+         \"af0682fb74e8c54910f2d4393339c070\"),\n+  \"-m\": (\"a3bf6aa3276309f4fc6a34aa114c95cd\",\n+         \"1b8dc055df72dde80d614482840fe342\"),\n+  \"-l\": (\"27e6d408b53c7ebc868fefa357689935\",\n+         \"b0b66b5c863aef5b46e8608fe1711615\"),\n }\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d67a43aa8cf2a6618bbd103869c477e449e7428d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 50 | Files Changed: 36 | Hunks: 49 | Methods Changed: 35 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 100 | Churn Cumulative: 101289 | Contributors (this commit): 320 | Commits (past 90d): 209 | Contributors (cumulative): 621 | DMM Complexity: None\n\nDIFF:\n@@ -481,7 +481,7 @@ def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n \n   x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n \n-  # Project wiht a pointwise 1x1 convolution.\n+  # Project with a pointwise 1x1 convolution.\n   x = layers.Conv2D(\n       pointwise_filters,\n       kernel_size=1,\n\n@@ -128,7 +128,7 @@ class AntirectifierBenchmark(tf.test.Benchmark):\n \n \n class Antirectifier(tf.keras.layers.Layer):\n-  \"\"\"Build simple custome layer.\"\"\"\n+  \"\"\"Build simple custom layer.\"\"\"\n \n   def __init__(self, initializer=\"he_normal\", **kwargs):\n     super(Antirectifier, self).__init__(**kwargs)\n\n@@ -599,7 +599,7 @@ class Callback:\n \n   1. You should pack all your callbacks into a single `callbacks.CallbackList`\n      so they can all be called together.\n-  2. You will need to manually call all the `on_*` methods at the apropriate\n+  2. You will need to manually call all the `on_*` methods at the appropriate\n      locations in your loop. Like this:\n \n      ```\n@@ -2360,7 +2360,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         of positive integers signify a range of batches to profile.\n \n     Raises:\n-      ValueError: If profile_batch is not an integer or a comma seperated pair\n+      ValueError: If profile_batch is not an integer or a comma separated pair\n                   of positive integers.\n \n     \"\"\"\n\n@@ -668,7 +668,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         return autocast_variable.create_autocast_variable(variable)\n       # Also the caching_device does not work with the mixed precision API,\n       # disable it if it is specified.\n-      # TODO(b/142020079): Reenable it once the bug is fixed.\n+      # TODO(b/142020079): Re-enable it once the bug is fixed.\n       if caching_device is not None:\n         tf_logging.warning(\n             '`caching_device` does not work with mixed precision API. Ignoring '\n@@ -3410,7 +3410,7 @@ def _apply_name_scope_on_model_declaration(enable):\n \n \n class BaseRandomLayer(Layer):\n-  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+  \"\"\"A layer handle the random number creation and savemodel behavior.\"\"\"\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, seed=None, force_generator=False, **kwargs):\n@@ -3430,7 +3430,7 @@ class BaseRandomLayer(Layer):\n       seed: optional integer, used to create RandomGenerator.\n       force_generator: boolean, default to False, whether to force the\n         RandomGenerator to use the code branch of tf.random.Generator.\n-      **kwargs: other keyward arguements that will be passed to the parent class\n+      **kwargs: other keyword arguments that will be passed to the parent class\n     \"\"\"\n     super().__init__(**kwargs)\n     self._random_generator = backend.RandomGenerator(\n\n@@ -413,7 +413,7 @@ class Layer(base_layer.Layer):\n         return autocast_variable.create_autocast_variable(variable)\n       # Also the caching_device does not work with the mixed precision API,\n       # disable it if it is specified.\n-      # TODO(b/142020079): Reenable it once the bug is fixed.\n+      # TODO(b/142020079): Re-enable it once the bug is fixed.\n       if caching_device is not None:\n         tf_logging.warning(\n             '`caching_device` does not work with mixed precision API. Ignoring '\n\n@@ -74,7 +74,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     \"\"\"Whether the current DataAdapter could handle the input x and y.\n \n     Structure wise, x and y can be single object, or list of objects if there\n-    multiple input/output, or dictionary of objects when the intput/output are\n+    multiple input/output, or dictionary of objects when the input/output are\n     named.\n \n     Args:\n\n@@ -715,7 +715,7 @@ class Functional(training_lib.Model):\n         return model\n       # The config does not contain all the information necessary to revive a\n       # Functional model. This happens when the user creates subclassed models\n-      # with a Functional constructor and has overriden the `get_config` method\n+      # with a Functional constructor and has overridden the `get_config` method\n       # to return a completely new dictionary.\n       try:\n         return cls(**config)\n@@ -769,7 +769,7 @@ class Functional(training_lib.Model):\n         for x in self.inputs])\n     input_batch_sizes.discard(None)\n     if len(input_batch_sizes) > 1:\n-      logging.warning('Found incompatiable static batch sizes among the '\n+      logging.warning('Found incompatible static batch sizes among the '\n                       f'inputs. Batch sizes: {sorted(input_batch_sizes)}')\n \n     for x in self.outputs:\n\n@@ -138,7 +138,7 @@ class FunctionalModelSlideTest(keras_parameterized.TestCase):\n     loaded_model = models.load_model(output_path)\n     self.assertEqual(model.summary(), loaded_model.summary())\n \n-    # Also make sure the orignal inputs and y can still be used to build model\n+    # Also make sure the original inputs and y can still be used to build model\n     new_model = models.Model(inputs, y)\n     # Make sure no new node is attached to layer2\n     self.assertLen(layer2.inbound_nodes, 2)\n\n@@ -1125,7 +1125,7 @@ class TrainingTest(keras_parameterized.TestCase):\n       training_module.Model([input1, input2], outputs)\n       self.assertEqual(\n           mock_warn.call_args_list[0][0][0],\n-          'Found incompatiable static batch sizes among the inputs. '\n+          'Found incompatible static batch sizes among the inputs. '\n           'Batch sizes: [2, 3]')\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -3511,7 +3511,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n \n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def DISABLED_test_add_metric_invalid_aggregation(self):\n-    # TODO(psv): Reenable test once it is fixed.\n+    # TODO(psv): Re-enable test once it is fixed.\n     x = layers_module.Input(shape=(1,))\n     y = layers_module.Dense(1, kernel_initializer='ones')(x)\n     model = training_module.Model(x, y)\n\n@@ -77,7 +77,7 @@ class RandomNormal(tf.compat.v1.random_normal_initializer):\n   Random seed behavior:\n   Also be aware that if you pass a seed to the TF2 initializer\n   API it will reuse that same seed for every single initialization\n-  (unlike the TF1 intializer)\n+  (unlike the TF1 initializer)\n \n   #### Structural Mapping to Native TF2\n \n@@ -192,7 +192,7 @@ class RandomUniform(tf.compat.v1.random_uniform_initializer):\n \n   Also be aware that if you pass a seed to the TF2 initializer\n   API it will reuse that same seed for every single initialization\n-  (unlike the TF1 intializer)\n+  (unlike the TF1 initializer)\n \n   #### Structural Mapping to Native TF2\n \n@@ -313,7 +313,7 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n   Random seed behavior:\n   Also be aware that if you pass a seed to the TF2 initializer\n   API it will reuse that same seed for every single initialization\n-  (unlike the TF1 intializer)\n+  (unlike the TF1 initializer)\n \n   #### Structural Mapping to Native TF2\n \n\n@@ -258,7 +258,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertIn(prediction0, (\"yes\", \"no\"))\n \n       prediction1 = loaded_serving_fn(\n-          tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]\n+          tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]\n       self.assertIn(prediction1, (\"yes\", \"no\"))\n \n \n\n@@ -66,7 +66,7 @@ def make_preprocessing_model(file_dir):\n   normalization = preprocessing.Normalization()\n   normalization.adapt(ds.map(lambda features, labels: features[\"float_col\"]))\n   float_out = normalization(float_in)\n-  # Lookup ints by adapting a vocab of interger IDs.\n+  # Lookup ints by adapting a vocab of integer IDs.\n   int_lookup = preprocessing.IntegerLookup()\n   int_lookup.adapt(ds.map(lambda features, labels: features[\"int_col\"]))\n   int_out = int_lookup(int_in)\n\n@@ -232,7 +232,7 @@ class TpuStrategyTest(tf.test.TestCase):\n     self.assertIn(prediction1, (\"yes\", \"no\"))\n \n     prediction2 = loaded_serving_fn(\n-        tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]\n+        tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]\n     self.assertIn(prediction2, (\"yes\", \"no\"))\n \n \n\n@@ -195,7 +195,7 @@ class Conv(Layer):\n                                        self.filters)\n \n     # compute_output_shape contains some validation logic for the input shape,\n-    # and make sure the output shape has all positive dimentions.\n+    # and make sure the output shape has all positive dimensions.\n     self.compute_output_shape(input_shape)\n \n     self.kernel = self.add_weight(\n\n@@ -114,7 +114,7 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n     predict2 = loaded_model(np.ones((20, 5, 10)))\n \n     self.assertAllClose(predict, predict2)\n-    # Make sure the model droput different value after loading\n+    # Make sure the model dropout different value after loading\n     train2 = loaded_model(np.ones((20, 5, 10)), training=True)\n     self.assertNotAllClose(train, train2)\n     self.assertIsNotNone(loaded_model.layers[1]._random_generator)\n\n@@ -215,7 +215,7 @@ class Lambda(Layer):\n           but are not tracked by said layer:\n           {variable_str}\n           The layer cannot safely ensure proper Variable reuse across multiple\n-          calls, and consquently this behavior is disallowed for safety. Lambda\n+          calls, and consequently this behavior is disallowed for safety. Lambda\n           layers are not well suited to stateful computation; instead, writing a\n           subclassed Layer is the recommend way to define layers with\n           Variables.\"\"\").format(\n\n@@ -279,7 +279,7 @@ class TFOpLambda(Layer):\n           'The following Variables were created within a Lambda layer '\n           f'({self.name}) but are not tracked by said layer: {variable_str}\\n'\n           'The layer cannot safely ensure proper Variable reuse '\n-          'across multiple calls, and consquently this behavior is disallowed '\n+          'across multiple calls, and consequently this behavior is disallowed '\n           'for safety reasons. Lambda layers are not well suited for stateful '\n           'computation; instead, writing a subclassed Layer is the recommend '\n           'way to define layers with Variables.')\n\n@@ -197,7 +197,7 @@ class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n     Args:\n       batch_input_shape: The input shape that will be used to test, including\n         the batch dimension.\n-      axis: A list of axises to normalize. Will be passed to the `axis` argument\n+      axis: A list of axes to normalize. Will be passed to the `axis` argument\n         of Layerlayer_normalization.\n       fp64_tol: The relative and absolute tolerance for float64.\n       fp32_tol: The relative and absolute tolerance for float32.\n@@ -255,7 +255,7 @@ class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n     Args:\n       batch_input_shape: The input shape that will be used to test, including\n         the batch dimension.\n-      axis: A list of axises to normalize. Will be passed to the `axis` argument\n+      axis: A list of axes to normalize. Will be passed to the `axis` argument\n         of Layerlayer_normalization.\n       fp64_tol: The relative and absolute tolerance for float64.\n       fp32_tol: The relative and absolute tolerance for float32.\n\n@@ -218,7 +218,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     if \"dtype\" not in kwargs or kwargs[\"dtype\"] is None:\n       kwargs[\"dtype\"] = tf.int64 if output_mode == INT else backend.floatx()\n     elif output_mode == \"int\" and not tf.as_dtype(kwargs[\"dtype\"]).is_integer:\n-      # Compat for when dtype was alwyas floating and ingored by the layer.\n+      # Compat for when dtype was always floating and ignored by the layer.\n       kwargs[\"dtype\"] = tf.int64\n \n     super().__init__(**kwargs)\n\n@@ -168,7 +168,7 @@ class Hashing(base_layer.Layer):\n     if 'dtype' not in kwargs or kwargs['dtype'] is None:\n       kwargs['dtype'] = tf.int64 if output_mode == INT else backend.floatx()\n     elif output_mode == 'int' and not tf.as_dtype(kwargs['dtype']).is_integer:\n-      # Compat for when dtype was alwyas floating and ingored by the layer.\n+      # Compat for when dtype was always floating and ignored by the layer.\n       kwargs['dtype'] = tf.int64\n \n     super().__init__(**kwargs)\n\n@@ -268,7 +268,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         # value of the index_lookup table.\n         self._default_value = self._oov_start_index()\n       else:\n-        # If we hav multiple OOV values, we need to do a further hashing step;\n+        # If we have multiple OOV values, we need to do a further hashing step;\n         # to make this easier, we set the OOV value to -1. (This lets us do a\n         # vectorized add and cast to boolean to determine locations where we\n         # need to do extra hashing.)\n@@ -293,7 +293,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       # to be uninitialized as a StaticHashTable cannot be initialized twice.\n       self.lookup_table = self._uninitialized_lookup_table()\n \n-    # Only set up adapt state if we did not recieve a vocab on construction.\n+    # Only set up adapt state if we did not receive a vocab on construction.\n     if not self._has_input_vocabulary:\n       # Add a custom weight handler to return the layers vocab as it's weight.\n       self._add_trackable(VocabWeightHandler(self), False)\n@@ -648,8 +648,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"Lookup table values for a dense Tensor, handling masking and OOV.\"\"\"\n     # When executing eagerly and tracing keras.Inputs, do not call lookup. This\n     # is critical for restoring SavedModel, which will first trace layer.call\n-    # and then attempt to restore the table. We need the table to be unitialized\n-    # for the restore to work, but calling the table unitialized would error.\n+    # and then attempt to restore the table. We need the table to be uninitialized\n+    # for the restore to work, but calling the table uninitialized would error.\n     if tf.executing_eagerly() and backend.is_keras_tensor(inputs):\n       lookups = tf.zeros_like(inputs, dtype=self._value_dtype)\n     else:\n\n@@ -123,7 +123,7 @@ class FunctionalPreprocessingStage(functional.Functional,\n \n   Args:\n     inputs: An input tensor (must be created via `tf.keras.Input()`), or a list,\n-      a dict, or a nested strcture of input tensors.\n+      a dict, or a nested structure of input tensors.\n     outputs: An output tensor, or a list, a dict or a nested structure of output\n       tensors.\n     name: String, optional. Name of the preprocessing stage.\n\n@@ -593,7 +593,7 @@ class Bidirectional(Wrapper):\n     additional_inputs = []\n     additional_specs = []\n     if initial_state is not None:\n-      # Check if `initial_state` can be splitted into half\n+      # Check if `initial_state` can be split into half\n       num_states = len(initial_state)\n       if num_states % 2 > 0:\n         raise ValueError(\n\n@@ -1695,7 +1695,7 @@ def _ragged_tensor_categorical_crossentropy(y_true,\n   When used by CategoricalCrossentropy() with the default reduction\n   (SUM_OVER_BATCH_SIZE), the reduction averages the loss over the\n   number of elements independent of the batch. E.g. if the RaggedTensor\n-  has 2 batches with [2, 1] values respectivly the resulting loss is\n+  has 2 batches with [2, 1] values respectively the resulting loss is\n   the sum of the individual loss values divided by 3.\n   \"\"\"\n   fn = functools.partial(\n\n@@ -2146,12 +2146,12 @@ class AUC(Metric):\n   Usage with `compile()` API:\n \n   ```python\n-  # Reports the AUC of a model outputing a probability.\n+  # Reports the AUC of a model outputting a probability.\n   model.compile(optimizer='sgd',\n                 loss=tf.keras.losses.BinaryCrossentropy(),\n                 metrics=[tf.keras.metrics.AUC()])\n \n-  # Reports the AUC of a model outputing a logit.\n+  # Reports the AUC of a model outputting a logit.\n   model.compile(optimizer='sgd',\n                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                 metrics=[tf.keras.metrics.AUC(from_logits=True)])\n\n@@ -457,7 +457,7 @@ def clone_model(model, input_tensors=None, clone_function=None):\n           model, input_tensors=input_tensors, layer_fn=clone_function)\n \n \n-# \"Clone\" a subclassed model by reseting all of the attributes.\n+# \"Clone\" a subclassed model by resetting all of the attributes.\n def _in_place_subclassed_model_reset(model):\n   \"\"\"Substitute for model cloning that works for subclassed models.\n \n\n@@ -278,7 +278,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     scope_name = self._name or \"optimizer\"\n     with tf.name_scope(scope_name):\n       with tf.init_scope():\n-        # Lift variable creation to init scope to avoid enviroment issues.\n+        # Lift variable creation to init scope to avoid environment issues.\n         self.build(trainable_variables)\n     grads = self._clip_gradients(grads)\n     grads_and_vars = list(zip(grads, trainable_variables))\n\n@@ -1345,7 +1345,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       self, slot_variable_position, slot_name, variable):\n     \"\"\"Returns the slot variable that should have a value restored into it.\n \n-    It is up to the caller to retore the value into the slot variable if a valid\n+    It is up to the caller to restore the value into the slot variable if a valid\n     slot variable is returned.\n \n     Called when a variable which has an associated slot variable is created or\n@@ -1463,7 +1463,7 @@ class RestoredOptimizer(OptimizerV2):\n   Holds slot variables and hyperparameters when an optimizer is restored from a\n   SavedModel. These variables may be referenced in functions along with ops\n   created by the original optimizer, but currently we do not support using the\n-  optimizer object iself (e.g. through `apply_gradients`).\n+  optimizer object itself (e.g. through `apply_gradients`).\n   \"\"\"\n   # TODO(allenl): Make the restored optimizer functional by tracing its apply\n   # methods.\n\n@@ -35,7 +35,7 @@ def _check_penalty_number(x):\n   if math.isinf(x) or math.isnan(x):\n     raise ValueError(\n         f'Value: {x} is not a valid regularization penalty number, '\n-        'an infinity nubmer or NaN are not valid value')\n+        'an infinity number or NaN are not valid value')\n \n \n def _none_to_default(inputs, default):\n\n@@ -170,7 +170,7 @@ class VocabularySavedModelSaver(LayerSavedModelSaver):\n   vocab as part of the config until saving, when we need to clear it to avoid\n   initializing a StaticHashTable twice (once when restoring the config and once\n   when restoring restoring module resources). After clearing the vocab, we\n-  presist a property to the layer indicating it was constructed with a vocab.\n+  persist a property to the layer indicating it was constructed with a vocab.\n   \"\"\"\n \n   @property\n\n@@ -524,7 +524,7 @@ class KerasObjectLoader:\n           generic_utils.serialize_keras_class_and_config(\n               class_name, config, shared_object_id=shared_object_id))\n     except (TypeError, KeyError) as e:\n-      # A name conflict has occured. The `class_name` is in the Keras native\n+      # A name conflict has occurred. The `class_name` is in the Keras native\n       # framework; however, the value in the framework is different from the\n       # user's class definition which confuses the KerasObjectLoader.\n       builtin_layer = layers_module.get_builtin_layer(class_name)\n\n@@ -584,7 +584,7 @@ class SequenceEnqueuer:\n       enqueuer.stop()\n   ```\n \n-  The `enqueuer.get()` should be an infinite stream of datas.\n+  The `enqueuer.get()` should be an infinite stream of data.\n   \"\"\"\n \n   def __init__(self, sequence,\n\n@@ -1038,7 +1038,7 @@ class Progbar:\n     if current:\n       # there are a few special scenarios here:\n       # 1) somebody is calling the progress bar without ever supplying step 1\n-      # 2) somebody is calling the progress bar and supplies step one mulitple\n+      # 2) somebody is calling the progress bar and supplies step one multiple\n       #    times, e.g. as part of a finalizing call\n       # in these cases, we just fall back to the simple calculation\n       if self._time_after_first_step is not None and current > 1:\n\n@@ -176,5 +176,5 @@ class DistributeKplTestUtils(tf.test.TestCase):\n     self.assertIn(prediction0.numpy().decode(\"UTF-8\"), (\"yes\", \"no\"))\n \n     prediction1 = loaded_serving_fn(\n-        tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]\n+        tf.constant([\"ironman\", \"ironman\", \"unknown\"]))[\"output_0\"]\n     self.assertIn(prediction1.numpy().decode(\"UTF-8\"), (\"yes\", \"no\"))\n\n@@ -466,7 +466,7 @@ def is_evenly_distributed_thresholds(thresholds):\n \n   We could leverage evenly distributed thresholds to use less memory when\n   calculate metrcis like AUC where each individual threshold need to be\n-  evaluted.\n+  evaluated.\n \n   Args:\n     thresholds: A python list or tuple, or 1D numpy array whose value is ranged\n\n@@ -74,7 +74,7 @@ def get_layer_index_bound_by_layer_name(model, layer_names):\n     layer_names: unique name of layer of the model, type(str)\n \n   Returns:\n-    retun the index value of layer based on its unique name (layer_names)\n+    return the index value of layer based on its unique name (layer_names)\n   \"\"\"\n   lower_index = []\n   upper_index = []\n@@ -186,7 +186,7 @@ def model_to_dot(model,\n     layer_range = get_layer_index_bound_by_layer_name(model, layer_range)\n     if layer_range[0] < 0 or layer_range[1] > len(model.layers):\n       raise ValueError('Both values in layer_range should be in range (0, '\n-                       f'{len(model.layers)}. Recieved: {layer_range}')\n+                       f'{len(model.layers)}. Received: {layer_range}')\n \n   sub_n_first_node = {}\n   sub_n_last_node = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
