{"custom_id": "scrapy#377bea4976d3a22da6f2a61e7ee42522d005609c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 20 | Files Changed: 4 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 78 | Churn Cumulative: 608 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ Adaptors related with extraction of data\n \"\"\"\n \n import urlparse\n+from scrapy.http import Response\n from scrapy.utils.python import flatten\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n \n@@ -50,8 +51,7 @@ class ExtractImages(object):\n     Output: list of unicodes\n     \"\"\"\n     def __init__(self, response=None, base_url=None):\n-        self.response = response\n-        self.base_url = base_url\n+        self.base_url = response.url if response else base_url\n \n     def extract_from_xpath(self, selector):\n         ret = []\n@@ -75,12 +75,9 @@ class ExtractImages(object):\n \n         return ret\n \n-    def __call__(self, locations):\n-        if isinstance(locations, tuple) and len(locations) > 1:\n-            self.base_url = locations[1]\n-            locations = locations[0]\n-\n-        if not self.response and not self.base_url:\n+    def __call__(self, (locations, base_url)):\n+        self.base_url = base_url.url if isinstance(base_url, Response) else base_url\n+        if not self.base_url:\n             raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n         \n         rel_links = []\n@@ -90,6 +87,4 @@ class ExtractImages(object):\n             else:\n                 rel_links.append(location)\n         rel_links = extract(rel_links)\n-\n-        base_url = self.base_url or self.response.url\n-        return [urlparse.urljoin(base_url, link) for link in rel_links]\n+        return [urlparse.urljoin(self.base_url, link) for link in rel_links]\n\n@@ -0,0 +1,20 @@\n+\"\"\"\n+This pipeline sets the guid for each scraped item.\n+It should be the previous to the validation pipeline (which is the last one).\n+\"\"\"\n+\n+from pydispatch import dispatcher\n+from scrapy.core import signals\n+\n+class SetGUIDPipeline(object):\n+    def __init__(self):\n+        self.spider = None\n+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n+\n+    def domain_opened(self, domain, spider):\n+        self.spider = spider\n+\n+    def process_item(self, domain, response, item):\n+        self.spider.set_guid(item)\n+        return item\n+\n\n@@ -6,13 +6,19 @@ basic crawling.\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.core.exceptions import UsageError\n+from scrapy.utils.misc import hash_values\n \n class BasicSpider(BaseSpider):\n-    \"\"\"BasicSpider extends BaseSpider by providing support for simple crawling\n+    \"\"\"\n+    BasicSpider extends BaseSpider by providing support for simple crawling\n     by following links contained in web pages. \n     \n     With BasicSpider you can write a basic spider very easily and quickly. For\n-    more information refer to the Scrapy tutorial\"\"\"\n+    more information refer to the Scrapy tutorial\n+    \"\"\"\n+\n+    gen_guid_attribs = ['supplier', 'site_id']\n+    gen_variant_guid_attribs = ['site_id']\n \n     def __init__(self):\n         super(BaseSpider, self).__init__()\n@@ -23,11 +29,7 @@ class BasicSpider(BaseSpider):\n             if attr.startswith('links_'):\n                 suffix = attr.split('_', 1)[1]\n                 value = getattr(self, attr)\n-                try:\n-                    callback = getattr(self, 'parse_%s' % suffix)\n-                except AttributeError:\n-                    raise UsageError(\"%s defines links_%s but doesn't provide a parse_%s method\" % \\\n-                                     (type(self).__name__, suffix, suffix))\n+                callback = getattr(self, 'parse_%s' % suffix, None)\n                 self._links_callback.append((value, callback))\n \n     def parse(self, response):\n@@ -47,7 +49,7 @@ class BasicSpider(BaseSpider):\n             for url, link_text in lx.extract_urls(response).iteritems():\n                 links_to_follow[url] = (callback, link_text)\n \n-        for url, cb_link in links_to_follow.iteritems():\n+        for url, (callback, link_text) in links_to_follow.iteritems():\n             request = Request(url=url, link_text=link_text)\n             request.append_callback(self._parse_wrapper, callback)\n             res.append(request)\n@@ -55,6 +57,9 @@ class BasicSpider(BaseSpider):\n \n     def _parse_wrapper(self, response, callback):\n         res = self._links_to_follow(response)\n-        res += callback(response) or ()\n+        res += callback(response) if callback else ()\n         return res\n \n+    def set_guid(self, item):\n+        item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n+\n\n@@ -2,6 +2,7 @@\n Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n import re\n+import sha\n \n from twisted.internet import defer\n \n@@ -95,3 +96,20 @@ def extract_regex(regex, text, encoding):\n         return [remove_entities(s, keep=['lt', 'amp']) for s in strings]\n     else:\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n+\n+def hash_values(*values):\n+    \"\"\"Hash a series of values. \n+    \n+    For example:\n+    >>> hash_values('some', 'values', 'to', 'hash')\n+    'f37f5dc65beaaea35af05e16e26d439fd150c576'\n+    \"\"\"\n+    hash = sha.new()\n+    for value in values:\n+        if value is None:\n+            message = \"hash_values was passed None at argument index %d. This is a bug in the calling code\" \\\n+                    % list(values).index(None)\n+            raise UsageError(message)\n+        hash.update(value)\n+    return hash.hexdigest()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c73dc5ad6c96f7f22c10c22f6e04375915848c02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 8 | Churn Cumulative: 67 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import re\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n+from scrapy.utils.url import canonicalize_url\n from scrapy.utils.misc import extract_regex\n from scrapy.utils.python import flatten\n \n@@ -26,6 +27,13 @@ def strip_list(value):\n def drop_empty(value):\n     return [ v for v in value if v ]\n \n+def normalize_urls(value):\n+    if hasattr(value, '__iter__'):\n+        return [canonicalize_url(url) for url in value]\n+    elif isinstance(value, basestring):\n+        return canonicalize_url(value)\n+    return ''\n+\n class Delist(object):\n     \"\"\"\n     Input: iterable with strings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d61cd6075631a55f6351196944ae5ad7af2baf5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 21 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 37 | Churn Cumulative: 296 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1 +1 @@\n-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline\n\n@@ -5,8 +5,11 @@ useful in some Scrapy implementations\n \n import hashlib\n \n+from pydispatch import dispatcher\n from pprint import PrettyPrinter\n+\n from scrapy.item import ScrapedItem, ItemDelta\n+from scrapy.core import signals\n from scrapy.core.exceptions import UsageError, DropItem\n \n class ValidationError(DropItem):\n@@ -26,6 +29,18 @@ class ValidationPipeline(object):\n         item.validate()\n         return item\n \n+class SetGUIDPipeline(object):\n+    def __init__(self):\n+        self.spider = None\n+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n+\n+    def domain_opened(self, domain, spider):\n+        self.spider = spider\n+\n+    def process_item(self, domain, response, item):\n+        self.spider.set_guid(item)\n+        return item\n+\n class RobustScrapedItem(ScrapedItem):\n     \"\"\"\n     A more robust scraped item class with a built-in validation mechanism and \n\n@@ -1,20 +0,0 @@\n-\"\"\"\n-This pipeline sets the guid for each scraped item.\n-It should be the previous to the validation pipeline (which is the last one).\n-\"\"\"\n-\n-from pydispatch import dispatcher\n-from scrapy.core import signals\n-\n-class SetGUIDPipeline(object):\n-    def __init__(self):\n-        self.spider = None\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-\n-    def domain_opened(self, domain, spider):\n-        self.spider = spider\n-\n-    def process_item(self, domain, response, item):\n-        self.spider.set_guid(item)\n-        return item\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9882679bbbf9adab1b78fe190153a91c4b455f4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 115 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,8 +75,7 @@ class ExtractImages(object):\n \n         return ret\n \n-    def __call__(self, (locations, base_url)):\n-        self.base_url = base_url.url if isinstance(base_url, Response) else base_url\n+    def __call__(self, locations):\n         if not self.base_url:\n             raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')\n         \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a2e288f22d8f1f8bcc931ad0ef0dc5f21b2bd05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 319 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 323 | Churn Cumulative: 906 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1 @@\n+from scrapy.contrib.codecs import x_mac_roman\n\n@@ -0,0 +1,314 @@\n+\"\"\"\n+Python Character Mapping Codec macintosh generated from 'VENDORS/APPLE/ROMAN.TXT' with gencodec.py.\n+\"\"\"\n+\n+import codecs\n+\n+### Codec APIs\n+\n+class Codec(codecs.Codec):\n+\n+    def encode(self,input,errors='strict'):\n+        return codecs.charmap_encode(input,errors,encoding_table)\n+\n+    def decode(self,input,errors='strict'):\n+        return codecs.charmap_decode(input,errors,decoding_table)\n+\n+class IncrementalEncoder(codecs.IncrementalEncoder):\n+    def encode(self, input, final=False):\n+        return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n+\n+class IncrementalDecoder(codecs.IncrementalDecoder):\n+    def decode(self, input, final=False):\n+        return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n+\n+class StreamWriter(Codec,codecs.StreamWriter):\n+    pass\n+\n+class StreamReader(Codec,codecs.StreamReader):\n+    pass\n+\n+### encodings module API\n+\n+def getregentry():\n+    return codecs.CodecInfo(\n+        name='x-mac-roman',\n+        encode=Codec().encode,\n+        decode=Codec().decode,\n+        incrementalencoder=IncrementalEncoder,\n+        incrementaldecoder=IncrementalDecoder,\n+        streamreader=StreamReader,\n+        streamwriter=StreamWriter,\n+    )\n+\n+\n+### Decoding Table\n+\n+decoding_table = (\n+    u'\\x00'     #  0x00 -> CONTROL CHARACTER\n+    u'\\x01'     #  0x01 -> CONTROL CHARACTER\n+    u'\\x02'     #  0x02 -> CONTROL CHARACTER\n+    u'\\x03'     #  0x03 -> CONTROL CHARACTER\n+    u'\\x04'     #  0x04 -> CONTROL CHARACTER\n+    u'\\x05'     #  0x05 -> CONTROL CHARACTER\n+    u'\\x06'     #  0x06 -> CONTROL CHARACTER\n+    u'\\x07'     #  0x07 -> CONTROL CHARACTER\n+    u'\\x08'     #  0x08 -> CONTROL CHARACTER\n+    u'\\t'       #  0x09 -> CONTROL CHARACTER\n+    u'\\n'       #  0x0A -> CONTROL CHARACTER\n+    u'\\x0b'     #  0x0B -> CONTROL CHARACTER\n+    u'\\x0c'     #  0x0C -> CONTROL CHARACTER\n+    u'\\r'       #  0x0D -> CONTROL CHARACTER\n+    u'\\x0e'     #  0x0E -> CONTROL CHARACTER\n+    u'\\x0f'     #  0x0F -> CONTROL CHARACTER\n+    u'\\x10'     #  0x10 -> CONTROL CHARACTER\n+    u'\\x11'     #  0x11 -> CONTROL CHARACTER\n+    u'\\x12'     #  0x12 -> CONTROL CHARACTER\n+    u'\\x13'     #  0x13 -> CONTROL CHARACTER\n+    u'\\x14'     #  0x14 -> CONTROL CHARACTER\n+    u'\\x15'     #  0x15 -> CONTROL CHARACTER\n+    u'\\x16'     #  0x16 -> CONTROL CHARACTER\n+    u'\\x17'     #  0x17 -> CONTROL CHARACTER\n+    u'\\x18'     #  0x18 -> CONTROL CHARACTER\n+    u'\\x19'     #  0x19 -> CONTROL CHARACTER\n+    u'\\x1a'     #  0x1A -> CONTROL CHARACTER\n+    u'\\x1b'     #  0x1B -> CONTROL CHARACTER\n+    u'\\x1c'     #  0x1C -> CONTROL CHARACTER\n+    u'\\x1d'     #  0x1D -> CONTROL CHARACTER\n+    u'\\x1e'     #  0x1E -> CONTROL CHARACTER\n+    u'\\x1f'     #  0x1F -> CONTROL CHARACTER\n+    u' '        #  0x20 -> SPACE\n+    u'!'        #  0x21 -> EXCLAMATION MARK\n+    u'\"'        #  0x22 -> QUOTATION MARK\n+    u'#'        #  0x23 -> NUMBER SIGN\n+    u'$'        #  0x24 -> DOLLAR SIGN\n+    u'%'        #  0x25 -> PERCENT SIGN\n+    u'&'        #  0x26 -> AMPERSAND\n+    u\"'\"        #  0x27 -> APOSTROPHE\n+    u'('        #  0x28 -> LEFT PARENTHESIS\n+    u')'        #  0x29 -> RIGHT PARENTHESIS\n+    u'*'        #  0x2A -> ASTERISK\n+    u'+'        #  0x2B -> PLUS SIGN\n+    u','        #  0x2C -> COMMA\n+    u'-'        #  0x2D -> HYPHEN-MINUS\n+    u'.'        #  0x2E -> FULL STOP\n+    u'/'        #  0x2F -> SOLIDUS\n+    u'0'        #  0x30 -> DIGIT ZERO\n+    u'1'        #  0x31 -> DIGIT ONE\n+    u'2'        #  0x32 -> DIGIT TWO\n+    u'3'        #  0x33 -> DIGIT THREE\n+    u'4'        #  0x34 -> DIGIT FOUR\n+    u'5'        #  0x35 -> DIGIT FIVE\n+    u'6'        #  0x36 -> DIGIT SIX\n+    u'7'        #  0x37 -> DIGIT SEVEN\n+    u'8'        #  0x38 -> DIGIT EIGHT\n+    u'9'        #  0x39 -> DIGIT NINE\n+    u':'        #  0x3A -> COLON\n+    u';'        #  0x3B -> SEMICOLON\n+    u'<'        #  0x3C -> LESS-THAN SIGN\n+    u'='        #  0x3D -> EQUALS SIGN\n+    u'>'        #  0x3E -> GREATER-THAN SIGN\n+    u'?'        #  0x3F -> QUESTION MARK\n+    u'@'        #  0x40 -> COMMERCIAL AT\n+    u'A'        #  0x41 -> LATIN CAPITAL LETTER A\n+    u'B'        #  0x42 -> LATIN CAPITAL LETTER B\n+    u'C'        #  0x43 -> LATIN CAPITAL LETTER C\n+    u'D'        #  0x44 -> LATIN CAPITAL LETTER D\n+    u'E'        #  0x45 -> LATIN CAPITAL LETTER E\n+    u'F'        #  0x46 -> LATIN CAPITAL LETTER F\n+    u'G'        #  0x47 -> LATIN CAPITAL LETTER G\n+    u'H'        #  0x48 -> LATIN CAPITAL LETTER H\n+    u'I'        #  0x49 -> LATIN CAPITAL LETTER I\n+    u'J'        #  0x4A -> LATIN CAPITAL LETTER J\n+    u'K'        #  0x4B -> LATIN CAPITAL LETTER K\n+    u'L'        #  0x4C -> LATIN CAPITAL LETTER L\n+    u'M'        #  0x4D -> LATIN CAPITAL LETTER M\n+    u'N'        #  0x4E -> LATIN CAPITAL LETTER N\n+    u'O'        #  0x4F -> LATIN CAPITAL LETTER O\n+    u'P'        #  0x50 -> LATIN CAPITAL LETTER P\n+    u'Q'        #  0x51 -> LATIN CAPITAL LETTER Q\n+    u'R'        #  0x52 -> LATIN CAPITAL LETTER R\n+    u'S'        #  0x53 -> LATIN CAPITAL LETTER S\n+    u'T'        #  0x54 -> LATIN CAPITAL LETTER T\n+    u'U'        #  0x55 -> LATIN CAPITAL LETTER U\n+    u'V'        #  0x56 -> LATIN CAPITAL LETTER V\n+    u'W'        #  0x57 -> LATIN CAPITAL LETTER W\n+    u'X'        #  0x58 -> LATIN CAPITAL LETTER X\n+    u'Y'        #  0x59 -> LATIN CAPITAL LETTER Y\n+    u'Z'        #  0x5A -> LATIN CAPITAL LETTER Z\n+    u'['        #  0x5B -> LEFT SQUARE BRACKET\n+    u'\\\\'       #  0x5C -> REVERSE SOLIDUS\n+    u']'        #  0x5D -> RIGHT SQUARE BRACKET\n+    u'^'        #  0x5E -> CIRCUMFLEX ACCENT\n+    u'_'        #  0x5F -> LOW LINE\n+    u'`'        #  0x60 -> GRAVE ACCENT\n+    u'a'        #  0x61 -> LATIN SMALL LETTER A\n+    u'b'        #  0x62 -> LATIN SMALL LETTER B\n+    u'c'        #  0x63 -> LATIN SMALL LETTER C\n+    u'd'        #  0x64 -> LATIN SMALL LETTER D\n+    u'e'        #  0x65 -> LATIN SMALL LETTER E\n+    u'f'        #  0x66 -> LATIN SMALL LETTER F\n+    u'g'        #  0x67 -> LATIN SMALL LETTER G\n+    u'h'        #  0x68 -> LATIN SMALL LETTER H\n+    u'i'        #  0x69 -> LATIN SMALL LETTER I\n+    u'j'        #  0x6A -> LATIN SMALL LETTER J\n+    u'k'        #  0x6B -> LATIN SMALL LETTER K\n+    u'l'        #  0x6C -> LATIN SMALL LETTER L\n+    u'm'        #  0x6D -> LATIN SMALL LETTER M\n+    u'n'        #  0x6E -> LATIN SMALL LETTER N\n+    u'o'        #  0x6F -> LATIN SMALL LETTER O\n+    u'p'        #  0x70 -> LATIN SMALL LETTER P\n+    u'q'        #  0x71 -> LATIN SMALL LETTER Q\n+    u'r'        #  0x72 -> LATIN SMALL LETTER R\n+    u's'        #  0x73 -> LATIN SMALL LETTER S\n+    u't'        #  0x74 -> LATIN SMALL LETTER T\n+    u'u'        #  0x75 -> LATIN SMALL LETTER U\n+    u'v'        #  0x76 -> LATIN SMALL LETTER V\n+    u'w'        #  0x77 -> LATIN SMALL LETTER W\n+    u'x'        #  0x78 -> LATIN SMALL LETTER X\n+    u'y'        #  0x79 -> LATIN SMALL LETTER Y\n+    u'z'        #  0x7A -> LATIN SMALL LETTER Z\n+    u'{'        #  0x7B -> LEFT CURLY BRACKET\n+    u'|'        #  0x7C -> VERTICAL LINE\n+    u'}'        #  0x7D -> RIGHT CURLY BRACKET\n+    u'~'        #  0x7E -> TILDE\n+    u'\\x7f'     #  0x7F -> CONTROL CHARACTER\n+    u'\\xc4'     #  0x80 -> LATIN CAPITAL LETTER A WITH DIAERESIS\n+    u'\\xc5'     #  0x81 -> LATIN CAPITAL LETTER A WITH RING ABOVE\n+    u'\\xc7'     #  0x82 -> LATIN CAPITAL LETTER C WITH CEDILLA\n+    u'\\xc9'     #  0x83 -> LATIN CAPITAL LETTER E WITH ACUTE\n+    u'\\xd1'     #  0x84 -> LATIN CAPITAL LETTER N WITH TILDE\n+    u'\\xd6'     #  0x85 -> LATIN CAPITAL LETTER O WITH DIAERESIS\n+    u'\\xdc'     #  0x86 -> LATIN CAPITAL LETTER U WITH DIAERESIS\n+    u'\\xe1'     #  0x87 -> LATIN SMALL LETTER A WITH ACUTE\n+    u'\\xe0'     #  0x88 -> LATIN SMALL LETTER A WITH GRAVE\n+    u'\\xe2'     #  0x89 -> LATIN SMALL LETTER A WITH CIRCUMFLEX\n+    u'\\xe4'     #  0x8A -> LATIN SMALL LETTER A WITH DIAERESIS\n+    u'\\xe3'     #  0x8B -> LATIN SMALL LETTER A WITH TILDE\n+    u'\\xe5'     #  0x8C -> LATIN SMALL LETTER A WITH RING ABOVE\n+    u'\\xe7'     #  0x8D -> LATIN SMALL LETTER C WITH CEDILLA\n+    u'\\xe9'     #  0x8E -> LATIN SMALL LETTER E WITH ACUTE\n+    u'\\xe8'     #  0x8F -> LATIN SMALL LETTER E WITH GRAVE\n+    u'\\xea'     #  0x90 -> LATIN SMALL LETTER E WITH CIRCUMFLEX\n+    u'\\xeb'     #  0x91 -> LATIN SMALL LETTER E WITH DIAERESIS\n+    u'\\xed'     #  0x92 -> LATIN SMALL LETTER I WITH ACUTE\n+    u'\\xec'     #  0x93 -> LATIN SMALL LETTER I WITH GRAVE\n+    u'\\xee'     #  0x94 -> LATIN SMALL LETTER I WITH CIRCUMFLEX\n+    u'\\xef'     #  0x95 -> LATIN SMALL LETTER I WITH DIAERESIS\n+    u'\\xf1'     #  0x96 -> LATIN SMALL LETTER N WITH TILDE\n+    u'\\xf3'     #  0x97 -> LATIN SMALL LETTER O WITH ACUTE\n+    u'\\xf2'     #  0x98 -> LATIN SMALL LETTER O WITH GRAVE\n+    u'\\xf4'     #  0x99 -> LATIN SMALL LETTER O WITH CIRCUMFLEX\n+    u'\\xf6'     #  0x9A -> LATIN SMALL LETTER O WITH DIAERESIS\n+    u'\\xf5'     #  0x9B -> LATIN SMALL LETTER O WITH TILDE\n+    u'\\xfa'     #  0x9C -> LATIN SMALL LETTER U WITH ACUTE\n+    u'\\xf9'     #  0x9D -> LATIN SMALL LETTER U WITH GRAVE\n+    u'\\xfb'     #  0x9E -> LATIN SMALL LETTER U WITH CIRCUMFLEX\n+    u'\\xfc'     #  0x9F -> LATIN SMALL LETTER U WITH DIAERESIS\n+    u'\\u2020'   #  0xA0 -> DAGGER\n+    u'\\xb0'     #  0xA1 -> DEGREE SIGN\n+    u'\\xa2'     #  0xA2 -> CENT SIGN\n+    u'\\xa3'     #  0xA3 -> POUND SIGN\n+    u'\\xa7'     #  0xA4 -> SECTION SIGN\n+    u'\\u2022'   #  0xA5 -> BULLET\n+    u'\\xb6'     #  0xA6 -> PILCROW SIGN\n+    u'\\xdf'     #  0xA7 -> LATIN SMALL LETTER SHARP S\n+    u'\\xae'     #  0xA8 -> REGISTERED SIGN\n+    u'\\xa9'     #  0xA9 -> COPYRIGHT SIGN\n+    u'\\u2122'   #  0xAA -> TRADE MARK SIGN\n+    u'\\xb4'     #  0xAB -> ACUTE ACCENT\n+    u'\\xa8'     #  0xAC -> DIAERESIS\n+    u'\\u2260'   #  0xAD -> NOT EQUAL TO\n+    u'\\xc6'     #  0xAE -> LATIN CAPITAL LETTER AE\n+    u'\\xd8'     #  0xAF -> LATIN CAPITAL LETTER O WITH STROKE\n+    u'\\u221e'   #  0xB0 -> INFINITY\n+    u'\\xb1'     #  0xB1 -> PLUS-MINUS SIGN\n+    u'\\u2264'   #  0xB2 -> LESS-THAN OR EQUAL TO\n+    u'\\u2265'   #  0xB3 -> GREATER-THAN OR EQUAL TO\n+    u'\\xa5'     #  0xB4 -> YEN SIGN\n+    u'\\xb5'     #  0xB5 -> MICRO SIGN\n+    u'\\u2202'   #  0xB6 -> PARTIAL DIFFERENTIAL\n+    u'\\u2211'   #  0xB7 -> N-ARY SUMMATION\n+    u'\\u220f'   #  0xB8 -> N-ARY PRODUCT\n+    u'\\u03c0'   #  0xB9 -> GREEK SMALL LETTER PI\n+    u'\\u222b'   #  0xBA -> INTEGRAL\n+    u'\\xaa'     #  0xBB -> FEMININE ORDINAL INDICATOR\n+    u'\\xba'     #  0xBC -> MASCULINE ORDINAL INDICATOR\n+    u'\\u03a9'   #  0xBD -> GREEK CAPITAL LETTER OMEGA\n+    u'\\xe6'     #  0xBE -> LATIN SMALL LETTER AE\n+    u'\\xf8'     #  0xBF -> LATIN SMALL LETTER O WITH STROKE\n+    u'\\xbf'     #  0xC0 -> INVERTED QUESTION MARK\n+    u'\\xa1'     #  0xC1 -> INVERTED EXCLAMATION MARK\n+    u'\\xac'     #  0xC2 -> NOT SIGN\n+    u'\\u221a'   #  0xC3 -> SQUARE ROOT\n+    u'\\u0192'   #  0xC4 -> LATIN SMALL LETTER F WITH HOOK\n+    u'\\u2248'   #  0xC5 -> ALMOST EQUAL TO\n+    u'\\u2206'   #  0xC6 -> INCREMENT\n+    u'\\xab'     #  0xC7 -> LEFT-POINTING DOUBLE ANGLE QUOTATION MARK\n+    u'\\xbb'     #  0xC8 -> RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK\n+    u'\\u2026'   #  0xC9 -> HORIZONTAL ELLIPSIS\n+    u'\\xa0'     #  0xCA -> NO-BREAK SPACE\n+    u'\\xc0'     #  0xCB -> LATIN CAPITAL LETTER A WITH GRAVE\n+    u'\\xc3'     #  0xCC -> LATIN CAPITAL LETTER A WITH TILDE\n+    u'\\xd5'     #  0xCD -> LATIN CAPITAL LETTER O WITH TILDE\n+    u'\\u0152'   #  0xCE -> LATIN CAPITAL LIGATURE OE\n+    u'\\u0153'   #  0xCF -> LATIN SMALL LIGATURE OE\n+    u'\\u2013'   #  0xD0 -> EN DASH\n+    u'\\u2014'   #  0xD1 -> EM DASH\n+    u'\\u201c'   #  0xD2 -> LEFT DOUBLE QUOTATION MARK\n+    u'\\u201d'   #  0xD3 -> RIGHT DOUBLE QUOTATION MARK\n+    u'\\u2018'   #  0xD4 -> LEFT SINGLE QUOTATION MARK\n+    u'\\u2019'   #  0xD5 -> RIGHT SINGLE QUOTATION MARK\n+    u'\\xf7'     #  0xD6 -> DIVISION SIGN\n+    u'\\u25ca'   #  0xD7 -> LOZENGE\n+    u'\\xff'     #  0xD8 -> LATIN SMALL LETTER Y WITH DIAERESIS\n+    u'\\u0178'   #  0xD9 -> LATIN CAPITAL LETTER Y WITH DIAERESIS\n+    u'\\u2044'   #  0xDA -> FRACTION SLASH\n+    u'\\u20ac'   #  0xDB -> EURO SIGN\n+    u'\\u2039'   #  0xDC -> SINGLE LEFT-POINTING ANGLE QUOTATION MARK\n+    u'\\u203a'   #  0xDD -> SINGLE RIGHT-POINTING ANGLE QUOTATION MARK\n+    u'\\ufb01'   #  0xDE -> LATIN SMALL LIGATURE FI\n+    u'\\ufb02'   #  0xDF -> LATIN SMALL LIGATURE FL\n+    u'\\u2021'   #  0xE0 -> DOUBLE DAGGER\n+    u'\\xb7'     #  0xE1 -> MIDDLE DOT\n+    u'\\u201a'   #  0xE2 -> SINGLE LOW-9 QUOTATION MARK\n+    u'\\u201e'   #  0xE3 -> DOUBLE LOW-9 QUOTATION MARK\n+    u'\\u2030'   #  0xE4 -> PER MILLE SIGN\n+    u'\\xc2'     #  0xE5 -> LATIN CAPITAL LETTER A WITH CIRCUMFLEX\n+    u'\\xca'     #  0xE6 -> LATIN CAPITAL LETTER E WITH CIRCUMFLEX\n+    u'\\xc1'     #  0xE7 -> LATIN CAPITAL LETTER A WITH ACUTE\n+    u'\\xcb'     #  0xE8 -> LATIN CAPITAL LETTER E WITH DIAERESIS\n+    u'\\xc8'     #  0xE9 -> LATIN CAPITAL LETTER E WITH GRAVE\n+    u'\\xcd'     #  0xEA -> LATIN CAPITAL LETTER I WITH ACUTE\n+    u'\\xce'     #  0xEB -> LATIN CAPITAL LETTER I WITH CIRCUMFLEX\n+    u'\\xcf'     #  0xEC -> LATIN CAPITAL LETTER I WITH DIAERESIS\n+    u'\\xcc'     #  0xED -> LATIN CAPITAL LETTER I WITH GRAVE\n+    u'\\xd3'     #  0xEE -> LATIN CAPITAL LETTER O WITH ACUTE\n+    u'\\xd4'     #  0xEF -> LATIN CAPITAL LETTER O WITH CIRCUMFLEX\n+    u'\\uf8ff'   #  0xF0 -> Apple logo\n+    u'\\xd2'     #  0xF1 -> LATIN CAPITAL LETTER O WITH GRAVE\n+    u'\\xda'     #  0xF2 -> LATIN CAPITAL LETTER U WITH ACUTE\n+    u'\\xdb'     #  0xF3 -> LATIN CAPITAL LETTER U WITH CIRCUMFLEX\n+    u'\\xd9'     #  0xF4 -> LATIN CAPITAL LETTER U WITH GRAVE\n+    u'\\u0131'   #  0xF5 -> LATIN SMALL LETTER DOTLESS I\n+    u'\\u02c6'   #  0xF6 -> MODIFIER LETTER CIRCUMFLEX ACCENT\n+    u'\\u02dc'   #  0xF7 -> SMALL TILDE\n+    u'\\xaf'     #  0xF8 -> MACRON\n+    u'\\u02d8'   #  0xF9 -> BREVE\n+    u'\\u02d9'   #  0xFA -> DOT ABOVE\n+    u'\\u02da'   #  0xFB -> RING ABOVE\n+    u'\\xb8'     #  0xFC -> CEDILLA\n+    u'\\u02dd'   #  0xFD -> DOUBLE ACUTE ACCENT\n+    u'\\u02db'   #  0xFE -> OGONEK\n+    u'\\u02c7'   #  0xFF -> CARON\n+)\n+\n+### Encoding table\n+encoding_table=codecs.charmap_build(decoding_table)\n+\n+### Registering the codec\n+def x_mac_roman_search_function(name):\n+    if name == 'x-mac-roman':\n+       return getregentry()\n+\n+codecs.register(x_mac_roman_search_function)\n\n@@ -8,19 +8,19 @@ from twisted.internet import defer, reactor, task\n from twisted.python.failure import Failure\n from pydispatch import dispatcher\n \n-from scrapy.core import signals\n from scrapy import log\n+from scrapy.conf import settings\n+from scrapy.core import signals\n from scrapy.core.scheduler import Scheduler\n from scrapy.core.downloader import Downloader\n-from scrapy.http import Response, Request\n from scrapy.core.exceptions import IgnoreRequest, HttpException, DontCloseDomain\n+from scrapy.http import Response, Request\n from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipeline\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n-from scrapy.conf import settings\n-\n+from scrapy.contrib import codecs\n \n class ExecutionEngine(object):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0574bbd44ac2510b03000c0528d23be2f815e031", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 19 | Methods Changed: 14 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 81 | Churn Cumulative: 260 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -33,22 +33,24 @@ class LinkExtractor(FixedSGMLParser):\n         FixedSGMLParser.__init__(self)\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.inside_link = False\n+        self.current_link = None\n \n-    def extract_urls(self, response):\n+    def extract_urls(self, response, unique=False):\n         self.reset()\n+        self.unique = unique\n         self.feed(response.body.to_string())\n         self.close()\n         \n         base_url = self.base_url if self.base_url else response.url\n-        urls = {}\n-        for link, text in self.links.iteritems():\n-            urls[urljoin(base_url, link)] = text\n-        return urls\n+        ret = []\n+        for link in self.links:\n+            link.url = urljoin(base_url, link.url)\n+            ret.append(link)\n+        return ret\n \n     def reset(self):\n         FixedSGMLParser.reset(self)\n-        self.links = {}\n+        self.links = []\n         self.base_url = None\n \n     def unknown_starttag(self, tag, attrs):\n@@ -57,12 +59,27 @@ class LinkExtractor(FixedSGMLParser):\n         if self.scan_tag(tag):\n             for attr, value in attrs:\n                 if self.scan_attr(attr):\n-                    self.links[value] = \"\"\n-                    self.inside_link = value\n+                    if not self.unique or not value in [link.url for link in self.links]:\n+                        link = Link(url=value)\n+                        self.links.append(link)\n+                        self.current_link = link\n \n     def unknown_endtag(self, tag):\n-        self.inside_link = False\n+        self.current_link = None\n \n     def handle_data(self, data):\n-        if self.inside_link and not self.links.get(self.inside_link, None):\n-            self.links[self.inside_link] = data\n+        if self.current_link and not self.current_link.text:\n+            self.current_link.text = data\n+\n+\n+class Link(object):\n+    \"\"\"\n+    Link objects represent an extracted link by the LinkExtractor.\n+    At the moment, it contains just the url and link text.\n+    \"\"\"\n+    def __init__(self, url, text=''):\n+        self.url = url\n+        self.text = text\n+\n+    def __eq__(self, other):\n+        return self.url == other.url and self.text == other.text\n\n@@ -50,27 +50,27 @@ class RegexLinkExtractor(LinkExtractor):\n         attr_func = lambda x: x in attrs\n         LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)\n \n-    def extract_urls(self, response):\n+    def extract_urls(self, response, unique=True):\n         if self.restrict_xpaths:\n             response = new_response_from_xpaths(response, self.restrict_xpaths)\n \n-        url_text = LinkExtractor.extract_urls(self, response)\n-        urls = [u for u in url_text.iterkeys() if _is_valid_url(u)]\n+        links = LinkExtractor.extract_urls(self, response, unique)\n+        links = [link for link in links if _is_valid_url(link.url)]\n \n         if self.allow_res:\n-            urls = [u for u in urls if _matches(u, self.allow_res)]\n+            links = [link for link in links if _matches(link.url, self.allow_res)]\n         if self.deny_res:\n-            urls = [u for u in urls if not _matches(u, self.deny_res)]\n+            links = [link for link in links if not _matches(link.url, self.deny_res)]\n         if self.allow_domains:\n-            urls = [u for u in urls if url_is_from_any_domain(u, self.allow_domains)]\n+            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]\n         if self.deny_domains:\n-            urls = [u for u in urls if not url_is_from_any_domain(u, self.deny_domains)]\n+            links = [link for link in links if not url_is_from_any_domain(link.url, self.deny_domains)]\n         \n-        res = {}\n         if self.canonicalize:\n-            for u in urls:\n-                res[canonicalize_url(u)] = url_text[u]\n-        else:\n-            for u in urls:\n-                res[u] = url_text[u]\n-        return res\n+            for link in links:\n+                link.url = canonicalize_url(link.url)\n+\n+        return links\n+\n+    def match(self, url):\n+        return any(regex.search(url) for regex in self.allow_res) and not any(regex.search(url) for regex in self.deny_res)\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.http import Response\n-from scrapy.link import LinkExtractor\n+from scrapy.link import LinkExtractor, Link\n \n class LinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n@@ -16,10 +16,10 @@ class LinkExtractorTestCase(unittest.TestCase):\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n         self.assertEqual(lx.extract_urls(response), \n-                         {'http://example.org/somepage/item/12.html': 'Item 12', \n-                          'http://example.org/about.html': 'About us', \n-                          'http://example.org/othercat.html': 'Other category', \n-                          'http://example.org/': ''})\n+                         [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), \n+                          Link(url='http://example.org/about.html', text='About us'),\n+                          Link(url='http://example.org/othercat.html', text='Other category'), \n+                          Link(url='http://example.org/', text='')])\n \n     def test_base_url(self):\n         html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n@@ -29,7 +29,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n         self.assertEqual(lx.extract_urls(response), \n-                         {'http://otherdomain.com/base/item/12.html': 'Item 12'})\n+                         [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#776818db71653a65dc6921bb0f7a7132cbdbddcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 51 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -22,9 +22,9 @@ class TestSpider(BaseSpider):\n     def parse(self, response):\n         xlink = LinkExtractor()\n         itemre = re.compile(self.itemurl_re)\n-        for url in xlink.extract_urls(response):\n-            if itemre.search(url):\n-                yield Request(url=url, callback=self.parse_item)\n+        for link in xlink.extract_urls(response):\n+            if itemre.search(link.url):\n+                yield Request(url=link.url, callback=self.parse_item)\n \n     def parse_item(self, response):\n         item = ScrapedItem()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2bab509796ddf13495a1e17c5b8a3846fec979d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 12 | Files Changed: 6 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 31 | Churn Cumulative: 2003 | Contributors (this commit): 2 | Commits (past 90d): 48 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages\n from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, Delist, Regex\n+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex\n from scrapy.utils.python import unique, flatten\n\n@@ -27,7 +27,7 @@ def strip_list(value):\n def drop_empty(value):\n     return [ v for v in value if v ]\n \n-def normalize_urls(value):\n+def canonicalize_urls(value):\n     if hasattr(value, '__iter__'):\n         return [canonicalize_url(url) for url in value]\n     elif isinstance(value, basestring):\n\n@@ -9,6 +9,7 @@ from pydispatch import dispatcher\n from pprint import PrettyPrinter\n \n from scrapy.item import ScrapedItem, ItemDelta\n+from scrapy.spider import spiders\n from scrapy.core import signals\n from scrapy.core.exceptions import UsageError, DropItem\n \n@@ -30,15 +31,8 @@ class ValidationPipeline(object):\n         return item\n \n class SetGUIDPipeline(object):\n-    def __init__(self):\n-        self.spider = None\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-\n-    def domain_opened(self, domain, spider):\n-        self.spider = spider\n-\n     def process_item(self, domain, response, item):\n-        self.spider.set_guid(item)\n+        spiders.fromdomain(domain).set_guid(item)\n         return item\n \n class RobustScrapedItem(ScrapedItem):\n\n@@ -31,3 +31,6 @@ class AdaptorDict(dict):\n         \n         return value\n \n+\n+    def __repr__(self):\n+        return \"<AdaptorDict [ %d attributes ] >\" % len(self.keys())\n\n@@ -22,10 +22,19 @@ class ScrapedItem(object):\n         \"\"\"\n         self._adaptors_dict[attrib] = adaptors\n \n-    def attribute(self, attrname, value, debug=False, override=False):\n+    def attribute(self, attrname, value, override=False, add=False, debug=False):\n         val = self._adaptors_dict.execute(attrname, value, debug)\n-        if not getattr(self, attrname, None) or override:\n+        curr_val = getattr(self, attrname, None)\n+        if not curr_val:\n             setattr(self, attrname, val)\n+        else:\n+            if override:\n+                setattr(self, attrname, val)\n+            elif add and all(hasattr(var, '__iter__') for var in (curr_val, val)):\n+                newval = []\n+                newval.extend(curr_val)\n+                newval.extend(val)\n+                setattr(self, attrname, newval)\n \n     def __sub__(self, other):\n         raise NotImplementedError\n\n@@ -48,6 +48,7 @@ class XPathSelector(object):\n                 return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath, response=self.response)])\n         else:\n             return XPathSelectorList([])\n+    __call__ = x\n \n     def re(self, regex):\n         \"\"\"Return a list of unicode strings by applying the regex over all\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#defcb45120b4a9bfd34c9858b09ccc5d7cecad62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 79 | Churn Cumulative: 208 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.4642857142857143\n\nDIFF:\n@@ -31,7 +31,12 @@ class Command(ScrapyCommand):\n         if responses:\n             response = responses[0]\n             spider = spiders.fromurl(response.url)\n-            result = spider.parse(response) if not opts.identify else spider.identify(response)\n+            if opts.identify:\n+                result = spider.identify(response)\n+            elif hasattr(spider, 'parse_url'):\n+                result = spider.parse_url(response)\n+            else:\n+                result = spider.parse(response)\n \n             items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]\n             links = [i for i in result if isinstance(i, Request)]\n@@ -44,3 +49,4 @@ class Command(ScrapyCommand):\n             if not opts.nolinks:\n                 print \"# Links\", \"-\"*68\n                 display.pprint(links)\n+\n\n@@ -1,30 +1,32 @@\n \"\"\"\n-This module contains BasicSpider, a spider class which provides support for\n-basic crawling.\n+This module contains some basic spiders for scraping websites (CrawlSpider)\n+and XML feeds (XMLFeedSpider).\n \"\"\"\n \n-from scrapy.http import Request\n+from scrapy.http import Request, Response, ResponseBody\n from scrapy.spider import BaseSpider\n+from scrapy.item import ScrapedItem\n from scrapy.core.exceptions import UsageError\n from scrapy.utils.misc import hash_values\n \n class BasicSpider(BaseSpider):\n     \"\"\"\n-    BasicSpider extends BaseSpider by providing support for simple crawling\n-    by following links contained in web pages. \n-    \n-    With BasicSpider you can write a basic spider very easily and quickly. For\n-    more information refer to the Scrapy tutorial\n+    This class is basically a BaseSpider with support for GUID generating\n     \"\"\"\n+    gen_guid_attribs = []\n+    gen_variant_guid_attribs = []\n \n-    gen_guid_attribs = ['supplier', 'site_id']\n-    gen_variant_guid_attribs = ['site_id']\n+    def set_guid(self, item):\n+        item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n    \n+class CrawlSpider(BasicSpider):\n+    \"\"\"\n+    This class works as a base class for spiders that crawl over websites\n+    \"\"\"\n     def __init__(self):\n         super(BaseSpider, self).__init__()\n         \n         self._links_callback = []\n-\n         for attr in dir(self):\n             if attr.startswith('links_'):\n                 suffix = attr.split('_', 1)[1]\n@@ -46,8 +48,10 @@ class BasicSpider(BaseSpider):\n         res = []\n         links_to_follow = {}\n         for lx, callback in self._links_callback:\n-            for url, link_text in lx.extract_urls(response).iteritems():\n-                links_to_follow[url] = (callback, link_text)\n+            links = lx.extract_urls(response)\n+            links = self.post_extract_links(links) if hasattr(self, 'post_extract_links') else links\n+            for link in links:\n+                links_to_follow[link.url] = (callback, link.text)\n \n         for url, (callback, link_text) in links_to_follow.iteritems():\n             request = Request(url=url, link_text=link_text)\n@@ -58,8 +62,45 @@ class BasicSpider(BaseSpider):\n     def _parse_wrapper(self, response, callback):\n         res = self._links_to_follow(response)\n         res += callback(response) if callback else ()\n+        for entry in res:\n+            if isinstance(entry, ScrapedItem):\n+               self.set_guid(entry)\n         return res\n \n-    def set_guid(self, item):\n-        item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n+    def parse_url(self, response):\n+        \"\"\"\n+        This method is called whenever you run scrapy with the 'parse' command\n+        over an URL.\n+        \"\"\"\n+        extractor_names = [attrname for attrname in dir(self) if attrname.startswith('links_')]\n+        ret = []\n+        for name in extractor_names:\n+            extractor = getattr(self, name)\n+            callback_name = 'parse_%s' % name[6:]\n+            if hasattr(self, callback_name):\n+                if extractor.match(response.url):\n+                    ret.extend(getattr(self, callback_name)(response))\n+        return ret\n+\n+class XMLFeedSpider(BasicSpider):\n+    \"\"\"\n+    This class intends to be the base class for spiders that scrape\n+    from XML feeds.\n+\n+    You can choose whether to parse the file using the iternodes tool,\n+    or not using it (which just splits the tags using xpath)\n+    \"\"\"\n+    iternodes = True\n+    itertag = 'product'\n+\n+    def parse(self, response):\n+        if not hasattr(self, 'parse_item'):\n+            raise NotConfigured('You must define parse_item method in order to scrape this feed')\n+\n+        if self.iternodes:\n+            nodes = xpathselector_iternodes(response, self.itertag)\n+        else:\n+            nodes = HtmlXPathSelector(response).x('//%s' % self.itertag)\n+\n+        return (self.parse_item(response, xSel) for xSel in nodes)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc13a5924a0e89934ddfe35c795fff9e41894af9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 911 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1 +1 @@\n-from scrapy.contrib.codecs import x_mac_roman\n+from scrapy.contrib.codecs.x_mac_roman import XMacRomanCodec\n\n@@ -311,4 +311,6 @@ def x_mac_roman_search_function(name):\n     if name == 'x-mac-roman':\n        return getregentry()\n \n+class XMacRomanCodec(object):\n+    def __init__(self):\n         codecs.register(x_mac_roman_search_function)\n\n@@ -20,7 +20,6 @@ from scrapy.item.pipeline import ItemPipeline\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n-from scrapy.contrib import codecs\n \n class ExecutionEngine(object):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a45754cf2e550ba498d25de982be0ead1eb3192", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 158 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ and XML feeds (XMLFeedSpider).\n from scrapy.http import Request, Response, ResponseBody\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n+from scrapy.xml.selector import XmlXPathSelector\n from scrapy.core.exceptions import UsageError\n from scrapy.utils.misc import hash_values\n \n@@ -14,7 +15,6 @@ class BasicSpider(BaseSpider):\n     This class is basically a BaseSpider with support for GUID generating\n     \"\"\"\n     gen_guid_attribs = []\n-    gen_variant_guid_attribs = []\n \n     def set_guid(self, item):\n         item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])\n@@ -100,7 +100,7 @@ class XMLFeedSpider(BasicSpider):\n         if self.iternodes:\n             nodes = xpathselector_iternodes(response, self.itertag)\n         else:\n-            nodes = HtmlXPathSelector(response).x('//%s' % self.itertag)\n+            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n \n         return (self.parse_item(response, xSel) for xSel in nodes)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ef65b97b58a75d25d47b6f0605cbf0cf38cee78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 160 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ and XML feeds (XMLFeedSpider).\n from scrapy.http import Request, Response, ResponseBody\n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n-from scrapy.xml.selector import XmlXPathSelector\n+from scrapy.xpath.selector import XmlXPathSelector\n from scrapy.core.exceptions import UsageError\n from scrapy.utils.misc import hash_values\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b46c20da29b73455974287abc81a6ae8da365c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 159 | Lines Deleted: 78 | Files Changed: 5 | Hunks: 11 | Methods Changed: 16 | Complexity Δ (Sum/Max): 16/17 | Churn Δ: 237 | Churn Cumulative: 515 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,7 +98,7 @@ class XMLFeedSpider(BasicSpider):\n             raise NotConfigured('You must define parse_item method in order to scrape this feed')\n \n         if self.iternodes:\n-            nodes = xpathselector_iternodes(response, self.itertag)\n+            nodes = xmliter(response, self.itertag)\n         else:\n             nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n \n\n@@ -0,0 +1,117 @@\n+import os\n+import unittest\n+\n+from scrapy.utils.iterators import csviter, xmliter\n+from scrapy.http import Response\n+\n+class UtilsXmlTestCase(unittest.TestCase):\n+    ### NOTE: Encoding issues have been found with BeautifulSoup for utf-16 files, utf-16 test removed ###\n+    def test_iterator(self):\n+        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n+  <product id=\"001\">\n+    <type>Type 1</type>\n+    <name>Name 1</name>\n+  </product>\n+  <product id=\"002\">\n+    <type>Type 2</type>\n+    <name>Name 2</name>\n+  </product>\n+</products>\n+        \"\"\"\n+        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        attrs = []\n+        for x in xmliter(response, 'product'):\n+            attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n+\n+        self.assertEqual(attrs, \n+                         [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n+\n+    def test_iterator_text(self):\n+        body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n+        \n+        self.assertEqual([x.x(\"text()\").extract() for x in xmliter(body, 'product')],\n+                         [[u'one'], [u'two']])\n+\n+    def test_iterator_exception(self):\n+        body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n+        \n+        iter = xmliter(body, 'product')\n+        iter.next()\n+        iter.next()\n+\n+        self.assertRaises(StopIteration, iter.next)\n+\n+class UtilsCsvTestCase(unittest.TestCase):\n+    sample_feed_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds', 'feed-sample3.csv')\n+\n+    def test_iterator_defaults(self):\n+        body = open(self.sample_feed_path).read()\n+\n+        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        csv = csviter(response)\n+\n+        result = [row for row in csv]\n+        self.assertEqual(result,\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n+        # explicit type check cuz' we no like stinkin' autocasting! yarrr\n+        for result_row in result:\n+            self.assert_(all((isinstance(k, unicode) for k in result_row.keys())))\n+            self.assert_(all((isinstance(v, unicode) for v in result_row.values())))\n+\n+    def test_iterator_delimiter(self):\n+        body = open(self.sample_feed_path).read().replace(',', '\\t')\n+\n+        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        csv = csviter(response, delimiter='\\t')\n+\n+        self.assertEqual([row for row in csv],\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n+    def test_iterator_headers(self):\n+        sample = open(self.sample_feed_path).read().splitlines()\n+        headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n+\n+        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        csv = csviter(response, headers=headers)\n+\n+        self.assertEqual([row for row in csv],\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n+    def test_iterator_falserow(self):\n+        body = open(self.sample_feed_path).read()\n+        body = '\\n'.join((body, 'a,b', 'a,b,c,d'))\n+\n+        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        csv = csviter(response)\n+\n+        self.assertEqual([row for row in csv],\n+                         [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n+                          {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'4', u'name': u'empty',   u'value': u''}])\n+\n+    def test_iterator_exception(self):\n+        body = open(self.sample_feed_path).read()\n+\n+        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        iter = csviter(response)\n+        iter.next()\n+        iter.next()\n+        iter.next()\n+        iter.next()\n+\n+        self.assertRaises(StopIteration, iter.next)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -1,37 +0,0 @@\n-import os\n-import unittest\n-\n-from scrapy.utils.xml import xpathselector_iternodes\n-from scrapy.http import Response\n-\n-class UtilsXmlTestCase(unittest.TestCase):\n-\n-    def test_iterator(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n-  <product id=\"001\">\n-    <type>Type 1</type>\n-    <name>Name 1</name>\n-  </product>\n-  <product id=\"002\">\n-    <type>Type 2</type>\n-    <name>Name 2</name>\n-  </product>\n-</products>\n-        \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        attrs = []\n-        for x in xpathselector_iternodes(response, 'product'):\n-            attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n-\n-        self.assertEqual(attrs, \n-                         [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n-\n-    def test_iterator_text(self):\n-        body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n-        \n-        self.assertEqual([x.x(\"text()\").extract() for x in xpathselector_iternodes(body, 'product')],\n-                         [[u'one'], [u'two']])\n-\n-if __name__ == \"__main__\":\n-    unittest.main()   \n\n@@ -1,23 +1,52 @@\n-import csv\n+import re, csv\n \n+from scrapy.xpath import XmlXPathSelector\n from scrapy.http import Response\n from scrapy import log\n \n-def csv_iter(response, delimiter=None, headers=None):\n-    if delimiter:\n-        csv_r = csv.reader(response.body.to_unicode().split('\\n'), delimiter=delimiter)\n+def _normalize_input(obj):\n+    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n+    if isinstance(obj, Response):\n+        return obj.body.to_unicode()\n+    elif isinstance(obj, str):\n+        return obj.decode('utf-8')\n     else:\n-        csv_r = csv.reader(response.body.to_unicode().split('\\n'))\n+        return obj\n+\n+def xmliter(obj, nodename):\n+    \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n+       given tha name of the node to iterate. Useful for parsing XML feeds.\n+\n+    obj can be:\n+    - a Response object\n+    - a unicode string\n+    - a string encoded as utf-8\n+    \"\"\"\n+    text = _normalize_input(obj)\n+\n+    r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n+    for match in r.finditer(text):\n+        nodetext = match.group()\n+        yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n+\n+def csviter(obj, delimiter=None, headers=None):\n+    def _getrow(csv_r):\n+        return [field.decode() for field in csv_r.next()]\n+\n+    lines = _normalize_input(obj).splitlines(True)\n+    if delimiter:\n+        csv_r = csv.reader(lines, delimiter=delimiter)\n+    else:\n+        csv_r = csv.reader(lines)\n \n     if not headers:\n-        headers = csv_r.next()\n+        headers = _getrow(csv_r)\n \n     while True:\n-        node = csv_r.next()\n-\n-        if len(node) != len(headers):\n-            log.msg(\"ignoring node %d (length: %d, should be: %d)\" % (csv_r.line_num, len(node), len(headers)), log.WARNING)\n+        row = _getrow(csv_r)\n+        if len(row) != len(headers):\n+            log.msg(\"ignoring row %d (length: %d, should be: %d)\" % (csv_r.line_num, len(row), len(headers)), log.WARNING)\n             continue\n-\n-        yield dict(zip(headers, node))\n+        else:\n+            yield dict(zip(headers, row))\n \n\n@@ -1,28 +0,0 @@\n-import re\n-\n-from scrapy.xpath import XmlXPathSelector\n-from scrapy.http import Response\n-\n-def xpathselector_iternodes(obj, nodename):\n-    \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n-       given tha name of the node to iterate. Useful for parsing XML feeds.\n-\n-    obj can be:\n-    - a Response object\n-    - a unicode string\n-    - a string encoded as utf-8\n-    \"\"\"\n-    \n-    assert isinstance(obj, (Response, basestring)), \"obj must be Response or basestring, not %s\" % type(obj).__name__\n-\n-    if isinstance(obj, Response):\n-        text = obj.body.to_unicode()\n-    elif isinstance(obj, str):\n-        text = obj.decode('utf-8')\n-    else:\n-        text = obj\n-\n-    r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n-    for match in r.finditer(text):\n-        nodetext = match.group()\n-        yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
