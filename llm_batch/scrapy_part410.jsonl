{"custom_id": "scrapy#095140f134745960751c8c4b34da7de6b91e4a82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 99 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,6 +17,8 @@ if TYPE_CHECKING:\n class BaseSpiderMiddleware:\n     \"\"\"Optional base class for spider middlewares.\n \n+    .. versionadded:: 2.13\n+\n     This class provides helper methods for asynchronous ``process_spider_output``\n     methods. Middlewares that don't have a ``process_spider_output`` method don't need\n     to use it.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eced5ca2d3c85c36b18f0da3d5d888e7e66a7014", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 132 | Lines Deleted: 73 | Files Changed: 4 | Hunks: 72 | Methods Changed: 33 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 205 | Churn Cumulative: 8430 | Contributors (this commit): 50 | Commits (past 90d): 14 | Contributors (cumulative): 103 | DMM Complexity: 0.3125\n\nDIFF:\n@@ -265,7 +265,7 @@ class ExecutionEngine:\n             self.crawl(result)\n             return None\n \n-        d = self.scraper.enqueue_scrape(result, request, self.spider)\n+        d = self.scraper.enqueue_scrape(result, request)\n         d.addErrback(\n             lambda f: logger.error(\n                 \"Error while enqueuing downloader output\",\n@@ -290,14 +290,14 @@ class ExecutionEngine:\n         \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n-        self._schedule_request(request, self.spider)\n+        self._schedule_request(request)\n         self.slot.nextcall.schedule()  # type: ignore[union-attr]\n \n-    def _schedule_request(self, request: Request, spider: Spider) -> None:\n+    def _schedule_request(self, request: Request) -> None:\n         request_scheduled_result = self.signals.send_catch_log(\n             signals.request_scheduled,\n             request=request,\n-            spider=spider,\n+            spider=self.spider,\n             dont_log=IgnoreRequest,\n         )\n         for handler, result in request_scheduled_result:\n@@ -305,7 +305,7 @@ class ExecutionEngine:\n                 return\n         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(\n-                signals.request_dropped, request=request, spider=spider\n+                signals.request_dropped, request=request, spider=self.spider\n             )\n \n     def download(self, request: Request) -> Deferred[Response]:\n@@ -438,7 +438,7 @@ class ExecutionEngine:\n         dfd.addBoth(lambda _: self.downloader.close())\n         dfd.addErrback(log_failure(\"Downloader close failure\"))\n \n-        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n+        dfd.addBoth(lambda _: self.scraper.close_spider())\n         dfd.addErrback(log_failure(\"Scraper close failure\"))\n \n         if hasattr(self.slot.scheduler, \"close\"):\n\n@@ -4,6 +4,7 @@ extracts information from them\"\"\"\n from __future__ import annotations\n \n import logging\n+import warnings\n from collections import deque\n from collections.abc import AsyncIterable, Iterator\n from typing import TYPE_CHECKING, Any, TypeVar, Union, cast\n@@ -13,7 +14,12 @@ from twisted.python.failure import Failure\n \n from scrapy import Spider, signals\n from scrapy.core.spidermw import SpiderMiddlewareManager\n-from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n+from scrapy.exceptions import (\n+    CloseSpider,\n+    DropItem,\n+    IgnoreRequest,\n+    ScrapyDeprecationWarning,\n+)\n from scrapy.http import Request, Response\n from scrapy.utils.defer import (\n     aiter_errback,\n@@ -110,27 +116,43 @@ class Scraper:\n         self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n         yield self.itemproc.open_spider(spider)\n \n-    def close_spider(self, spider: Spider) -> Deferred[Spider]:\n+    def close_spider(self, spider: Spider | None = None) -> Deferred[Spider]:\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to Scraper.close_spider() is deprecated.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n         self.slot.closing = Deferred()\n         self.slot.closing.addCallback(self.itemproc.close_spider)\n-        self._check_if_closing(spider)\n+        self._check_if_closing()\n         return self.slot.closing\n \n     def is_idle(self) -> bool:\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n         return not self.slot\n \n-    def _check_if_closing(self, spider: Spider) -> None:\n+    def _check_if_closing(self) -> None:\n         assert self.slot is not None  # typing\n+        assert self.crawler.spider\n         if self.slot.closing and self.slot.is_idle():\n-            self.slot.closing.callback(spider)\n+            assert self.crawler.spider\n+            self.slot.closing.callback(self.crawler.spider)\n \n     def enqueue_scrape(\n-        self, result: Response | Failure, request: Request, spider: Spider\n+        self, result: Response | Failure, request: Request, spider: Spider | None = None\n     ) -> _HandleOutputDeferred:\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to Scraper.enqueue_scrape() is deprecated.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n         if self.slot is None:\n             raise RuntimeError(\"Scraper slot not assigned\")\n         dfd = self.slot.add_response_request(result, request)\n@@ -138,8 +160,8 @@ class Scraper:\n         def finish_scraping(_: _T) -> _T:\n             assert self.slot is not None\n             self.slot.finish_response(result, request)\n-            self._check_if_closing(spider)\n-            self._scrape_next(spider)\n+            self._check_if_closing()\n+            self._scrape_next()\n             return _\n \n         dfd.addBoth(finish_scraping)\n@@ -148,20 +170,20 @@ class Scraper:\n                 \"Scraper bug processing %(request)s\",\n                 {\"request\": request},\n                 exc_info=failure_to_exc_info(f),\n-                extra={\"spider\": spider},\n+                extra={\"spider\": self.crawler.spider},\n             )\n         )\n-        self._scrape_next(spider)\n+        self._scrape_next()\n         return dfd\n \n-    def _scrape_next(self, spider: Spider) -> None:\n+    def _scrape_next(self) -> None:\n         assert self.slot is not None  # typing\n         while self.slot.queue:\n             response, request, deferred = self.slot.next_response_request_deferred()\n-            self._scrape(response, request, spider).chainDeferred(deferred)\n+            self._scrape(response, request).chainDeferred(deferred)\n \n     def _scrape(\n-        self, result: Response | Failure, request: Request, spider: Spider\n+        self, result: Response | Failure, request: Request\n     ) -> _HandleOutputDeferred:\n         \"\"\"\n         Handle the downloaded response or failure through the spider callback/errback\n@@ -171,40 +193,49 @@ class Scraper:\n                 f\"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}\"\n             )\n         dfd: Deferred[Iterable[Any] | AsyncIterable[Any]] = self._scrape2(\n-            result, request, spider\n+            result, request\n         )  # returns spider's processed output\n-        dfd.addErrback(self.handle_spider_error, request, result, spider)\n+        dfd.addErrback(self.handle_spider_error, request, result)\n         dfd2: _HandleOutputDeferred = dfd.addCallback(\n-            self.handle_spider_output, request, cast(Response, result), spider\n+            self.handle_spider_output, request, cast(Response, result)\n         )\n         return dfd2\n \n     def _scrape2(\n-        self, result: Response | Failure, request: Request, spider: Spider\n+        self, result: Response | Failure, request: Request\n     ) -> Deferred[Iterable[Any] | AsyncIterable[Any]]:\n         \"\"\"\n         Handle the different cases of request's result been a Response or a Failure\n         \"\"\"\n         if isinstance(result, Response):\n             # Deferreds are invariant so Mutable*Chain isn't matched to *Iterable\n+            assert self.crawler.spider\n             return self.spidermw.scrape_response(  # type: ignore[return-value]\n-                self.call_spider, result, request, spider\n+                self.call_spider, result, request, self.crawler.spider\n             )\n         # else result is a Failure\n-        dfd = self.call_spider(result, request, spider)\n-        dfd.addErrback(self._log_download_errors, result, request, spider)\n+        dfd = self.call_spider(result, request)\n+        dfd.addErrback(self._log_download_errors, result, request)\n         return dfd\n \n     def call_spider(\n-        self, result: Response | Failure, request: Request, spider: Spider\n+        self, result: Response | Failure, request: Request, spider: Spider | None = None\n     ) -> Deferred[Iterable[Any] | AsyncIterable[Any]]:\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to Scraper.call_spider() is deprecated.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n+        assert self.crawler.spider\n         dfd: Deferred[Any]\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n             assert result.request\n-            callback = result.request.callback or spider._parse\n-            warn_on_generator_with_return_value(spider, callback)\n+            callback = result.request.callback or self.crawler.spider._parse\n+            warn_on_generator_with_return_value(self.crawler.spider, callback)\n             dfd = defer_succeed(result)\n             dfd.addCallbacks(\n                 callback=callback, callbackKeywords=result.request.cb_kwargs\n@@ -214,7 +245,9 @@ class Scraper:\n             result.request = request  # type: ignore[attr-defined]\n             dfd = defer_fail(result)\n             if request.errback:\n-                warn_on_generator_with_return_value(spider, request.errback)\n+                warn_on_generator_with_return_value(\n+                    self.crawler.spider, request.errback\n+                )\n                 dfd.addErrback(request.errback)\n         dfd2: Deferred[Iterable[Any] | AsyncIterable[Any]] = dfd.addCallback(\n             iterate_spider_output\n@@ -226,29 +259,44 @@ class Scraper:\n         _failure: Failure,\n         request: Request,\n         response: Response | Failure,\n-        spider: Spider,\n+        spider: Spider | None = None,\n     ) -> None:\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to Scraper.handle_spider_error() is deprecated.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n+        assert self.crawler.spider\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n             assert self.crawler.engine is not None  # typing\n-            self.crawler.engine.close_spider(spider, exc.reason or \"cancelled\")\n+            self.crawler.engine.close_spider(\n+                self.crawler.spider, exc.reason or \"cancelled\"\n+            )\n             return\n-        logkws = self.logformatter.spider_error(_failure, request, response, spider)\n+        logkws = self.logformatter.spider_error(\n+            _failure, request, response, self.crawler.spider\n+        )\n         logger.log(\n             *logformatter_adapter(logkws),\n             exc_info=failure_to_exc_info(_failure),\n-            extra={\"spider\": spider},\n+            extra={\"spider\": self.crawler.spider},\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,\n             failure=_failure,\n             response=response,\n-            spider=spider,\n+            spider=self.crawler.spider,\n         )\n         assert self.crawler.stats\n-        self.crawler.stats.inc_value(\"spider_exceptions/count\", spider=spider)\n         self.crawler.stats.inc_value(\n-            f\"spider_exceptions/{_failure.value.__class__.__name__}\", spider=spider\n+            \"spider_exceptions/count\", spider=self.crawler.spider\n+        )\n+        self.crawler.stats.inc_value(\n+            f\"spider_exceptions/{_failure.value.__class__.__name__}\",\n+            spider=self.crawler.spider,\n         )\n \n     def handle_spider_output(\n@@ -256,41 +304,40 @@ class Scraper:\n         result: Iterable[_T] | AsyncIterable[_T],\n         request: Request,\n         response: Response,\n-        spider: Spider,\n+        spider: Spider | None = None,\n     ) -> _HandleOutputDeferred:\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to Scraper.handle_spider_output() is deprecated.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n         if not result:\n             return defer_succeed(None)\n         it: Iterable[_T] | AsyncIterable[_T]\n         dfd: Deferred[_ParallelResult]\n         if isinstance(result, AsyncIterable):\n-            it = aiter_errback(\n-                result, self.handle_spider_error, request, response, spider\n-            )\n+            it = aiter_errback(result, self.handle_spider_error, request, response)\n             dfd = parallel_async(\n                 it,\n                 self.concurrent_items,\n                 self._process_spidermw_output,\n-                request,\n                 response,\n-                spider,\n             )\n         else:\n-            it = iter_errback(\n-                result, self.handle_spider_error, request, response, spider\n-            )\n+            it = iter_errback(result, self.handle_spider_error, request, response)\n             dfd = parallel(\n                 it,\n                 self.concurrent_items,\n                 self._process_spidermw_output,\n-                request,\n                 response,\n-                spider,\n             )\n         # returning Deferred[_ParallelResult] instead of Deferred[Union[_ParallelResult, None]]\n         return dfd  # type: ignore[return-value]\n \n     def _process_spidermw_output(\n-        self, output: Any, request: Request, response: Response, spider: Spider\n+        self, output: Any, response: Response\n     ) -> Deferred[Any] | None:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n@@ -314,7 +361,7 @@ class Scraper:\n         assert self.crawler.spider is not None  # typing\n         self.slot.itemproc_size += 1\n         dfd = self.itemproc.process_item(item, self.crawler.spider)\n-        dfd.addBoth(self._itemproc_finished, item, response, self.crawler.spider)\n+        dfd.addBoth(self._itemproc_finished, item, response)\n         return dfd\n \n     def _log_download_errors(\n@@ -322,7 +369,6 @@ class Scraper:\n         spider_failure: Failure,\n         download_failure: Failure,\n         request: Request,\n-        spider: Spider,\n     ) -> Failure | None:\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here).\n@@ -332,24 +378,25 @@ class Scraper:\n         ExecutionEngine._handle_downloader_output() as \"result\"\n         \"\"\"\n         if not download_failure.check(IgnoreRequest):\n+            assert self.crawler.spider\n             if download_failure.frames:\n                 logkws = self.logformatter.download_error(\n-                    download_failure, request, spider\n+                    download_failure, request, self.crawler.spider\n                 )\n                 logger.log(\n                     *logformatter_adapter(logkws),\n-                    extra={\"spider\": spider},\n+                    extra={\"spider\": self.crawler.spider},\n                     exc_info=failure_to_exc_info(download_failure),\n                 )\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:\n                     logkws = self.logformatter.download_error(\n-                        download_failure, request, spider, errmsg\n+                        download_failure, request, self.crawler.spider, errmsg\n                     )\n                     logger.log(\n                         *logformatter_adapter(logkws),\n-                        extra={\"spider\": spider},\n+                        extra={\"spider\": self.crawler.spider},\n                     )\n \n         if spider_failure is not download_failure:\n@@ -357,41 +404,54 @@ class Scraper:\n         return None\n \n     def _itemproc_finished(\n-        self, output: Any, item: Any, response: Response | None, spider: Spider\n+        self, output: Any, item: Any, response: Response | None\n     ) -> Deferred[Any]:\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n         assert self.slot is not None  # typing\n+        assert self.crawler.spider\n         self.slot.itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                logkws = self.logformatter.dropped(item, ex, response, spider)\n+                logkws = self.logformatter.dropped(\n+                    item, ex, response, self.crawler.spider\n+                )\n                 if logkws is not None:\n-                    logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n+                    logger.log(\n+                        *logformatter_adapter(logkws),\n+                        extra={\"spider\": self.crawler.spider},\n+                    )\n                 return self.signals.send_catch_log_deferred(\n                     signal=signals.item_dropped,\n                     item=item,\n                     response=response,\n-                    spider=spider,\n+                    spider=self.crawler.spider,\n                     exception=output.value,\n                 )\n             assert ex\n-            logkws = self.logformatter.item_error(item, ex, response, spider)\n+            logkws = self.logformatter.item_error(\n+                item, ex, response, self.crawler.spider\n+            )\n             logger.log(\n                 *logformatter_adapter(logkws),\n-                extra={\"spider\": spider},\n+                extra={\"spider\": self.crawler.spider},\n                 exc_info=failure_to_exc_info(output),\n             )\n             return self.signals.send_catch_log_deferred(\n                 signal=signals.item_error,\n                 item=item,\n                 response=response,\n-                spider=spider,\n+                spider=self.crawler.spider,\n                 failure=output,\n             )\n-        logkws = self.logformatter.scraped(output, response, spider)\n+        logkws = self.logformatter.scraped(output, response, self.crawler.spider)\n         if logkws is not None:\n-            logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n-        return self.signals.send_catch_log_deferred(\n-            signal=signals.item_scraped, item=output, response=response, spider=spider\n+            logger.log(\n+                *logformatter_adapter(logkws), extra={\"spider\": self.crawler.spider}\n+            )\n+        return self.signals.send_catch_log_deferred(\n+            signal=signals.item_scraped,\n+            item=output,\n+            response=response,\n+            spider=self.crawler.spider,\n         )\n\n@@ -40,7 +40,7 @@ logger = logging.getLogger(__name__)\n \n _T = TypeVar(\"_T\")\n ScrapeFunc = Callable[\n-    [Union[Response, Failure], Request, Spider], Union[Iterable[_T], AsyncIterable[_T]]\n+    [Union[Response, Failure], Request], Union[Iterable[_T], AsyncIterable[_T]]\n ]\n \n \n@@ -86,8 +86,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             except _InvalidOutput:\n                 raise\n             except Exception:\n-                return scrape_func(Failure(), request, spider)\n-        return scrape_func(response, request, spider)\n+                return scrape_func(Failure(), request)\n+        return scrape_func(response, request)\n \n     def _evaluate_iterable(\n         self,\n\n@@ -487,18 +487,17 @@ def test_request_scheduled_signal(caplog):\n         if \"drop\" in request.url:\n             raise IgnoreRequest\n \n-    spider = MySpider()\n-    crawler = get_crawler(spider.__class__)\n+    crawler = get_crawler(MySpider)\n     engine = ExecutionEngine(crawler, lambda _: None)\n     engine.downloader._slot_gc_loop.stop()\n     scheduler = TestScheduler()\n     engine.slot = Slot((), None, Mock(), scheduler)\n     crawler.signals.connect(signal_handler, request_scheduled)\n     keep_request = Request(\"https://keep.example\")\n-    engine._schedule_request(keep_request, spider)\n+    engine._schedule_request(keep_request)\n     drop_request = Request(\"https://drop.example\")\n     caplog.set_level(DEBUG)\n-    engine._schedule_request(drop_request, spider)\n+    engine._schedule_request(drop_request)\n     assert scheduler.enqueued == [keep_request], (\n         f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23c206af35a8a7772d63cd63fdccee085dc38e40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 7 | Methods Changed: 11 | Complexity Δ (Sum/Max): 8/10 | Churn Δ: 93 | Churn Cumulative: 3985 | Contributors (this commit): 39 | Commits (past 90d): 10 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -374,12 +374,14 @@ class Scraper:\n         errors that got propagated thru here).\n \n         spider_failure: the value passed into the errback of self.call_spider()\n+        (likely raised in the request errback)\n+\n         download_failure: the value passed into _scrape2() from\n         ExecutionEngine._handle_downloader_output() as \"result\"\n+        (likely raised in the download handler or a downloader middleware)\n         \"\"\"\n         if not download_failure.check(IgnoreRequest):\n             assert self.crawler.spider\n-            if download_failure.frames:\n             logkws = self.logformatter.download_error(\n                 download_failure, request, self.crawler.spider\n             )\n@@ -388,18 +390,8 @@ class Scraper:\n                 extra={\"spider\": self.crawler.spider},\n                 exc_info=failure_to_exc_info(download_failure),\n             )\n-            else:\n-                errmsg = download_failure.getErrorMessage()\n-                if errmsg:\n-                    logkws = self.logformatter.download_error(\n-                        download_failure, request, self.crawler.spider, errmsg\n-                    )\n-                    logger.log(\n-                        *logformatter_adapter(logkws),\n-                        extra={\"spider\": self.crawler.spider},\n-                    )\n-\n         if spider_failure is not download_failure:\n+            # a request errback raised a different exception, it needs to be handled later\n             return spider_failure\n         return None\n \n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import json\n import logging\n import unittest\n@@ -723,3 +725,78 @@ class TestCrawlSpider(TestCase):\n         assert crawler.spider.meta[\n             \"failure\"\n         ].value.response.headers == crawler.spider.meta.get(\"headers_received\")\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback(self):\n+        failures = []\n+\n+        def eb(failure: Failure) -> Failure:\n+            failures.append(failure)\n+            return failure\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert len(failures) == 1\n+        assert \"HTTP status code is not handled or not allowed\" in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_silence(self):\n+        failures = []\n+\n+        def eb(failure: Failure) -> None:\n+            failures.append(failure)\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert len(failures) == 1\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_exception(self):\n+        def eb(failure: Failure) -> None:\n+            raise ValueError(\"foo\")\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert \"Spider error processing\" in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_downloader_error(self):\n+        failures = []\n+\n+        def eb(failure: Failure) -> Failure:\n+            failures.append(failure)\n+            return failure\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n+            )\n+        assert len(failures) == 1\n+        assert \"Error downloading\" in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_exception_downloader_error(self):\n+        def eb(failure: Failure) -> None:\n+            raise ValueError(\"foo\")\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n+            )\n+        assert \"Error downloading\" in str(log)\n+        assert \"Spider error processing\" in str(log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da9078c4bb942be8f55495d9a44ea522f3cdcbc4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 24 | Churn Cumulative: 1984 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ from twisted.trial.unittest import TestCase\n \n from scrapy import signals\n from scrapy.crawler import CrawlerRunner\n-from scrapy.exceptions import StopDownload\n+from scrapy.exceptions import CloseSpider, StopDownload\n from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import to_unicode\n@@ -800,3 +800,25 @@ class TestCrawlSpider(TestCase):\n             )\n         assert \"Error downloading\" in str(log)\n         assert \"Spider error processing\" in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_raise_closespider(self):\n+        def cb(response):\n+            raise CloseSpider\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(seed=self.mockserver.url(\"/\"), callback_func=cb)\n+        assert \"Closing spider (cancelled)\" in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_raise_closespider_reason(self):\n+        def cb(response):\n+            raise CloseSpider(\"my_reason\")\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(seed=self.mockserver.url(\"/\"), callback_func=cb)\n+        assert \"Closing spider (my_reason)\" in str(log)\n+        assert \"Spider error processing\" not in str(log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5dfe7cd7b87ffc8bb287934fa3d6ffbcd63da332", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 1999 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -188,11 +188,18 @@ class TestCrawl(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_items(self):\n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(StartRequestsItemSpider)\n+            crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n         assert len(log.records) == 0\n+        assert items == [{\"name\": \"test item\"}]\n \n     @defer.inlineCallbacks\n     def test_start_requests_unsupported_output(self):\n@@ -201,11 +208,19 @@ class TestCrawl(TestCase):\n         things fail when ItemAdapter is actually used on the corresponding\n         non-item object.\"\"\"\n \n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(StartRequestsGoodAndBadOutput)\n+            crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n             yield crawler.crawl(mockserver=self.mockserver)\n \n         assert len(log.records) == 0\n+        assert len(items) == 3\n+        assert not any(isinstance(item, Request) for item in items)\n \n     @defer.inlineCallbacks\n     def test_start_requests_laziness(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff1ac75c9ef538b49212dbb1d4112b3653efab12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2778 | Contributors (this commit): 24 | Commits (past 90d): 7 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import sys\n+\n from twisted.internet.defer import Deferred\n \n import scrapy\n@@ -14,7 +16,7 @@ class SleepingSpider(scrapy.Spider):\n         from twisted.internet import reactor\n \n         d = Deferred()\n-        reactor.callLater(int(self.sleep), d.callback, None)\n+        reactor.callLater(int(sys.argv[1]), d.callback, None)\n         await maybe_deferred_to_future(d)\n \n \n\n@@ -890,7 +890,7 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_shutdown_graceful(self):\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n-        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=3\")\n+        args = self.get_script_args(\"sleeping.py\", \"3\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n@@ -904,7 +904,7 @@ class TestCrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         from twisted.internet import reactor\n \n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n-        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=10\")\n+        args = self.get_script_args(\"sleeping.py\", \"10\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a1edbd473e47b15183e57d975d71db0cb3a2197", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 105 | Files Changed: 2 | Hunks: 45 | Methods Changed: 20 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 191 | Churn Cumulative: 2914 | Contributors (this commit): 18 | Commits (past 90d): 8 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,17 +1,18 @@\n+from __future__ import annotations\n+\n import asyncio\n from gzip import BadGzipFile\n from unittest import mock\n \n import pytest\n-from twisted.internet import defer\n-from twisted.internet.defer import Deferred\n-from twisted.python.failure import Failure\n+from twisted.internet.defer import Deferred, succeed\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n+from scrapy.utils.defer import deferred_f_from_coro_f, maybe_deferred_to_future\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n \n@@ -29,38 +30,36 @@ class TestManagerBase(TestCase):\n     def tearDown(self):\n         return self.crawler.engine.close_spider(self.spider)\n \n-    def _download(self, request, response=None):\n+    async def _download(\n+        self, request: Request, response: Response | None = None\n+    ) -> Response | Request:\n         \"\"\"Executes downloader mw manager's download method and returns\n-        the result (Request or Response) or raise exception in case of\n+        the result (Request or Response) or raises exception in case of\n         failure.\n         \"\"\"\n         if not response:\n             response = Response(request.url)\n \n-        def download_func(request, spider):\n-            return response\n+        def download_func(request: Request, spider: Spider) -> Deferred[Response]:\n+            return succeed(response)\n \n-        dfd = self.mwman.download(download_func, request, self.spider)\n-        # catch deferred result and return the value\n-        results = []\n-        dfd.addBoth(results.append)\n-        self._wait(dfd)\n-        ret = results[0]\n-        if isinstance(ret, Failure):\n-            ret.raiseException()\n-        return ret\n+        return await maybe_deferred_to_future(\n+            self.mwman.download(download_func, request, self.spider)\n+        )\n \n \n class TestDefaults(TestManagerBase):\n     \"\"\"Tests default behavior with default settings\"\"\"\n \n-    def test_request_response(self):\n+    @deferred_f_from_coro_f\n+    async def test_request_response(self):\n         req = Request(\"http://example.com/index.html\")\n         resp = Response(req.url, status=200)\n-        ret = self._download(req, resp)\n+        ret = await self._download(req, resp)\n         assert isinstance(ret, Response), \"Non-response returned\"\n \n-    def test_3xx_and_invalid_gzipped_body_must_redirect(self):\n+    @deferred_f_from_coro_f\n+    async def test_3xx_and_invalid_gzipped_body_must_redirect(self):\n         \"\"\"Regression test for a failure when redirecting a compressed\n         request.\n \n@@ -85,13 +84,14 @@ class TestDefaults(TestManagerBase):\n                 \"Location\": \"http://example.com/login\",\n             },\n         )\n-        ret = self._download(request=req, response=resp)\n+        ret = await self._download(req, resp)\n         assert isinstance(ret, Request), f\"Not redirected: {ret!r}\"\n         assert to_bytes(ret.url) == resp.headers[\"Location\"], (\n             \"Not redirected to location header\"\n         )\n \n-    def test_200_and_invalid_gzipped_body_must_fail(self):\n+    @deferred_f_from_coro_f\n+    async def test_200_and_invalid_gzipped_body_must_fail(self):\n         req = Request(\"http://example.com\")\n         body = b\"<p>You are being redirected</p>\"\n         resp = Response(\n@@ -106,13 +106,14 @@ class TestDefaults(TestManagerBase):\n             },\n         )\n         with pytest.raises(BadGzipFile):\n-            self._download(request=req, response=resp)\n+            await self._download(req, resp)\n \n \n class TestResponseFromProcessRequest(TestManagerBase):\n     \"\"\"Tests middleware returning a response from process_request.\"\"\"\n \n-    def test_download_func_not_called(self):\n+    @deferred_f_from_coro_f\n+    async def test_download_func_not_called(self):\n         resp = Response(\"http://example.com/index.html\")\n \n         class ResponseMiddleware:\n@@ -123,19 +124,17 @@ class TestResponseFromProcessRequest(TestManagerBase):\n \n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        self._wait(dfd)\n-\n-        assert results[0] is resp\n+        result = await maybe_deferred_to_future(\n+            self.mwman.download(download_func, req, self.spider)\n+        )\n+        assert result is resp\n         assert not download_func.called\n \n \n-class TestProcessRequestInvalidOutput(TestManagerBase):\n+class TestInvalidOutput(TestManagerBase):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_process_request(self):\n         \"\"\"Invalid return value for process_request method should raise an exception\"\"\"\n-\n-    def test_invalid_process_request(self):\n         req = Request(\"http://example.com/index.html\")\n \n         class InvalidProcessRequestMiddleware:\n@@ -143,18 +142,12 @@ class TestProcessRequestInvalidOutput(TestManagerBase):\n                 return 1\n \n         self.mwman._add_middleware(InvalidProcessRequestMiddleware())\n-        download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        assert isinstance(results[0], Failure)\n-        assert isinstance(results[0].value, _InvalidOutput)\n+        with pytest.raises(_InvalidOutput):\n+            await self._download(req)\n \n-\n-class TestProcessResponseInvalidOutput(TestManagerBase):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_process_response(self):\n         \"\"\"Invalid return value for process_response method should raise an exception\"\"\"\n-\n-    def test_invalid_process_response(self):\n         req = Request(\"http://example.com/index.html\")\n \n         class InvalidProcessResponseMiddleware:\n@@ -162,18 +155,12 @@ class TestProcessResponseInvalidOutput(TestManagerBase):\n                 return 1\n \n         self.mwman._add_middleware(InvalidProcessResponseMiddleware())\n-        download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        assert isinstance(results[0], Failure)\n-        assert isinstance(results[0].value, _InvalidOutput)\n+        with pytest.raises(_InvalidOutput):\n+            await self._download(req)\n \n-\n-class TestProcessExceptionInvalidOutput(TestManagerBase):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_process_exception(self):\n         \"\"\"Invalid return value for process_exception method should raise an exception\"\"\"\n-\n-    def test_invalid_process_exception(self):\n         req = Request(\"http://example.com/index.html\")\n \n         class InvalidProcessExceptionMiddleware:\n@@ -184,18 +171,15 @@ class TestProcessExceptionInvalidOutput(TestManagerBase):\n                 return 1\n \n         self.mwman._add_middleware(InvalidProcessExceptionMiddleware())\n-        download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        assert isinstance(results[0], Failure)\n-        assert isinstance(results[0].value, _InvalidOutput)\n+        with pytest.raises(_InvalidOutput):\n+            await self._download(req)\n \n \n class TestMiddlewareUsingDeferreds(TestManagerBase):\n     \"\"\"Middlewares using Deferreds should work\"\"\"\n \n-    def test_deferred(self):\n+    @deferred_f_from_coro_f\n+    async def test_deferred(self):\n         resp = Response(\"http://example.com/index.html\")\n \n         class DeferredMiddleware:\n@@ -211,12 +195,10 @@ class TestMiddlewareUsingDeferreds(TestManagerBase):\n         self.mwman._add_middleware(DeferredMiddleware())\n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        self._wait(dfd)\n-\n-        assert results[0] is resp\n+        result = await maybe_deferred_to_future(\n+            self.mwman.download(download_func, req, self.spider)\n+        )\n+        assert result is resp\n         assert not download_func.called\n \n \n@@ -224,27 +206,27 @@ class TestMiddlewareUsingDeferreds(TestManagerBase):\n class TestMiddlewareUsingCoro(TestManagerBase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n-    def test_asyncdef(self):\n+    @deferred_f_from_coro_f\n+    async def test_asyncdef(self):\n         resp = Response(\"http://example.com/index.html\")\n \n         class CoroMiddleware:\n             async def process_request(self, request, spider):\n-                await defer.succeed(42)\n+                await succeed(42)\n                 return resp\n \n         self.mwman._add_middleware(CoroMiddleware())\n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        self._wait(dfd)\n-\n-        assert results[0] is resp\n+        result = await maybe_deferred_to_future(\n+            self.mwman.download(download_func, req, self.spider)\n+        )\n+        assert result is resp\n         assert not download_func.called\n \n     @pytest.mark.only_asyncio\n-    def test_asyncdef_asyncio(self):\n+    @deferred_f_from_coro_f\n+    async def test_asyncdef_asyncio(self):\n         resp = Response(\"http://example.com/index.html\")\n \n         class CoroMiddleware:\n@@ -255,10 +237,8 @@ class TestMiddlewareUsingCoro(TestManagerBase):\n         self.mwman._add_middleware(CoroMiddleware())\n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        dfd = self.mwman.download(download_func, req, self.spider)\n-        results = []\n-        dfd.addBoth(results.append)\n-        self._wait(dfd)\n-\n-        assert results[0] is resp\n+        result = await maybe_deferred_to_future(\n+            self.mwman.download(download_func, req, self.spider)\n+        )\n+        assert result is resp\n         assert not download_func.called\n\n@@ -1,12 +1,12 @@\n from __future__ import annotations\n \n from collections.abc import AsyncIterator, Iterable\n+from typing import Any\n from unittest import mock\n \n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.spidermw import SpiderMiddlewareManager\n@@ -14,7 +14,11 @@ from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n from scrapy.utils.asyncgen import collect_asyncgen\n-from scrapy.utils.defer import deferred_from_coro, maybe_deferred_to_future\n+from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n+    deferred_from_coro,\n+    maybe_deferred_to_future,\n+)\n from scrapy.utils.test import get_crawler\n \n \n@@ -26,53 +30,51 @@ class TestSpiderMiddleware(TestCase):\n         self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n \n-    def _scrape_response(self):\n+    async def _scrape_response(self) -> Any:\n         \"\"\"Execute spider mw manager's scrape_response method and return the result.\n         Raise exception in case of failure.\n         \"\"\"\n         scrape_func = mock.MagicMock()\n-        dfd = self.mwman.scrape_response(\n+        return await maybe_deferred_to_future(\n+            self.mwman.scrape_response(\n                 scrape_func, self.response, self.request, self.spider\n             )\n-        # catch deferred result and return the value\n-        results = []\n-        dfd.addBoth(results.append)\n-        self._wait(dfd)\n-        return results[0]\n+        )\n \n \n class TestProcessSpiderInputInvalidOutput(TestSpiderMiddleware):\n     \"\"\"Invalid return value for process_spider_input method\"\"\"\n \n-    def test_invalid_process_spider_input(self):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_process_spider_input(self):\n         class InvalidProcessSpiderInputMiddleware:\n             def process_spider_input(self, response, spider):\n                 return 1\n \n         self.mwman._add_middleware(InvalidProcessSpiderInputMiddleware())\n-        result = self._scrape_response()\n-        assert isinstance(result, Failure)\n-        assert isinstance(result.value, _InvalidOutput)\n+        with pytest.raises(_InvalidOutput):\n+            await self._scrape_response()\n \n \n class TestProcessSpiderOutputInvalidOutput(TestSpiderMiddleware):\n     \"\"\"Invalid return value for process_spider_output method\"\"\"\n \n-    def test_invalid_process_spider_output(self):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_process_spider_output(self):\n         class InvalidProcessSpiderOutputMiddleware:\n             def process_spider_output(self, response, result, spider):\n                 return 1\n \n         self.mwman._add_middleware(InvalidProcessSpiderOutputMiddleware())\n-        result = self._scrape_response()\n-        assert isinstance(result, Failure)\n-        assert isinstance(result.value, _InvalidOutput)\n+        with pytest.raises(_InvalidOutput):\n+            await self._scrape_response()\n \n \n class TestProcessSpiderExceptionInvalidOutput(TestSpiderMiddleware):\n     \"\"\"Invalid return value for process_spider_exception method\"\"\"\n \n-    def test_invalid_process_spider_exception(self):\n+    @deferred_f_from_coro_f\n+    async def test_invalid_process_spider_exception(self):\n         class InvalidProcessSpiderOutputExceptionMiddleware:\n             def process_spider_exception(self, response, exception, spider):\n                 return 1\n@@ -83,15 +85,15 @@ class TestProcessSpiderExceptionInvalidOutput(TestSpiderMiddleware):\n \n         self.mwman._add_middleware(InvalidProcessSpiderOutputExceptionMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n-        result = self._scrape_response()\n-        assert isinstance(result, Failure)\n-        assert isinstance(result.value, _InvalidOutput)\n+        with pytest.raises(_InvalidOutput):\n+            await self._scrape_response()\n \n \n class TestProcessSpiderExceptionReRaise(TestSpiderMiddleware):\n     \"\"\"Re raise the exception by returning None\"\"\"\n \n-    def test_process_spider_exception_return_none(self):\n+    @deferred_f_from_coro_f\n+    async def test_process_spider_exception_return_none(self):\n         class ProcessSpiderExceptionReturnNoneMiddleware:\n             def process_spider_exception(self, response, exception, spider):\n                 return None\n@@ -102,9 +104,8 @@ class TestProcessSpiderExceptionReRaise(TestSpiderMiddleware):\n \n         self.mwman._add_middleware(ProcessSpiderExceptionReturnNoneMiddleware())\n         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())\n-        result = self._scrape_response()\n-        assert isinstance(result, Failure)\n-        assert isinstance(result.value, ZeroDivisionError)\n+        with pytest.raises(ZeroDivisionError):\n+            await self._scrape_response()\n \n \n class TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
