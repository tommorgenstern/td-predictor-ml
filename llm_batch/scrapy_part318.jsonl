{"custom_id": "scrapy#7afcd634ab7d71a465cd9406091d371dca105789", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1476 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,6 @@ from twisted.web.client import ResponseFailed\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5cf403295d44bd2531ee3fe2f07057cf155e9804", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 272 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -51,15 +51,6 @@ class RecoveryAsyncGenSpider(RecoverySpider):\n             yield r\n \n \n-class RecoveryMiddleware:\n-    def process_spider_exception(self, response, exception, spider):\n-        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n-        return [\n-            {'from': 'process_spider_exception'},\n-            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n-        ]\n-\n-\n # ================================================================================\n # (1) exceptions from a spider middleware's process_spider_input method\n class FailProcessSpiderInputMiddleware:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e9b52c3e0e4eb51aaad37e796b3ee810fd919a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 11 | Methods Changed: 12 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 34 | Churn Cumulative: 603 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,9 +41,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         process_spider_exception = getattr(mw, 'process_spider_exception', None)\n         self.methods['process_spider_exception'].appendleft(process_spider_exception)\n \n-    def scrape_response(self, scrape_func, response, request, spider):\n-\n-        def process_spider_input(response):\n+    def _process_spider_input(self, scrape_func, response, request, spider):\n         for method in self.methods['process_spider_input']:\n             try:\n                 result = method(response=response, spider=spider)\n@@ -57,17 +55,18 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 return scrape_func(Failure(), request, spider)\n         return scrape_func(response, request, spider)\n \n-        def _evaluate_iterable(iterable, exception_processor_index, recover_to):\n+    def _evaluate_iterable(self, response, spider, iterable, exception_processor_index, recover_to):\n         try:\n             for r in iterable:\n                 yield r\n         except Exception as ex:\n-                exception_result = process_spider_exception(Failure(ex), exception_processor_index)\n+            exception_result = self._process_spider_exception(response, spider, Failure(ex),\n+                                                              exception_processor_index)\n             if isinstance(exception_result, Failure):\n                 raise\n             recover_to.extend(exception_result)\n \n-        def process_spider_exception(_failure, start_index=0):\n+    def _process_spider_exception(self, response, spider, _failure, start_index=0):\n         exception = _failure.value\n         # don't handle _InvalidOutput exception\n         if isinstance(exception, _InvalidOutput):\n@@ -80,7 +79,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             if _isiterable(result):\n                 # stop exception handling by handing control over to the\n                 # process_spider_output chain if an iterable has been returned\n-                    return process_spider_output(result, method_index + 1)\n+                return self._process_spider_output(response, spider, result, method_index + 1)\n             elif result is None:\n                 continue\n             else:\n@@ -89,7 +88,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 raise _InvalidOutput(msg)\n         return _failure\n \n-        def process_spider_output(result, start_index=0):\n+    def _process_spider_output(self, response, spider, result, start_index=0):\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n         recovered = MutableChain()\n@@ -102,12 +101,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 # might fail directly if the output value is not a generator\n                 result = method(response=response, result=result, spider=spider)\n             except Exception as ex:\n-                    exception_result = process_spider_exception(Failure(ex), method_index + 1)\n+                exception_result = self._process_spider_exception(response, spider, Failure(ex), method_index + 1)\n                 if isinstance(exception_result, Failure):\n                     raise\n                 return exception_result\n             if _isiterable(result):\n-                    result = _evaluate_iterable(result, method_index + 1, recovered)\n+                result = self._evaluate_iterable(response, spider, result, method_index + 1, recovered)\n             else:\n                 msg = (f\"Middleware {_fname(method)} must return an \"\n                        f\"iterable, got {type(result)}\")\n@@ -115,12 +114,19 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n         return MutableChain(result, recovered)\n \n-        def process_callback_output(result):\n+    def _process_callback_output(self, response, spider, result):\n         recovered = MutableChain()\n-            result = _evaluate_iterable(result, 0, recovered)\n-            return MutableChain(process_spider_output(result), recovered)\n+        result = self._evaluate_iterable(response, spider, result, 0, recovered)\n+        return MutableChain(self._process_spider_output(response, spider, result), recovered)\n \n-        dfd = mustbe_deferred(process_spider_input, response)\n+    def scrape_response(self, scrape_func, response, request, spider):\n+        def process_callback_output(result):\n+            return self._process_callback_output(response, spider, result)\n+\n+        def process_spider_exception(_failure):\n+            return self._process_spider_exception(response, spider, _failure)\n+\n+        dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)\n         dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n         return dfd\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#15b501c0898150de686ee42dd4d78a411891e795", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1365 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -346,7 +346,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             elif isinstance(event, SettingsAcknowledged):\n                 self.settings_acknowledged(event)\n             elif isinstance(event, UnknownFrameReceived):\n-                logger.warning(f'Unknown frame received: {event.frame}')\n+                logger.warning('Unknown frame received: %s', event.frame)\n \n     # Event handler functions starts here\n     def connection_terminated(self, event: ConnectionTerminated) -> None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67cff0e8a949a83220ae6d7d2c2ca40e6c8a8207", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 2 | Churn Cumulative: 605 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             exception_result = self._process_spider_exception(response, spider, Failure(ex),\n                                                               exception_processor_index)\n             if isinstance(exception_result, Failure):\n-                raise\n+                raise  # pylint: disable=E0704\n             recover_to.extend(exception_result)\n \n         def _evaluate_normal_iterable(iterable):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abbbfbbb38a87b9f7259fe47493d5810b2439963", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 17 | Churn Cumulative: 103 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,10 @@\n+from pytest import mark\n from twisted.trial import unittest\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n from scrapy.utils.defer import (\n+    deferred_f_from_coro_f,\n     iter_errback,\n     mustbe_deferred,\n     process_chain,\n@@ -117,3 +119,18 @@ class IterErrbackTest(unittest.TestCase):\n         self.assertEqual(out, [0, 1, 2, 3, 4])\n         self.assertEqual(len(errors), 1)\n         self.assertIsInstance(errors[0].value, ZeroDivisionError)\n+\n+\n+class AsyncDefTestsuiteTest(unittest.TestCase):\n+    @deferred_f_from_coro_f\n+    async def test_deferred_f_from_coro_f(self):\n+        pass\n+\n+    @deferred_f_from_coro_f\n+    async def test_deferred_f_from_coro_f_generator(self):\n+        yield\n+\n+    @mark.xfail(reason=\"Checks that the test is actually executed\", strict=True)\n+    @deferred_f_from_coro_f\n+    async def test_deferred_f_from_coro_f_xfail(self):\n+        raise Exception(\"This is expected to be raised\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e80f37bd3fa0f2262e898f712541bb7a12e89110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 19 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from unittest import mock\n from twisted.internet import defer, error, reactor\n from twisted.trial import unittest\n from twisted.web import server\n+from twisted.web.error import SchemeNotSupported\n \n from scrapy.core.downloader.handlers.http2 import H2DownloadHandler\n from scrapy.http import Request\n@@ -48,6 +49,12 @@ class Https2TestCase(Https11TestCase):\n             reactor.callLater(.1, d.callback, logger)\n             yield d\n \n+    @defer.inlineCallbacks\n+    def test_unsupported_scheme(self):\n+        request = Request(\"ftp://unsupported.scheme\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        yield self.assertFailure(d, SchemeNotSupported)\n+\n     def test_download_broken_content_cause_data_loss(self, url='broken'):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4418f78941fc9d89ada9ad0436ebd0ae4ece1017", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1373 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -233,13 +233,11 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n     def handshakeCompleted(self) -> None:\n         \"\"\"We close the connection with InvalidNegotiatedProtocol exception\n         when the connection was not made via h2 protocol\"\"\"\n-        negotiated_protocol = self.transport.negotiatedProtocol\n-        if isinstance(negotiated_protocol, bytes):\n-            negotiated_protocol = str(self.transport.negotiatedProtocol, 'utf-8')\n-        if negotiated_protocol != 'h2':\n+        protocol = self.transport.negotiatedProtocol\n+        if protocol is not None and protocol != b\"h2\":\n             # Here we have not initiated the connection yet\n             # So, no need to send a GOAWAY frame to the remote\n-            self._lose_connection_with_error([InvalidNegotiatedProtocol(negotiated_protocol)])\n+            self._lose_connection_with_error([InvalidNegotiatedProtocol(protocol.decode(\"utf-8\"))])\n \n     def _check_received_data(self, data: bytes) -> None:\n         \"\"\"Checks for edge cases where the connection to remote fails\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49af7c4c8b71f8643fe13c20261f97323117f980", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 200 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,8 @@ from pathlib import Path\n \n import pytest\n \n+from scrapy.utils.reactor import install_reactor\n+\n from tests.keys import generate_keys\n \n \n@@ -40,6 +42,14 @@ def pytest_collection_modifyitems(session, config, items):\n         pass\n \n \n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--reactor\",\n+        default=\"default\",\n+        choices=[\"default\", \"asyncio\"],\n+    )\n+\n+\n @pytest.fixture(scope='class')\n def reactor_pytest(request):\n     if not request.cls:\n@@ -55,5 +65,10 @@ def only_asyncio(request, reactor_pytest):\n         pytest.skip('This test is only run with --reactor=asyncio')\n \n \n+def pytest_configure(config):\n+    if config.getoption(\"--reactor\") == \"asyncio\":\n+        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n+\n # Generate localhost certificate files, needed by some tests\n generate_keys()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40eab1d473a78369702490a901b5d304a40baf3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 3 | Churn Cumulative: 106 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -5,14 +5,13 @@ from twisted.python.failure import Failure\n \n from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.defer import (\n+    aiter_errback,\n     deferred_f_from_coro_f,\n     iter_errback,\n-    aiter_errback,\n     mustbe_deferred,\n     process_chain,\n     process_chain_both,\n     process_parallel,\n-    deferred_f_from_coro_f,\n )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9a5385146c741825d8c0b089cf8e8318dca9403", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 14 | Files Changed: 4 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 24 | Churn Cumulative: 1123 | Contributors (this commit): 13 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,10 +3,10 @@ Depth Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+\n import logging\n \n from scrapy.http import Request\n-from scrapy.utils.middlewares import process_iterable_helper\n \n logger = logging.getLogger(__name__)\n \n@@ -55,4 +55,4 @@ class DepthMiddleware:\n             if self.verbose_stats:\n                 self.stats.inc_value('request_depth_count/0', spider=spider)\n \n-        return process_iterable_helper(result or (), in_predicate=_filter)\n+        return (r for r in result or () if _filter(r))\n\n@@ -10,7 +10,6 @@ import warnings\n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.middlewares import process_iterable_helper\n \n logger = logging.getLogger(__name__)\n \n@@ -27,11 +26,11 @@ class OffsiteMiddleware:\n         return o\n \n     def process_spider_output(self, response, result, spider):\n-        def in_predicate(x):\n-            if not isinstance(x, Request):\n-                return True\n+        for x in result:\n+            if isinstance(x, Request):\n                 if x.dont_filter or self.should_follow(x, spider):\n-                return True\n+                    yield x\n+                else:\n                     domain = urlparse_cached(x).hostname\n                     if domain and domain not in self.domains_seen:\n                         self.domains_seen.add(domain)\n@@ -40,9 +39,8 @@ class OffsiteMiddleware:\n                             {'domain': domain, 'request': x}, extra={'spider': spider})\n                         self.stats.inc_value('offsite/domains', spider=spider)\n                     self.stats.inc_value('offsite/filtered', spider=spider)\n-            return False\n-\n-        return process_iterable_helper(result or (), in_predicate=in_predicate)\n+            else:\n+                yield x\n \n     def should_follow(self, request, spider):\n         regex = self.host_regex\n\n@@ -10,7 +10,6 @@ from w3lib.url import safe_url_string\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n-from scrapy.utils.middlewares import process_iterable_helper\n from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n from scrapy.utils.url import strip_url\n@@ -338,7 +337,7 @@ class RefererMiddleware:\n                 if referrer is not None:\n                     r.headers.setdefault('Referer', referrer)\n             return r\n-        return process_iterable_helper(result or (), processor=_set_referer)\n+        return (_set_referer(r) for r in result or ())\n \n     def request_scheduled(self, request, spider):\n         # check redirected request to patch \"Referer\" header if necessary\n\n@@ -8,7 +8,6 @@ import logging\n \n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n-from scrapy.utils.middlewares import process_iterable_helper\n \n logger = logging.getLogger(__name__)\n \n@@ -35,4 +34,4 @@ class UrlLengthMiddleware:\n             else:\n                 return True\n \n-        return process_iterable_helper(result or (), in_predicate=_filter)\n+        return (r for r in result or () if _filter(r))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5fc27b1e6fc8532dc5bc08fc5abf5a0daa8ef0c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 55 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 55 | Churn Cumulative: 274 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -75,61 +75,6 @@ def get_retry_request(\n         return None\n \n \n-def retry_request(\n-    request,\n-    *,\n-    reason,\n-    spider,\n-    max_retry_times=None,\n-    priority_adjust=None,\n-):\n-    new_request = get_retry_request(\n-        request,\n-        reason=reason,\n-        spider=spider,\n-        max_retry_times=max_retry_times,\n-        priority_adjust=priority_adjust,\n-    )\n-    if new_request:\n-        return [new_request]\n-    return []\n-\n-\n-class RetrySpiderMixin:\n-\n-    def get_retry_request(\n-        self,\n-        request,\n-        *,\n-        reason,\n-        max_retry_times=None,\n-        priority_adjust=None,\n-    ):\n-        return get_retry_request(\n-            request,\n-            reason=reason,\n-            spider=self,\n-            max_retry_times=max_retry_times,\n-            priority_adjust=priority_adjust,\n-        )\n-\n-    def retry_request(\n-        self,\n-        request,\n-        *,\n-        reason,\n-        max_retry_times=None,\n-        priority_adjust=None,\n-    ):\n-        return retry_request(\n-            request,\n-            reason=reason,\n-            spider=self,\n-            max_retry_times=max_retry_times,\n-            priority_adjust=priority_adjust,\n-        )\n-\n-\n class RetryMiddleware:\n \n     # IOError is raised by the HttpCompression middleware when trying to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#825462615a8df8e2274cf71c8a9bbea68c89262a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 276 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -34,8 +34,8 @@ logger = logging.getLogger(__name__)\n def get_retry_request(\n     request,\n     *,\n-    reason,\n     spider,\n+    reason='unspecified',\n     max_retry_times=None,\n     priority_adjust=None,\n ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec836dcc9290f50bad27c874cd0c25a87781735c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 283 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,9 +69,12 @@ def get_retry_request(\n         return new_request\n     else:\n         stats.inc_value('retry/max_reached')\n-        logger.error(\"Gave up retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n+        logger.error(\n+            \"Gave up retrying %(request)s (failed %(retry_times)d times): \"\n+            \"%(reason)s\",\n             {'request': request, 'retry_times': retry_times, 'reason': reason},\n-                        extra={'spider': spider})\n+            extra={'spider': spider},\n+        )\n         return None\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ab990181c6502624ceb0cca6783d99b30d30c20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 49 | Churn Cumulative: 332 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,16 +39,51 @@ def get_retry_request(\n     max_retry_times=None,\n     priority_adjust=None,\n ):\n+    \"\"\"\n+    Returns a new :class:`~scrapy.Request` object to retry the specified\n+    request, or ``None`` if retries of the specified request have been\n+    exhausted.\n+\n+    For example, in a :class:`~scrapy.Spider` callback, you could use it as\n+    follows::\n+\n+        def parse(self, response):\n+            if not response.text:\n+                new_request = get_retry_request(\n+                    response.request,\n+                    spider=self,\n+                    reason='empty',\n+                )\n+                if new_request:\n+                    yield new_request\n+                return\n+\n+    *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n+    retry request. It is used to access the :ref:`settings <topics-settings>`\n+    and :ref:`stats <topics-stats>`, and to provide extra logging context (see\n+    :func:`logging.debug`).\n+\n+    *reason* is a string or an :class:`Exception` object that indicates the\n+    reason why the request needs to be retried. It is used to name retry stats.\n+\n+    *max_retry_times* is a number that determines the maximum number of times\n+    that *request* can be retried. If not specified or ``None``, the number is\n+    read from the :reqmeta:`max_retry_times` meta key of the request. If the\n+    :reqmeta:`max_retry_times` meta key is not defined or ``None``, the number\n+    is read from the :setting:`RETRY_TIMES` setting.\n+\n+    *priority_adjust* is a number that determines how the priority of the new\n+    request changes in relation to *request*. If not specified, the number is\n+    read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n+    \"\"\"\n     settings = spider.crawler.settings\n     stats = spider.crawler.stats\n     retry_times = request.meta.get('retry_times', 0) + 1\n-    request_max_retry_times = request.meta.get(\n-        'max_retry_times',\n-        max_retry_times,\n-    )\n-    if request_max_retry_times is None:\n-        request_max_retry_times = settings.getint('RETRY_TIMES')\n-    if retry_times <= request_max_retry_times:\n+    if max_retry_times is None:\n+        max_retry_times = request.meta.get('max_retry_times')\n+    if max_retry_times is None:\n+        max_retry_times = settings.getint('RETRY_TIMES')\n+    if retry_times <= max_retry_times:\n         logger.debug(\n             \"Retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n             {'request': request, 'retry_times': retry_times, 'reason': reason},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80f5003c88f8ee4c5529c1a6d7fc3981efef511d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 457 | Lines Deleted: 50 | Files Changed: 2 | Hunks: 26 | Methods Changed: 30 | Complexity Δ (Sum/Max): 28/27 | Churn Δ: 507 | Churn Cumulative: 1165 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: 0.9948979591836735\n\nDIFF:\n@@ -10,6 +10,7 @@ Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n import logging\n+from inspect import isclass\n \n from twisted.internet import defer\n from twisted.internet.error import (\n@@ -96,6 +97,8 @@ def get_retry_request(\n             priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n         new_request.priority = request.priority + priority_adjust\n \n+        if isclass(reason):\n+            reason = reason()\n         if isinstance(reason, Exception):\n             reason = global_object_name(reason.__class__)\n \n@@ -149,10 +152,12 @@ class RetryMiddleware:\n             return self._retry(request, exception, spider)\n \n     def _retry(self, request, reason, spider):\n+        max_retry_times = request.meta.get('max_retry_times', self.max_retry_times)\n+        priority_adjust = request.meta.get('priority_adjust', self.priority_adjust)\n         return get_retry_request(\n             request,\n             reason=reason,\n             spider=spider,\n-            max_retry_times=self.max_retry_times,\n-            priority_adjust=self.priority_adjust,\n+            max_retry_times=max_retry_times,\n+            priority_adjust=priority_adjust,\n         )\n\n@@ -4,18 +4,22 @@ from twisted.internet.error import (\n     ConnectError,\n     ConnectionDone,\n     ConnectionLost,\n-    ConnectionRefusedError,\n     DNSLookupError,\n     TCPTimedOutError,\n-    TimeoutError,\n )\n from twisted.web.client import ResponseFailed\n \n-from scrapy.downloadermiddlewares.retry import RetryMiddleware\n-from scrapy.spiders import Spider\n+from scrapy.downloadermiddlewares.retry import (\n+    get_retry_request,\n+    RetryMiddleware,\n+)\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n+from testfixtures import LogCapture\n+\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n@@ -119,82 +123,480 @@ class RetryTest(unittest.TestCase):\n \n \n class MaxRetryTimesTest(unittest.TestCase):\n-    def setUp(self):\n-        self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler._create_spider('foo')\n-        self.mw = RetryMiddleware.from_crawler(self.crawler)\n-        self.mw.max_retry_times = 2\n-        self.invalid_url = 'http://www.scrapytest.org/invalid_url'\n+\n+    invalid_url = 'http://www.scrapytest.org/invalid_url'\n+\n+    def get_spider_and_middleware(self, settings=None):\n+        crawler = get_crawler(Spider, settings or {})\n+        spider = crawler._create_spider('foo')\n+        middleware = RetryMiddleware.from_crawler(crawler)\n+        return spider, middleware\n \n     def test_with_settings_zero(self):\n-\n-        # SETTINGS: RETRY_TIMES = 0\n-        self.mw.max_retry_times = 0\n-\n+        max_retry_times = 0\n+        settings = {'RETRY_TIMES': max_retry_times}\n+        spider, middleware = self.get_spider_and_middleware(settings)\n         req = Request(self.invalid_url)\n-        self._test_retry(req, DNSLookupError('foo'), self.mw.max_retry_times)\n+        self._test_retry(\n+            req,\n+            DNSLookupError('foo'),\n+            max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n \n     def test_with_metakey_zero(self):\n-\n-        # SETTINGS: meta(max_retry_times) = 0\n-        meta_max_retry_times = 0\n-\n-        req = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n-        self._test_retry(req, DNSLookupError('foo'), meta_max_retry_times)\n+        max_retry_times = 0\n+        spider, middleware = self.get_spider_and_middleware()\n+        meta = {'max_retry_times': max_retry_times}\n+        req = Request(self.invalid_url, meta=meta)\n+        self._test_retry(\n+            req,\n+            DNSLookupError('foo'),\n+            max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n \n     def test_without_metakey(self):\n-\n-        # SETTINGS: RETRY_TIMES is NON-ZERO\n-        self.mw.max_retry_times = 5\n-\n+        max_retry_times = 5\n+        settings = {'RETRY_TIMES': max_retry_times}\n+        spider, middleware = self.get_spider_and_middleware(settings)\n         req = Request(self.invalid_url)\n-        self._test_retry(req, DNSLookupError('foo'), self.mw.max_retry_times)\n+        self._test_retry(\n+            req,\n+            DNSLookupError('foo'),\n+            max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n \n     def test_with_metakey_greater(self):\n-\n-        # SETINGS: RETRY_TIMES < meta(max_retry_times)\n-        self.mw.max_retry_times = 2\n         meta_max_retry_times = 3\n+        middleware_max_retry_times = 2\n \n         req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n         req2 = Request(self.invalid_url)\n \n-        self._test_retry(req1, DNSLookupError('foo'), meta_max_retry_times)\n-        self._test_retry(req2, DNSLookupError('foo'), self.mw.max_retry_times)\n+        settings = {'RETRY_TIMES': middleware_max_retry_times}\n+        spider, middleware = self.get_spider_and_middleware(settings)\n+\n+        self._test_retry(\n+            req1,\n+            DNSLookupError('foo'),\n+            meta_max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n+        self._test_retry(\n+            req2,\n+            DNSLookupError('foo'),\n+            middleware_max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n \n     def test_with_metakey_lesser(self):\n-\n-        # SETINGS: RETRY_TIMES > meta(max_retry_times)\n-        self.mw.max_retry_times = 5\n         meta_max_retry_times = 4\n+        middleware_max_retry_times = 5\n \n         req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})\n         req2 = Request(self.invalid_url)\n \n-        self._test_retry(req1, DNSLookupError('foo'), meta_max_retry_times)\n-        self._test_retry(req2, DNSLookupError('foo'), self.mw.max_retry_times)\n+        settings = {'RETRY_TIMES': middleware_max_retry_times}\n+        spider, middleware = self.get_spider_and_middleware(settings)\n+\n+        self._test_retry(\n+            req1,\n+            DNSLookupError('foo'),\n+            meta_max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n+        self._test_retry(\n+            req2,\n+            DNSLookupError('foo'),\n+            middleware_max_retry_times,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n \n     def test_with_dont_retry(self):\n+        max_retry_times = 4\n+        spider, middleware = self.get_spider_and_middleware()\n+        meta = {\n+            'max_retry_times': max_retry_times,\n+            'dont_retry': True,\n+        }\n+        req = Request(self.invalid_url, meta=meta)\n+        self._test_retry(\n+            req,\n+            DNSLookupError('foo'),\n+            0,\n+            spider=spider,\n+            middleware=middleware,\n+        )\n \n-        # SETTINGS: meta(max_retry_times) = 4\n-        meta_max_retry_times = 4\n-\n-        req = Request(self.invalid_url, meta={\n-            'max_retry_times': meta_max_retry_times, 'dont_retry': True\n-        })\n-\n-        self._test_retry(req, DNSLookupError('foo'), 0)\n-\n-    def _test_retry(self, req, exception, max_retry_times):\n+    def _test_retry(\n+        self,\n+        req,\n+        exception,\n+        max_retry_times,\n+        spider=None,\n+        middleware=None,\n+    ):\n+        spider = spider or self.spider\n+        middleware = middleware or self.mw\n \n         for i in range(0, max_retry_times):\n-            req = self.mw.process_exception(req, exception, self.spider)\n+            req = middleware.process_exception(req, exception, spider)\n             assert isinstance(req, Request)\n \n         # discard it\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = middleware.process_exception(req, exception, spider)\n         self.assertEqual(req, None)\n \n \n+class GetRetryRequestTest(unittest.TestCase):\n+\n+    def get_spider(self, settings=None):\n+        crawler = get_crawler(Spider, settings or {})\n+        return crawler._create_spider('foo')\n+\n+    def test_basic_usage(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+            )\n+        self.assertIsInstance(new_request, Request)\n+        self.assertNotEqual(new_request, request)\n+        self.assertEqual(new_request.dont_filter, True)\n+        expected_retry_times = 1\n+        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)\n+        self.assertEqual(new_request.priority, -1)\n+        expected_reason = \"unspecified\"\n+        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_max_retries_reached(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        max_retry_times = 0\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                max_retry_times=max_retry_times,\n+            )\n+        self.assertEqual(new_request, None)\n+        self.assertEqual(\n+            spider.crawler.stats.get_value('retry/max_reached'),\n+            1\n+        )\n+        failure_count = max_retry_times + 1\n+        expected_reason = \"unspecified\"\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"ERROR\",\n+                f\"Gave up retrying {request} (failed {failure_count} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_one_retry(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                max_retry_times=1,\n+            )\n+        self.assertIsInstance(new_request, Request)\n+        self.assertNotEqual(new_request, request)\n+        self.assertEqual(new_request.dont_filter, True)\n+        expected_retry_times = 1\n+        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)\n+        self.assertEqual(new_request.priority, -1)\n+        expected_reason = \"unspecified\"\n+        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_two_retries(self):\n+        spider = self.get_spider()\n+        request = Request('https://example.com')\n+        new_request = request\n+        max_retry_times = 2\n+        for index in range(max_retry_times):\n+            with LogCapture() as log:\n+                new_request = get_retry_request(\n+                    new_request,\n+                    spider=spider,\n+                    max_retry_times=max_retry_times,\n+                )\n+            self.assertIsInstance(new_request, Request)\n+            self.assertNotEqual(new_request, request)\n+            self.assertEqual(new_request.dont_filter, True)\n+            expected_retry_times = index+1\n+            self.assertEqual(new_request.meta['retry_times'], expected_retry_times)\n+            self.assertEqual(new_request.priority, -expected_retry_times)\n+            expected_reason = \"unspecified\"\n+            for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+                value = spider.crawler.stats.get_value(stat)\n+                self.assertEqual(value, expected_retry_times)\n+            log.check_present(\n+                (\n+                    \"scrapy.downloadermiddlewares.retry\",\n+                    \"DEBUG\",\n+                    f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                    f\"{expected_reason}\",\n+                )\n+            )\n+\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                new_request,\n+                spider=spider,\n+                max_retry_times=max_retry_times,\n+            )\n+        self.assertEqual(new_request, None)\n+        self.assertEqual(\n+            spider.crawler.stats.get_value('retry/max_reached'),\n+            1\n+        )\n+        failure_count = max_retry_times + 1\n+        expected_reason = \"unspecified\"\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"ERROR\",\n+                f\"Gave up retrying {request} (failed {failure_count} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_no_spider(self):\n+        request = Request('https://example.com')\n+        with self.assertRaises(TypeError):\n+            get_retry_request(request)\n+\n+    def test_max_retry_times_setting(self):\n+        max_retry_times = 0\n+        spider = self.get_spider({'RETRY_TIMES': max_retry_times})\n+        request = Request('https://example.com')\n+        new_request = get_retry_request(\n+            request,\n+            spider=spider,\n+        )\n+        self.assertEqual(new_request, None)\n+\n+    def test_max_retry_times_meta(self):\n+        max_retry_times = 0\n+        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})\n+        meta = {'max_retry_times': max_retry_times}\n+        request = Request('https://example.com', meta=meta)\n+        new_request = get_retry_request(\n+            request,\n+            spider=spider,\n+        )\n+        self.assertEqual(new_request, None)\n+\n+    def test_max_retry_times_argument(self):\n+        max_retry_times = 0\n+        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})\n+        meta = {'max_retry_times': max_retry_times + 1}\n+        request = Request('https://example.com', meta=meta)\n+        new_request = get_retry_request(\n+            request,\n+            spider=spider,\n+            max_retry_times=max_retry_times,\n+        )\n+        self.assertEqual(new_request, None)\n+\n+    def test_priority_adjust_setting(self):\n+        priority_adjust = 1\n+        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust})\n+        request = Request('https://example.com')\n+        new_request = get_retry_request(\n+            request,\n+            spider=spider,\n+        )\n+        self.assertEqual(new_request.priority, priority_adjust)\n+\n+    def test_priority_adjust_argument(self):\n+        priority_adjust = 1\n+        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust+1})\n+        request = Request('https://example.com')\n+        new_request = get_retry_request(\n+            request,\n+            spider=spider,\n+            priority_adjust=priority_adjust,\n+        )\n+        self.assertEqual(new_request.priority, priority_adjust)\n+\n+    def test_log_extra_retry_success(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        with LogCapture(attributes=('spider',)) as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+            )\n+        log.check_present(spider)\n+\n+    def test_log_extra_retries_exceeded(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        with LogCapture(attributes=('spider',)) as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                max_retry_times=0,\n+            )\n+        log.check_present(spider)\n+\n+    def test_reason_string(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        expected_reason = 'because'\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                reason=expected_reason,\n+            )\n+        expected_retry_times = 1\n+        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):\n+            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_reason_builtin_exception(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        expected_reason = NotImplementedError()\n+        expected_reason_string = 'builtins.NotImplementedError'\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                reason=expected_reason,\n+            )\n+        expected_retry_times = 1\n+        stat = spider.crawler.stats.get_value(\n+            f'retry/reason_count/{expected_reason_string}'\n+        )\n+        self.assertEqual(stat, 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_reason_builtin_exception_class(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        expected_reason = NotImplementedError\n+        expected_reason_string = 'builtins.NotImplementedError'\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                reason=expected_reason,\n+            )\n+        expected_retry_times = 1\n+        stat = spider.crawler.stats.get_value(\n+            f'retry/reason_count/{expected_reason_string}'\n+        )\n+        self.assertEqual(stat, 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_reason_custom_exception(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        expected_reason = IgnoreRequest()\n+        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                reason=expected_reason,\n+            )\n+        expected_retry_times = 1\n+        stat = spider.crawler.stats.get_value(\n+            f'retry/reason_count/{expected_reason_string}'\n+        )\n+        self.assertEqual(stat, 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+    def test_reason_custom_exception_class(self):\n+        request = Request('https://example.com')\n+        spider = self.get_spider()\n+        expected_reason = IgnoreRequest\n+        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'\n+        with LogCapture() as log:\n+            new_request = get_retry_request(\n+                request,\n+                spider=spider,\n+                reason=expected_reason,\n+            )\n+        expected_retry_times = 1\n+        stat = spider.crawler.stats.get_value(\n+            f'retry/reason_count/{expected_reason_string}'\n+        )\n+        self.assertEqual(stat, 1)\n+        log.check_present(\n+            (\n+                \"scrapy.downloadermiddlewares.retry\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed {expected_retry_times} times): \"\n+                f\"{expected_reason}\",\n+            )\n+        )\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
