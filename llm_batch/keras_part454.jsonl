{"custom_id": "keras#751f64d8a20b53ea38c0907510aabe814283445e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 10 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 18520 | Contributors (this commit): 40 | Commits (past 90d): 53 | Contributors (cumulative): 95 | DMM Complexity: None\n\nDIFF:\n@@ -359,7 +359,7 @@ def EfficientNet(\n     if weights == \"imagenet\":\n         # Note that the normaliztion layer uses square value of STDDEV as the\n         # variance for the layer: result = (input - mean) / sqrt(var)\n-        # However, the orginal implemenetation uses (input - mean) / var to\n+        # However, the original implemenetation uses (input - mean) / var to\n         # normalize the input, we need to divide another sqrt(var) to match the\n         # original implementation.\n         # See https://github.com/tensorflow/tensorflow/issues/49930 for more details\n\n@@ -3324,7 +3324,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n         )\n         self.assertEqual(2, self._count_trace_file(logdir=self.logdir))\n \n-    # Test case that replicates a Github issue.\n+    # Test case that replicates a GitHub issue.\n     # https://github.com/tensorflow/tensorflow/issues/37543\n     def test_TensorBoard_autoTrace_profileTwiceGraphMode(self):\n         tf.compat.v1.disable_eager_execution()\n\n@@ -44,7 +44,7 @@ def load_data(\n     This was originally generated by parsing and preprocessing the classic\n     Reuters-21578 dataset, but the preprocessing code is no longer packaged\n     with Keras. See this\n-    [github discussion](https://github.com/keras-team/keras/issues/12072)\n+    [GitHub discussion](https://github.com/keras-team/keras/issues/12072)\n     for more info.\n \n     Each newswire is encoded as a list of word indexes (integers).\n\n@@ -338,7 +338,7 @@ def _init_state_variable_for_rng(model, layout_map):\n     Since the BaseRandomLayer in keras explicitly untrack the tf.random.Generator,\n     the variable in it will stay as LazyInitVariable, which cause runtime error if\n     we don't replace them with proper DVariable. Since user usually are not\n-    aware the existance of those variable, we will just give them replicated\n+    aware the existence of those variable, we will just give them replicated\n     layout since they are tiny.\n \n     Args:\n\n@@ -411,7 +411,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n         supplied with a vocabulary.\n \n         During `adapt()`, the layer will build a vocabulary of all integer tokens\n-        seen in the dataset, sorted by occurance count, with ties broken by sort\n+        seen in the dataset, sorted by occurrence count, with ties broken by sort\n         order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n         is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n         adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n\n@@ -363,7 +363,7 @@ class StringLookup(index_lookup.IndexLookup):\n         supplied with a vocabulary.\n \n         During `adapt()`, the layer will build a vocabulary of all string tokens\n-        seen in the dataset, sorted by occurance count, with ties broken by sort\n+        seen in the dataset, sorted by occurrence count, with ties broken by sort\n         order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n         is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n         adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n\n@@ -416,7 +416,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         dataset or supplied with a vocabulary.\n \n         During `adapt()`, the layer will build a vocabulary of all string tokens\n-        seen in the dataset, sorted by occurance count, with ties broken by sort\n+        seen in the dataset, sorted by occurrence count, with ties broken by sort\n         order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n         is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n         adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n\n@@ -87,7 +87,7 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n         self.assertTrue(layer._could_use_gpu_kernel)\n \n     def test_static_shape_inference_LSTM(self):\n-        # Github issue: 15165\n+        # GitHub issue: 15165\n         timesteps = 3\n         embedding_dim = 4\n         units = 2\n@@ -954,7 +954,7 @@ class LSTMLayerTest(test_combinations.TestCase):\n         )\n \n     def test_static_shape_inference_LSTM(self):\n-        # Github issue: 15165\n+        # GitHub issue: 15165\n         timesteps = 3\n         embedding_dim = 4\n         units = 2\n\n@@ -506,7 +506,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertEqual(len(layer.get_losses_for(x)), 1)\n \n     def testNameScopeIsConsistentWithVariableScope(self):\n-        # Github issue 13429.\n+        # GitHub issue 13429.\n \n         class MyLayer(base_tf_layers.Layer):\n             def build(self, input_shape):\n\n@@ -683,7 +683,7 @@ class VariableScopeTest(tf.test.TestCase):\n     @tf_test_utils.run_in_graph_and_eager_modes\n     @run_inside_wrap_function_in_eager_mode\n     def testReuseScopeWithoutNameScopeCollision(self):\n-        # Github issue: #13429\n+        # GitHub issue: #13429\n         with self.cached_session():\n             with tf.compat.v1.variable_scope(\"outer\"):\n                 with tf.compat.v1.variable_scope(\"inner\") as inner:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e3627ae6a1e1be80bdac6799b36ae7c29f5ae41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 635 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -273,7 +273,9 @@ class Conv(Layer):\n             inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n \n         if self.groups > 1:\n-            outputs = self._jit_compiled_convolution_op(inputs, self.kernel)\n+            outputs = self._jit_compiled_convolution_op(\n+                inputs, tf.convert_to_tensor(self.kernel)\n+            )\n         else:\n             outputs = self.convolution_op(inputs, self.kernel)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44dda1af649ebabf18859222d615481cba97aa7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 966 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2080,5 +2080,5 @@ class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):\n                 self.assertAllClose(v1, v2)\n \n \n-if __name__ == \"__main__\":\n+if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa6d9107a498f7c2403ff28c7b389a1a0c5cc083", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2499 | Lines Deleted: 2168 | Files Changed: 38 | Hunks: 872 | Methods Changed: 358 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4667 | Churn Cumulative: 92962 | Contributors (this commit): 166 | Commits (past 90d): 170 | Contributors (cumulative): 460 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -153,8 +153,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       trainable: Boolean, whether the layer's variables should be trainable.\n       name: String name of the layer.\n       dtype: The dtype of the layer's computations and weights. Can also be a\n-        `tf.keras.mixed_precision.Policy`, which allows the computation and weight\n-        dtype to differ. Default of `None` means to use\n+        `tf.keras.mixed_precision.Policy`, which allows the computation and\n+        weight dtype to differ. Default of `None` means to use\n         `tf.keras.mixed_precision.global_policy()`, which is a float32 policy\n         unless set to different value.\n       dynamic: Set this to `True` if your layer should only be run eagerly, and\n@@ -169,8 +169,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       dtype: The dtype of the layer's weights.\n       variable_dtype: Alias of `dtype`.\n       compute_dtype: The dtype of the layer's computations. Layers automatically\n-        cast inputs to this dtype which causes the computations and output to also\n-        be in this dtype. When mixed precision is used with a\n+        cast inputs to this dtype which causes the computations and output to\n+        also be in this dtype. When mixed precision is used with a\n         `tf.keras.mixed_precision.Policy`, this will be different than\n         `variable_dtype`.\n       dtype_policy: The layer's dtype policy. See the\n@@ -195,21 +195,22 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       state. `__call__()` will automatically build the layer (if it has not been\n       built yet) by calling `build()`.\n     * `call(self, inputs, *args, **kwargs)`: Called in `__call__` after making\n-      sure `build()` has been called. `call()` performs the logic of applying the\n-      layer to the `inputs`. The first invocation may additionally create state\n-      that could not be conveniently created in `build()`; see its docstring\n-      for details.\n+      sure `build()` has been called. `call()` performs the logic of applying\n+      the layer to the `inputs`. The first invocation may additionally create\n+      state that could not be conveniently created in `build()`; see its\n+      docstring for details.\n       Two reserved keyword arguments you can optionally use in `call()` are:\n         - `training` (boolean, whether the call is in inference mode or training\n           mode). See more details in [the layer/model subclassing guide](\n           https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method)\n         - `mask` (boolean tensor encoding masked timesteps in the input, used\n-          in RNN layers). See more details in [the layer/model subclassing guide](\n+          in RNN layers). See more details in\n+          [the layer/model subclassing guide](\n           https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method)\n-      A typical signature for this method is `call(self, inputs)`, and user could\n-      optionally add `training` and `mask` if the layer need them. `*args` and\n-      `**kwargs` is only useful for future extension when more input parameters\n-      are planned to be added.\n+      A typical signature for this method is `call(self, inputs)`, and user\n+      could optionally add `training` and `mask` if the layer need them. `*args`\n+      and `**kwargs` is only useful for future extension when more input\n+      parameters are planned to be added.\n     * `get_config(self)`: Returns a dictionary containing the configuration used\n       to initialize this layer. If the keys differ from the arguments\n       in `__init__`, then override `from_config(self)` as well.\n@@ -352,12 +353,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 f\"but got: {trainable}\"\n             )\n         self._trainable = trainable\n-        # A stateful layer is a layer whose updates are run during inference too,\n-        # for instance stateful RNNs.\n+        # A stateful layer is a layer whose updates are run during inference\n+        # too, for instance stateful RNNs.\n         self._stateful = False\n-        # Indicates whether `build` needs to be called upon layer call, to create\n-        # the layer's weights. (Note that the first call() may also create weights,\n-        # independent of build().)\n+        # Indicates whether `build` needs to be called upon layer call, to\n+        # create the layer's weights. (Note that the first call() may also\n+        # create weights, independent of build().)\n         self.built = False\n         # Provides information about which inputs are compatible with the layer.\n         self._input_spec = None\n@@ -384,26 +385,27 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         self._updates = []\n         # Object to store all thread local layer properties.\n         self._thread_local = threading.local()\n-        # A list of zero-argument lambdas which return Tensors, used for variable\n-        # regularizers.\n+        # A list of zero-argument lambdas which return Tensors, used for\n+        # variable regularizers.\n         self._callable_losses = []\n         # A list of symbolic Tensors containing activity regularizers and losses\n         # manually added through `add_loss` in graph-building mode.\n         self._losses = []\n-        # A list of metric instances corresponding to the symbolic metric tensors\n-        # added using the `add_metric` API.\n+        # A list of metric instances corresponding to the symbolic metric\n+        # tensors added using the `add_metric` API.\n         self._metrics = []\n-        # Ensures the same metric is not added multiple times in `MirroredStrategy`.\n+        # Ensures the same metric is not added multiple times in\n+        # `MirroredStrategy`.\n         self._metrics_lock = threading.Lock()\n \n         # Note that models also have a dtype policy, as they are layers. For\n-        # functional models, the policy is only used in Model.compile, which wraps\n-        # the optimizer with a LossScaleOptimizer if the policy name is\n-        # \"mixed_float16\". Subclassed models additionally use the policy's compute\n-        # and variable dtypes, as like any ordinary layer.\n+        # functional models, the policy is only used in Model.compile, which\n+        # wraps the optimizer with a LossScaleOptimizer if the policy name is\n+        # \"mixed_float16\". Subclassed models additionally use the policy's\n+        # compute and variable dtypes, as like any ordinary layer.\n         self._set_dtype_policy(dtype)\n-        # Boolean indicating whether the layer automatically casts its inputs to the\n-        # layer's compute_dtype.\n+        # Boolean indicating whether the layer automatically casts its inputs to\n+        # the layer's compute_dtype.\n         self._autocast = kwargs.get(\n             \"autocast\", base_layer_utils.v2_dtype_behavior_enabled()\n         )\n@@ -421,12 +423,14 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         self._init_call_fn_args()\n \n-        # Whether the `call` method can be used to build a TF graph without issues.\n-        # This attribute has no effect if the model is created using the Functional\n-        # API. Instead, `model.dynamic` is determined based on the internal layers.\n+        # Whether the `call` method can be used to build a TF graph without\n+        # issues.  This attribute has no effect if the model is created using\n+        # the Functional API. Instead, `model.dynamic` is determined based on\n+        # the internal layers.\n         if not isinstance(dynamic, bool):\n             raise TypeError(\n-                f\"Expected `dynamic` argument to be a boolean, but got: {dynamic}\"\n+                \"Expected `dynamic` argument to be a boolean, \"\n+                f\"but got: {dynamic}\"\n             )\n         self._dynamic = dynamic\n \n@@ -450,11 +454,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # Manage initial weight values if passed.\n         self._initial_weights = kwargs.get(\"weights\", None)\n \n-        # Whether the layer will track any layers that is set as attribute on itself\n-        # as sub-layers, the weights from the sub-layers will be included in the\n-        # parent layer's variables() as well.\n-        # Default to True, which means auto tracking is turned on. Certain subclass\n-        # might want to turn it off, like Sequential model.\n+        # Whether the layer will track any layers that is set as attribute on\n+        # itself as sub-layers, the weights from the sub-layers will be included\n+        # in the parent layer's variables() as well.  Default to True, which\n+        # means auto tracking is turned on. Certain subclass might want to turn\n+        # it off, like Sequential model.\n         self._auto_track_sub_layers = True\n \n         # For backwards compat reasons, most built-in layers do not guarantee\n@@ -498,10 +502,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def call(self, inputs, *args, **kwargs):  # pylint: disable=unused-argument\n         \"\"\"This is where the layer's logic lives.\n \n-        The `call()` method may not create state (except in its first invocation,\n-        wrapping the creation of variables or other resources in `tf.init_scope()`).\n-        It is recommended to create state in `__init__()`, or the `build()` method\n-        that is called automatically before `call()` executes the first time.\n+        The `call()` method may not create state (except in its first\n+        invocation, wrapping the creation of variables or other resources in\n+        `tf.init_scope()`).  It is recommended to create state in `__init__()`,\n+        or the `build()` method that is called automatically before `call()`\n+        executes the first time.\n \n         Args:\n           inputs: Input tensor, or dict/list/tuple of input tensors.\n@@ -509,7 +514,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             - `inputs` must be explicitly passed. A layer cannot have zero\n               arguments, and `inputs` cannot be provided via the default value\n               of a keyword argument.\n-            - NumPy array or Python scalar values in `inputs` get cast as tensors.\n+            - NumPy array or Python scalar values in `inputs` get cast as\n+              tensors.\n             - Keras mask metadata is only collected from `inputs`.\n             - Layers are built (`build(input_shape)` method)\n               using shape info from `inputs` only.\n@@ -517,7 +523,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             - Mixed precision input casting is only applied to `inputs`.\n               If a layer has tensor arguments in `*args` or `**kwargs`, their\n               casting behavior in mixed precision should be handled manually.\n-            - The SavedModel input specification is generated using `inputs` only.\n+            - The SavedModel input specification is generated using `inputs`\n+              only.\n             - Integration with various ecosystem packages like TFMOT, TFLite,\n               TF.js, etc is only supported for `inputs` and not for tensors in\n               positional and keyword arguments.\n@@ -529,10 +536,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             - `training`: Boolean scalar tensor of Python boolean indicating\n               whether the `call` is meant for training or inference.\n             - `mask`: Boolean input mask. If the layer's `call()` method takes a\n-              `mask` argument, its default value will be set to the mask generated\n-              for `inputs` by the previous layer (if `input` did come from a layer\n-              that generated a corresponding mask, i.e. if it came from a Keras\n-              layer with masking support).\n+              `mask` argument, its default value will be set to the mask\n+              generated for `inputs` by the previous layer (if `input` did come\n+              from a layer that generated a corresponding mask, i.e. if it came\n+              from a Keras layer with masking support).\n \n         Returns:\n           A tensor or list/tuple of tensors.\n@@ -569,14 +576,15 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             is set to `ON_READ`.\n           constraint: Constraint instance (callable).\n           use_resource: Whether to use a `ResourceVariable` or not.\n-             See [this guide](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)  # pylint: disable=line-too-long\n+            See [this guide](\n+            https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n             for more information.\n           synchronization: Indicates when a distributed a variable will be\n             aggregated. Accepted values are constants defined in the class\n-            `tf.VariableSynchronization`. By default the synchronization is set to\n-            `AUTO` and the current `DistributionStrategy` chooses\n-            when to synchronize. If `synchronization` is set to `ON_READ`,\n-            `trainable` must not be set to `True`.\n+            `tf.VariableSynchronization`. By default the synchronization is set\n+            to `AUTO` and the current `DistributionStrategy` chooses when to\n+            synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n+            must not be set to `True`.\n           aggregation: Indicates how a distributed variable will be aggregated.\n             Accepted values are constants defined in the class\n             `tf.VariableAggregation`.\n@@ -588,7 +596,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         Raises:\n           ValueError: When giving unsupported dtype and no initializer or when\n-            trainable has been set to True with synchronization set as `ON_READ`.\n+            trainable has been set to True with synchronization set as\n+            `ON_READ`.\n         \"\"\"\n         if shape is None:\n             shape = ()\n@@ -604,20 +613,22 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             ]:\n                 raise TypeError(\"Unknown keyword argument:\", kwarg)\n         collections_arg = kwargs.pop(\"collections\", None)\n-        # 'experimental_autocast' can be set to False by the caller to indicate an\n-        # AutoCastVariable should never be created.\n+        # 'experimental_autocast' can be set to False by the caller to indicate\n+        # an AutoCastVariable should never be created.\n         autocast = kwargs.pop(\"experimental_autocast\", True)\n-        # See the docstring for tf.Variable about the details for caching_device.\n+        # See the docstring for tf.Variable about the details for\n+        # caching_device.\n         caching_device = kwargs.pop(\"caching_device\", None)\n \n         layout = kwargs.pop(\"layout\", None)\n-        # Specially handling of auto layout fetch, based on the variable name and\n-        # attribute name. For built-in keras layers, usually the variable name, eg\n-        # 'kernel', will match with a 'kernel_layout' attribute name on the\n-        # instance. We will try to do this auto fetch if layout is not explicitly\n-        # specified. This is mainly a quick workaround for not applying too many\n-        # interface change to built-in layers, until DTensor is a public API.\n-        # Also see dtensor.utils.allow_initializer_layout for more details.\n+        # Specially handling of auto layout fetch, based on the variable name\n+        # and attribute name. For built-in keras layers, usually the variable\n+        # name, eg 'kernel', will match with a 'kernel_layout' attribute name on\n+        # the instance. We will try to do this auto fetch if layout is not\n+        # explicitly specified. This is mainly a quick workaround for not\n+        # applying too many interface change to built-in layers, until DTensor\n+        # is a public API.  Also see dtensor.utils.allow_initializer_layout for\n+        # more details.\n         # TODO(scottzhu): Remove this once dtensor is public to end user.\n         if not layout and name:\n             layout = getattr(self, name + \"_layout\", None)\n@@ -626,7 +637,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             dtype = self.dtype or backend.floatx()\n         dtype = tf.as_dtype(dtype)\n         if self._dtype_policy.variable_dtype is None:\n-            # The policy is \"_infer\", so we infer the policy from the variable dtype.\n+            # The policy is \"_infer\", so we infer the policy from the variable\n+            # dtype.\n             self._set_dtype_policy(policy.Policy(dtype.base_dtype.name))\n         initializer = initializers.get(initializer)\n         regularizer = regularizers.get(regularizer)\n@@ -636,12 +648,13 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             if trainable:\n                 raise ValueError(\n                     \"Synchronization value can be set to \"\n-                    \"VariableSynchronization.ON_READ only for non-trainable variables. \"\n-                    \"You have specified trainable=True and \"\n+                    \"VariableSynchronization.ON_READ only for non-trainable \"\n+                    \"variables. You have specified trainable=True and \"\n                     \"synchronization=VariableSynchronization.ON_READ.\"\n                 )\n             else:\n-                # Set trainable to be false when variable is to be synced on read.\n+                # Set trainable to be false when variable is to be synced on\n+                # read.\n                 trainable = False\n         elif trainable is None:\n             trainable = True\n@@ -655,11 +668,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             # If dtype is DT_BOOL, provide a default value `FALSE`\n             elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n                 initializer = initializers.get(\"zeros\")\n-            # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?\n+            # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX\n+            # here?\n             elif \"getter\" not in kwargs:\n-                # When `getter` is specified, it's possibly fine for `initializer` to be\n-                # None since it's up to the custom `getter` to raise error in case it\n-                # indeed needs `initializer`.\n+                # When `getter` is specified, it's possibly fine for\n+                # `initializer` to be None since it's up to the custom `getter`\n+                # to raise error in case it indeed needs `initializer`.\n                 raise ValueError(\n                     f\"An initializer for variable {name} of type \"\n                     f\"{dtype.base_dtype} is required for layer \"\n@@ -679,13 +693,13 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 variable = old_getter(*args, **kwargs)\n                 return autocast_variable.create_autocast_variable(variable)\n \n-            # Also the caching_device does not work with the mixed precision API,\n-            # disable it if it is specified.\n+            # Also the caching_device does not work with the mixed precision\n+            # API, disable it if it is specified.\n             # TODO(b/142020079): Re-enable it once the bug is fixed.\n             if caching_device is not None:\n                 tf_logging.warning(\n-                    \"`caching_device` does not work with mixed precision API. Ignoring \"\n-                    \"user specified `caching_device`.\"\n+                    \"`caching_device` does not work with mixed precision API. \"\n+                    \"Ignoring user specified `caching_device`.\"\n                 )\n                 caching_device = None\n         if layout:\n@@ -745,9 +759,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         information, nor the layer class name. These are handled\n         by `Network` (one layer of abstraction above).\n \n-        Note that `get_config()` does not guarantee to return a fresh copy of dict\n-        every time it is called. The callers should make a copy of the returned dict\n-        if they want to modify it.\n+        Note that `get_config()` does not guarantee to return a fresh copy of\n+        dict every time it is called. The callers should make a copy of the\n+        returned dict if they want to modify it.\n \n         Returns:\n             Python dictionary.\n@@ -833,13 +847,13 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             An input shape tuple.\n         \"\"\"\n         if tf.executing_eagerly():\n-            # In this case we build the model first in order to do shape inference.\n-            # This is acceptable because the framework only calls\n-            # `compute_output_shape` on shape values that the layer would later be\n-            # built for. It would however cause issues in case a user attempts to\n-            # use `compute_output_shape` manually with shapes that are incompatible\n-            # with the shape the Layer will be called on (these users will have to\n-            # implement `compute_output_shape` themselves).\n+            # In this case we build the model first in order to do shape\n+            # inference.  This is acceptable because the framework only calls\n+            # `compute_output_shape` on shape values that the layer would later\n+            # be built for. It would however cause issues in case a user\n+            # attempts to use `compute_output_shape` manually with shapes that\n+            # are incompatible with the shape the Layer will be called on (these\n+            # users will have to implement `compute_output_shape` themselves).\n             self._maybe_build(input_shape)\n             graph_name = str(self.name) + \"_scratch_graph\"\n             with tf.__internal__.FuncGraph(graph_name).as_default():\n@@ -859,8 +873,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                     outputs = self(inputs, training=False)\n                 except TypeError as e:\n                     raise NotImplementedError(\n-                        \"We could not automatically infer the static shape of the \"\n-                        \"layer's output. Please implement the \"\n+                        \"We could not automatically infer the static shape of \"\n+                        \"the layer's output. Please implement the \"\n                         \"`compute_output_shape` method on your layer (%s).\"\n                         % self.__class__.__name__\n                     ) from e\n@@ -886,8 +900,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             objects, describing a candidate input for the layer.\n \n         Returns:\n-          Single TensorSpec or nested structure of TensorSpec objects, describing\n-            how the layer would transform the provided input.\n+          Single TensorSpec or nested structure of TensorSpec objects,\n+            describing how the layer would transform the provided input.\n \n         Raises:\n           TypeError: If input_signature contains a non-TensorSpec object.\n@@ -908,8 +922,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         dtype = self._compute_dtype\n         if dtype is None:\n             input_dtypes = [s.dtype for s in tf.nest.flatten(input_signature)]\n-            # Default behavior when self.dtype is None, is to use the first input's\n-            # dtype.\n+            # Default behavior when self.dtype is None, is to use the first\n+            # input's dtype.\n             dtype = input_dtypes[0]\n         return tf.nest.map_structure(\n             lambda s: tf.TensorSpec(dtype=dtype, shape=s), output_shape\n@@ -953,7 +967,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           Output tensor(s).\n \n         Note:\n-          - The following optional keyword arguments are reserved for specific uses:\n+          - The following optional keyword arguments are reserved for specific\n+            uses:\n             * `training`: Boolean scalar tensor of Python boolean indicating\n               whether the `call` is meant for training or inference.\n             * `mask`: Boolean input mask.\n@@ -965,8 +980,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           - If the layer is not built, the method will call `build`.\n \n         Raises:\n-          ValueError: if the layer's `call` method returns None (an invalid value).\n-          RuntimeError: if `super().__init__()` was not called in the constructor.\n+          ValueError: if the layer's `call` method returns None (an invalid\n+            value).\n+          RuntimeError: if `super().__init__()` was not called in the\n+            constructor.\n         \"\"\"\n         if not hasattr(self, \"_thread_local\"):\n             raise RuntimeError(\n@@ -976,8 +993,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # `inputs` (the first arg in the method spec) is special cased in\n         # layer call due to historical reasons.\n         # This special casing currently takes the form of:\n-        # - 'inputs' must be explicitly passed. A layer cannot have zero arguments,\n-        #   and inputs cannot have been provided via the default value of a kwarg.\n+        # - 'inputs' must be explicitly passed. A layer cannot have zero\n+        #   arguments, and inputs cannot have been provided via the default\n+        #   value of a kwarg.\n         # - numpy/scalar values in `inputs` get converted to tensors\n         # - implicit masks / mask metadata are only collected from 'inputs`\n         # - Layers are built using shape info from 'inputs' only\n@@ -987,8 +1005,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         inputs, args, kwargs = self._call_spec.split_out_first_arg(args, kwargs)\n         input_list = tf.nest.flatten(inputs)\n \n-        # Functional Model construction mode is invoked when `Layer`s are called on\n-        # symbolic `KerasTensor`s, i.e.:\n+        # Functional Model construction mode is invoked when `Layer`s are called\n+        # on symbolic `KerasTensor`s, i.e.:\n         # >> inputs = tf.keras.Input(10)\n         # >> outputs = MyLayer()(inputs)  # Functional construction mode.\n         # >> model = tf.keras.Model(inputs, outputs)\n@@ -1012,10 +1030,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             )\n             input_list = tf.nest.flatten(inputs)\n \n-        # Handle `mask` propagation from previous layer to current layer. Masks can\n-        # be propagated explicitly via the `mask` argument, or implicitly via\n-        # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n-        # explicitly take priority.\n+        # Handle `mask` propagation from previous layer to current layer. Masks\n+        # can be propagated explicitly via the `mask` argument, or implicitly\n+        # via setting the `_keras_mask` attribute on the inputs to a Layer.\n+        # Masks passed explicitly take priority.\n         input_masks, mask_is_implicit = self._get_input_masks(\n             inputs, input_list, args, kwargs\n         )\n@@ -1023,9 +1041,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             kwargs[\"mask\"] = input_masks\n \n         # Training mode for `Layer.call` is set via (in order of priority):\n-        # (1) The `training` argument passed to this `Layer.call`, if it is not None\n+        # (1) The `training` argument passed to this `Layer.call`, if it is not\n+        #  None\n         # (2) The training mode of an outer `Layer.call`.\n-        # (3) The default mode set by `tf.keras.backend.set_learning_phase` (if set)\n+        # (3) The default mode set by `tf.keras.backend.set_learning_phase` (if\n+        #  set)\n         # (4) Any non-None default value for `training` specified in the call\n         #  signature\n         # (5) False (treating the layer as if it's in inference)\n@@ -1034,8 +1054,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         )\n \n         # Losses are cleared for all sublayers on the outermost `Layer.call`.\n-        # Losses are not cleared on inner `Layer.call`s, because sublayers can be\n-        # called multiple times.\n+        # Losses are not cleared on inner `Layer.call`s, because sublayers can\n+        # be called multiple times.\n         if not call_context.in_call:\n             self._clear_losses()\n \n@@ -1060,7 +1080,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n             call_fn = traceback_utils.inject_argument_info_in_traceback(\n                 call_fn,\n-                object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})',\n+                object_name=f'layer \"{self.name}\" \" \\\n+                f\"(type {self.__class__.__name__})',\n             )\n             with contextlib.ExitStack() as namescope_stack:\n                 if _is_name_scope_on_model_declaration_enabled:\n@@ -1164,11 +1185,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         When this value is changed during training (e.g. with a\n         `tf.keras.callbacks.Callback`) you need to call the parent\n-        `tf.keras.Model.make_train_function` with `force=True` in order to recompile\n-        the training graph.\n+        `tf.keras.Model.make_train_function` with `force=True` in order to\n+        recompile the training graph.\n \n         Args:\n-          value: Boolean with the desired state for the layer's trainable attribute.\n+          value: Boolean with the desired state for the layer's trainable\n+            attribute.\n         \"\"\"\n         for layer in self._flatten_layers():\n             layer._trainable = value\n@@ -1187,18 +1209,18 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def input_spec(self):\n         \"\"\"`InputSpec` instance(s) describing the input format for this layer.\n \n-        When you create a layer subclass, you can set `self.input_spec` to enable\n-        the layer to run input compatibility checks when it is called.\n-        Consider a `Conv2D` layer: it can only be called on a single input tensor\n-        of rank 4. As such, you can set, in `__init__()`:\n+        When you create a layer subclass, you can set `self.input_spec` to\n+        enable the layer to run input compatibility checks when it is called.\n+        Consider a `Conv2D` layer: it can only be called on a single input\n+        tensor of rank 4. As such, you can set, in `__init__()`:\n \n         ```python\n         self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n         ```\n \n         Now, if you try to call the layer on an input that isn't rank 4\n-        (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n-        error:\n+        (for instance, an input of shape `(2,)`, it will raise a\n+        nicely-formatted error:\n \n         ```\n         ValueError: Input 0 of layer conv2d is incompatible with the layer:\n@@ -1254,8 +1276,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def non_trainable_weights(self):\n         \"\"\"List of all non-trainable weights tracked by this layer.\n \n-        Non-trainable weights are *not* updated during training. They are expected\n-        to be updated manually in `call()`.\n+        Non-trainable weights are *not* updated during training. They are\n+        expected to be updated manually in `call()`.\n \n         Returns:\n           A list of non-trainable variables.\n@@ -1300,9 +1322,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def losses(self):\n         \"\"\"List of losses added using the `add_loss()` API.\n \n-        Variable regularization tensors are created when this property is accessed,\n-        so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n-        propagate gradients back to the corresponding variables.\n+        Variable regularization tensors are created when this property is\n+        accessed, so it is eager safe: accessing `losses` under a\n+        `tf.GradientTape` will propagate gradients back to the corresponding\n+        variables.\n \n         Examples:\n \n@@ -1341,12 +1364,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         \"\"\"\n         collected_losses = []\n         for layer in self._flatten_layers():\n-            # If any eager losses are present, we assume the model to be part of an\n-            # eager training loop (either a custom one or the one used when\n+            # If any eager losses are present, we assume the model to be part of\n+            # an eager training loop (either a custom one or the one used when\n             # `run_eagerly=True`) and so we always return just the eager losses.\n             if layer._eager_losses:\n-                # Filter placeholder losses that may have been added by revived layers.\n-                # (see base_layer_utils for details).\n+                # Filter placeholder losses that may have been added by revived\n+                # layers.  (see base_layer_utils for details).\n                 if (\n                     layer._eager_losses[0]\n                     is not base_layer_utils.REVIVED_LOSS_PLACEHOLDER\n@@ -1363,11 +1386,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def add_loss(self, losses, **kwargs):\n         \"\"\"Add loss tensor(s), potentially dependent on layer inputs.\n \n-        Some losses (for instance, activity regularization losses) may be dependent\n-        on the inputs passed when calling a layer. Hence, when reusing the same\n-        layer on different inputs `a` and `b`, some entries in `layer.losses` may\n-        be dependent on `a` and some on `b`. This method automatically keeps track\n-        of dependencies.\n+        Some losses (for instance, activity regularization losses) may be\n+        dependent on the inputs passed when calling a layer. Hence, when reusing\n+        the same layer on different inputs `a` and `b`, some entries in\n+        `layer.losses` may be dependent on `a` and some on `b`. This method\n+        automatically keeps track of dependencies.\n \n         This method can be used inside a subclassed layer or model's `call`\n         function, in which case `losses` should be a Tensor or list of Tensors.\n@@ -1384,7 +1407,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         This method can also be called directly on a Functional Model during\n         construction. In this case, any loss Tensors passed to this Model must\n         be symbolic and be able to be traced back to the model's `Input`s. These\n-        losses become part of the model's topology and are tracked in `get_config`.\n+        losses become part of the model's topology and are tracked in\n+        `get_config`.\n \n         Example:\n \n@@ -1397,10 +1421,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         model.add_loss(tf.abs(tf.reduce_mean(x)))\n         ```\n \n-        If this is not the case for your loss (if, for example, your loss references\n-        a `Variable` of one of the model's layers), you can wrap your loss in a\n-        zero-argument lambda. These losses are not tracked as part of the model's\n-        topology since they can't be serialized.\n+        If this is not the case for your loss (if, for example, your loss\n+        references a `Variable` of one of the model's layers), you can wrap your\n+        loss in a zero-argument lambda. These losses are not tracked as part of\n+        the model's topology since they can't be serialized.\n \n         Example:\n \n@@ -1415,8 +1439,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         ```\n \n         Args:\n-          losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n-            may also be zero-argument callables which create a loss tensor.\n+          losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n+            losses may also be zero-argument callables which create a loss\n+            tensor.\n           **kwargs: Used for backwards compatibility only.\n         \"\"\"\n         kwargs.pop(\"inputs\", None)\n@@ -1431,7 +1456,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 with autocast_variable.enable_auto_cast_variables(None):\n                     loss = loss()\n             if loss is None:\n-                return None  # Will be filtered out when computing the .losses property\n+                # Will be filtered out when computing the .losses property\n+                return None\n             if not tf.is_tensor(loss):\n                 loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n             loss._unconditional_loss = True  # pylint: disable=protected-access\n@@ -1537,8 +1563,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         ```\n \n         Note: Calling `add_metric()` with the result of a metric object on a\n-        Functional Model, as shown in the example below, is not supported. This is\n-        because we cannot trace the metric result tensor back to the model's inputs.\n+        Functional Model, as shown in the example below, is not supported. This\n+        is because we cannot trace the metric result tensor back to the model's\n+        inputs.\n \n         ```python\n         inputs = tf.keras.Input(shape=(10,))\n@@ -1553,9 +1580,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           name: String metric name.\n           **kwargs: Additional keyword arguments for backward compatibility.\n             Accepted values:\n-            `aggregation` - When the `value` tensor provided is not the result of\n-            calling a `keras.Metric` instance, it will be aggregated by default\n-            using a `keras.Metric.Mean`.\n+            `aggregation` - When the `value` tensor provided is not the result\n+            of calling a `keras.Metric` instance, it will be aggregated by\n+            default using a `keras.Metric.Mean`.\n         \"\"\"\n         kwargs_keys = list(kwargs.keys())\n         if len(kwargs_keys) > 1 or (\n@@ -1571,12 +1598,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         in_call_context = base_layer_utils.call_context().in_call\n \n         if name is None and not from_metric_obj:\n-            # Eg. `self.add_metric(math_ops.reduce_sum(x))`\n-            # In eager mode, we use metric name to lookup a metric. Without a name,\n-            # a new Mean metric wrapper will be created on every model/layer call.\n-            # So, we raise an error when no name is provided.\n-            # We will do the same for symbolic mode for consistency although a name\n-            # will be generated if no name is provided.\n+            # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we\n+            # use metric name to lookup a metric. Without a name, a new Mean\n+            # metric wrapper will be created on every model/layer call. So, we\n+            # raise an error when no name is provided. We will do the same for\n+            # symbolic mode for consistency although a name will be generated if\n+            # no name is provided.\n \n             # We will not raise this error in the foll use case for the sake of\n             # consistency as name in provided in the metric constructor.\n@@ -1600,11 +1627,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         if in_call_context or not getattr(self, \"_is_graph_network\", False):\n             # TF Function path should take the eager path.\n \n-            # If the given metric is available in `metrics` list we just update state\n-            # on it, otherwise we create a new metric instance and\n+            # If the given metric is available in `metrics` list we just update\n+            # state on it, otherwise we create a new metric instance and\n             # add it to the `metrics` list.\n             metric_obj = getattr(value, \"_metric_obj\", None)\n-            # Tensors that come from a Metric object already updated the Metric state.\n+            # Tensors that come from a Metric object already updated the Metric\n+            # state.\n             should_update_state = not metric_obj\n             name = metric_obj.name if metric_obj else name\n \n@@ -1615,7 +1643,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 elif metric_obj:\n                     self._metrics.append(metric_obj)\n                 else:\n-                    # Build the metric object with the value's dtype if it defines one\n+                    # Build the metric object with the value's dtype if it\n+                    # defines one\n                     metric_obj = metrics_mod.Mean(\n                         name=name, dtype=getattr(value, \"dtype\", None)\n                     )\n@@ -1640,16 +1669,16 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def add_update(self, updates):\n         \"\"\"Add update op(s), potentially dependent on layer inputs.\n \n-        Weight updates (for instance, the updates of the moving mean and variance\n-        in a BatchNormalization layer) may be dependent on the inputs passed\n-        when calling a layer. Hence, when reusing the same layer on\n+        Weight updates (for instance, the updates of the moving mean and\n+        variance in a BatchNormalization layer) may be dependent on the inputs\n+        passed when calling a layer. Hence, when reusing the same layer on\n         different inputs `a` and `b`, some entries in `layer.updates` may be\n         dependent on `a` and some on `b`. This method automatically keeps track\n         of dependencies.\n \n-        This call is ignored when eager execution is enabled (in that case, variable\n-        updates are run on the fly and thus do not need to be tracked for later\n-        execution).\n+        This call is ignored when eager execution is enabled (in that case,\n+        variable updates are run on the fly and thus do not need to be tracked\n+        for later execution).\n \n         Args:\n           updates: Update op, or list/tuple of update ops, or zero-arg callable\n@@ -1677,9 +1706,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         weights must be instantiated before calling this function, by calling\n         the layer.\n \n-        For example, a `Dense` layer returns a list of two values: the kernel matrix\n-        and the bias vector. These can be used to set the weights of another\n-        `Dense` layer:\n+        For example, a `Dense` layer returns a list of two values: the kernel\n+        matrix and the bias vector. These can be used to set the weights of\n+        another `Dense` layer:\n \n         >>> layer_a = tf.keras.layers.Dense(1,\n         ...   kernel_initializer=tf.constant_initializer(1.))\n@@ -1765,13 +1794,13 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         \"\"\"Returns the current weights of the layer, as NumPy arrays.\n \n         The weights of a layer represent the state of the layer. This function\n-        returns both trainable and non-trainable weight values associated with this\n-        layer as a list of NumPy arrays, which can in turn be used to load state\n-        into similarly parameterized layers.\n+        returns both trainable and non-trainable weight values associated with\n+        this layer as a list of NumPy arrays, which can in turn be used to load\n+        state into similarly parameterized layers.\n \n-        For example, a `Dense` layer returns a list of two values: the kernel matrix\n-        and the bias vector. These can be used to set the weights of another\n-        `Dense` layer:\n+        For example, a `Dense` layer returns a list of two values: the kernel\n+        matrix and the bias vector. These can be used to set the weights of\n+        another `Dense` layer:\n \n         >>> layer_a = tf.keras.layers.Dense(1,\n         ...   kernel_initializer=tf.constant_initializer(1.))\n@@ -1809,9 +1838,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def finalize_state(self):\n         \"\"\"Finalizes the layers state after updating layer weights.\n \n-        This function can be subclassed in a layer and will be called after updating\n-        a layer weights. It can be overridden to finalize any additional layer state\n-        after a weight update.\n+        This function can be subclassed in a layer and will be called after\n+        updating a layer weights. It can be overridden to finalize any\n+        additional layer state after a weight update.\n \n         This function will be called after weights of a layer have been restored\n         from a loaded model.\n@@ -2143,14 +2172,14 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n         the weights.\n \n-        Layers automatically cast their inputs to the compute dtype, which causes\n-        computations and the output to be in the compute dtype as well. This is done\n-        by the base Layer class in `Layer.__call__`, so you do not have to insert\n-        these casts if implementing your own layer.\n+        Layers automatically cast their inputs to the compute dtype, which\n+        causes computations and the output to be in the compute dtype as well.\n+        This is done by the base Layer class in `Layer.__call__`, so you do not\n+        have to insert these casts if implementing your own layer.\n \n-        Layers often perform certain internal computations in higher precision when\n-        `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n-        will still typically be float16 or bfloat16 in such cases.\n+        Layers often perform certain internal computations in higher precision\n+        when `compute_dtype` is float16 or bfloat16 for numeric stability. The\n+        output will still typically be float16 or bfloat16 in such cases.\n \n         Returns:\n           The layer's compute dtype.\n@@ -2174,9 +2203,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         \"\"\"Return Functional API nodes downstream of this layer.\"\"\"\n         return self._outbound_nodes\n \n-    ##############################################################################\n+    ############################################################################\n     # Methods & attributes below are public aliases of other methods.          #\n-    ##############################################################################\n+    ############################################################################\n \n     @property\n     @doc_controls.do_not_generate_docs\n@@ -2185,8 +2214,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         Alias of `self.weights`.\n \n-        Note: This will not track the weights of nested `tf.Modules` that are not\n-        themselves Keras layers.\n+        Note: This will not track the weights of nested `tf.Modules` that are\n+        not themselves Keras layers.\n \n         Returns:\n           A list of variables.\n@@ -2214,17 +2243,17 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         )\n         return self.add_weight(*args, **kwargs)\n \n-    ##############################################################################\n-    # Methods & attributes below are all private and only used by the framework. #\n-    ##############################################################################\n+    ############################################################################\n+    # Methods & attributes below are all private and only used by the framework.\n+    ############################################################################\n \n     # See tf.Module for the usage of this property.\n     # The key for _obj_reference_counts_dict is a Trackable, which could be a\n     # variable or layer etc. tf.Module._flatten will fail to flatten the key\n     # since it is trying to convert Trackable to a string. This attribute can be\n     # ignored even after the fix of nest lib, since the trackable object should\n-    # already been available as individual attributes. _obj_reference_counts_dict\n-    # just contains a copy of them.\n+    # already been available as individual attributes.\n+    # _obj_reference_counts_dict just contains a copy of them.\n     _TF_MODULE_IGNORED_PROPERTIES = frozenset(\n         itertools.chain(\n             (\"_obj_reference_counts_dict\",),\n@@ -2325,7 +2354,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             )\n \n     def _infer_output_signature(self, inputs, args, kwargs, input_masks):\n-        \"\"\"Call the layer on input KerasTensors and returns output KerasTensors.\"\"\"\n+        \"\"\"Call the layer on input KerasTensors, returns output KerasTensors.\"\"\"\n \n         keras_tensor_inputs = inputs\n         call_fn = self.call\n@@ -2347,8 +2376,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})',\n         )\n \n-        # We enter a scratch graph and build placeholder inputs inside of it that\n-        # match the input args.\n+        # We enter a scratch graph and build placeholder inputs inside of it\n+        # that match the input args.\n         # We then call the layer inside of the scratch graph to identify the\n         # output signatures, then we build KerasTensors corresponding to those\n         # outputs.\n@@ -2377,7 +2406,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 ):\n                     # Build layer if applicable (if the `build` method has been\n                     # overridden).\n-                    # TODO(kaftan): do we maybe_build here, or have we already done it?\n+                    # TODO(kaftan): do we maybe_build here, or have we already\n+                    # done it?\n                     self._maybe_build(inputs)\n                     inputs = self._maybe_cast_inputs(inputs)\n                     outputs = call_fn(inputs, *args, **kwargs)\n@@ -2418,10 +2448,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             inputs = tf.nest.map_structure(_convert_non_tensor, inputs)\n             input_list = tf.nest.flatten(inputs)\n \n-        # Handle `mask` propagation from previous layer to current layer. Masks can\n-        # be propagated explicitly via the `mask` argument, or implicitly via\n-        # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n-        # explicitly take priority.\n+        # Handle `mask` propagation from previous layer to current layer. Masks\n+        # can be propagated explicitly via the `mask` argument, or implicitly\n+        # via setting the `_keras_mask` attribute on the inputs to a Layer.\n+        # Masks passed explicitly take priority.\n         mask_arg_passed_by_framework = False\n         input_masks, mask_is_implicit = self._get_input_masks(\n             inputs, input_list, args, kwargs\n@@ -2449,19 +2479,20 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             # Priority 3: `learning_phase()` has been set.\n             elif backend.global_learning_phase_is_set():\n                 training_value = backend.learning_phase()\n-                # Force the training_value to be bool type which matches to the contract\n-                # for layer/model call args.\n+                # Force the training_value to be bool type which matches to the\n+                # contract for layer/model call args.\n                 if tf.is_tensor(training_value):\n                     training_value = tf.cast(training_value, tf.bool)\n                 else:\n                     training_value = bool(training_value)\n-            # Priority 4: trace layer with the default training argument specified\n-            # in the `call` signature (or in inference mode if the `call` signature\n-            # specifies no non-None default).\n+            # Priority 4: trace layer with the default training argument\n+            # specified in the `call` signature (or in inference mode if the\n+            # `call` signature specifies no non-None default).\n             else:\n                 training_value = self._call_spec.default_training_arg\n-            # In cases (2), (3), (4) the training argument is passed automatically\n-            # by the framework, and will not be hard-coded into the model.\n+            # In cases (2), (3), (4) the training argument is passed\n+            # automatically by the framework, and will not be hard-coded into\n+            # the model.\n             if self._expects_training_arg:\n                 args, kwargs = self._call_spec.set_arg_value(\n                     \"training\", training_value, args, kwargs\n@@ -2471,7 +2502,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         with call_context.enter(\n             layer=self, inputs=inputs, build_graph=True, training=training_value\n         ):\n-            # Check input assumptions set after layer building, e.g. input shape.\n+            # Check input assumptions set after layer building, e.g. input\n+            # shape.\n             outputs = self._keras_tensor_symbolic_call(\n                 inputs, input_masks, args, kwargs\n             )\n@@ -2520,8 +2552,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                     else:\n                         training_mode = bool(training_mode)\n                 # (4) We default to using `call`'s default value for `training`,\n-                # or treating the layer as if it is in inference if no non-None default\n-                # is specified in the `call` signature.\n+                # or treating the layer as if it is in inference if no non-None\n+                # default is specified in the `call` signature.\n                 else:\n                     training_mode = self._call_spec.default_training_arg\n \n@@ -2532,7 +2564,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         else:\n             if \"training\" in kwargs:\n                 # `training` was passed to this `Layer` but is not needed for\n-                # `Layer.call`. It will set the default mode for inner `Layer.call`s.\n+                # `Layer.call`. It will set the default mode for inner\n+                # `Layer.call`s.\n                 training_mode = kwargs.pop(\"training\")\n             else:\n                 # Grab the current `training` mode from any outer `Layer.call`.\n@@ -2595,10 +2628,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             self._dtype_policy.name == \"mixed_float16\"\n             and not loss_scale_optimizer.strategy_supports_loss_scaling()\n         ):\n-            # Although only loss scaling doesn't support certain strategies, to avoid\n-            # confusion, we disallow the 'mixed_float16' policy with unsupported\n-            # strategies. This is because 'mixed_float16' requires loss scaling for\n-            # numeric stability.\n+            # Although only loss scaling doesn't support certain strategies, to\n+            # avoid confusion, we disallow the 'mixed_float16' policy with\n+            # unsupported strategies. This is because 'mixed_float16' requires\n+            # loss scaling for numeric stability.\n             strategy = tf.distribute.get_strategy()\n             raise ValueError(\n                 \"Mixed precision is not supported with the \"\n@@ -2609,7 +2642,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             )\n \n         # Performance optimization: cache the compute dtype as a Dtype object or\n-        # None, so that str to Dtype conversion doesn't happen in Layer.__call__.\n+        # None, so that str to Dtype conversion doesn't happen in\n+        # Layer.__call__.\n         # TODO(b/157486353): Investigate returning DTypes in Policy.\n         if self._dtype_policy.compute_dtype:\n             self._compute_dtype_object = tf.as_dtype(\n@@ -2675,9 +2709,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # TODO(reedwm): Deprecate, then remove the _dtype property.\n     @property\n     def _dtype(self):\n-        # This is equivalent to returning self.dtype . We do not return self.dtype\n-        # as it would cause infinite recursion in a few subclasses, which override\n-        # \"dtype\" to return self._dtype.\n+        # This is equivalent to returning self.dtype . We do not return\n+        # self.dtype as it would cause infinite recursion in a few subclasses,\n+        # which override \"dtype\" to return self._dtype.\n         return self._dtype_policy.variable_dtype\n \n     @_dtype.setter\n@@ -2719,8 +2753,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             return\n         if len(match) > 1:\n             raise ValueError(\n-                \"Please provide different names for the metrics you have added. \"\n-                'We found {} metrics with the name: \"{}\"'.format(\n+                \"Please provide different names for the metrics you have \"\n+                'added. We found {} metrics with the name: \"{}\"'.format(\n                     len(match), name\n                 )\n             )\n@@ -2805,8 +2839,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     def _get_input_masks(self, inputs, input_list, args, kwargs):\n         if not self._supports_masking and not self._expects_mask_arg:\n-            # Input masks only need to be retrieved if they are needed for `call`\n-            # or `compute_mask`.\n+            # Input masks only need to be retrieved if they are needed for\n+            # `call` or `compute_mask`.\n             input_masks = None\n             implicit_mask = False\n         elif self._call_spec.arg_was_passed(\"mask\", args, kwargs):\n@@ -2818,7 +2852,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 input_masks = None\n                 implicit_mask = False\n             else:\n-                # Only do expensive `nest` op when masking is actually being used.\n+                # Only do expensive `nest` op when masking is actually being\n+                # used.\n                 input_masks = tf.nest.pack_sequence_as(inputs, input_masks)\n                 implicit_mask = True\n         return input_masks, implicit_mask\n@@ -2837,11 +2872,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             outputs_copy.append(x)\n         outputs = tf.nest.pack_sequence_as(outputs, outputs_copy)\n \n-        # Create node, Node wires itself to inbound and outbound layers.\n-        # The Node constructor actually updates this layer's self._inbound_nodes,\n+        # Create node, Node wires itself to inbound and outbound layers.  The\n+        # Node constructor actually updates this layer's self._inbound_nodes,\n         # sets _keras_history on the outputs, and adds itself to the\n-        # `_outbound_nodes` of the layers that produced the inputs to this\n-        # layer call.\n+        # `_outbound_nodes` of the layers that produced the inputs to this layer\n+        # call.\n         node_module.Node(\n             self, call_args=args, call_kwargs=kwargs, outputs=outputs\n         )\n@@ -2866,8 +2901,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             The layer's attribute `attr` at the node of index `node_index`.\n \n         Raises:\n-            RuntimeError: If the layer has no inbound nodes, or if called in Eager\n-            mode.\n+            RuntimeError: If the layer has no inbound nodes, or if called in\n+                Eager mode.\n             ValueError: If the index provided does not match any node.\n         \"\"\"\n         if not self._inbound_nodes:\n@@ -2913,16 +2948,17 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                     )\n                 except ValueError:\n                     pass\n-            # Only call `build` if the user has manually overridden the build method.\n+            # Only call `build` if the user has manually overridden the build\n+            # method.\n             if not hasattr(self.build, \"_is_default\"):\n-                # Any setup work performed only once should happen in an `init_scope`\n-                # to avoid creating symbolic Tensors that will later pollute any eager\n-                # operations.\n+                # Any setup work performed only once should happen in an\n+                # `init_scope` to avoid creating symbolic Tensors that will\n+                # later pollute any eager operations.\n                 with tf_utils.maybe_init_scope(self):\n                     self.build(input_shapes)  # pylint:disable=not-callable\n-            # We must set also ensure that the layer is marked as built, and the build\n-            # shape is stored since user defined build functions may not be calling\n-            # `super.build()`\n+            # We must set also ensure that the layer is marked as built, and the\n+            # build shape is stored since user defined build functions may not\n+            # be calling `super.build()`\n             Layer.build(self, input_shapes)\n \n         # Optionally load weight values specified at layer instantiation.\n@@ -2952,7 +2988,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     @property\n     def _obj_reference_counts(self):\n-        \"\"\"A dictionary counting the number of attributes referencing an object.\"\"\"\n+        \"\"\"A dict counting the number of attributes referencing an object.\"\"\"\n         self._maybe_create_attribute(\n             \"_obj_reference_counts_dict\",\n             object_identity.ObjectIdentityDictionary(),\n@@ -2964,10 +3000,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         \"\"\"Create the attribute with the default value if it hasn't been created.\n \n         This is useful for fields that is used for tracking purpose,\n-        _trainable_weights, or _layers. Note that user could create a layer subclass\n-        and assign an internal field before invoking the Layer.__init__(), the\n-        __setattr__() need to create the tracking fields and __init__() need to not\n-        override them.\n+        _trainable_weights, or _layers. Note that user could create a layer\n+        subclass and assign an internal field before invoking the\n+        Layer.__init__(), the __setattr__() need to create the tracking fields\n+        and __init__() need to not override them.\n \n         Args:\n           name: String, the name of the attribute.\n@@ -2977,18 +3013,19 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             self.__setattr__(name, default_value)\n \n     def __delattr__(self, name):\n-        # For any super.__delattr__() call, we will directly use the implementation\n-        # in Trackable and skip the behavior in AutoTrackable. The Layer was\n-        # originally use Trackable as base class, the change of using Module as base\n-        # class forced us to have AutoTrackable in the class hierarchy.\n+        # For any super.__delattr__() call, we will directly use the\n+        # implementation in Trackable and skip the behavior in AutoTrackable.\n+        # The Layer was originally use Trackable as base class, the change of\n+        # using Module as base class forced us to have AutoTrackable in the\n+        # class hierarchy.\n         #\n         # TODO(b/180760306) Keeping the status quo of skipping _delattr__ and\n         # __setattr__ in AutoTrackable may be unsustainable.\n         existing_value = getattr(self, name, None)\n \n-        # If this value is replacing an existing object assigned to an attribute, we\n-        # should clean it out to avoid leaking memory. First we check if there are\n-        # other attributes referencing it.\n+        # If this value is replacing an existing object assigned to an\n+        # attribute, we should clean it out to avoid leaking memory. First we\n+        # check if there are other attributes referencing it.\n         reference_counts = self._obj_reference_counts\n         if existing_value not in reference_counts:\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n@@ -2998,8 +3035,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         reference_count = reference_counts[existing_value]\n         if reference_count > 1:\n-            # There are other remaining references. We can't remove this object from\n-            # _layers etc.\n+            # There are other remaining references. We can't remove this object\n+            # from _layers etc.\n             reference_counts[existing_value] = reference_count - 1\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n                 name\n@@ -3059,9 +3096,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             except AttributeError:\n                 raise AttributeError(\n                     (\n-                        'Can\\'t set the attribute \"{}\", likely because it conflicts with '\n-                        \"an existing read-only @property of the object. Please choose a \"\n-                        \"different name.\"\n+                        'Can\\'t set the attribute \"{}\", likely because it '\n+                        \"conflicts with an existing read-only @property of the \"\n+                        \"object. Please choose a different name.\"\n                     ).format(name)\n                 )\n             return\n@@ -3074,8 +3111,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         reference_counts = self._obj_reference_counts\n         reference_counts[value] = reference_counts.get(value, 0) + 1\n \n-        # Clean out the old attribute, which clears _layers and _trainable_weights\n-        # if necessary.\n+        # Clean out the old attribute, which clears _layers and\n+        # _trainable_weights if necessary.\n         try:\n             self.__delattr__(name)\n         except AttributeError:\n@@ -3111,8 +3148,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             if not isinstance(val, tf.Variable):\n                 continue\n \n-            # Users may add extra weights/variables\n-            # simply by assigning them to attributes (invalid for graph networks)\n+            # Users may add extra weights/variables simply by assigning them to\n+            # attributes (invalid for graph networks)\n             self._maybe_create_attribute(\"_trainable_weights\", [])\n             self._maybe_create_attribute(\"_non_trainable_weights\", [])\n             if val.trainable:\n@@ -3126,8 +3163,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n             backend.track_variable(val)\n \n-        # TODO(b/180760306) Skip the auto trackable from tf.Module to keep status\n-        # quo. See the comment at __delattr__.\n+        # TODO(b/180760306) Skip the auto trackable from tf.Module to keep\n+        # status quo. See the comment at __delattr__.\n         super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n             name, value\n         )  # pylint: disable=bad-super-call\n@@ -3197,7 +3234,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                     trackable_obj,\n                     tf.__internal__.tracking.TrackableDataStructure,\n                 ):\n-                    # Data structures are introspected even with `recursive=False`.\n+                    # Data structures are introspected even with\n+                    # `recursive=False`.\n                     tracked_values = trackable_obj._values\n                     if tracked_values:\n                         deque.extendleft(reversed(tracked_values))\n@@ -3227,11 +3265,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     @property\n     def _eager_losses(self):\n         # A list of loss values containing activity regularizers and losses\n-        # manually added through `add_loss` during eager execution. It is cleared\n-        # after every batch.\n-        # Because we plan on eventually allowing a same model instance to be trained\n-        # in eager mode or graph mode alternatively, we need to keep track of\n-        # eager losses and symbolic losses via separate attributes.\n+        # manually added through `add_loss` during eager execution. It is\n+        # cleared after every batch. Because we plan on eventually allowing a\n+        # same model instance to be trained in eager mode or graph mode\n+        # alternatively, we need to keep track of eager losses and symbolic\n+        # losses via separate attributes.\n         if not hasattr(self._thread_local, \"_eager_losses\"):\n             self._thread_local._eager_losses = []\n         return self._thread_local._eager_losses\n@@ -3312,8 +3350,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         if save_type == \"savedmodel\":\n             cache = kwargs[\"cache\"]\n             # TODO(b/213628533): This must be called before super() to ensure\n-            # that any input shape changes are applied before getting the config of\n-            # the model.\n+            # that any input shape changes are applied before getting the config\n+            # of the model.\n             children = self._trackable_saved_model_saver.trackable_children(\n                 cache\n             )\n@@ -3326,8 +3364,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     def _use_input_spec_as_call_signature(self):\n         # Whether input spec can be used as the call signature when tracing the\n         # Layer for SavedModel. By default, this is set to `True` for layers\n-        # exported from the Keras library, because the layers more rigidly define\n-        # the `input_specs` property (many custom layers only set the `ndims`)\n+        # exported from the Keras library, because the layers more rigidly\n+        # define the `input_specs` property (many custom layers only set the\n+        # `ndims`)\n         return (\n             get_canonical_name_for_symbol(type(self), api_name=\"keras\")\n             is not None\n@@ -3385,8 +3424,8 @@ class TensorFlowOpLayer(Layer):\n     def __init__(\n         self, node_def, name, constants=None, trainable=True, dtype=None\n     ):\n-        # Pass autocast=False, as if inputs are cast, input types might not match\n-        # Operation type.\n+        # Pass autocast=False, as if inputs are cast, input types might not\n+        # match Operation type.\n         super(TensorFlowOpLayer, self).__init__(\n             name=_TF_OP_LAYER_NAME_PREFIX + name,\n             trainable=trainable,\n@@ -3422,8 +3461,8 @@ class TensorFlowOpLayer(Layer):\n     def _make_node_def(self, graph):\n         node_def = tf.compat.v1.NodeDef()\n         node_def.CopyFrom(self.node_def)\n-        # Used in TPUReplicateContext to indicate whether this node has been cloned\n-        # and to not add TPU attributes.\n+        # Used in TPUReplicateContext to indicate whether this node has been\n+        # cloned and to not add TPU attributes.\n         node_def.attr[\"_cloned\"].b = True\n         node_def.name = graph.unique_name(node_def.name)\n         return node_def\n@@ -3439,8 +3478,8 @@ class TensorFlowOpLayer(Layer):\n                 if value is not None:\n                     constant = tf.constant(value, name=node_def.input[index])\n                 inputs.insert(index, constant)\n-            # TODO(b/183990973): We should drop or consolidate these private api calls\n-            # for adding an op to the graph and recording its gradient.\n+            # TODO(b/183990973): We should drop or consolidate these private api\n+            # calls for adding an op to the graph and recording its gradient.\n             c_op = tf.__internal__.create_c_op(\n                 graph, node_def, inputs, control_inputs=[]\n             )\n@@ -3468,14 +3507,15 @@ class TensorFlowOpLayer(Layer):\n \n     @tf.function\n     def _defun_call(self, inputs):\n-        \"\"\"Wraps the op creation method in an Eager function for `run_eagerly`.\"\"\"\n+        \"\"\"Wraps op creation method in an Eager function for `run_eagerly`.\"\"\"\n         return self._make_op(inputs)\n \n     def get_config(self):\n         config = super(TensorFlowOpLayer, self).get_config()\n         config.update(\n             {\n-                # `__init__` prefixes the name. Revert to the constructor argument.\n+                # `__init__` prefixes the name. Revert to the constructor\n+                # argument.\n                 \"name\": config[\"name\"][len(_TF_OP_LAYER_NAME_PREFIX) :],\n                 \"node_def\": json_format.MessageToDict(self.node_def),\n                 \"constants\": {\n@@ -3490,7 +3530,8 @@ class AddLoss(Layer):\n     \"\"\"Adds its inputs as a loss.\n \n     Attributes:\n-      unconditional: Whether or not the loss should be conditioned on the inputs.\n+      unconditional: Whether or not the loss should be conditioned on the\n+        inputs.\n     \"\"\"\n \n     def __init__(self, unconditional, **kwargs):\n@@ -3596,17 +3637,19 @@ class BaseRandomLayer(Layer):\n         @no_automatic_dependency_tracking. This is to skip the auto\n         tracking of self._random_generator instance, which is an AutoTrackable.\n         The backend.RandomGenerator could contain a tf.random.Generator instance\n-        which will have tf.Variable as the internal state. We want to avoid saving\n-        that state into model.weights and checkpoints for backward compatibility\n-        reason. In the meantime, we still need to make them visible to SavedModel\n-        when it is tracing the tf.function for the `call()`.\n+        which will have tf.Variable as the internal state. We want to avoid\n+        saving that state into model.weights and checkpoints for backward\n+        compatibility reason. In the meantime, we still need to make them\n+        visible to SavedModel when it is tracing the tf.function for the\n+        `call()`.\n         See _list_extra_dependencies_for_serialization below for more details.\n \n         Args:\n           seed: optional integer, used to create RandomGenerator.\n           force_generator: boolean, default to False, whether to force the\n             RandomGenerator to use the code branch of tf.random.Generator.\n-          **kwargs: other keyword arguments that will be passed to the parent class\n+          **kwargs: other keyword arguments that will be passed to the parent\n+            *class\n         \"\"\"\n         super().__init__(**kwargs)\n         self._random_generator = backend.RandomGenerator(\n@@ -3619,8 +3662,8 @@ class BaseRandomLayer(Layer):\n         if save_type == \"savedmodel\":\n             cache = kwargs[\"cache\"]\n             # TODO(b/213628533): This must be called before super() to ensure\n-            # that any input shape changes are applied before getting the config of\n-            # the model.\n+            # that any input shape changes are applied before getting the config\n+            # of the model.\n             children = self._trackable_saved_model_saver.trackable_children(\n                 cache\n             )\n\n@@ -1056,13 +1056,15 @@ class SymbolicSupportTest(test_combinations.TestCase):\n         x1, x2 = input_layer.Input((3,)), input_layer.Input((3,))\n         tf.concat([x1, x2], axis=1)\n \n-        # Mixing Keras symbolic tensors and graph tensors from the same graph works.\n+        # Mixing Keras symbolic tensors and graph tensors from the same graph\n+        # works.\n         with backend.get_graph().as_default():\n             x1 = input_layer.Input((3,))\n         x2 = input_layer.Input((3,))\n         tf.matmul(x1, x2)\n \n-        # Creating same op type (matmul) multiple times in the Keras graph works.\n+        # Creating same op type (matmul) multiple times in the Keras graph\n+        # works.\n         x1 = input_layer.Input((3,))\n         x2 = input_layer.Input((3,))\n         tf.matmul(x1, x2)\n@@ -1111,11 +1113,12 @@ class SymbolicSupportTest(test_combinations.TestCase):\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n     def test_reraising_exception(self):\n-        # When layer is not dynamic, we have some pattern matching during exception\n-        # handling to detect when the user is trying to use python control flow.\n-        # When an exception is thrown but the pattern doesn't match, we want to\n-        # preserve the originating stack trace. An early implementation of this\n-        # logic lost the stack trace. We test the correct behavior here.\n+        # When layer is not dynamic, we have some pattern matching during\n+        # exception handling to detect when the user is trying to use python\n+        # control flow.  When an exception is thrown but the pattern doesn't\n+        # match, we want to preserve the originating stack trace. An early\n+        # implementation of this logic lost the stack trace. We test the correct\n+        # behavior here.\n \n         class TypeErrorLayer(base_layer.Layer):\n             def call(self, inputs):\n@@ -1445,10 +1448,12 @@ class NameScopingTest(test_combinations.TestCase):\n                 \"call_scope/model/outer/Dense2/BiasAdd/ReadVariableOp/resource\",\n                 \"call_scope/model/outer/Dense2/BiasAdd/ReadVariableOp\",\n                 \"call_scope/model/outer/Dense2/BiasAdd\",\n-                \"call_scope/model/outer/inner/Dense3/MatMul/ReadVariableOp/resource\",\n+                \"call_scope/model/outer/inner/Dense3/MatMul/ReadVariableOp/\"\n+                \"resource\",\n                 \"call_scope/model/outer/inner/Dense3/MatMul/ReadVariableOp\",\n                 \"call_scope/model/outer/inner/Dense3/MatMul\",\n-                \"call_scope/model/outer/inner/Dense3/BiasAdd/ReadVariableOp/resource\",\n+                \"call_scope/model/outer/inner/Dense3/BiasAdd/ReadVariableOp/\"\n+                \"resource\",\n                 \"call_scope/model/outer/inner/Dense3/BiasAdd/ReadVariableOp\",\n                 \"call_scope/model/outer/inner/Dense3/BiasAdd\",\n                 \"call_scope/model/outer/Dense4/MatMul/ReadVariableOp/resource\",\n@@ -1508,24 +1513,38 @@ class NameScopingTest(test_combinations.TestCase):\n             [\n                 \"call_scope/Const\",\n                 \"call_scope/model/Cast\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp/resource\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/\"\n+                \"ReadVariableOp/resource\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/\"\n+                \"ReadVariableOp\",\n                 \"call_scope/model/outer/ThreeDenses/NestedDense1/MatMul\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp/resource\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/\"\n+                \"ReadVariableOp/resource\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/\"\n+                \"ReadVariableOp\",\n                 \"call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd\",\n-                \"call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp/resource\",\n-                \"call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp\",\n-                \"call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul\",\n-                \"call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp/resource\",\n-                \"call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp\",\n-                \"call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp/resource\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/MatMul\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp/resource\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd\",\n+                \"call_scope/model/outer/ThreeDenses/inner1/inner2/\"\n+                \"NestedDense2/MatMul/ReadVariableOp/resource\",\n+                \"call_scope/model/outer/ThreeDenses/inner1/inner2/\"\n+                \"NestedDense2/MatMul/ReadVariableOp\",\n+                \"call_scope/model/outer/ThreeDenses/inner1/inner2/\"\n+                \"NestedDense2/MatMul\",\n+                \"call_scope/model/outer/ThreeDenses/inner1/inner2/\"\n+                \"NestedDense2/BiasAdd/ReadVariableOp/resource\",\n+                \"call_scope/model/outer/ThreeDenses/inner1/inner2/\"\n+                \"NestedDense2/BiasAdd/ReadVariableOp\",\n+                \"call_scope/model/outer/ThreeDenses/inner1/inner2/\"\n+                \"NestedDense2/BiasAdd\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n+                \"MatMul/ReadVariableOp/resource\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n+                \"MatMul/ReadVariableOp\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/\" \"MatMul\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n+                \"BiasAdd/ReadVariableOp/resource\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n+                \"BiasAdd/ReadVariableOp\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/\" \"BiasAdd\",\n                 \"call_scope/model/OuterDense/MatMul/ReadVariableOp/resource\",\n                 \"call_scope/model/OuterDense/MatMul/ReadVariableOp\",\n                 \"call_scope/model/OuterDense/MatMul\",\n@@ -1855,7 +1874,8 @@ class DTypeTest(test_combinations.TestCase):\n     def input_cast_to_dtype(self):\n         layer = AddLayer()\n \n-        # Input should be cast to layer.dtype, so output should also be layer.dtype\n+        # Input should be cast to layer.dtype, so output should also be\n+        # layer.dtype\n         self.assertEqual(layer(self._const(\"float64\")).dtype, \"float32\")\n \n         layer = AddLayer(dtype=\"float64\")\n\n@@ -15,6 +15,7 @@\n \"\"\"Contains private utilities used mainly by the base Layer class.\"\"\"\n \n import tensorflow.compat.v2 as tf\n+import tensorflow.compat.v1 as tf1\n \n import functools\n import threading\n@@ -29,8 +30,8 @@ _call_context = threading.local()\n \n \n def create_mean_metric(value, name=None):\n-    # import keras will import base_layer and then this module, and metric relies\n-    # on base_layer, which result into a cyclic dependency.\n+    # import keras will import base_layer and then this module, and metric\n+    # relies on base_layer, which result into a cyclic dependency.\n     from keras import (\n         metrics as metrics_module,\n     )  # pylint: disable=g-import-not-at-top\n@@ -61,8 +62,8 @@ def make_variable(\n     `variable_scope.get_variable()` directly, so we use a subcomponent\n     that has fewer constraints (`variable_scope.variable()`).\n \n-    In the longer term, it seems like a similar \"default variable creator\" method\n-    should exist in `Trackable` instead. When this happens, we can get\n+    In the longer term, it seems like a similar \"default variable creator\"\n+    method should exist in `Trackable` instead. When this happens, we can get\n     rid of this temporary solution.\n \n     TODO(fchollet): remove this method when no longer needed.\n@@ -129,7 +130,7 @@ def make_variable(\n         # (that is to say, in TF2), we can use tf.Variable.\n         # However, this breaks legacy (Estimator) checkpoints because\n         # it changes variable names. Remove this when V1 is fully deprecated.\n-        return tf.compat.v1.Variable(\n+        return tf1.Variable(\n             initial_value=init_val,\n             name=name,\n             trainable=trainable,\n@@ -180,7 +181,7 @@ def have_all_keras_metadata(tensors):\n \n \n def generate_placeholders_from_shape(shape):\n-    return tf.compat.v1.placeholder(shape=shape, dtype=backend.floatx())\n+    return tf1.placeholder(shape=shape, dtype=backend.floatx())\n \n \n def create_keras_history(tensors):\n@@ -224,8 +225,8 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n \n     Args:\n       tensors: A structure of Tensors for which to create Keras metadata.\n-      processed_ops: Set. TensorFlow operations that have already been wrapped in\n-        `TensorFlowOpLayer` instances.\n+      processed_ops: Set. TensorFlow operations that have already been wrapped\n+        in `TensorFlowOpLayer` instances.\n       created_layers: List. The `TensorFlowOpLayer` instances created.\n \n     Returns:\n@@ -233,7 +234,7 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n       have been wrapped in `TensorFlowOpLayer` instances. Second element is\n       a list of the `TensorFlowOpLayer` instances created.\n     \"\"\"\n-    if tf.compat.v1.executing_eagerly_outside_functions():\n+    if tf1.executing_eagerly_outside_functions():\n         raise ValueError(\n             \"`create_keras_history` should only be called if eager is disabled!\"\n         )\n@@ -248,9 +249,7 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n     for tensor in tensor_list:\n         if getattr(tensor, \"_keras_history\", None) is not None:\n             continue\n-        if isinstance(\n-            tensor, (tf.SparseTensor, tf.compat.v1.SparseTensorValue)\n-        ):\n+        if isinstance(tensor, (tf.SparseTensor, tf1.SparseTensorValue)):\n             sparse_ops.append(tensor.op)\n             continue\n         if tf_utils.is_ragged(tensor):\n@@ -271,10 +270,10 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n                     # a constant. Variables cannot be supported.\n                     ds_with_session = (\n                         tf.distribute.in_cross_replica_context()\n-                        and not tf.compat.v1.executing_eagerly_outside_functions()\n+                        and not tf1.executing_eagerly_outside_functions()\n                     )\n                     using_xla = control_flow_util.GraphOrParentsInXlaContext(\n-                        tf.compat.v1.get_default_graph()\n+                        tf1.get_default_graph()\n                     )\n                     if (\n                         ds_with_session\n@@ -282,8 +281,9 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n                         or _UNSAFE_GRAPH_OP_LAYER_CREATION\n                     ):\n                         # In Legacy Graph mode, evaluating here makes Session be\n-                        # configured improperly. The downside of this is that saving\n-                        # via `get_config` breaks, but SavedModel still works.\n+                        # configured improperly. The downside of this is that\n+                        # saving via `get_config` breaks, but SavedModel still\n+                        # works.\n                         constants[i] = op_input\n                     else:\n                         with tf.init_scope():\n@@ -298,7 +298,7 @@ def _create_keras_history_helper(tensors, processed_ops, created_layers):\n                 node_def, constants=constants, name=name\n             )\n             created_layers.append(op_layer)\n-            op_layer._set_connectivity_metadata(  # pylint: disable=protected-access\n+            op_layer._set_connectivity_metadata(\n                 args=(layer_inputs,), kwargs={}, outputs=op.outputs\n             )\n             processed_ops.update([op])\n@@ -376,7 +376,7 @@ def is_in_eager_or_tf_function():\n def is_in_tf_function():\n     \"\"\"Returns if inside of a tf.function.\"\"\"\n     # Check if running in V1 graph mode.\n-    if not tf.compat.v1.executing_eagerly_outside_functions():\n+    if not tf1.executing_eagerly_outside_functions():\n         return False\n     if not tf.inside_function():\n         return False\n@@ -384,7 +384,7 @@ def is_in_tf_function():\n     if is_in_keras_graph():\n         return False\n     # Check for a v1 `wrap_function` FuncGraph.\n-    graph = tf.compat.v1.get_default_graph()\n+    graph = tf1.get_default_graph()\n     if getattr(graph, \"name\", False) and graph.name.startswith(\n         \"wrapped_function\"\n     ):\n@@ -477,14 +477,14 @@ class CallContext:\n       build_graph: Whether currently inside a Graph or FuncGraph.\n       training: Whether currently executing in training or inference mode.\n       saving: Whether currently saving to SavedModel.\n-      frozen: Whether currently executing inside a `Layer` with `trainable` set to\n-        `False`.\n+      frozen: Whether currently executing inside a `Layer` with `trainable` set\n+        to `False`.\n       in_keras_graph: Whether executing inside the Keras Graph.\n     \"\"\"\n \n     def __init__(self):\n-        # Handle `in_call` separately as it is the most-read attr and reading it is\n-        # on the hot path.\n+        # Handle `in_call` separately as it is the most-read attr and reading it\n+        # is on the hot path.\n         self.in_call = False\n         self._state = {\n             \"layer\": None,\n@@ -616,8 +616,8 @@ def from_saved_model(layer):\n def check_graph_consistency(tensor=None, method=\"add_loss\", force_raise=False):\n     \"\"\"Checks that tensors passed to `add_*` method match the Keras graph.\n \n-    When one of the `add_*` method is called inside a V2 conditional branch,\n-    the underlying tensor gets created in a FuncGraph managed by control_flow_v2.\n+    When one of the `add_*` method is called inside a V2 conditional branch, the\n+    underlying tensor gets created in a FuncGraph managed by control_flow_v2.\n     We need to raise clear error messages in such cases.\n \n     Args:\n@@ -630,7 +630,7 @@ def check_graph_consistency(tensor=None, method=\"add_loss\", force_raise=False):\n       RuntimeError: In case of an out-of-graph tensor.\n     \"\"\"\n     if force_raise or (\n-        tf.compat.v1.executing_eagerly_outside_functions()\n+        tf1.executing_eagerly_outside_functions()\n         and hasattr(tensor, \"graph\")\n         and tensor.graph.is_control_flow_graph\n     ):\n@@ -659,18 +659,17 @@ def check_graph_consistency(tensor=None, method=\"add_loss\", force_raise=False):\n           return self.dense(x)\n       \"\"\"\n             raise RuntimeError(\n-                \"You are using a layer with `activity_regularizer` in a control flow \"\n-                \"branch, e.g.:\\n{bad_example}\\nThis is currently not supported. \"\n-                \"Please move your call to the layer with `activity_regularizer` out \"\n-                \"of the control flow branch, e.g.:\\n{correct_example}\\n\"\n-                \"You can also resolve this by marking your outer model/layer dynamic\"\n-                \" (eager-only) by passing `dynamic=True` to the layer constructor. \"\n-                \"Any kind of control flow is supported with dynamic layers. \"\n-                \"Note that using `dynamic=True` requires you to implement static \"\n-                \"shape inference in the `compute_output_shape(input_shape)` \"\n-                \"method.\".format(\n-                    bad_example=bad_example, correct_example=correct_example\n-                )\n+                \"You are using a layer with `activity_regularizer` in a \"\n+                f\"control flow branch, e.g.:\\n{bad_example}\\nThis is currently \"\n+                \"not supported. Please move your call to the layer with \"\n+                \"`activity_regularizer` out of the control flow branch, \"\n+                f\"e.g.:\\n{correct_example}\\nYou can also resolve this by \"\n+                \"marking your outer model/layer dynamic (eager-only) by \"\n+                \"passing `dynamic=True` to the layer constructor. Any kind of \"\n+                \"control flow is supported with dynamic layers. Note that \"\n+                \"using `dynamic=True` requires you to implement static shape \"\n+                \"inference in the `compute_output_shape(input_shape)` \"\n+                \"method.\"\n             )\n \n         if method == \"add_metric\":\n@@ -776,15 +775,15 @@ V2_DTYPE_BEHAVIOR = None\n def enable_v2_dtype_behavior():\n     \"\"\"Enable the V2 dtype behavior for Keras layers.\n \n-    By default, the V2 dtype behavior is enabled in TensorFlow 2, so this function\n-    is only useful if `tf.compat.v1.disable_v2_behavior` has been called. Since\n-    mixed precision requires V2 dtype behavior to be enabled, this function allows\n-    you to use mixed precision in Keras layers if `disable_v2_behavior` has been\n-    called.\n+    By default, the V2 dtype behavior is enabled in TensorFlow 2, so this\n+    function is only useful if `tf.compat.v1.disable_v2_behavior` has been\n+    called. Since mixed precision requires V2 dtype behavior to be enabled, this\n+    function allows you to use mixed precision in Keras layers if\n+    `disable_v2_behavior` has been called.\n \n-    When enabled, the dtype of Keras layers defaults to floatx (which is typically\n-    float32) instead of None. In addition, layers will automatically cast\n-    floating-point inputs to the layer's dtype.\n+    When enabled, the dtype of Keras layers defaults to floatx (which is\n+    typically float32) instead of None. In addition, layers will automatically\n+    cast floating-point inputs to the layer's dtype.\n \n     >>> x = tf.ones((4, 4, 4, 4), dtype='float64')\n     >>> layer = tf.keras.layers.Conv2D(filters=4, kernel_size=2)\n@@ -796,12 +795,12 @@ def enable_v2_dtype_behavior():\n \n     A layer author can opt-out their layer from the automatic input casting by\n     passing `autocast=False` to the base Layer's constructor. This disables the\n-    autocasting part of the V2 behavior for that layer, but not the defaulting to\n-    floatx part of the V2 behavior.\n+    autocasting part of the V2 behavior for that layer, but not the defaulting\n+    to floatx part of the V2 behavior.\n \n-    When a global `tf.keras.mixed_precision.Policy` is set, a Keras layer's dtype\n-    will default to the global policy instead of floatx. Layers will automatically\n-    cast inputs to the policy's compute_dtype.\n+    When a global `tf.keras.mixed_precision.Policy` is set, a Keras layer's\n+    dtype will default to the global policy instead of floatx. Layers will\n+    automatically cast inputs to the policy's compute_dtype.\n     \"\"\"\n     global V2_DTYPE_BEHAVIOR\n     V2_DTYPE_BEHAVIOR = True\n@@ -827,13 +826,14 @@ def v2_dtype_behavior_enabled():\n class TrackableWeightHandler:\n     \"\"\"Keras wrapper for handling tracking.Trackable object saving and restoring.\n \n-    This class handles Trackables in both V1 and V2 modes, ensuring that they can\n-    be saved and restored with the correct data and without adding additional ops\n-    on every save.\n+    This class handles Trackables in both V1 and V2 modes, ensuring that they\n+    can be saved and restored with the correct data and without adding\n+    additional ops on every save.\n \n     Attributes:\n       trackable: The trackable to wrap.\n-      num_tensors: The number of tensors that this trackable requires for saving.\n+      num_tensors: The number of tensors that this trackable requires for\n+        saving.\n     \"\"\"\n \n     def __init__(self, trackable):\n@@ -855,12 +855,12 @@ class TrackableWeightHandler:\n         elif len(saveables) == 1:\n             saveable = list(saveables)[0]\n \n-            if tf.compat.v1.executing_eagerly_outside_functions():\n-                # If we're in eager mode, we need to defer calling the Trackable's\n-                # saveable() callable until data export time.\n-                # However, it is safe to call the saveable as many times as we want, so\n-                # we will call it now to figure out how many tensors this Trackable will\n-                # produce.\n+            if tf1.executing_eagerly_outside_functions():\n+                # If we're in eager mode, we need to defer calling the\n+                # Trackable's saveable() callable until data export time.\n+                # However, it is safe to call the saveable as many times as we\n+                # want, so we will call it now to figure out how many tensors\n+                # this Trackable will produce.\n                 self._saveable = saveable\n                 self._num_tensors = len(self._saveable().specs)\n                 self._setter = lambda weights: self._saveable().restore(\n@@ -870,17 +870,17 @@ class TrackableWeightHandler:\n                     spec.tensor for spec in self._saveable().specs\n                 ]\n             else:\n-                # If we're in Graph mode, we need to evaluate the Saveable only once and\n-                # cache the resulting restore graph. Failing to do this will result in\n-                # new assignment ops being added to the graph each time set_weights() is\n-                # called.\n+                # If we're in Graph mode, we need to evaluate the Saveable only\n+                # once and cache the resulting restore graph. Failing to do this\n+                # will result in new assignment ops being added to the graph\n+                # each time set_weights() is called.\n                 self._placeholder_tensors = []\n                 self._saveable = saveable()\n                 self._num_tensors = len(self._saveable.specs)\n                 for spec in self._saveable.specs:\n                     tensor = spec.tensor\n                     self._placeholder_tensors.append(\n-                        tf.compat.v1.placeholder(tensor.dtype, tensor.shape)\n+                        tf1.placeholder(tensor.dtype, tensor.shape)\n                     )\n                 self._assign_op = self._saveable.restore(\n                     self._placeholder_tensors, None\n@@ -891,8 +891,8 @@ class TrackableWeightHandler:\n                 ]\n         else:\n             raise ValueError(\n-                \"Only Trackables with one Saveable are supported. The Trackable \"\n-                f\"{trackable} has {len(saveables)} Saveables.\"\n+                \"Only Trackables with one Saveable are supported. \"\n+                f\"The Trackable {trackable} has {len(saveables)} Saveables.\"\n             )\n \n     @property\n@@ -904,7 +904,8 @@ class TrackableWeightHandler:\n             raise ValueError(\n                 f\"Weight handler for trackable {self._trackable} received \"\n                 \"an incorrect number of weights: \"\n-                f\"expected {self._num_tensors} weights, got {len(weights)} weights.\"\n+                f\"expected {self._num_tensors} weights, \"\n+                f\"got {len(weights)} weights.\"\n             )\n         self._setter(weights)\n \n@@ -929,7 +930,7 @@ def no_ragged_support(inputs, layer_name):\n \n \n def is_split_variable(v):\n-    \"\"\"Returns True if `v` is either a PartionedVariable or a ShardedVariable.\"\"\"\n+    \"\"\"Returns True if `v` is a PartionedVariable or a ShardedVariable.\"\"\"\n     return hasattr(v, \"_variable_list\") or hasattr(v, \"_variables\")\n \n \n\n@@ -26,10 +26,10 @@ from keras.engine import base_layer_utils\n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class TrackableWeightHandlerTest(test_combinations.TestCase):\n     def get_table_handler(self):\n-        # Note: There is some repetition in these tests' setup. However, Tensorflow\n-        # does not play nicely with a separate setUp() call (causing errors related\n-        # to graph building), so we have to use a called setup instead of a setUp()\n-        # call.\n+        # Note: There is some repetition in these tests' setup. However,\n+        # Tensorflow does not play nicely with a separate setUp() call (causing\n+        # errors related to graph building), so we have to use a called setup\n+        # instead of a setUp() call.\n         table = tf.lookup.experimental.MutableHashTable(\n             key_dtype=tf.string, value_dtype=tf.int32, default_value=0\n         )\n\n@@ -109,19 +109,20 @@ class Layer(base_layer.Layer):\n     computations and variables. A layer's dtype can be queried via the\n     `Layer.dtype` property. The dtype is specified with the `dtype` constructor\n     argument. In TensorFlow 2, the dtype defaults to `tf.keras.backend.floatx()`\n-    if no dtype is passed. `floatx()` itself defaults to \"float32\". Additionally,\n-    layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed\n-    precision is used, layers may have different computation and variable dtypes.\n-    See `tf.keras.mixed_precision.Policy` for details on layer dtypes.\n+    if no dtype is passed. `floatx()` itself defaults to \"float32\".\n+    Additionally, layers will cast their inputs to the layer's dtype in\n+    TensorFlow 2. When mixed precision is used, layers may have different\n+    computation and variable dtypes.  See `tf.keras.mixed_precision.Policy` for\n+    details on layer dtypes.\n     \"\"\"\n \n-    # See tf.Module for the usage of this property.\n-    # The key for _obj_reference_counts_dict is a Trackable, which could be a\n-    # variable or layer etc. tf.Module._flatten will fail to flatten the key\n-    # since it is trying to convert Trackable to a string. This attribute can be\n-    # ignored even after the fix of nest lib, since the trackable object should\n-    # already been available as individual attributes. _obj_reference_counts_dict\n-    # just contains a copy of them.\n+    # See tf.Module for the usage of this property.  The key for\n+    # _obj_reference_counts_dict is a Trackable, which could be a variable or\n+    # layer etc. tf.Module._flatten will fail to flatten the key since it is\n+    # trying to convert Trackable to a string. This attribute can be ignored\n+    # even after the fix of nest lib, since the trackable object should already\n+    # been available as individual attributes. _obj_reference_counts_dict just\n+    # contains a copy of them.\n     _TF_MODULE_IGNORED_PROPERTIES = frozenset(\n         itertools.chain(\n             (\"_obj_reference_counts_dict\",),\n@@ -156,11 +157,11 @@ class Layer(base_layer.Layer):\n         # Indicates whether the layer's weights are updated during training\n         # and whether the layer's updates are run during training.\n         self._trainable = trainable\n-        # A stateful layer is a layer whose updates are run during inference too,\n-        # for instance stateful RNNs.\n+        # A stateful layer is a layer whose updates are run during inference\n+        # too, for instance stateful RNNs.\n         self._stateful = False\n-        # Indicates whether `build` needs to be called upon layer call, to create\n-        # the layer's weights.\n+        # Indicates whether `build` needs to be called upon layer call, to\n+        # create the layer's weights.\n         self.built = False\n         self._build_input_shape = None\n         # Provides information about which inputs are compatible with the layer.\n@@ -176,24 +177,24 @@ class Layer(base_layer.Layer):\n         self._updates = []\n         # Object to store all thread local layer properties.\n         self._thread_local = threading.local()\n-        # A list of zero-argument lambdas which return Tensors, used for variable\n-        # regularizers.\n+        # A list of zero-argument lambdas which return Tensors, used for\n+        # variable regularizers.\n         self._callable_losses = []\n         # A list of symbolic Tensors containing activity regularizers and losses\n         # manually added through `add_loss` in graph-building mode.\n         self._losses = []\n-        # A list of metric instances corresponding to the symbolic metric tensors\n-        # added using the `add_metric` API.\n+        # A list of metric instances corresponding to the symbolic metric\n+        # tensors added using the `add_metric` API.\n         self._metrics = []\n \n         # Note that models also have a dtype policy, as they are layers. For\n-        # functional models, the policy is only used in Model.compile, which wraps\n-        # the optimizer with a LossScaleOptimizer if the policy name is\n-        # \"mixed_float16\". Subclassed models additionally use the policy's compute\n-        # and variable dtypes, as like any ordinary layer.\n+        # functional models, the policy is only used in Model.compile, which\n+        # wraps the optimizer with a LossScaleOptimizer if the policy name is\n+        # \"mixed_float16\". Subclassed models additionally use the policy's\n+        # compute and variable dtypes, as like any ordinary layer.\n         self._set_dtype_policy(dtype)\n-        # Boolean indicating whether the layer automatically casts its inputs to the\n-        # layer's compute_dtype.\n+        # Boolean indicating whether the layer automatically casts its inputs to\n+        # the layer's compute_dtype.\n         self._autocast = kwargs.get(\n             \"autocast\", base_layer_utils.v2_dtype_behavior_enabled()\n         )\n@@ -211,9 +212,10 @@ class Layer(base_layer.Layer):\n \n         self._init_call_fn_args()\n \n-        # Whether the `call` method can be used to build a TF graph without issues.\n-        # This attribute has no effect if the model is created using the Functional\n-        # API. Instead, `model.dynamic` is determined based on the internal layers.\n+        # Whether the `call` method can be used to build a TF graph without\n+        # issues.  This attribute has no effect if the model is created using\n+        # the Functional API. Instead, `model.dynamic` is determined based on\n+        # the internal layers.\n         self._dynamic = dynamic\n \n         # Manage input shape information if passed.\n@@ -236,11 +238,11 @@ class Layer(base_layer.Layer):\n         # Manage initial weight values if passed.\n         self._initial_weights = kwargs.get(\"weights\", None)\n \n-        # Whether the layer will track any layers that is set as attribute on itself\n-        # as sub-layers, the weights from the sub-layers will be included in the\n-        # parent layer's variables() as well.\n-        # Default to True, which means auto tracking is turned on. Certain subclass\n-        # might want to turn it off, like Sequential model.\n+        # Whether the layer will track any layers that is set as attribute on\n+        # itself as sub-layers, the weights from the sub-layers will be included\n+        # in the parent layer's variables() as well.  Default to True, which\n+        # means auto tracking is turned on. Certain subclass might want to turn\n+        # it off, like Sequential model.\n         self._auto_track_sub_layers = True\n \n         # Mark this layer as having been originally built as a tf1 layer/model\n@@ -331,7 +333,8 @@ class Layer(base_layer.Layer):\n         Args:\n           name: Variable name.\n           shape: Variable shape. Defaults to scalar if unspecified.\n-          dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n+          dtype: The type of the variable. Defaults to `self.dtype` or\n+            `float32`.\n           initializer: Initializer instance (callable).\n           regularizer: Regularizer instance (callable).\n           trainable: Boolean, whether the variable should be part of the layer's\n@@ -344,10 +347,10 @@ class Layer(base_layer.Layer):\n           use_resource: Whether to use `ResourceVariable`.\n           synchronization: Indicates when a distributed a variable will be\n             aggregated. Accepted values are constants defined in the class\n-            `tf.VariableSynchronization`. By default the synchronization is set to\n-            `AUTO` and the current `DistributionStrategy` chooses\n-            when to synchronize. If `synchronization` is set to `ON_READ`,\n-            `trainable` must not be set to `True`.\n+            `tf.VariableSynchronization`. By default the synchronization is set\n+            to `AUTO` and the current `DistributionStrategy` chooses when to\n+            synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n+            must not be set to `True`.\n           aggregation: Indicates how a distributed variable will be aggregated.\n             Accepted values are constants defined in the class\n             `tf.VariableAggregation`.\n@@ -355,15 +358,16 @@ class Layer(base_layer.Layer):\n             `collections`, `experimental_autocast` and `caching_device`.\n \n         Returns:\n-          The created variable. Usually either a `Variable` or `ResourceVariable`\n-          instance. If `partitioner` is not `None`, a `PartitionedVariable`\n-          instance is returned.\n+          The created variable. Usually either a `Variable` or\n+          `ResourceVariable` instance. If `partitioner` is not `None`, a\n+          `PartitionedVariable` instance is returned.\n \n         Raises:\n           RuntimeError: If called with partitioned variable regularization and\n             eager execution is enabled.\n           ValueError: When giving unsupported dtype and no initializer or when\n-            trainable has been set to True with synchronization set as `ON_READ`.\n+            trainable has been set to True with synchronization set as\n+            `ON_READ`.\n         \"\"\"\n         if shape is None:\n             shape = ()\n@@ -379,17 +383,19 @@ class Layer(base_layer.Layer):\n         has_custom_getter = \"getter\" in kwargs\n         getter = kwargs.pop(\"getter\", base_layer_utils.make_variable)\n         collections_arg = kwargs.pop(\"collections\", None)\n-        # 'experimental_autocast' can be set to False by the caller to indicate an\n-        # AutoCastVariable should never be created.\n+        # 'experimental_autocast' can be set to False by the caller to indicate\n+        # an AutoCastVariable should never be created.\n         autocast = kwargs.pop(\"experimental_autocast\", True)\n-        # See the docstring for tf.Variable about the details for caching_device.\n+        # See the docstring for tf.Variable about the details for\n+        # caching_device.\n         caching_device = kwargs.pop(\"caching_device\", None)\n \n         if dtype is None:\n             dtype = self.dtype or backend.floatx()\n         dtype = tf.as_dtype(dtype)\n         if self._dtype_policy.variable_dtype is None:\n-            # The policy is \"_infer\", so we infer the policy from the variable dtype.\n+            # The policy is \"_infer\", so we infer the policy from the variable\n+            # dtype.\n             self._set_dtype_policy(policy.Policy(dtype.base_dtype.name))\n         initializer = initializers.get(initializer)\n         regularizer = regularizers.get(regularizer)\n@@ -399,12 +405,13 @@ class Layer(base_layer.Layer):\n             if trainable:\n                 raise ValueError(\n                     \"Synchronization value can be set to \"\n-                    \"VariableSynchronization.ON_READ only for non-trainable variables. \"\n-                    \"You have specified trainable=True and \"\n+                    \"VariableSynchronization.ON_READ only for non-trainable \"\n+                    \"variables. You have specified trainable=True and \"\n                     \"synchronization=VariableSynchronization.ON_READ.\"\n                 )\n             else:\n-                # Set trainable to be false when variable is to be synced on read.\n+                # Set trainable to be false when variable is to be synced on\n+                # read.\n                 trainable = False\n         elif trainable is None:\n             trainable = True\n@@ -418,11 +425,12 @@ class Layer(base_layer.Layer):\n             # If dtype is DT_BOOL, provide a default value `FALSE`\n             elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n                 initializer = tf.compat.v1.zeros_initializer()\n-            # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?\n+            # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX\n+            # here?\n             elif not has_custom_getter:\n-                # When `getter` is specified, it's possibly fine for `initializer` to be\n-                # None since it's up to the custom `getter` to raise error in case it\n-                # indeed needs `initializer`.\n+                # When `getter` is specified, it's possibly fine for\n+                # `initializer` to be None since it's up to the custom `getter`\n+                # to raise error in case it indeed needs `initializer`.\n                 raise ValueError(\n                     \"An initializer for variable %s of type %s is required\"\n                     \" for layer %s\" % (name, dtype.base_dtype, self.name)\n@@ -441,13 +449,13 @@ class Layer(base_layer.Layer):\n                 variable = old_getter(*args, **kwargs)\n                 return autocast_variable.create_autocast_variable(variable)\n \n-            # Also the caching_device does not work with the mixed precision API,\n-            # disable it if it is specified.\n+            # Also the caching_device does not work with the mixed precision\n+            # API, disable it if it is specified.\n             # TODO(b/142020079): Re-enable it once the bug is fixed.\n             if caching_device is not None:\n                 tf_logging.warning(\n-                    \"`caching_device` does not work with mixed precision API. Ignoring \"\n-                    \"user specified `caching_device`.\"\n+                    \"`caching_device` does not work with mixed precision API. \"\n+                    \"Ignoring user specified `caching_device`.\"\n                 )\n                 caching_device = None\n \n@@ -567,13 +575,13 @@ class Layer(base_layer.Layer):\n             An input shape tuple.\n         \"\"\"\n         if tf.executing_eagerly():\n-            # In this case we build the model first in order to do shape inference.\n-            # This is acceptable because the framework only calls\n-            # `compute_output_shape` on shape values that the layer would later be\n-            # built for. It would however cause issues in case a user attempts to\n-            # use `compute_output_shape` manually with shapes that are incompatible\n-            # with the shape the Layer will be called on (these users will have to\n-            # implement `compute_output_shape` themselves).\n+            # In this case we build the model first in order to do shape\n+            # inference.  This is acceptable because the framework only calls\n+            # `compute_output_shape` on shape values that the layer would later\n+            # be built for. It would however cause issues in case a user\n+            # attempts to use `compute_output_shape` manually with shapes that\n+            # are incompatible with the shape the Layer will be called on (these\n+            # users will have to implement `compute_output_shape` themselves).\n             self._maybe_build(input_shape)\n             with tf.compat.v1.get_default_graph().as_default():\n                 graph = tf.__internal__.FuncGraph(\"graph\")\n@@ -589,8 +597,8 @@ class Layer(base_layer.Layer):\n                         outputs = self(inputs, training=False)\n                     except TypeError as e:\n                         raise NotImplementedError(\n-                            \"We could not automatically infer the static shape of the \"\n-                            \"layer's output. Please implement the \"\n+                            \"We could not automatically infer the static \"\n+                            \"shape of the layer's output. Please implement the \"\n                             \"`compute_output_shape` method on your layer (%s).\"\n                             % self.__class__.__name__\n                         ) from e\n@@ -613,8 +621,8 @@ class Layer(base_layer.Layer):\n             objects, describing a candidate input for the layer.\n \n         Returns:\n-          Single TensorSpec or nested structure of TensorSpec objects, describing\n-            how the layer would transform the provided input.\n+          Single TensorSpec or nested structure of TensorSpec objects,\n+            describing how the layer would transform the provided input.\n \n         Raises:\n           TypeError: If input_signature contains a non-TensorSpec object.\n@@ -635,8 +643,8 @@ class Layer(base_layer.Layer):\n         dtype = self._compute_dtype\n         if dtype is None:\n             input_dtypes = [s.dtype for s in tf.nest.flatten(input_signature)]\n-            # Default behavior when self.dtype is None, is to use the first input's\n-            # dtype.\n+            # Default behavior when self.dtype is None, is to use the first\n+            # input's dtype.\n             dtype = input_dtypes[0]\n         return tf.nest.map_structure(\n             lambda s: tf.TensorSpec(dtype=dtype, shape=s), output_shape\n@@ -679,7 +687,8 @@ class Layer(base_layer.Layer):\n           Output tensor(s).\n \n         Note:\n-          - The following optional keyword arguments are reserved for specific uses:\n+          - The following optional keyword arguments are reserved for specific\n+            uses:\n             * `training`: Boolean scalar tensor of Python boolean indicating\n               whether the `call` is meant for training or inference.\n             * `mask`: Boolean input mask.\n@@ -690,8 +699,10 @@ class Layer(base_layer.Layer):\n             a Keras layer with masking support.\n \n         Raises:\n-          ValueError: if the layer's `call` method returns None (an invalid value).\n-          RuntimeError: if `super().__init__()` was not called in the constructor.\n+          ValueError: if the layer's `call` method returns None (an invalid\n+            value).\n+          RuntimeError: if `super().__init__()` was not called in the\n+            constructor.\n         \"\"\"\n         self._assert_built_as_v1()\n \n@@ -714,10 +725,10 @@ class Layer(base_layer.Layer):\n         call_context = base_layer_utils.call_context()\n         input_list = tf.nest.flatten(inputs)\n \n-        # We will attempt to build a TF graph if & only if all inputs are symbolic.\n-        # This is always the case in graph mode. It can also be the case in eager\n-        # mode when all inputs can be traced back to `keras.Input()` (when building\n-        # models using the functional API).\n+        # We will attempt to build a TF graph if & only if all inputs are\n+        # symbolic.  This is always the case in graph mode. It can also be the\n+        # case in eager mode when all inputs can be traced back to\n+        # `keras.Input()` (when building models using the functional API).\n         build_graph = tf_utils.are_all_symbolic_tensors(input_list)\n \n         # Accept NumPy and scalar inputs by converting to Tensors.\n@@ -733,10 +744,10 @@ class Layer(base_layer.Layer):\n             inputs = tf.nest.map_structure(_convert_non_tensor, inputs)\n             input_list = tf.nest.flatten(inputs)\n \n-        # Handle `mask` propagation from previous layer to current layer. Masks can\n-        # be propagated explicitly via the `mask` argument, or implicitly via\n-        # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n-        # explicitly take priority.\n+        # Handle `mask` propagation from previous layer to current layer. Masks\n+        # can be propagated explicitly via the `mask` argument, or implicitly\n+        # via setting the `_keras_mask` attribute on the inputs to a Layer.\n+        # Masks passed explicitly take priority.\n         mask_arg_passed_by_framework = False\n         input_masks = self._collect_input_masks(inputs, args, kwargs)\n         if (\n@@ -766,15 +777,16 @@ class Layer(base_layer.Layer):\n             # Priority 3a: `learning_phase()` has been set.\n             elif backend.global_learning_phase_is_set():\n                 training_value = backend.learning_phase()\n-            # Priority 3b: Pass the `learning_phase()` if in the Keras FuncGraph.\n+            # Priority 3b: Pass the `learning_phase()` if in the Keras\n+            # FuncGraph.\n             elif build_graph:\n                 with backend.get_graph().as_default():\n                     if base_layer_utils.is_in_keras_graph():\n                         training_value = backend.learning_phase()\n \n             if self._expects_training_arg and training_value is not None:\n-                # Force the training_value to be bool type which matches to the contract\n-                # for layer/model call args.\n+                # Force the training_value to be bool type which matches to the\n+                # contract for layer/model call args.\n                 if tf.is_tensor(training_value):\n                     training_value = tf.cast(training_value, tf.bool)\n                 else:\n@@ -785,16 +797,18 @@ class Layer(base_layer.Layer):\n                 training_arg_passed_by_framework = True\n \n         # Only create Keras history if at least one tensor originates from a\n-        # `keras.Input`. Otherwise this Layer may be being used outside the Keras\n-        # framework.\n+        # `keras.Input`. Otherwise this Layer may be being used outside the\n+        # Keras framework.\n         if build_graph and base_layer_utils.needs_keras_history(inputs):\n             base_layer_utils.create_keras_history(inputs)\n \n         with call_context.enter(self, inputs, build_graph, training_value):\n-            # Check input assumptions set after layer building, e.g. input shape.\n+            # Check input assumptions set after layer building, e.g. input\n+            # shape.\n             if build_graph:\n-                # Symbolic execution on symbolic tensors. We will attempt to build\n-                # the corresponding TF subgraph inside `backend.get_graph()`\n+                # Symbolic execution on symbolic tensors. We will attempt to\n+                # build the corresponding TF subgraph inside\n+                # `backend.get_graph()`\n                 input_spec.assert_input_compatibility(\n                     self.input_spec, inputs, self.name\n                 )\n@@ -807,12 +821,12 @@ class Layer(base_layer.Layer):\n                     self._maybe_build(inputs)\n                     cast_inputs = self._maybe_cast_inputs(inputs)\n \n-                    # Wrapping `call` function in autograph to allow for dynamic control\n-                    # flow and control dependencies in call. We are limiting this to\n-                    # subclassed layers as autograph is strictly needed only for\n-                    # subclassed layers and models.\n-                    # tf_convert will respect the value of autograph setting in the\n-                    # enclosing tf.function, if any.\n+                    # Wrapping `call` function in autograph to allow for dynamic\n+                    # control flow and control dependencies in call. We are\n+                    # limiting this to subclassed layers as autograph is\n+                    # strictly needed only for subclassed layers and models.\n+                    # tf_convert will respect the value of autograph setting in\n+                    # the enclosing tf.function, if any.\n                     if base_layer_utils.is_subclassed(\n                         self\n                     ) and not base_layer_utils.from_saved_model(self):\n@@ -840,12 +854,14 @@ class Layer(base_layer.Layer):\n                                 + '\\n\"\"\"'\n                             )\n                     else:\n-                        # We will use static shape inference to return symbolic tensors\n-                        # matching the specifications of the layer outputs.\n-                        # Since `self.dynamic` is True, we will never attempt to\n-                        # run the underlying TF graph (which is disconnected).\n-                        # TODO(fchollet): consider py_func as an alternative, which\n-                        # would enable us to run the underlying graph if needed.\n+                        # We will use static shape inference to return symbolic\n+                        # tensors matching the specifications of the layer\n+                        # outputs.  Since `self.dynamic` is True, we will never\n+                        # attempt to run the underlying TF graph (which is\n+                        # disconnected).\n+                        # TODO(fchollet): consider py_func as an alternative,\n+                        # which would enable us to run the underlying graph if\n+                        # needed.\n                         outputs = self._symbolic_call(inputs)\n \n                     if outputs is None:\n@@ -871,11 +887,11 @@ class Layer(base_layer.Layer):\n                     self._handle_activity_regularization(inputs, outputs)\n                     self._set_mask_metadata(inputs, outputs, input_masks)\n                     if hasattr(self, \"_set_inputs\") and not self.inputs:\n-                        # Subclassed network: explicitly set metadata normally set by\n-                        # a call to self._set_inputs().\n-                        # TODO(b/120997007): This should be done in Eager as well, but\n-                        # causes garbage collection issues because of the placeholders\n-                        # created on the default Keras graph.\n+                        # Subclassed network: explicitly set metadata normally\n+                        # set by a call to self._set_inputs().\n+                        # TODO(b/120997007): This should be done in Eager as\n+                        # well, but causes garbage collection issues because of\n+                        # the placeholders created on the default Keras graph.\n                         self._set_save_spec(inputs, args, kwargs)\n                         self._set_inputs(inputs, outputs)\n             else:\n@@ -899,17 +915,18 @@ class Layer(base_layer.Layer):\n             raise ValueError(\n                 \"Your Layer or Model is in an invalid state. \"\n                 \"This can happen for the following cases:\\n \"\n-                \"1. You might be interleaving estimator/non-estimator models or \"\n-                \"interleaving models/layers made in tf.compat.v1.Graph.as_default() \"\n-                \"with models/layers created outside of it. \"\n+                \"1. You might be interleaving estimator/non-estimator models \"\n+                \"or interleaving models/layers made in \"\n+                \"tf.compat.v1.Graph.as_default() with models/layers created \"\n+                \"outside of it. \"\n                 \"Converting a model to an estimator (via model_to_estimator) \"\n-                \"invalidates all models/layers made before the conversion (even \"\n-                \"if they were not the model converted to an estimator). \"\n+                \"invalidates all models/layers made before the conversion \"\n+                \"(even if they were not the model converted to an estimator). \"\n                 \"Similarly, making a layer or a model inside a \"\n-                \"a tf.compat.v1.Graph invalidates all layers/models you previously \"\n-                \"made outside of the graph.\\n\"\n-                \"2. You might be using a custom keras layer implementation with \"\n-                \" custom __init__ which didn't call super().__init__. \"\n+                \"a tf.compat.v1.Graph invalidates all layers/models you \"\n+                \"previously made outside of the graph.\\n\"\n+                \"2. You might be using a custom keras layer implementation \"\n+                \"with custom __init__ which didn't call super().__init__. \"\n                 \" Please check the implementation of %s and its bases.\"\n                 % (type(self),)\n             )\n@@ -987,14 +1004,16 @@ class Layer(base_layer.Layer):\n                         except ValueError as e:\n                             if \"InaccessibleTensorError\" in type(e).__name__:\n                                 # For one specific case of error we try to raise\n-                                # a more meaningful error message about the graph if we can.\n-                                # This error is an internal TF symbol that is not\n-                                # publicly exposed, so we check the name directly rather\n-                                # than using a direct import.\n+                                # a more meaningful error message about the\n+                                # graph if we can.  This error is an internal TF\n+                                # symbol that is not publicly exposed, so we\n+                                # check the name directly rather than using a\n+                                # direct import.\n                                 base_layer_utils.check_graph_consistency(\n                                     method=\"add_update\", force_raise=True\n                                 )\n-                            raise  # check_graph_consistency may not always raise.\n+                            # check_graph_consistency may not always raise.\n+                            raise\n                     base_layer_utils.check_graph_consistency(\n                         u, method=\"add_update\"\n                     )\n@@ -1005,9 +1024,10 @@ class Layer(base_layer.Layer):\n     def losses(self):\n         \"\"\"Losses which are associated with this `Layer`.\n \n-        Variable regularization tensors are created when this property is accessed,\n-        so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n-        propagate gradients back to the corresponding variables.\n+        Variable regularization tensors are created when this property is\n+        accessed, so it is eager safe: accessing `losses` under a\n+        `tf.GradientTape` will propagate gradients back to the corresponding\n+        variables.\n \n         Returns:\n           A list of tensors.\n@@ -1015,8 +1035,8 @@ class Layer(base_layer.Layer):\n         collected_losses = []\n         all_layers = self._flatten_layers()\n         for layer in all_layers:\n-            # If any eager losses are present, we assume the model to be part of an\n-            # eager training loop (either a custom one or the one used when\n+            # If any eager losses are present, we assume the model to be part of\n+            # an eager training loop (either a custom one or the one used when\n             # `run_eagerly=True`) and so we always return just the eager losses.\n             collected_losses.extend(layer._losses)\n             for regularizer in layer._callable_losses:\n@@ -1029,11 +1049,11 @@ class Layer(base_layer.Layer):\n     def add_loss(self, losses, inputs=None):\n         \"\"\"Add loss tensor(s), potentially dependent on layer inputs.\n \n-        Some losses (for instance, activity regularization losses) may be dependent\n-        on the inputs passed when calling a layer. Hence, when reusing the same\n-        layer on different inputs `a` and `b`, some entries in `layer.losses` may\n-        be dependent on `a` and some on `b`. This method automatically keeps track\n-        of dependencies.\n+        Some losses (for instance, activity regularization losses) may be\n+        dependent on the inputs passed when calling a layer. Hence, when reusing\n+        the same layer on different inputs `a` and `b`, some entries in\n+        `layer.losses` may be dependent on `a` and some on `b`. This method\n+        automatically keeps track of dependencies.\n \n         This method can be used inside a subclassed layer or model's `call`\n         function, in which case `losses` should be a Tensor or list of Tensors.\n@@ -1050,7 +1070,8 @@ class Layer(base_layer.Layer):\n         This method can also be called directly on a Functional Model during\n         construction. In this case, any loss Tensors passed to this Model must\n         be symbolic and be able to be traced back to the model's `Input`s. These\n-        losses become part of the model's topology and are tracked in `get_config`.\n+        losses become part of the model's topology and are tracked in\n+        `get_config`.\n \n         Example:\n \n@@ -1063,10 +1084,10 @@ class Layer(base_layer.Layer):\n         model.add_loss(tf.abs(tf.reduce_mean(x)))\n         ```\n \n-        If this is not the case for your loss (if, for example, your loss references\n-        a `Variable` of one of the model's layers), you can wrap your loss in a\n-        zero-argument lambda. These losses are not tracked as part of the model's\n-        topology since they can't be serialized.\n+        If this is not the case for your loss (if, for example, your loss\n+        references a `Variable` of one of the model's layers), you can wrap your\n+        loss in a zero-argument lambda. These losses are not tracked as part of\n+        the model's topology since they can't be serialized.\n \n         Example:\n \n@@ -1080,26 +1101,28 @@ class Layer(base_layer.Layer):\n         ```\n \n         Args:\n-          losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n-            may also be zero-argument callables which create a loss tensor.\n+          losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n+            losses may also be zero-argument callables which create a loss\n+            tensor.\n           inputs: Ignored when executing eagerly. If anything other than None is\n             passed, it signals the losses are conditional on some of the layer's\n             inputs, and thus they should only be run where these inputs are\n             available. This is the case for activity regularization losses, for\n             instance. If `None` is passed, the losses are assumed\n-            to be unconditional, and will apply across all dataflows of the layer\n-            (e.g. weight regularization losses).\n+            to be unconditional, and will apply across all dataflows of the\n+            layer (e.g. weight regularization losses).\n         \"\"\"\n \n         def _tag_unconditional(loss):\n-            \"\"\"Process the loss and tag it by setting loss._unconditional_loss.\"\"\"\n+            \"\"\"Process the loss and tag it by setting ._unconditional_loss.\"\"\"\n             if callable(loss):\n                 # We run the loss without autocasting, as regularizers are often\n                 # numerically unstable in float16.\n                 with autocast_variable.enable_auto_cast_variables(None):\n                     loss = loss()\n             if loss is None:\n-                return None  # Will be filtered out when computing the .losses property\n+                # Will be filtered out when computing the .losses property\n+                return None\n             if not tf.is_tensor(loss):\n                 loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n             loss._unconditional_loss = (\n@@ -1159,12 +1182,13 @@ class Layer(base_layer.Layer):\n \n         Args:\n           value: Metric tensor.\n-          aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n-            it indicates that the metric tensor provided has been aggregated\n-            already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n-            `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n-            given metric tensor will be sample-wise reduced using `mean` function.\n-            eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n+          aggregation: Sample-wise metric reduction function. If\n+            `aggregation=None`, it indicates that the metric tensor provided has\n+            been aggregated already. eg, `bin_acc = BinaryAccuracy(name='acc')`\n+            followed by `model.add_metric(bin_acc(y_true, y_pred))`. If\n+            aggregation='mean', the given metric tensor will be sample-wise\n+            reduced using `mean` function.  eg,\n+            `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n             aggregation='mean')`.\n           name: String metric name.\n \n@@ -1173,8 +1197,8 @@ class Layer(base_layer.Layer):\n         \"\"\"\n         if aggregation is not None and aggregation != \"mean\":\n             raise ValueError(\n-                \"We currently support only `mean` sample-wise metric aggregation. \"\n-                \"You provided aggregation=`%s`\" % aggregation\n+                \"We currently support only `mean` sample-wise metric \"\n+                \"aggregation. You provided aggregation=`%s`\" % aggregation\n             )\n \n         from_metric_obj = hasattr(value, \"_metric_obj\")\n@@ -1183,11 +1207,11 @@ class Layer(base_layer.Layer):\n \n         if name is None and not from_metric_obj:\n             # Eg. `self.add_metric(math_ops.reduce_sum(x), aggregation='mean')`\n-            # In eager mode, we use metric name to lookup a metric. Without a name,\n-            # a new Mean metric wrapper will be created on every model/layer call.\n-            # So, we raise an error when no name is provided.\n-            # We will do the same for symbolic mode for consistency although a name\n-            # will be generated if no name is provided.\n+            # In eager mode, we use metric name to lookup a metric. Without a\n+            # name, a new Mean metric wrapper will be created on every\n+            # model/layer call. So, we raise an error when no name is provided.\n+            # We will do the same for symbolic mode for consistency although a\n+            # name will be generated if no name is provided.\n \n             # We will not raise this error in the foll use case for the sake of\n             # consistency as name in provided in the metric constructor.\n@@ -1232,19 +1256,19 @@ class Layer(base_layer.Layer):\n     def add_update(self, updates):\n         \"\"\"Add update op(s), potentially dependent on layer inputs.\n \n-        Weight updates (for instance, the updates of the moving mean and variance\n-        in a BatchNormalization layer) may be dependent on the inputs passed\n-        when calling a layer. Hence, when reusing the same layer on\n+        Weight updates (for instance, the updates of the moving mean and\n+        variance in a BatchNormalization layer) may be dependent on the inputs\n+        passed when calling a layer. Hence, when reusing the same layer on\n         different inputs `a` and `b`, some entries in `layer.updates` may be\n         dependent on `a` and some on `b`. This method automatically keeps track\n         of dependencies.\n \n-        The `get_updates_for` method allows to retrieve the updates relevant to a\n-        specific set of inputs.\n+        The `get_updates_for` method allows to retrieve the updates relevant to\n+        a specific set of inputs.\n \n-        This call is ignored when eager execution is enabled (in that case, variable\n-        updates are run on the fly and thus do not need to be tracked for later\n-        execution).\n+        This call is ignored when eager execution is enabled (in that case,\n+        variable updates are run on the fly and thus do not need to be tracked\n+        for later execution).\n \n         Args:\n           updates: Update op, or list/tuple of update ops, or zero-arg callable\n@@ -1311,8 +1335,8 @@ class Layer(base_layer.Layer):\n         the layer.\n \n         For example, a Dense layer returns a list of two values-- per-output\n-        weights and the bias value. These can be used to set the weights of another\n-        Dense layer:\n+        weights and the bias value. These can be used to set the weights of\n+        another Dense layer:\n \n         >>> a = tf.keras.layers.Dense(1,\n         ...   kernel_initializer=tf.constant_initializer(1.))\n@@ -1381,8 +1405,8 @@ class Layer(base_layer.Layer):\n                 ref_shape = param.shape\n                 if not ref_shape.is_compatible_with(weight_shape):\n                     raise ValueError(\n-                        \"Layer weight shape %s not compatible with provided weight \"\n-                        \"shape %s\" % (ref_shape, weight_shape)\n+                        \"Layer weight shape %s not compatible with provided \"\n+                        \"weight shape %s\" % (ref_shape, weight_shape)\n                     )\n                 weight_value_tuples.append((param, weight))\n                 weight_index += 1\n@@ -1393,13 +1417,13 @@ class Layer(base_layer.Layer):\n         \"\"\"Returns the current weights of the layer.\n \n         The weights of a layer represent the state of the layer. This function\n-        returns both trainable and non-trainable weight values associated with this\n-        layer as a list of Numpy arrays, which can in turn be used to load state\n-        into similarly parameterized layers.\n+        returns both trainable and non-trainable weight values associated with\n+        this layer as a list of Numpy arrays, which can in turn be used to load\n+        state into similarly parameterized layers.\n \n         For example, a Dense layer returns a list of two values-- per-output\n-        weights and the bias value. These can be used to set the weights of another\n-        Dense layer:\n+        weights and the bias value. These can be used to set the weights of\n+        another Dense layer:\n \n         >>> a = tf.keras.layers.Dense(1,\n         ...   kernel_initializer=tf.constant_initializer(1.))\n@@ -1772,18 +1796,18 @@ class Layer(base_layer.Layer):\n     @property\n     @doc_controls.do_not_doc_inheritable\n     def inbound_nodes(self):\n-        \"\"\"Deprecated, do NOT use! Only for compatibility with external Keras.\"\"\"\n+        \"\"\"Deprecated, do NOT use! Only for external Keras compatibility .\"\"\"\n         return self._inbound_nodes\n \n     @property\n     @doc_controls.do_not_doc_inheritable\n     def outbound_nodes(self):\n-        \"\"\"Deprecated, do NOT use! Only for compatibility with external Keras.\"\"\"\n+        \"\"\"Deprecated, do NOT use! Only for external Keras compatibility .\"\"\"\n         return self._outbound_nodes\n \n-    ##############################################################################\n+    ###########################################################################\n     # Methods & attributes below are public aliases of other methods.         #\n-    ##############################################################################\n+    ###########################################################################\n \n     @property\n     def variables(self):\n@@ -1804,9 +1828,9 @@ class Layer(base_layer.Layer):\n     def non_trainable_variables(self):\n         return self.non_trainable_weights\n \n-    ##############################################################################\n-    # Methods & attributes below are all private and only used by the framework. #\n-    ##############################################################################\n+    ############################################################################\n+    # Methods & attributes below are all private and only used by the framework.\n+    ############################################################################\n \n     @property\n     def _inbound_nodes(self):\n@@ -1847,10 +1871,10 @@ class Layer(base_layer.Layer):\n             self._dtype_policy.name == \"mixed_float16\"\n             and not loss_scale_optimizer.strategy_supports_loss_scaling()\n         ):\n-            # Although only loss scaling doesn't support certain strategies, to avoid\n-            # confusion, we disallow the 'mixed_float16' policy with unsupported\n-            # strategies. This is because 'mixed_float16' requires loss scaling for\n-            # numeric stability.\n+            # Although only loss scaling doesn't support certain strategies, to\n+            # avoid confusion, we disallow the 'mixed_float16' policy with\n+            # unsupported strategies. This is because 'mixed_float16' requires\n+            # loss scaling for numeric stability.\n             strategy = tf.distribute.get_strategy()\n             raise ValueError(\n                 \"Mixed precision is not supported with the \"\n@@ -1861,7 +1885,8 @@ class Layer(base_layer.Layer):\n             )\n \n         # Performance optimization: cache the compute dtype as a Dtype object or\n-        # None, so that str to Dtype conversion doesn't happen in Layer.__call__.\n+        # None, so that str to Dtype conversion doesn't happen in\n+        # Layer.__call__.\n         if self._dtype_policy.compute_dtype:\n             self._compute_dtype_object = tf.as_dtype(\n                 self._dtype_policy.compute_dtype\n@@ -1876,7 +1901,8 @@ class Layer(base_layer.Layer):\n \n         Unless mixed-precision is used, this is the same as `Layer.dtype`.\n \n-        If self._autocast is True, layer's will cast floating-point inputs to this.\n+        If self._autocast is True, layer's will cast floating-point inputs to\n+        this.\n \n         Returns:\n           The layer's compute dtype.\n@@ -1912,8 +1938,8 @@ class Layer(base_layer.Layer):\n                 ):\n                     return tf.cast(x, compute_dtype)\n                 elif isinstance(x, tf.TensorSpec) and x.dtype.is_floating:\n-                    # Inputs may be TensorSpecs when this function is called from\n-                    # model._set_inputs.\n+                    # Inputs may be TensorSpecs when this function is called\n+                    # from model._set_inputs.\n                     return tf.TensorSpec(x.shape, compute_dtype, x.name)\n                 else:\n                     return x\n@@ -1927,9 +1953,9 @@ class Layer(base_layer.Layer):\n     # TODO(reedwm): Deprecate, then remove the _dtype property.\n     @property\n     def _dtype(self):\n-        # This is equivalent to returning self.dtype . We do not return self.dtype\n-        # as it would cause infinite recursion in a few subclasses, which override\n-        # \"dtype\" to return self._dtype.\n+        # This is equivalent to returning self.dtype . We do not return\n+        # self.dtype as it would cause infinite recursion in a few subclasses,\n+        # which override \"dtype\" to return self._dtype.\n         return self._dtype_policy.variable_dtype\n \n     @_dtype.setter\n@@ -1955,8 +1981,8 @@ class Layer(base_layer.Layer):\n             return\n         if len(match) > 1:\n             raise ValueError(\n-                \"Please provide different names for the metrics you have added. \"\n-                'We found {} metrics with the name: \"{}\"'.format(\n+                \"Please provide different names for the metrics you have \"\n+                'added. We found {} metrics with the name: \"{}\"'.format(\n                     len(match), name\n                 )\n             )\n@@ -1966,10 +1992,10 @@ class Layer(base_layer.Layer):\n         base_layer_utils.check_graph_consistency(value, method=\"add_metric\")\n         match = self._get_existing_metric(name)\n         if aggregation is None:\n-            # Iterate over the metrics and check if the given metric exists already.\n-            # This can happen when a metric instance is created in subclassed model\n-            # layer `__init__` and we have tracked that instance already in\n-            # model.__setattr__.\n+            # Iterate over the metrics and check if the given metric exists\n+            # already.  This can happen when a metric instance is created in\n+            # subclassed model layer `__init__` and we have tracked that\n+            # instance already in model.__setattr__.\n             if match:\n                 result_tensor = value\n                 metric_obj = match\n@@ -1980,14 +2006,16 @@ class Layer(base_layer.Layer):\n                 self._metrics.append(metric_obj)\n             else:\n                 raise ValueError(\n-                    \"We do not support adding an aggregated metric result tensor that \"\n-                    \"is not the output of a `tf.keras.metrics.Metric` metric instance. \"\n-                    \"Without having access to the metric instance we cannot reset the \"\n-                    \"state of a metric after every epoch during training. You can \"\n-                    \"create a `tf.keras.metrics.Metric` instance and pass the result \"\n-                    \"here or pass an un-aggregated result with `aggregation` parameter \"\n-                    \"set as `mean`. For example: `self.add_metric(tf.reduce_sum(inputs)\"\n-                    \", name='mean_activation', aggregation='mean')`\"\n+                    \"We do not support adding an aggregated metric result \"\n+                    \"tensor that is not the output of a \"\n+                    \"`tf.keras.metrics.Metric` metric instance. Without \"\n+                    \"having access to the metric instance we cannot reset the \"\n+                    \"state of a metric after every epoch during training. You \"\n+                    \"can create a `tf.keras.metrics.Metric` instance and pass \"\n+                    \"the result here or pass an un-aggregated result with \"\n+                    \"`aggregation` parameter set as `mean`. For example: \"\n+                    \"`self.add_metric(tf.reduce_sum(inputs), \"\n+                    \"name='mean_activation', aggregation='mean')` \"\n                 )\n         else:\n             # If a non-aggregated tensor is given as input (ie. `aggregation` is\n@@ -2080,7 +2108,7 @@ class Layer(base_layer.Layer):\n                     output._keras_mask._keras_history_checked = True\n \n     def _collect_input_masks(self, inputs, args, kwargs):\n-        \"\"\"Checks if `mask` argument was passed, else gathers mask from inputs.\"\"\"\n+        \"\"\"Checks if mask argument was passed, else gathers mask from inputs.\"\"\"\n         if self._call_spec.arg_was_passed(\"mask\", args, kwargs):\n             return self._call_spec.get_arg_value(\"mask\", args, kwargs)\n \n@@ -2113,8 +2141,8 @@ class Layer(base_layer.Layer):\n             The layer's attribute `attr` at the node of index `node_index`.\n \n         Raises:\n-            RuntimeError: If the layer has no inbound nodes, or if called in Eager\n-            mode.\n+            RuntimeError: If the layer has no inbound nodes, or if called in\n+                Eager mode.\n             ValueError: If the index provided does not match any node.\n         \"\"\"\n         if not self._inbound_nodes:\n@@ -2155,16 +2183,17 @@ class Layer(base_layer.Layer):\n             input_shapes = None\n             if all(hasattr(x, \"shape\") for x in input_list):\n                 input_shapes = tf.nest.map_structure(lambda x: x.shape, inputs)\n-            # Only call `build` if the user has manually overridden the build method.\n+            # Only call `build` if the user has manually overridden the build\n+            # method.\n             if not hasattr(self.build, \"_is_default\"):\n-                # Any setup work performed only once should happen in an `init_scope`\n-                # to avoid creating symbolic Tensors that will later pollute any eager\n-                # operations.\n+                # Any setup work performed only once should happen in an\n+                # `init_scope` to avoid creating symbolic Tensors that will\n+                # later pollute any eager operations.\n                 with tf_utils.maybe_init_scope(self):\n                     self.build(input_shapes)\n-            # We must set also ensure that the layer is marked as built, and the build\n-            # shape is stored since user defined build functions may not be calling\n-            # `super.build()`\n+            # We must set also ensure that the layer is marked as built, and the\n+            # build shape is stored since user defined build functions may not\n+            # be calling `super.build()`\n             Layer.build(self, input_shapes)\n \n         # Optionally load weight values specified at layer instantiation.\n@@ -2206,7 +2235,7 @@ class Layer(base_layer.Layer):\n \n     @property\n     def _obj_reference_counts(self):\n-        \"\"\"A dictionary counting the number of attributes referencing an object.\"\"\"\n+        \"\"\"A dict counting the number of attributes referencing an object.\"\"\"\n         self._maybe_create_attribute(\n             \"_obj_reference_counts_dict\",\n             object_identity.ObjectIdentityDictionary(),\n@@ -2218,10 +2247,10 @@ class Layer(base_layer.Layer):\n         \"\"\"Create the attribute with the default value if it hasn't been created.\n \n         This is useful for fields that is used for tracking purpose,\n-        _trainable_weights, or _layers. Note that user could create a layer subclass\n-        and assign an internal field before invoking the Layer.__init__(), the\n-        __setattr__() need to create the tracking fields and __init__() need to not\n-        override them.\n+        _trainable_weights, or _layers. Note that user could create a layer\n+        subclass and assign an internal field before invoking the\n+        Layer.__init__(), the __setattr__() need to create the tracking fields\n+        and __init__() need to not override them.\n \n         Args:\n           name: String, the name of the attribute.\n@@ -2231,18 +2260,19 @@ class Layer(base_layer.Layer):\n             self.__setattr__(name, default_value)\n \n     def __delattr__(self, name):\n-        # For any super.__delattr__() call, we will directly use the implementation\n-        # in Trackable and skip the behavior in AutoTrackable. The Layer was\n-        # originally use Trackable as base class, the change of using Module as base\n-        # class forced us to have AutoTrackable in the class hierarchy.\n+        # For any super.__delattr__() call, we will directly use the\n+        # implementation in Trackable and skip the behavior in AutoTrackable.\n+        # The Layer was originally use Trackable as base class, the change of\n+        # using Module as base class forced us to have AutoTrackable in the\n+        # class hierarchy.\n         #\n         # TODO(b/180760306) Keeping the status quo of skipping _delattr__ and\n         # __setattr__ in AutoTrackable may be unsustainable.\n         existing_value = getattr(self, name, None)\n \n-        # If this value is replacing an existing object assigned to an attribute, we\n-        # should clean it out to avoid leaking memory. First we check if there are\n-        # other attributes referencing it.\n+        # If this value is replacing an existing object assigned to an\n+        # attribute, we should clean it out to avoid leaking memory. First we\n+        # check if there are other attributes referencing it.\n         reference_counts = self._obj_reference_counts\n         if existing_value not in reference_counts:\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n@@ -2252,8 +2282,8 @@ class Layer(base_layer.Layer):\n \n         reference_count = reference_counts[existing_value]\n         if reference_count > 1:\n-            # There are other remaining references. We can't remove this object from\n-            # _layers etc.\n+            # There are other remaining references. We can't remove this object\n+            # from _layers etc.\n             reference_counts[existing_value] = reference_count - 1\n             super(tf.__internal__.tracking.AutoTrackable, self).__delattr__(\n                 name\n@@ -2313,14 +2343,15 @@ class Layer(base_layer.Layer):\n             except AttributeError:\n                 raise AttributeError(\n                     (\n-                        'Can\\'t set the attribute \"{}\", likely because it conflicts with '\n-                        \"an existing read-only @property of the object. Please choose a \"\n-                        \"different name.\"\n+                        'Can\\'t set the attribute \"{}\", likely because it '\n+                        \"conflicts with an existing read-only @property of the \"\n+                        \"object. Please choose a different name.\"\n                     ).format(name)\n                 )\n             return\n \n-        # Keep track of trackable objects, for the needs of `Network.save_weights`.\n+        # Keep track of trackable objects, for the needs of\n+        # `Network.save_weights`.\n         value = tf.__internal__.tracking.sticky_attribute_assignment(\n             trackable=self, value=value, name=name\n         )\n@@ -2328,8 +2359,8 @@ class Layer(base_layer.Layer):\n         reference_counts = self._obj_reference_counts\n         reference_counts[value] = reference_counts.get(value, 0) + 1\n \n-        # Clean out the old attribute, which clears _layers and _trainable_weights\n-        # if necessary.\n+        # Clean out the old attribute, which clears _layers and\n+        # _trainable_weights if necessary.\n         try:\n             self.__delattr__(name)\n         except AttributeError:\n@@ -2346,8 +2377,8 @@ class Layer(base_layer.Layer):\n             ):\n                 self._metrics.append(val)\n \n-        # TODO(scottzhu): Need to track Module object as well for weight tracking.\n-        # Be careful about metric if it becomes a Module in future.\n+        # TODO(scottzhu): Need to track Module object as well for weight\n+        # tracking.  Be careful about metric if it becomes a Module in future.\n         # Append value to self._layers if relevant\n         if getattr(self, \"_auto_track_sub_layers\", True) and (\n             isinstance(value, Layer) or base_layer_utils.has_weights(value)\n@@ -2371,8 +2402,8 @@ class Layer(base_layer.Layer):\n             if not isinstance(val, tf.Variable):\n                 continue\n \n-            # Users may add extra weights/variables\n-            # simply by assigning them to attributes (invalid for graph networks)\n+            # Users may add extra weights/variables simply by assigning them to\n+            # attributes (invalid for graph networks)\n             self._maybe_create_attribute(\"_trainable_weights\", [])\n             self._maybe_create_attribute(\"_non_trainable_weights\", [])\n             if val.trainable:\n@@ -2386,8 +2417,8 @@ class Layer(base_layer.Layer):\n \n             backend.track_variable(val)\n \n-        # TODO(b/180760306) Skip the auto trackable from tf.Module to keep status\n-        # quo. See the comment at __delattr__.\n+        # TODO(b/180760306) Skip the auto trackable from tf.Module to keep\n+        # status quo. See the comment at __delattr__.\n         super(tf.__internal__.tracking.AutoTrackable, self).__setattr__(\n             name, value\n         )  # pylint: disable=bad-super-call\n@@ -2435,8 +2466,8 @@ class Layer(base_layer.Layer):\n         if save_type == \"savedmodel\":\n             cache = kwargs[\"cache\"]\n             # TODO(b/213628533): This must be called before super() to ensure\n-            # that any input shape changes are applied before getting the config of\n-            # the model.\n+            # that any input shape changes are applied before getting the config\n+            # of the model.\n             children = self._trackable_saved_model_saver.trackable_children(\n                 cache\n             )\n\n@@ -43,8 +43,8 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     instead.\n \n     Preprocessing layers are layers whose state gets computed before model\n-    training starts. They do not get updated during training.\n-    Most preprocessing layers implement an `adapt()` method for state computation.\n+    training starts. They do not get updated during training. Most\n+    preprocessing layers implement an `adapt()` method for state computation.\n \n     The `PreprocessingLayer` class is the base class you would subclass to\n     implement your own preprocessing layers.\n@@ -86,9 +86,10 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     def finalize_state(self):\n         \"\"\"Finalize the statistics for the preprocessing layer.\n \n-        This method is called at the end of `adapt` or after restoring a serialized\n-        preprocessing layer's state. This method handles any one-time operations\n-        that should occur on the layer's state before `Layer.__call__`.\n+        This method is called at the end of `adapt` or after restoring a\n+        serialized preprocessing layer's state. This method handles any one-time\n+        operations that should occur on the layer's state before\n+        `Layer.__call__`.\n         \"\"\"\n         pass\n \n@@ -138,9 +139,10 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n         \"\"\"Configures the layer for `adapt`.\n \n         Arguments:\n-          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s logic\n-            will not be wrapped in a `tf.function`. Recommended to leave this as\n-            `None` unless your `Model` cannot be run inside a `tf.function`.\n+          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n+            logic will not be wrapped in a `tf.function`. Recommended to leave\n+            this as `None` unless your `Model` cannot be run inside a\n+            `tf.function`.\n           steps_per_execution: Int. Defaults to 1. The number of batches to run\n             during each `tf.function` call. Running multiple batches inside a\n             single `tf.function` call can greatly improve performance on TPUs or\n@@ -160,17 +162,18 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n         \"\"\"Fits the state of the preprocessing layer to the data being passed.\n \n         After calling `adapt` on a layer, a preprocessing layer's state will not\n-        update during training. In order to make preprocessing layers efficient in\n-        any distribution context, they are kept constant with respect to any\n-        compiled `tf.Graph`s that call the layer. This does not affect the layer use\n-        when adapting each layer only once, but if you adapt a layer multiple times\n-        you will need to take care to re-compile any compiled functions as follows:\n+        update during training. In order to make preprocessing layers efficient\n+        in any distribution context, they are kept constant with respect to any\n+        compiled `tf.Graph`s that call the layer. This does not affect the layer\n+        use when adapting each layer only once, but if you adapt a layer\n+        multiple times you will need to take care to re-compile any compiled\n+        functions as follows:\n \n-         * If you are adding a preprocessing layer to a `keras.Model`, you need to\n-           call `model.compile` after each subsequent call to `adapt`.\n-         * If you are calling a preprocessing layer inside `tf.data.Dataset.map`,\n-           you should call `map` again on the input `tf.data.Dataset` after each\n-           `adapt`.\n+         * If you are adding a preprocessing layer to a `keras.Model`, you need\n+           to call `model.compile` after each subsequent call to `adapt`.\n+         * If you are calling a preprocessing layer inside\n+          `tf.data.Dataset.map`, you should call `map` again on the input\n+          `tf.data.Dataset` after each `adapt`.\n          * If you are using a `tf.function` directly which calls a preprocessing\n            layer, you need to call `tf.function` again on your callable after\n            each subsequent call to `adapt`.\n@@ -206,20 +209,21 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n          array([1.], dtype=float32),\n          array([2.], dtype=float32)]\n \n-        `adapt()` is meant only as a single machine utility to compute layer state.\n-        To analyze a dataset that cannot fit on a single machine, see\n-        [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+        `adapt()` is meant only as a single machine utility to compute layer\n+        state.  To analyze a dataset that cannot fit on a single machine, see\n+        [Tensorflow Transform](\n+        https://www.tensorflow.org/tfx/transform/get_started)\n         for a multi-machine, map-reduce solution.\n \n         Arguments:\n             data: The data to train on. It can be passed either as a tf.data\n               Dataset, or as a numpy array.\n             batch_size: Integer or `None`.\n-                Number of samples per state update.\n-                If unspecified, `batch_size` will default to 32.\n-                Do not specify the `batch_size` if your data is in the\n-                form of datasets, generators, or `keras.utils.Sequence` instances\n-                (since they generate batches).\n+                Number of samples per state update. If unspecified,\n+                `batch_size` will default to 32.  Do not specify the\n+                `batch_size` if your data is in the form of datasets,\n+                generators, or `keras.utils.Sequence` instances (since they\n+                generate batches).\n             steps: Integer or `None`.\n                 Total number of steps (batches of samples)\n                 When training with input tensors such as\n@@ -275,8 +279,8 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     def _adapt_maybe_build(self, data):\n         if not self.built:\n             try:\n-                # If this is a Numpy array or tensor, we can get shape from .shape.\n-                # If not, an attribute error will be thrown.\n+                # If this is a Numpy array or tensor, we can get shape from\n+                # .shape.  If not, an attribute error will be thrown.\n                 data_shape = data.shape\n                 data_shape_nones = tuple([None] * len(data.shape))\n             except AttributeError:\n\n@@ -130,7 +130,8 @@ class LossesContainer(Container):\n         self._built = False\n \n     def get_config(self):\n-        # In case `self._losses` is a single string where we convert it to a list.\n+        # In case `self._losses` is a single string where we convert it to a\n+        # list.\n         self._losses = tf.nest.flatten(self._losses)\n         return {\n             \"losses\": [\n@@ -216,13 +217,16 @@ class LossesContainer(Container):\n         \"\"\"Computes the overall loss.\n \n         Args:\n-          y_true: An arbitrary structure of Tensors representing the ground truth.\n-          y_pred: An arbitrary structure of Tensors representing a Model's outputs.\n+          y_true: An arbitrary structure of Tensors representing the ground\n+            truth.\n+          y_pred: An arbitrary structure of Tensors representing a Model's\n+            outputs.\n           sample_weight: An arbitrary structure of Tensors representing the\n             per-sample loss weights. If one Tensor is passed, it is used for all\n             losses. If multiple Tensors are passed, the structure should match\n             `y_pred`.\n-          regularization_losses: Additional losses to be added to the total loss.\n+          regularization_losses: Additional losses to be added to the total\n+            loss.\n \n         Returns:\n           The total loss as a `tf.Tensor`, or `None` if no loss results.\n@@ -259,7 +263,8 @@ class LossesContainer(Container):\n             loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n \n             total_loss_mean_value = loss_value\n-            # Correct for the `Mean` loss metrics counting each replica as a batch.\n+            # Correct for the `Mean` loss metrics counting each replica as a\n+            # batch.\n             if loss_obj.reduction == losses_utils.ReductionV2.SUM:\n                 total_loss_mean_value *= (\n                     tf.distribute.get_strategy().num_replicas_in_sync\n@@ -398,17 +403,17 @@ class MetricsContainer(Container):\n     def _check_duplicated_metrics(self, metrics, weighted_metrics):\n         \"\"\"Check and raise error when user provided metrics has any duplications.\n \n-        Note that metrics are stateful container, a shared metric instance between\n-        model.metric and model.weighted_metric will make the same intance to be\n-        udpated twice, and report wrong value.\n+        Note that metrics are stateful container, a shared metric instance\n+        between model.metric and model.weighted_metric will make the same\n+        intance to be udpated twice, and report wrong value.\n \n         Args:\n           metrics: User provided metrics list.\n           weighted_metrics: User provided weighted metrics list.\n \n         Raises:\n-          ValueError, when duplicated metrics instance discovered in user provided\n-            metrics and weighted metrics.\n+          ValueError, when duplicated metrics instance discovered in user\n+            provided metrics and weighted metrics.\n         \"\"\"\n         seen = set()\n         duplicated = []\n@@ -439,7 +444,7 @@ class MetricsContainer(Container):\n \n     @property\n     def unweighted_metrics(self):\n-        \"\"\"Metrics in this container that should not be passed `sample_weight`.\"\"\"\n+        \"\"\"Metrics in the container that should not be passed sample_weight.\"\"\"\n         if not self._built:\n             return None\n         return tf.nest.flatten(self._metrics)\n@@ -473,8 +478,8 @@ class MetricsContainer(Container):\n             self._weighted_metrics\n         )\n \n-        # Convert to `Metric` objects, potentially disambiguating based on output\n-        # properties.\n+        # Convert to `Metric` objects, potentially disambiguating based on\n+        # output properties.\n         self._metrics = tf.__internal__.nest.map_structure_up_to(\n             y_pred, self._get_metric_objects, self._metrics, y_true, y_pred\n         )\n@@ -509,7 +514,8 @@ class MetricsContainer(Container):\n     def _set_metric_names(self):\n         \"\"\"Sets unique metric names.\"\"\"\n         # For multi-output models, prepend the output name to the metric name.\n-        # For weighted metrics, prepend \"weighted_\" if the name would be non-unique.\n+        # For weighted metrics, prepend \"weighted_\" if the name would be\n+        # non-unique.\n         # pylint: disable=protected-access\n         metric_names = set()\n         is_multi_output = len(self._output_names) > 1\n@@ -525,7 +531,8 @@ class MetricsContainer(Container):\n                 if m._name in metric_names:\n                     raise ValueError(\n                         f\"Found two metrics with the same name: {m._name}. \"\n-                        \"All the metrics added to the model need to have unique names.\"\n+                        \"All the metrics added to the model need to have \"\n+                        \"unique names.\"\n                     )\n                 metric_names.add(m._name)\n \n@@ -542,14 +549,15 @@ class MetricsContainer(Container):\n \n                 if wm._name in metric_names:\n                     raise ValueError(\n-                        f\"Found two weighted metrics with the same name: {wm._name}.\"\n-                        \"All the metrics added to the model need to have unique names.\"\n+                        \"Found two weighted metrics with the same name: \"\n+                        f\"{wm._name}.All the metrics added to the model need \"\n+                        \"to have unique names.\"\n                     )\n                 metric_names.add(wm._name)\n         # pylint: enable=protected-access\n \n     def _create_ordered_metrics(self):\n-        \"\"\"Cache the flat order needed when returning metrics, for backwards compat.\"\"\"\n+        \"\"\"Cache the flat order needed when return metrics, for backcompat.\"\"\"\n         self._metrics_in_order = []\n         for output_metrics, output_weighted_metrics in zip(\n             self._metrics, self._weighted_metrics\n\n@@ -356,14 +356,14 @@ class LossesContainerTest(test_combinations.TestCase):\n         self.assertEqual(loss_container._losses[1].name, \"custom_loss_class\")\n \n     def test_ragged_tensor_output(self):\n-        \"\"\"Ensure that ragged tensors can be passed as targets and predictions.\"\"\"\n+        \"\"\"Ensure ragged tensors can be passed as targets and predictions.\"\"\"\n \n         def custom_loss_fn(y_true, y_pred):\n             \"\"\"MSE supports RaggedTensors directly.\"\"\"\n             return losses_mod.mse(y_true, y_pred)\n \n         class CustomLossClass(losses_mod.Loss):\n-            \"\"\"User defined loss function must implement RaggedTensor support.\"\"\"\n+            \"\"\"User defined loss func must implement RaggedTensor support.\"\"\"\n \n             def call(self, y_true, y_pred):\n                 losses = tf.ragged.map_flat_values(\n\n@@ -54,10 +54,10 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     to simplify the training code path, all the input data object will be\n     converted to `tf.data.Dataset` if possible.\n \n-    Note that since this class is mainly targeted for TF 2.0, it might have a lot\n-    of assumptions under the hood, e.g. eager context by default, distribution\n-    strategy, etc. In the meantime, some legacy feature support might be dropped,\n-    eg, Iterator from dataset API in v1, etc.\n+    Note that since this class is mainly targeted for TF 2.0, it might have a\n+    lot of assumptions under the hood, e.g. eager context by default,\n+    distribution strategy, etc. In the meantime, some legacy feature support\n+    might be dropped, eg, Iterator from dataset API in v1, etc.\n \n     The sample usage of this class is like:\n \n@@ -78,9 +78,9 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     def can_handle(x, y=None):\n         \"\"\"Whether the current DataAdapter could handle the input x and y.\n \n-        Structure wise, x and y can be single object, or list of objects if there\n-        multiple input/output, or dictionary of objects when the input/output are\n-        named.\n+        Structure wise, x and y can be single object, or list of objects if\n+        there multiple input/output, or dictionary of objects when the\n+        input/output are named.\n \n         Args:\n           x: input features.\n@@ -95,23 +95,25 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     def __init__(self, x, y=None, **kwargs):\n         \"\"\"Create a DataAdapter based on data inputs.\n \n-        The caller must make sure to call `can_handle()` first before invoking this\n-        method. Provide unsupported data type will result into unexpected behavior.\n+        The caller must make sure to call `can_handle()` first before invoking\n+        this method. Provide unsupported data type will result into unexpected\n+        behavior.\n \n         Args:\n           x: input features.\n           y: target labels. Note that y could be None in the case of prediction.\n-          **kwargs: Other keyword arguments for DataAdapter during the construction\n-            of the tf.dataset.Dataset. For example:\n+          **kwargs: Other keyword arguments for DataAdapter during the\n+            construction of the tf.dataset.Dataset. For example:\n             - Numpy data might have `sample_weights` which will be used for\n               weighting the loss function during training.\n-            - Numpy data might need to have `batch_size` parameter when constructing\n-              the dataset and iterator.\n+            - Numpy data might need to have `batch_size` parameter when\n+              constructing the dataset and iterator.\n             - Certain input might need to be distribution strategy aware. When\n-              `distribution_strategy` is passed, the created dataset need to respect\n-              the strategy.\n-            DataAdapter might choose to ignore any keyword argument if it doesn't\n-            use it, or raise exception if any required argument is not provided.\n+              `distribution_strategy` is passed, the created dataset need to\n+              respect the strategy.\n+            DataAdapter might choose to ignore any keyword argument if it\n+            doesn't use it, or raise exception if any required argument is not\n+            provided.\n         \"\"\"\n         if not self.can_handle(x, y):\n             raise ValueError(\n@@ -122,9 +124,9 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     def get_dataset(self):\n         \"\"\"Get a dataset instance for the current DataAdapter.\n \n-        Note that the dataset returned does not repeat for epoch, so caller might\n-        need to create new iterator for the same dataset at the beginning of the\n-        epoch. This behavior might change in the future.\n+        Note that the dataset returned does not repeat for epoch, so caller\n+        might need to create new iterator for the same dataset at the beginning\n+        of the epoch. This behavior might change in the future.\n \n         Returns:\n           A `tf.data.Dataset`. Caller might use the dataset in different\n@@ -137,15 +139,15 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     def get_size(self):\n         \"\"\"Return the size (number of batches) for the dataset created.\n \n-        For certain type of the data input, the number of batches is known, eg for\n-        Numpy data, the size is same as (number_of_element / batch_size). Whereas\n-        for dataset or python generator, the size is unknown since it may or may not\n-        have an end state.\n+        For certain type of the data input, the number of batches is known, eg\n+        for Numpy data, the size is same as (number_of_element / batch_size).\n+        Whereas for dataset or python generator, the size is unknown since it\n+        may or may not have an end state.\n \n         Returns:\n-          int, the number of batches for the dataset, or None if it is unknown. The\n-          caller could use this to control the loop of training, show progress bar,\n-          or handle unexpected StopIteration error.\n+          int, the number of batches for the dataset, or None if it is unknown.\n+          The caller could use this to control the loop of training, show\n+          progress bar, or handle unexpected StopIteration error.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -257,8 +259,8 @@ class TensorLikeDataAdapter(DataAdapter):\n         ).pop()\n         _check_data_cardinality(inputs)\n \n-        # If batch_size is not passed but steps is, calculate from the input data.\n-        # Default to 32 for backwards compat.\n+        # If batch_size is not passed but steps is, calculate from the input\n+        # data.  Default to 32 for backwards compat.\n         if not batch_size:\n             batch_size = int(math.ceil(num_samples / steps)) if steps else 32\n \n@@ -288,32 +290,34 @@ class TensorLikeDataAdapter(DataAdapter):\n             indices_dataset = indices_dataset.repeat(epochs)\n \n         def permutation(_):\n-            # It turns out to be more performant to make a new set of indices rather\n-            # than reusing the same range Tensor. (presumably because of buffer\n-            # forwarding.)\n+            # It turns out to be more performant to make a new set of indices\n+            # rather than reusing the same range Tensor. (presumably because of\n+            # buffer forwarding.)\n             indices = tf.range(num_samples, dtype=tf.int64)\n             if shuffle and shuffle != \"batch\":\n                 indices = tf.random.shuffle(indices)\n             return indices\n \n-        # We prefetch a single element. Computing large permutations can take quite\n-        # a while so we don't want to wait for prefetching over an epoch boundary to\n-        # trigger the next permutation. On the other hand, too many simultaneous\n-        # shuffles can contend on a hardware level and degrade all performance.\n+        # We prefetch a single element. Computing large permutations can take\n+        # quite a while so we don't want to wait for prefetching over an epoch\n+        # boundary to trigger the next permutation. On the other hand, too many\n+        # simultaneous shuffles can contend on a hardware level and degrade all\n+        # performance.\n         indices_dataset = indices_dataset.map(permutation).prefetch(1)\n \n         def slice_batch_indices(indices):\n             \"\"\"Convert a Tensor of indices into a dataset of batched indices.\n \n-            This step can be accomplished in several ways. The most natural is to\n-            slice the Tensor in a Dataset map. (With a condition on the upper index to\n-            handle the partial batch.) However it turns out that coercing the Tensor\n-            into a shape which is divisible by the batch size (and handling the last\n-            partial batch separately) allows for a much more favorable memory access\n-            pattern and improved performance.\n+            This step can be accomplished in several ways. The most natural is\n+            to slice the Tensor in a Dataset map. (With a condition on the upper\n+            index to handle the partial batch.) However it turns out that\n+            coercing the Tensor into a shape which is divisible by the batch\n+            size (and handling the last partial batch separately) allows for a\n+            much more favorable memory access pattern and improved performance.\n \n             Args:\n-              indices: Tensor which determines the data order for an entire epoch.\n+              indices: Tensor which determines the data order for an entire\n+                epoch.\n \n             Returns:\n               A Dataset of batched indices.\n@@ -377,8 +381,8 @@ class TensorLikeDataAdapter(DataAdapter):\n \n         dataset = dataset.map(grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n \n-        # Default optimizations are disabled to avoid the overhead of (unnecessary)\n-        # input pipeline graph serialization and deserialization\n+        # Default optimizations are disabled to avoid the overhead of\n+        # (unnecessary) input pipeline graph serialization and deserialization\n         options = tf.data.Options()\n         options.experimental_optimization.apply_default_optimizations = False\n         if self._shuffle:\n@@ -451,10 +455,11 @@ class GenericArrayLikeDataAdapter(TensorLikeDataAdapter):\n \n     def __init__(self, *args, **kwargs):\n         logging.warning(\n-            \"Keras is training/fitting/evaluating on array-like data. Keras may \"\n-            \"not be optimized for this format, so if your input data format is \"\n-            \"supported by TensorFlow I/O (https://github.com/tensorflow/io) we \"\n-            \"recommend using that to load a Dataset instead.\"\n+            \"Keras is training/fitting/evaluating on array-like data. Keras \"\n+            \"may not be optimized for this format, so if your input data \"\n+            \"format is supported by TensorFlow I/O \"\n+            \"(https://github.com/tensorflow/io) we recommend using that to \"\n+            \"load a Dataset instead.\"\n         )\n \n         super().__init__(*args, **kwargs)\n@@ -541,9 +546,10 @@ class DatasetCreatorAdapter(DataAdapter):\n             return True\n \n     def should_recreate_iterator(self):\n-        # We expect users to shuffle the dataset in their `dataset_fn` supplied to\n-        # `DatasetCreator`. Since that is a buffered shuffle, we intend to not reset\n-        # the dataset so the batches that are not shuffled can still be pulled.\n+        # We expect users to shuffle the dataset in their `dataset_fn` supplied\n+        # to `DatasetCreator`. Since that is a buffered shuffle, we intend to\n+        # not reset the dataset so the batches that are not shuffled can still\n+        # be pulled.\n         return False\n \n     def get_size(self):\n@@ -574,8 +580,8 @@ class CompositeTensorDataAdapter(DataAdapter):\n             flat_inputs += tf.nest.flatten(y)\n \n         def _is_composite(v):\n-            # Dataset/iterator/DistributedDataset inherits from CompositeTensor but\n-            # should be handled by DatasetAdapter and GeneratorAdapter.\n+            # Dataset/iterator/DistributedDataset inherits from CompositeTensor\n+            # but should be handled by DatasetAdapter and GeneratorAdapter.\n             if (\n                 tf_utils.is_extension_type(v)\n                 and not isinstance(v, (tf.data.Dataset, tf.data.Iterator))\n@@ -623,8 +629,8 @@ class CompositeTensorDataAdapter(DataAdapter):\n         if shuffle:\n             dataset = dataset.shuffle(num_samples)\n \n-        # If batch_size is not passed but steps is, calculate from the input data.\n-        # Default to 32 for backwards compatibility.\n+        # If batch_size is not passed but steps is, calculate from the input\n+        # data.  Default to 32 for backwards compatibility.\n         if not batch_size:\n             batch_size = int(math.ceil(num_samples / steps)) if steps else 32\n \n@@ -739,8 +745,8 @@ class DatasetAdapter(DataAdapter):\n \n     def __init__(self, x, y=None, sample_weights=None, steps=None, **kwargs):\n         super().__init__(x, y, **kwargs)\n-        # Note that the dataset instance is immutable, its fine to reuse the user\n-        # provided dataset.\n+        # Note that the dataset instance is immutable, its fine to reuse the\n+        # user provided dataset.\n         self._dataset = x\n \n         # The user-provided steps.\n@@ -833,8 +839,8 @@ class GeneratorDataAdapter(DataAdapter):\n         model=None,\n         **kwargs\n     ):\n-        # Generators should never shuffle as exhausting the generator in order to\n-        # shuffle the batches is inefficient.\n+        # Generators should never shuffle as exhausting the generator in order\n+        # to shuffle the batches is inefficient.\n         kwargs.pop(\"shuffle\", None)\n \n         if not is_none_or_empty(y):\n@@ -850,8 +856,8 @@ class GeneratorDataAdapter(DataAdapter):\n \n         super().__init__(x, y, **kwargs)\n \n-        # Since we have to know the dtype of the python generator when we build the\n-        # dataset, we have to look at a batch to infer the structure.\n+        # Since we have to know the dtype of the python generator when we build\n+        # the dataset, we have to look at a batch to infer the structure.\n         peek, x = self._peek_and_restore(x)\n         peek = self._standardize_batch(peek)\n         peek = _process_tensorlike(peek)\n@@ -864,10 +870,10 @@ class GeneratorDataAdapter(DataAdapter):\n                     lambda x: model(x, training=False), args=(concrete_x,)\n                 )\n             except NotImplementedError:\n-                # The above call may fail if the model is a container-like class that\n-                # does not implement its own forward pass (e.g. a GAN or VAE where the\n-                # forward pass is handled by subcomponents).\n-                # Such a model does not need to be built.\n+                # The above call may fail if the model is a container-like class\n+                # that does not implement its own forward pass (e.g. a GAN or\n+                # VAE where the forward pass is handled by subcomponents).  Such\n+                # a model does not need to be built.\n                 pass\n \n         self._first_batch_size = int(tf.nest.flatten(peek)[0].shape[0])\n@@ -880,8 +886,9 @@ class GeneratorDataAdapter(DataAdapter):\n \n         output_signature = tf.nest.map_structure(_get_tensor_spec, peek)\n \n-        # Note that dataset API takes a callable that creates a generator object,\n-        # rather than generator itself, which is why we define a function here.\n+        # Note that dataset API takes a callable that creates a generator\n+        # object, rather than generator itself, which is why we define a\n+        # function here.\n         generator_fn = self._handle_multiprocessing(\n             x, workers, use_multiprocessing, max_queue_size\n         )\n@@ -1163,17 +1170,16 @@ def broadcast_sample_weight_modes(target_structure, sample_weight_modes):\n                 tf.nest.map_structure(lambda _: \"...\", sample_weight_modes)\n             )\n \n-            # Attempt to coerce sample_weight_modes to the target structure. This\n-            # implicitly depends on the fact that Model flattens outputs for its\n-            # internal representation.\n+            # Attempt to coerce sample_weight_modes to the target structure.\n+            # This implicitly depends on the fact that Model flattens outputs\n+            # for its internal representation.\n             try:\n                 sample_weight_modes = tf.nest.pack_sequence_as(\n                     target_structure, tf.nest.flatten(sample_weight_modes)\n                 )\n                 logging.warning(\n-                    \"sample_weight modes were coerced from\\n  {}\\n    to  \\n  {}\".format(\n-                        target_str, mode_str\n-                    )\n+                    \"sample_weight modes were coerced from\\n  \"\n+                    \"{}\\n    to  \\n  {}\".format(target_str, mode_str)\n                 )\n             except (ValueError, TypeError):\n                 raise ValueError(\n@@ -1389,7 +1395,7 @@ class DataHandler:\n \n         This will be `None` in the case where:\n \n-        (1) A `Dataset` of unknown cardinality was passed to the `DataHandler`, and\n+        (1) A `Dataset` of unknown cardinality was passed to the `DataHandler`,\n         (2) `steps_per_epoch` was not provided, and\n         (3) The first epoch of iteration has not yet completed.\n \n@@ -1429,10 +1435,10 @@ class DataHandler:\n         size = tf.data.experimental.cardinality(dataset)\n         if size == tf.data.experimental.INFINITE_CARDINALITY and steps is None:\n             raise ValueError(\n-                \"When passing an infinitely repeating dataset, please specify a \"\n-                \"`steps_per_epoch` value so that epoch level \"\n-                \"callbacks continue to work. The value can be arbitrary, or a number \"\n-                \"that you think correctly defines the size of an epoch. \"\n+                \"When passing an infinitely repeating dataset, please specify \"\n+                \"a `steps_per_epoch` value so that epoch level \"\n+                \"callbacks continue to work. The value can be arbitrary, or a \"\n+                \"number that you think correctly defines the size of an epoch. \"\n                 \"Epoch-level callbacks will then be called at this interval.\"\n             )\n         if size >= 0:\n@@ -1451,8 +1457,8 @@ class DataHandler:\n         ):\n             raise ValueError(\n                 \"Could not infer the size of the data. With \"\n-                \"`steps_per_execution > 1`, you must specify the number of steps \"\n-                \"to run.\"\n+                \"`steps_per_execution > 1`, you must specify the number of \"\n+                \"steps to run.\"\n             )\n \n \n@@ -1475,17 +1481,17 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n             data_adapter_cls = select_data_adapter(x, y)\n             return data_adapter_cls(x=x, y=y, **kwargs).get_dataset()\n \n-        # This check is needed because types like `tf.data.Dataset` don't work with\n-        # PSS yet. So only apply this logic to the types we can support.\n+        # This check is needed because types like `tf.data.Dataset` don't work\n+        # with PSS yet. So only apply this logic to the types we can support.\n         if isinstance(x, _get_tensor_types()) and isinstance(\n             y, _get_tensor_types()\n         ):\n             return dataset_creator.DatasetCreator(_dataset_fn)\n         else:\n             raise NotImplementedError(\n-                \"Only `tf.keras.utils.experimental.DatasetCreator`, `tf.Tensor`, \"\n-                \"numpy arrays and pandas dataframes are supported types at this \"\n-                \"time.\"\n+                \"Only `tf.keras.utils.experimental.DatasetCreator`, \"\n+                \"`tf.Tensor`, numpy arrays and pandas dataframes are \"\n+                \"supported types at this time.\"\n             )\n \n     def _configure_dataset_and_inferred_steps(\n@@ -1499,7 +1505,8 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n                     x, options=x.input_options\n                 )\n \n-            self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n+            coordinator = self._model._cluster_coordinator\n+            self._dataset = coordinator.create_per_worker_dataset(\n                 per_worker_dataset_fn\n             )\n         else:\n@@ -1507,9 +1514,8 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n             if not _is_distributed_dataset(x):\n                 x = strategy.experimental_distribute_dataset(x)\n \n-            self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n-                x\n-            )\n+            coordinator = self._model._cluster_coordinator\n+            self._dataset = coordinator.create_per_worker_dataset(x)\n \n         if steps_per_epoch == -1:\n             self._inferred_steps = None\n@@ -1518,7 +1524,7 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n             self._inferred_steps = steps_per_epoch\n \n     def sync(self):\n-        self._model._cluster_coordinator.join()  # pylint: disable=protected-access\n+        self._model._cluster_coordinator.join()\n \n \n @keras_export(\"keras.__internal__.utils.get_data_handler\", v1=[])\n@@ -1541,8 +1547,10 @@ def get_data_handler(*args, **kwargs):\n \n       # Assume x is a tf.data Dataset.\n       data_handler = data_adapter.get_data_handler(x=x)\n-      for epo_idx, iterator in data_handler.enumerate_epochs():  # Epoch iteration\n-          with data_handler.catch_stop_iteration(): # Stop on dataset exhaustion.\n+      # Epoch iteration\n+      for epo_idx, iterator in data_handler.enumerate_epochs():\n+          # Stop on dataset exhaustion.\n+          with data_handler.catch_stop_iteration():\n             for step in data_handler.steps(): # Step iteration\n                 step_result = step(iterator)\n     ```\n@@ -1595,7 +1603,8 @@ def _make_class_weight_map_fn(class_weight):\n \n         if tf.nest.is_nested(y):\n             raise ValueError(\n-                \"`class_weight` is only supported for Models with a single output.\"\n+                \"`class_weight` is only supported for Models with a single \"\n+                \"output.\"\n             )\n \n         if y.shape.rank > 2:\n@@ -1630,8 +1639,8 @@ def train_validation_split(arrays, validation_split):\n       arrays: Tensors to split. Allowed inputs are arbitrarily nested structures\n         of Tensors and NumPy arrays.\n       validation_split: Float between 0 and 1. The proportion of the dataset to\n-        include in the validation split. The rest of the dataset will be included\n-        in the training split.\n+        include in the validation split. The rest of the dataset will be\n+        included in the training split.\n     Returns:\n       `(train_arrays, validation_arrays)`\n     \"\"\"\n@@ -1663,10 +1672,11 @@ def train_validation_split(arrays, validation_split):\n \n     if split_at == 0 or split_at == batch_dim:\n         raise ValueError(\n-            \"Training data contains {batch_dim} samples, which is not sufficient \"\n-            \"to split it into a validation and training set as specified by \"\n-            \"`validation_split={validation_split}`. Either provide more data, or a \"\n-            \"different value for the `validation_split` argument.\".format(\n+            \"Training data contains {batch_dim} samples, which is not \"\n+            \"sufficient to split it into a validation and training set as \"\n+            \"specified by `validation_split={validation_split}`. Either \"\n+            \"provide more data, or a different value for the \"\n+            \"`validation_split` argument.\".format(\n                 batch_dim=batch_dim, validation_split=validation_split\n             )\n         )\n@@ -1731,8 +1741,8 @@ def unpack_x_y_sample_weight(data):\n       data: A tuple of the form `(x,)`, `(x, y)`, or `(x, y, sample_weight)`.\n \n     Returns:\n-      The unpacked tuple, with `None`s for `y` and `sample_weight` if they are not\n-      provided.\n+      The unpacked tuple, with `None`s for `y` and `sample_weight` if they are\n+      not provided.\n     \"\"\"\n     if isinstance(data, list):\n         data = tuple(data)\n\n@@ -138,8 +138,8 @@ class TestBatchSequence(data_utils.Sequence):\n         \"\"\"Creates a keras.utils.Sequence with increasing batch_size.\n \n         Args:\n-            batch_size (Union[int, List[int]]): Can be a list containing two values:\n-              start and end batch_size\n+            batch_size (Union[int, List[int]]): Can be a list containing two\n+                values: start and end batch_size\n             feature_shape (int): Number of features in a sample\n             epochs (int, optional): Number of epochs\n         \"\"\"\n@@ -147,8 +147,8 @@ class TestBatchSequence(data_utils.Sequence):\n         self.feature_shape = feature_shape\n \n         self._epochs = epochs\n-        # we use `on_epoch_end` method to prepare data for the next epoch\n-        # set current epoch to `-1`, so that `on_epoch_end` will increase it to `0`\n+        # we use `on_epoch_end` method to prepare data for the next epoch set\n+        # current epoch to `-1`, so that `on_epoch_end` will increase it to `0`\n         self._current_epoch = -1\n         # actual batch size will be set inside `on_epoch_end`\n         self._current_batch_size = 0\n@@ -683,8 +683,8 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n             tf.convert_to_tensor(self.arraylike_input)\n \n         # Then train on the array like.\n-        # It should not be converted to a tensor directly (which would force it into\n-        # memory), only the sliced data should be converted.\n+        # It should not be converted to a tensor directly (which would force it\n+        # into memory), only the sliced data should be converted.\n         self.model.compile(\n             loss=\"sparse_categorical_crossentropy\",\n             optimizer=\"sgd\",\n@@ -973,8 +973,8 @@ class GeneratorDataAdapterTest(DataAdapterTestBase):\n             max_queue_size=10,\n             steps_per_epoch=10,\n         )\n-        # Fit twice to ensure there isn't any duplication that prevent the worker\n-        # from starting.\n+        # Fit twice to ensure there isn't any duplication that prevent the\n+        # worker from starting.\n         self.model.fit(\n             self.iterator_input,\n             workers=1,\n@@ -1075,8 +1075,8 @@ class KerasSequenceAdapterTest(DataAdapterTestBase):\n             max_queue_size=10,\n             steps_per_epoch=10,\n         )\n-        # Fit twice to ensure there isn't any duplication that prevent the worker\n-        # from starting.\n+        # Fit twice to ensure there isn't any duplication that prevent the\n+        # worker from starting.\n         self.model.fit(\n             self.sequence_input,\n             workers=1,\n\n@@ -80,9 +80,10 @@ class TestDeferredSequential(test_combinations.TestCase):\n         self.assertLen(model.inputs, 1)\n         self.assertLen(model.outputs, 1)\n         # Inconsistency here: with eager `fit`, the model is built with shape\n-        # (2, 6), but with graph function `fit`, it is built with shape `(None, 6)`.\n-        # This is likely due to our assumption \"the batch size should be dynamic\"\n-        # at the level of `Model`. TODO(fchollet): investigate and resolve.\n+        # (2, 6), but with graph function `fit`, it is built with shape `(None,\n+        # 6)`.  This is likely due to our assumption \"the batch size should be\n+        # dynamic\" at the level of `Model`. TODO(fchollet): investigate and\n+        # resolve.\n         self.assertEqual(model.inputs[0].shape.as_list()[-1], 6)\n         self.assertEqual(model.outputs[0].shape.as_list()[-1], 2)\n \n\n@@ -200,8 +200,8 @@ class FeatureColumnsIntegrationTest(test_combinations.TestCase):\n         feature_layer = df.DenseFeatures([col_a, col_b], name=\"fc\")\n         dense = keras.layers.Dense(4)\n \n-        # This seems problematic.... We probably need something for DenseFeatures\n-        # the way Input is for InputLayer.\n+        # This seems problematic.... We probably need something for\n+        # DenseFeatures the way Input is for InputLayer.\n         output = dense(feature_layer)\n \n         model = keras.models.Model([feature_layer], [output])\n@@ -230,8 +230,8 @@ class FeatureColumnsIntegrationTest(test_combinations.TestCase):\n         fc2 = df.DenseFeatures([col_b, col_c], name=\"fc2\")\n         dense = keras.layers.Dense(4)\n \n-        # This seems problematic.... We probably need something for DenseFeatures\n-        # the way Input is for InputLayer.\n+        # This seems problematic.... We probably need something for\n+        # DenseFeatures the way Input is for InputLayer.\n         output = dense(fc1) + dense(fc2)\n \n         model = keras.models.Model([fc1, fc2], [output])\n\n@@ -68,9 +68,9 @@ class Functional(training_lib.Model):\n     model = keras.Model(inputs, outputs)\n     ```\n \n-    A `Functional` model constructed using the Functional API can also include raw\n-    TensorFlow functions, with the exception of functions that create Variables\n-    or assign ops.\n+    A `Functional` model constructed using the Functional API can also include\n+    raw TensorFlow functions, with the exception of functions that create\n+    Variables or assign ops.\n \n     Example:\n \n@@ -113,12 +113,14 @@ class Functional(training_lib.Model):\n         originated from `tf.keras.Input()`).\n       outputs: List of output tensors.\n       name: String, optional. Name of the model.\n-      trainable: Boolean, optional. If the model's variables should be trainable.\n+      trainable: Boolean, optional. If the model's variables should be\n+        trainable.\n     \"\"\"\n \n     # See tf.Module for the usage of this property.\n-    # The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail to\n-    # flatten the key since it is trying to convert Trackable/Layer to a string.\n+    # The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail\n+    # to flatten the key since it is trying to convert Trackable/Layer to a\n+    # string.\n     _TF_MODULE_IGNORED_PROPERTIES = frozenset(\n         itertools.chain(\n             (\n@@ -136,17 +138,17 @@ class Functional(training_lib.Model):\n     def __init__(self, inputs, outputs, name=None, trainable=True, **kwargs):\n         # This is used by the Model class, since we have some logic to swap the\n         # class in the __new__ method, which will lead to __init__ get invoked\n-        # twice. Using the skip_init to skip one of the invocation of __init__ to\n-        # avoid any side effects\n+        # twice. Using the skip_init to skip one of the invocation of __init__\n+        # to avoid any side effects\n         skip_init = kwargs.pop(\"skip_init\", False)\n         if skip_init:\n             return\n         generic_utils.validate_kwargs(kwargs, {})\n         super().__init__(name=name, trainable=trainable)\n-        # Check if the inputs contain any intermediate `KerasTensor` (not created\n-        # by tf.keras.Input()). In this case we need to clone the `Node` and\n-        # `KerasTensor` objects to mimic rebuilding a new model from new inputs.\n-        # This feature is only enabled in TF2 not in v1 graph mode.\n+        # Check if the inputs contain any intermediate `KerasTensor` (not\n+        # created by tf.keras.Input()). In this case we need to clone the `Node`\n+        # and `KerasTensor` objects to mimic rebuilding a new model from new\n+        # inputs.  This feature is only enabled in TF2 not in v1 graph mode.\n         if tf.compat.v1.executing_eagerly_outside_functions():\n             if not all(\n                 [\n@@ -161,8 +163,8 @@ class Functional(training_lib.Model):\n \n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _init_graph_network(self, inputs, outputs):\n-        # This method is needed for Sequential to reinitialize graph network when\n-        # layer is added or removed.\n+        # This method is needed for Sequential to reinitialize graph network\n+        # when layer is added or removed.\n \n         base_layer.keras_api_gauge.get_cell(\"Functional\").set(True)\n         self._is_graph_network = True\n@@ -208,8 +210,9 @@ class Functional(training_lib.Model):\n             lambda x: x.shape, inputs\n         )\n         self._compute_output_and_mask_jointly = True\n-        # `_expects_training_arg` is True since the `training` argument is always\n-        # present in the signature of the `call` method of a graph network.\n+        # `_expects_training_arg` is True since the `training` argument is\n+        # always present in the signature of the `call` method of a graph\n+        # network.\n         self._call_spec.expects_training_arg = True\n         self._call_spec.expects_mask_arg = True\n         # A graph network does not autocast inputs, as its layers will cast them\n@@ -222,10 +225,10 @@ class Functional(training_lib.Model):\n         self._output_coordinates = []\n \n         # This is for performance optimization when calling the Network on new\n-        # inputs. Every time the Network is called on a set on input tensors,\n-        # we compute the output tensors, output masks and output shapes in one pass,\n-        # then cache them here. When any of these outputs is queried later, we\n-        # retrieve it from there instead of recomputing it.\n+        # inputs. Every time the Network is called on a set on input tensors, we\n+        # compute the output tensors, output masks and output shapes in one\n+        # pass, then cache them here. When any of these outputs is queried\n+        # later, we retrieve it from there instead of recomputing it.\n         self._output_mask_cache = {}\n         self._output_tensor_cache = {}\n         self._output_shape_cache = {}\n@@ -277,11 +280,12 @@ class Functional(training_lib.Model):\n             self.input_names.append(layer.name)\n             if layer.is_placeholder:\n                 self._feed_input_names.append(layer.name)\n-                # Use batch_input_shape here because non-eager composite tensors may not\n-                # have a shape attribute that's meaningful (sparse, for instance, has\n-                # a tensor that's non-constant and needs to be fed). This means that\n-                # input layers that create placeholders will need to have the\n-                # batch_input_shape attr to allow for input shape validation.\n+                # Use batch_input_shape here because non-eager composite tensors\n+                # may not have a shape attribute that's meaningful (sparse, for\n+                # instance, has a tensor that's non-constant and needs to be\n+                # fed). This means that input layers that create placeholders\n+                # will need to have the batch_input_shape attr to allow for\n+                # input shape validation.\n                 self._feed_input_shapes.append(layer._batch_input_shape)\n                 self._feed_inputs.append(layer.input)\n \n@@ -289,9 +293,9 @@ class Functional(training_lib.Model):\n         self._set_save_spec(self._nested_inputs)\n         tf_utils.assert_no_legacy_layers(self.layers)\n \n-        # Note that this method is used by both functional and sequential models,\n-        # so we can't just have this method in functional.__init__, which will miss\n-        #  the coverage of sequential model.\n+        # Note that this method is used by both functional and sequential\n+        # models, so we can't just have this method in functional.__init__,\n+        # which will miss the coverage of sequential model.\n         if self._layout_map is not None:\n             layout_map_lib._map_functional_model_variable(\n                 self, self._layout_map\n@@ -405,8 +409,8 @@ class Functional(training_lib.Model):\n     def _set_output_names(self):\n         \"\"\"Assigns unique names to the Network's outputs.\n \n-        Output layers with multiple output tensors would otherwise lead to duplicate\n-        names in self.output_names.\n+        Output layers with multiple output tensors would otherwise lead to\n+        duplicate names in self.output_names.\n         \"\"\"\n         uniquified = []\n         output_names = set()\n@@ -430,20 +434,20 @@ class Functional(training_lib.Model):\n         for layer_index, layer in enumerate(self.layers):\n             try:\n                 if layer.weights:\n-                    # Keep a separate index for layers which have weights. This allows\n-                    # users to insert Layers without weights anywhere in the network\n-                    # without breaking checkpoints.\n+                    # Keep a separate index for layers which have weights. This\n+                    # allows users to insert Layers without weights anywhere in\n+                    # the network without breaking checkpoints.\n                     dependencies[\n                         \"layer_with_weights-%d\" % weight_layer_index\n                     ] = layer\n                     weight_layer_index += 1\n             except ValueError:\n-                # The layer might have weights, but may not be built yet. We just treat\n-                # it as layer without weight.\n+                # The layer might have weights, but may not be built yet. We\n+                # just treat it as layer without weight.\n                 pass\n \n-            # Even if it doesn't have weights, we should still track everything in\n-            # case it has/will have Trackable dependencies.\n+            # Even if it doesn't have weights, we should still track everything\n+            # in case it has/will have Trackable dependencies.\n             dependencies[\"layer-%d\" % layer_index] = layer\n         return dependencies\n \n@@ -491,8 +495,8 @@ class Functional(training_lib.Model):\n \n         Args:\n             inputs: A tensor or list of tensors.\n-            training: Boolean or boolean scalar tensor, indicating whether to run\n-              the `Network` in training mode or inference mode.\n+            training: Boolean or boolean scalar tensor, indicating whether to\n+                run the `Network` in training mode or inference mode.\n             mask: A mask or list of masks. A mask can be\n                 either a tensor or None (no mask).\n \n@@ -565,7 +569,8 @@ class Functional(training_lib.Model):\n                     layer_input_shapes = tf.nest.pack_sequence_as(\n                         layer_inputs, layer_input_shapes\n                     )\n-                    # Layers expect shapes to be tuples for `compute_output_shape`.\n+                    # Layers expect shapes to be tuples for\n+                    # `compute_output_shape`.\n                     layer_input_shapes = tf_utils.convert_shapes(\n                         layer_input_shapes, to_tuples=True\n                     )\n@@ -605,8 +610,8 @@ class Functional(training_lib.Model):\n         if not name:\n             cls_name = self.__class__.__name__\n             if self.__class__ == Functional:\n-                # Hide the functional class name from user, since its not a public\n-                # visible class. Use \"Model\" instead,\n+                # Hide the functional class name from user, since its not a\n+                # public visible class. Use \"Model\" instead,\n                 cls_name = \"Model\"\n             self._name = backend.unique_object_name(\n                 generic_utils.to_snake_case(cls_name), zero_based=zero_based\n@@ -681,29 +686,31 @@ class Functional(training_lib.Model):\n             if not tf.nest.is_nested(ref_inputs):\n                 ref_inputs = [self._nested_inputs]\n             if isinstance(ref_inputs, dict):\n-                # In the case that the graph is constructed with dict input tensors,\n-                # We will use the original dict key to map with the keys in the input\n-                # data. Note that the model.inputs is using nest.flatten to process the\n-                # input tensors, which means the dict input tensors are ordered by their\n-                # keys.\n+                # In the case that the graph is constructed with dict input\n+                # tensors, We will use the original dict key to map with the\n+                # keys in the input data. Note that the model.inputs is using\n+                # nest.flatten to process the input tensors, which means the\n+                # dict input tensors are ordered by their keys.\n                 ref_input_names = sorted(ref_inputs.keys())\n             else:\n                 ref_input_names = [\n                     inp._keras_history.layer.name for inp in ref_inputs\n                 ]\n \n-            # Raise an warning if there are more input data comparing to input tensor\n+            # Raise an warning if there are more input data comparing to input\n+            # tensor\n             if len(tensors) > len(ref_input_names):\n                 warnings.warn(\n-                    \"Input dict contained keys {} which did not match any model input. \"\n-                    \"They will be ignored by the model.\".format(\n+                    \"Input dict contained keys {} which did not match any \"\n+                    \"model input. They will be ignored by the model.\".format(\n                         [n for n in tensors.keys() if n not in ref_input_names]\n                     ),\n                     stacklevel=2,\n                 )\n \n             try:\n-                # Flatten in the order `Input`s were passed during Model construction.\n+                # Flatten in the order `Input`s were passed during Model\n+                # construction.\n                 return [tensors[n] for n in ref_input_names]\n             except KeyError:\n                 # TODO(b/151582614)\n@@ -715,34 +722,36 @@ class Functional(training_lib.Model):\n     def _conform_to_reference_input(self, tensor, ref_input):\n         \"\"\"Set shape and dtype based on `keras.Input`s.\"\"\"\n         if isinstance(tensor, tf.Tensor):\n-            # Allow (None,) and (None, 1) Tensors to be passed interchangeably. Use\n-            # the shape specified by the `keras.Input`.\n+            # Allow (None,) and (None, 1) Tensors to be passed interchangeably.\n+            # Use the shape specified by the `keras.Input`.\n             t_shape = tensor.shape\n             t_rank = t_shape.rank\n             ref_shape = ref_input.shape\n             ref_rank = ref_shape.rank\n             keras_history = getattr(tensor, \"_keras_history\", None)\n             if t_rank is not None and ref_rank is not None:\n-                # Should squeeze last dimension.\n-                # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).\n+                # Should squeeze last dimension.  True if tensor is (BATCH, ...,\n+                # 1) and reference is (BATCH, ...).\n                 if t_rank == ref_rank + 1 and t_shape[-1] == 1:\n                     tensor = tf.squeeze(tensor, axis=-1)\n-                # Should expand last_dimension.\n-                # True if tensor is (BATCH, ...) and reference is (BATCH, ..., 1).\n+                # Should expand last_dimension.  True if tensor is (BATCH, ...)\n+                # and reference is (BATCH, ..., 1).\n                 elif t_rank == ref_rank - 1 and ref_shape[-1] == 1:\n                     tensor = tf.expand_dims(tensor, axis=-1)\n             if keras_history is not None:  # Restore keras history.\n                 tensor._keras_history = keras_history\n \n-            # Add shape hints to Tensors that may have None shape dims but have shapes\n-            # defined by the `keras.Input` (not applicable in eager mode).\n+            # Add shape hints to Tensors that may have None shape dims but have\n+            # shapes defined by the `keras.Input` (not applicable in eager\n+            # mode).\n             if not tf.executing_eagerly():\n                 try:\n                     tensor.set_shape(tensor.shape.merge_with(ref_input.shape))\n                 except ValueError:\n                     logging.warning(\n-                        \"Model was constructed with shape {} for input {}, but it was \"\n-                        \"called on an input with incompatible shape {}.\".format(\n+                        \"Model was constructed with shape {} for input {}, \"\n+                        \"but it was called on an input with incompatible \"\n+                        \"shape {}.\".format(\n                             ref_input.shape, ref_input, tensor.shape\n                         )\n                     )\n@@ -751,8 +760,8 @@ class Functional(training_lib.Model):\n             tensor = tf.cast(tensor, dtype=ref_input.dtype)\n         elif tf_utils.is_extension_type(tensor):\n             # Dtype casting (If the extension type has a non-variant dtype and\n-            # supports being cast).  Only cast if necessary (since some extension\n-            # types may not implement tf.cast).\n+            # supports being cast).  Only cast if necessary (since some\n+            # extension types may not implement tf.cast).\n             tensor_dtype = getattr(tensor, \"dtype\", None)\n             ref_input_dtype = getattr(ref_input, \"dtype\", None)\n             if (\n@@ -834,17 +843,17 @@ class Functional(training_lib.Model):\n     def _insert_layers(self, layers, relevant_nodes=None):\n         \"\"\"Inserts Layers into the Network after Network creation.\n \n-        This is only valid for Keras Graph Networks.  Layers added via this function\n-        will be included in the `call` computation and `get_config` of this Network.\n-        They will not be added to the Network's outputs.\n+        This is only valid for Keras Graph Networks.  Layers added via this\n+        function will be included in the `call` computation and `get_config` of\n+        this Network.  They will not be added to the Network's outputs.\n \n         Args:\n           layers: Arbitrary nested structure of Layers. Layers must be reachable\n-            from one or more of the `keras.Input` Tensors that correspond to this\n-            Network's inputs.\n-          relevant_nodes: Nodes from the Layers that should be considered part of\n-            this Network. If `None`, all Nodes will be considered part of this\n-            Network.\n+            from one or more of the `keras.Input` Tensors that correspond to\n+            this Network's inputs.\n+          relevant_nodes: Nodes from the Layers that should be considered part\n+            of this Network. If `None`, all Nodes will be considered part of\n+            this Network.\n \n         Raises:\n           ValueError: If the layers depend on `Input`s not found in this Model.\n@@ -882,8 +891,8 @@ class Functional(training_lib.Model):\n         i = 0\n         while unprocessed_nodes:\n             i += 1\n-            # Do a sanity check. This can occur if `Input`s from outside this Model\n-            # are being relied on.\n+            # Do a sanity check. This can occur if `Input`s from outside this\n+            # Model are being relied on.\n             if i > 10000:\n                 raise ValueError(\n                     \"Layers could not be added due to missing \" \"dependencies.\"\n@@ -920,8 +929,8 @@ class Functional(training_lib.Model):\n     def _compute_tensor_usage_count(self):\n         \"\"\"Compute the #. of tensor usages for all the output tensors of layers.\n \n-        The computed tensor usage count is saved as `self._tensor_usage_count`. This\n-        is later used for saving memory in eager computation by releasing\n+        The computed tensor usage count is saved as `self._tensor_usage_count`.\n+        This is later used for saving memory in eager computation by releasing\n         no-longer-needed tensors as early as possible.\n         \"\"\"\n         tensor_usage_count = collections.Counter()\n@@ -958,8 +967,8 @@ class Functional(training_lib.Model):\n         new_nodes, new_layers = _map_subgraph_network(\n             self.inputs, [symbolic_loss]\n         )\n-        # Losses must be keyed on inputs no matter what in order to be supported in\n-        # DistributionStrategy.\n+        # Losses must be keyed on inputs no matter what in order to be supported\n+        # in DistributionStrategy.\n         add_loss_layer = base_layer.AddLoss(\n             unconditional=False, dtype=symbolic_loss.dtype\n         )\n@@ -984,8 +993,8 @@ class Functional(training_lib.Model):\n \n     def _get_save_spec(self, dynamic_batch=True, inputs_only=True):\n         if getattr(self, \"_has_explicit_input_shape\", True):\n-            # Functional models and Sequential models that have an explicit input\n-            # shape should use the batch size set by the input layer.\n+            # Functional models and Sequential models that have an explicit\n+            # input shape should use the batch size set by the input layer.\n             dynamic_batch = False\n         return super()._get_save_spec(dynamic_batch, inputs_only)\n \n@@ -1096,9 +1105,10 @@ def _map_graph_network(inputs, outputs):\n                 for x in tf.nest.flatten(node.keras_inputs):\n                     if id(x) not in computable_tensors:\n                         raise ValueError(\n-                            f\"Graph disconnected: cannot obtain value for tensor {x} \"\n-                            f'at layer \"{layer.name}\". The following previous layers '\n-                            f\"were accessed without issue: {layers_with_complete_input}\"\n+                            f\"Graph disconnected: cannot obtain value for \"\n+                            f'tensor {x} at layer \"{layer.name}\". '\n+                            \"The following previous layers were accessed \"\n+                            f\"without issue: {layers_with_complete_input}\"\n                         )\n                 for x in tf.nest.flatten(node.outputs):\n                     computable_tensors.add(id(x))\n@@ -1123,8 +1133,8 @@ def _build_map(outputs):\n     _keras_history connectivity metadata of `outputs`.\n \n     Args:\n-      outputs: the output tensors whose _keras_history metadata should be walked.\n-      This may be an arbitrary nested structure.\n+      outputs: the output tensors whose _keras_history metadata should be\n+        walked. This may be an arbitrary nested structure.\n \n     Returns:\n       A tuple like (ordered_nodes, layer_to_first_traversal_index)\n@@ -1218,8 +1228,8 @@ def _map_subgraph_network(inputs, outputs):\n def _should_skip_first_node(layer):\n     \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\n     # Networks that are constructed with an Input layer/shape start with a\n-    # pre-existing node linking their input to output. This node is excluded from\n-    # the network config.\n+    # pre-existing node linking their input to output. This node is excluded\n+    # from the network config.\n     if layer._self_tracked_trackables:\n         return (\n             isinstance(layer, Functional)\n@@ -1327,8 +1337,8 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n             node_data: Nested structure of `ListWrapper`.\n \n         Returns:\n-            Whether the node was processed (i.e. the layer was called on the inputs\n-            specified by the node data)\n+            Whether the node was processed (i.e. the layer was called on the\n+            inputs specified by the node data)\n \n         Raises:\n             ValueError: In case of improperly formatted `node_data`.\n@@ -1453,8 +1463,8 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n                     if process_node(layer, node_data):\n                         layer_nodes.pop(0)\n                     else:\n-                        # If a node can't be processed, stop processing the nodes of\n-                        # the current layer to maintain node ordering.\n+                        # If a node can't be processed, stop processing the\n+                        # nodes of the current layer to maintain node ordering.\n                         unprocessed_nodes[layer] = layer_nodes\n                         break\n \n@@ -1595,9 +1605,9 @@ class ModuleWrapper(base_layer.Layer):\n \n         Args:\n           module: The `tf.Module` instance to be wrapped.\n-          method_name: (Optional) str. The name of the method to use as the forward\n-            pass of the module. If not set, defaults to '__call__' if defined, or\n-            'call'.\n+          method_name: (Optional) str. The name of the method to use as the\n+            forward pass of the module. If not set, defaults to '__call__' if\n+            defined, or 'call'.\n           **kwargs: Additional keywrod arguments. See `tf.keras.layers.Layer`.\n \n         Raises:\n\n@@ -1082,12 +1082,12 @@ class NetworkConstructionTest(test_combinations.TestCase):\n \n         if share_already_used_layer:\n             # We have had model serialization/deserialization break in the past:\n-            # when a layer was previously used to construct other functional models\n-            # and had a non-empty list of inbound nodes before being used to define\n-            # the model being serialized/deserialized.\n-            # (The serialization/deserialization was not correctly adjusting\n-            # the node_index serialization/deserialization).\n-            # So, we explicitly test this case.\n+            # when a layer was previously used to construct other functional\n+            # models and had a non-empty list of inbound nodes before being used\n+            # to define the model being serialized/deserialized. (The\n+            # serialization/deserialization was not correctly adjusting the\n+            # node_index serialization/deserialization). So, we explicitly test\n+            # this case.\n             training_lib.Model([input1], identity_layer(input1))\n \n         outputs = MaybeAdd()(input1, x2=identity_layer(input2))\n@@ -1197,12 +1197,12 @@ class NetworkConstructionTest(test_combinations.TestCase):\n         identity_layer = IdentityLayer()\n         if share_already_used_layer:\n             # We have had model serialization/deserialization break in the past:\n-            # when a layer was previously used to construct other functional models\n-            # and had a non-empty list of inbound nodes before being used to define\n-            # the model being serialized/deserialized.\n-            # (The serialization/deserialization was not correctly adjusting\n-            # the node_index serialization/deserialization).\n-            # So, we explicitly test this case.\n+            # when a layer was previously used to construct other functional\n+            # models and had a non-empty list of inbound nodes before being used\n+            # to define the model being serialized/deserialized. (The\n+            # serialization/deserialization was not correctly adjusting the\n+            # node_index serialization/deserialization). So, we explicitly test\n+            # this case.\n             training_lib.Model([input2], identity_layer(input2))\n \n         outputs = MaybeAdd()(3.0, x2=identity_layer(input2))\n@@ -1231,13 +1231,14 @@ class NetworkConstructionTest(test_combinations.TestCase):\n     @test_combinations.generate(test_combinations.keras_mode_combinations())\n     def test_dont_cast_composite_unless_necessary(self):\n         if not tf.executing_eagerly():\n-            return  # Creating Keras inputs from a type_spec only supported in eager.\n+            # Creating Keras inputs from a type_spec only supported in eager.\n+            return\n \n         # TODO(edloper): Change this to tf.experimental.ExtensionTyep once\n         # it's been released.\n         class MyType(extension_type.ExtensionType):\n-            # TODO(edloper) Remove _shape and _dtype once Keras has been switched\n-            # to use .shape and .dtype instead.\n+            # TODO(edloper) Remove _shape and _dtype once Keras has been\n+            # switched to use .shape and .dtype instead.\n             value: tf.Tensor\n             _shape = property(lambda self: self.value.shape)\n             shape = property(lambda self: self.value.shape)\n@@ -1629,8 +1630,8 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n             )\n             # As a side-effect, compute_output_shape builds the layer.\n             self.assertTrue(layer.built)\n-            # We can still query the layer's compute_output_shape with compatible\n-            # input shapes.\n+            # We can still query the layer's compute_output_shape with\n+            # compatible input shapes.\n             self.assertEqual(\n                 layer.compute_output_shape((6, 3)).as_list(), [6, 4]\n             )\n@@ -1802,8 +1803,8 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n         )\n \n         if not tf.executing_eagerly():\n-            # Note: this doesn't work in eager due to DeferredTensor/ops compatibility\n-            # issue.\n+            # Note: this doesn't work in eager due to DeferredTensor/ops\n+            # compatibility issue.\n             mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]\n             mask_outputs += [\n                 model.layers[2].compute_mask(\n@@ -1827,9 +1828,10 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n         outputs = layers.Dense(1)(inputs)\n         model = training_lib.Model(inputs, outputs)\n         config = model.get_config()\n-        # Checks that single inputs and outputs are still saved as 1-element lists.\n-        # Saving as 1-element lists or not is equivalent in TF Keras, but only the\n-        # 1-element list format is supported in TF.js and keras-team/Keras.\n+        # Checks that single inputs and outputs are still saved as 1-element\n+        # lists.  Saving as 1-element lists or not is equivalent in TF Keras,\n+        # but only the 1-element list format is supported in TF.js and\n+        # keras-team/Keras.\n         self.assertLen(config[\"input_layers\"], 1)\n         self.assertLen(config[\"output_layers\"], 1)\n \n@@ -1964,7 +1966,8 @@ class NestedNetworkTest(test_combinations.TestCase):\n         result = self.evaluate(result_tensor)\n         self.assertAllEqual(result, [[2.0]])\n \n-        # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.\n+        # TODO(b/122726584): Investigate why concrete batch is flaky in some\n+        # builds.\n         output_shape = network.compute_output_shape(\n             {\"x1\": (None, 1), \"x2\": (None, 1)}\n         )\n@@ -2343,8 +2346,8 @@ class CacheCorrectnessTest(test_combinations.TestCase):\n \n         network.sub_network = sub_network\n \n-        # Adding to the topology should invalidate the cache and reflect in the top\n-        # level network.\n+        # Adding to the topology should invalidate the cache and reflect in the\n+        # top level network.\n         self.assertEqual(network.dynamic, True)\n         self.assertEqual(layer_0.dynamic_count, 2)\n         self.assertEqual(layer_1.dynamic_count, 1)\n@@ -2355,8 +2358,8 @@ class CacheCorrectnessTest(test_combinations.TestCase):\n         self.assertEqual(layer_0.dynamic_count, 3)\n         self.assertEqual(layer_1.dynamic_count, 2)\n \n-        # Now that we've removed the dynamic layer deep in the layer hierarchy, we\n-        # need to make sure that that bubbles up through all the levels.\n+        # Now that we've removed the dynamic layer deep in the layer hierarchy,\n+        # we need to make sure that that bubbles up through all the levels.\n         sub_network.sub_layers.pop()\n         self.assertEqual(network.dynamic, False)\n         self.assertEqual(layer_0.dynamic_count, 4)\n@@ -2469,7 +2472,8 @@ class CacheCorrectnessTest(test_combinations.TestCase):\n \n         if tf.executing_eagerly():\n             # In v2, construction still works when no `training` is specified\n-            # When no value passed during construction, it uses the local default.\n+            # When no value passed during construction, it uses the local\n+            # default.\n             inputs = input_layer_lib.Input(10)\n             outputs = my_layer(inputs)\n             network = functional.Functional(inputs, outputs)\n@@ -2477,7 +2481,8 @@ class CacheCorrectnessTest(test_combinations.TestCase):\n             self.assertAllEqual(network(x, training=False), _call(x, False))\n             self.assertAllEqual(network(x), _call(x, True))  # Use local default\n \n-        # `None` value passed positionally during construction is ignored at runtime\n+        # `None` value passed positionally during construction is ignored at\n+        # runtime\n         inputs = input_layer_lib.Input(10)\n         outputs = my_layer(inputs, None)\n         network = functional.Functional(inputs, outputs)\n@@ -2486,11 +2491,12 @@ class CacheCorrectnessTest(test_combinations.TestCase):\n         if tf.executing_eagerly():\n             self.assertAllEqual(network(x), _call(x, True))  # Use local default\n         else:\n-            # in v1 training would have defaulted to using the `None` inside the layer\n-            # if training is not passed at runtime\n+            # in v1 training would have defaulted to using the `None` inside the\n+            # layer if training is not passed at runtime\n             self.assertAllEqual(network(x), _call(x, None))\n \n-        # `None` value passed as kwarg during construction is ignored at runtime.\n+        # `None` value passed as kwarg during construction is ignored at\n+        # runtime.\n         inputs = input_layer_lib.Input(10)\n         outputs = my_layer(inputs, training=None)\n         network = functional.Functional(inputs, outputs)\n@@ -2499,8 +2505,8 @@ class CacheCorrectnessTest(test_combinations.TestCase):\n         if tf.executing_eagerly():\n             self.assertAllEqual(network(x), _call(x, True))  # Use local default\n         else:\n-            # in v1 training would have defaulted to using the `None` inside the layer\n-            # if training is not passed at runtime\n+            # in v1 training would have defaulted to using the `None` inside the\n+            # layer if training is not passed at runtime\n             self.assertAllEqual(network(x), _call(x, None))\n \n \n@@ -2613,7 +2619,8 @@ class MultipleInheritanceModelTest(test_combinations.TestCase):\n         self.assertEqual(m.get_foo(), \"123\")\n \n     def testFunctionalSubclassPostMixin(self):\n-        # Make sure the the mixin class is also init correct when the order changed.\n+        # Make sure the the mixin class is also init correct when the order\n+        # changed.\n \n         class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):\n             pass\n\n@@ -68,12 +68,13 @@ def find_nodes_by_inputs_and_outputs(inputs, outputs):\n     \"\"\"\n     # We walk the graph bottom up, starting from output nodes, and keep tracing\n     # the upstream node, until we find all the inputs nodes. We don't use top\n-    # down search here since we don't know whether a certain node is in the graph\n-    # between inputs and outputs, e.g. a functional graph could have multiple\n-    # outputs, and the user could choose a subset of them to build the model.\n-    # The bottom up approach will ensure all the nodes we visit are actually\n-    # in use. If we reach the top and didn't find the nodes in the `inputs`,\n-    # that's an error, since the user didn't specify the correct inputs.\n+    # down search here since we don't know whether a certain node is in the\n+    # graph between inputs and outputs, e.g. a functional graph could have\n+    # multiple outputs, and the user could choose a subset of them to build the\n+    # model. The bottom up approach will ensure all the nodes we visit are\n+    # actually in use. If we reach the top and didn't find the nodes in the\n+    # `inputs`, that's an error, since the user didn't specify the correct\n+    # inputs.\n     start_keras_tensors = tf.nest.flatten(outputs)\n     end_keras_tensors = tf.nest.flatten(inputs)\n \n@@ -106,10 +107,10 @@ def find_nodes_by_inputs_and_outputs(inputs, outputs):\n                 continue\n \n             inbound_node = kt.node\n-            # In case this is the tf.keras.Input node, we have reached the end of the\n-            # tracing of upstream nodes. Any further tracing will just be an\n-            # infinite loop. we should raise an error here since we didn't find the\n-            # input in the user-specified inputs.\n+            # In case this is the tf.keras.Input node, we have reached the end\n+            # of the tracing of upstream nodes. Any further tracing will just be\n+            # an infinite loop. we should raise an error here since we didn't\n+            # find the input in the user-specified inputs.\n             if inbound_node.is_input:\n                 raise ValueError(\n                     \"Found input tensor cannot be reached given provided \"\n@@ -119,7 +120,8 @@ def find_nodes_by_inputs_and_outputs(inputs, outputs):\n                 )\n             nodes_to_visit.append(inbound_node)\n \n-    # Do a final check and make sure we have reached all the user-specified inputs\n+    # Do a final check and make sure we have reached all the user-specified\n+    # inputs\n     if end_ids != end_ids_found:\n         unvisited_inputs = [\n             kt for kt in end_keras_tensors if id(kt) not in end_ids_found\n@@ -135,37 +137,40 @@ def clone_graph_nodes(inputs, outputs):\n     \"\"\"Clone the `Node` between the inputs and output tensors.\n \n     This function is used to create a new functional model from any intermediate\n-    keras tensors. The clone of the nodes mimic the behavior of reconstructing the\n-    functional graph network by re-executing all the __call__ methods. The cloned\n-    nodes will be appended to the layers.\n+    keras tensors. The clone of the nodes mimic the behavior of reconstructing\n+    the functional graph network by re-executing all the __call__ methods. The\n+    cloned nodes will be appended to the layers.\n \n-    Note that a new tf.keras.Inputs will be created for any items in the `inputs`\n+    Note that a new tf.keras.Inputs will be created for any items in the\n+    `inputs`\n \n     Args:\n       inputs: A nested structure of keras_tensors.\n       outputs: A nested structure of keras_tensors.\n \n     Returns:\n-      A pair of inputs and outputs, with cloned keras_tensors. They can be used to\n-      create a new functional model.\n+      A pair of inputs and outputs, with cloned keras_tensors. They can be used\n+      to create a new functional model.\n     \"\"\"\n     nodes_to_clone = find_nodes_by_inputs_and_outputs(inputs, outputs)\n     cloned_inputs = []\n     cloned_outputs = []\n     # We not only need to create copies of Nodes (mimic the calls), also need to\n-    # clone keras_tensors to avoid the override of _keras_history attached on the\n-    # keras_tensor. The following dict is used to track any keras tensor we cloned\n-    # The key is the string ID of the original keras tensor, and value is the\n-    # cloned keras_tensor instance.\n+    # clone keras_tensors to avoid the override of _keras_history attached on\n+    # the keras_tensor. The following dict is used to track any keras tensor we\n+    # cloned The key is the string ID of the original keras tensor, and value is\n+    # the cloned keras_tensor instance.\n     kt_id_mapping = {}\n \n     for kt_input in tf.nest.flatten(inputs):\n         if kt_input.node.is_input:\n-            # For any existing keras_tensor from tf.keras.Input, we leave them as is.\n+            # For any existing keras_tensor from tf.keras.Input, we leave them\n+            # as is.\n             cloned_inputs.append(kt_input)\n             kt_id_mapping[id(kt_input)] = kt_input\n         else:\n-            # We need to create a new tf.keras.Input for any intermediate keras_tensor\n+            # We need to create a new tf.keras.Input for any intermediate\n+            # keras_tensor\n             cpy = _clone_keras_tensor(kt_input)\n             cloned_input = input_layer_module.Input(tensor=cpy)\n             cloned_inputs.append(cloned_input)\n@@ -174,10 +179,10 @@ def clone_graph_nodes(inputs, outputs):\n \n     for kt_output in tf.nest.flatten(outputs):\n         cpy = _clone_keras_tensor(kt_output)\n-        # We reuse the _keras_history here, which contains the old information. It\n-        # is used in the Node constructor to check if the tensor \"is_keras_tensor()\"\n-        # The history will be override by the Node constructor anyway for the\n-        # corresponding layer output anyway.\n+        # We reuse the _keras_history here, which contains the old information.\n+        # It is used in the Node constructor to check if the tensor\n+        # \"is_keras_tensor()\" The history will be override by the Node\n+        # constructor anyway for the corresponding layer output anyway.\n         cpy._keras_history = (\n             kt_output._keras_history\n         )  # pylint: disable=protected-access\n@@ -191,12 +196,11 @@ def clone_graph_nodes(inputs, outputs):\n         output_copy = clone_keras_tensors(node.output_tensors, kt_id_mapping)\n         call_args_copy = clone_keras_tensors(node.call_args, kt_id_mapping)\n         call_kwargs_copy = clone_keras_tensors(node.call_kwargs, kt_id_mapping)\n-        # Creating new nodes based on the existing node information.\n-        # Node wires itself to inbound and outbound layers.\n-        # The Node constructor actually updates this layer's self._inbound_nodes,\n-        # sets _keras_history on the outputs, and adds itself to the\n-        # `_outbound_nodes` of the layers that produced the inputs to this\n-        # layer call.\n+        # Creating new nodes based on the existing node information.  Node wires\n+        # itself to inbound and outbound layers.  The Node constructor actually\n+        # updates this layer's self._inbound_nodes, sets _keras_history on the\n+        # outputs, and adds itself to the `_outbound_nodes` of the layers that\n+        # produced the inputs to this layer call.\n         node_module.Node(\n             node.layer,\n             call_args=call_args_copy,\n@@ -211,9 +215,9 @@ def clone_keras_tensors(args, keras_tensor_mapping):\n \n     For any KerasTensor instance in the `args`, a new copy of KerasTensor will\n     be created if it has not been cloned yet (by checking the\n-    `keras_tensor_mapping`). For any other types, the instance will be unchanged.\n-    This function is useful for cloning the Nodes since KerasTensor can't be\n-    reused across the models.\n+    `keras_tensor_mapping`). For any other types, the instance will be\n+    unchanged. This function is useful for cloning the Nodes since KerasTensor\n+    can't be reused across the models.\n \n     Args:\n       args: A nested structure of objects, which could contain KerasTensor.\n@@ -254,7 +258,7 @@ def _clone_keras_tensor(kt):\n       An identical copy of the input KerasTensor.\n     \"\"\"\n     # Create a scratch graph since we don't intend to use the placeholders.\n-    with backend._scratch_graph() as scratch_graph:  # pylint: disable=protected-access\n+    with backend._scratch_graph() as scratch_graph:\n         with scratch_graph.as_default():\n             placeholder = keras_tensor.keras_tensor_to_placeholder(kt)\n             return keras_tensor.keras_tensor_from_tensor(placeholder)\n\n@@ -157,7 +157,8 @@ class FunctionalModelSlideTest(test_combinations.TestCase):\n         loaded_model = models.load_model(output_path)\n         self.assertEqual(model.summary(), loaded_model.summary())\n \n-        # Also make sure the original inputs and y can still be used to build model\n+        # Also make sure the original inputs and y can still be used to build\n+        # model\n         new_model = models.Model(inputs, y)\n         # Make sure no new node is attached to layer2\n         self.assertLen(layer2.inbound_nodes, 2)\n@@ -195,8 +196,8 @@ class FunctionalModelSlideTest(test_combinations.TestCase):\n         # Make sure we have 8 layers, 3 for inputs, 2 for dense and 3 for Add.\n         # Note that dense1 is still in use by input1.\n         self.assertLen(model.layers, 8)\n-        # Since the layers are not ordered, let's check class of the layers to make\n-        # sure it match the expectation.\n+        # Since the layers are not ordered, let's check class of the layers to\n+        # make sure it match the expectation.\n         class_count = collections.Counter([l.__class__ for l in model.layers])\n         self.assertEqual(class_count[input_layer_lib.InputLayer], 3)\n         self.assertEqual(class_count[layers.Dense], 2)\n\n@@ -47,8 +47,9 @@ class InputLayer(base_layer.Layer):\n     It is generally recommend to use the Keras Functional model via `Input`,\n     (which creates an `InputLayer`) without directly using `InputLayer`.\n \n-    When using `InputLayer` with the Keras Sequential model, it can be skipped by\n-    moving the `input_shape` parameter to the first layer after the `InputLayer`.\n+    When using `InputLayer` with the Keras Sequential model, it can be skipped\n+    by moving the `input_shape` parameter to the first layer after the\n+    `InputLayer`.\n \n     This class can create placeholders for `tf.Tensors`, `tf.SparseTensors`, and\n     `tf.RaggedTensors` by choosing `sparse=True` or `ragged=True`. Note that\n@@ -74,8 +75,8 @@ class InputLayer(base_layer.Layer):\n     ```\n \n     Args:\n-        input_shape: Shape tuple (not including the batch axis), or `TensorShape`\n-          instance (not including the batch axis).\n+        input_shape: Shape tuple (not including the batch axis), or\n+            `TensorShape` instance (not including the batch axis).\n         batch_size: Optional input batch size (integer or `None`).\n         dtype: Optional datatype of the input. When not provided, the Keras\n             default `float` type will be used.\n@@ -89,9 +90,9 @@ class InputLayer(base_layer.Layer):\n             ragged dimensions. For more information about `tf.RaggedTensor`, see\n             [this guide](https://www.tensorflow.org/guide/ragged_tensor).\n             Default to `False`.\n-        type_spec: A `tf.TypeSpec` object to create Input from. This `tf.TypeSpec`\n-            represents the entire batch. When provided, all other args except\n-            name must be `None`.\n+        type_spec: A `tf.TypeSpec` object to create Input from. This\n+            `tf.TypeSpec` represents the entire batch. When provided, all other\n+            args except name must be `None`.\n         name: Optional name of the layer (string).\n     \"\"\"\n \n@@ -139,8 +140,8 @@ class InputLayer(base_layer.Layer):\n                     \"InputLayer, not both at the same time.\"\n                 )\n             # Set the input shape and batch size from the batch_input_shape.\n-            # Note that batch_input_shape can be None (unknown rank) or [] (scalar),\n-            # in which case the batch size must be None.\n+            # Note that batch_input_shape can be None (unknown rank) or []\n+            # (scalar), in which case the batch size must be None.\n             if batch_input_shape:\n                 batch_size = batch_input_shape[0]\n                 input_shape = batch_input_shape[1:]\n@@ -206,7 +207,8 @@ class InputLayer(base_layer.Layer):\n             try:\n                 self._batch_input_shape = tuple(input_tensor.shape.as_list())\n             except ValueError:\n-                # If the shape cannot be represented as a tuple (e.g. unknown rank)\n+                # If the shape cannot be represented as a tuple (e.g. unknown\n+                # rank)\n                 self._batch_input_shape = None\n         elif input_tensor is None:\n             if input_shape is not None:\n@@ -243,7 +245,8 @@ class InputLayer(base_layer.Layer):\n             try:\n                 self._batch_input_shape = tuple(input_tensor.shape.as_list())\n             except ValueError:\n-                # If the shape cannot be represented as a tuple (e.g. unknown rank)\n+                # If the shape cannot be represented as a tuple (e.g. unknown\n+                # rank)\n                 self._batch_input_shape = None\n         # Create an input node.\n         input_tensor._keras_mask = None\n@@ -296,9 +299,9 @@ def Input(  # pylint: disable=invalid-name\n ):\n     \"\"\"`Input()` is used to instantiate a Keras tensor.\n \n-    A Keras tensor is a symbolic tensor-like object,\n-    which we augment with certain attributes that allow us to build a Keras model\n-    just by knowing the inputs and outputs of the model.\n+    A Keras tensor is a symbolic tensor-like object, which we augment with\n+    certain attributes that allow us to build a Keras model just by knowing the\n+    inputs and outputs of the model.\n \n     For instance, if `a`, `b` and `c` are Keras tensors,\n     it becomes possible to do:\n@@ -325,8 +328,8 @@ def Input(  # pylint: disable=invalid-name\n             than creating a new placeholder tensor.\n         ragged: A boolean specifying whether the placeholder to be created is\n             ragged. Only one of 'ragged' and 'sparse' can be True. In this case,\n-            values of 'None' in the 'shape' argument represent ragged dimensions.\n-            For more information about RaggedTensors, see\n+            values of 'None' in the 'shape' argument represent ragged\n+            dimensions.  For more information about RaggedTensors, see\n             [this guide](https://www.tensorflow.org/guide/ragged_tensors).\n         type_spec: A `tf.TypeSpec` object to create the input placeholder from.\n             When provided, all other args except name must be None.\n@@ -363,8 +366,8 @@ def Input(  # pylint: disable=invalid-name\n     used as inputs to TensorFlow ops. All variable usages must happen within\n     Keras layers to make sure they will be tracked by the model's weights.\n \n-    The Keras Input can also create a placeholder from an arbitrary `tf.TypeSpec`,\n-    e.g:\n+    The Keras Input can also create a placeholder from an arbitrary\n+    `tf.TypeSpec`, e.g:\n \n     ```python\n     x = Input(type_spec=tf.RaggedTensorSpec(shape=[None, None],\n@@ -372,21 +375,22 @@ def Input(  # pylint: disable=invalid-name\n     y = x.values\n     model = Model(x, y)\n     ```\n-    When passing an arbitrary `tf.TypeSpec`, it must represent the signature of an\n-    entire batch instead of just one example.\n+    When passing an arbitrary `tf.TypeSpec`, it must represent the signature of\n+    an entire batch instead of just one example.\n \n     Raises:\n       ValueError: If both `sparse` and `ragged` are provided.\n       ValueError: If both `shape` and (`batch_input_shape` or `batch_shape`) are\n         provided.\n       ValueError: If `shape`, `tensor` and `type_spec` are None.\n-      ValueError: If arguments besides `type_spec` are non-None while `type_spec`\n-                  is passed.\n+      ValueError: If arguments besides `type_spec` are non-None while\n+        `type_spec` is passed.\n       ValueError: if any unrecognized parameters are provided.\n     \"\"\"\n     if sparse and ragged:\n         raise ValueError(\n-            \"Cannot set both `sparse` and `ragged` to `True` in a Keras `Input`.\"\n+            \"Cannot set both `sparse` and `ragged` to `True` in a \"\n+            \"Keras `Input`.\"\n         )\n \n     input_layer_config = {\n\n@@ -60,7 +60,8 @@ class InputSpec:\n     class MyLayer(Layer):\n         def __init__(self):\n             super(MyLayer, self).__init__()\n-            # The layer will accept inputs with shape (?, 28, 28) & (?, 28, 28, 1)\n+            # The layer will accept inputs with\n+            # shape (?, 28, 28) & (?, 28, 28, 1)\n             # and raise an appropriate error message otherwise.\n             self.input_spec = InputSpec(\n                 shape=(None, 28, 28, 1),\n@@ -109,9 +110,8 @@ class InputSpec:\n             max_axis = max(self.axes)\n             if max_axis > max_dim:\n                 raise ValueError(\n-                    \"Axis {} is greater than the maximum allowed value: {}\".format(\n-                        max_axis, max_dim\n-                    )\n+                    \"Axis {} is greater than the maximum \"\n+                    \"allowed value: {}\".format(max_axis, max_dim)\n                 )\n \n     def __repr__(self):\n@@ -202,10 +202,10 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n \n     inputs = tf.nest.flatten(inputs)\n     for x in inputs:\n-        # Having a shape/dtype is the only commonality of the various tensor-like\n-        # objects that may be passed. The most common kind of invalid type we are\n-        # guarding for is a Layer instance (Functional API), which does not\n-        # have a `shape` attribute.\n+        # Having a shape/dtype is the only commonality of the various\n+        # tensor-like objects that may be passed. The most common kind of\n+        # invalid type we are guarding for is a Layer instance (Functional API),\n+        # which does not have a `shape` attribute.\n         if not hasattr(x, \"shape\"):\n             raise TypeError(f\"Inputs to a layer should be tensors. Got: {x}\")\n \n@@ -275,7 +275,8 @@ def assert_input_compatibility(input_spec, inputs, layer_name):\n                         f'Input {input_index} of layer \"{layer_name}\" is '\n                         f\"incompatible with the layer: expected axis {axis} \"\n                         f\"of input shape to have value {value}, \"\n-                        f\"but received input with shape {display_shape(x.shape)}\"\n+                        \"but received input with \"\n+                        f\"shape {display_shape(x.shape)}\"\n                     )\n         # Check shape.\n         if spec.shape is not None and shape.rank is not None:\n\n@@ -55,7 +55,8 @@ class KerasTensor:\n       * creating a scratch `FuncGraph`\n       * making placeholders in the scratch graph that match the input typespecs\n       * Calling `layer.call` on these placeholders\n-      * extracting the signatures of the outputs before clearing the scratch graph\n+      * extracting the signatures of the outputs before clearing the scratch\n+        graph\n \n     (Note: names assigned to KerasTensors by this process are not guaranteed to\n     be unique, and are subject to implementation details).\n@@ -64,9 +65,9 @@ class KerasTensor:\n     structures get maintained, with elements swapped between KerasTensors and\n     placeholders.\n \n-    In rare cases (such as when directly manipulating shapes using Keras layers),\n-    the layer may be able to partially infer the value of the output in addition\n-    to just inferring the signature.\n+    In rare cases (such as when directly manipulating shapes using Keras\n+    layers), the layer may be able to partially infer the value of the output in\n+    addition to just inferring the signature.\n     When this happens, the returned KerasTensor will also contain the inferred\n     value information. Follow-on layers can use this information.\n     during their own output signature inference.\n@@ -89,8 +90,8 @@ class KerasTensor:\n     Higher-order APIs that take methods which produce tensors (e.g. `tf.while`,\n     `tf.map_fn`, `tf.cond`) also do not currently support dispatching. So, you\n     cannot directly pass KerasTensors as inputs to these APIs either. If you\n-    want to use these APIs inside of a Functional model, you must put them inside\n-    of a custom layer.\n+    want to use these APIs inside of a Functional model, you must put them\n+    inside of a custom layer.\n \n     Args:\n       type_spec: The `tf.TypeSpec` for the symbolic input created by\n@@ -123,30 +124,32 @@ class KerasTensor:\n         if not isinstance(type_spec, structure.NoneTensorSpec):\n             if not hasattr(type_spec, \"shape\"):\n                 raise ValueError(\n-                    \"KerasTensor only supports TypeSpecs that have a shape field; got \"\n-                    f\"{type(type_spec).__qualname__}, which does not have a shape.\"\n+                    \"KerasTensor only supports TypeSpecs that have a shape \"\n+                    f\"field; got {type(type_spec).__qualname__}, \"\n+                    \"which does not have a shape.\"\n                 )\n             if not isinstance(type_spec.shape, tf.TensorShape):\n                 raise TypeError(\n                     \"KerasTensor requires that wrapped TypeSpec's shape is a \"\n-                    f\"TensorShape; got TypeSpec {type(type_spec).__qualname__}, whose \"\n-                    \"shape field has unexpected type \"\n+                    f\"TensorShape; got TypeSpec {type(type_spec).__qualname__}\"\n+                    \", whose shape field has unexpected type \"\n                     f\"{type(type_spec.dtype).__qualname__}.\"\n                 )\n \n     @property\n     def type_spec(self):\n-        \"\"\"Returns the `tf.TypeSpec` symbolically inferred for this Keras output.\"\"\"\n+        \"\"\"Returns the `tf.TypeSpec` symbolically inferred for Keras output.\"\"\"\n         return self._type_spec\n \n     @property\n     def shape(self):\n-        \"\"\"Returns the `TensorShape` symbolically inferred for this Keras output.\"\"\"\n+        \"\"\"Returns the `TensorShape` symbolically inferred for Keras output.\"\"\"\n         return self._type_spec.shape\n \n     @classmethod\n     def from_tensor(cls, tensor):\n-        \"\"\"Convert a traced (composite)tensor to a representative KerasTensor.\"\"\"\n+        \"\"\"Convert a traced (composite)tensor to a representative\n+        KerasTensor.\"\"\"\n         if isinstance(tensor, tf.Tensor):\n             name = getattr(tensor, \"name\", None)\n             type_spec = tf.type_spec_from_value(tensor)\n@@ -157,23 +160,28 @@ class KerasTensor:\n                 and type_spec.shape.rank < 2\n             ):\n                 # If this tensor might be representing shape information,\n-                # (dtype=int32, rank of 0 or 1, not too large to represent a shape)\n-                # we attempt to capture any value information tensorflow's\n-                # shape handling can extract from the current scratch graph.\n+                # (dtype=int32, rank of 0 or 1, not too large to represent a\n+                # shape) we attempt to capture any value information\n+                # tensorflow's shape handling can extract from the current\n+                # scratch graph.\n                 #\n                 # Even though keras layers each trace in their own scratch\n-                # graph, this shape value info extraction allows us to capture\n-                # a sizable and useful subset of the C++ shape value inference TF can do\n-                # if all tf ops appear in the same graph when using shape ops.\n+                # graph, this shape value info extraction allows us to capture a\n+                # sizable and useful subset of the C++ shape value inference TF\n+                # can do if all tf ops appear in the same graph when using shape\n+                # ops.\n                 #\n                 # Examples of things this cannot infer concrete dimensions for\n-                # that the full single-graph C++ shape inference sometimes can are:\n-                # * cases where the shape tensor is cast out of int32 before being\n-                #   manipulated w/ floating point numbers then converted back\n-                # * cases where int32 tensors w/ rank >= 2 are manipulated before being\n-                #   used as a shape tensor\n+                # that the full single-graph C++ shape inference sometimes can\n+                # are:\n+                # * cases where the shape tensor is cast out of int32 before\n+                #   being manipulated w/ floating point numbers then converted\n+                #   back\n+                # * cases where int32 tensors w/ rank >= 2 are manipulated\n+                #   before being used as a shape tensor\n                 # * cases where int32 tensors too large to represent shapes are\n-                #   manipulated to a smaller size before being used as a shape tensor\n+                #   manipulated to a smaller size before being used as a shape\n+                #   tensor\n                 inferred_value = tf.ones(shape=tensor).shape\n                 if inferred_value.dims:\n                     inferred_value = inferred_value.as_list()\n@@ -197,24 +205,27 @@ class KerasTensor:\n \n     def _to_placeholder(self):\n         \"\"\"Convert this KerasTensor to a placeholder in a graph.\"\"\"\n-        # If there is an inferred value for this tensor, inject the inferred value\n+        # If there is an inferred value for this tensor, inject the inferred\n+        # value\n         if self._inferred_value is not None:\n-            # If we suspect this KerasTensor might be representing a shape tensor,\n-            # and we were able to extract value information with TensorFlow's shape\n-            # handling when making the KerasTensor, we construct the placeholder by\n-            # re-injecting the inferred value information into the graph. We\n-            # do this injection through the shape of a placeholder, because that\n-            # allows us to specify partially-unspecified shape values.\n+            # If we suspect this KerasTensor might be representing a shape\n+            # tensor, and we were able to extract value information with\n+            # TensorFlow's shape handling when making the KerasTensor, we\n+            # construct the placeholder by re-injecting the inferred value\n+            # information into the graph. We do this injection through the shape\n+            # of a placeholder, because that allows us to specify\n+            # partially-unspecified shape values.\n             #\n-            # See the comment on value extraction inside `from_tensor` for more info.\n+            # See the comment on value extraction inside `from_tensor` for more\n+            # info.\n             inferred_value = tf.shape(\n                 tf.compat.v1.placeholder(\n                     shape=self._inferred_value, dtype=tf.int32\n                 )\n             )\n             if self.type_spec.shape.rank == 0:\n-                # `tf.shape` always returns a rank-1, we may need to turn it back to a\n-                # scalar.\n+                # `tf.shape` always returns a rank-1, we may need to turn it\n+                # back to a scalar.\n                 inferred_value = inferred_value[0]\n             return inferred_value\n \n@@ -270,12 +281,12 @@ class KerasTensor:\n \n     def __array__(self, dtype=None):\n         raise TypeError(\n-            f\"You are passing {self}, an intermediate Keras symbolic input/output, \"\n-            \"to a TF API that does not allow registering custom dispatchers, such \"\n-            \"as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. \"\n-            \"Keras Functional model construction only supports \"\n-            \"TF API calls that *do* support dispatching, such as `tf.math.add` or \"\n-            \"`tf.reshape`. \"\n+            f\"You are passing {self}, an intermediate Keras symbolic \"\n+            \"input/output, to a TF API that does not allow registering custom \"\n+            \"dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, \"\n+            \"or `tf.map_fn`. Keras Functional model construction only supports \"\n+            \"TF API calls that *do* support dispatching, such as `tf.math.add` \"\n+            \"or `tf.reshape`. \"\n             \"Other APIs cannot be called directly on symbolic Keras\"\n             \"inputs/outputs. You can work around \"\n             \"this limitation by putting the operation in a custom Keras layer \"\n@@ -288,7 +299,8 @@ class KerasTensor:\n         return True\n \n     def set_shape(self, shape):\n-        \"\"\"Updates the shape of this KerasTensor. Mimics `tf.Tensor.set_shape()`.\"\"\"\n+        \"\"\"Updates the shape of this KerasTensor. Mimics\n+        `tf.Tensor.set_shape()`.\"\"\"\n         if not isinstance(shape, tf.TensorShape):\n             shape = tf.TensorShape(shape)\n         if not self.shape.is_compatible_with(shape):\n@@ -354,9 +366,10 @@ class KerasTensor:\n             )\n         if not isinstance(type_spec.dtype, tf.DType):\n             raise TypeError(\n-                \"KerasTensor requires that wrapped TypeSpec's dtype is a DType; got \"\n-                f\"TypeSpec {type(type_spec).__qualname__}, whose dtype field has \"\n-                f\"unexpected type {type(type_spec.dtype).__qualname__}.\"\n+                \"KerasTensor requires that wrapped TypeSpec's dtype is a \"\n+                f\"DType; got TypeSpec {type(type_spec).__qualname__}, whose \"\n+                \"dtype field has unexpected type \"\n+                f\"{type(type_spec.dtype).__qualname__}.\"\n             )\n         return type_spec.dtype\n \n@@ -365,8 +378,8 @@ class KerasTensor:\n \n         The primary use case for this API is to put KerasTensors in a\n         set/dictionary. We can't put tensors in a set/dictionary as\n-        `tensor.__hash__()` is not available and tensor equality (`==`) is supposed\n-        to produce a tensor representing if the two inputs are equal.\n+        `tensor.__hash__()` is not available and tensor equality (`==`) is\n+        supposed to produce a tensor representing if the two inputs are equal.\n \n         See the documentation of `tf.Tensor.ref()` for more info.\n         \"\"\"\n@@ -376,9 +389,9 @@ class KerasTensor:\n     def node(self):\n         \"\"\"Find the corresponding `Node` that produce this keras_tensor.\n \n-        During functional model construction, Keras will attach `KerasHistory` to\n-        keras tensor to track the connectivity between calls of layers. Return\n-        None if there isn't any KerasHistory attached to this tensor.\n+        During functional model construction, Keras will attach `KerasHistory`\n+        to keras tensor to track the connectivity between calls of layers.\n+        Return None if there isn't any KerasHistory attached to this tensor.\n         \"\"\"\n         if hasattr(self, \"_keras_history\"):\n             layer, node_index, _ = self._keras_history\n@@ -402,7 +415,8 @@ class KerasTensor:\n \n     @property\n     def name(self):\n-        \"\"\"Returns the (non-unique, optional) name of this symbolic Keras value.\"\"\"\n+        \"\"\"Returns the (non-unique, optional) name of this symbolic Keras\n+        value.\"\"\"\n         return self._name\n \n     @classmethod\n@@ -424,7 +438,8 @@ class KerasTensor:\n     ):  # pylint: disable=invalid-name\n         \"\"\"Overload an operator with the same implementation as a base Tensor class.\n \n-        We pull the operator out of the class dynamically to avoid ordering issues.\n+        We pull the operator out of the class dynamically to avoid ordering\n+        issues.\n \n         Args:\n           tensor_class: The (Composite)Tensor to get the method from.\n@@ -605,7 +620,8 @@ class UserRegisteredTypeKerasTensor(KerasTensor):\n \n \n class _KerasTensorIterator:\n-    \"\"\"Iterates over the leading dim of a KerasTensor. Performs 0 error checks.\"\"\"\n+    \"\"\"Iterates over the leading dim of a KerasTensor. Performs 0 error\n+    checks.\"\"\"\n \n     def __init__(self, tensor, dim0):\n         self._tensor = tensor\n@@ -690,9 +706,9 @@ def type_spec_with_shape(spec, shape):\n     \"\"\"Returns a copy of TypeSpec `spec` with its shape set to `shape`.\"\"\"\n     if isinstance(spec, tf.TensorSpec):\n         # pylint: disable=protected-access\n-        # TODO(b/203201161) Figure out why mutation is needed here, and remove it.\n-        # (TensorSpec objects should be immutable; and we should not be modifying\n-        # private fields.)\n+        # TODO(b/203201161) Figure out why mutation is needed here, and remove\n+        # it. (TensorSpec objects should be immutable; and we should not be\n+        # modifying private fields.)\n         shape = tf.TensorShape(shape)\n         spec._shape = shape\n         return spec\n@@ -711,7 +727,8 @@ def type_spec_with_shape(spec, shape):\n         # RaggedTensorSpec, and SparseTensorSpec.\n         return spec.with_shape(shape)\n     else:\n-        # TODO(edloper): Consider moving this check to the KerasTensor constructor.\n+        # TODO(edloper): Consider moving this check to the KerasTensor\n+        # constructor.\n         raise ValueError(\n             \"Keras requires TypeSpec to have a `with_shape` method \"\n             \"that returns a copy of `self` with an updated shape.\"\n\n@@ -105,9 +105,9 @@ class KerasTensorTest(test_combinations.TestCase):\n \n         kt = tf.reshape(kt, shape=(3, 5, 2))\n         expected_str = (\n-            \"KerasTensor(type_spec=TensorSpec(shape=(3, 5, 2), dtype=tf.float32, \"\n-            \"name=None), name='tf.reshape/Reshape:0', description=\\\"created \"\n-            \"by layer 'tf.reshape'\\\")\"\n+            \"KerasTensor(type_spec=TensorSpec(shape=(3, 5, 2), \"\n+            \"dtype=tf.float32, name=None), name='tf.reshape/Reshape:0', \"\n+            \"description=\\\"created by layer 'tf.reshape'\\\")\"\n         )\n         expected_repr = (\n             \"<KerasTensor: shape=(3, 5, 2) dtype=float32 (created \"\n@@ -119,9 +119,9 @@ class KerasTensorTest(test_combinations.TestCase):\n         kts = tf.unstack(kt)\n         for i in range(3):\n             expected_str = (\n-                \"KerasTensor(type_spec=TensorSpec(shape=(5, 2), dtype=tf.float32, \"\n-                \"name=None), name='tf.unstack/unstack:%s', description=\\\"created \"\n-                \"by layer 'tf.unstack'\\\")\" % (i,)\n+                \"KerasTensor(type_spec=TensorSpec(shape=(5, 2), \"\n+                \"dtype=tf.float32, name=None), name='tf.unstack/unstack:%s', \"\n+                \"description=\\\"created by layer 'tf.unstack'\\\")\" % (i,)\n             )\n             expected_repr = (\n                 \"<KerasTensor: shape=(5, 2) dtype=float32 \"\n@@ -194,15 +194,15 @@ class KerasTensorTest(test_combinations.TestCase):\n             kt.set_shape([3, 3])\n \n     def test_set_shape_equals_expected_shape(self):\n-        # Tests b/203201161: DenseSpec has both a _shape and a _shape_tuple field,\n-        # and we need to be sure both get updated.\n+        # Tests b/203201161: DenseSpec has both a _shape and a _shape_tuple\n+        # field, and we need to be sure both get updated.\n         kt = keras_tensor.KerasTensor(tf.TensorSpec([8, None], tf.int32))\n         kt.set_shape([8, 3])\n         self.assertEqual(kt.type_spec, tf.TensorSpec([8, 3], tf.int32))\n \n     def test_type_spec_with_shape_equals_expected_shape(self):\n-        # Tests b/203201161: DenseSpec has both a _shape and a _shape_tuple field,\n-        # and we need to be sure both get updated.\n+        # Tests b/203201161: DenseSpec has both a _shape and a _shape_tuple\n+        # field, and we need to be sure both get updated.\n         spec1 = tf.TensorSpec([8, None], tf.int32)\n         spec2 = keras_tensor.type_spec_with_shape(spec1, [8, 3])\n         expected = tf.TensorSpec([8, 3], tf.int32)\n\n@@ -35,18 +35,19 @@ _COMPOSITE_TYPE = {\"_TYPE\": \"COMPOSITE\"}\n class Node:\n     \"\"\"A `Node` describes a layer `__call__()` event.\n \n-    A Functional model is a DAG with `Node` instances as nodes, and `KerasTensor`\n-    instances as edges. Nodes aren't `Layer` instances, because a single layer\n-    could be called multiple times, which would result in graph cycles.\n+    A Functional model is a DAG with `Node` instances as nodes, and\n+    `KerasTensor` instances as edges. Nodes aren't `Layer` instances, because a\n+    single layer could be called multiple times, which would result in graph\n+    cycles.\n \n     A `__call__()` event involves input tensors (and other input arguments),\n     the layer that was called, and the resulting output tensors.\n     A `Node` will include all this information.\n \n     Since a single `Layer` could be called multiple times, the `Node` instances\n-    are stored on layers as a list. Each time a layer is called\n-    a node is added to `layer._inbound_nodes`. Each time the output of a layer is\n-    used by another layer, a node is added to `layer._outbound_nodes`.\n+    are stored on layers as a list. Each time a layer is called a node is added\n+    to `layer._inbound_nodes`. Each time the output of a layer is used by\n+    another layer, a node is added to `layer._outbound_nodes`.\n \n     Every `KerasTensor` instance has a `KerasHistory` object attached,\n     which tracks the `Node` that records the `__call__()` event that created\n@@ -134,12 +135,14 @@ class Node:\n \n     @property\n     def keras_inputs(self):\n-        \"\"\"Tensors input to this node that can be traced back to a `keras.Input`.\"\"\"\n+        \"\"\"Tensors input to this node that can be traced back to a\n+        `keras.Input`.\"\"\"\n         return self._keras_inputs\n \n     @property\n     def parent_nodes(self):\n-        \"\"\"Returns all the `Node`s whose output this node immediately depends on.\"\"\"\n+        \"\"\"Returns all the `Node`s whose output this node immediately depends\n+        on.\"\"\"\n         node_deps = []\n         for kt in self.keras_inputs:\n             layer = kt._keras_history.layer\n@@ -205,8 +208,8 @@ class Node:\n             if isinstance(t, tf.Tensor):\n                 return backend.get_value(t).tolist()\n \n-            # Not using json_utils to serialize both constant Tensor and constant\n-            # CompositeTensor for saving format backward compatibility.\n+            # Not using json_utils to serialize both constant Tensor and\n+            # constant CompositeTensor for saving format backward compatibility.\n             if isinstance(t, tf.__internal__.CompositeTensor):\n                 return (_COMPOSITE_TYPE, json_utils.Encoder().encode(t))\n \n@@ -237,10 +240,11 @@ class Node:\n                 new_node_index = node_conversion_map.get(node_key, 0)\n                 data = [kh.layer.name, new_node_index, kh.tensor_index, kwargs]\n             else:\n-                # If an element in the first call argument did not originate as a\n-                # keras tensor and is a constant value, we save it using the format\n-                # ['_CONSTANT_VALUE', -1, serialized_tensor_or_python_constant]\n-                # (potentially including serialized kwargs in an optional 4th argument).\n+                # If an element in the first call argument did not originate as\n+                # a keras tensor and is a constant value, we save it using the\n+                # format ['_CONSTANT_VALUE', -1,\n+                # serialized_tensor_or_python_constant] (potentially including\n+                # serialized kwargs in an optional 4th argument).\n                 data = [_CONSTANT_VALUE, -1, _serialize_keras_tensor(t), kwargs]\n             return tf_utils.ListWrapper(data)\n \n@@ -321,10 +325,10 @@ class KerasHistory(\n \n     Attributes:\n       layer: The Layer that produced the Tensor.\n-      node_index: The specific call to the Layer that produced this Tensor. Layers\n-        can be called multiple times in order to share weights. A new node is\n-        created every time a Layer is called. The corresponding node that\n-        represents the call event that produced the Tensor can be found at\n+      node_index: The specific call to the Layer that produced this Tensor.\n+        Layers can be called multiple times in order to share weights. A new\n+        node is created every time a Layer is called. The corresponding node\n+        that represents the call event that produced the Tensor can be found at\n         `layer._inbound_nodes[node_index]`.\n       tensor_index: The output index for this Tensor. Always zero if the Layer\n         that produced this Tensor only has one output. Nested structures of\n\n@@ -59,7 +59,7 @@ class PartialBatchPaddingHandler:\n         return backend.concatenate([padding_mask, mask], axis=0)\n \n     def pad_batch(self, *dataset_batch_elements):\n-        \"\"\"Pads out the batch dimension of a tensor to the complete batch size.\"\"\"\n+        \"\"\"Pads the batch dimension of a tensor to the complete batch size.\"\"\"\n \n         def _pad(batch):\n             \"\"\"Helper function to pad nested data within each batch elements.\"\"\"\n\n@@ -107,7 +107,8 @@ class Sequential(functional.Functional):\n           layers: Optional list of layers to add to the model.\n           name: Optional name for the model.\n         \"\"\"\n-        # Skip the init in FunctionalModel since model doesn't have input/output yet\n+        # Skip the init in FunctionalModel since model doesn't have input/output\n+        # yet\n         super(\n             functional.Functional, self\n         ).__init__(  # pylint: disable=bad-super-call\n@@ -127,10 +128,10 @@ class Sequential(functional.Functional):\n         # have an input shape.\n         self._graph_initialized = False\n \n-        # Unfortunately some Sequential models using custom layers or FeatureColumn\n-        # layers have multiple inputs. This is fundamentally incompatible with\n-        # most of the Sequential API, and we have to disable a number of features\n-        # for such models.\n+        # Unfortunately some Sequential models using custom layers or\n+        # FeatureColumn layers have multiple inputs. This is fundamentally\n+        # incompatible with most of the Sequential API, and we have to disable a\n+        # number of features for such models.\n         self._use_legacy_deferred_behavior = False\n \n         # Add to the model any layers passed to the constructor.\n@@ -168,8 +169,9 @@ class Sequential(functional.Functional):\n                 multiple output tensors, or is already connected\n                 somewhere else (forbidden in `Sequential` models).\n         \"\"\"\n-        # If we are passed a Keras tensor created by keras.Input(), we can extract\n-        # the input layer from its keras history and use that without any loss of\n+        # If we are passed a Keras tensor created by keras.Input(), we can\n+        # extract the input layer from its keras history and use that without\n+        # any loss of\n         # generality.\n         if hasattr(layer, \"_keras_history\"):\n             origin_layer = layer._keras_history[0]\n@@ -189,8 +191,8 @@ class Sequential(functional.Functional):\n         if not self._is_layer_name_unique(layer):\n             raise ValueError(\n                 \"All layers added to a Sequential model \"\n-                f'should have unique names. Name \"{layer.name}\" is already the name '\n-                \"of a layer in this model. Update the `name` argument \"\n+                f'should have unique names. Name \"{layer.name}\" is already '\n+                \"the name of a layer in this model. Update the `name` argument \"\n                 \"to pass a unique name.\"\n             )\n \n@@ -199,7 +201,8 @@ class Sequential(functional.Functional):\n         self._maybe_create_attribute(\"_self_tracked_trackables\", [])\n         if not self._self_tracked_trackables:\n             if isinstance(layer, input_layer.InputLayer):\n-                # Case where the user passes an Input or InputLayer layer via `add`.\n+                # Case where the user passes an Input or InputLayer layer via\n+                # `add`.\n                 set_inputs = True\n             else:\n                 batch_shape, dtype = training_utils.get_input_shape_and_dtype(\n@@ -281,7 +284,8 @@ class Sequential(functional.Functional):\n             not tf.__internal__.tf2.enabled()\n             or not tf.compat.v1.executing_eagerly_outside_functions()\n         ):\n-            # This behavior is disabled in V1 or when eager execution is disabled.\n+            # This behavior is disabled in V1 or when eager execution is\n+            # disabled.\n             return\n         if (\n             not self._has_explicit_input_shape\n@@ -311,33 +315,36 @@ class Sequential(functional.Functional):\n                     layer_input = inputs\n                     created_nodes = set()\n                     for layer in self.layers:\n-                        # Clear nodes previously created via this method. This prevents\n-                        # node accumulation and ensures that e.g. `layer.output` is\n-                        # always connected to `model.inputs`\n-                        # (this is important e.g. for the feature extraction use case).\n-                        # We don't just do `layer._inbound_nodes = []` in order\n-                        # not to break shared layers added to Sequential models (which is\n-                        # technically illegal as per the `add()` docstring,\n-                        # but wasn't previously disabled).\n+                        # Clear nodes previously created via this method. This\n+                        # prevents node accumulation and ensures that e.g.\n+                        # `layer.output` is always connected to `model.inputs`\n+                        # (this is important e.g. for the feature extraction use\n+                        # case).  We don't just do `layer._inbound_nodes = []`\n+                        # in order not to break shared layers added to\n+                        # Sequential models (which is technically illegal as per\n+                        # the `add()` docstring, but wasn't previously\n+                        # disabled).\n                         clear_previously_created_nodes(\n                             layer, self._created_nodes\n                         )\n                         try:\n-                            # Create Functional API connection by calling the current layer\n+                            # Create Functional API connection by calling the\n+                            # current layer\n                             layer_output = layer(layer_input)\n                         except:  # pylint:disable=bare-except\n-                            # Functional API calls may fail for a number of reasons:\n-                            # 1) The layer may be buggy. In this case it will be easier for\n-                            # the user to debug if we fail on the first call on concrete data,\n-                            # instead of our own call on a symbolic input.\n-                            # 2) The layer is dynamic (graph-incompatible) and hasn't\n-                            # overridden `compute_output_shape`. In this case, it is\n-                            # impossible to build a graph network.\n-                            # 3) The layer is otherwise incompatible with the Functional API\n-                            # (e.g. this is the case for some probabilistic layers that rely\n-                            # on hacks and that do not return tensors).\n-                            # In all these cases, we should avoid creating a graph network\n-                            # (or we simply can't).\n+                            # Functional API calls may fail for a number of\n+                            # reasons: 1) The layer may be buggy. In this case\n+                            # it will be easier for the user to debug if we fail\n+                            # on the first call on concrete data, instead of our\n+                            # own call on a symbolic input. 2) The layer is\n+                            # dynamic (graph-incompatible) and hasn't overridden\n+                            # `compute_output_shape`. In this case, it is\n+                            # impossible to build a graph network. 3) The layer\n+                            # is otherwise incompatible with the Functional API\n+                            # (e.g. this is the case for some probabilistic\n+                            # layers that rely on hacks and that do not return\n+                            # tensors). In all these cases, we should avoid\n+                            # creating a graph network (or we simply can't).\n                             self._use_legacy_deferred_behavior = True\n                             return\n                         if len(tf.nest.flatten(layer_output)) != 1:\n@@ -348,13 +355,14 @@ class Sequential(functional.Functional):\n                         outputs = layer_output\n                     self._created_nodes = created_nodes\n                     try:\n-                        # Initialize a graph Network. This call will never fail for\n-                        # a stack of valid Keras layers.\n-                        # However some users have layers that are fundamentally incompatible\n-                        # with the Functional API, which do not return tensors. In this\n-                        # case, we fall back to the legacy deferred behavior.\n-                        # TODO(fchollet): consider raising here, as we should not be\n-                        # supporting such layers.\n+                        # Initialize a graph Network. This call will never fail\n+                        # for a stack of valid Keras layers. However some users\n+                        # have layers that are fundamentally incompatible with\n+                        # the Functional API, which do not return tensors. In\n+                        # this case, we fall back to the legacy deferred\n+                        # behavior.\n+                        # TODO(fchollet): consider raising here, as we should\n+                        # not be supporting such layers.\n                         self._init_graph_network(inputs, outputs)\n                         self._graph_initialized = True\n                     except:  # pylint:disable=bare-except\n@@ -381,9 +389,9 @@ class Sequential(functional.Functional):\n         # If applicable, update the static input shape of the model.\n         if not self._has_explicit_input_shape:\n             if not tf.is_tensor(inputs) and not isinstance(inputs, tf.Tensor):\n-                # This is a Sequential with multiple inputs. This is technically an\n-                # invalid use case of Sequential, but we tolerate it for backwards\n-                # compatibility.\n+                # This is a Sequential with multiple inputs. This is technically\n+                # an invalid use case of Sequential, but we tolerate it for\n+                # backwards compatibility.\n                 self._use_legacy_deferred_behavior = True\n                 self._build_input_shape = tf.nest.map_structure(\n                     _get_shape_tuple, inputs\n@@ -407,9 +415,10 @@ class Sequential(functional.Functional):\n \n         outputs = inputs  # handle the corner case where self.layers is empty\n         for layer in self.layers:\n-            # During each iteration, `inputs` are the inputs to `layer`, and `outputs`\n-            # are the outputs of `layer` applied to `inputs`. At the end of each\n-            # iteration `inputs` is set to `outputs` to prepare for the next layer.\n+            # During each iteration, `inputs` are the inputs to `layer`, and\n+            # `outputs` are the outputs of `layer` applied to `inputs`. At the\n+            # end of each iteration `inputs` is set to `outputs` to prepare for\n+            # the next layer.\n             kwargs = {}\n             argspec = self._layer_call_argspecs[layer].args\n             if \"mask\" in argspec:\n@@ -444,9 +453,10 @@ class Sequential(functional.Functional):\n     def get_config(self):\n         layer_configs = []\n         for layer in super().layers:\n-            # `super().layers` include the InputLayer if available (it is filtered out\n-            # of `self.layers`). Note that `self._self_tracked_trackables` is managed\n-            # by the tracking infrastructure and should not be used.\n+            # `super().layers` include the InputLayer if available (it is\n+            # filtered out of `self.layers`). Note that\n+            # `self._self_tracked_trackables` is managed by the tracking\n+            # infrastructure and should not be used.\n             layer_configs.append(generic_utils.serialize_keras_object(layer))\n         config = {\"name\": self.name, \"layers\": copy.deepcopy(layer_configs)}\n         if not self._is_graph_network and self._build_input_shape is not None:\n@@ -502,8 +512,8 @@ class Sequential(functional.Functional):\n     def _assert_weights_created(self):\n         if self._graph_initialized:\n             return\n-        # When the graph has not been initialized, use the Model's implementation to\n-        # to check if the weights has been created.\n+        # When the graph has not been initialized, use the Model's\n+        # implementation to to check if the weights has been created.\n         super(\n             functional.Functional, self\n         )._assert_weights_created()  # pylint: disable=bad-super-call\n\n@@ -547,7 +547,8 @@ class TestSequential(test_combinations.TestCase):\n class TestSequentialEagerIntegration(test_combinations.TestCase):\n     @test_combinations.run_all_keras_modes\n     def test_defun_on_call(self):\n-        # Check that one can subclass Sequential and place the `call` in a `defun`.\n+        # Check that one can subclass Sequential and place the `call` in a\n+        # `defun`.\n \n         class MySequential(keras.Sequential):\n             def __init__(self, name=None):\n\n@@ -204,8 +204,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         base_layer.keras_api_gauge.get_cell(\"model\").set(True)\n \n         # Special case for Subclassed Functional Model, which we couldn't detect\n-        # when __new__ is called. We only realize it is a functional model when it\n-        # calls super.__init__ with input and output tensor.\n+        # when __new__ is called. We only realize it is a functional model when\n+        # it calls super.__init__ with input and output tensor.\n         from keras.engine import (\n             functional,\n         )  # pylint: disable=g-import-not-at-top\n@@ -230,8 +230,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             inject_functional_model_class(self.__class__)\n             functional.Functional.__init__(self, *args, **model_kwargs)\n \n-            # In case there is any multiple inheritance here, we need to call the\n-            # __init__ for any class that appears after the Functional class.\n+            # In case there is any multiple inheritance here, we need to call\n+            # the __init__ for any class that appears after the Functional\n+            # class.\n             clz_to_init = []\n             found_functional_class = False\n             for clz in self.__class__.__bases__:\n@@ -245,8 +246,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 for clz in clz_to_init:\n                     clz.__init__(self, *args, **other_kwargs)\n             elif other_kwargs:\n-                # In case there are unused kwargs, we should raise an error to user, in\n-                # case they have a typo in the param name.\n+                # In case there are unused kwargs, we should raise an error to\n+                # user, in case they have a typo in the param name.\n                 raise TypeError(\n                     \"The following keyword arguments passed to `Model` aren't \"\n                     \"supported: {}.\".format(other_kwargs)\n@@ -257,7 +258,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # The following are implemented as property functions:\n         # self.trainable_weights\n         # self.non_trainable_weights\n-        # `inputs` / `outputs` will only appear in kwargs if either are misspelled.\n+        # `inputs` / `outputs` will only appear in kwargs if either are\n+        # misspelled.\n         generic_utils.validate_kwargs(\n             kwargs,\n             {\n@@ -297,7 +299,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._maybe_create_attribute(\"_is_compiled\", False)\n         self._maybe_create_attribute(\"optimizer\", None)\n \n-        # Model must be created under scope of DistStrat it will be trained with.\n+        # Model must be created under scope of DistStrat it will be trained\n+        # with.\n         if tf.distribute.has_strategy():\n             self._distribution_strategy = tf.distribute.get_strategy()\n         else:\n@@ -321,9 +324,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._init_batch_counters()\n         self._base_model_initialized = True\n \n-        # `jit_compile` starts off with None as default and gets overwritten by the\n-        # value specified in `Model.compile`, and this is effective for `fit`,\n-        # `evaluate`, and `predict`.\n+        # `jit_compile` starts off with None as default and gets overwritten by\n+        # the value specified in `Model.compile`, and this is effective for\n+        # `fit`, `evaluate`, and `predict`.\n         self._jit_compile = None\n \n         self._layout_map = layout_map_lib.get_current_layout_map()\n@@ -368,10 +371,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # SavedModel (and hence serialize_model_as_bytecode) only support\n             # built models, but if the model is not built,\n             # it may be possible to serialize as a plain Python object,\n-            # as long as the constituent parts (layers, optimizers, losses, etc.)\n-            # can be serialized as plain Python objects.\n-            # Thus we call up the superclass hierarchy to get an implementation of\n-            # __reduce__ that can pickle this Model as a plain Python object.\n+            # as long as the constituent parts (layers, optimizers, losses,\n+            # etc.) can be serialized as plain Python objects.  Thus we call up\n+            # the superclass hierarchy to get an implementation of __reduce__\n+            # that can pickle this Model as a plain Python object.\n             return super().__reduce__()\n \n     def __deepcopy__(self, memo):\n@@ -397,8 +400,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def build(self, input_shape):\n         \"\"\"Builds the model based on input shapes received.\n \n-        This is to be used for subclassed models, which do not know at instantiation\n-        time what their inputs look like.\n+        This is to be used for subclassed models, which do not know at\n+        instantiation time what their inputs look like.\n \n         This method only exists for users who want to call `model.build()` in a\n         standalone way (as a substitute for calling the model on real data to\n@@ -406,20 +409,22 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         never throw unexpected errors in an unrelated workflow).\n \n         Args:\n-         input_shape: Single tuple, `TensorShape` instance, or list/dict of shapes,\n-           where shapes are tuples, integers, or `TensorShape` instances.\n+         input_shape: Single tuple, `TensorShape` instance, or list/dict of\n+           shapes, where shapes are tuples, integers, or `TensorShape`\n+           instances.\n \n         Raises:\n           ValueError:\n             1. In case of invalid user-provided data (not of type tuple,\n                list, `TensorShape`, or dict).\n             2. If the model requires call arguments that are agnostic\n-               to the input shapes (positional or keyword arg in call signature).\n+               to the input shapes (positional or keyword arg in call\n+               signature).\n             3. If not all layers were properly built.\n             4. If float type inputs are not supported within the layers.\n \n-          In each of these cases, the user should build their model by calling it\n-          on real tensor data.\n+          In each of these cases, the user should build their model by calling\n+          it on real tensor data.\n         \"\"\"\n         if self._is_graph_network:\n             super().build(input_shape)\n@@ -440,10 +445,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             )\n \n         if input_shape and not self.inputs:\n-            # We create placeholders for the `None`s in the shape and build the model\n-            # in a Graph. Since tf.Variable is compatible with both eager execution\n-            # and graph building, the variables created after building the model in\n-            # a Graph are still valid when executing eagerly.\n+            # We create placeholders for the `None`s in the shape and build the\n+            # model in a Graph. Since tf.Variable is compatible with both eager\n+            # execution and graph building, the variables created after building\n+            # the model in a Graph are still valid when executing eagerly.\n             if tf.executing_eagerly():\n                 graph = tf.__internal__.FuncGraph(\"build_graph\")\n             else:\n@@ -473,7 +478,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 kwargs = {}\n                 call_signature = self._call_spec.full_argspec\n                 call_args = call_signature.args\n-                # Exclude `self`, `inputs`, and any argument with a default value.\n+                # Exclude `self`, `inputs`, and any argument with a default\n+                # value.\n                 if len(call_args) > 2:\n                     if call_signature.defaults:\n                         call_args = call_args[2 : -len(call_signature.defaults)]\n@@ -481,25 +487,28 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                         call_args = call_args[2:]\n                     for arg in call_args:\n                         if arg == \"training\":\n-                            # Case where `training` is a positional arg with no default.\n+                            # Case where `training` is a positional arg with no\n+                            # default.\n                             kwargs[\"training\"] = False\n                         else:\n-                            # Has invalid call signature with unknown positional arguments.\n+                            # Has invalid call signature with unknown positional\n+                            # arguments.\n                             raise ValueError(\n-                                \"Currently, you cannot build your model if it has \"\n-                                \"positional or keyword arguments that are not \"\n-                                \"inputs to the model, but are required for its \"\n-                                \"`call()` method. Instead, in order to instantiate \"\n-                                \"and build your model, `call()` your model on real \"\n-                                \"tensor data with all expected call arguments. The argument \"\n-                                \"for `call()` can be a single list/tuple that contains \"\n-                                \"multiple inputs.\"\n+                                \"Currently, you cannot build your model if it \"\n+                                \"has positional or keyword arguments that are \"\n+                                \"not inputs to the model, but are required for \"\n+                                \"its `call()` method. Instead, in order to \"\n+                                \"instantiate and build your model, `call()` \"\n+                                \"your model on real tensor data with all \"\n+                                \"expected call arguments. The argument \"\n+                                \"for `call()` can be a single list/tuple that \"\n+                                \"contains multiple inputs.\"\n                             )\n                 elif len(call_args) < 2:\n                     # Signature without `inputs`.\n                     raise ValueError(\n-                        \"You can only call `build()` on a model if its `call()` \"\n-                        \"method accepts an `inputs` argument.\"\n+                        \"You can only call `build()` on a model if its \"\n+                        \"`call()` method accepts an `inputs` argument.\"\n                     )\n                 try:\n                     self.call(x, **kwargs)\n@@ -568,10 +577,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         Args:\n             inputs: Input tensor, or dict/list/tuple of input tensors.\n-            training: Boolean or boolean scalar tensor, indicating whether to run\n-              the `Network` in training mode or inference mode.\n-            mask: A mask or list of masks. A mask can be either a boolean tensor or\n-              None (no mask). For more details, check the guide\n+            training: Boolean or boolean scalar tensor, indicating whether to\n+              run the `Network` in training mode or inference mode.\n+            mask: A mask or list of masks. A mask can be either a boolean tensor\n+              or None (no mask). For more details, check the guide\n               [here](https://www.tensorflow.org/guide/keras/masking_and_padding).\n \n         Returns:\n@@ -621,28 +630,30 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               `y_true` should have shape\n               `(batch_size, d0, .. dN)` (except in the case of\n               sparse loss functions such as\n-              sparse categorical crossentropy which expects integer arrays of shape\n-              `(batch_size, d0, .. dN-1)`).\n+              sparse categorical crossentropy which expects integer arrays of\n+              shape `(batch_size, d0, .. dN-1)`).\n               `y_pred` should have shape `(batch_size, d0, .. dN)`.\n               The loss function should return a float tensor.\n               If a custom `Loss` instance is\n               used and reduction is set to `None`, return value has shape\n               `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n-              values; otherwise, it is a scalar. If the model has multiple outputs,\n-              you can use a different loss on each output by passing a dictionary\n-              or a list of losses. The loss value that will be minimized by the\n-              model will then be the sum of all individual losses, unless\n-              `loss_weights` is specified.\n-            metrics: List of metrics to be evaluated by the model during training\n-              and testing. Each of this can be a string (name of a built-in\n-              function), function or a `tf.keras.metrics.Metric` instance. See\n-              `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n-              function is any callable with the signature `result = fn(y_true,\n+              values; otherwise, it is a scalar. If the model has multiple\n+              outputs, you can use a different loss on each output by passing a\n+              dictionary or a list of losses. The loss value that will be\n+              minimized by the model will then be the sum of all individual\n+              losses, unless `loss_weights` is specified.\n+            metrics: List of metrics to be evaluated by the model during\n+              training and testing. Each of this can be a string (name of a\n+              built-in function), function or a `tf.keras.metrics.Metric`\n+              instance. See `tf.keras.metrics`. Typically you will use\n+              `metrics=['accuracy']`.\n+              A function is any callable with the signature `result = fn(y_true,\n               y_pred)`. To specify different metrics for different outputs of a\n               multi-output model, you could also pass a dictionary, such as\n               `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n               You can also pass a list to specify a metric or a list of metrics\n-              for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]`\n+              for each output, such as\n+              `metrics=[['accuracy'], ['accuracy', 'mse']]`\n               or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n               strings 'accuracy' or 'acc', we convert this to one of\n               `tf.keras.metrics.BinaryAccuracy`,\n@@ -650,17 +661,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n               function used and the model output shape. We do a similar\n               conversion for the strings 'crossentropy' and 'ce' as well.\n-              The metrics passed here are evaluated without sample weighting; if you\n-              would like sample weighting to apply, you can specify your\n+              The metrics passed here are evaluated without sample weighting; if\n+              you would like sample weighting to apply, you can specify your\n               metrics via the `weighted_metrics` argument instead.\n-            loss_weights: Optional list or dictionary specifying scalar coefficients\n-              (Python floats) to weight the loss contributions of different model\n-              outputs. The loss value that will be minimized by the model will then\n-              be the *weighted sum* of all individual losses, weighted by the\n-              `loss_weights` coefficients.\n-                If a list, it is expected to have a 1:1 mapping to the model's\n-                  outputs. If a dict, it is expected to map output names (strings)\n-                  to scalar coefficients.\n+            loss_weights: Optional list or dictionary specifying scalar\n+              coefficients (Python floats) to weight the loss contributions of\n+              different model outputs. The loss value that will be minimized by\n+              the model will then be the *weighted sum* of all individual\n+              losses, weighted by the `loss_weights` coefficients.  If a list,\n+              it is expected to have a 1:1 mapping to the model's outputs. If a\n+              dict, it is expected to map output names (strings) to scalar\n+              coefficients.\n             weighted_metrics: List of metrics to be evaluated and weighted by\n               `sample_weight` or `class_weight` during training and testing.\n             run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n@@ -668,19 +679,19 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               this as `None` unless your `Model` cannot be run inside a\n               `tf.function`. `run_eagerly=True` is not supported when using\n               `tf.distribute.experimental.ParameterServerStrategy`.\n-            steps_per_execution: Int. Defaults to 1. The number of batches to run\n-              during each `tf.function` call. Running multiple batches inside a\n-              single `tf.function` call can greatly improve performance on TPUs or\n-              small models with a large Python overhead. At most, one full epoch\n-              will be run each execution. If a number larger than the size of the\n-              epoch is passed, the execution will be truncated to the size of the\n-              epoch. Note that if `steps_per_execution` is set to `N`,\n-              `Callback.on_batch_begin` and `Callback.on_batch_end` methods will\n-              only be called every `N` batches (i.e. before/after each `tf.function`\n-              execution).\n+            steps_per_execution: Int. Defaults to 1. The number of batches to\n+              run during each `tf.function` call. Running multiple batches\n+              inside a single `tf.function` call can greatly improve performance\n+              on TPUs or small models with a large Python overhead. At most, one\n+              full epoch will be run each execution. If a number larger than the\n+              size of the epoch is passed, the execution will be truncated to\n+              the size of the epoch. Note that if `steps_per_execution` is set\n+              to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n+              methods will only be called every `N` batches (i.e. before/after\n+              each `tf.function` execution).\n             jit_compile: If `True`, compile the model training step with XLA.\n-              [XLA](https://www.tensorflow.org/xla) is an optimizing compiler for\n-              machine learning.\n+              [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n+              for machine learning.\n               `jit_compile` is not enabled for by default.\n               This option cannot be enabled with `run_eagerly=True`.\n               Note that `jit_compile=True`\n@@ -688,8 +699,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               For more information on supported operations please refer to the\n               [XLA documentation](https://www.tensorflow.org/xla).\n               Also refer to\n-              [known XLA issues](https://www.tensorflow.org/xla/known_issues) for\n-              more details.\n+              [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n+              for more details.\n             **kwargs: Arguments supported for backwards compatibility only.\n         \"\"\"\n         base_layer.keras_api_gauge.get_cell(\"compile\").set(True)\n@@ -706,8 +717,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     )\n \n             # When compiling from an already-serialized model, we do not want to\n-            # reapply some processing steps (e.g. metric renaming for multi-output\n-            # models, which have prefixes added for each corresponding output name).\n+            # reapply some processing steps (e.g. metric renaming for\n+            # multi-output models, which have prefixes added for each\n+            # corresponding output name).\n             from_serialized = kwargs.pop(\"from_serialized\", False)\n \n             self._validate_compile(optimizer, metrics, **kwargs)\n@@ -749,8 +761,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             if self.dtype_policy.name == \"mixed_float16\" and not isinstance(\n                 opt, lso.LossScaleOptimizer\n             ):\n-                # Loss scaling is necessary with mixed_float16 for models to converge to\n-                # the same accuracy as with float32.\n+                # Loss scaling is necessary with mixed_float16 for models to\n+                # converge to the same accuracy as with float32.\n                 opt = lso.LossScaleOptimizer(opt)\n             return opt\n \n@@ -763,8 +775,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self.predict_function = None\n         # Used to cache the `tf.function`'ed `train_function` to be logged in\n         # TensorBoard, since the original `train_function` is not necessarily\n-        # a `tf.function` (e.g., with ParameterServerStrategy, the `train_function`\n-        # is a scheduling of the actual training function to a remote worker).\n+        # a `tf.function` (e.g., with ParameterServerStrategy, the\n+        # `train_function` is a scheduling of the actual training function to a\n+        # remote worker).\n         self.train_tf_function = None\n \n         # Used to cache `trainable` attr of `Layer`s for `fit`.\n@@ -786,8 +799,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def metrics(self):\n         \"\"\"Returns the model's metrics added using `compile()`, `add_metric()` APIs.\n \n-        Note: Metrics passed to `compile()` are available only after a `keras.Model`\n-        has been trained/evaluated on actual data.\n+        Note: Metrics passed to `compile()` are available only after a\n+        `keras.Model` has been trained/evaluated on actual data.\n \n         Examples:\n \n@@ -821,8 +834,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"\n         metrics = []\n         if self._is_compiled:\n-            # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer` objects\n-            # so that attr names are not load-bearing.\n+            # TODO(omalleyt): Track `LossesContainer` and `MetricsContainer`\n+            # objects so that attr names are not load-bearing.\n             if self.compiled_loss is not None:\n                 metrics += self.compiled_loss.metrics\n             if self.compiled_metrics is not None:\n@@ -868,8 +881,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         \"\"\"\n \n-        # This property includes all output names including `loss` and per-output\n-        # losses for backward compatibility.\n+        # This property includes all output names including `loss` and\n+        # per-output losses for backward compatibility.\n         return [m.name for m in self.metrics]\n \n     @property\n@@ -882,8 +895,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Settable attribute indicating whether the model should run eagerly.\n \n         Running eagerly means that your model will be run step by step,\n-        like Python code. Your model might run slower, but it should become easier\n-        for you to debug it by stepping into individual layer calls.\n+        like Python code. Your model might run slower, but it should become\n+        easier for you to debug it by stepping into individual layer calls.\n \n         By default, we will attempt to compile your model to a static graph to\n         deliver the best execution performance.\n@@ -911,7 +924,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # Run eagerly logic, by priority:\n         # (1) Dynamic models must be run eagerly.\n         # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.\n-        # (3) Not explicitly setting run_eagerly defaults to TF's global setting.\n+        # (3) Not explicitly setting run_eagerly defaults to TF's global\n+        # setting.\n         return (\n             self.dynamic\n             or self._run_eagerly\n@@ -934,10 +948,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             `add_loss`.\n         \"\"\"\n \n-        # `self.loss` references the loss added via `compile` call. If users have\n-        # provided such, the target must be provided; otherwise it's a user error.\n-        # Note that `self.loss` does not include losses added via `add_loss`, and it\n-        # is a valid use when such loss from `add_loss` exists and target does not.\n+        # `self.loss` references the loss added via `compile` call. If users\n+        # have provided such, the target must be provided; otherwise it's a user\n+        # error.  Note that `self.loss` does not include losses added via\n+        # `add_loss`, and it is a valid use when such loss from `add_loss`\n+        # exists and target does not.\n         if self.loss and y is None:\n             raise ValueError(\n                 \"Target data is missing. Your model was compiled with \"\n@@ -945,13 +960,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 \"and therefore expects target data to be provided in `fit()`.\"\n             )\n \n-        # For training, there must be compiled loss or regularization loss to exist\n-        # in order to apply the gradients. If one is not found, it means no loss\n-        # was supplied via `compile` or `add_loss`.\n+        # For training, there must be compiled loss or regularization loss to\n+        # exist in order to apply the gradients. If one is not found, it means\n+        # no loss was supplied via `compile` or `add_loss`.\n         elif loss is None:\n             raise ValueError(\n-                \"No loss found. You may have forgotten to provide a `loss` argument \"\n-                \"in the `compile()` method.\"\n+                \"No loss found. You may have forgotten to provide a `loss` \"\n+                \"argument in the `compile()` method.\"\n             )\n \n     def train_step(self, data):\n@@ -959,15 +974,16 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         This method can be overridden to support custom training logic.\n         For concrete examples of how to override this method see\n-        [Customizing what happends in fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n+        [Customizing what happends in fit](\n+        https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n         This method is called by `Model.make_train_function`.\n \n-        This method should contain the mathematical logic for one step of training.\n-        This typically includes the forward pass, loss calculation, backpropagation,\n-        and metric updates.\n+        This method should contain the mathematical logic for one step of\n+        training.  This typically includes the forward pass, loss calculation,\n+        backpropagation, and metric updates.\n \n-        Configuration details for *how* this logic is run (e.g. `tf.function` and\n-        `tf.distribute.Strategy` settings), should be left to\n+        Configuration details for *how* this logic is run (e.g. `tf.function`\n+        and `tf.distribute.Strategy` settings), should be left to\n         `Model.make_train_function`, which can also be overridden.\n \n         Args:\n@@ -1037,8 +1053,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           sample_weight: Sample weights for weighting the loss function.\n \n         Returns:\n-          The total loss as a `tf.Tensor`, or `None` if no loss results (which is\n-          the case when called by `Model.test_step`).\n+          The total loss as a `tf.Tensor`, or `None` if no loss results (which\n+          is the case when called by `Model.test_step`).\n         \"\"\"\n         del x  # The default implementation does not use `x`.\n         return self.compiled_loss(\n@@ -1057,8 +1073,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n           def compute_metrics(self, x, y, y_pred, sample_weight):\n \n-            # This super call updates `self.compiled_metrics` and returns results\n-            # for all metrics listed in `self.metrics`.\n+            # This super call updates `self.compiled_metrics` and returns\n+            # results for all metrics listed in `self.metrics`.\n             metric_results = super(MyModel, self).compute_metrics(\n                 x, y, y_pred, sample_weight)\n \n@@ -1160,15 +1176,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 self.train_tf_function = train_function\n \n             if self._cluster_coordinator:\n-                self.train_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+                self.train_function = (\n+                    lambda it: self._cluster_coordinator.schedule(\n                         train_function, args=(it,)\n                     )\n+                )\n             else:\n                 self.train_function = train_function\n \n-        # If we're using a coordinator, use the value of self._steps_per_execution\n-        # at the time the function is called/scheduled, and not when it is actually\n-        # executed.\n+        # If we're using a coordinator, use the value of\n+        # self._steps_per_execution at the time the function is\n+        # called/scheduled, and not when it is actually executed.\n         elif self._cluster_coordinator:\n \n             def train_function(iterator, steps_per_execution):\n@@ -1183,7 +1201,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 )\n                 self.train_tf_function = train_function\n \n-            self.train_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+            self.train_function = lambda it: self._cluster_coordinator.schedule(\n                 train_function, args=(it, self._steps_per_execution.value())\n             )\n         else:\n@@ -1239,8 +1257,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               - A `tf.data` dataset. Should return a tuple\n                 of either `(inputs, targets)` or\n                 `(inputs, targets, sample_weights)`.\n-              - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n-                or `(inputs, targets, sample_weights)`.\n+              - A generator or `keras.utils.Sequence` returning `(inputs,\n+                targets)` or `(inputs, targets, sample_weights)`.\n               - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n                 callable that takes a single argument of type\n                 `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n@@ -1248,11 +1266,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 per-replica batching and sharding logic for the `Dataset`.\n                 See `tf.keras.utils.experimental.DatasetCreator` doc for more\n                 information.\n-              A more detailed description of unpacking behavior for iterator types\n-              (Dataset, generator, Sequence) is given below. If these include\n-              `sample_weights` as a third component, note that sample weighting\n-              applies to the `weighted_metrics` argument but not the `metrics`\n-              argument in `compile()`. If using\n+              A more detailed description of unpacking behavior for iterator\n+              types (Dataset, generator, Sequence) is given below. If these\n+              include `sample_weights` as a third component, note that sample\n+              weighting applies to the `weighted_metrics` argument but not the\n+              `metrics` argument in `compile()`. If using\n               `tf.distribute.experimental.ParameterServerStrategy`, only\n               `DatasetCreator` type is supported for `x`.\n             y: Target data. Like the input data `x`,\n@@ -1265,8 +1283,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 Number of samples per gradient update.\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n-                form of datasets, generators, or `keras.utils.Sequence` instances\n-                (since they generate batches).\n+                form of datasets, generators, or `keras.utils.Sequence`\n+                instances (since they generate batches).\n             epochs: Integer. Number of epochs to train the model.\n                 An epoch is an iteration over the entire `x` and `y`\n                 data provided\n@@ -1286,15 +1304,16 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 environment).\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n-                See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n-                and `tf.keras.callbacks.History` callbacks are created automatically\n+                See `tf.keras.callbacks`. Note\n+                `tf.keras.callbacks.ProgbarLogger` and\n+                `tf.keras.callbacks.History` callbacks are created automatically\n                 and need not be passed into `model.fit`.\n                 `tf.keras.callbacks.ProgbarLogger` is created or not based on\n                 `verbose` argument to `model.fit`.\n                 Callbacks with batch-level calls are currently unsupported with\n-                `tf.distribute.experimental.ParameterServerStrategy`, and users are\n-                advised to implement epoch-level calls instead with an appropriate\n-                `steps_per_epoch` value.\n+                `tf.distribute.experimental.ParameterServerStrategy`, and users\n+                are advised to implement epoch-level calls instead with an\n+                appropriate `steps_per_epoch` value.\n             validation_split: Float between 0 and 1.\n                 Fraction of the training data to be used as validation data.\n                 The model will set apart this fraction of the training data,\n@@ -1302,8 +1321,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 the loss and any model metrics\n                 on this data at the end of each epoch.\n                 The validation data is selected from the last samples\n-                in the `x` and `y` data provided, before shuffling. This argument is\n-                not supported when `x` is a dataset, generator or\n+                in the `x` and `y` data provided, before shuffling. This\n+                argument is not supported when `x` is a dataset, generator or\n                 `keras.utils.Sequence` instance.\n                 If both `validation_data` and `validation_split` are provided,\n                 `validation_data` will override `validation_split`.\n@@ -1312,21 +1331,22 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             validation_data: Data on which to evaluate\n                 the loss and any model metrics at the end of each epoch.\n                 The model will not be trained on this data. Thus, note the fact\n-                that the validation loss of data provided using `validation_split`\n-                or `validation_data` is not affected by regularization layers like\n-                noise and dropout.\n+                that the validation loss of data provided using\n+                `validation_split` or `validation_data` is not affected by\n+                regularization layers like noise and dropout.\n                 `validation_data` will override `validation_split`.\n                 `validation_data` could be:\n                   - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n-                  - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n+                  - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n+                    arrays.\n                   - A `tf.data.Dataset`.\n                   - A Python generator or `keras.utils.Sequence` returning\n                   `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n                 `validation_data` is not yet supported with\n                 `tf.distribute.experimental.ParameterServerStrategy`.\n             shuffle: Boolean (whether to shuffle the training data\n-                before each epoch) or str (for 'batch'). This argument is ignored\n-                when `x` is a generator or an object of tf.data.Dataset.\n+                before each epoch) or str (for 'batch'). This argument is\n+                ignored when `x` is a generator or an object of tf.data.Dataset.\n                 'batch' is a special option for dealing\n                 with the limitations of HDF5 data; it shuffles in batch-sized\n                 chunks. Has no effect when `steps_per_epoch` is not `None`.\n@@ -1344,14 +1364,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 or in the case of temporal data,\n                 you can pass a 2D array with shape\n                 `(samples, sequence_length)`,\n-                to apply a different weight to every timestep of every sample. This\n-                argument is not supported when `x` is a dataset, generator, or\n-               `keras.utils.Sequence` instance, instead provide the sample_weights\n-                as the third element of `x`.\n+                to apply a different weight to every timestep of every sample.\n+                This argument is not supported when `x` is a dataset, generator,\n+                or `keras.utils.Sequence` instance, instead provide the\n+                sample_weights as the third element of `x`.\n                 Note that sample weighting does not apply to metrics specified\n-                via the `metrics` argument in `compile()`. To apply sample weighting\n-                to your metrics, you can specify them via the `weighted_metrics` in\n-                `compile()` instead.\n+                via the `metrics` argument in `compile()`. To apply sample\n+                weighting to your metrics, you can specify them via the\n+                `weighted_metrics` in `compile()` instead.\n             initial_epoch: Integer.\n                 Epoch at which to start training\n                 (useful for resuming a previous training run).\n@@ -1363,39 +1383,42 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 the number of samples in your dataset divided by\n                 the batch size, or 1 if that cannot be determined. If x is a\n                 `tf.data` dataset, and 'steps_per_epoch'\n-                is None, the epoch will run until the input dataset is exhausted.\n-                When passing an infinitely repeating dataset, you must specify the\n-                `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n-                will run indefinitely with an infinitely repeating dataset.\n-                This argument is not supported with array inputs.\n+                is None, the epoch will run until the input dataset is\n+                exhausted.  When passing an infinitely repeating dataset, you\n+                must specify the `steps_per_epoch` argument. If\n+                `steps_per_epoch=-1` the training will run indefinitely with an\n+                infinitely repeating dataset.  This argument is not supported\n+                with array inputs.\n                 When using `tf.distribute.experimental.ParameterServerStrategy`:\n                   * `steps_per_epoch=None` is not supported.\n             validation_steps: Only relevant if `validation_data` is provided and\n                 is a `tf.data` dataset. Total number of steps (batches of\n                 samples) to draw before stopping when performing validation\n-                at the end of every epoch. If 'validation_steps' is None, validation\n-                will run until the `validation_data` dataset is exhausted. In the\n-                case of an infinitely repeated dataset, it will run into an\n-                infinite loop. If 'validation_steps' is specified and only part of\n-                the dataset will be consumed, the evaluation will start from the\n-                beginning of the dataset at each epoch. This ensures that the same\n-                validation samples are used every time.\n+                at the end of every epoch. If 'validation_steps' is None,\n+                validation will run until the `validation_data` dataset is\n+                exhausted. In the case of an infinitely repeated dataset, it\n+                will run into an infinite loop. If 'validation_steps' is\n+                specified and only part of the dataset will be consumed, the\n+                evaluation will start from the beginning of the dataset at each\n+                epoch. This ensures that the same validation samples are used\n+                every time.\n             validation_batch_size: Integer or `None`.\n                 Number of samples per validation batch.\n                 If unspecified, will default to `batch_size`.\n-                Do not specify the `validation_batch_size` if your data is in the\n-                form of datasets, generators, or `keras.utils.Sequence` instances\n-                (since they generate batches).\n-            validation_freq: Only relevant if validation data is provided. Integer\n-                or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n-                If an integer, specifies how many training epochs to run before a\n-                new validation run is performed, e.g. `validation_freq=2` runs\n-                validation every 2 epochs. If a Container, specifies the epochs on\n-                which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n-                validation at the end of the 1st, 2nd, and 10th epochs.\n-            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n-                input only. Maximum size for the generator queue.\n-                If unspecified, `max_queue_size` will default to 10.\n+                Do not specify the `validation_batch_size` if your data is in\n+                the form of datasets, generators, or `keras.utils.Sequence`\n+                instances (since they generate batches).\n+            validation_freq: Only relevant if validation data is provided.\n+              Integer or `collections.abc.Container` instance (e.g. list, tuple,\n+              etc.).  If an integer, specifies how many training epochs to run\n+              before a new validation run is performed, e.g. `validation_freq=2`\n+              runs validation every 2 epochs. If a Container, specifies the\n+              epochs on which to run validation, e.g.\n+              `validation_freq=[1, 2, 10]` runs validation at the end of the\n+              1st, 2nd, and 10th epochs.\n+            max_queue_size: Integer. Used for generator or\n+              `keras.utils.Sequence` input only. Maximum size for the generator\n+              queue.  If unspecified, `max_queue_size` will default to 10.\n             workers: Integer. Used for generator or `keras.utils.Sequence` input\n                 only. Maximum number of processes to spin up\n                 when using process-based threading. If unspecified, `workers`\n@@ -1405,31 +1428,34 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 threading. If unspecified, `use_multiprocessing` will default to\n                 `False`. Note that because this implementation relies on\n                 multiprocessing, you should not pass non-picklable arguments to\n-                the generator as they can't be passed easily to children processes.\n+                the generator as they can't be passed easily to children\n+                processes.\n \n         Unpacking behavior for iterator-like inputs:\n             A common pattern is to pass a tf.data.Dataset, generator, or\n           tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n-          yield not only features (x) but optionally targets (y) and sample weights.\n-          Keras requires that the output of such iterator-likes be unambiguous. The\n-          iterator should return a tuple of length 1, 2, or 3, where the optional\n-          second and third elements will be used for y and sample_weight\n-          respectively. Any other type provided will be wrapped in a length one\n-          tuple, effectively treating everything as 'x'. When yielding dicts, they\n-          should still adhere to the top-level tuple structure.\n+          yield not only features (x) but optionally targets (y) and sample\n+          weights.  Keras requires that the output of such iterator-likes be\n+          unambiguous. The iterator should return a tuple of length 1, 2, or 3,\n+          where the optional second and third elements will be used for y and\n+          sample_weight respectively. Any other type provided will be wrapped in\n+          a length one tuple, effectively treating everything as 'x'. When\n+          yielding dicts, they should still adhere to the top-level tuple\n+          structure.\n           e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n           features, targets, and weights from the keys of a single dict.\n-            A notable unsupported data type is the namedtuple. The reason is that\n-          it behaves like both an ordered datatype (tuple) and a mapping\n+            A notable unsupported data type is the namedtuple. The reason is\n+          that it behaves like both an ordered datatype (tuple) and a mapping\n           datatype (dict). So given a namedtuple of the form:\n               `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n           it is ambiguous whether to reverse the order of the elements when\n           interpreting the value. Even worse is a tuple of the form:\n               `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n-          where it is unclear if the tuple was intended to be unpacked into x, y,\n-          and sample_weight or passed through as a single element to `x`. As a\n-          result the data processing code will simply raise a ValueError if it\n-          encounters a namedtuple. (Along with instructions to remedy the issue.)\n+          where it is unclear if the tuple was intended to be unpacked into x,\n+          y, and sample_weight or passed through as a single element to `x`. As\n+          a result the data processing code will simply raise a ValueError if it\n+          encounters a namedtuple. (Along with instructions to remedy the\n+          issue.)\n \n         Returns:\n             A `History` object. Its `History.history` attribute is\n@@ -1454,8 +1480,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         verbose = _get_verbosity(verbose, self.distribute_strategy)\n \n         if validation_split and validation_data is None:\n-            # Create the validation data using the training data. Only supported for\n-            # `Tensor` and `NumPy` input.\n+            # Create the validation data using the training data. Only supported\n+            # for `Tensor` and `NumPy` input.\n             (\n                 x,\n                 y,\n@@ -1544,7 +1570,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                             tmp_logs = self.train_function(iterator)\n                             if data_handler.should_sync:\n                                 context.async_wait()\n-                            logs = tmp_logs  # No error, now safe to assign to logs.\n+                            # No error, now safe to assign to logs.\n+                            logs = tmp_logs\n                             end_step = step + data_handler.step_increment\n                             callbacks.on_train_batch_end(end_step, logs)\n                             if self.stop_training:\n@@ -1627,8 +1654,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         This typically includes the forward pass, loss calculation, and metrics\n         updates.\n \n-        Configuration details for *how* this logic is run (e.g. `tf.function` and\n-        `tf.distribute.Strategy` settings), should be left to\n+        Configuration details for *how* this logic is run (e.g. `tf.function`\n+        and `tf.distribute.Strategy` settings), should be left to\n         `Model.make_test_function`, which can also be overridden.\n \n         Args:\n@@ -1713,15 +1740,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 )\n \n             if self._cluster_coordinator:\n-                self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+                self.test_function = (\n+                    lambda it: self._cluster_coordinator.schedule(\n                         test_function, args=(it,)\n                     )\n+                )\n             else:\n                 self.test_function = test_function\n \n-        # If we're using a coordinator, use the value of self._steps_per_execution\n-        # at the time the function is called/scheduled, and not when it is actually\n-        # executed.\n+        # If we're using a coordinator, use the value of\n+        # self._steps_per_execution at the time the function is\n+        # called/scheduled, and not when it is actually executed.\n         elif self._cluster_coordinator:\n \n             def test_function(iterator, steps_per_execution):\n@@ -1735,7 +1764,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     test_function, reduce_retracing=True\n                 )\n \n-            self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+            self.test_function = lambda it: self._cluster_coordinator.schedule(\n                 test_function, args=(it, self._steps_per_execution.value())\n             )\n         else:\n@@ -1785,22 +1814,22 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               - A `tf.data` dataset. Should return a tuple\n                 of either `(inputs, targets)` or\n                 `(inputs, targets, sample_weights)`.\n-              - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n-                or `(inputs, targets, sample_weights)`.\n-              A more detailed description of unpacking behavior for iterator types\n-              (Dataset, generator, Sequence) is given in the `Unpacking behavior\n-              for iterator-like inputs` section of `Model.fit`.\n+              - A generator or `keras.utils.Sequence` returning `(inputs,\n+                targets)` or `(inputs, targets, sample_weights)`.\n+              A more detailed description of unpacking behavior for iterator\n+              types (Dataset, generator, Sequence) is given in the `Unpacking\n+              behavior for iterator-like inputs` section of `Model.fit`.\n             y: Target data. Like the input data `x`, it could be either Numpy\n               array(s) or TensorFlow tensor(s). It should be consistent with `x`\n-              (you cannot have Numpy inputs and tensor targets, or inversely). If\n-              `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n-              should not be specified (since targets will be obtained from the\n-              iterator/dataset).\n+              (you cannot have Numpy inputs and tensor targets, or inversely).\n+              If `x` is a dataset, generator or `keras.utils.Sequence` instance,\n+              `y` should not be specified (since targets will be obtained from\n+              the iterator/dataset).\n             batch_size: Integer or `None`. Number of samples per batch of\n-              computation. If unspecified, `batch_size` will default to 32. Do not\n-              specify the `batch_size` if your data is in the form of a dataset,\n-              generators, or `keras.utils.Sequence` instances (since they generate\n-              batches).\n+              computation. If unspecified, `batch_size` will default to 32. Do\n+              not specify the `batch_size` if your data is in the form of a\n+              dataset, generators, or `keras.utils.Sequence` instances (since\n+              they generate batches).\n             verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = single line.\n                 `\"auto\"` defaults to 1 for most cases, and to 2 when used with\n@@ -1809,36 +1838,39 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 recommended when not running interactively (e.g. in a production\n                 environment).\n             sample_weight: Optional Numpy array of weights for the test samples,\n-              used for weighting the loss function. You can either pass a flat (1D)\n-              Numpy array with the same length as the input samples\n+              used for weighting the loss function. You can either pass a flat\n+              (1D) Numpy array with the same length as the input samples\n                 (1:1 mapping between weights and samples), or in the case of\n                   temporal data, you can pass a 2D array with shape `(samples,\n-                  sequence_length)`, to apply a different weight to every timestep\n-                  of every sample. This argument is not supported when `x` is a\n-                  dataset, instead pass sample weights as the third element of `x`.\n+                  sequence_length)`, to apply a different weight to every\n+                  timestep of every sample. This argument is not supported when\n+                  `x` is a dataset, instead pass sample weights as the third\n+                  element of `x`.\n             steps: Integer or `None`. Total number of steps (batches of samples)\n               before declaring the evaluation round finished. Ignored with the\n-              default value of `None`. If x is a `tf.data` dataset and `steps` is\n-              None, 'evaluate' will run until the dataset is exhausted. This\n+              default value of `None`. If x is a `tf.data` dataset and `steps`\n+              is None, 'evaluate' will run until the dataset is exhausted. This\n               argument is not supported with array inputs.\n             callbacks: List of `keras.callbacks.Callback` instances. List of\n               callbacks to apply during evaluation. See\n               [callbacks](/api_docs/python/tf/keras/callbacks).\n-            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n-              input only. Maximum size for the generator queue. If unspecified,\n-              `max_queue_size` will default to 10.\n+            max_queue_size: Integer. Used for generator or\n+              `keras.utils.Sequence` input only. Maximum size for the generator\n+              queue. If unspecified, `max_queue_size` will default to 10.\n             workers: Integer. Used for generator or `keras.utils.Sequence` input\n-              only. Maximum number of processes to spin up when using process-based\n-              threading. If unspecified, `workers` will default to 1.\n+              only. Maximum number of processes to spin up when using\n+              process-based threading. If unspecified, `workers` will default to\n+              1.\n             use_multiprocessing: Boolean. Used for generator or\n               `keras.utils.Sequence` input only. If `True`, use process-based\n               threading. If unspecified, `use_multiprocessing` will default to\n               `False`. Note that because this implementation relies on\n-              multiprocessing, you should not pass non-picklable arguments to the\n-              generator as they can't be passed easily to children processes.\n-            return_dict: If `True`, loss and metric results are returned as a dict,\n-              with each key being the name of the metric. If `False`, they are\n-              returned as a list.\n+              multiprocessing, you should not pass non-picklable arguments to\n+              the generator as they can't be passed easily to children\n+              processes.\n+            return_dict: If `True`, loss and metric results are returned as a\n+              dict, with each key being the name of the metric. If `False`, they\n+              are returned as a list.\n             **kwargs: Unused at this time.\n \n         See the discussion of `Unpacking behavior for iterator-like inputs` for\n@@ -1881,7 +1913,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             ):\n                 data_handler = self._eval_data_handler\n             else:\n-                # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n+                # Creates a `tf.data.Dataset` and handles batch and epoch\n+                # iteration.\n                 data_handler = data_adapter.get_data_handler(\n                     x=x,\n                     y=y,\n@@ -1924,7 +1957,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                             tmp_logs = self.test_function(iterator)\n                             if data_handler.should_sync:\n                                 context.async_wait()\n-                            logs = tmp_logs  # No error, now safe to assign to logs.\n+                            # No error, now safe to assign to logs.\n+                            logs = tmp_logs\n                             end_step = step + data_handler.step_increment\n                             callbacks.on_test_batch_end(end_step, logs)\n             logs = tf_utils.sync_to_numpy_or_python_type(logs)\n@@ -1941,11 +1975,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         This method can be overridden to support custom inference logic.\n         This method is called by `Model.make_predict_function`.\n \n-        This method should contain the mathematical logic for one step of inference.\n-        This typically includes the forward pass.\n+        This method should contain the mathematical logic for one step of\n+        inference.  This typically includes the forward pass.\n \n-        Configuration details for *how* this logic is run (e.g. `tf.function` and\n-        `tf.distribute.Strategy` settings), should be left to\n+        Configuration details for *how* this logic is run (e.g. `tf.function`\n+        and `tf.distribute.Strategy` settings), should be left to\n         `Model.make_predict_function`, which can also be overridden.\n \n         Args:\n@@ -2065,9 +2099,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     ):\n         \"\"\"Generates output predictions for the input samples.\n \n-        Computation is done in batches. This method is designed for batch processing\n-        of large numbers of inputs. It is not intended for use inside of loops\n-        that iterate over your data and process small numbers of inputs at a time.\n+        Computation is done in batches. This method is designed for batch\n+        processing of large numbers of inputs. It is not intended for use inside\n+        of loops that iterate over your data and process small numbers of inputs\n+        at a time.\n \n         For small numbers of inputs that fit in one batch,\n         directly use `__call__()` for faster execution, e.g.,\n@@ -2084,8 +2119,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         Note: See [this FAQ entry](\n         https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n-        for more details about the difference between `Model` methods `predict()`\n-        and `__call__()`.\n+        for more details about the difference between `Model` methods\n+        `predict()` and `__call__()`.\n \n         Args:\n             x: Input samples. It could be:\n@@ -2095,9 +2130,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 (in case the model has multiple inputs).\n               - A `tf.data` dataset.\n               - A generator or `keras.utils.Sequence` instance.\n-              A more detailed description of unpacking behavior for iterator types\n-              (Dataset, generator, Sequence) is given in the `Unpacking behavior\n-              for iterator-like inputs` section of `Model.fit`.\n+              A more detailed description of unpacking behavior for iterator\n+              types (Dataset, generator, Sequence) is given in the `Unpacking\n+              behavior for iterator-like inputs` section of `Model.fit`.\n             batch_size: Integer or `None`.\n                 Number of samples per batch.\n                 If unspecified, `batch_size` will default to 32.\n@@ -2119,9 +2154,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during prediction.\n                 See [callbacks](/api_docs/python/tf/keras/callbacks).\n-            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n-                input only. Maximum size for the generator queue.\n-                If unspecified, `max_queue_size` will default to 10.\n+            max_queue_size: Integer. Used for generator or\n+                `keras.utils.Sequence` input only. Maximum size for the\n+                generator queue. If unspecified, `max_queue_size` will default\n+                to 10.\n             workers: Integer. Used for generator or `keras.utils.Sequence` input\n                 only. Maximum number of processes to spin up when using\n                 process-based threading. If unspecified, `workers` will default\n@@ -2131,12 +2167,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 threading. If unspecified, `use_multiprocessing` will default to\n                 `False`. Note that because this implementation relies on\n                 multiprocessing, you should not pass non-picklable arguments to\n-                the generator as they can't be passed easily to children processes.\n+                the generator as they can't be passed easily to children\n+                processes.\n \n         See the discussion of `Unpacking behavior for iterator-like inputs` for\n-        `Model.fit`. Note that Model.predict uses the same interpretation rules as\n-        `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n-        three methods.\n+        `Model.fit`. Note that Model.predict uses the same interpretation rules\n+        as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for\n+        all three methods.\n \n         Returns:\n             Numpy array(s) of predictions.\n@@ -2153,9 +2190,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         self._check_call_args(\"predict\")\n         _disallow_inside_tf_function(\"predict\")\n \n-        # TODO(yashkatariya): Cache model on the coordinator for faster prediction.\n-        # If running under PSS, then swap it with OneDeviceStrategy so that\n-        # execution will run on the coordinator.\n+        # TODO(yashkatariya): Cache model on the coordinator for faster\n+        # prediction.  If running under PSS, then swap it with OneDeviceStrategy\n+        # so that execution will run on the coordinator.\n         original_pss_strategy = None\n         if (\n             self.distribute_strategy._should_use_with_coordinator\n@@ -2187,9 +2224,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     x = x.with_options(options)\n                 except ValueError:\n                     warnings.warn(\n-                        \"Using Model.predict with MultiWorkerMirroredStrategy or \"\n-                        \"TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \"\n-                        \"result. Consider setting it to AutoShardPolicy.DATA.\",\n+                        \"Using Model.predict with MultiWorkerMirroredStrategy \"\n+                        \"or TPUStrategy and AutoShardPolicy.FILE might lead to \"\n+                        \"out-of-order result. Consider setting it to \"\n+                        \"AutoShardPolicy.DATA.\",\n                         stacklevel=2,\n                     )\n \n@@ -2264,9 +2302,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             batch_outputs, potentially_ragged_concat, outputs\n         )\n \n-        # If originally PSS strategy was used, then replace it back since predict\n-        # is running under `OneDeviceStrategy` after the swap and once its done\n-        # we need to replace it back to PSS again.\n+        # If originally PSS strategy was used, then replace it back since\n+        # predict is running under `OneDeviceStrategy` after the swap and once\n+        # its done we need to replace it back to PSS again.\n         if original_pss_strategy is not None:\n             self._distribution_strategy = original_pss_strategy\n \n@@ -2316,20 +2354,21 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             y: Target data. Like the input data `x`, it could be either Numpy\n               array(s) or TensorFlow tensor(s).\n             sample_weight: Optional array of the same length as x, containing\n-              weights to apply to the model's loss for each sample. In the case of\n-              temporal data, you can pass a 2D array with shape (samples,\n+              weights to apply to the model's loss for each sample. In the case\n+              of temporal data, you can pass a 2D array with shape (samples,\n               sequence_length), to apply a different weight to every timestep of\n               every sample.\n-            class_weight: Optional dictionary mapping class indices (integers) to a\n-              weight (float) to apply to the model's loss for the samples from this\n-              class during training. This can be useful to tell the model to \"pay\n-              more attention\" to samples from an under-represented class.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+              to a weight (float) to apply to the model's loss for the samples\n+              from this class during training. This can be useful to tell the\n+              model to \"pay more attention\" to samples from an under-represented\n+              class.\n             reset_metrics: If `True`, the metrics returned will be only for this\n-              batch. If `False`, the metrics will be statefully accumulated across\n-              batches.\n-            return_dict: If `True`, loss and metric results are returned as a dict,\n-              with each key being the name of the metric. If `False`, they are\n-              returned as a list.\n+              batch. If `False`, the metrics will be statefully accumulated\n+              across batches.\n+            return_dict: If `True`, loss and metric results are returned as a\n+              dict, with each key being the name of the metric. If `False`, they\n+              are returned as a list.\n \n         Returns:\n             Scalar training loss\n@@ -2377,22 +2416,22 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                   model has multiple inputs).\n               - A TensorFlow tensor, or a list of tensors (in case the model has\n                   multiple inputs).\n-              - A dict mapping input names to the corresponding array/tensors, if\n-                  the model has named inputs.\n+              - A dict mapping input names to the corresponding array/tensors,\n+                  if the model has named inputs.\n             y: Target data. Like the input data `x`, it could be either Numpy\n               array(s) or TensorFlow tensor(s). It should be consistent with `x`\n               (you cannot have Numpy inputs and tensor targets, or inversely).\n             sample_weight: Optional array of the same length as x, containing\n-              weights to apply to the model's loss for each sample. In the case of\n-              temporal data, you can pass a 2D array with shape (samples,\n+              weights to apply to the model's loss for each sample. In the case\n+              of temporal data, you can pass a 2D array with shape (samples,\n               sequence_length), to apply a different weight to every timestep of\n               every sample.\n             reset_metrics: If `True`, the metrics returned will be only for this\n-              batch. If `False`, the metrics will be statefully accumulated across\n-              batches.\n-            return_dict: If `True`, loss and metric results are returned as a dict,\n-              with each key being the name of the metric. If `False`, they are\n-              returned as a list.\n+              batch. If `False`, the metrics will be statefully accumulated\n+              across batches.\n+            return_dict: If `True`, loss and metric results are returned as a\n+              dict, with each key being the name of the metric. If `False`, they\n+              are returned as a list.\n \n         Returns:\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -2401,7 +2440,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             the display labels for the scalar outputs.\n \n         Raises:\n-            RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`.\n+            RuntimeError: If `model.test_on_batch` is wrapped in a\n+              `tf.function`.\n         \"\"\"\n         self._assert_compile_was_called()\n         self._check_call_args(\"test_on_batch\")\n@@ -2435,7 +2475,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             Numpy array(s) of predictions.\n \n         Raises:\n-            RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`.\n+            RuntimeError: If `model.predict_on_batch` is wrapped in a\n+              `tf.function`.\n         \"\"\"\n         self._check_call_args(\"predict_on_batch\")\n         _disallow_inside_tf_function(\"predict_on_batch\")\n@@ -2468,8 +2509,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n \n         DEPRECATED:\n-          `Model.fit` now supports generators, so there is no longer any need to use\n-          this endpoint.\n+          `Model.fit` now supports generators, so there is no longer any need to\n+          use this endpoint.\n         \"\"\"\n         warnings.warn(\n             \"`Model.fit_generator` is deprecated and \"\n@@ -2508,8 +2549,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Evaluates the model on a data generator.\n \n         DEPRECATED:\n-          `Model.evaluate` now supports generators, so there is no longer any need\n-          to use this endpoint.\n+          `Model.evaluate` now supports generators, so there is no longer any\n+          need to use this endpoint.\n         \"\"\"\n         warnings.warn(\n             \"`Model.evaluate_generator` is deprecated and \"\n@@ -2543,8 +2584,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Generates predictions for the input samples from a data generator.\n \n         DEPRECATED:\n-          `Model.predict` now supports generators, so there is no longer any need\n-          to use this endpoint.\n+          `Model.predict` now supports generators, so there is no longer any\n+          need to use this endpoint.\n         \"\"\"\n         warnings.warn(\n             \"`Model.predict_generator` is deprecated and \"\n@@ -2628,30 +2669,31 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\n \n         Please see `tf.keras.models.save_model` or the\n-        [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n+        [Serialization and Saving guide](\n+        https://keras.io/guides/serialization_and_saving/)\n         for details.\n \n         Args:\n-            filepath: String, PathLike, path to SavedModel or H5 file to save the\n-                model.\n+            filepath: String, PathLike, path to SavedModel or H5 file to save\n+                the model.\n             overwrite: Whether to silently overwrite any existing file at the\n                 target location, or provide the user with a manual prompt.\n             include_optimizer: If True, save optimizer's state together.\n             save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n-                model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n-                and 'h5' in TF 1.X.\n-            signatures: Signatures to save with the SavedModel. Applicable to the\n-                'tf' format only. Please see the `signatures` argument in\n+                model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF\n+                2.X, and 'h5' in TF 1.X.\n+            signatures: Signatures to save with the SavedModel. Applicable to\n+                the 'tf' format only. Please see the `signatures` argument in\n                 `tf.saved_model.save` for details.\n             options: (only applies to SavedModel format)\n                 `tf.saved_model.SaveOptions` object that specifies options for\n                 saving to SavedModel.\n             save_traces: (only applies to SavedModel format) When enabled, the\n                 SavedModel will store the function traces for each layer. This\n-                can be disabled, so that only the configs of each layer are stored.\n-                Defaults to `True`. Disabling this will decrease serialization time\n-                and reduce file size, but it requires that all custom layers/models\n-                implement a `get_config()` method.\n+                can be disabled, so that only the configs of each layer are\n+                stored.  Defaults to `True`. Disabling this will decrease\n+                serialization time and reduce file size, but it requires that\n+                all custom layers/models implement a `get_config()` method.\n \n         Example:\n \n@@ -2697,49 +2739,50 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               - For every weight in the layer, a dataset\n                   storing the weight value, named after the weight tensor.\n \n-        When saving in TensorFlow format, all objects referenced by the network are\n-        saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n-        instances or `Optimizer` instances assigned to object attributes. For\n-        networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n-        outputs)`, `Layer` instances used by the network are tracked/saved\n-        automatically. For user-defined classes which inherit from `tf.keras.Model`,\n-        `Layer` instances must be assigned to object attributes, typically in the\n-        constructor. See the documentation of `tf.train.Checkpoint` and\n-        `tf.keras.Model` for details.\n+        When saving in TensorFlow format, all objects referenced by the network\n+        are saved in the same format as `tf.train.Checkpoint`, including any\n+        `Layer` instances or `Optimizer` instances assigned to object\n+        attributes. For networks constructed from inputs and outputs using\n+        `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network\n+        are tracked/saved automatically. For user-defined classes which inherit\n+        from `tf.keras.Model`, `Layer` instances must be assigned to object\n+        attributes, typically in the constructor. See the documentation of\n+        `tf.train.Checkpoint` and `tf.keras.Model` for details.\n \n         While the formats are the same, do not mix `save_weights` and\n-        `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n-        loaded using `Model.load_weights`. Checkpoints saved using\n+        `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should\n+        be loaded using `Model.load_weights`. Checkpoints saved using\n         `tf.train.Checkpoint.save` should be restored using the corresponding\n         `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n         `save_weights` for training checkpoints.\n \n-        The TensorFlow format matches objects and variables by starting at a root\n-        object, `self` for `save_weights`, and greedily matching attribute\n-        names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n-        is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n-        means saving a `tf.keras.Model` using `save_weights` and loading into a\n-        `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n-        the `Model`'s variables. See the\n-        [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint)\n-        for details on the TensorFlow format.\n+        The TensorFlow format matches objects and variables by starting at a\n+        root object, `self` for `save_weights`, and greedily matching attribute\n+        names. For `Model.save` this is the `Model`, and for `Checkpoint.save`\n+        this is the `Checkpoint` even if the `Checkpoint` has a model attached.\n+        This means saving a `tf.keras.Model` using `save_weights` and loading\n+        into a `tf.train.Checkpoint` with a `Model` attached (or vice versa)\n+        will not match the `Model`'s variables. See the\n+        [guide to training checkpoints](\n+        https://www.tensorflow.org/guide/checkpoint) for details on\n+        the TensorFlow format.\n \n         Args:\n-            filepath: String or PathLike, path to the file to save the weights to.\n-                When saving in TensorFlow format, this is the prefix used for\n-                checkpoint files (multiple files are generated). Note that the '.h5'\n-                suffix causes weights to be saved in HDF5 format.\n+            filepath: String or PathLike, path to the file to save the weights\n+                to. When saving in TensorFlow format, this is the prefix used\n+                for checkpoint files (multiple files are generated). Note that\n+                the '.h5' suffix causes weights to be saved in HDF5 format.\n             overwrite: Whether to silently overwrite any existing file at the\n                 target location, or provide the user with a manual prompt.\n             save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n-                '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n-                `None` defaults to 'tf'.\n+                '.keras' will default to HDF5 if `save_format` is `None`.\n+                Otherwise `None` defaults to 'tf'.\n             options: Optional `tf.train.CheckpointOptions` object that specifies\n                 options for saving weights.\n \n         Raises:\n-            ImportError: If `h5py` is not available when attempting to save in HDF5\n-                format.\n+            ImportError: If `h5py` is not available when attempting to save in\n+                HDF5 format.\n         \"\"\"\n         self._assert_weights_created()\n         filepath = io_utils.path_to_string(filepath)\n@@ -2757,8 +2800,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 save_format = \"h5\"\n             else:\n                 raise ValueError(\n-                    f\"Unknown format. Received: `save_format`={save_format}. Was \"\n-                    'expecting one of {\"tf\", \"h5\"}.'\n+                    f\"Unknown format. Received: `save_format`={save_format}. \"\n+                    'Was expecting one of {\"tf\", \"h5\"}.'\n                 )\n         if save_format == \"tf\" and filepath_is_h5:\n             raise ValueError(\n@@ -2769,8 +2812,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         if save_format == \"h5\" and h5py is None:\n             raise ImportError(\n-                \"`save_weights` requires h5py when saving in hdf5, but h5py is not \"\n-                \"available. Try installing h5py package.\"\n+                \"`save_weights` requires h5py when saving in hdf5, but h5py is \"\n+                \"not available. Try installing h5py package.\"\n             )\n         if save_format == \"tf\":\n             check_filepath = filepath + \".index\"\n@@ -2790,7 +2833,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 backend.get_session()\n             self._checkpoint.write(filepath, options=options)\n \n-            # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\n+            # Record this checkpoint so it's visible from\n+            # tf.train.latest_checkpoint.\n             tf.__internal__.train.update_checkpoint_state(\n                 save_dir=os.path.dirname(filepath),\n                 model_checkpoint_path=filepath,\n@@ -2805,48 +2849,49 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n \n         If `by_name` is False weights are loaded based on the network's\n-        topology. This means the architecture should be the same as when the weights\n-        were saved.  Note that layers that don't have weights are not taken into\n-        account in the topological ordering, so adding or removing layers is fine as\n-        long as they don't have weights.\n+        topology. This means the architecture should be the same as when the\n+        weights were saved.  Note that layers that don't have weights are not\n+        taken into account in the topological ordering, so adding or removing\n+        layers is fine as long as they don't have weights.\n \n-        If `by_name` is True, weights are loaded into layers only if they share the\n-        same name. This is useful for fine-tuning or transfer-learning models where\n-        some of the layers have changed.\n+        If `by_name` is True, weights are loaded into layers only if they share\n+        the same name. This is useful for fine-tuning or transfer-learning\n+        models where some of the layers have changed.\n \n-        Only topological loading (`by_name=False`) is supported when loading weights\n-        from the TensorFlow format. Note that topological loading differs slightly\n-        between TensorFlow and HDF5 formats for user-defined classes inheriting from\n-        `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n-        TensorFlow format loads based on the object-local names of attributes to\n-        which layers are assigned in the `Model`'s constructor.\n+        Only topological loading (`by_name=False`) is supported when loading\n+        weights from the TensorFlow format. Note that topological loading\n+        differs slightly between TensorFlow and HDF5 formats for user-defined\n+        classes inheriting from `tf.keras.Model`: HDF5 loads based on a\n+        flattened list of weights, while the TensorFlow format loads based on\n+        the object-local names of attributes to which layers are assigned in the\n+        `Model`'s constructor.\n \n         Args:\n-            filepath: String, path to the weights file to load. For weight files in\n-                TensorFlow format, this is the file prefix (the same as was passed\n-                to `save_weights`). This can also be a path to a SavedModel\n-                saved from `model.save`.\n+            filepath: String, path to the weights file to load. For weight files\n+                in TensorFlow format, this is the file prefix (the same as was\n+                passed to `save_weights`). This can also be a path to a\n+                SavedModel saved from `model.save`.\n             by_name: Boolean, whether to load weights by name or by topological\n                 order. Only topological loading is supported for weight files in\n                 TensorFlow format.\n-            skip_mismatch: Boolean, whether to skip loading of layers where there is\n-                a mismatch in the number of weights, or a mismatch in the shape of\n-                the weight (only valid when `by_name=True`).\n+            skip_mismatch: Boolean, whether to skip loading of layers where\n+                there is a mismatch in the number of weights, or a mismatch in\n+                the shape of the weight (only valid when `by_name=True`).\n             options: Optional `tf.train.CheckpointOptions` object that specifies\n                 options for loading weights.\n \n         Returns:\n-            When loading a weight file in TensorFlow format, returns the same status\n-            object as `tf.train.Checkpoint.restore`. When graph building, restore\n-            ops are run automatically as soon as the network is built (on first call\n-            for user-defined classes inheriting from `Model`, immediately if it is\n-            already built).\n+            When loading a weight file in TensorFlow format, returns the same\n+            status object as `tf.train.Checkpoint.restore`. When graph building,\n+            restore ops are run automatically as soon as the network is built\n+            (on first call for user-defined classes inheriting from `Model`,\n+            immediately if it is already built).\n \n             When loading weights in HDF5 format, returns `None`.\n \n         Raises:\n-            ImportError: If `h5py` is not available and the weight file is in HDF5\n-                format.\n+            ImportError: If `h5py` is not available and the weight file is in\n+              HDF5 format.\n             ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n               `False`.\n         \"\"\"\n@@ -2862,8 +2907,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 )\n         if skip_mismatch and not by_name:\n             raise ValueError(\n-                \"When calling model.load_weights, skip_mismatch can only be set to \"\n-                \"True when by_name is True.\"\n+                \"When calling model.load_weights, skip_mismatch can only be \"\n+                \"set to True when by_name is True.\"\n             )\n \n         filepath, save_format = _detect_save_format(filepath)\n@@ -2871,9 +2916,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             status = self._checkpoint.read(filepath, options)\n             if by_name:\n                 raise NotImplementedError(\n-                    \"Weights may only be loaded based on topology into Models when \"\n-                    \"loading TensorFlow-formatted weights (got by_name=True to \"\n-                    \"load_weights).\"\n+                    \"Weights may only be loaded based on topology into Models \"\n+                    \"when loading TensorFlow-formatted weights \"\n+                    \"(got by_name=True to load_weights).\"\n                 )\n             if not tf.executing_eagerly():\n                 session = backend.get_session()\n@@ -2887,14 +2932,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             status = None\n             if h5py is None:\n                 raise ImportError(\n-                    \"`load_weights` requires h5py package when loading weights from \"\n-                    \"HDF5. Try installing h5py.\"\n+                    \"`load_weights` requires h5py package when loading weights \"\n+                    \"from HDF5. Try installing h5py.\"\n                 )\n             if not self._is_graph_network and not self.built:\n                 raise ValueError(\n-                    \"Unable to load weights saved in HDF5 format into a subclassed \"\n-                    \"Model which has not created its variables yet. Call the Model \"\n-                    \"first, then load the weights.\"\n+                    \"Unable to load weights saved in HDF5 format into a \"\n+                    \"subclassed Model which has not created its variables yet. \"\n+                    \"Call the Model first, then load the weights.\"\n                 )\n             self._assert_weights_created()\n             with h5py.File(filepath, \"r\") as f:\n@@ -2934,29 +2979,30 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def get_config(self):\n         \"\"\"Returns the config of the `Model`.\n \n-        Config is a Python dictionary (serializable) containing the configuration of\n-        an object, which in this case is a `Model`. This allows the `Model` to be\n-        be reinstantiated later (without its trained weights) from this\n-        configuration.\n+        Config is a Python dictionary (serializable) containing the\n+        configuration of an object, which in this case is a `Model`. This allows\n+        the `Model` to be be reinstantiated later (without its trained weights)\n+        from this configuration.\n \n-        Note that `get_config()` does not guarantee to return a fresh copy of dict\n-        every time it is called. The callers should make a copy of the returned dict\n-        if they want to modify it.\n+        Note that `get_config()` does not guarantee to return a fresh copy of\n+        dict every time it is called. The callers should make a copy of the\n+        returned dict if they want to modify it.\n \n-        Developers of subclassed `Model` are advised to override this method, and\n-        continue to update the dict from `super(MyModel, self).get_config()`\n+        Developers of subclassed `Model` are advised to override this method,\n+        and continue to update the dict from `super(MyModel, self).get_config()`\n         to provide the proper configuration of this `Model`. The default config\n-        is an empty dict. Optionally, raise `NotImplementedError` to allow Keras to\n-        attempt a default serialization.\n+        is an empty dict. Optionally, raise `NotImplementedError` to allow Keras\n+        to attempt a default serialization.\n \n         Returns:\n             Python dictionary containing the configuration of this `Model`.\n         \"\"\"\n \n-        # Return an empty dict here because otherwise subclass model developers may\n-        # see their model's `__init__()` be fed with unexpected keyword argument, if\n-        # their `__init__()` takes no argument for example, and they don't override\n-        # `from_config()`, which would use `cls(**config)` as a result.\n+        # Return an empty dict here because otherwise subclass model developers\n+        # may see their model's `__init__()` be fed with unexpected keyword\n+        # argument, if their `__init__()` takes no argument for example, and\n+        # they don't override `from_config()`, which would use `cls(**config)`\n+        # as a result.\n         config = {}\n \n         if saving_lib._ENABLED:  # pylint: disable=protected-access\n@@ -2976,9 +3022,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n         # `from_config` assumes `cls` is either `Functional` or a child class of\n-        # `Functional`. In the case that `cls` is meant to behave like a child class\n-        # of `Functional` but only inherits from the `Model` class, we have to call\n-        # `cls(...)` instead of `Functional.from_config`.\n+        # `Functional`. In the case that `cls` is meant to behave like a child\n+        # class of `Functional` but only inherits from the `Model` class, we\n+        # have to call `cls(...)` instead of `Functional.from_config`.\n         from keras.engine import (\n             functional,\n         )  # pylint: disable=g-import-not-at-top\n@@ -3000,11 +3046,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 functional.connect_ancillary_layers(model, layers)\n                 return model\n \n-            # The config does not contain all the information necessary to revive a\n-            # Functional model. This happens when the user creates subclassed models\n-            # where `get_config()` is returning insufficient information to be\n-            # considered a Functional model. In this case, we fall back to provide\n-            # all config into the constructor of the class.\n+            # The config does not contain all the information necessary to\n+            # revive a Functional model. This happens when the user creates\n+            # subclassed models where `get_config()` is returning insufficient\n+            # information to be considered a Functional model. In this case, we\n+            # fall back to provide all config into the constructor of the class.\n             optimizer, loss = None, None\n \n             optimizer_dict = config.pop(\"optimizer\", {})\n@@ -3048,7 +3094,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         `keras.models.model_from_json(json_string, custom_objects={})`.\n \n         Args:\n-            **kwargs: Additional keyword arguments to be passed to `json.dumps()`.\n+            **kwargs: Additional keyword arguments to be passed to\n+                *`json.dumps()`.\n \n         Returns:\n             A JSON string.\n@@ -3124,8 +3171,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def weights(self):\n         \"\"\"Returns the list of all layer variables/weights.\n \n-        Note: This will not track the weights of nested `tf.Modules` that are not\n-        themselves Keras layers.\n+        Note: This will not track the weights of nested `tf.Modules` that are\n+        not themselves Keras layers.\n \n         Returns:\n           A list of variables.\n@@ -3245,8 +3292,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Defines the save spec so that serialization is able to trace model call.\n \n         The TensorSpecs of the call function `inputs`, `args`, and `kwargs` are\n-        saved into a tuple of `([inputs] + args, kwargs)`. The input `TensorSpec`\n-        names are updated to match the built `input_names`.\n+        saved into a tuple of `([inputs] + args, kwargs)`. The input\n+        `TensorSpec` names are updated to match the built `input_names`.\n \n         The specs can be retrieved with the `save_spec` property.\n \n@@ -3285,8 +3332,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def save_spec(self, dynamic_batch=True):\n         \"\"\"Returns the `tf.TensorSpec` of call inputs as a tuple `(args, kwargs)`.\n \n-        This value is automatically defined after calling the model for the first\n-        time. Afterwards, you can use it when exporting the model for serving:\n+        This value is automatically defined after calling the model for the\n+        first time. Afterwards, you can use it when exporting the model for\n+        serving:\n \n         ```python\n         model = tf.keras.Model(...)\n@@ -3298,12 +3346,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           ...\n           return outputs\n \n-        # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this example, is\n-        # an empty dict since functional models do not use keyword arguments.\n+        # arg_specs is `[tf.TensorSpec(...), ...]`. kwarg_specs, in this\n+        # example, is an empty dict since functional models do not use keyword\n+        # arguments.\n         arg_specs, kwarg_specs = model.save_spec()\n \n         model.save(path, signatures={\n-          'serving_default': serve.get_concrete_function(*arg_specs, **kwarg_specs)\n+          'serving_default': serve.get_concrete_function(*arg_specs,\n+                                                         **kwarg_specs)\n         })\n         ```\n \n@@ -3325,12 +3375,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Asserts that all the weights for the model have been created.\n \n         For a non-dynamic model, the weights must already be created after the\n-        layer has been called. For a dynamic model, the exact list of weights can\n-        never be known for certain since it may change at any time during execution.\n+        layer has been called. For a dynamic model, the exact list of weights\n+        can never be known for certain since it may change at any time during\n+        execution.\n \n-        We run this check right before accessing weights or getting the Numpy value\n-        for the current weights. Otherwise, if the layer has never been called,\n-        the user would just get an empty list, which is misleading.\n+        We run this check right before accessing weights or getting the Numpy\n+        value for the current weights. Otherwise, if the layer has never been\n+        called, the user would just get an empty list, which is misleading.\n \n         Raises:\n           ValueError: if the weights of the network have not yet been created.\n@@ -3343,9 +3394,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             and self.__class__ != Model\n             and not self.built\n         ):\n-            # For any model that has customized build() method but hasn't\n-            # been invoked yet, this will cover both sequential and subclass model.\n-            # Also make sure to exclude Model class itself which has build() defined.\n+            # For any model that has customized build() method but hasn't been\n+            # invoked yet, this will cover both sequential and subclass model.\n+            # Also make sure to exclude Model class itself which has build()\n+            # defined.\n             raise ValueError(\n                 f\"Weights for model {self.name} have not yet been \"\n                 \"created. \"\n@@ -3391,15 +3443,15 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         distribute_arg = kwargs.pop(\"distribute\", None)\n         if distribute_arg is not None:\n             raise ValueError(\n-                \"`distribute` argument in compile is not available in TF 2.0. Please \"\n-                \"create the model under the `strategy.scope()`. Received: \"\n-                f\"{distribute_arg}.\"\n+                \"`distribute` argument in compile is not available in TF 2.0. \"\n+                \"Please create the model under the `strategy.scope()`. \"\n+                f\"Received: {distribute_arg}.\"\n             )\n         target_tensor_arg = kwargs.pop(\"target_tensors\", None)\n         if target_tensor_arg is not None:\n             raise ValueError(\n-                \"`target_tensors` argument is not supported when executing eagerly. \"\n-                f\"Received: {target_tensor_arg}.\"\n+                \"`target_tensors` argument is not supported when executing \"\n+                f\"eagerly. Received: {target_tensor_arg}.\"\n             )\n         invalid_kwargs = set(kwargs) - {\"sample_weight_mode\"}\n         if invalid_kwargs:\n@@ -3416,12 +3468,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             for v in self.variables:\n                 if not strategy.extended.variable_created_in_scope(v):\n                     raise ValueError(\n-                        f\"Variable ({v}) was not created in the distribution strategy \"\n-                        f\"scope of ({strategy}). It is most likely because some \"\n-                        \"layers, model, or optimizer was being created outside the \"\n-                        \"distribution strategy scope. Try to make sure your code looks \"\n-                        \"similar to the following.\\n\"\n-                        \"with strategy.scope():\\n\"\n+                        f\"Variable ({v}) was not created in the distribution \"\n+                        f\"strategy scope of ({strategy}). It is most likely \"\n+                        \"because some layers, model, or optimizer was being \"\n+                        \"created outside the distribution strategy scope. Try \"\n+                        \"to make sure your code looks similar \"\n+                        \"to the following.\\nwith strategy.scope():\\n\"\n                         \"  model=_create_model()\\n\"\n                         \"  model.compile(...)\"\n                     )\n@@ -3433,13 +3485,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             for v in getattr(metric, \"variables\", []):\n                 if not strategy.extended.variable_created_in_scope(v):\n                     raise ValueError(\n-                        f\"Metric ({metric}) passed to `model.compile` was created inside \"\n-                        \"a different distribution strategy scope than the model. All \"\n-                        \"metrics must be created in the same distribution strategy \"\n-                        f\"scope as the model (in this case {strategy}). If you pass in a \"\n-                        \"string identifier for a metric to compile, the metric will \"\n-                        \"automatically be created in the correct distribution \"\n-                        \"strategy scope.\"\n+                        f\"Metric ({metric}) passed to `model.compile` was \"\n+                        \"created inside a different distribution strategy \"\n+                        \"scope than the model. All metrics must be created \"\n+                        \"in the same distribution strategy \"\n+                        f\"scope as the model (in this case {strategy}). \"\n+                        \"If you pass in a string identifier for a metric to \"\n+                        \"compile, the metric will automatically be created \"\n+                        \"in the correct distribution strategy scope.\"\n                     )\n \n         # Model metrics must be created in the same distribution strategy scope\n@@ -3448,12 +3501,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             for v in getattr(opt, \"_weights\", []):\n                 if not strategy.extended.variable_created_in_scope(v):\n                     raise ValueError(\n-                        f\"Optimizer ({optimizer}) passed to `model.compile` was created \"\n-                        \"inside a different distribution strategy scope than the model. \"\n-                        \"All optimizers must be created in the same distribution \"\n-                        f\"strategy scope as the model (in this case {strategy}). If you \"\n-                        \"pass in a string identifier for an optimizer to compile, the \"\n-                        \"optimizer will automatically be created in the correct \"\n+                        f\"Optimizer ({optimizer}) passed to `model.compile` \"\n+                        \"was created inside a different distribution strategy \"\n+                        \"scope than the model. All optimizers must be created \"\n+                        \"in the same distribution strategy scope as the model \"\n+                        f\"(in this case {strategy}). If you pass in a string \"\n+                        \"identifier for an optimizer to compile, the optimizer \"\n+                        \"will automatically be created in the correct \"\n                         \"distribution strategy scope.\"\n                     )\n \n@@ -3468,8 +3522,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n         Returns:\n           If the training is recovering from previous failure under multi-worker\n-          training setting, return the epoch the training is supposed to continue\n-          at. Otherwise, return the `initial_epoch` the user passes in.\n+          training setting, return the epoch the training is supposed to\n+          continue at. Otherwise, return the `initial_epoch` the user passes in.\n         \"\"\"\n         if self._training_state is not None:\n             return self._training_state.maybe_load_initial_epoch_from_ckpt(\n@@ -3512,14 +3566,15 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         ):\n             logging.warning(\n                 \"`evaluate()` received a value for `sample_weight`, but \"\n-                \"`weighted_metrics` were not provided.  Did you mean to pass metrics \"\n-                \"to `weighted_metrics` in `compile()`?  If this is intentional \"\n-                \"you can pass `weighted_metrics=[]` to `compile()` in order to \"\n-                \"silence this warning.\"\n+                \"`weighted_metrics` were not provided.  Did you mean to pass \"\n+                \"metrics to `weighted_metrics` in `compile()`?  If this is \"\n+                \"intentional you can pass `weighted_metrics=[]` to `compile()` \"\n+                \"in order to silence this warning.\"\n             )\n \n     def _set_inputs(self, inputs, outputs=None, training=None):\n-        \"\"\"This method is for compat with Modelv1. Only inputs are needed here.\"\"\"\n+        \"\"\"This method is for compat with Modelv1. Only inputs are needed\n+        here.\"\"\"\n         self._set_save_spec(inputs)\n \n     @property\n@@ -3569,8 +3624,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         \"\"\"Used for saving or cloning a Model.\n \n         Args:\n-          user_metrics: Whether to return user-supplied metrics or `Metric` objects.\n-            Defaults to returning the user-supplied metrics.\n+          user_metrics: Whether to return user-supplied metrics or `Metric`\n+            objects. Defaults to returning the user-supplied metrics.\n \n         Returns:\n           Dictionary of arguments that were used when compiling the model.\n@@ -3711,9 +3766,10 @@ def _get_verbosity(verbose, distribute_strategy):\n     if verbose == \"auto\":\n         if (\n             distribute_strategy._should_use_with_coordinator\n-            or not io_utils.is_interactive_logging_enabled()  # pylint: disable=protected-access\n+            or not io_utils.is_interactive_logging_enabled()\n         ):\n-            # Default to epoch-level logging for PSStrategy or using absl logging.\n+            # Default to epoch-level logging for PSStrategy or using absl\n+            # logging.\n             return 2\n         else:\n             return 1  # Default to batch-level logging otherwise.\n@@ -3742,7 +3798,7 @@ def _tpu_multi_host_concat(v, strategy):\n def _collective_all_reduce_multi_worker(strategy):\n     return (\n         isinstance(strategy, tf.distribute.MultiWorkerMirroredStrategy)\n-    ) and strategy.extended._in_multi_worker_mode()  # pylint: disable=protected-access\n+    ) and strategy.extended._in_multi_worker_mode()\n \n \n # TODO(wxinyi): merge this with _tpu_multi_host_concat once we have all_gather\n@@ -3798,10 +3854,11 @@ def _disallow_inside_tf_function(method_name):\n     if tf.inside_function():\n         error_msg = (\n             \"Detected a call to `Model.{method_name}` inside a `tf.function`. \"\n-            \"`Model.{method_name} is a high-level endpoint that manages its own \"\n-            \"`tf.function`. Please move the call to `Model.{method_name}` outside \"\n-            \"of all enclosing `tf.function`s. Note that you can call a `Model` \"\n-            \"directly on `Tensor`s inside a `tf.function` like: `model(x)`.\"\n+            \"`Model.{method_name} is a high-level endpoint that manages its \"\n+            \"own `tf.function`. Please move the call to `Model.{method_name}` \"\n+            \"outside of all enclosing `tf.function`s. Note that you can call a \"\n+            \"`Model` directly on `Tensor`s inside a `tf.function` like: \"\n+            \"`model(x)`.\"\n         ).format(method_name=method_name)\n         raise RuntimeError(error_msg)\n \n\n@@ -71,7 +71,8 @@ class ValidationDatasetAndValidationSplit(\n         train_dataset = _create_dataset(num_samples=200, batch_size=10)\n         eval_dataset = _create_dataset(num_samples=50, batch_size=25)\n \n-        # Make sure model.fit doesn't raise an error because of the mocking alone.\n+        # Make sure model.fit doesn't raise an error because of the mocking\n+        # alone.\n         mock_train_validation_split_return = (\n             (train_dataset, None, None),\n             eval_dataset,\n@@ -123,8 +124,8 @@ class ValidationDatasetNoLimitTest(test_combinations.TestCase):\n         evaluation = model.evaluate(x=eval_dataset)\n \n         # If the fit call used the entire dataset, then the final val MAE error\n-        # from the fit history should be equal to the final element in the output\n-        # of evaluating the model on the same eval dataset.\n+        # from the fit history should be equal to the final element in the\n+        # output of evaluating the model on the same eval dataset.\n         self.assertAlmostEqual(\n             history.history[\"val_mean_absolute_error\"][-1],\n             evaluation[-1],\n\n@@ -75,7 +75,8 @@ def model_iteration(\n           logged to a file, so verbose=2 is recommended when not running\n           interactively (eg, in a production environment).\n         callbacks: List of callbacks to be called during training\n-        val_inputs: Either a list or dictionary of arrays, or a dataset instance.\n+        val_inputs: Either a list or dictionary of arrays, or a dataset\n+          instance.\n         val_targets: List/dictionary of target arrays.\n         val_sample_weights: Optional list of sample weight arrays.\n         shuffle: Whether to shuffle the data at the beginning of each epoch\n@@ -89,13 +90,13 @@ def model_iteration(\n         validation_steps: Number of steps to run validation for (only if doing\n           validation from data tensors). Ignored with the default value of\n           `None`.\n-        validation_freq: Only relevant if validation data is provided. Integer or\n-          `collections.abc.Container` instance (e.g. list, tuple, etc.). If an\n-          integer, specifies how many training epochs to run before a new\n-          validation run is performed, e.g. `validation_freq=2` runs\n-          validation every 2 epochs. If a Container, specifies the epochs on\n-          which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n-          validation at the end of the 1st, 2nd, and 10th epochs.\n+        validation_freq: Only relevant if validation data is provided. Integer\n+          or `collections.abc.Container` instance (e.g. list, tuple, etc.). If\n+          an integer, specifies how many training epochs to run before a new\n+          validation run is performed, e.g. `validation_freq=2` runs validation\n+          every 2 epochs. If a Container, specifies the epochs on which to run\n+          validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the\n+          end of the 1st, 2nd, and 10th epochs.\n         mode: One of ModeKeys.TRAIN/ModeKeys.TEST/ModeKeys.PREDICT.\n         validation_in_fit: if true, then this method is invoked from within\n           training iteration (for validation). In the case where `val_inputs` is\n@@ -160,15 +161,16 @@ def model_iteration(\n     inputs = input_iterator or inputs\n     if validation_in_fit and prepared_feed_values_from_dataset:\n         # When invoking validation in training loop, avoid creating iterator and\n-        # list of feed values for the same validation dataset multiple times (which\n-        # essentially would call `iterator.get_next()` that slows down execution and\n-        # leads to OOM errors eventually.\n+        # list of feed values for the same validation dataset multiple times\n+        # (which essentially would call `iterator.get_next()` that slows down\n+        # execution and leads to OOM errors eventually.\n         ins = inputs\n     else:\n         ins = _prepare_feed_values(model, inputs, targets, sample_weights, mode)\n-        # `ins` is a function when a distribute strategy is used in Eager mode.  In\n-        # that case `is_dataset` is True.  The code branches that have requirements\n-        # about the type of `ins` do not trigger in the distributed case.\n+        # `ins` is a function when a distribute strategy is used in Eager mode.\n+        # In that case `is_dataset` is True.  The code branches that have\n+        # requirements about the type of `ins` do not trigger in the distributed\n+        # case.\n \n     if not is_dataset:\n         num_samples_or_steps = _get_num_samples_or_steps(\n@@ -177,10 +179,10 @@ def model_iteration(\n     else:\n         num_samples_or_steps = steps_per_epoch\n \n-    # Update sample_weight_mode of the model if sample_weights is specified by the\n-    # user. We need to call this function after we have a handle on the inputs\n-    # (both numpy arrays and datasets) in order to determine if the user has\n-    # specified sample_weights.\n+    # Update sample_weight_mode of the model if sample_weights is specified by\n+    # the user. We need to call this function after we have a handle on the\n+    # inputs (both numpy arrays and datasets) in order to determine if the user\n+    # has specified sample_weights.\n     _update_sample_weight_mode(model, mode, ins)\n \n     # Get step function and loop type. As part of building the execution\n@@ -188,8 +190,8 @@ def model_iteration(\n     # sample_weight_mode value.\n     f = _make_execution_function(model, mode)\n \n-    # Prepare validation data. Hold references to the iterator and the input list\n-    # to properly reinitialize and reuse in multiple validation passes.\n+    # Prepare validation data. Hold references to the iterator and the input\n+    # list to properly reinitialize and reuse in multiple validation passes.\n     val_iterator = None\n     if isinstance(val_inputs, (tf.compat.v1.data.Dataset, tf.data.Dataset)):\n         if validation_steps is None:\n@@ -277,8 +279,8 @@ def model_iteration(\n         # Setup work for each epoch\n         epoch_logs = {}\n         if mode != ModeKeys.PREDICT:\n-            # Collecting and resetting metrics has non-zero cost and will needlessly\n-            # slow down model.predict.\n+            # Collecting and resetting metrics has non-zero cost and will\n+            # needlessly slow down model.predict.\n             model.reset_metrics()\n         if mode == ModeKeys.TRAIN:\n             callbacks.on_epoch_begin(epoch, epoch_logs)\n@@ -299,7 +301,8 @@ def model_iteration(\n \n                 # Get outputs.\n                 try:\n-                    # `ins` can be callable in tf.distribute.Strategy + eager case.\n+                    # `ins` can be callable in tf.distribute.Strategy + eager\n+                    # case.\n                     if not callable(ins) or (\n                         model._distribution_strategy\n                         and not distributed_training_utils_v1.is_distributing_by_cloning(\n@@ -313,31 +316,34 @@ def model_iteration(\n                 except tf.errors.OutOfRangeError:\n                     if is_dataset:\n                         # The dataset passed by the user ran out of batches.\n-                        # Now we know the cardinality of the dataset.\n-                        # If steps_per_epoch was specified, then running out of data is\n-                        # unexpected, so we stop training and inform the user.\n+                        # Now we know the cardinality of the dataset.  If\n+                        # steps_per_epoch was specified, then running out of\n+                        # data is unexpected, so we stop training and inform the\n+                        # user.\n                         if steps_per_epoch:\n                             callbacks.model.stop_training = True\n                             logging.warning(\n-                                \"Your dataset ran out of data; interrupting training. \"\n-                                \"Make sure that your dataset can generate at least \"\n-                                \"`%s * epochs` batches (in this case, %d batches). \"\n-                                \"You may need to use the repeat() function when \"\n-                                \"building your dataset.\"\n+                                \"Your dataset ran out of data; interrupting \"\n+                                \"training. Make sure that your dataset can \"\n+                                \"generate at least `%s * epochs` batches (in \"\n+                                \"this case, %d batches). You may need to use \"\n+                                \"the repeat() function when building your \"\n+                                \"dataset.\"\n                                 % (steps_name, steps_per_epoch * epochs)\n                             )\n                         elif step > 0:\n                             steps_per_epoch = step\n                             aggregator.steps = steps_per_epoch\n                     else:\n-                        # We ran out of batches while the user passed an iterator (legacy).\n+                        # We ran out of batches while the user passed an\n+                        # iterator (legacy).\n                         callbacks.model.stop_training = True\n                         logging.warning(\n                             \"Your dataset iterator ran out of data; \"\n-                            \"interrupting training. Make sure that your iterator \"\n-                            \"can generate at least `%s * epochs` \"\n-                            \"batches (in this case, %d batches). You may need to\"\n-                            \"use the repeat() function when building your \"\n+                            \"interrupting training. Make sure that your \"\n+                            \"iterator can generate at least `%s * epochs` \"\n+                            \"batches (in this case, %d batches). You may need \"\n+                            \"to use the repeat() function when building your \"\n                             \"dataset.\" % (steps_name, steps_per_epoch * epochs)\n                         )\n                     break\n@@ -376,9 +382,9 @@ def model_iteration(\n                 batch_ids = index_array[batch_start:batch_end]\n                 # Slice into a batch.\n                 if len(batches) == 1:\n-                    # If we only have one batch, do not slice. This takes care of\n-                    # composite tensors in non-Dataset modes; we currently don't support\n-                    # slicing them.\n+                    # If we only have one batch, do not slice. This takes care\n+                    # of composite tensors in non-Dataset modes; we currently\n+                    # don't support slicing them.\n                     # TODO(b/133517906): Add slicing support.\n                     ins_batch = ins\n                 else:\n@@ -439,8 +445,9 @@ def model_iteration(\n         ):\n \n             if model._compile_distribution:\n-                # Since we create a new clone from the original model we need to copy\n-                # the weights back to the original model before we can run validation.\n+                # Since we create a new clone from the original model we need to\n+                # copy the weights back to the original model before we can run\n+                # validation.\n                 distributed_training_utils_v1._copy_weights_to_original_model(\n                     model, ModeKeys.TRAIN\n                 )\n@@ -482,7 +489,8 @@ def model_iteration(\n \n     if model._distribution_strategy:\n         if model._compile_distribution:\n-            # TODO(priyag, psv): Copy back metrics to the original model as well?\n+            # TODO(priyag, psv): Copy back metrics to the original model as\n+            # well?\n             distributed_training_utils_v1._copy_weights_to_original_model(\n                 model, mode\n             )\n@@ -518,7 +526,7 @@ def _print_train_info(num_samples_or_steps, val_samples_or_steps, is_dataset):\n \n \n def _get_num_samples_or_steps(ins, batch_size, steps_per_epoch):\n-    \"\"\"Returns total number of samples (when training in batch mode) or steps.\"\"\"\n+    \"\"\"Returns total number of samples when training in batch mode or steps.\"\"\"\n     if steps_per_epoch:\n         return steps_per_epoch\n     return training_utils_v1.check_num_samples(\n@@ -550,10 +558,10 @@ def _prepare_feed_values(model, inputs, targets, sample_weights, mode):\n                 model, inputs, targets, sample_weights, mode\n             )\n \n-        # In the eager case, we want to call the input method per step, so return\n-        # a lambda from here that can be called. Note that this is applicable only\n-        # in Distribution Strategy case as it follows the same code path for both\n-        # eager and graph modes.\n+        # In the eager case, we want to call the input method per step, so\n+        # return a lambda from here that can be called. Note that this is\n+        # applicable only in Distribution Strategy case as it follows the same\n+        # code path for both eager and graph modes.\n         # TODO(priyag,omalleyt): Either we should move the training DS with\n         # IteratorBase to use training_generator code path, or figure out how to\n         # set a symbolic Iterator out of a Dataset when in eager mode.\n@@ -614,7 +622,8 @@ def _make_execution_function(model, mode):\n def _update_sample_weight_mode(model, mode, inputs):\n     \"\"\"Updates the sample_weight_mode of a given model.\"\"\"\n     # Add a quick return to prevent us from calling model._feed_targets that\n-    # accesses certain model properties that may not be set in the `PREDICT` mode.\n+    # accesses certain model properties that may not be set in the `PREDICT`\n+    # mode.\n     if mode == ModeKeys.PREDICT:\n         return\n \n@@ -657,8 +666,8 @@ class ArrayLikeTrainingLoop(training_utils_v1.TrainingLoop):\n \n     This is the default handler for most of the input data types, includes\n     symbolic tensors or Numpy array-like, Datasets and iterators in graph mode\n-    (since they generate symbolic tensors). This Function is used to handle model\n-    with `run_eagerly` = False.\n+    (since they generate symbolic tensors). This Function is used to handle\n+    model with `run_eagerly` = False.\n     \"\"\"\n \n     def fit(\n\n@@ -146,7 +146,8 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n         ):\n             model.fit(dataset, dataset, epochs=1, steps_per_epoch=2, verbose=0)\n \n-        # With an infinite dataset, `steps_per_epoch`/`steps` argument is required.\n+        # With an infinite dataset, `steps_per_epoch`/`steps` argument is\n+        # required.\n         with self.assertRaises(ValueError):\n             model.fit(dataset, epochs=1, verbose=0)\n         with self.assertRaises(ValueError):\n@@ -252,10 +253,10 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n             (inputs, targets, sample_weights)\n         ).batch(2)\n         result = model.evaluate(ds, verbose=1)\n-        # The per sample loss is multiplied by the corresponding sample weight. The\n-        # average of these weighted losses is the return value of the `evaluate`\n-        # call. For example, in the test above the average weighted loss is\n-        # calculated in the following manner:\n+        # The per sample loss is multiplied by the corresponding sample weight.\n+        # The average of these weighted losses is the return value of the\n+        # `evaluate` call. For example, in the test above the average weighted\n+        # loss is calculated in the following manner:\n         # ((2-0)^2) * 0.25 + ((4-1)^2) * 0.5 + ((6-2)^2 * 0.75) + ((8-3)^2 * 1)\n         #  equals 42.5 / 4 = 10.625\n         self.assertEqual(result, 10.625)\n@@ -321,7 +322,8 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n             history.history[\"loss\"],\n             [inputs[:20].sum() / 20, inputs[20:].sum() / 20],\n         )\n-        # The validation dataset will be reset at the end of each validation run.\n+        # The validation dataset will be reset at the end of each validation\n+        # run.\n         self.assertAllClose(\n             history.history[\"val_loss\"],\n             [inputs[:20].sum() / 20, inputs[:20].sum() / 20],\n@@ -357,7 +359,8 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n \n             with self.assertRaisesRegex(\n                 ValueError,\n-                r\"expected (.*?) to have shape \\(3,\\) but got array with shape \\(1,\\)\",\n+                r\"expected (.*?) to have shape \\(3,\\) \"\n+                r\"but got array with shape \\(1,\\)\",\n             ):\n                 model.train_on_batch(dataset)\n \n@@ -498,8 +501,9 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n         with tf.compat.v1.test.mock.patch.object(\n             logging, \"warning\"\n         ) as mock_log:\n-            # steps_per_epoch (200) is greater than the dataset size (100). As this is\n-            # unexpected, training will stop and not make it to the second epoch.\n+            # steps_per_epoch (200) is greater than the dataset size (100). As\n+            # this is unexpected, training will stop and not make it to the\n+            # second epoch.\n             history = model.fit(\n                 dataset,\n                 epochs=2,\n@@ -513,8 +517,8 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n             )\n             self.assertIn(\n                 \"can generate at least \"\n-                \"`steps_per_epoch * epochs` batches (in this case, 400 batches). \"\n-                \"You may need to use the repeat() function when \"\n+                \"`steps_per_epoch * epochs` batches (in this case, \"\n+                \"400 batches). You may need to use the repeat() function when \"\n                 \"building your dataset.\",\n                 str(mock_log.call_args),\n             )\n@@ -557,8 +561,8 @@ class TestTrainingWithDataset(test_combinations.TestCase):\n         )\n \n         # Create eval dataset with generator, so that dataset won't contain the\n-        # overall size metadata. Without eval_steps, we expect to run through all\n-        # the data in this dataset every epoch.\n+        # overall size metadata. Without eval_steps, we expect to run through\n+        # all the data in this dataset every epoch.\n         def gen():\n             for _ in range(100):\n                 yield (\n\n@@ -73,11 +73,12 @@ def _make_train_step_fn(model, mode, strategy, output_labels):\n         else:\n             targets = None\n \n-        # When input feature is a dictionary of tensors, dictionary is flattended\n-        # to an array and passed as a model input. This results in input mismatch\n-        # when model input layer names are not sorted in alphabetical order as\n-        # `nest.flatten()`sorts dictionary elements by keys. As so, transform input\n-        # tensors into an array and order it along `model._feed_input_names`.\n+        # When input feature is a dictionary of tensors, dictionary is\n+        # flattended to an array and passed as a model input. This results in\n+        # input mismatch when model input layer names are not sorted in\n+        # alphabetical order as `nest.flatten()`sorts dictionary elements by\n+        # keys. As so, transform input tensors into an array and order it along\n+        # `model._feed_input_names`.\n         if isinstance(inputs, dict):\n             inputs = [\n                 inputs[input_name] for input_name in model._feed_input_names\n@@ -118,14 +119,14 @@ def _make_train_step_fn(model, mode, strategy, output_labels):\n             if label == \"loss\":\n                 reduce_op = tf.distribute.ReduceOp.SUM\n             else:\n-                # We reduce all other metrics using mean for now. This is temporary\n-                # workaround until new metrics are in place.\n+                # We reduce all other metrics using mean for now. This is\n+                # temporary workaround until new metrics are in place.\n                 reduce_op = tf.distribute.ReduceOp.MEAN\n             ctx.set_last_step_output(label, output, reduce_op)\n \n-        # TODO(priyag, sourabhbajaj): Ignoring these things from the combined_fn:\n-        # feed_dict, session kwargs, run options, run_metadata for now. These should\n-        # be handled appropriately\n+        # TODO(priyag, sourabhbajaj): Ignoring these things from the\n+        # combined_fn: feed_dict, session kwargs, run options, run_metadata for\n+        # now. These should be handled appropriately\n         return combined_fn.updates_op\n \n     return _step_fn\n@@ -160,9 +161,9 @@ def experimental_tpu_fit_loop(\n         validation_steps: Number of steps to run validation for\n             (only if doing validation from data tensors).\n             Ignored with the default value of `None`.\n-        validation_freq: Only relevant if validation data is provided. Integer or\n-            `collections.abc.Container` instance (e.g. list, tuple, etc.). If an\n-            integer, specifies how many training epochs to run before a new\n+        validation_freq: Only relevant if validation data is provided. Integer\n+            or `collections.abc.Container` instance (e.g. list, tuple, etc.). If\n+            an integer, specifies how many training epochs to run before a new\n             validation run is performed, e.g. `validation_freq=2` runs\n             validation every 2 epochs. If a Container, specifies the epochs on\n             which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n@@ -291,13 +292,14 @@ def experimental_tpu_fit_loop(\n             logging.info(\"Running validation at fit epoch: %s\", epoch)\n \n             if model._compile_distribution:\n-                # Since we create a new clone from the original model we need to copy\n-                # the weights back to the original model before we can run validation.\n+                # Since we create a new clone from the original model we need to\n+                # copy the weights back to the original model before we can run\n+                # validation.\n                 dist_utils._copy_weights_to_original_model(\n                     model, ModeKeys.TRAIN\n                 )\n \n-            val_outs = experimental_tpu_test_loop(  # pylint: disable=undefined-variable\n+            val_outs = experimental_tpu_test_loop(\n                 model,\n                 val_dataset,\n                 steps=validation_steps,\n@@ -442,7 +444,8 @@ def experimental_tpu_test_loop(\n                 # Loss is stateless metrics.\n                 outs[i] += batch_outs[label]\n             else:\n-                # For all stateful metrics, the aggregation is handled by mirrored vars.\n+                # For all stateful metrics, the aggregation is handled by\n+                # mirrored vars.\n                 outs[i] = batch_outs[label]\n \n         batch_logs = cbks.make_logs(model, batch_logs, outs, mode)\n@@ -602,7 +605,8 @@ def experimental_tpu_predict_loop(\n             )\n             break\n \n-        # TODO(priyag): maybe need to unwrap the outputs first for MirroredStrategy.\n+        # TODO(priyag): maybe need to unwrap the outputs first for\n+        # MirroredStrategy.\n         for i in range(num_model_outputs):\n             output_start_index = i * current_strategy.num_replicas_in_sync\n             output_end_index = (\n@@ -880,7 +884,7 @@ class DistributionSingleWorkerTrainingLoop(training_utils_v1.TrainingLoop):\n \n \n def _train_with_multi_worker(method):\n-    \"\"\"Decorator that handles multi worker training with distribution strategy.\"\"\"\n+    \"\"\"Decorator handles multi worker training with distribution strategy.\"\"\"\n \n     def wrapper(model, **kwargs):\n         def _worker_fn(_):\n\n@@ -30,7 +30,8 @@ class TrainingTest(test_combinations.TestCase):\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_dynamic_model_has_trainable_weights(self):\n         if not tf.executing_eagerly():\n-            # Only test Eager modes, as Graph mode is not relevant for dynamic models.\n+            # Only test Eager modes, as Graph mode is not relevant for dynamic\n+            # models.\n             return\n \n         class DynamicModel(keras.Model):\n@@ -49,8 +50,8 @@ class TrainingTest(test_combinations.TestCase):\n         self.assertEqual(hist.history[\"loss\"][-1], 1)\n         self.assertEqual(len(model.trainable_weights), 2)\n         loss = model.train_on_batch(np.zeros((1, 1)), np.zeros((1, 1)))\n-        # The loss must have been updated if the trainable weights are taken into\n-        # account during tracking.\n+        # The loss must have been updated if the trainable weights are taken\n+        # into account during tracking.\n         self.assertLess(loss, 1)\n \n     @test_combinations.run_with_all_model_types(exclude_models=\"sequential\")\n@@ -354,7 +355,7 @@ class CorrectnessTest(test_combinations.TestCase):\n     def test_nested_model_learning_phase(\n         self, training, expected_training_loss, expected_validation_loss\n     ):\n-        \"\"\"Tests that learning phase is correctly set in an intermediate layer.\"\"\"\n+        \"\"\"Tests learning phase is correctly set in an intermediate layer.\"\"\"\n \n         def _make_unregularized_model():\n             inputs = keras.Input((4,))\n\n@@ -53,8 +53,8 @@ def _eager_metrics_fn(model, outputs, targets, sample_weights=None, masks=None):\n     # Invoke all(weighted and unweighted) metrics.\n     metric_results = []\n     if targets:\n-        # Insert None values corresponding to the targets that need to be skipped\n-        # on the model.\n+        # Insert None values corresponding to the targets that need to be\n+        # skipped on the model.\n         if len(model._targets) != len(targets):\n             new_targets = [\n                 None if t is None else targets.pop(0) for t in model._targets\n@@ -103,8 +103,8 @@ def _model_loss(\n \n     Returns:\n        Returns the model output, total loss, loss value calculated using the\n-       specified loss function and masks for each output. The total loss includes\n-       regularization losses and applies masking and sample weighting\n+       specified loss function and masks for each output. The total loss\n+       includes regularization losses and applies masking and sample weighting\n        to the loss value.\n     \"\"\"\n     # TODO(psv): Dedup code here with graph mode prepare_total_loss() fn.\n@@ -176,7 +176,8 @@ def _model_loss(\n                     if weights is None:\n                         weights = mask\n                     else:\n-                        # Update dimensions of weights to match with mask if possible.\n+                        # Update dimensions of weights to match with mask if\n+                        # possible.\n                         weights = tf.cast(weights, outs[i].dtype)\n                         (\n                             mask,\n@@ -196,8 +197,8 @@ def _model_loss(\n                     )\n                     loss_reduction = loss_fn.reduction\n \n-                    # `AUTO` loss reduction defaults to `SUM_OVER_BATCH_SIZE` for all\n-                    # compile use cases.\n+                    # `AUTO` loss reduction defaults to `SUM_OVER_BATCH_SIZE`\n+                    # for all compile use cases.\n                     if loss_reduction == losses_utils.ReductionV2.AUTO:\n                         loss_reduction = (\n                             losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\n@@ -212,7 +213,8 @@ def _model_loss(\n                     # Here we assume that the class takes care of loss reduction\n                     # because if this class returns a vector value we cannot\n                     # differentiate between use case where a custom optimizer\n-                    # expects a vector loss value vs unreduced per-sample loss value.\n+                    # expects a vector loss value vs unreduced per-sample loss\n+                    # value.\n                     output_loss = loss_fn(\n                         targets[i], outs[i], sample_weight=weights\n                     )\n@@ -222,8 +224,8 @@ def _model_loss(\n \n             # If the number of outputs is 1 then we don't append the loss metric\n             # associated with each model output. When there are multiple outputs\n-            # associated with a model, each output's loss is calculated and returned\n-            # as part of the loss_metrics.\n+            # associated with a model, each output's loss is calculated and\n+            # returned as part of the loss_metrics.\n             if len(model.outputs) > 1:\n                 # Keep track of the stateful output loss result.\n                 output_losses.append(output_loss_metrics[i](output_loss))\n@@ -263,9 +265,9 @@ def _process_single_batch(\n         output_loss_metrics: List of metrics that are used to aggregated output\n           loss values.\n         sample_weights: Optional list of sample weight arrays.\n-        training: The boolean represents if the weights of the model are updated.\n-                'fit' methods will set this to True while 'evaluate' methods will\n-                set this to False.\n+        training: The boolean represents if the weights of the model are\n+          updated. 'fit' methods will set this to True while 'evaluate' methods\n+          will set this to False.\n \n     Returns:\n         output of the model, total loss, the loss and the mask\n@@ -295,8 +297,8 @@ def _process_single_batch(\n         if training:\n             trainable_weights = model.trainable_weights\n             if trainable_weights:\n-                # TODO(tanzheny) b/132690565: Provide mechanism for user to override\n-                # model.train_on_batch.\n+                # TODO(tanzheny) b/132690565: Provide mechanism for user to\n+                # override model.train_on_batch.\n                 if hasattr(model, \"_backwards\"):\n                     model._backwards(tape, scaled_total_loss)\n                 else:\n\n@@ -76,19 +76,20 @@ def model_iteration(\n           `keras.utils.data_utils.Sequence` object or Eager Iterator or Dataset.\n         validation_steps: Total number of steps (batches of samples) before\n           declaring validation finished.\n-        validation_freq: Only relevant if validation data is provided. Integer or\n-          `collections.abc.Container` instance (e.g. list, tuple, etc.). If an\n-          integer, specifies how many training epochs to run before a new\n-          validation run is performed, e.g. `validation_freq=2` runs\n-          validation every 2 epochs. If a Container, specifies the epochs on\n-          which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n-          validation at the end of the 1st, 2nd, and 10th epochs.\n-        class_weight: Dictionary mapping class indices to a weight for the class.\n+        validation_freq: Only relevant if validation data is provided. Integer\n+          or `collections.abc.Container` instance (e.g. list, tuple, etc.). If\n+          an integer, specifies how many training epochs to run before a new\n+          validation run is performed, e.g. `validation_freq=2` runs validation\n+          every 2 epochs. If a Container, specifies the epochs on which to run\n+          validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the\n+          end of the 1st, 2nd, and 10th epochs.\n+        class_weight: Dictionary mapping class indices to a weight for the\n+            class.\n         max_queue_size: Integer. Maximum size for the generator queue. If\n           unspecified, `max_queue_size` will default to 10.\n         workers: Integer. Maximum number of processes to spin up when using\n-          process-based threading. If unspecified, `workers` will default to 1. If\n-          0, will execute the generator on the main thread.\n+          process-based threading. If unspecified, `workers` will default to 1.\n+          If 0, will execute the generator on the main thread.\n         use_multiprocessing: Boolean. If `True`, use process-based threading. If\n           unspecified, `use_multiprocessing` will default to `False`. Note that\n           because this implementation relies on multiprocessing, you should not\n@@ -238,25 +239,26 @@ def model_iteration(\n             batch_data = _get_next_batch(generator)\n             if batch_data is None:\n                 if is_dataset:\n-                    # The dataset passed by the user ran out of batches.\n-                    # Now we know the cardinality of the dataset.\n-                    # If steps_per_epoch was specified, then running out of data is\n-                    # unexpected, so we stop training and inform the user.\n+                    # The dataset passed by the user ran out of batches.  Now we\n+                    # know the cardinality of the dataset.  If steps_per_epoch\n+                    # was specified, then running out of data is unexpected, so\n+                    # we stop training and inform the user.\n                     if steps_per_epoch:\n                         callbacks.model.stop_training = True\n                         logging.warning(\n-                            \"Your dataset ran out of data; interrupting training. \"\n-                            \"Make sure that your dataset can generate at least \"\n-                            \"`%s * epochs` batches (in this case, %d batches). \"\n-                            \"You may need to use the repeat() function when \"\n-                            \"building your dataset.\"\n+                            \"Your dataset ran out of data; interrupting \"\n+                            \"training. Make sure that your dataset can \"\n+                            \"generate at least `%s * epochs` batches (in \"\n+                            \"this case, %d batches). You may need to use \"\n+                            \"the repeat() function when building your dataset.\"\n                             % (steps_name, steps_per_epoch * epochs)\n                         )\n                     elif step > 0:\n                         steps_per_epoch = step\n                         aggregator.steps = steps_per_epoch\n                 else:\n-                    # We ran out of batches while the user passed an iterator (legacy).\n+                    # We ran out of batches while the user passed an iterator\n+                    # (legacy).\n                     callbacks.model.stop_training = True\n                     logging.warning(\n                         \"Your dataset iterator ran out of data; \"\n@@ -285,8 +287,9 @@ def model_iteration(\n                 aggregator.create(batch_outs)\n \n                 if is_deferred:\n-                    # Set callbacks params. We do this here when model is compiled only\n-                    # in the first iteration of this loop (deferred build scenario).\n+                    # Set callbacks params. We do this here when model is\n+                    # compiled only in the first iteration of this loop\n+                    # (deferred build scenario).\n                     cbks.set_callback_parameters(\n                         callbacks,\n                         model,\n@@ -417,17 +420,17 @@ def _validate_arguments(\n       is_dataset: Boolean, whether data is a dataset instance.\n       use_multiprocessing: Boolean. If `True`, use process-based threading. If\n         unspecified, `use_multiprocessing` will default to `False`. Note that\n-        because this implementation relies on multiprocessing, you should not pass\n-        non-picklable arguments to the generator as they can't be passed easily to\n-        children processes.\n+        because this implementation relies on multiprocessing, you should not\n+        pass non-picklable arguments to the generator as they can't be passed\n+        easily to children processes.\n       workers: Integer. Maximum number of processes to spin up when using\n         process-based threading. If unspecified, `workers` will default to 1. If\n         0, will execute the generator on the main thread.\n-      steps_per_epoch: Total number of steps (batches of samples) before declaring\n-        one epoch finished and starting the next epoch. Ignored with the default\n-        value of `None`.\n-      validation_data: Either a tuple of NumPy/Tensor inputs (i.e. `(x,)` or `(x,\n-        y)` or `(x, y, sample_weights)`) or a generator or\n+      steps_per_epoch: Total number of steps (batches of samples) before\n+        declaring one epoch finished and starting the next epoch. Ignored with\n+        the default value of `None`.\n+      validation_data: Either a tuple of NumPy/Tensor inputs (i.e. `(x,)` or\n+        `(x, y)` or `(x, y, sample_weights)`) or a generator or\n         `keras.utils.data_utils.Sequence` object or Eager Iterator or Dataset.\n       validation_steps: Total number of steps (batches of samples) before\n         declaring validation finished.\n@@ -481,11 +484,11 @@ def convert_to_generator_like(\n \n     Args:\n       data: Either a generator or `keras.utils.data_utils.Sequence` object or\n-        `Dataset`, `Iterator`, or a {1,2,3}-tuple of NumPy arrays or EagerTensors.\n-        If a tuple, the elements represent `(x, y, sample_weights)` and may be\n-        `None` or `[None]`.\n-      batch_size: Used when creating a generator out of tuples of NumPy arrays or\n-        EagerTensors.\n+        `Dataset`, `Iterator`, or a {1,2,3}-tuple of NumPy arrays or\n+        EagerTensors.  If a tuple, the elements represent `(x, y,\n+        sample_weights)` and may be `None` or `[None]`.\n+      batch_size: Used when creating a generator out of tuples of NumPy arrays\n+        or EagerTensors.\n       steps_per_epoch: Steps of the generator to run each epoch. If `None` the\n         number of steps will be read from the data (for\n         `keras.utils.data_utils.Sequence` types).\n@@ -500,7 +503,8 @@ def convert_to_generator_like(\n         inputs.\n     \"\"\"\n     if isinstance(data, tuple):\n-        # Scrub `Nones` that might have been passed for `targets`, `sample_weights`.\n+        # Scrub `Nones` that might have been passed for `targets`,\n+        # `sample_weights`.\n         data = tuple(\n             ele\n             for ele in data\n@@ -522,7 +526,8 @@ def convert_to_generator_like(\n     if batch_size is None:\n         raise ValueError(\n             \"When passing input data as arrays, do not specify \"\n-            \"`steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\"\n+            \"`steps_per_epoch`/`steps` argument. \"\n+            \"Please use `batch_size` instead.\"\n         )\n     steps_per_epoch = int(math.ceil(num_samples / batch_size))\n \n@@ -598,7 +603,8 @@ def _make_execution_function(model, mode, class_weight=None):\n \n \n def _get_num_samples_or_steps(data, steps_per_epoch):\n-    \"\"\"Returns number of samples or steps, and whether to use steps count mode.\"\"\"\n+    \"\"\"Returns number of samples or steps, and whether to use steps count\n+    mode.\"\"\"\n     flat_inputs = tf.nest.flatten(data)\n     if hasattr(flat_inputs[0], \"shape\"):\n         return int(flat_inputs[0].shape[0]), False\n@@ -610,9 +616,9 @@ class GeneratorOrSequenceTrainingLoop(training_utils_v1.TrainingLoop):\n \n     Input is Python generator, or Sequence object.\n \n-    The difference between this class and `GeneratorLikeTrainingFunction` is that\n-    this class only handles inputs that with x, y and sample_weight fused into one\n-    param.\n+    The difference between this class and `GeneratorLikeTrainingFunction` is\n+    that this class only handles inputs that with x, y and sample_weight fused\n+    into one param.\n     \"\"\"\n \n     def fit(\n@@ -813,8 +819,8 @@ class GeneratorLikeTrainingLoop(training_utils_v1.TrainingLoop):\n \n     This is the default handler for most of the input data types, includes\n     symbolic tensors or Numpy array-like, Datasets and iterators in graph mode\n-    (since they generate symbolic tensors). This Function is used to handle model\n-    with `run_eagerly` = True.\n+    (since they generate symbolic tensors). This Function is used to handle\n+    model with `run_eagerly` = True.\n     \"\"\"\n \n     def fit(\n\n@@ -35,8 +35,8 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n \n         Tests `sparse_categorical_crossentropy`, `categorical_crossentropy`,\n         and `binary_crossentropy`.\n-        Verifies that evaluate gives the same result with either `channels_first`\n-        or `channels_last` image_data_format.\n+        Verifies that evaluate gives the same result with either\n+        `channels_first` or `channels_last` image_data_format.\n         \"\"\"\n \n         def prepare_simple_model(input_tensor, loss_name, target):\n@@ -45,19 +45,19 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n             num_channels = None\n             activation = None\n             if loss_name == \"sparse_categorical_crossentropy\":\n-                loss = lambda y_true, y_pred: backend.sparse_categorical_crossentropy(  # pylint: disable=g-long-lambda\n+                loss = lambda y_true, y_pred: backend.sparse_categorical_crossentropy(\n                     y_true, y_pred, axis=axis\n                 )\n                 num_channels = int(np.amax(target) + 1)\n                 activation = \"softmax\"\n             elif loss_name == \"categorical_crossentropy\":\n-                loss = lambda y_true, y_pred: backend.categorical_crossentropy(  # pylint: disable=g-long-lambda\n+                loss = lambda y_true, y_pred: backend.categorical_crossentropy(\n                     y_true, y_pred, axis=axis\n                 )\n                 num_channels = target.shape[axis]\n                 activation = \"softmax\"\n             elif loss_name == \"binary_crossentropy\":\n-                loss = lambda y_true, y_pred: backend.binary_crossentropy(  # pylint: disable=g-long-lambda, unnecessary-lambda\n+                loss = lambda y_true, y_pred: backend.binary_crossentropy(\n                     y_true, y_pred\n                 )\n                 num_channels = target.shape[axis]\n@@ -88,8 +88,9 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n                     [[[[8.0, 7.1, 0.0], [4.5, 2.6, 0.55], [0.9, 4.2, 11.2]]]],\n                     dtype=np.float32,\n                 )\n-                # Labels for testing 4-class sparse_categorical_crossentropy, 4-class\n-                # categorical_crossentropy, and 2-class binary_crossentropy:\n+                # Labels for testing 4-class sparse_categorical_crossentropy,\n+                # 4-class categorical_crossentropy, and 2-class\n+                # binary_crossentropy:\n                 labels_channels_first = [\n                     np.array(\n                         [[[[0, 1, 3], [2, 1, 0], [2, 2, 1]]]], dtype=np.float32\n@@ -115,14 +116,15 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n                         dtype=np.float32,\n                     ),\n                 ]  # pylint: disable=line-too-long\n-                # Compute one loss for each loss function in the list `losses_to_test`:\n+                # Compute one loss for each loss function in the list\n+                # `losses_to_test`:\n                 loss_channels_last = [0.0, 0.0, 0.0]\n                 loss_channels_first = [0.0, 0.0, 0.0]\n \n                 old_data_format = backend.image_data_format()\n \n-                # Evaluate a simple network with channels last, with all three loss\n-                # functions:\n+                # Evaluate a simple network with channels last, with all three\n+                # loss functions:\n                 backend.set_image_data_format(\"channels_last\")\n                 data = np.moveaxis(data_channels_first, 1, -1)\n                 for index, loss_function in enumerate(losses_to_test):\n@@ -133,8 +135,8 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n                         x=data, y=labels, batch_size=1, verbose=0\n                     )\n \n-                # Evaluate the same network with channels first, with all three loss\n-                # functions:\n+                # Evaluate the same network with channels first, with all three\n+                # loss functions:\n                 backend.set_image_data_format(\"channels_first\")\n                 data = data_channels_first\n                 for index, loss_function in enumerate(losses_to_test):\n\n@@ -28,7 +28,7 @@ from keras.testing_infra import test_utils\n \n \n def _conv2d_filter(**kwargs):\n-    \"\"\"Convolution with non-default strides and dilation rate is not supported.\"\"\"\n+    \"\"\"Conv with non-default strides and dilation rate is not supported.\"\"\"\n     return kwargs[\"strides\"] <= 1 or kwargs[\"dilation_rate\"] <= 1\n \n \n\n@@ -162,8 +162,8 @@ class TrainingTest(test_combinations.TestCase):\n                 \"sgd\", loss=\"mse\", run_eagerly=False, jit_compile=True\n             )\n             # Added a string op unsupported by XLA compiler to make sure that an\n-            # error is thrown, This ensures that the graph is indeed being compiled\n-            # using XLA\n+            # error is thrown, This ensures that the graph is indeed being\n+            # compiled using XLA\n             with self.assertRaisesRegex(\n                 tf.errors.InvalidArgumentError, \"Graph execution error\"\n             ):\n@@ -627,7 +627,8 @@ class TrainingTest(test_combinations.TestCase):\n         if tf.executing_eagerly():\n             # In TF2 to avoid any ambiguity when there are nested lists\n             # the entire input gets converted to a\n-            # single numpy array (& it only works in the case of a single io model)\n+            # single numpy array (& it only works in the case of a single io\n+            # model)\n             model.fit(\n                 np.ndarray.tolist(input_a_np),\n                 np.ndarray.tolist(input_b_np),\n@@ -636,13 +637,14 @@ class TrainingTest(test_combinations.TestCase):\n                 verbose=2,\n             )\n         else:\n-            # In TF1 there was logic to try disambiguating between the individual\n-            # inputs when lists are nested. This allowed multi-io functional models\n-            # to support lists of scalars as input, but it caused ambiguity issues\n-            # for subclass models & made it trickier to pass multi-dimensional inputs\n-            # as lists of scalars to single io models. This was an excessive amount\n-            # of complexity for what boiled down to a convenience method we were\n-            # mainly just using for writing tests.\n+            # In TF1 there was logic to try disambiguating between the\n+            # individual inputs when lists are nested. This allowed multi-io\n+            # functional models to support lists of scalars as input, but it\n+            # caused ambiguity issues for subclass models & made it trickier to\n+            # pass multi-dimensional inputs as lists of scalars to single io\n+            # models. This was an excessive amount of complexity for what boiled\n+            # down to a convenience method we were mainly just using for writing\n+            # tests.\n             model.fit(\n                 [np.ndarray.tolist(input_a_np)],\n                 [np.ndarray.tolist(input_b_np)],\n@@ -971,8 +973,9 @@ class TrainingTest(test_combinations.TestCase):\n     def test_weight_deduplication(self):\n         class WatchingLayer(layers_module.Layer):\n             def __init__(self, dense_to_track):\n-                # This will cause the kernel and bias to be double counted, effectively\n-                # doubling the learning rate if weights are not deduped.\n+                # This will cause the kernel and bias to be double counted,\n+                # effectively doubling the learning rate if weights are not\n+                # deduped.\n                 self._kernel = dense_to_track.kernel\n                 self._bias = dense_to_track.bias\n                 super().__init__()\n@@ -987,9 +990,9 @@ class TrainingTest(test_combinations.TestCase):\n \n         model = training_module.Model(inp, output)\n \n-        # 0.25 is the edge of the radius of convergence for the double apply case.\n-        # At lr=0.24, the double apply case will very slowly descend while the\n-        # correct case will drop very quickly.\n+        # 0.25 is the edge of the radius of convergence for the double apply\n+        # case. At lr=0.24, the double apply case will very slowly descend\n+        # while the correct case will drop very quickly.\n         model.compile(\n             loss=\"mse\",\n             optimizer=optimizer_v2.gradient_descent.SGD(0.24),\n@@ -1101,8 +1104,8 @@ class TrainingTest(test_combinations.TestCase):\n         for v, w in zip(model.trainable_variables, [v1, v2, v5, v6]):\n             self.assertIs(v, w)\n         self.assertEqual(len(model.non_trainable_variables), 0)\n-        # Make sure losses, layers, and updates aren't broken by having a Template\n-        # in the mix, which does not expose any updates or losses.\n+        # Make sure losses, layers, and updates aren't broken by having a\n+        # Template in the mix, which does not expose any updates or losses.\n         self.assertEqual([], model.layers)\n         self.assertEqual([], model.updates)\n         self.assertEqual([], model.losses)\n@@ -1524,8 +1527,8 @@ class TrainingTest(test_combinations.TestCase):\n         class AssertTypeLayer(layers_module.Layer):\n             def call(self, inputs):\n                 assert inputs.dtype.name == self.dtype, (\n-                    \"Input tensor has type %s which does not match assert type %s\"\n-                    % (inputs.dtype.name, self.assert_type)\n+                    \"Input tensor has type %s which does not match assert \"\n+                    \"type %s\" % (inputs.dtype.name, self.assert_type)\n                 )\n                 return inputs + 1.0\n \n@@ -2151,8 +2154,8 @@ class TrainingTest(test_combinations.TestCase):\n         input_3 = layers_module.Input((3,), batch_size=3)\n         output = model(input_1, input_2, keyword_input=input_3, training=True)\n         functional = training_module.Model([input_1, input_2, input_3], output)\n-        # Functional models should ignore dynamic_batch if the input layers have a\n-        # known batch size.\n+        # Functional models should ignore dynamic_batch if the input layers have\n+        # a known batch size.\n         spec = functional.save_spec(dynamic_batch=True)\n         input_specs = spec[0][0]\n         self.assertEqual(input_specs[0].shape.as_list(), [1, 1])\n@@ -2561,7 +2564,8 @@ class LossWeightingTest(test_combinations.TestCase):\n             )\n             model.fit(x, y, epochs=1, batch_size=10)\n \n-            # sample_weight_mode is a not a list/dict and mode value is `temporal`\n+            # sample_weight_mode is a not a list/dict and mode value is\n+            # `temporal`\n             model.compile(\n                 optimizer,\n                 loss=\"mse\",\n@@ -4164,8 +4168,8 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n                 eval_result = self.model.evaluate(val_ds_2)\n                 if abs(eval_result) > 1e-7:\n                     raise AssertionError(\n-                        \"Expected to hit the zeros dataset but got high loss value of %s\"\n-                        % eval_result\n+                        \"Expected to hit the zeros dataset but got high loss \"\n+                        \"value of %s\" % eval_result\n                     )\n \n         history = model.fit(\n\n@@ -31,8 +31,8 @@ def slice_arrays(arrays, indices, contiguous=True):\n \n     Args:\n       arrays: Single array or list of arrays.\n-      indices: List of indices in the array that should be included in the output\n-        batch.\n+      indices: List of indices in the array that should be included in the\n+        output batch.\n       contiguous: Boolean flag indicating whether the indices are contiguous.\n \n     Returns:\n@@ -136,8 +136,8 @@ class RespectCompiledTrainableState:\n     respect this requirement, it may be necessary to set the trainable value of\n     layers to their compile time values before beginning a training endpoint and\n     restore the values before returning from said endpoint. This scope checks if\n-    any layer's trainable state has changed since Model compile, and performs this\n-    set and un-set bookkeeping.\n+    any layer's trainable state has changed since Model compile, and performs\n+    this set and un-set bookkeeping.\n \n     However, the trainable state of a layer changes quite infrequently, if ever,\n     for many kinds of workflows. Moreover, updating every layer in a model is an\n@@ -159,7 +159,8 @@ class RespectCompiledTrainableState:\n             self._model._compiled_trainable_state\n         )  # pylint: disable=protected-access\n \n-        # Check to see if any layer's trainable state has changed since `compile`.\n+        # Check to see if any layer's trainable state has changed since\n+        # `compile`.\n         for layer, trainable in self._compiled_trainable_state.items():\n             if (\n                 layer in self._current_trainable_state\n\n@@ -57,8 +57,8 @@ class Aggregator(object, metaclass=abc.ABCMeta):\n       use_steps: Whether the loop is using `step` or `batch_size`.\n       num_samples: Total number of samples: `batch_size * num_batches`.\n       steps: Total number of steps.\n-      batch_size: Batch size. It is used for validation checks between inputs and\n-        outputs.\n+      batch_size: Batch size. It is used for validation checks between inputs\n+        and outputs.\n       results: What to return at the end of the aggregation loop.\n     \"\"\"\n \n@@ -86,10 +86,10 @@ class Aggregator(object, metaclass=abc.ABCMeta):\n \n         Args:\n           batch_outs: A list of batch-level outputs.\n-          batch_start: The start index of this batch. Always `None` if `use_steps`\n+          batch_start: The start index of this batch. Always `None` if\n+            `use_steps` is `True`.\n+          batch_end: The end index of this batch. Always `None` if `use_steps`\n             is `True`.\n-          batch_end: The end index of this batch. Always `None` if `use_steps` is\n-            `True`.\n         \"\"\"\n         raise NotImplementedError(\"Must be implemented in subclasses.\")\n \n@@ -137,7 +137,8 @@ class MetricsAggregator(Aggregator):\n \n def _append_sparse_tensor_value(target, to_append):\n     \"\"\"Append sparse tensor value objects.\"\"\"\n-    # Make sure the sparse tensors are of the same size (except for the 0th dim).\n+    # Make sure the sparse tensors are of the same size (except for the 0th\n+    # dim).\n     if len(target.dense_shape) != len(to_append.dense_shape):\n         raise RuntimeError(\n             \"Unable to concatenate %s and %s. The inner dense shapes do not \"\n@@ -163,11 +164,11 @@ def _append_sparse_tensor_value(target, to_append):\n     max_dim0_value = target.dense_shape[0]\n     new_indices = target.indices\n     for index in to_append.indices:\n-        # Here, we iterate through the sparse indices of the tensor to append. For\n-        # each index, we update its zeroth value (the batch index) by adding the\n-        # number of batch items in the tensor we are appending to (so an index\n-        # of [0, 0, 1] for a value that is being appended to a tensor with 0th dim\n-        # size 3 would become [3, 0, 1].)\n+        # Here, we iterate through the sparse indices of the tensor to append.\n+        # For each index, we update its zeroth value (the batch index) by adding\n+        # the number of batch items in the tensor we are appending to (so an\n+        # index of [0, 0, 1] for a value that is being appended to a tensor with\n+        # 0th dim size 3 would become [3, 0, 1].)\n         index[0] += base_dim0_value\n         max_dim0_value = max(max_dim0_value, index[0])\n         new_indices = np.append(new_indices, [index], axis=0)\n@@ -286,8 +287,9 @@ class ConcatAggregator(Aggregator):\n         # #samples is < batch size and != input batch #samples.\n         if self.batch_size and self.batch_size < batch_element.shape[0]:\n             raise ValueError(\n-                \"Mismatch between expected batch size and model output batch size. \"\n-                \"Output shape = {}, expected output shape = shape {}\".format(\n+                \"Mismatch between expected batch size and model output batch \"\n+                \"size. Output shape = {}, \"\n+                \"expected output shape = shape {}\".format(\n                     batch_element.shape,\n                     (self.batch_size,) + batch_element.shape[1:],\n                 )\n@@ -337,19 +339,20 @@ class SliceAggregator(Aggregator):\n     structure of tensor-likes.\n \n     NumPy copies are an operation that threads handle quite well because all of\n-    the heavy lifting is in c and does not need the GIL. Moreover, we can perform\n-    lock-free writes to the same buffer in multiple threads because the nature of\n-    result aggregation guarantees that either the indices are disjoint or the\n-    aggregator will throw an exception in finalize. Moreover, because aggregation\n-    is performed on the slowest varying dimension, assignments for a given batch\n-    will write to contiguous blocks of memory, further minimizing contention.\n+    the heavy lifting is in c and does not need the GIL. Moreover, we can\n+    perform lock-free writes to the same buffer in multiple threads because the\n+    nature of result aggregation guarantees that either the indices are disjoint\n+    or the aggregator will throw an exception in finalize. Moreover, because\n+    aggregation is performed on the slowest varying dimension, assignments for a\n+    given batch will write to contiguous blocks of memory, further minimizing\n+    contention.\n \n     There is, however, some scheduling and context switching overhead which will\n-    offset the gains from pipelining the slice assignment. Below a given threshold\n-    it is faster to simply assign in the main thread rather than enqueue the\n-    assignment in a side thread. The exact threshold will vary from system to\n-    system, but the time is not very sensitive to the exact transition so a value\n-    of 2 ** 14 was chosen which should be reasonable on most systems.\n+    offset the gains from pipelining the slice assignment. Below a given\n+    threshold it is faster to simply assign in the main thread rather than\n+    enqueue the assignment in a side thread. The exact threshold will vary from\n+    system to system, but the time is not very sensitive to the exact transition\n+    so a value of 2 ** 14 was chosen which should be reasonable on most systems.\n     \"\"\"\n \n     _BINARY_SIZE_THRESHOLD = 2**14\n@@ -383,8 +386,9 @@ class SliceAggregator(Aggregator):\n         if batch_end - batch_start == self.num_samples:\n             if self.num_samples != batch_element.shape[0]:\n                 raise ValueError(\n-                    \"Mismatch between expected batch size and model output batch size. \"\n-                    \"Output shape = {}, expected output shape = shape {}\".format(\n+                    \"Mismatch between expected batch size and model \"\n+                    \"output batch size. Output shape = {}, \"\n+                    \"expected output shape = shape {}\".format(\n                         batch_element.shape, self.results.shape\n                     )\n                 )\n@@ -392,8 +396,8 @@ class SliceAggregator(Aggregator):\n             self.results = batch_element\n             return\n \n-        # This is an approximate threshold, so we don't need to consider the number\n-        # of bytes per element.\n+        # This is an approximate threshold, so we don't need to consider the\n+        # number of bytes per element.\n         num_elements = np.prod(batch_element.shape)\n         if num_elements < self._BINARY_SIZE_THRESHOLD:\n             self.results[batch_start:batch_end] = batch_element\n@@ -411,10 +415,10 @@ class SliceAggregator(Aggregator):\n             self.results[batch_start:batch_end] = batch_element\n \n         except Exception as e:  # pylint: disable=broad-except\n-            # `_slice_assign` should only be called in threads and exceptions raised\n-            # in threads do not carry over to the main thread. So instead we perform a\n-            # a broad catch in the thread and then store the exception to be re-raised\n-            # in the main thread.\n+            # `_slice_assign` should only be called in threads and exceptions\n+            # raised in threads do not carry over to the main thread. So instead\n+            # we perform a a broad catch in the thread and then store the\n+            # exception to be re-raised in the main thread.\n             self._errors.append(e)\n \n         finally:\n@@ -450,9 +454,10 @@ class OutputsAggregator(Aggregator):\n \n         for batch_element in batch_outs:\n             if is_composite_or_composite_value(batch_element):\n-                # If the output is not a ndarray, it will be either a composite tensor\n-                # or a composite tensor's Value object. In either case, we can't\n-                # allocate an array to hold the object - we'll handle it later.\n+                # If the output is not a ndarray, it will be either a composite\n+                # tensor or a composite tensor's Value object. In either case,\n+                # we can't allocate an array to hold the object - we'll handle\n+                # it later.\n                 self.results.append(ConcatAggregator(self.batch_size))\n             elif isinstance(batch_element, np.ndarray):\n                 self.results.append(\n@@ -463,8 +468,9 @@ class OutputsAggregator(Aggregator):\n                     )\n                 )\n             else:\n-                # This is not a ndarray, a CompositeTensor, or a CompositeTensorValue.\n-                # Fail fast rather than trying to concatenate it.\n+                # This is not a ndarray, a CompositeTensor, or a\n+                # CompositeTensorValue.  Fail fast rather than trying to\n+                # concatenate it.\n                 raise RuntimeError(\n                     \"Attempted to aggregate unsupported object {}.\".format(\n                         batch_element\n@@ -960,7 +966,8 @@ def collect_per_output_metric_info(\n         metrics: a list or a list of lists or a dict of metric functions.\n         output_names: a list of the names (strings) of model outputs.\n         output_shapes: a list of the shapes (strings) of model outputs.\n-        loss_fns: a list of the loss functions corresponding to the model outputs.\n+        loss_fns: a list of the loss functions corresponding to the model\n+          outputs.\n         from_serialized: whether the model the metrics are being sourced from is\n           being initialized from a serialized format.\n         is_weighted: Boolean indicating whether the given metrics are weighted.\n@@ -1033,14 +1040,15 @@ def collect_per_output_metric_info(\n                 from_serialized  # pylint: disable=protected-access\n             )\n \n-            # If the metric function is not stateful, we create a stateful version.\n+            # If the metric function is not stateful, we create a stateful\n+            # version.\n             if not isinstance(metric_fn, metrics_module.Metric):\n                 metric_fn = metrics_module.MeanMetricWrapper(\n                     metric_fn, name=metric_name\n                 )\n-                # If the metric is being revived from something stateless, such as a\n-                # string (e.g. \"accuracy\"), we may need to later reapply transformations\n-                # such as renaming.\n+                # If the metric is being revived from something stateless, such\n+                # as a string (e.g. \"accuracy\"), we may need to later reapply\n+                # transformations such as renaming.\n                 metric_fn._from_serialized = (\n                     False  # pylint: disable=protected-access\n                 )\n@@ -1087,9 +1095,9 @@ def standardize_weights(\n         y: Numpy array or Tensor of model targets to be weighted.\n         sample_weight: User-provided `sample_weight` argument.\n         class_weight: User-provided `class_weight` argument.\n-        sample_weight_mode: One of `None` or `\"temporal\"`. `\"temporal\"` indicated\n-          that we expect 2D weight data that will be applied to the last 2\n-          dimensions of the targets (i.e. we are weighting timesteps, not\n+        sample_weight_mode: One of `None` or `\"temporal\"`. `\"temporal\"`\n+          indicated that we expect 2D weight data that will be applied to the\n+          last 2 dimensions of the targets (i.e. we are weighting timesteps, not\n           samples).\n \n     Returns:\n@@ -1184,8 +1192,9 @@ def standardize_weights(\n             class_sample_weight = tf.compat.v1.gather(weight_vector, y_classes)\n             tf.debugging.check_numerics(\n                 class_sample_weight,\n-                \"Invalid classes or class weights detected. NaN values indicate that \"\n-                \"an appropriate class weight could not be determined.\",\n+                \"Invalid classes or class weights detected. NaN values \"\n+                \"indicate that an appropriate class weight could not be \"\n+                \"determined.\",\n             )\n             class_sample_weight = tf.cast(class_sample_weight, backend.floatx())\n             if sample_weight is not None:\n@@ -1233,9 +1242,9 @@ def has_symbolic_tensors(ls):\n \n def has_tensors(ls):\n     \"\"\"Returns true if `ls` contains tensors.\"\"\"\n-    # Note: at some point in time ragged tensors didn't count as tensors, so this\n-    # returned false for ragged tensors. Making this return true fails some tests\n-    # which would then require a steps_per_epoch argument.\n+    # Note: at some point in time ragged tensors didn't count as tensors, so\n+    # this returned false for ragged tensors. Making this return true fails some\n+    # tests which would then require a steps_per_epoch argument.\n     if isinstance(ls, (list, tuple)):\n         return any(\n             tf.is_tensor(v) and not isinstance(v, tf.RaggedTensor) for v in ls\n@@ -1259,7 +1268,8 @@ def get_metric_name(metric, weighted=False):\n         The metric name.\n     \"\"\"\n     if tf.__internal__.tf2.enabled():\n-        # We keep the string that the user has set in compile as the metric name.\n+        # We keep the string that the user has set in compile as the metric\n+        # name.\n         if isinstance(metric, str):\n             return metric\n \n@@ -1288,8 +1298,8 @@ def get_metric_function(metric, output_shape=None, loss_fn=None):\n \n     Args:\n         metric: Metric function name or reference.\n-        output_shape: The shape of the output that this metric will be calculated\n-          for.\n+        output_shape: The shape of the output that this metric will be\n+          calculated for.\n         loss_fn: The loss function used.\n \n     Returns:\n@@ -1315,9 +1325,9 @@ def get_metric_function(metric, output_shape=None, loss_fn=None):\n             return metrics_module.binary_accuracy\n         elif is_sparse_categorical_crossentropy:\n             return metrics_module.sparse_categorical_accuracy\n-        # If the output_shape[-1] is not 1, then we know output is `categorical`.\n-        # We assume it is sparse categorical only if loss is explicitly given\n-        # as sparse categorical crossentropy loss.\n+        # If the output_shape[-1] is not 1, then we know output is\n+        # `categorical`.  We assume it is sparse categorical only if loss is\n+        # explicitly given as sparse categorical crossentropy loss.\n         return metrics_module.categorical_accuracy\n     else:\n         if output_shape[-1] == 1 or is_binary_crossentropy:\n@@ -1358,7 +1368,8 @@ def get_loss_function(loss):\n     if tf_inspect.isclass(loss) and issubclass(loss, losses.Loss):\n         # It is not safe to assume that the loss takes no constructor arguments.\n         raise ValueError(\n-            'Received uninstantiated Loss class: {}\\nPlease call loss \"\"classes '\n+            \"Received uninstantiated Loss class: {}\\n\"\n+            \"Please call loss classes \"\n             \"before passing them to Model.compile.\".format(loss)\n         )\n \n@@ -1391,11 +1402,11 @@ def validate_dataset_input(x, y, sample_weight, validation_split=None):\n       x: Input data. A `tf.data` dataset or iterator.\n       y: Target data. It could be either Numpy array(s) or TensorFlow tensor(s).\n         Expected to be `None` when `x` is a dataset iterator.\n-      sample_weight: An optional sample-weight array passed by the user to weight\n-        the importance of each sample in `x`. Expected to be `None` when `x` is a\n-        dataset iterator\n-      validation_split: Float between 0 and 1. Fraction of the training data to be\n-        used as validation data. Expected to be `None` when `x` is a dataset\n+      sample_weight: An optional sample-weight array passed by the user to\n+        weight the importance of each sample in `x`. Expected to be `None` when\n+        `x` is a dataset iterator\n+      validation_split: Float between 0 and 1. Fraction of the training data to\n+        be used as validation data. Expected to be `None` when `x` is a dataset\n         iterator.\n \n     Raises:\n@@ -1432,8 +1443,8 @@ def validate_input_types(inp, orig_inp, allow_dict=True, field_name=\"inputs\"):\n     if isinstance(inp, (list, tuple)):\n         if not all(isinstance(v, np.ndarray) or tf.is_tensor(v) for v in inp):\n             raise ValueError(\n-                \"Please provide as model inputs either a single array or a list of \"\n-                \"arrays. You passed: {}={}\".format(field_name, str(orig_inp))\n+                \"Please provide as model inputs either a single array or a \"\n+                f\"list of arrays. You passed: {field_name}={str(orig_inp)}\"\n             )\n     elif isinstance(inp, dict):\n         if not allow_dict:\n@@ -1655,8 +1666,8 @@ def prepare_loss_functions(loss, output_names):\n         loss: String (name of objective function), objective function or\n           `tf.losses.Loss` instance. See `tf.losses`. If the model has multiple\n           outputs, you can use a different loss on each output by passing a\n-          dictionary or a list of losses. The loss value that will be minimized by\n-          the model will then be the sum of all individual losses.\n+          dictionary or a list of losses. The loss value that will be minimized\n+          by the model will then be the sum of all individual losses.\n         output_names: List of model output names.\n \n     Returns:\n@@ -1673,8 +1684,8 @@ def prepare_loss_functions(loss, output_names):\n             if name not in loss:\n                 logging.warning(\n                     \"Output {0} missing from loss dictionary. We assume \"\n-                    \"this was done on purpose. The fit and evaluate APIs will not be \"\n-                    \"expecting any data to be passed to {0}.\".format(name)\n+                    \"this was done on purpose. The fit and evaluate APIs will \"\n+                    f\"not be expecting any data to be passed to {name}.\"\n                 )\n             loss_functions.append(get_loss_function(loss.get(name, None)))\n     elif isinstance(loss, str):\n@@ -1704,8 +1715,8 @@ def prepare_loss_weights(training_endpoints, loss_weights=None):\n         training_endpoints: List of model training endpoints.\n         loss_weights: Optional list or dictionary specifying scalar coefficients\n           (Python floats) to weight the loss contributions of different model\n-          outputs. The loss value that will be minimized by the model will then be\n-          the *weighted sum* of all individual losses, weighted by the\n+          outputs. The loss value that will be minimized by the model will then\n+          be the *weighted sum* of all individual losses, weighted by the\n           `loss_weights` coefficients. If a list, it is expected to have a 1:1\n           mapping to the model's outputs. If a dict, it is expected to map\n           output names (strings) to scalar coefficients.\n@@ -1880,7 +1891,8 @@ def infer_steps_for_dataset(\n     Args:\n         model: Keras model instance.\n         dataset: Input data of type tf.data.Dataset.\n-        steps: Number of steps to draw from the dataset (may be None if unknown).\n+        steps: Number of steps to draw from the dataset (may be None if\n+          unknown).\n         epochs: Number of times to iterate over the dataset.\n         steps_name: The string name of the steps argument, either `steps`,\n           `validation_steps`, or `steps_per_epoch`. Only used for error message\n@@ -1888,9 +1900,9 @@ def infer_steps_for_dataset(\n \n     Returns:\n       Integer or `None`. Inferred number of steps to loop through the dataset.\n-      `None` is returned if 1) the size of the dataset is unknown and `steps` was\n-      not specified, or 2) this is multi-worker training and auto sharding is\n-      enabled.\n+      `None` is returned if 1) the size of the dataset is unknown and `steps`\n+      was not specified, or 2) this is multi-worker training and auto sharding\n+      is enabled.\n \n     Raises:\n       ValueError: In case of invalid argument values.\n@@ -1901,7 +1913,8 @@ def infer_steps_for_dataset(\n         != tf.data.experimental.AutoShardPolicy.OFF\n     ):\n         # If the dataset would be auto-sharded, we should not infer a local\n-        # steps_per_epoch due to the possible imbalanced sharding between workers.\n+        # steps_per_epoch due to the possible imbalanced sharding between\n+        # workers.\n         return None\n \n     size = backend.get_value(tf.data.experimental.cardinality(dataset))\n@@ -1992,8 +2005,9 @@ class ModelInputs:\n             if isinstance(v, np.ndarray):\n                 # We fix the placeholder shape except the batch size.\n                 # This is suboptimal, but it is the best we can do with the info\n-                # we have. The user should call `model._set_inputs(placeholders)`\n-                # to specify custom placeholders if the need arises.\n+                # we have. The user should call\n+                # `model._set_inputs(placeholders)` to specify custom\n+                # placeholders if the need arises.\n                 shape = (None,) + tuple(v.shape[1:])\n                 if shape == (None,):\n                     shape = (None, 1)\n@@ -2040,9 +2054,9 @@ def should_run_validation(validation_freq, epoch):\n     \"\"\"Checks if validation should be run this epoch.\n \n     Args:\n-      validation_freq: Integer or list. If an integer, specifies how many training\n-        epochs to run before a new validation run is performed. If a list,\n-        specifies the epochs on which to run validation.\n+      validation_freq: Integer or list. If an integer, specifies how many\n+        training epochs to run before a new validation run is performed. If a\n+        list, specifies the epochs on which to run validation.\n       epoch: Integer, the number of the training epoch just completed.\n \n     Returns:\n@@ -2106,9 +2120,9 @@ def unpack_validation_data(validation_data, raise_if_ambiguous=True):\n \n     Args:\n       validation_data: dataset, dataset iterator, or numpy, tensor tuple.\n-      raise_if_ambiguous: boolean on whether to fail if validation_data cannot be\n-        parsed. Otherwise simply return validation_data, None, None and defer the\n-        decision to the caller.\n+      raise_if_ambiguous: boolean on whether to fail if validation_data cannot\n+        be parsed. Otherwise simply return validation_data, None, None and defer\n+        the decision to the caller.\n \n     Returns:\n       tuple of 3, (x, y, sample_weights) for numpy and tensor input.\n@@ -2185,7 +2199,7 @@ class TrainingLoop:\n         steps_per_epoch=None,\n         validation_steps=None,\n         validation_freq=1,\n-        **kwargs\n+        **kwargs,\n     ):\n         \"\"\"Train the model with the inputs and targets.\"\"\"\n         raise NotImplementedError()\n@@ -2200,9 +2214,10 @@ class TrainingLoop:\n         sample_weight=None,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n-        \"\"\"Returns the loss value & metrics values for the model in test mode.\"\"\"\n+        \"\"\"Returns the loss value & metrics values for the model in test\n+        mode.\"\"\"\n         raise NotImplementedError()\n \n     def predict(\n@@ -2213,6 +2228,6 @@ class TrainingLoop:\n         verbose=0,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         raise NotImplementedError()\n\n@@ -130,8 +130,8 @@ class Model(training_lib.Model):\n             self._set_strategy(tf.distribute.get_strategy())\n \n         # This flag is used to track if the user is using the deprecated path of\n-        # passing distribution strategy to compile rather than creating the model\n-        # under distribution strategy scope.\n+        # passing distribution strategy to compile rather than creating the\n+        # model under distribution strategy scope.\n         self._compile_distribution = False\n \n         self._run_eagerly = None\n@@ -167,39 +167,40 @@ class Model(training_lib.Model):\n         \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n \n         If `by_name` is False weights are loaded based on the network's\n-        topology. This means the architecture should be the same as when the weights\n-        were saved.  Note that layers that don't have weights are not taken into\n-        account in the topological ordering, so adding or removing layers is fine as\n-        long as they don't have weights.\n+        topology. This means the architecture should be the same as when the\n+        weights were saved.  Note that layers that don't have weights are not\n+        taken into account in the topological ordering, so adding or removing\n+        layers is fine as long as they don't have weights.\n \n-        If `by_name` is True, weights are loaded into layers only if they share the\n-        same name. This is useful for fine-tuning or transfer-learning models where\n-        some of the layers have changed.\n+        If `by_name` is True, weights are loaded into layers only if they share\n+        the same name. This is useful for fine-tuning or transfer-learning\n+        models where some of the layers have changed.\n \n-        Only topological loading (`by_name=False`) is supported when loading weights\n-        from the TensorFlow format. Note that topological loading differs slightly\n-        between TensorFlow and HDF5 formats for user-defined classes inheriting from\n-        `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n-        TensorFlow format loads based on the object-local names of attributes to\n-        which layers are assigned in the `Model`'s constructor.\n+        Only topological loading (`by_name=False`) is supported when loading\n+        weights from the TensorFlow format. Note that topological loading\n+        differs slightly between TensorFlow and HDF5 formats for user-defined\n+        classes inheriting from `tf.keras.Model`: HDF5 loads based on a\n+        flattened list of weights, while the TensorFlow format loads based on\n+        the object-local names of attributes to which layers are assigned in the\n+        `Model`'s constructor.\n \n         Args:\n-            filepath: String, path to the weights file to load. For weight files in\n-                TensorFlow format, this is the file prefix (the same as was passed\n-                to `save_weights`).\n+            filepath: String, path to the weights file to load. For weight files\n+                in TensorFlow format, this is the file prefix (the same as was\n+                passed to `save_weights`).\n             by_name: Boolean, whether to load weights by name or by topological\n                 order. Only topological loading is supported for weight files in\n                 TensorFlow format.\n-            skip_mismatch: Boolean, whether to skip loading of layers where there is\n-                a mismatch in the number of weights, or a mismatch in the shape of\n-                the weight (only valid when `by_name=True`).\n+            skip_mismatch: Boolean, whether to skip loading of layers where\n+                there is a mismatch in the number of weights, or a mismatch in\n+                the shape of the weight (only valid when `by_name=True`).\n \n         Returns:\n-            When loading a weight file in TensorFlow format, returns the same status\n-            object as `tf.train.Checkpoint.restore`. When graph building, restore\n-            ops are run automatically as soon as the network is built (on first call\n-            for user-defined classes inheriting from `Model`, immediately if it is\n-            already built).\n+            When loading a weight file in TensorFlow format, returns the same\n+            status object as `tf.train.Checkpoint.restore`. When graph building,\n+            restore ops are run automatically as soon as the network is built\n+            (on first call for user-defined classes inheriting from `Model`,\n+            immediately if it is already built).\n \n             When loading weights in HDF5 format, returns `None`.\n \n@@ -238,20 +239,21 @@ class Model(training_lib.Model):\n             optimizer: String (name of optimizer) or optimizer instance.\n                 See `tf.keras.optimizers`.\n             loss: String (name of objective function), objective function or\n-                `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n-                function is any callable with the signature\n+                `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An\n+                objective function is any callable with the signature\n                 `scalar_loss = fn(y_true, y_pred)`. If the model has multiple\n-                outputs, you can use a different loss on each output by passing a\n-                dictionary or a list of losses. The loss value that will be\n+                outputs, you can use a different loss on each output by passing\n+                a dictionary or a list of losses. The loss value that will be\n                 minimized by the model will then be the sum of all individual\n                 losses.\n-            metrics: List of metrics to be evaluated by the model during training\n-                and testing. Typically you will use `metrics=['accuracy']`.\n-                To specify different metrics for different outputs of a\n-                multi-output model, you could also pass a dictionary, such as\n-                `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n-                You can also pass a list (len = len(outputs)) of lists of metrics\n-                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n+            metrics: List of metrics to be evaluated by the model during\n+                training and testing. Typically you will use\n+                `metrics=['accuracy']`.  To specify different metrics for\n+                different outputs of a multi-output model, you could also pass a\n+                dictionary, such as `metrics={'output_a': 'accuracy',\n+                'output_b': ['accuracy', 'mse']}`.  You can also pass a list\n+                (len = len(outputs)) of lists of metrics such as\n+                `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n                 `metrics=['accuracy', ['accuracy', 'mse']]`.\n             loss_weights: Optional list or dictionary specifying scalar\n                 coefficients (Python floats) to weight the loss contributions\n@@ -348,8 +350,9 @@ class Model(training_lib.Model):\n                 or self._experimental_run_tf_function\n             ):\n                 raise ValueError(\n-                    \"Distribute argument in compile is not available in TF 2.0 please \"\n-                    \"create the model under the distribution strategy scope.\"\n+                    \"Distribute argument in compile is not available in TF 2.0 \"\n+                    \"please create the model under the distribution strategy \"\n+                    \"scope.\"\n                 )\n             logging.warning(\n                 \"Distribute argument in compile is deprecated please \"\n@@ -359,10 +362,11 @@ class Model(training_lib.Model):\n             self._compile_distribution = True\n         else:\n             if tf.distribute.has_strategy():\n-                # When the user builds the model in the DS scope and cross replica\n-                # context we want distribution strategy to be set but when building the\n-                # replica copies of the models internally we should not be compiling\n-                # with distribution strategy and use the default compilation path.\n+                # When the user builds the model in the DS scope and cross\n+                # replica context we want distribution strategy to be set but\n+                # when building the replica copies of the models internally we\n+                # should not be compiling with distribution strategy and use the\n+                # default compilation path.\n                 if tf.distribute.in_cross_replica_context():\n                     self._distribution_strategy = tf.distribute.get_strategy()\n \n@@ -391,8 +395,9 @@ class Model(training_lib.Model):\n                 target_tensors,\n                 weighted_metrics,\n             )\n-        # We've disabled automatic dependency tracking for this method, but do want\n-        # to add a checkpoint dependency on the optimizer if it's trackable.\n+        # We've disabled automatic dependency tracking for this method, but do\n+        # want to add a checkpoint dependency on the optimizer if it's\n+        # trackable.\n         if isinstance(self.optimizer, tf.__internal__.tracking.Trackable):\n             self._track_trackable(\n                 self.optimizer, name=\"optimizer\", overwrite=True\n@@ -408,11 +413,12 @@ class Model(training_lib.Model):\n                 \"running a model eagerly.\"\n             )\n \n-        # _training_endpoints contains a list of _TrainingEndpoint object, which has\n-        # all the model output/target/loss and related metadata.\n+        # _training_endpoints contains a list of _TrainingEndpoint object, which\n+        # has all the model output/target/loss and related metadata.\n         self._training_endpoints = []\n \n-        # Used to freeze the behavior of the Model once `compile` has been called.\n+        # Used to freeze the behavior of the Model once `compile` has been\n+        # called.\n         self._compiled_trainable_state = self._get_trainable_state()\n \n         # Set tf.distribute.Strategy specific parameters.\n@@ -426,17 +432,17 @@ class Model(training_lib.Model):\n             not tf.executing_eagerly()\n             and self._distribution_strategy is not None\n         ):\n-            # Ensures a Session is created and configured correctly for Distribution\n-            # Strategy.\n+            # Ensures a Session is created and configured correctly for\n+            # Distribution Strategy.\n             backend.configure_and_create_distributed_session(\n                 self._distribution_strategy\n             )\n         # Initialize model metric attributes.\n         self._init_metric_attributes()\n         if not self.built or not self.inputs or not self.outputs:\n-            # Model is not compilable because it does not know its number of inputs\n-            # and outputs, nor their shapes and names. We will compile after the first\n-            # time the model gets called on training data.\n+            # Model is not compilable because it does not know its number of\n+            # inputs and outputs, nor their shapes and names. We will compile\n+            # after the first time the model gets called on training data.\n             return\n         self._is_compiled = True\n         base_layer.keras_api_gauge.get_cell(\"compile\").set(True)\n@@ -480,7 +486,8 @@ class Model(training_lib.Model):\n                 masks=self._prepare_output_masks(),\n             )\n \n-            # Prepare sample weight modes. List with the same length as model outputs.\n+            # Prepare sample weight modes. List with the same length as model\n+            # outputs.\n             training_utils_v1.prepare_sample_weight_modes(\n                 self._training_endpoints, sample_weight_mode\n             )\n@@ -498,16 +505,18 @@ class Model(training_lib.Model):\n             # Collected trainable weights, sorted in topological order.\n             self._collected_trainable_weights = self.trainable_weights\n \n-            # Validate all variables were correctly created in distribution scope.\n+            # Validate all variables were correctly created in distribution\n+            # scope.\n             if self._distribution_strategy and not self._compile_distribution:\n                 for v in self.variables:\n                     strategy = self._distribution_strategy\n                     if not strategy.extended.variable_created_in_scope(v):\n                         raise ValueError(\n-                            \"Variable (%s) was not created in the distribution strategy \"\n-                            \"scope of (%s). It is most likely due to not all layers or \"\n-                            \"the model or optimizer being created outside the distribution \"\n-                            \"strategy scope. Try to make sure your code looks similar \"\n+                            \"Variable (%s) was not created in the distribution \"\n+                            \"strategy scope of (%s). It is most likely due to \"\n+                            \"not all layers or the model or optimizer being \"\n+                            \"created outside the distribution strategy scope. \"\n+                            \"Try to make sure your code looks similar \"\n                             \"to the following.\\n\"\n                             \"with strategy.scope():\\n\"\n                             \"  model=_create_model()\\n\"\n@@ -521,13 +530,14 @@ class Model(training_lib.Model):\n \n     @property\n     def metrics(self):\n-        \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.\"\"\"\n+        \"\"\"Returns the model's metrics added using `compile`, `add_metric`\n+        APIs.\"\"\"\n         metrics = []\n         if self._is_compiled:\n             if not hasattr(self, \"_v1_compile_was_called\"):\n                 # See b/155687393 for more details, the model is created as a v2\n-                # instance but converted to v1. Fallback to use base Model to retrieve\n-                # the metrics.\n+                # instance but converted to v1. Fallback to use base Model to\n+                # retrieve the metrics.\n                 return super().metrics\n             metrics += self._compile_metric_functions\n         metrics.extend(self._metrics)\n@@ -542,14 +552,14 @@ class Model(training_lib.Model):\n     def metrics_names(self):\n         \"\"\"Returns the model's display labels for all outputs.\"\"\"\n \n-        # This property includes all output names including `loss` and per-output\n-        # losses for backward compatibility.\n+        # This property includes all output names including `loss` and\n+        # per-output losses for backward compatibility.\n         metrics_names = [\"loss\"]\n         if self._is_compiled:\n             if not hasattr(self, \"_v1_compile_was_called\"):\n                 # See b/155687393 for more details, the model is created as a v2\n-                # instance but converted to v1. Fallback to use base Model to retrieve\n-                # the metrics name\n+                # instance but converted to v1. Fallback to use base Model to\n+                # retrieve the metrics name\n                 return super().metrics_names\n \n             # Add output loss metric names to the metric names list.\n@@ -571,8 +581,8 @@ class Model(training_lib.Model):\n         \"\"\"Settable attribute indicating whether the model should run eagerly.\n \n         Running eagerly means that your model will be run step by step,\n-        like Python code. Your model might run slower, but it should become easier\n-        for you to debug it by stepping into individual layer calls.\n+        like Python code. Your model might run slower, but it should become\n+        easier for you to debug it by stepping into individual layer calls.\n \n         By default, we will attempt to compile your model to a static graph to\n         deliver the best execution performance.\n@@ -691,8 +701,8 @@ class Model(training_lib.Model):\n               - A `tf.data` dataset. Should return a tuple\n                 of either `(inputs, targets)` or\n                 `(inputs, targets, sample_weights)`.\n-              - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n-                or `(inputs, targets, sample weights)`.\n+              - A generator or `keras.utils.Sequence` returning `(inputs,\n+                targets)` or `(inputs, targets, sample weights)`.\n             y: Target data. Like the input data `x`,\n               it could be either Numpy array(s) or TensorFlow tensor(s).\n               It should be consistent with `x` (you cannot have Numpy inputs and\n@@ -704,8 +714,8 @@ class Model(training_lib.Model):\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n                 form of symbolic tensors, datasets,\n-                generators, or `keras.utils.Sequence` instances (since they generate\n-                batches).\n+                generators, or `keras.utils.Sequence` instances (since they\n+                generate batches).\n             epochs: Integer. Number of epochs to train the model.\n                 An epoch is an iteration over the entire `x` and `y`\n                 data provided.\n@@ -729,8 +739,8 @@ class Model(training_lib.Model):\n                 the loss and any model metrics\n                 on this data at the end of each epoch.\n                 The validation data is selected from the last samples\n-                in the `x` and `y` data provided, before shuffling. This argument is\n-                not supported when `x` is a dataset, generator or\n+                in the `x` and `y` data provided, before shuffling. This\n+                argument is not supported when `x` is a dataset, generator or\n                `keras.utils.Sequence` instance.\n             validation_data: Data on which to evaluate\n                 the loss and any model metrics at the end of each epoch.\n@@ -763,10 +773,10 @@ class Model(training_lib.Model):\n                 `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n-                `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n-                supported when `x` is a dataset, generator, or\n-               `keras.utils.Sequence` instance, instead provide the sample_weights\n-                as the third element of `x`.\n+                `sample_weight_mode=\"temporal\"` in `compile()`. This argument is\n+                not supported when `x` is a dataset, generator, or\n+                `keras.utils.Sequence` instance, instead provide the\n+                sample_weights as the third element of `x`.\n             initial_epoch: Integer.\n                 Epoch at which to start training\n                 (useful for resuming a previous training run).\n@@ -778,28 +788,30 @@ class Model(training_lib.Model):\n                 the number of samples in your dataset divided by\n                 the batch size, or 1 if that cannot be determined. If x is a\n                 `tf.data` dataset, and 'steps_per_epoch'\n-                is None, the epoch will run until the input dataset is exhausted.\n-                This argument is not supported with array inputs.\n+                is None, the epoch will run until the input dataset is\n+                exhausted.  This argument is not supported with array inputs.\n             validation_steps: Only relevant if `validation_data` is provided and\n                 is a `tf.data` dataset. Total number of steps (batches of\n-                samples) to draw before stopping when performing validation\n-                at the end of every epoch. If 'validation_steps' is None, validation\n-                will run until the `validation_data` dataset is exhausted. In the\n-                case of a infinite dataset, it will run into a infinite loop.\n-                If 'validation_steps' is specified and only part of the dataset\n-                will be consumed, the evaluation will start from the beginning of\n-                the dataset at each epoch. This ensures that the same validation\n-                samples are used every time.\n-            validation_freq: Only relevant if validation data is provided. Integer\n-                or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n-                If an integer, specifies how many training epochs to run before a\n-                new validation run is performed, e.g. `validation_freq=2` runs\n-                validation every 2 epochs. If a Container, specifies the epochs on\n-                which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n-                validation at the end of the 1st, 2nd, and 10th epochs.\n-            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n-                input only. Maximum size for the generator queue.\n-                If unspecified, `max_queue_size` will default to 10.\n+                samples) to draw before stopping when performing validation at\n+                the end of every epoch. If 'validation_steps' is None,\n+                validation will run until the `validation_data` dataset is\n+                exhausted. In the case of a infinite dataset, it will run into a\n+                infinite loop.  If 'validation_steps' is specified and only part\n+                of the dataset will be consumed, the evaluation will start from\n+                the beginning of the dataset at each epoch. This ensures that\n+                the same validation samples are used every time.\n+            validation_freq: Only relevant if validation data is provided.\n+                Integer or `collections.abc.Container` instance (e.g. list,\n+                tuple, etc.).  If an integer, specifies how many training epochs\n+                to run before a new validation run is performed, e.g.\n+                `validation_freq=2` runs validation every 2 epochs. If a\n+                Container, specifies the epochs on which to run validation, e.g.\n+                `validation_freq=[1, 2, 10]` runs validation at the end of the\n+                1st, 2nd, and 10th epochs.\n+            max_queue_size: Integer. Used for generator or\n+                `keras.utils.Sequence` input only. Maximum size for the\n+                generator queue.  If unspecified, `max_queue_size` will default\n+                to 10.\n             workers: Integer. Used for generator or `keras.utils.Sequence` input\n                 only. Maximum number of processes to spin up\n                 when using process-based threading. If unspecified, `workers`\n@@ -810,7 +822,8 @@ class Model(training_lib.Model):\n                 threading. If unspecified, `use_multiprocessing` will default to\n                 `False`. Note that because this implementation relies on\n                 multiprocessing, you should not pass non-picklable arguments to\n-                the generator as they can't be passed easily to children processes.\n+                the generator as they can't be passed easily to children\n+                processes.\n             **kwargs: Used for backwards compatibility.\n \n         Returns:\n@@ -892,15 +905,15 @@ class Model(training_lib.Model):\n               It should be consistent with `x` (you cannot have Numpy inputs and\n               tensor targets, or inversely).\n               If `x` is a dataset, generator or\n-              `keras.utils.Sequence` instance, `y` should not be specified (since\n-              targets will be obtained from the iterator/dataset).\n+              `keras.utils.Sequence` instance, `y` should not be specified\n+              (since targets will be obtained from the iterator/dataset).\n             batch_size: Integer or `None`.\n                 Number of samples per batch of computation.\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n                 form of symbolic tensors, dataset,\n-                generators, or `keras.utils.Sequence` instances (since they generate\n-                batches).\n+                generators, or `keras.utils.Sequence` instances (since they\n+                generate batches).\n             verbose: 0 or 1. Verbosity mode.\n                 0 = silent, 1 = progress bar.\n             sample_weight: Optional Numpy array of weights for\n@@ -913,9 +926,9 @@ class Model(training_lib.Model):\n                 `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n-                `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n-                supported when `x` is a dataset, instead pass\n-                sample weights as the third element of `x`.\n+                `sample_weight_mode=\"temporal\"` in `compile()`. This argument is\n+                not supported when `x` is a dataset, instead pass sample weights\n+                as the third element of `x`.\n             steps: Integer or `None`.\n                 Total number of steps (batches of samples)\n                 before declaring the evaluation round finished.\n@@ -926,9 +939,10 @@ class Model(training_lib.Model):\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during evaluation.\n                 See [callbacks](/api_docs/python/tf/keras/callbacks).\n-            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n-                input only. Maximum size for the generator queue.\n-                If unspecified, `max_queue_size` will default to 10.\n+            max_queue_size: Integer. Used for generator or\n+                `keras.utils.Sequence` input only. Maximum size for the\n+                generator queue.  If unspecified, `max_queue_size` will default\n+                to 10.\n             workers: Integer. Used for generator or `keras.utils.Sequence` input\n                 only. Maximum number of processes to spin up when using\n                 process-based threading. If unspecified, `workers` will default\n@@ -938,7 +952,8 @@ class Model(training_lib.Model):\n                 threading. If unspecified, `use_multiprocessing` will default to\n                 `False`. Note that because this implementation relies on\n                 multiprocessing, you should not pass non-picklable arguments to\n-                the generator as they can't be passed easily to children processes.\n+                the generator as they can't be passed easily to children\n+                processes.\n \n         Returns:\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -997,8 +1012,8 @@ class Model(training_lib.Model):\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n                 form of symbolic tensors, dataset,\n-                generators, or `keras.utils.Sequence` instances (since they generate\n-                batches).\n+                generators, or `keras.utils.Sequence` instances (since they\n+                generate batches).\n             verbose: Verbosity mode, 0 or 1.\n             steps: Total number of steps (batches of samples)\n                 before declaring the prediction round finished.\n@@ -1008,9 +1023,10 @@ class Model(training_lib.Model):\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during prediction.\n                 See [callbacks](/api_docs/python/tf/keras/callbacks).\n-            max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n-                input only. Maximum size for the generator queue.\n-                If unspecified, `max_queue_size` will default to 10.\n+            max_queue_size: Integer. Used for generator or\n+                `keras.utils.Sequence` input only. Maximum size for the\n+                generator queue. If unspecified, `max_queue_size` will default\n+                to 10.\n             workers: Integer. Used for generator or `keras.utils.Sequence` input\n                 only. Maximum number of processes to spin up when using\n                 process-based threading. If unspecified, `workers` will default\n@@ -1020,7 +1036,8 @@ class Model(training_lib.Model):\n                 threading. If unspecified, `use_multiprocessing` will default to\n                 `False`. Note that because this implementation relies on\n                 multiprocessing, you should not pass non-picklable arguments to\n-                the generator as they can't be passed easily to children processes.\n+                the generator as they can't be passed easily to children\n+                processes.\n \n \n         Returns:\n@@ -1082,23 +1099,24 @@ class Model(training_lib.Model):\n               - A `tf.data` dataset.\n             y: Target data. Like the input data `x`, it could be either Numpy\n               array(s) or TensorFlow tensor(s). It should be consistent with `x`\n-              (you cannot have Numpy inputs and tensor targets, or inversely). If\n-              `x` is a dataset, `y` should not be specified\n+              (you cannot have Numpy inputs and tensor targets, or inversely).\n+              If `x` is a dataset, `y` should not be specified\n               (since targets will be obtained from the iterator).\n             sample_weight: Optional array of the same length as x, containing\n-              weights to apply to the model's loss for each sample. In the case of\n-              temporal data, you can pass a 2D array with shape (samples,\n+              weights to apply to the model's loss for each sample. In the case\n+              of temporal data, you can pass a 2D array with shape (samples,\n               sequence_length), to apply a different weight to every timestep of\n               every sample. In this case you should make sure to specify\n               sample_weight_mode=\"temporal\" in compile(). This argument is not\n               supported when `x` is a dataset.\n-            class_weight: Optional dictionary mapping class indices (integers) to a\n-              weight (float) to apply to the model's loss for the samples from this\n-              class during training. This can be useful to tell the model to \"pay\n-              more attention\" to samples from an under-represented class.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+              to a weight (float) to apply to the model's loss for the samples\n+              from this class during training. This can be useful to tell the\n+              model to \"pay more attention\" to samples from an under-represented\n+              class.\n             reset_metrics: If `True`, the metrics returned will be only for this\n-              batch. If `False`, the metrics will be statefully accumulated across\n-              batches.\n+              batch. If `False`, the metrics will be statefully accumulated\n+              across batches.\n \n         Returns:\n             Scalar training loss\n@@ -1113,9 +1131,9 @@ class Model(training_lib.Model):\n         self._assert_compile_was_called()\n         self._check_call_args(\"train_on_batch\")\n \n-        # If at this point we are in the replica context, then it is okay to execute\n-        # the Eager code path.  The expected way to get here is to call `fit` that\n-        # calls `train_on_batch` on each replica.\n+        # If at this point we are in the replica context, then it is okay to\n+        # execute the Eager code path.  The expected way to get here is to call\n+        # `fit` that calls `train_on_batch` on each replica.\n         if (\n             self._distribution_strategy\n             and tf.distribute.in_cross_replica_context()\n@@ -1133,10 +1151,10 @@ class Model(training_lib.Model):\n             extract_tensors_from_dataset=True,\n         )\n \n-        # If `self._distribution_strategy` is True, then we are in a replica context\n-        # at this point because of the check above.  `train_on_batch` is being run\n-        # for each replica by `self._distribution_strategy` and the same code path\n-        # as Eager is expected to be taken.\n+        # If `self._distribution_strategy` is True, then we are in a replica\n+        # context at this point because of the check above.  `train_on_batch` is\n+        # being run for each replica by `self._distribution_strategy` and the\n+        # same code path as Eager is expected to be taken.\n         if self.run_eagerly or self._distribution_strategy:\n             output_dict = training_eager_v1.train_on_batch(\n                 self,\n@@ -1187,7 +1205,8 @@ class Model(training_lib.Model):\n               it could be either Numpy array(s) or TensorFlow tensor(s).\n               It should be consistent with `x` (you cannot have Numpy inputs and\n               tensor targets, or inversely). If `x` is a dataset `y` should\n-              not be specified (since targets will be obtained from the iterator).\n+              not be specified (since targets will be obtained from the\n+              iterator).\n             sample_weight: Optional array of the same length as x, containing\n                 weights to apply to the model's loss for each sample.\n                 In the case of temporal data, you can pass a 2D array\n@@ -1197,8 +1216,8 @@ class Model(training_lib.Model):\n                 sample_weight_mode=\"temporal\" in compile(). This argument is not\n                 supported when `x` is a dataset.\n             reset_metrics: If `True`, the metrics returned will be only for this\n-              batch. If `False`, the metrics will be statefully accumulated across\n-              batches.\n+              batch. If `False`, the metrics will be statefully accumulated\n+              across batches.\n \n         Returns:\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -1225,8 +1244,8 @@ class Model(training_lib.Model):\n             x, y, sample_weight=sample_weight, extract_tensors_from_dataset=True\n         )\n \n-        # If `self._distribution_strategy` is True, then we are in a replica context\n-        # at this point.\n+        # If `self._distribution_strategy` is True, then we are in a replica\n+        # context at this point.\n         if self.run_eagerly or self._distribution_strategy:\n             output_dict = training_eager_v1.test_on_batch(\n                 self,\n@@ -1283,19 +1302,20 @@ class Model(training_lib.Model):\n             and tf.distribute.in_cross_replica_context()\n         ):\n             raise NotImplementedError(\n-                \"`predict_on_batch` is not supported for models distributed with\"\n-                \" tf.distribute.Strategy.\"\n+                \"`predict_on_batch` is not supported for models distributed \"\n+                \"with tf.distribute.Strategy.\"\n             )\n         # Validate and standardize user data.\n         inputs, _, _ = self._standardize_user_data(\n             x, extract_tensors_from_dataset=True\n         )\n-        # If `self._distribution_strategy` is True, then we are in a replica context\n-        # at this point.\n+        # If `self._distribution_strategy` is True, then we are in a replica\n+        # context at this point.\n         if self.run_eagerly or self._distribution_strategy:\n             inputs = training_utils_v1.cast_if_floating_dtype(inputs)\n             if isinstance(inputs, collections.abc.Sequence):\n-                # Unwrap lists with only one input, as we do when training on batch\n+                # Unwrap lists with only one input, as we do when training on\n+                # batch\n                 if len(inputs) == 1:\n                     inputs = inputs[0]\n \n@@ -1328,8 +1348,8 @@ class Model(training_lib.Model):\n         \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n \n         DEPRECATED:\n-          `Model.fit` now supports generators, so there is no longer any need to use\n-          this endpoint.\n+          `Model.fit` now supports generators, so there is no longer any need to\n+          use this endpoint.\n         \"\"\"\n         warnings.warn(\n             \"`model.fit_generator` is deprecated and \"\n@@ -1367,8 +1387,8 @@ class Model(training_lib.Model):\n         \"\"\"Evaluates the model on a data generator.\n \n         DEPRECATED:\n-          `Model.evaluate` now supports generators, so there is no longer any need\n-          to use this endpoint.\n+          `Model.evaluate` now supports generators, so there is no longer any\n+          need to use this endpoint.\n         \"\"\"\n         warnings.warn(\n             \"`Model.evaluate_generator` is deprecated and \"\n@@ -1401,8 +1421,8 @@ class Model(training_lib.Model):\n         \"\"\"Generates predictions for the input samples from a data generator.\n \n         DEPRECATED:\n-          `Model.predict` now supports generators, so there is no longer any need\n-          to use this endpoint.\n+          `Model.predict` now supports generators, so there is no longer any\n+          need to use this endpoint.\n         \"\"\"\n         warnings.warn(\n             \"`Model.predict_generator` is deprecated and \"\n@@ -1498,8 +1518,8 @@ class Model(training_lib.Model):\n     def _validate_compile_param_for_distribution_strategy(\n         self, run_eagerly, sample_weight_mode, target_tensors, weighted_metrics\n     ):\n-        # Validate that arguments passed by the user to `compile` are supported by\n-        # tf.distribute.Strategy.\n+        # Validate that arguments passed by the user to `compile` are supported\n+        # by tf.distribute.Strategy.\n         if self._distribution_strategy:\n             if sample_weight_mode:\n                 raise NotImplementedError(\n@@ -1534,8 +1554,8 @@ class Model(training_lib.Model):\n \n     def _process_target_tensor_for_compile(self, target_tensors):\n         if self.run_eagerly:\n-            # target tensor is not supported with run_eagerly. Create a list with None\n-            # as placeholder for each output.\n+            # target tensor is not supported with run_eagerly. Create a list\n+            # with None as placeholder for each output.\n             return [None for _ in self.output_names]\n \n         if target_tensors is not None and not (\n@@ -1546,7 +1566,8 @@ class Model(training_lib.Model):\n                     raise ValueError(\n                         \"When passing a list as `target_tensors`, \"\n                         \"it should have one entry per model output. \"\n-                        \"The model has %s outputs, but you passed target_tensors=%s\"\n+                        \"The model has %s outputs, \"\n+                        \"but you passed target_tensors=%s\"\n                         % (len(self.outputs), target_tensors)\n                     )\n             elif isinstance(target_tensors, dict):\n@@ -1555,7 +1576,8 @@ class Model(training_lib.Model):\n                 ).difference(self.output_names)\n                 if unexpected_target_tensor_names:\n                     raise ValueError(\n-                        'Unknown entry in `target_tensors` dictionary: \"{name}\". '\n+                        \"Unknown entry in `target_tensors` dictionary: \"\n+                        '\"{name}\". '\n                         \"Only expected the following keys: {keys}\".format(\n                             name=unexpected_target_tensor_names,\n                             keys=str(self.output_names),\n@@ -1575,13 +1597,14 @@ class Model(training_lib.Model):\n                 )\n         else:\n             # In case target tensor is empty or None, create a list with Nones\n-            # that has same length as self.output_names. With that, the None check of\n-            # target tensor can be skipped downstream.\n+            # that has same length as self.output_names. With that, the None\n+            # check of target tensor can be skipped downstream.\n             target_tensors = [None for _ in self.output_names]\n         return target_tensors\n \n     def _compile_eagerly(self, metrics, weighted_metrics, sample_weight_mode):\n-        # Prepare sample weight modes. List with the same length as model outputs.\n+        # Prepare sample weight modes. List with the same length as model\n+        # outputs.\n         training_utils_v1.prepare_sample_weight_modes(\n             self._training_endpoints, sample_weight_mode\n         )\n@@ -1606,15 +1629,15 @@ class Model(training_lib.Model):\n           1. Set sample weight mode to be 'temporal' for output i, if `compile`\n             sample_weight_mode was set to `temporal` and sample weight inputs\n             are given for one or more outputs.\n-          2. Set sample weight mode to be 'samplewise' for output i, if `compile`\n-            sample_weight_mode was not set and sample weight inputs are given for\n-            one or more outputs.\n+          2. Set sample weight mode to be 'samplewise' for output i, if\n+            `compile` sample_weight_mode was not set and sample weight inputs\n+            are given for one or more outputs.\n           3. Reset sample weight mode to None for output i if sample weight mode\n             was set but there is no sample weight input.\n \n         Args:\n-          sample_weights: List of sample weights of the same length as model outputs\n-            or None.\n+          sample_weights: List of sample weights of the same length as model\n+            outputs or None.\n         \"\"\"\n         if not self._is_compiled:\n             return\n@@ -1642,15 +1665,16 @@ class Model(training_lib.Model):\n     def _compile_weights_loss_and_weighted_metrics(self, sample_weights=None):\n         \"\"\"Compiles the model loss and weighted metric sub-graphs.\n \n-        This may be used to set graph tensors as sample weights (instead of creating\n-        placeholders). This functionality is necessary for\n-        `tf.keras.estimator.model_to_estimator`, which calls Keras models in a v1\n-        graph, and creates iterator tensors for inputs, targets, and sample weights.\n+        This may be used to set graph tensors as sample weights (instead of\n+        creating placeholders). This functionality is necessary for\n+        `tf.keras.estimator.model_to_estimator`, which calls Keras models in a\n+        v1 graph, and creates iterator tensors for inputs, targets, and sample\n+        weights.\n \n         Args:\n-          sample_weights: List of tensors to use as the sample weights. Must be the\n-            same length as the number of outputs. If left as `None`, placeholders\n-            are used instead.\n+          sample_weights: List of tensors to use as the sample weights. Must be\n+            the same length as the number of outputs. If left as `None`,\n+            placeholders are used instead.\n         \"\"\"\n         with backend.get_graph().as_default():\n             if sample_weights is not None:\n@@ -1729,7 +1753,8 @@ class Model(training_lib.Model):\n                         if sample_weight is None:\n                             sample_weight = mask\n                         else:\n-                            # Update dimensions of weights to match with mask if possible.\n+                            # Update dimensions of weights to match with mask if\n+                            # possible.\n                             (\n                                 mask,\n                                 _,\n@@ -1748,8 +1773,8 @@ class Model(training_lib.Model):\n                         )\n                         loss_reduction = loss_fn.reduction\n \n-                        # `AUTO` loss reduction defaults to `SUM_OVER_BATCH_SIZE` for all\n-                        # compile use cases.\n+                        # `AUTO` loss reduction defaults to\n+                        # `SUM_OVER_BATCH_SIZE` for all compile use cases.\n                         if loss_reduction == losses_utils.ReductionV2.AUTO:\n                             loss_reduction = (\n                                 losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\n@@ -1760,11 +1785,12 @@ class Model(training_lib.Model):\n                             weighted_losses, reduction=loss_reduction\n                         )\n                     else:\n-                        # Compute the stateless loss value for a custom loss class.\n-                        # Here we assume that the class takes care of loss reduction\n-                        # because if this class returns a vector value we cannot\n-                        # differentiate between use case where a custom optimizer\n-                        # expects a vector loss value vs unreduced per-sample loss value.\n+                        # Compute the stateless loss value for a custom loss\n+                        # class.  Here we assume that the class takes care of\n+                        # loss reduction because if this class returns a vector\n+                        # value we cannot differentiate between use case where a\n+                        # custom optimizer expects a vector loss value vs\n+                        # unreduced per-sample loss value.\n                         output_loss = loss_fn(\n                             y_true, y_pred, sample_weight=sample_weight\n                         )\n@@ -1776,8 +1802,8 @@ class Model(training_lib.Model):\n                     # Keep track of stateful result tensor for the loss.\n                     endpoint.output_loss_metric(output_loss)\n \n-                # Scale output loss for distribution. For custom losses we assume\n-                # reduction was mean.\n+                # Scale output loss for distribution. For custom losses we\n+                # assume reduction was mean.\n                 if (\n                     loss_reduction\n                     == losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\n@@ -1864,10 +1890,9 @@ class Model(training_lib.Model):\n         ) or tf_inspect.isgenerator(x):\n             if batch_size is not None:\n                 raise ValueError(\n-                    \"The `batch_size` argument must not be specified for the given \"\n-                    \"input type. Received input: {}, batch_size: {}\".format(\n-                        x, batch_size\n-                    )\n+                    \"The `batch_size` argument must not be specified for the \"\n+                    \"given input type. Received input: \"\n+                    \"{}, batch_size: {}\".format(x, batch_size)\n                 )\n             return\n \n@@ -1882,7 +1907,8 @@ class Model(training_lib.Model):\n             )\n             if static_batch_size is not None:\n \n-                # Determine number of times the user-supplied batch size will be split.\n+                # Determine number of times the user-supplied batch size will be\n+                # split.\n                 if (\n                     self._distribution_strategy\n                     and distributed_training_utils.global_batch_size_supported(\n@@ -1915,7 +1941,8 @@ class Model(training_lib.Model):\n                             )\n                         )\n \n-                # Check Dataset/Iterator batch size is consistent with InputLayer.\n+                # Check Dataset/Iterator batch size is consistent with\n+                # InputLayer.\n                 if isinstance(\n                     x,\n                     (\n@@ -1933,7 +1960,8 @@ class Model(training_lib.Model):\n                         if ds_batch_size % num_splits_for_ds != 0:\n                             raise ValueError(\n                                 \"The batch output shape of your `Dataset` {} \"\n-                                \"cannot be divisible by number of replicas {}\".format(\n+                                \"cannot be divisible by number of \"\n+                                \"replicas {}\".format(\n                                     ds_batch_size, num_splits_for_ds\n                                 )\n                             )\n@@ -2006,15 +2034,15 @@ class Model(training_lib.Model):\n     def _add_unique_metric_name(self, metric_name, metric_fn, output_index):\n         \"\"\"Makes the metric name unique.\n \n-          If there are multiple outputs for which the metrics are calculated, the\n-          metric names have to be made unique by appending an integer.\n+          If there are multiple outputs for which the metrics are calculated,\n+          the metric names have to be made unique by appending an integer.\n \n         Args:\n-          metric_name: Metric name that corresponds to the metric specified by the\n-              user. For example: 'acc'.\n+          metric_name: Metric name that corresponds to the metric specified by\n+            the user. For example: 'acc'.\n           metric_fn: The Metric object.\n-          output_index: The index of the model output for which the metric name is\n-            being added.\n+          output_index: The index of the model output for which the metric name\n+            is being added.\n \n         Returns:\n           string, name of the model's unique metric name\n@@ -2024,9 +2052,10 @@ class Model(training_lib.Model):\n             # If we're loading from an already-serialized model, we've already\n             # prepended the output name, and we don't want to do it again.\n             #\n-            # Alternatively, we may be receiving a stateless metric (e.g. the string\n-            # \"accuracy\") rather than a `Metric` object, in which case we want to\n-            # prepend the output name even if we are loading a serialized model.\n+            # Alternatively, we may be receiving a stateless metric (e.g. the\n+            # string \"accuracy\") rather than a `Metric` object, in which case we\n+            # want to prepend the output name even if we are loading a\n+            # serialized model.\n             if not getattr(metric_fn, \"_from_serialized\", False):\n                 metric_name = \"%s_%s\" % (\n                     self.output_names[output_index],\n@@ -2043,15 +2072,16 @@ class Model(training_lib.Model):\n \n     def _init_metric_attributes(self):\n         \"\"\"Initialized model metric attributes.\"\"\"\n-        # List of stateful metric functions. Used for resetting metric state during\n-        # training/eval.\n+        # List of stateful metric functions. Used for resetting metric state\n+        # during training/eval.\n         self._compile_metric_functions = []\n \n     def _set_per_output_metric_attributes(self, metrics_dict, output_index):\n         \"\"\"Sets the metric attributes on the model for the given output.\n \n         Args:\n-          metrics_dict: A dict with metric names as keys and metric fns as values.\n+          metrics_dict: A dict with metric names as keys and metric fns as\n+            values.\n           output_index: The index of the model output for which the metric\n             attributes are added.\n \n@@ -2064,7 +2094,8 @@ class Model(training_lib.Model):\n                 metric_name, metric_fn, output_index\n             )\n \n-            # Update the name on the metric class to be the unique generated name.\n+            # Update the name on the metric class to be the unique generated\n+            # name.\n             metric_fn._name = metric_name  # pylint: disable=protected-access\n             updated_metrics_dict[metric_name] = metric_fn\n             # Keep track of metric name and function.\n@@ -2094,8 +2125,8 @@ class Model(training_lib.Model):\n             )\n \n         # Create a metric wrapper for each output loss. This computes mean of an\n-        # output loss across mini-batches (irrespective of how we reduce within a\n-        # batch).\n+        # output loss across mini-batches (irrespective of how we reduce within\n+        # a batch).\n         if len(self._training_endpoints) > 1:\n             for endpoint in self._training_endpoints:\n                 if not endpoint.should_skip_target():\n@@ -2112,7 +2143,8 @@ class Model(training_lib.Model):\n         \"\"\"Calls metric functions for a single output.\n \n         Args:\n-          metrics_dict: A dict with metric names as keys and metric fns as values.\n+          metrics_dict: A dict with metric names as keys and metric fns as\n+            values.\n           y_true: Target output.\n           y_pred: Predicted output.\n           mask: Computed mask value for the current output.\n@@ -2145,23 +2177,23 @@ class Model(training_lib.Model):\n         Args:\n           outputs: List of outputs (predictions).\n           targets: List of targets.\n-          skip_target_masks: Optional. List of boolean for whether the corresponding\n-            target should be ignored or not.\n+          skip_target_masks: Optional. List of boolean for whether the\n+            corresponding target should be ignored or not.\n           sample_weights: Optional list of sample weight arrays.\n           masks: List of computed output mask values.\n           return_weighted_metrics: Flag that indicates whether weighted metrics\n-            should be computed instead of unweighted metrics. This flag is ignored\n-            when `return_weighted_and_unweighted_metrics` is enabled.\n+            should be computed instead of unweighted metrics. This flag is\n+            ignored when `return_weighted_and_unweighted_metrics` is enabled.\n           return_weighted_and_unweighted_metrics: Flag that is used to indicate\n-            whether both weighted and unweighted metrics should be computed. When\n-            this is not enabled, we use `return_weighted_metrics` param to indicate\n-            whether weighted or unweighted metrics should be returned.\n+            whether both weighted and unweighted metrics should be computed.\n+            When this is not enabled, we use `return_weighted_metrics` param to\n+            indicate whether weighted or unweighted metrics should be returned.\n \n         Returns:\n           A list of metric result tensors.\n         \"\"\"\n-        # TODO(scottzhu): Update this to use the new training_endpoints. Currently\n-        # the eager and graph logic is bit different.\n+        # TODO(scottzhu): Update this to use the new training_endpoints.\n+        # Currently the eager and graph logic is bit different.\n         skip_target_masks = skip_target_masks or [False] * len(outputs)\n         metric_results = []\n         with backend.name_scope(\"metrics\"):\n@@ -2360,8 +2392,8 @@ class Model(training_lib.Model):\n     ):\n         \"\"\"Runs validation checks on input and target data passed by the user.\n \n-        This is called when using tf.distribute.Strategy to train, evaluate or serve\n-        the model.\n+        This is called when using tf.distribute.Strategy to train, evaluate or\n+        serve the model.\n \n         Args:\n           x: Input data. A numpy array or `tf.data` dataset.\n@@ -2369,17 +2401,18 @@ class Model(training_lib.Model):\n           sample_weight: An optional sample-weight array passed by the user to\n             weight the importance of each sample in `x`.\n           class_weight: An optional class-weight array by the user to\n-            weight the importance of samples in `x` based on the class they belong\n-            to, as conveyed by `y`.\n-          batch_size: Integer batch size. If provided, it is used to run additional\n-            validation checks on stateful models.\n+            weight the importance of samples in `x` based on the class they\n+            belong to, as conveyed by `y`.\n+          batch_size: Integer batch size. If provided, it is used to run\n+            additional validation checks on stateful models.\n           validation_split: Float between 0 and 1.\n             Fraction of the training data to be used as validation data.\n-          shuffle: Boolean whether to shuffle the training data before each epoch.\n+          shuffle: Boolean whether to shuffle the training data before each\n+            epoch.\n           epochs: Integer epochs. If > 1, repeat the numpy training data epochs\n             times when converting to training dataset.\n-          allow_partial_batch: Boolean whether to enforce that all batches have the\n-            same size.\n+          allow_partial_batch: Boolean whether to enforce that all batches have\n+            the same size.\n \n         Returns:\n           Dataset instance.\n@@ -2407,16 +2440,16 @@ class Model(training_lib.Model):\n         # Validates `steps` and `shuffle` arguments right at the beginning\n         # since we use it to construct the dataset object.\n         # TODO(anjalisridhar): Remove this check once we refactor the\n-        # _standardize_user_data code path. This check is already present elsewhere\n-        # in the codebase.\n+        # _standardize_user_data code path. This check is already present\n+        # elsewhere in the codebase.\n         if isinstance(x, tf.data.Dataset):\n             if shuffle:\n                 training_utils_v1.verify_dataset_shuffled(x)\n \n         strategy = self._distribution_strategy\n         with strategy.scope():\n-            # We should be sure to call get_session() inside the strategy.scope()\n-            # so the strategy can affect the session options.\n+            # We should be sure to call get_session() inside the\n+            # strategy.scope() so the strategy can affect the session options.\n             if tf.compat.v1.executing_eagerly_outside_functions():\n                 session = None\n             else:\n@@ -2441,24 +2474,25 @@ class Model(training_lib.Model):\n                     in_tuple, session=session\n                 )\n                 if shuffle:\n-                    # We want a buffer size that is larger than the batch size provided by\n-                    # the user and provides sufficient randomness. Note that larger\n-                    # numbers introduce more memory usage based on the size of each\n-                    # sample.\n+                    # We want a buffer size that is larger than the batch size\n+                    # provided by the user and provides sufficient randomness.\n+                    # Note that larger numbers introduce more memory usage based\n+                    # on the size of each sample.\n                     ds = ds.shuffle(max(1024, batch_size * 8))\n                 if epochs > 1:\n                     ds = ds.repeat(epochs)\n \n-                # We need to use the drop_remainder argument to get a known static\n-                # input shape which is required for TPUs.\n+                # We need to use the drop_remainder argument to get a known\n+                # static input shape which is required for TPUs.\n                 drop_remainder = (\n                     not allow_partial_batch\n                     and strategy.extended.experimental_require_static_shapes\n                 )\n \n-                # TODO(b/131720208): We still drop remainder here if number of examples\n-                # is divisible by batch size, as sometimes dynamic padder will time out\n-                # with keras.metrics.CategoricalAccuracy() metric.\n+                # TODO(b/131720208): We still drop remainder here if number of\n+                # examples is divisible by batch size, as sometimes dynamic\n+                # padder will time out with keras.metrics.CategoricalAccuracy()\n+                # metric.\n                 if backend.is_tpu_strategy(strategy) and not drop_remainder:\n                     dataset_size = first_x_value.shape[0]\n                     if dataset_size % batch_size == 0:\n@@ -2490,8 +2524,9 @@ class Model(training_lib.Model):\n \n         Also standardizes the data to lists of arrays, in order.\n \n-        Also builds and compiles the model on the fly if it is a subclassed model\n-        that has never been called before (and thus has no inputs/outputs).\n+        Also builds and compiles the model on the fly if it is a subclassed\n+        model that has never been called before (and thus has no\n+        inputs/outputs).\n \n         This is a purely internal method, subject to refactoring at any time.\n \n@@ -2507,38 +2542,39 @@ class Model(training_lib.Model):\n           y: Target data. Like the input data `x`,\n             it could be either Numpy array(s) or TensorFlow tensor(s).\n             It should be consistent with `x` (you cannot have Numpy inputs and\n-            tensor targets, or inversely). If `x` is a dataset, `y` should not be\n-            specified (since targets will be obtained from the iterator).\n+            tensor targets, or inversely). If `x` is a dataset, `y` should not\n+            be specified (since targets will be obtained from the iterator).\n           sample_weight: An optional sample-weight array passed by the user to\n             weight the importance of each sample in `x`.\n           class_weight: An optional class-weight array by the user to\n-            weight the importance of samples in `x` based on the class they belong\n-            to, as conveyed by `y`. If both `sample_weight` and `class_weight` are\n-            provided, the weights are multiplied.\n-          batch_size: Integer batch size. If provided, it is used to run additional\n-            validation checks on stateful models.\n-          check_steps: boolean, True if we want to check for validity of `steps` and\n-            False, otherwise. For example, when we are standardizing one batch of\n-            data for train_on_batch/predict_on_batch/test_on_batch APIs, `steps`\n-            value is not required and we should not check for its validity in these\n-            cases.\n+            weight the importance of samples in `x` based on the class they\n+            belong to, as conveyed by `y`. If both `sample_weight` and\n+            `class_weight` are provided, the weights are multiplied.\n+          batch_size: Integer batch size. If provided, it is used to run\n+            additional validation checks on stateful models.\n+          check_steps: boolean, True if we want to check for validity of `steps`\n+            and False, otherwise. For example, when we are standardizing one\n+            batch of data for train_on_batch/predict_on_batch/test_on_batch\n+            APIs, `steps` value is not required and we should not check for its\n+            validity in these cases.\n           steps_name: The public API's parameter name for `steps`.\n-          steps: Integer or `None`. Total number of steps (batches of samples) to\n-            execute.\n+          steps: Integer or `None`. Total number of steps (batches of samples)\n+            to execute.\n           validation_split: Float between 0 and 1.\n             Fraction of the training data to be used as validation data.\n-          shuffle: Boolean whether to shuffle the training data before each epoch.\n+          shuffle: Boolean whether to shuffle the training data before each\n+            epoch.\n           extract_tensors_from_dataset: Boolean. When `x` is a dataset instance,\n             this indicates whether to extract actual tensors from the dataset or\n             instead output the dataset instance itself.\n             Set to True when calling from `train_on_batch`/etc.\n \n         Returns:\n-          A tuple of 3: inputs (arrays or dicts, depending on whether `x` was a dict\n-          or not), target arrays, sample-weight arrays.\n-          If the model's input and targets are symbolic, these lists are empty\n-          (since the model takes no user-provided data, instead the data comes\n-          from the symbolic inputs/targets).\n+          A tuple of 3: inputs (arrays or dicts, depending on whether `x` was a\n+          dict or not), target arrays, sample-weight arrays.  If the model's\n+          input and targets are symbolic, these lists are empty (since the model\n+          takes no user-provided data, instead the data comes from the symbolic\n+          inputs/targets).\n \n         Raises:\n           ValueError: In case of invalid user-provided data.\n@@ -2593,21 +2629,21 @@ class Model(training_lib.Model):\n             is_build_called = False\n             y_input = y\n \n-        # Second, we compile the model on the fly if necessary, mostly for subclass\n-        # models.\n+        # Second, we compile the model on the fly if necessary, mostly for\n+        # subclass models.\n         is_compile_called = False\n         if not self._is_compiled and self.optimizer:\n             self._compile_from_inputs(all_inputs, y_input, x, y)\n             is_compile_called = True\n \n-        # In graph mode, if we had just set inputs and targets as symbolic tensors\n-        # by invoking build and compile on the model respectively, we do not have to\n-        # feed anything to the model. Model already has input and target data as\n-        # part of the graph.\n-        # Note: in this case, `any` and `all` are equivalent since we disallow\n-        # mixed symbolic/value inputs.\n+        # In graph mode, if we had just set inputs and targets as symbolic\n+        # tensors by invoking build and compile on the model respectively, we do\n+        # not have to feed anything to the model. Model already has input and\n+        # target data as part of the graph.  Note: in this case, `any` and `all`\n+        # are equivalent since we disallow mixed symbolic/value inputs.\n \n-        # self.run_eagerly is not free to compute, so we want to reuse the value.\n+        # self.run_eagerly is not free to compute, so we want to reuse the\n+        # value.\n         run_eagerly = self.run_eagerly\n \n         if (\n@@ -2647,7 +2683,8 @@ class Model(training_lib.Model):\n             feed_input_names = self.input_names\n             feed_input_shapes = None\n         elif not self._is_graph_network:\n-            # Case: symbolic-mode subclassed network. Do not do shape validation.\n+            # Case: symbolic-mode subclassed network. Do not do shape\n+            # validation.\n             feed_input_names = self._feed_input_names\n             feed_input_shapes = None\n         else:\n@@ -2669,14 +2706,15 @@ class Model(training_lib.Model):\n \n         # Get typespecs for the input data and sanitize it if necessary.\n         # TODO(momernick): This should be capable of doing full input validation\n-        # at all times - validate that this is so and refactor the standardization\n-        # code.\n+        # at all times - validate that this is so and refactor the\n+        # standardization code.\n         if isinstance(x, tf.data.Dataset):\n             x_shapes = tf.data.experimental.get_structure(x)\n             if isinstance(x_shapes, tuple):\n-                # If the output of a Dataset is a tuple, we assume it's either of the\n-                # form (x_data, y_data) or (x_data, y_data, sample_weights). In either\n-                # case, we only care about x_data here.\n+                # If the output of a Dataset is a tuple, we assume it's either\n+                # of the form (x_data, y_data) or (x_data, y_data,\n+                # sample_weights). In either case, we only care about x_data\n+                # here.\n                 x_shapes = x_shapes[0]\n         else:\n             flat_inputs = tf.nest.flatten(x, expand_composites=False)\n@@ -2728,7 +2766,8 @@ class Model(training_lib.Model):\n                 y,\n                 feed_output_names,\n                 # Don't enforce target shapes to match output shapes.\n-                # Precise checks will be run in `check_loss_and_target_compatibility`.\n+                # Precise checks will be run in\n+                # `check_loss_and_target_compatibility`.\n                 shapes=None,\n                 check_batch_axis=False,  # Don't enforce the batch size.\n                 exception_prefix=\"target\",\n@@ -2753,7 +2792,8 @@ class Model(training_lib.Model):\n             if not self._distribution_strategy:\n                 training_utils_v1.check_array_lengths(x, y, sample_weights)\n                 if self._is_graph_network and not run_eagerly:\n-                    # Additional checks to avoid users mistakenly using improper loss fns.\n+                    # Additional checks to avoid users mistakenly using improper\n+                    # loss fns.\n                     training_utils_v1.check_loss_and_target_compatibility(\n                         y, self._feed_loss_fns, feed_output_shapes\n                     )\n@@ -2786,7 +2826,8 @@ class Model(training_lib.Model):\n         return x, y, sample_weights\n \n     def _build_model_with_inputs(self, inputs, targets):\n-        \"\"\"Build the model (set model inputs/outputs), mainly for subclass model.\"\"\"\n+        \"\"\"Build the model (set model inputs/outputs), mainly for subclass\n+        model.\"\"\"\n         processed_inputs = []\n         is_dict_inputs = False\n         orig_inputs = inputs\n@@ -2813,21 +2854,22 @@ class Model(training_lib.Model):\n             processed_inputs.append(inputs)\n         # Now that we have a flat set of inputs, we make sure that none of them\n         # are CompositeTensors or CompositeTensorValues of any type (or scipy\n-        # sparse arrays, which we treat as SparseTensor values). We cannot safely\n-        # infer input data from an arbitrary composite tensor, so we don't try -\n-        # users should explicitly add composite tensor inputs to their subclassed\n-        # models.\n+        # sparse arrays, which we treat as SparseTensor values). We cannot\n+        # safely infer input data from an arbitrary composite tensor, so we\n+        # don't try - users should explicitly add composite tensor inputs to\n+        # their subclassed models.\n         for input_tensor in processed_inputs:\n             if training_utils_v1.is_composite_or_composite_value(input_tensor):\n                 # TODO(b/132691975): Document subclass-model CT input handling.\n                 raise ValueError(\n-                    \"All SparseTensor and RaggedTensor inputs must be explicitly \"\n-                    \"declared using a keras.Input() with sparse=True or ragged=True. \"\n-                    \"We found an undeclared input %s. For Sequential models, please \"\n-                    \"add a keras.Input() as your first Layer. For subclassed models, \"\n-                    \"please call self._set_inputs() on your input set, which you can \"\n-                    \"create using keras.Input() for each input to your model.\"\n-                    % (input_tensor,)\n+                    \"All SparseTensor and RaggedTensor inputs must be \"\n+                    \"explicitly declared using a keras.Input() with \"\n+                    \"sparse=True or ragged=True. We found an undeclared \"\n+                    \"input %s. For Sequential models, please add a \"\n+                    \"keras.Input() as your first Layer. For subclassed models, \"\n+                    \"please call self._set_inputs() on your input set, which \"\n+                    \"you can create using keras.Input() for each input to your \"\n+                    \"model.\" % (input_tensor,)\n                 )\n         # Build the model using the retrieved inputs (value or symbolic).\n         # If values are generated from a dataset, then in symbolic-mode\n@@ -2841,8 +2883,8 @@ class Model(training_lib.Model):\n             ),\n         ):\n             if not self.inputs:\n-                # For subclassed models, a robust input spec is not available so we\n-                # must cast to the model dtype.\n+                # For subclassed models, a robust input spec is not available so\n+                # we must cast to the model dtype.\n                 inputs = training_utils_v1.cast_if_floating_dtype(\n                     inputs, self.dtype\n                 )\n@@ -2921,25 +2963,27 @@ class Model(training_lib.Model):\n     def _set_inputs(self, inputs, outputs=None, training=None):\n         \"\"\"Set model's input and output specs based on the input data received.\n \n-        This is to be used for Model subclasses, which do not know at instantiation\n-        time what their inputs look like.\n+        This is to be used for Model subclasses, which do not know at\n+        instantiation time what their inputs look like.\n \n         Args:\n-          inputs: Single array, or list of arrays. The arrays could be placeholders,\n-            Numpy arrays, data tensors, or TensorSpecs.\n+          inputs: Single array, or list of arrays. The arrays could be\n+            placeholders, Numpy arrays, data tensors, or TensorSpecs.\n             - if placeholders: the model is built on top of these placeholders,\n-              and we expect Numpy data to be fed for them when calling `fit`/etc.\n+              and we expect Numpy data to be fed for them when calling\n+              `fit`/etc.\n             - if Numpy data or TensorShapes: we create placeholders matching the\n-              TensorShapes or shapes of the Numpy arrays. We expect Numpy data to be\n-              fed for these placeholders when calling `fit`/etc.\n+              TensorShapes or shapes of the Numpy arrays. We expect Numpy data\n+              to be fed for these placeholders when calling `fit`/etc.\n             - if data tensors: the model is built on top of these tensors.\n-              We do not expect any Numpy data to be provided when calling `fit`/etc.\n+              We do not expect any Numpy data to be provided when calling\n+              `fit`/etc.\n           outputs: None, a data tensor, or a list of tensors. If None, the\n             outputs will be determined by invoking `self.call()`, otherwise the\n             provided value will be used.\n           training: Boolean or None. Only relevant in symbolic mode. Specifies\n-            whether to build the model's graph in inference mode (False), training\n-            mode (True), or using the Keras learning phase (None).\n+            whether to build the model's graph in inference mode (False),\n+            training mode (True), or using the Keras learning phase (None).\n         Raises:\n           ValueError: If dict inputs are passed to a Sequential Model where the\n             first layer isn't FeatureLayer.\n@@ -2950,8 +2994,9 @@ class Model(training_lib.Model):\n         if outputs is None:\n             kwargs = {}\n             if self._expects_training_arg:\n-                # In V2 mode, feeding `training=None` is not allowed because any value\n-                # explicitly passed by the user is respected, even `None`.`\n+                # In V2 mode, feeding `training=None` is not allowed because any\n+                # value explicitly passed by the user is respected, even\n+                # `None`.`\n                 if (\n                     training is None\n                     and not tf.compat.v1.executing_eagerly_outside_functions()\n@@ -2996,8 +3041,9 @@ class Model(training_lib.Model):\n         # when saving to determine the correct dtype in the input signature.\n         inputs = self._maybe_cast_inputs(inputs)\n \n-        # On-the-fly setting of symbolic model inputs (either by using the tensor\n-        # provided, or by creating a placeholder if Numpy data was provided).\n+        # On-the-fly setting of symbolic model inputs (either by using the\n+        # tensor provided, or by creating a placeholder if Numpy data was\n+        # provided).\n         model_inputs = training_utils_v1.ModelInputs(inputs)\n         inputs = model_inputs.get_symbolic_inputs()\n         self.inputs = model_inputs.get_symbolic_inputs(\n@@ -3020,8 +3066,8 @@ class Model(training_lib.Model):\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _set_output_attrs(self, outputs):\n         \"\"\"Sets attributes related to the outputs of the Model.\"\"\"\n-        # NOTE(taylorrobie): This convention cannot be changed without updating the\n-        #                    data adapter since it assumes nest.flatten ordering.\n+        # NOTE(taylorrobie): This convention cannot be changed without updating\n+        # the data adapter since it assumes nest.flatten ordering.\n         outputs = tf.nest.flatten(outputs)\n         self.outputs = outputs\n         self.output_names = training_utils_v1.generic_output_names(outputs)\n@@ -3111,8 +3157,8 @@ class Model(training_lib.Model):\n \n         Returns:\n           If the training is recovering from previous failure under multi-worker\n-          training setting, return the epoch the training is supposed to continue\n-          at. Otherwise, return the `initial_epoch` the user passes in.\n+          training setting, return the epoch the training is supposed to\n+          continue at. Otherwise, return the `initial_epoch` the user passes in.\n         \"\"\"\n         if self._training_state is not None:\n             return self._training_state.maybe_load_initial_epoch_from_ckpt(\n@@ -3216,7 +3262,8 @@ class DistributedCallbackModel(Model):\n \n     def load_weights(self, filepath, by_name=False):\n         self._original_model.load_weights(filepath, by_name=False)\n-        # Copy the weights from the original model to each of the replicated models.\n+        # Copy the weights from the original model to each of the replicated\n+        # models.\n         orig_model_weights = self._original_model.get_weights()\n         distributed_training_utils_v1.set_weights(\n             self._original_model._distribution_strategy,\n@@ -3242,8 +3289,8 @@ class _TrainingEndpoint:\n     In the case of model with multiple outputs, there is a one-to-one mapping\n     between model output (y_pred), model target (y_true), loss, metrics etc.\n     By unifying these entities into one class, different entity can access\n-    information between each other, rather than currently access different list of\n-    attributes of the model.\n+    information between each other, rather than currently access different list\n+    of attributes of the model.\n     \"\"\"\n \n     def __init__(\n@@ -3259,9 +3306,9 @@ class _TrainingEndpoint:\n     ):\n         \"\"\"Initialize the _TrainingEndpoint.\n \n-        Note that the output and output_name should be stable as long as the model\n-        structure doesn't change. The training_target suppose to be mutable since\n-        the information is provided via `compile()`\n+        Note that the output and output_name should be stable as long as the\n+        model structure doesn't change. The training_target suppose to be\n+        mutable since the information is provided via `compile()`\n \n         Args:\n           output: the output tensor of the model.\n@@ -3270,10 +3317,10 @@ class _TrainingEndpoint:\n           loss_weight: float, the weights for the loss.\n           training_target: the _TrainingTarget for the model.\n           output_loss_metric: the metric object for the loss function.\n-          sample_weight: the weights for how a sample is weighted during metric and\n-            loss calculation. Could be None.\n-          sample_weight_mode: string, 'temporal', 'samplewise' or None. The mode for\n-            how the sample_weight is populated.\n+          sample_weight: the weights for how a sample is weighted during metric\n+            and loss calculation. Could be None.\n+          sample_weight_mode: string, 'temporal', 'samplewise' or None. The mode\n+            for how the sample_weight is populated.\n         \"\"\"\n         self._output = output\n         self._output_name = output_name\n@@ -3337,8 +3384,8 @@ class _TrainingEndpoint:\n                 \"instance has already been populated\"\n             )\n         if run_eagerly:\n-            # When run_eagerly, the target tensor is ignored, and the None placeholder\n-            # is created instead.\n+            # When run_eagerly, the target tensor is ignored, and the None\n+            # placeholder is created instead.\n             self.training_target = _TrainingTarget(\n                 None, feedable=True, skip_target_weights=False\n             )\n@@ -3585,7 +3632,8 @@ def _get_metrics_from_layers(layers):\n     for layer in layers:\n         if isinstance(layer, Model):\n             # We cannot call 'metrics' on the model because we do not want to\n-            # include the metrics that were added in compile API of a nested model.\n+            # include the metrics that were added in compile API of a nested\n+            # model.\n             metrics.extend(layer._metrics)  # pylint: disable=protected-access\n             metrics.extend(_get_metrics_from_layers(layer.layers))\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
