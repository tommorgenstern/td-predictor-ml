{"custom_id": "keras#998b39227d417719561ab8aa537b49993a8bbf66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 127 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -282,7 +282,7 @@ class Trainer:\n                 self.loss_tracker = metrics.Mean(name='loss')\n \n             def compute_loss(self, x, y, y_pred, sample_weight, training=True):\n-                loss = ops.means((y_pred - y) ** 2)\n+                loss = ops.mean((y_pred - y) ** 2)\n                 loss += ops.sum(self.losses)\n                 self.loss_tracker.update_state(loss)\n                 return loss\n@@ -408,22 +408,28 @@ class Trainer:\n         \"\"\"Update metric states and collect all metrics to be returned.\n \n         Subclasses can optionally override this method to provide custom metric\n-        updating and collection logic.\n+        updating and collection logic. Custom metrics are not passed in\n+        `compile()`, they can be created in `__init__` or `build`. They are\n+        automatically tracked and returned by `self.metrics`.\n \n         Example:\n \n         ```python\n         class MyModel(Sequential):\n+            def __init__(self, *args, **kwargs):\n+                super().__init__(*args, **kwargs)\n+                self.custom_metric = MyMetric(name=\"custom_metric\")\n+\n             def compute_metrics(self, x, y, y_pred, sample_weight):\n-                # This super call updates `self.compiled_metrics` and returns\n+                # This super call updates metrics from `compile` and returns\n                 # results for all metrics listed in `self.metrics`.\n                 metric_results = super().compute_metrics(\n                     x, y, y_pred, sample_weight)\n \n-                # Note that `self.custom_metric` is not listed\n-                # in `self.metrics`.\n+                # `metric_results` contains the previous result for\n+                # `custom_metric`, this is where we update it.\n                 self.custom_metric.update_state(x, y, y_pred, sample_weight)\n-                metric_results['metric_name'] = self.custom_metric.result()\n+                metric_results['custom_metric'] = self.custom_metric.result()\n                 return metric_results\n         ```\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#996cd6296b939e80019991f8229857da6053f02e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 31 | Files Changed: 3 | Hunks: 13 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 47 | Churn Cumulative: 664 | Contributors (this commit): 9 | Commits (past 90d): 18 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -308,35 +308,21 @@ def average_pool(\n     data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_last\":\n         inputs = _transpose_spatial_inputs(inputs)\n-    padding_value = 0\n     if padding == \"same\":\n-        spatial_shape = inputs.shape[2:]\n-        num_spatial_dims = len(spatial_shape)\n-        padding_value = []\n-        uneven_padding = []\n-\n-        for i in range(num_spatial_dims):\n-            padding_size = _compute_padding_length(\n-                spatial_shape[i], pool_size[i], strides[i]\n+        # Torch does not natively support `\"same\"` padding, we need to manually\n+        # apply the right amount of padding to `inputs`.\n+        inputs, padding = _apply_same_padding(\n+            inputs, pool_size, strides, operation_type=\"pooling\"\n         )\n-            # Torch only supports even padding on each dim, to replicate the\n-            # behavior of \"same\" padding of `tf.keras` as much as possible,\n-            # we need to pad evenly using the shorter padding.\n-            padding_value.append(padding_size[0])\n-            if padding_size[0] != padding_size[1]:\n-                # Handle unequal padding.\n-                # `torch.nn.pad` sets padding value in the reverse order.\n-                uneven_padding = [0, 1] + uneven_padding\n-        # Only call tnn.pad when needed.\n-        if len(uneven_padding) > 0:\n-            inputs = tnn.pad(inputs, uneven_padding)\n+    else:\n+        padding = 0\n \n     if num_spatial_dims == 1:\n         outputs = tnn.avg_pool1d(\n             inputs,\n             kernel_size=pool_size,\n             stride=strides,\n-            padding=padding_value,\n+            padding=padding,\n             count_include_pad=False,\n         )\n     elif num_spatial_dims == 2:\n@@ -344,7 +330,7 @@ def average_pool(\n             inputs,\n             kernel_size=pool_size,\n             stride=strides,\n-            padding=padding_value,\n+            padding=padding,\n             count_include_pad=False,\n         )\n     elif num_spatial_dims == 3:\n@@ -352,7 +338,7 @@ def average_pool(\n             inputs,\n             kernel_size=pool_size,\n             stride=strides,\n-            padding=padding_value,\n+            padding=padding,\n             count_include_pad=False,\n         )\n     else:\n\n@@ -168,8 +168,11 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 4, 4, 4)),\n+        (2, 1, \"same\", \"channels_last\", (3, 5, 5, 4), (3, 5, 5, 4)),\n+        (2, 1, \"valid\", \"channels_first\", (3, 5, 5, 4), (3, 5, 4, 3)),\n         (2, 1, \"same\", \"channels_first\", (3, 5, 5, 4), (3, 5, 5, 4)),\n         ((2, 3), (2, 2), \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 2, 2, 4)),\n+        ((2, 3), (2, 2), \"same\", \"channels_last\", (3, 5, 5, 4), (3, 3, 3, 4)),\n     )\n     def test_average_pooling2d(\n         self,\n\n@@ -1313,7 +1313,7 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n     def test_average_pool_valid_padding(self):\n         data_format = backend.config.image_data_format()\n-        # Test 1D max pooling.\n+        # Test 1D average pooling.\n         if data_format == \"channels_last\":\n             input_shape = (2, 20, 3)\n         else:\n@@ -1324,7 +1324,7 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np_avgpool1d(x, 2, 1, padding=\"valid\", data_format=data_format),\n         )\n \n-        # Test 2D max pooling.\n+        # Test 2D average pooling.\n         if data_format == \"channels_last\":\n             input_shape = (2, 10, 9, 3)\n         else:\n@@ -1335,13 +1335,9 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np_avgpool2d(x, 2, 1, padding=\"valid\", data_format=data_format),\n         )\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"Torch outputs differently from TF when using `same` padding.\",\n-    )\n     def test_average_pool_same_padding(self):\n         data_format = backend.config.image_data_format()\n-        # Test 1D max pooling.\n+        # Test 1D average pooling.\n         if data_format == \"channels_last\":\n             input_shape = (2, 20, 3)\n         else:\n@@ -1353,7 +1349,7 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np_avgpool1d(x, 2, 2, padding=\"same\", data_format=data_format),\n         )\n \n-        # Test 2D max pooling.\n+        # Test 2D average pooling.\n         if data_format == \"channels_last\":\n             input_shape = (2, 10, 9, 3)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0fd7302fa666d51311e12f78926627510831e685", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 236 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 236 | Churn Cumulative: 284 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,7 @@ from keras.src.optimizers.adam import Adam\n from keras.src.optimizers.adamax import Adamax\n from keras.src.optimizers.adamw import AdamW\n from keras.src.optimizers.ftrl import Ftrl\n+from keras.src.optimizers.lamb import Lamb\n from keras.src.optimizers.lion import Lion\n from keras.src.optimizers.loss_scale_optimizer import LossScaleOptimizer\n from keras.src.optimizers.nadam import Nadam\n\n@@ -16,6 +16,7 @@ from keras.src.optimizers.adam import Adam\n from keras.src.optimizers.adamax import Adamax\n from keras.src.optimizers.adamw import AdamW\n from keras.src.optimizers.ftrl import Ftrl\n+from keras.src.optimizers.lamb import Lamb\n from keras.src.optimizers.lion import Lion\n from keras.src.optimizers.loss_scale_optimizer import LossScaleOptimizer\n from keras.src.optimizers.nadam import Nadam\n\n@@ -0,0 +1,158 @@\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.optimizers import optimizer\n+\n+\n+@keras_export(\"keras.optimizers.Lamb\")\n+class Lamb(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Lamb algorithm.\n+\n+    Lamb is a stochastic gradient descent method that\n+    uses layer-wise adaptive moments to adjusts the\n+    learning rate for each parameter based on the ratio of the\n+    norm of the weight to the norm of the gradient\n+    This helps to stabilize the training process and improves convergence\n+    especially for large batch sizes.\n+\n+    Based on\n+    [Yang et al.](https://arxiv.org/pdf/1904.00962)\n+\n+    Args:\n+        learning_rate: A float, a\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n+            a callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to `0.001`.\n+        beta_1: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 1st moment estimates. Defaults to\n+            `0.9`.\n+        beta_2: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 2nd moment estimates. Defaults to\n+            `0.999`.\n+        epsilon: A small constant for numerical stability.\n+            Defaults to `1e-7`.\n+        {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n+        gradient_accumulation_steps=None,\n+        name=\"lamb\",\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+            loss_scale_factor=loss_scale_factor,\n+            gradient_accumulation_steps=gradient_accumulation_steps,\n+            **kwargs,\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Lamb optimizer has 2 types of variables: momentums and velocities\n+\n+        Args:\n+            var_list: list of model variables to build Lamb variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._momentums = []\n+        self._velocities = []\n+        for var in var_list:\n+            self._momentums.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"momentum\"\n+                )\n+            )\n+            self._velocities.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"velocity\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+\n+        beta_1_power = ops.power(\n+            ops.cast(self.beta_1, variable.dtype), local_step\n+        )\n+        beta_2_power = ops.power(\n+            ops.cast(self.beta_2, variable.dtype), local_step\n+        )\n+\n+        m = self._momentums[self._get_variable_index(variable)]\n+        v = self._velocities[self._get_variable_index(variable)]\n+\n+        self.assign_add(\n+            m, ops.multiply(ops.subtract(gradient, m), 1 - self.beta_1)\n+        )\n+\n+        self.assign_add(\n+            v,\n+            ops.multiply(\n+                ops.subtract(ops.square(gradient), v), 1 - self.beta_2\n+            ),\n+        )\n+\n+        m_t_hat = ops.divide(m, (1.0 - beta_1_power))\n+        v_sqrt = ops.add(\n+            ops.sqrt(ops.divide(v, (1.0 - beta_2_power))), self.epsilon\n+        )\n+\n+        update = ops.divide(m_t_hat, v_sqrt)\n+        w_norm = ops.sqrt(ops.sum(ops.power(variable, 2)))\n+        g_norm = ops.sqrt(ops.sum(ops.power(update, 2)))\n+\n+        # ratio = w_norm / g_norm if w_norm > 0 and g_norm > 0 else 1\n+        ratio = ops.where(\n+            ops.greater(w_norm, 0),\n+            ops.where(ops.greater(g_norm, 0), (w_norm / g_norm), 1.0),\n+            1.0,\n+        )\n+\n+        self.assign_sub(variable, ratio * lr * update)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+            }\n+        )\n+        return config\n+\n+\n+Lamb.__doc__ = Lamb.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,76 @@\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src import testing\n+from keras.src.optimizers.lamb import Lamb\n+\n+\n+class LambTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Lamb(\n+            learning_rate=0.5,\n+            beta_1=0.5,\n+            beta_2=0.67,\n+            epsilon=1e-5,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Lamb(learning_rate=0.5)\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [-0.3693, 0.6306, 1.6306, 2.6306], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            ops.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Lamb(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Lamb(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Lamb(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Lamb()\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n+\n+        golden = np.tile(\n+            [[0.999], [0.9982], [0.9974], [0.9965], [0.9955]], (1, 10)\n+        )\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Lamb(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Lamb(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43bcf1598be6800943cd45f56740211d5a1e8bba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 164 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,9 +14,6 @@ class Lamb(optimizer.Optimizer):\n     This helps to stabilize the training process and improves convergence\n     especially for large batch sizes.\n \n-    Based on\n-    [Yang et al.](https://arxiv.org/pdf/1904.00962)\n-\n     Args:\n         learning_rate: A float, a\n             `keras.optimizers.schedules.LearningRateSchedule` instance, or\n@@ -33,6 +30,9 @@ class Lamb(optimizer.Optimizer):\n         epsilon: A small constant for numerical stability.\n             Defaults to `1e-7`.\n         {{base_optimizer_keyword_args}}\n+\n+    References:\n+        - [Yang et al.](https://arxiv.org/pdf/1904.00962)\n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b291f7c83a72155afe69b0c06f94a74a21e96311", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 31 | Files Changed: 4 | Hunks: 26 | Methods Changed: 15 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 105 | Churn Cumulative: 611 | Contributors (this commit): 10 | Commits (past 90d): 20 | Contributors (cumulative): 18 | DMM Complexity: 0.9310344827586207\n\nDIFF:\n@@ -116,18 +116,18 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n             v.value if isinstance(v, backend.Variable) else v\n             for v in trainable_variables\n         ]\n+        grads_and_vars = list(zip(grads, trainable_variables))\n+        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n         tf.__internal__.distribute.interim.maybe_merge_call(\n             self._distributed_tf_update_step,\n             self._distribution_strategy,\n-            list(zip(grads, trainable_variables)),\n+            grads_and_vars,\n             learning_rate,\n         )\n \n     def _distributed_tf_update_step(\n         self, distribution, grads_and_vars, learning_rate\n     ):\n-        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n-\n         def apply_grad_to_update_var(var, grad, learning_rate):\n             return self.update_step(grad, var, learning_rate)\n \n\n@@ -3,6 +3,8 @@\n import numpy as np\n import pytest\n import tensorflow as tf\n+import tf_keras\n+from absl.testing import parameterized\n from tensorflow.python.eager import context\n \n from keras.src import backend\n@@ -14,7 +16,7 @@ from keras.src.optimizers.sgd import SGD\n     backend.backend() != \"tensorflow\",\n     reason=\"The distribute test can only run with TF backend.\",\n )\n-class OptimizerDistributeTest(testing.TestCase):\n+class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n     def setUp(self):\n         super().setUp()\n         # Need at least 2 devices for distribution related tests.\n@@ -39,20 +41,23 @@ class OptimizerDistributeTest(testing.TestCase):\n             )\n         self.run_class_serialization_test(optimizer)\n \n-    def test_single_step(self):\n+    @parameterized.parameters([(SGD,), (tf_keras.optimizers.SGD)])\n+    def test_single_step(self, optimizer_fn):\n         with self.strategy.scope():\n-            optimizer = SGD(\n+            optimizer = optimizer_fn(\n                 learning_rate=0.5,\n                 momentum=0.06,\n             )\n-            grads = tf.constant([1.0, 6.0, 7.0, 2.0])\n-            vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+            # use tf variable to work both in k2 & k3.\n+            vars = tf.Variable([1.0, 2.0, 3.0, 4.0])\n \n-            self.strategy.run(\n-                lambda: optimizer.apply_gradients(zip([grads], [vars]))\n-            )\n+            def update():\n+                grads = tf.constant([1.0, 6.0, 7.0, 2.0])\n+                optimizer.apply_gradients(zip([grads], [vars]))\n+\n+            self.strategy.run(update)\n             self.assertAllClose(\n-                vars, [0.5, -1.0, -0.5, 3.0], rtol=1e-4, atol=1e-4\n+                vars, [0.0, -4.0, -4.0, 2.0], rtol=1e-4, atol=1e-4\n             )\n \n     def test_weight_decay(self):\n@@ -91,31 +96,32 @@ class OptimizerDistributeTest(testing.TestCase):\n     def test_correctness_with_golden(self):\n         with self.strategy.scope():\n             optimizer = SGD(nesterov=True)\n-\n             x = backend.Variable(np.ones([10]))\n-            grads = np.arange(0.1, 1.1, 0.1)\n-            first_grads = np.full((10,), 0.01)\n+\n+            def update_grads():\n+                grads = backend.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n+                optimizer.apply_gradients(zip([grads], [x]))\n+\n+            def update_first_grads():\n+                first_grads = backend.convert_to_tensor(np.full((10,), 0.01))\n+                optimizer.apply_gradients(zip([first_grads], [x]))\n \n         # fmt: off\n         golden = np.array(\n-            [[0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n-            0.9999, 0.9999], [0.9989, 0.9979, 0.9969, 0.9959, 0.9949, 0.9939,\n-            0.9929, 0.9919, 0.9909, 0.9899], [0.9979, 0.9959, 0.9939, 0.9919,\n-            0.9899, 0.9879, 0.9859, 0.9839, 0.9819, 0.9799], [0.9969, 0.9939,\n-            0.9909, 0.9879, 0.9849, 0.9819, 0.9789, 0.9759, 0.9729, 0.9699],\n-            [0.9959, 0.9919, 0.9879, 0.9839, 0.9799, 0.9759, 0.9719, 0.9679,\n-            0.9639, 0.9599]]\n+            [\n+                [0.9980, 0.9960, 0.9940, 0.9920, 0.9900, 0.9880, 0.9860, 0.9840, 0.9820, 0.9800],\n+                [0.9978, 0.9958, 0.9938, 0.9918, 0.9898, 0.9878, 0.9858, 0.9838, 0.9818, 0.9798],\n+                [0.9976, 0.9956, 0.9936, 0.9916, 0.9896, 0.9876, 0.9856, 0.9836, 0.9816, 0.9796],\n+                [0.9974, 0.9954, 0.9934, 0.9914, 0.9894, 0.9874, 0.9854, 0.9834, 0.9814, 0.9794],\n+                [0.9972, 0.9952, 0.9932, 0.9912, 0.9892, 0.9872, 0.9852, 0.9832, 0.9812, 0.9792],\n+            ]\n         )\n         # fmt: on\n \n-        self.strategy.run(\n-            lambda: optimizer.apply_gradients(zip([first_grads], [x]))\n-        )\n+        self.strategy.run(update_grads)\n         for i in range(5):\n             self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n-            self.strategy.run(\n-                lambda: optimizer.apply_gradients(zip([grads], [x]))\n-            )\n+            self.strategy.run(update_first_grads)\n \n     def test_clip_norm(self):\n         with self.strategy.scope():\n@@ -188,7 +194,7 @@ class OptimizerDistributeTest(testing.TestCase):\n             )\n             self.assertAllClose(optimizer.iterations, 2)\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n-            self.assertAllClose(v, [[0.0, 1.0], [1.0, 2.0]])\n+            self.assertAllClose(v, [[-1.0, 0.0], [-1.0, 0.0]])\n             self.assertAllClose(\n                 optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n             )\n\n@@ -291,6 +291,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             steps_per_execution=self.steps_per_execution,\n         )\n \n+        self._symbolic_build(iterator=epoch_iterator)\n+\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -406,6 +408,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n+        self._symbolic_build(iterator=epoch_iterator)\n+\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -525,7 +529,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n     ):\n         self._assert_compile_called(\"train_on_batch\")\n-        self.make_train_function()\n         if class_weight is not None:\n             if sample_weight is not None:\n                 raise ValueError(\n@@ -538,6 +541,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 y, class_weight\n             )\n \n+        # Maybe build model\n+        self._symbolic_build(data_batch=(x, y, sample_weight))\n+        self.make_train_function()\n+\n         def data():\n             yield (x, y, sample_weight)\n \n@@ -555,11 +562,14 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n     ):\n         self._assert_compile_called(\"test_on_batch\")\n-        self.make_test_function()\n \n         def data():\n             yield (x, y, sample_weight)\n \n+        # Maybe build model\n+        self._symbolic_build(data_batch=(x, y, sample_weight))\n+        self.make_test_function()\n+\n         logs = self.test_function(data())\n         logs = tree.map_structure(lambda x: np.array(x), logs)\n         if return_dict:\n@@ -621,6 +631,27 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             x=None, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n \n+    def _symbolic_build(self, iterator=None, data_batch=None):\n+        # Unlike jax/torch iterator, tf iterator returns an iterator instead\n+        # of data batch in `iterator.enumerate_epoch()`.\n+        if iterator is not None:\n+            for _, it in iterator.enumerate_epoch():\n+                maybe_distributed_data_batch = next(it)\n+                has_distributed_values = tree.map_structure(\n+                    lambda x: isinstance(x, tf.distribute.DistributedValues),\n+                    maybe_distributed_data_batch,\n+                )\n+                if all(tree.flatten(has_distributed_values)):\n+                    data_batch = self.distribute_strategy.reduce(\n+                        \"MEAN\",\n+                        maybe_distributed_data_batch,\n+                        axis=None,\n+                    )\n+                else:\n+                    data_batch = maybe_distributed_data_batch\n+                break\n+        super()._symbolic_build(data_batch=data_batch)\n+\n \n class TFEpochIterator(EpochIterator):\n     def __init__(self, distribute_strategy=None, *args, **kwargs):\n\n@@ -1878,8 +1878,12 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         reason=\"synchronized=True only implemented for TF backend\",\n     )\n     def test_moments_sync_with_distribution_strategy(self, dtype):\n+        from tensorflow.python.eager import context\n+\n         from keras.src.utils.module_utils import tensorflow as tf\n \n+        context._reset_context()\n+\n         # Config 2 CPUs for testing.\n         logical_cpus = tf.config.list_logical_devices(\"CPU\")\n         if len(logical_cpus) == 1:\n@@ -1909,6 +1913,8 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(variance.values[0], 8.75)\n             self.assertEqual(variance.values[0], 8.75)\n \n+        context._reset_context()\n+\n     def test_batch_normalization(self):\n         x = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n         mean = np.array([0.2, 0.3, 0.4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#040bf7755e81626cd451752e7aab114d7bb608ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 94 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -632,6 +632,12 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         )\n \n     def _symbolic_build(self, iterator=None, data_batch=None):\n+        if self._distribute_strategy is None:\n+            # When no distribution strategy is set, defer building\n+            # to when the train/test/predict function gets traced.\n+            # This maximizes backwards compatibility.\n+            return\n+\n         # Unlike jax/torch iterator, tf iterator returns an iterator instead\n         # of data batch in `iterator.enumerate_epoch()`.\n         if iterator is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24b8536181deb428c253d52fe9d0197f7e1c8c3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 75 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,6 @@\n import numpy as np\n import pytest\n import tensorflow as tf\n-import tf_keras\n from absl.testing import parameterized\n from tensorflow.python.eager import context\n \n@@ -41,8 +40,14 @@ class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n             )\n         self.run_class_serialization_test(optimizer)\n \n-    @parameterized.parameters([(SGD,), (tf_keras.optimizers.SGD)])\n-    def test_single_step(self, optimizer_fn):\n+    @parameterized.parameters([(\"keras_sgd\",), (\"tf_keras_sgd\",)])\n+    def test_single_step(self, optimizer_type):\n+        if optimizer_type == \"tf_keras_sgd\":\n+            import tf_keras\n+\n+            optimizer_fn = tf_keras.optimizers.SGD\n+        else:\n+            optimizer_fn = SGD\n         with self.strategy.scope():\n             optimizer = optimizer_fn(\n                 learning_rate=0.5,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#576daec845cbc83cebb040e018ba9fdae1902738", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 78 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,9 +43,12 @@ class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters([(\"keras_sgd\",), (\"tf_keras_sgd\",)])\n     def test_single_step(self, optimizer_type):\n         if optimizer_type == \"tf_keras_sgd\":\n+            try:\n                 import tf_keras\n \n                 optimizer_fn = tf_keras.optimizers.SGD\n+            except (ImportError, AttributeError):\n+                self.skipTest(\"tf_keras not installed\")\n         else:\n             optimizer_fn = SGD\n         with self.strategy.scope():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f7e8050f14c4661f17b8be2b217f784e387c21a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 43 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -26,9 +26,9 @@ class Cifar100LoadDataTest(testing.TestCase):\n     def test_dtypes(self):\n         (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n         self.assertEqual(x_train.dtype, np.uint8)\n-        self.assertEqual(y_train.dtype, np.uint8)\n+        self.assertEqual(y_train.dtype, np.int64)\n         self.assertEqual(x_test.dtype, np.uint8)\n-        self.assertEqual(y_test.dtype, np.uint8)\n+        self.assertEqual(y_test.dtype, np.int64)\n \n     def test_invalid_label_mode(self):\n         with self.assertRaises(ValueError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#902f9da309fbf6318b20c9fe33d53b5ab4938644", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 65 | Churn Cumulative: 166 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 0.9318181818181818\n\nDIFF:\n@@ -137,6 +137,12 @@ class Sequential(Model):\n         if isinstance(self._layers[0], InputLayer) and len(self._layers) > 1:\n             input_shape = self._layers[0].batch_shape\n             self.build(input_shape)\n+        elif hasattr(self._layers[0], \"input_shape\") and len(self._layers) > 1:\n+            # We can build the Sequential model if the first layer has the\n+            # `input_shape` property. This is most commonly found in Functional\n+            # model.\n+            input_shape = self._layers[0].input_shape\n+            self.build(input_shape)\n \n     def _lock_state(self):\n         # Unlike other layers, Sequential is mutable after build.\n\n@@ -150,6 +150,58 @@ class SequentialTest(testing.TestCase):\n         y = model(x)\n         self.assertEqual(y.shape, (2, 3, 4))\n \n+    def test_basic_flow_with_functional_model_as_first_layer(self):\n+        # Build functional model\n+        inputs = Input((16, 16, 3))\n+        outputs = layers.Conv2D(4, 3, padding=\"same\")(inputs)\n+        functional_model = Model(inputs=inputs, outputs=outputs)\n+\n+        model = Sequential(\n+            [functional_model, layers.Flatten(), layers.Dense(1)]\n+        )\n+        model.summary()\n+        self.assertEqual(len(model.layers), 3)\n+        self.assertTrue(model.built)\n+        for layer in model.layers:\n+            self.assertTrue(layer.built)\n+\n+        # Test eager call\n+        x = np.random.random((1, 16, 16, 3))\n+        y = model(x)\n+        self.assertEqual(type(model._functional), Functional)\n+        self.assertEqual(tuple(y.shape), (1, 1))\n+\n+        # Test symbolic call\n+        x = backend.KerasTensor((1, 16, 16, 3))\n+        y = model(x)\n+        self.assertEqual(y.shape, (1, 1))\n+\n+    def test_basic_flow_with_sequential_model_as_first_layer(self):\n+        # Build sequential model\n+        sequential_model = Sequential(\n+            [Input((16, 16, 3)), layers.Conv2D(4, 3, padding=\"same\")]\n+        )\n+\n+        model = Sequential(\n+            [sequential_model, layers.Flatten(), layers.Dense(1)]\n+        )\n+        model.summary()\n+        self.assertEqual(len(model.layers), 3)\n+        self.assertTrue(model.built)\n+        for layer in model.layers:\n+            self.assertTrue(layer.built)\n+\n+        # Test eager call\n+        x = np.random.random((1, 16, 16, 3))\n+        y = model(x)\n+        self.assertEqual(type(model._functional), Functional)\n+        self.assertEqual(tuple(y.shape), (1, 1))\n+\n+        # Test symbolic call\n+        x = backend.KerasTensor((1, 16, 16, 3))\n+        y = model(x)\n+        self.assertEqual(y.shape, (1, 1))\n+\n     def test_dict_inputs(self):\n         class DictLayer(layers.Layer):\n             def call(self, inputs):\n\n@@ -96,12 +96,15 @@ def format_layer_shape(layer):\n             )\n     else:\n         try:\n+            if hasattr(layer, \"output_shape\"):\n+                output_shapes = layer.output_shape\n+            else:\n                 outputs = layer.compute_output_shape(**layer._build_shapes_dict)\n-        except NotImplementedError:\n-            return \"?\"\n                 output_shapes = tree.map_shape_structure(\n                     lambda x: format_shape(x), outputs\n                 )\n+        except NotImplementedError:\n+            return \"?\"\n     if len(output_shapes) == 1:\n         return output_shapes[0]\n     out = str(output_shapes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#808b78618e5928797bc928b99c66922c41454698", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 51 | Files Changed: 13 | Hunks: 29 | Methods Changed: 22 | Complexity Δ (Sum/Max): 1/4 | Churn Δ: 98 | Churn Cumulative: 652 | Contributors (this commit): 6 | Commits (past 90d): 26 | Contributors (cumulative): 31 | DMM Complexity: 0.25\n\nDIFF:\n@@ -142,7 +142,7 @@ def distribute_data_input(inputs, layout):\n                 f\"{num_split}\"\n             )\n         global_batch_size = per_process_batch_size * jax.process_count()\n-        per_replica_batches = jax.numpy.split(inputs, num_split, axis=0)\n+        per_replica_batches = np.split(inputs, num_split, axis=0)\n     elif mesh_rank == 2:\n         # Data+Model parallel\n         # In this case, we need to check if the mesh batch dim shape is large\n\n@@ -243,8 +243,6 @@ class ArrayDataAdapter(DataAdapter):\n         return dataset.prefetch(tf.data.AUTOTUNE)\n \n     def get_jax_iterator(self):\n-        from keras.src.backend.jax.core import convert_to_tensor\n-\n         inputs = array_slicing.convert_to_sliceable(\n             self._inputs, target_backend=\"jax\"\n         )\n@@ -252,7 +250,6 @@ class ArrayDataAdapter(DataAdapter):\n         def slice_and_convert_to_jax(sliceable, indices=None):\n             x = sliceable[indices]\n             x = sliceable.convert_to_jax_compatible(x)\n-            x = convert_to_tensor(x)\n             return x\n \n         return self._get_iterator(slice_and_convert_to_jax, inputs)\n\n@@ -92,7 +92,7 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n             if array_type in (\"tf_sparse\", \"jax_sparse\", \"scipy_sparse\"):\n                 expected_class = jax_sparse.JAXSparse\n             else:\n-                expected_class = jax.Array\n+                expected_class = np.ndarray\n         elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n\n@@ -91,8 +91,7 @@ class Sliceable:\n     def convert_to_jax_compatible(cls, x):\n         \"\"\"Convert a tensor to something that the JAX backend can consume.\n \n-        This can be a `JAX` array, NumPy array or any other type of tensor that\n-        `keras.backend.jax.core.convert_to_tensor()` can consume.\n+        This can be a `JAX` array, `JAXSparse` or a NumPy array.\n         Only called after slicing using `__getitem__`.\n         Used to convert sparse tensors and densify ragged tensors.\n \n@@ -147,7 +146,7 @@ class TensorflowSliceable(Sliceable):\n class TensorflowRaggedSliceable(TensorflowSliceable):\n     @classmethod\n     def convert_to_jax_compatible(cls, x):\n-        return x.to_tensor()\n+        return cls.convert_to_numpy(x)\n \n     @classmethod\n     def convert_to_torch_compatible(cls, x):\n@@ -180,7 +179,7 @@ class TensorflowSparseSliceable(TensorflowSliceable):\n         return tf_sparse.sparse_to_dense(x)\n \n \n-class JaxSliceable(Sliceable):\n+class JaxSparseSliceable(Sliceable):\n     def __getitem__(self, indices):\n         return self.array[indices, ...]\n \n@@ -190,8 +189,6 @@ class JaxSliceable(Sliceable):\n \n         return convert_to_numpy(x)\n \n-\n-class JaxSparseSliceable(JaxSliceable):\n     @classmethod\n     def convert_to_tf_dataset_compatible(cls, array):\n         return to_tensorflow_sparse_wrapper(\n@@ -386,7 +383,8 @@ def convert_to_sliceable(arrays, target_backend=None):\n             if data_adapter_utils.is_jax_sparse(x):\n                 sliceable_class = JaxSparseSliceable\n             else:\n-                sliceable_class = JaxSliceable\n+                x = np.asarray(x)\n+                sliceable_class = NumpySliceable\n         elif data_adapter_utils.is_torch_tensor(x):\n             sliceable_class = TorchSliceable\n         elif pandas is not None and isinstance(x, pandas.DataFrame):\n@@ -433,14 +431,14 @@ def convert_to_sliceable(arrays, target_backend=None):\n         if target_backend == \"tensorflow\":\n             return sliceable_class.convert_to_tf_dataset_compatible(x)\n \n-        # With dense arrays, with JAX as either input or output, it is faster to\n-        # use NumPy as an intermediary representation, so wrap input array in a\n-        # NumPy array, which should not use extra memory. For the input case,\n-        # see https://github.com/google/jax/issues/1276 for an explanation of\n+        # With dense arrays and JAX as output, it is faster to use NumPy as an\n+        # intermediary representation, so wrap input array in a NumPy array,\n+        # which should not use extra memory.\n+        # See https://github.com/google/jax/issues/1276 for an explanation of\n         # why slicing a NumPy array is faster than slicing a JAX array.\n-        if sliceable_class == JaxSliceable or (\n-            target_backend == \"jax\"\n-            and sliceable_class in (TensorflowSliceable, TorchSliceable)\n+        if target_backend == \"jax\" and sliceable_class in (\n+            TensorflowSliceable,\n+            TorchSliceable,\n         ):\n             x = np.asarray(x)\n             sliceable_class = NumpySliceable\n\n@@ -30,7 +30,8 @@ class DataAdapter:\n         raise NotImplementedError\n \n     def get_jax_iterator(self):\n-        \"\"\"Get a Python iterable for the `DataAdapter`, that yields JAX arrays.\n+        \"\"\"Get a Python iterable for the `DataAdapter`, that yields arrays that\n+        that can be fed to JAX. NumPy arrays are preferred for performance.\n \n         Returns:\n             A Python iterator.\n\n@@ -176,10 +176,21 @@ def get_tensor_spec(batches):\n \n \n def get_jax_iterator(iterable):\n-    from keras.src.backend.jax.core import convert_to_tensor\n+    import jax\n+    import jax.experimental.sparse as jax_sparse\n+\n+    def convert_to_jax_compatible(x):\n+        if isinstance(x, (jax.Array, jax_sparse.JAXSparse, np.ndarray)):\n+            return x\n+        elif is_scipy_sparse(x):\n+            return scipy_sparse_to_jax_sparse(x)\n+        elif is_tensorflow_sparse(x):\n+            return tf_sparse_to_jax_sparse(x)\n+        else:\n+            return np.asarray(x)\n \n     for batch in iterable:\n-        yield tree.map_structure(convert_to_tensor, batch)\n+        yield tree.map_structure(convert_to_jax_compatible, batch)\n \n \n def get_numpy_iterator(iterable):\n@@ -289,16 +300,20 @@ def scipy_sparse_to_tf_sparse(x):\n \n \n def scipy_sparse_to_jax_sparse(x):\n+    import jax\n     import jax.experimental.sparse as jax_sparse\n \n+    with jax.default_device(jax.local_devices(backend=\"cpu\")[0]):\n         return jax_sparse.BCOO.from_scipy_sparse(x)\n \n \n def tf_sparse_to_jax_sparse(x):\n+    import jax\n     import jax.experimental.sparse as jax_sparse\n \n     values = np.asarray(x.values)\n     indices = np.asarray(x.indices)\n+    with jax.default_device(jax.local_devices(backend=\"cpu\")[0]):\n         return jax_sparse.BCOO((values, indices), shape=x.shape)\n \n \n\n@@ -26,17 +26,7 @@ class GeneratorDataAdapter(DataAdapter):\n         return data_adapter_utils.get_numpy_iterator(self.generator)\n \n     def get_jax_iterator(self):\n-        from keras.src.backend.jax.core import convert_to_tensor\n-\n-        def convert_to_jax(x):\n-            if data_adapter_utils.is_scipy_sparse(x):\n-                return data_adapter_utils.scipy_sparse_to_jax_sparse(x)\n-            elif data_adapter_utils.is_tensorflow_sparse(x):\n-                return data_adapter_utils.tf_sparse_to_jax_sparse(x)\n-            return convert_to_tensor(x)\n-\n-        for batch in self.generator:\n-            yield tree.map_structure(convert_to_jax, batch)\n+        return data_adapter_utils.get_jax_iterator(self.generator)\n \n     def get_tf_dataset(self):\n         from keras.src.utils.module_utils import tensorflow as tf\n\n@@ -73,7 +73,9 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n             expected_class = tf.Tensor\n         elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-            expected_class = jax.Array\n+            expected_class = (\n+                jax.Array if generator_type == \"jax\" else np.ndarray\n+            )\n         elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n\n@@ -177,7 +177,7 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             expected_class = tf.Tensor\n         elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-            expected_class = jax.Array\n+            expected_class = jax.Array if dataset_type == \"jax\" else np.ndarray\n         elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n\n@@ -41,20 +41,15 @@ class TFDatasetAdapter(DataAdapter):\n             yield tree.map_structure(convert_to_numpy, batch)\n \n     def get_jax_iterator(self):\n-        import jax.experimental.sparse as jax_sparse\n-\n-        from keras.src.backend.jax.core import convert_to_tensor\n         from keras.src.backend.tensorflow.core import convert_to_numpy\n         from keras.src.utils.module_utils import tensorflow as tf\n \n         def convert_to_jax(x):\n-            # We use numpy as an intermediary because the conversion\n-            # tf -> numpy -> jax is more than 2x faster than tf -> jax.\n             if isinstance(x, tf.SparseTensor):\n-                values = convert_to_numpy(x.values)\n-                indices = convert_to_numpy(x.indices)\n-                return jax_sparse.BCOO((values, indices), shape=x.shape)\n-            return convert_to_tensor(convert_to_numpy(x))\n+                return data_adapter_utils.tf_sparse_to_jax_sparse(x)\n+            else:\n+                # We use numpy as an intermediary because it is faster.\n+                return convert_to_numpy(x)\n \n         for batch in self._dataset:\n             yield tree.map_structure(convert_to_jax, batch)\n\n@@ -32,7 +32,7 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n             expected_class = tf.Tensor\n         elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-            expected_class = jax.Array\n+            expected_class = np.ndarray\n         elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n\n@@ -39,9 +39,8 @@ class TorchDataLoaderAdapter(DataAdapter):\n             )\n \n     def get_jax_iterator(self):\n-        # We use numpy as an intermediary because the conversion\n-        # torch -> numpy -> jax is faster than torch -> jax.\n-        return data_adapter_utils.get_jax_iterator(self.get_numpy_iterator())\n+        # We use numpy as an intermediary because it is faster.\n+        return self.get_numpy_iterator()\n \n     def get_tf_dataset(self):\n         from keras.src.utils.module_utils import tensorflow as tf\n\n@@ -1,6 +1,5 @@\n import math\n \n-import jax\n import numpy as np\n import tensorflow as tf\n import torch\n@@ -35,7 +34,7 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n             expected_class = tf.Tensor\n         elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-            expected_class = jax.Array\n+            expected_class = np.ndarray\n         elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n@@ -104,7 +103,7 @@ class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n             expected_class = tf.Tensor\n         elif backend.backend() == \"jax\":\n             it = adapter.get_jax_iterator()\n-            expected_class = jax.Array\n+            expected_class = np.ndarray\n         elif backend.backend() == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
