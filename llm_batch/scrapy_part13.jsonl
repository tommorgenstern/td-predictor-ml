{"custom_id": "scrapy#b57148dfad069b2c77d281ae05e20121dca74779", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 14 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ if sys.version_info < (2,5):\n     sys.exit(1)\n \n # add external python libraries bundled into scrapy\n-sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), \"lib\"))\n+sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), \"xlib\"))\n \n # monkey patches to fix external library issues\n from scrapy.patches import monkeypatches\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41a4f22c27ea96e7e32cafc6b1e4a68a7585db34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 437 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -71,6 +71,9 @@ class MediaPipeline(object):\n         return wad\n \n     def _download(self, request, info, fp):\n+        def _bugtrap(_failure):\n+            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n+\n         dwld = mustbe_deferred(self.download, request, info)\n         dwld.addCallbacks(\n                 callback=self.media_downloaded,\n@@ -79,12 +82,9 @@ class MediaPipeline(object):\n                 errbackArgs=(request, info),\n                 )\n         dwld.addBoth(self._downloaded, info, fp)\n+        dwld.addErrback(_bugtrap)\n         info.downloading[fp] = (request, dwld)\n \n-        def _bugtrap(_failure):\n-            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n-        dwld.errback(_bugtrap)\n-\n     def _downloaded(self, result, info, fp):\n         info.downloaded[fp] = result # cache result\n         waiting = info.waiting[fp] # client list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5048491f8dae81cf7910eb54e542df4ae0427cbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 445 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -21,16 +21,16 @@ class DomainInfo(object):\n \n class MediaPipeline(object):\n     def __init__(self):\n-        self.cache = {}\n+        self.domaininfo = {}\n \n     def open_domain(self, domain):\n-        self.cache[domain] = DomainInfo(domain)\n+        self.domaininfo[domain] = DomainInfo(domain)\n \n     def close_domain(self, domain):\n-        del self.cache[domain]\n+        del self.domaininfo[domain]\n \n     def process_item(self, domain, response, item):\n-        info = self.cache[domain]\n+        info = self.domaininfo[domain]\n         requests = self.get_media_requests(item, info)\n         assert requests is None or hasattr(requests, '__iter__'), \\\n                 'get_media_requests should return None or iterable'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4807f2eb82e1b4aa4dfb506511550ead30c60497", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 470 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,10 +55,6 @@ class MediaPipeline(object):\n         return dlst\n \n     def _enqueue(self, request, info):\n-        result = self.media_to_download(request, info)\n-        if result is not None:\n-            return defer_result(result)\n-\n         fp = request.fingerprint()\n         if fp in info.downloaded:\n             return defer_result(info.downloaded[fp])\n@@ -74,6 +70,10 @@ class MediaPipeline(object):\n         def _bugtrap(_failure):\n             log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' % (_failure), log.ERROR, domain=info.domain)\n \n+        result = self.media_to_download(request, info)\n+        if result is not None:\n+            dwld = defer_result(result)\n+        else:\n             dwld = mustbe_deferred(self.download, request, info)\n             dwld.addCallbacks(\n                     callback=self.media_downloaded,\n@@ -81,6 +81,7 @@ class MediaPipeline(object):\n                     errback=self.media_failed,\n                     errbackArgs=(request, info),\n                     )\n+\n         dwld.addBoth(self._downloaded, info, fp)\n         dwld.addErrback(_bugtrap)\n         info.downloading[fp] = (request, dwld)\n@@ -110,18 +111,14 @@ class MediaPipeline(object):\n     def media_to_download(self, request, info):\n         \"\"\" Ongoing request hook pre-cache\n \n-        This method is called every time an item media is enqueue for download.\n+        This method is called every time a media is requested for download, and\n+        only once for the same request because return value is cached as media\n+        result.\n \n         returning a non-None value implies:\n-            - call `item_media_downloaded` with this value as input unless value is Failure instance\n-            - call `item_media_failed` if value is Failure instance\n-            - prevent downloading, this means calling `download` method.\n-            - prevent taking the value from the cached result for this request.\n-\n-        Take in count that this method doesn't cache returned value as request result.\n-\n-        Override `download` method if you want to hook after checking for cached\n-        result, and if you want to store returned valued as result for request.\n+            - the return value is cached and piped into `item_media_downloaded` or `item_media_failed`\n+            - prevents downloading, this means calling `download` method.\n+            - `media_downloaded` or `media_failed` isn't called.\n \n         \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7932a8a5b649b0ef98b5f6fbcdef0cd4e9b2154d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 183 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 34/34 | Churn Δ: 183 | Churn Cumulative: 183 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.839622641509434\n\nDIFF:\n@@ -0,0 +1,183 @@\n+from __future__ import with_statement\n+\n+import re\n+import os\n+import time\n+import sha\n+import urllib\n+import urlparse\n+from cStringIO import StringIO\n+\n+import Image\n+\n+from scrapy.core import log\n+from scrapy.stats import stats\n+from scrapy.http import Request\n+from scrapy.core.exceptions import DropItem, NotConfigured\n+from scrapy.core.exceptions import HttpException\n+from scrapy.conf import settings\n+\n+from scrapy.contrib.pipeline.media import MediaPipeline\n+\n+# the age at which we download images again\n+IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n+\n+class NoimagesDrop(DropItem):\n+    pass\n+\n+class ImageException(Exception):\n+    \"\"\"General image error exception\"\"\"\n+\n+\n+class ImagesPipeline(MediaPipeline):\n+    MEDIA_TYPE = 'image'\n+    THUMBS = None\n+#     THUMBS = (\n+#             (\"50\", (50, 50)),\n+#             (\"110\", (110, 110)),\n+#             (\"270\", (270, 270))\n+#     )\n+    MIN_WIDTH = 0\n+    MIN_HEIGHT = 0\n+\n+    def __init__(self):\n+        if not settings['IMAGES_DIR']:\n+            raise NotConfigured\n+\n+        self.BASEDIRNAME = settings['IMAGES_DIR']\n+        self.mkdir(self.BASEDIRNAME)\n+\n+        self.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n+        self.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n+        MediaPipeline.__init__(self)\n+\n+    def media_to_download(self, request, info):\n+        relative, absolute = self._get_paths(request)\n+        if not should_download(absolute):\n+            self.inc_stats(info.domain, 'uptodate')\n+            referer = request.headers.get('Referer')\n+            log.msg('Image (uptodate): Downloaded %s from %s referred in <%s>' % \\\n+                    (self.MEDIA_TYPE, request, referer), level=log.DEBUG, domain=info.domain)\n+            return relative\n+\n+    def media_downloaded(self, response, request, info):\n+        mtype = self.MEDIA_TYPE\n+        referer = request.headers.get('Referer')\n+\n+        if not response or not response.body.to_string():\n+            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: Empty image (no-content)' % (mtype, request, referer)\n+            log.msg(msg, level=log.WARNING, domain=info.domain)\n+            raise ImageException(msg)\n+\n+        result = self.save_image(response, request, info) # save and thumbs response\n+\n+        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n+        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n+        log.msg(msg, level=log.DEBUG, domain=info.domain)\n+        self.inc_stats(info.domain, status)\n+        return result\n+\n+    def media_failed(self, failure, request, info):\n+        referer = request.headers.get('Referer')\n+        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else failure.getTraceback()\n+        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n+        log.msg(msg, level=log.WARNING, domain=info.domain)\n+        raise ImageException(msg)\n+\n+    def save_image(self, response, request, info):\n+        mtype = self.MEDIA_TYPE\n+        relpath, abspath = self._get_paths(request)\n+        dirname = os.path.dirname(abspath)\n+        self.mkdir(dirname, info)\n+\n+        try:\n+            save_image_with_thumbnails(response, abspath, self.THUMBS, self.MIN_WIDTH, self.MIN_HEIGHT)\n+        except ImageException, ex:\n+            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n+            raise ex\n+        except Exception, ex:\n+            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' % (mtype, request, referer, ex)\n+            log.msg(msg, level=log.WARNING, domain=info.domain)\n+            raise ImageException(msg)\n+\n+        return relpath # success value sent as input result for item_media_downloaded\n+\n+    def _get_paths(self, request):\n+        relative = image_path(request.url)\n+        absolute = os.path.join(self.BASEDIRNAME, relative)\n+        return relative, absolute\n+\n+    def mkdir(self, dirname, info=None):\n+        already_created = info.extra.setdefault('created_directories', set()) if info else set()\n+        if dirname not in already_created:\n+            if not os.path.exists(dirname):\n+                os.makedirs(dirname)\n+            already_created.add(dirname)\n+\n+    def inc_stats(self, domain, status):\n+        stats.incpath('%s/image_count' % domain)\n+        stats.incpath('%s/image_status_count/%s' % (domain, status))\n+\n+\n+\n+def should_download(path):\n+    \"\"\"Should the image downloader download the image to the location specified\n+    \"\"\"\n+    try:\n+        mtime = os.path.getmtime(path)\n+        age_seconds = time.time() - mtime\n+        age_days = age_seconds / 60 / 60 / 24\n+        return age_days > IMAGE_EXPIRES\n+    except:\n+        return True\n+\n+_MULTIPLE_SLASHES_REGEXP = re.compile(r\"\\/{2,}\")\n+_FINAL_SLASH_REGEXP = re.compile(r\"\\/$\")\n+def image_path(url):\n+    \"\"\"Return the relative path on the target filesystem for an image to be\n+    downloaded to.\n+    \"\"\"\n+    _, netloc, urlpath, query, _ = urlparse.urlsplit(url)\n+    urlpath = _MULTIPLE_SLASHES_REGEXP.sub('/', urlpath)\n+    urlpath = _FINAL_SLASH_REGEXP.sub('.jpg', urlpath)\n+    if os.sep != '/':\n+        urlpath.replace('/', os.sep)\n+    if query:\n+        img_path = os.path.join(netloc, sha.sha(url).hexdigest())\n+    else:\n+        img_path = os.path.join(netloc, urlpath[1:])\n+    return urllib.unquote(img_path)\n+\n+\n+def thumbnail_name(image, sizestr):\n+    \"\"\"Get the name of a thumbnail image given the name of the original file.\n+\n+    There will can be many types of thumbnails, so we will have a \"name\" for\n+    each type.\n+    \"\"\"\n+    return os.path.splitext(image)[0] + '_' + sizestr + '.jpg'\n+\n+def save_scaled_image(image, img_path, name, size):\n+    thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n+    thumb.thumbnail(size, Image.ANTIALIAS)\n+    filename = thumbnail_name(img_path, name)\n+    thumb.save(filename, 'JPEG')\n+\n+def save_image_with_thumbnails(response, path, thumbsizes, min_width=0, min_height=0):\n+    memoryfile = StringIO(response.body.to_string())\n+    im = Image.open(memoryfile)\n+    if im.mode != 'RGB':\n+        log.msg(\"Found non-RGB image during scraping %s\" % path, level=log.WARNING)\n+    for name, size in thumbsizes or []:\n+        save_scaled_image(im, path, name, size)\n+    try:\n+        im.save(path)\n+    except Exception, ex:\n+        log.msg(\"Image (processing-error): cannot process %s, so writing direct file: Error: %s\" % (path, ex))\n+        f = open(path, 'wb')\n+        f.write(response.body.to_string())\n+        f.close()\n+    width, height = im.size\n+    if width < min_width or height < min_height:\n+        raise ImageException(\"Image too small (%dx%d < %dx%d): %s\" % (width, height, min_width, min_height, response.url))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f2d36199b20069c9d7d5af3170f33f9147321ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 472 | Contributors (this commit): 1 | Commits (past 90d): 13 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,7 @@ class MediaPipeline(object):\n         if fp not in info.downloading:\n             self._download(request, info, fp)\n \n-        wad = defer.Deferred()\n+        wad = request.deferred or defer.Deferred()\n         waiting = info.waiting.setdefault(fp, []).append(wad)\n         return wad\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93c313853d8a47534494196875cd7be63e50a913", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 484 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,15 +55,17 @@ class MediaPipeline(object):\n         return dlst\n \n     def _enqueue(self, request, info):\n-        fp = request.fingerprint()\n-        if fp in info.downloaded:\n-            return defer_result(info.downloaded[fp])\n+        wad = request.deferred or defer.Deferred()\n \n+        fp = request.fingerprint()\n+        if fp not in info.downloaded:\n+            cached = info.downloaded[fp]\n+            defer_result(cached).chainDeferred(wad)\n+        else:\n+            info.waiting.setdefault(fp, []).append(wad)\n             if fp not in info.downloading:\n                 self._download(request, info, fp)\n \n-        wad = request.deferred or defer.Deferred()\n-        waiting = info.waiting.setdefault(fp, []).append(wad)\n         return wad\n \n     def _download(self, request, info, fp):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a85e53053dadfb7ee5673dda26e7b953834f2f07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 29 | Churn Cumulative: 29 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,29 @@\n+from setuptools import setup, find_packages\n+import os, os.path, glob\n+\n+def findfiles(pattern, base='.'):\n+    matches = []\n+    for root, _, _ in os.walk(base):\n+        matches.extend(glob.glob(os.path.join(root, pattern)))\n+    return matches\n+\n+\n+name = 'scrapy'\n+\n+setup (\n+    name = name,\n+    version = '0.1',\n+    description = '',\n+    long_description = '',\n+    author = '',\n+    author_email = '',\n+    license = '',\n+    url = 'http://scrapy.org',\n+\n+    packages = [name] + ['%s.%s' % (name,p) for p in find_packages('scrapy')],\n+    package_data = {name:\n+        findfiles('*.tmpl', 'scrapy/templates')\n+    },\n+    data_files = [],\n+    scripts = ['scrapy/bin/scrapy-admin.py'],\n+)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#818d9baf69b41e70687641a8f006fea946e4d63f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 36 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -20,10 +20,9 @@ setup (\n     license = '',\n     url = 'http://scrapy.org',\n \n-    packages = [name] + ['%s.%s' % (name,p) for p in find_packages('scrapy')],\n-    package_data = {name:\n-        findfiles('*.tmpl', 'scrapy/templates')\n+    packages = find_packages(),\n+    package_data = {\n+        '': ['*.tmpl'],\n     },\n-    data_files = [],\n     scripts = ['scrapy/bin/scrapy-admin.py'],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6d4a37a98cdff22cf4aeefcb7bb818d6164ff08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ setup(\n \n     packages = find_packages(),\n     package_data = {\n-        '': ['*.tmpl'],\n+        'scrapy': ['templates/*.tmpl'],\n     },\n     scripts = ['scrapy/bin/scrapy-admin.py'],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfb56e39bb39caf9e86bfdcf5ef9d22838dba984", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -8,11 +8,9 @@ def findfiles(pattern, base='.'):\n     return matches\n \n \n-name = 'scrapy'\n-\n setup(\n-    name = name,\n-    version = '0.1',\n+    name = 'scrapy',\n+    version = '0.8',\n     description = '',\n     long_description = '',\n     author = '',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ce7ae4df8f19ff5df2ffd4e46aefd18dfb3e1c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 199 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -140,7 +140,7 @@ class Downloader(object):\n             return\n \n         # Instanciate site specific handling based on info provided by spider\n-        delay = getattr(spider, 'download_delay', None)\n+        delay = settings.getint('DOWNLOAD_DELAY', None) or getattr(spider, 'download_delay', None)\n         maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n         site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n         self.sites[domain] = site\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#133e9242d3d65d4da546a13bae67b840898c62d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 85 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,12 +6,11 @@ from __future__ import with_statement\n import os\n import urlparse\n \n-from twisted.web.client import HTTPClientFactory\n from twisted.internet import defer, reactor\n from twisted.web import error as web_error\n \n from scrapy.core import signals\n-from scrapy.http import Request, Response, Headers\n+from scrapy.http import Request, Response, Headers, HTTPClientFactory\n from scrapy.core.exceptions import UsageError, HttpException\n from scrapy.utils.misc import defer_succeed\n from scrapy.conf import settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#332460851b6aaaa54007ec98bb72d0ef21bbf330", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 289 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,11 +6,12 @@ from __future__ import with_statement\n import os\n import urlparse\n \n+from twisted.web.client import HTTPClientFactory\n from twisted.internet import defer, reactor\n from twisted.web import error as web_error\n \n from scrapy.core import signals\n-from scrapy.http import Request, Response, Headers, HTTPClientFactory\n+from scrapy.http import Request, Response, Headers\n from scrapy.core.exceptions import UsageError, HttpException\n from scrapy.utils.misc import defer_succeed\n from scrapy.conf import settings\n\n@@ -140,7 +140,7 @@ class Downloader(object):\n             return\n \n         # Instanciate site specific handling based on info provided by spider\n-        delay = settings.getint('DOWNLOAD_DELAY', None) or getattr(spider, 'download_delay', None)\n+        delay = settings.getint('DOWNLOAD_DELAY') or getattr(spider, 'download_delay', None)\n         maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n         site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n         self.sites[domain] = site\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddd7393a0ab3b6938eaa72b8171af79a27a92746", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 203 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -140,7 +140,7 @@ class Downloader(object):\n             return\n \n         # Instanciate site specific handling based on info provided by spider\n-        delay = settings.getint('DOWNLOAD_DELAY') or getattr(spider, 'download_delay', None)\n+        delay = getattr(spider, 'download_delay', None)\n         maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n         site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n         self.sites[domain] = site\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d87f979d2a16b3bb6d7ef0adf6439001914b0038", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 486 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,7 @@ class MediaPipeline(object):\n         wad = request.deferred or defer.Deferred()\n \n         fp = request.fingerprint()\n-        if fp not in info.downloaded:\n+        if fp in info.downloaded:\n             cached = info.downloaded[fp]\n             defer_result(cached).chainDeferred(wad)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#baabdb004dec9f42ef0163d2981276dea4c88800", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 242 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -73,8 +73,8 @@ def main():\n         try:\n             open(opts.output, \"w\").write(output)\n         except IOError:\n-            open(\"/tmp/decobot-schedule.tmp\", \"w\").write(output)\n-            print \"Could not open file %s for writing. Output dumped to /tmp/decobot-schedule.tmp instead.\" % opts.output\n+            open(\"/tmp/scrapy-cluster-schedule.tmp\", \"w\").write(output)\n+            print \"Could not open file %s for writing. Output dumped to /tmp/scrapy-cluster-schedule.tmp instead.\" % opts.output\n \n if __name__ == '__main__':\n     main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62975817af43b3a94437cb564a9a3ee47642dcc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,8 @@ import re\n \n from scrapy.spider import BaseSpider\n from scrapy.item import ScrapedItem\n-from decobot.utils.link_extraction import follow_link_pattern\n+from scrapy.link import LinkExtractor\n+from scrapy.http import Request\n \n class TestSpider(BaseSpider):\n     domain_name = \"scrapytest.org\"\n@@ -19,7 +20,11 @@ class TestSpider(BaseSpider):\n     price_re = re.compile(\">Price: \\$(.*?)<\", re.M)\n \n     def parse(self, response):\n-        return follow_link_pattern(response.body.to_string(), self.parse_item, response.url, self.itemurl_re)\n+        xlink = LinkExtractor()\n+        itemre = re.compile(self.itemurl_re)\n+        for url in xlink.extract_urls(response):\n+            if itemre.search(url):\n+                yield Request(url=url, callback=self.parse_item)\n \n     def parse_item(self, response):\n         item = ScrapedItem()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4d388c946e2ced7258c032208712d11814d1e50f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,11 @@\n+from django.contrib import admin\n+\n+from scrapyorg.download.models import DownloadLink\n+\n+\n+class DownloadLinkAdmin(admin.ModelAdmin):\n+    list_display = (\"text\", \"address\", \"public_link\", \"created\")\n+    list_filter = (\"public\", \"created\")\n+    search_fields = (\"address\", \"text\")\n+\n+admin.site.register(DownloadLink, DownloadLinkAdmin)\n\n@@ -10,12 +10,11 @@ class DownloadLink(models.Model):\n     address = models.CharField(_(\"address\"), max_length=1024, blank=False)\n     text = models.CharField(_(\"link text\"), max_length=512, blank=False,\n                             default=_(\"download\"))\n-    public = models.BooleanField(_(\"public\"), core=True, blank=False,\n-                                 default=True)\n+    public = models.BooleanField(_(\"public\"), blank=False, default=True)\n \n     # automatic dates\n-    created = models.DateTimeField(_(\"created\"), core=True, editable=False)\n-    updated = models.DateTimeField(_(\"updated\"), core=True, editable=False)\n+    created = models.DateTimeField(_(\"created\"), editable=False)\n+    updated = models.DateTimeField(_(\"updated\"), editable=False)\n \n     def toggle_public(self):\n         self.public = not self.public\n@@ -41,10 +40,10 @@ class DownloadLink(models.Model):\n     public_link.short_description = u\"public\"\n     public_link.allow_tags = True\n \n-    class Admin:\n-        list_display = (\"text\", \"address\", \"public_link\", \"created\")\n-        list_filter = (\"public\", \"created\")\n-        save_on_top = True\n+#     class Admin:\n+#         list_display = (\"text\", \"address\", \"public_link\", \"created\")\n+#         list_filter = (\"public\", \"created\")\n+#         save_on_top = True\n \n     class Meta:\n         verbose_name = _(\"download link\")\n\n@@ -1,23 +1,28 @@\n-from django.conf.urls.defaults import *\n-from django.views.generic.simple import direct_to_template\n from django.conf import settings\n+from django.conf.urls.defaults import *\n+from django.contrib import admin\n+\n+\n+admin.autodiscover()\n \n \n urlpatterns = patterns('',\n-    (r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n+    url(r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n \n     # admin\n-    (r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n-    (r\"^admin/\", include(\"django.contrib.admin.urls\")),\n+    url(r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n+    url(r'^admin/(.*)', admin.site.root),\n )\n \n \n if settings.DEBUG: # devel\n     urlpatterns += patterns('',         \n-        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:], 'django.views.static.serve', {'document_root': settings.MEDIA_ROOT}),\n+        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:],\n+          'django.views.static.serve',\n+          {'document_root': settings.MEDIA_ROOT}),\n     )\n     \n # last resort, it's an article\n urlpatterns += patterns('',         \n-    (r\"\", include(\"scrapyorg.article.urls\")),\n+    url(r\"\", include(\"scrapyorg.article.urls\")),\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd28045b99ed05d9cfa9b524f979c306bc58c61a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 82 | Files Changed: 6 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 82 | Churn Cumulative: 86 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,14 +0,0 @@\n-from django.contrib.syndication.feeds import Feed\n-from django_website.apps.blog.models import Entry\n-import datetime\n-\n-class WeblogEntryFeed(Feed):\n-    title = \"The Django weblog\"\n-    link = \"http://www.djangoproject.com/weblog/\"\n-    description = \"Latest news about Django, the Python Web framework.\"\n-\n-    def items(self):\n-        return Entry.objects.filter(pub_date__lte=datetime.datetime.now())[:10]\n-\n-    def item_pubdate(self, item):\n-        return item.pub_date\n\n@@ -1,25 +0,0 @@\n-import datetime\n-from django.db import models\n-\n-\n-class Entry(models.Model):\n-    pub_date = models.DateTimeField()\n-    slug = models.SlugField(unique_for_date='pub_date')\n-    headline = models.CharField(max_length=200)\n-    summary = models.TextField(help_text=\"Use raw HTML.\")\n-    body = models.TextField(help_text=\"Use raw HTML.\")\n-    author = models.CharField(max_length=100)\n-\n-    class Meta:\n-        verbose_name_plural = 'entries'\n-        ordering = ('-pub_date',)\n-        get_latest_by = 'pub_date'\n-\n-    class Admin:\n-        list_display = ('pub_date', 'headline', 'author')\n-\n-    def __unicode__(self):\n-        return self.headline\n-\n-    def get_absolute_url(self):\n-        return \"/weblog/%s/%s/\" % (self.pub_date.strftime(\"%Y/%b/%d\").lower(), self.slug)\n\n\n@@ -1,28 +0,0 @@\n-import datetime\n-\n-from django import template\n-\n-from scrapyorg.blog.models import Entry\n-\n-\n-class LatestBlogEntriesNode(template.Node):\n-    def __init__(self, num, varname):\n-        self.num, self.varname = num, varname\n-\n-    def render(self, context):\n-        context[self.varname] = list(Entry.objects.filter(pub_date__lte=datetime.datetime.now())[:self.num])\n-        return ''\n-\n-def do_get_latest_blog_entries(parser, token):\n-    \"\"\"\n-    {% get_latest_blog_entries 2 as latest_entries %}\n-    \"\"\"\n-    bits = token.contents.split()\n-    if len(bits) != 4:\n-        raise template.TemplateSyntaxError, \"'%s' tag takes three arguments\" % bits[0]\n-    if bits[2] != 'as':\n-        raise template.TemplateSyntaxError, \"First argument to '%s' tag must be 'as'\" % bits[0]\n-    return LatestBlogEntriesNode(bits[1], bits[3])\n-\n-register = template.Library()\n-register.tag('get_latest_blog_entries', do_get_latest_blog_entries)\n\n@@ -1,15 +0,0 @@\n-from django.conf.urls.defaults import *\n-from scrapyorg.blog.models import Entry # relative import\n-\n-info_dict = {\n-    'queryset': Entry.objects.all(),\n-    'date_field': 'pub_date',\n-}\n-\n-urlpatterns = patterns('django.views.generic.date_based',\n-   (r'^(?P<year>\\d{4})/(?P<month>[a-z]{3})/(?P<day>\\w{1,2})/(?P<slug>[\\w-]+)/$', 'object_detail', dict(info_dict, slug_field='slug')),\n-   (r'^(?P<year>\\d{4})/(?P<month>[a-z]{3})/(?P<day>\\w{1,2})/$', 'archive_day', info_dict),\n-   (r'^(?P<year>\\d{4})/(?P<month>[a-z]{3})/$', 'archive_month', info_dict),\n-   (r'^(?P<year>\\d{4})/$', 'archive_year', info_dict),\n-   (r'^/?$', 'archive_index', info_dict),\n-)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3f66b7d3961fa9c9655917257d3587869a08eb0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 75 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,6 @@ INSTALLED_APPS = (\n     'django.contrib.markup',\n     'scrapyorg.article',\n     'scrapyorg.download',\n-    'scrapyorg.blog',\n )\n \n # Override previous settings with values in local_settings.py settings file.\n\n@@ -7,8 +7,6 @@ admin.autodiscover()\n \n \n urlpatterns = patterns('',\n-    url(r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n-\n     # admin\n     url(r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n     url(r'^admin/(.*)', admin.site.root),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4842fe0679b40e3d973430eb1efddc5f39ffda00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 260 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from twisted.python import failure\n \n from scrapy.core.exceptions import UsageError\n from scrapy.utils.python import flatten\n+from scrapy.utils.markup import remove_entities\n \n def dict_updatedefault(D, E, **F):\n     \"\"\"\n@@ -172,14 +173,6 @@ def convert_entity(m, keep_reserved=False):\n     except KeyError:\n         return '&%s;' % m.group(2)\n \n-\n-def unquote_html(s, keep_reserved=False):\n-    \"\"\"Convert a HTML quoted string into normal string (ISO-8859-1).\n-   \n-    Works with &#XX; and with &nbsp; &gt; etc.\n-    \"\"\"\n-    return re.sub(re.compile(r'&(#?)(.+?);', re.U), lambda m: convert_entity(m, keep_reserved), s)\n-\n def extract_regex(regex, text, encoding):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n     \n@@ -198,9 +191,9 @@ def extract_regex(regex, text, encoding):\n     strings = flatten(strings)\n \n     if isinstance(text, unicode):\n-        return [unquote_html(s, keep_reserved=True) for s in strings]\n+        return [remove_entities(s, keep=['lt', 'amp']) for s in strings]\n     else:\n-        return [unquote_html(unicode(s, encoding), keep_reserved=True) for s in strings]\n+        return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n _regex_type = type(re.compile(\"\", 0))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbfc6341c2558e4df8edef50f5760843e20f2b17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 18 | Churn Cumulative: 278 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,6 @@\n Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n import re\n-import htmlentitydefs\n \n from twisted.internet import defer, reactor\n from twisted.python import failure\n@@ -156,23 +155,6 @@ def load_class(class_path):\n \n     return cls\n \n-def convert_entity(m, keep_reserved=False):\n-    \"\"\"\n-    Convert a HTML entity into unicode string\n-    \"\"\"\n-    if m.group(1)=='#':\n-        try:\n-            return unichr(int(m.group(2)))\n-        except ValueError:\n-            return '&#%s;' % m.group(2)\n-    try:\n-        if not (keep_reserved and m.group(2) in ['lt', 'amp']):\n-            return unichr(htmlentitydefs.name2codepoint[m.group(2)])\n-        else:\n-            return '&%s;' % m.group(2)\n-    except KeyError:\n-        return '&%s;' % m.group(2)\n-\n def extract_regex(regex, text, encoding):\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6637d8cf68268d3bfc92a2382f8adec3b1acb46f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 10 | Churn Cumulative: 288 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -176,13 +176,3 @@ def extract_regex(regex, text, encoding):\n         return [remove_entities(s, keep=['lt', 'amp']) for s in strings]\n     else:\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n-\n-_regex_type = type(re.compile(\"\", 0))\n-\n-def location_str(location):\n-    \"\"\"Return a human friendly representation of a parser location\"\"\"\n-\n-    if isinstance(location, _regex_type):\n-        return \"(regex) \" + location.pattern\n-    else:\n-        return location\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb8e0d9bdd7524d8dfd1fe5122701641bebc2ffa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 96 | Files Changed: 14 | Hunks: 18 | Methods Changed: 25 | Complexity Δ (Sum/Max): 0/20 | Churn Δ: 194 | Churn Cumulative: 3056 | Contributors (this commit): 3 | Commits (past 90d): 69 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n from twisted.internet import defer\n \n-from scrapy.utils.misc import mustbe_deferred, defer_result\n+from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.core import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import DropItem, NotConfigured\n\n@@ -1,6 +1,6 @@\n from functools import wraps\n from twisted.internet import defer\n-from scrapy.utils.misc import mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred\n \n from .http import HttpResponse\n \n\n@@ -61,7 +61,7 @@ import re\n \n from twisted.web import server, resource\n from twisted.internet import reactor\n-from scrapy.utils.misc import mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred\n \n from .http import HttpResponse, build_httprequest\n \n\n@@ -13,7 +13,7 @@ from twisted.web import error as web_error\n from scrapy.core import signals\n from scrapy.http import Request, Response, Headers\n from scrapy.core.exceptions import UsageError, HttpException\n-from scrapy.utils.misc import defer_succeed\n+from scrapy.utils.defer import defer_succeed\n from scrapy.conf import settings\n \n def download_any(request, spider):\n\n@@ -12,7 +12,7 @@ from scrapy.core.downloader.handlers import download_any\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.core import log\n from scrapy.conf import settings\n-from scrapy.utils.misc import chain_deferred\n+from scrapy.utils.defer import chain_deferred, mustbe_deferred\n \n \n class SiteDetails(object):\n@@ -126,7 +126,6 @@ class Downloader(object):\n         def _finish(result):\n             self.process_queue(spider)\n \n-        from scrapy.utils.misc import mustbe_deferred\n         dwld = mustbe_deferred(self.download_function, request, spider)\n         dwld.addBoth(_remove)\n         chain_deferred(dwld, deferred)\n\n@@ -4,7 +4,8 @@ request-response middleware extension\n from scrapy.core import signals, log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class, mustbe_deferred\n+from scrapy.utils.misc import load_class\n+from scrapy.utils.defer import mustbe_deferred\n from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n \n\n@@ -17,7 +17,7 @@ from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipeline\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n-from scrapy.utils.misc import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n+from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n from scrapy.conf import settings\n \n \n\n@@ -5,7 +5,7 @@ from scrapy.core import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.utils.misc import load_class\n+from scrapy.utils.defer import load_class\n from scrapy.utils.url import is_url\n from scrapy.conf import settings\n \n\n@@ -8,7 +8,7 @@ from scrapy.core.scheduler.filter import GroupFilter\n from scrapy.core import log\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n-from scrapy.utils.misc import defer_fail\n+from scrapy.utils.defer import defer_fail\n from scrapy.conf import settings\n \n \n\n@@ -10,7 +10,7 @@ from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n from scrapy.utils.c14n import canonicalize\n-from scrapy.utils.misc import chain_deferred\n+from scrapy.utils.defer import chain_deferred\n \n class Request(object):\n     def __init__(self, url, callback=None, context=None, method=None, body=None, headers=None, cookies=None,\n\n@@ -1,7 +1,8 @@\n from scrapy.core import log\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.item import ScrapedItem\n-from scrapy.utils.misc import load_class, defer_succeed, mustbe_deferred\n+from scrapy.utils.misc import load_class\n+from scrapy.utils.defer import defer_succeed, mustbe_deferred\n from scrapy.conf import settings\n \n class ItemPipeline(object):\n\n@@ -7,7 +7,8 @@ from twisted.python.failure import Failure\n \n from scrapy.core import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_class, mustbe_deferred\n+from scrapy.utils.misc import load_class\n+from scrapy.utils.defer import mustbe_deferred\n from scrapy.conf import settings\n \n def _isiterable(possible_iterator):\n\n@@ -0,0 +1,81 @@\n+\"\"\"\n+Helper functions for dealing with Twisted deferreds\n+\"\"\"\n+\n+from twisted.internet import defer, reactor\n+from twisted.python import failure\n+\n+def defer_fail(_failure):\n+    \"\"\"same as twsited.internet.defer.fail, but delay calling errback \"\"\"\n+    d = defer.Deferred()\n+    reactor.callLater(0, d.errback, _failure)\n+    return d\n+\n+def defer_succeed(result):\n+    \"\"\"same as twsited.internet.defer.succed, but delay calling callback\"\"\"\n+    d = defer.Deferred()\n+    reactor.callLater(0, d.callback, result)\n+    return d\n+\n+def defer_result(result):\n+    if isinstance(result, defer.Deferred):\n+        return result\n+    elif isinstance(result, failure.Failure):\n+        return defer_fail(result)\n+    else:\n+        return defer_succeed(result)\n+\n+def mustbe_deferred(f, *args, **kw):\n+    \"\"\"same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback\"\"\"\n+    try:\n+        result = f(*args, **kw)\n+    except:\n+        return defer_fail(failure.Failure())\n+    else:\n+        return defer_result(result)\n+\n+def chain_deferred(d1, d2):\n+    if callable(d2):\n+        d2 = lambda_deferred(d2)\n+\n+    def _pause(_):\n+        d2.pause()\n+        reactor.callLater(0, d2.unpause)\n+        return _\n+\n+    def _reclaim(_):\n+        return d2\n+\n+    #d1.addBoth(_pause) ## needs more debugging before reenable it\n+    d1.chainDeferred(d2)\n+    d1.addBoth(_reclaim)\n+    return d1\n+\n+def lambda_deferred(func):\n+    deferred = defer.Deferred()\n+    def _success(res):\n+        d = func()\n+        d.callback(res)\n+        return d\n+    def _fail(res):\n+        d = func()\n+        d.errback(res)\n+        return d\n+    return deferred.addCallbacks(_success, _fail)\n+\n+def deferred_degenerate(generator, container=None, next_delay=0):\n+    generator = iter(generator or [])\n+    deferred = defer.Deferred()\n+    container = container or []\n+    def _next():\n+        try:\n+            container.append(generator.next())\n+        except StopIteration:\n+            reactor.callLater(0, deferred.callback, container)\n+        except:\n+            reactor.callLater(0, deferred.errback, failure.Failure())\n+        else:\n+            reactor.callLater(next_delay, _next)\n+    _next()\n+    return deferred\n+\n\n@@ -3,12 +3,12 @@ Auxiliary functions which doesn't fit anywhere else\n \"\"\"\n import re\n \n-from twisted.internet import defer, reactor\n-from twisted.python import failure\n+from twisted.internet import defer\n \n from scrapy.core.exceptions import UsageError\n from scrapy.utils.python import flatten\n from scrapy.utils.markup import remove_entities\n+from scrapy.utils.defer import defer_succeed\n \n def dict_updatedefault(D, E, **F):\n     \"\"\"\n@@ -28,69 +28,6 @@ def dict_updatedefault(D, E, **F):\n     for k in F:\n         D.setdefault(k, F[k])\n \n-\n-def defer_fail(_failure):\n-    \"\"\"same as twsited.internet.defer.fail, but delay calling errback \"\"\"\n-    d = defer.Deferred()\n-    reactor.callLater(0, d.errback, _failure)\n-    return d\n-\n-\n-def defer_succeed(result):\n-    \"\"\"same as twsited.internet.defer.succed, but delay calling callback\"\"\"\n-    d = defer.Deferred()\n-    reactor.callLater(0, d.callback, result)\n-    return d\n-\n-def defer_result(result):\n-    if isinstance(result, defer.Deferred):\n-        return result\n-    elif isinstance(result, failure.Failure):\n-        return defer_fail(result)\n-    else:\n-        return defer_succeed(result)\n-\n-def mustbe_deferred(f, *args, **kw):\n-    \"\"\"same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback\"\"\"\n-    try:\n-        result = f(*args, **kw)\n-    except:\n-        return defer_fail(failure.Failure())\n-    else:\n-        return defer_result(result)\n-\n-\n-def chain_deferred(d1, d2):\n-    if callable(d2):\n-        d2 = lambda_deferred(d2)\n-\n-    def _pause(_):\n-        d2.pause()\n-        reactor.callLater(0, d2.unpause)\n-        return _\n-\n-    def _reclaim(_):\n-        return d2\n-\n-    #d1.addBoth(_pause) ## needs more debugging before reenable it\n-    d1.chainDeferred(d2)\n-    d1.addBoth(_reclaim)\n-    return d1\n-\n-\n-def lambda_deferred(func):\n-    deferred = defer.Deferred()\n-    def _success(res):\n-        d = func()\n-        d.callback(res)\n-        return d\n-    def _fail(res):\n-        d = func()\n-        d.errback(res)\n-        return d\n-    return deferred.addCallbacks(_success, _fail)\n-\n-\n def memoize(cache, hash):\n     def decorator(func):\n         def wrapper(*args, **kwargs):\n@@ -110,24 +47,6 @@ def memoize(cache, hash):\n         return wrapper\n     return decorator\n \n-\n-def deferred_degenerate(generator, container=None, next_delay=0):\n-    generator = iter(generator or [])\n-    deferred = defer.Deferred()\n-    container = container or []\n-    def _next():\n-        try:\n-            container.append(generator.next())\n-        except StopIteration:\n-            reactor.callLater(0, deferred.callback, container)\n-        except:\n-            reactor.callLater(0, deferred.errback, failure.Failure())\n-        else:\n-            reactor.callLater(next_delay, _next)\n-    _next()\n-    return deferred\n-\n-\n def stats_getpath(dict_, path, default=None):\n     for key in path.split('/'):\n         if key in dict_:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
