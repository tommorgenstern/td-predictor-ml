{"custom_id": "keras#b076e227da6beaf87d6c84eff1a92285e4662acf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 46 | Churn Cumulative: 1824 | Contributors (this commit): 21 | Commits (past 90d): 8 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -487,12 +487,24 @@ class Bidirectional(Wrapper):\n             return self.forward_layer.updates + self.backward_layer.updates\n         return []\n \n+    def get_updates_for(self, inputs=None):\n+        forward_updates = self.forward_layer.get_updates_for(inputs)\n+        backward_updates = self.backward_layer.get_updates_for(inputs)\n+        return (super(Wrapper, self).get_updates_for(inputs) +\n+                forward_updates + backward_updates)\n+\n     @property\n     def losses(self):\n         if hasattr(self.forward_layer, 'losses'):\n             return self.forward_layer.losses + self.backward_layer.losses\n         return []\n \n+    def get_losses_for(self, inputs=None):\n+        forward_losses = self.forward_layer.get_losses_for(inputs)\n+        backward_losses = self.backward_layer.get_losses_for(inputs)\n+        return (super(Wrapper, self).get_losses_for(inputs) +\n+                forward_losses + backward_losses)\n+\n     @property\n     def constraints(self):\n         constraints = {}\n\n@@ -556,5 +556,39 @@ def test_Bidirectional_trainable():\n     assert len(layer.trainable_weights) == 6\n \n \n+@keras_test\n+def test_Bidirectional_updates():\n+    x = Input(shape=(3, 2))\n+    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n+    assert len(layer.updates) == 0\n+    assert len(layer.get_updates_for(None)) == 0\n+    assert len(layer.get_updates_for(x)) == 0\n+    layer.forward_layer.add_update(0, inputs=x)\n+    layer.forward_layer.add_update(1, inputs=None)\n+    layer.backward_layer.add_update(0, inputs=x)\n+    layer.backward_layer.add_update(1, inputs=None)\n+    assert len(layer.updates) == 4\n+    assert len(layer.get_updates_for(None)) == 2\n+    assert len(layer.get_updates_for(x)) == 2\n+\n+\n+@keras_test\n+def test_Bidirectional_losses():\n+    x = Input(shape=(3, 2))\n+    layer = wrappers.Bidirectional(\n+        layers.SimpleRNN(3, kernel_regularizer='l1', bias_regularizer='l1'))\n+    _ = layer(x)\n+    assert len(layer.losses) == 4\n+    assert len(layer.get_losses_for(None)) == 4\n+    assert len(layer.get_losses_for(x)) == 0\n+    layer.forward_layer.add_loss(0, inputs=x)\n+    layer.forward_layer.add_loss(1, inputs=None)\n+    layer.backward_layer.add_loss(0, inputs=x)\n+    layer.backward_layer.add_loss(1, inputs=None)\n+    assert len(layer.losses) == 8\n+    assert len(layer.get_losses_for(None)) == 6\n+    assert len(layer.get_losses_for(x)) == 2\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58fd1f0589d33aeb33c4129cfedfb7737495efc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1736 | Lines Deleted: 1492 | Files Changed: 8 | Hunks: 115 | Methods Changed: 73 | Complexity Δ (Sum/Max): -7/118 | Churn Δ: 3228 | Churn Cumulative: 14349 | Contributors (this commit): 114 | Commits (past 90d): 34 | Contributors (cumulative): 156 | DMM Complexity: 0.6306818181818182\n\nDIFF:\n@@ -1,4 +1,3 @@\n-# -*- coding: utf-8 -*-\n \"\"\"Training-related part of the Keras engine.\n \"\"\"\n from __future__ import absolute_import\n@@ -7,548 +6,39 @@ from __future__ import print_function\n \n import warnings\n import copy\n-import numpy as np\n-from scipy.sparse import issparse\n \n from .network import Network\n from .base_layer import Layer\n+from .training_utils import collect_metrics\n+from .training_utils import check_array_length_consistency\n+from .training_utils import check_loss_and_target_compatibility\n+from .training_utils import standardize_class_weights\n+from .training_utils import standardize_input_data\n+from .training_utils import standardize_sample_weights\n+from .training_utils import standardize_weights\n+from .training_utils import weighted_masked_objective\n+from . import training_arrays\n+from . import training_generator\n from .. import backend as K\n from .. import optimizers\n from .. import losses\n from .. import metrics as metrics_module\n-from ..utils.data_utils import Sequence\n-from ..utils.data_utils import GeneratorEnqueuer\n-from ..utils.data_utils import OrderedEnqueuer\n-from ..utils.generic_utils import Progbar\n-from .. import callbacks as cbks\n+from ..utils.generic_utils import slice_arrays\n from ..legacy import interfaces\n \n \n-def _standardize_input_data(data, names, shapes=None,\n-                            check_batch_axis=True,\n-                            exception_prefix=''):\n-    \"\"\"Normalizes inputs and targets provided by users.\n-\n-    Users may pass data as a list of arrays, dictionary of arrays,\n-    or as a single array. We normalize this to an ordered list of\n-    arrays (same order as `names`), while checking that the provided\n-    arrays have shapes that match the network's expectations.\n-\n-    # Arguments\n-        data: User-provided input data (polymorphic).\n-        names: List of expected array names.\n-        shapes: Optional list of expected array shapes.\n-        check_batch_axis: Boolean; whether to check that\n-            the batch axis of the arrays matches the expected\n-            value found in `shapes`.\n-        exception_prefix: String prefix used for exception formatting.\n-\n-    # Returns\n-        List of standardized input arrays (one array per model input).\n-\n-    # Raises\n-        ValueError: in case of improperly formatted user-provided data.\n-    \"\"\"\n-    if not names:\n-        if data is not None and hasattr(data, '__len__') and len(data):\n-            raise ValueError('Error when checking model ' +\n-                             exception_prefix + ': '\n-                             'expected no data, but got:', data)\n-        return []\n-    if data is None:\n-        return [None for _ in range(len(names))]\n-\n-    if isinstance(data, dict):\n-        try:\n-            data = [data[x].values if data[x].__class__.__name__ == 'DataFrame' else data[x] for x in names]\n-        except KeyError as e:\n-            raise ValueError(\n-                'No data provided for \"' + e.args[0] + '\". Need data '\n-                'for each key in: ' + str(names))\n-    elif isinstance(data, list):\n-        if len(names) == 1 and data and isinstance(data[0], (float, int)):\n-            data = [np.asarray(data)]\n-        else:\n-            data = [x.values if x.__class__.__name__ == 'DataFrame' else x for x in data]\n-    else:\n-        data = data.values if data.__class__.__name__ == 'DataFrame' else data\n-        data = [data]\n-    data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]\n-\n-    if len(data) != len(names):\n-        if data and hasattr(data[0], 'shape'):\n-            raise ValueError(\n-                'Error when checking model ' + exception_prefix +\n-                ': the list of Numpy arrays that you are passing to '\n-                'your model is not the size the model expected. '\n-                'Expected to see ' + str(len(names)) + ' array(s), '\n-                'but instead got the following list of ' +\n-                str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n-        elif len(names) > 1:\n-            raise ValueError(\n-                'Error when checking model ' + exception_prefix +\n-                ': you are passing a list as input to your model, '\n-                'but the model expects a list of ' + str(len(names)) +\n-                ' Numpy arrays instead. The list you passed was: ' +\n-                str(data)[:200])\n-        elif len(data) == 1 and not hasattr(data[0], 'shape'):\n-            raise TypeError(\n-                'Error when checking model ' + exception_prefix +\n-                ': data should be a Numpy array, or list/dict of '\n-                'Numpy arrays. Found: ' + str(data)[:200] + '...')\n-        elif len(names) == 1:\n-            data = [np.asarray(data)]\n-\n-    # Check shapes compatibility.\n-    if shapes:\n-        for i in range(len(names)):\n-            if shapes[i] is not None:\n-                data_shape = data[i].shape\n-                shape = shapes[i]\n-                if data[i].ndim != len(shape):\n-                    raise ValueError(\n-                        'Error when checking ' + exception_prefix +\n-                        ': expected ' + names[i] + ' to have ' +\n-                        str(len(shape)) + ' dimensions, but got array '\n-                        'with shape ' + str(data_shape))\n-                if not check_batch_axis:\n-                    data_shape = data_shape[1:]\n-                    shape = shape[1:]\n-                for dim, ref_dim in zip(data_shape, shape):\n-                    if ref_dim != dim and ref_dim:\n-                        raise ValueError(\n-                            'Error when checking ' + exception_prefix +\n-                            ': expected ' + names[i] + ' to have shape ' +\n-                            str(shape) + ' but got array with shape ' +\n-                            str(data_shape))\n-    return data\n-\n-\n-def _standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n-    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n-\n-    # Arguments\n-        x_weight: User-provided `sample_weight` or `class_weight` argument.\n-        output_names: List of output names (strings) in the model.\n-        weight_type: A string used purely for exception printing.\n-\n-    # Returns\n-        A list of `sample_weight` or `class_weight` where there are exactly\n-            one element per model output.\n-\n-    # Raises\n-        ValueError: In case of invalid user-provided argument.\n-    \"\"\"\n-    if x_weight is None or len(x_weight) == 0:\n-        return [None for _ in output_names]\n-    if len(output_names) == 1:\n-        if isinstance(x_weight, list) and len(x_weight) == 1:\n-            return x_weight\n-        if isinstance(x_weight, dict) and output_names[0] in x_weight:\n-            return [x_weight[output_names[0]]]\n-        else:\n-            return [x_weight]\n-    if isinstance(x_weight, list):\n-        if len(x_weight) != len(output_names):\n-            raise ValueError('Provided `' + weight_type + '` was a list of ' +\n-                             str(len(x_weight)) +\n-                             ' elements, but the model has ' +\n-                             str(len(output_names)) + ' outputs. '\n-                             'You should provide one `' + weight_type + '`'\n-                             'array per model output.')\n-        return x_weight\n-    if isinstance(x_weight, dict):\n-        x_weights = []\n-        for name in output_names:\n-            x_weights.append(x_weight.get(name))\n-        return x_weights\n-    else:\n-        raise TypeError('The model has multiple outputs, so `' +\n-                        weight_type + '` '\n-                        'should be either a list or a dict. '\n-                        'Provided `' + weight_type +\n-                        '` type not understood: ' +\n-                        str(x_weight))\n-\n-\n-def _standardize_class_weights(class_weight, output_names):\n-    return _standardize_sample_or_class_weights(class_weight,\n-                                                output_names,\n-                                                'class_weight')\n-\n-\n-def _standardize_sample_weights(sample_weight, output_names):\n-    return _standardize_sample_or_class_weights(sample_weight,\n-                                                output_names,\n-                                                'sample_weight')\n-\n-\n-def _check_array_lengths(inputs, targets, weights=None):\n-    \"\"\"Checks if batch axes are the same for numpy arrays.\n-\n-    # Arguments\n-        inputs: list of Numpy arrays of inputs.\n-        targets: list of Numpy arrays of targets.\n-        weights: list of Numpy arrays of sample weights.\n-\n-    # Raises\n-        ValueError: in case of incorrectly formatted data.\n-    \"\"\"\n-    def set_of_lengths(x):\n-        # return a set with the variation between\n-        # different shapes, with None => 0\n-        if x is None:\n-            return {0}\n-        else:\n-            return set([0 if y is None else y.shape[0] for y in x])\n-\n-    set_x = set_of_lengths(inputs)\n-    set_y = set_of_lengths(targets)\n-    set_w = set_of_lengths(weights)\n-    if len(set_x) > 1:\n-        raise ValueError('All input arrays (x) should have '\n-                         'the same number of samples. Got array shapes: ' +\n-                         str([x.shape for x in inputs]))\n-    if len(set_y) > 1:\n-        raise ValueError('All target arrays (y) should have '\n-                         'the same number of samples. Got array shapes: ' +\n-                         str([y.shape for y in targets]))\n-    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n-        raise ValueError('Input arrays should have '\n-                         'the same number of samples as target arrays. '\n-                         'Found ' + str(list(set_x)[0]) + ' input samples '\n-                         'and ' + str(list(set_y)[0]) + ' target samples.')\n-    if len(set_w) > 1:\n-        raise ValueError('All sample_weight arrays should have '\n-                         'the same number of samples. Got array shapes: ' +\n-                         str([w.shape for w in weights]))\n-    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n-        raise ValueError('Sample_weight arrays should have '\n-                         'the same number of samples as target arrays. Got ' +\n-                         str(list(set_y)[0]) + ' input samples and ' +\n-                         str(list(set_w)[0]) + ' target samples.')\n-\n-\n-def _check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n-    \"\"\"Does validation on the compatibility of targets and loss functions.\n-\n-    This helps prevent users from using loss functions incorrectly.\n-\n-    # Arguments\n-        targets: list of Numpy arrays of targets.\n-        loss_fns: list of loss functions.\n-        output_shapes: list of shapes of model outputs.\n-\n-    # Raises\n-        ValueError: if a loss function or target array\n-            is incompatible with an output.\n-    \"\"\"\n-    key_losses = {losses.mean_squared_error,\n-                  losses.binary_crossentropy,\n-                  losses.categorical_crossentropy}\n-    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n-        if y is None or loss is None:\n-            continue\n-        if loss is losses.categorical_crossentropy:\n-            if y.shape[-1] == 1:\n-                raise ValueError(\n-                    'You are passing a target array of shape ' + str(y.shape) +\n-                    ' while using as loss `categorical_crossentropy`. '\n-                    '`categorical_crossentropy` expects '\n-                    'targets to be binary matrices (1s and 0s) '\n-                    'of shape (samples, classes). '\n-                    'If your targets are integer classes, '\n-                    'you can convert them to the expected format via:\\n'\n-                    '```\\n'\n-                    'from keras.utils import to_categorical\\n'\n-                    'y_binary = to_categorical(y_int)\\n'\n-                    '```\\n'\n-                    '\\n'\n-                    'Alternatively, you can use the loss function '\n-                    '`sparse_categorical_crossentropy` instead, '\n-                    'which does expect integer targets.')\n-        if loss in key_losses:\n-            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n-                if out_dim is not None and target_dim != out_dim:\n-                    raise ValueError(\n-                        'A target array with shape ' + str(y.shape) +\n-                        ' was passed for an output of shape ' + str(shape) +\n-                        ' while using as loss `' + loss.__name__ + '`. '\n-                        'This loss expects '\n-                        'targets to have the same shape '\n-                        'as the output.')\n-\n-\n-def _collect_metrics(metrics, output_names):\n-    \"\"\"Maps metric functions to model outputs.\n-\n-    # Arguments\n-        metrics: a list or dict of metric functions.\n-        output_names: a list of the names (strings) of model outputs.\n-\n-    # Returns\n-        A list (one entry per model output) of lists of metric functions.\n-        For instance, if the model has 2 outputs, and for the first output\n-        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n-        and just \"binary_accuracy\" for the second output,\n-        the list would look like:\n-            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n-\n-    # Raises\n-        TypeError: if an incorrect type is passed for the `metrics` argument.\n-    \"\"\"\n-    if not metrics:\n-        return [[] for _ in output_names]\n-    if isinstance(metrics, list):\n-        # we then apply all metrics to all outputs.\n-        return [copy.copy(metrics) for _ in output_names]\n-    elif isinstance(metrics, dict):\n-        nested_metrics = []\n-        for name in output_names:\n-            output_metrics = metrics.get(name, [])\n-            if not isinstance(output_metrics, list):\n-                output_metrics = [output_metrics]\n-            nested_metrics.append(output_metrics)\n-        return nested_metrics\n-    else:\n-        raise TypeError('Type of `metrics` argument not understood. '\n-                        'Expected a list or dictionary, found: ' +\n-                        str(metrics))\n-\n-\n-def _batch_shuffle(index_array, batch_size):\n-    \"\"\"Shuffles an array in a batch-wise fashion.\n-\n-    Useful for shuffling HDF5 arrays\n-    (where one cannot access arbitrary indices).\n-\n-    # Arguments\n-        index_array: array of indices to be shuffled.\n-        batch_size: integer.\n-\n-    # Returns\n-        The `index_array` array, shuffled in a batch-wise fashion.\n-    \"\"\"\n-    batch_count = int(len(index_array) / batch_size)\n-    # to reshape we need to be cleanly divisible by batch size\n-    # we stash extra items and reappend them after shuffling\n-    last_batch = index_array[batch_count * batch_size:]\n-    index_array = index_array[:batch_count * batch_size]\n-    index_array = index_array.reshape((batch_count, batch_size))\n-    np.random.shuffle(index_array)\n-    index_array = index_array.flatten()\n-    return np.append(index_array, last_batch)\n-\n-\n-def _make_batches(size, batch_size):\n-    \"\"\"Returns a list of batch indices (tuples of indices).\n-\n-    # Arguments\n-        size: Integer, total size of the data to slice into batches.\n-        batch_size: Integer, batch size.\n-\n-    # Returns\n-        A list of tuples of array indices.\n-    \"\"\"\n-    num_batches = (size + batch_size - 1) // batch_size  # round up\n-    return [(i * batch_size, min(size, (i + 1) * batch_size))\n-            for i in range(num_batches)]\n-\n-\n-def _slice_arrays(arrays, start=None, stop=None):\n-    \"\"\"Slice an array or list of arrays.\n-\n-    This takes an array-like, or a list of\n-    array-likes, and outputs:\n-        - arrays[start:stop] if `arrays` is an array-like\n-        - [x[start:stop] for x in arrays] if `arrays` is a list\n-\n-    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n-\n-    # Arguments\n-        arrays: Single array or list of arrays.\n-        start: can be an integer index (start index)\n-            or a list/array of indices\n-        stop: integer (stop index); should be None if\n-            `start` was a list.\n-\n-    # Returns\n-        A slice of the array(s).\n-    \"\"\"\n-    if arrays is None:\n-        return [None]\n-    elif isinstance(arrays, list):\n-        if hasattr(start, '__len__'):\n-            # hdf5 datasets only support list objects as indices\n-            if hasattr(start, 'shape'):\n-                start = start.tolist()\n-            return [None if x is None else x[start] for x in arrays]\n-        else:\n-            return [None if x is None else x[start:stop] for x in arrays]\n-    else:\n-        if hasattr(start, '__len__'):\n-            if hasattr(start, 'shape'):\n-                start = start.tolist()\n-            return arrays[start]\n-        elif hasattr(start, '__getitem__'):\n-            return arrays[start:stop]\n-        else:\n-            return [None]\n-\n-\n-def _weighted_masked_objective(fn):\n-    \"\"\"Adds support for masking and sample-weighting to an objective function.\n-\n-    It transforms an objective function `fn(y_true, y_pred)`\n-    into a sample-weighted, cost-masked objective function\n-    `fn(y_true, y_pred, weights, mask)`.\n-\n-    # Arguments\n-        fn: The objective function to wrap,\n-            with signature `fn(y_true, y_pred)`.\n-\n-    # Returns\n-        A function with signature `fn(y_true, y_pred, weights, mask)`.\n-    \"\"\"\n-    if fn is None:\n-        return None\n-\n-    def weighted(y_true, y_pred, weights, mask=None):\n-        \"\"\"Wrapper function.\n-\n-        # Arguments\n-            y_true: `y_true` argument of `fn`.\n-            y_pred: `y_pred` argument of `fn`.\n-            weights: Weights tensor.\n-            mask: Mask tensor.\n-\n-        # Returns\n-            Scalar tensor.\n-        \"\"\"\n-        # score_array has ndim >= 2\n-        score_array = fn(y_true, y_pred)\n-        if mask is not None:\n-            # Cast the mask to floatX to avoid float64 upcasting in Theano\n-            mask = K.cast(mask, K.floatx())\n-            # mask should have the same shape as score_array\n-            score_array *= mask\n-            #  the loss per batch should be proportional\n-            #  to the number of unmasked samples.\n-            score_array /= K.mean(mask)\n-\n-        # apply sample weighting\n-        if weights is not None:\n-            # reduce score_array to same ndim as weight array\n-            ndim = K.ndim(score_array)\n-            weight_ndim = K.ndim(weights)\n-            score_array = K.mean(score_array, axis=list(range(weight_ndim, ndim)))\n-            score_array *= weights\n-            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n-        return K.mean(score_array)\n-    return weighted\n-\n-\n-def _standardize_weights(y, sample_weight=None, class_weight=None,\n-                         sample_weight_mode=None):\n-    \"\"\"Performs sample weight validation and standardization.\n-\n-    Everything gets normalized to a single sample-wise (or timestep-wise)\n-    weight array.\n-\n-    # Arguments\n-        y: Numpy array of model targets to be weighted.\n-        sample_weight: User-provided `sample_weight` argument.\n-        class_weight: User-provided `class_weight` argument.\n-        sample_weight_mode: One of `None` or `\"temporal\"`.\n-            `\"temporal\"` indicated that we expect 2D weight data\n-            that will be applied to the last 2 dimensions of\n-            the targets (i.e. we are weighting timesteps, not samples).\n-\n-    # Returns\n-        A numpy array of target weights, one entry per sample to weight.\n-\n-    # Raises\n-        ValueError: In case of invalid user-provided arguments.\n-    \"\"\"\n-    if sample_weight_mode is not None:\n-        if sample_weight_mode != 'temporal':\n-            raise ValueError('\"sample_weight_mode '\n-                             'should be None or \"temporal\". '\n-                             'Found: ' + str(sample_weight_mode))\n-        if len(y.shape) < 3:\n-            raise ValueError('Found a sample_weight array for '\n-                             'an input with shape ' +\n-                             str(y.shape) + '. '\n-                             'Timestep-wise sample weighting (use of '\n-                             'sample_weight_mode=\"temporal\") is restricted to '\n-                             'outputs that are at least 3D, i.e. that have '\n-                             'a time dimension.')\n-        if sample_weight is not None and len(sample_weight.shape) != 2:\n-            raise ValueError('Found a sample_weight array with shape ' +\n-                             str(sample_weight.shape) + '. '\n-                             'In order to use timestep-wise sample weighting, '\n-                             'you should pass a 2D sample_weight array.')\n-    else:\n-        if sample_weight is not None and len(sample_weight.shape) != 1:\n-            raise ValueError('Found a sample_weight array with shape ' +\n-                             str(sample_weight.shape) + '. '\n-                             'In order to use timestep-wise sample weights, '\n-                             'you should specify '\n-                             'sample_weight_mode=\"temporal\" '\n-                             'in compile(). If you just mean to use '\n-                             'sample-wise weights, make sure your '\n-                             'sample_weight array is 1D.')\n-\n-    if sample_weight is not None:\n-        if len(sample_weight.shape) > len(y.shape):\n-            raise ValueError('Found a sample_weight with shape' +\n-                             str(sample_weight.shape) + '.'\n-                             'Expected sample_weight with rank '\n-                             'less than or equal to ' + str(len(y.shape)))\n-\n-        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n-            raise ValueError('Found a sample_weight array with shape ' +\n-                             str(sample_weight.shape) + ' for an input with shape ' +\n-                             str(y.shape) + '. '\n-                             'sample_weight cannot be broadcast.')\n-        return sample_weight\n-    elif isinstance(class_weight, dict):\n-        if len(y.shape) > 2:\n-            raise ValueError('`class_weight` not supported for '\n-                             '3+ dimensional targets.')\n-        if y.shape[1] > 1:\n-            y_classes = np.argmax(y, axis=1)\n-        elif y.shape[1] == 1:\n-            y_classes = np.reshape(y, y.shape[0])\n-        else:\n-            y_classes = y\n-\n-        weights = np.asarray([class_weight[cls] for cls in y_classes\n-                              if cls in class_weight])\n-\n-        if len(weights) != len(y_classes):\n-            # subtract the sets to pick all missing classes\n-            existing_classes = set(y_classes)\n-            existing_class_weight = set(class_weight.keys())\n-            raise ValueError('`class_weight` must contain all classes in the data.'\n-                             ' The classes %s exist in the data but not in '\n-                             '`class_weight`.'\n-                             % (existing_classes - existing_class_weight))\n-        return weights\n-    else:\n-        if sample_weight_mode is None:\n-            return np.ones((y.shape[0],), dtype=K.floatx())\n-        else:\n-            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n-\n-\n class Model(Network):\n     \"\"\"The `Model` class adds training & evaluation routines to a `Network`.\n     \"\"\"\n \n-    def compile(self, optimizer, loss=None, metrics=None, loss_weights=None,\n-                sample_weight_mode=None, weighted_metrics=None,\n-                target_tensors=None, **kwargs):\n+    def compile(self, optimizer,\n+                loss=None,\n+                metrics=None,\n+                loss_weights=None,\n+                sample_weight_mode=None,\n+                weighted_metrics=None,\n+                target_tensors=None,\n+                **kwargs):\n         \"\"\"Configures the model for training.\n \n         # Arguments\n@@ -636,7 +126,8 @@ class Model(Network):\n             loss_function = losses.get(loss)\n             loss_functions = [loss_function for _ in range(len(self.outputs))]\n         self.loss_functions = loss_functions\n-        weighted_losses = [_weighted_masked_objective(fn) for fn in loss_functions]\n+        weighted_losses = [\n+            weighted_masked_objective(fn) for fn in loss_functions]\n         skip_target_indices = []\n         skip_target_weighing_indices = []\n         self._feed_outputs = []\n@@ -719,7 +210,8 @@ class Model(Network):\n                     target = None\n                 if target is None or K.is_placeholder(target):\n                     if target is None:\n-                        target = K.placeholder(ndim=len(shape),\n+                        target = K.placeholder(\n+                            ndim=len(shape),\n                             name=name + '_target',\n                             sparse=K.is_sparse(self.outputs[i]),\n                             dtype=K.dtype(self.outputs[i]))\n@@ -805,7 +297,8 @@ class Model(Network):\n         self._feed_sample_weight_modes = []\n         for i in range(len(self.outputs)):\n             if i not in skip_target_weighing_indices:\n-                self._feed_sample_weight_modes.append(self.sample_weight_modes[i])\n+                self._feed_sample_weight_modes.append(\n+                    self.sample_weight_modes[i])\n \n         # Prepare metrics.\n         self.metrics = metrics or []\n@@ -849,21 +342,12 @@ class Model(Network):\n \n         # List of same size as output_names.\n         # contains tuples (metrics for output, names of metrics).\n-        nested_metrics = _collect_metrics(metrics, self.output_names)\n-        nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n+        nested_metrics = collect_metrics(metrics, self.output_names)\n+        nested_weighted_metrics = collect_metrics(weighted_metrics,\n+                                                  self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         self.stateful_metric_functions = []\n-        with K.name_scope('metrics'):\n-            for i in range(len(self.outputs)):\n-                if i in skip_target_indices:\n-                    continue\n-\n-                y_true = self.targets[i]\n-                y_pred = self.outputs[i]\n-                weights = sample_weights[i]\n-                output_metrics = nested_metrics[i]\n-                output_weighted_metrics = nested_weighted_metrics[i]\n \n         def handle_metrics(metrics, weights=None):\n             metric_name_prefix = 'weighted_' if weights is not None else ''\n@@ -881,7 +365,8 @@ class Model(Network):\n                         elif metric in ('crossentropy', 'ce'):\n                             metric_fn = metrics_module.binary_crossentropy\n                     elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n-                                # case: categorical accuracy/crossentropy with sparse targets\n+                        # case: categorical accuracy/crossentropy\n+                        # with sparse targets\n                         if metric in ('accuracy', 'acc'):\n                             metric_fn = metrics_module.sparse_categorical_accuracy\n                         elif metric in ('crossentropy', 'ce'):\n@@ -896,11 +381,11 @@ class Model(Network):\n                             suffix = 'acc'\n                     elif metric in ('crossentropy', 'ce'):\n                             suffix = 'ce'\n-                            weighted_metric_fn = _weighted_masked_objective(metric_fn)\n+                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                     metric_name = metric_name_prefix + suffix\n                 else:\n                     metric_fn = metrics_module.get(metric)\n-                            weighted_metric_fn = _weighted_masked_objective(metric_fn)\n+                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                     # Get metric name as string\n                     if hasattr(metric_fn, 'name'):\n                         metric_name = metric_fn.name\n@@ -932,7 +417,16 @@ class Model(Network):\n                     self.stateful_metric_names.append(metric_name)\n                     self.stateful_metric_functions.append(metric_fn)\n                     self.metrics_updates += metric_fn.updates\n+        with K.name_scope('metrics'):\n+            for i in range(len(self.outputs)):\n+                if i in skip_target_indices:\n+                    continue\n \n+                y_true = self.targets[i]\n+                y_pred = self.outputs[i]\n+                weights = sample_weights[i]\n+                output_metrics = nested_metrics[i]\n+                output_weighted_metrics = nested_weighted_metrics[i]\n                 handle_metrics(output_metrics)\n                 handle_metrics(output_weighted_metrics, weights=weights)\n \n@@ -981,8 +475,10 @@ class Model(Network):\n             raise RuntimeError('You must compile your model before using it.')\n         self._check_trainable_weights_consistency()\n         if self.train_function is None:\n-            inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+            inputs = (self._feed_inputs +\n+                      self._feed_targets +\n+                      self._feed_sample_weights)\n+            if self._uses_dynamic_learning_phase():\n                 inputs += [K.learning_phase()]\n \n             with K.name_scope('training'):\n@@ -990,9 +486,12 @@ class Model(Network):\n                     training_updates = self.optimizer.get_updates(\n                         params=self._collected_trainable_weights,\n                         loss=self.total_loss)\n-                updates = self.updates + training_updates + self.metrics_updates\n+                updates = (self.updates +\n+                           training_updates +\n+                           self.metrics_updates)\n                 # Gets loss and metrics. Updates weights at each call.\n-                self.train_function = K.function(inputs,\n+                self.train_function = K.function(\n+                    inputs,\n                     [self.total_loss] + self.metrics_tensors,\n                     updates=updates,\n                     name='train_function',\n@@ -1002,12 +501,15 @@ class Model(Network):\n         if not hasattr(self, 'test_function'):\n             raise RuntimeError('You must compile your model before using it.')\n         if self.test_function is None:\n-            inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+            inputs = (self._feed_inputs +\n+                      self._feed_targets +\n+                      self._feed_sample_weights)\n+            if self._uses_dynamic_learning_phase():\n                 inputs += [K.learning_phase()]\n             # Return loss and metrics, no gradient updates.\n             # Does update the network states.\n-            self.test_function = K.function(inputs,\n+            self.test_function = K.function(\n+                inputs,\n                 [self.total_loss] + self.metrics_tensors,\n                 updates=self.state_updates + self.metrics_updates,\n                 name='test_function',\n@@ -1017,7 +519,7 @@ class Model(Network):\n         if not hasattr(self, 'predict_function'):\n             self.predict_function = None\n         if self.predict_function is None:\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+            if self._uses_dynamic_learning_phase():\n                 inputs = self._feed_inputs + [K.learning_phase()]\n             else:\n                 inputs = self._feed_inputs\n@@ -1030,422 +532,9 @@ class Model(Network):\n                                                name='predict_function',\n                                                **kwargs)\n \n-    def _check_num_samples(self, ins, batch_size=None, steps=None, steps_name='steps'):\n-        \"\"\"Determine the number of samples provided for training and evaluation.\n-\n-        The number of samples is not defined when running with `steps`,\n-        in which case the number of samples is set to `None`.\n-\n-        # Arguments\n-            ins: List of tensors to be fed to the Keras function.\n-            batch_size: Integer batch size or `None` if not defined.\n-            steps: Total number of steps (batches of samples)\n-                before declaring `_predict_loop` finished.\n-                Ignored with the default value of `None`.\n-            steps_name: The public API's parameter name for `steps`.\n-\n-        # Raises\n-            ValueError: when `steps` is `None` and the attribute `ins.shape`\n-            does not exist. Also raises ValueError when `steps` is not `None`\n-            and `batch_size` is not `None` because they are mutually\n-            exclusive.\n-\n-        # Returns\n-            When steps is `None`, returns the number of samples to be\n-            processed based on the size of the first dimension of the\n-            first input numpy array. When steps is not `None` and\n-            `batch_size` is `None`, returns `None`.\n-\n-        # Raises\n-            ValueError: In case of invalid arguments.\n-        \"\"\"\n-        if steps is not None:\n-            num_samples = None\n-            if batch_size is not None:\n-                raise ValueError('If ' + steps_name +\n-                                 ' is set, the `batch_size` must be None.')\n-        elif ins and hasattr(ins[0], 'shape'):\n-            num_samples = ins[0].shape[0]\n-        else:\n-            raise ValueError('Either the input data should have '\n-                             'a defined shape, or ' + steps_name +\n-                             ' should be specified.')\n-        return num_samples\n-\n-    def _fit_loop(self, f, ins, out_labels=None, batch_size=None,\n-                  epochs=100, verbose=1, callbacks=None,\n-                  val_f=None, val_ins=None, shuffle=True,\n-                  callback_metrics=None, initial_epoch=0,\n-                  steps_per_epoch=None, validation_steps=None):\n-        \"\"\"Abstract fit function for `f(ins)`.\n-\n-        Assume that f returns a list, labeled by out_labels.\n-\n-        # Arguments\n-            f: Keras function returning a list of tensors\n-            ins: List of tensors to be fed to `f`\n-            out_labels: List of strings, display names of\n-                the outputs of `f`\n-            batch_size: Integer batch size or None if unknown.\n-            epochs: Number of times to iterate over the data\n-            verbose: Verbosity mode, 0, 1 or 2\n-            callbacks: List of callbacks to be called during training\n-            val_f: Keras function to call for validation\n-            val_ins: List of tensors to be fed to `val_f`\n-            shuffle: Whether to shuffle the data at the beginning of each epoch\n-            callback_metrics: List of strings, the display names of the metrics\n-                passed to the callbacks. They should be the\n-                concatenation of list the display names of the outputs of\n-                 `f` and the list of display names of the outputs of `f_val`.\n-            initial_epoch: Epoch at which to start training\n-                (useful for resuming a previous training run)\n-            steps_per_epoch: Total number of steps (batches of samples)\n-                before declaring one epoch finished and starting the\n-                next epoch. Ignored with the default value of `None`.\n-            validation_steps: Number of steps to run validation for\n-                (only if doing validation from data tensors).\n-                Ignored with the default value of `None`.\n-\n-        # Returns\n-            `History` object.\n-        \"\"\"\n-        do_validation = False\n-        if val_f and val_ins:\n-            do_validation = True\n-            if verbose and ins and hasattr(ins[0], 'shape') and hasattr(val_ins[0], 'shape'):\n-                print('Train on %d samples, validate on %d samples' %\n-                      (ins[0].shape[0], val_ins[0].shape[0]))\n-        if validation_steps:\n-            do_validation = True\n-            if steps_per_epoch is None:\n-                raise ValueError('Can only use `validation_steps` '\n-                                 'when doing step-wise '\n-                                 'training, i.e. `steps_per_epoch` '\n-                                 'must be set.')\n-\n-        num_train_samples = self._check_num_samples(ins, batch_size,\n-                                                    steps_per_epoch,\n-                                                    'steps_per_epoch')\n-        if num_train_samples is not None:\n-            index_array = np.arange(num_train_samples)\n-\n-        self.history = cbks.History()\n-        _callbacks = [cbks.BaseLogger(\n-            stateful_metrics=self.stateful_metric_names)]\n-        if verbose:\n-            if steps_per_epoch is not None:\n-                count_mode = 'steps'\n-            else:\n-                count_mode = 'samples'\n-            _callbacks.append(\n-                cbks.ProgbarLogger(\n-                    count_mode,\n-                    stateful_metrics=self.stateful_metric_names))\n-        _callbacks += (callbacks or []) + [self.history]\n-        callbacks = cbks.CallbackList(_callbacks)\n-        out_labels = out_labels or []\n-\n-        # it's possible to callback a different model than self\n-        # (used by Sequential models)\n-        if hasattr(self, 'callback_model') and self.callback_model:\n-            callback_model = self.callback_model\n-        else:\n-            callback_model = self\n-\n-        callbacks.set_model(callback_model)\n-        callbacks.set_params({\n-            'batch_size': batch_size,\n-            'epochs': epochs,\n-            'steps': steps_per_epoch,\n-            'samples': num_train_samples,\n-            'verbose': verbose,\n-            'do_validation': do_validation,\n-            'metrics': callback_metrics or [],\n-        })\n-        callbacks.on_train_begin()\n-        callback_model.stop_training = False\n-        for cbk in callbacks:\n-            cbk.validation_data = val_ins\n-\n-        # To prevent a slowdown, we find beforehand the arrays that need conversion.\n-        feed = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n-        indices_for_conversion_to_dense = []\n-        for i in range(len(feed)):\n-            if issparse(ins[i]) and not K.is_sparse(feed[i]):\n-                indices_for_conversion_to_dense.append(i)\n-\n-        for epoch in range(initial_epoch, epochs):\n-            # Reset stateful metrics\n-            for m in self.stateful_metric_functions:\n-                m.reset_states()\n-            callbacks.on_epoch_begin(epoch)\n-            epoch_logs = {}\n-            if steps_per_epoch is not None:\n-                for step_index in range(steps_per_epoch):\n-                    batch_logs = {}\n-                    batch_logs['batch'] = step_index\n-                    batch_logs['size'] = 1\n-                    callbacks.on_batch_begin(step_index, batch_logs)\n-                    outs = f(ins)\n-\n-                    if not isinstance(outs, list):\n-                        outs = [outs]\n-                    for l, o in zip(out_labels, outs):\n-                        batch_logs[l] = o\n-\n-                    callbacks.on_batch_end(step_index, batch_logs)\n-                    if callback_model.stop_training:\n-                        break\n-\n-                if do_validation:\n-                    val_outs = self._test_loop(val_f, val_ins,\n-                                               batch_size=batch_size,\n-                                               steps=validation_steps,\n-                                               verbose=0)\n-                    if not isinstance(val_outs, list):\n-                        val_outs = [val_outs]\n-                    # Same labels assumed.\n-                    for l, o in zip(out_labels, val_outs):\n-                        epoch_logs['val_' + l] = o\n-            else:\n-                if shuffle == 'batch':\n-                    index_array = _batch_shuffle(index_array, batch_size)\n-                elif shuffle:\n-                    np.random.shuffle(index_array)\n-\n-                batches = _make_batches(num_train_samples, batch_size)\n-                for batch_index, (batch_start, batch_end) in enumerate(batches):\n-                    batch_ids = index_array[batch_start:batch_end]\n-                    try:\n-                        if isinstance(ins[-1], float):\n-                            # Do not slice the training phase flag.\n-                            ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n-                        else:\n-                            ins_batch = _slice_arrays(ins, batch_ids)\n-                    except TypeError:\n-                        raise TypeError('TypeError while preparing batch. '\n-                                        'If using HDF5 input data, '\n-                                        'pass shuffle=\"batch\".')\n-                    batch_logs = {}\n-                    batch_logs['batch'] = batch_index\n-                    batch_logs['size'] = len(batch_ids)\n-                    callbacks.on_batch_begin(batch_index, batch_logs)\n-                    for i in indices_for_conversion_to_dense:\n-                        ins_batch[i] = ins_batch[i].toarray()\n-\n-                    outs = f(ins_batch)\n-                    if not isinstance(outs, list):\n-                        outs = [outs]\n-                    for l, o in zip(out_labels, outs):\n-                        batch_logs[l] = o\n-\n-                    callbacks.on_batch_end(batch_index, batch_logs)\n-                    if callback_model.stop_training:\n-                        break\n-\n-                    if batch_index == len(batches) - 1:  # Last batch.\n-                        if do_validation:\n-                            val_outs = self._test_loop(val_f, val_ins,\n-                                                       batch_size=batch_size,\n-                                                       verbose=0)\n-                            if not isinstance(val_outs, list):\n-                                val_outs = [val_outs]\n-                            # Same labels assumed.\n-                            for l, o in zip(out_labels, val_outs):\n-                                epoch_logs['val_' + l] = o\n-            callbacks.on_epoch_end(epoch, epoch_logs)\n-            if callback_model.stop_training:\n-                break\n-        callbacks.on_train_end()\n-        return self.history\n-\n-    def _predict_loop(self, f, ins, batch_size=32, verbose=0, steps=None):\n-        \"\"\"Abstract method to loop over some data in batches.\n-\n-        # Arguments\n-            f: Keras function returning a list of tensors.\n-            ins: list of tensors to be fed to `f`.\n-            batch_size: integer batch size.\n-            verbose: verbosity mode.\n-            steps: Total number of steps (batches of samples)\n-                before declaring `_predict_loop` finished.\n-                Ignored with the default value of `None`.\n-\n-        # Returns\n-            Array of predictions (if the model has a single output)\n-            or list of arrays of predictions\n-            (if the model has multiple outputs).\n-        \"\"\"\n-        num_samples = self._check_num_samples(ins, batch_size,\n-                                              steps,\n-                                              'steps')\n-        if verbose == 1:\n-            if steps is not None:\n-                progbar = Progbar(target=steps)\n-            else:\n-                progbar = Progbar(target=num_samples)\n-\n-        indices_for_conversion_to_dense = []\n-        for i in range(len(self._feed_inputs)):\n-            if issparse(ins[i]) and not K.is_sparse(self._feed_inputs[i]):\n-                indices_for_conversion_to_dense.append(i)\n-\n-        if steps is not None:\n-            # Step-based predictions.\n-            # Since we do not know how many samples\n-            # we will see, we cannot pre-allocate\n-            # the returned Numpy arrays.\n-            # Instead, we store one array per batch seen\n-            # and concatenate them upon returning.\n-            unconcatenated_outs = []\n-            for step in range(steps):\n-                batch_outs = f(ins)\n-                if not isinstance(batch_outs, list):\n-                    batch_outs = [batch_outs]\n-                if step == 0:\n-                    for batch_out in batch_outs:\n-                        unconcatenated_outs.append([])\n-                for i, batch_out in enumerate(batch_outs):\n-                    unconcatenated_outs[i].append(batch_out)\n-                if verbose == 1:\n-                    progbar.update(step + 1)\n-            if len(unconcatenated_outs) == 1:\n-                return np.concatenate(unconcatenated_outs[0], axis=0)\n-            return [np.concatenate(unconcatenated_outs[i], axis=0)\n-                    for i in range(len(unconcatenated_outs))]\n-        else:\n-            # Sample-based predictions.\n-            outs = []\n-            batches = _make_batches(num_samples, batch_size)\n-            index_array = np.arange(num_samples)\n-            for batch_index, (batch_start, batch_end) in enumerate(batches):\n-                batch_ids = index_array[batch_start:batch_end]\n-                if ins and isinstance(ins[-1], float):\n-                    # Do not slice the training phase flag.\n-                    ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n-                else:\n-                    ins_batch = _slice_arrays(ins, batch_ids)\n-                for i in indices_for_conversion_to_dense:\n-                    ins_batch[i] = ins_batch[i].toarray()\n-\n-                batch_outs = f(ins_batch)\n-                if not isinstance(batch_outs, list):\n-                    batch_outs = [batch_outs]\n-                if batch_index == 0:\n-                    # Pre-allocate the results arrays.\n-                    for batch_out in batch_outs:\n-                        shape = (num_samples,) + batch_out.shape[1:]\n-                        outs.append(np.zeros(shape, dtype=batch_out.dtype))\n-                for i, batch_out in enumerate(batch_outs):\n-                    outs[i][batch_start:batch_end] = batch_out\n-                if verbose == 1:\n-                    progbar.update(batch_end)\n-            if len(outs) == 1:\n-                return outs[0]\n-            return outs\n-\n-    def _test_loop(self, f, ins, batch_size=None, verbose=0, steps=None):\n-        \"\"\"Abstract method to loop over some data in batches.\n-\n-        # Arguments\n-            f: Keras function returning a list of tensors.\n-            ins: list of tensors to be fed to `f`.\n-            batch_size: integer batch size or `None`.\n-            verbose: verbosity mode.\n-            steps: Total number of steps (batches of samples)\n-                before declaring predictions finished.\n-                Ignored with the default value of `None`.\n-\n-        # Returns\n-            Scalar loss (if the model has a single output and no metrics)\n-            or list of scalars (if the model has multiple outputs\n-            and/or metrics). The attribute `model.metrics_names` will give you\n-            the display labels for the scalar outputs.\n-        \"\"\"\n-\n-        if hasattr(self, 'metrics'):\n-            for m in self.stateful_metric_functions:\n-                m.reset_states()\n-            stateful_metric_indices = [\n-                i for i, name in enumerate(self.metrics_names)\n-                if str(name) in self.stateful_metric_names]\n-        else:\n-            stateful_metric_indices = []\n-\n-        num_samples = self._check_num_samples(ins, batch_size,\n-                                              steps,\n-                                              'steps')\n-        outs = []\n-        if verbose == 1:\n-            if steps is not None:\n-                progbar = Progbar(target=steps)\n-            else:\n-                progbar = Progbar(target=num_samples)\n-\n-        # To prevent a slowdown, we find beforehand the arrays that need conversion.\n-        feed = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n-        indices_for_conversion_to_dense = []\n-        for i in range(len(feed)):\n-            if issparse(ins[i]) and not K.is_sparse(feed[i]):\n-                indices_for_conversion_to_dense.append(i)\n-\n-        if steps is not None:\n-            for step in range(steps):\n-                batch_outs = f(ins)\n-                if isinstance(batch_outs, list):\n-                    if step == 0:\n-                        for _ in enumerate(batch_outs):\n-                            outs.append(0.)\n-                    for i, batch_out in enumerate(batch_outs):\n-                        if i in stateful_metric_indices:\n-                            outs[i] = float(batch_out)\n-                        else:\n-                            outs[i] += batch_out\n-                else:\n-                    if step == 0:\n-                        outs.append(0.)\n-                    outs[0] += batch_outs\n-                if verbose == 1:\n-                    progbar.update(step + 1)\n-            for i in range(len(outs)):\n-                if i not in stateful_metric_indices:\n-                    outs[i] /= steps\n-        else:\n-            batches = _make_batches(num_samples, batch_size)\n-            index_array = np.arange(num_samples)\n-            for batch_index, (batch_start, batch_end) in enumerate(batches):\n-                batch_ids = index_array[batch_start:batch_end]\n-                if isinstance(ins[-1], float):\n-                    # Do not slice the training phase flag.\n-                    ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n-                else:\n-                    ins_batch = _slice_arrays(ins, batch_ids)\n-                for i in indices_for_conversion_to_dense:\n-                    ins_batch[i] = ins_batch[i].toarray()\n-\n-                batch_outs = f(ins_batch)\n-                if isinstance(batch_outs, list):\n-                    if batch_index == 0:\n-                        for batch_out in enumerate(batch_outs):\n-                            outs.append(0.)\n-                    for i, batch_out in enumerate(batch_outs):\n-                        if i in stateful_metric_indices:\n-                            outs[i] = batch_out\n-                        else:\n-                            outs[i] += batch_out * len(batch_ids)\n-                else:\n-                    if batch_index == 0:\n-                        outs.append(0.)\n-                    outs[0] += batch_outs * len(batch_ids)\n-\n-                if verbose == 1:\n-                    progbar.update(batch_end)\n-            for i in range(len(outs)):\n-                if i not in stateful_metric_indices:\n-                    outs[i] /= num_samples\n-        if len(outs) == 1:\n-            return outs[0]\n-        return outs\n+    def _uses_dynamic_learning_phase(self):\n+        return (self.uses_learning_phase and\n+                not isinstance(K.learning_phase(), int))\n \n     def _standardize_user_data(self, x, y,\n                                sample_weight=None, class_weight=None,\n@@ -1456,7 +545,8 @@ class Model(Network):\n                                'Use `model.compile(optimizer, loss)`.')\n \n         output_shapes = []\n-        for output_shape, loss_fn in zip(self._feed_output_shapes, self._feed_loss_fns):\n+        for output_shape, loss_fn in zip(self._feed_output_shapes,\n+                                         self._feed_loss_fns):\n             if loss_fn is losses.sparse_categorical_crossentropy:\n                 output_shapes.append(output_shape[:-1] + (1,))\n             elif (not hasattr(loss_fn, '__name__') or\n@@ -1468,27 +558,33 @@ class Model(Network):\n                 output_shapes.append(None)\n             else:\n                 output_shapes.append(output_shape)\n-        # `check_batch_axis` is set to False since `x` may contain multiple batches\n+        # `check_batch_axis` is set to False since `x`\n+        # may contain multiple batches\n         # and in general `x[0].shape[0] != self._feed_input_shapes[0][0]`\n-        x = _standardize_input_data(x, self._feed_input_names,\n+        x = standardize_input_data(x,\n+                                   self._feed_input_names,\n                                    self._feed_input_shapes,\n                                    check_batch_axis=False,\n                                    exception_prefix='input')\n-        y = _standardize_input_data(y, self._feed_output_names,\n+        y = standardize_input_data(y,\n+                                   self._feed_output_names,\n                                    output_shapes,\n                                    check_batch_axis=False,\n                                    exception_prefix='target')\n-        sample_weights = _standardize_sample_weights(sample_weight,\n+        sample_weights = standardize_sample_weights(sample_weight,\n                                                     self._feed_output_names)\n-        class_weights = _standardize_class_weights(class_weight,\n+        class_weights = standardize_class_weights(class_weight,\n                                                   self._feed_output_names)\n-        sample_weights = [_standardize_weights(ref, sw, cw, mode)\n+        sample_weights = [standardize_weights(ref, sw, cw, mode)\n                           for (ref, sw, cw, mode)\n-                          in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n+                          in zip(y,\n+                                 sample_weights,\n+                                 class_weights,\n+                                 self._feed_sample_weight_modes)]\n \n         if check_array_lengths:\n-            _check_array_lengths(x, y, sample_weights)\n-        _check_loss_and_target_compatibility(y,\n+            check_array_length_consistency(x, y, sample_weights)\n+        check_loss_and_target_compatibility(y,\n                                             self._feed_loss_fns,\n                                             self._feed_output_shapes)\n         if self.stateful and batch_size:\n@@ -1516,7 +612,7 @@ class Model(Network):\n             steps_per_epoch=None,\n             validation_steps=None,\n             **kwargs):\n-        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n+        \"\"\"Trains the model for a given number of epochs (iterations on a dataset).\n \n         # Arguments\n             x: Numpy array of training data (if the model has a single input),\n@@ -1648,7 +744,7 @@ class Model(Network):\n                 val_x, val_y,\n                 sample_weight=val_sample_weight,\n                 batch_size=batch_size)\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+            if self._uses_dynamic_learning_phase():\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n             else:\n                 val_ins = val_x + val_y + val_sample_weights\n@@ -1659,23 +755,25 @@ class Model(Network):\n                 split_at = int(x[0].shape[0] * (1. - validation_split))\n             else:\n                 split_at = int(len(x[0]) * (1. - validation_split))\n-            x, val_x = (_slice_arrays(x, 0, split_at), _slice_arrays(x, split_at))\n-            y, val_y = (_slice_arrays(y, 0, split_at), _slice_arrays(y, split_at))\n+            x, val_x = (slice_arrays(x, 0, split_at),\n+                        slice_arrays(x, split_at))\n+            y, val_y = (slice_arrays(y, 0, split_at),\n+                        slice_arrays(y, split_at))\n             sample_weights, val_sample_weights = (\n-                _slice_arrays(sample_weights, 0, split_at),\n-                _slice_arrays(sample_weights, split_at))\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+                slice_arrays(sample_weights, 0, split_at),\n+                slice_arrays(sample_weights, split_at))\n+            if self._uses_dynamic_learning_phase():\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n             else:\n                 val_ins = val_x + val_y + val_sample_weights\n \n         elif validation_steps:\n             do_validation = True\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+            if self._uses_dynamic_learning_phase():\n                 val_ins = [0.]\n \n         # Prepare input arrays and training function.\n-        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+        if self._uses_dynamic_learning_phase():\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n@@ -1688,17 +786,23 @@ class Model(Network):\n         if do_validation:\n             self._make_test_function()\n             val_f = self.test_function\n-            callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n+            callback_metrics = copy.copy(out_labels) + [\n+                'val_' + n for n in out_labels]\n         else:\n             callback_metrics = copy.copy(out_labels)\n             val_f = None\n             val_ins = []\n \n-        # Delegate logic to `_fit_loop`.\n-        return self._fit_loop(f, ins, out_labels=out_labels,\n-                              batch_size=batch_size, epochs=epochs,\n-                              verbose=verbose, callbacks=callbacks,\n-                              val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n+        # Delegate logic to `fit_loop`.\n+        return training_arrays.fit_loop(self, f, ins,\n+                                        out_labels=out_labels,\n+                                        batch_size=batch_size,\n+                                        epochs=epochs,\n+                                        verbose=verbose,\n+                                        callbacks=callbacks,\n+                                        val_f=val_f,\n+                                        val_ins=val_ins,\n+                                        shuffle=shuffle,\n                                         callback_metrics=callback_metrics,\n                                         initial_epoch=initial_epoch,\n                                         steps_per_epoch=steps_per_epoch,\n@@ -1766,14 +870,14 @@ class Model(Network):\n             x, y,\n             sample_weight=sample_weight,\n             batch_size=batch_size)\n-        # Prepare inputs, delegate logic to `_test_loop`.\n-        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+        # Prepare inputs, delegate logic to `test_loop`.\n+        if self._uses_dynamic_learning_phase():\n             ins = x + y + sample_weights + [0.]\n         else:\n             ins = x + y + sample_weights\n         self._make_test_function()\n         f = self.test_function\n-        return self._test_loop(f, ins,\n+        return training_arrays.test_loop(self, f, ins,\n                                          batch_size=batch_size,\n                                          verbose=verbose,\n                                          steps=steps)\n@@ -1812,7 +916,8 @@ class Model(Network):\n                              'you should specify the `steps` '\n                              'argument.')\n         # Validate user data.\n-        x = _standardize_input_data(x, self._feed_input_names,\n+        x = standardize_input_data(x,\n+                                   self._feed_input_names,\n                                    self._feed_input_shapes,\n                                    check_batch_axis=False)\n         if self.stateful:\n@@ -1824,15 +929,17 @@ class Model(Network):\n                                  str(x[0].shape[0]) + ' samples. '\n                                  'Batch size: ' + str(batch_size) + '.')\n \n-        # Prepare inputs, delegate logic to `_predict_loop`.\n-        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+        # Prepare inputs, delegate logic to `predict_loop`.\n+        if self._uses_dynamic_learning_phase():\n             ins = x + [0.]\n         else:\n             ins = x\n         self._make_predict_function()\n         f = self.predict_function\n-        return self._predict_loop(f, ins, batch_size=batch_size,\n-                                  verbose=verbose, steps=steps)\n+        return training_arrays.predict_loop(self, f, ins,\n+                                            batch_size=batch_size,\n+                                            verbose=verbose,\n+                                            steps=steps)\n \n     def train_on_batch(self, x, y,\n                        sample_weight=None,\n@@ -1875,7 +982,7 @@ class Model(Network):\n             x, y,\n             sample_weight=sample_weight,\n             class_weight=class_weight)\n-        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+        if self._uses_dynamic_learning_phase():\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n@@ -1916,7 +1023,7 @@ class Model(Network):\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight)\n-        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+        if self._uses_dynamic_learning_phase():\n             ins = x + y + sample_weights + [0.]\n         else:\n             ins = x + y + sample_weights\n@@ -1935,9 +1042,10 @@ class Model(Network):\n         # Returns\n             Numpy array(s) of predictions.\n         \"\"\"\n-        x = _standardize_input_data(x, self._feed_input_names,\n+        x = standardize_input_data(x,\n+                                   self._feed_input_names,\n                                    self._feed_input_shapes)\n-        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+        if self._uses_dynamic_learning_phase():\n             ins = x + [0.]\n         else:\n             ins = x\n@@ -1948,8 +1056,7 @@ class Model(Network):\n         return outputs\n \n     @interfaces.legacy_generator_methods_support\n-    def fit_generator(self,\n-                      generator,\n+    def fit_generator(self, generator,\n                       steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n@@ -1962,7 +1069,7 @@ class Model(Network):\n                       use_multiprocessing=False,\n                       shuffle=True,\n                       initial_epoch=0):\n-        \"\"\"Trains the model on data generated batch-by-batch by a Python generator or an instance of `Sequence`.\n+        \"\"\"Trains the model on data generated batch-by-batch by a Python generator (or an instance of `Sequence`).\n \n         The generator is run in parallel to the model, for efficiency.\n         For instance, this allows you to do real-time data augmentation\n@@ -2027,7 +1134,8 @@ class Model(Network):\n             class_weight: Optional dictionary mapping class indices (integers)\n                 to a weight (float) value, used for weighting the loss function\n                 (during training only). This can be useful to tell the model to\n-                \"pay more attention\" to samples from an under-represented class.\n+                \"pay more attention\" to samples\n+                from an under-represented class.\n             max_queue_size: Integer. Maximum size for the generator queue.\n                 If unspecified, `max_queue_size` will default to 10.\n             workers: Integer. Maximum number of processes to spin up\n@@ -2037,7 +1145,8 @@ class Model(Network):\n             use_multiprocessing: Boolean.\n                 If `True`, use process-based threading.\n                 If unspecified, `use_multiprocessing` will default to `False`.\n-                Note that because this implementation relies on multiprocessing,\n+                Note that because this implementation\n+                relies on multiprocessing,\n                 you should not pass non-picklable arguments to the generator\n                 as they can't be passed easily to children processes.\n             shuffle: Boolean. Whether to shuffle the order of the batches at\n@@ -2073,216 +1182,24 @@ class Model(Network):\n                             steps_per_epoch=10000, epochs=10)\n         ```\n         \"\"\"\n-        wait_time = 0.01  # in seconds\n-        epoch = initial_epoch\n-\n-        do_validation = bool(validation_data)\n-        self._make_train_function()\n-        if do_validation:\n-            self._make_test_function()\n-\n-        is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing and workers > 1:\n-            warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' and multiple workers may duplicate your data.'\n-                            ' Please consider using the`keras.utils.Sequence'\n-                            ' class.'))\n-        if steps_per_epoch is None:\n-            if is_sequence:\n-                steps_per_epoch = len(generator)\n-            else:\n-                raise ValueError('`steps_per_epoch=None` is only valid for a'\n-                                 ' generator based on the `keras.utils.Sequence`'\n-                                 ' class. Please specify `steps_per_epoch` or use'\n-                                 ' the `keras.utils.Sequence` class.')\n-\n-        # python 2 has 'next', 3 has '__next__'\n-        # avoid any explicit version checks\n-        val_gen = (hasattr(validation_data, 'next') or\n-                   hasattr(validation_data, '__next__') or\n-                   isinstance(validation_data, Sequence))\n-        if (val_gen and not isinstance(validation_data, Sequence) and\n-                not validation_steps):\n-            raise ValueError('`validation_steps=None` is only valid for a'\n-                             ' generator based on the `keras.utils.Sequence`'\n-                             ' class. Please specify `validation_steps` or use'\n-                             ' the `keras.utils.Sequence` class.')\n-\n-        # Prepare display labels.\n-        out_labels = self.metrics_names\n-        callback_metrics = out_labels + ['val_' + n for n in out_labels]\n-\n-        # prepare callbacks\n-        self.history = cbks.History()\n-        _callbacks = [cbks.BaseLogger(\n-            stateful_metrics=self.stateful_metric_names)]\n-        if verbose:\n-            _callbacks.append(\n-                cbks.ProgbarLogger(\n-                    count_mode='steps',\n-                    stateful_metrics=self.stateful_metric_names))\n-        _callbacks += (callbacks or []) + [self.history]\n-        callbacks = cbks.CallbackList(_callbacks)\n-\n-        # it's possible to callback a different model than self:\n-        if hasattr(self, 'callback_model') and self.callback_model:\n-            callback_model = self.callback_model\n-        else:\n-            callback_model = self\n-        callbacks.set_model(callback_model)\n-        callbacks.set_params({\n-            'epochs': epochs,\n-            'steps': steps_per_epoch,\n-            'verbose': verbose,\n-            'do_validation': do_validation,\n-            'metrics': callback_metrics,\n-        })\n-        callbacks.on_train_begin()\n-\n-        enqueuer = None\n-        val_enqueuer = None\n-\n-        try:\n-            if do_validation and not val_gen:\n-                # Prepare data for validation\n-                if len(validation_data) == 2:\n-                    val_x, val_y = validation_data\n-                    val_sample_weight = None\n-                elif len(validation_data) == 3:\n-                    val_x, val_y, val_sample_weight = validation_data\n-                else:\n-                    raise ValueError('`validation_data` should be a tuple '\n-                                     '`(val_x, val_y, val_sample_weight)` '\n-                                     'or `(val_x, val_y)`. Found: ' +\n-                                     str(validation_data))\n-                val_x, val_y, val_sample_weights = self._standardize_user_data(\n-                    val_x, val_y, val_sample_weight)\n-                val_data = val_x + val_y + val_sample_weights\n-                if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                    val_data += [0.]\n-                for cbk in callbacks:\n-                    cbk.validation_data = val_data\n-\n-            if workers > 0:\n-                if is_sequence:\n-                    enqueuer = OrderedEnqueuer(generator,\n-                                               use_multiprocessing=use_multiprocessing,\n-                                               shuffle=shuffle)\n-                else:\n-                    enqueuer = GeneratorEnqueuer(generator,\n-                                                 use_multiprocessing=use_multiprocessing,\n-                                                 wait_time=wait_time)\n-                enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n-                output_generator = enqueuer.get()\n-            else:\n-                if is_sequence:\n-                    output_generator = iter(generator)\n-                else:\n-                    output_generator = generator\n-\n-            callback_model.stop_training = False\n-            # Construct epoch logs.\n-            epoch_logs = {}\n-            while epoch < epochs:\n-                for m in self.stateful_metric_functions:\n-                    m.reset_states()\n-                callbacks.on_epoch_begin(epoch)\n-                steps_done = 0\n-                batch_index = 0\n-                while steps_done < steps_per_epoch:\n-                    generator_output = next(output_generator)\n-\n-                    if not hasattr(generator_output, '__len__'):\n-                        raise ValueError('Output of generator should be '\n-                                         'a tuple `(x, y, sample_weight)` '\n-                                         'or `(x, y)`. Found: ' +\n-                                         str(generator_output))\n-\n-                    if len(generator_output) == 2:\n-                        x, y = generator_output\n-                        sample_weight = None\n-                    elif len(generator_output) == 3:\n-                        x, y, sample_weight = generator_output\n-                    else:\n-                        raise ValueError('Output of generator should be '\n-                                         'a tuple `(x, y, sample_weight)` '\n-                                         'or `(x, y)`. Found: ' +\n-                                         str(generator_output))\n-                    # build batch logs\n-                    batch_logs = {}\n-                    if x is None or len(x) == 0:\n-                        # Handle data tensors support when no input given\n-                        # step-size = 1 for data tensors\n-                        batch_size = 1\n-                    elif isinstance(x, list):\n-                        batch_size = x[0].shape[0]\n-                    elif isinstance(x, dict):\n-                        batch_size = list(x.values())[0].shape[0]\n-                    else:\n-                        batch_size = x.shape[0]\n-                    batch_logs['batch'] = batch_index\n-                    batch_logs['size'] = batch_size\n-                    callbacks.on_batch_begin(batch_index, batch_logs)\n-\n-                    outs = self.train_on_batch(x, y,\n-                                               sample_weight=sample_weight,\n-                                               class_weight=class_weight)\n-\n-                    if not isinstance(outs, list):\n-                        outs = [outs]\n-                    for l, o in zip(out_labels, outs):\n-                        batch_logs[l] = o\n-\n-                    callbacks.on_batch_end(batch_index, batch_logs)\n-\n-                    batch_index += 1\n-                    steps_done += 1\n-\n-                    # Epoch finished.\n-                    if steps_done >= steps_per_epoch and do_validation:\n-                        if val_gen:\n-                            val_outs = self.evaluate_generator(\n-                                validation_data,\n-                                validation_steps,\n+        return training_generator.fit_generator(\n+            self, generator,\n+            steps_per_epoch=steps_per_epoch,\n+            epochs=epochs,\n+            verbose=verbose,\n+            callbacks=callbacks,\n+            validation_data=validation_data,\n+            validation_steps=validation_steps,\n+            class_weight=class_weight,\n+            max_queue_size=max_queue_size,\n             workers=workers,\n             use_multiprocessing=use_multiprocessing,\n-                                max_queue_size=max_queue_size)\n-                        else:\n-                            # No need for try/except because\n-                            # data has already been validated.\n-                            val_outs = self.evaluate(\n-                                val_x, val_y,\n-                                batch_size=batch_size,\n-                                sample_weight=val_sample_weights,\n-                                verbose=0)\n-                        if not isinstance(val_outs, list):\n-                            val_outs = [val_outs]\n-                        # Same labels assumed.\n-                        for l, o in zip(out_labels, val_outs):\n-                            epoch_logs['val_' + l] = o\n-\n-                    if callback_model.stop_training:\n-                        break\n-\n-                callbacks.on_epoch_end(epoch, epoch_logs)\n-                epoch += 1\n-                if callback_model.stop_training:\n-                    break\n-\n-        finally:\n-            try:\n-                if enqueuer is not None:\n-                    enqueuer.stop()\n-            finally:\n-                if val_enqueuer is not None:\n-                    val_enqueuer.stop()\n-\n-        callbacks.on_train_end()\n-        return self.history\n+            shuffle=shuffle,\n+            initial_epoch=initial_epoch)\n \n     @interfaces.legacy_generator_methods_support\n-    def evaluate_generator(self, generator, steps=None,\n+    def evaluate_generator(self, generator,\n+                           steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False,\n@@ -2326,117 +1243,17 @@ class Model(Network):\n             ValueError: In case the generator yields\n                 data in an invalid format.\n         \"\"\"\n-        self._make_test_function()\n-\n-        stateful_metric_indices = []\n-        if hasattr(self, 'metrics'):\n-            for m in self.stateful_metric_functions:\n-                m.reset_states()\n-            stateful_metric_indices = [\n-                i for i, name in enumerate(self.metrics_names)\n-                if str(name) in self.stateful_metric_names]\n-        else:\n-            stateful_metric_indices = []\n-\n-        steps_done = 0\n-        wait_time = 0.01\n-        outs_per_batch = []\n-        batch_sizes = []\n-        is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing and workers > 1:\n-            warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' and multiple workers may duplicate your data.'\n-                            ' Please consider using the`keras.utils.Sequence'\n-                            ' class.'))\n-        if steps is None:\n-            if is_sequence:\n-                steps = len(generator)\n-            else:\n-                raise ValueError('`steps=None` is only valid for a generator'\n-                                 ' based on the `keras.utils.Sequence` class.'\n-                                 ' Please specify `steps` or use the'\n-                                 ' `keras.utils.Sequence` class.')\n-        enqueuer = None\n-\n-        try:\n-            if workers > 0:\n-                if is_sequence:\n-                    enqueuer = OrderedEnqueuer(generator,\n-                                               use_multiprocessing=use_multiprocessing)\n-                else:\n-                    enqueuer = GeneratorEnqueuer(generator,\n+        return training_generator.evaluate_generator(\n+            self, generator,\n+            steps=steps,\n+            max_queue_size=max_queue_size,\n+            workers=workers,\n             use_multiprocessing=use_multiprocessing,\n-                                                 wait_time=wait_time)\n-                enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n-                output_generator = enqueuer.get()\n-            else:\n-                if is_sequence:\n-                    output_generator = iter(generator)\n-                else:\n-                    output_generator = generator\n-\n-            if verbose == 1:\n-                progbar = Progbar(target=steps)\n-\n-            while steps_done < steps:\n-                generator_output = next(output_generator)\n-                if not hasattr(generator_output, '__len__'):\n-                    raise ValueError('Output of generator should be a tuple '\n-                                     '(x, y, sample_weight) '\n-                                     'or (x, y). Found: ' +\n-                                     str(generator_output))\n-                if len(generator_output) == 2:\n-                    x, y = generator_output\n-                    sample_weight = None\n-                elif len(generator_output) == 3:\n-                    x, y, sample_weight = generator_output\n-                else:\n-                    raise ValueError('Output of generator should be a tuple '\n-                                     '(x, y, sample_weight) '\n-                                     'or (x, y). Found: ' +\n-                                     str(generator_output))\n-                outs = self.test_on_batch(x, y, sample_weight=sample_weight)\n-                if not isinstance(outs, list):\n-                    outs = [outs]\n-                outs_per_batch.append(outs)\n-\n-                if x is None or len(x) == 0:\n-                    # Handle data tensors support when no input given\n-                    # step-size = 1 for data tensors\n-                    batch_size = 1\n-                elif isinstance(x, list):\n-                    batch_size = x[0].shape[0]\n-                elif isinstance(x, dict):\n-                    batch_size = list(x.values())[0].shape[0]\n-                else:\n-                    batch_size = x.shape[0]\n-                if batch_size == 0:\n-                    raise ValueError('Received an empty batch. '\n-                                     'Batches should at least contain one item.')\n-\n-                steps_done += 1\n-                batch_sizes.append(batch_size)\n-                if verbose == 1:\n-                    progbar.update(steps_done)\n-\n-        finally:\n-            if enqueuer is not None:\n-                enqueuer.stop()\n-\n-        averages = []\n-        for i in range(len(outs)):\n-            if i not in stateful_metric_indices:\n-                averages.append(np.average([out[i] for out in outs_per_batch],\n-                                           weights=batch_sizes))\n-            else:\n-                averages.append(float(outs_per_batch[-1][i]))\n-        if len(averages) == 1:\n-            return averages[0]\n-        return averages\n+            verbose=verbose)\n \n     @interfaces.legacy_generator_methods_support\n-    def predict_generator(self, generator, steps=None,\n+    def predict_generator(self, generator,\n+                          steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,\n@@ -2476,91 +1293,10 @@ class Model(Network):\n             ValueError: In case the generator yields\n                 data in an invalid format.\n         \"\"\"\n-        self._make_predict_function()\n-\n-        steps_done = 0\n-        wait_time = 0.01\n-        all_outs = []\n-        is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing and workers > 1:\n-            warnings.warn(\n-                UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' and multiple workers may duplicate your data.'\n-                            ' Please consider using the`keras.utils.Sequence'\n-                            ' class.'))\n-        if steps is None:\n-            if is_sequence:\n-                steps = len(generator)\n-            else:\n-                raise ValueError('`steps=None` is only valid for a generator'\n-                                 ' based on the `keras.utils.Sequence` class.'\n-                                 ' Please specify `steps` or use the'\n-                                 ' `keras.utils.Sequence` class.')\n-        enqueuer = None\n-\n-        try:\n-            if workers > 0:\n-                if is_sequence:\n-                    enqueuer = OrderedEnqueuer(generator,\n-                                               use_multiprocessing=use_multiprocessing)\n-                else:\n-                    enqueuer = GeneratorEnqueuer(generator,\n+        return training_generator.predict_generator(\n+            self, generator,\n+            steps=steps,\n+            max_queue_size=max_queue_size,\n+            workers=workers,\n             use_multiprocessing=use_multiprocessing,\n-                                                 wait_time=wait_time)\n-                enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n-                output_generator = enqueuer.get()\n-            else:\n-                if is_sequence:\n-                    output_generator = iter(generator)\n-                else:\n-                    output_generator = generator\n-\n-            if verbose == 1:\n-                progbar = Progbar(target=steps)\n-\n-            while steps_done < steps:\n-                generator_output = next(output_generator)\n-                if isinstance(generator_output, tuple):\n-                    # Compatibility with the generators\n-                    # used for training.\n-                    if len(generator_output) == 2:\n-                        x, _ = generator_output\n-                    elif len(generator_output) == 3:\n-                        x, _, _ = generator_output\n-                    else:\n-                        raise ValueError('Output of generator should be '\n-                                         'a tuple `(x, y, sample_weight)` '\n-                                         'or `(x, y)`. Found: ' +\n-                                         str(generator_output))\n-                else:\n-                    # Assumes a generator that only\n-                    # yields inputs (not targets and sample weights).\n-                    x = generator_output\n-\n-                outs = self.predict_on_batch(x)\n-                if not isinstance(outs, list):\n-                    outs = [outs]\n-\n-                if not all_outs:\n-                    for out in outs:\n-                        all_outs.append([])\n-\n-                for i, out in enumerate(outs):\n-                    all_outs[i].append(out)\n-                steps_done += 1\n-                if verbose == 1:\n-                    progbar.update(steps_done)\n-\n-        finally:\n-            if enqueuer is not None:\n-                enqueuer.stop()\n-\n-        if len(all_outs) == 1:\n-            if steps_done == 1:\n-                return all_outs[0][0]\n-            else:\n-                return np.concatenate(all_outs[0])\n-        if steps_done == 1:\n-            return [out[0] for out in all_outs]\n-        else:\n-            return [np.concatenate(out) for out in all_outs]\n+            verbose=verbose)\n\n@@ -0,0 +1,416 @@\n+\"\"\"Part of the training engine related to plain array data (e.g. Numpy).\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import numpy as np\n+from scipy.sparse import issparse\n+\n+from .training_utils import batch_shuffle\n+from .training_utils import make_batches\n+from .training_utils import check_num_samples\n+from .. import backend as K\n+from .. import callbacks as cbks\n+from ..utils.generic_utils import Progbar\n+from ..utils.generic_utils import slice_arrays\n+\n+\n+def fit_loop(model, f, ins,\n+             out_labels=None,\n+             batch_size=None,\n+             epochs=100,\n+             verbose=1,\n+             callbacks=None,\n+             val_f=None,\n+             val_ins=None,\n+             shuffle=True,\n+             callback_metrics=None,\n+             initial_epoch=0,\n+             steps_per_epoch=None,\n+             validation_steps=None):\n+    \"\"\"Abstract fit function for `f(ins)`.\n+\n+    Assumes that f returns a list, labeled by out_labels.\n+\n+    # Arguments\n+        model: Keras model instance.\n+        f: Keras function returning a list of tensors\n+        ins: List of tensors to be fed to `f`\n+        out_labels: List of strings, display names of\n+            the outputs of `f`\n+        batch_size: Integer batch size or None if unknown.\n+        epochs: Number of times to iterate over the data\n+        verbose: Verbosity mode, 0, 1 or 2\n+        callbacks: List of callbacks to be called during training\n+        val_f: Keras function to call for validation\n+        val_ins: List of tensors to be fed to `val_f`\n+        shuffle: Whether to shuffle the data at the beginning of each epoch\n+        callback_metrics: List of strings, the display names of the metrics\n+            passed to the callbacks. They should be the\n+            concatenation of list the display names of the outputs of\n+             `f` and the list of display names of the outputs of `f_val`.\n+        initial_epoch: Epoch at which to start training\n+            (useful for resuming a previous training run)\n+        steps_per_epoch: Total number of steps (batches of samples)\n+            before declaring one epoch finished and starting the\n+            next epoch. Ignored with the default value of `None`.\n+        validation_steps: Number of steps to run validation for\n+            (only if doing validation from data tensors).\n+            Ignored with the default value of `None`.\n+\n+    # Returns\n+        `History` object.\n+    \"\"\"\n+    do_validation = False\n+    if val_f and val_ins:\n+        do_validation = True\n+        if (verbose and ins and\n+           hasattr(ins[0], 'shape') and hasattr(val_ins[0], 'shape')):\n+            print('Train on %d samples, validate on %d samples' %\n+                  (ins[0].shape[0], val_ins[0].shape[0]))\n+    if validation_steps:\n+        do_validation = True\n+        if steps_per_epoch is None:\n+            raise ValueError('Can only use `validation_steps` '\n+                             'when doing step-wise '\n+                             'training, i.e. `steps_per_epoch` '\n+                             'must be set.')\n+\n+    num_train_samples = check_num_samples(ins,\n+                                          batch_size=batch_size,\n+                                          steps=steps_per_epoch,\n+                                          steps_name='steps_per_epoch')\n+    if num_train_samples is not None:\n+        index_array = np.arange(num_train_samples)\n+\n+    model.history = cbks.History()\n+    _callbacks = [cbks.BaseLogger(\n+        stateful_metrics=model.stateful_metric_names)]\n+    if verbose:\n+        if steps_per_epoch is not None:\n+            count_mode = 'steps'\n+        else:\n+            count_mode = 'samples'\n+        _callbacks.append(\n+            cbks.ProgbarLogger(\n+                count_mode,\n+                stateful_metrics=model.stateful_metric_names))\n+    _callbacks += (callbacks or []) + [model.history]\n+    callbacks = cbks.CallbackList(_callbacks)\n+    out_labels = out_labels or []\n+\n+    # it's possible to callback a different model than itself\n+    # (used by Sequential models)\n+    if hasattr(model, 'callback_model') and model.callback_model:\n+        callback_model = model.callback_model\n+    else:\n+        callback_model = model\n+\n+    callbacks.set_model(callback_model)\n+    callbacks.set_params({\n+        'batch_size': batch_size,\n+        'epochs': epochs,\n+        'steps': steps_per_epoch,\n+        'samples': num_train_samples,\n+        'verbose': verbose,\n+        'do_validation': do_validation,\n+        'metrics': callback_metrics or [],\n+    })\n+    callbacks.on_train_begin()\n+    callback_model.stop_training = False\n+    for cbk in callbacks:\n+        cbk.validation_data = val_ins\n+\n+    # To prevent a slowdown,\n+    # we find beforehand the arrays that need conversion.\n+    feed = (model._feed_inputs +\n+            model._feed_targets +\n+            model._feed_sample_weights)\n+    indices_for_conversion_to_dense = []\n+    for i in range(len(feed)):\n+        if issparse(ins[i]) and not K.is_sparse(feed[i]):\n+            indices_for_conversion_to_dense.append(i)\n+\n+    for epoch in range(initial_epoch, epochs):\n+        # Reset stateful metrics\n+        for m in model.stateful_metric_functions:\n+            m.reset_states()\n+        callbacks.on_epoch_begin(epoch)\n+        epoch_logs = {}\n+        if steps_per_epoch is not None:\n+            for step_index in range(steps_per_epoch):\n+                batch_logs = {}\n+                batch_logs['batch'] = step_index\n+                batch_logs['size'] = 1\n+                callbacks.on_batch_begin(step_index, batch_logs)\n+                outs = f(ins)\n+\n+                if not isinstance(outs, list):\n+                    outs = [outs]\n+                for l, o in zip(out_labels, outs):\n+                    batch_logs[l] = o\n+\n+                callbacks.on_batch_end(step_index, batch_logs)\n+                if callback_model.stop_training:\n+                    break\n+\n+            if do_validation:\n+                val_outs = test_loop(model, val_f, val_ins,\n+                                     batch_size=batch_size,\n+                                     steps=validation_steps,\n+                                     verbose=0)\n+                if not isinstance(val_outs, list):\n+                    val_outs = [val_outs]\n+                # Same labels assumed.\n+                for l, o in zip(out_labels, val_outs):\n+                    epoch_logs['val_' + l] = o\n+        else:\n+            if shuffle == 'batch':\n+                index_array = batch_shuffle(index_array, batch_size)\n+            elif shuffle:\n+                np.random.shuffle(index_array)\n+\n+            batches = make_batches(num_train_samples, batch_size)\n+            for batch_index, (batch_start, batch_end) in enumerate(batches):\n+                batch_ids = index_array[batch_start:batch_end]\n+                try:\n+                    if isinstance(ins[-1], float):\n+                        # Do not slice the training phase flag.\n+                        ins_batch = slice_arrays(\n+                            ins[:-1], batch_ids) + [ins[-1]]\n+                    else:\n+                        ins_batch = slice_arrays(ins, batch_ids)\n+                except TypeError:\n+                    raise TypeError('TypeError while preparing batch. '\n+                                    'If using HDF5 input data, '\n+                                    'pass shuffle=\"batch\".')\n+                batch_logs = {}\n+                batch_logs['batch'] = batch_index\n+                batch_logs['size'] = len(batch_ids)\n+                callbacks.on_batch_begin(batch_index, batch_logs)\n+                for i in indices_for_conversion_to_dense:\n+                    ins_batch[i] = ins_batch[i].toarray()\n+\n+                outs = f(ins_batch)\n+                if not isinstance(outs, list):\n+                    outs = [outs]\n+                for l, o in zip(out_labels, outs):\n+                    batch_logs[l] = o\n+\n+                callbacks.on_batch_end(batch_index, batch_logs)\n+                if callback_model.stop_training:\n+                    break\n+\n+                if batch_index == len(batches) - 1:  # Last batch.\n+                    if do_validation:\n+                        val_outs = test_loop(model, val_f, val_ins,\n+                                             batch_size=batch_size,\n+                                             verbose=0)\n+                        if not isinstance(val_outs, list):\n+                            val_outs = [val_outs]\n+                        # Same labels assumed.\n+                        for l, o in zip(out_labels, val_outs):\n+                            epoch_logs['val_' + l] = o\n+        callbacks.on_epoch_end(epoch, epoch_logs)\n+        if callback_model.stop_training:\n+            break\n+    callbacks.on_train_end()\n+    return model.history\n+\n+\n+def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):\n+    \"\"\"Abstract method to loop over some data in batches.\n+\n+    # Arguments\n+        model: Keras model instance.\n+        f: Keras function returning a list of tensors.\n+        ins: list of tensors to be fed to `f`.\n+        batch_size: integer batch size.\n+        verbose: verbosity mode.\n+        steps: Total number of steps (batches of samples)\n+            before declaring `predict_loop` finished.\n+            Ignored with the default value of `None`.\n+\n+    # Returns\n+        Array of predictions (if the model has a single output)\n+        or list of arrays of predictions\n+        (if the model has multiple outputs).\n+    \"\"\"\n+    num_samples = check_num_samples(ins,\n+                                    batch_size=batch_size,\n+                                    steps=steps,\n+                                    steps_name='steps')\n+    if verbose == 1:\n+        if steps is not None:\n+            progbar = Progbar(target=steps)\n+        else:\n+            progbar = Progbar(target=num_samples)\n+\n+    indices_for_conversion_to_dense = []\n+    for i in range(len(model._feed_inputs)):\n+        if issparse(ins[i]) and not K.is_sparse(model._feed_inputs[i]):\n+            indices_for_conversion_to_dense.append(i)\n+\n+    if steps is not None:\n+        # Step-based predictions.\n+        # Since we do not know how many samples\n+        # we will see, we cannot pre-allocate\n+        # the returned Numpy arrays.\n+        # Instead, we store one array per batch seen\n+        # and concatenate them upon returning.\n+        unconcatenated_outs = []\n+        for step in range(steps):\n+            batch_outs = f(ins)\n+            if not isinstance(batch_outs, list):\n+                batch_outs = [batch_outs]\n+            if step == 0:\n+                for batch_out in batch_outs:\n+                    unconcatenated_outs.append([])\n+            for i, batch_out in enumerate(batch_outs):\n+                unconcatenated_outs[i].append(batch_out)\n+            if verbose == 1:\n+                progbar.update(step + 1)\n+        if len(unconcatenated_outs) == 1:\n+            return np.concatenate(unconcatenated_outs[0], axis=0)\n+        return [np.concatenate(unconcatenated_outs[i], axis=0)\n+                for i in range(len(unconcatenated_outs))]\n+    else:\n+        # Sample-based predictions.\n+        outs = []\n+        batches = make_batches(num_samples, batch_size)\n+        index_array = np.arange(num_samples)\n+        for batch_index, (batch_start, batch_end) in enumerate(batches):\n+            batch_ids = index_array[batch_start:batch_end]\n+            if ins and isinstance(ins[-1], float):\n+                # Do not slice the training phase flag.\n+                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n+            else:\n+                ins_batch = slice_arrays(ins, batch_ids)\n+            for i in indices_for_conversion_to_dense:\n+                ins_batch[i] = ins_batch[i].toarray()\n+\n+            batch_outs = f(ins_batch)\n+            if not isinstance(batch_outs, list):\n+                batch_outs = [batch_outs]\n+            if batch_index == 0:\n+                # Pre-allocate the results arrays.\n+                for batch_out in batch_outs:\n+                    shape = (num_samples,) + batch_out.shape[1:]\n+                    outs.append(np.zeros(shape, dtype=batch_out.dtype))\n+            for i, batch_out in enumerate(batch_outs):\n+                outs[i][batch_start:batch_end] = batch_out\n+            if verbose == 1:\n+                progbar.update(batch_end)\n+        if len(outs) == 1:\n+            return outs[0]\n+        return outs\n+\n+\n+def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):\n+    \"\"\"Abstract method to loop over some data in batches.\n+\n+    # Arguments\n+        model: Keras model instance.\n+        f: Keras function returning a list of tensors.\n+        ins: list of tensors to be fed to `f`.\n+        batch_size: integer batch size or `None`.\n+        verbose: verbosity mode.\n+        steps: Total number of steps (batches of samples)\n+            before declaring predictions finished.\n+            Ignored with the default value of `None`.\n+\n+    # Returns\n+        Scalar loss (if the model has a single output and no metrics)\n+        or list of scalars (if the model has multiple outputs\n+        and/or metrics). The attribute `model.metrics_names` will give you\n+        the display labels for the scalar outputs.\n+    \"\"\"\n+\n+    if hasattr(model, 'metrics'):\n+        for m in model.stateful_metric_functions:\n+            m.reset_states()\n+        stateful_metric_indices = [\n+            i for i, name in enumerate(model.metrics_names)\n+            if str(name) in model.stateful_metric_names]\n+    else:\n+        stateful_metric_indices = []\n+\n+    num_samples = check_num_samples(ins,\n+                                    batch_size=batch_size,\n+                                    steps=steps,\n+                                    steps_name='steps')\n+    outs = []\n+    if verbose == 1:\n+        if steps is not None:\n+            progbar = Progbar(target=steps)\n+        else:\n+            progbar = Progbar(target=num_samples)\n+\n+    # To prevent a slowdown,\n+    # we find beforehand the arrays that need conversion.\n+    feed = (model._feed_inputs +\n+            model._feed_targets +\n+            model._feed_sample_weights)\n+    indices_for_conversion_to_dense = []\n+    for i in range(len(feed)):\n+        if issparse(ins[i]) and not K.is_sparse(feed[i]):\n+            indices_for_conversion_to_dense.append(i)\n+\n+    if steps is not None:\n+        for step in range(steps):\n+            batch_outs = f(ins)\n+            if isinstance(batch_outs, list):\n+                if step == 0:\n+                    for _ in enumerate(batch_outs):\n+                        outs.append(0.)\n+                for i, batch_out in enumerate(batch_outs):\n+                    if i in stateful_metric_indices:\n+                        outs[i] = float(batch_out)\n+                    else:\n+                        outs[i] += batch_out\n+            else:\n+                if step == 0:\n+                    outs.append(0.)\n+                outs[0] += batch_outs\n+            if verbose == 1:\n+                progbar.update(step + 1)\n+        for i in range(len(outs)):\n+            if i not in stateful_metric_indices:\n+                outs[i] /= steps\n+    else:\n+        batches = make_batches(num_samples, batch_size)\n+        index_array = np.arange(num_samples)\n+        for batch_index, (batch_start, batch_end) in enumerate(batches):\n+            batch_ids = index_array[batch_start:batch_end]\n+            if isinstance(ins[-1], float):\n+                # Do not slice the training phase flag.\n+                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n+            else:\n+                ins_batch = slice_arrays(ins, batch_ids)\n+            for i in indices_for_conversion_to_dense:\n+                ins_batch[i] = ins_batch[i].toarray()\n+\n+            batch_outs = f(ins_batch)\n+            if isinstance(batch_outs, list):\n+                if batch_index == 0:\n+                    for batch_out in enumerate(batch_outs):\n+                        outs.append(0.)\n+                for i, batch_out in enumerate(batch_outs):\n+                    if i in stateful_metric_indices:\n+                        outs[i] = batch_out\n+                    else:\n+                        outs[i] += batch_out * len(batch_ids)\n+            else:\n+                if batch_index == 0:\n+                    outs.append(0.)\n+                outs[0] += batch_outs * len(batch_ids)\n+\n+            if verbose == 1:\n+                progbar.update(batch_end)\n+        for i in range(len(outs)):\n+            if i not in stateful_metric_indices:\n+                outs[i] /= num_samples\n+    if len(outs) == 1:\n+        return outs[0]\n+    return outs\n\n@@ -0,0 +1,461 @@\n+\"\"\"Part of the training engine related to Python generators of array data.\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import warnings\n+import numpy as np\n+\n+from .. import backend as K\n+from ..utils.data_utils import Sequence\n+from ..utils.data_utils import GeneratorEnqueuer\n+from ..utils.data_utils import OrderedEnqueuer\n+from ..utils.generic_utils import Progbar\n+from .. import callbacks as cbks\n+\n+\n+def fit_generator(model,\n+                  generator,\n+                  steps_per_epoch=None,\n+                  epochs=1,\n+                  verbose=1,\n+                  callbacks=None,\n+                  validation_data=None,\n+                  validation_steps=None,\n+                  class_weight=None,\n+                  max_queue_size=10,\n+                  workers=1,\n+                  use_multiprocessing=False,\n+                  shuffle=True,\n+                  initial_epoch=0):\n+    \"\"\"See docstring for `Model.fit_generator`.\"\"\"\n+    wait_time = 0.01  # in seconds\n+    epoch = initial_epoch\n+\n+    do_validation = bool(validation_data)\n+    model._make_train_function()\n+    if do_validation:\n+        model._make_test_function()\n+\n+    is_sequence = isinstance(generator, Sequence)\n+    if not is_sequence and use_multiprocessing and workers > 1:\n+        warnings.warn(\n+            UserWarning('Using a generator with `use_multiprocessing=True`'\n+                        ' and multiple workers may duplicate your data.'\n+                        ' Please consider using the`keras.utils.Sequence'\n+                        ' class.'))\n+    if steps_per_epoch is None:\n+        if is_sequence:\n+            steps_per_epoch = len(generator)\n+        else:\n+            raise ValueError('`steps_per_epoch=None` is only valid for a'\n+                             ' generator based on the '\n+                             '`keras.utils.Sequence`'\n+                             ' class. Please specify `steps_per_epoch` '\n+                             'or use the `keras.utils.Sequence` class.')\n+\n+    # python 2 has 'next', 3 has '__next__'\n+    # avoid any explicit version checks\n+    val_gen = (hasattr(validation_data, 'next') or\n+               hasattr(validation_data, '__next__') or\n+               isinstance(validation_data, Sequence))\n+    if (val_gen and not isinstance(validation_data, Sequence) and\n+            not validation_steps):\n+        raise ValueError('`validation_steps=None` is only valid for a'\n+                         ' generator based on the `keras.utils.Sequence`'\n+                         ' class. Please specify `validation_steps` or use'\n+                         ' the `keras.utils.Sequence` class.')\n+\n+    # Prepare display labels.\n+    out_labels = model.metrics_names\n+    callback_metrics = out_labels + ['val_' + n for n in out_labels]\n+\n+    # prepare callbacks\n+    model.history = cbks.History()\n+    _callbacks = [cbks.BaseLogger(\n+        stateful_metrics=model.stateful_metric_names)]\n+    if verbose:\n+        _callbacks.append(\n+            cbks.ProgbarLogger(\n+                count_mode='steps',\n+                stateful_metrics=model.stateful_metric_names))\n+    _callbacks += (callbacks or []) + [model.history]\n+    callbacks = cbks.CallbackList(_callbacks)\n+\n+    # it's possible to callback a different model than self:\n+    if hasattr(model, 'callback_model') and model.callback_model:\n+        callback_model = model.callback_model\n+    else:\n+        callback_model = model\n+    callbacks.set_model(callback_model)\n+    callbacks.set_params({\n+        'epochs': epochs,\n+        'steps': steps_per_epoch,\n+        'verbose': verbose,\n+        'do_validation': do_validation,\n+        'metrics': callback_metrics,\n+    })\n+    callbacks.on_train_begin()\n+\n+    enqueuer = None\n+    val_enqueuer = None\n+\n+    try:\n+        if do_validation and not val_gen:\n+            # Prepare data for validation\n+            if len(validation_data) == 2:\n+                val_x, val_y = validation_data\n+                val_sample_weight = None\n+            elif len(validation_data) == 3:\n+                val_x, val_y, val_sample_weight = validation_data\n+            else:\n+                raise ValueError('`validation_data` should be a tuple '\n+                                 '`(val_x, val_y, val_sample_weight)` '\n+                                 'or `(val_x, val_y)`. Found: ' +\n+                                 str(validation_data))\n+            val_x, val_y, val_sample_weights = model._standardize_user_data(\n+                val_x, val_y, val_sample_weight)\n+            val_data = val_x + val_y + val_sample_weights\n+            if model.uses_learning_phase and not isinstance(K.learning_phase(),\n+                                                            int):\n+                val_data += [0.]\n+            for cbk in callbacks:\n+                cbk.validation_data = val_data\n+\n+        if workers > 0:\n+            if is_sequence:\n+                enqueuer = OrderedEnqueuer(\n+                    generator,\n+                    use_multiprocessing=use_multiprocessing,\n+                    shuffle=shuffle)\n+            else:\n+                enqueuer = GeneratorEnqueuer(\n+                    generator,\n+                    use_multiprocessing=use_multiprocessing,\n+                    wait_time=wait_time)\n+            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n+            output_generator = enqueuer.get()\n+        else:\n+            if is_sequence:\n+                output_generator = iter(generator)\n+            else:\n+                output_generator = generator\n+\n+        callback_model.stop_training = False\n+        # Construct epoch logs.\n+        epoch_logs = {}\n+        while epoch < epochs:\n+            for m in model.stateful_metric_functions:\n+                m.reset_states()\n+            callbacks.on_epoch_begin(epoch)\n+            steps_done = 0\n+            batch_index = 0\n+            while steps_done < steps_per_epoch:\n+                generator_output = next(output_generator)\n+\n+                if not hasattr(generator_output, '__len__'):\n+                    raise ValueError('Output of generator should be '\n+                                     'a tuple `(x, y, sample_weight)` '\n+                                     'or `(x, y)`. Found: ' +\n+                                     str(generator_output))\n+\n+                if len(generator_output) == 2:\n+                    x, y = generator_output\n+                    sample_weight = None\n+                elif len(generator_output) == 3:\n+                    x, y, sample_weight = generator_output\n+                else:\n+                    raise ValueError('Output of generator should be '\n+                                     'a tuple `(x, y, sample_weight)` '\n+                                     'or `(x, y)`. Found: ' +\n+                                     str(generator_output))\n+                # build batch logs\n+                batch_logs = {}\n+                if x is None or len(x) == 0:\n+                    # Handle data tensors support when no input given\n+                    # step-size = 1 for data tensors\n+                    batch_size = 1\n+                elif isinstance(x, list):\n+                    batch_size = x[0].shape[0]\n+                elif isinstance(x, dict):\n+                    batch_size = list(x.values())[0].shape[0]\n+                else:\n+                    batch_size = x.shape[0]\n+                batch_logs['batch'] = batch_index\n+                batch_logs['size'] = batch_size\n+                callbacks.on_batch_begin(batch_index, batch_logs)\n+\n+                outs = model.train_on_batch(x, y,\n+                                            sample_weight=sample_weight,\n+                                            class_weight=class_weight)\n+\n+                if not isinstance(outs, list):\n+                    outs = [outs]\n+                for l, o in zip(out_labels, outs):\n+                    batch_logs[l] = o\n+\n+                callbacks.on_batch_end(batch_index, batch_logs)\n+\n+                batch_index += 1\n+                steps_done += 1\n+\n+                # Epoch finished.\n+                if steps_done >= steps_per_epoch and do_validation:\n+                    if val_gen:\n+                        val_outs = model.evaluate_generator(\n+                            validation_data,\n+                            validation_steps,\n+                            workers=workers,\n+                            use_multiprocessing=use_multiprocessing,\n+                            max_queue_size=max_queue_size)\n+                    else:\n+                        # No need for try/except because\n+                        # data has already been validated.\n+                        val_outs = model.evaluate(\n+                            val_x, val_y,\n+                            batch_size=batch_size,\n+                            sample_weight=val_sample_weights,\n+                            verbose=0)\n+                    if not isinstance(val_outs, list):\n+                        val_outs = [val_outs]\n+                    # Same labels assumed.\n+                    for l, o in zip(out_labels, val_outs):\n+                        epoch_logs['val_' + l] = o\n+\n+                if callback_model.stop_training:\n+                    break\n+\n+            callbacks.on_epoch_end(epoch, epoch_logs)\n+            epoch += 1\n+            if callback_model.stop_training:\n+                break\n+\n+    finally:\n+        try:\n+            if enqueuer is not None:\n+                enqueuer.stop()\n+        finally:\n+            if val_enqueuer is not None:\n+                val_enqueuer.stop()\n+\n+    callbacks.on_train_end()\n+    return model.history\n+\n+\n+def evaluate_generator(model, generator,\n+                       steps=None,\n+                       max_queue_size=10,\n+                       workers=1,\n+                       use_multiprocessing=False,\n+                       verbose=0):\n+    \"\"\"See docstring for `Model.evaluate_generator`.\"\"\"\n+    model._make_test_function()\n+\n+    stateful_metric_indices = []\n+    if hasattr(model, 'metrics'):\n+        for m in model.stateful_metric_functions:\n+            m.reset_states()\n+        stateful_metric_indices = [\n+            i for i, name in enumerate(model.metrics_names)\n+            if str(name) in model.stateful_metric_names]\n+    else:\n+        stateful_metric_indices = []\n+\n+    steps_done = 0\n+    wait_time = 0.01\n+    outs_per_batch = []\n+    batch_sizes = []\n+    is_sequence = isinstance(generator, Sequence)\n+    if not is_sequence and use_multiprocessing and workers > 1:\n+        warnings.warn(\n+            UserWarning('Using a generator with `use_multiprocessing=True`'\n+                        ' and multiple workers may duplicate your data.'\n+                        ' Please consider using the`keras.utils.Sequence'\n+                        ' class.'))\n+    if steps is None:\n+        if is_sequence:\n+            steps = len(generator)\n+        else:\n+            raise ValueError('`steps=None` is only valid for a generator'\n+                             ' based on the `keras.utils.Sequence` class.'\n+                             ' Please specify `steps` or use the'\n+                             ' `keras.utils.Sequence` class.')\n+    enqueuer = None\n+\n+    try:\n+        if workers > 0:\n+            if is_sequence:\n+                enqueuer = OrderedEnqueuer(\n+                    generator,\n+                    use_multiprocessing=use_multiprocessing)\n+            else:\n+                enqueuer = GeneratorEnqueuer(\n+                    generator,\n+                    use_multiprocessing=use_multiprocessing,\n+                    wait_time=wait_time)\n+            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n+            output_generator = enqueuer.get()\n+        else:\n+            if is_sequence:\n+                output_generator = iter(generator)\n+            else:\n+                output_generator = generator\n+\n+        if verbose == 1:\n+            progbar = Progbar(target=steps)\n+\n+        while steps_done < steps:\n+            generator_output = next(output_generator)\n+            if not hasattr(generator_output, '__len__'):\n+                raise ValueError('Output of generator should be a tuple '\n+                                 '(x, y, sample_weight) '\n+                                 'or (x, y). Found: ' +\n+                                 str(generator_output))\n+            if len(generator_output) == 2:\n+                x, y = generator_output\n+                sample_weight = None\n+            elif len(generator_output) == 3:\n+                x, y, sample_weight = generator_output\n+            else:\n+                raise ValueError('Output of generator should be a tuple '\n+                                 '(x, y, sample_weight) '\n+                                 'or (x, y). Found: ' +\n+                                 str(generator_output))\n+            outs = model.test_on_batch(x, y, sample_weight=sample_weight)\n+            if not isinstance(outs, list):\n+                outs = [outs]\n+            outs_per_batch.append(outs)\n+\n+            if x is None or len(x) == 0:\n+                # Handle data tensors support when no input given\n+                # step-size = 1 for data tensors\n+                batch_size = 1\n+            elif isinstance(x, list):\n+                batch_size = x[0].shape[0]\n+            elif isinstance(x, dict):\n+                batch_size = list(x.values())[0].shape[0]\n+            else:\n+                batch_size = x.shape[0]\n+            if batch_size == 0:\n+                raise ValueError('Received an empty batch. '\n+                                 'Batches should contain '\n+                                 'at least one item.')\n+            steps_done += 1\n+            batch_sizes.append(batch_size)\n+            if verbose == 1:\n+                progbar.update(steps_done)\n+\n+    finally:\n+        if enqueuer is not None:\n+            enqueuer.stop()\n+\n+    averages = []\n+    for i in range(len(outs)):\n+        if i not in stateful_metric_indices:\n+            averages.append(np.average([out[i] for out in outs_per_batch],\n+                                       weights=batch_sizes))\n+        else:\n+            averages.append(float(outs_per_batch[-1][i]))\n+    if len(averages) == 1:\n+        return averages[0]\n+    return averages\n+\n+\n+def predict_generator(model, generator,\n+                      steps=None,\n+                      max_queue_size=10,\n+                      workers=1,\n+                      use_multiprocessing=False,\n+                      verbose=0):\n+    \"\"\"See docstring for `Model.predict_generator`.\"\"\"\n+    model._make_predict_function()\n+\n+    steps_done = 0\n+    wait_time = 0.01\n+    all_outs = []\n+    is_sequence = isinstance(generator, Sequence)\n+    if not is_sequence and use_multiprocessing and workers > 1:\n+        warnings.warn(\n+            UserWarning('Using a generator with `use_multiprocessing=True`'\n+                        ' and multiple workers may duplicate your data.'\n+                        ' Please consider using the`keras.utils.Sequence'\n+                        ' class.'))\n+    if steps is None:\n+        if is_sequence:\n+            steps = len(generator)\n+        else:\n+            raise ValueError('`steps=None` is only valid for a generator'\n+                             ' based on the `keras.utils.Sequence` class.'\n+                             ' Please specify `steps` or use the'\n+                             ' `keras.utils.Sequence` class.')\n+    enqueuer = None\n+\n+    try:\n+        if workers > 0:\n+            if is_sequence:\n+                enqueuer = OrderedEnqueuer(\n+                    generator,\n+                    use_multiprocessing=use_multiprocessing)\n+            else:\n+                enqueuer = GeneratorEnqueuer(\n+                    generator,\n+                    use_multiprocessing=use_multiprocessing,\n+                    wait_time=wait_time)\n+            enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n+            output_generator = enqueuer.get()\n+        else:\n+            if is_sequence:\n+                output_generator = iter(generator)\n+            else:\n+                output_generator = generator\n+\n+        if verbose == 1:\n+            progbar = Progbar(target=steps)\n+\n+        while steps_done < steps:\n+            generator_output = next(output_generator)\n+            if isinstance(generator_output, tuple):\n+                # Compatibility with the generators\n+                # used for training.\n+                if len(generator_output) == 2:\n+                    x, _ = generator_output\n+                elif len(generator_output) == 3:\n+                    x, _, _ = generator_output\n+                else:\n+                    raise ValueError('Output of generator should be '\n+                                     'a tuple `(x, y, sample_weight)` '\n+                                     'or `(x, y)`. Found: ' +\n+                                     str(generator_output))\n+            else:\n+                # Assumes a generator that only\n+                # yields inputs (not targets and sample weights).\n+                x = generator_output\n+\n+            outs = model.predict_on_batch(x)\n+            if not isinstance(outs, list):\n+                outs = [outs]\n+\n+            if not all_outs:\n+                for out in outs:\n+                    all_outs.append([])\n+\n+            for i, out in enumerate(outs):\n+                all_outs[i].append(out)\n+            steps_done += 1\n+            if verbose == 1:\n+                progbar.update(steps_done)\n+\n+    finally:\n+        if enqueuer is not None:\n+            enqueuer.stop()\n+\n+    if len(all_outs) == 1:\n+        if steps_done == 1:\n+            return all_outs[0][0]\n+        else:\n+            return np.concatenate(all_outs[0])\n+    if steps_done == 1:\n+        return [out[0] for out in all_outs]\n+    else:\n+        return [np.concatenate(out) for out in all_outs]\n\n@@ -0,0 +1,546 @@\n+\"\"\"Training-related utilities.\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import copy\n+import numpy as np\n+\n+from .. import backend as K\n+from .. import losses\n+\n+\n+def standardize_input_data(data,\n+                           names,\n+                           shapes=None,\n+                           check_batch_axis=True,\n+                           exception_prefix=''):\n+    \"\"\"Normalizes inputs and targets provided by users.\n+\n+    Users may pass data as a list of arrays, dictionary of arrays,\n+    or as a single array. We normalize this to an ordered list of\n+    arrays (same order as `names`), while checking that the provided\n+    arrays have shapes that match the network's expectations.\n+\n+    # Arguments\n+        data: User-provided input data (polymorphic).\n+        names: List of expected array names.\n+        shapes: Optional list of expected array shapes.\n+        check_batch_axis: Boolean; whether to check that\n+            the batch axis of the arrays matches the expected\n+            value found in `shapes`.\n+        exception_prefix: String prefix used for exception formatting.\n+\n+    # Returns\n+        List of standardized input arrays (one array per model input).\n+\n+    # Raises\n+        ValueError: in case of improperly formatted user-provided data.\n+    \"\"\"\n+    if not names:\n+        if data is not None and hasattr(data, '__len__') and len(data):\n+            raise ValueError('Error when checking model ' +\n+                             exception_prefix + ': '\n+                             'expected no data, but got:', data)\n+        return []\n+    if data is None:\n+        return [None for _ in range(len(names))]\n+\n+    if isinstance(data, dict):\n+        try:\n+            data = [data[x].values if data[x].__class__.__name__ == 'DataFrame'\n+                    else data[x] for x in names]\n+        except KeyError as e:\n+            raise ValueError(\n+                'No data provided for \"' + e.args[0] + '\". Need data '\n+                'for each key in: ' + str(names))\n+    elif isinstance(data, list):\n+        if len(names) == 1 and data and isinstance(data[0], (float, int)):\n+            data = [np.asarray(data)]\n+        else:\n+            data = [x.values if x.__class__.__name__ == 'DataFrame'\n+                    else x for x in data]\n+    else:\n+        data = data.values if data.__class__.__name__ == 'DataFrame' else data\n+        data = [data]\n+    data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1\n+            else x for x in data]\n+\n+    if len(data) != len(names):\n+        if data and hasattr(data[0], 'shape'):\n+            raise ValueError(\n+                'Error when checking model ' + exception_prefix +\n+                ': the list of Numpy arrays that you are passing to '\n+                'your model is not the size the model expected. '\n+                'Expected to see ' + str(len(names)) + ' array(s), '\n+                'but instead got the following list of ' +\n+                str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n+        elif len(names) > 1:\n+            raise ValueError(\n+                'Error when checking model ' + exception_prefix +\n+                ': you are passing a list as input to your model, '\n+                'but the model expects a list of ' + str(len(names)) +\n+                ' Numpy arrays instead. The list you passed was: ' +\n+                str(data)[:200])\n+        elif len(data) == 1 and not hasattr(data[0], 'shape'):\n+            raise TypeError(\n+                'Error when checking model ' + exception_prefix +\n+                ': data should be a Numpy array, or list/dict of '\n+                'Numpy arrays. Found: ' + str(data)[:200] + '...')\n+        elif len(names) == 1:\n+            data = [np.asarray(data)]\n+\n+    # Check shapes compatibility.\n+    if shapes:\n+        for i in range(len(names)):\n+            if shapes[i] is not None:\n+                data_shape = data[i].shape\n+                shape = shapes[i]\n+                if data[i].ndim != len(shape):\n+                    raise ValueError(\n+                        'Error when checking ' + exception_prefix +\n+                        ': expected ' + names[i] + ' to have ' +\n+                        str(len(shape)) + ' dimensions, but got array '\n+                        'with shape ' + str(data_shape))\n+                if not check_batch_axis:\n+                    data_shape = data_shape[1:]\n+                    shape = shape[1:]\n+                for dim, ref_dim in zip(data_shape, shape):\n+                    if ref_dim != dim and ref_dim:\n+                        raise ValueError(\n+                            'Error when checking ' + exception_prefix +\n+                            ': expected ' + names[i] + ' to have shape ' +\n+                            str(shape) + ' but got array with shape ' +\n+                            str(data_shape))\n+    return data\n+\n+\n+def standardize_sample_or_class_weights(x_weight,\n+                                        output_names,\n+                                        weight_type):\n+    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n+\n+    # Arguments\n+        x_weight: User-provided `sample_weight` or `class_weight` argument.\n+        output_names: List of output names (strings) in the model.\n+        weight_type: A string used purely for exception printing.\n+\n+    # Returns\n+        A list of `sample_weight` or `class_weight` where there are exactly\n+            one element per model output.\n+\n+    # Raises\n+        ValueError: In case of invalid user-provided argument.\n+    \"\"\"\n+    if x_weight is None or len(x_weight) == 0:\n+        return [None for _ in output_names]\n+    if len(output_names) == 1:\n+        if isinstance(x_weight, list) and len(x_weight) == 1:\n+            return x_weight\n+        if isinstance(x_weight, dict) and output_names[0] in x_weight:\n+            return [x_weight[output_names[0]]]\n+        else:\n+            return [x_weight]\n+    if isinstance(x_weight, list):\n+        if len(x_weight) != len(output_names):\n+            raise ValueError('Provided `' + weight_type + '` was a list of ' +\n+                             str(len(x_weight)) +\n+                             ' elements, but the model has ' +\n+                             str(len(output_names)) + ' outputs. '\n+                             'You should provide one `' + weight_type + '`'\n+                             'array per model output.')\n+        return x_weight\n+    if isinstance(x_weight, dict):\n+        x_weights = []\n+        for name in output_names:\n+            x_weights.append(x_weight.get(name))\n+        return x_weights\n+    else:\n+        raise TypeError('The model has multiple outputs, so `' +\n+                        weight_type + '` '\n+                        'should be either a list or a dict. '\n+                        'Provided `' + weight_type +\n+                        '` type not understood: ' +\n+                        str(x_weight))\n+\n+\n+def standardize_class_weights(class_weight, output_names):\n+    return standardize_sample_or_class_weights(class_weight,\n+                                               output_names,\n+                                               'class_weight')\n+\n+\n+def standardize_sample_weights(sample_weight, output_names):\n+    return standardize_sample_or_class_weights(sample_weight,\n+                                               output_names,\n+                                               'sample_weight')\n+\n+\n+def check_array_length_consistency(inputs, targets, weights=None):\n+    \"\"\"Checks if batch axes are the same for numpy arrays.\n+\n+    # Arguments\n+        inputs: list of Numpy arrays of inputs.\n+        targets: list of Numpy arrays of targets.\n+        weights: list of Numpy arrays of sample weights.\n+\n+    # Raises\n+        ValueError: in case of incorrectly formatted data.\n+    \"\"\"\n+    def set_of_lengths(x):\n+        # return a set with the variation between\n+        # different shapes, with None => 0\n+        if x is None:\n+            return {0}\n+        else:\n+            return set([0 if y is None else y.shape[0] for y in x])\n+\n+    set_x = set_of_lengths(inputs)\n+    set_y = set_of_lengths(targets)\n+    set_w = set_of_lengths(weights)\n+    if len(set_x) > 1:\n+        raise ValueError('All input arrays (x) should have '\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([x.shape for x in inputs]))\n+    if len(set_y) > 1:\n+        raise ValueError('All target arrays (y) should have '\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([y.shape for y in targets]))\n+    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n+        raise ValueError('Input arrays should have '\n+                         'the same number of samples as target arrays. '\n+                         'Found ' + str(list(set_x)[0]) + ' input samples '\n+                         'and ' + str(list(set_y)[0]) + ' target samples.')\n+    if len(set_w) > 1:\n+        raise ValueError('All sample_weight arrays should have '\n+                         'the same number of samples. Got array shapes: ' +\n+                         str([w.shape for w in weights]))\n+    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n+        raise ValueError('Sample_weight arrays should have '\n+                         'the same number of samples as target arrays. Got ' +\n+                         str(list(set_y)[0]) + ' input samples and ' +\n+                         str(list(set_w)[0]) + ' target samples.')\n+\n+\n+def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n+    \"\"\"Does validation on the compatibility of targets and loss functions.\n+\n+    This helps prevent users from using loss functions incorrectly.\n+\n+    # Arguments\n+        targets: list of Numpy arrays of targets.\n+        loss_fns: list of loss functions.\n+        output_shapes: list of shapes of model outputs.\n+\n+    # Raises\n+        ValueError: if a loss function or target array\n+            is incompatible with an output.\n+    \"\"\"\n+    key_losses = {losses.mean_squared_error,\n+                  losses.binary_crossentropy,\n+                  losses.categorical_crossentropy}\n+    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n+        if y is None or loss is None:\n+            continue\n+        if loss is losses.categorical_crossentropy:\n+            if y.shape[-1] == 1:\n+                raise ValueError(\n+                    'You are passing a target array of shape ' + str(y.shape) +\n+                    ' while using as loss `categorical_crossentropy`. '\n+                    '`categorical_crossentropy` expects '\n+                    'targets to be binary matrices (1s and 0s) '\n+                    'of shape (samples, classes). '\n+                    'If your targets are integer classes, '\n+                    'you can convert them to the expected format via:\\n'\n+                    '```\\n'\n+                    'from keras.utils import to_categorical\\n'\n+                    'y_binary = to_categorical(y_int)\\n'\n+                    '```\\n'\n+                    '\\n'\n+                    'Alternatively, you can use the loss function '\n+                    '`sparse_categorical_crossentropy` instead, '\n+                    'which does expect integer targets.')\n+        if loss in key_losses:\n+            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n+                if out_dim is not None and target_dim != out_dim:\n+                    raise ValueError(\n+                        'A target array with shape ' + str(y.shape) +\n+                        ' was passed for an output of shape ' + str(shape) +\n+                        ' while using as loss `' + loss.__name__ + '`. '\n+                        'This loss expects '\n+                        'targets to have the same shape '\n+                        'as the output.')\n+\n+\n+def collect_metrics(metrics, output_names):\n+    \"\"\"Maps metric functions to model outputs.\n+\n+    # Arguments\n+        metrics: a list or dict of metric functions.\n+        output_names: a list of the names (strings) of model outputs.\n+\n+    # Returns\n+        A list (one entry per model output) of lists of metric functions.\n+        For instance, if the model has 2 outputs, and for the first output\n+        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n+        and just \"binary_accuracy\" for the second output,\n+        the list would look like:\n+            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n+\n+    # Raises\n+        TypeError: if an incorrect type is passed for the `metrics` argument.\n+    \"\"\"\n+    if not metrics:\n+        return [[] for _ in output_names]\n+    if isinstance(metrics, list):\n+        # we then apply all metrics to all outputs.\n+        return [copy.copy(metrics) for _ in output_names]\n+    elif isinstance(metrics, dict):\n+        nested_metrics = []\n+        for name in output_names:\n+            output_metrics = metrics.get(name, [])\n+            if not isinstance(output_metrics, list):\n+                output_metrics = [output_metrics]\n+            nested_metrics.append(output_metrics)\n+        return nested_metrics\n+    else:\n+        raise TypeError('Type of `metrics` argument not understood. '\n+                        'Expected a list or dictionary, found: ' +\n+                        str(metrics))\n+\n+\n+def batch_shuffle(index_array, batch_size):\n+    \"\"\"Shuffles an array in a batch-wise fashion.\n+\n+    Useful for shuffling HDF5 arrays\n+    (where one cannot access arbitrary indices).\n+\n+    # Arguments\n+        index_array: array of indices to be shuffled.\n+        batch_size: integer.\n+\n+    # Returns\n+        The `index_array` array, shuffled in a batch-wise fashion.\n+    \"\"\"\n+    batch_count = int(len(index_array) / batch_size)\n+    # to reshape we need to be cleanly divisible by batch size\n+    # we stash extra items and reappend them after shuffling\n+    last_batch = index_array[batch_count * batch_size:]\n+    index_array = index_array[:batch_count * batch_size]\n+    index_array = index_array.reshape((batch_count, batch_size))\n+    np.random.shuffle(index_array)\n+    index_array = index_array.flatten()\n+    return np.append(index_array, last_batch)\n+\n+\n+def make_batches(size, batch_size):\n+    \"\"\"Returns a list of batch indices (tuples of indices).\n+\n+    # Arguments\n+        size: Integer, total size of the data to slice into batches.\n+        batch_size: Integer, batch size.\n+\n+    # Returns\n+        A list of tuples of array indices.\n+    \"\"\"\n+    num_batches = (size + batch_size - 1) // batch_size  # round up\n+    return [(i * batch_size, min(size, (i + 1) * batch_size))\n+            for i in range(num_batches)]\n+\n+\n+def weighted_masked_objective(fn):\n+    \"\"\"Adds support for masking and sample-weighting to an objective function.\n+\n+    It transforms an objective function `fn(y_true, y_pred)`\n+    into a sample-weighted, cost-masked objective function\n+    `fn(y_true, y_pred, weights, mask)`.\n+\n+    # Arguments\n+        fn: The objective function to wrap,\n+            with signature `fn(y_true, y_pred)`.\n+\n+    # Returns\n+        A function with signature `fn(y_true, y_pred, weights, mask)`.\n+    \"\"\"\n+    if fn is None:\n+        return None\n+\n+    def weighted(y_true, y_pred, weights, mask=None):\n+        \"\"\"Wrapper function.\n+\n+        # Arguments\n+            y_true: `y_true` argument of `fn`.\n+            y_pred: `y_pred` argument of `fn`.\n+            weights: Weights tensor.\n+            mask: Mask tensor.\n+\n+        # Returns\n+            Scalar tensor.\n+        \"\"\"\n+        # score_array has ndim >= 2\n+        score_array = fn(y_true, y_pred)\n+        if mask is not None:\n+            # Cast the mask to floatX to avoid float64 upcasting in Theano\n+            mask = K.cast(mask, K.floatx())\n+            # mask should have the same shape as score_array\n+            score_array *= mask\n+            #  the loss per batch should be proportional\n+            #  to the number of unmasked samples.\n+            score_array /= K.mean(mask)\n+\n+        # apply sample weighting\n+        if weights is not None:\n+            # reduce score_array to same ndim as weight array\n+            ndim = K.ndim(score_array)\n+            weight_ndim = K.ndim(weights)\n+            score_array = K.mean(score_array,\n+                                 axis=list(range(weight_ndim, ndim)))\n+            score_array *= weights\n+            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n+        return K.mean(score_array)\n+    return weighted\n+\n+\n+def standardize_weights(y,\n+                        sample_weight=None,\n+                        class_weight=None,\n+                        sample_weight_mode=None):\n+    \"\"\"Performs sample weight validation and standardization.\n+\n+    Everything gets normalized to a single sample-wise (or timestep-wise)\n+    weight array.\n+\n+    # Arguments\n+        y: Numpy array of model targets to be weighted.\n+        sample_weight: User-provided `sample_weight` argument.\n+        class_weight: User-provided `class_weight` argument.\n+        sample_weight_mode: One of `None` or `\"temporal\"`.\n+            `\"temporal\"` indicated that we expect 2D weight data\n+            that will be applied to the last 2 dimensions of\n+            the targets (i.e. we are weighting timesteps, not samples).\n+\n+    # Returns\n+        A numpy array of target weights, one entry per sample to weight.\n+\n+    # Raises\n+        ValueError: In case of invalid user-provided arguments.\n+    \"\"\"\n+    if sample_weight_mode is not None:\n+        if sample_weight_mode != 'temporal':\n+            raise ValueError('\"sample_weight_mode '\n+                             'should be None or \"temporal\". '\n+                             'Found: ' + str(sample_weight_mode))\n+        if len(y.shape) < 3:\n+            raise ValueError('Found a sample_weight array for '\n+                             'an input with shape ' +\n+                             str(y.shape) + '. '\n+                             'Timestep-wise sample weighting (use of '\n+                             'sample_weight_mode=\"temporal\") is restricted to '\n+                             'outputs that are at least 3D, i.e. that have '\n+                             'a time dimension.')\n+        if sample_weight is not None and len(sample_weight.shape) != 2:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) + '. '\n+                             'In order to use timestep-wise sample weighting, '\n+                             'you should pass a 2D sample_weight array.')\n+    else:\n+        if sample_weight is not None and len(sample_weight.shape) != 1:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) + '. '\n+                             'In order to use timestep-wise sample weights, '\n+                             'you should specify '\n+                             'sample_weight_mode=\"temporal\" '\n+                             'in compile(). If you just mean to use '\n+                             'sample-wise weights, make sure your '\n+                             'sample_weight array is 1D.')\n+\n+    if sample_weight is not None:\n+        if len(sample_weight.shape) > len(y.shape):\n+            raise ValueError('Found a sample_weight with shape' +\n+                             str(sample_weight.shape) + '.'\n+                             'Expected sample_weight with rank '\n+                             'less than or equal to ' + str(len(y.shape)))\n+\n+        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n+            raise ValueError('Found a sample_weight array with shape ' +\n+                             str(sample_weight.shape) +\n+                             ' for an input with shape ' +\n+                             str(y.shape) + '. '\n+                             'sample_weight cannot be broadcast.')\n+        return sample_weight\n+    elif isinstance(class_weight, dict):\n+        if len(y.shape) > 2:\n+            raise ValueError('`class_weight` not supported for '\n+                             '3+ dimensional targets.')\n+        if y.shape[1] > 1:\n+            y_classes = np.argmax(y, axis=1)\n+        elif y.shape[1] == 1:\n+            y_classes = np.reshape(y, y.shape[0])\n+        else:\n+            y_classes = y\n+\n+        weights = np.asarray([class_weight[cls] for cls in y_classes\n+                              if cls in class_weight])\n+\n+        if len(weights) != len(y_classes):\n+            # subtract the sets to pick all missing classes\n+            existing_classes = set(y_classes)\n+            existing_class_weight = set(class_weight.keys())\n+            raise ValueError('`class_weight` must contain '\n+                             'all classes in the data.'\n+                             ' The classes %s exist in the data but not in '\n+                             '`class_weight`.'\n+                             % (existing_classes - existing_class_weight))\n+        return weights\n+    else:\n+        if sample_weight_mode is None:\n+            return np.ones((y.shape[0],), dtype=K.floatx())\n+        else:\n+            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n+\n+\n+def check_num_samples(ins,\n+                      batch_size=None,\n+                      steps=None,\n+                      steps_name='steps'):\n+    \"\"\"Checks the number of samples provided for training and evaluation.\n+\n+    The number of samples is not defined when running with `steps`,\n+    in which case the number of samples is set to `None`.\n+\n+    # Arguments\n+        ins: List of tensors to be fed to the Keras function.\n+        batch_size: Integer batch size or `None` if not defined.\n+        steps: Total number of steps (batches of samples)\n+            before declaring `predict_loop` finished.\n+            Ignored with the default value of `None`.\n+        steps_name: The public API's parameter name for `steps`.\n+\n+    # Raises\n+        ValueError: when `steps` is `None` and the attribute `ins.shape`\n+        does not exist. Also raises ValueError when `steps` is not `None`\n+        and `batch_size` is not `None` because they are mutually\n+        exclusive.\n+\n+    # Returns\n+        When steps is `None`, returns the number of samples to be\n+        processed based on the size of the first dimension of the\n+        first input numpy array. When steps is not `None` and\n+        `batch_size` is `None`, returns `None`.\n+\n+    # Raises\n+        ValueError: In case of invalid arguments.\n+    \"\"\"\n+    if steps is not None:\n+        num_samples = None\n+        if batch_size is not None:\n+            raise ValueError('If ' + steps_name +\n+                             ' is set, the `batch_size` must be None.')\n+    elif ins and hasattr(ins[0], 'shape'):\n+        num_samples = ins[0].shape[0]\n+    else:\n+        raise ValueError('Either the input data should have '\n+                         'a defined shape, or ' + steps_name +\n+                         ' should be specified.')\n+    return num_samples\n\n@@ -475,3 +475,44 @@ def is_all_none(iterable_or_element):\n         if element is not None:\n             return False\n     return True\n+\n+\n+def slice_arrays(arrays, start=None, stop=None):\n+    \"\"\"Slices an array or list of arrays.\n+\n+    This takes an array-like, or a list of\n+    array-likes, and outputs:\n+        - arrays[start:stop] if `arrays` is an array-like\n+        - [x[start:stop] for x in arrays] if `arrays` is a list\n+\n+    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n+\n+    # Arguments\n+        arrays: Single array or list of arrays.\n+        start: can be an integer index (start index)\n+            or a list/array of indices\n+        stop: integer (stop index); should be None if\n+            `start` was a list.\n+\n+    # Returns\n+        A slice of the array(s).\n+    \"\"\"\n+    if arrays is None:\n+        return [None]\n+    elif isinstance(arrays, list):\n+        if hasattr(start, '__len__'):\n+            # hdf5 datasets only support list objects as indices\n+            if hasattr(start, 'shape'):\n+                start = start.tolist()\n+            return [None if x is None else x[start] for x in arrays]\n+        else:\n+            return [None if x is None else x[start:stop] for x in arrays]\n+    else:\n+        if hasattr(start, '__len__'):\n+            if hasattr(start, 'shape'):\n+                start = start.tolist()\n+            return arrays[start]\n+        elif hasattr(start, '__getitem__'):\n+            return arrays[start:stop]\n+        else:\n+            return [None]\n\n@@ -10,10 +10,8 @@ from keras import losses\n from keras.layers import Dense, Dropout\n from keras.engine import Input\n from keras.engine.training import Model\n-from keras.engine.training import _check_loss_and_target_compatibility\n-from keras.engine.training import _weighted_masked_objective\n-from keras.engine.training import _check_array_lengths\n-from keras.engine.training import _slice_arrays\n+from keras.engine import training_utils\n+from keras.utils.generic_utils import slice_arrays\n from keras.models import Sequential\n from keras import backend as K\n from keras.utils import Sequence\n@@ -30,54 +28,56 @@ class RandomSequence(Sequence):\n         return self.sequence_length\n \n     def __getitem__(self, idx):\n-        return [np.random.random((self.batch_size, 3)), np.random.random((self.batch_size, 3))], [\n-            np.random.random((self.batch_size, 4)),\n-            np.random.random((self.batch_size, 3))]\n+        return ([np.random.random((self.batch_size, 3)),\n+                 np.random.random((self.batch_size, 3))],\n+                [np.random.random((self.batch_size, 4)),\n+                 np.random.random((self.batch_size, 3))])\n \n     def on_epoch_end(self):\n         pass\n \n \n @keras_test\n-def test_check_array_lengths():\n-    _check_array_lengths(None, None, None)\n+def test_check_array_length_consistency():\n+    training_utils.check_array_length_consistency(None, None, None)\n     a_np = np.random.random((4, 3, 3))\n-    _check_array_lengths(a_np, a_np, a_np)\n-    _check_array_lengths([a_np, a_np], [a_np, a_np], [a_np, a_np])\n-    _check_array_lengths([None], [None], [None])\n+    training_utils.check_array_length_consistency(a_np, a_np, a_np)\n+    training_utils.check_array_length_consistency(\n+        [a_np, a_np], [a_np, a_np], [a_np, a_np])\n+    training_utils.check_array_length_consistency([None], [None], [None])\n \n     b_np = np.random.random((3, 4))\n     with pytest.raises(ValueError):\n-        _check_array_lengths(a_np, None, None)\n+        training_utils.check_array_length_consistency(a_np, None, None)\n     with pytest.raises(ValueError):\n-        _check_array_lengths(a_np, a_np, None)\n+        training_utils.check_array_length_consistency(a_np, a_np, None)\n     with pytest.raises(ValueError):\n-        _check_array_lengths([a_np], [None], None)\n+        training_utils.check_array_length_consistency([a_np], [None], None)\n     with pytest.raises(ValueError):\n-        _check_array_lengths([a_np], [b_np], None)\n+        training_utils.check_array_length_consistency([a_np], [b_np], None)\n     with pytest.raises(ValueError):\n-        _check_array_lengths([a_np], None, [b_np])\n+        training_utils.check_array_length_consistency([a_np], None, [b_np])\n \n \n @keras_test\n-def test_slice_arrays():\n+def testslice_arrays():\n     input_a = np.random.random((10, 3))\n-    _slice_arrays(None)\n-    _slice_arrays(input_a, 0)\n-    _slice_arrays(input_a, 0, 1)\n-    _slice_arrays(input_a, stop=2)\n+    slice_arrays(None)\n+    slice_arrays(input_a, 0)\n+    slice_arrays(input_a, 0, 1)\n+    slice_arrays(input_a, stop=2)\n     input_a = [None, [1, 1], None, [1, 1]]\n-    _slice_arrays(input_a, 0)\n-    _slice_arrays(input_a, 0, 1)\n-    _slice_arrays(input_a, stop=2)\n+    slice_arrays(input_a, 0)\n+    slice_arrays(input_a, 0, 1)\n+    slice_arrays(input_a, stop=2)\n     input_a = [None]\n-    _slice_arrays(input_a, 0)\n-    _slice_arrays(input_a, 0, 1)\n-    _slice_arrays(input_a, stop=2)\n+    slice_arrays(input_a, 0)\n+    slice_arrays(input_a, 0, 1)\n+    slice_arrays(input_a, stop=2)\n     input_a = None\n-    _slice_arrays(input_a, 0)\n-    _slice_arrays(input_a, 0, 1)\n-    _slice_arrays(input_a, stop=2)\n+    slice_arrays(input_a, 0)\n+    slice_arrays(input_a, 0, 1)\n+    slice_arrays(input_a, stop=2)\n \n \n @keras_test\n@@ -88,7 +88,8 @@ def test_weighted_masked_objective():\n     def mask_dummy(y_true=None, y_pred=None, weight=None):\n         return K.placeholder(y_true.shape)\n \n-    weighted_function = _weighted_masked_objective(losses.categorical_crossentropy)\n+    weighted_function = training_utils.weighted_masked_objective(\n+        losses.categorical_crossentropy)\n     weighted_function(a, a, None)\n \n \n@@ -115,7 +116,8 @@ def test_model_methods():\n \n     # training/testing doesn't work before compiling.\n     with pytest.raises(RuntimeError):\n-        model.train_on_batch([input_a_np, input_b_np], [output_a_np, output_b_np])\n+        model.train_on_batch([input_a_np, input_b_np],\n+                             [output_a_np, output_b_np])\n \n     model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                   sample_weight_mode=None)\n@@ -149,11 +151,14 @@ def test_model_methods():\n     out = model.fit([input_a_np, input_b_np],\n                     [output_a_np, output_b_np],\n                     epochs=1, batch_size=4,\n-                    validation_data=([input_a_np, input_b_np], [output_a_np, output_b_np]))\n+                    validation_data=([input_a_np, input_b_np],\n+                                     [output_a_np, output_b_np]))\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     [output_a_np, output_b_np],\n                     epochs=1, batch_size=4, validation_split=0.5,\n-                    validation_data=({'input_a': input_a_np, 'input_b': input_b_np}, [output_a_np, output_b_np]))\n+                    validation_data=({'input_a': input_a_np,\n+                                      'input_b': input_b_np},\n+                                     [output_a_np, output_b_np]))\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     {'dense_1': output_a_np, 'dropout': output_b_np},\n                     epochs=1, batch_size=4, validation_split=0.5,\n@@ -171,7 +176,8 @@ def test_model_methods():\n \n     # predict_on_batch\n     out = model.predict_on_batch([input_a_np, input_b_np])\n-    out = model.predict_on_batch({'input_a': input_a_np, 'input_b': input_b_np})\n+    out = model.predict_on_batch({'input_a': input_a_np,\n+                                  'input_b': input_b_np})\n \n     # predict, evaluate\n     input_a_np = np.random.random((10, 3))\n@@ -180,7 +186,9 @@ def test_model_methods():\n     output_a_np = np.random.random((10, 4))\n     output_b_np = np.random.random((10, 3))\n \n-    out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4)\n+    out = model.evaluate([input_a_np, input_b_np],\n+                         [output_a_np, output_b_np],\n+                         batch_size=4)\n     out = model.predict([input_a_np, input_b_np], batch_size=4)\n \n     # with sample_weight\n@@ -256,8 +264,10 @@ def test_model_methods():\n \n     def gen_data(batch_sz):\n         while True:\n-            yield ([np.random.random((batch_sz, 3)), np.random.random((batch_sz, 3))],\n-                   [np.random.random((batch_sz, 4)), np.random.random((batch_sz, 3))])\n+            yield ([np.random.random((batch_sz, 3)),\n+                    np.random.random((batch_sz, 3))],\n+                   [np.random.random((batch_sz, 4)),\n+                    np.random.random((batch_sz, 3))])\n \n     out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n                               initial_epoch=2, callbacks=[tracker_cb])\n@@ -284,8 +294,12 @@ def test_model_methods():\n     output_a_np = np.random.random((10, 4))\n     output_b_np = np.random.random((10, 3))\n \n-    out = model.fit([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4, epochs=1)\n-    out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4)\n+    out = model.fit([input_a_np, input_b_np],\n+                    [output_a_np, output_b_np],\n+                    batch_size=4, epochs=1)\n+    out = model.evaluate([input_a_np, input_b_np],\n+                         [output_a_np, output_b_np],\n+                         batch_size=4)\n     out = model.predict([input_a_np, input_b_np], batch_size=4)\n \n     # enable verbose for evaluate_generator\n@@ -320,7 +334,8 @@ def test_model_methods():\n     with pytest.raises(ValueError):\n         out = model.train_on_batch([input_a_np, input_b_np],\n                                    [output_a_np, output_b_np],\n-                                   sample_weight=[sample_weight[1], sample_weight[1][:2]])\n+                                   sample_weight=[sample_weight[1],\n+                                                  sample_weight[1][:2]])\n \n     # `sample_weight` is neither a dict nor a list.\n     with pytest.raises(TypeError):\n@@ -353,7 +368,8 @@ def test_model_methods():\n \n     # `sample_weight_mode` does not match output_names.\n     with pytest.raises(ValueError):\n-        model.compile(optimizer, loss='mse', sample_weight_mode={'lstm': 'temporal'})\n+        model.compile(optimizer, loss='mse',\n+                      sample_weight_mode={'lstm': 'temporal'})\n \n     # `sample_weight_mode` does not match output_names.\n     with pytest.raises(ValueError):\n@@ -361,14 +377,16 @@ def test_model_methods():\n \n     # `sample_weight_mode` matches output_names partially.\n     with pytest.raises(ValueError):\n-        model.compile(optimizer, loss='mse', sample_weight_mode={'dense_1': 'temporal'})\n+        model.compile(optimizer, loss='mse',\n+                      sample_weight_mode={'dense_1': 'temporal'})\n \n     # `loss` does not exist.\n     with pytest.raises(ValueError):\n         model.compile(optimizer, loss=[])\n \n     model.compile(optimizer, loss=['mse', 'mae'])\n-    model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2, 'dropout': 0.8})\n+    model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,\n+                                                       'dropout': 0.8})\n     model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])\n \n     # the rank of weight arrays should be 1.\n@@ -377,7 +395,8 @@ def test_model_methods():\n                                    [output_a_np, output_b_np],\n                                    sample_weight=[None, np.random.random((10, 20, 30))])\n \n-    model.compile(optimizer, loss='mse', sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n+    model.compile(optimizer, loss='mse',\n+                  sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n     model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])\n \n     # the rank of output arrays should be at least 3D.\n@@ -390,22 +409,29 @@ def test_model_methods():\n                   sample_weight_mode=None)\n     trained_epochs = []\n     trained_batches = []\n-    out = model.fit_generator(generator=RandomSequence(3), steps_per_epoch=3, epochs=5,\n-                              initial_epoch=0, validation_data=RandomSequence(4),\n-                              validation_steps=3, callbacks=[tracker_cb])\n+    out = model.fit_generator(generator=RandomSequence(3),\n+                              steps_per_epoch=3,\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=RandomSequence(4),\n+                              validation_steps=3,\n+                              callbacks=[tracker_cb])\n     assert trained_epochs == [0, 1, 2, 3, 4]\n     assert trained_batches == list(range(3)) * 5\n \n     # steps_per_epoch will be equal to len of sequence if it's unspecified\n     trained_epochs = []\n     trained_batches = []\n-    out = model.fit_generator(generator=RandomSequence(3), epochs=5,\n-                              initial_epoch=0, validation_data=RandomSequence(4),\n+    out = model.fit_generator(generator=RandomSequence(3),\n+                              epochs=5,\n+                              initial_epoch=0,\n+                              validation_data=RandomSequence(4),\n                               callbacks=[tracker_cb])\n     assert trained_epochs == [0, 1, 2, 3, 4]\n     assert trained_batches == list(range(12)) * 5\n \n-    # fit_generator will throw an exception if steps is unspecified for regular generator\n+    # fit_generator will throw an exception\n+    # if steps is unspecified for regular generator\n     with pytest.raises(ValueError):\n         def gen_data():\n             while True:\n@@ -429,7 +455,8 @@ def test_model_methods():\n                               max_queue_size=2,\n                               workers=2)\n \n-    # Need range check here as filling of the queue depends on sleep in the enqueuers\n+    # Need range check here as filling\n+    # of the queue depends on sleep in the enqueuers\n     assert 6 <= gen_counters[0] <= 8\n     # 12 = (epoch * workers * validation steps * max_queue_size)\n     assert 3 <= gen_counters[1] <= 12\n@@ -442,7 +469,8 @@ def test_model_methods():\n                               workers=2)\n \n     # 12 = (epoch * workers * validation steps * max_queue_size)\n-    # Need range check here as filling of the queue depends on sleep in the enqueuers\n+    # Need range check here as filling\n+    # of the queue depends on sleep in the enqueuers\n     assert 3 <= gen_counters[0] <= 12\n \n     # predict_generator output shape behavior should be consistent\n@@ -453,40 +481,42 @@ def test_model_methods():\n     batch_size = 5\n     sequence_length = 1\n     shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n-    out = model.predict_generator(RandomSequence(batch_size,\n-                                                 sequence_length=sequence_length))\n+    out = model.predict_generator(\n+        RandomSequence(batch_size, sequence_length=sequence_length))\n     assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n \n     # Multiple outputs and multiple steps.\n     batch_size = 5\n     sequence_length = 2\n     shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n-    out = model.predict_generator(RandomSequence(batch_size,\n-                                                 sequence_length=sequence_length))\n+    out = model.predict_generator(\n+        RandomSequence(batch_size, sequence_length=sequence_length))\n     assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n \n     # Create a model with a single output.\n     single_output_model = Model([a, b], a_2)\n-    single_output_model.compile(optimizer, loss, metrics=[], sample_weight_mode=None)\n+    single_output_model.compile(optimizer, loss,\n+                                metrics=[], sample_weight_mode=None)\n \n     # Single output and one step.\n     batch_size = 5\n     sequence_length = 1\n     shape_0, _ = expected_shape(batch_size, sequence_length)\n-    out = single_output_model.predict_generator(RandomSequence(batch_size,\n-                                                sequence_length=sequence_length))\n+    out = single_output_model.predict_generator(\n+        RandomSequence(batch_size, sequence_length=sequence_length))\n     assert np.shape(out) == shape_0\n \n     # Single output and multiple steps.\n     batch_size = 5\n     sequence_length = 2\n     shape_0, _ = expected_shape(batch_size, sequence_length)\n-    out = single_output_model.predict_generator(RandomSequence(batch_size,\n-                                                sequence_length=sequence_length))\n+    out = single_output_model.predict_generator(\n+        RandomSequence(batch_size, sequence_length=sequence_length))\n     assert np.shape(out) == shape_0\n \n \n-@pytest.mark.skipif(sys.version_info < (3,), reason='Cannot catch warnings in python 2')\n+@pytest.mark.skipif(sys.version_info < (3,),\n+                    reason='Cannot catch warnings in python 2')\n @keras_test\n def test_warnings():\n     a = Input(shape=(3,), name='input_a')\n@@ -506,16 +536,24 @@ def test_warnings():\n \n     def gen_data(batch_sz):\n         while True:\n-            yield ([np.random.random((batch_sz, 3)), np.random.random((batch_sz, 3))],\n-                   [np.random.random((batch_sz, 4)), np.random.random((batch_sz, 3))])\n+            yield ([np.random.random((batch_sz, 3)),\n+                    np.random.random((batch_sz, 3))],\n+                   [np.random.random((batch_sz, 4)),\n+                    np.random.random((batch_sz, 3))])\n \n     with pytest.warns(Warning) as w:\n-        out = model.fit_generator(gen_data(4), steps_per_epoch=10, use_multiprocessing=True, workers=2)\n+        out = model.fit_generator(gen_data(4),\n+                                  steps_per_epoch=10,\n+                                  use_multiprocessing=True,\n+                                  workers=2)\n     warning_raised = any(['Sequence' in str(w_.message) for w_ in w])\n     assert warning_raised, 'No warning raised when using generator with processes.'\n \n     with pytest.warns(None) as w:\n-        out = model.fit_generator(RandomSequence(3), steps_per_epoch=4, use_multiprocessing=True, workers=2)\n+        out = model.fit_generator(RandomSequence(3),\n+                                  steps_per_epoch=4,\n+                                  use_multiprocessing=True,\n+                                  workers=2)\n     assert all(['Sequence' not in str(w_.message) for w_ in w]), 'A warning was raised for Sequence.'\n \n \n@@ -530,7 +568,8 @@ def test_sparse_inputs_targets():\n     model = Model([in1, in2], [out1, out2])\n     model.predict(test_inputs, batch_size=2)\n     model.compile('rmsprop', 'mse')\n-    model.fit(test_inputs, test_outputs, epochs=1, batch_size=2, validation_split=0.5)\n+    model.fit(test_inputs, test_outputs,\n+              epochs=1, batch_size=2, validation_split=0.5)\n     model.evaluate(test_inputs, test_outputs, batch_size=2)\n \n \n@@ -546,7 +585,8 @@ def test_sparse_placeholder_fit():\n     model = Model([in1, in2], [out1, out2])\n     model.predict(test_inputs, batch_size=2)\n     model.compile('rmsprop', 'mse')\n-    model.fit(test_inputs, test_outputs, epochs=1, batch_size=2, validation_split=0.5)\n+    model.fit(test_inputs, test_outputs,\n+              epochs=1, batch_size=2, validation_split=0.5)\n     model.evaluate(test_inputs, test_outputs, batch_size=2)\n \n \n@@ -588,15 +628,18 @@ def test_with_list_as_targets():\n @keras_test\n def test_check_not_failing():\n     a = np.random.random((2, 1, 3))\n-    _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [a.shape])\n-    _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [(2, None, 3)])\n+    training_utils.check_loss_and_target_compatibility(\n+        [a], [losses.categorical_crossentropy], [a.shape])\n+    training_utils.check_loss_and_target_compatibility(\n+        [a], [losses.categorical_crossentropy], [(2, None, 3)])\n \n \n @keras_test\n def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n     with pytest.raises(ValueError) as exc:\n-        _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [a.shape])\n+        training_utils.check_loss_and_target_compatibility(\n+            [a], [losses.categorical_crossentropy], [a.shape])\n \n     assert 'You are passing a target array' in str(exc)\n \n@@ -605,7 +648,8 @@ def test_check_last_is_one():\n def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n     with pytest.raises(ValueError) as exc:\n-        _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [(2, 3, 6)])\n+        training_utils.check_loss_and_target_compatibility(\n+            [a], [losses.categorical_crossentropy], [(2, 3, 6)])\n \n     assert 'targets to have the same shape' in str(exc)\n \n\n@@ -13,7 +13,7 @@ from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import model_from_json, model_from_yaml\n from keras import losses\n-from keras.engine.training import _make_batches\n+from keras.engine.training_utils import make_batches\n \n \n input_dim = 16\n@@ -112,7 +112,7 @@ def test_sequential(in_tmpdir):\n     def data_generator(x, y, batch_size=50):\n         index_array = np.arange(len(x))\n         while 1:\n-            batches = _make_batches(len(x_test), batch_size)\n+            batches = make_batches(len(x_test), batch_size)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 x_batch = x[batch_ids]\n\n@@ -2,7 +2,7 @@ import numpy as np\n import pytest\n \n from keras.models import Sequential\n-from keras.engine.training import _weighted_masked_objective\n+from keras.engine.training_utils import weighted_masked_objective\n from keras.layers import TimeDistributed, Masking, Dense\n from keras.utils.test_utils import keras_test\n from keras import losses\n@@ -26,7 +26,7 @@ def test_masking():\n \n @keras_test\n def test_loss_masking():\n-    weighted_loss = _weighted_masked_objective(losses.get('mae'))\n+    weighted_loss = weighted_masked_objective(losses.get('mae'))\n     shape = (3, 4, 2)\n     x = np.arange(24).reshape(shape)\n     y = 2 * x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
