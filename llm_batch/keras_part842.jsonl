{"custom_id": "keras#40021e6b864f8e84c33c33c81b71bb0149cae371", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 242 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import concurrent\n import inspect\n import platform\n import warnings\n@@ -967,15 +968,13 @@ class Trainer:\n \n     def _pythonify_logs(self, logs):\n         result = {}\n+        with concurrent.futures.ThreadPoolExecutor() as executor:\n             for key, value in sorted(logs.items()):\n                 if isinstance(value, dict):\n                     result.update(self._pythonify_logs(value))\n                 else:\n-                try:\n-                    value = float(value)\n-                except:\n-                    pass\n-                result[key] = value\n+                    future_value = executor.submit(_async_float_cast, value)\n+                result[key] = future_value.result()\n         return result\n \n     def _get_metrics_result_or_logs(self, logs):\n@@ -1124,3 +1123,11 @@ def model_supports_jit(model):\n     if all(x.supports_jit for x in model._flatten_layers()):\n         return True\n     return False\n+\n+\n+def _async_float_cast(value):\n+    try:\n+        value = float(value)\n+    except:\n+        pass\n+    return value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#adcf0e19b4c0268c1a9e44386a698f27a2a79282", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 244 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import concurrent\n+import concurrent.futures\n import inspect\n import platform\n import warnings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b8bb2fe3794a9df17fbf5e02d94e31e7128e654", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 246 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -974,6 +974,8 @@ class Trainer:\n                     result.update(self._pythonify_logs(value))\n                 else:\n                     future_value = executor.submit(_async_float_cast, value)\n+                result[key] = future_value\n+        for key, future_value in result.items():\n             result[key] = future_value.result()\n         return result\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8fe0ead724915c63e7fac33859ed8e26955bf25f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 268 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 13 | Complexity Δ (Sum/Max): 30/21 | Churn Δ: 268 | Churn Cumulative: 606 | Contributors (this commit): 10 | Commits (past 90d): 10 | Contributors (cumulative): 14 | DMM Complexity: 0.7717391304347826\n\nDIFF:\n@@ -556,6 +556,154 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         map_saveable_variables(self, store=store, visited_saveables=set())\n         return store\n \n+    def get_state_tree(self):\n+        \"\"\"Retrieves tree-like structure of model variables.\n+\n+        This method allows retrieval of different model variables (trainable,\n+        non-trainable, optimizer, and metrics). The variables are returned in a\n+        nested dictionary format, where the keys correspond to the variable\n+        names and the values are the nested representations of the variables.\n+\n+        Returns:\n+            dict: A dictionary containing the nested representations of the\n+                requested variables. The keys are the variable names, and the\n+                values are the corresponding nested dictionaries.\n+\n+        Example:\n+        ```python\n+        model = Sequential([\n+            Input(shape=(1,), name=\"my_input\"),\n+            layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"),\n+        ], name=\"my_sequential\")\n+        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n+        model.fit(np.array([[1.0]]), np.array([[1.0]]))\n+        state_tree = model.get_state_tree()\n+        ```\n+        `state_tree` is a dictionary like this:\n+        ```\n+        state_tree = {\n+            'trainable_variables': {\n+                'my_sequential': {\n+                    'my_dense': {\n+                        'bias': Array([0.00099997], dtype=float32),\n+                        'kernel': Array([[0.6412131]], dtype=float32),\n+                    }\n+                }\n+            },\n+            'non_trainable_variables': {},\n+            'optimizer_variables': {\n+                'adam': {\n+                    'iteration': Array(1, dtype=int32),\n+                    'learning_rate': Array(0.001, dtype=float32),\n+                }\n+            },\n+            'metrics_variables': {\n+                'loss': {\n+                    'count': Array(1.0, dtype=float32),\n+                    'total': Array(0.11916193, dtype=float32),\n+                },\n+                'mean_absolute_error': {\n+                    'count': Array(1.0, dtype=float32),\n+                    'total': Array(0.3451984, dtype=float32),\n+                },\n+            },\n+        }\n+        ```\n+        \"\"\"\n+        variables = {}\n+        variables[\"trainable_variables\"] = self._create_nested_dict(\n+            self.trainable_variables\n+        )\n+        variables[\"non_trainable_variables\"] = self._create_nested_dict(\n+            self.non_trainable_variables\n+        )\n+        variables[\"optimizer_variables\"] = self._create_nested_dict(\n+            self.optimizer.variables\n+        )\n+        variables[\"metrics_variables\"] = self._create_nested_dict(\n+            self.metrics_variables\n+        )\n+        return variables\n+\n+    def _create_nested_dict(self, variables):\n+        flat_dict = {}\n+        for v in variables:\n+            if v.path in flat_dict:\n+                raise ValueError(\n+                    \"The following variable path is found twice in the model: \"\n+                    f\"'{v.path}'. `get_state_tree()` can only be called when \"\n+                    \"all variable paths are unique. Make sure to give unique \"\n+                    \"names to your layers (and other objects).\"\n+                )\n+            flat_dict[v.path] = v.value\n+\n+        nested_dict = {}\n+        for path, value in flat_dict.items():\n+            parts = path.split(\"/\")\n+            current_dict = nested_dict\n+            for part in parts[:-1]:\n+                if part not in current_dict:\n+                    current_dict[part] = {}\n+                current_dict = current_dict[part]\n+            current_dict[parts[-1]] = value\n+\n+        return nested_dict\n+\n+    def set_state_tree(self, state_tree):\n+        \"\"\"Assigns values to variables of the model.\n+\n+        This method takes a dictionary of nested variable values, which\n+        represents the state tree of the model, and assigns them to the\n+        corresponding variables of the model. The dictionary keys represent the\n+        variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),\n+        and the values are nested dictionaries containing the variable\n+        paths and their corresponding values.\n+\n+        Args:\n+            state_tree: A dictionary representing the state tree of the model.\n+                The keys are the variable names, and the values are nested\n+                dictionaries representing the variable paths and their values.\n+        \"\"\"\n+        for k, v in state_tree.items():\n+            path_value_dict = self._flatten_nested_dict(v)\n+            if k == \"trainable_variables\":\n+                self._assign_variable_values(\n+                    self.trainable_variables, path_value_dict\n+                )\n+            elif k == \"non_trainable_variables\":\n+                self._assign_variable_values(\n+                    self.non_trainable_variables, path_value_dict\n+                )\n+            elif k == \"optimizer_variables\":\n+                self._assign_variable_values(\n+                    self.optimizer.variables, path_value_dict\n+                )\n+            elif k == \"metrics_variables\":\n+                self._assign_variable_values(\n+                    self.metrics_variables, path_value_dict\n+                )\n+            else:\n+                raise ValueError(f\"Unknown variable name: {k}\")\n+\n+    def _assign_variable_values(self, variables, path_value_dict):\n+        for path, value in path_value_dict.items():\n+            for variable in variables:\n+                if variable.path == path:\n+                    variable.assign(value)\n+\n+    def _flatten_nested_dict(self, nested_dict):\n+        flat_dict = {}\n+\n+        def _flatten(current_dict, prefix=\"\"):\n+            for key, value in current_dict.items():\n+                if isinstance(value, dict):\n+                    _flatten(value, prefix + key + \"/\")\n+                else:\n+                    flat_dict[prefix + key] = value\n+\n+        _flatten(nested_dict)\n+        return flat_dict\n+\n \n @keras_export(\"keras.models.model_from_json\")\n def model_from_json(json_string, custom_objects=None):\n\n@@ -93,6 +93,32 @@ def _get_model_with_custom_compute_loss():\n     return model\n \n \n+def _get_model_with_duplicate_variable_path():\n+    class MyModel(Model):\n+        def __init__(self):\n+            super().__init__()\n+            self.dense1 = layers.Dense(4, activation=\"relu\", name=\"layer1\")\n+            self.dense2 = layers.Dense(4, activation=\"relu\", name=\"layer1\")\n+            self.dense3 = layers.Dense(2)\n+\n+        def call(self, x):\n+            x = self.dense1(x)\n+            x = self.dense2(x)\n+            return self.dense3(x)\n+\n+    model = MyModel()\n+    x = np.random.random((1, 16))\n+    model(x)\n+    return model\n+\n+\n+def _get_variable_value_by_path(variables, path):\n+    for v in variables:\n+        if v.path == path:\n+            return v.value\n+    raise ValueError(f\"No variable was find with path = {path}\")\n+\n+\n @pytest.mark.requires_trainable_backend\n class ModelTest(testing.TestCase, parameterized.TestCase):\n     def test_functional_rerouting(self):\n@@ -750,3 +776,97 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         if mode == \"float8\":\n             # kernel + bias + scale * 3 + amax_history * 3 == 8\n             self.assertEqual(len(model.weights), 3 * 8)\n+\n+    def test_get_state_tree(self):\n+        model = _get_model_single_output()\n+        model.compile(loss=\"mse\", optimizer=\"adam\")\n+        state_tree = model.get_state_tree()\n+        self.assertAllClose(\n+            state_tree[\"trainable_variables\"][\"output_a\"][\"kernel\"],\n+            _get_variable_value_by_path(\n+                model.trainable_variables, \"output_a/kernel\"\n+            ),\n+        )\n+        self.assertAllClose(\n+            state_tree[\"trainable_variables\"][\"output_a\"][\"bias\"],\n+            _get_variable_value_by_path(\n+                model.trainable_variables, \"output_a/bias\"\n+            ),\n+        )\n+        self.assertEqual(\n+            state_tree[\"non_trainable_variables\"],\n+            {},\n+        )\n+        self.assertEqual(\n+            state_tree[\"metrics_variables\"][\"loss\"][\"count\"],\n+            _get_variable_value_by_path(model.metrics_variables, \"loss/count\"),\n+        )\n+        self.assertEqual(\n+            state_tree[\"metrics_variables\"][\"loss\"][\"total\"],\n+            _get_variable_value_by_path(model.metrics_variables, \"loss/total\"),\n+        )\n+        self.assertEqual(\n+            state_tree[\"optimizer_variables\"][\"adam\"][\"iteration\"],\n+            _get_variable_value_by_path(\n+                model.optimizer.variables, \"adam/iteration\"\n+            ),\n+        )\n+        self.assertEqual(\n+            state_tree[\"optimizer_variables\"][\"adam\"][\"learning_rate\"],\n+            _get_variable_value_by_path(\n+                model.optimizer.variables, \"adam/learning_rate\"\n+            ),\n+        )\n+\n+    def test_set_state_tree(self):\n+        variables = {\n+            \"optimizer_variables\": {\n+                \"adam\": {\n+                    \"iteration\": 0,\n+                    \"learning_rate\": 0.00001,\n+                }\n+            },\n+            \"trainable_variables\": {\n+                \"output_a\": {\n+                    \"bias\": [0.5],\n+                    \"kernel\": [[0.6], [0.7], [1.8]],\n+                }\n+            },\n+        }\n+\n+        model = _get_model_single_output()\n+        model.compile(optimizer=\"adam\")\n+        model.set_state_tree(variables)\n+\n+        self.assertEqual(\n+            variables[\"optimizer_variables\"][\"adam\"][\"iteration\"],\n+            _get_variable_value_by_path(\n+                model.optimizer.variables, \"adam/iteration\"\n+            ),\n+        )\n+        self.assertEqual(\n+            variables[\"optimizer_variables\"][\"adam\"][\"learning_rate\"],\n+            _get_variable_value_by_path(\n+                model.optimizer.variables, \"adam/learning_rate\"\n+            ),\n+        )\n+        self.assertAllClose(\n+            variables[\"trainable_variables\"][\"output_a\"][\"bias\"],\n+            _get_variable_value_by_path(\n+                model.trainable_variables, \"output_a/bias\"\n+            ),\n+        )\n+        self.assertAllClose(\n+            variables[\"trainable_variables\"][\"output_a\"][\"kernel\"],\n+            _get_variable_value_by_path(\n+                model.trainable_variables, \"output_a/kernel\"\n+            ),\n+        )\n+\n+    def test_get_state_tree_with_duplicate_path(self):\n+        model = _get_model_with_duplicate_variable_path()\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The following variable path is found twice in the model\",\n+        ):\n+            model.get_state_tree()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7b5a5d6ebac694aa629ef20e104671f40999f2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 17 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 78 | Churn Cumulative: 684 | Contributors (this commit): 10 | Commits (past 90d): 12 | Contributors (cumulative): 15 | DMM Complexity: 0.3076923076923077\n\nDIFF:\n@@ -556,7 +556,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         map_saveable_variables(self, store=store, visited_saveables=set())\n         return store\n \n-    def get_state_tree(self):\n+    def get_state_tree(self, value_format=\"backend_tensor\"):\n         \"\"\"Retrieves tree-like structure of model variables.\n \n         This method allows retrieval of different model variables (trainable,\n@@ -568,64 +568,75 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n             dict: A dictionary containing the nested representations of the\n                 requested variables. The keys are the variable names, and the\n                 values are the corresponding nested dictionaries.\n+            value_format: One of `\"backend_tensor\"`, `\"numpy_array\"`.\n+                The kind of array to return as the leaves of the nested\n+                    state tree.\n \n         Example:\n+\n         ```python\n-        model = Sequential([\n-            Input(shape=(1,), name=\"my_input\"),\n-            layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"),\n+        model = keras.Sequential([\n+            keras.Input(shape=(1,), name=\"my_input\"),\n+            keras.layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"),\n         ], name=\"my_sequential\")\n         model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n         model.fit(np.array([[1.0]]), np.array([[1.0]]))\n         state_tree = model.get_state_tree()\n         ```\n-        `state_tree` is a dictionary like this:\n+\n+        The `state_tree` dictionary returned looks like:\n+\n         ```\n-        state_tree = {\n+        {\n+            'metrics_variables': {\n+                'loss': {\n+                    'count': ...,\n+                    'total': ...,\n+                },\n+                'mean_absolute_error': {\n+                    'count': ...,\n+                    'total': ...,\n+                }\n+            },\n             'trainable_variables': {\n                 'my_sequential': {\n                     'my_dense': {\n-                        'bias': Array([0.00099997], dtype=float32),\n-                        'kernel': Array([[0.6412131]], dtype=float32),\n+                        'bias': ...,\n+                        'kernel': ...,\n                     }\n                 }\n             },\n             'non_trainable_variables': {},\n             'optimizer_variables': {\n                 'adam': {\n-                    'iteration': Array(1, dtype=int32),\n-                    'learning_rate': Array(0.001, dtype=float32),\n+                        'iteration': ...,\n+                        'learning_rate': ...,\n+                        'my_sequential_my_dense_bias_momentum': ...,\n+                        'my_sequential_my_dense_bias_velocity': ...,\n+                        'my_sequential_my_dense_kernel_momentum': ...,\n+                        'my_sequential_my_dense_kernel_velocity': ...,\n+                    }\n+                }\n             }\n-            },\n-            'metrics_variables': {\n-                'loss': {\n-                    'count': Array(1.0, dtype=float32),\n-                    'total': Array(0.11916193, dtype=float32),\n-                },\n-                'mean_absolute_error': {\n-                    'count': Array(1.0, dtype=float32),\n-                    'total': Array(0.3451984, dtype=float32),\n-                },\n-            },\n         }\n         ```\n         \"\"\"\n         variables = {}\n         variables[\"trainable_variables\"] = self._create_nested_dict(\n-            self.trainable_variables\n+            self.trainable_variables, value_format\n         )\n         variables[\"non_trainable_variables\"] = self._create_nested_dict(\n-            self.non_trainable_variables\n+            self.non_trainable_variables, value_format\n         )\n         variables[\"optimizer_variables\"] = self._create_nested_dict(\n-            self.optimizer.variables\n+            self.optimizer.variables, value_format\n         )\n         variables[\"metrics_variables\"] = self._create_nested_dict(\n-            self.metrics_variables\n+            self.metrics_variables, value_format\n         )\n         return variables\n \n-    def _create_nested_dict(self, variables):\n+    def _create_nested_dict(self, variables, value_format):\n         flat_dict = {}\n         for v in variables:\n             if v.path in flat_dict:\n@@ -635,7 +646,16 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                     \"all variable paths are unique. Make sure to give unique \"\n                     \"names to your layers (and other objects).\"\n                 )\n+            if value_format == \"backend_tensor\":\n                 flat_dict[v.path] = v.value\n+            elif value_format == \"numpy_array\":\n+                flat_dict[v.path] = v.numpy()\n+            else:\n+                raise ValueError(\n+                    \"Invalid `value_format` argument. Expected one of \"\n+                    \"{'numpy_array', 'backend_tensor'}. Received: \"\n+                    f\"value_format={value_format}\"\n+                )\n \n         nested_dict = {}\n         for path, value in flat_dict.items():\n\n@@ -818,6 +818,12 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n             ),\n         )\n \n+        # Test with numpy\n+        state_tree = model.get_state_tree(value_format=\"numpy_array\")\n+        self.assertIsInstance(\n+            state_tree[\"trainable_variables\"][\"output_a\"][\"kernel\"], np.ndarray\n+        )\n+\n     def test_set_state_tree(self):\n         variables = {\n             \"optimizer_variables\": {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85e36818774c664a478181c5aea0bb1f2e251d25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 28 | Churn Cumulative: 119 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ from keras.src import initializers\n from keras.src import ops\n from keras.src import regularizers\n from keras.src.api_export import keras_export\n-from keras.src.backend import standardize_dtype\n from keras.src.layers.input_spec import InputSpec\n from keras.src.layers.layer import Layer\n \n@@ -244,11 +243,10 @@ class BatchNormalization(Layer):\n                     f\"mask.shape={mask.shape}, inputs.shape={inputs.shape}\"\n                 )\n \n-        input_dtype = standardize_dtype(inputs.dtype)\n-        if input_dtype in (\"float16\", \"bfloat16\"):\n-            # BN is prone to overflowing for float16/bfloat16 inputs, so we opt\n-            # out BN for mixed precision.\n-            inputs = ops.cast(inputs, \"float32\")\n+        compute_dtype = backend.result_type(inputs.dtype, \"float32\")\n+        # BN is prone to overflow with float16/bfloat16 inputs, so we upcast to\n+        # float32 for the subsequent computations.\n+        inputs = ops.cast(inputs, compute_dtype)\n \n         moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n         moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n@@ -286,9 +284,7 @@ class BatchNormalization(Layer):\n             scale=gamma,\n             epsilon=self.epsilon,\n         )\n-        if input_dtype in (\"float16\", \"bfloat16\"):\n-            outputs = ops.cast(outputs, input_dtype)\n-        return outputs\n+        return ops.cast(outputs, self.compute_dtype)\n \n     def get_config(self):\n         base_config = super().get_config()\n\n@@ -1,3 +1,4 @@\n+from keras.src import backend\n from keras.src import constraints\n from keras.src import initializers\n from keras.src import ops\n@@ -179,7 +180,6 @@ class LayerNormalization(Layer):\n         self.built = True\n \n     def call(self, inputs):\n-        inputs = ops.cast(inputs, self.compute_dtype)\n         # Compute the axes along which to reduce the mean / variance\n         input_shape = inputs.shape\n         ndims = len(input_shape)\n@@ -199,11 +199,10 @@ class LayerNormalization(Layer):\n                 return ops.reshape(v, broadcast_shape)\n             return v\n \n-        input_dtype = inputs.dtype\n-        if input_dtype in (\"float16\", \"bfloat16\") and self.dtype == \"float32\":\n-            # If mixed precision is used, cast inputs to float32 so that\n-            # this is at least as numerically stable as the fused version.\n-            inputs = ops.cast(inputs, \"float32\")\n+        compute_dtype = backend.result_type(inputs.dtype, \"float32\")\n+        # LN is prone to overflow with float16/bfloat16 inputs, so we upcast to\n+        # float32 for the subsequent computations.\n+        inputs = ops.cast(inputs, compute_dtype)\n \n         if self.rms_scaling:\n             # Calculate outputs with only variance and gamma if rms scaling\n@@ -231,8 +230,7 @@ class LayerNormalization(Layer):\n                 res = res + beta\n \n             outputs = inputs * inv + res\n-\n-        return ops.cast(outputs, input_dtype)\n+        return ops.cast(outputs, self.compute_dtype)\n \n     def compute_output_shape(self, input_shape):\n         if isinstance(self.axis, int):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2dda2be44726415bec1a3fd9820b4ea2a8ee07c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 354 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -162,11 +162,16 @@ class RandomCrop(BaseImagePreprocessingLayer):\n             or new_height != self.height\n             or new_width != self.width\n         ):\n+            # Resize images if size mismatch or\n+            # if size mismatch cannot be determined\n+            # (in the case of a TF dynamic shape).\n             images = self.backend.image.resize(\n                 images,\n                 size=(self.height, self.width),\n                 data_format=self.data_format,\n             )\n+            # Resize may have upcasted the outputs\n+            images = self.backend.cast(images, self.compute_dtype)\n         return images\n \n     def augment_labels(self, labels, transformation, training=True):\n\n@@ -616,7 +616,11 @@ class TestCase(unittest.TestCase):\n                     tree.flatten(output_data), tree.flatten(output_spec)\n                 ):\n                     dtype = standardize_dtype(tensor.dtype)\n-                    self.assertEqual(dtype, spec.dtype)\n+                    self.assertEqual(\n+                        dtype,\n+                        spec.dtype,\n+                        f\"expected output dtype {spec.dtype}, got {dtype}\",\n+                    )\n                 for weight in layer.weights:\n                     dtype = standardize_dtype(weight.dtype)\n                     if is_float_dtype(dtype):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65e8051114317095ae1a40ed27f0b877749ab9f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 442 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ import shutil\n \n import namex\n \n-package = \"keras\"\n+PACKAGE = \"keras\"\n BUILD_DIR_NAME = \"tmp_build_dir\"\n \n \n@@ -28,7 +28,7 @@ def copy_source_to_build_directory(root_path):\n         shutil.rmtree(build_dir)\n     os.mkdir(build_dir)\n     shutil.copytree(\n-        package, os.path.join(build_dir, package), ignore=ignore_files\n+        PACKAGE, os.path.join(build_dir, PACKAGE), ignore=ignore_files\n     )\n     return build_dir\n \n@@ -163,12 +163,12 @@ def update_package_init(template_fname, dest_fname, api_module):\n def build():\n     # Backup the `keras/__init__.py` and restore it on error in api gen.\n     root_path = os.path.dirname(os.path.abspath(__file__))\n-    code_api_dir = os.path.join(root_path, package, \"api\")\n-    code_init_fname = os.path.join(root_path, package, \"__init__.py\")\n+    code_api_dir = os.path.join(root_path, PACKAGE, \"api\")\n+    code_init_fname = os.path.join(root_path, PACKAGE, \"__init__.py\")\n     # Create temp build dir\n     build_dir = copy_source_to_build_directory(root_path)\n-    build_api_dir = os.path.join(build_dir, package, \"api\")\n-    build_init_fname = os.path.join(build_dir, package, \"__init__.py\")\n+    build_api_dir = os.path.join(build_dir, PACKAGE, \"api\")\n+    build_init_fname = os.path.join(build_dir, PACKAGE, \"__init__.py\")\n     build_api_init_fname = os.path.join(build_api_dir, \"__init__.py\")\n     try:\n         os.chdir(build_dir)\n@@ -184,7 +184,7 @@ def build():\n         # Add __version__ to `api/`.\n         export_version_string(build_api_init_fname)\n         # Creates `_tf_keras` with full keras API\n-        create_legacy_directory(package_dir=os.path.join(build_dir, package))\n+        create_legacy_directory(package_dir=os.path.join(build_dir, PACKAGE))\n         # Update toplevel init with all `api/` imports.\n         api_module = importlib.import_module(f\"{BUILD_DIR_NAME}.keras.api\")\n         update_package_init(code_init_fname, build_init_fname, api_module)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db456b7f7addeb4e4f19cef83e9d22cd53ce773e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 517 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,6 @@\n \"\"\"Converter functions for working with bounding box formats.\"\"\"\n \n from keras.src import ops\n-from keras.src.api_export import keras_export\n from keras.src.utils import tf_utils\n \n \n@@ -190,7 +189,6 @@ FROM_XYXY_CONVERTERS = {\n }\n \n \n-@keras_export(\"keras.utils.bounding_boxes.convert_format\")\n def convert_format(\n     boxes, source, target, images=None, image_shape=None, dtype=\"float32\"\n ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e94e0ba2f30aa04c50288468af2da85738f57e3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 911 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -614,7 +614,7 @@ def extract_patches(\n \n     Args:\n         images: Input image or batch of images. Must be 3D or 4D.\n-        size: Patch size int or tuple (patch_height, patch_widht)\n+        size: Patch size int or tuple (patch_height, patch_width)\n         strides: strides along height and width. If not specified, or\n             if `None`, it defaults to the same value as `size`.\n         dilation_rate: This is the input stride, specifying how far two\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d569e438b02462f2ec6922fa39dbdbeab6ff22e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 350 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,7 @@ RESIZE_INTERPOLATIONS = (\n     \"lanczos3\",\n     \"lanczos5\",\n     \"bicubic\",\n+    \"area\",\n )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20c67d6bc2fc0d6f33ca293b3984485a79ab0587", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 926 | Lines Deleted: 196 | Files Changed: 15 | Hunks: 121 | Methods Changed: 127 | Complexity Δ (Sum/Max): 187/31 | Churn Δ: 1122 | Churn Cumulative: 2032 | Contributors (this commit): 2 | Commits (past 90d): 35 | Contributors (cumulative): 16 | DMM Complexity: 0.6837606837606838\n\nDIFF:\n@@ -82,6 +82,9 @@ from keras.src.layers.preprocessing.category_encoding import CategoryEncoding\n from keras.src.layers.preprocessing.discretization import Discretization\n from keras.src.layers.preprocessing.hashed_crossing import HashedCrossing\n from keras.src.layers.preprocessing.hashing import Hashing\n+from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n+    AutoContrast,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n@@ -107,6 +110,9 @@ from keras.src.layers.preprocessing.image_preprocessing.random_zoom import (\n     RandomZoom,\n )\n from keras.src.layers.preprocessing.image_preprocessing.resizing import Resizing\n+from keras.src.layers.preprocessing.image_preprocessing.solarization import (\n+    Solarization,\n+)\n from keras.src.layers.preprocessing.index_lookup import IndexLookup\n from keras.src.layers.preprocessing.integer_lookup import IntegerLookup\n from keras.src.layers.preprocessing.mel_spectrogram import MelSpectrogram\n\n@@ -0,0 +1,103 @@\n+from keras.src import backend\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.ops.core import _saturate_cast\n+\n+\n+@keras_export(\"keras.layers.AutoContrast\")\n+class AutoContrast(BaseImagePreprocessingLayer):\n+    \"\"\"Performs the auto-contrast operation on an image.\n+\n+    Auto contrast stretches the values of an image across the entire available\n+    `value_range`. This makes differences between pixels more obvious. An\n+    example of this is if an image only has values `[0, 1]` out of the range\n+    `[0, 255]`, auto contrast will change the `1` values to be `255`.\n+\n+    This layer is active at both training and inference time.\n+\n+    Args:\n+        value_range: Range of values the incoming images will have.\n+            Represented as a two number tuple written `(low, high)`.\n+            This is typically either `(0, 1)` or `(0, 255)` depending\n+            on how your preprocessing pipeline is set up.\n+            Defaults to `(0, 255)`.\n+    \"\"\"\n+\n+    _USE_BASE_FACTOR = False\n+    _VALUE_RANGE_VALIDATION_ERROR = (\n+        \"The `value_range` argument should be a list of two numbers. \"\n+    )\n+\n+    def __init__(\n+        self,\n+        value_range=(0, 255),\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self._set_value_range(value_range)\n+\n+    def _set_value_range(self, value_range):\n+        if not isinstance(value_range, (tuple, list)):\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        if len(value_range) != 2:\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        self.value_range = sorted(value_range)\n+\n+    def transform_images(self, images, transformation=None, training=True):\n+        original_images = images\n+        images = self._transform_value_range(\n+            images,\n+            original_range=self.value_range,\n+            target_range=(0, 255),\n+            dtype=self.compute_dtype,\n+        )\n+\n+        images = self.backend.cast(images, self.compute_dtype)\n+        low = self.backend.numpy.min(images, axis=(1, 2), keepdims=True)\n+        high = self.backend.numpy.max(images, axis=(1, 2), keepdims=True)\n+        scale = 255.0 / (high - low)\n+        offset = -low * scale\n+\n+        images = images * scale + offset\n+        results = self.backend.numpy.clip(images, 0.0, 255.0)\n+        results = self._transform_value_range(\n+            results,\n+            original_range=(0, 255),\n+            target_range=self.value_range,\n+            dtype=self.compute_dtype,\n+        )\n+        # don't process NaN channels\n+        results = self.backend.numpy.where(\n+            self.backend.numpy.is_nan(results), original_images, results\n+        )\n+        if results.dtype == images.dtype:\n+            return results\n+        if backend.is_int_dtype(images.dtype):\n+            results = self.backend.numpy.round(results)\n+        return _saturate_cast(results, images.dtype, self.backend)\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        return bounding_boxes\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"value_range\": self.value_range})\n+        return config\n\n@@ -0,0 +1,95 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras.src import layers\n+from keras.src import ops\n+from keras.src import testing\n+\n+\n+class AutoContrastTest(testing.TestCase, parameterized.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.AutoContrast,\n+            init_kwargs={\n+                \"value_range\": (20, 200),\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    def test_constant_channels_dont_get_nanned(self):\n+        img = np.array([1, 1], dtype=\"float32\")\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=0)\n+\n+        layer = layers.AutoContrast(value_range=(0, 255))\n+        ys = layer(img)\n+\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 1.0))\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 1.0))\n+\n+    def test_auto_contrast_expands_value_range(self):\n+        img = np.array([0, 128], dtype=\"float32\")\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=0)\n+\n+        layer = layers.AutoContrast(value_range=(0, 255))\n+        ys = layer(img)\n+\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 0.0))\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 255.0))\n+\n+    def test_auto_contrast_different_values_per_channel(self):\n+        img = np.array(\n+            [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]],\n+            dtype=\"float32\",\n+        )\n+        img = np.expand_dims(img, axis=0)\n+\n+        layer = layers.AutoContrast(value_range=(0, 255))\n+        ys = layer(img)\n+\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0, ..., 0]) == 0.0))\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0, ..., 1]) == 0.0))\n+\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0, ..., 0]) == 255.0))\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0, ..., 1]) == 255.0))\n+\n+        self.assertAllClose(\n+            ys,\n+            [\n+                [\n+                    [[0.0, 0.0, 0.0], [85.0, 85.0, 85.0]],\n+                    [[170.0, 170.0, 170.0], [255.0, 255.0, 255.0]],\n+                ]\n+            ],\n+        )\n+\n+    def test_auto_contrast_expands_value_range_uint8(self):\n+        img = np.array([0, 128], dtype=\"uint8\")\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=0)\n+\n+        layer = layers.AutoContrast(value_range=(0, 255))\n+        ys = layer(img)\n+\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 0.0))\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 255.0))\n+\n+    def test_auto_contrast_properly_converts_value_range(self):\n+        img = np.array([0, 0.5], dtype=\"float32\")\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=-1)\n+        img = np.expand_dims(img, axis=0)\n+\n+        layer = layers.AutoContrast(value_range=(0, 1))\n+        ys = layer(img)\n+\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 0.0))\n+        self.assertTrue(np.any(ops.convert_to_numpy(ys[0]) == 1.0))\n\n@@ -1,4 +1,4 @@\n-from keras.src.backend import config\n+from keras.src.backend import config as backend_config\n from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.validation import (  # noqa: E501\n     densify_bounding_boxes,\n )\n@@ -7,15 +7,23 @@ from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n \n class BaseImagePreprocessingLayer(TFDataLayer):\n \n+    _USE_BASE_FACTOR = True\n     _FACTOR_BOUNDS = (-1, 1)\n \n     def __init__(\n-        self, factor=0.0, bounding_box_format=None, data_format=None, **kwargs\n+        self, factor=None, bounding_box_format=None, data_format=None, **kwargs\n     ):\n         super().__init__(**kwargs)\n         self.bounding_box_format = bounding_box_format\n-        self.data_format = data_format or config.image_data_format()\n+        self.data_format = backend_config.standardize_data_format(data_format)\n+        if self._USE_BASE_FACTOR:\n+            factor = factor or 0.0\n             self._set_factor(factor)\n+        elif factor is not None:\n+            raise ValueError(\n+                f\"Layer {self.__class__.__name__} does not take \"\n+                f\"a `factor` argument. Received: factor={factor}\"\n+            )\n \n     def _set_factor(self, factor):\n         error_msg = (\n@@ -35,7 +43,10 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n                 raise ValueError(error_msg)\n             lower, upper = sorted(factor)\n         elif isinstance(factor, (int, float)):\n-            if factor < 0 or factor > self._FACTOR_BOUNDS[1]:\n+            if (\n+                factor < self._FACTOR_BOUNDS[0]\n+                or factor > self._FACTOR_BOUNDS[1]\n+            ):\n                 raise ValueError(error_msg)\n             factor = abs(factor)\n             lower, upper = [max(-factor, self._FACTOR_BOUNDS[0]), factor]\n@@ -43,55 +54,55 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n             raise ValueError(error_msg)\n         self.factor = lower, upper\n \n-    def get_random_transformation(self, data, seed=None):\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        return None\n+\n+    def transform_images(self, images, transformation, training=True):\n         raise NotImplementedError()\n \n-    def augment_images(self, images, transformation, training=True):\n+    def transform_labels(self, labels, transformation, training=True):\n         raise NotImplementedError()\n \n-    def augment_labels(self, labels, transformation, training=True):\n-        raise NotImplementedError()\n-\n-    def augment_bounding_boxes(\n+    def transform_bounding_boxes(\n         self, bounding_boxes, transformation, training=True\n     ):\n         raise NotImplementedError()\n \n-    def augment_segmentation_masks(\n+    def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n     ):\n         raise NotImplementedError()\n \n-    def augment_single_image(self, image, transformation, training=True):\n+    def transform_single_image(self, image, transformation, training=True):\n         images = self.backend.numpy.expand_dims(image, axis=0)\n-        outputs = self.augment_images(\n+        outputs = self.transform_images(\n             images, transformation=transformation, training=training\n         )\n         return self.backend.numpy.squeeze(outputs, axis=0)\n \n-    def augment_single_label(self, label, transformation, training=True):\n+    def transform_single_label(self, label, transformation, training=True):\n         labels = self.backend.numpy.expand_dims(label, axis=0)\n-        outputs = self.augment_labels(\n+        outputs = self.transform_labels(\n             labels, transformation=transformation, training=training\n         )\n         return self.backend.numpy.squeeze(outputs, axis=0)\n \n-    def augment_single_bounding_box(\n+    def transform_single_bounding_box(\n         self, bounding_box, transformation, training=True\n     ):\n         bounding_boxes = self.backend.numpy.expand_dims(bounding_box, axis=0)\n-        outputs = self.augment_bounding_boxes(\n+        outputs = self.transform_bounding_boxes(\n             bounding_boxes, transformation=transformation, training=training\n         )\n         return self.backend.numpy.squeeze(outputs, axis=0)\n \n-    def augment_single_segmentation_mask(\n+    def transform_single_segmentation_mask(\n         self, segmentation_mask, transformation, training=True\n     ):\n         segmentation_masks = self.backend.numpy.expand_dims(\n             segmentation_mask, axis=0\n         )\n-        outputs = self.augment_segmentation_masks(\n+        outputs = self.transform_segmentation_masks(\n             segmentation_masks, transformation=transformation, training=training\n         )\n         return self.backend.numpy.squeeze(outputs, axis=0)\n@@ -112,13 +123,13 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n         if isinstance(data, dict):\n             is_batched = self._is_batched(data[\"images\"])\n             if is_batched:\n-                data[\"images\"] = self.augment_images(\n+                data[\"images\"] = self.transform_images(\n                     self.backend.convert_to_tensor(data[\"images\"]),\n                     transformation=transformation,\n                     training=training,\n                 )\n             else:\n-                data[\"images\"] = self.augment_single_image(\n+                data[\"images\"] = self.transform_single_image(\n                     self.backend.convert_to_tensor(data[\"images\"]),\n                     transformation=transformation,\n                     training=training,\n@@ -136,26 +147,26 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n                     data[\"bounding_boxes\"], backend=self.backend\n                 )\n                 if is_batched:\n-                    data[\"bounding_boxes\"] = self.augment_bounding_boxes(\n+                    data[\"bounding_boxes\"] = self.transform_bounding_boxes(\n                         bounding_boxes,\n                         transformation=transformation,\n                         training=training,\n                     )\n                 else:\n-                    data[\"bounding_boxes\"] = self.augment_single_bounding_box(\n+                    data[\"bounding_boxes\"] = self.transform_single_bounding_box(\n                         bounding_boxes,\n                         transformation=transformation,\n                         training=training,\n                     )\n             if \"labels\" in data:\n                 if is_batched:\n-                    data[\"labels\"] = self.augment_labels(\n+                    data[\"labels\"] = self.transform_labels(\n                         self.backend.convert_to_tensor(data[\"labels\"]),\n                         transformation=transformation,\n                         training=training,\n                     )\n                 else:\n-                    data[\"labels\"] = self.augment_single_label(\n+                    data[\"labels\"] = self.transform_single_label(\n                         self.backend.convert_to_tensor(data[\"labels\"]),\n                         transformation=transformation,\n                         training=training,\n@@ -163,7 +174,7 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n             if \"segmentation_masks\" in data:\n                 if is_batched:\n                     data[\"segmentation_masks\"] = (\n-                        self.augment_segmentation_masks(\n+                        self.transform_segmentation_masks(\n                             data[\"segmentation_masks\"],\n                             transformation=transformation,\n                             training=training,\n@@ -171,7 +182,7 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n                     )\n                 else:\n                     data[\"segmentation_masks\"] = (\n-                        self.augment_single_segmentation_mask(\n+                        self.transform_single_segmentation_mask(\n                             data[\"segmentation_masks\"],\n                             transformation=transformation,\n                             training=training,\n@@ -181,12 +192,12 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n \n         # `data` is just images.\n         if self._is_batched(data):\n-            return self.augment_images(\n+            return self.transform_images(\n                 self.backend.convert_to_tensor(data),\n                 transformation=transformation,\n                 training=training,\n             )\n-        return self.augment_single_image(\n+        return self.transform_single_image(\n             self.backend.convert_to_tensor(data),\n             transformation=transformation,\n             training=training,\n@@ -201,3 +212,68 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n                 }\n             )\n         return config\n+\n+    def _transform_value_range(\n+        self, images, original_range, target_range, dtype=\"float32\"\n+    ):\n+        \"\"\"Convert input values from `original_range` to `target_range`.\n+\n+        This function is intended to be used in preprocessing layers that\n+        rely upon color values. This allows us to assume internally that\n+        the input tensor is always in the range `(0, 255)`.\n+\n+        Args:\n+            images: the set of images to transform to the target range.\n+            original_range: the value range to transform from.\n+            target_range: the value range to transform to.\n+            dtype: the dtype to compute the conversion with,\n+                defaults to \"float32\".\n+\n+        Returns:\n+            a new Tensor with values in the target range.\n+\n+        Example:\n+\n+        ```python\n+        original_range = [0, 1]\n+        target_range = [0, 255]\n+        images = layer.preprocessing.transform_value_range(\n+            images,\n+            original_range,\n+            target_range\n+        )\n+        images = ops.minimum(images + 10, 255)\n+        images = layer.preprocessing.transform_value_range(\n+            images,\n+            target_range,\n+            original_range\n+        )\n+        ```\n+        \"\"\"\n+        if (\n+            original_range[0] == target_range[0]\n+            and original_range[1] == target_range[1]\n+        ):\n+            return images\n+\n+        images = self.backend.cast(images, dtype=dtype)\n+        original_min_value, original_max_value = self._unwrap_value_range(\n+            original_range, dtype=dtype\n+        )\n+        target_min_value, target_max_value = self._unwrap_value_range(\n+            target_range, dtype=dtype\n+        )\n+\n+        # images in the [0, 1] scale\n+        images = (images - original_min_value) / (\n+            original_max_value - original_min_value\n+        )\n+\n+        scale_factor = target_max_value - target_min_value\n+        return (images * scale_factor) + target_min_value\n+\n+    def _unwrap_value_range(self, value_range, dtype=\"float32\"):\n+        min_value, max_value = value_range\n+        min_value = self.backend.cast(min_value, dtype=dtype)\n+        max_value = self.backend.cast(max_value, dtype=dtype)\n+        return min_value, max_value\n\n@@ -1,11 +1,12 @@\n-from keras.src import backend\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.utils import image_utils\n \n \n @keras_export(\"keras.layers.CenterCrop\")\n-class CenterCrop(TFDataLayer):\n+class CenterCrop(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which crops images.\n \n     This layers crops the central portion of the images to a target size. If an\n@@ -45,14 +46,30 @@ class CenterCrop(TFDataLayer):\n             `\"channels_last\"`.\n     \"\"\"\n \n+    _USE_BASE_FACTOR = False\n+\n     def __init__(self, height, width, data_format=None, **kwargs):\n-        super().__init__(**kwargs)\n+        super().__init__(data_format=data_format, **kwargs)\n         self.height = height\n         self.width = width\n-        self.data_format = backend.standardize_data_format(data_format)\n \n-    def call(self, inputs):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        raise NotImplementedError\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training=training\n+        )\n+\n+    def transform_images(self, images, transformation=None, training=True):\n+        inputs = self.backend.cast(images, self.compute_dtype)\n         if self.data_format == \"channels_first\":\n             init_height = inputs.shape[-2]\n             init_width = inputs.shape[-1]\n@@ -101,7 +118,6 @@ class CenterCrop(TFDataLayer):\n                     w_start : w_start + self.width,\n                     :,\n                 ]\n-\n         return image_utils.smart_resize(\n             inputs,\n             [self.height, self.width],\n\n@@ -119,7 +119,7 @@ class RandomBrightness(BaseImagePreprocessingLayer):\n         rgb_delta = rgb_delta * (self.value_range[1] - self.value_range[0])\n         return {\"rgb_delta\": rgb_delta}\n \n-    def augment_images(self, images, transformation, training=True):\n+    def transform_images(self, images, transformation, training=True):\n         if training:\n             rgb_delta = transformation[\"rgb_delta\"]\n             rgb_delta = self.backend.cast(rgb_delta, images.dtype)\n@@ -129,15 +129,15 @@ class RandomBrightness(BaseImagePreprocessingLayer):\n             )\n         return images\n \n-    def augment_labels(self, labels, transformation, training=True):\n+    def transform_labels(self, labels, transformation, training=True):\n         return labels\n \n-    def augment_bounding_boxes(\n+    def transform_bounding_boxes(\n         self, bounding_boxes, transformation, training=True\n     ):\n         return bounding_boxes\n \n-    def augment_segmentation_masks(\n+    def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n     ):\n         return segmentation_masks\n\n@@ -85,7 +85,7 @@ class RandomContrast(BaseImagePreprocessingLayer):\n         )\n         return {\"contrast_factor\": factor}\n \n-    def augment_images(self, images, transformation, training=True):\n+    def transform_images(self, images, transformation, training=True):\n         if training:\n             constrast_factor = transformation[\"contrast_factor\"]\n             outputs = self._adjust_constrast(images, constrast_factor)\n@@ -94,15 +94,15 @@ class RandomContrast(BaseImagePreprocessingLayer):\n             return outputs\n         return images\n \n-    def augment_labels(self, labels, transformation, training=True):\n+    def transform_labels(self, labels, transformation, training=True):\n         return labels\n \n-    def augment_bounding_boxes(\n+    def transform_bounding_boxes(\n         self, bounding_boxes, transformation, training=True\n     ):\n         return bounding_boxes\n \n-    def augment_segmentation_masks(\n+    def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n     ):\n         return segmentation_masks\n\n@@ -118,7 +118,7 @@ class RandomCrop(BaseImagePreprocessingLayer):\n \n         return h_start, w_start\n \n-    def augment_images(self, images, transformation, training=True):\n+    def transform_images(self, images, transformation, training=True):\n         images = self.backend.cast(images, self.compute_dtype)\n         crop_box_hstart, crop_box_wstart = transformation\n         crop_height = self.height\n@@ -174,10 +174,10 @@ class RandomCrop(BaseImagePreprocessingLayer):\n             images = self.backend.cast(images, self.compute_dtype)\n         return images\n \n-    def augment_labels(self, labels, transformation, training=True):\n+    def transform_labels(self, labels, transformation, training=True):\n         return labels\n \n-    def augment_bounding_boxes(\n+    def transform_bounding_boxes(\n         self, bounding_boxes, transformation, training=True\n     ):\n         \"\"\"\n@@ -223,10 +223,10 @@ class RandomCrop(BaseImagePreprocessingLayer):\n             \"labels\": bounding_boxes[\"labels\"],\n         }\n \n-    def augment_segmentation_masks(\n+    def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n     ):\n-        return self.augment_images(segmentation_masks, transformation)\n+        return self.transform_images(segmentation_masks, transformation)\n \n     def compute_output_shape(self, input_shape, *args, **kwargs):\n         input_shape = list(input_shape)\n\n@@ -1,5 +1,7 @@\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n \n HORIZONTAL = \"horizontal\"\n@@ -8,7 +10,7 @@ HORIZONTAL_AND_VERTICAL = \"horizontal_and_vertical\"\n \n \n @keras_export(\"keras.layers.RandomFlip\")\n-class RandomFlip(TFDataLayer):\n+class RandomFlip(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which randomly flips images during training.\n \n     This layer will flip the images horizontally and or vertically based on the\n@@ -39,40 +41,93 @@ class RandomFlip(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs):\n-        super().__init__(**kwargs)\n+    _USE_BASE_FACTOR = False\n+\n+    def __init__(\n+        self,\n+        mode=HORIZONTAL_AND_VERTICAL,\n+        seed=None,\n+        data_format=None,\n+        **kwargs\n+    ):\n+        super().__init__(data_format=data_format, **kwargs)\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n         self.mode = mode\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n-    def _randomly_flip_inputs(self, inputs):\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+        shape = self.backend.core.shape(images)\n+        if len(shape) == 3:\n+            flips_shape = (1, 1, 1)\n+        else:\n+            flips_shape = (shape[0], 1, 1, 1)\n+\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n+\n+        flips = self.backend.numpy.less_equal(\n+            self.backend.random.uniform(shape=flips_shape, seed=seed), 0.5\n+        )\n+        return {\"flips\": flips}\n+\n+    def transform_images(self, images, transformation, training=True):\n+        images = self.backend.cast(images, self.compute_dtype)\n+        if training:\n+            return self._flip_inputs(images, transformation)\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        raise NotImplementedError\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training=training\n+        )\n+\n+    def _flip_inputs(self, inputs, transformation):\n+        if transformation is None:\n+            return inputs\n+\n+        flips = transformation[\"flips\"]\n         inputs_shape = self.backend.shape(inputs)\n         unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-            inputs_shape = self.backend.shape(inputs)\n \n-        batch_size = inputs_shape[0]\n         flipped_outputs = inputs\n-        seed_generator = self._get_seed_generator(self.backend._backend)\n+        if self.data_format == \"channels_last\":\n+            horizontal_axis = -2\n+            vertical_axis = -3\n+        else:\n+            horizontal_axis = -1\n+            vertical_axis = -2\n+\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n             flipped_outputs = self.backend.numpy.where(\n-                self.backend.random.uniform(\n-                    shape=(batch_size, 1, 1, 1), seed=seed_generator\n-                )\n-                <= 0.5,\n-                self.backend.numpy.flip(flipped_outputs, axis=-2),\n+                flips,\n+                self.backend.numpy.flip(flipped_outputs, axis=horizontal_axis),\n                 flipped_outputs,\n             )\n         if self.mode == VERTICAL or self.mode == HORIZONTAL_AND_VERTICAL:\n             flipped_outputs = self.backend.numpy.where(\n-                self.backend.random.uniform(\n-                    shape=(batch_size, 1, 1, 1), seed=seed_generator\n-                )\n-                <= 0.5,\n-                self.backend.numpy.flip(flipped_outputs, axis=-3),\n+                flips,\n+                self.backend.numpy.flip(flipped_outputs, axis=vertical_axis),\n                 flipped_outputs,\n             )\n         if unbatched:\n@@ -81,17 +136,16 @@ class RandomFlip(TFDataLayer):\n             )\n         return flipped_outputs\n \n-    def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        if training:\n-            return self._randomly_flip_inputs(inputs)\n-        else:\n-            return inputs\n-\n     def compute_output_shape(self, input_shape):\n         return input_shape\n \n     def get_config(self):\n         config = super().get_config()\n-        config.update({\"seed\": self.seed, \"mode\": self.mode})\n+        config.update(\n+            {\n+                \"seed\": self.seed,\n+                \"mode\": self.mode,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n@@ -1,13 +1,14 @@\n import numpy as np\n \n-from keras.src import backend\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n \n \n @keras_export(\"keras.layers.RandomRotation\")\n-class RandomRotation(TFDataLayer):\n+class RandomRotation(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which randomly rotates images during training.\n \n     This layer will apply random rotations to each image, filling empty space\n@@ -67,10 +68,6 @@ class RandomRotation(TFDataLayer):\n             the boundaries when `fill_mode=\"constant\"`.\n     \"\"\"\n \n-    _FACTOR_VALIDATION_ERROR = (\n-        \"The `factor` argument should be a number (or a list of two numbers) \"\n-        \"in the range [-1.0, 1.0]. \"\n-    )\n     _VALUE_RANGE_VALIDATION_ERROR = (\n         \"The `value_range` argument should be a list of two numbers. \"\n     )\n@@ -89,16 +86,13 @@ class RandomRotation(TFDataLayer):\n         data_format=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n+        super().__init__(factor=factor, data_format=data_format, **kwargs)\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n-        self._set_factor(factor)\n         self._set_value_range(value_range)\n-        self.data_format = backend.standardize_data_format(data_format)\n         self.fill_mode = fill_mode\n         self.interpolation = interpolation\n         self.fill_value = fill_value\n-\n         self.supports_jit = False\n \n         if self.fill_mode not in self._SUPPORTED_FILL_MODE:\n@@ -125,29 +119,32 @@ class RandomRotation(TFDataLayer):\n             )\n         self.value_range = sorted(value_range)\n \n-    def _set_factor(self, factor):\n-        if isinstance(factor, (tuple, list)):\n-            if len(factor) != 2:\n-                raise ValueError(\n-                    self._FACTOR_VALIDATION_ERROR + f\"Received: factor={factor}\"\n-                )\n-            self._check_factor_range(factor[0])\n-            self._check_factor_range(factor[1])\n-            self._factor = sorted(factor)\n-        elif isinstance(factor, (int, float)):\n-            self._check_factor_range(factor)\n-            factor = abs(factor)\n-            self._factor = [-factor, factor]\n-        else:\n-            raise ValueError(\n-                self._FACTOR_VALIDATION_ERROR + f\"Received: factor={factor}\"\n+    def transform_images(self, images, transformation, training=True):\n+        images = self.backend.cast(images, self.compute_dtype)\n+        if training:\n+            return self.backend.image.affine_transform(\n+                images=images,\n+                transform=transformation[\"rotation_matrix\"],\n+                interpolation=self.interpolation,\n+                fill_mode=self.fill_mode,\n+                fill_value=self.fill_value,\n+                data_format=self.data_format,\n             )\n+        return images\n \n-    def _check_factor_range(self, input_number):\n-        if input_number > 1.0 or input_number < -1.0:\n-            raise ValueError(\n-                self._FACTOR_VALIDATION_ERROR\n-                + f\"Received: input_number={input_number}\"\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        raise NotImplementedError\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training=training\n         )\n \n     \"\"\"\n@@ -159,8 +156,14 @@ class RandomRotation(TFDataLayer):\n     This function is returning the 8 elements barring the final 1 as a 1D array\n     \"\"\"\n \n-    def _get_rotation_matrix(self, inputs):\n-        shape = self.backend.core.shape(inputs)\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+        shape = self.backend.core.shape(images)\n         if len(shape) == 4:\n             if self.data_format == \"channels_last\":\n                 batch_size = shape[0]\n@@ -179,15 +182,16 @@ class RandomRotation(TFDataLayer):\n                 image_height = shape[1]\n                 image_width = shape[2]\n \n-        lower = self._factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n-        upper = self._factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n+        lower = self.factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n+        upper = self.factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n \n-        seed_generator = self._get_seed_generator(self.backend._backend)\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n         angle = self.backend.random.uniform(\n             shape=(batch_size,),\n             minval=lower,\n             maxval=upper,\n-            seed=seed_generator,\n+            seed=seed,\n         )\n \n         cos_theta = self.backend.numpy.cos(angle)\n@@ -205,7 +209,7 @@ class RandomRotation(TFDataLayer):\n             - (sin_theta * (image_width - 1) + cos_theta * (image_height - 1))\n         ) / 2.0\n \n-        outputs = self.backend.numpy.concatenate(\n+        rotation_matrix = self.backend.numpy.concatenate(\n             [\n                 self.backend.numpy.cos(angle)[:, None],\n                 -self.backend.numpy.sin(angle)[:, None],\n@@ -218,31 +222,17 @@ class RandomRotation(TFDataLayer):\n             axis=1,\n         )\n         if len(shape) == 3:\n-            outputs = self.backend.numpy.squeeze(outputs, axis=0)\n-        return outputs\n-\n-    def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        if training:\n-            rotation_matrix = self._get_rotation_matrix(inputs)\n-            transformed_image = self.backend.image.affine_transform(\n-                images=inputs,\n-                transform=rotation_matrix,\n-                interpolation=self.interpolation,\n-                fill_mode=self.fill_mode,\n-                fill_value=self.fill_value,\n-                data_format=self.data_format,\n+            rotation_matrix = self.backend.numpy.squeeze(\n+                rotation_matrix, axis=0\n             )\n-            return transformed_image\n-        else:\n-            return inputs\n+        return {\"rotation_matrix\": rotation_matrix}\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n \n     def get_config(self):\n         config = {\n-            \"factor\": self._factor,\n+            \"factor\": self.factor,\n             \"value_range\": self.value_range,\n             \"data_format\": self.data_format,\n             \"fill_mode\": self.fill_mode,\n\n@@ -1,11 +1,12 @@\n-from keras.src import backend\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n \n \n @keras_export(\"keras.layers.RandomTranslation\")\n-class RandomTranslation(TFDataLayer):\n+class RandomTranslation(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which randomly translates images during training.\n \n     This layer will apply random translations to each image during training,\n@@ -81,6 +82,7 @@ class RandomTranslation(TFDataLayer):\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n+    _USE_BASE_FACTOR = False\n     _FACTOR_VALIDATION_ERROR = (\n         \"The `factor` argument should be a number (or a list of two numbers) \"\n         \"in the range [-1.0, 1.0]. \"\n@@ -99,7 +101,7 @@ class RandomTranslation(TFDataLayer):\n         data_format=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n+        super().__init__(data_format=data_format, **kwargs)\n         self.height_factor = height_factor\n         self.height_lower, self.height_upper = self._set_factor(\n             height_factor, \"height_factor\"\n@@ -125,7 +127,6 @@ class RandomTranslation(TFDataLayer):\n         self.interpolation = interpolation\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n         self.supports_jit = False\n \n     def _set_factor(self, factor, factor_name):\n@@ -156,41 +157,65 @@ class RandomTranslation(TFDataLayer):\n                 + f\"Received: input_number={input_number}\"\n             )\n \n-    def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n+    def transform_images(self, images, transformation, training=True):\n+        images = self.backend.cast(images, self.compute_dtype)\n         if training:\n-            return self._randomly_translate_inputs(inputs)\n-        else:\n-            return inputs\n+            return self._translate_inputs(images, transformation)\n+        return images\n \n-    def _randomly_translate_inputs(self, inputs):\n-        inputs_shape = self.backend.shape(inputs)\n-        unbatched = len(inputs_shape) == 3\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        raise NotImplementedError\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training=training\n+        )\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+\n+        images_shape = self.backend.shape(images)\n+        unbatched = len(images_shape) == 3\n         if unbatched:\n-            inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n-            inputs_shape = self.backend.shape(inputs)\n-\n-        batch_size = inputs_shape[0]\n-        if self.data_format == \"channels_first\":\n-            height = inputs_shape[-2]\n-            width = inputs_shape[-1]\n+            images_shape = self.backend.shape(images)\n+            batch_size = 1\n         else:\n-            height = inputs_shape[-3]\n-            width = inputs_shape[-2]\n+            batch_size = images_shape[0]\n+        if self.data_format == \"channels_first\":\n+            height = images_shape[-2]\n+            width = images_shape[-1]\n+        else:\n+            height = images_shape[-3]\n+            width = images_shape[-2]\n+\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n \n-        seed_generator = self._get_seed_generator(self.backend._backend)\n         height_translate = self.backend.random.uniform(\n             minval=self.height_lower,\n             maxval=self.height_upper,\n             shape=[batch_size, 1],\n-            seed=seed_generator,\n+            seed=seed,\n         )\n         height_translate = self.backend.numpy.multiply(height_translate, height)\n         width_translate = self.backend.random.uniform(\n             minval=self.width_lower,\n             maxval=self.width_upper,\n             shape=[batch_size, 1],\n-            seed=seed_generator,\n+            seed=seed,\n         )\n         width_translate = self.backend.numpy.multiply(width_translate, width)\n         translations = self.backend.cast(\n@@ -199,7 +224,18 @@ class RandomTranslation(TFDataLayer):\n             ),\n             dtype=\"float32\",\n         )\n+        return {\"translations\": translations}\n \n+    def _translate_inputs(self, inputs, transformation):\n+        if transformation is None:\n+            return inputs\n+\n+        inputs_shape = self.backend.shape(inputs)\n+        unbatched = len(inputs_shape) == 3\n+        if unbatched:\n+            inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+\n+        translations = transformation[\"translations\"]\n         outputs = self.backend.image.affine_transform(\n             inputs,\n             transform=self._get_translation_matrix(translations),\n\n@@ -1,11 +1,13 @@\n from keras.src import backend\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n \n \n @keras_export(\"keras.layers.RandomZoom\")\n-class RandomZoom(TFDataLayer):\n+class RandomZoom(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which randomly zooms images during training.\n \n     This layer will randomly zoom in or out on each axis of an image\n@@ -87,8 +89,10 @@ class RandomZoom(TFDataLayer):\n     >>> out_img = layer(input_img)\n     \"\"\"\n \n+    _USE_BASE_FACTOR = False\n     _FACTOR_VALIDATION_ERROR = (\n-        \"The `factor` argument should be a number (or a list of two numbers) \"\n+        \"The `height_factor` and `width_factor` arguments \"\n+        \"should be a number (or a list of two numbers) \"\n         \"in the range [-1.0, 1.0]. \"\n     )\n     _SUPPORTED_FILL_MODE = (\"reflect\", \"wrap\", \"constant\", \"nearest\")\n@@ -132,7 +136,6 @@ class RandomZoom(TFDataLayer):\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n         self.data_format = backend.standardize_data_format(data_format)\n-\n         self.supports_jit = False\n \n     def _set_factor(self, factor, factor_name):\n@@ -163,21 +166,84 @@ class RandomZoom(TFDataLayer):\n                 + f\"Received: input_number={input_number}\"\n             )\n \n-    def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n+    def transform_images(self, images, transformation, training=True):\n+        images = self.backend.cast(images, self.compute_dtype)\n         if training:\n-            return self._randomly_zoom_inputs(inputs)\n+            return self._zoom_inputs(images, transformation)\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        raise NotImplementedError\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return self.transform_images(\n+            segmentation_masks, transformation, training=training\n+        )\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n         else:\n+            images = data\n+        images_shape = self.backend.shape(images)\n+        if len(images_shape) == 4:\n+            zoom_factor_shape = (images_shape[0], 1)\n+        else:\n+            zoom_factor_shape = (1, 1)\n+\n+        if not training:\n+            return {\n+                \"height_zoom\": self.backend.numpy.zeros(zoom_factor_shape),\n+                \"width_zoom\": self.backend.numpy.zeros(zoom_factor_shape),\n+            }\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n+\n+        height_zoom = self.backend.random.uniform(\n+            minval=1.0 + self.height_lower,\n+            maxval=1.0 + self.height_upper,\n+            shape=zoom_factor_shape,\n+            seed=seed,\n+        )\n+        if self.width_factor is not None:\n+            width_zoom = self.backend.random.uniform(\n+                minval=1.0 + self.width_lower,\n+                maxval=1.0 + self.width_upper,\n+                shape=zoom_factor_shape,\n+                seed=seed,\n+            )\n+        else:\n+            width_zoom = height_zoom\n+        return {\n+            \"height_zoom\": height_zoom,\n+            \"width_zoom\": width_zoom,\n+        }\n+\n+    def _zoom_inputs(self, inputs, transformation):\n+        if transformation is None:\n             return inputs\n \n-    def _randomly_zoom_inputs(self, inputs):\n+        width_zoom = transformation[\"width_zoom\"]\n+        height_zoom = transformation[\"height_zoom\"]\n+        zooms = self.backend.cast(\n+            self.backend.numpy.concatenate([width_zoom, height_zoom], axis=1),\n+            dtype=\"float32\",\n+        )\n+\n         inputs_shape = self.backend.shape(inputs)\n         unbatched = len(inputs_shape) == 3\n         if unbatched:\n             inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n             inputs_shape = self.backend.shape(inputs)\n-\n-        batch_size = inputs_shape[0]\n         if self.data_format == \"channels_first\":\n             height = inputs_shape[-2]\n             width = inputs_shape[-1]\n@@ -185,27 +251,6 @@ class RandomZoom(TFDataLayer):\n             height = inputs_shape[-3]\n             width = inputs_shape[-2]\n \n-        seed_generator = self._get_seed_generator(self.backend._backend)\n-        height_zoom = self.backend.random.uniform(\n-            minval=1.0 + self.height_lower,\n-            maxval=1.0 + self.height_upper,\n-            shape=[batch_size, 1],\n-            seed=seed_generator,\n-        )\n-        if self.width_factor is not None:\n-            width_zoom = self.backend.random.uniform(\n-                minval=1.0 + self.width_lower,\n-                maxval=1.0 + self.width_upper,\n-                shape=[batch_size, 1],\n-                seed=seed_generator,\n-            )\n-        else:\n-            width_zoom = height_zoom\n-        zooms = self.backend.cast(\n-            self.backend.numpy.concatenate([width_zoom, height_zoom], axis=1),\n-            dtype=\"float32\",\n-        )\n-\n         outputs = self.backend.image.affine_transform(\n             inputs,\n             transform=self._get_zoom_matrix(zooms, height, width),\n\n@@ -1,11 +1,13 @@\n from keras.src import backend\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.ops.core import _saturate_cast\n \n \n @keras_export(\"keras.layers.Resizing\")\n-class Resizing(TFDataLayer):\n+class Resizing(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which resizes images.\n \n     This layer resizes an image input to a target height and width. The input\n@@ -60,6 +62,8 @@ class Resizing(TFDataLayer):\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n+    _USE_BASE_FACTOR = False\n+\n     def __init__(\n         self,\n         height,\n@@ -82,10 +86,10 @@ class Resizing(TFDataLayer):\n         self.fill_mode = fill_mode\n         self.fill_value = fill_value\n \n-    def call(self, inputs):\n+    def transform_images(self, images, transformation=None, training=True):\n         size = (self.height, self.width)\n         resized = self.backend.image.resize(\n-            inputs,\n+            images,\n             size=size,\n             interpolation=self.interpolation,\n             data_format=self.data_format,\n@@ -94,11 +98,24 @@ class Resizing(TFDataLayer):\n             fill_mode=self.fill_mode,\n             fill_value=self.fill_value,\n         )\n-        if resized.dtype == inputs.dtype:\n+        if resized.dtype == images.dtype:\n             return resized\n-        if backend.is_int_dtype(inputs.dtype):\n+        if backend.is_int_dtype(images.dtype):\n             resized = self.backend.numpy.round(resized)\n-        return _saturate_cast(resized, inputs.dtype, self.backend)\n+        return _saturate_cast(resized, images.dtype, self.backend)\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation=None, training=True\n+    ):\n+        return self.transform_images(segmentation_masks)\n+\n+    def transform_labels(self, labels, transformation=None, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        raise NotImplementedError\n \n     def compute_output_shape(self, input_shape):\n         input_shape = list(input_shape)\n\n@@ -0,0 +1,208 @@\n+from keras.src import backend\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.ops.core import _saturate_cast\n+from keras.src.random.seed_generator import SeedGenerator\n+\n+\n+@keras_export(\"keras.layers.Solarization\")\n+class Solarization(BaseImagePreprocessingLayer):\n+    \"\"\"Applies `(max_value - pixel + min_value)` for each pixel in the image.\n+\n+    When created without `threshold` parameter, the layer performs solarization\n+    to all values. When created with specified `threshold` the layer only\n+    augments pixels that are above the `threshold` value.\n+\n+    Args:\n+        addition_factor: (Optional)  A tuple of two floats or a single float,\n+            between 0 and 1.\n+            For each augmented image a value is\n+            sampled from the provided range. If a float is passed, the range is\n+            interpreted as `(0, addition_factor)`. If specified, this value\n+            (times the value range of input images, e.g. 255), is\n+            added to each pixel before solarization and thresholding.\n+            Defaults to 0.0.\n+        threshold_factor: (Optional)  A tuple of two floats or a single float.\n+            For each augmented image a value is\n+            sampled from the provided range. If a float is passed, the range is\n+            interpreted as `(0, threshold_factor)`. If specified, only pixel\n+            values above this threshold will be solarized.\n+        value_range: a tuple or a list of two elements. The first value\n+            represents the lower bound for values in input images, the second\n+            represents the upper bound. Images passed to the layer should have\n+            values within `value_range`. Typical values to pass\n+            are `(0, 255)` (RGB image) or `(0., 1.)` (scaled image).\n+        seed: Integer. Used to create a random seed.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+\n+    Example:\n+\n+    ```python\n+    (images, labels), _ = keras.datasets.cifar10.load_data()\n+    print(images[0, 0, 0])\n+    # [59 62 63]\n+    # Note that images are Tensor with values in the range [0, 255]\n+    solarization = Solarization(value_range=(0, 255))\n+    images = solarization(images)\n+    print(images[0, 0, 0])\n+    # [196, 193, 192]\n+    ```\n+    \"\"\"\n+\n+    _USE_BASE_FACTOR = False\n+    _VALUE_RANGE_VALIDATION_ERROR = (\n+        \"The `value_range` argument should be a list of two numbers. \"\n+    )\n+    _FACTOR_VALIDATION_ERROR = (\n+        \"The `addition_factor` and `threshold_factor` arguments \"\n+        \"should be a number (or a list of two numbers) \"\n+        \"in the range [0, 1]. \"\n+    )\n+\n+    def __init__(\n+        self,\n+        addition_factor=0.0,\n+        threshold_factor=0.0,\n+        value_range=(0, 255),\n+        seed=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+        self.addition_factor = self._set_factor(\n+            addition_factor, \"addition_factor\"\n+        )\n+        self.threshold_factor = self._set_factor(\n+            threshold_factor, \"threshold_factor\"\n+        )\n+        self._set_value_range(value_range)\n+\n+    def _set_value_range(self, value_range):\n+        if not isinstance(value_range, (tuple, list)):\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        if len(value_range) != 2:\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        self.value_range = sorted(value_range)\n+\n+    def _set_factor(self, factor, factor_name):\n+        if isinstance(factor, (tuple, list)):\n+            if len(factor) != 2:\n+                raise ValueError(\n+                    self._FACTOR_VALIDATION_ERROR\n+                    + f\"Received: {factor_name}={factor}\"\n+                )\n+            self._check_factor_range(factor[0])\n+            self._check_factor_range(factor[1])\n+            lower, upper = sorted(factor)\n+        elif isinstance(factor, (int, float)):\n+            self._check_factor_range(factor)\n+            lower, upper = [0, factor]\n+        else:\n+            raise ValueError(\n+                self._FACTOR_VALIDATION_ERROR\n+                + f\"Received: {factor_name}={factor}\"\n+            )\n+        return lower, upper\n+\n+    def _check_factor_range(self, input_number):\n+        if input_number > 1.0 or input_number < 0:\n+            raise ValueError(\n+                self._FACTOR_VALIDATION_ERROR\n+                + f\"Received: input_number={input_number}\"\n+            )\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if not training:\n+            return None\n+\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+        images_shape = self.backend.shape(images)\n+        if len(images_shape) == 4:\n+            factor_shape = (images_shape[0], 1, 1, 1)\n+        else:\n+            factor_shape = (1, 1, 1)\n+\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n+\n+        return {\n+            \"additions\": self.backend.random.uniform(\n+                minval=self.addition_factor[0],\n+                maxval=self.addition_factor[1] * 255,\n+                shape=factor_shape,\n+                seed=seed,\n+                dtype=self.compute_dtype,\n+            ),\n+            \"thresholds\": self.backend.random.uniform(\n+                minval=self.threshold_factor[0],\n+                maxval=self.threshold_factor[1] * 255,\n+                shape=factor_shape,\n+                seed=seed,\n+                dtype=self.compute_dtype,\n+            ),\n+        }\n+\n+    def transform_images(self, images, transformation, training=True):\n+        images = self.backend.cast(images, self.compute_dtype)\n+        if transformation is None:\n+            return images\n+\n+        thresholds = transformation[\"thresholds\"]\n+        additions = transformation[\"additions\"]\n+        images = self._transform_value_range(\n+            images,\n+            original_range=self.value_range,\n+            target_range=(0, 255),\n+            dtype=self.compute_dtype,\n+        )\n+        results = images + additions\n+        results = self.backend.numpy.clip(results, 0, 255)\n+        results = self.backend.numpy.where(\n+            results < thresholds, results, 255 - results\n+        )\n+        results = self._transform_value_range(\n+            results,\n+            original_range=(0, 255),\n+            target_range=self.value_range,\n+            dtype=self.compute_dtype,\n+        )\n+        if results.dtype == images.dtype:\n+            return results\n+        if backend.is_int_dtype(images.dtype):\n+            results = self.backend.numpy.round(results)\n+        return _saturate_cast(results, images.dtype, self.backend)\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        return bounding_boxes\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"value_range\": self.value_range,\n+            \"addition_factor\": self.addition_factor,\n+            \"threshold_factor\": self.threshold_factor,\n+            \"seed\": self.seed,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,84 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras.src import layers\n+from keras.src import ops\n+from keras.src import random\n+from keras.src import testing\n+\n+\n+class SolarizationTest(testing.TestCase, parameterized.TestCase):\n+    def _test_input_output(self, layer, input_value, expected_value, dtype):\n+        input = np.ones(shape=(2, 224, 224, 3), dtype=dtype) * input_value\n+        expected_output = ops.clip(\n+            (\n+                np.ones(shape=(2, 224, 224, 3), dtype=layer.compute_dtype)\n+                * expected_value\n+            ),\n+            0,\n+            255,\n+        )\n+        output = layer(input)\n+        self.assertAllClose(output, expected_output)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.Solarization,\n+            init_kwargs={\n+                \"addition_factor\": 0.75,\n+                \"value_range\": (20, 200),\n+                \"threshold_factor\": (0, 1),\n+                \"seed\": 1,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    @parameterized.named_parameters(\n+        (\"0_255\", 0, 255),\n+        (\"64_191\", 64, 191),\n+        (\"127_128\", 127, 128),\n+        (\"191_64\", 191, 64),\n+        (\"255_0\", 255, 0),\n+    )\n+    def test_output_values(self, input_value, expected_value):\n+        solarization = layers.Solarization(value_range=(0, 255))\n+\n+        self._test_input_output(\n+            layer=solarization,\n+            input_value=input_value,\n+            expected_value=expected_value,\n+            dtype=\"uint8\",\n+        )\n+\n+    @parameterized.named_parameters(\n+        (\"0_0\", 0, 0),\n+        (\"191_64\", 191, 64),\n+        (\"255_0\", 255, 0),\n+    )\n+    def test_only_values_above_threshold_are_solarized(\n+        self, input_value, output_value\n+    ):\n+        solarization = layers.Solarization(\n+            threshold_factor=(128.0 / 255.0, 128.0 / 255.0),\n+            value_range=(0, 255),\n+        )\n+\n+        self._test_input_output(\n+            layer=solarization,\n+            input_value=input_value,\n+            expected_value=output_value,\n+            dtype=\"uint8\",\n+        )\n+\n+    def test_random_augmentation_applied_per_sample(self):\n+        image = random.uniform((16, 16, 3), minval=0, maxval=255)\n+        images = ops.stack([image, image])\n+        layer = layers.Solarization(\n+            value_range=(0, 255), threshold_factor=0.5, addition_factor=0.5\n+        )\n+        outputs = layer(images)\n+        self.assertNotAllClose(outputs[0], outputs[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
