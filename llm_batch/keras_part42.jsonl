{"custom_id": "keras#20dc637cd6e71bcbc84bfc788e163260fc832d50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 12 | Churn Cumulative: 684 | Contributors (this commit): 12 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -316,3 +316,15 @@ class Graph(Layer):\n \n     def count_params(self):\n         return sum([layer.count_params() for layer in self.nodes.values()])\n+\n+    def get_weights(self):\n+        weights = []\n+        for layer in self.nodes.values():\n+            weights += layer.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        for layer in self.nodes.values():\n+            nb_param = len(layer.get_weights())\n+            layer.set_weights(weights[:nb_param])\n+            weights = weights[nb_param:]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b65c665d2a5d55a8aed4a56cc4dfa81ef3624c3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 42/42 | Churn Δ: 8 | Churn Cumulative: 2842 | Contributors (this commit): 29 | Commits (past 90d): 75 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1012,10 +1012,10 @@ class Lambda(Layer):\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(self.previous.output_shape[1:])\n+            shape = output_shape_func(self.previous.output_shape)\n             if type(shape) not in {list, tuple}:\n                 raise Exception(\"output_shape function must return a tuple\")\n-            return (self.input_shape[0], ) + tuple(shape)\n+            return tuple(shape)\n \n     def get_output(self, train=False):\n         func = marshal.loads(self.function)\n@@ -1089,10 +1089,10 @@ class LambdaMerge(Lambda):\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func([shape[1:] for shape in input_shapes])\n+            shape = output_shape_func(input_shapes)\n             if type(shape) not in {list, tuple}:\n                 raise Exception(\"output_shape function must return a tuple\")\n-            return (input_shapes[0][0], ) + tuple(shape)\n+            return tuple(shape)\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1bd779463eea46292cd20526a99747fcd94a840", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 28 | Churn Cumulative: 1309 | Contributors (this commit): 10 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -112,7 +112,9 @@ class SimpleRNN(Recurrent):\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n \n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n@@ -210,7 +212,9 @@ class SimpleDeepRNN(Recurrent):\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n \n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n@@ -327,7 +331,9 @@ class GRU(Recurrent):\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n \n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n@@ -460,7 +466,9 @@ class LSTM(Recurrent):\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n \n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n@@ -581,7 +589,9 @@ class JZS1(Recurrent):\n             non_sequences=[self.U_r, self.U_h],\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n@@ -703,7 +713,9 @@ class JZS2(Recurrent):\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n \n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n@@ -817,7 +829,9 @@ class JZS3(Recurrent):\n             truncate_gradient=self.truncate_gradient,\n             go_backwards=self.go_backwards)\n         \n-        if self.return_sequences:\n+        if self.return_sequences and self.go_backwards:\n+            return outputs[::-1].dimshuffle((1, 0, 2))\n+        elif self.return_sequences:\n             return outputs.dimshuffle((1, 0, 2))\n         return outputs[-1]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24b5e80667c8998d7e5e9689085fecc92a9506d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 14 | Churn Cumulative: 828 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,20 +62,28 @@ def transpose(x):\n # ELEMENT-WISE OPERATIONS\n \n def max(x, axis=None, keepdims=False):\n+    if axis is not None:\n+        axis = axis % len(x.get_shape())\n     return tf.reduce_max(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def min(x, axis=None, keepdims=False):\n+    if axis is not None:\n+        axis = axis % len(x.get_shape())\n     return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n     '''Sum of the values in a tensor, alongside the specified axis.\n     '''\n+    if axis is not None:\n+        axis = axis % len(x.get_shape())\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def mean(x, axis=None, keepdims=False):\n+    if axis is not None:\n+        axis = axis % len(x.get_shape())\n     return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n@@ -84,6 +92,8 @@ def any(x, axis=None, keepdims=False):\n \n     Return array of int8 (0s and 1s).\n     '''\n+    if axis is not None:\n+        axis = axis % len(x.get_shape())\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\n     return tf.cast(x, tf.int8)\n@@ -108,7 +118,7 @@ def abs(x):\n \n \n def sqrt(x):\n-    x = tf.clip_by_value(x, _EPSILON, np.inf)\n+    x = tf.clip_by_value(x, 0., np.inf)\n     return tf.sqrt(x)\n \n \n\n@@ -138,7 +138,7 @@ def abs(x):\n \n \n def sqrt(x):\n-    x = T.clip(x, _EPSILON, np.inf)\n+    x = T.clip(x, 0., np.inf)\n     return T.sqrt(x)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#368df8ef04dfc289e3c6d5a177336f97bb233ae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 440 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from __future__ import absolute_import\n-import theano.tensor as T\n+from . import backend as K\n \n \n class Regularizer(object):\n@@ -25,8 +25,8 @@ class WeightRegularizer(Regularizer):\n         self.p = p\n \n     def __call__(self, loss):\n-        loss += T.sum(abs(self.p)) * self.l1\n-        loss += T.sum(self.p ** 2) * self.l2\n+        loss += K.sum(K.abs(self.p)) * self.l1\n+        loss += K.sum(self.p ** 2) * self.l2\n         return loss\n \n     def get_config(self):\n@@ -44,8 +44,9 @@ class ActivityRegularizer(Regularizer):\n         self.layer = layer\n \n     def __call__(self, loss):\n-        loss += self.l1 * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n-        loss += self.l2 * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n+        output = self.layer.get_output(True)\n+        loss += self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n+        loss += self.l2 * K.sum(K.mean(output ** 2, axis=0))\n         return loss\n \n     def get_config(self):\n@@ -81,4 +82,5 @@ identity = Regularizer\n \n from .utils.generic_utils import get_from_module\n def get(identifier, kwargs=None):\n-    return get_from_module(identifier, globals(), 'regularizer', instantiate=True, kwargs=kwargs)\n+    return get_from_module(identifier, globals(), 'regularizer',\n+                           instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33ed943ad59fc99f469c0d9d52bfd8462542bf74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 53 | Files Changed: 1 | Hunks: 34 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 781 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,20 +1,18 @@\n from __future__ import absolute_import\n-import theano\n-import theano.tensor as T\n-\n-from .utils.theano_utils import shared_zeros, shared_scalar, floatX\n+from . import backend as K\n+import numpy as np\n from .utils.generic_utils import get_from_module\n from six.moves import zip\n \n \n def clip_norm(g, c, n):\n     if c > 0:\n-        g = T.switch(T.ge(n, c), g * c / n, g)\n+        g = K.switch(n >= c, g * c / n, g)\n     return g\n \n \n def kl_divergence(p, p_hat):\n-    return p_hat - p + p * T.log(p / p_hat)\n+    return p_hat - p + p * K.log(p / p_hat)\n \n \n class Optimizer(object):\n@@ -23,27 +21,23 @@ class Optimizer(object):\n         self.updates = []\n \n     def get_state(self):\n-        return [u[0].get_value() for u in self.updates]\n+        return [K.get_value(u[0]) for u in self.updates]\n \n     def set_state(self, value_list):\n         assert len(self.updates) == len(value_list)\n         for u, v in zip(self.updates, value_list):\n-            u[0].set_value(floatX(v))\n+            K.set_value(u[0], v)\n \n     def get_updates(self, params, constraints, loss):\n         raise NotImplementedError\n \n     def get_gradients(self, loss, params):\n-\n-        grads = T.grad(loss, params)\n-\n+        grads = K.gradients(loss, params)\n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n-            norm = T.sqrt(sum([T.sum(g ** 2) for g in grads]))\n+            norm = K.sqrt(sum([K.sum(g ** 2) for g in grads]))\n             grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n-\n         if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n-            grads = [T.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n-\n+            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n         return grads\n \n     def get_config(self):\n@@ -52,13 +46,14 @@ class Optimizer(object):\n \n class SGD(Optimizer):\n \n-    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n+    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False,\n+                 *args, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n-        self.iterations = shared_scalar(0)\n-        self.lr = shared_scalar(lr)\n-        self.momentum = shared_scalar(momentum)\n-        self.decay = shared_scalar(decay)\n+        self.iterations = K.variable(0.)\n+        self.lr = K.variable(lr)\n+        self.momentum = K.variable(momentum)\n+        self.decay = K.variable(decay)\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n@@ -66,7 +61,7 @@ class SGD(Optimizer):\n         self.updates = [(self.iterations, self.iterations + 1.)]\n \n         for p, g, c in zip(params, grads, constraints):\n-            m = shared_zeros(p.get_value().shape)  # momentum\n+            m = K.variable(np.zeros(K.get_value(p).shape))  # momentum\n             v = self.momentum * m - lr * g  # velocity\n             self.updates.append((m, v))\n \n@@ -80,9 +75,9 @@ class SGD(Optimizer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"lr\": float(self.lr.get_value()),\n-                \"momentum\": float(self.momentum.get_value()),\n-                \"decay\": float(self.decay.get_value()),\n+                \"lr\": float(K.get_value(self.lr)),\n+                \"momentum\": float(K.get_value(self.momentum)),\n+                \"decay\": float(K.get_value(self.decay)),\n                 \"nesterov\": self.nesterov}\n \n \n@@ -90,26 +85,27 @@ class RMSprop(Optimizer):\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n-        self.lr = shared_scalar(lr)\n-        self.rho = shared_scalar(rho)\n+        self.lr = K.variable(lr)\n+        self.rho = K.variable(rho)\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        accumulators = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n         self.updates = []\n \n         for p, g, a, c in zip(params, grads, accumulators, constraints):\n-            new_a = self.rho * a + (1 - self.rho) * g ** 2  # update accumulator\n+            # update accumulator\n+            new_a = self.rho * a + (1 - self.rho) * g ** 2\n             self.updates.append((a, new_a))\n \n-            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n+            new_p = p - self.lr * g / K.sqrt(new_a + self.epsilon)\n             self.updates.append((p, c(new_p)))  # apply constraints\n         return self.updates\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"lr\": float(self.lr.get_value()),\n-                \"rho\": float(self.rho.get_value()),\n+                \"lr\": float(K.get_value(self.lr)),\n+                \"rho\": float(K.get_value(self.rho)),\n                 \"epsilon\": self.epsilon}\n \n \n@@ -117,23 +113,23 @@ class Adagrad(Optimizer):\n     def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n-        self.lr = shared_scalar(lr)\n+        self.lr = K.variable(lr)\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        accumulators = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n         self.updates = []\n \n         for p, g, a, c in zip(params, grads, accumulators, constraints):\n             new_a = a + g ** 2  # update accumulator\n             self.updates.append((a, new_a))\n-            new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n+            new_p = p - self.lr * g / K.sqrt(new_a + self.epsilon)\n             self.updates.append((p, c(new_p)))  # apply constraints\n         return self.updates\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"lr\": float(self.lr.get_value()),\n+                \"lr\": float(K.get_value(self.lr)),\n                 \"epsilon\": self.epsilon}\n \n \n@@ -144,22 +140,22 @@ class Adadelta(Optimizer):\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n-        self.lr = shared_scalar(lr)\n+        self.lr = K.variable(lr)\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        accumulators = [shared_zeros(p.get_value().shape) for p in params]\n-        delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n+        accumulators = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        delta_accumulators = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n         self.updates = []\n \n         for p, g, a, d_a, c in zip(params, grads, accumulators,\n                                    delta_accumulators, constraints):\n-            new_a = self.rho * a + (1 - self.rho) * g ** 2  # update accumulator\n+            # update accumulator\n+            new_a = self.rho * a + (1 - self.rho) * g ** 2\n             self.updates.append((a, new_a))\n \n             # use the new accumulator and the *old* delta_accumulator\n-            update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a +\n-                                                             self.epsilon)\n+            update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n \n             new_p = p - self.lr * update\n             self.updates.append((p, c(new_p)))  # apply constraints\n@@ -171,8 +167,8 @@ class Adadelta(Optimizer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"lr\": float(self.lr.get_value()),\n-                \"rho\": self.rho,\n+                \"lr\": float(K.get_value(self.lr)),\n+                \"rho\": float(K.get_value(self.rho)),\n                 \"epsilon\": self.epsilon}\n \n \n@@ -182,26 +178,29 @@ class Adam(Optimizer):\n \n         Default parameters follow those provided in the original paper.\n     '''\n-    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, *args, **kwargs):\n+    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8,\n+                 *args, **kwargs):\n         super(Adam, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n-        self.iterations = shared_scalar(0)\n-        self.lr = shared_scalar(lr)\n+        self.iterations = K.variable(0)\n+        self.lr = K.variable(lr)\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n         self.updates = [(self.iterations, self.iterations+1.)]\n \n         t = self.iterations + 1\n-        lr_t = self.lr * T.sqrt(1-self.beta_2**t)/(1-self.beta_1**t)\n+        lr_t = self.lr * K.sqrt(1 - self.beta_2 ** t) / (1 - self.beta_1 ** t)\n \n         for p, g, c in zip(params, grads, constraints):\n-            m = theano.shared(p.get_value() * 0.)  # zero init of moment\n-            v = theano.shared(p.get_value() * 0.)  # zero init of velocity\n+            # zero init of moment\n+            m = K.variable(np.zeros(K.get_value(p).shape))\n+            # zero init of velocity\n+            v = K.variable(np.zeros(K.get_value(p).shape))\n \n             m_t = (self.beta_1 * m) + (1 - self.beta_1) * g\n             v_t = (self.beta_2 * v) + (1 - self.beta_2) * (g ** 2)\n-            p_t = p - lr_t * m_t / (T.sqrt(v_t) + self.epsilon)\n+            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n \n             self.updates.append((m, m_t))\n             self.updates.append((v, v_t))\n@@ -210,7 +209,7 @@ class Adam(Optimizer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"lr\": float(self.lr.get_value()),\n+                \"lr\": float(K.get_value(self.lr)),\n                 \"beta_1\": self.beta_1,\n                 \"beta_2\": self.beta_2,\n                 \"epsilon\": self.epsilon}\n@@ -224,5 +223,5 @@ adam = Adam\n \n \n def get(identifier, kwargs=None):\n-    return get_from_module(identifier, globals(), 'optimizer', instantiate=True,\n-                           kwargs=kwargs)\n+    return get_from_module(identifier, globals(), 'optimizer',\n+                           instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ffa18e3905ac8dac016111063db4bbacbc788c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 281 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,61 +1,51 @@\n from __future__ import absolute_import\n-import theano\n-import theano.tensor as T\n import numpy as np\n-\n-\n-if theano.config.floatX == 'float64':\n-    epsilon = 1.0e-9\n-else:\n-    epsilon = 1.0e-7\n+from . import backend as K\n \n \n def mean_squared_error(y_true, y_pred):\n-    return T.sqr(y_pred - y_true).mean(axis=-1)\n+    return K.mean(K.square(y_pred - y_true), axis=-1)\n \n \n def root_mean_squared_error(y_true, y_pred):\n-    return T.sqrt(T.sqr(y_pred - y_true).mean(axis=-1))\n+    return K.mean(K.square(K.square(y_pred - y_true), axis=-1))\n \n \n def mean_absolute_error(y_true, y_pred):\n-    return T.abs_(y_pred - y_true).mean(axis=-1)\n+    return K.mean(K.abs(y_pred - y_true), axis=-1)\n \n \n def mean_absolute_percentage_error(y_true, y_pred):\n-    return T.abs_((y_true - y_pred) / T.clip(T.abs_(y_true), epsilon, np.inf)).mean(axis=-1) * 100.\n+    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K._EPSILON, np.inf))\n+    return 100. * K.mean(diff, axis=-1)\n \n \n def mean_squared_logarithmic_error(y_true, y_pred):\n-    return T.sqr(T.log(T.clip(y_pred, epsilon, np.inf) + 1.) - T.log(T.clip(y_true, epsilon, np.inf) + 1.)).mean(axis=-1)\n+    first_log = K.log(K.clip(y_pred, K._EPSILON, np.inf) + 1.)\n+    second_log = K.log(K.clip(y_true, K._EPSILON, np.inf) + 1.)\n+    return K.mean(K.square(first_log - second_log), axis=-1)\n \n \n def squared_hinge(y_true, y_pred):\n-    return T.sqr(T.maximum(1. - y_true * y_pred, 0.)).mean(axis=-1)\n+    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n \n \n def hinge(y_true, y_pred):\n-    return T.maximum(1. - y_true * y_pred, 0.).mean(axis=-1)\n+    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)\n \n \n def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n-    y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n-    # scale preds so that the class probas of each sample sum to 1\n-    y_pred /= y_pred.sum(axis=-1, keepdims=True)\n-    cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n-    return cce\n+    return K.mean(K.categorical_crossentropy(y_pred, y_true), axis=-1)\n \n \n def binary_crossentropy(y_true, y_pred):\n-    y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n-    bce = T.nnet.binary_crossentropy(y_pred, y_true).mean(axis=-1)\n-    return bce\n+    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n \n \n def poisson_loss(y_true, y_pred):\n-    return T.mean(y_pred - y_true * T.log(y_pred + epsilon), axis=-1)\n+    return K.mean(y_pred - y_true * K.log(y_pred + K._EPSILON), axis=-1)\n \n # aliases\n mse = MSE = mean_squared_error\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ed913da1108b63c69d48d30b395ae35e576dc9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 31 | Files Changed: 4 | Hunks: 20 | Methods Changed: 15 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 55 | Churn Cumulative: 1117 | Contributors (this commit): 21 | Commits (past 90d): 19 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,33 +1,29 @@\n from __future__ import absolute_import\n-import theano.tensor as T\n+from . import backend as K\n \n \n def softmax(x):\n-    return T.nnet.softmax(x.reshape((-1, x.shape[-1]))).reshape(x.shape)\n-\n-\n-def time_distributed_softmax(x):\n-    return softmax(x)\n+    return K.softmax(x)\n \n \n def softplus(x):\n-    return T.nnet.softplus(x)\n+    return K.softplus(x)\n \n \n-def relu(x):\n-    return T.nnet.relu(x)\n+def relu(x, alpha=0., max_value=None):\n+    return K.relu(x, alpha=alpha, max_value=max_value)\n \n \n def tanh(x):\n-    return T.tanh(x)\n+    return K.tanh(x)\n \n \n def sigmoid(x):\n-    return T.nnet.sigmoid(x)\n+    return K.sigmoid(x)\n \n \n def hard_sigmoid(x):\n-    return T.nnet.hard_sigmoid(x)\n+    return K.hard_sigmoid(x)\n \n \n def linear(x):\n\n@@ -2,7 +2,9 @@ from __future__ import absolute_import\n from __future__ import print_function\n \n import numpy as np\n-import time, json, warnings\n+import time\n+import json\n+import warnings\n \n from collections import deque\n from .utils.generic_utils import Progbar\n\n@@ -1,7 +1,5 @@\n from __future__ import absolute_import\n-import theano\n-import theano.tensor as T\n-import numpy as np\n+from . import backend as K\n \n \n class Constraint(object):\n@@ -17,8 +15,8 @@ class MaxNorm(Constraint):\n         self.m = m\n \n     def __call__(self, p):\n-        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n-        desired = T.clip(norms, 0, self.m)\n+        norms = K.sqrt(K.sum(K.sqr(p), axis=0))\n+        desired = K.clip(norms, 0, self.m)\n         p = p * (desired / (1e-7 + norms))\n         return p\n \n@@ -29,14 +27,14 @@ class MaxNorm(Constraint):\n \n class NonNeg(Constraint):\n     def __call__(self, p):\n-        p = theano.shared(p)\n-        p *= T.ge(p, 0.)\n+        p = K.variable(p)\n+        p *= (p >= 0.)\n         return p\n \n \n class UnitNorm(Constraint):\n     def __call__(self, p):\n-        return p / T.sqrt(T.sum(p**2, axis=-1, keepdims=True))\n+        return p / K.sqrt(K.sum(p**2, axis=-1, keepdims=True))\n \n identity = Constraint\n maxnorm = MaxNorm\n\n@@ -1,9 +1,6 @@\n from __future__ import absolute_import\n-import theano\n-import theano.tensor as T\n import numpy as np\n-\n-from .utils.theano_utils import sharedX, shared_zeros, shared_ones\n+from . import backend as K\n \n \n def get_fans(shape):\n@@ -13,11 +10,11 @@ def get_fans(shape):\n \n \n def uniform(shape, scale=0.05):\n-    return sharedX(np.random.uniform(low=-scale, high=scale, size=shape))\n+    return K.variable(np.random.uniform(low=-scale, high=scale, size=shape))\n \n \n def normal(shape, scale=0.05):\n-    return sharedX(np.random.randn(*shape) * scale)\n+    return K.variable(np.random.randn(*shape) * scale)\n \n \n def lecun_uniform(shape):\n@@ -66,22 +63,22 @@ def orthogonal(shape, scale=1.1):\n     # pick the one with the correct shape\n     q = u if u.shape == flat_shape else v\n     q = q.reshape(shape)\n-    return sharedX(scale * q[:shape[0], :shape[1]])\n+    return K.variable(scale * q[:shape[0], :shape[1]])\n \n \n def identity(shape, scale=1):\n     if len(shape) != 2 or shape[0] != shape[1]:\n         raise Exception(\"Identity matrix initialization can only be used for 2D square matrices\")\n     else:\n-        return sharedX(scale * np.identity(shape[0]))\n+        return K.variable(scale * np.identity(shape[0]))\n \n \n def zero(shape):\n-    return shared_zeros(shape)\n+    return K.zeros(shape)\n \n \n def one(shape):\n-    return shared_ones(shape)\n+    return K.ones(shape)\n \n \n from .utils.generic_utils import get_from_module\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b8970abd37bb621dd8a74f64162b2fad73e4200", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 201 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from keras.layers.core import TimeDistributedDense, Activation\n from keras.layers.recurrent import LSTM\n from keras.optimizers import Adam\n \n-from seya.sandbox.ntm import NeuralTuringMachine as NTM\n+from keras.layers.ntm import NeuralTuringMachine as NTM\n \n \"\"\"\n Copy Problem defined in Graves et. al [0]\n@@ -44,16 +44,15 @@ model.compile(loss='binary_crossentropy', optimizer=sgd)\n \n # LSTM - Run this for comparison\n \n-# sgd = Adam(lr=lr, clipnorm=clipnorm)\n-#\n-# model4 = Sequential()\n-# model4.add(LSTM(input_dim=input_dim, output_dim=h_dim*2, return_sequences=True))\n-# model4.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n-# model4.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n-# model4.add(TimeDistributedDense(input_dim))\n-# model4.add(Activation('sigmoid'))\n-#\n-# model4.compile(loss='binary_crossentropy', optimizer=sgd)\n+sgd = Adam(lr=lr, clipnorm=clipnorm)\n+lstm = Sequential()\n+lstm.add(LSTM(input_dim=input_dim, output_dim=h_dim*2, return_sequences=True))\n+lstm.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n+lstm.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n+lstm.add(TimeDistributedDense(input_dim))\n+lstm.add(Activation('sigmoid'))\n+\n+lstm.compile(loss='binary_crossentropy', optimizer=sgd)\n \n ###### DATASET ########\n \n@@ -110,22 +109,20 @@ def test_model(model, file_name, min_size=100):\n ##### TRAIN ######\n nb_epoch = 4000\n progbar = generic_utils.Progbar(nb_epoch)\n-ACC = []\n for e in range(nb_epoch):\n     I, V, sw = get_sample(n_bits=input_dim, max_size=20, min_size=1, batch_size=100)\n \n-    loss = trained.train_on_batch(I, V, sample_weight=sw)\n+    loss1 = model.train_on_batch(I, V, sample_weight=sw)\n+    loss2 = lstm.train_on_batch(I, V, sample_weight=sw)\n \n-    progbar.add(1, values=[(\"loss\", loss)])\n+    progbar.add(1, values=[(\"NTM\", loss1), (\"LSTM\", loss2)])\n \n     if e % 500 == 0:\n         print(\"\")\n-        acc = test_model(trained, filename)\n-        l = []\n-        for a in [acc,]:\n-            print(\"acc: {}\".format(a))\n-            l.append(a)\n-        ACC.append(l)\n+        acc1 = test_model(model, 'ntm.png')\n+        acc2 = test_model(model, 'lstm.png')\n+        print(\"NTM  test acc: {}\".format(a))\n+        print(\"LSTM test acc: {}\".format(a))\n \n ##### VISUALIZATION #####\n X = model.get_input()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab19cf7b07b02a6f94b44f499e4239b0ec5a6254", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 346 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,8 +11,10 @@ tol = 1e-4\n \n \n def _update_controller(self, inp, h_tm1, M, mask):\n-    \"\"\"We have to update the inner RNN inside the NTM, this\n-    is the function to do it. Pretty much copy+pasta from Keras\n+    \"\"\" Update inner RNN controler\n+    We have to update the inner RNN inside the Neural Turing Machine, this\n+    is an almost literal copy of keras.layers.recurrent.GRU and \n+    keras.layers.recurrent.LSTM see these clases for further details.\n     \"\"\"\n     x = T.concatenate([inp, M], axis=-1)\n     # get inputs\n@@ -57,12 +59,11 @@ def _update_controller(self, inp, h_tm1, M, mask):\n \n \n def _circulant(leng, n_shifts):\n-    \"\"\"\n-    I confess, I'm actually proud of this hack. I hope you enjoy!\n+    \"\"\" Generate circulant copies of a vector.\n     This will generate a tensor with `n_shifts` of rotated versions the\n     identity matrix. When this tensor is multiplied by a vector\n     the result are `n_shifts` shifted versions of that vector. Since\n-    everything is done with inner products, everything is differentiable.\n+    everything is done with inner products, this operation is differentiable.\n \n     Paramters:\n     ----------\n@@ -99,16 +100,19 @@ def _cosine_distance(M, k):\n class NeuralTuringMachine(Recurrent):\n     \"\"\" Neural Turing Machines\n     \n-    Non obvious parameter:\n-    ----------------------\n+    Parameters:\n+    -----------\n     shift_range: int, number of available shifts, ex. if 3, avilable shifts are\n                  (-1, 0, 1)\n     n_slots: number of memory locations\n     m_length: memory length at each location\n+    inner_rnn: str, supported values are 'gru' and 'lstm'\n+    output_dim: hidden state size (RNN controller output_dim)\n \n-    Known issues:\n-    -------------\n+    Known issues and TODO:\n+    ----------------------\n     Theano may complain when n_slots == 1.\n+    Add multiple reading and writing heads.\n \n     \"\"\"\n     def __init__(self, output_dim, n_slots, m_length, shift_range=3,\n@@ -290,13 +294,13 @@ class NeuralTuringMachine(Recurrent):\n \n     def get_full_output(self, train=False):\n         \"\"\"\n-        This method is for research and visualization purposes. Use it as\n+        This method is for research and visualization purposes. Use it as:\n         X = model.get_input()  # full model\n         Y = ntm.get_output()    # this layer\n         F = theano.function([X], Y, allow_input_downcast=True)\n         [memory, read_address, write_address, rnn_state] = F(x)\n \n-        if inner_rnn == \"lstm\" use it as\n+        if inner_rnn == \"lstm\" use it as:\n         [memory, read_address, write_address, rnn_cell, rnn_state] = F(x)\n \n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#747d4a30b13922890332dc51d4673a4dd0fd2c13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 352 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -171,10 +171,10 @@ class NeuralTuringMachine(Recurrent):\n         # get_w  parameters for reading operation\n         self.W_k_read = self.rnn.init((self.output_dim, self.m_length))\n         self.b_k_read = self.rnn.init((self.m_length, ))\n-        self.W_c_read = self.rnn.init((self.output_dim, 3))  # 3 = beta, g, gamma see eq. 5, 7, 9\n+        self.W_c_read = self.rnn.init((self.output_dim, 3))  # 3 = beta, g, gamma see eq. 5, 7, 9 in Graves et. al 2014\n         self.b_c_read = shared_zeros((3))\n         self.W_s_read = self.rnn.init((self.output_dim, self.shift_range))\n-        self.b_s_read = shared_zeros((self.shift_range))  # b_s lol! not intentional\n+        self.b_s_read = shared_zeros((self.shift_range)) \n \n         # get_w  parameters for writing operation\n         self.W_k_write = self.rnn.init((self.output_dim, self.m_length))\n@@ -231,8 +231,6 @@ class NeuralTuringMachine(Recurrent):\n         return k, beta, g, gamma, s\n \n     def _get_initial_states(self, batch_size):\n-        # init_M = self.M + T.unbroadcast(alloc_zeros_matrix(batch_size, self.n_slots, self.m_length), 0, 1, 2)\n-        # init_M = self.M.dimshuffle('x', 0, 1).repeat(batch_size, axis=0)\n         init_M = self.M.dimshuffle(0, 'x', 'x').repeat(\n            batch_size, axis=0).repeat(self.n_slots, axis=1).repeat(\n                self.m_length, axis=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd05964135bfd8c4182a42f915d51e6e45c2a216", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 32 | Files Changed: 2 | Hunks: 17 | Methods Changed: 19 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 115 | Churn Cumulative: 943 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,10 @@ def variable(value, dtype=_FLOATX, name=None):\n     return v\n \n \n+def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n+    return tf.placeholder(dtype, shape=shape, name=name)\n+\n+\n def shape(x):\n     return x.get_shape()\n \n@@ -45,8 +49,12 @@ def ones(shape, dtype=_FLOATX, name=None):\n     return variable(np.ones(shape), dtype, name)\n \n \n-def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n-    return tf.placeholder(dtype, shape=shape, name=name)\n+def ones_like(x, name=None):\n+    return tf.ones_like(x)\n+\n+\n+def zeros_like(x, name=None):\n+    return tf.zeros_like(x)\n \n \n # LINEAR ALGEBRA\n@@ -59,6 +67,15 @@ def transpose(x):\n     return tf.transpose(x)\n \n \n+def embedding(reference, indices):\n+    '''reference: a tensor.\n+    indices: an int tensor of indices.\n+\n+    Return: a tensor of same type as reference.\n+    '''\n+    return tf.gather(reference, indices)\n+\n+\n # ELEMENT-WISE OPERATIONS\n \n def max(x, axis=None, keepdims=False):\n@@ -81,6 +98,12 @@ def sum(x, axis=None, keepdims=False):\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n+def mul(x, axis=None, keepdims=False):\n+    '''Multiply the values in a tensor, alongside the specified axis.\n+    '''\n+    return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n+\n+\n def mean(x, axis=None, keepdims=False):\n     if axis is not None:\n         axis = axis % len(x.get_shape())\n@@ -118,7 +141,8 @@ def abs(x):\n \n \n def sqrt(x):\n-    x = tf.clip_by_value(x, 0., np.inf)\n+    x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n+                         tf.cast(np.inf, dtype=_FLOATX))\n     return tf.sqrt(x)\n \n \n@@ -137,7 +161,8 @@ def round(x):\n def clip(x, min_value, max_value):\n     if max_value < min_value:\n         max_value = min_value\n-    return tf.clip_by_value(x, min_value, max_value)\n+    return tf.clip_by_value(x, tf.cast(min_value, dtype=_FLOATX),\n+                            tf.cast(max_value, dtype=_FLOATX))\n \n \n def equal(x, y):\n@@ -149,7 +174,7 @@ def maximum(x, y):\n \n \n def minimum(x, y):\n-    return tf.maximum(x, y)\n+    return tf.minimum(x, y)\n \n \n # SHAPE OPERATIONS\n@@ -247,7 +272,8 @@ def relu(x, alpha=0., max_value=None):\n     negative_part = tf.nn.relu(-x)\n     x = tf.nn.relu(x)\n     if max_value is not None:\n-        x = tf.clip_by_value(x, 0., max_value)\n+        x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n+                             tf.cast(max_value, dtype=_FLOATX))\n     x -= alpha * negative_part\n     return x\n \n@@ -270,7 +296,8 @@ def categorical_crossentropy(output, target, from_logits=False):\n                                 reduction_indices=len(output.get_shape())-1,\n                                 keep_dims=True)\n         # manual computation of crossentropy\n-        output = tf.clip_by_value(output, _EPSILON, 1.-_EPSILON)\n+        output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n+                                  tf.cast(1.-_EPSILON, dtype=_FLOATX))\n         return - tf.reduce_sum(target * tf.log(output),\n                                reduction_indices=len(output.get_shape())-1)\n     else:\n@@ -283,7 +310,8 @@ def binary_crossentropy(output, target, from_logits=False):\n     '''\n     if not from_logits:\n         # transform back to logits\n-        output = tf.clip_by_value(output, _EPSILON, 1.-_EPSILON)\n+        output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n+                                  tf.cast(1.-_EPSILON, dtype=_FLOATX))\n         output = tf.log(output / (1 - output))\n     return tf.nn.sigmoid_cross_entropy_with_logits(output, target)\n \n@@ -294,7 +322,8 @@ def sigmoid(x):\n \n def hard_sigmoid(x):\n     x = (0.2 * x) + 0.5\n-    x = tf.clip_by_value(x, 0., 1.)\n+    x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n+                         tf.cast(1., dtype=_FLOATX))\n     return x\n \n \n\n@@ -28,7 +28,29 @@ if _on_gpu():\n def variable(value, dtype=_FLOATX, name=None):\n     '''Instantiate a tensor variable.\n     '''\n-    return theano.shared(np.asarray(value, dtype=dtype), name=name)\n+    value = np.asarray(value, dtype=dtype)\n+    return theano.shared(value=value, name=name, strict=False)\n+\n+\n+def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n+    '''Instantiate an input data placeholder variable.\n+    '''\n+    if shape is None and ndim is None:\n+        raise Exception('Specify either a shape or ndim value.')\n+    if shape is not None:\n+        ndim = len(shape)\n+    if ndim == 0:\n+        return T.scalar(name=name, dtype=dtype)\n+    elif ndim == 1:\n+        return T.vector(name=name, dtype=dtype)\n+    elif ndim == 2:\n+        return T.matrix(name=name, dtype=dtype)\n+    elif ndim == 3:\n+        return T.tensor3(name=name, dtype=dtype)\n+    elif ndim == 4:\n+        return T.tensor4(name=name, dtype=dtype)\n+    else:\n+        raise Exception('ndim too large: ' + str(ndim))\n \n \n def shape(x):\n@@ -58,25 +80,12 @@ def ones(shape, dtype=_FLOATX, name=None):\n     return variable(np.ones(shape), dtype, name)\n \n \n-def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n-    '''Instantiate an input data placeholder variable.\n-    '''\n-    if shape is None and ndim is None:\n-        raise Exception('Specify either a shape or ndim value.')\n-    if shape is not None:\n-        ndim = len(shape)\n-    if ndim == 0:\n-        return T.scalar(name=name, dtype=dtype)\n-    elif ndim == 1:\n-        return T.vector(name=name, dtype=dtype)\n-    elif ndim == 2:\n-        return T.matrix(name=name, dtype=dtype)\n-    elif ndim == 3:\n-        return T.tensor3(name=name, dtype=dtype)\n-    elif ndim == 4:\n-        return T.tensor4(name=name, dtype=dtype)\n-    else:\n-        raise Exception('ndim too large: ' + str(ndim))\n+def ones_like(x):\n+    return T.ones_like(x)\n+\n+\n+def zeros_like(x):\n+    return T.zeros_like(x)\n \n \n # LINEAR ALGEBRA\n@@ -95,6 +104,15 @@ def transpose(x):\n     return T.transpose(x)\n \n \n+def embedding(reference, indices):\n+    '''reference: a tensor.\n+    indices: an int tensor of indices.\n+\n+    Return: a tensor of same type as reference.\n+    '''\n+    return reference[indices]\n+\n+\n # ELEMENT-WISE OPERATIONS\n \n def max(x, axis=None, keepdims=False):\n@@ -111,6 +129,12 @@ def sum(x, axis=None, keepdims=False):\n     return T.sum(x, axis=axis, keepdims=keepdims)\n \n \n+def mul(x, axis=None, keepdims=False):\n+    '''Multiply the values in a tensor, alongside the specified axis.\n+    '''\n+    return T.mul(x, axis=axis, keepdims=keepdims)\n+\n+\n def mean(x, axis=None, keepdims=False):\n     return T.mean(x, axis=axis, keepdims=keepdims)\n \n@@ -169,7 +193,7 @@ def maximum(x, y):\n \n \n def minimum(x, y):\n-    return T.maximum(x, y)\n+    return T.minimum(x, y)\n \n \n # SHAPE OPERATIONS\n@@ -422,6 +446,4 @@ batched_tensordot\n \n addbroadcast -> remove usage?\n unbroadcast -> remove usage?\n-\n-embedding: make sure that [] operator works for TF\n '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e519f7aa701dc435ce42ac5f6bcacf19f8efa09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 97 | Files Changed: 4 | Hunks: 22 | Methods Changed: 13 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 122 | Churn Cumulative: 846 | Contributors (this commit): 12 | Commits (past 90d): 24 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,7 @@ class MaxNorm(Constraint):\n         self.m = m\n \n     def __call__(self, p):\n-        norms = K.sqrt(K.sum(K.sqr(p), axis=0))\n+        norms = K.sqrt(K.sum(K.square(p), axis=0))\n         desired = K.clip(norms, 0, self.m)\n         p = p * (desired / (1e-7 + norms))\n         return p\n\n@@ -1,7 +1,6 @@\n from .. import initializations\n from ..layers.core import Layer, MaskedLayer\n-from ..utils.theano_utils import shared_zeros, shared_ones, sharedX\n-import theano.tensor as T\n+from .. import backend as K\n import numpy as np\n \n \n@@ -12,7 +11,7 @@ class LeakyReLU(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return T.nnet.relu(X, self.alpha)\n+        return K.relu(X, alpha=self.alpha)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -24,7 +23,8 @@ class LeakyReLU(MaskedLayer):\n class PReLU(MaskedLayer):\n     '''\n         Reference:\n-            Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n+            Delving Deep into Rectifiers: Surpassing Human-Level\n+            Performance on ImageNet Classification\n                 http://arxiv.org/pdf/1502.01852v1.pdf\n     '''\n     def __init__(self, init='zero', weights=None, **kwargs):\n@@ -43,7 +43,7 @@ class PReLU(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        pos = T.nnet.relu(X)\n+        pos = K.relu(X)\n         neg = self.alphas * (X - abs(X)) * 0.5\n         return pos + neg\n \n@@ -59,7 +59,8 @@ class ParametricSoftplus(MaskedLayer):\n         Parametric Softplus of the form: alpha * log(1 + exp(beta * X))\n \n         Reference:\n-            Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n+            Inferring Nonlinear Neuronal Computation\n+            Based on Physiologically Plausible Inputs\n             http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n     '''\n     def __init__(self, alpha_init=0.2, beta_init=5.0,\n@@ -71,8 +72,8 @@ class ParametricSoftplus(MaskedLayer):\n \n     def build(self):\n         input_shape = self.input_shape[1:]\n-        self.alphas = sharedX(self.alpha_init * np.ones(input_shape))\n-        self.betas = sharedX(self.beta_init * np.ones(input_shape))\n+        self.alphas = K.variable(self.alpha_init * np.ones(input_shape))\n+        self.betas = K.variable(self.beta_init * np.ones(input_shape))\n         self.params = [self.alphas, self.betas]\n \n         if self.initial_weights is not None:\n@@ -81,7 +82,7 @@ class ParametricSoftplus(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return T.nnet.softplus(self.betas * X) * self.alphas\n+        return K.softplus(self.betas * X) * self.alphas\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -105,7 +106,7 @@ class ThresholdedLinear(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return T.switch(abs(X) < self.theta, 0, X)\n+        return K.switch(K.abs(X) < self.theta, 0, X)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -128,7 +129,7 @@ class ThresholdedReLU(MaskedLayer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        return T.switch(X > self.theta, X, 0)\n+        return K.switch(X > self.theta, X, 0)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n\n@@ -1,10 +1,8 @@\n from __future__ import absolute_import\n-import theano\n-import theano.tensor as T\n+from .. import backend as K\n \n from .. import activations, initializations, regularizers, constraints\n from ..layers.core import Layer, MaskedLayer\n-from ..utils.theano_utils import sharedX\n \n from ..constraints import unitnorm\n \n@@ -39,7 +37,7 @@ class Embedding(Layer):\n         super(Embedding, self).__init__(**kwargs)\n \n     def build(self):\n-        self.input = T.imatrix()\n+        self.input = K.placeholder(ndim=2, dtype='int32')\n         self.W = self.init((self.input_dim, self.output_dim))\n         self.params = [self.W]\n         self.regularizers = []\n@@ -59,7 +57,7 @@ class Embedding(Layer):\n         if not self.mask_zero:\n             return None\n         else:\n-            return T.ones_like(X) * (1 - T.eq(X, 0))\n+            return K.ones_like(X) * (1 - K.equal(X, 0))\n \n     @property\n     def output_shape(self):\n@@ -67,7 +65,7 @@ class Embedding(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        out = self.W[X]\n+        out = K.embedding(self.W, X)\n         return out\n \n     def get_config(self):\n@@ -82,72 +80,3 @@ class Embedding(Layer):\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None}\n         base_config = super(Embedding, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class WordContextProduct(Layer):\n-    '''\n-        This layer turns a pair of words (a pivot word + a context word,\n-        ie. a word from the same context, or a random, out-of-context word),\n-        identified by their index in a vocabulary, into two dense representations\n-        (word representation and context representation).\n-\n-        Then it returns activation(dot(pivot_embedding, context_embedding)),\n-        which can be trained to encode the probability\n-        of finding the context word in the context of the pivot word\n-        (or reciprocally depending on your training procedure).\n-\n-        The layer ingests integer tensors of shape:\n-        (nb_samples, 2)\n-        and outputs a float tensor of shape\n-        (nb_samples, 1)\n-\n-        The 2nd dimension encodes (pivot, context).\n-        input_dim is the size of the vocabulary.\n-\n-        For more context, see Mikolov et al.:\n-            Efficient Estimation of Word representations in Vector Space\n-            http://arxiv.org/pdf/1301.3781v3.pdf\n-    '''\n-    input_ndim = 2\n-\n-    def __init__(self, input_dim, proj_dim=128,\n-                 init='uniform', activation='sigmoid', weights=None, **kwargs):\n-\n-        super(WordContextProduct, self).__init__(**kwargs)\n-        self.input_dim = input_dim\n-        self.proj_dim = proj_dim\n-        self.init = initializations.get(init)\n-        self.activation = activations.get(activation)\n-\n-        self.input = T.imatrix()\n-        # two different embeddings for pivot word and its context\n-        # because p(w|c) != p(c|w)\n-        self.W_w = self.init((input_dim, proj_dim))\n-        self.W_c = self.init((input_dim, proj_dim))\n-\n-        self.params = [self.W_w, self.W_c]\n-\n-        if weights is not None:\n-            self.set_weights(weights)\n-\n-    @property\n-    def output_shape(self):\n-        return (self.input_shape[0], 1)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        w = self.W_w[X[:, 0]]  # nb_samples, proj_dim\n-        c = self.W_c[X[:, 1]]  # nb_samples, proj_dim\n-\n-        dot = T.sum(w * c, axis=1)\n-        dot = theano.tensor.reshape(dot, (X.shape[0], 1))\n-        return self.activation(dot)\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"input_dim\": self.input_dim,\n-                  \"proj_dim\": self.proj_dim,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__}\n-        base_config = super(WordContextProduct, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -1,9 +1,6 @@\n from __future__ import absolute_import\n-import numpy as np\n from .core import MaskedLayer\n-import theano\n-import theano.tensor as T\n-from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n+from .. import backend as K\n \n \n class GaussianNoise(MaskedLayer):\n@@ -13,15 +10,15 @@ class GaussianNoise(MaskedLayer):\n     def __init__(self, sigma, **kwargs):\n         super(GaussianNoise, self).__init__(**kwargs)\n         self.sigma = sigma\n-        self.srng = RandomStreams(seed=np.random.randint(10e6))\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n         if not train or self.sigma == 0:\n             return X\n         else:\n-            return X + self.srng.normal(size=X.shape, avg=0.0, std=self.sigma,\n-                                        dtype=theano.config.floatX)\n+            return X + K.random_normal(shape=K.shape(X),\n+                                       mean=0.,\n+                                       std=self.sigma)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -41,13 +38,14 @@ class GaussianDropout(MaskedLayer):\n     def __init__(self, p, **kwargs):\n         super(GaussianDropout, self).__init__(**kwargs)\n         self.p = p\n-        self.srng = RandomStreams(seed=np.random.randint(10e6))\n \n     def get_output(self, train):\n         X = self.get_input(train)\n         if train:\n-            # self.p refers to drop probability rather than retain probability (as in paper) to match Dropout layer syntax\n-            X *= self.srng.normal(size=X.shape, avg=1.0, std=T.sqrt(self.p / (1.0 - self.p)), dtype=theano.config.floatX)\n+            # self.p refers to drop probability rather than\n+            # retain probability (as in paper), for consistency\n+            X *= K.random_normal(shape=K.shape(X), mean=1.0,\n+                                 std=self.p / (1.0 - self.p))\n         return X\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
