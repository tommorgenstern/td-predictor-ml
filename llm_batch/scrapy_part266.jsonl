{"custom_id": "scrapy#9b47dc6a703310d13c9470e50d4b14f81ee893c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 499 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,7 @@ setup(\n         'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n-        # 'Programming Language :: Python :: 3.8',  not supported yet\n+        'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: Implementation :: CPython',\n         'Programming Language :: Python :: Implementation :: PyPy',\n         'Topic :: Internet :: WWW/HTTP',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4432136ffff4d8af42f7a485c17ab7fbbb228078", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 180 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,6 @@ import unittest\n import email.utils\n from contextlib import contextmanager\n import pytest\n-import sys\n \n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spiders import Spider\n@@ -161,10 +160,7 @@ class LeveldbStorageTest(DefaultStorageTest):\n         pytest.importorskip('leveldb')\n     except SystemError:\n         # Happens in python 3.8\n-        pytest.mark.skipif(\n-            sys.version_info >= (3, 8),\n-            reason='leveldb not supported in python 3.8',\n-        )\n+        pytest.skip(\"'SystemError: bad call flags' occurs on Python 3.8\")\n     storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c51fb959e2985faf6f21fe7f03d2fb8160de064f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 183 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ import unittest\n import email.utils\n from contextlib import contextmanager\n import pytest\n+import sys\n \n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spiders import Spider\n@@ -160,7 +161,7 @@ class LeveldbStorageTest(DefaultStorageTest):\n         pytest.importorskip('leveldb')\n     except SystemError:\n         # Happens in python 3.8\n-        pytest.skip(\"'SystemError: bad call flags' occurs on Python 3.8\")\n+        pytestmark = pytest.skip(\"'SystemError: bad call flags' occurs on Python 3.8\")\n     storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b73d217de5647a68c7b8dfda747cd3d0685c226d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 185 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -161,7 +161,7 @@ class LeveldbStorageTest(DefaultStorageTest):\n         pytest.importorskip('leveldb')\n     except SystemError:\n         # Happens in python 3.8\n-        pytestmark = pytest.skip(\"'SystemError: bad call flags' occurs on Python 3.8\")\n+        pytestmark = pytest.mark.skip(\"'SystemError: bad call flags' occurs on Python 3.8\")\n     storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93e3dc1b826e44d1a5a24fbb39c090ce426aa862", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 189 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -155,13 +155,13 @@ class FilesystemStorageGzipTest(FilesystemStorageTest):\n         new_settings.setdefault('HTTPCACHE_GZIP', True)\n         return super(FilesystemStorageTest, self)._get_settings(**new_settings)\n \n+\n class LeveldbStorageTest(DefaultStorageTest):\n \n     try:\n         pytest.importorskip('leveldb')\n     except SystemError:\n-        # Happens in python 3.8\n-        pytestmark = pytest.mark.skip(\"'SystemError: bad call flags' occurs on Python 3.8\")\n+        pytestmark = pytest.mark.skip(\"Test module skipped - 'SystemError: bad call flags' occurs when >= Python 3.8\")\n     storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94f060fcc84853f28f3f91b6dde1d61c8e19251e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 259 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class LogFormatter(object):\n     *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.\n         The final log message is computed as ``msg % args``.\n \n-    Users can define their own ``LogFormatter`` class if they want to customise how\n+    Users can define their own ``LogFormatter`` class if they want to customize how\n     each action is logged or if they want to omit it entirely. In order to omit\n     logging an action the method must return ``None``.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66cbceeb0a9104fc0fa238898e38d0d9ce9cbcf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 23 | Churn Cumulative: 278 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: 0.8\n\nDIFF:\n@@ -1,5 +1,5 @@\n import logging\n-from six.moves.urllib.parse import urljoin\n+from six.moves.urllib.parse import urljoin, urlparse\n \n from w3lib.url import safe_url_string\n \n@@ -70,7 +70,10 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n \n-        location = safe_url_string(response.headers['location'])\n+        location = safe_url_string(response.headers['Location'])\n+        if response.headers['Location'].startswith(b'//'):\n+            request_scheme = urlparse(request.url).scheme\n+            location = request_scheme + '://' + location.lstrip('/')\n \n         redirected_url = urljoin(request.url, location)\n \n\n@@ -106,6 +106,22 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         del rsp.headers['Location']\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n+    def test_redirect_302_relative(self):\n+        url = 'http://www.example.com/302'\n+        url2 = '///i8n.example2.com/302'\n+        url3 = 'http://i8n.example2.com/302'\n+        req = Request(url, method='HEAD')\n+        rsp = Response(url, headers={'Location': url2}, status=302)\n+\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, url3)\n+        self.assertEqual(req2.method, 'HEAD')\n+\n+        # response without Location header but with status code is 3XX should be ignored\n+        del rsp.headers['Location']\n+        assert self.mw.process_response(req, rsp, self.spider) is rsp\n+\n \n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d6da78eda3cc0bba1bfdf70194fdf655fac8aeb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 184 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -16,7 +16,7 @@ from scrapy.utils.httpobj import urlparse_cached\n \n \n _fingerprint_cache = weakref.WeakKeyDictionary()\n-def request_fingerprint(request, include_headers=None):\n+def request_fingerprint(request, include_headers=None, keep_fragments=False):\n     \"\"\"\n     Return the request fingerprint.\n \n@@ -42,15 +42,21 @@ def request_fingerprint(request, include_headers=None):\n     the fingeprint. If you want to include specific headers use the\n     include_headers argument, which is a list of Request headers to include.\n \n+    Also, servers usually ignore fragments in urls when handling requests,\n+    so they are also ignored by default when calculating the fingerprint.\n+    If you want to include them, set the keep_fragments argument to True\n+    (for instance when handling requests with a headless browser).\n+\n     \"\"\"\n     if include_headers:\n         include_headers = tuple(to_bytes(h.lower())\n                                  for h in sorted(include_headers))\n     cache = _fingerprint_cache.setdefault(request, {})\n-    if include_headers not in cache:\n+    cache_key = (include_headers, keep_fragments)\n+    if cache_key not in cache:\n         fp = hashlib.sha1()\n         fp.update(to_bytes(request.method))\n-        fp.update(to_bytes(canonicalize_url(request.url)))\n+        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))\n         fp.update(request.body or b'')\n         if include_headers:\n             for hdr in include_headers:\n@@ -58,8 +64,8 @@ def request_fingerprint(request, include_headers=None):\n                     fp.update(hdr)\n                     for v in request.headers.getlist(hdr):\n                         fp.update(v)\n-        cache[include_headers] = fp.hexdigest()\n-    return cache[include_headers]\n+        cache[cache_key] = fp.hexdigest()\n+    return cache[cache_key]\n \n \n def request_authenticate(request, username, password):\n\n@@ -17,7 +17,7 @@ class UtilsRequestTest(unittest.TestCase):\n         self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2))\n \n         # make sure caching is working\n-        self.assertEqual(request_fingerprint(r1), _fingerprint_cache[r1][None])\n+        self.assertEqual(request_fingerprint(r1), _fingerprint_cache[r1][(None, False)])\n \n         r1 = Request(\"http://www.example.com/members/offers.html\")\n         r2 = Request(\"http://www.example.com/members/offers.html\")\n@@ -42,6 +42,13 @@ class UtilsRequestTest(unittest.TestCase):\n         self.assertEqual(request_fingerprint(r3, include_headers=['accept-language', 'sessionid']),\n                          request_fingerprint(r3, include_headers=['SESSIONID', 'Accept-Language']))\n \n+        r1 = Request(\"http://www.example.com/test.html\")\n+        r2 = Request(\"http://www.example.com/test.html#fragment\")\n+        self.assertEqual(request_fingerprint(r1), request_fingerprint(r2))\n+        self.assertEqual(request_fingerprint(r1), request_fingerprint(r1, keep_fragments=True))\n+        self.assertNotEqual(request_fingerprint(r2), request_fingerprint(r2, keep_fragments=True))\n+        self.assertNotEqual(request_fingerprint(r1), request_fingerprint(r2, keep_fragments=True))\n+\n         r1 = Request(\"http://www.example.com\")\n         r2 = Request(\"http://www.example.com\", method='POST')\n         r3 = Request(\"http://www.example.com\", method='POST', body=b'request body')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#229e722a03aced0fb62ec8c631b036c3e13e2188", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 800 | Contributors (this commit): 23 | Commits (past 90d): 7 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -14,8 +14,8 @@ del pkgutil\n \n # Check minimum required Python version\n import sys\n-if sys.version_info < (2, 7):\n-    print(\"Scrapy %s requires Python 2.7\" % __version__)\n+if sys.version_info < (3, 5):\n+    print(\"Scrapy %s requires Python 3.5\" % __version__)\n     sys.exit(1)\n \n # Ignore noisy twisted deprecation warnings\n\n@@ -50,8 +50,6 @@ setup(\n         'License :: OSI Approved :: BSD License',\n         'Operating System :: OS Independent',\n         'Programming Language :: Python',\n-        'Programming Language :: Python :: 2',\n-        'Programming Language :: Python :: 2.7',\n         'Programming Language :: Python :: 3',\n         'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n@@ -63,10 +61,9 @@ setup(\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*',\n+    python_requires='>=3.5',\n     install_requires=[\n-        'Twisted>=16.0.0;python_version==\"2.7\"',\n-        'Twisted>=17.9.0;python_version>=\"3.5\"',\n+        'Twisted>=17.9.0',\n         'cryptography>=2.0',\n         'cssselect>=0.9.1',\n         'lxml>=3.5.0',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#15c55d0c1d4a4384fd9725d47c305f52918e3831", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 80 | Files Changed: 2 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 80 | Churn Cumulative: 465 | Contributors (this commit): 15 | Commits (past 90d): 14 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -347,77 +347,6 @@ class FilesystemCacheStorage(object):\n             return pickle.load(f)\n \n \n-class LeveldbCacheStorage(object):\n-\n-    def __init__(self, settings):\n-        warn(\"The LevelDB storage backend is deprecated.\",\n-             ScrapyDeprecationWarning, stacklevel=2)\n-        import leveldb\n-        self._leveldb = leveldb\n-        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n-        self.db = None\n-\n-    def open_spider(self, spider):\n-        dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n-        self.db = self._leveldb.LevelDB(dbpath)\n-\n-        logger.debug(\"Using LevelDB cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n-\n-    def close_spider(self, spider):\n-        # Do compactation each time to save space and also recreate files to\n-        # avoid them being removed in storages with timestamp-based autoremoval.\n-        self.db.CompactRange()\n-        del self.db\n-        garbage_collect()\n-\n-    def retrieve_response(self, spider, request):\n-        data = self._read_data(spider, request)\n-        if data is None:\n-            return  # not cached\n-        url = data['url']\n-        status = data['status']\n-        headers = Headers(data['headers'])\n-        body = data['body']\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n-        response = respcls(url=url, headers=headers, status=status, body=body)\n-        return response\n-\n-    def store_response(self, spider, request, response):\n-        key = self._request_key(request)\n-        data = {\n-            'status': response.status,\n-            'url': response.url,\n-            'headers': dict(response.headers),\n-            'body': response.body,\n-        }\n-        batch = self._leveldb.WriteBatch()\n-        batch.Put(key + b'_data', pickle.dumps(data, protocol=2))\n-        batch.Put(key + b'_time', to_bytes(str(time())))\n-        self.db.Write(batch)\n-\n-    def _read_data(self, spider, request):\n-        key = self._request_key(request)\n-        try:\n-            ts = self.db.Get(key + b'_time')\n-        except KeyError:\n-            return  # not found or invalid entry\n-\n-        if 0 < self.expiration_secs < time() - float(ts):\n-            return  # expired\n-\n-        try:\n-            data = self.db.Get(key + b'_data')\n-        except KeyError:\n-            return  # invalid entry\n-        else:\n-            return pickle.loads(data)\n-\n-    def _request_key(self, request):\n-        return to_bytes(request_fingerprint(request))\n-\n-\n-\n def parse_cachecontrol(header):\n     \"\"\"Parse Cache-Control header\n \n\n@@ -156,15 +156,6 @@ class FilesystemStorageGzipTest(FilesystemStorageTest):\n         return super(FilesystemStorageTest, self)._get_settings(**new_settings)\n \n \n-class LeveldbStorageTest(DefaultStorageTest):\n-\n-    try:\n-        pytest.importorskip('leveldb')\n-    except SystemError:\n-        pytestmark = pytest.mark.skip(\"Test module skipped - 'SystemError: bad call flags' occurs when >= Python 3.8\")\n-    storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'\n-\n-\n class DummyPolicyTest(_BaseTest):\n \n     policy_class = 'scrapy.extensions.httpcache.DummyPolicy'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b44bd6f8250579dc9ddc25d5e0f10d8b3790f5ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 406 | Files Changed: 6 | Hunks: 8 | Methods Changed: 30 | Complexity Δ (Sum/Max): -44/0 | Churn Δ: 408 | Churn Cumulative: 2689 | Contributors (this commit): 25 | Commits (past 90d): 10 | Contributors (cumulative): 51 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import six\n import pytest\n \n \n@@ -8,7 +7,6 @@ collect_ignore = [\n ]\n \n \n-if six.PY3:\n for line in open('tests/py3-ignores.txt'):\n     file_path = line.strip()\n     if file_path and file_path[0] != '#':\n\n@@ -633,7 +633,8 @@ class Http11MockServerTestCase(unittest.TestCase):\n         # download_maxsize < 100, hence the CancelledError\n         self.assertIsInstance(failure.value, defer.CancelledError)\n \n-        if six.PY2:\n+        # See issue https://twistedmatrix.com/trac/ticket/8175\n+        raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n         request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n         request = request.replace(url=self.mockserver.url('/xpayload'))\n         yield crawler.crawl(seed=request)\n@@ -642,9 +643,6 @@ class Http11MockServerTestCase(unittest.TestCase):\n         self.assertTrue(failure == None)\n         reason = crawler.spider.meta['close_reason']\n         self.assertTrue(reason, 'finished')\n-        else:\n-            # See issue https://twistedmatrix.com/trac/ticket/8175\n-            raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n \n \n class UriResource(resource.Resource):\n\n@@ -1,233 +0,0 @@\n-# -*- coding: utf-8 -*-\n-import unittest\n-from scrapy.linkextractors.regex import RegexLinkExtractor\n-from scrapy.http import HtmlResponse\n-from scrapy.link import Link\n-from scrapy.linkextractors.htmlparser import HtmlParserLinkExtractor\n-from scrapy.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n-from tests import get_testdata\n-\n-from tests.test_linkextractors import Base\n-\n-\n-class BaseSgmlLinkExtractorTestCase(unittest.TestCase):\n-    # XXX: should we move some of these tests to base link extractor tests?\n-\n-    def test_basic(self):\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\">&gt;&gt;</a></p>\n-        <p><a href=\"/\" /></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n-\n-        lx = BaseSgmlLinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='http://example.org/somepage/item/12.html', text='Item 12'),\n-                          Link(url='http://example.org/about.html', text='About us'),\n-                          Link(url='http://example.org/othercat.html', text='Other category'),\n-                          Link(url='http://example.org/', text='>>'),\n-                          Link(url='http://example.org/', text='')])\n-\n-    def test_base_url(self):\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n-\n-        lx = BaseSgmlLinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n-\n-        # base url is an absolute path and relative to host\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p></body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/somepage/index.html\", body=html)\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='https://example.org/item/12.html', text='Item 12')])\n-\n-        # base url has no scheme\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"//noschemedomain.com/path/to/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p></body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/somepage/index.html\", body=html)\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='https://noschemedomain.com/path/to/item/12.html', text='Item 12')])\n-\n-    def test_link_text_wrong_encoding(self):\n-        html = \"\"\"<body><p><a href=\"item/12.html\">Wrong: \\xed</a></p></body></html>\"\"\"\n-        response = HtmlResponse(\"http://www.example.com\", body=html, encoding='utf-8')\n-        lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://www.example.com/item/12.html', text=u'Wrong: \\ufffd'),\n-        ])\n-\n-    def test_extraction_encoding(self):\n-        body = get_testdata('link_extractor', 'linkextractor_noenc.html')\n-        response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n-        response_noenc = HtmlResponse(url='http://example.com/noenc', body=body)\n-        body = get_testdata('link_extractor', 'linkextractor_latin1.html')\n-        response_latin1 = HtmlResponse(url='http://example.com/latin1', body=body)\n-\n-        lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response_utf8), [\n-            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-            Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n-        ])\n-\n-        self.assertEqual(lx.extract_links(response_noenc), [\n-            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-            Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n-        ])\n-\n-        # document encoding does not affect URL path component, only query part\n-        # >>> u'sample_ñ.html'.encode('utf8')\n-        # b'sample_\\xc3\\xb1.html'\n-        # >>> u\"sample_á.html\".encode('utf8')\n-        # b'sample_\\xc3\\xa1.html'\n-        # >>> u\"sample_ö.html\".encode('utf8')\n-        # b'sample_\\xc3\\xb6.html'\n-        # >>> u\"£32\".encode('latin1')\n-        # b'\\xa332'\n-        # >>> u\"µ\".encode('latin1')\n-        # b'\\xb5'\n-        self.assertEqual(lx.extract_links(response_latin1), [\n-            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-            Link(url='http://example.com/sample_%C3%A1.html', text='sample \\xe1 text'.decode('latin1')),\n-            Link(url='http://example.com/sample_%C3%B6.html?price=%A332&%B5=unit', text=''),\n-        ])\n-\n-    def test_matches(self):\n-        url1 = 'http://lotsofstuff.com/stuff1/index'\n-        url2 = 'http://evenmorestuff.com/uglystuff/index'\n-\n-        lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.matches(url1), True)\n-        self.assertEqual(lx.matches(url2), True)\n-\n-\n-class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n-\n-    def setUp(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n-        self.response = HtmlResponse(url='http://example.com/index', body=body)\n-\n-    def test_extraction(self):\n-        # Default arguments\n-        lx = HtmlParserLinkExtractor()\n-        self.assertEqual(lx.extract_links(self.response), [\n-            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-            Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n-            Link(url='http://example.com/sample3.html#foo', text=u'sample 3 repetition with fragment'),\n-            Link(url='http://www.google.com/something', text=u''),\n-            Link(url='http://example.com/innertag.html', text=u'inner tag'),\n-            Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n-        ])\n-\n-    def test_link_wrong_href(self):\n-        html = \"\"\"\n-        <a href=\"http://example.org/item1.html\">Item 1</a>\n-        <a href=\"http://[example.org/item2.html\">Item 2</a>\n-        <a href=\"http://example.org/item3.html\">Item 3</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n-        lx = HtmlParserLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n-            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n-        ])\n-\n-\n-class SgmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n-    extractor_cls = SgmlLinkExtractor\n-    escapes_whitespace = True\n-\n-    def test_deny_extensions(self):\n-        html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n-        response = HtmlResponse(\"http://example.org/\", body=html)\n-        lx = SgmlLinkExtractor(deny_extensions=\"jpg\")\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://example.org/page.html', text=u'asd'),\n-        ])\n-\n-    def test_attrs_sgml(self):\n-        html = \"\"\"<html><area href=\"sample1.html\"></area>\n-        <a ref=\"sample2.html\">sample text 2</a></html>\"\"\"\n-        response = HtmlResponse(\"http://example.com/index.html\", body=html)\n-        lx = SgmlLinkExtractor(attrs=\"href\")\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://example.com/sample1.html', text=u''),\n-        ])\n-\n-    def test_link_nofollow(self):\n-        html = \"\"\"\n-        <a href=\"page.html?action=print\" rel=\"nofollow\">Printer-friendly page</a>\n-        <a href=\"about.html\">About us</a>\n-        <a href=\"http://google.com/something\" rel=\"external nofollow\">Something</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/page.html\", body=html)\n-        lx = SgmlLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n-            Link(url='http://example.org/about.html', text=u'About us', nofollow=False),\n-            Link(url='http://google.com/something', text=u'Something', nofollow=True),\n-        ])\n-\n-\n-class RegexLinkExtractorTestCase(unittest.TestCase):\n-    # XXX: RegexLinkExtractor is not deprecated yet, but it must be rewritten\n-    # not to depend on SgmlLinkExractor. Its speed is also much worse\n-    # than it should be.\n-\n-    def setUp(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n-        self.response = HtmlResponse(url='http://example.com/index', body=body)\n-\n-    def test_extraction(self):\n-        # Default arguments\n-        lx = RegexLinkExtractor()\n-        self.assertEqual(lx.extract_links(self.response),\n-                         [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-                          Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-                          Link(url='http://example.com/sample3.html#foo', text=u'sample 3 repetition with fragment'),\n-                          Link(url='http://www.google.com/something', text=u''),\n-                          Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n-\n-    def test_link_wrong_href(self):\n-        html = \"\"\"\n-        <a href=\"http://example.org/item1.html\">Item 1</a>\n-        <a href=\"http://[example.org/item2.html\">Item 2</a>\n-        <a href=\"http://example.org/item3.html\">Item 3</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n-        lx = RegexLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n-            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n-        ])\n-\n-    def test_html_base_href(self):\n-        html = \"\"\"\n-        <html>\n-            <head>\n-                <base href=\"http://b.com/\">\n-            </head>\n-            <body>\n-                <a href=\"test.html\"></a>\n-            </body>\n-        </html>\n-        \"\"\"\n-        response = HtmlResponse(\"http://a.com/\", body=html)\n-        lx = RegexLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://b.com/test.html', text=u'', nofollow=False),\n-        ])\n-\n-    @unittest.expectedFailure\n-    def test_extraction(self):\n-        # RegexLinkExtractor doesn't parse URLs with leading/trailing\n-        # whitespaces correctly.\n-        super(RegexLinkExtractorTestCase, self).test_extraction()\n\n@@ -1,120 +0,0 @@\n-import json\n-import os\n-import time\n-\n-from six.moves.urllib.parse import urlsplit, urlunsplit\n-from threading import Thread\n-from libmproxy import controller, proxy\n-from netlib import http_auth\n-from testfixtures import LogCapture\n-\n-from twisted.internet import defer\n-from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_crawler\n-from scrapy.http import Request\n-from tests.spiders import SimpleSpider, SingleRequestSpider\n-from tests.mockserver import MockServer\n-\n-\n-class HTTPSProxy(controller.Master, Thread):\n-\n-    def __init__(self):\n-        password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n-        authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n-        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n-            'keys', 'mitmproxy-ca.pem')\n-        server = proxy.ProxyServer(proxy.ProxyConfig(\n-            authenticator = authenticator,\n-            cacert = cert_path),\n-            0)\n-        self.server = server\n-        Thread.__init__(self)\n-        controller.Master.__init__(self, server)\n-\n-    def http_address(self):\n-        return 'http://scrapy:scrapy@%s:%d' % self.server.socket.getsockname()\n-\n-\n-def _wrong_credentials(proxy_url):\n-    bad_auth_proxy = list(urlsplit(proxy_url))\n-    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')\n-    return urlunsplit(bad_auth_proxy)\n-\n-class ProxyConnectTestCase(TestCase):\n-\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n-        self._oldenv = os.environ.copy()\n-\n-        self._proxy = HTTPSProxy()\n-        self._proxy.start()\n-\n-        # Wait for the proxy to start.\n-        time.sleep(1.0)\n-        os.environ['https_proxy'] = self._proxy.http_address()\n-        os.environ['http_proxy'] = self._proxy.http_address()\n-\n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n-        self._proxy.shutdown()\n-        os.environ = self._oldenv\n-\n-    @defer.inlineCallbacks\n-    def test_https_connect_tunnel(self):\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_noconnect(self):\n-        proxy = os.environ['https_proxy']\n-        os.environ['https_proxy'] = proxy + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_connect_tunnel_error(self):\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n-        self._assert_got_tunnel_error(l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_tunnel_auth_error(self):\n-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        # The proxy returns a 407 error code but it does not reach the client;\n-        # he just sees a TunnelError.\n-        self._assert_got_tunnel_error(l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n-        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n-        crawler = get_crawler(SingleRequestSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(seed=request)\n-        self._assert_got_response_code(200, l)\n-        echo = json.loads(crawler.spider.meta['responses'][0].body)\n-        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n-\n-    @defer.inlineCallbacks\n-    def test_https_noconnect_auth_error(self):\n-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(407, l)\n-\n-    def _assert_got_response_code(self, code, log):\n-        print(log)\n-        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n-\n-    def _assert_got_tunnel_error(self, log):\n-        print(log)\n-        self.assertIn('TunnelError', str(log))\n\n@@ -163,33 +163,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n                 gc.collect()\n         self.assertFalse(len(wk._weakdict))\n \n-    @unittest.skipUnless(six.PY2, \"deprecated function\")\n-    def test_stringify_dict(self):\n-        d = {'a': 123, u'b': b'c', u'd': u'e', object(): u'e'}\n-        d2 = stringify_dict(d, keys_only=False)\n-        self.assertEqual(d, d2)\n-        self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n-\n-    @unittest.skipUnless(six.PY2, \"deprecated function\")\n-    def test_stringify_dict_tuples(self):\n-        tuples = [('a', 123), (u'b', 'c'), (u'd', u'e'), (object(), u'e')]\n-        d = dict(tuples)\n-        d2 = stringify_dict(tuples, keys_only=False)\n-        self.assertEqual(d, d2)\n-        self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()), d2.keys())\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n-\n-    @unittest.skipUnless(six.PY2, \"deprecated function\")\n-    def test_stringify_dict_keys_only(self):\n-        d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}\n-        d2 = stringify_dict(d)\n-        self.assertEqual(d, d2)\n-        self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n-\n     def test_get_func_args(self):\n         def f1(a, b, c):\n             pass\n\n@@ -78,26 +78,6 @@ class ParseUrlTestCase(unittest.TestCase):\n                 to_bytes(x) if not isinstance(x, int) else x for x in test)\n             self.assertEqual(client._parse(url), test, url)\n \n-    def test_externalUnicodeInterference(self):\n-        \"\"\"\n-        L{client._parse} should return C{str} for the scheme, host, and path\n-        elements of its return tuple, even when passed an URL which has\n-        previously been passed to L{urlparse} as a C{unicode} string.\n-        \"\"\"\n-        if not six.PY2:\n-            raise unittest.SkipTest(\n-                \"Applies only to Py2, as urls can be ONLY unicode on Py3\")\n-        badInput = u'http://example.com/path'\n-        goodInput = badInput.encode('ascii')\n-        self._parse(badInput)  # cache badInput in urlparse_cached\n-        scheme, netloc, host, port, path = self._parse(goodInput)\n-        self.assertTrue(isinstance(scheme, str))\n-        self.assertTrue(isinstance(netloc, str))\n-        self.assertTrue(isinstance(host, str))\n-        self.assertTrue(isinstance(path, str))\n-        self.assertTrue(isinstance(port, int))\n-\n-\n \n class ScrapyHTTPPageGetterTests(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0d6f4917d782f2396a702e90c36ffbc8bb42e7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 134 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 134 | Churn Cumulative: 379 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,134 @@\n+import json\n+import os\n+import re\n+from subprocess import Popen, PIPE\n+import sys\n+import time\n+\n+from six.moves.urllib.parse import urlsplit, urlunsplit\n+from testfixtures import LogCapture\n+\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy.utils.test import get_crawler\n+from scrapy.http import Request\n+from tests.spiders import SimpleSpider, SingleRequestSpider\n+from tests.mockserver import MockServer\n+\n+\n+class MitmProxy:\n+    auth_user = 'scrapy'\n+    auth_pass = 'scrapy'\n+\n+    def start(self):\n+        from scrapy.utils.test import get_testenv\n+        script = \"\"\"\n+import sys\n+from mitmproxy.tools.main import mitmdump\n+sys.argv[0] = \"mitmdump\"\n+sys.exit(mitmdump())\n+        \"\"\"\n+        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n+            'keys', 'mitmproxy-ca.pem')\n+        self.proc = Popen([sys.executable,\n+                           '-c', script,\n+                           '--listen-host', '127.0.0.1',\n+                           '--listen-port', '0',\n+                           '--proxyauth', '%s:%s' % (self.auth_user, self.auth_pass),\n+                           '--certs', cert_path,\n+                           '--ssl-insecure',\n+                           ],\n+                           stdout=PIPE, env=get_testenv())\n+        line = self.proc.stdout.readline().decode('utf-8')\n+        host_port = re.search(r'listening at http://([^:]+:\\d+)', line).group(1)\n+        address = 'http://%s:%s@%s' % (self.auth_user, self.auth_pass, host_port)\n+        return address\n+\n+    def stop(self):\n+        self.proc.kill()\n+        self.proc.wait()\n+        time.sleep(0.2)\n+\n+\n+def _wrong_credentials(proxy_url):\n+    bad_auth_proxy = list(urlsplit(proxy_url))\n+    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')\n+    return urlunsplit(bad_auth_proxy)\n+\n+\n+class ProxyConnectTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self._oldenv = os.environ.copy()\n+\n+        self._proxy = MitmProxy()\n+        proxy_url = self._proxy.start()\n+        os.environ['https_proxy'] = proxy_url\n+        os.environ['http_proxy'] = proxy_url\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+        self._proxy.stop()\n+        os.environ = self._oldenv\n+\n+    @defer.inlineCallbacks\n+    def test_https_connect_tunnel(self):\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(200, l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_noconnect(self):\n+        proxy = os.environ['https_proxy']\n+        os.environ['https_proxy'] = proxy + '?noconnect'\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(200, l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_connect_tunnel_error(self):\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n+        self._assert_got_tunnel_error(l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_tunnel_auth_error(self):\n+        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        # The proxy returns a 407 error code but it does not reach the client;\n+        # he just sees a TunnelError.\n+        self._assert_got_tunnel_error(l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n+        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(seed=request)\n+        self._assert_got_response_code(200, l)\n+        echo = json.loads(crawler.spider.meta['responses'][0].body)\n+        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n+\n+    @defer.inlineCallbacks\n+    def test_https_noconnect_auth_error(self):\n+        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(407, l)\n+\n+    def _assert_got_response_code(self, code, log):\n+        print(log)\n+        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n+\n+    def _assert_got_tunnel_error(self, log):\n+        print(log)\n+        self.assertIn('TunnelError', str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#439e37fc7b2c192b4591d29dde74c9f589763cc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 383 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ from subprocess import Popen, PIPE\n import sys\n import time\n \n+import pytest\n from six.moves.urllib.parse import urlsplit, urlunsplit\n from testfixtures import LogCapture\n \n@@ -81,6 +82,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n+    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n     @defer.inlineCallbacks\n     def test_https_noconnect(self):\n         proxy = os.environ['https_proxy']\n@@ -90,6 +92,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n+    @pytest.mark.xfail(reason='Python 3 fails this earlier')\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n@@ -117,6 +120,7 @@ class ProxyConnectTestCase(TestCase):\n         echo = json.loads(crawler.spider.meta['responses'][0].body)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n+    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n     @defer.inlineCallbacks\n     def test_https_noconnect_auth_error(self):\n         os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#186f9d88acd9043cf5d8f21358c758c42e433105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1200 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -634,7 +634,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n         self.assertIsInstance(failure.value, defer.CancelledError)\n \n         # See issue https://twistedmatrix.com/trac/ticket/8175\n-        raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n+        raise unittest.SkipTest(\"xpayload fails on PY3\")\n         request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n         request = request.replace(url=self.mockserver.url('/xpayload'))\n         yield crawler.crawl(seed=request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bbd9f4be90e734df5c290aeb399e8e512f834224", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 172 | Files Changed: 28 | Hunks: 58 | Methods Changed: 23 | Complexity Δ (Sum/Max): -27/8 | Churn Δ: 192 | Churn Cumulative: 11313 | Contributors (this commit): 104 | Commits (past 90d): 106 | Contributors (cumulative): 332 | DMM Complexity: 0.639344262295082\n\nDIFF:\n@@ -1,16 +1,6 @@\n-import six\n from six.moves import copyreg\n \n \n-if six.PY2:\n-    from urlparse import urlparse\n-    \n-    # workaround for https://bugs.python.org/issue9374 - Python < 2.7.4\n-    if urlparse('s3://bucket/key?key=value').query != 'key=value':\n-        from urlparse import uses_query\n-        uses_query.append('s3')\n-\n-\n # Undo what Twisted's perspective broker adds to pickle register\n # to prevent bugs like Twisted#7989 while serializing requests\n import twisted.persisted.styles  # NOQA\n\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-import sys, six\n+import sys\n from w3lib.url import is_url\n \n from scrapy.commands import ScrapyCommand\n@@ -45,8 +45,7 @@ class Command(ScrapyCommand):\n             self._print_bytes(response.body)\n \n     def _print_bytes(self, bytes_):\n-        bytes_writer = sys.stdout if six.PY2 else sys.stdout.buffer\n-        bytes_writer.write(bytes_ + b'\\n')\n+        sys.stdout.buffer.write(bytes_ + b'\\n')\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n\n@@ -88,20 +88,9 @@ class Crawler(object):\n             yield self.engine.open_spider(self.spider, start_requests)\n             yield defer.maybeDeferred(self.engine.start)\n         except Exception:\n-            # In Python 2 reraising an exception after yield discards\n-            # the original traceback (see https://bugs.python.org/issue7563),\n-            # so sys.exc_info() workaround is used.\n-            # This workaround also works in Python 3, but it is not needed,\n-            # and it is slower, so in Python 3 we use native `raise`.\n-            if six.PY2:\n-                exc_info = sys.exc_info()\n-\n             self.crawling = False\n             if self.engine is not None:\n                 yield self.engine.close()\n-\n-            if six.PY2:\n-                six.reraise(*exc_info)\n             raise\n \n     def _create_spider(self, *args, **kwargs):\n\n@@ -216,7 +216,7 @@ class CsvItemExporter(BaseItemExporter):\n             write_through=True,\n             encoding=self.encoding,\n             newline='' # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n-        ) if six.PY3 else file\n+        )\n         self.csv_writer = csv.writer(self.stream, **kwargs)\n         self._headers_not_written = True\n         self._join_multivalued = join_multivalued\n\n@@ -10,7 +10,6 @@ import logging\n import posixpath\n from tempfile import NamedTemporaryFile\n from datetime import datetime\n-import six\n from six.moves.urllib.parse import urlparse, unquote\n from ftplib import FTP\n \n@@ -65,7 +64,7 @@ class StdoutFeedStorage(object):\n \n     def __init__(self, uri, _stdout=None):\n         if not _stdout:\n-            _stdout = sys.stdout if six.PY2 else sys.stdout.buffer\n+            _stdout = sys.stdout.buffer\n         self._stdout = _stdout\n \n     def open(self, spider):\n\n@@ -104,8 +104,7 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n                 el = el.getparent()\n                 if el is None:\n                     break\n-        encoded = formxpath if six.PY3 else formxpath.encode('unicode_escape')\n-        raise ValueError('No <form> element found with %s' % encoded)\n+        raise ValueError('No <form> element found with %s' % formxpath)\n \n     # If we get here, it means that either formname was None\n     # or invalid\n\n@@ -32,9 +32,6 @@ class TextResponse(Response):\n \n     def _set_url(self, url):\n         if isinstance(url, six.text_type):\n-            if six.PY2 and self.encoding is None:\n-                raise TypeError(\"Cannot convert unicode url - %s \"\n-                                \"has no encoding\" % type(self).__name__)\n             self._url = to_native_str(url, self.encoding)\n         else:\n             super(TextResponse, self)._set_url(url)\n\n@@ -4,8 +4,8 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n-import collections\n from abc import ABCMeta\n+from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n from warnings import warn\n@@ -16,12 +16,6 @@ from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n \n-if six.PY2:\n-    MutableMapping = collections.MutableMapping\n-else:\n-    MutableMapping = collections.abc.MutableMapping\n-\n-\n class BaseItem(object_ref):\n     \"\"\"Base class for all scraped items.\n \n\n@@ -4,12 +4,6 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n-import warnings\n-import six\n-\n-from scrapy.utils.python import to_bytes\n-\n-\n class Link(object):\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n \n@@ -17,11 +11,6 @@ class Link(object):\n \n     def __init__(self, url, text='', fragment='', nofollow=False):\n         if not isinstance(url, str):\n-            if six.PY2:\n-                warnings.warn(\"Link urls must be str objects. \"\n-                              \"Assuming utf-8 encoding (which could be wrong)\")\n-                url = to_bytes(url, encoding='utf8')\n-            else:\n             got = url.__class__.__name__\n             raise TypeError(\"Link urls must be str objects, got %s\" % got)\n         self.url = url\n\n@@ -9,16 +9,11 @@ try:\n     from cStringIO import StringIO as BytesIO\n except ImportError:\n     from io import BytesIO\n-import six\n \n from email.utils import COMMASPACE, formatdate\n from six.moves.email_mime_multipart import MIMEMultipart\n from six.moves.email_mime_text import MIMEText\n from six.moves.email_mime_base import MIMEBase\n-if six.PY2:\n-    from email.MIMENonMultipart import MIMENonMultipart\n-    from email import Encoders\n-else:\n from email.mime.nonmultipart import MIMENonMultipart\n from email import encoders as Encoders\n \n\n@@ -1,19 +1,13 @@\n import six\n import json\n import copy\n-import collections\n+from collections.abc import MutableMapping\n from importlib import import_module\n from pprint import pformat\n \n from scrapy.settings import default_settings\n \n \n-if six.PY2:\n-    MutableMapping = collections.MutableMapping\n-else:\n-    MutableMapping = collections.abc.MutableMapping\n-\n-\n SETTINGS_PRIORITIES = {\n     'default': 0,\n     'command': 10,\n\n@@ -17,8 +17,6 @@ import sys\n from importlib import import_module\n from os.path import join, abspath, dirname\n \n-import six\n-\n AJAXCRAWL_ENABLED = False\n \n AUTOTHROTTLE_ENABLED = False\n@@ -179,7 +177,7 @@ HTTPCACHE_ALWAYS_STORE = False\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n-HTTPCACHE_DBM_MODULE = 'anydbm' if six.PY2 else 'dbm'\n+HTTPCACHE_DBM_MODULE = 'dbm'\n HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n HTTPCACHE_GZIP = False\n \n\n@@ -1,7 +1,6 @@\n \"\"\"Boto/botocore helpers\"\"\"\n \n from __future__ import absolute_import\n-import six\n \n from scrapy.exceptions import NotConfigured\n \n@@ -11,11 +10,4 @@ def is_botocore():\n         import botocore\n         return True\n     except ImportError:\n-        if six.PY2:\n-            try:\n-                import boto\n-                return False\n-            except ImportError:\n-                raise NotConfigured('missing botocore or boto library')\n-        else:\n         raise NotConfigured('missing botocore library')\n\n@@ -1,13 +1,10 @@\n+from configparser import ConfigParser\n import os\n import sys\n import numbers\n from operator import itemgetter\n \n import six\n-if six.PY2:\n-    from ConfigParser import SafeConfigParser as ConfigParser\n-else:\n-    from configparser import ConfigParser\n \n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n\n@@ -7,6 +7,7 @@ This module must not depend on any module outside the Standard Library.\n \n import copy\n import collections\n+from collections.abc import Mapping\n import warnings\n \n import six\n@@ -14,12 +15,6 @@ import six\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-if six.PY2:\n-    Mapping = collections.Mapping\n-else:\n-    Mapping = collections.abc.Mapping\n-\n-\n class MultiValueDictKeyError(KeyError):\n     def __init__(self, *args, **kwargs):\n         warnings.warn(\n@@ -252,7 +247,6 @@ class MergeDict(object):\n     first occurrence will be used.\n     \"\"\"\n     def __init__(self, *dicts):\n-        if not six.PY2:\n         warnings.warn(\n             \"scrapy.utils.datatypes.MergeDict is deprecated in favor \"\n             \"of collections.ChainMap (introduced in Python 3.3)\",\n\n@@ -6,7 +6,6 @@ except ImportError:\n     from io import BytesIO\n from gzip import GzipFile\n \n-import six\n import re\n \n from scrapy.utils.decorators import deprecated\n@@ -17,12 +16,6 @@ from scrapy.utils.decorators import deprecated\n #   (regression or bug-fix compared to Python 3.4)\n # - read1(), which fetches data before raising EOFError on next call\n #   works here but is only available from Python>=3.3\n-# - scrapy does not support Python 3.2\n-# - Python 2.7 GzipFile works fine with standard read() + extrabuf\n-if six.PY2:\n-    def read1(gzf, size=-1):\n-        return gzf.read(size)\n-else:\n def read1(gzf, size=-1):\n     return gzf.read1(size)\n \n@@ -37,7 +30,7 @@ def gunzip(data):\n     chunk = b'.'\n     while chunk:\n         try:\n-            chunk = read1(f, 8196)\n+            chunk = f.read1(8196)\n             output_list.append(chunk)\n         except (IOError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n\n@@ -102,11 +102,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     def row_to_unicode(row_):\n         return [to_unicode(field, encoding) for field in row_]\n \n-    # Python 3 csv reader input object needs to return strings\n-    if six.PY3:\n     lines = StringIO(_body_or_str(obj, unicode=True))\n-    else:\n-        lines = BytesIO(_body_or_str(obj, unicode=False))\n \n     kwargs = {}\n     if delimiter: kwargs[\"delimiter\"] = delimiter\n\n@@ -113,9 +113,6 @@ def to_bytes(text, encoding=None, errors='strict'):\n def to_native_str(text, encoding=None, errors='strict'):\n     \"\"\" Return str representation of ``text``\n     (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n-    if six.PY2:\n-        return to_bytes(text, encoding, errors)\n-    else:\n     return to_unicode(text, encoding, errors)\n \n \n@@ -189,7 +186,7 @@ def _getargspec_py23(func):\n     \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n                                                         defaults)\n \n-    Identical to inspect.getargspec() in python2, but uses\n+    Was identical to inspect.getargspec() in python2, but uses\n     inspect.getfullargspec() for python3 behind the scenes to avoid\n     DeprecationWarning.\n \n@@ -199,9 +196,6 @@ def _getargspec_py23(func):\n     >>> _getargspec_py23(f)\n     ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n     \"\"\"\n-    if six.PY2:\n-        return inspect.getargspec(func)\n-\n     return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])\n \n \n\n@@ -35,18 +35,3 @@ def get_testdata(*paths):\n     path = os.path.join(tests_datadir, *paths)\n     with open(path, 'rb') as f:\n         return f.read()\n-\n-\n-# FIXME: delete after dropping py2 support\n-# Monkey patch the unittest module to prevent the\n-# DeprecationWarning about assertRaisesRegexp -> assertRaisesRegex\n-import six\n-if six.PY2:\n-    import unittest\n-    import twisted.trial.unittest\n-    if not getattr(unittest.TestCase, 'assertRegex', None):\n-        unittest.TestCase.assertRegex = unittest.TestCase.assertRegexpMatches\n-    if not getattr(unittest.TestCase, 'assertRaisesRegex', None):\n-        unittest.TestCase.assertRaisesRegex = unittest.TestCase.assertRaisesRegexp\n-    if not getattr(twisted.trial.unittest.TestCase, 'assertRaisesRegex', None):\n-        twisted.trial.unittest.TestCase.assertRaisesRegex = twisted.trial.unittest.TestCase.assertRaisesRegexp\n\n@@ -3,13 +3,12 @@ import cgi\n import unittest\n import re\n import json\n+from urllib.parse import unquote_to_bytes\n import warnings\n \n import six\n from six.moves import xmlrpc_client as xmlrpclib\n from six.moves.urllib.parse import urlparse, parse_qs, unquote\n-if six.PY3:\n-    from urllib.parse import unquote_to_bytes\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_native_str\n@@ -1064,8 +1063,7 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs, {})\n \n         xpath = u\"//form[@name='\\u03b1']\"\n-        encoded = xpath if six.PY3 else xpath.encode('unicode_escape')\n-        self.assertRaisesRegex(ValueError, re.escape(encoded),\n+        self.assertRaisesRegex(ValueError, re.escape(xpath),\n                                self.request_class.from_response,\n                                response, formxpath=xpath)\n \n@@ -1208,9 +1206,6 @@ def _qs(req, encoding='utf-8', to_unicode=False):\n         qs = req.body\n     else:\n         qs = req.url.partition('?')[2]\n-    if six.PY2:\n-        uqs = unquote(to_native_str(qs, encoding))\n-    elif six.PY3:\n     uqs = unquote_to_bytes(qs)\n     if to_unicode:\n         uqs = uqs.decode(encoding)\n\n@@ -21,7 +21,6 @@ class BaseResponseTest(unittest.TestCase):\n         # Response requires url in the consturctor\n         self.assertRaises(Exception, self.response_class)\n         self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))\n-        if not six.PY2:\n         self.assertRaises(TypeError, self.response_class, b\"http://example.com\")\n         # body can be str or None\n         self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))\n\n@@ -62,10 +62,6 @@ class ItemTest(unittest.TestCase):\n         i['number'] = 123\n         itemrepr = repr(i)\n \n-        if six.PY2:\n-            self.assertEqual(itemrepr,\n-                             \"{'name': u'John Doe', 'number': 123}\")\n-        else:\n         self.assertEqual(itemrepr,\n                          \"{'name': 'John Doe', 'number': 123}\")\n \n\n@@ -1,6 +1,4 @@\n import unittest\n-import warnings\n-import six\n \n from scrapy.link import Link\n \n@@ -46,12 +44,5 @@ class LinkTest(unittest.TestCase):\n         self._assert_same_links(l1, l2)\n \n     def test_non_str_url_py2(self):\n-        if six.PY2:\n-            with warnings.catch_warnings(record=True) as w:\n-                link = Link(u\"http://www.example.com/\\xa3\")\n-                self.assertIsInstance(link.url, str)\n-                self.assertEqual(link.url, b'http://www.example.com/\\xc2\\xa3')\n-            assert len(w) == 1, \"warning not issued\"\n-        else:\n         with self.assertRaises(TypeError):\n             Link(b\"http://www.example.com/\\xc2\\xa3\")\n\n@@ -3,7 +3,6 @@ from twisted.trial import unittest\n from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-import six\n \n class M1(object):\n \n@@ -66,14 +65,6 @@ class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_methods(self):\n         mwman = TestMiddlewareManager(M1(), M2(), M3())\n-        if six.PY2:\n-            self.assertEqual([x.im_class for x in mwman.methods['open_spider']],\n-                [M1, M2])\n-            self.assertEqual([x.im_class for x in mwman.methods['close_spider']],\n-                [M2, M1])\n-            self.assertEqual([x.im_class for x in mwman.methods['process']],\n-                [M1, M3])\n-        else:\n         self.assertEqual([x.__self__.__class__ for x in mwman.methods['open_spider']],\n             [M1, M2])\n         self.assertEqual([x.__self__.__class__ for x in mwman.methods['close_spider']],\n\n@@ -1,7 +1,6 @@\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-import six\n \n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n@@ -161,9 +160,4 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n         self.assertEqual(exceptions['takes_less'].exc_info[0], TypeError)\n         self.assertEqual(str(exceptions['takes_less'].exc_info[1]), \"parse_takes_less() got an unexpected keyword argument 'number'\")\n         self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)\n-        # py2 and py3 messages are different\n-        exc_message = str(exceptions['takes_more'].exc_info[1])\n-        if six.PY2:\n-            self.assertEqual(exc_message, \"parse_takes_more() takes exactly 5 arguments (4 given)\")\n-        elif six.PY3:\n-            self.assertEqual(exc_message, \"parse_takes_more() missing 1 required positional argument: 'other'\")\n+        self.assertEqual(str(exceptions['takes_more'].exc_info[1]), \"parse_takes_more() missing 1 required positional argument: 'other'\")\n\n@@ -60,9 +60,6 @@ class SettingsAttributeTest(unittest.TestCase):\n \n class BaseSettingsTest(unittest.TestCase):\n \n-    if six.PY3:\n-        assertItemsEqual = unittest.TestCase.assertCountEqual\n-\n     def setUp(self):\n         self.settings = BaseSettings()\n \n@@ -152,7 +149,7 @@ class BaseSettingsTest(unittest.TestCase):\n         self.settings.setmodule(\n             'tests.test_settings.default_settings', 10)\n \n-        self.assertItemsEqual(six.iterkeys(self.settings.attributes),\n+        self.assertCountEqual(six.iterkeys(self.settings.attributes),\n                               six.iterkeys(ctrl_attributes))\n \n         for key in six.iterkeys(ctrl_attributes):\n@@ -343,9 +340,6 @@ class BaseSettingsTest(unittest.TestCase):\n \n class SettingsTest(unittest.TestCase):\n \n-    if six.PY3:\n-        assertItemsEqual = unittest.TestCase.assertCountEqual\n-\n     def setUp(self):\n         self.settings = Settings()\n \n\n@@ -1,12 +1,7 @@\n+from collections.abc import Mapping, MutableMapping\n import copy\n import unittest\n \n-import six\n-if six.PY2:\n-    from collections import Mapping, MutableMapping\n-else:\n-    from collections.abc import Mapping, MutableMapping\n-\n from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n \n \n\n@@ -231,12 +231,11 @@ class UtilsPythonTestCase(unittest.TestCase):\n             self.assertEqual(get_func_args(\" \".join), [])\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         else:\n-            stripself = not six.PY2  # PyPy3 exposes them as methods\n             self.assertEqual(\n-                get_func_args(six.text_type.split, stripself), ['sep', 'maxsplit'])\n-            self.assertEqual(get_func_args(\" \".join, stripself), ['list'])\n+                get_func_args(six.text_type.split, True), ['sep', 'maxsplit'])\n+            self.assertEqual(get_func_args(\" \".join, True), ['list'])\n             self.assertEqual(\n-                get_func_args(operator.itemgetter(2), stripself), ['obj'])\n+                get_func_args(operator.itemgetter(2), True), ['obj'])\n \n \n     def test_without_none_values(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
