{"custom_id": "scrapy#269724a2b7c726be7a441ef829c7a958e44d5e44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 16 | Churn Cumulative: 244 | Contributors (this commit): 4 | Commits (past 90d): 39 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,7 +84,6 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.memusage.MemoryUsage': 0,\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n     'scrapy.contrib.closedomain.CloseDomain': 0,\n-    'scrapy.contrib.debug.StackTraceDump': 0,\n }\n \n GROUPSETTINGS_ENABLED = False\n\n@@ -3,8 +3,11 @@ Extensions for debugging Scrapy\n \n See documentation in docs/topics/extensions.rst\n \"\"\"\n+\n import signal\n import traceback\n+from pdb import Pdb\n+\n \n class StackTraceDump(object):\n     def __init__(self):\n@@ -17,3 +20,15 @@ class StackTraceDump(object):\n     def dump_stacktrace(self, signum, frame):\n         print \"Got signal. Dumping stack trace...\"\n         traceback.print_stack(frame)\n+\n+\n+class Debugger(object):\n+    def __init__(self):\n+        try:\n+            signal.signal(signal.SIGUSR2, self._enter_debugger)\n+        except AttributeError:\n+            # win32 platforms don't support SIGUSR signals\n+            pass\n+\n+    def _enter_debugger(self, signum, frame):\n+        Pdb().set_trace(frame.f_back)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#734464825b61f7853b23bc32df68704df84a38db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 291 | Contributors (this commit): 3 | Commits (past 90d): 40 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -49,6 +49,8 @@ DOWNLOAD_TIMEOUT = 180      # 3mins\n \n DOWNLOADER_DEBUG = False\n \n+DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n+\n DOWNLOADER_MIDDLEWARES = {}\n \n DOWNLOADER_MIDDLEWARES_BASE = {\n\n@@ -18,11 +18,12 @@ from scrapy.core.exceptions import NotSupported\n from scrapy.utils.defer import defer_succeed\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.signal import send_catch_log\n+from scrapy.utils.misc import load_object\n from scrapy.core.downloader.responsetypes import responsetypes\n-from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory\n from scrapy.conf import settings\n \n \n+HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n ssl_supported = 'ssl' in optional_features\n \n@@ -44,7 +45,7 @@ def create_factory(request, spider):\n     \"\"\"Return HTTPClientFactory for the given Request\"\"\"\n     url = urlparse.urldefrag(request.url)[0]\n     timeout = getattr(spider, \"download_timeout\", None) or default_timeout\n-    factory = ScrapyHTTPClientFactory.from_request(request, timeout)\n+    factory = HTTPClientFactory.from_request(request, timeout)\n \n     def _create_response(body):\n         body = body or ''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f1bb8dc2a348e296584dd6cfe20e90cab9322ae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 54 | Files Changed: 6 | Hunks: 18 | Methods Changed: 13 | Complexity Δ (Sum/Max): -2/8 | Churn Δ: 67 | Churn Cumulative: 614 | Contributors (this commit): 3 | Commits (past 90d): 63 | Contributors (cumulative): 11 | DMM Complexity: 0.7931034482758621\n\nDIFF:\n@@ -9,4 +9,4 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        print \"\\n\".join(spiders.asdict().keys())\n+        print \"\\n\".join(spiders.list())\n\n@@ -57,8 +57,6 @@ class ScrapyCommand(object):\n             help=\"log level (default: %s)\" % settings['LOGLEVEL'])\n         group.add_option(\"--nolog\", action=\"store_true\", dest=\"nolog\", \\\n             help=\"disable logging completely\")\n-        group.add_option(\"--default-spider\", dest=\"default_spider\", default=None, \\\n-            help=\"use this spider when arguments are urls and no spider is found\")\n         group.add_option(\"--spider\", dest=\"spider\", default=None, \\\n             help=\"always use this spider when arguments are urls\")\n         group.add_option(\"--profile\", dest=\"profile\", metavar=\"FILE\", default=None, \\\n@@ -101,10 +99,6 @@ class ScrapyCommand(object):\n         if opts.nolog:\n             settings.overrides['LOG_ENABLED'] = False\n \n-        if opts.default_spider:\n-            from scrapy.spider import spiders\n-            spiders.default_domain = opts.default_spider\n-            \n         if opts.spider:\n             from scrapy.spider import spiders\n             spiders.force_domain = opts.spider\n\n@@ -37,8 +37,6 @@ DEFAULT_REQUEST_HEADERS = {\n     'Accept-Language': 'en',\n }\n \n-DEFAULT_SPIDER = None\n-\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n\n@@ -13,44 +13,35 @@ from scrapy.spider.models import ISpider\n from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.url import url_is_from_spider\n-from scrapy.utils.misc import load_object\n \n class TwistedPluginSpiderManager(object):\n     \"\"\"Spider locator and manager\"\"\"\n \n     def __init__(self):\n         self.loaded = False\n-        self.default_domain = None\n         self.force_domain = None\n-        self.spider_modules = None\n+        self._invaliddict = {}\n+        self._spiders = {}\n \n     def fromdomain(self, domain):\n-        return self.asdict().get(domain)\n+        return self._spiders.get(domain)\n \n     def fromurl(self, url):\n         if self.force_domain:\n-            return self.asdict().get(self.force_domain)\n+            return self._spiders.get(self.force_domain)\n         domain = urlparse.urlparse(url).hostname\n         domain = str(domain).replace('www.', '')\n         if domain:\n-            if domain in self.asdict():         # try first locating by domain\n-                return self.asdict()[domain]\n+            if domain in self._spiders:         # try first locating by domain\n+                return self._spiders[domain]\n             else:                               # else search spider by spider\n-                plist = self.asdict().values()\n+                plist = self._spiders.values()\n                 for p in plist:\n                     if url_is_from_spider(url, p):\n                         return p\n-        spider = self.asdict().get(self.default_domain)\n-        if not spider:                          # create a custom spider\n-            spiderclassname = settings.get('DEFAULT_SPIDER')\n-            if spiderclassname:\n-                spider = load_object(spiderclassname)(domain)\n-                self.add_spider(spider)\n \n-        return spider\n-\n-    def asdict(self):\n-        return self._spiders\n+    def list(self):\n+        return self._spiders.keys()\n \n     def load(self, spider_modules=None):\n         if spider_modules is None:\n@@ -66,32 +57,8 @@ class TwistedPluginSpiderManager(object):\n         self.loaded = True\n \n     def add_spider(self, spider):\n-        try:\n         ISpider.validateInvariants(spider)\n         self._spiders[spider.domain_name] = spider\n-        except Exception, e:\n-            self._invaliddict[spider.domain_name] = spider\n-            # we can't use the log module here because it may not be available yet\n-            print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n-\n-    def reload(self, spider_modules=None, skip_domains=None):\n-        \"\"\"Reload spiders by trying to discover any spiders added under the\n-        spiders module/packages, removes any spiders removed.\n-\n-        If skip_domains is passed those spiders won't be reloaded.\n-        \"\"\"\n-        skip_domains = set(skip_domains or [])\n-        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n-        for m in modules:\n-            reload(m)\n-        reloaded = 0\n-        pdict = self.asdict()\n-        for domain, spider in pdict.iteritems():\n-            if not domain in skip_domains:\n-                reload(sys.modules[spider.__module__])\n-                reloaded += 1\n-        self.load(spider_modules=spider_modules)  # second call to update spider instances\n-        log.msg(\"Reloaded %d/%d scrapy spiders\" % (reloaded, len(pdict)), level=log.DEBUG)\n \n     def _getspiders(self, interface, package):\n         \"\"\"This is an override of twisted.plugin.getPlugin, because we're\n\n@@ -35,7 +35,7 @@ class Spiderctl(object):\n         if wc_request.args:\n             changes = self.webconsole_control(wc_request)\n \n-        enabled_domains = spiders.asdict().keys()\n+        enabled_domains = spiders.list()\n         self.scheduled = scrapyengine.domain_scheduler.pending_domains\n         self.idle = [d for d in enabled_domains if d not in self.scheduled\n                                                         and d not in self.running\n@@ -139,7 +139,7 @@ class Spiderctl(object):\n \n     def webconsole_control(self, wc_request):\n         args = wc_request.args\n-        enabled_domains = spiders.asdict().keys()\n+        enabled_domains = spiders.list()\n         s = \"<hr />\\n\"\n \n         if \"stop_running_domains\" in args:\n\n@@ -43,7 +43,7 @@ def get_or_create_spider(url):\n     # XXX: hack to allow downloading pages from unknown domains\n     spider = spiders.fromurl(url)\n     if not spider:\n-        domain = str(urlparse.urlparse(url).hostname or spiders.default_domain)\n+        domain = urlparse.urlparse(url).hostname\n         spider = BaseSpider()\n         spider.domain_name = domain\n         spiders.add_spider(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0174bee4bc81f7d7e674c2e403ec94b5d1578481", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 33 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 45 | Churn Cumulative: 47 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,49 +1,28 @@\n-import urlparse\n+from urlparse import urlparse\n \n from scrapy.spider import spiders\n from scrapy.http import Request\n from scrapy.core.manager import scrapymanager\n from scrapy.spider import BaseSpider\n \n-def fetch(urls, perdomain=False):\n-    \"\"\"Download a set of urls. This is starts a reactor and is suitable\n-    for calling from a main method - not for calling from within the \n-    framework.\n+def fetch(urls):\n+    \"\"\"Download the given urls and return a list of the successfully downloaded\n+    responses.\n \n-    It will return a list of Response objects for all pages successfully\n-    downloaded.\n+    Suitable for for calling from a script, shouldn't be called from spiders.\n     \"\"\"\n-    bigbucket = {} # big bucket to store response objects (per domain)\n-\n-    def _add(response, domain):\n-        if not domain in bigbucket:\n-            bigbucket[domain] = []\n-        bigbucket[domain] += [response]\n-\n-    # url clasification by domain\n-    requestdata = set()\n-    for url in urls:\n-        domain = get_or_create_spider(url).domain_name\n-        request = Request(url=url, callback=lambda r: _add(r, domain), dont_filter=True)\n-        requestdata.add(request)\n-\n-    scrapymanager.runonce(*requestdata)\n-\n-    # returns bigbucket as dict or flatted\n-    if perdomain:\n-        return bigbucket\n-\n-    flatbucket = []\n-    for domain, responses in bigbucket.iteritems():\n-        flatbucket.extend(responses)\n-    return flatbucket\n-\n+    map(get_or_create_spider, urls)\n+    responses = []\n+    requests = [Request(url, callback=responses.append, dont_filter=True) \\\n+        for url in urls]\n+    scrapymanager.runonce(*requests)\n+    return responses\n \n def get_or_create_spider(url):\n     # XXX: hack to allow downloading pages from unknown domains\n     spider = spiders.fromurl(url)\n     if not spider:\n-        domain = urlparse.urlparse(url).hostname\n+        domain = urlparse(url).hostname\n         spider = BaseSpider()\n         spider.domain_name = domain\n         spiders.add_spider(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96bc6780d331f13b4502ed0af446fd06d1bb2ddf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 349 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -220,7 +220,7 @@ class ImagesPipeline(MediaPipeline):\n             log.msg(str(ex), level=log.WARNING, domain=info.domain)\n             raise ex\n \n-        return {'scraped_url': request.url, 'path': key, 'checksum': checksum}\n+        return {'url': request.url, 'path': key, 'checksum': checksum}\n \n     def media_failed(self, failure, request, info):\n         referer = request.headers.get('Referer')\n@@ -249,7 +249,7 @@ class ImagesPipeline(MediaPipeline):\n             self.inc_stats(info.domain, 'uptodate')\n \n             checksum = result.get('checksum', None)\n-            return {'scraped_url': request.url, 'path': key, 'checksum': checksum}\n+            return {'url': request.url, 'path': key, 'checksum': checksum}\n \n         key = self.image_key(request.url)\n         dfd = defer.maybeDeferred(self.store.stat_image, key, info)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d49dc2fb53e413e36d33505e05a3c2c33217d38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 353 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -170,7 +170,7 @@ class ImagesPipeline(MediaPipeline):\n     MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n     MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n     EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n-    THUMBS = settings.getlist('IMAGES_THUMBS')\n+    THUMBS = settings['IMAGES_THUMBS']\n     STORE_SCHEMES = {\n             '': FSImagesStore,\n             'file': FSImagesStore,\n@@ -278,7 +278,7 @@ class ImagesPipeline(MediaPipeline):\n         image, buf = self.convert_image(orig_image)\n         yield key, image, buf\n \n-        for thumb_id, size in self.THUMBS or []:\n+        for thumb_id, size in self.THUMBS.iteritems() or []:\n             thumb_key = self.thumb_key(request.url, thumb_id)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_key, thumb_image, thumb_buf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e854d0d6ef0d7a27910bbc2d56462509cff50acc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,8 +19,6 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", \\\n-            help=\"disable scraped item pipeline\")\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n             help=\"don't follow links (for use with URLs only)\")\n         parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", \\\n@@ -28,9 +26,6 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n-        if opts.nopipeline:\n-            settings.overrides['ITEM_PIPELINES'] = []\n-\n         if opts.nofollow:\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c292e335095c92da25311b1801499d7c7d15a7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 30 | Churn Cumulative: 77 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,8 +21,6 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n             help=\"don't follow links (for use with URLs only)\")\n-        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", \\\n-            help=\"use the provided callback for starting to crawl the given url\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -30,32 +28,4 @@ class Command(ScrapyCommand):\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n-        if opts.callback:\n-            requests = []\n-            for a in args:\n-                if is_url(a):\n-                    spider = spiders.fromurl(a)\n-                    urls = [a]\n-                else:\n-                    spider = spiders.fromdomain(a)\n-                    urls = spider.start_urls if hasattr(spider.start_urls, \\\n-                        '__iter__') else [spider.start_urls]\n-\n-                if spider:\n-                    if hasattr(spider, opts.callback):\n-                        requests.extend(Request(url=url, callback=getattr(spider, \\\n-                            opts.callback)) for url in urls)\n-                    else:\n-                        log.msg('Callback %s doesnt exist in spider %s' % \\\n-                            (opts.callback, spider.domain_name), log.ERROR)\n-                else:\n-                    log.msg('Cannot find spider for %s' % a, log.ERROR)\n-\n-            if requests:\n-                args = requests\n-            else:\n-                log.msg('Cannot create any requests with the provided arguments', \\\n-                    log.ERROR)\n-                return\n-\n         scrapymanager.runonce(*args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#655cfe138d6194c75a392ca5aadbbf11267bf5a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 81 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,6 @@\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n-from scrapy.utils.url import is_url\n-from scrapy.spider import spiders\n-from scrapy.http import Request\n-from scrapy import log\n \n \n class Command(ScrapyCommand):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1381c1e50acf01eb87f61cd829885d3122b36695", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 20 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -54,14 +54,6 @@ if root_dir != '':\n     os.chdir(root_dir)\n scrapy_dir = 'scrapy'\n \n-# Generate dropin.cache for spiders under tests\n-# if dropin.cache is missing, running tests under python site-packages fails with permission errors\n-import os\n-os.environ['SCRAPY_SETTINGS_DISABLED']='1'\n-from twisted.plugin import getPlugins, IPlugin\n-import scrapy.tests.test_spiders\n-list(getPlugins(IPlugin, scrapy.tests.test_spiders))\n-\n def is_not_module(filename):\n     return os.path.splitext(f)[1] not in ['.py', '.pyc', '.pyo']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f30fee6ea5403b86f9bced54ac48af39f54457a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 13/8 | Churn Δ: 117 | Churn Cumulative: 117 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,51 @@\n+\"\"\"\n+Simple script to follow links from a start url. The links are followed in no\n+particular order.\n+\n+Usage:\n+count_and_follow_links.py <start_url> <links_to_follow>\n+\n+Example:\n+count_and_follow_links.py http://scrapy.org/ 20\n+\n+For each page visisted, this script will print the page body size and the\n+number of links found.\n+\"\"\"\n+\n+import sys\n+from urlparse import urljoin\n+\n+from scrapy.crawler import Crawler\n+from scrapy.selector import HtmlXPathSelector\n+from scrapy.http import Request, HtmlResponse\n+\n+links_followed = 0\n+\n+def parse(response):\n+    global links_followed\n+    links_followed += 1\n+    if links_followed >= links_to_follow:\n+        crawler.stop()\n+\n+    # ignore non-HTML responses\n+    if not isinstance(response, HtmlResponse):\n+        return\n+\n+    links = HtmlXPathSelector(response).select('//a/@href').extract()\n+    abslinks = [urljoin(response.url, l) for l in links]\n+\n+    print \"page %2d/%d: %s\" % (links_followed, links_to_follow, response.url)\n+    print \"  size : %d bytes\" % len(response.body)\n+    print \"  links: %d\" % len(links)\n+    print\n+\n+    return [Request(l, callback=parse) for l in abslinks]\n+\n+if len(sys.argv) != 3:\n+    print __doc__\n+    sys.exit(2)\n+\n+start_url, links_to_follow = sys.argv[1], int(sys.argv[2])\n+request = Request(start_url, callback=parse)\n+crawler = Crawler()\n+crawler.crawl(request)\n\n@@ -0,0 +1,66 @@\n+\"\"\"\n+Crawler class\n+\n+The Crawler class can be used to crawl pages using the Scrapy crawler from\n+outside a Scrapy project, for example, from a standalone script. \n+\n+To use it, instantiate it and call the \"crawl\" method with one (or more)\n+requests. For example:\n+\n+    >>> from scrapy.crawler import Crawler\n+    >>> from scrapy.http import Request\n+    >>> def parse_response(response):\n+    ...     print \"Visited: %s\" % response.url\n+    ...\n+    >>> request = Request('http://scrapy.org', callback=parse_response)\n+    >>> crawler = Crawler()\n+    >>> crawler.crawl(request)\n+    Visited http://scrapy.org \n+    >>>\n+\n+Request callbacks follow the same API of spiders callback, which means that all\n+requests returned from the callbacks will be followed.\n+\n+See examples/scripts/count_and_follow_links.py for a more detailed example.\n+\n+WARNING: The Crawler class currently has a big limitation - it cannot be used\n+more than once in the same Python process. This is due to the fact that Twisted\n+reactors cannot be restarted. Hopefully, this limitation will be removed in the\n+future.\n+\"\"\"\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.core.manager import scrapymanager\n+from scrapy.core.engine import scrapyengine\n+from scrapy.conf import settings as scrapy_settings\n+from scrapy import log\n+\n+class Crawler(object):\n+\n+    def __init__(self, enable_log=False, stop_on_error=False, silence_errors=False, \\\n+            settings=None):\n+        self.stop_on_error = stop_on_error\n+        self.silence_errors = silence_errors\n+        # disable offsite middleware (by default) because it prevents free crawling\n+        if settings is not None:\n+            settings.overrides.update(settings)\n+        scrapy_settings.overrides['SPIDER_MIDDLEWARES'] = {\n+            'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': None}\n+        scrapy_settings.overrides['LOG_ENABLED'] = enable_log\n+        scrapymanager.configure()\n+        dispatcher.connect(self._logmessage_received, signal=log.logmessage_received)\n+\n+    def crawl(self, *args):\n+        scrapymanager.runonce(*args)\n+\n+    def stop(self):\n+        scrapyengine.stop()\n+        log.log_level = log.SILENT\n+        scrapyengine.kill()\n+\n+    def _logmessage_received(self, message, level):\n+        if level <= log.ERROR:\n+            if not self.silence_errors:\n+                print \"Crawler error: %s\" % message\n+            if self.stop_on_error:\n+                self.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90407d2789386a12a201de39f7415e51bda100d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 68 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ requests. For example:\n     >>> request = Request('http://scrapy.org', callback=parse_response)\n     >>> crawler = Crawler()\n     >>> crawler.crawl(request)\n-    Visited http://scrapy.org \n+    Visited: http://scrapy.org \n     >>>\n \n Request callbacks follow the same API of spiders callback, which means that all\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00873cd16c5ed5db9c3c175527eea808ef596371", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 201 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,7 @@ from scrapy.conf import settings\n from scrapy.utils.url import url_is_from_spider\n \n class TwistedPluginSpiderManager(object):\n-    \"\"\"Spider locator and manager\"\"\"\n+    \"\"\"Spider manager based in Twisted Plugin System\"\"\"\n \n     def __init__(self):\n         self.loaded = False\n@@ -53,12 +53,9 @@ class TwistedPluginSpiderManager(object):\n         modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n         for module in modules:\n             for spider in self._getspiders(ISpider, module):\n-                self.add_spider(spider)\n-        self.loaded = True\n-\n-    def add_spider(self, spider):\n                 ISpider.validateInvariants(spider)\n                 self._spiders[spider.domain_name] = spider\n+        self.loaded = True\n \n     def _getspiders(self, interface, package):\n         \"\"\"This is an override of twisted.plugin.getPlugin, because we're\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99467d4e6e485fa24467b4b02230028d924c4055", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 46 | Files Changed: 6 | Hunks: 33 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 94 | Churn Cumulative: 1487 | Contributors (this commit): 5 | Commits (past 90d): 64 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,13 +2,13 @@\n Dupe Filter classes implement a mechanism for filtering duplicate requests.\n They must implement the following methods:\n \n-* open_domain(domain)\n-  open a domain for tracking duplicates (typically used to reserve resources)\n+* open_spider(spider)\n+  open a spider for tracking duplicates (typically used to reserve resources)\n \n-* close_domain(domain)\n-  close a domain (typically used for freeing resources)\n+* close_spider(spider)\n+  close a spider (typically used for freeing resources)\n \n-* request_seen(domain, request, dont_record=False)\n+* request_seen(spider, request, dont_record=False)\n   return ``True`` if the request was seen before, or ``False`` otherwise. If\n   ``dont_record`` is ``True`` the request must not be recorded as seen.\n \n@@ -18,13 +18,13 @@ from scrapy.utils.request import request_fingerprint\n \n \n class NullDupeFilter(dict):\n-    def open_domain(self, domain):\n+    def open_spider(self, spider):\n         pass\n \n-    def close_domain(self, domain):\n+    def close_spider(self, spider):\n         pass\n \n-    def request_seen(self, domain, request, dont_record=False):\n+    def request_seen(self, spider, request, dont_record=False):\n         return False\n \n \n@@ -34,16 +34,16 @@ class RequestFingerprintDupeFilter(object):\n     def __init__(self):\n         self.fingerprints = {}\n \n-    def open_domain(self, domain):\n-        self.fingerprints[domain] = set()\n+    def open_spider(self, spider):\n+        self.fingerprints[spider] = set()\n \n-    def close_domain(self, domain):\n-        del self.fingerprints[domain]\n+    def close_spider(self, spider):\n+        del self.fingerprints[spider]\n \n-    def request_seen(self, domain, request, dont_record=False):\n+    def request_seen(self, spider, request, dont_record=False):\n         fp = request_fingerprint(request)\n-        if fp in self.fingerprints[domain]:\n+        if fp in self.fingerprints[spider]:\n             return True\n         if not dont_record:\n-            self.fingerprints[domain].add(fp)\n+            self.fingerprints[spider].add(fp)\n         return False\n\n@@ -17,13 +17,13 @@ class DuplicatesFilterMiddleware(object):\n \n         self.dupefilter = load_object(clspath)()\n \n-    def enqueue_request(self, domain, request):\n-        seen = self.dupefilter.request_seen(domain, request)\n+    def enqueue_request(self, spider, request):\n+        seen = self.dupefilter.request_seen(spider, request)\n         if seen and not request.dont_filter:\n             raise IgnoreRequest('Skipped (request already seen)')\n \n-    def open_domain(self, domain):\n-        self.dupefilter.open_domain(domain)\n+    def open_spider(self, spider):\n+        self.dupefilter.open_spider(spider)\n \n-    def close_domain(self, domain):\n-        self.dupefilter.close_domain(domain)\n+    def close_spider(self, spider):\n+        self.dupefilter.close_spider(spider)\n\n@@ -161,6 +161,7 @@ class ExecutionEngine(object):\n \n     def crawl(self, request, spider):\n         schd = mustbe_deferred(self.schedule, request, spider)\n+        schd.addErrback(log.err, \"Unhandled error on engine.crawl()\")\n         schd.addBoth(self.scraper.enqueue_scrape, request, spider)\n         schd.addErrback(log.err, \"Unhandled error on engine.crawl()\")\n         schd.addBoth(lambda _: self.next_request(spider))\n\n@@ -46,7 +46,7 @@ class SchedulerMiddlewareManager(object):\n         self.loaded = True\n \n     def _add_middleware(self, mw):\n-        for name in ['enqueue_request', 'open_domain', 'close_domain']:\n+        for name in ['enqueue_request', 'open_spider', 'close_spider']:\n             mwfunc = getattr(mw, name, None)\n             if mwfunc:\n                 self.mw_cbs[name].append(mwfunc)\n@@ -54,7 +54,7 @@ class SchedulerMiddlewareManager(object):\n     def enqueue_request(self, wrappedfunc, spider, request):\n         def _enqueue_request(request):\n             for mwfunc in self.mw_cbs['enqueue_request']:\n-                result = mwfunc(domain=spider.domain_name, request=request)\n+                result = mwfunc(spider=spider, request=request)\n                 assert result is None or isinstance(result, (Response, Deferred)), \\\n                         'Middleware %s.enqueue_request must return None, Response or Deferred, got %s' % \\\n                         (mwfunc.im_self.__class__.__name__, result.__class__.__name__)\n@@ -66,10 +66,10 @@ class SchedulerMiddlewareManager(object):\n         return deferred\n \n     def open_spider(self, spider):\n-        for mwfunc in self.mw_cbs['open_domain']:\n-            mwfunc(spider.domain_name)\n+        for mwfunc in self.mw_cbs['open_spider']:\n+            mwfunc(spider)\n \n     def close_spider(self, spider):\n-        for mwfunc in self.mw_cbs['close_domain']:\n-            mwfunc(spider.domain_name)\n+        for mwfunc in self.mw_cbs['close_spider']:\n+            mwfunc(spider)\n \n\n@@ -1,36 +1,37 @@\n import unittest\n \n from scrapy.http import Request\n+from scrapy.spider import BaseSpider\n from scrapy.contrib.dupefilter import RequestFingerprintDupeFilter, NullDupeFilter\n \n \n class RequestFingerprintDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n-        domain = 'scrapytest.org'\n+        spider = BaseSpider()\n         filter = RequestFingerprintDupeFilter()\n-        filter.open_domain(domain)\n+        filter.open_spider(spider)\n \n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        assert not filter.request_seen(domain, r1)\n-        assert filter.request_seen(domain, r1)\n+        assert not filter.request_seen(spider, r1)\n+        assert filter.request_seen(spider, r1)\n \n-        assert not filter.request_seen(domain, r2)\n-        assert filter.request_seen(domain, r3)\n+        assert not filter.request_seen(spider, r2)\n+        assert filter.request_seen(spider, r3)\n \n-        filter.close_domain(domain)\n+        filter.close_spider(spider)\n \n \n class NullDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n-        domain = 'scrapytest.org'\n+        spider = BaseSpider()\n         filter = NullDupeFilter()\n-        filter.open_domain(domain)\n+        filter.open_spider(spider)\n \n         r1 = Request('http://scrapytest.org/1')\n-        assert not filter.request_seen(domain, r1)\n-        filter.close_domain(domain)\n+        assert not filter.request_seen(spider, r1)\n+        filter.close_spider(spider)\n\n@@ -1,20 +1,20 @@\n import unittest\n \n from scrapy.http import Request\n+from scrapy.spider import BaseSpider\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.contrib.schedulermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n \n \n-DOMAIN = 'scrapytest.org'\n-\n class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n         self.mw = DuplicatesFilterMiddleware()\n-        self.mw.open_domain(DOMAIN)\n+        self.spider = BaseSpider()\n+        self.mw.open_spider(self.spider)\n \n     def tearDown(self):\n-        self.mw.close_domain(DOMAIN)\n+        self.mw.close_spider(self.spider)\n \n     def test_process_spider_output(self):\n \n@@ -23,7 +23,7 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         r3 = Request('http://scrapytest.org/2')\n         r4 = Request('http://scrapytest.org/1')\n \n-        assert not self.mw.enqueue_request(DOMAIN, r1)\n-        assert not self.mw.enqueue_request(DOMAIN, r2)\n-        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, DOMAIN, r3)\n-        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, DOMAIN, r4)\n+        assert not self.mw.enqueue_request(self.spider, r1)\n+        assert not self.mw.enqueue_request(self.spider, r2)\n+        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, self.spider, r3)\n+        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, self.spider, r4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f64dfe57980e002f7ebaec356bb6990b861402d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1345 | Contributors (this commit): 4 | Commits (past 90d): 57 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,10 +73,11 @@ class TwistedPluginSpiderManager(object):\n             sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n             sys.exit(2)\n \n-    def close_domain(self, domain):\n+    def close_spider(self, spider):\n         \"\"\"Reload spider module to release any resources held on to by the\n         spider\n         \"\"\"\n+        domain = spider.domain_name\n         if domain not in self._spiders:\n             return\n         spider = self._spiders[domain]\n\n@@ -301,7 +301,7 @@ class ExecutionEngine(object):\n         send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n             domain=domain, spider=spider, reason=reason)\n         stats.close_domain(domain, reason=reason)\n-        spiders.close_domain(domain)\n+        spiders.close_spider(spider)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n         self._mainloop()\n         if not self.open_spiders:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2322312c63d2027a04d8762a1aef18750ed09d0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1143 | Contributors (this commit): 4 | Commits (past 90d): 49 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -197,7 +197,7 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(\"Crawled %s (referer: <%s>)\" % (response, referer), \\\n+                log.msg(\"Crawled %s (referer: <%s>)\" % (request, referer), \\\n                     level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e07f76edd3a83e8b68530ec23c10d04dd987cdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -21,17 +21,17 @@ class FifoSpiderScheduler(object):\n     \"\"\"Basic spider scheduler based on a FIFO queue\"\"\"\n \n     def __init__(self):\n-        self.pending_spiders = []\n+        self._pending_spiders = []\n \n     def next_spider(self) :\n-        if self.pending_spiders:\n-            return self.pending_spiders.pop(0)\n+        if self._pending_spiders:\n+            return self._pending_spiders.pop(0)\n \n     def add_spider(self, spider):\n-        self.pending_spiders.append(spider)\n+        self._pending_spiders.append(spider)\n \n     def remove_pending_spider(self, spider):\n-        self.pending_spiders = [d for d in self.pending_spiders if d != spider]\n+        self._pending_spiders = [d for d in self._pending_spiders if d != spider]\n \n     def has_pending_spider(self, spider):\n-        return spider in self.pending_spiders\n+        return spider in self._pending_spiders\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc463bc9e8b45c87407d9bcd1f40d8bcdd7f4c94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1249 | Contributors (this commit): 4 | Commits (past 90d): 54 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.\n For more information see docs/topics/architecture.rst\n \n \"\"\"\n-from datetime import datetime\n+from time import time\n \n from twisted.internet import reactor, task, defer\n from twisted.python.failure import Failure\n@@ -49,7 +49,7 @@ class ExecutionEngine(object):\n         \"\"\"Start the execution engine\"\"\"\n         if self.running:\n             return\n-        self.start_time = datetime.utcnow()\n+        self.start_time = time()\n         send_catch_log(signal=signals.engine_started, sender=self.__class__)\n         self._mainloop_task.start(5.0, now=True)\n         reactor.callWhenRunning(self._mainloop)\n\n@@ -1,5 +1,7 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n+from time import time\n+\n from scrapy.core.engine import scrapyengine\n \n def get_engine_status(engine=None):\n@@ -8,7 +10,7 @@ def get_engine_status(engine=None):\n         engine = scrapyengine\n \n     global_tests = [\n-        \"datetime.utcnow()-engine.start_time\",\n+        \"time()-engine.start_time\",\n         \"engine.is_idle()\",\n         \"engine.scheduler.is_idle()\",\n         \"len(engine.scheduler.pending_requests)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47aa716630252ae0699b21a7201d5a2b5450ee3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ See docs/topics/webconsole.rst\n \n import re\n import socket\n-from datetime import datetime\n+from time import time\n \n from twisted.internet import reactor\n from twisted.web import server, resource\n@@ -38,11 +38,12 @@ def banner(module=None):\n     s += \"<head><title>Scrapy</title></head>\\n\"\n     s += \"<body>\\n\"\n     s += \"<h1><a href='/'>Scrapy web console</a></h1>\\n\"\n-    now = datetime.now()\n-    uptime = now - scrapyengine.start_time\n-    s += \"<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%s</b> | Time: <b>%s</b> </p>\\n\" % (settings['BOT_NAME'], socket.gethostname(), str(uptime), str(now.replace(microsecond=0)))\n+    uptime = time() - scrapyengine.start_time\n+    s += \"<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%ds</b></p>\\n\" % \\\n+        (settings['BOT_NAME'], socket.gethostname(), uptime)\n     if module:\n-        s += \"<h2><a href='/%s/'>%s</a></h2>\\n\" % (module.webconsole_id, module.webconsole_name)\n+        s += \"<h2><a href='/%s/'>%s</a></h2>\\n\" % (module.webconsole_id, \\\n+            module.webconsole_name)\n     return s\n \n class WebConsoleResource(resource.Resource):\n@@ -52,7 +53,8 @@ class WebConsoleResource(resource.Resource):\n     def modules(self):\n         if not hasattr(self, '_modules'):\n             self._modules = {}\n-            for _, obj in dispatcher.send(signal=webconsole_discover_module, sender=self.__class__):\n+            for _, obj in dispatcher.send(signal=webconsole_discover_module, \\\n+                    sender=self.__class__):\n                 self._modules[obj.webconsole_id] = obj\n         return self._modules\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fcbbb5001e093d4f2b5a522061f833be7a514370", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 85 | Files Changed: 1 | Hunks: 19 | Methods Changed: 8 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 120 | Churn Cumulative: 168 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ class Spiderctl(object):\n     webconsole_name = 'Spider control panel'\n \n     def __init__(self):\n-        self.running = set()\n+        self.running = {}\n         self.finished = set()\n         dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n         dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n@@ -24,42 +24,41 @@ class Spiderctl(object):\n         from scrapy.management.web import webconsole_discover_module\n         dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n \n-    def domain_opened(self, domain, spider):\n-        self.running.add(domain)\n+    def domain_opened(self, spider):\n+        self.running[spider.domain_name] = spider\n \n-    def domain_closed(self, domain, spider):\n-        self.running.remove(domain)\n-        self.finished.add(domain)\n+    def domain_closed(self, spider):\n+        del self.running[spider.domain_name]\n+        self.finished.add(spider.domain_name)\n \n     def webconsole_render(self, wc_request):\n         if wc_request.args:\n             changes = self.webconsole_control(wc_request)\n \n-        enabled_domains = spiders.list()\n-        self.scheduled = scrapyengine.domain_scheduler.pending_domains\n-        self.idle = [d for d in enabled_domains if d not in self.scheduled\n+        self.scheduled = [s.domain_name for s in scrapyengine.spider_scheduler._pending_spiders]\n+        self.idle = [d for d in self.enabled_domains if d not in self.scheduled\n                                                         and d not in self.running\n                                                         and d not in self.finished]\n \n         s = banner(self)\n         s += '<table border=1\">\\n'\n-        s += \"<tr><th>Running (%d/%d)</th><th>Scheduled (%d)</th><th>Finished (%d)</th><th>Idle (%d)</th></tr>\\n\" % \\\n-                (len(self.running),\n+        s += \"<tr><th>Idle (%d)</th><th>Scheduled (%d)</th><th>Running (%d/%d)</th><th>Finished (%d)</th></tr>\\n\" % \\\n+                (len(self.idle),\n+                 len(self.running),\n                  settings['CONCURRENT_DOMAINS'],\n                  len(self.scheduled),\n-                 len(self.finished),\n-                 len(self.idle))\n+                 len(self.finished))\n         s += \"<tr>\\n\"\n \n-        # running\n+        # idle\n         s += \"<td valign='top'>\\n\"\n         s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"stop_running_domains\" multiple=\"multiple\">\\n'\n-        for domain in sorted(self.running):\n+        s += '<select name=\"add_pending_domains\" multiple=\"multiple\">\\n'\n+        for domain in sorted(self.idle):\n             s += \"<option>%s</option>\\n\" % domain\n         s += '</select><br>\\n'\n         s += '<br />'\n-        s += '<input type=\"submit\" value=\"Stop selected\">\\n'\n+        s += '<input type=\"submit\" value=\"Schedule selected\">\\n'\n         s += '</form>\\n'\n         s += \"</td>\\n\"\n \n@@ -76,6 +75,18 @@ class Spiderctl(object):\n \n         s += \"</td>\\n\"\n \n+        # running\n+        s += \"<td valign='top'>\\n\"\n+        s += '<form method=\"post\" action=\".\">\\n'\n+        s += '<select name=\"stop_running_domains\" multiple=\"multiple\">\\n'\n+        for domain in sorted(self.running):\n+            s += \"<option>%s</option>\\n\" % domain\n+        s += '</select><br>\\n'\n+        s += '<br />'\n+        s += '<input type=\"submit\" value=\"Stop selected\">\\n'\n+        s += '</form>\\n'\n+        s += \"</td>\\n\"\n+\n         # finished\n         s += \"<td valign='top'>\\n\"\n         s += '<form method=\"post\" action=\".\">\\n'\n@@ -88,45 +99,7 @@ class Spiderctl(object):\n         s += '</form>\\n'\n         s += \"</td>\\n\"\n \n-        # idle\n-        s += \"<td valign='top'>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += '<select name=\"add_pending_domains\" multiple=\"multiple\">\\n'\n-        for domain in sorted(self.idle):\n-            s += \"<option>%s</option>\\n\" % domain\n-        s += '</select><br>\\n'\n-        s += '<br />'\n-        s += '<input type=\"submit\" value=\"Schedule selected\">\\n'\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n         s += \"</tr>\\n\"\n-        s += \"<tr>\\n\"\n-\n-        s += \"<td>&nbsp;</td>\\n\"\n-\n-        s += \"<td>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += \"<textarea name='bulk_remove_domains' rows='10' cols='25'></textarea>\\n\"\n-        s += '<br>\\n'\n-        s += '<input type=\"submit\" value=\"Bulk remove domains\"><br />\\n'\n-        s += \"<span style='font-size: small'>(enter one domain per line)</span>\\n\"\n-        s += '</form>\\n'\n-        s += '</td>\\n'\n-\n-        s += \"<td>&nbsp;</td>\\n\"\n-\n-        s += \"<td>\\n\"\n-        s += '<form method=\"post\" action=\".\">\\n'\n-        s += \"<textarea name='bulk_schedule_domains' rows='10' cols='25'></textarea>\\n\"\n-        s += '<br>\\n'\n-        s += '<input type=\"submit\" value=\"Bulk schedule domains\"><br/>\\n'\n-        s += \"<span style='font-size: small'>(enter one domain per line)</span>\\n\"\n-        s += '</form>\\n'\n-        s += \"</td>\\n\"\n-\n-        s += \"</tr>\\n\"\n-\n         s += \"</table>\\n\"\n \n         if wc_request.args:\n@@ -139,36 +112,26 @@ class Spiderctl(object):\n \n     def webconsole_control(self, wc_request):\n         args = wc_request.args\n-        enabled_domains = spiders.list()\n         s = \"<hr />\\n\"\n \n         if \"stop_running_domains\" in args:\n             s += \"<p>\"\n-            s += \"Stopped spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"stop_running_domains\"])\n+            stopped_domains = []\n             for domain in args[\"stop_running_domains\"]:\n-                scrapyengine.close_domain(domain)\n+                if domain in self.running:\n+                    scrapyengine.close_spider(self.running[domain])\n+                    stopped_domains.append(domain)\n+            s += \"Stopped spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(stopped_domains)\n             s += \"</p>\"\n         if \"remove_pending_domains\" in args:\n             removed = []\n             for domain in args[\"remove_pending_domains\"]:\n-                if scrapyengine.domain_scheduler.remove_pending_domain(domain):\n+                if scrapyengine.spider_scheduler.remove_pending_domain(domain):\n                     removed.append(domain)\n             if removed:\n                 s += \"<p>\"\n                 s += \"Removed scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"remove_pending_domains\"])\n                 s += \"</p>\"\n-        if \"bulk_remove_domains\" in args:\n-            scheduled = []\n-            to_remove = set([d.strip() for d in args[\"bulk_remove_domains\"][0].split(\"\\n\")])\n-            removed = set([d for d in scrapyengine.scheduler.pending_domains if d in to_remove])\n-            scrapyengine.scheduler.pending_domains = [d for d in scrapyengine.scheduler.pending_domains if d not in removed]\n-            if removed:\n-                s += \"<p>\"\n-                s += \"Removed: <ul><li>%s</li></ul>\" % \"</li><li>\".join(removed)\n-                s += \"</p>\"\n-                s += \"<p>\"\n-                s += \"Not removed: <ul><li>%s</li></ul>\" % \"</li><li>\".join(to_remove - removed)\n-                s += \"</p>\"\n         if \"add_pending_domains\" in args:\n             for domain in args[\"add_pending_domains\"]:\n                 if domain not in scrapyengine.scheduler.pending_requests:\n@@ -176,20 +139,6 @@ class Spiderctl(object):\n             s += \"<p>\"\n             s += \"Scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"add_pending_domains\"])\n             s += \"</p>\"\n-        if \"bulk_schedule_domains\" in args:\n-            to_schedule = set([d.strip() for d in args[\"bulk_schedule_domains\"][0].split(\"\\n\")])\n-            scheduled = set()\n-            for domain in to_schedule:\n-                if domain in enabled_domains and domain not in scrapyengine.scheduler.pending_requests:\n-                    scrapymanager.crawl(domain)\n-                    scheduled.add(domain)\n-            if scheduled:\n-                s += \"<p>\"\n-                s += \"Scheduled: <ul><li>%s</li></ul>\" % \"</li><li>\".join(scheduled)\n-                s += \"</p>\"\n-                s += \"<p>\"\n-                s += \"Not scheduled: <ul><li>%s</li></ul>\" % \"</li><li>\".join(to_schedule - scheduled)\n-                s += \"</p>\"\n         if \"rerun_finished_domains\" in args:\n             for domain in args[\"rerun_finished_domains\"]:\n                 if domain not in scrapyengine.scheduler.pending_requests:\n@@ -202,4 +151,5 @@ class Spiderctl(object):\n         return s\n         \n     def webconsole_discover_module(self):\n+        self.enabled_domains = spiders.list()\n         return self\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
