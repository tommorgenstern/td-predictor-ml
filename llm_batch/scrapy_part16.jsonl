{"custom_id": "scrapy#0ed5978abf0e1d00856ae3c8a64b0000aa9fd89d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 223 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 12 | Methods Changed: 28 | Complexity Δ (Sum/Max): 60/58 | Churn Δ: 236 | Churn Cumulative: 966 | Contributors (this commit): 2 | Commits (past 90d): 32 | Contributors (cumulative): 6 | DMM Complexity: 0.6771653543307087\n\nDIFF:\n@@ -1,12 +1,28 @@\n+import re\n+\n from traceback import format_exc\n \n-class AdaptorPipe:\n+from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n+from scrapy.utils.python import unique, flatten\n+from scrapy.utils.markup import replace_tags, remove_entities\n+from scrapy.utils.misc import extract_regex\n \n-    def __init__(self, adaptors_dict=None):\n+class AdaptorPipe:\n+    def __init__(self, adaptors_pipe=None):\n         \"\"\"\n         Receives a dictionary that maps attribute_name to a list of adaptor functions\n         \"\"\"\n-        self.pipes = adaptors_dict or {}\n+        self.pipes = adaptors_pipe or {}\n+\n+    def append_adaptor(self, attrname, adaptor):\n+        \"\"\"\n+        Add an adaptor at the end of the provided attribute's pipeline\n+        \"\"\"\n+        if callable(adaptor):\n+            if self.pipes.get(attrname):\n+                self.pipes[attrname].append(adaptor)\n+            else:\n+                self.pipes[attrname] = [adaptor]\n                 \n     def execute(self, attrname, value, debug=False):\n         \"\"\"\n@@ -26,3 +42,192 @@ class AdaptorPipe:\n                 return\n \n         return value\n+\n+\n+############\n+# Adaptors #\n+############\n+def extract(location):\n+    \"\"\"\n+    This adaptor extracts a list of strings\n+    from 'location', which can be either a list (or tuple),\n+    or an XPathSelector.\n+    \n+    This function *always* returns a list.\n+    \"\"\"\n+    if not location:\n+        return []\n+    elif isinstance(location, (list, tuple)):\n+        return flatten([extract(o) for o in location])\n+    elif isinstance(location, XPathSelector):\n+        return location.extract()\n+    elif isinstance(location, str):\n+        return [location]\n+\n+def _absolutize_links(rel_links, current_url, base_url):\n+    abs_links = []\n+    for link in rel_links:\n+        if link.startswith('/'):\n+            abs_links.append('%s%s' % (base_url, link))\n+        elif link.startswith('http://'):\n+            abs_links.append(link)\n+        else:\n+            abs_links.append('%s/%s' % (current_url, link))\n+    return abs_links\n+    \n+def extract_links(locations):\n+    \"\"\"\n+    This adaptor receives either an XPathSelector containing\n+    the desired locations for finding urls, or a tuple like (xpath, regexp)\n+    containing the xpath locations to look in, and a regular expression\n+    to parse those locations.\n+    \n+    In any case, this adaptor returns a list of absolute urls extracted.\n+    \"\"\"\n+    ret = []\n+    if locations:\n+        regexp = None\n+        if isinstance(locations, XPathSelector):\n+            locations = XPathSelectorList([locations])\n+        elif isinstance(locations, tuple):\n+            locations, regexp = locations\n+    \n+        if isinstance(locations, XPathSelectorList):\n+            if regexp:\n+                ret = locations.re(regexp)\n+            else:\n+                for selector in locations:\n+                    if selector.xmlNode.type == 'element':\n+                        if selector.xmlNode.name == 'a':\n+                            children = selector.x('child::*')\n+                            if len(children) > 1:\n+                                ret.extend(selector.x('.//@href'))\n+                                ret.extend(selector.x('.//@src'))\n+                            else:\n+                                ret.extend(selector.x('@href'))\n+                        elif selector.xmlNode.name == 'img':\n+                            ret.extend(selector.x('@src'))\n+                        else:\n+                            ret.extend(selector.x('.//@href'))\n+                            ret.extend(selector.x('.//@src'))\n+                    elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n+                        ret.append(selector)\n+                ret = [selector.extract() for selector in ret]\n+            current_url, base_url = re.search(r'((http://(?:www\\.)?[\\w\\d\\.-]+?)(?:/|$).*)/', locations[0].response.url).groups()\n+            ret = _absolutize_links(ret, current_url, base_url)\n+    return ret\n+\n+def to_unicode(value):\n+    \"\"\"\n+    Receives a list of strings, converts\n+    it to unicode, and returns a new list.\n+    \"\"\"\n+    if isinstance(value, (list, tuple)):\n+        return [ unicode(v) for v in value ]\n+    else:\n+        raise TypeError('to_unicode adaptor must receive a list or a tuple.')\n+    \n+def regex(expr):\n+    \"\"\"\n+    This factory function returns a ready-to-use\n+    adaptor for the specified regular expression.\n+    This adaptor will accept either an XPathSelectorList\n+    or a list of strings, and will apply the provided regular\n+    expression to each of its members.\n+    \n+    This adaptor always returns a list of strings.\n+    \"\"\"\n+    def _regex(value):\n+        if isinstance(value, (XPathSelector, XPathSelectorList)):\n+            return value.re(expr)\n+        elif isinstance(value, list) and value:\n+            return extract_regex(expr, value, 'utf-8')\n+        return value\n+    return _regex\n+\n+def unquote_all(value):\n+    \"\"\"\n+    Receives a list of strings, removes all of the\n+    entities the strings may have, and returns\n+    a new list\n+    \"\"\"\n+    return [ remove_entities(v) for v in value ]\n+\n+def unquote(value):\n+    \"\"\"\n+    Receives a list of strings, removes all of the entities\n+    the strings may have (except for &lt; and &amp;), and\n+    returns a new list\n+    \"\"\"\n+    return [ remove_entities(v, keep=['lt', 'amp']) for v in value ]\n+\n+def remove_tags(value):\n+    return [ replace_tags(v) for v in value ]\n+    \n+_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n+def _remove_root(value):\n+    m = _remove_root_re.search(value)\n+    if m:\n+        value = m.group(1)\n+    return value\n+\n+def remove_root(value):\n+    return [ _remove_root(v) for v in value ]\n+    \n+_clean_spaces_re = re.compile(\"\\s+\", re.U)\n+def remove_multispaces(value):\n+    return [ _clean_spaces_re.sub(' ', v) for v in value ]\n+\n+def strip(value):\n+    return [ v.strip() for v in value ]\n+\n+def drop_empty_elements(value):\n+    return [ v for v in value if v ]\n+\n+def delist(value):\n+    return ' '.join(value)\n+    \n+\n+\n+#############\n+# Pipelines #\n+#############\n+\"\"\"\n+The following methods automatically generate adaptor pipelines\n+for some basic datatypes, according to the parameters you pass them.\n+\"\"\"\n+def single_pipeline(do_unquote=True):\n+    pipe = [ extract,\n+             unique,\n+             to_unicode,\n+             drop_empty_elements,\n+             remove_tags,\n+             remove_root,\n+             remove_multispaces,\n+             strip,\n+           ]\n+    if do_unquote:\n+        pipe.append(unquote)\n+    return pipe + [delist]\n+           \n+def url_pipeline():\n+    return [ extract_links,\n+             unique,\n+             to_unicode,\n+             drop_empty_elements,\n+           ]\n+               \n+def list_pipeline():\n+    return [ extract,\n+             unique,\n+             to_unicode,\n+             drop_empty_elements,\n+             unquote,\n+             remove_tags,\n+             remove_root,\n+             strip,\n+           ]\n+                \n+def list_join_pipeline(delimiter='\\t'):\n+    return list_pipeline() + [delimiter.join]\n+    \n\\ No newline at end of file\n\n@@ -7,15 +7,18 @@ class ScrapedItem(object):\n     that identifies uniquely the given scraped item.\n     \"\"\"\n \n-    def setadaptors(self, adaptors_dict):\n-        \"\"\"Set adaptors to use for this item. Receives a dict of adaptors and\n-        returns the item itself\"\"\"\n-        self.adaptors_dict = AdaptorPipe(adaptors_dict)\n+    def setadaptors(self, adaptors_pipe):\n+        \"\"\"\n+        Set adaptors to use for this item. Receives a dict of adaptors and\n+        returns the item itself.\n+        \"\"\"\n+        object.__setattr__(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))\n         return self\n     \n+    def append_adaptor(self, attrname, adaptor):\n+        self._adaptors_pipe.append_adaptor(attrname, adaptor)\n+            \n     def attribute(self, attrname, value):\n-        value = self.adaptors_pipe.execute(attrname, value)\n-        if not hasattr(self, attrname):\n+        value = self._adaptors_pipe.execute(attrname, value)\n+        if not hasattr(self, attrname) or not getattr(self, attrname):\n             setattr(self, attrname, value)\n-\n-                \n\n@@ -32,6 +32,7 @@ class XPathSelector(object):\n             self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n+        self.response = response\n \n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on the current XPathSelector and\n@@ -41,9 +42,10 @@ class XPathSelector(object):\n             xpath_result = self.doc.xpathContext.xpathEval(xpath)\n             cls = type(self)\n             if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([cls(node=node, parent=self, expr=xpath) for node in xpath_result])\n+                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, response=self.response)\n+                                          for node in xpath_result])\n             else:\n-                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath)])\n+                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath, response=self.response)])\n         else:\n             return XPathSelectorList([])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3433272d71db44a95ce49a7190f79067eaeaff61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 8 | Methods Changed: 18 | Complexity Δ (Sum/Max): 17/12 | Churn Δ: 106 | Churn Cumulative: 673 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: 0.9428571428571428\n\nDIFF:\n@@ -57,11 +57,13 @@ def extract(location):\n     \"\"\"\n     if not location:\n         return []\n-    elif isinstance(location, (list, tuple)):\n+    elif isinstance(location, XPathSelectorList):\n         return flatten([extract(o) for o in location])\n     elif isinstance(location, XPathSelector):\n         return location.extract()\n-    elif isinstance(location, str):\n+    elif isinstance(location, (list, tuple)):\n+        return flatten(location)\n+    elif isinstance(location, basestring):\n         return [location]\n \n def _absolutize_links(rel_links, current_url, base_url):\n@@ -103,6 +105,8 @@ def extract_links(locations):\n                             if len(children) > 1:\n                                 ret.extend(selector.x('.//@href'))\n                                 ret.extend(selector.x('.//@src'))\n+                            elif len(children) == 1 and children[0].xmlNode.name == 'img':\n+                                ret.extend(children.x('@src'))\n                             else:\n                                 ret.extend(selector.x('@href'))\n                         elif selector.xmlNode.name == 'img':\n@@ -113,7 +117,7 @@ def extract_links(locations):\n                     elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:\n                         ret.append(selector)\n                 ret = [selector.extract() for selector in ret]\n-            current_url, base_url = re.search(r'((http://(?:www\\.)?[\\w\\d\\.-]+?)(?:/|$).*)/', locations[0].response.url).groups()\n+            current_url, base_url = re.search(r'((http://(?:www\\.)?[\\w\\d\\.-]+?)(?:/|$).*)', locations[0].response.url).groups()\n             ret = _absolutize_links(ret, current_url, base_url)\n     return ret\n \n@@ -141,7 +145,7 @@ def regex(expr):\n         if isinstance(value, (XPathSelector, XPathSelectorList)):\n             return value.re(expr)\n         elif isinstance(value, list) and value:\n-            return extract_regex(expr, value, 'utf-8')\n+            return flatten([extract_regex(expr, string, 'utf-8') for string in value])\n         return value\n     return _regex\n \n@@ -217,16 +221,18 @@ def url_pipeline():\n              drop_empty_elements,\n            ]\n                \n-def list_pipeline():\n-    return [ extract,\n-             unique,\n+def list_pipeline(extract=True):\n+    pipe = []\n+    if extract:\n+        pipe.append(extract) \n+    return pipe.extend([ unique,\n                          to_unicode,\n                          drop_empty_elements,\n                          unquote,\n                          remove_tags,\n                          remove_root,\n                          strip,\n-           ]\n+                       ])\n                 \n def list_join_pipeline(delimiter='\\t'):\n     return list_pipeline() + [delimiter.join]\n\n@@ -0,0 +1,84 @@\n+# -*- coding: utf8 -*-\n+import unittest\n+from scrapy.item import adaptors\n+from scrapy.xpath.selector import XmlXPathSelector\n+from scrapy.http import Response, ResponseBody\n+\n+class AdaptorsTestCase(unittest.TestCase):\n+    def test_extract(self):\n+        sample_xsel = XmlXPathSelector(text='<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>')\n+        self.assertEqual(adaptors.extract(sample_xsel.x('/')),\n+                         ['<xml id=\"2\"><tag1>foo<tag2>bar</tag2></tag1><tag3 value=\"mytag\">test</tag3></xml>'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),\n+                         ['<tag1>foo<tag2>bar</tag2></tag1>', '<tag3 value=\"mytag\">test</tag3>'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('xml/@id')), ['2'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1')), ['<tag1>foo<tag2>bar</tag2></tag1>'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1//text()')),\n+                         ['foo', 'bar'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//text()')),\n+                         ['foo', 'bar', 'test'])\n+        self.assertEqual(adaptors.extract(sample_xsel.x('//tag3/@value')), ['mytag'])\n+        \n+    def test_extract_links(self):\n+        test_data = \"\"\"<html><body>\n+                         <div>\n+                           <a href=\"lala1.html\">lala1</a>\n+                           <a href=\"/lala2.html\">lala2</a>\n+                           <a href=\"http://foobar.com/lala3.html\">lala3</a>\n+                           <a href=\"lala4.html\"><img src=\"lala4.jpg\" /></a>\n+                           <a onclick=\"javascript: opensomething('/my_html1.html');\">something1</a>\n+                           <a onclick=\"javascript: opensomething('my_html2.html');\">something2</a>\n+                         </div>\n+                       </body></html>\"\"\"\n+        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=ResponseBody(test_data))\n+        sample_xsel = XmlXPathSelector(sample_response)\n+        self.assertEqual(adaptors.extract_links(sample_xsel.x('//@href')),\n+                         [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n+                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.html'])\n+        self.assertEqual(adaptors.extract_links(sample_xsel.x('//a')),\n+                         [u'http://foobar.com/dummy/lala1.html', u'http://foobar.com/lala2.html',\n+                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.jpg'])\n+        self.assertEqual(adaptors.extract_links((sample_xsel.x('//a[@onclick]'), r'opensomething\\(\\'(.*?)\\'\\)')),\n+                         [u'http://foobar.com/my_html1.html', u'http://foobar.com/dummy/my_html2.html'])\n+        \n+    def test_to_unicode(self):\n+        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'luluñ', 1, 'áé']),\n+                         [u'lala', u'lele', u'lulu\\xf1', u'1', u'\\xe1\\xe9'])\n+        \n+    def test_regex(self):\n+        func = adaptors.regex(r'href=\"(.*?)\"')\n+        self.assertEqual(func(['<a href=\"lala.com\">dsa</a><a href=\"pepe.co.uk\"></a>',\n+                               '<a href=\"das.biz\">href=\"lelelel.net\"</a>']),\n+                         ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])\n+        \n+    def test_unquote_all(self):\n+        self.assertEqual(adaptors.unquote_all([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&welcome', u'<br />&'])\n+        \n+    def test_unquote(self):\n+        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\\xa9&amp;welcome', u'&lt;br />&amp;'])\n+        \n+    def test_remove_tags(self):\n+        test_data = ['<a href=\"lala\">adsaas<br /></a>', '<div id=\"1\"><table>dsadasf</table></div>']\n+        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])\n+        \n+    def test_remove_root(self):\n+        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br /></div>']),\n+                         ['lallaa<a href=\"coso\">dsfsdfds</a>pepepep<br />'])\n+        \n+    def test_remove_multispaces(self):\n+        self.assertEqual(adaptors.remove_multispaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n+                         [' hello, whats up?', 'testing testingtesting testing'])\n+        \n+    def test_strip(self):\n+        self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n+                         ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n+        \n+    def test_drop_empty_elements(self):\n+        self.assertEqual(adaptors.drop_empty_elements([1, 2, None, 5, None, 6, None, 'hi']),\n+                         [1, 2, 5, 6, 'hi'])\n+        \n+    def test_delist(self):\n+        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),\n+                         'hi there fellas. this is my test.')\n+        \n+    \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#125414c15ae338d51ac263e89e42334d76dbdb6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 31 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,31 @@\n+import unittest\n+from scrapy.contrib.pipeline.images import thumbnail_name, image_path\n+\n+class ImagesPipelineTestCase(unittest.TestCase):\n+    def test_image_path(self):\n+        self.assertEqual(image_path(\"https://dev.mydeco.com/mydeco.gif\"),\n+                         'dev.mydeco.com/mydeco.gif')\n+        self.assertEqual(image_path(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"),\n+                         'www.maddiebrown.co.uk/catalogue-items/image_54642_12175_95307.jpg')\n+        self.assertEqual(image_path(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\"),\n+                         'dev.mydeco.com/two/dirs/with spaces+signs.gif')\n+        self.assertEqual(image_path(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"),\n+                         'www.dfsonline.co.uk/4507be485f38b0da8a0be9eb2e1dfab8a19223f2')\n+        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532/\"),\n+                         'www.dorma.co.uk/images/product_details/2532.jpg')\n+        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+                         'www.dorma.co.uk/images/product_details/2532')\n+\n+    def test_thumbnail_name(self):\n+        name = '50'\n+        self.assertEqual(thumbnail_name(\"/tmp/foo.jpg\", name),  \n+                         '/tmp/foo_50.jpg')\n+        self.assertEqual(thumbnail_name(\"foo.png\", name),\n+                         'foo_50.jpg')\n+        self.assertEqual(thumbnail_name(\"/tmp/foo\", name),\n+                         '/tmp/foo_50.jpg')\n+        self.assertEqual(thumbnail_name(\"/tmp/some.name/foo\", name),\n+                         '/tmp/some.name/foo_50.jpg')\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1146e1803e6303a563dfe9c762e7af894ecb0791", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 154 | Churn Cumulative: 154 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,154 @@\n+import time\n+import hashlib\n+import rfc822\n+from cStringIO import StringIO\n+\n+import Image\n+import boto\n+\n+from scrapy import log\n+from scrapy.stats import stats\n+from scrapy.core.exceptions import DropItem, NotConfigured\n+from scrapy.core.exceptions import HttpException\n+from scrapy.contrib.pipeline.media import MediaPipeline\n+from scrapy.conf import settings\n+\n+class NoimagesDrop(DropItem):\n+    \"\"\"Product with no images exception\"\"\"\n+\n+class ImageException(Exception):\n+    \"\"\"General image error exception\"\"\"\n+\n+class S3ImagesPipeline(MediaPipeline):\n+    MEDIA_TYPE = 'image'\n+    THUMBS = (\n+        (\"50\", (50, 50)),\n+        (\"110\", (110, 110)),\n+        (\"270\", (270, 270))\n+    )\n+\n+    def __init__(self):\n+        if not settings['S3_IMAGES']:\n+            raise NotConfigured\n+\n+        # days to wait before redownloading images\n+        self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)\n+        \n+        self.bucket_name = settings['S3_BUCKET']\n+        self.prefix = settings['S3_PREFIX']\n+        access_key = settings['AWS_ACCESS_KEY_ID']\n+        secret_key = settings['AWS_SECRET_ACCESS_KEY']\n+        conn = boto.connect_s3(access_key, secret_key)\n+        self.bucket = conn.get_bucket(self.bucket_name)\n+\n+        MediaPipeline.__init__(self)\n+\n+    def media_to_download(self, request, info):\n+        key = self.s3_image_key(request.url)\n+        if not self.s3_should_download(request.url):\n+            self.inc_stats(info.domain, 'uptodate')\n+            referer = request.headers.get('Referer')\n+            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n+                    (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n+            return key\n+\n+    def media_downloaded(self, response, request, info):\n+        mtype = self.MEDIA_TYPE\n+        referer = request.headers.get('Referer')\n+\n+        if not response or not response.body.to_string():\n+            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: Empty image (no-content)' % (mtype, request, referer)\n+            log.msg(msg, level=log.WARNING, domain=info.domain)\n+            raise ImageException(msg)\n+\n+        result = self.save_image(response, request, info) # save and thumbs response\n+\n+        status = 'cached' if getattr(response, 'cached', False) else 'downloaded'\n+        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n+        log.msg(msg, level=log.DEBUG, domain=info.domain)\n+        self.inc_stats(info.domain, status)\n+        return result\n+\n+    def media_failed(self, failure, request, info):\n+        referer = request.headers.get('Referer')\n+        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n+        msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n+        log.msg(msg, level=log.WARNING, domain=info.domain)\n+        raise ImageException(msg)\n+\n+    def save_image(self, response, request, info):\n+        try:\n+            key = self.s3_image_key(request.url)\n+            self.s3_store_image(response, request.url)\n+        except ImageException, ex:\n+            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n+            raise ex\n+        except Exception, ex:\n+            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n+            raise ex\n+\n+        return key # success value sent as input result for item_media_downloaded\n+\n+    def inc_stats(self, domain, status):\n+        stats.incpath('%s/image_count' % domain)\n+        stats.incpath('%s/image_status_count/%s' % (domain, status))\n+\n+    def s3_image_key(self, url):\n+        \"\"\"Return the relative path on the target filesystem for an image to be\n+        downloaded to.\n+        \"\"\"\n+        image_guid = hashlib.sha1(url).hexdigest()\n+        return '%s/full/%s.jpg' % (self.prefix, image_guid)\n+\n+    def s3_thumb_key(self, url, thumb_id):\n+        \"\"\"Return the relative path on the target filesystem for an image to be\n+        downloaded to.\n+        \"\"\"\n+        image_guid = hashlib.sha1(url).hexdigest()\n+        return '%s/thumbs/%s/%s.jpg' % (self.prefix, thumb_id, image_guid)\n+\n+    def s3_should_download(self, url):\n+        \"\"\"Return if the image should be downloaded by checking if it's already in\n+        the S3 storage and not too old\"\"\"\n+        key = self.s3_image_key(url)\n+        k = self.bucket.get_key(key)\n+        if k is None:\n+            return True\n+        modified_tuple = rfc822.parsedate_tz(k.last_modified)\n+        modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n+        age_seconds = time.time() - modified_stamp\n+        age_days = age_seconds / 60 / 60 / 24\n+        return age_days > self.image_refresh_days\n+\n+    def s3_store_image(self, response, url):\n+        \"\"\"Upload image to S3 storage\"\"\"\n+        buf = StringIO(response.body.to_string())\n+        image = Image.open(buf)\n+        key = self.s3_image_key(url)\n+        self._s3_put_image(image, key)\n+        self.s3_store_thumbnails(image, url)\n+\n+    def s3_store_thumbnails(self, image, url):\n+        \"\"\"Upload image thumbnails to S3 storage\"\"\"\n+        for thumb_id, size in self.THUMBS or []:\n+            thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n+            thumb.thumbnail(size, Image.ANTIALIAS)\n+            key = self.s3_thumb_key(url, thumb_id)\n+            self._s3_put_image(thumb, key)\n+\n+    def s3_public_url(self, key):\n+        return \"http://%s.s3.amazonaws.com/%s\" % (self.bucket_name, key)\n+\n+    def _s3_put_image(self, image, key):\n+        buf = StringIO()\n+        try:\n+            image.save(buf, 'JPEG')\n+        except Exception, ex:\n+            raise ImageException(\"Cannot process image. Error: %s\" % ex)\n+\n+        buf.seek(0)\n+        k = self.bucket.new_key(key)\n+        k.content_type = 'image/jpeg'\n+        k.set_contents_from_file(buf, policy='public-read')\n+        log.msg(\"Uploaded to S3: %s\" % self.s3_public_url(key), level=log.DEBUG)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5fddea58a75cacbecf0e7d340f08767e2591005c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 23 | Churn Cumulative: 23 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,23 @@\n+import csv\n+\n+from scrapy.http import Response\n+from scrapy import log\n+\n+def csv_iter(response, delimiter=None, headers=None):\n+    if delimiter:\n+        csv_r = csv.reader(response.body.to_unicode().split('\\n'), delimiter=delimiter)\n+    else:\n+        csv_r = csv.reader(response.body.to_unicode().split('\\n'))\n+\n+    if not headers:\n+        headers = csv_r.next()\n+\n+    while True:\n+        node = csv_r.next()\n+\n+        if len(node) != len(headers):\n+            log.msg(\"ignoring node %d (length: %d, should be: %d)\" % (csv_r.line_num, len(node), len(headers)), log.WARNING)\n+            continue\n+\n+        yield dict(zip(headers, node))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ede33a46dbfac083fb20447ee7449589eb82412", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 605 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -200,16 +200,18 @@ def delist(value):\n The following methods automatically generate adaptor pipelines\n for some basic datatypes, according to the parameters you pass them.\n \"\"\"\n-def single_pipeline(do_unquote=True):\n+def single_pipeline(remove_root=True, remove_tags=True, do_unquote=True):\n     pipe = [ extract,\n              unique,\n              to_unicode,\n              drop_empty_elements,\n-             remove_tags,\n-             remove_root,\n              remove_multispaces,\n              strip,\n            ]\n+    if remove_root:\n+        pipe.insert(4, remove_root)\n+    if remove_tags:\n+        pipe.insert(5, remove_tags)\n     if do_unquote:\n         pipe.append(unquote)\n     return pipe + [delist]\n@@ -221,18 +223,18 @@ def url_pipeline():\n              drop_empty_elements,\n            ]\n                \n-def list_pipeline(extract=True):\n+def list_pipeline(do_extract=True):\n     pipe = []\n-    if extract:\n+    if do_extract:\n         pipe.append(extract)\n-    return pipe.extend([ unique,\n+    return pipe + [ unique,\n                     to_unicode,\n                     drop_empty_elements,\n                     unquote,\n                     remove_tags,\n                     remove_root,\n                     strip,\n-                       ])\n+                  ]\n                 \n def list_join_pipeline(delimiter='\\t'):\n     return list_pipeline() + [delimiter.join]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#52ee3781db491e452033d6d2f523b597fdaa483d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 35 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 39 | Churn Cumulative: 266 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,26 +1,14 @@\n from functools import wraps\n-from twisted.internet import defer\n+\n from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.serialization import serialize as _serialize, unserialize as _unserialize\n \n from .http import HttpResponse\n \n-\n-JSONCALLBACK_RE = '^[a-zA-Z][a-zA-Z_.-]*$'\n JSON_CONTENT_TYPES = ('application/json',)\n \n-\n-def serialize(obj):\n-    global _serialize\n-    if not _serialize:\n-        _loadserializers()\n-    return _serialize(obj)\n-\n-def unserialize(obj):\n-    global _unserialize\n-    if not _unserialize:\n-        _loadserializers()\n-    return _unserialize(obj)\n-\n+serialize = lambda x: _serialize(x, 'json')\n+unserialize = lambda x: _unserialize(x, 'json')\n \n class JsonException(Exception):\n     pass\n@@ -109,21 +97,3 @@ def _x_http_method_override(request):\n     \"\"\"\n     return request.HEADERS.get('x-http-method-override', request.method).upper()\n \n-\n-## json serializers\n-\n-_serialize = _unserialize = None\n-def _loadserializers():\n-    global _serialize, _unserialize\n-    try:\n-        import cjson\n-        _serialize = cjson.encode\n-        _unserialize = cjson.decode\n-    except ImportError:\n-        try:\n-            import simplejson\n-            _serialize = simplejson.dumps\n-            _unserialize = simplejson.loads\n-        except ImportError:\n-            assert 0, 'json serialization needs cjson or simplejson modules'\n-\n\n@@ -1,6 +1,5 @@\n import re\n import urllib\n-import simplejson\n import hashlib\n from twisted.internet import defer\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00f28e3edf46d237bebc9576bd44c1d88953dac3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 239 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-import os, pickle\n+import os\n+import cPickle as pickle\n \n SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')\n \n\n@@ -1,7 +1,7 @@\n \"\"\"\n Store implementations using a MySQL DB\n \"\"\"\n-import pickle\n+import cPickle as pickle\n from datetime import datetime\n \n from scrapy.utils.misc import stats_getpath\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#150f4ed81530390c344c714f597a89f76bced797", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 54 | Files Changed: 3 | Hunks: 48 | Methods Changed: 27 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 180 | Churn Cumulative: 675 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,21 +8,24 @@ from scrapy import log\n from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured\n \n-class Broker(pb.Referenceable):\n+class ClusterCrawlerBroker(pb.Referenceable):\n+    \"\"\"ClusterCrawlerBroker is the class that's used for communication between\n+    the cluster worker and the crawling proces\"\"\"\n+\n     def __init__(self, crawler, remote):\n         self.__remote = remote\n         self.__crawler = crawler\n-        try:\n         deferred = self.__remote.callRemote(\"register_crawler\", os.getpid(), self)\n-        except pb.DeadReferenceError:\n-            self._set_status(None)\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n-        else:\n         deferred.addCallbacks(callback=lambda x: None, errback=lambda reason: log.msg(reason, log.ERROR))\n+\n     def remote_stop(self):\n         scrapymanager.stop()\n     \n-class ClusterCrawler:\n+class ClusterCrawler(object):\n+    \"\"\"ClusterCrawler is an extension that instances a ClusterCrawlerBroker\n+    which is used to control a crawling process from the cluster worker. It\n+    also registers that broker to the local cluster worker\"\"\"\n+\n     def __init__(self):\n         if not settings.getbool('CLUSTER_CRAWLER_ENABLED'):\n             raise NotConfigured\n@@ -33,6 +36,6 @@ class ClusterCrawler:\n         reactor.connectTCP(\"localhost\", settings.getint('CLUSTER_WORKER_PORT'), factory)\n         d = factory.getRootObject()\n         def _set_worker(obj):\n-            self.worker = Broker(self, obj)\n+            self.worker = ClusterCrawlerBroker(self, obj)\n         d.addCallbacks(callback=_set_worker, errback=lambda reason: log.msg(reason, log.ERROR))\n         \n\n@@ -1,8 +1,9 @@\n-import sys, datetime\n-import pickle\n+from __future__ import with_statement\n+\n+import datetime\n+import cPickle as pickle\n \n from pydispatch import dispatcher\n-\n from twisted.spread import pb\n from twisted.internet import reactor\n \n@@ -21,7 +22,8 @@ def my_import(name):\n         mod = getattr(mod, comp)\n     return mod\n \n-class Broker(pb.Referenceable):\n+class ClusterMasterBroker(pb.Referenceable):\n+\n     def __init__(self, remote, name, master):\n         self.__remote = remote\n         self.alive = False\n@@ -143,6 +145,9 @@ class Broker(pb.Referenceable):\n                 self.master.statistics[\"domains\"][\"lost\"].remove(domain)\n \n class ScrapyPBClientFactory(pb.PBClientFactory):\n+\n+    noisy = False\n+\n     def __init__(self, master, nodename):\n         pb.PBClientFactory.__init__(self)\n         self.master = master\n@@ -151,14 +156,16 @@ class ScrapyPBClientFactory(pb.PBClientFactory):\n     def clientConnectionLost(self, *args, **kargs):\n         pb.PBClientFactory.clientConnectionLost(self, *args, **kargs)\n         del self.master.nodes[self.nodename]\n-        log.msg(\"Removed node %s.\" % self.nodename )\n+        log.msg(\"Lost connection to %s. Node removed\" % self.nodename )\n \n-class ClusterMaster:\n+class ClusterMaster(object):\n \n     def __init__(self):\n \n-        if not (settings.getbool('CLUSTER_MASTER_ENABLED')):\n+        if not settings.getbool('CLUSTER_MASTER_ENABLED'):\n             raise NotConfigured\n+        if not settings['CLUSTER_MASTER_STATEFILE']:\n+            raise NotConfigured(\"ClusterMaster: Missing CLUSTER_MASTER_STATEFILE setting\")\n \n         # import groups settings\n         if settings.getbool('GROUPSETTINGS_ENABLED'):\n@@ -167,13 +174,14 @@ class ClusterMaster:\n             self.get_spider_groupsettings = lambda x: {}\n         # load pending domains\n         try:\n-            self.pending = pickle.load( open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"r\") )\n+            statefile = open(settings[\"CLUSTER_MASTER_STATEFILE\"], \"r\")\n+            self.pending = pickle.load(statefile)\n         except IOError:\n             self.pending = []\n         self.loading = []\n         self.nodes = {}\n         self.start_time = datetime.datetime.utcnow()\n-        #on how statistics works, see self.update_nodes() and Broker.remote_update()\n+        # for more info about statistics see self.update_nodes() and ClusterMasterBroker.remote_update()\n         self.statistics = {\"domains\": {\"running\": set(), \"scraped\": {}, \"lost_count\": {}, \"lost\": set()}, \"scraped_count\": 0 }\n         self.global_settings = {}\n         # load cluster global settings\n@@ -185,20 +193,12 @@ class ClusterMaster:\n         \n     def load_nodes(self):\n         \"\"\"Loads nodes listed in CLUSTER_MASTER_NODES setting\"\"\"\n-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n-            self.load_node(name, url)\n+        for name, hostport in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n+            self.load_node(name, hostport)\n             \n-    def load_node(self, name, url):\n-        \"\"\"Creates the remote reference for each worker node\"\"\"\n-        def _make_callback(_factory, _name, _url):\n-\n-            def _errback(_reason):\n-                log.msg(\"Could not get remote node %s in %s: %s.\" % (_name, _url, _reason), log.ERROR)\n-\n-            d = _factory.getRootObject()\n-            d.addCallbacks(callback=lambda obj: self.add_node(obj, _name), errback=_errback)\n-\n-        server, port = url.split(\":\")\n+    def load_node(self, name, hostport):\n+        \"\"\"Creates the remote reference for a worker node\"\"\"\n+        server, port = hostport.split(\":\")\n         port = int(port)\n         log.msg(\"Connecting to cluster worker %s...\" % name)\n         log.msg(\"Server: %s, Port: %s\" % (server, port))\n@@ -206,18 +206,23 @@ class ClusterMaster:\n         try:\n             reactor.connectTCP(server, port, factory)\n         except Exception, err:\n-            log.msg(\"Could not connect to node %s in %s: %s.\" % (name, url, reason), log.ERROR)\n+            log.msg(\"Could not connect to node %s in %s: %s.\" % (name, hostport, err), log.ERROR)\n         else:\n-            _make_callback(factory, name, url)\n+            def _errback(_reason):\n+                log.msg(\"Could not connect to remote node %s (%s): %s.\" % (name, hostport, _reason), log.ERROR)\n+\n+            d = factory.getRootObject()\n+            d.addCallbacks(callback=lambda obj: self.add_node(obj, name), errback=_errback)\n \n     def update_nodes(self):\n-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n+        \"\"\"Update worker nodes statistics\"\"\"\n+        for name, hostport in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n             if name in self.nodes and self.nodes[name].alive:\n-                log.msg(\"Updating node. name: %s, url: %s\" % (name, url) )\n+                log.msg(\"Updating node. name: %s, host: %s\" % (name, hostport) )\n                 self.nodes[name].update_status()\n             else:\n-                log.msg(\"Reloading node. name: %s, url: %s\" % (name, url) )\n-                self.load_node(name, url)\n+                log.msg(\"Reloading node. name: %s, host: %s\" % (name, hostport) )\n+                self.load_node(name, hostport)\n         \n         real_running = set(self.running.keys())\n         lost = self.statistics[\"domains\"][\"running\"].difference(real_running)\n@@ -227,7 +232,7 @@ class ClusterMaster:\n             \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n-        node = Broker(cworker, name, self)\n+        node = ClusterMasterBroker(cworker, name, self)\n         self.nodes[name] = node\n         log.msg(\"Added cluster worker %s\" % name)\n \n@@ -241,6 +246,7 @@ class ClusterMaster:\n         raise NotImplemented\n \n     def schedule(self, domains, spider_settings=None, priority=DEFAULT_PRIORITY):\n+        \"\"\"Schedule the domains passed\"\"\"\n         i = 0\n         for p in self.pending:\n             if p['priority'] <= priority:\n@@ -261,6 +267,7 @@ class ClusterMaster:\n                 self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n \n     def stop(self, domains):\n+        \"\"\"Stop the given domains\"\"\"\n         to_stop = {}\n         for domain in domains:\n             node = self.running.get(domain, None)\n@@ -325,6 +332,8 @@ class ClusterMaster:\n     def _engine_started(self):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n+\n     def _engine_stopped(self):\n-        pickle.dump( self.pending, open(settings[\"CLUSTER_MASTER_CACHEFILE\"], \"w\") )\n-        log.msg(\"Pending saved in %s\" % settings[\"CLUSTER_MASTER_CACHEFILE\"])\n+        with open(settings[\"CLUSTER_MASTER_STATEFILE\"], \"w\") as f:\n+            pickle.dump(self.pending, f)\n+            log.msg(\"Cluster master state saved in %s\" % settings[\"CLUSTER_MASTER_STATEFILE\"])\n\n@@ -1,4 +1,8 @@\n-import sys, os, time, datetime, pickle\n+import sys\n+import os\n+import time\n+import datetime\n+import cPickle as pickle\n \n from twisted.internet import protocol, reactor\n from twisted.spread import pb\n@@ -9,6 +13,7 @@ from scrapy.conf import settings\n from scrapy.core.engine import scrapyengine\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n+\n     def __init__(self, procman, domain, logfile=None, spider_settings=None):\n         self.procman = procman\n         self.domain = domain\n@@ -17,18 +22,46 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.status = \"starting\"\n         self.pid = -1\n         self.env = {}\n-        #We conserve original setting format for info purposes (avoid lots of unnecesary \"SCRAPY_\")\n+        # We preserve the original settings format for info purposes (avoid\n+        # lots of unnecesary \"SCRAPY_\")\n         self.scrapy_settings = spider_settings or {}\n-        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': 0, 'CLUSTER_CRAWLER_ENABLED': 1, 'WEBCONSOLE_ENABLED': 0})\n+        self.scrapy_settings.update({'LOGFILE': self.logfile, \n+                                     'CLUSTER_WORKER_ENABLED': 0, \n+                                     'CLUSTER_CRAWLER_ENABLED': 1, \n+                                     'WEBCONSOLE_ENABLED': 0})\n         pickled_settings = pickle.dumps(self.scrapy_settings)\n         self.env[\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\"] = pickled_settings\n-        self.env[\"PYTHONPATH\"] = \":\".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.\n+        # we nee to pass the worker python path to the crawling process so it\n+        # knows where to find the local_scrapy_settings\n+        self.env[\"PYTHONPATH\"] = \":\".join(sys.path)\n \n     def __str__(self):\n         return \"<ScrapyProcess domain=%s, pid=%s, status=%s>\" % (self.domain, self.pid, self.status)\n \n-    def as_dict(self):\n-        return {\"domain\": self.domain, \"pid\": self.pid, \"status\": self.status, \"settings\": self.scrapy_settings, \"logfile\": self.logfile, \"starttime\": self.start_time}\n+    def status(self):\n+        \"\"\"Return this scrapy process status as a dict.\n+        \n+        The keys are: \n+\n+        domain:\n+          the domain being crawled\n+        pid:\n+          the pid of this process\n+        status:\n+          the status of this process (starting, running)\n+        settings:\n+          the scrapy settings overrided for this process by the worker\n+        logfile:\n+          the log file being used\n+        starttime:\n+          the start time of this process as a UTC datetime object\n+        \"\"\"\n+        return {\"domain\": self.domain, \n+                \"pid\": self.pid, \n+                \"status\": self.status, \n+                \"settings\": self.scrapy_settings, \n+                \"logfile\": self.logfile, \n+                \"starttime\": self.start_time}\n \n     def connectionMade(self):\n         self.pid = self.transport.pid\n@@ -52,22 +85,43 @@ class ClusterWorker(pb.Root):\n \n         self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n         self.logdir = settings['CLUSTER_LOGDIR']\n-        self.running = {}#a dict domain->ScrapyProcessControl \n-        self.crawlers = {}#a dict pid->scrapy process remote pb connection\n+        self.running = {} # dict of domain->ScrapyProcessControl \n+        self.crawlers = {} # dict of pid->scrapy process remote pb connection\n         self.starttime = datetime.datetime.utcnow()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n-        log.msg(\"PYTHONPATH: %s\" % repr(sys.path))\n+        log.msg(\"Using sys.path: %s\" % repr(sys.path), level=log.DEBUG)\n \n     def status(self, rcode=0, rstring=None):\n+        \"\"\"Return the status of this worker as dict.\n+        \n+        The keys of the dict are:\n+        \n+        running:\n+          list of dicts of processes running by this worker. for information\n+          about the dict see ScrapyProcessControl.status()\n+        starttime:\n+          the start time of this worker as a UTC datetime object\n+        timestamp: \n+          the current timestamp as a UTC datetime object\n+        maxproc: \n+          the maximum number of processes supported by this worker\n+        loadavg: \n+          the load average of this worker. see os.getloadavg()\n+        logdir: \n+          the log directory used by this worker\n+        callresponse: \n+          response to the request performed. only available when there was a request\n+        \"\"\"\n+\n         status = {}\n-        status[\"running\"] = [ self.running[k].as_dict() for k in self.running.keys() ]\n+        status[\"running\"] = [self.running[k].status() for k in self.running.keys()]\n         status[\"starttime\"] = self.starttime\n         status[\"timestamp\"] = datetime.datetime.utcnow()\n         status[\"maxproc\"] = self.maxproc\n         status[\"loadavg\"] = os.getloadavg()\n         status[\"logdir\"] = self.logdir\n-        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n+        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"No request\")\n         return status\n \n     def update_master(self, domain, domain_status):\n@@ -75,16 +129,17 @@ class ClusterWorker(pb.Root):\n             deferred = self.__master.callRemote(\"update\", self.status(), domain, domain_status)\n         except pb.DeadReferenceError:\n             self.__master = None\n-            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+            log.msg(\"Lost connection to master\", log.ERROR)\n         else:\n             deferred.addCallbacks(callback=lambda x: x, errback=lambda reason: log.msg(reason, log.ERROR))\n         \n     def remote_set_master(self, master):\n+        \"\"\"Set the master for this worker\"\"\"\n         self.__master = master\n         return self.status()\n \n     def remote_stop(self, domain):\n-        \"\"\"Stop running domain.\"\"\"\n+        \"\"\"Stop a running domain\"\"\"\n         if domain in self.running:\n             proc = self.running[domain]\n             log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n@@ -97,10 +152,12 @@ class ClusterWorker(pb.Root):\n             return self.status(1, \"%s: domain not running.\" % domain)\n \n     def remote_status(self):\n+        \"\"\"Return worker status as a dict. For infomation about the keys see\n+        the the status() method\"\"\" \n         return self.status()\n \n     def remote_run(self, domain, spider_settings=None):\n-        \"\"\"Spawn process to run the given domain.\"\"\"\n+        \"\"\"Start scraping the given domain by spawning a process\"\"\"\n         if len(self.running) < self.maxproc:\n             if not domain in self.running:\n                 logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n@@ -118,10 +175,13 @@ class ClusterWorker(pb.Root):\n                     log.msg(\"Unable to svn update: %s\" % e, level=log.WARNING)\n                 except ImportError:\n                     log.msg(\"pysvn module not available.\", level=log.WARNING)\n-                proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n+                reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n                 return self.status(0, \"Started process %s.\" % scrapy_proc)\n+            else:\n                 return self.status(2, \"Domain %s already running.\" % domain )\n+        else:\n             return self.status(1, \"No free slot to run another process.\")\n \n     def remote_register_crawler(self, pid, crawler):\n+        \"\"\"Register the crawler to the list of crawlers managed by this worker\"\"\"\n         self.crawlers[pid] = crawler\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
