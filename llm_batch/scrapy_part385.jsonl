{"custom_id": "scrapy#e47110f9a5a16f0628e53e16b9cb5f6a4f9721d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 190 | Lines Deleted: 128 | Files Changed: 21 | Hunks: 117 | Methods Changed: 77 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 318 | Churn Cumulative: 18142 | Contributors (this commit): 122 | Commits (past 90d): 79 | Contributors (cumulative): 364 | DMM Complexity: 0.9487179487179487\n\nDIFF:\n@@ -153,7 +153,7 @@ class Command(BaseRunSpiderCommand):\n     @overload\n     def iterate_spider_output(self, result: _T) -> Iterable[Any]: ...\n \n-    def iterate_spider_output(self, result: Any) -> Union[Iterable[Any], Deferred]:\n+    def iterate_spider_output(self, result: Any) -> Union[Iterable[Any], Deferred[Any]]:\n         if inspect.isasyncgen(result):\n             d = deferred_from_coro(\n                 collect_asyncgen(aiter_errback(result, self.handle_exception))\n@@ -233,7 +233,7 @@ class Command(BaseRunSpiderCommand):\n         response: Response,\n         callback: Callable,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Deferred:\n+    ) -> Deferred[Any]:\n         cb_kwargs = cb_kwargs or {}\n         d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n         return d\n@@ -345,7 +345,7 @@ class Command(BaseRunSpiderCommand):\n     def prepare_request(\n         self, spider: Spider, request: Request, opts: argparse.Namespace\n     ) -> Request:\n-        def callback(response: Response, **cb_kwargs: Any) -> Deferred:\n+        def callback(response: Response, **cb_kwargs: Any) -> Deferred[List[Any]]:\n             # memorize first request\n             if not self.first_response:\n                 self.first_response = response\n\n@@ -10,6 +10,7 @@ from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n     from twisted.internet.defer import Deferred\n+    from twisted.internet.interfaces import IConnector\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n@@ -45,7 +46,7 @@ class HTTP10DownloadHandler:\n         self._connect(factory)\n         return factory.deferred\n \n-    def _connect(self, factory: ScrapyHTTPClientFactory) -> Deferred:\n+    def _connect(self, factory: ScrapyHTTPClientFactory) -> IConnector:\n         from twisted.internet import reactor\n \n         host, port = to_unicode(factory.host), factory.port\n\n@@ -21,7 +21,7 @@ if TYPE_CHECKING:\n     from twisted.internet.base import ReactorBase\n     from twisted.internet.endpoints import HostnameEndpoint\n \n-    from scrapy.http.request import Request\n+    from scrapy.http import Request, Response\n     from scrapy.settings import Settings\n     from scrapy.spiders import Spider\n \n@@ -39,16 +39,18 @@ class H2ConnectionPool:\n         self._connections: Dict[ConnectionKeyT, H2ClientProtocol] = {}\n \n         # Save all requests that arrive before the connection is established\n-        self._pending_requests: Dict[ConnectionKeyT, Deque[Deferred]] = {}\n+        self._pending_requests: Dict[\n+            ConnectionKeyT, Deque[Deferred[H2ClientProtocol]]\n+        ] = {}\n \n     def get_connection(\n         self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n-    ) -> Deferred:\n+    ) -> Deferred[H2ClientProtocol]:\n         if key in self._pending_requests:\n             # Received a request while connecting to remote\n             # Create a deferred which will fire with the H2ClientProtocol\n             # instance\n-            d: Deferred = Deferred()\n+            d: Deferred[H2ClientProtocol] = Deferred()\n             self._pending_requests[key].append(d)\n             return d\n \n@@ -63,17 +65,17 @@ class H2ConnectionPool:\n \n     def _new_connection(\n         self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n-    ) -> Deferred:\n+    ) -> Deferred[H2ClientProtocol]:\n         self._pending_requests[key] = deque()\n \n-        conn_lost_deferred: Deferred = Deferred()\n+        conn_lost_deferred: Deferred[List[BaseException]] = Deferred()\n         conn_lost_deferred.addCallback(self._remove_connection, key)\n \n         factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n         conn_d = endpoint.connect(factory)\n         conn_d.addCallback(self.put_connection, key)\n \n-        d: Deferred = Deferred()\n+        d: Deferred[H2ClientProtocol] = Deferred()\n         self._pending_requests[key].append(d)\n         return d\n \n@@ -141,7 +143,7 @@ class H2Agent:\n         \"\"\"\n         return uri.scheme, uri.host, uri.port\n \n-    def request(self, request: Request, spider: Spider) -> Deferred:\n+    def request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         uri = URI.fromBytes(bytes(request.url, encoding=\"utf-8\"))\n         try:\n             endpoint = self.get_endpoint(uri)\n@@ -149,9 +151,11 @@ class H2Agent:\n             return defer.fail(Failure())\n \n         key = self.get_key(uri)\n-        d = self._pool.get_connection(key, uri, endpoint)\n-        d.addCallback(lambda conn: conn.request(request, spider))\n-        return d\n+        d: Deferred[H2ClientProtocol] = self._pool.get_connection(key, uri, endpoint)\n+        d2: Deferred[Response] = d.addCallback(\n+            lambda conn: conn.request(request, spider)\n+        )\n+        return d2\n \n \n class ScrapyProxyH2Agent(H2Agent):\n\n@@ -33,7 +33,7 @@ from twisted.protocols.policies import TimeoutMixin\n from zope.interface import implementer\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n     from ipaddress import IPv4Address, IPv6Address\n@@ -88,7 +88,10 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n     IDLE_TIMEOUT = 240\n \n     def __init__(\n-        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n+        self,\n+        uri: URI,\n+        settings: Settings,\n+        conn_lost_deferred: Deferred[List[BaseException]],\n     ) -> None:\n         \"\"\"\n         Arguments:\n@@ -99,7 +102,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             conn_lost_deferred -- Deferred fires with the reason: Failure to notify\n                 that connection was lost\n         \"\"\"\n-        self._conn_lost_deferred = conn_lost_deferred\n+        self._conn_lost_deferred: Deferred[List[BaseException]] = conn_lost_deferred\n \n         config = H2Configuration(client_side=True, header_encoding=\"utf-8\")\n         self.conn = H2Connection(config=config)\n@@ -215,14 +218,14 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         data = self.conn.data_to_send()\n         self.transport.write(data)\n \n-    def request(self, request: Request, spider: Spider) -> Deferred:\n+    def request(self, request: Request, spider: Spider) -> Deferred[Response]:\n         if not isinstance(request, Request):\n             raise TypeError(\n                 f\"Expected scrapy.http.Request, received {request.__class__.__qualname__}\"\n             )\n \n         stream = self._new_stream(request, spider)\n-        d = stream.get_response()\n+        d: Deferred[Response] = stream.get_response()\n \n         # Add the stream to the request pool\n         self._pending_request_stream_pool.append(stream)\n@@ -436,7 +439,10 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n @implementer(IProtocolNegotiationFactory)\n class H2ClientFactory(Factory):\n     def __init__(\n-        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred\n+        self,\n+        uri: URI,\n+        settings: Settings,\n+        conn_lost_deferred: Deferred[List[BaseException]],\n     ) -> None:\n         self.uri = uri\n         self.settings = settings\n\n@@ -20,7 +20,7 @@ if TYPE_CHECKING:\n     from hpack import HeaderTuple\n \n     from scrapy.core.http2.protocol import H2ClientProtocol\n-    from scrapy.http import Request\n+    from scrapy.http import Request, Response\n \n \n logger = logging.getLogger(__name__)\n@@ -154,7 +154,7 @@ class Stream:\n             else:\n                 self.close(StreamCloseReason.CANCELLED)\n \n-        self._deferred_response: Deferred = Deferred(_cancel)\n+        self._deferred_response: Deferred[Response] = Deferred(_cancel)\n \n     def __repr__(self) -> str:\n         return f\"Stream(id={self.stream_id!r})\"\n@@ -180,7 +180,7 @@ class Stream:\n             and not self.metadata[\"reached_warnsize\"]\n         )\n \n-    def get_response(self) -> Deferred:\n+    def get_response(self) -> Deferred[Response]:\n         \"\"\"Simply return a Deferred which fires when response\n         from the asynchronous request is available\n         \"\"\"\n\n@@ -13,6 +13,7 @@ from typing import (\n     Generator,\n     Iterable,\n     Iterator,\n+    List,\n     Optional,\n     Set,\n     Tuple,\n@@ -34,7 +35,6 @@ from scrapy.logformatter import LogFormatter\n from scrapy.pipelines import ItemPipelineManager\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import (\n-    DeferredListResultListT,\n     aiter_errback,\n     defer_fail,\n     defer_succeed,\n@@ -54,7 +54,7 @@ logger = logging.getLogger(__name__)\n \n \n _T = TypeVar(\"_T\")\n-_ParallelResult = DeferredListResultListT[Iterator[Any]]\n+_ParallelResult = List[Tuple[bool, Iterator[Any]]]\n \n if TYPE_CHECKING:\n     # parameterized Deferreds require Twisted 21.7.0\n@@ -374,7 +374,7 @@ class Scraper:\n \n     def _itemproc_finished(\n         self, output: Any, item: Any, response: Response, spider: Spider\n-    ) -> Deferred:\n+    ) -> Deferred[Any]:\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n         assert self.slot is not None  # typing\n         self.slot.itemproc_size -= 1\n\n@@ -302,7 +302,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             recovered = MutableChain()\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n         result = await maybe_deferred_to_future(\n-            self._process_spider_output(response, spider, result)\n+            cast(\n+                \"Deferred[Union[Iterable[_T], AsyncIterable[_T]]]\",\n+                self._process_spider_output(response, spider, result),\n+            )\n         )\n         if isinstance(result, AsyncIterable):\n             return MutableAsyncChain(result, recovered)\n@@ -339,7 +342,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     def process_start_requests(\n         self, start_requests: Iterable[Request], spider: Spider\n-    ) -> Deferred:\n+    ) -> Deferred[Iterable[Request]]:\n         return self._process_chain(\"process_start_requests\", start_requests, spider)\n \n     # This method is only needed until _async compatibility methods are removed.\n\n@@ -4,7 +4,18 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Set, Type, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Dict,\n+    Generator,\n+    Optional,\n+    Set,\n+    Type,\n+    TypeVar,\n+    Union,\n+    cast,\n+)\n \n from twisted.internet.defer import (\n     Deferred,\n@@ -54,6 +65,8 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n+\n \n class Crawler:\n     def __init__(\n@@ -140,7 +153,7 @@ class Crawler:\n         )\n \n     @inlineCallbacks\n-    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n+    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred[Any], Any, None]:\n         if self.crawling:\n             raise RuntimeError(\"Crawling already taking place\")\n         if self._started:\n@@ -172,7 +185,7 @@ class Crawler:\n         return ExecutionEngine(self, lambda _: self.stop())\n \n     @inlineCallbacks\n-    def stop(self) -> Generator[Deferred, Any, None]:\n+    def stop(self) -> Generator[Deferred[Any], Any, None]:\n         \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n         fired when the crawler is stopped.\"\"\"\n         if self.crawling:\n@@ -256,7 +269,7 @@ class CrawlerRunner:\n         self.settings = settings\n         self.spider_loader = self._get_spider_loader(settings)\n         self._crawlers: Set[Crawler] = set()\n-        self._active: Set[Deferred] = set()\n+        self._active: Set[Deferred[None]] = set()\n         self.bootstrap_failed = False\n \n     def crawl(\n@@ -264,7 +277,7 @@ class CrawlerRunner:\n         crawler_or_spidercls: Union[Type[Spider], str, Crawler],\n         *args: Any,\n         **kwargs: Any,\n-    ) -> Deferred:\n+    ) -> Deferred[None]:\n         \"\"\"\n         Run a crawler with the provided arguments.\n \n@@ -294,12 +307,12 @@ class CrawlerRunner:\n         crawler = self.create_crawler(crawler_or_spidercls)\n         return self._crawl(crawler, *args, **kwargs)\n \n-    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> Deferred:\n+    def _crawl(self, crawler: Crawler, *args: Any, **kwargs: Any) -> Deferred[None]:\n         self.crawlers.add(crawler)\n         d = crawler.crawl(*args, **kwargs)\n         self._active.add(d)\n \n-        def _done(result: Any) -> Any:\n+        def _done(result: _T) -> _T:\n             self.crawlers.discard(crawler)\n             self._active.discard(d)\n             self.bootstrap_failed |= not getattr(crawler, \"spider\", None)\n@@ -335,7 +348,7 @@ class CrawlerRunner:\n         # temporary cast until self.spider_loader is typed\n         return Crawler(cast(Type[Spider], spidercls), self.settings)\n \n-    def stop(self) -> Deferred:\n+    def stop(self) -> Deferred[Any]:\n         \"\"\"\n         Stops simultaneously all the crawling jobs taking place.\n \n@@ -344,7 +357,7 @@ class CrawlerRunner:\n         return DeferredList([c.stop() for c in list(self.crawlers)])\n \n     @inlineCallbacks\n-    def join(self) -> Generator[Deferred, Any, None]:\n+    def join(self) -> Generator[Deferred[Any], Any, None]:\n         \"\"\"\n         join()\n \n@@ -460,7 +473,7 @@ class CrawlerProcess(CrawlerRunner):\n             )\n         reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n-    def _graceful_stop_reactor(self) -> Deferred:\n+    def _graceful_stop_reactor(self) -> Deferred[Any]:\n         d = self.stop()\n         d.addBoth(self._stop_reactor)\n         return d\n\n@@ -7,7 +7,7 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Union\n+from typing import TYPE_CHECKING, Dict, Optional, TypeVar, Union\n \n from twisted.internet.defer import Deferred, maybeDeferred\n \n@@ -31,6 +31,8 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n+\n \n class RobotsTxtMiddleware:\n     DOWNLOAD_PRIORITY: int = 1000\n@@ -43,7 +45,9 @@ class RobotsTxtMiddleware:\n             \"ROBOTSTXT_USER_AGENT\", None\n         )\n         self.crawler: Crawler = crawler\n-        self._parsers: Dict[str, Union[RobotParser, Deferred, None]] = {}\n+        self._parsers: Dict[\n+            str, Union[RobotParser, Deferred[Optional[RobotParser]], None]\n+        ] = {}\n         self._parserimpl: RobotParser = load_object(\n             crawler.settings.get(\"ROBOTSTXT_PARSER\")\n         )\n@@ -55,14 +59,18 @@ class RobotsTxtMiddleware:\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def process_request(self, request: Request, spider: Spider) -> Optional[Deferred]:\n+    def process_request(\n+        self, request: Request, spider: Spider\n+    ) -> Optional[Deferred[None]]:\n         if request.meta.get(\"dont_obey_robotstxt\"):\n             return None\n         if request.url.startswith(\"data:\") or request.url.startswith(\"file:\"):\n             return None\n-        d: Deferred = maybeDeferred(self.robot_parser, request, spider)\n-        d.addCallback(self.process_request_2, request, spider)\n-        return d\n+        d: Deferred[Optional[RobotParser]] = maybeDeferred(\n+            self.robot_parser, request, spider  # type: ignore[arg-type]\n+        )\n+        d2: Deferred[None] = d.addCallback(self.process_request_2, request, spider)\n+        return d2\n \n     def process_request_2(\n         self, rp: Optional[RobotParser], request: Request, spider: Spider\n@@ -86,7 +94,7 @@ class RobotsTxtMiddleware:\n \n     def robot_parser(\n         self, request: Request, spider: Spider\n-    ) -> Union[RobotParser, Deferred, None]:\n+    ) -> Union[RobotParser, Deferred[Optional[RobotParser]], None]:\n         url = urlparse_cached(request)\n         netloc = url.netloc\n \n@@ -109,9 +117,9 @@ class RobotsTxtMiddleware:\n \n         parser = self._parsers[netloc]\n         if isinstance(parser, Deferred):\n-            d: Deferred = Deferred()\n+            d: Deferred[Optional[RobotParser]] = Deferred()\n \n-            def cb(result: Any) -> Any:\n+            def cb(result: Optional[RobotParser]) -> Optional[RobotParser]:\n                 d.callback(result)\n                 return result\n \n\n@@ -31,10 +31,10 @@ class BaseDupeFilter:\n     def request_seen(self, request: Request) -> bool:\n         return False\n \n-    def open(self) -> Optional[Deferred]:\n+    def open(self) -> Optional[Deferred[None]]:\n         pass\n \n-    def close(self, reason: str) -> Optional[Deferred]:\n+    def close(self, reason: str) -> Optional[Deferred[None]]:\n         pass\n \n     def log(self, request: Request, spider: Spider) -> None:\n\n@@ -149,7 +149,7 @@ class FeedStorageProtocol(Protocol):\n         \"\"\"Open the storage for the given spider. It must return a file-like\n         object that will be used for the exporters\"\"\"\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n+    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n         \"\"\"Store the given file stream\"\"\"\n \n \n@@ -162,7 +162,7 @@ class BlockingFeedStorage:\n \n         return NamedTemporaryFile(prefix=\"feed-\", dir=path)\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n+    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n         return deferToThread(self._store_in_thread, file)\n \n     def _store_in_thread(self, file: IO[bytes]) -> None:\n@@ -192,7 +192,7 @@ class StdoutFeedStorage:\n     def open(self, spider: Spider) -> IO[bytes]:\n         return self._stdout\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n+    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n         pass\n \n \n@@ -211,7 +211,7 @@ class FileFeedStorage:\n             dirname.mkdir(parents=True)\n         return Path(self.path).open(self.write_mode)\n \n-    def store(self, file: IO[bytes]) -> Optional[Deferred]:\n+    def store(self, file: IO[bytes]) -> Optional[Deferred[None]]:\n         file.close()\n         return None\n \n@@ -483,7 +483,7 @@ _FeedSlot = create_deprecated_class(\n \n \n class FeedExporter:\n-    _pending_deferreds: List[Deferred] = []\n+    _pending_deferreds: List[Deferred[None]] = []\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n@@ -570,7 +570,7 @@ class FeedExporter:\n             self.crawler.signals.send_catch_log_deferred(signals.feed_exporter_closed)\n         )\n \n-    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Optional[Deferred]:\n+    def _close_slot(self, slot: FeedSlot, spider: Spider) -> Optional[Deferred[None]]:\n         def get_file(slot_: FeedSlot) -> IO[bytes]:\n             assert slot_.file\n             if isinstance(slot_.file, PostProcessingManager):\n@@ -590,7 +590,7 @@ class FeedExporter:\n             return None\n \n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n-        d: Deferred = maybeDeferred(slot.storage.store, get_file(slot))\n+        d: Deferred[None] = maybeDeferred(slot.storage.store, get_file(slot))  # type: ignore[arg-type]\n \n         d.addCallback(\n             self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n@@ -621,7 +621,7 @@ class FeedExporter:\n         self.crawler.stats.inc_value(f\"feedexport/failed_count/{slot_type}\")\n \n     def _handle_store_success(\n-        self, f: Failure, logmsg: str, spider: Spider, slot_type: str\n+        self, result: Any, logmsg: str, spider: Spider, slot_type: str\n     ) -> None:\n         logger.info(\"Stored %s\", logmsg, extra={\"spider\": spider})\n         assert self.crawler.stats\n\n@@ -39,7 +39,7 @@ class StatsMailer:\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_closed(self, spider: Spider) -> Optional[Deferred]:\n+    def spider_closed(self, spider: Spider) -> Optional[Deferred[None]]:\n         spider_stats = self.stats.get_stats(spider)\n         body = \"Global stats\\n\\n\"\n         body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n\n@@ -103,7 +103,7 @@ class MailSender:\n         mimetype: str = \"text/plain\",\n         charset: Optional[str] = None,\n         _callback: Optional[Callable[..., None]] = None,\n-    ) -> Optional[Deferred]:\n+    ) -> Optional[Deferred[None]]:\n         from twisted.internet import reactor\n \n         msg: MIMEBase\n@@ -155,7 +155,9 @@ class MailSender:\n             )\n             return None\n \n-        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or \"utf-8\"))\n+        dfd: Deferred[Any] = self._sendmail(\n+            rcpts, msg.as_string().encode(charset or \"utf-8\")\n+        )\n         dfd.addCallback(self._sent_ok, to, cc, subject, len(attachs))\n         dfd.addErrback(self._sent_failed, to, cc, subject, len(attachs))\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n@@ -198,11 +200,11 @@ class MailSender:\n         )\n         return failure\n \n-    def _sendmail(self, to_addrs: List[str], msg: bytes) -> Deferred:\n+    def _sendmail(self, to_addrs: List[str], msg: bytes) -> Deferred[Any]:\n         from twisted.internet import reactor\n \n         msg_io = BytesIO(msg)\n-        d: Deferred = Deferred()\n+        d: Deferred[Any] = Deferred()\n \n         factory = self._create_sender_factory(to_addrs, msg_io, d)\n \n@@ -216,7 +218,7 @@ class MailSender:\n         return d\n \n     def _create_sender_factory(\n-        self, to_addrs: List[str], msg: IO[bytes], d: Deferred\n+        self, to_addrs: List[str], msg: IO[bytes], d: Deferred[Any]\n     ) -> ESMTPSenderFactory:\n         from twisted.mail.smtp import ESMTPSenderFactory\n \n\n@@ -13,6 +13,7 @@ from typing import (\n     List,\n     Optional,\n     Tuple,\n+    TypeVar,\n     Union,\n     cast,\n )\n@@ -24,16 +25,22 @@ from scrapy.utils.misc import build_from_crawler, build_from_settings, load_obje\n if TYPE_CHECKING:\n     from twisted.internet.defer import Deferred\n \n+    # typing.Concatenate and typing.ParamSpec require Python 3.10\n     # typing.Self requires Python 3.11\n-    from typing_extensions import Self\n+    from typing_extensions import Concatenate, ParamSpec, Self\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n     from scrapy.settings import Settings\n \n+    _P = ParamSpec(\"_P\")\n+\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n+_T2 = TypeVar(\"_T2\")\n+\n \n class MiddlewareManager:\n     \"\"\"Base class for implementing middleware managers\"\"\"\n@@ -98,16 +105,22 @@ class MiddlewareManager:\n         if hasattr(mw, \"close_spider\"):\n             self.methods[\"close_spider\"].appendleft(mw.close_spider)\n \n-    def _process_parallel(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n-        methods = cast(Iterable[Callable], self.methods[methodname])\n+    def _process_parallel(\n+        self, methodname: str, obj: _T, *args: Any\n+    ) -> Deferred[List[_T2]]:\n+        methods = cast(\n+            \"Iterable[Callable[Concatenate[_T, _P], _T2]]\", self.methods[methodname]\n+        )\n         return process_parallel(methods, obj, *args)\n \n-    def _process_chain(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n-        methods = cast(Iterable[Callable], self.methods[methodname])\n+    def _process_chain(self, methodname: str, obj: _T, *args: Any) -> Deferred[_T]:\n+        methods = cast(\n+            \"Iterable[Callable[Concatenate[_T, _P], _T]]\", self.methods[methodname]\n+        )\n         return process_chain(methods, obj, *args)\n \n-    def open_spider(self, spider: Spider) -> Deferred:\n+    def open_spider(self, spider: Spider) -> Deferred[List[None]]:\n         return self._process_parallel(\"open_spider\", spider)\n \n-    def close_spider(self, spider: Spider) -> Deferred:\n+    def close_spider(self, spider: Spider) -> Deferred[List[None]]:\n         return self._process_parallel(\"close_spider\", spider)\n\n@@ -4,6 +4,8 @@ See documentation in docs/topics/shell.rst\n \n \"\"\"\n \n+from __future__ import annotations\n+\n import os\n import signal\n from typing import Any, Callable, Dict, Optional, Tuple, Union\n@@ -92,7 +94,9 @@ class Shell:\n                 self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n             )\n \n-    def _schedule(self, request: Request, spider: Optional[Spider]) -> defer.Deferred:\n+    def _schedule(\n+        self, request: Request, spider: Optional[Spider]\n+    ) -> defer.Deferred[Any]:\n         if is_asyncio_reactor_installed():\n             # set the asyncio event loop for the current thread\n             event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n@@ -209,7 +213,7 @@ def inspect_response(response: Response, spider: Spider) -> None:\n     signal.signal(signal.SIGINT, sigint_handler)\n \n \n-def _request_deferred(request: Request) -> defer.Deferred:\n+def _request_deferred(request: Request) -> defer.Deferred[Any]:\n     \"\"\"Wrap a request inside a Deferred.\n \n     This function is harmful, do not use it until you know what you are doing.\n@@ -228,7 +232,7 @@ def _request_deferred(request: Request) -> defer.Deferred:\n         request.errback = request_errback\n         return result\n \n-    d: defer.Deferred = defer.Deferred()\n+    d: defer.Deferred[Any] = defer.Deferred()\n     d.addBoth(_restore_callbacks)\n     if request.callback:\n         d.addCallback(request.callback)\n\n@@ -50,7 +50,9 @@ class SignalManager:\n         kwargs.setdefault(\"sender\", self.sender)\n         return _signal.send_catch_log(signal, **kwargs)\n \n-    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:\n+    def send_catch_log_deferred(\n+        self, signal: Any, **kwargs: Any\n+    ) -> Deferred[List[Tuple[Any, Any]]]:\n         \"\"\"\n         Like :meth:`send_catch_log` but supports returning\n         :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n@@ -7,9 +7,7 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Union, cast\n-\n-from twisted.internet.defer import Deferred\n+from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, cast\n \n from scrapy import signals\n from scrapy.http import Request, Response\n@@ -19,6 +17,8 @@ from scrapy.utils.url import url_is_from_spider\n if TYPE_CHECKING:\n     from collections.abc import Callable\n \n+    from twisted.internet.defer import Deferred\n+\n     # typing.Concatenate requires Python 3.10\n     # typing.Self requires Python 3.11\n     from typing_extensions import Concatenate, Self\n@@ -105,10 +105,10 @@ class Spider(object_ref):\n         return url_is_from_spider(request.url, cls)\n \n     @staticmethod\n-    def close(spider: Spider, reason: str) -> Union[Deferred, None]:\n+    def close(spider: Spider, reason: str) -> Optional[Deferred[None]]:\n         closed = getattr(spider, \"closed\", None)\n         if callable(closed):\n-            return cast(Union[Deferred, None], closed(reason))\n+            return cast(\"Optional[Deferred[None]]\", closed(reason))\n         return None\n \n     def __repr__(self) -> str:\n\n@@ -49,13 +49,8 @@ if TYPE_CHECKING:\n _T = TypeVar(\"_T\")\n _T2 = TypeVar(\"_T2\")\n \n-# copied from twisted.internet.defer\n-_SelfResultT = TypeVar(\"_SelfResultT\")\n-_DeferredListResultItemT = Tuple[bool, _SelfResultT]\n-DeferredListResultListT = List[_DeferredListResultItemT[_SelfResultT]]\n \n-\n-def defer_fail(_failure: Failure) -> Deferred:\n+def defer_fail(_failure: Failure) -> Deferred[Any]:\n     \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n     next reactor loop\n \n@@ -64,7 +59,7 @@ def defer_fail(_failure: Failure) -> Deferred:\n     \"\"\"\n     from twisted.internet import reactor\n \n-    d: Deferred = Deferred()\n+    d: Deferred[Any] = Deferred()\n     reactor.callLater(0.1, d.errback, _failure)\n     return d\n \n@@ -78,12 +73,12 @@ def defer_succeed(result: _T) -> Deferred[_T]:\n     \"\"\"\n     from twisted.internet import reactor\n \n-    d: Deferred = Deferred()\n+    d: Deferred[_T] = Deferred()\n     reactor.callLater(0.1, d.callback, result)\n     return d\n \n \n-def defer_result(result: Any) -> Deferred:\n+def defer_result(result: Any) -> Deferred[Any]:\n     if isinstance(result, Deferred):\n         return result\n     if isinstance(result, failure.Failure):\n@@ -138,14 +133,14 @@ def parallel(\n     callable: Callable[Concatenate[_T, _P], _T2],\n     *args: _P.args,\n     **named: _P.kwargs,\n-) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n+) -> Deferred[List[Tuple[bool, Iterator[_T2]]]]:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n     Taken from: https://jcalderone.livejournal.com/24285.html\n     \"\"\"\n     coop = Cooperator()\n-    work = (callable(elem, *args, **named) for elem in iterable)\n+    work: Iterator[_T2] = (callable(elem, *args, **named) for elem in iterable)\n     return DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n@@ -198,16 +193,16 @@ class _AsyncCooperatorAdapter(Iterator[Deferred]):\n     def __init__(\n         self,\n         aiterable: AsyncIterable[_T],\n-        callable: Callable[Concatenate[_T, _P], _T2],\n+        callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]],\n         *callable_args: _P.args,\n         **callable_kwargs: _P.kwargs,\n     ):\n         self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n-        self.callable: Callable[Concatenate[_T, _P], _T2] = callable\n+        self.callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]] = callable\n         self.callable_args: Tuple[Any, ...] = callable_args\n         self.callable_kwargs: Dict[str, Any] = callable_kwargs\n         self.finished: bool = False\n-        self.waiting_deferreds: List[Deferred] = []\n+        self.waiting_deferreds: List[Deferred[Any]] = []\n         self.anext_deferred: Optional[Deferred[_T]] = None\n \n     def _callback(self, result: _T) -> None:\n@@ -241,12 +236,12 @@ class _AsyncCooperatorAdapter(Iterator[Deferred]):\n         self.anext_deferred = deferred_from_coro(self.aiterator.__anext__())\n         self.anext_deferred.addCallbacks(self._callback, self._errback)\n \n-    def __next__(self) -> Deferred:\n+    def __next__(self) -> Deferred[Any]:\n         # This puts a new Deferred into self.waiting_deferreds and returns it.\n         # It also calls __anext__() if needed.\n         if self.finished:\n             raise StopIteration\n-        d: Deferred = Deferred()\n+        d: Deferred[Any] = Deferred()\n         self.waiting_deferreds.append(d)\n         if not self.anext_deferred:\n             self._call_anext()\n@@ -256,25 +251,29 @@ class _AsyncCooperatorAdapter(Iterator[Deferred]):\n def parallel_async(\n     async_iterable: AsyncIterable[_T],\n     count: int,\n-    callable: Callable[Concatenate[_T, _P], _T2],\n+    callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]],\n     *args: _P.args,\n     **named: _P.kwargs,\n-) -> Deferred[DeferredListResultListT[Iterator[_T2]]]:\n-    \"\"\"Like parallel but for async iterators\"\"\"\n+) -> Deferred[List[Tuple[bool, Iterator[Deferred[Any]]]]]:\n+    \"\"\"Like ``parallel`` but for async iterators\"\"\"\n     coop = Cooperator()\n-    work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)\n-    dl: Deferred = DeferredList([coop.coiterate(work) for _ in range(count)])\n+    work: Iterator[Deferred[Any]] = _AsyncCooperatorAdapter(\n+        async_iterable, callable, *args, **named\n+    )\n+    dl: Deferred[List[Tuple[bool, Iterator[Deferred[Any]]]]] = DeferredList(\n+        [coop.coiterate(work) for _ in range(count)]\n+    )\n     return dl\n \n \n def process_chain(\n-    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n-    input: Any,\n+    callbacks: Iterable[Callable[Concatenate[_T, _P], _T]],\n+    input: _T,\n     *a: _P.args,\n     **kw: _P.kwargs,\n-) -> Deferred:\n+) -> Deferred[_T]:\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n-    d: Deferred = Deferred()\n+    d: Deferred[_T] = Deferred()\n     for x in callbacks:\n         d.addCallback(x, *a, **kw)\n     d.callback(input)\n@@ -307,19 +306,21 @@ def process_chain_both(\n \n \n def process_parallel(\n-    callbacks: Iterable[Callable[Concatenate[_T, _P], Any]],\n-    input: Any,\n+    callbacks: Iterable[Callable[Concatenate[_T, _P], _T2]],\n+    input: _T,\n     *a: _P.args,\n     **kw: _P.kwargs,\n-) -> Deferred:\n+) -> Deferred[List[_T2]]:\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n-    d: Deferred = DeferredList(dfds, fireOnOneErrback=True, consumeErrors=True)\n-    d.addCallback(lambda r: [x[1] for x in r])\n-    d.addErrback(lambda f: f.value.subFailure)\n-    return d\n+    d: Deferred[List[Tuple[bool, _T2]]] = DeferredList(\n+        dfds, fireOnOneErrback=True, consumeErrors=True\n+    )\n+    d2: Deferred[List[_T2]] = d.addCallback(lambda r: [x[1] for x in r])\n+    d2.addErrback(lambda f: f.value.subFailure)\n+    return d2\n \n \n def iter_errback(\n@@ -404,7 +405,7 @@ def deferred_f_from_coro_f(\n \n def maybeDeferred_coro(\n     f: Callable[_P, Any], *args: _P.args, **kw: _P.kwargs\n-) -> Deferred:\n+) -> Deferred[Any]:\n     \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n     try:\n         result = f(*args, **kw)\n@@ -420,7 +421,7 @@ def maybeDeferred_coro(\n     return defer.succeed(result)\n \n \n-def deferred_to_future(d: Deferred) -> Future:\n+def deferred_to_future(d: Deferred[_T]) -> Future[_T]:\n     \"\"\"\n     .. versionadded:: 2.6.0\n \n@@ -442,7 +443,7 @@ def deferred_to_future(d: Deferred) -> Future:\n     return d.asFuture(_get_asyncio_event_loop())\n \n \n-def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n+def maybe_deferred_to_future(d: Deferred[_T]) -> Union[Deferred[_T], Future[_T]]:\n     \"\"\"\n     .. versionadded:: 2.6.0\n \n\n@@ -1,5 +1,7 @@\n \"\"\"Helper functions for working with signals\"\"\"\n \n+from __future__ import annotations\n+\n import collections.abc\n import logging\n from typing import Any as TypingAny\n@@ -27,7 +29,7 @@ def send_catch_log(\n     signal: TypingAny = Any,\n     sender: TypingAny = Anonymous,\n     *arguments: TypingAny,\n-    **named: TypingAny\n+    **named: TypingAny,\n ) -> List[Tuple[TypingAny, TypingAny]]:\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n@@ -73,8 +75,8 @@ def send_catch_log_deferred(\n     signal: TypingAny = Any,\n     sender: TypingAny = Anonymous,\n     *arguments: TypingAny,\n-    **named: TypingAny\n-) -> Deferred:\n+    **named: TypingAny,\n+) -> Deferred[List[Tuple[TypingAny, TypingAny]]]:\n     \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n     Returns a deferred that gets fired once all signal handlers deferreds were\n     fired.\n@@ -92,23 +94,25 @@ def send_catch_log_deferred(\n \n     dont_log = named.pop(\"dont_log\", None)\n     spider = named.get(\"spider\", None)\n-    dfds = []\n+    dfds: List[Deferred[Tuple[TypingAny, TypingAny]]] = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n-        d = maybeDeferred_coro(\n+        d: Deferred[TypingAny] = maybeDeferred_coro(\n             robustApply, receiver, signal=signal, sender=sender, *arguments, **named\n         )\n         d.addErrback(logerror, receiver)\n         # TODO https://pylint.readthedocs.io/en/latest/user_guide/messages/warning/cell-var-from-loop.html\n-        d.addBoth(\n+        d2: Deferred[Tuple[TypingAny, TypingAny]] = d.addBoth(\n             lambda result: (\n                 receiver,  # pylint: disable=cell-var-from-loop  # noqa: B023\n                 result,\n             )\n         )\n-        dfds.append(d)\n-    d = DeferredList(dfds)\n-    d.addCallback(lambda out: [x[1] for x in out])\n-    return d\n+        dfds.append(d2)\n+    dl = DeferredList(dfds)\n+    d3: Deferred[List[Tuple[TypingAny, TypingAny]]] = dl.addCallback(\n+        lambda out: [x[1] for x in out]\n+    )\n+    return d3\n \n \n def disconnect_all(signal: TypingAny = Any, sender: TypingAny = Any) -> None:\n\n@@ -30,6 +30,7 @@ from scrapy.utils.boto import is_botocore_available\n \n if TYPE_CHECKING:\n     from twisted.internet.defer import Deferred\n+    from twisted.web.client import Response as TxResponse\n \n \n _T = TypeVar(\"_T\")\n@@ -159,7 +160,7 @@ def mock_google_cloud_storage() -> Tuple[Any, Any, Any]:\n     return (client_mock, bucket_mock, blob_mock)\n \n \n-def get_web_client_agent_req(url: str) -> Deferred:\n+def get_web_client_agent_req(url: str) -> Deferred[TxResponse]:\n     from twisted.internet import reactor\n     from twisted.web.client import Agent  # imports twisted.internet.reactor\n \n\n@@ -22,7 +22,7 @@ class ProcessTest:\n         args: Iterable[str],\n         check_code: bool = True,\n         settings: Optional[str] = None,\n-    ) -> Deferred:\n+    ) -> Deferred[TestProcessProtocol]:\n         from twisted.internet import reactor\n \n         env = os.environ.copy()\n@@ -49,7 +49,7 @@ class ProcessTest:\n \n class TestProcessProtocol(ProcessProtocol):\n     def __init__(self) -> None:\n-        self.deferred: Deferred = Deferred()\n+        self.deferred: Deferred[TestProcessProtocol] = Deferred()\n         self.out: bytes = b\"\"\n         self.err: bytes = b\"\"\n         self.exitcode: Optional[int] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41e15e93e7459673e93ff2591462b47b7ae01566", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2466 | Contributors (this commit): 33 | Commits (past 90d): 4 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -23,13 +23,6 @@ from twisted.internet.defer import (\n     inlineCallbacks,\n     maybeDeferred,\n )\n-\n-try:\n-    # zope >= 5.0 only supports MultipleInvalid\n-    from zope.interface.exceptions import MultipleInvalid\n-except ImportError:\n-    MultipleInvalid = None\n-\n from zope.interface.verify import verifyClass\n \n from scrapy import Spider, signals\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#558b1d11d2f1e3063aba59d444fdb93d42a9ddb9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 43 | Files Changed: 8 | Hunks: 46 | Methods Changed: 33 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 98 | Churn Cumulative: 5606 | Contributors (this commit): 54 | Commits (past 90d): 39 | Contributors (cumulative): 138 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,6 @@ from typing import (\n     TYPE_CHECKING,\n     Any,\n     AsyncGenerator,\n-    Callable,\n     Coroutine,\n     Dict,\n     Iterable,\n@@ -38,6 +37,7 @@ from scrapy.utils.spider import spidercls_for_request\n if TYPE_CHECKING:\n     from twisted.python.failure import Failure\n \n+    from scrapy.http.request import CallbackT\n     from scrapy.spiders import Spider\n \n \n@@ -218,8 +218,8 @@ class Command(BaseRunSpiderCommand):\n         opts: argparse.Namespace,\n         depth: int,\n         spider: Spider,\n-        callback: Callable,\n-    ) -> Tuple[List[Any], List[Request], argparse.Namespace, int, Spider, Callable]:\n+        callback: CallbackT,\n+    ) -> Tuple[List[Any], List[Request], argparse.Namespace, int, Spider, CallbackT]:\n         items, requests = [], []\n         for x in spider_output:\n             if is_item(x):\n@@ -231,7 +231,7 @@ class Command(BaseRunSpiderCommand):\n     def run_callback(\n         self,\n         response: Response,\n-        callback: Callable,\n+        callback: CallbackT,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> Deferred[Any]:\n         cb_kwargs = cb_kwargs or {}\n@@ -240,7 +240,7 @@ class Command(BaseRunSpiderCommand):\n \n     def get_callback_from_rules(\n         self, spider: Spider, response: Response\n-    ) -> Union[Callable, str, None]:\n+    ) -> Union[CallbackT, str, None]:\n         if getattr(spider, \"rules\", None):\n             for rule in spider.rules:  # type: ignore[attr-defined]\n                 if rule.link_extractor.matches(response.url):\n@@ -286,7 +286,7 @@ class Command(BaseRunSpiderCommand):\n     def scraped_data(\n         self,\n         args: Tuple[\n-            List[Any], List[Request], argparse.Namespace, int, Spider, Callable\n+            List[Any], List[Request], argparse.Namespace, int, Spider, CallbackT\n         ],\n     ) -> List[Any]:\n         items, requests, opts, depth, spider, callback = args\n@@ -313,8 +313,8 @@ class Command(BaseRunSpiderCommand):\n         spider: Spider,\n         opts: argparse.Namespace,\n         response: Optional[Response] = None,\n-    ) -> Callable:\n-        cb: Union[str, Callable, None] = None\n+    ) -> CallbackT:\n+        cb: Union[str, CallbackT, None] = None\n         if response:\n             cb = response.meta[\"_callback\"]\n         if not cb:\n\n@@ -16,6 +16,7 @@ from typing import (\n     Optional,\n     Tuple,\n     Type,\n+    cast,\n )\n from unittest import TestCase, TestResult\n \n@@ -62,7 +63,7 @@ class Contract:\n                     if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                         raise TypeError(\"Contracts don't support async callbacks\")\n                     return list(  # pylint: disable=return-in-finally\n-                        iterate_spider_output(cb_result)\n+                        cast(Iterable[Any], iterate_spider_output(cb_result))\n                     )\n \n             request.callback = wrapper\n@@ -79,7 +80,7 @@ class Contract:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n-                output = list(iterate_spider_output(cb_result))\n+                output = list(cast(Iterable[Any], iterate_spider_output(cb_result)))\n                 try:\n                     results.startTest(self.testcase_post)\n                     self.post_process(output)\n@@ -195,7 +196,7 @@ class ContractsManager:\n         def cb_wrapper(response: Response, **cb_kwargs: Any) -> None:\n             try:\n                 output = cb(response, **cb_kwargs)\n-                output = list(iterate_spider_output(output))\n+                output = list(cast(Iterable[Any], iterate_spider_output(output)))\n             except Exception:\n                 case = _create_testcase(method, \"callback\")\n                 results.addError(case, sys.exc_info())\n\n@@ -12,7 +12,6 @@ from typing import (\n     TYPE_CHECKING,\n     Any,\n     AnyStr,\n-    Callable,\n     Dict,\n     Iterable,\n     List,\n@@ -37,8 +36,17 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n+    from twisted.python.failure import Failure\n+\n+    # typing.Concatenate requires Python 3.10\n     # typing.NotRequired and typing.Self require Python 3.11\n-    from typing_extensions import NotRequired, Self\n+    from typing_extensions import Concatenate, NotRequired, Self\n+\n+    from scrapy.http import Response\n+\n+    CallbackT = Callable[Concatenate[Response, ...], Any]\n \n \n class VerboseCookie(TypedDict):\n@@ -110,7 +118,7 @@ class Request(object_ref):\n     def __init__(\n         self,\n         url: str,\n-        callback: Optional[Callable] = None,\n+        callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n@@ -119,7 +127,7 @@ class Request(object_ref):\n         encoding: str = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable] = None,\n+        errback: Optional[Callable[[Failure], Any]] = None,\n         flags: Optional[List[str]] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> None:\n@@ -137,8 +145,8 @@ class Request(object_ref):\n             )\n         if not (callable(errback) or errback is None):\n             raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n-        self.callback: Optional[Callable] = callback\n-        self.errback: Optional[Callable] = errback\n+        self.callback: Optional[CallbackT] = callback\n+        self.errback: Optional[Callable[[Failure], Any]] = errback\n \n         self.cookies: CookiesT = cookies or {}\n         self.headers: Headers = Headers(headers or {}, encoding=encoding)\n\n@@ -27,7 +27,7 @@ from urllib.parse import urljoin\n \n from scrapy.exceptions import NotSupported\n from scrapy.http.headers import Headers\n-from scrapy.http.request import CookiesT, Request\n+from scrapy.http.request import Request\n from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n \n@@ -35,10 +35,12 @@ if TYPE_CHECKING:\n     from ipaddress import IPv4Address, IPv6Address\n \n     from twisted.internet.ssl import Certificate\n+    from twisted.python.failure import Failure\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.http.request import CallbackT, CookiesT\n     from scrapy.selector import SelectorList\n \n \n@@ -196,7 +198,7 @@ class Response(object_ref):\n     def follow(\n         self,\n         url: Union[str, Link],\n-        callback: Optional[Callable] = None,\n+        callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n@@ -205,7 +207,7 @@ class Response(object_ref):\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable] = None,\n+        errback: Optional[Callable[[Failure], Any]] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n         flags: Optional[List[str]] = None,\n     ) -> Request:\n@@ -249,7 +251,7 @@ class Response(object_ref):\n     def follow_all(\n         self,\n         urls: Iterable[Union[str, Link]],\n-        callback: Optional[Callable] = None,\n+        callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n@@ -258,7 +260,7 @@ class Response(object_ref):\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable] = None,\n+        errback: Optional[Callable[[Failure], Any]] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n         flags: Optional[List[str]] = None,\n     ) -> Iterable[Request]:\n\n@@ -41,7 +41,9 @@ from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n if TYPE_CHECKING:\n-    from scrapy.http.request import CookiesT, Request\n+    from twisted.python.failure import Failure\n+\n+    from scrapy.http.request import CallbackT, CookiesT, Request\n     from scrapy.selector import Selector, SelectorList\n \n \n@@ -179,7 +181,7 @@ class TextResponse(Response):\n     def follow(\n         self,\n         url: Union[str, Link, parsel.Selector],\n-        callback: Optional[Callable] = None,\n+        callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n@@ -188,7 +190,7 @@ class TextResponse(Response):\n         encoding: Optional[str] = None,\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable] = None,\n+        errback: Optional[Callable[[Failure], Any]] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n         flags: Optional[List[str]] = None,\n     ) -> Request:\n@@ -232,7 +234,7 @@ class TextResponse(Response):\n     def follow_all(\n         self,\n         urls: Union[Iterable[Union[str, Link]], parsel.SelectorList, None] = None,\n-        callback: Optional[Callable] = None,\n+        callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n@@ -241,7 +243,7 @@ class TextResponse(Response):\n         encoding: Optional[str] = None,\n         priority: int = 0,\n         dont_filter: bool = False,\n-        errback: Optional[Callable] = None,\n+        errback: Optional[Callable[[Failure], Any]] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n         flags: Optional[List[str]] = None,\n         css: Optional[str] = None,\n\n@@ -15,20 +15,16 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n if TYPE_CHECKING:\n-    from collections.abc import Callable\n-\n     from twisted.internet.defer import Deferred\n \n-    # typing.Concatenate requires Python 3.10\n     # typing.Self requires Python 3.11\n-    from typing_extensions import Concatenate, Self\n+    from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n+    from scrapy.http.request import CallbackT\n     from scrapy.settings import BaseSettings, _SettingsKeyT\n     from scrapy.utils.log import SpiderLoggerAdapter\n \n-    CallbackT = Callable[Concatenate[Response, ...], Any]\n-\n \n class Spider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n\n@@ -39,6 +39,7 @@ if TYPE_CHECKING:\n     from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n+    from scrapy.http.request import CallbackT\n \n \n _T = TypeVar(\"_T\")\n@@ -73,7 +74,7 @@ class Rule:\n     def __init__(\n         self,\n         link_extractor: Optional[LinkExtractor] = None,\n-        callback: Union[Callable, str, None] = None,\n+        callback: Union[CallbackT, str, None] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n         follow: Optional[bool] = None,\n         process_links: Union[ProcessLinksT, str, None] = None,\n@@ -81,7 +82,7 @@ class Rule:\n         errback: Union[Callable[[Failure], Any], str, None] = None,\n     ):\n         self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n-        self.callback: Union[Callable, str, None] = callback\n+        self.callback: Union[CallbackT, str, None] = callback\n         self.errback: Union[Callable[[Failure], Any], str, None] = errback\n         self.cb_kwargs: Dict[str, Any] = cb_kwargs or {}\n         self.process_links: Union[ProcessLinksT, str] = process_links or _identity\n@@ -92,7 +93,7 @@ class Rule:\n \n     def _compile(self, spider: Spider) -> None:\n         # this replaces method names with methods and we can't express this in type hints\n-        self.callback = _get_method(self.callback, spider)\n+        self.callback = cast(\"CallbackT\", _get_method(self.callback, spider))\n         self.errback = cast(Callable[[Failure], Any], _get_method(self.errback, spider))\n         self.process_links = cast(\n             ProcessLinksT, _get_method(self.process_links, spider)\n@@ -122,7 +123,9 @@ class CrawlSpider(Spider):\n     def parse_start_url(self, response: Response, **kwargs: Any) -> Any:\n         return []\n \n-    def process_results(self, response: Response, results: Any) -> Any:\n+    def process_results(\n+        self, response: Response, results: Iterable[Any]\n+    ) -> Iterable[Any]:\n         return results\n \n     def _build_request(self, rule_index: int, link: Link) -> Request:\n@@ -152,7 +155,7 @@ class CrawlSpider(Spider):\n         rule = self._rules[cast(int, response.meta[\"rule\"])]\n         return self._parse_response(\n             response,\n-            cast(Callable, rule.callback),\n+            cast(\"CallbackT\", rule.callback),\n             {**rule.cb_kwargs, **cb_kwargs},\n             rule.follow,\n         )\n@@ -166,7 +169,7 @@ class CrawlSpider(Spider):\n     async def _parse_response(\n         self,\n         response: Response,\n-        callback: Optional[Callable],\n+        callback: Optional[CallbackT],\n         cb_kwargs: Dict[str, Any],\n         follow: bool = True,\n     ) -> AsyncIterable[Any]:\n\n@@ -5,7 +5,6 @@ import re\n from typing import (\n     TYPE_CHECKING,\n     Any,\n-    Callable,\n     Dict,\n     Iterable,\n     List,\n@@ -27,6 +26,7 @@ if TYPE_CHECKING:\n     from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n+    from scrapy.http.request import CallbackT\n \n logger = logging.getLogger(__name__)\n \n@@ -34,7 +34,7 @@ logger = logging.getLogger(__name__)\n class SitemapSpider(Spider):\n     sitemap_urls: Sequence[str] = ()\n     sitemap_rules: Sequence[\n-        Tuple[Union[re.Pattern[str], str], Union[str, Callable]]\n+        Tuple[Union[re.Pattern[str], str], Union[str, CallbackT]]\n     ] = [(\"\", \"parse\")]\n     sitemap_follow: Sequence[Union[re.Pattern[str], str]] = [\"\"]\n     sitemap_alternate_links: bool = False\n@@ -54,10 +54,10 @@ class SitemapSpider(Spider):\n \n     def __init__(self, *a: Any, **kw: Any):\n         super().__init__(*a, **kw)\n-        self._cbs: List[Tuple[re.Pattern[str], Callable]] = []\n+        self._cbs: List[Tuple[re.Pattern[str], CallbackT]] = []\n         for r, c in self.sitemap_rules:\n             if isinstance(c, str):\n-                c = cast(Callable, getattr(self, c))\n+                c = cast(\"CallbackT\", getattr(self, c))\n             self._cbs.append((regex(r), c))\n         self._follow: List[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
