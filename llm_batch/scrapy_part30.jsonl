{"custom_id": "scrapy#e5a764cd2ae32fce66cfd85950f5fd62ff6c9f4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 89 | Files Changed: 4 | Hunks: 22 | Methods Changed: 19 | Complexity Δ (Sum/Max): -1/22 | Churn Δ: 189 | Churn Cumulative: 759 | Contributors (this commit): 2 | Commits (past 90d): 29 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,77 @@\n+\"\"\"\n+This module provides additional LinkExtractors, apart from the ones in scrapy.link\n+and scrapy.link.extractors.\n+\n+\"\"\"\n+import urlparse\n+\n+from scrapy.link import Link\n+from scrapy.utils.url import canonicalize_url\n+from scrapy.utils.python import unicode_to_str\n+from scrapy.xpath import HtmlXPathSelector\n+\n+class HTMLImageLinkExtractor(object):\n+    '''HTMLImageLinkExtractor objects are intended to extract image links from HTML pages\n+    given certain xpath locations.\n+\n+    These locations can be passed in a list/tuple either when instanciating the LinkExtractor,\n+    or whenever you call extract_links.\n+    If no locations are specified in any of these places, a default pattern '//img' will be used.\n+    If locations are specified when instanciating the LinkExtractor, and also when calling extract_links,\n+    both locations will be used for that call of extract_links'''\n+\n+    def __init__(self, locations=None, unique=True, canonicalize=True):\n+        self.locations = tuple(locations) if hasattr(locations, '__iter__') else tuple()\n+        self.unique = unique\n+        self.canonicalize = canonicalize\n+\n+    def extract_from_selector(self, selector, parent=None):\n+        ret = []\n+        def _add_link(url_sel, alt_sel=None):\n+            url = url_sel.extract()\n+            alt = alt_sel.extract() if alt_sel else ('', )\n+            if url:\n+                ret.append(Link(unicode_to_str(url[0]), alt[0]))\n+\n+        if selector.xmlNode.type == 'element':\n+            if selector.xmlNode.name == 'img':\n+                _add_link(selector.x('@src'), selector.x('@alt') or selector.x('@title'))\n+            else:\n+                children = selector.x('child::*')\n+                if len(children):\n+                    for child in children:\n+                        ret.extend(self.extract_from_selector(child, parent=selector))\n+                elif selector.xmlNode.name == 'a' and not parent:\n+                    _add_link(selector.x('@href'), selector.x('@title'))\n+        else:\n+            _add_link(selector)\n+\n+        return ret\n+\n+    def extract_links(self, response):\n+        xs = HtmlXPathSelector(response)\n+        base_url = xs.x('//base/@href').extract()\n+        base_url = unicode_to_str(base_url[0]) if base_url else unicode_to_str(response.url)\n+\n+        links = []\n+        for location in self.locations:\n+            selector_res = xs.x(location)\n+            for selector in selector_res:\n+                links.extend(self.extract_from_selector(selector))\n+\n+        seen, ret = set(), []\n+        for link in links:\n+            link.url = urlparse.urljoin(base_url, link.url)\n+            if self.unique:\n+                if link.url in seen:\n+                    continue\n+                else:\n+                    seen.add(link.url)\n+            if self.canonicalize:\n+                link.url = canonicalize_url(link.url)\n+            ret.append(link)\n+\n+        return ret\n+\n+    def matches(self, url):\n+        return False\n\n@@ -29,15 +29,15 @@ class LinkExtractor(FixedSGMLParser):\n       * a function which receives an attribute name and returns whether to scan it\n     \"\"\"\n \n-    def __init__(self, tag=\"a\", attr=\"href\"):\n+    def __init__(self, tag=\"a\", attr=\"href\", unique=False):\n         FixedSGMLParser.__init__(self)\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.current_link = None\n-\n-    def _extract_links(self, response_text, response_url, unique):\n-        self.reset()\n         self.unique = unique\n+\n+    def _extract_links(self, response_text, response_url):\n+        self.reset()\n         self.feed(response_text)\n         self.close()\n \n@@ -46,9 +46,9 @@ class LinkExtractor(FixedSGMLParser):\n             link.url = urljoin(base_url, link.url).strip()\n             yield link\n \n-    def extract_links(self, response, unique=False):\n+    def extract_links(self, response):\n         # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body.to_string(), response.url, unique)\n+        return self._extract_links(response.body.to_string(), response.url)\n \n     def reset(self):\n         FixedSGMLParser.reset(self)\n\n@@ -5,11 +5,9 @@ This module provides some LinkExtractors, which extend that base LinkExtractor\n \"\"\"\n \n import re\n-import urlparse\n \n-from scrapy.link import LinkExtractor, Link\n+from scrapy.link import LinkExtractor\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n-from scrapy.utils.python import unicode_to_str\n from scrapy.xpath import HtmlXPathSelector\n \n _re_type = type(re.compile(\"\", 0))\n@@ -42,24 +40,25 @@ class RegexLinkExtractor(LinkExtractor):\n     \"\"\"\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n-                 tags=('a', 'area'), attrs=('href'), canonicalize=True):\n+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):\n         self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in allow]\n         self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in deny]\n         self.allow_domains = set(allow_domains)\n         self.deny_domains = set(deny_domains)\n         self.restrict_xpaths = restrict_xpaths\n         self.canonicalize = canonicalize\n+        self.unique = unique\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n         LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)\n \n-    def extract_links(self, response, unique=True):\n+    def extract_links(self, response):\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n             html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.x(xpath_expr).extract()) for xpath_expr in self.restrict_xpaths)\n-            links = self._extract_links(html_slice, response.url, unique)\n+            links = self._extract_links(html_slice, response.url)\n         else:\n-            links = LinkExtractor.extract_links(self, response, unique)\n+            links = LinkExtractor.extract_links(self, response)\n \n         links = (link for link in links if _is_valid_url(link.url))\n \n@@ -86,72 +85,3 @@ class RegexLinkExtractor(LinkExtractor):\n         allowed = [regex.search(url) for regex in self.allow_res] if self.allow_res else [True]\n         denied = [regex.search(url) for regex in self.deny_res] if self.deny_res else []\n         return any(allowed) and not any(denied)\n-\n-class ImageLinkExtractor(object):\n-    '''ImageLinkExtractor objects are intended to extract image links from HTML pages\n-    given certain xpath locations.\n-\n-    These locations can be passed in a list/tuple either when instanciating the LinkExtractor,\n-    or whenever you call extract_links.\n-    If no locations are specified in any of these places, a default pattern '//img' will be used.\n-    If locations are specified when instanciating the LinkExtractor, and also when calling extract_links,\n-    both locations will be used for that call of extract_links'''\n-\n-    def __init__(self, locations=None):\n-        self.locations = tuple(locations) if hasattr(locations, '__iter__') else tuple()\n-\n-    def extract_from_selector(self, selector, parent=None):\n-        ret = []\n-        def _add_link(url_sel, alt_sel=None):\n-            url = url_sel.extract()\n-            alt = alt_sel.extract() if alt_sel else ('', )\n-            if url:\n-                ret.append(Link(unicode_to_str(url[0]), alt[0]))\n-\n-        if selector.xmlNode.type == 'element':\n-            if selector.xmlNode.name == 'img':\n-                _add_link(selector.x('@src'), selector.x('@alt') or selector.x('@title'))\n-            else:\n-                children = selector.x('child::*')\n-                if len(children):\n-                    for child in children:\n-                        ret.extend(self.extract_from_selector(child, parent=selector))\n-                elif selector.xmlNode.name == 'a' and not parent:\n-                    _add_link(selector.x('@href'), selector.x('@title'))\n-        else:\n-            _add_link(selector)\n-\n-        return ret\n-\n-    def extract_links(self, response, locations=None, unique=True):\n-        xs = HtmlXPathSelector(response)\n-        base_url = xs.x('//base/@href').extract()\n-        base_url = unicode_to_str(base_url[0]) if base_url else unicode_to_str(response.url)\n-\n-        if hasattr(locations, '__iter__'):\n-            locations = self.locations + tuple(locations)\n-        elif self.locations:\n-            locations = self.locations\n-        else:\n-            locations = ('//img', )\n-\n-        links = []\n-        for location in locations:\n-            selector_res = xs.x(location)\n-            for selector in selector_res:\n-                links.extend(self.extract_from_selector(selector))\n-\n-        seen, ret = set(), []\n-        for link in links:\n-            link.url = urlparse.urljoin(base_url, link.url)\n-            if unique:\n-                if link.url in seen:\n-                    continue\n-                else:\n-                    seen.add(link.url)\n-            ret.append(link)\n-\n-        return ret\n-\n-    def matches(self, url):\n-        return False\n\n@@ -3,7 +3,8 @@ import unittest\n \n from scrapy.http.response import Response, ResponseBody\n from scrapy.link import LinkExtractor, Link\n-from scrapy.link.extractors import RegexLinkExtractor, ImageLinkExtractor\n+from scrapy.link.extractors import RegexLinkExtractor\n+from scrapy.contrib.link_extractors import HTMLImageLinkExtractor\n \n class LinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n@@ -66,7 +67,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n         self.assertEqual(lx.matches('http://blah2.com/blah2'), False)\n \n-#class ImageLinkExtractorTestCase(unittest.TestCase):\n+#class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n #    def setUp(self):\n #        body = open(os.path.join(os.path.dirname(__file__), 'sample_data/image_linkextractor.html'), 'r').read()\n #        self.response = Response(url='http://examplesite.com/index', domain='examplesite.com', body=ResponseBody(body))\n@@ -79,28 +80,31 @@ class LinkExtractorTestCase(unittest.TestCase):\n \n #    def test_extraction(self):\n #        '''Test the extractor's behaviour among different situations'''\n-#        lx = ImageLinkExtractor()\n \n-#        links_1 = lx.extract_links(self.response) # using default locations (//img)\n+#        lx = HTMLImageLinkExtractor(locations=('//img', ))\n+#        links_1 = lx.extract_links(self.response)\n #        self.assertEqual(links_1,\n #            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n #              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n #              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n \n-#        links_2 = lx.extract_links(self.response, unique=False) # using default locations and unique=False\n+#        lx = HTMLImageLinkExtractor(locations=('//img', ), unique=False)\n+#        links_2 = lx.extract_links(self.response, unique=False)\n #        self.assertEqual(links_2,\n #            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n #              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n #              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4'),\n #              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4 repetition') ])\n \n-#        links_3 = lx.extract_links(self.response, locations=('//div[@id=\"wrapper\"]', ))\n+#        lx = HTMLImageLinkExtractor(locations=('//div[@id=\"wrapper\"]', )\n+#        links_3 = lx.extract_links(self.response)\n #        self.assertEqual(links_3,\n #            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n #              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n #              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n \n-#        links_4 = lx.extract_links(self.response, locations=('//a', ))\n+#        lx = HTMLImageLinkExtractor(locations=('//a', )\n+#        links_4 = lx.extract_links(self.response)\n #        self.assertEqual(links_4,\n #            [ Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n #              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1edf0e381e08c13525555653f4287caaec11389", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 24 | Churn Cumulative: 706 | Contributors (this commit): 2 | Commits (past 90d): 31 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,9 +42,12 @@ class LinkExtractor(FixedSGMLParser):\n         self.close()\n \n         base_url = self.base_url if self.base_url else response_url\n+\n+        links = []\n         for link in self.links:\n             link.url = urljoin(base_url, link.url).strip()\n-            yield link\n+            links.append(link)\n+        return links\n \n     def extract_links(self, response):\n         # wrapper needed to allow to work directly with text\n\n@@ -60,19 +60,20 @@ class RegexLinkExtractor(LinkExtractor):\n         else:\n             links = LinkExtractor.extract_links(self, response)\n \n-        links = (link for link in links if _is_valid_url(link.url))\n+        links = [link for link in links if _is_valid_url(link.url)]\n \n         if self.allow_res:\n-            links = (link for link in links if _matches(link.url, self.allow_res))\n+            links = [link for link in links if _matches(link.url, self.allow_res)]\n         if self.deny_res:\n-            links = (link for link in links if not _matches(link.url, self.deny_res))\n+            links = [link for link in links if not _matches(link.url, self.deny_res)]\n         if self.allow_domains:\n-            links = (link for link in links if url_is_from_any_domain(link.url, self.allow_domains))\n+            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]\n         if self.deny_domains:\n-            links = (link for link in links if not url_is_from_any_domain(link.url, self.deny_domains))\n+            links = [link for link in links if not url_is_from_any_domain(link.url, self.deny_domains)]\n \n         if self.canonicalize:\n-            links = (canonicalize_url(link.url) for link in links)\n+            for link in links:\n+                link.url = canonicalize_url(link.url)\n \n         return links\n \n\n@@ -18,8 +18,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n-        links = [link for link in lx.extract_links(response)]\n-        self.assertEqual(links,\n+        self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), \n                           Link(url='http://example.org/about.html', text='About us'),\n                           Link(url='http://example.org/othercat.html', text='Other category'), \n@@ -32,8 +31,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n-        links = [link for link in lx.extract_links(response)]\n-        self.assertEqual(links,\n+        self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n     def test_matches(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a4a4a33cae05c4e33dbf25dc0774277c0d4a867", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 28 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,28 @@\n+import unittest\n+from scrapy.http.response import Response\n+from scrapy.utils.response import body_or_str\n+\n+class ResponseUtilsTest(unittest.TestCase):\n+    dummy_response = Response(domain='example.org', url='http://example.org/', body='dummy_response')\n+\n+    def test_input(self):\n+        self.assertTrue(isinstance(body_or_str(self.dummy_response), basestring))\n+        self.assertTrue(isinstance(body_or_str('text'), basestring))\n+        self.assertRaises(Exception, body_or_str, 2)\n+\n+    def test_extraction(self):\n+        self.assertEqual(body_or_str(self.dummy_response), 'dummy_response')\n+        self.assertEqual(body_or_str('text'), 'text')\n+\n+    def test_encoding(self):\n+        self.assertTrue(isinstance(body_or_str(self.dummy_response, unicode=False), str))\n+        self.assertTrue(isinstance(body_or_str(self.dummy_response, unicode=True), unicode))\n+\n+        self.assertTrue(isinstance(body_or_str('text', unicode=False), str))\n+        self.assertTrue(isinstance(body_or_str('text', unicode=True), unicode))\n+\n+        self.assertTrue(isinstance(body_or_str(u'text', unicode=False), str))\n+        self.assertTrue(isinstance(body_or_str(u'text', unicode=True), unicode))\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#95495f1d9d13e9de00b9bee7c9f4694ad012e2fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 79 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -30,10 +30,6 @@ domain_idle = object()\n # status is a string and its possible values are: \"finished\" or \"cancelled\"\n domain_closed = object()\n \n-# domain has been initialized (successful or not)\n-# args: domain, spider, status\n-domain_initialized = object()\n-\n # New request received from spiders\n # args: request, spider, response\n # response is the response (fed to the spider) which generated the request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c791155aac6d04b5dc80be1cd1a514e6c5e82b46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 304 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -43,18 +43,18 @@ _unreserved_marks = \"-_.!~*'()\" #RFC 2396 sec 2.3\n _safe_chars = urllib.always_safe + '%' + _reserved + _unreserved_marks\n \n def safe_url_string(url, use_encoding='utf8'):\n-    \"\"\"Convert a unicode (or utf8 string) object into a legal URL.\n+    \"\"\"Convert a unicode object (using 'use_encoding' as the encoding), or an already\n+    encoded string into a legal URL.\n \n     Illegal characters are escaped (RFC-3986)\n \n-    It is safe to call this function multiple times. Do not pass this\n-    function strings in encodings other than utf8.\n+    It is safe to call this function multiple times.\n \n     The use_encoding argument is the encoding to use to determine the numerical\n     values in the escaping. For urls on html pages, you should use the original\n     encoding of that page.\n     \"\"\"\n-    s = url.encode(use_encoding)\n+    s = url.encode(use_encoding) if isinstance(url, unicode) else url\n     return urllib.quote(s,  _safe_chars)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63662ba6ffe0c8e6d161dc109015d0c2e1619d47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 118 | Lines Deleted: 30 | Files Changed: 3 | Hunks: 18 | Methods Changed: 10 | Complexity Δ (Sum/Max): 14/12 | Churn Δ: 148 | Churn Cumulative: 854 | Contributors (this commit): 2 | Commits (past 90d): 34 | Contributors (cumulative): 6 | DMM Complexity: 0.4927536231884058\n\nDIFF:\n@@ -3,21 +3,22 @@ LinkExtractor provides en efficient way to extract links from pages\n \"\"\"\n \n from scrapy.utils.python import FixedSGMLParser\n-from scrapy.utils.url import urljoin_rfc as urljoin\n+from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n \n class LinkExtractor(FixedSGMLParser):\n     \"\"\"LinkExtractor are used to extract links from web pages. They are\n     instantiated and later \"applied\" to a Response using the extract_links\n-    method which must receive a Response object and return a dict whoose keys\n-    are the (absolute) urls to follow, and its values any arbitrary data. In\n-    this case the values are the text of the hyperlink.\n+    method which must receive a Response object and return a list of Link objects\n+    containing the (absolute) urls to follow, and the links texts.\n \n     This is the base LinkExtractor class that provides enough basic\n     functionality for extracting links to follow, but you could override this\n     class or create a new one if you need some additional functionality. The\n     only requisite is that the new (or overrided) class must provide a\n-    extract_links method that receives a Response and returns a dict with the\n-    links to follow as its keys.\n+    extract_links method that receives a Response and returns a list of Link objects.\n+\n+    This LinkExtractor always returns percent-encoded URLs, using the detected encoding\n+    from the response.\n \n     The constructor arguments are:\n \n@@ -36,22 +37,38 @@ class LinkExtractor(FixedSGMLParser):\n         self.current_link = None\n         self.unique = unique\n \n-    def _extract_links(self, response_text, response_url):\n+    def _extract_links(self, response_text, response_url, response_encoding):\n         self.reset()\n         self.feed(response_text)\n         self.close()\n \n-        base_url = self.base_url if self.base_url else response_url\n+        links = self.links\n+        if self.unique:\n+            seen = set()\n+            def _seen(url):\n+                if url in seen:\n+                    return True\n+                else:\n+                    seen.add(url)\n+                    return False\n \n-        links = []\n-        for link in self.links:\n+            links = [link for link in links if not _seen(link.url)]\n+\n+        ret = []\n+        base_url = self.base_url if self.base_url else response_url\n+        for link in links:\n             link.url = urljoin(base_url, link.url).strip()\n-            links.append(link)\n-        return links\n+            link.url = safe_url_string(link.url, response_encoding)\n+            link.text = link.text.decode(response_encoding)\n+            ret.append(link)\n+\n+        return ret\n \n     def extract_links(self, response):\n         # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body.to_string(), response.url)\n+        return self._extract_links(response.body.to_string(),\n+            response.url,\n+            response.body.get_real_encoding())\n \n     def reset(self):\n         FixedSGMLParser.reset(self)\n@@ -64,7 +81,6 @@ class LinkExtractor(FixedSGMLParser):\n         if self.scan_tag(tag):\n             for attr, value in attrs:\n                 if self.scan_attr(attr):\n-                    if not self.unique or not value in [link.url for link in self.links]:\n                     link = Link(url=value)\n                     self.links.append(link)\n                     self.current_link = link\n\n@@ -47,10 +47,9 @@ class RegexLinkExtractor(LinkExtractor):\n         self.deny_domains = set(deny_domains)\n         self.restrict_xpaths = restrict_xpaths\n         self.canonicalize = canonicalize\n-        self.unique = unique\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n-        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)\n+        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func, unique=unique)\n \n     def extract_links(self, response):\n         if self.restrict_xpaths:\n\n@@ -34,6 +34,27 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n+    def test_extraction_encoding(self):\n+        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n+        body = open(os.path.join(base_path, 'linkextractor_noenc.html'), 'r').read()\n+        response_utf8 = Response(url='http://example.com/utf8', domain='example.com', body=ResponseBody(body), headers={'Content-Type': ['text/html; charset=utf-8']})\n+        response_noenc = Response(url='http://example.com/noenc', domain='example.com', body=ResponseBody(body))\n+        body = open(os.path.join(base_path, 'linkextractor_latin1.html'), 'r').read()\n+        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=ResponseBody(body))\n+\n+        lx = LinkExtractor()\n+        self.assertEqual(lx.extract_links(response_utf8),\n+            [ Link(url='http://example.com/sample_%C3%B1.html', text=''),\n+              Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')) ])\n+\n+        self.assertEqual(lx.extract_links(response_noenc),\n+            [ Link(url='http://example.com/sample_%C3%B1.html', text=''),\n+              Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')) ])\n+\n+        self.assertEqual(lx.extract_links(response_latin1),\n+            [ Link(url='http://example.com/sample_%F1.html', text=''),\n+              Link(url='http://example.com/sample_%E1.html', text='sample \\xe1 text'.decode('latin1')) ])\n+\n     def test_matches(self):\n         url1 = 'http://lotsofstuff.com/stuff1/index'\n         url2 = 'http://evenmorestuff.com/uglystuff/index'\n@@ -42,6 +63,57 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches(url1), True)\n         self.assertEqual(lx.matches(url2), True)\n \n+class RegexLinkExtractorTestCase(unittest.TestCase):\n+    def setUp(self):\n+        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n+        body = open(os.path.join(base_path, 'regex_linkextractor.html'), 'r').read()\n+        self.response = Response(url='http://example.com/index', domain='example.com', body=ResponseBody(body))\n+\n+    def test_urls_type(self):\n+        '''Test that the resulting urls are regular strings and not a unicode objects'''\n+        lx = RegexLinkExtractor()\n+        self.assertTrue(all(isinstance(link.url, str) for link in lx.extract_links(self.response)))\n+\n+    def test_extraction(self):\n+        '''Test the extractor's behaviour among different situations'''\n+\n+        lx = RegexLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n+              Link(url='http://example.com/sample2.html', text=''),\n+              Link(url='http://example.com/sample3.html', text='sample 3 text'),\n+              Link(url='http://www.google.com/something', text='') ])\n+\n+        lx = RegexLinkExtractor(allow=('sample', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n+              Link(url='http://example.com/sample2.html', text=''),\n+              Link(url='http://example.com/sample3.html', text='sample 3 text') ])\n+\n+        lx = RegexLinkExtractor(allow=('sample', ), unique=False)\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n+              Link(url='http://example.com/sample2.html', text=''),\n+              Link(url='http://example.com/sample3.html', text='sample 3 text'),\n+              Link(url='http://example.com/sample3.html', text='sample 3 repetition') ])\n+\n+        lx = RegexLinkExtractor(allow=('sample', ), deny=('3', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n+              Link(url='http://example.com/sample2.html', text='') ])\n+\n+        lx = RegexLinkExtractor(allow_domains=('google.com', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://www.google.com/something', text='') ])\n+\n+        lx = RegexLinkExtractor(tags=('img', ), attrs=('src', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample2.jpg', text='') ])\n+\n+    def test_matches(self):\n+        url1 = 'http://lotsofstuff.com/stuff1/index'\n+        url2 = 'http://evenmorestuff.com/uglystuff/index'\n+\n         lx = RegexLinkExtractor(allow=(r'stuff1', ))\n         self.assertEqual(lx.matches(url1), True)\n         self.assertEqual(lx.matches(url2), False)\n@@ -67,8 +139,9 @@ class LinkExtractorTestCase(unittest.TestCase):\n \n #class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n #    def setUp(self):\n-#        body = open(os.path.join(os.path.dirname(__file__), 'sample_data/image_linkextractor.html'), 'r').read()\n-#        self.response = Response(url='http://examplesite.com/index', domain='examplesite.com', body=ResponseBody(body))\n+#        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n+#        body = open(os.path.join(base_path 'image_linkextractor.html'), 'r').read()\n+#        self.response = Response(url='http://example.com/index', domain='example.com', body=ResponseBody(body))\n \n #    def test_urls_type(self):\n #        '''Test that the resulting urls are regular strings and not a unicode objects'''\n@@ -82,30 +155,30 @@ class LinkExtractorTestCase(unittest.TestCase):\n #        lx = HTMLImageLinkExtractor(locations=('//img', ))\n #        links_1 = lx.extract_links(self.response)\n #        self.assertEqual(links_1,\n-#            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n-#              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n+#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n+#              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://example.com/sample4.jpg', text=u'sample 4') ])\n \n #        lx = HTMLImageLinkExtractor(locations=('//img', ), unique=False)\n #        links_2 = lx.extract_links(self.response, unique=False)\n #        self.assertEqual(links_2,\n-#            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n-#              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4'),\n-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4 repetition') ])\n+#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n+#              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://example.com/sample4.jpg', text=u'sample 4'),\n+#              Link(url='http://example.com/sample4.jpg', text=u'sample 4 repetition') ])\n \n #        lx = HTMLImageLinkExtractor(locations=('//div[@id=\"wrapper\"]', )\n #        links_3 = lx.extract_links(self.response)\n #        self.assertEqual(links_3,\n-#            [ Link(url='http://examplesite.com/sample1.jpg', text=u'sample 1'),\n-#              Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])\n+#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),\n+#              Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://example.com/sample4.jpg', text=u'sample 4') ])\n \n #        lx = HTMLImageLinkExtractor(locations=('//a', )\n #        links_4 = lx.extract_links(self.response)\n #        self.assertEqual(links_4,\n-#            [ Link(url='http://examplesite.com/sample2.jpg', text=u'sample 2'),\n-#              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])\n+#            [ Link(url='http://example.com/sample2.jpg', text=u'sample 2'),\n+#              Link(url='http://example.com/sample3.html', text=u'sample 3') ])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47881c6e64f80ad8cce72c5eb0732eb5d87bee7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 16 | Churn Cumulative: 449 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # converting an utf-8 encoded string to unicode\n         self.assertEqual(str_to_unicode('lel\\xc3\\xb1e'), u'lel\\xf1e')\n \n+        # converting a latin-1 encoded string to unicode\n+        self.assertEqual(str_to_unicode('lel\\xf1e', 'latin-1'), u'lel\\xf1e')\n+\n         # converting a unicode to unicode should return the same object\n         self.assertEqual(str_to_unicode(u'\\xf1e\\xf1e\\xf1e'), u'\\xf1e\\xf1e\\xf1e')\n \n@@ -17,6 +20,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # converting a unicode object to an utf-8 encoded string\n         self.assertEqual(unicode_to_str(u'\\xa3 49'), '\\xc2\\xa3 49')\n \n+        # converting a unicode object to a latin-1 encoded string\n+        self.assertEqual(unicode_to_str(u'\\xa3 49', 'latin-1'), '\\xa3 49')\n+\n         # converting a regular string to string should return the same object\n         self.assertEqual(unicode_to_str('lel\\xf1e'), 'lel\\xf1e')\n \n\n@@ -53,17 +53,17 @@ def unique(list_):\n     return result\n \n \n-def str_to_unicode(text):\n+def str_to_unicode(text, encoding='utf-8'):\n     if isinstance(text, str):\n-        return text.decode('utf-8')\n+        return text.decode(encoding)\n     elif isinstance(text, unicode):\n         return text\n     else:\n         raise TypeError('str_to_unicode can only receive a string object')\n \n-def unicode_to_str(text):\n+def unicode_to_str(text, encoding='utf-8'):\n     if isinstance(text, unicode):\n-        return text.encode('utf-8')\n+        return text.encode(encoding)\n     elif isinstance(text, str):\n         return text\n     else:\n\n@@ -54,7 +54,7 @@ def safe_url_string(url, use_encoding='utf8'):\n     values in the escaping. For urls on html pages, you should use the original\n     encoding of that page.\n     \"\"\"\n-    s = url.encode(use_encoding) if isinstance(url, unicode) else url\n+    s = unicode_to_str(url, use_encoding)\n     return urllib.quote(s,  _safe_chars)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e95a28ef3d055384a1ce1b14029820143c1bfad2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 21 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,3 +4,18 @@ def setup(app):\n         rolename      = \"setting\",\n         indextemplate = \"pair: %s; setting\",\n     )\n+    app.add_crossref_type(\n+        directivename = \"topic\",\n+        rolename      = \"topic\",\n+        indextemplate = \"pair: %s; topic\",\n+    )\n+    app.add_crossref_type(\n+        directivename = \"signal\",\n+        rolename      = \"signal\",\n+        indextemplate = \"pair: %s; signal\",\n+    )\n+    app.add_crossref_type(\n+        directivename = \"exception\",\n+        rolename      = \"exception\",\n+        indextemplate = \"pair: %s; exception\",\n+    )\n\n@@ -46,9 +46,9 @@ copyright = u'2008, Insophia'\n # built documents.\n #\n # The short X.Y version.\n-version = '1.0'\n+version = '0.7'\n # The full version, including alpha/beta/rc tags.\n-release = '1.0'\n+release = '0.7'\n \n # The language for content autogenerated by Sphinx. Refer to documentation\n # for a list of supported languages.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd2f92f040aa6a7f4ba852034a8f379a7cb3faed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 133 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,8 @@\n \"\"\"\n Scrapy core exceptions\n+\n+These exceptions are documented in docs/ref/exceptions.rst. Please don't add\n+new exceptions here without documenting them there.\n \"\"\"\n \n # Internal\n\n@@ -1,6 +1,10 @@\n \"\"\"\n Scrapy core signals\n+\n+These signals are documented in docs/ref/signals.rst. Please don't add new\n+signals here without documenting them there.\n \"\"\"\n+\n from pydispatch import dispatcher\n \n from scrapy import log\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc36ad002fe555e0caf55597a51451185c059c5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 717 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ from scrapy.core.downloader import Downloader\n from scrapy.core.exceptions import IgnoreRequest, HttpException, DontCloseDomain\n from scrapy.http import Response, Request\n from scrapy.item import ScrapedItem\n-from scrapy.item.pipeline import ItemPipeline\n+from scrapy.item.pipeline import ItemPipelineManager\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n@@ -59,7 +59,7 @@ class ExecutionEngine(object):\n         self.downloader = downloader or Downloader()\n         self.spidermiddleware = SpiderMiddlewareManager()\n         self._scraping = {}\n-        self.pipeline = ItemPipeline()\n+        self.pipeline = ItemPipelineManager()\n         # key dictionary of per domain lists of initial requests to scrape\n         self.starters = {}\n \n\n@@ -5,7 +5,7 @@ from scrapy.utils.misc import load_class\n from scrapy.utils.defer import defer_succeed, mustbe_deferred\n from scrapy.conf import settings\n \n-class ItemPipeline(object):\n+class ItemPipelineManager(object):\n \n     def __init__(self):\n         self.loaded = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8a6e2fc7c7729a3f7dbea5a91290ed3c5f5445f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 117 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,14 +41,15 @@ def flatten(x):\n     return result\n \n \n-def unique(list_):\n+def unique(list_, key=lambda x: x):\n     \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n     seen = {}\n     result = []\n     for item in list_:\n-        if item in seen: \n+        seenkey = key(item)\n+        if seenkey in seen: \n             continue\n-        seen[item] = 1\n+        seen[seenkey] = 1\n         result.append(item)\n     return result\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c69d94fb459b8b3faae1597eb16b93e20fb4832a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 119 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -84,7 +84,7 @@ def re_rsearch(pattern, text, chunk_size=1024):\n     the start position of the match, and the ending (regarding the entire text).\n     \"\"\"\n     def _chunk_iter():\n-        offset = text_len = len(text)\n+        offset = len(text)\n         while True:\n             offset -= (chunk_size * 1024)\n             if offset <= 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#541451afb3ed8584da902b274dc47148cbe4e4fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 16 | Churn Cumulative: 219 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@\n LinkExtractor provides en efficient way to extract links from pages\n \"\"\"\n \n-from scrapy.utils.python import FixedSGMLParser\n+from scrapy.utils.python import FixedSGMLParser, unique as unique_list\n from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n \n class LinkExtractor(FixedSGMLParser):\n@@ -28,6 +28,8 @@ class LinkExtractor(FixedSGMLParser):\n     * attr (string or function)\n       * an attribute name which is used to search for links (defaults to \"href\")\n       * a function which receives an attribute name and returns whether to scan it\n+    * unique - if True the same urls won't be extracted twice, otherwise the\n+      same urls will be extracted multiple times (with potentially different link texts)\n     \"\"\"\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False):\n@@ -42,17 +44,7 @@ class LinkExtractor(FixedSGMLParser):\n         self.feed(response_text)\n         self.close()\n \n-        links = self.links\n-        if self.unique:\n-            seen = set()\n-            def _seen(url):\n-                if url in seen:\n-                    return True\n-                else:\n-                    seen.add(url)\n-                    return False\n-\n-            links = [link for link in links if not _seen(link.url)]\n+        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n \n         ret = []\n         base_url = self.base_url if self.base_url else response_url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#093c7e121b53f0bdff33677f91459192d400472d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 10 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008, Insophia'\n+copyright = u'2008-2009, Insophia'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -116,11 +116,11 @@ html_static_path = ['_static']\n \n # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n # using the given strftime format.\n-#html_last_updated_fmt = '%b %d, %Y'\n+html_last_updated_fmt = '%b %d, %Y'\n \n # If true, SmartyPants will be used to convert quotes and dashes to\n # typographically correct entities.\n-#html_use_smartypants = True\n+html_use_smartypants = True\n \n # Custom sidebar templates, maps document names to template names.\n #html_sidebars = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a9845b3e89b6368a524aa4db201ca7605d55ce7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 87 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 93 | Churn Cumulative: 281 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,11 @@\n \"\"\"\n-request-response middleware extension\n+This module implements the Downloader Middleware manager. For more information\n+see the Downloader Middleware doc in:\n+\n+docs/topics/downloader-middleware.rst\n+\n \"\"\"\n+\n from scrapy.core import signals\n from scrapy import log\n from scrapy.http import Request, Response\n@@ -11,93 +16,7 @@ from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n \n class DownloaderMiddlewareManager(object):\n-    \"\"\"Request-Response Middleware Manager\n \n-    Middleware is a framework of hooks into Scrapy's request/response\n-    processing.  It's a light, low-level \"spider\" system for globally altering\n-    Scrapy's input and/or output.\n-\n-    Middleware is heavily based on Django middleware system, at the point that\n-    it tries to mimic Django middleware behaviour. For Scrapy, the Django's\n-    view function has the same meaning of the final download handler function\n-    to use for the request's url.\n-\n-    To activate a middleware component, add it to the DOWNLOADER_MIDDLEWARES list\n-    in your Scrapy settings.  In DOWNLOADER_MIDDLEWARES, each middleware component\n-    is represented by a string: the full Python path to the middleware's class\n-    name. For example:\n-\n-    DOWNLOADER_MIDDLEWARES = (\n-            'scrapy.contrib.middleware.common.SpiderMiddleware',\n-            'scrapy.contrib.middleware.common.CommonMiddleware',\n-            'scrapy.contrib.middleware.redirect.RedirectMiddleware',\n-            'scrapy.contrib.middleware.cache.CacheMiddleware',\n-    )\n-\n-    Writing your own middleware is easy. Each middleware component is a single\n-    Python class that defines one or more of the following methods:\n-\n-\n-    process_request(self, request, spider)\n-\n-        `request` is a Request object.\n-        `spider` is a BaseSpider object\n-\n-        This method is called in each request until scrapy decides which\n-        download function to use.\n-\n-        process_request() should return either None, Response or Request.\n-\n-        If returns None, Scrapy will continue processing this request,\n-        executing any other middleware and, then, the appropiate download\n-        function.\n-\n-        If returns a Response object, Scrapy won't bother calling ANY other\n-        request or exception middleware, or the appropiate download function;\n-        it'll return that Response. Response middleware is always called on\n-        every response.\n-\n-        If returns a Request object, returned request is used to instruct a\n-        redirection. Redirection is handled inside middleware scope, and\n-        original request don't finish until redirected request is completed.\n-\n-\n-    process_response(self, request, response, spider):\n-\n-        `request` is a Request object\n-        `response` is a Response object\n-        `spider` is a BaseSpider object\n-\n-        process_response MUST return a Response object. It could alter the given\n-        response, or it could create a brand-new Response.\n-        To drop the response entirely an IgnoreRequest exception must be raised.\n-\n-\n-    process_exception(self, request, exception, spider)\n-\n-        `request` is a Request object.\n-        `exception` is an Exception object\n-        `spider` is a BaseSpider object\n-\n-        Scrapy calls process_exception() when a download handler or\n-        process_request middleware raises an exception.\n-\n-        process_exception() should return either None, Response or Request object.\n-\n-        if it returns None, Scrapy will continue processing this exception,\n-        executing any other exception middleware, until no middleware left and\n-        default exception handling kicks in.\n-\n-        If it returns a Response object, the response middleware kicks in, and\n-        won't bother calling ANY other exception middleware.\n-\n-        If it returns a Request object, returned request is used to instruct a\n-        immediate redirection. Redirection is handled inside middleware scope,\n-        and original request don't finish until redirected request is\n-        completed. This stop process_exception middleware as returning\n-        Response does.\n-\n-    \"\"\"\n     def __init__(self):\n         self.loaded = False\n         self.request_middleware = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
