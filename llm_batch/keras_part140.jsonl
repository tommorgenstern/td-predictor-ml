{"custom_id": "keras#3071e0de2f498f1df86f91f9729e20aacc8025df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 36 | Churn Cumulative: 2277 | Contributors (this commit): 37 | Commits (past 90d): 16 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,6 +18,13 @@ from pkg_resources import parse_version\n \n \n class CallbackList(object):\n+    \"\"\"Container abstracting a list of callbacks.\n+\n+    # Arguments\n+        callbacks: List of `Callback` instances.\n+        queue_length: Queue length for keeping\n+            running statistics over callback execution time.\n+    \"\"\"\n \n     def __init__(self, callbacks=None, queue_length=10):\n         callbacks = callbacks or []\n@@ -146,11 +153,9 @@ class Callback(object):\n \n \n class BaseLogger(Callback):\n-    \"\"\"Callback that accumulates epoch averages of\n-    the metrics being monitored.\n+    \"\"\"Callback that accumulates epoch averages of metrics.\n \n-    This callback is automatically applied to\n-    every Keras model.\n+    This callback is automatically applied to every Keras model.\n     \"\"\"\n \n     def on_epoch_begin(self, epoch, logs=None):\n@@ -219,8 +224,7 @@ class ProgbarLogger(Callback):\n \n \n class History(Callback):\n-    \"\"\"Callback that records events\n-    into a `History` object.\n+    \"\"\"Callback that records events into a `History` object.\n \n     This callback is automatically applied to\n     every Keras model. The `History` object\n@@ -424,19 +428,24 @@ class RemoteMonitor(Callback):\n     JSON-encoded dictionary of event data.\n \n     # Arguments\n-        root: Root url of the target server.\n-        path: Path relative to `root` to which the events will be sent.\n-        field: JSON field under which the data will be stored.\n-        headers: Optional custom HTTP headers.\n+        root: String; root url of the target server.\n+        path: String; path relative to `root` to which the events will be sent.\n+        field: String; JSON field under which the data will be stored.\n+        headers: Dictionary; optional custom HTTP headers.\n+            Defaults to:\n+            `{'Accept': 'application/json',\n+              'Content-Type': 'application/json'}`\n     \"\"\"\n \n     def __init__(self,\n                  root='http://localhost:9000',\n                  path='/publish/epoch/end/',\n                  field='data',\n-                 headers={'Accept': 'application/json',\n-                          'Content-Type': 'application/json'}):\n+                 headers=None):\n         super(RemoteMonitor, self).__init__()\n+        if headers is None:\n+            headers = {'Accept': 'application/json',\n+                       'Content-Type': 'application/json'}\n         self.root = root\n         self.path = path\n         self.field = field\n@@ -515,7 +524,7 @@ class TensorBoard(Callback):\n                  write_graph=True,\n                  write_images=False):\n         super(TensorBoard, self).__init__()\n-        if K._BACKEND != 'tensorflow':\n+        if K.backend() != 'tensorflow':\n             raise RuntimeError('TensorBoard callback only works '\n                                'with the TensorFlow backend.')\n         self.log_dir = log_dir\n@@ -718,6 +727,7 @@ class ReduceLROnPlateau(Callback):\n \n class CSVLogger(Callback):\n     \"\"\"Callback that streams epoch results to a csv file.\n+\n     Supports all values that can be represented as a string,\n     including 1D iterables such as np.ndarray.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97b0f9f6e41a16c37276bcc2ad06f1ee0c139c6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 74 | Churn Cumulative: 249 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,10 @@\n from __future__ import absolute_import\n from . import backend as K\n+from .utils.generic_utils import get_from_module\n \n \n class Constraint(object):\n+\n     def __call__(self, p):\n         return p\n \n@@ -11,26 +13,29 @@ class Constraint(object):\n \n \n class MaxNorm(Constraint):\n-    '''Constrain the weights incident to each hidden unit to have a norm less than or equal to a desired value.\n+    \"\"\"MaxNorm weight constraint.\n+\n+    Constrains the weights incident to each hidden unit\n+    to have a norm less than or equal to a desired value.\n \n     # Arguments\n         m: the maximum norm for the incoming weights.\n-        axis: integer, axis along which to calculate weight norms. For instance,\n-            in a `Dense` layer the weight matrix has shape (input_dim, output_dim),\n-            set `axis` to `0` to constrain each weight vector of length (input_dim).\n-            In a `MaxoutDense` layer the weight tensor has shape (nb_feature, input_dim, output_dim),\n-            set `axis` to `1` to constrain each weight vector of length (input_dim),\n-            i.e. constrain the filters incident to the `max` operation.\n-            In a `Convolution2D` layer with the Theano backend, the weight tensor\n-            has shape (nb_filter, stack_size, nb_row, nb_col), set `axis` to `[1,2,3]`\n-            to constrain the weights of each filter tensor of size (stack_size, nb_row, nb_col).\n-            In a `Convolution2D` layer with the TensorFlow backend, the weight tensor\n-            has shape (nb_row, nb_col, stack_size, nb_filter), set `axis` to `[0,1,2]`\n-            to constrain the weights of each filter tensor of size (nb_row, nb_col, stack_size).\n+        axis: integer, axis along which to calculate weight norms.\n+            For instance, in a `Dense` layer the weight matrix\n+            has shape `(input_dim, output_dim)`,\n+            set `axis` to `0` to constrain each weight vector\n+            of length `(input_dim,)`.\n+            In a `Convolution2D` layer with `dim_ordering=\"tf\"`,\n+            the weight tensor has shape\n+            `(rows, cols, input_depth, output_depth)`,\n+            set `axis` to `[0, 1, 2]`\n+            to constrain the weights of each filter tensor of size\n+            `(rows, cols, input_depth)`.\n \n     # References\n         - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n-    '''\n+    \"\"\"\n+\n     def __init__(self, m=2, axis=0):\n         self.m = m\n         self.axis = axis\n@@ -48,46 +53,51 @@ class MaxNorm(Constraint):\n \n \n class NonNeg(Constraint):\n-    '''Constrain the weights to be non-negative.\n-    '''\n+    \"\"\"Constrains the weights to be non-negative.\n+    \"\"\"\n+\n     def __call__(self, p):\n         p *= K.cast(p >= 0., K.floatx())\n         return p\n \n \n class UnitNorm(Constraint):\n-    '''Constrain the weights incident to each hidden unit to have unit norm.\n+    \"\"\"Constrains the weights incident to each hidden unit to have unit norm.\n \n     # Arguments\n-        axis: integer, axis along which to calculate weight norms. For instance,\n-            in a `Dense` layer the weight matrix has shape (input_dim, output_dim),\n-            set `axis` to `0` to constrain each weight vector of length (input_dim).\n-            In a `MaxoutDense` layer the weight tensor has shape (nb_feature, input_dim, output_dim),\n-            set `axis` to `1` to constrain each weight vector of length (input_dim),\n-            i.e. constrain the filters incident to the `max` operation.\n-            In a `Convolution2D` layer with the Theano backend, the weight tensor\n-            has shape (nb_filter, stack_size, nb_row, nb_col), set `axis` to `[1,2,3]`\n-            to constrain the weights of each filter tensor of size (stack_size, nb_row, nb_col).\n-            In a `Convolution2D` layer with the TensorFlow backend, the weight tensor\n-            has shape (nb_row, nb_col, stack_size, nb_filter), set `axis` to `[0,1,2]`\n-            to constrain the weights of each filter tensor of size (nb_row, nb_col, stack_size).\n-    '''\n+        axis: integer, axis along which to calculate weight norms.\n+            For instance, in a `Dense` layer the weight matrix\n+            has shape `(input_dim, output_dim)`,\n+            set `axis` to `0` to constrain each weight vector\n+            of length `(input_dim,)`.\n+            In a `Convolution2D` layer with `dim_ordering=\"tf\"`,\n+            the weight tensor has shape\n+            `(rows, cols, input_depth, output_depth)`,\n+            set `axis` to `[0, 1, 2]`\n+            to constrain the weights of each filter tensor of size\n+            `(rows, cols, input_depth)`.\n+    \"\"\"\n+\n     def __init__(self, axis=0):\n         self.axis = axis\n \n     def __call__(self, p):\n-        return p / (K.epsilon() + K.sqrt(K.sum(K.square(p), axis=self.axis, keepdims=True)))\n+        return p / (K.epsilon() + K.sqrt(K.sum(K.square(p),\n+                                         axis=self.axis,\n+                                         keepdims=True)))\n \n     def get_config(self):\n         return {'name': self.__class__.__name__,\n                 'axis': self.axis}\n \n \n+# Aliases.\n+\n maxnorm = MaxNorm\n nonneg = NonNeg\n unitnorm = UnitNorm\n \n-from .utils.generic_utils import get_from_module\n+\n def get(identifier, kwargs=None):\n     return get_from_module(identifier, globals(), 'constraint',\n                            instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b261704cfccedaaef0845808f4a1028e53e175b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1592 | Contributors (this commit): 16 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from keras.layers import convolutional, pooling\n \n \n # TensorFlow does not support full convolution.\n-if K._BACKEND == 'theano':\n+if K.backend() == 'theano':\n     _convolution_border_modes = ['valid', 'same', 'full']\n else:\n     _convolution_border_modes = ['valid', 'same']\n@@ -214,7 +214,7 @@ def test_atrous_conv_2d():\n                            input_shape=(nb_samples, nb_row, nb_col, stack_size))\n \n \n-@pytest.mark.skipif(K._BACKEND != 'tensorflow', reason=\"Requires TF backend\")\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n @keras_test\n def test_separable_conv_2d():\n     nb_samples = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a04d968422861e086e69f9dbeb80ca0cca5394bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 63 | Files Changed: 4 | Hunks: 49 | Methods Changed: 29 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 189 | Churn Cumulative: 1862 | Contributors (this commit): 29 | Commits (past 90d): 14 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ def get_fans(shape, dim_ordering='th'):\n         fan_in = shape[0]\n         fan_out = shape[1]\n     elif len(shape) == 4 or len(shape) == 5:\n-        # assuming convolution kernels (2D or 3D).\n+        # Assuming convolution kernels (2D or 3D).\n         # TH kernel shape: (depth, input_depth, ...)\n         # TF kernel shape: (..., input_depth, depth)\n         if dim_ordering == 'th':\n@@ -23,7 +23,7 @@ def get_fans(shape, dim_ordering='th'):\n         else:\n             raise ValueError('Invalid dim_ordering: ' + dim_ordering)\n     else:\n-        # no specific assumptions\n+        # No specific assumptions.\n         fan_in = np.sqrt(np.prod(shape))\n         fan_out = np.sqrt(np.prod(shape))\n     return fan_in, fan_out\n@@ -38,7 +38,10 @@ def normal(shape, scale=0.05, name=None):\n \n \n def lecun_uniform(shape, name=None, dim_ordering='th'):\n-    ''' Reference: LeCun 98, Efficient Backprop\n+    '''LeCun uniform variance scaling initializer.\n+\n+    # Reference\n+        LeCun 98, Efficient Backprop\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n     '''\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n@@ -47,7 +50,10 @@ def lecun_uniform(shape, name=None, dim_ordering='th'):\n \n \n def glorot_normal(shape, name=None, dim_ordering='th'):\n-    ''' Reference: Glorot & Bengio, AISTATS 2010\n+    '''Glorot normal variance scaling initializer.\n+\n+    # Reference\n+        Glorot & Bengio, AISTATS 2010\n     '''\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(2. / (fan_in + fan_out))\n@@ -61,7 +67,10 @@ def glorot_uniform(shape, name=None, dim_ordering='th'):\n \n \n def he_normal(shape, name=None, dim_ordering='th'):\n-    ''' Reference:  He et al., http://arxiv.org/abs/1502.01852\n+    '''He normal variance scaling initializer.\n+\n+    # Reference\n+        He et al., http://arxiv.org/abs/1502.01852\n     '''\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(2. / fan_in)\n@@ -69,18 +78,23 @@ def he_normal(shape, name=None, dim_ordering='th'):\n \n \n def he_uniform(shape, name=None, dim_ordering='th'):\n+    '''He uniform variance scaling initializer.\n+    '''\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(6. / fan_in)\n     return uniform(shape, s, name=name)\n \n \n def orthogonal(shape, scale=1.1, name=None):\n-    ''' From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n+    '''Orthogonal initializer.\n+\n+    # Reference\n+        Saxe et al., http://arxiv.org/abs/1312.6120\n     '''\n     flat_shape = (shape[0], np.prod(shape[1:]))\n     a = np.random.normal(0.0, 1.0, flat_shape)\n     u, _, v = np.linalg.svd(a, full_matrices=False)\n-    # pick the one with the correct shape\n+    # Pick the one with the correct shape.\n     q = u if u.shape == flat_shape else v\n     q = q.reshape(shape)\n     return K.variable(scale * q[:shape[0], :shape[1]], name=name)\n\n@@ -4,132 +4,157 @@ from .utils.generic_utils import get_from_module\n \n \n def binary_accuracy(y_true, y_pred):\n-    '''Calculates the mean accuracy rate across all predictions for binary\n+    \"\"\"Binary accuracy metric.\n+\n+    Computes the mean accuracy rate across all predictions for binary\n     classification problems.\n-    '''\n+    \"\"\"\n     return K.mean(K.equal(y_true, K.round(y_pred)))\n \n \n def categorical_accuracy(y_true, y_pred):\n-    '''Calculates the mean accuracy rate across all predictions for\n+    \"\"\"Categorical accuracy metric.\n+\n+    Computes the mean accuracy rate across all predictions for\n     multiclass classification problems.\n-    '''\n+    \"\"\"\n     return K.mean(K.equal(K.argmax(y_true, axis=-1),\n                   K.argmax(y_pred, axis=-1)))\n \n \n def sparse_categorical_accuracy(y_true, y_pred):\n-    '''Same as categorical_accuracy, but useful when the predictions are for\n+    \"\"\"Sparse version of the categorical accuracy metric.\n+\n+    Same as categorical_accuracy, but useful when the predictions are for\n     sparse targets.\n-    '''\n+    \"\"\"\n     return K.mean(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n \n \n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n-    '''Calculates the top-k categorical accuracy rate, i.e. success when the\n+    \"\"\"Categorical accuracy metric for top-k accuracy.\n+\n+    Computes the top-k categorical accuracy rate, i.e. success when the\n     target class is within the top-k predictions provided.\n-    '''\n+    \"\"\"\n     return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n \n \n def mean_squared_error(y_true, y_pred):\n-    '''Calculates the mean squared error (mse) rate\n+    \"\"\"Mean squared error metric.\n+\n+    Computes the mean squared error (mse) rate\n     between predicted and target values.\n-    '''\n+    \"\"\"\n     return K.mean(K.square(y_pred - y_true))\n \n \n def mean_absolute_error(y_true, y_pred):\n-    '''Calculates the mean absolute error (mae) rate\n+    \"\"\"Mean absolute error metric.\n+\n+    Computes the mean absolute error (mae) rate\n     between predicted and target values.\n-    '''\n+    \"\"\"\n     return K.mean(K.abs(y_pred - y_true))\n \n \n def mean_absolute_percentage_error(y_true, y_pred):\n-    '''Calculates the mean absolute percentage error (mape) rate\n+    \"\"\"Mean absolute percentage error metric.\n+\n+    Computes the mean absolute percentage error (mape) rate\n     between predicted and target values.\n-    '''\n-    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), np.inf))\n+    \"\"\"\n+    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n+                                            K.epsilon(),\n+                                            np.inf))\n     return 100. * K.mean(diff)\n \n \n def mean_squared_logarithmic_error(y_true, y_pred):\n-    '''Calculates the mean squared logarithmic error (msle) rate\n+    \"\"\"Mean squared logarithmic error metric.\n+\n+    Computes the mean squared logarithmic error (msle) rate\n     between predicted and target values.\n-    '''\n+    \"\"\"\n     first_log = K.log(K.clip(y_pred, K.epsilon(), np.inf) + 1.)\n     second_log = K.log(K.clip(y_true, K.epsilon(), np.inf) + 1.)\n     return K.mean(K.square(first_log - second_log))\n \n \n def hinge(y_true, y_pred):\n-    '''Calculates the hinge loss, which is defined as\n+    \"\"\"Hinge loss metric.\n+\n+    Computes the hinge loss, which is defined as\n     `max(1 - y_true * y_pred, 0)`.\n-    '''\n+    \"\"\"\n     return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n \n \n def squared_hinge(y_true, y_pred):\n-    '''Calculates the squared value of the hinge loss.\n-    '''\n+    \"\"\"Computes the squared value of the hinge loss.\n+    \"\"\"\n     return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n \n \n def categorical_crossentropy(y_true, y_pred):\n-    '''Calculates the cross-entropy value for multiclass classification\n+    \"\"\"Categorical cross-entropy metric.\n+\n+    Computes the cross-entropy value for multiclass classification\n     problems. Note: Expects a binary class matrix instead of a vector\n     of scalar classes.\n-    '''\n+    \"\"\"\n     return K.mean(K.categorical_crossentropy(y_pred, y_true))\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    '''Calculates the cross-entropy value for multiclass classification\n+    \"\"\"Sparse version of the categorical cross-entropy metric.\n+\n+    Computes the cross-entropy value for multiclass classification\n     problems with sparse targets. Note: Expects an array of integer\n     classes. Labels shape must have the same number of dimensions as\n     output shape. If you get a shape error, add a length-1 dimension\n     to labels.\n-    '''\n+    \"\"\"\n     return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n \n \n def binary_crossentropy(y_true, y_pred):\n-    '''Calculates the cross-entropy value for binary classification\n-    problems.\n-    '''\n+    \"\"\"Computes the cross-entropy value for binary classification problems.\n+    \"\"\"\n     return K.mean(K.binary_crossentropy(y_pred, y_true))\n \n \n def kullback_leibler_divergence(y_true, y_pred):\n-    '''Calculates the Kullback-Leibler (KL) divergence between prediction\n-    and target values.\n-    '''\n+    \"\"\"Computes the KLdivergence between prediction and target values.\n+    \"\"\"\n     y_true = K.clip(y_true, K.epsilon(), 1)\n     y_pred = K.clip(y_pred, K.epsilon(), 1)\n     return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n \n \n def poisson(y_true, y_pred):\n-    '''Calculates the poisson function over prediction and target values.\n-    '''\n+    \"\"\"Computes the poisson function over prediction and target values.\n+    \"\"\"\n     return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n \n \n def cosine_proximity(y_true, y_pred):\n-    '''Calculates the cosine similarity between the prediction and target\n-    values.\n-    '''\n+    \"\"\"Computes the cosine similarity between the prediction and target values.\n+    \"\"\"\n     y_true = K.l2_normalize(y_true, axis=-1)\n     y_pred = K.l2_normalize(y_pred, axis=-1)\n     return -K.mean(y_true * y_pred)\n \n \n def matthews_correlation(y_true, y_pred):\n-    '''Calculates the Matthews correlation coefficient measure for quality\n+    \"\"\"Matthews correlation metric.\n+\n+    It is only computed as a batch-wise average, not globally.\n+\n+    Computes the Matthews correlation coefficient measure for quality\n     of binary classification problems.\n-    '''\n+    \"\"\"\n     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n     y_pred_neg = 1 - y_pred_pos\n \n@@ -149,9 +174,13 @@ def matthews_correlation(y_true, y_pred):\n \n \n def precision(y_true, y_pred):\n-    '''Calculates the precision, a metric for multi-label classification of\n+    \"\"\"Precision metric.\n+\n+    Only computes a batch-wise average of precision.\n+\n+    Computes the precision, a metric for multi-label classification of\n     how many selected items are relevant.\n-    '''\n+    \"\"\"\n     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n     precision = true_positives / (predicted_positives + K.epsilon())\n@@ -159,9 +188,13 @@ def precision(y_true, y_pred):\n \n \n def recall(y_true, y_pred):\n-    '''Calculates the recall, a metric for multi-label classification of\n+    \"\"\"Recall metric.\n+\n+    Only computes a batch-wise average of recall.\n+\n+    Computes the recall, a metric for multi-label classification of\n     how many relevant items are selected.\n-    '''\n+    \"\"\"\n     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n     recall = true_positives / (possible_positives + K.epsilon())\n@@ -169,7 +202,10 @@ def recall(y_true, y_pred):\n \n \n def fbeta_score(y_true, y_pred, beta=1):\n-    '''Calculates the F score, the weighted harmonic mean of precision and recall.\n+    \"\"\"Computes the F score.\n+\n+    The F score is the weighted harmonic mean of precision and recall.\n+    Here it is only computed as a batch-wise average, not globally.\n \n     This is useful for multi-label classification, where input samples can be\n     classified as sets of labels. By only using accuracy (precision) a model\n@@ -182,7 +218,7 @@ def fbeta_score(y_true, y_pred, beta=1):\n     With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n     correct classes becomes more important, and with beta > 1 the metric is\n     instead weighted towards penalizing incorrect class assignments.\n-    '''\n+    \"\"\"\n     if beta < 0:\n         raise ValueError('The lowest choosable beta is zero (only precision).')\n \n@@ -198,8 +234,10 @@ def fbeta_score(y_true, y_pred, beta=1):\n \n \n def fmeasure(y_true, y_pred):\n-    '''Calculates the f-measure, the harmonic mean of precision and recall.\n-    '''\n+    \"\"\"Computes the f-measure, the harmonic mean of precision and recall.\n+\n+    Here it is only computed as a batch-wise average, not globally.\n+    \"\"\"\n     return fbeta_score(y_true, y_pred, beta=1)\n \n \n\n@@ -13,7 +13,9 @@ def mean_absolute_error(y_true, y_pred):\n \n \n def mean_absolute_percentage_error(y_true, y_pred):\n-    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true), K.epsilon(), np.inf))\n+    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n+                                            K.epsilon(),\n+                                            np.inf))\n     return 100. * K.mean(diff, axis=-1)\n \n \n@@ -38,7 +40,8 @@ def categorical_crossentropy(y_true, y_pred):\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    '''expects an array of integer classes.\n+    '''Expects an array of integer classes.\n+\n     Note: labels shape must have the same number of dimensions as output shape.\n     If you get a shape error, add a length-1 dimension to labels.\n     '''\n@@ -65,7 +68,8 @@ def cosine_proximity(y_true, y_pred):\n     return -K.mean(y_true * y_pred, axis=-1)\n \n \n-# aliases\n+# Aliases.\n+\n mse = MSE = mean_squared_error\n mae = MAE = mean_absolute_error\n mape = MAPE = mean_absolute_percentage_error\n\n@@ -24,13 +24,14 @@ class Regularizer(object):\n \n \n class EigenvalueRegularizer(Regularizer):\n-    '''This takes a constant that controls\n-    the regularization by Eigenvalue Decay on the\n-    current layer and outputs the regularized\n-    loss (evaluated on the training data) and\n-    the original loss (evaluated on the\n-    validation data).\n-    '''\n+    \"\"\"Regularizer based on the eignvalues of a weight matrix.\n+\n+    Only available for tensors of rank 2.\n+\n+    # Arguments\n+        k: Float; modulates the amount of regularization to apply.\n+    \"\"\"\n+\n     def __init__(self, k):\n         self.k = k\n \n@@ -58,6 +59,12 @@ class EigenvalueRegularizer(Regularizer):\n \n \n class L1L2Regularizer(Regularizer):\n+    \"\"\"Regularizer for L1 and L2 regularization.\n+\n+    # Arguments\n+        l1: Float; L1 regularization factor.\n+        l2: Float; L2 regularization factor.\n+    \"\"\"\n \n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#833c0b23f5b6607700f58de71c8e90f20e37cbd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2291 | Contributors (this commit): 37 | Commits (past 90d): 17 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -34,13 +34,13 @@ class CallbackList(object):\n     def append(self, callback):\n         self.callbacks.append(callback)\n \n-    def _set_params(self, params):\n+    def set_params(self, params):\n         for callback in self.callbacks:\n-            callback._set_params(params)\n+            callback.set_params(params)\n \n-    def _set_model(self, model):\n+    def set_model(self, model):\n         for callback in self.callbacks:\n-            callback._set_model(model)\n+            callback.set_model(model)\n \n     def on_epoch_begin(self, epoch, logs=None):\n         logs = logs or {}\n@@ -127,10 +127,10 @@ class Callback(object):\n     def __init__(self):\n         pass\n \n-    def _set_params(self, params):\n+    def set_params(self, params):\n         self.params = params\n \n-    def _set_model(self, model):\n+    def set_model(self, model):\n         self.model = model\n \n     def on_epoch_begin(self, epoch, logs=None):\n@@ -533,7 +533,7 @@ class TensorBoard(Callback):\n         self.write_graph = write_graph\n         self.write_images = write_images\n \n-    def _set_model(self, model):\n+    def set_model(self, model):\n         import tensorflow as tf\n         import keras.backend.tensorflow_backend as KTF\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a6525be4fc839ce6d561b3239c6514f02b2e445e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 73 | Files Changed: 3 | Hunks: 63 | Methods Changed: 31 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 155 | Churn Cumulative: 3355 | Contributors (this commit): 39 | Commits (past 90d): 22 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class MaxNorm(Constraint):\n     def __call__(self, p):\n         norms = K.sqrt(K.sum(K.square(p), axis=self.axis, keepdims=True))\n         desired = K.clip(norms, 0, self.m)\n-        p = p * (desired / (K.epsilon() + norms))\n+        p *= (desired / (K.epsilon() + norms))\n         return p\n \n     def get_config(self):\n\n@@ -25,13 +25,24 @@ from .. import callbacks as cbks\n \n \n def standardize_input_data(data, names, shapes=None,\n-                           check_batch_dim=True,\n+                           check_batch_axis=True,\n                            exception_prefix=''):\n-    '''Users may pass data as a list of arrays, dictionary of arrays,\n+    \"\"\"Normalize inputs and targets provided by users.\n+\n+    Users may pass data as a list of arrays, dictionary of arrays,\n     or as a single array. We normalize this to an ordered list of\n     arrays (same order as `names`), while checking that the provided\n     arrays have shapes that match the network's expectations.\n-    '''\n+\n+    # Arguments\n+        data: User-provided input data (polymorphic).\n+        names: List of expected array names.\n+        shapes: Optional list of expected array shapes.\n+        check_batch_axis: Boolean; whether to check that\n+            the batch axis of the arrays matches the expected\n+            value found in `shapes`.\n+        exception_prefix: String prefix used for exception formatting.\n+    \"\"\"\n     if isinstance(data, dict):\n         arrays = []\n         for name in names:\n@@ -100,7 +111,7 @@ def standardize_input_data(data, names, shapes=None,\n                                  ' dimensions, but got array with shape ' +\n                                  str(array.shape))\n             for j, (dim, ref_dim) in enumerate(zip(array.shape, shapes[i])):\n-                if not j and not check_batch_dim:\n+                if not j and not check_batch_axis:\n                     # skip the first axis\n                     continue\n                 if ref_dim:\n@@ -159,10 +170,10 @@ def standardize_sample_weights(sample_weight, output_names):\n                                                'sample_weight')\n \n \n-def check_array_lengths(X, Y, W):\n-    x_lengths = [x.shape[0] for x in X]\n-    y_lengths = [y.shape[0] for y in Y]\n-    w_lengths = [w.shape[0] for w in W]\n+def check_array_lengths(inputs, targets, weights):\n+    x_lengths = [x.shape[0] for x in inputs]\n+    y_lengths = [y.shape[0] for y in targets]\n+    w_lengths = [w.shape[0] for w in weights]\n     set_x = set(x_lengths)\n     if len(set_x) != 1:\n         raise ValueError('All input arrays (x) should have '\n@@ -243,10 +254,10 @@ def collect_metrics(metrics, output_names):\n \n \n def batch_shuffle(index_array, batch_size):\n-    '''This shuffles an array in a batch-wise fashion.\n+    \"\"\"This shuffles an array in a batch-wise fashion.\n     Useful for shuffling HDF5 arrays\n     (where one cannot access arbitrary indices).\n-    '''\n+    \"\"\"\n     batch_count = int(len(index_array) / batch_size)\n     # to reshape we need to be cleanly divisible by batch size\n     # we stash extra items and reappend them after shuffling\n@@ -259,27 +270,27 @@ def batch_shuffle(index_array, batch_size):\n \n \n def make_batches(size, batch_size):\n-    '''Returns a list of batch indices (tuples of indices).\n-    '''\n+    \"\"\"Returns a list of batch indices (tuples of indices).\n+    \"\"\"\n     nb_batch = int(np.ceil(size / float(batch_size)))\n     return [(i * batch_size, min(size, (i + 1) * batch_size))\n             for i in range(0, nb_batch)]\n \n \n def slice_X(X, start=None, stop=None):\n-    '''This takes an array-like, or a list of\n+    \"\"\"This takes an array-like, or a list of\n     array-likes, and outputs:\n         - X[start:stop] if X is an array-like\n         - [x[start:stop] for x in X] if X in a list\n \n     Can also work on list/array of indices: `slice_X(x, indices)`\n \n-    # Arguments:\n+    # Arguments\n         start: can be an integer index (start index)\n             or a list/array of indices\n         stop: integer (stop index); should be None if\n             `start` was a list.\n-    '''\n+    \"\"\"\n     if isinstance(X, list):\n         if hasattr(start, '__len__'):\n             # hdf5 datasets only support list objects as indices\n@@ -298,10 +309,10 @@ def slice_X(X, start=None, stop=None):\n \n \n def weighted_objective(fn):\n-    '''Transforms an objective function `fn(y_true, y_pred)`\n+    \"\"\"Transforms an objective function `fn(y_true, y_pred)`\n     into a sample-weighted, cost-masked objective function\n     `fn(y_true, y_pred, weights, mask)`.\n-    '''\n+    \"\"\"\n     def weighted(y_true, y_pred, weights, mask=None):\n         # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n@@ -329,9 +340,9 @@ def weighted_objective(fn):\n \n def standardize_weights(y, sample_weight=None, class_weight=None,\n                         sample_weight_mode=None):\n-    '''Performs weight input validation and standardization\n+    \"\"\"Performs weight input validation and standardization\n     to a single sample-wise (or timestep-wise) weight array.\n-    '''\n+    \"\"\"\n     if sample_weight_mode is not None:\n         if sample_weight_mode != 'temporal':\n             raise ValueError('\"sample_weight_mode '\n@@ -387,12 +398,10 @@ def standardize_weights(y, sample_weight=None, class_weight=None,\n \n def generator_queue(generator, max_q_size=10,\n                     wait_time=0.05, nb_worker=1, pickle_safe=False):\n-    '''Builds a queue out of a data generator.\n+    \"\"\"Builds a queue out of a data generator.\n     If pickle_safe, use a multiprocessing approach. Else, use threading.\n     Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n-\n-    '''\n-\n+    \"\"\"\n     generator_threads = []\n     if pickle_safe:\n         q = multiprocessing.Queue(maxsize=max_q_size)\n@@ -442,7 +451,7 @@ class Model(Container):\n \n     def compile(self, optimizer, loss, metrics=None, loss_weights=None,\n                 sample_weight_mode=None, **kwargs):\n-        '''Configures the model for training.\n+        \"\"\"Configures the model for training.\n \n         # Arguments\n             optimizer: str (name of optimizer) or optimizer object.\n@@ -465,7 +474,7 @@ class Model(Container):\n                 dictionary or a list of modes.\n             kwargs: when using the Theano backend, these arguments\n                 are passed into K.function. Ignored for Tensorflow backend.\n-        '''\n+        \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n         self.sample_weight_mode = sample_weight_mode\n         self.loss = loss\n@@ -754,7 +763,7 @@ class Model(Container):\n                   nb_epoch=100, verbose=1, callbacks=None,\n                   val_f=None, val_ins=None, shuffle=True,\n                   callback_metrics=None, initial_epoch=0):\n-        '''Abstract fit function for f(ins).\n+        \"\"\"Abstract fit function for f(ins).\n         Assume that f returns a list, labeled by out_labels.\n \n         # Arguments\n@@ -778,7 +787,7 @@ class Model(Container):\n \n         # Returns\n             `History` object.\n-        '''\n+        \"\"\"\n         do_validation = False\n         if val_f and val_ins:\n             do_validation = True\n@@ -803,8 +812,8 @@ class Model(Container):\n         else:\n             callback_model = self\n \n-        callbacks._set_model(callback_model)\n-        callbacks._set_params({\n+        callbacks.set_model(callback_model)\n+        callbacks.set_params({\n             'batch_size': batch_size,\n             'nb_epoch': nb_epoch,\n             'nb_sample': nb_train_sample,\n@@ -868,7 +877,7 @@ class Model(Container):\n         return self.history\n \n     def _predict_loop(self, f, ins, batch_size=32, verbose=0):\n-        '''Abstract method to loop over some data in batches.\n+        \"\"\"Abstract method to loop over some data in batches.\n \n         # Arguments\n             f: Keras function returning a list of tensors.\n@@ -880,7 +889,7 @@ class Model(Container):\n             Array of predictions (if the model has a single output)\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n-        '''\n+        \"\"\"\n         nb_sample = ins[0].shape[0]\n         outs = []\n         if verbose == 1:\n@@ -912,7 +921,7 @@ class Model(Container):\n         return outs\n \n     def _test_loop(self, f, ins, batch_size=32, verbose=0):\n-        '''Abstract method to loop over some data in batches.\n+        \"\"\"Abstract method to loop over some data in batches.\n \n         # Arguments\n             f: Keras function returning a list of tensors.\n@@ -925,7 +934,7 @@ class Model(Container):\n             or list of scalars (if the model has multiple outputs\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         nb_sample = ins[0].shape[0]\n         outs = []\n         if verbose == 1:\n@@ -962,7 +971,7 @@ class Model(Container):\n \n     def _standardize_user_data(self, x, y,\n                                sample_weight=None, class_weight=None,\n-                               check_batch_dim=True, batch_size=None):\n+                               check_batch_axis=True, batch_size=None):\n         if not hasattr(self, 'optimizer'):\n             raise RuntimeError('You must compile a model before '\n                                'training/testing. '\n@@ -978,11 +987,11 @@ class Model(Container):\n                 output_shapes.append(output_shape)\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes,\n-                                   check_batch_dim=False,\n+                                   check_batch_axis=False,\n                                    exception_prefix='model input')\n         y = standardize_input_data(y, self.output_names,\n                                    output_shapes,\n-                                   check_batch_dim=False,\n+                                   check_batch_axis=False,\n                                    exception_prefix='model target')\n         sample_weights = standardize_sample_weights(sample_weight,\n                                                     self.output_names)\n@@ -1005,7 +1014,7 @@ class Model(Container):\n     def fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None,\n             validation_split=0., validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0):\n-        '''Trains the model for a fixed number of epochs (iterations on a dataset).\n+        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n \n         # Arguments\n             x: Numpy array of training data,\n@@ -1059,13 +1068,13 @@ class Model(Container):\n         # Returns\n             A `History` instance. Its `history` attribute contains\n             all information collected during training.\n-        '''\n+        \"\"\"\n         # validate user data\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight,\n             class_weight=class_weight,\n-            check_batch_dim=False,\n+            check_batch_axis=False,\n             batch_size=batch_size)\n         # prepare validation data\n         if validation_data:\n@@ -1080,7 +1089,7 @@ class Model(Container):\n             val_x, val_y, val_sample_weights = self._standardize_user_data(\n                 val_x, val_y,\n                 sample_weight=val_sample_weight,\n-                check_batch_dim=False,\n+                check_batch_axis=False,\n                 batch_size=batch_size)\n             self._make_test_function()\n             val_f = self.test_function\n@@ -1144,7 +1153,7 @@ class Model(Container):\n                               initial_epoch=initial_epoch)\n \n     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n-        '''Returns the loss value and metrics values for the model\n+        \"\"\"Returns the loss value and metrics values for the model\n         in test mode. Computation is done in batches.\n \n         # Arguments\n@@ -1165,12 +1174,12 @@ class Model(Container):\n             or list of scalars (if the model has multiple outputs\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         # validate user data\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight,\n-            check_batch_dim=False,\n+            check_batch_axis=False,\n             batch_size=batch_size)\n         # prepare inputs, delegate logic to _test_loop\n         if self.uses_learning_phase and not isinstance(K.learning_phase, int):\n@@ -1184,7 +1193,7 @@ class Model(Container):\n                                verbose=verbose)\n \n     def predict(self, x, batch_size=32, verbose=0):\n-        '''Generates output predictions for the input samples,\n+        \"\"\"Generates output predictions for the input samples,\n         processing the samples in a batched way.\n \n         # Arguments\n@@ -1195,11 +1204,11 @@ class Model(Container):\n \n         # Returns\n             A Numpy array of predictions.\n-        '''\n+        \"\"\"\n         # validate user data\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes,\n-                                   check_batch_dim=False)\n+                                   check_batch_axis=False)\n         if self.stateful:\n             if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0:\n                 raise ValueError('In a stateful network, '\n@@ -1221,7 +1230,7 @@ class Model(Container):\n \n     def train_on_batch(self, x, y,\n                        sample_weight=None, class_weight=None):\n-        '''Runs a single gradient update on a single batch of data.\n+        \"\"\"Runs a single gradient update on a single batch of data.\n \n         # Arguments\n             x: Numpy array of training data,\n@@ -1254,12 +1263,12 @@ class Model(Container):\n             or list of scalars (if the model has multiple outputs\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight,\n             class_weight=class_weight,\n-            check_batch_dim=True)\n+            check_batch_axis=True)\n         if self.uses_learning_phase and not isinstance(K.learning_phase, int):\n             ins = x + y + sample_weights + [1.]\n         else:\n@@ -1271,7 +1280,7 @@ class Model(Container):\n         return outputs\n \n     def test_on_batch(self, x, y, sample_weight=None):\n-        '''Test the model on a single batch of samples.\n+        \"\"\"Test the model on a single batch of samples.\n \n         # Arguments\n             x: Numpy array of test data,\n@@ -1297,11 +1306,11 @@ class Model(Container):\n             or list of scalars (if the model has multiple outputs\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight,\n-            check_batch_dim=True)\n+            check_batch_axis=True)\n         if self.uses_learning_phase and not isinstance(K.learning_phase, int):\n             ins = x + y + sample_weights + [0.]\n         else:\n@@ -1313,8 +1322,8 @@ class Model(Container):\n         return outputs\n \n     def predict_on_batch(self, x):\n-        '''Returns predictions for a single batch of samples.\n-        '''\n+        \"\"\"Returns predictions for a single batch of samples.\n+        \"\"\"\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes)\n         if self.uses_learning_phase and not isinstance(K.learning_phase, int):\n@@ -1333,7 +1342,7 @@ class Model(Container):\n                       class_weight=None,\n                       max_q_size=10, nb_worker=1, pickle_safe=False,\n                       initial_epoch=0):\n-        '''Fits the model on data generated batch-by-batch by\n+        \"\"\"Fits the model on data generated batch-by-batch by\n         a Python generator.\n         The generator is run in parallel to the model, for efficiency.\n         For instance, this allows you to do real-time data augmentation\n@@ -1394,7 +1403,7 @@ class Model(Container):\n             model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n                                 samples_per_epoch=10000, nb_epoch=10)\n         ```\n-        '''\n+        \"\"\"\n         wait_time = 0.01  # in seconds\n         epoch = initial_epoch\n \n@@ -1426,8 +1435,8 @@ class Model(Container):\n             callback_model = self.callback_model\n         else:\n             callback_model = self\n-        callbacks._set_model(callback_model)\n-        callbacks._set_params({\n+        callbacks.set_model(callback_model)\n+        callbacks.set_params({\n             'nb_epoch': nb_epoch,\n             'nb_sample': samples_per_epoch,\n             'verbose': verbose,\n@@ -1569,7 +1578,7 @@ class Model(Container):\n \n     def evaluate_generator(self, generator, val_samples,\n                            max_q_size=10, nb_worker=1, pickle_safe=False):\n-        '''Evaluates the model on a data generator. The generator should\n+        \"\"\"Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n \n         Arguments:\n@@ -1595,7 +1604,7 @@ class Model(Container):\n             or list of scalars (if the model has multiple outputs\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n-        '''\n+        \"\"\"\n         self._make_test_function()\n \n         processed_samples = 0\n@@ -1668,7 +1677,7 @@ class Model(Container):\n \n     def predict_generator(self, generator, val_samples,\n                           max_q_size=10, nb_worker=1, pickle_safe=False):\n-        '''Generates predictions for the input samples from a data generator.\n+        \"\"\"Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n \n@@ -1689,7 +1698,7 @@ class Model(Container):\n \n         # Returns\n             Numpy array(s) of predictions.\n-        '''\n+        \"\"\"\n         self._make_predict_function()\n \n         processed_samples = 0\n\n@@ -38,23 +38,23 @@ def normal(shape, scale=0.05, name=None):\n \n \n def lecun_uniform(shape, name=None, dim_ordering='th'):\n-    '''LeCun uniform variance scaling initializer.\n+    \"\"\"LeCun uniform variance scaling initializer.\n \n     # Reference\n         LeCun 98, Efficient Backprop\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n-    '''\n+    \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     scale = np.sqrt(3. / fan_in)\n     return uniform(shape, scale, name=name)\n \n \n def glorot_normal(shape, name=None, dim_ordering='th'):\n-    '''Glorot normal variance scaling initializer.\n+    \"\"\"Glorot normal variance scaling initializer.\n \n     # Reference\n         Glorot & Bengio, AISTATS 2010\n-    '''\n+    \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(2. / (fan_in + fan_out))\n     return normal(shape, s, name=name)\n@@ -67,30 +67,30 @@ def glorot_uniform(shape, name=None, dim_ordering='th'):\n \n \n def he_normal(shape, name=None, dim_ordering='th'):\n-    '''He normal variance scaling initializer.\n+    \"\"\"He normal variance scaling initializer.\n \n     # Reference\n         He et al., http://arxiv.org/abs/1502.01852\n-    '''\n+    \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(2. / fan_in)\n     return normal(shape, s, name=name)\n \n \n def he_uniform(shape, name=None, dim_ordering='th'):\n-    '''He uniform variance scaling initializer.\n-    '''\n+    \"\"\"He uniform variance scaling initializer.\n+    \"\"\"\n     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(6. / fan_in)\n     return uniform(shape, s, name=name)\n \n \n def orthogonal(shape, scale=1.1, name=None):\n-    '''Orthogonal initializer.\n+    \"\"\"Orthogonal initializer.\n \n     # Reference\n         Saxe et al., http://arxiv.org/abs/1312.6120\n-    '''\n+    \"\"\"\n     flat_shape = (shape[0], np.prod(shape[1:]))\n     a = np.random.normal(0.0, 1.0, flat_shape)\n     u, _, v = np.linalg.svd(a, full_matrices=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
