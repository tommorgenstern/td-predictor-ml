{"custom_id": "scrapy#8662d3587df74841d4ea640c0432446569e59262", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 7 | Churn Cumulative: 742 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -337,9 +338,9 @@ class FeedExporter:\n                     spider=spider,\n                     template_uri=slot.template_uri,\n                 ))\n-                self.slots[idx] = None\n-        self.slots = [slot for slot in self.slots if slot is not None]\n-        self.slots.extend(slots)\n+            else:\n+                slots.append(slot)\n+        self.slots = slots\n \n     def _load_components(self, setting_prefix):\n         conf = without_none_values(self.settings.getwithbase(setting_prefix))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df8a1d1c0108b15bebdad064d8b7bc61a894f062", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 82 | Files Changed: 13 | Hunks: 60 | Methods Changed: 30 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 160 | Churn Cumulative: 4443 | Contributors (this commit): 41 | Commits (past 90d): 49 | Contributors (cumulative): 117 | DMM Complexity: None\n\nDIFF:\n@@ -146,14 +146,13 @@ class Shell:\n         b.append(\"Useful shortcuts:\")\n         if self.inthread:\n             b.append(\"  fetch(url[, redirect=True]) \"\n-                     \"Fetch URL and update local objects \"\n-                     \"(by default, redirects are followed)\")\n+                     \"Fetch URL and update local objects (by default, redirects are followed)\")\n             b.append(\"  fetch(req)                  \"\n                      \"Fetch a scrapy.Request and update local objects \")\n         b.append(\"  shelp()           Shell help (print this help)\")\n         b.append(\"  view(response)    View response in a browser\")\n \n-        return \"\\n\".join(\"[s] %s\" % l for l in b)\n+        return \"\\n\".join(\"[s] %s\" % line for line in b)\n \n     def _is_relevant(self, value):\n         return isinstance(value, self.relevant_classes)\n\n@@ -96,5 +96,4 @@ def iterloc(it, alt=False):\n \n         # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n         if alt and 'alternate' in d:\n-            for l in d['alternate']:\n-                yield l\n+            yield from d['alternate']\n\n@@ -37,7 +37,7 @@ class TopLevelFormatter(logging.Filter):\n         self.loggers = loggers or []\n \n     def filter(self, record):\n-        if any(record.name.startswith(l + '.') for l in self.loggers):\n+        if any(record.name.startswith(logger + '.') for logger in self.loggers):\n             record.name = record.name.split('.', 1)[0]\n         return True\n \n\n@@ -23,8 +23,10 @@ class VersionTest(ProcessTest, unittest.TestCase):\n     def test_verbose_output(self):\n         encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         _, out, _ = yield self.execute(['-v'])\n-        headers = [l.partition(\":\")[0].strip()\n-                   for l in out.strip().decode(encoding).splitlines()]\n+        headers = [\n+            line.partition(\":\")[0].strip()\n+            for line in out.strip().decode(encoding).splitlines()\n+        ]\n         self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2',\n                                    'cssselect', 'parsel', 'w3lib',\n                                    'Twisted', 'Python', 'pyOpenSSL',\n\n@@ -104,44 +104,44 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_retry_503(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n-        self._assert_retried(l)\n+        self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(\"http://localhost:65432/status?n=503\", mockserver=self.mockserver)\n-        self._assert_retried(l)\n+        self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_retry_dns_error(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             # try to fetch the homepage of a non-existent domain\n             yield crawler.crawl(\"http://dns.resolution.invalid./\", mockserver=self.mockserver)\n-        self._assert_retried(l)\n+        self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n-        with LogCapture('scrapy', level=logging.ERROR) as l:\n+        with LogCapture('scrapy', level=logging.ERROR) as log:\n             crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n \n-        self.assertEqual(len(l.records), 1)\n-        record = l.records[0]\n+        self.assertEqual(len(log.records), 1)\n+        record = log.records[0]\n         self.assertIsNotNone(record.exc_info)\n         self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n-        with LogCapture('scrapy', level=logging.ERROR) as l:\n+        with LogCapture('scrapy', level=logging.ERROR) as log:\n             crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n \n-        self.assertEqual(len(l.records), 1)\n-        record = l.records[0]\n+        self.assertEqual(len(log.records), 1)\n+        record = log.records[0]\n         self.assertIsNotNone(record.exc_info)\n         self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n@@ -187,25 +187,25 @@ foo body\n with multiples lines\n '''})\n         crawler = self.runner.create_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/raw?{0}\".format(query)), mockserver=self.mockserver)\n-        self.assertEqual(str(l).count(\"Got response 200\"), 1)\n+        self.assertEqual(str(log).count(\"Got response 200\"), 1)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n         crawler = self.runner.create_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver)\n-        self._assert_retried(l)\n+        self._assert_retried(log)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_aborted(self):\n         # connection lost before receiving data\n         crawler = self.runner.create_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver)\n-        self._assert_retried(l)\n+        self._assert_retried(log)\n \n     def _assert_retried(self, log):\n         self.assertEqual(str(log).count(\"Retrying\"), 2)\n\n@@ -63,7 +63,7 @@ class CookiesMiddlewareTest(TestCase):\n         mw = CookiesMiddleware.from_crawler(crawler)\n         with LogCapture('scrapy.downloadermiddlewares.cookies',\n                         propagate=False,\n-                        level=logging.DEBUG) as l:\n+                        level=logging.DEBUG) as log:\n             req = Request('http://scrapytest.org/')\n             res = Response('http://scrapytest.org/',\n                            headers={'Set-Cookie': 'C1=value1; path=/'})\n@@ -71,7 +71,7 @@ class CookiesMiddlewareTest(TestCase):\n             req2 = Request('http://scrapytest.org/sub1/')\n             mw.process_request(req2, crawler.spider)\n \n-            l.check(\n+            log.check(\n                 ('scrapy.downloadermiddlewares.cookies',\n                  'DEBUG',\n                  'Received cookies from: <200 http://scrapytest.org/>\\n'\n@@ -87,7 +87,7 @@ class CookiesMiddlewareTest(TestCase):\n         mw = CookiesMiddleware.from_crawler(crawler)\n         with LogCapture('scrapy.downloadermiddlewares.cookies',\n                         propagate=False,\n-                        level=logging.DEBUG) as l:\n+                        level=logging.DEBUG) as log:\n             req = Request('http://scrapytest.org/')\n             res = Response('http://scrapytest.org/',\n                            headers={'Set-Cookie': 'C1=value1; path=/'})\n@@ -95,7 +95,7 @@ class CookiesMiddlewareTest(TestCase):\n             req2 = Request('http://scrapytest.org/sub1/')\n             mw.process_request(req2, crawler.spider)\n \n-            l.check()\n+            log.check()\n \n     def test_do_not_break_on_non_utf8_header(self):\n         req = Request('http://scrapytest.org/')\n\n@@ -160,7 +160,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n             shutil.rmtree(path)\n \n     def test_log(self):\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': False,\n                         'DUPEFILTER_CLASS': __name__ + '.FromCrawlerRFPDupeFilter'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n@@ -177,7 +177,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n             dupefilter.log(r2, spider)\n \n             assert crawler.stats.get_value('dupefilter/filtered') == 2\n-            l.check_present(('scrapy.dupefilters', 'DEBUG',\n+            log.check_present(('scrapy.dupefilters', 'DEBUG',\n                 ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n                 ' - no more duplicates will be shown'\n                 ' (see DUPEFILTER_DEBUG to show all duplicates)')))\n@@ -185,7 +185,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n             dupefilter.close('finished')\n \n     def test_log_debug(self):\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': True,\n                         'DUPEFILTER_CLASS': __name__ + '.FromCrawlerRFPDupeFilter'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n@@ -203,10 +203,10 @@ class RFPDupeFilterTest(unittest.TestCase):\n             dupefilter.log(r2, spider)\n \n             assert crawler.stats.get_value('dupefilter/filtered') == 2\n-            l.check_present(('scrapy.dupefilters', 'DEBUG',\n+            log.check_present(('scrapy.dupefilters', 'DEBUG',\n                 ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n                 ' (referer: None)')))\n-            l.check_present(('scrapy.dupefilters', 'DEBUG',\n+            log.check_present(('scrapy.dupefilters', 'DEBUG',\n                 ('Filtered duplicate request: <GET http://scrapytest.org/index.html>'\n                 ' (referer: http://scrapytest.org/INDEX.html)')))\n \n\n@@ -63,21 +63,21 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         fail = Failure(Exception())\n         results = [(True, 1), (False, fail)]\n \n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             new_item = self.pipe.item_completed(results, item, self.info)\n \n         assert new_item is item\n-        assert len(l.records) == 1\n-        record = l.records[0]\n+        assert len(log.records) == 1\n+        record = log.records[0]\n         assert record.levelname == 'ERROR'\n         self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n \n         # disable failure logging and check again\n         self.pipe.LOG_FAILED_RESULTS = False\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             new_item = self.pipe.item_completed(results, item, self.info)\n         assert new_item is item\n-        assert len(l.records) == 0\n+        assert len(log.records) == 0\n \n     @inlineCallbacks\n     def test_default_process_item(self):\n\n@@ -76,35 +76,35 @@ class ProxyConnectTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel(self):\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n+        self._assert_got_response_code(200, log)\n \n     @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info.minor >= 6)\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n-        self._assert_got_tunnel_error(l)\n+        self._assert_got_tunnel_error(log)\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n         os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         # The proxy returns a 407 error code but it does not reach the client;\n         # he just sees a TunnelError.\n-        self._assert_got_tunnel_error(l)\n+        self._assert_got_tunnel_error(log)\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_without_leak_proxy_authorization_header(self):\n         request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n         crawler = get_crawler(SingleRequestSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             yield crawler.crawl(seed=request)\n-        self._assert_got_response_code(200, l)\n+        self._assert_got_response_code(200, log)\n         echo = json.loads(crawler.spider.meta['responses'][0].text)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n\n@@ -89,12 +89,12 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n \n     def test_serialize_loader(self):\n         q = self.queue()\n-        l = TestLoader()\n-        q.push(l)\n-        l2 = q.pop()\n-        assert isinstance(l2, TestLoader)\n-        assert l2.default_item_class is TestItem\n-        self.assertEqual(l2.name_out('x'), 'xx')\n+        loader = TestLoader()\n+        q.push(loader)\n+        loader2 = q.pop()\n+        assert isinstance(loader2, TestLoader)\n+        assert loader2.default_item_class is TestItem\n+        self.assertEqual(loader2.name_out('x'), 'xx')\n \n     def test_serialize_request_recursive(self):\n         q = self.queue()\n@@ -173,12 +173,12 @@ class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n \n     def test_serialize_loader(self):\n         q = self.queue()\n-        l = TestLoader()\n-        q.push(l)\n-        l2 = q.pop()\n-        assert isinstance(l2, TestLoader)\n-        assert l2.default_item_class is TestItem\n-        self.assertEqual(l2.name_out('x'), 'xx')\n+        loader = TestLoader()\n+        q.push(loader)\n+        loader2 = q.pop()\n+        assert isinstance(loader2, TestLoader)\n+        assert loader2.default_item_class is TestItem\n+        self.assertEqual(loader2.name_out('x'), 'xx')\n \n     def test_serialize_request_recursive(self):\n         q = self.queue()\n\n@@ -34,31 +34,27 @@ class TopLevelFormatterTest(unittest.TestCase):\n \n     def test_top_level_logger(self):\n         logger = logging.getLogger('test')\n-        with self.handler as l:\n+        with self.handler as log:\n             logger.warning('test log msg')\n-\n-        l.check(('test', 'WARNING', 'test log msg'))\n+        log.check(('test', 'WARNING', 'test log msg'))\n \n     def test_children_logger(self):\n         logger = logging.getLogger('test.test1')\n-        with self.handler as l:\n+        with self.handler as log:\n             logger.warning('test log msg')\n-\n-        l.check(('test', 'WARNING', 'test log msg'))\n+        log.check(('test', 'WARNING', 'test log msg'))\n \n     def test_overlapping_name_logger(self):\n         logger = logging.getLogger('test2')\n-        with self.handler as l:\n+        with self.handler as log:\n             logger.warning('test log msg')\n-\n-        l.check(('test2', 'WARNING', 'test log msg'))\n+        log.check(('test2', 'WARNING', 'test log msg'))\n \n     def test_different_name_logger(self):\n         logger = logging.getLogger('different')\n-        with self.handler as l:\n+        with self.handler as log:\n             logger.warning('test log msg')\n-\n-        l.check(('different', 'WARNING', 'test log msg'))\n+        log.check(('different', 'WARNING', 'test log msg'))\n \n \n class LogCounterHandlerTest(unittest.TestCase):\n@@ -107,6 +103,6 @@ class StreamLoggerTest(unittest.TestCase):\n         sys.stdout = self.stdout\n \n     def test_redirect(self):\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             print('test log msg')\n-        l.check(('test', 'ERROR', 'test log msg'))\n+        log.check(('test', 'ERROR', 'test log msg'))\n\n@@ -67,12 +67,12 @@ class UtilsMiscTestCase(unittest.TestCase):\n         assert hasattr(arg_to_iter(100), '__iter__')\n         assert hasattr(arg_to_iter('lala'), '__iter__')\n         assert hasattr(arg_to_iter([1, 2, 3]), '__iter__')\n-        assert hasattr(arg_to_iter(l for l in 'abcd'), '__iter__')\n+        assert hasattr(arg_to_iter(c for c in 'abcd'), '__iter__')\n \n         self.assertEqual(list(arg_to_iter(None)), [])\n         self.assertEqual(list(arg_to_iter('lala')), ['lala'])\n         self.assertEqual(list(arg_to_iter(100)), [100])\n-        self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])\n+        self.assertEqual(list(arg_to_iter(c for c in 'abc')), ['a', 'b', 'c'])\n         self.assertEqual(list(arg_to_iter([1, 2, 3])), [1, 2, 3])\n         self.assertEqual(list(arg_to_iter({'a': 1})), [{'a': 1}])\n         self.assertEqual(list(arg_to_iter(TestItem(name=\"john\"))), [TestItem(name=\"john\")])\n\n@@ -20,7 +20,7 @@ class SendCatchLogTest(unittest.TestCase):\n \n         dispatcher.connect(self.error_handler, signal=test_signal)\n         dispatcher.connect(self.ok_handler, signal=test_signal)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             result = yield defer.maybeDeferred(\n                 self._get_result, test_signal, arg='test',\n                 handlers_called=handlers_called\n@@ -28,8 +28,8 @@ class SendCatchLogTest(unittest.TestCase):\n \n         assert self.error_handler in handlers_called\n         assert self.ok_handler in handlers_called\n-        self.assertEqual(len(l.records), 1)\n-        record = l.records[0]\n+        self.assertEqual(len(log.records), 1)\n+        record = log.records[0]\n         self.assertIn('error_handler', record.getMessage())\n         self.assertEqual(record.levelname, 'ERROR')\n         self.assertEqual(result[0][0], self.error_handler)\n@@ -95,8 +95,8 @@ class SendCatchLogTest2(unittest.TestCase):\n \n         test_signal = object()\n         dispatcher.connect(test_handler, test_signal)\n-        with LogCapture() as l:\n+        with LogCapture() as log:\n             send_catch_log(test_signal)\n-        self.assertEqual(len(l.records), 1)\n-        self.assertIn(\"Cannot return deferreds from signal handler\", str(l))\n+        self.assertEqual(len(log.records), 1)\n+        self.assertIn(\"Cannot return deferreds from signal handler\", str(log))\n         dispatcher.disconnect(test_handler, test_signal)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fffb0a5b6a7eaf365ecd7ef43e7e45cf2ea7ff2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 20 | Files Changed: 8 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 4581 | Contributors (this commit): 45 | Commits (past 90d): 31 | Contributors (cumulative): 93 | DMM Complexity: None\n\nDIFF:\n@@ -218,8 +218,8 @@ class EngineTest(unittest.TestCase):\n     def _assert_visited_urls(self):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\",\n                            \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n-        urls_visited = set([rp[0].url for rp in self.run.respplug])\n-        urls_expected = set([self.run.geturl(p) for p in must_be_visited])\n+        urls_visited = {rp[0].url for rp in self.run.respplug}\n+        urls_expected = {self.run.geturl(p) for p in must_be_visited}\n         assert urls_expected <= urls_visited, \"URLs not visited: %s\" % list(urls_expected - urls_visited)\n \n     def _assert_scheduled_requests(self, urls_to_visit=None):\n@@ -227,8 +227,8 @@ class EngineTest(unittest.TestCase):\n \n         paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n \n-        urls_requested = set([rq[0].url for rq in self.run.reqplug])\n-        urls_expected = set([self.run.geturl(p) for p in paths_expected])\n+        urls_requested = {rq[0].url for rq in self.run.reqplug}\n+        urls_expected = {self.run.geturl(p) for p in paths_expected}\n         assert urls_expected <= urls_requested\n         scheduled_requests_count = len(self.run.reqplug)\n         dropped_requests_count = len(self.run.reqdropped)\n\n@@ -549,8 +549,8 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n         self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n         fs = _qs(r1)\n-        self.assertEqual(set(fs[b'test']), set([b'val1', b'val2']))\n-        self.assertEqual(set(fs[b'one']), set([b'two', b'three']))\n+        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})\n+        self.assertEqual(set(fs[b'one']), {b'two', b'three'})\n         self.assertEqual(fs[b'test2'], [b'xxx'])\n         self.assertEqual(fs[b'six'], [b'seven'])\n \n@@ -1047,7 +1047,7 @@ class FormRequestTest(RequestTest):\n             </form>''')\n         req = self.request_class.from_response(res)\n         fs = _qs(req)\n-        self.assertEqual(set(fs), set([b'h2', b'i2', b'i1', b'i3', b'h1', b'i5', b'i4']))\n+        self.assertEqual(set(fs), {b'h2', b'i2', b'i1', b'i3', b'h1', b'i5', b'i4'})\n \n     def test_from_response_xpath(self):\n         response = _buildresponse(\n\n@@ -51,10 +51,10 @@ class FileDownloadCrawlTestCase(TestCase):\n     store_setting_key = 'FILES_STORE'\n     media_key = 'files'\n     media_urls_key = 'file_urls'\n-    expected_checksums = set([\n+    expected_checksums = {\n         '5547178b89448faf0015a13f904c936e',\n         'c2281c83670e31d8aaab7cb642b824db',\n-        'ed3f6538dc15d4d9179dae57319edc5f'])\n+        'ed3f6538dc15d4d9179dae57319edc5f'}\n \n     def setUp(self):\n         self.mockserver = MockServer()\n\n@@ -18,7 +18,7 @@ try:\n except ImportError:\n     skip = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n else:\n-    encoders = set(('jpeg_encoder', 'jpeg_decoder'))\n+    encoders = {'jpeg_encoder', 'jpeg_decoder'}\n     if not encoders.issubset(set(Image.core.__dict__)):\n         skip = 'Missing JPEG encoders'\n \n\n@@ -42,7 +42,7 @@ class SpiderLoaderTest(unittest.TestCase):\n     def test_list(self):\n         self.assertEqual(\n             set(self.spider_loader.list()),\n-            set(['spider1', 'spider2', 'spider3', 'spider4']))\n+            {'spider1', 'spider2', 'spider3', 'spider4'})\n \n     def test_load(self):\n         spider1 = self.spider_loader.load(\"spider1\")\n@@ -57,7 +57,7 @@ class SpiderLoaderTest(unittest.TestCase):\n             ['spider2'])\n         self.assertEqual(\n             set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),\n-            set(['spider1', 'spider2']))\n+            {'spider1', 'spider2'})\n         self.assertEqual(\n             self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n@@ -151,7 +151,7 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             self.assertNotIn(\"'spider4'\", msg)\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n+            self.assertEqual(spiders, {'spider1', 'spider2', 'spider3', 'spider4'})\n \n     def test_multiple_dupename_warning(self):\n         # copy 2 spider modules so as to have duplicate spider name\n@@ -177,4 +177,4 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             self.assertNotIn(\"'spider4'\", msg)\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n+            self.assertEqual(spiders, {'spider1', 'spider2', 'spider3', 'spider4'})\n\n@@ -217,7 +217,7 @@ class SequenceExcludeTest(unittest.TestCase):\n \n     def test_set(self):\n         \"\"\"Anything that is not in the supplied sequence will evaluate as 'in' the container.\"\"\"\n-        seq = set([-3, \"test\", 1.1])\n+        seq = {-3, \"test\", 1.1}\n         d = SequenceExclude(seq)\n         self.assertIn(0, d)\n         self.assertIn(\"foo\", d)\n\n@@ -26,20 +26,20 @@ class UtilsMiscTestCase(unittest.TestCase):\n             'tests.test_utils_misc.test_walk_modules.mod.mod0',\n             'tests.test_utils_misc.test_walk_modules.mod1',\n         ]\n-        self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n+        self.assertEqual({m.__name__ for m in mods}, set(expected))\n \n         mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')\n         expected = [\n             'tests.test_utils_misc.test_walk_modules.mod',\n             'tests.test_utils_misc.test_walk_modules.mod.mod0',\n         ]\n-        self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n+        self.assertEqual({m.__name__ for m in mods}, set(expected))\n \n         mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')\n         expected = [\n             'tests.test_utils_misc.test_walk_modules.mod1',\n         ]\n-        self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n+        self.assertEqual({m.__name__ for m in mods}, set(expected))\n \n         self.assertRaises(ImportError, walk_modules, 'nomodule999')\n \n@@ -54,7 +54,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n                 'testegg.spiders.b',\n                 'testegg'\n             ]\n-            self.assertEqual(set([m.__name__ for m in mods]), set(expected))\n+            self.assertEqual({m.__name__ for m in mods}, set(expected))\n         finally:\n             sys.path.remove(egg)\n \n\n@@ -55,7 +55,7 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))\n \n-        spider = Spider(name='example.com', allowed_domains=set(('example.com', 'example.net')))\n+        spider = Spider(name='example.com', allowed_domains={'example.com', 'example.net'})\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n \n         spider = Spider(name='example.com', allowed_domains=('example.com', 'example.net'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cdcf8b08b8f1e68c5b107b6ae39b2da1aedd245", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 762 | Contributors (this commit): 22 | Commits (past 90d): 12 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -180,7 +180,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n \n \n class _FeedSlot:\n-    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, template_uri):\n+    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, uri_template):\n         self.file = file\n         self.exporter = exporter\n         self.storage = storage\n@@ -188,7 +188,7 @@ class _FeedSlot:\n         self.batch_id = batch_id\n         self.format = format\n         self.store_empty = store_empty\n-        self.template_uri = template_uri\n+        self.uri_template = uri_template\n         self.uri = uri\n         # flags\n         self.itemcount = 0\n@@ -260,7 +260,7 @@ class FeedExporter:\n                 uri=uri % uri_params,\n                 feed=feed,\n                 spider=spider,\n-                template_uri=uri,\n+                uri_template=uri,\n             ))\n \n     def close_spider(self, spider):\n@@ -288,7 +288,7 @@ class FeedExporter:\n                                             extra={'spider': spider}))\n         return d\n \n-    def _start_new_batch(self, batch_id, uri, feed, spider, template_uri):\n+    def _start_new_batch(self, batch_id, uri, feed, spider, uri_template):\n         \"\"\"\n         Redirect the output data stream to a new file.\n         Execute multiple times if 'FEED_STORAGE_BATCH_ITEM_COUNT' setting is specified.\n@@ -296,7 +296,7 @@ class FeedExporter:\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n         :param spider: user spider\n-        :param template_uri: template uri which contains %(batch_time)s or %(batch_id)s to create new uri\n+        :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)s to create new uri\n         \"\"\"\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n@@ -315,7 +315,7 @@ class FeedExporter:\n             format=feed['format'],\n             store_empty=feed['store_empty'],\n             batch_id=batch_id,\n-            template_uri=template_uri,\n+            uri_template=uri_template,\n         )\n         if slot.store_empty:\n             slot.start_exporting()\n@@ -329,14 +329,14 @@ class FeedExporter:\n             slot.itemcount += 1\n             # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_ITEM_COUNT and close the old one\n             if self.storage_batch_size and slot.itemcount == self.storage_batch_size:\n-                uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'], slot)\n+                uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n                 self._close_slot(slot, spider)\n                 slots.append(self._start_new_batch(\n                     batch_id=slot.batch_id + 1,\n-                    uri=slot.template_uri % uri_params,\n-                    feed=self.feeds[slot.template_uri],\n+                    uri=slot.uri_template % uri_params,\n+                    feed=self.feeds[slot.uri_template],\n                     spider=spider,\n-                    template_uri=slot.template_uri,\n+                    uri_template=slot.uri_template,\n                 ))\n             else:\n                 slots.append(slot)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36c3c9713e59f5d22bf51354920b5093e2d30b73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 779 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ import re\n import sys\n from subprocess import Popen, PIPE\n from urllib.parse import urlsplit, urlunsplit\n+from unittest import skipIf\n \n import pytest\n from testfixtures import LogCapture\n@@ -56,6 +57,8 @@ def _wrong_credentials(proxy_url):\n     return urlunsplit(bad_auth_proxy)\n \n \n+@skipIf(sys.version_info < (3, 5, 4),\n+        \"requires mitmproxy < 3.0.0, which these tests do not support\")\n class ProxyConnectTestCase(TestCase):\n \n     def setUp(self):\n@@ -80,7 +83,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, log)\n \n-    @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info.minor >= 6)\n+    @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info >= (3, 6))\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cdd00e21f4bfe22ba9b8fabe034a5e4d34dab75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 605 | Contributors (this commit): 20 | Commits (past 90d): 16 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -107,7 +107,7 @@ class ScrapyCommand:\n         raise NotImplementedError\n \n \n-class BaseRunSpiderCommands(ScrapyCommand):\n+class BaseRunSpiderCommand(ScrapyCommand):\n     \"\"\"\n     The BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\n     performing common functionality between crawl.py and runspider.py\n\n@@ -1,8 +1,8 @@\n-from scrapy.commands import BaseRunSpiderCommands\n+from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n \n \n-class Command(BaseRunSpiderCommands):\n+class Command(BaseRunSpiderCommand):\n \n     requires_project = True\n \n\n@@ -4,7 +4,7 @@ from importlib import import_module\n \n from scrapy.utils.spider import iter_spider_classes\n from scrapy.exceptions import UsageError\n-from scrapy.commands import BaseRunSpiderCommands\n+from scrapy.commands import BaseRunSpiderCommand\n \n \n def _import_file(filepath):\n@@ -23,7 +23,7 @@ def _import_file(filepath):\n     return module\n \n \n-class Command(BaseRunSpiderCommands):\n+class Command(BaseRunSpiderCommand):\n \n     requires_project = False\n     default_settings = {'SPIDER_LOADER_WARN_ONLY': True}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10ae1a284f759b541d086e3d1a13cda96b6e2040", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 3026 | Contributors (this commit): 26 | Commits (past 90d): 27 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -243,11 +243,11 @@ class FeedExporter:\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n-        self.storage_batch_size = self.settings.get('FEED_STORAGE_BATCH_ITEM_COUNT', None)\n+        self.storage_batch_item_count = self.settings.get('FEED_STORAGE_BATCH_ITEM_COUNT', None)\n         for uri, feed in self.feeds.items():\n             if not self._storage_supported(uri):\n                 raise NotConfigured\n-            if not self._batch_deliveries_supported(uri):\n+            if not self._settings_are_valid(uri):\n                 raise NotConfigured\n             if not self._exporter_supported(feed['format']):\n                 raise NotConfigured\n@@ -328,7 +328,7 @@ class FeedExporter:\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n             # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_ITEM_COUNT and close the old one\n-            if self.storage_batch_size and slot.itemcount == self.storage_batch_size:\n+            if self.storage_batch_item_count and slot.itemcount == self.storage_batch_item_count:\n                 uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)\n                 self._close_slot(slot, spider)\n                 slots.append(self._start_new_batch(\n@@ -357,12 +357,12 @@ class FeedExporter:\n             return True\n         logger.error(\"Unknown feed format: %(format)s\", {'format': format})\n \n-    def _batch_deliveries_supported(self, uri):\n+    def _settings_are_valid(self, uri):\n         \"\"\"\n         If FEED_STORAGE_BATCH_ITEM_COUNT setting is specified uri has to contain %(batch_time)s or %(batch_id)s\n         to distinguish different files of partial output\n         \"\"\"\n-        if self.storage_batch_size is None or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n+        if not self.storage_batch_item_count or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n             return True\n         logger.warning('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified')\n         return False\n\n@@ -986,7 +986,7 @@ class FeedExportTest(FeedExportTestBase):\n         self.assertEqual(data['csv'], b'')\n \n \n-class PartialDeliveriesTest(FeedExportTestBase):\n+class BatchDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n     _file_mark = '_%(batch_time)s_#%(batch_id)s_'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#604fe33bad36f1269677e98d0bfec1f60c95aa53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 78 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -108,8 +108,7 @@ class ScrapyCommand:\n \n class BaseRunSpiderCommand(ScrapyCommand):\n     \"\"\"\n-    The BaseRunSpiderCommands class inherits the ScrapyCommand class and it Used for\n-    performing common functionality between crawl.py and runspider.py\n+    Common class used to share functionality between the crawl and runspider commands\n     \"\"\"\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a915af2e4592e8a0367c44a73c95cee3f835887d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 37 | Files Changed: 4 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 66 | Churn Cumulative: 687 | Contributors (this commit): 27 | Commits (past 90d): 8 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -2,33 +2,11 @@\n Scrapy - a web crawling and web scraping framework written for Python\n \"\"\"\n \n-__all__ = ['__version__', 'version_info', 'twisted_version',\n-           'Spider', 'Request', 'FormRequest', 'Selector', 'Item', 'Field']\n-\n-# Scrapy version\n import pkgutil\n-__version__ = pkgutil.get_data(__package__, 'VERSION').decode('ascii').strip()\n-version_info = tuple(int(v) if v.isdigit() else v\n-                     for v in __version__.split('.'))\n-del pkgutil\n-\n-# Check minimum required Python version\n import sys\n-if sys.version_info < (3, 5):\n-    print(\"Scrapy %s requires Python 3.5\" % __version__)\n-    sys.exit(1)\n-\n-# Ignore noisy twisted deprecation warnings\n import warnings\n-warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n-del warnings\n-\n-# Apply monkey patches to fix issues in external libraries\n-from scrapy import _monkeypatches\n-del _monkeypatches\n \n from twisted import version as _txv\n-twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n # Declare top-level shortcuts\n from scrapy.spiders import Spider\n@@ -36,4 +14,29 @@ from scrapy.http import Request, FormRequest\n from scrapy.selector import Selector\n from scrapy.item import Item, Field\n \n+\n+__all__ = [\n+    '__version__', 'version_info', 'twisted_version', 'Spider',\n+    'Request', 'FormRequest', 'Selector', 'Item', 'Field',\n+]\n+\n+\n+# Scrapy and Twisted versions\n+__version__ = pkgutil.get_data(__package__, 'VERSION').decode('ascii').strip()\n+version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split('.'))\n+twisted_version = (_txv.major, _txv.minor, _txv.micro)\n+\n+\n+# Check minimum required Python version\n+if sys.version_info < (3, 5):\n+    print(\"Scrapy %s requires Python 3.5\" % __version__)\n+    sys.exit(1)\n+\n+\n+# Ignore noisy twisted deprecation warnings\n+warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n+\n+\n+del pkgutil\n del sys\n+del warnings\n\n@@ -1,11 +0,0 @@\n-import copyreg\n-\n-\n-# Undo what Twisted's perspective broker adds to pickle register\n-# to prevent bugs like Twisted#7989 while serializing requests\n-import twisted.persisted.styles  # NOQA\n-# Remove only entries with twisted serializers for non-twisted types.\n-for k, v in frozenset(copyreg.dispatch_table.items()):\n-    if not str(getattr(k, '__module__', '')).startswith('twisted') \\\n-            and str(getattr(v, '__module__', '')).startswith('twisted'):\n-        copyreg.dispatch_table.pop(k)\n\n@@ -133,4 +133,4 @@ class FilteringLinkExtractor:\n \n \n # Top-level imports\n-from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor  # noqa: F401\n+from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n\n@@ -110,6 +110,6 @@ class Spider(object_ref):\n \n \n # Top-level imports\n-from scrapy.spiders.crawl import CrawlSpider, Rule  # noqa: F401\n-from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider  # noqa: F401\n-from scrapy.spiders.sitemap import SitemapSpider  # noqa: F401\n+from scrapy.spiders.crawl import CrawlSpider, Rule\n+from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider\n+from scrapy.spiders.sitemap import SitemapSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a22f97052e9b14631a977600c1f59bd468c85601", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 436 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,6 @@\n \"\"\"\n Selectors\n \"\"\"\n-from scrapy.selector.unified import *  # noqa: F401\n+\n+# top-level imports\n+from scrapy.selector.unified import Selector, SelectorList\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bcc40c40771ad223471ab77ad47233c498312095", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 718 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ class TextResponse(Response):\n         \"\"\"Return body as unicode\"\"\"\n         warnings.warn('Response.body_as_unicode() is deprecated, '\n                       'please use Response.text instead.',\n-                      ScrapyDeprecationWarning)\n+                      ScrapyDeprecationWarning, stacklevel=2)\n         return self.text\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a7d070f3bb350cbe1f7b580350d5f491f59d47d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 774 | Contributors (this commit): 22 | Commits (past 90d): 14 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -364,7 +364,7 @@ class FeedExporter:\n         \"\"\"\n         if not self.storage_batch_item_count or '%(batch_time)s' in uri or '%(batch_id)s' in uri:\n             return True\n-        logger.warning('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified')\n+        logger.error('%(batch_time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_ITEM_COUNT setting is specified')\n         return False\n \n     def _storage_supported(self, uri):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2a0cca0fe6bc8342efa6034a78b9b8161aa2177", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 244 | Contributors (this commit): 18 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -100,6 +100,9 @@ exclude_trees = ['.build']\n # The name of the Pygments (syntax highlighting) style to use.\n pygments_style = 'sphinx'\n \n+# List of Sphinx warnings that will not be raised\n+suppress_warnings = ['epub.unknown_project_files']\n+\n \n # Options for HTML output\n # -----------------------\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#276721a5dc96a4239645db1a78fe056df671e10e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 183 | Lines Deleted: 36 | Files Changed: 7 | Hunks: 33 | Methods Changed: 18 | Complexity Δ (Sum/Max): 25/7 | Churn Δ: 219 | Churn Cumulative: 5091 | Contributors (this commit): 45 | Commits (past 90d): 33 | Contributors (cumulative): 91 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from urllib.parse import urldefrag\n from twisted.internet import defer, protocol, ssl\n from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n+from twisted.python.failure import Failure\n from twisted.web.client import Agent, HTTPConnectionPool, ResponseDone, ResponseFailed, URI\n from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n@@ -21,7 +22,7 @@ from zope.interface import implementer\n from scrapy import signals\n from scrapy.core.downloader.tls import openssl_methods\n from scrapy.core.downloader.webclient import _parse\n-from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.exceptions import ScrapyDeprecationWarning, StopDownload\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.misc import create_instance, load_object\n@@ -431,7 +432,7 @@ class ScrapyAgent:\n     def _cb_bodydone(self, result, request, url):\n         headers = Headers(result[\"txresponse\"].headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n-        return respcls(\n+        response = respcls(\n             url=url,\n             status=int(result[\"txresponse\"].code),\n             headers=headers,\n@@ -440,6 +441,14 @@ class ScrapyAgent:\n             certificate=result[\"certificate\"],\n             ip_address=result[\"ip_address\"],\n         )\n+        if result.get(\"failure\"):\n+            # This failure is not the same object that will reach the errback,\n+            # so we need to temporarily store the response in the exception.\n+            # It will be moved to the failure in core/scraper.py\n+            failure = result[\"failure\"]\n+            failure.value.response = response\n+            return failure\n+        return response\n \n \n @implementer(IBodyProducer)\n@@ -477,6 +486,16 @@ class _ResponseReader(protocol.Protocol):\n         self._ip_address = None\n         self._crawler = crawler\n \n+    def _finish_response(self, flags=None, failure=None):\n+        self._finished.callback({\n+            \"txresponse\": self._txresponse,\n+            \"body\": self._bodybuf.getvalue(),\n+            \"flags\": flags,\n+            \"certificate\": self._certificate,\n+            \"ip_address\": self._ip_address,\n+            \"failure\": failure,\n+        })\n+\n     def connectionMade(self):\n         if self._certificate is None:\n             with suppress(AttributeError):\n@@ -493,12 +512,21 @@ class _ResponseReader(protocol.Protocol):\n         self._bodybuf.write(bodyBytes)\n         self._bytes_received += len(bodyBytes)\n \n-        self._crawler.signals.send_catch_log(\n+        bytes_received_result = self._crawler.signals.send_catch_log(\n             signal=signals.bytes_received,\n             data=bodyBytes,\n             request=self._request,\n             spider=self._crawler.spider,\n         )\n+        for handler, result in bytes_received_result:\n+            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n+                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n+                             {\"request\": self._request, \"handler\": handler.__qualname__})\n+                self.transport._producer.loseConnection()\n+                self._finish_response(\n+                    flags=[\"download_stopped\"],\n+                    failure=result if result.value.fail else None,\n+                )\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n@@ -521,36 +549,17 @@ class _ResponseReader(protocol.Protocol):\n         if self._finished.called:\n             return\n \n-        body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n-            self._finished.callback({\n-                \"txresponse\": self._txresponse,\n-                \"body\": body,\n-                \"flags\": None,\n-                \"certificate\": self._certificate,\n-                \"ip_address\": self._ip_address,\n-            })\n+            self._finish_response()\n             return\n \n         if reason.check(PotentialDataLoss):\n-            self._finished.callback({\n-                \"txresponse\": self._txresponse,\n-                \"body\": body,\n-                \"flags\": [\"partial\"],\n-                \"certificate\": self._certificate,\n-                \"ip_address\": self._ip_address,\n-            })\n+            self._finish_response(flags=[\"partial\"])\n             return\n \n         if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n             if not self._fail_on_dataloss:\n-                self._finished.callback({\n-                    \"txresponse\": self._txresponse,\n-                    \"body\": body,\n-                    \"flags\": [\"dataloss\"],\n-                    \"certificate\": self._certificate,\n-                    \"ip_address\": self._ip_address,\n-                })\n+                self._finish_response(flags=[\"dataloss\"])\n                 return\n \n             elif not self._fail_on_dataloss_warned:\n\n@@ -11,7 +11,7 @@ from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errba\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n-from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n+from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest, StopDownload\n from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import _BaseItem\n@@ -147,6 +147,14 @@ class Scraper:\n \n     def call_spider(self, result, request, spider):\n         result.request = request\n+        # StopDownload exceptions: make the partial response an attribute of the failure\n+        if (\n+            isinstance(result, Failure)\n+            and isinstance(result.value, StopDownload)\n+            and hasattr(result.value, \"response\")\n+        ):\n+            result.response = result.value.response\n+            delattr(result.value, \"response\")\n         dfd = defer_result(result)\n         callback = request.callback or spider.parse\n         warn_on_generator_with_return_value(spider, callback)\n\n@@ -41,6 +41,18 @@ class CloseSpider(Exception):\n         self.reason = reason\n \n \n+class StopDownload(Exception):\n+    \"\"\"\n+    Stop the download of the body for a given response.\n+    The 'fail' boolean parameter indicates whether or not the resulting partial response\n+    should be handled by the request errback. Note that 'fail' is a keyword-only argument.\n+    \"\"\"\n+\n+    def __init__(self, *, fail=True):\n+        super().__init__()\n+        self.fail = fail\n+\n+\n # Items\n \n \n@@ -59,6 +71,7 @@ class NotSupported(Exception):\n \n class UsageError(Exception):\n     \"\"\"To indicate a command-line usage error\"\"\"\n+\n     def __init__(self, *a, **kw):\n         self.print_help = kw.pop('print_help', True)\n         super(UsageError, self).__init__(*a, **kw)\n\n@@ -5,13 +5,14 @@ import logging\n from twisted.internet.defer import DeferredList, Deferred\n from twisted.python.failure import Failure\n \n-from pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n-    getAllReceivers, disconnect\n+from pydispatch.dispatcher import Anonymous, Any, disconnect, getAllReceivers, liveReceivers\n from pydispatch.robustapply import robustApply\n \n+from scrapy.exceptions import StopDownload\n from scrapy.utils.defer import maybeDeferred_coro\n from scrapy.utils.log import failure_to_exc_info\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -23,7 +24,7 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n     \"\"\"\n-    dont_log = named.pop('dont_log', _IgnoredException)\n+    dont_log = (named.pop('dont_log', _IgnoredException), StopDownload)\n     spider = named.get('spider', None)\n     responses = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n\n@@ -7,6 +7,8 @@ from urllib.parse import urlencode\n \n from twisted.internet import defer\n \n+from scrapy import signals\n+from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.item import Item\n from scrapy.linkextractors import LinkExtractor\n@@ -267,3 +269,34 @@ class CrawlSpiderWithErrback(MockServerSpider, CrawlSpider):\n \n     def errback(self, failure):\n         self.logger.info('[errback] status %i', failure.value.response.status)\n+\n+\n+class BytesReceivedCallbackSpider(MetaSpider):\n+\n+    full_response_length = 2**18\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        crawler.signals.connect(spider.bytes_received, signals.bytes_received)\n+        return spider\n+\n+    def start_requests(self):\n+        body = b\"a\" * self.full_response_length\n+        url = self.mockserver.url(\"/alpayload\")\n+        yield Request(url, method=\"POST\", body=body, errback=self.errback)\n+\n+    def parse(self, response):\n+        self.meta[\"response\"] = response\n+\n+    def errback(self, failure):\n+        self.meta[\"failure\"] = failure\n+\n+    def bytes_received(self, data, request, spider):\n+        raise StopDownload(fail=False)\n+\n+\n+class BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):\n+\n+    def bytes_received(self, data, request, spider):\n+        raise StopDownload(fail=True)\n\n@@ -9,17 +9,31 @@ from pytest import mark\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.internet.ssl import Certificate\n+from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n \n from scrapy import signals\n from scrapy.crawler import CrawlerRunner\n+from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n+from scrapy.http.response import Response\n from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n-from tests.spiders import (FollowAllSpider, DelaySpider, SimpleSpider, BrokenStartRequestsSpider,\n-                           SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback,\n-                           AsyncDefSpider, AsyncDefAsyncioSpider, AsyncDefAsyncioReturnSpider,\n-                           AsyncDefAsyncioReqsReturnSpider)\n+from tests.spiders import (\n+    AsyncDefAsyncioReqsReturnSpider,\n+    AsyncDefAsyncioReturnSpider,\n+    AsyncDefAsyncioSpider,\n+    AsyncDefSpider,\n+    BrokenStartRequestsSpider,\n+    BytesReceivedCallbackSpider,\n+    BytesReceivedErrbackSpider,\n+    CrawlSpiderWithErrback,\n+    DelaySpider,\n+    DuplicateStartRequestsSpider,\n+    FollowAllSpider,\n+    SimpleSpider,\n+    SingleRequestSpider,\n+)\n \n \n class CrawlTestCase(TestCase):\n@@ -457,3 +471,21 @@ with multiples lines\n         ip_address = crawler.spider.meta['responses'][0].ip_address\n         self.assertIsInstance(ip_address, IPv4Address)\n         self.assertEqual(str(ip_address), gethostbyname(expected_netloc))\n+\n+    @defer.inlineCallbacks\n+    def test_stop_download_callback(self):\n+        crawler = self.runner.create_crawler(BytesReceivedCallbackSpider)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n+        self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n+        self.assertLess(len(crawler.spider.meta[\"response\"].text), crawler.spider.full_response_length)\n+\n+    @defer.inlineCallbacks\n+    def test_stop_download_errback(self):\n+        crawler = self.runner.create_crawler(BytesReceivedErrbackSpider)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertIsNone(crawler.spider.meta.get(\"response\"))\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"].response, Response)\n+        self.assertLess(len(crawler.spider.meta[\"failure\"].response.text), crawler.spider.full_response_length)\n\n@@ -16,13 +16,15 @@ import sys\n from collections import defaultdict\n from urllib.parse import urlparse\n \n+from pydispatch import dispatcher\n+from testfixtures import LogCapture\n from twisted.internet import reactor, defer\n from twisted.trial import unittest\n from twisted.web import server, static, util\n-from pydispatch import dispatcher\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n+from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n@@ -90,7 +92,7 @@ def start_test_site(debug=False):\n     r = static.File(root_dir)\n     r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n     r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n-    numbers = [str(x).encode(\"utf8\") for x in range(2**14)]\n+    numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n     r.putChild(b\"numbers\", static.Data(b\"\".join(numbers), \"text/plain\"))\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n@@ -188,6 +190,16 @@ class CrawlerRun:\n         self.signals_caught[sig] = signalargs\n \n \n+class StopDownloadCrawlerRun(CrawlerRun):\n+    \"\"\"\n+    Make sure raising the StopDownload exception stops the download of the response body\n+    \"\"\"\n+\n+    def bytes_received(self, data, request, spider):\n+        super().bytes_received(data, request, spider)\n+        raise StopDownload(fail=False)\n+\n+\n class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n@@ -316,7 +328,7 @@ class EngineTest(unittest.TestCase):\n                 # signal was fired multiple times\n                 self.assertTrue(len(data) > 1)\n                 # bytes were received in order\n-                numbers = [str(x).encode(\"utf8\") for x in range(2**14)]\n+                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n                 self.assertEqual(joined_data, b\"\".join(numbers))\n \n     def _assert_signals_caught(self):\n@@ -357,6 +369,45 @@ class EngineTest(unittest.TestCase):\n         self.assertEqual(len(e.open_spiders), 0)\n \n \n+class StopDownloadEngineTest(EngineTest):\n+\n+    @defer.inlineCallbacks\n+    def test_crawler(self):\n+        for spider in TestSpider, DictItemsSpider:\n+            self.run = StopDownloadCrawlerRun(spider)\n+            with LogCapture() as log:\n+                yield self.run.run()\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   \"Download stopped for <GET http://localhost:{}/redirected> from signal handler\"\n+                                   \" StopDownloadCrawlerRun.bytes_received\".format(self.run.portno)))\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   \"Download stopped for <GET http://localhost:{}/> from signal handler\"\n+                                   \" StopDownloadCrawlerRun.bytes_received\".format(self.run.portno)))\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   \"Download stopped for <GET http://localhost:{}/numbers> from signal handler\"\n+                                   \" StopDownloadCrawlerRun.bytes_received\".format(self.run.portno)))\n+            self._assert_visited_urls()\n+            self._assert_scheduled_requests(urls_to_visit=9)\n+            self._assert_downloaded_responses()\n+            self._assert_signals_caught()\n+            self._assert_bytes_received()\n+\n+    def _assert_bytes_received(self):\n+        self.assertEqual(9, len(self.run.bytes))\n+        for request, data in self.run.bytes.items():\n+            joined_data = b\"\".join(data)\n+            self.assertTrue(len(data) == 1)  # signal was fired only once\n+            if self.run.getpath(request.url) == \"/numbers\":\n+                # Received bytes are not the complete response. The exact amount depends\n+                # on the buffer size, which can vary, so we only check that the amount\n+                # of received bytes is strictly less than the full response.\n+                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n+                self.assertTrue(len(joined_data) < len(b\"\".join(numbers)))\n+\n+\n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n         start_test_site(debug=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
