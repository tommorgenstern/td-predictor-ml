{"custom_id": "scrapy#f61be3f078f47ab4c849c58d41b986cfee7b0030", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 443 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ def canonical_string(method, path, headers, expires=None):\n     interesting_headers = {}\n     for key in headers:\n         lk = key.lower()\n-        if lk in set('content-md5', 'content-type', 'date') or lk.startswith(AMAZON_HEADER_PREFIX):\n+        if lk in set(['content-md5', 'content-type', 'date']) or lk.startswith(AMAZON_HEADER_PREFIX):\n             interesting_headers[lk] = headers[key].strip()\n \n     # these keys get empty strings if they don't exist\n\n@@ -40,7 +40,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n     def s3request(self, key, method, body=None, headers=None):\n         url = 'http://%s.s3.amazonaws.com/%s' % (self.bucket_name, key)\n         req = Request(url, method=method, body=body, headers=headers)\n-        sign_request(req, self.access_key, self.secret_keself.secret_keyy)\n+        sign_request(req, self.access_key, self.secret_key)\n         return req\n \n     def image_downloaded(self, response, request, info):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a60f6c1cb6afb58a358d4505753a7a4c3e0caa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 313 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,9 +21,9 @@ from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n class S3ImagesPipeline(BaseImagesPipeline):\n     MEDIA_TYPE = 'image'\n     THUMBS = (\n-        (\"50\", (50, 50)),\n-        (\"110\", (110, 110)),\n-        (\"270\", (270, 270))\n+#             ('50', (50, 50)),\n+#             ('110', (110, 110)),\n+#             ('270', (270, 270))\n     )\n \n     def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#011adf4dd7537301f30a156ff2503b59207f4993", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 447 | Contributors (this commit): 1 | Commits (past 90d): 20 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages\n from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex\n+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, canonicalize_urls, Delist, Regex\n from scrapy.utils.python import unique, flatten\n\n@@ -37,17 +37,22 @@ def clean_spaces(value):\n     _clean_spaces_re = re.compile(\"\\s+\", re.U)\n     return [ _clean_spaces_re.sub(' ', v.decode('utf-8')) for v in value ]\n \n-def strip_list(value):\n+def strip(value):\n     \"\"\"\n     Removes any spaces at both the start and the ending\n-    of each string in the provided list.\n+    of the provided string or list.\n     E.g:\n-      >> strip_list([' hi   ', 'buddies  '])\n+      >> strip([' hi   ', 'buddies  '])\n       [u'hi', u'buddies']\n+      >> strip(' hi buddies    ')\n+      u'hi buddies'\n \n-    Input: iterable of unicodes\n-    Output: list of unicodes\n+    Input: unicode or iterable of unicodes\n+    Output: unicode or list of unicodes\n     \"\"\"\n+    if isinstance(value, basestring):\n+        return unicode(value.strip())\n+    elif hasattr(value, '__iter__'):\n         return [ unicode(v.strip()) for v in value ]\n \n def drop_empty(value):\n\n@@ -69,9 +69,11 @@ class AdaptorsTestCase(unittest.TestCase):\n         self.assertEqual(adaptors.clean_spaces(['  hello,  whats     up?', 'testing testingtesting      testing']),\n                          [' hello, whats up?', 'testing testingtesting testing'])\n         \n-    def test_strip_list(self):\n-        self.assertEqual(adaptors.strip_list([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n+    def test_strip(self):\n+        self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),\n                          ['hi there, sweety ;D', 'I CAN HAZ TEST??'])\n+        self.assertEqual(adaptors.strip('      hello there, this is my test     '),\n+                         'hello there, this is my test')\n         \n     def test_drop_empty_elements(self):\n         self.assertEqual(adaptors.drop_empty([1, 2, None, 5, 0, 6, False, 'hi']),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33b20be7b5a0ff47dd9830ef29ecbc8532753a83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1270 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,13 +11,16 @@ class AdaptorPipe(list):\n     in order to filter the input.\n     \"\"\"\n     def __init__(self, adaptors):\n-        super(AdaptorPipe, self).__init__([adaptor for adaptor in adaptors if callable(adaptor)])\n+        for adaptor in adaptors:\n+            if not callable(adaptor):\n+                raise TypeError(\"%s is not a callable\" % adaptor)\n+        super(AdaptorPipe, self).__init__(adaptors)\n+        self.debug = settings.getbool('ADAPTORS_DEBUG')\n \n     def __call__(self, value, **kwargs):\n         \"\"\"\n         Execute the adaptor pipeline for this attribute.\n         \"\"\"\n-        debug = kwargs.pop('debug', all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE']))\n \n         for adaptor in self:\n             if inspect.isfunction(adaptor):\n@@ -30,7 +33,7 @@ class AdaptorPipe(list):\n                 name = adaptor.__class__.__name__ if hasattr(adaptor, '__class__') else adaptor.__name__\n \n             try:\n-                if debug:\n+                if self.debug:\n                     print \"  %07s | input >\" % name, repr(value)\n \n                 if 'adaptor_args' in func_args:\n@@ -38,7 +41,7 @@ class AdaptorPipe(list):\n                 else:\n                     value = adaptor(value)\n \n-                if debug:\n+                if self.debug:\n                     print \"  %07s | output >\" % name, repr(value)\n             except Exception:\n                 print \"Error in '%s' adaptor. Traceback text:\" % name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81416c4dd1049a89e10a4914dab25cb474faf121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 15 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 41 | Churn Cumulative: 354 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import md5\n import time\n import hmac\n import base64\n@@ -18,6 +19,16 @@ from scrapy.conf import settings\n from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n \n \n+def md5sum(buffer):\n+    m = md5.new()\n+    buffer.seek(0)\n+    while 1:\n+        d = buffer.read(8096)\n+        if not d: break\n+        m.update(d)\n+    return m.hexdigest()\n+\n+\n class S3ImagesPipeline(BaseImagesPipeline):\n     MEDIA_TYPE = 'image'\n     THUMBS = (\n@@ -46,7 +57,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n     def image_downloaded(self, response, request, info):\n         try:\n             key = self.s3_image_key(request.url)\n-            self.s3_store_image(response, request.url, info)\n+            etag = self.s3_store_image(response, request.url, info)\n         except ImageException, ex:\n             log.msg(str(ex), level=log.WARNING, domain=info.domain)\n             raise ex\n@@ -54,7 +65,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             log.msg(str(ex), level=log.WARNING, domain=info.domain)\n             raise ex\n \n-        return key # success value sent as input result for item_media_downloaded\n+        return '%s#%s' % (key, etag) # success value sent as input result for item_media_downloaded\n \n     def s3_image_key(self, url):\n         \"\"\"Return the relative path on the target filesystem for an image to be\n@@ -74,33 +85,32 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         \"\"\"Return if the image should be downloaded by checking if it's already in\n         the S3 storage and not too old\"\"\"\n \n-        def _on200(response):\n+        def _onsuccess(response):\n             if 'Last-Modified' not in response.headers:\n-                return True\n+                return # returning None force download\n \n+            # check if last modified date did not expires\n             last_modified = response.headers['Last-Modified'][0]\n             modified_tuple = rfc822.parsedate_tz(last_modified)\n             modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n             age_seconds = time.time() - modified_stamp\n             age_days = age_seconds / 60 / 60 / 24\n-            return age_days > self.image_refresh_days\n \n-        def _non200(_failure):\n-            return True\n+            if age_days > self.image_refresh_days:\n+                return # returning None force download\n \n-        def _evaluate(should):\n-            if not should:\n-                self.inc_stats(info.domain, 'uptodate')\n+            etag = response.headers['Etag'][0].strip('\"')\n             referer = request.headers.get('Referer')\n             log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n                     (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n-                return key\n+\n+            self.inc_stats(info.domain, 'uptodate')\n+            return '%s#%s' % (key, etag)\n \n         key = self.s3_image_key(request.url)\n         req = self.s3request(key, method='HEAD')\n         dfd = self.download(req, info)\n-        dfd.addCallbacks(_on200, _non200)\n-        dfd.addCallback(_evaluate)\n+        dfd.addCallbacks(_onsuccess, lambda _:None)\n         dfd.addErrback(log.err, 'S3ImagesPipeline.media_to_download')\n         return dfd\n \n@@ -109,8 +119,9 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         buf = StringIO(response.body.to_string())\n         image = Image.open(buf)\n         key = self.s3_image_key(url)\n-        self._s3_put_image(image, key, info)\n+        _, jpegbuf = self._s3_put_image(image, key, info)\n         self.s3_store_thumbnails(image, url, info)\n+        return md5sum(jpegbuf) # Etag\n \n     def s3_store_thumbnails(self, image, url, info):\n         \"\"\"Upload image thumbnails to S3 storage\"\"\"\n@@ -134,6 +145,6 @@ class S3ImagesPipeline(BaseImagesPipeline):\n                 }\n \n         req = self.s3request(key, method='PUT', body=buf.read(), headers=headers)\n-        return self.download(req, info)\n+        return self.download(req, info), buf\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c61d0648e61e1dc694af7625963131630dd4ada2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 357 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import md5\n import time\n import hmac\n import base64\n@@ -20,7 +19,7 @@ from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n \n \n def md5sum(buffer):\n-    m = md5.new()\n+    m = hashlib.md5()\n     buffer.seek(0)\n     while 1:\n         d = buffer.read(8096)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89bd6ede437c3d0ca99c8d551670644847fdc53c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 242 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 242 | Churn Cumulative: 317 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,187 @@\n+# -*- coding: utf-8 -*-\n+#\n+# Scrapy documentation build configuration file, created by\n+# sphinx-quickstart on Mon Nov 24 12:02:52 2008.\n+#\n+# This file is execfile()d with the current directory set to its containing dir.\n+#\n+# The contents of this file are pickled, so don't put values in the namespace\n+# that aren't pickleable (module imports are okay, they're removed automatically).\n+#\n+# All configuration values have a default; values that are commented out\n+# serve to show the default.\n+\n+import sys, os\n+\n+# If your extensions are in another directory, add it here. If the directory\n+# is relative to the documentation root, use os.path.abspath to make it\n+# absolute, like shown here.\n+#sys.path.append(os.path.abspath('.'))\n+\n+# General configuration\n+# ---------------------\n+\n+# Add any Sphinx extension module names here, as strings. They can be extensions\n+# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n+extensions = []\n+\n+# Add any paths that contain templates here, relative to this directory.\n+templates_path = ['../templates/docs']\n+\n+# The suffix of source filenames.\n+source_suffix = '.rst'\n+\n+# The encoding of source files.\n+#source_encoding = 'utf-8'\n+\n+# The master toctree document.\n+master_doc = 'index'\n+\n+# General information about the project.\n+project = u'Scrapy'\n+copyright = u'2008, Insophia'\n+\n+# The version info for the project you're documenting, acts as replacement for\n+# |version| and |release|, also used in various other places throughout the\n+# built documents.\n+#\n+# The short X.Y version.\n+version = '1.0'\n+# The full version, including alpha/beta/rc tags.\n+release = '1.0'\n+\n+# The language for content autogenerated by Sphinx. Refer to documentation\n+# for a list of supported languages.\n+language = 'en'\n+\n+# There are two options for replacing |today|: either, you set today to some\n+# non-false value, then it is used:\n+#today = ''\n+# Else, today_fmt is used as the format for a strftime call.\n+#today_fmt = '%B %d, %Y'\n+\n+# List of documents that shouldn't be included in the build.\n+#unused_docs = []\n+\n+# List of directories, relative to source directory, that shouldn't be searched\n+# for source files.\n+exclude_trees = ['.build']\n+\n+# The reST default role (used for this markup: `text`) to use for all documents.\n+#default_role = None\n+\n+# If true, '()' will be appended to :func: etc. cross-reference text.\n+#add_function_parentheses = True\n+\n+# If true, the current module name will be prepended to all description\n+# unit titles (such as .. function::).\n+#add_module_names = True\n+\n+# If true, sectionauthor and moduleauthor directives will be shown in the\n+# output. They are ignored by default.\n+#show_authors = False\n+\n+# The name of the Pygments (syntax highlighting) style to use.\n+pygments_style = 'sphinx'\n+\n+\n+# Options for HTML output\n+# -----------------------\n+\n+# The style sheet to use for HTML and HTML Help pages. A file of that name\n+# must exist either in Sphinx' static/ path, or in one of the custom paths\n+# given in html_static_path.\n+html_style = 'default.css'\n+\n+# The name for this set of Sphinx documents.  If None, it defaults to\n+# \"<project> v<release> documentation\".\n+#html_title = None\n+\n+# A shorter title for the navigation bar.  Default is the same as html_title.\n+#html_short_title = None\n+\n+# The name of an image file (relative to this directory) to place at the top\n+# of the sidebar.\n+#html_logo = None\n+\n+# The name of an image file (within the static path) to use as favicon of the\n+# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n+# pixels large.\n+#html_favicon = None\n+\n+# Add any paths that contain custom static files (such as style sheets) here,\n+# relative to this directory. They are copied after the builtin static files,\n+# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n+html_static_path = ['.static']\n+\n+# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n+# using the given strftime format.\n+#html_last_updated_fmt = '%b %d, %Y'\n+\n+# If true, SmartyPants will be used to convert quotes and dashes to\n+# typographically correct entities.\n+#html_use_smartypants = True\n+\n+# Custom sidebar templates, maps document names to template names.\n+#html_sidebars = {}\n+\n+# Additional templates that should be rendered to pages, maps page names to\n+# template names.\n+#html_additional_pages = {}\n+\n+# If false, no module index is generated.\n+#html_use_modindex = True\n+\n+# If false, no index is generated.\n+#html_use_index = True\n+\n+# If true, the index is split into individual pages for each letter.\n+#html_split_index = False\n+\n+# If true, the reST sources are included in the HTML build as _sources/<name>.\n+html_copy_source = False\n+\n+# If true, an OpenSearch description file will be output, and all pages will\n+# contain a <link> tag referring to it.  The value of this option must be the\n+# base URL from which the finished HTML is served.\n+#html_use_opensearch = ''\n+\n+# If nonempty, this is the file name suffix for HTML files (e.g. \".xhtml\").\n+#html_file_suffix = ''\n+\n+# Output file base name for HTML help builder.\n+htmlhelp_basename = 'Scrapydoc'\n+\n+\n+# Options for LaTeX output\n+# ------------------------\n+\n+# The paper size ('letter' or 'a4').\n+#latex_paper_size = 'letter'\n+\n+# The font size ('10pt', '11pt' or '12pt').\n+#latex_font_size = '10pt'\n+\n+# Grouping the document tree into LaTeX files. List of tuples\n+# (source start file, target name, title, author, document class [howto/manual]).\n+latex_documents = [\n+  ('index', 'Scrapy.tex', ur'Scrapy Documentation',\n+   ur'Insophia', 'manual'),\n+]\n+\n+# The name of an image file (relative to this directory) to place at the top of\n+# the title page.\n+#latex_logo = None\n+\n+# For \"manual\" documents, if this is true, then toplevel headings are parts,\n+# not chapters.\n+#latex_use_parts = False\n+\n+# Additional stuff for the LaTeX preamble.\n+#latex_preamble = ''\n+\n+# Documents to append as an appendix to all manuals.\n+#latex_appendices = []\n+\n+# If false, no module index is generated.\n+#latex_use_modindex = True\n\n\n@@ -0,0 +1,3 @@\n+from django.db import models\n+\n+# Create your models here.\n\n@@ -0,0 +1,18 @@\n+from django.conf import settings\n+from django.conf.urls.defaults import *\n+\n+from scrapyorg.docs.views import index, document\n+\n+\n+urlpatterns = patterns('',\n+    (r'^$', index),\n+    (r'^(?P<url>[\\w./-]*)/$', document),\n+)\n+\n+if settings.DEBUG: # devel\n+    urlpatterns += patterns('',         \n+        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:],\n+          'django.views.static.serve',\n+          {'document_root': settings.MEDIA_ROOT}),\n+    )\n+\n\n@@ -0,0 +1,28 @@\n+import cPickle as pickle\n+import os\n+\n+from django.conf import settings\n+from django.http import Http404\n+from django.shortcuts import render_to_response\n+from django.template import RequestContext\n+\n+\n+def index(request):\n+    return document(request, '')\n+\n+\n+def document(request, url):\n+    docroot = settings.DOC_PICKLE_ROOT\n+\n+    if os.path.exists(os.path.join(docroot, url, 'index.fpickle')):\n+        docpath = os.path.join(docroot, url, 'index.fpickle')\n+    elif os.path.exists(os.path.join(docroot, url + '.fpickle')):\n+        docpath = os.path.join(docroot, url + '.fpickle')\n+    else:\n+        raise Http404(\"'%s' does not exist\" % url)\n+\n+    docfile = open(docpath, 'rb')\n+    doc = pickle.load(docfile)\n+\n+    return render_to_response('docs/doc.html', {'doc': doc},\n+                              context_instance=RequestContext(request))\n\n@@ -79,8 +79,11 @@ INSTALLED_APPS = (\n     'django.contrib.markup',\n     'scrapyorg.article',\n     'scrapyorg.download',\n+    'scrapyorg.docs',\n )\n \n+DOC_PICKLE_ROOT = os.path.join(PROJECT_ROOT, 'docs', 'pickle')\n+\n # Override previous settings with values in local_settings.py settings file.\n try:\n     from local_settings import *\n\n@@ -10,6 +10,9 @@ urlpatterns = patterns('',\n     # admin\n     url(r\"^admin/download/downloadlink/\", include(\"scrapyorg.download.urls\")),\n     url(r'^admin/(.*)', admin.site.root),\n+\n+    # docs\n+    url(r\"^docs/\", include(\"scrapyorg.docs.urls\")),\n )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#537a3457cfaa0cf4d8ca4448e3655dd6beab2518", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 364 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -95,7 +95,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             age_seconds = time.time() - modified_stamp\n             age_days = age_seconds / 60 / 60 / 24\n \n-            if age_days > self.image_refresh_days:\n+            if age_days < self.image_refresh_days:\n                 return # returning None force download\n \n             etag = response.headers['Etag'][0].strip('\"')\n@@ -136,13 +136,16 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             image.save(buf, 'JPEG')\n         except Exception, ex:\n             raise ImageException(\"Cannot process image. Error: %s\" % ex)\n-        buf.seek(0)\n \n+        width, height = image.size\n         headers = {\n                 'Content-Type': 'image/jpeg',\n                 'X-Amz-Acl': 'public-read',\n+                'X-Amz-Meta-Width': str(width),\n+                'X-Amz-Meta-Height': str(height),\n                 }\n \n+        buf.seek(0)\n         req = self.s3request(key, method='PUT', body=buf.read(), headers=headers)\n         return self.download(req, info), buf\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe49bc2011a233c22095f6eb22c7fc907fbc1924", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 22 | Churn Cumulative: 444 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -149,12 +149,18 @@ class UrlUtilsTest(unittest.TestCase):\n                                           u\"http://user:pass@www.example.com/do?a=1#frag\")\n \n     def test_check_valid_urlencode(self):\n-        self.assertFalse(check_valid_urlencode(r'http://www.example.com/pictures detail CAN43664.jpg'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/pictures%20detail%20CAN43664.jpg'))\n-\n         self.assertFalse(check_valid_urlencode(r'http://www.example.com/pictures\\detail\\CAN43664.jpg'))\n         self.assertTrue(check_valid_urlencode('http://www.example.com/pictures%5Cdetail%5CCAN43664.jpg'))\n \n+        self.assertFalse(check_valid_urlencode('http://www.example.com/pictures detail CAN43664.jpg'))\n+        self.assertTrue(check_valid_urlencode('http://www.example.com/pictures+detail%20CAN43664.jpg'))\n+\n+        self.assertFalse(check_valid_urlencode('http://www.example.com/?q=foo bar&q2=foo2 bar2'))\n+        self.assertTrue(check_valid_urlencode('http://www.example.com/?q=foo+bar&q2=foo2%20bar2'))\n+\n+        self.assertFalse(check_valid_urlencode('http://www.example.com/product,little:london$set%'))\n+        self.assertTrue(check_valid_urlencode('http://www.example.com/product%2Clittle%3Alondon%24set%25'))\n+\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n@@ -148,7 +148,11 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     return urlparse.urlunparse(parts)\n \n def check_valid_urlencode(url):\n-    \"\"\" check that the url-path is properly quoted\n-    TODO should also check the parameters \"\"\"\n+    \"\"\" check that the url-path and arguments are properly quoted \"\"\"\n+    def check_str(s):\n+        return all((s_plus == urllib.quote(urllib.unquote(s_plus)) for s_plus in s.split('+')))\n+    def check_param(p):\n+        return all((check_str(s) for s in p.split('=', 1)))\n+\n     split_result = urlparse.urlsplit(url)\n-    return split_result[2] == urllib.quote(urllib.unquote(split_result[2]))\n+    return check_str(split_result[2]) and all((check_param(p) for p in split_result[3].split('&')))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#04bd65de003e5ddaf49fa84e24993dec80bd5ced", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 224 | Lines Deleted: 90 | Files Changed: 7 | Hunks: 9 | Methods Changed: 21 | Complexity Δ (Sum/Max): 30/34 | Churn Δ: 314 | Churn Cumulative: 1245 | Contributors (this commit): 2 | Commits (past 90d): 43 | Contributors (cumulative): 11 | DMM Complexity: 0.6140350877192983\n\nDIFF:\n@@ -1 +1 @@\n-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline\n+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline\n\n@@ -30,11 +30,6 @@ class ValidationPipeline(object):\n         item.validate()\n         return item\n \n-class SetGUIDPipeline(object):\n-    def process_item(self, domain, response, item):\n-        spiders.fromdomain(domain).set_guid(item)\n-        return item\n-\n class RobustScrapedItem(ScrapedItem):\n     \"\"\"\n     A more robust scraped item class with a built-in validation mechanism and \n\n@@ -1,83 +0,0 @@\n-\"\"\"\n-This module contains the basic crawling spider that you can use to inherit your\n-spider from.\n-\"\"\"\n-\n-from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n-from scrapy.item import ScrapedItem\n-\n-class CrawlSpider(BaseSpider):\n-    \"\"\"\n-    This is the base class for crawling spiders. It is based on a list of\n-    crawling rules (stored in the \"rules\" attribute) which specify how links\n-    are extracted and followed, and how pages are processed (by using a\n-    callback function).\n-\n-    For more info about rules see the Rule class.\n-    \"\"\"\n-\n-    def parse(self, response):\n-        \"\"\"Method called by the framework core for all the start_urls. Do not\n-        override this function, override parse_start_url instead.\n-        \"\"\"\n-        if response.url in self.start_urls:\n-            return self._parse_wrapper(response, self.parse_start_url, cb_kwargs={}, follow=True) \n-        else:\n-            return self.parse_url(response)\n-\n-    def parse_start_url(self, response):\n-        \"\"\"Callback function for processing start_urls. It must return a list\n-        of ScrapedItems and/or Requests.\n-        \"\"\"\n-        return []\n-\n-    def _requests_to_follow(self, response):\n-        requests = []\n-        seen = set()\n-        for rule in self.rules:\n-            callback = rule.callback if callable(rule.callback) else getattr(self, rule.callback, None)\n-            links = [l for l in rule.link_extractor.extract_urls(response) if l not in seen]\n-            seen.union(links)\n-            for link in links:\n-                r = Request(url=link.url, link_text=link.text)\n-                r.append_callback(self._parse_wrapper, callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n-                requests.append(r)\n-        return requests\n-\n-    def _parse_wrapper(self, response, callback, cb_kwargs, follow):\n-        res = []\n-        if follow:\n-            res.extend(self._requests_to_follow(response))\n-        if callback:\n-            res.extend(callback(response, **cb_kwargs) or ())\n-        return res\n-\n-\n-class Rule(object):\n-    \"\"\"\n-    A rule for crawling, which receives the following constructor arguments:\n-\n-    link_extractor (required)\n-       A LinkExtractor which defines the policy for extracting links\n-    callback (optional)\n-       A function to use to process the page once it has been downloaded. If\n-       callback is omitted the page is not procesed, just crawled. If callback\n-       is a string (instead of callable) a method of the spider class with that\n-       name is used as the callback function\n-    cb_kwargs (optional)\n-       A dict specifying keyword arguments to pass to the callback function\n-    follow (optional)\n-       If True, links will be followed from the pages crawled by this rule.\n-       It defaults to True when no callback is specified or False when a\n-       callback is specified\n-    \"\"\"\n-    \n-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None):\n-        self.link_extractor = link_extractor\n-        self.callback = callback\n-        self.cb_kwargs = cb_kwargs or {}\n-        if follow is None:\n-            self.follow = False if callback else True\n-        else:\n-            self.follow = follow \n\n@@ -0,0 +1,2 @@\n+from scrapy.contrib.spiders2.crawl import CrawlSpider, Rule\n+from scrapy.contrib.spiders2.feed import XMLFeedSpider, CSVFeedSpider\n\n@@ -0,0 +1,140 @@\n+from scrapy.http import Request\n+from scrapy.spider import BaseSpider\n+from scrapy.item import ScrapedItem\n+\n+class Rule(object):\n+    \"\"\"\n+    A rule for crawling, which receives the following constructor arguments:\n+\n+    link_extractor (required)\n+       A LinkExtractor which defines the policy for extracting links\n+    callback (optional)\n+       A function to use to process the page once it has been downloaded. If\n+       callback is omitted the page is not procesed, just crawled. If callback\n+       is a string (instead of callable) a method of the spider class with that\n+       name is used as the callback function\n+    cb_kwargs (optional)\n+       A dict specifying keyword arguments to pass to the callback function\n+    follow (optional)\n+       If True, links will be followed from the pages crawled by this rule.\n+       It defaults to True when no callback is specified or False when a\n+       callback is specified\n+    link_filter (optional)\n+       Can be either a callable, or a string with the name of a method defined\n+       in the spider's class.\n+       This method will be called with the list of extracted links matching\n+       this rule (if any) and must return another list of links.\n+    \"\"\"\n+\n+    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, link_filter=None):\n+        self.link_extractor = link_extractor\n+        self.callback = callback\n+        self.cb_kwargs = cb_kwargs or {}\n+        self.link_filter = link_filter\n+        if follow is None:\n+            self.follow = False if callback else True\n+        else:\n+            self.follow = follow\n+\n+class CrawlSpider(BaseSpider):\n+    \"\"\"\n+    Class for spiders that crawl over web pages and extract/parse their links\n+    given some crawling rules.\n+\n+    These crawling rules are established by setting the 'rules' class attribute,\n+    which is a tuple of Rule objects.\n+    When the spider is running, it iterates over these rules with each response\n+    and do what it has to (extract links if follow=True, and return items/requests if\n+    there's a parsing method defined in the rule).\n+    \"\"\"\n+    rules = ()\n+\n+    def __init__(self):\n+        def _get_method(method):\n+            if isinstance(method, basestring):\n+                return getattr(self, method, None)\n+            elif method and callable(method):\n+                return method\n+\n+        super(CrawlSpider, self).__init__()\n+        if not hasattr(self, 'rules'):\n+            return\n+        for rule in self.rules:\n+            rule.callback = _get_method(rule.callback)\n+            rule.link_filter = _get_method(rule.link_filter)\n+\n+    def parse(self, response):\n+        \"\"\"This function is called by the core for all the start_urls. Do not\n+        override this function, override parse_start_url instead.\"\"\"\n+        if response.url in self.start_urls:\n+            return self._parse_wrapper(response, self.parse_start_url, cb_kwargs={}, follow=True)\n+        else:\n+            return self.parse_url(response)\n+\n+    def parse_start_url(self, response):\n+        \"\"\"Callback function for processing start_urls. It must return a list\n+        of ScrapedItems and/or Requests.\"\"\"\n+        return []\n+\n+    def scraped_item(self, response, item):\n+        \"\"\"\n+        This method is called for each item returned by the spider, and it's intended\n+        to do anything that it's needed before returning the item to the core, specially\n+        setting its GUID.\n+        It receives and returns an item\n+        \"\"\"\n+        return item\n+\n+    def _requests_to_follow(self, response):\n+        \"\"\"\n+        This method iterates over each of the spider's rules, extracts the links\n+        matching each case, filters them (if needed), and returns a list of unique\n+        requests per response.\n+        \"\"\"\n+        requests = []\n+        seen = set()\n+        for rule in self.rules:\n+            links = [link for link in rule.link_extractor.extract_urls(response) if link not in seen]\n+            if rule.link_filter:\n+                links = rule.link_filter(links)\n+            seen = seen.union(links)\n+            for link in links:\n+                r = Request(url=link.url, link_text=link.text)\n+                r.append_callback(self._parse_wrapper, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)\n+                requests.append(r)\n+        return requests\n+\n+    def _parse_wrapper(self, response, callback, cb_kwargs, follow):\n+        \"\"\"\n+        This is were any response (except the ones from the start urls) arrives, and\n+        were it's decided whether to extract links or not from it, and if it will\n+        be parsed or not.\n+        It returns a list of requests/items.\n+        \"\"\"\n+        res = []\n+        if follow:\n+            res.extend(self._requests_to_follow(response))\n+        if callback:\n+            cb_res = callback(response, **cb_kwargs) or ()\n+            for entry in cb_res:\n+                if isinstance(entry, ScrapedItem):\n+                    entry = self.scraped_item(response, entry)\n+            res.extend(cb_res)\n+        return res\n+\n+    def parse_url(self, response):\n+        \"\"\"\n+        This method is called whenever you run scrapy with the 'parse' command\n+        over an URL.\n+        \"\"\"\n+        ret = set()\n+        for rule in self.rules:\n+            links = [link for link in rule.link_extractor.extract_urls(response) if link not in ret]\n+            if rule.link_filter:\n+                links = rule.link_filter(links)\n+            ret = ret.union(links)\n+            \n+            if rule.callback and rule.link_extractor.match(response.url):\n+                ret = ret.union(rule.callback(response))\n+        return list(ret)\n+\n\n@@ -0,0 +1,79 @@\n+from scrapy.spider import BaseSpider\n+from scrapy.item import ScrapedItem\n+from scrapy.utils.iterators import xmliter, csviter\n+from scrapy.xpath.selector import XmlXPathSelector\n+from scrapy.core.exceptions import NotConfigured\n+\n+class XMLFeedSpider(BaseSpider):\n+    \"\"\"\n+    This class intends to be the base class for spiders that scrape\n+    from XML feeds.\n+\n+    You can choose whether to parse the file using the iternodes tool,\n+    or not using it (which just splits the tags using xpath)\n+    \"\"\"\n+    iternodes = True\n+    itertag = 'item'\n+\n+    def item_scraped(self, response, item):\n+        \"\"\"\n+        This method is called for each item returned by the spider, and it's intended\n+        to do anything that it's needed before returning the item to the core, specially\n+        setting its GUID.\n+        It receives and returns an item\n+        \"\"\"\n+        return item\n+\n+    def parse_item_wrapper(self, response, xSel):\n+        ret = self.parse_item(response, xSel)\n+        if isinstance(ret, ScrapedItem):\n+            self.scraped_item(response, ret)\n+        return ret\n+\n+    def parse(self, response):\n+        if not hasattr(self, 'parse_item'):\n+            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')\n+\n+        if self.iternodes:\n+            nodes = xmliter(response, self.itertag)\n+        else:\n+            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)\n+\n+        return (self.parse_item_wrapper(response, xSel) for xSel in nodes)\n+\n+class CSVFeedSpider(BaseSpider):\n+    \"\"\"\n+    Spider for parsing CSV feeds.\n+    It receives a CSV file in a response; iterates through each of its rows,\n+    and calls parse_row with a dict containing each field's data.\n+\n+    You can set some options regarding the CSV file, such as the delimiter\n+    and the file's headers.\n+    \"\"\"\n+    delimiter = None # When this is None, python's csv module's default delimiter is used\n+    headers = None\n+\n+    def scraped_item(self, response, item):\n+        \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n+        return item\n+\n+    def adapt_feed(self, response):\n+        \"\"\"You can override this function in order to make any changes you want\n+        to into the feed before parsing it. This function may return either a\n+        response or a string.  \"\"\"\n+\n+        return response\n+\n+    def parse_row_wrapper(self, response, row):\n+        ret = self.parse_row(response, row)\n+        if isinstance(ret, ScrapedItem):\n+            self.scraped_item(response, ret)\n+        return ret\n+\n+    def parse(self, response):\n+        if not hasattr(self, 'parse_row'):\n+            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n+\n+        feed = self.adapt_feed(response)\n+        return (self.parse_row_wrapper(feed, row) for row in csviter(response, self.delimiter, self.headers))\n+\n\n@@ -1,5 +1,5 @@\n-from scrapy.item.adaptors import AdaptorPipe\n from scrapy.conf import settings\n+from scrapy.item.adaptors import AdaptorPipe\n \n class ScrapedItem(object):\n     \"\"\"\n@@ -7,6 +7,7 @@ class ScrapedItem(object):\n     'guid' attribute is required, and is an attribute\n     that identifies uniquely the given scraped item.\n     \"\"\"\n+    _adaptors_dict = {}\n \n     def set_adaptors(self, adaptors_dict):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26b4be78e30c65f2cb78fdc0e088cc26b90fbfe6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 147 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -57,8 +57,6 @@ class CrawlSpider(BaseSpider):\n                 return method\n \n         super(CrawlSpider, self).__init__()\n-        if not hasattr(self, 'rules'):\n-            return\n         for rule in self.rules:\n             rule.callback = _get_method(rule.callback)\n             rule.link_filter = _get_method(rule.link_filter)\n@@ -106,9 +104,8 @@ class CrawlSpider(BaseSpider):\n \n     def _parse_wrapper(self, response, callback, cb_kwargs, follow):\n         \"\"\"\n-        This is were any response (except the ones from the start urls) arrives, and\n-        were it's decided whether to extract links or not from it, and if it will\n-        be parsed or not.\n+        This is were any response arrives, and were it's decided whether\n+        to extract links or not from it, and if it will be parsed or not.\n         It returns a list of requests/items.\n         \"\"\"\n         res = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40b3f06f0bc65485a5f8dcc9c707627d08c9ae3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 520 | Contributors (this commit): 2 | Commits (past 90d): 24 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,14 +44,15 @@ class ScrapedItem(object):\n             if val or val is False:\n                 curr_val = getattr(self, attrname, None)\n                 if not curr_val:\n-                    setattr(self, attrname, val)\n+                    newval = val\n                 else:\n                     if override:\n-                        setattr(self, attrname, val)\n-                    elif add and all(hasattr(var, '__iter__') for var in (curr_val, val)):\n-                        newval = []\n-                        newval.extend(curr_val)\n-                        newval.extend(val)\n+                        newval = val\n+                    elif add:\n+                        if all(isinstance(var, basestring) for var in (curr_val, val)):\n+                            newval = '%s %s' % (curr_val, val)\n+                        elif all(hasattr(var, '__iter__') for var in (curr_val, val)):\n+                            newval = curr_val + val\n                 setattr(self, attrname, newval)\n         elif value:\n             setattr(self, attrname, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8eab5802cdc33e1e7b26c4f891dcb6b6baa98df5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 529 | Contributors (this commit): 2 | Commits (past 90d): 25 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,16 +44,15 @@ class ScrapedItem(object):\n             if val or val is False:\n                 curr_val = getattr(self, attrname, None)\n                 if not curr_val:\n-                    newval = val\n+                    setattr(self, attrname, val)\n                 else:\n                     if override:\n-                        newval = val\n+                        setattr(self, attrname, val)\n                     elif add:\n                         if all(isinstance(var, basestring) for var in (curr_val, val)):\n-                            newval = '%s %s' % (curr_val, val)\n+                            setattr(self, attrname, '%s\\t%s' % (curr_val, val))\n                         elif all(hasattr(var, '__iter__') for var in (curr_val, val)):\n-                            newval = curr_val + val\n-                setattr(self, attrname, newval)\n+                            setattr(self, attrname, curr_val + val)\n         elif value:\n             setattr(self, attrname, value)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0958eecfcaf4b73048ba68c8805d3de3476a25dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 263 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -138,6 +138,7 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n \n+    url = str(url) # URLs *must* be a string; otherwise urllib may fail.\n     parts = list(urlparse.urlparse(url))\n     keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n     keyvals.sort()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce172e8520defec19d1cca02abb723bbb67e3f0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 264 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -138,7 +138,6 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n \n-    url = str(url) # URLs *must* be a string; otherwise urllib may fail.\n     parts = list(urlparse.urlparse(url))\n     keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n     keyvals.sort()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#874331a5b753fa37f86fcd35de53c2e24aca7509", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 366 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -95,7 +95,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             age_seconds = time.time() - modified_stamp\n             age_days = age_seconds / 60 / 60 / 24\n \n-            if age_days < self.image_refresh_days:\n+            if age_days > self.image_refresh_days:\n                 return # returning None force download\n \n             etag = response.headers['Etag'][0].strip('\"')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d08ab5f98fda30bd7faf68e4f48ad67baee679e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 459 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -158,8 +158,9 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertFalse(check_valid_urlencode('http://www.example.com/?q=foo bar&q2=foo2 bar2'))\n         self.assertTrue(check_valid_urlencode('http://www.example.com/?q=foo+bar&q2=foo2%20bar2'))\n \n-        self.assertFalse(check_valid_urlencode('http://www.example.com/product,little:london$set%'))\n-        self.assertTrue(check_valid_urlencode('http://www.example.com/product%2Clittle%3Alondon%24set%25'))\n+        self.assertFalse(check_valid_urlencode('http://www.example.com/.,:;!@$%^*()_-[]{}|'))\n+        self.assertTrue(check_valid_urlencode('http://www.example.com/.,:;!@%24%25%5E*()_-%5B%5D%7B%7D%7C'))\n+        self.assertTrue(check_valid_urlencode('http://www.example.com/.%2C%3A%3B%21%40%24%25%5E%2A%28%29_-%5B%5D%7B%7D%7C'))\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -150,9 +150,11 @@ def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n def check_valid_urlencode(url):\n     \"\"\" check that the url-path and arguments are properly quoted \"\"\"\n     def check_str(s):\n-        return all((s_plus == urllib.quote(urllib.unquote(s_plus)) for s_plus in s.split('+')))\n+        for ignore_char in ',:;!@*()':\n+            s = s.replace(ignore_char, '')\n+        return all(s_plus == urllib.quote(urllib.unquote(s_plus)) for s_plus in s.split('+'))\n     def check_param(p):\n-        return all((check_str(s) for s in p.split('=', 1)))\n+        return all(check_str(s) for s in p.split('=', 1))\n \n     split_result = urlparse.urlsplit(url)\n-    return check_str(split_result[2]) and all((check_param(p) for p in split_result[3].split('&')))\n+    return check_str(split_result.path) and all(check_param(p) for p in split_result.query.split('&'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
