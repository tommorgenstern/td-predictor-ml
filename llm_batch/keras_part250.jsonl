{"custom_id": "keras#e6d21795d363165b36962e71ea7ba7948185e26f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 64 | Churn Cumulative: 1421 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -22,38 +22,44 @@ class Layer(object):\n     \"\"\"Abstract base layer class.\n \n     # Properties\n-        name: String, must be unique within a model.\n+        input, output: Input/output tensor(s). Note that if the layer\n+            is used more than once (shared layer), this is ill-defined\n+            and will raise an exception. In such cases, use\n+            `layer.get_input_at(node_index)`.\n+        input_mask, output_mask: Mask tensors. Same caveats apply as\n+            input, output.\n+        input_shape: Shape tuple. Provided for convenience, but note\n+            that there may be cases in which this attribute is\n+            ill-defined (e.g. a shared layer with multiple input\n+            shapes), in which case requesting `input_shape` will raise\n+            an Exception. Prefer using\n+            `layer.get_input_shape_for(input_shape)`, or\n+            `layer.get_input_shape_at(node_index)`.\n         input_spec: List of InputSpec class instances\n             each entry describes one required input:\n                 - ndim\n                 - dtype\n             A layer with `n` input tensors must have\n             an `input_spec` of length `n`.\n+        name: String, must be unique within a model.\n+        non_trainable_weights: List of variables.\n+        output_shape: Shape tuple. See `input_shape`.\n+        stateful: Boolean indicating whether the layer carries\n+            additional non-weight state. Used in, for instance, RNN\n+            cells to carry information between batches.\n+        supports_masking: Boolean indicator of whether the layer\n+            supports masking, typically for unused timesteps in a\n+            sequence.\n         trainable: Boolean, whether the layer weights\n             will be updated during training.\n+        trainable_weights: List of variables.\n         uses_learning_phase: Whether any operation\n             of the layer uses `K.in_training_phase()`\n             or `K.in_test_phase()`.\n-        input_shape: Shape tuple. Provided for convenience,\n-            but note that there may be cases in which this\n-            attribute is ill-defined (e.g. a shared layer\n-            with multiple input shapes), in which case\n-            requesting `input_shape` will raise an Exception.\n-            Prefer using `layer.get_input_shape_for(input_shape)`,\n-            or `layer.get_input_shape_at(node_index)`.\n-        output_shape: Shape tuple. See above.\n-        inbound_nodes: List of nodes.\n-        outbound_nodes: List of nodes.\n-        input, output: Input/output tensor(s). Note that if the layer is used\n-            more than once (shared layer), this is ill-defined\n-            and will raise an exception. In such cases, use\n-            `layer.get_input_at(node_index)`.\n-        input_mask, output_mask: Same as above, for masks.\n-        trainable_weights: List of variables.\n-        non_trainable_weights: List of variables.\n         weights: The concatenation of the lists trainable_weights and\n             non_trainable_weights (in this order).\n \n+\n     # Methods\n         call(x, mask=None): Where the layer's logic lives.\n         __call__(x, mask=None): Wrapper around the layer logic (`call`).\n@@ -63,26 +69,26 @@ class Layer(object):\n                 - Add layer to tensor history\n             If layer is not built:\n                 - Build from x._keras_shape\n+        compute_mask(x, mask)\n+        compute_output_shape(input_shape)\n+        count_params()\n+        get_config()\n+        get_input_at(node_index)\n+        get_input_mask_at(node_index)\n+        get_input_shape_at(node_index)\n+        get_output_at(node_index)\n+        get_output_mask_at(node_index)\n+        get_output_shape_at(node_index)\n         get_weights()\n         set_weights(weights)\n-        get_config()\n-        count_params()\n-        compute_output_shape(input_shape)\n-        compute_mask(x, mask)\n-        get_input_at(node_index)\n-        get_output_at(node_index)\n-        get_input_shape_at(node_index)\n-        get_output_shape_at(node_index)\n-        get_input_mask_at(node_index)\n-        get_output_mask_at(node_index)\n \n     # Class Methods\n         from_config(config)\n \n     # Internal methods:\n-        build(input_shape)\n         _add_inbound_node(layer, index=0)\n         assert_input_compatibility()\n+        build(input_shape)\n     \"\"\"\n \n     def __init__(self, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#84aa7b5f4167afaa8e4b24f2449ac76c11d5e8df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 36 | Churn Cumulative: 20004 | Contributors (this commit): 154 | Commits (past 90d): 52 | Contributors (cumulative): 239 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1063,7 +1063,7 @@ def _moments(x, axes=None, shift=None, keep_dims=False):\n     return mean, variance\n \n \n-def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n+def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n     # The mean / var / beta / gamma may be processed by broadcast\n     # so it may have an extra batch axis with 1, it is not needed\n     # in cntk, need to remove those dummy axis.\n\n@@ -1850,7 +1850,7 @@ def normalize_batch_in_training(x, gamma, beta,\n                                                           epsilon=epsilon)\n \n \n-def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n+def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n     \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n \n     I.e. returns:\n@@ -1862,11 +1862,39 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n         var: Variance of batch.\n         beta: Tensor with which to center the input.\n         gamma: Tensor by which to scale the input.\n+        axis: Integer, the axis that should be normalized.\n+            (typically the features axis).\n         epsilon: Fuzz factor.\n \n     # Returns\n         A tensor.\n     \"\"\"\n+    if ndim(x) == 4:\n+        # The CPU implementation of FusedBatchNorm only support NHWC\n+        if axis == 1:\n+            tf_data_format = 'NCHW'\n+        elif axis == 3:\n+            tf_data_format = 'NHWC'\n+        else:\n+            tf_data_format = None\n+\n+        if tf_data_format == 'NHWC' or tf_data_format == 'NCHW' and _has_nchw_support():\n+            if beta is None:\n+                beta = zeros_like(mean)\n+            if gamma is None:\n+                gamma = ones_like(mean)\n+            y, _, _ = tf.nn.fused_batch_norm(\n+                x,\n+                gamma,\n+                beta,\n+                epsilon=epsilon,\n+                mean=mean,\n+                variance=var,\n+                data_format=tf_data_format,\n+                is_training=False\n+            )\n+            return y\n+    # default\n     return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n \n \n\n@@ -731,7 +731,7 @@ def normalize_batch_in_training(x, gamma, beta,\n     return normed, mean, T.inv(stdinv ** 2)\n \n \n-def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n+def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n     \"\"\"Apply batch normalization on x given mean, var, beta and gamma.\n     \"\"\"\n     # TODO remove this if statement when Theano without\n\n@@ -161,6 +161,7 @@ class BatchNormalization(Layer):\n                     broadcast_moving_variance,\n                     broadcast_beta,\n                     broadcast_gamma,\n+                    axis=self.axis,\n                     epsilon=self.epsilon)\n             else:\n                 return K.batch_normalization(\n@@ -169,6 +170,7 @@ class BatchNormalization(Layer):\n                     self.moving_variance,\n                     self.beta,\n                     self.gamma,\n+                    axis=self.axis,\n                     epsilon=self.epsilon)\n \n         # If the learning phase is *static* and set to inference:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#794f814343b655e01de73578d71e8b8a53523687", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4121 | Contributors (this commit): 56 | Commits (past 90d): 14 | Contributors (cumulative): 56 | DMM Complexity: None\n\nDIFF:\n@@ -486,7 +486,7 @@ def load_img(path, grayscale=False, target_size=None,\n def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n     return [os.path.join(root, f)\n             for root, _, files in os.walk(directory) for f in files\n-            if re.match(r'([\\w]+\\.(?:' + ext + '))', f)]\n+            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]\n \n \n class ImageDataGenerator(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9bff5b1a55f51ad974c3a3489d7ca2d80f26f770", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2975 | Contributors (this commit): 72 | Commits (past 90d): 10 | Contributors (cumulative): 72 | DMM Complexity: None\n\nDIFF:\n@@ -938,7 +938,7 @@ class ReduceLROnPlateau(Callback):\n         if 'epsilon' in kwargs:\n             min_delta = kwargs.pop('epsilon')\n             warnings.warn('`epsilon` argument is deprecated and '\n-                          'will be removed, use `min_delta` insted.')\n+                          'will be removed, use `min_delta` instead.')\n         self.factor = factor\n         self.min_lr = min_lr\n         self.min_delta = min_delta\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#315a80ae3cb65da2aa6308abd483f2f8643f1c8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 74 | Files Changed: 7 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 118 | Churn Cumulative: 4029 | Contributors (this commit): 17 | Commits (past 90d): 28 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -119,16 +119,11 @@ def DenseNet(blocks,\n              classes=1000):\n     \"\"\"Instantiates the DenseNet architecture.\n \n-    Optionally loads weights pre-trained\n-    on ImageNet. Note that when using TensorFlow,\n-    for best performance you should set\n-    `image_data_format='channels_last'` in your Keras config\n-    at ~/.keras/keras.json.\n-\n-    The model and the weights are compatible with\n-    TensorFlow, Theano, and CNTK. The data format\n-    convention used by the model is the one\n-    specified in your Keras config file.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         blocks: numbers of building blocks for the four dense layers.\n\n@@ -183,18 +183,10 @@ def InceptionResNetV2(include_top=True,\n     \"\"\"Instantiates the Inception-ResNet v2 architecture.\n \n     Optionally loads weights pre-trained on ImageNet.\n-    Note that when using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in your Keras config\n-    at `~/.keras/keras.json`.\n-\n-    The model and the weights are compatible with TensorFlow, Theano and\n-    CNTK backends. The data format convention used by the model is\n-    the one specified in your Keras config file.\n-\n-    Note that the default input image size for this model is 299x299, instead\n-    of 224x224 as in the VGG16 and ResNet models. Also, the input preprocessing\n-    function is different (i.e., do not use `imagenet_utils.preprocess_input()`\n-    with this model. Use `preprocess_input()` defined in this module instead).\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         include_top: whether to include the fully-connected\n\n@@ -92,16 +92,11 @@ def InceptionV3(include_top=True,\n                 classes=1000):\n     \"\"\"Instantiates the Inception v3 architecture.\n \n-    Optionally loads weights pre-trained\n-    on ImageNet. Note that when using TensorFlow,\n-    for best performance you should set\n-    `image_data_format='channels_last'` in your Keras config\n-    at ~/.keras/keras.json.\n-    The model and the weights are compatible with both\n-    TensorFlow and Theano. The data format\n-    convention used by the model is the one\n-    specified in your Keras config file.\n-    Note that the default input image size for this model is 299x299.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         include_top: whether to include the fully-connected\n\n@@ -80,10 +80,11 @@ def NASNet(input_shape=None,\n            default_size=None):\n     '''Instantiates a NASNet model.\n \n-    Note that only TensorFlow is supported for now,\n-    therefore it only works with the data format\n-    `image_data_format='channels_last'` in your Keras config\n-    at `~/.keras/keras.json`.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         input_shape: Optional shape tuple, the input shape\n@@ -315,10 +316,11 @@ def NASNetLarge(input_shape=None,\n                 classes=1000):\n     '''Instantiates a NASNet model in ImageNet mode.\n \n-    Note that only TensorFlow is supported for now,\n-    therefore it only works with the data format\n-    `image_data_format='channels_last'` in your Keras config\n-    at `~/.keras/keras.json`.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         input_shape: Optional shape tuple, only to be specified\n@@ -381,10 +383,11 @@ def NASNetMobile(input_shape=None,\n                  classes=1000):\n     '''Instantiates a Mobile NASNet model in ImageNet mode.\n \n-    Note that only TensorFlow is supported for now,\n-    therefore it only works with the data format\n-    `image_data_format='channels_last'` in your Keras config\n-    at `~/.keras/keras.json`.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         input_shape: Optional shape tuple, only to be specified\n\n@@ -132,16 +132,11 @@ def ResNet50(include_top=True, weights='imagenet',\n              classes=1000):\n     \"\"\"Instantiates the ResNet50 architecture.\n \n-    Optionally loads weights pre-trained\n-    on ImageNet. Note that when using TensorFlow,\n-    for best performance you should set\n-    `image_data_format='channels_last'` in your Keras config\n-    at ~/.keras/keras.json.\n-\n-    The model and the weights are compatible with both\n-    TensorFlow and Theano. The data format\n-    convention used by the model is the one\n-    specified in your Keras config file.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         include_top: whether to include the fully-connected\n\n@@ -40,16 +40,11 @@ def VGG16(include_top=True, weights='imagenet',\n           classes=1000):\n     \"\"\"Instantiates the VGG16 architecture.\n \n-    Optionally loads weights pre-trained\n-    on ImageNet. Note that when using TensorFlow,\n-    for best performance you should set\n-    `image_data_format='channels_last'` in your Keras config\n-    at ~/.keras/keras.json.\n-\n-    The model and the weights are compatible with both\n-    TensorFlow and Theano. The data format\n-    convention used by the model is the one\n-    specified in your Keras config file.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         include_top: whether to include the 3 fully-connected\n\n@@ -40,16 +40,11 @@ def VGG19(include_top=True, weights='imagenet',\n           classes=1000):\n     \"\"\"Instantiates the VGG19 architecture.\n \n-    Optionally loads weights pre-trained\n-    on ImageNet. Note that when using TensorFlow,\n-    for best performance you should set\n-    `image_data_format='channels_last'` in your Keras config\n-    at ~/.keras/keras.json.\n-\n-    The model and the weights are compatible with both\n-    TensorFlow and Theano. The data format\n-    convention used by the model is the one\n-    specified in your Keras config file.\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+    When using TensorFlow, for best performance you should\n+    set `\"image_data_format\": \"channels_last\"` in the config.\n \n     # Arguments\n         include_top: whether to include the 3 fully-connected\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe066966b5afa96f2f6b9f71ec0c71158b44068d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 20 | Churn Cumulative: 2146 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,6 +86,7 @@ from ..layers import BatchNormalization\n from ..layers import Conv2D\n from ..layers import DepthwiseConv2D\n from ..layers import GlobalAveragePooling2D\n+from ..layers import GlobalMaxPooling2D\n from ..layers import Add\n from ..layers import Flatten\n from ..layers import Dense\n@@ -149,6 +150,7 @@ def MobileNetV2(input_shape=None,\n                 include_top=True,\n                 weights='imagenet',\n                 input_tensor=None,\n+                pooling=None,\n                 classes=1000):\n     \"\"\"Instantiates the MobileNetV2 architecture.\n \n@@ -187,6 +189,18 @@ def MobileNetV2(input_shape=None,\n         input_tensor: optional Keras tensor (i.e. output of\n             `layers.Input()`)\n             to use as image input for the model.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model\n+                will be the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a\n+                2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n@@ -408,6 +422,11 @@ def MobileNetV2(input_shape=None,\n         x = GlobalAveragePooling2D()(x)\n         x = Dense(classes, activation='softmax',\n                   use_bias=True, name='Logits')(x)\n+    else:\n+        if pooling == 'avg':\n+            x = GlobalAveragePooling2D()(x)\n+        elif pooling == 'max':\n+            x = GlobalMaxPooling2D()(x)\n \n     # Ensure that the model takes into account\n     # any potential predecessors of `input_tensor`.\n\n@@ -142,7 +142,6 @@ def test_mobilenet():\n     _test_application_basic(app)\n     _test_application_notop(app, last_dim)\n     _test_application_variable_input_channels(app, last_dim)\n-    if app == applications.MobileNet:\n     _test_app_pooling(app, last_dim)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3664246de4bb8ef3232dd84ff47fc2694c69e58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 188 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 34/17 | Churn Δ: 197 | Churn Cumulative: 21097 | Contributors (this commit): 152 | Commits (past 90d): 33 | Contributors (cumulative): 208 | DMM Complexity: 0.4051724137931034\n\nDIFF:\n@@ -585,7 +585,26 @@ def var(x, axis=None, keepdims=False):\n def any(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical OR).\n     \"\"\"\n-    return T.any(x, axis=axis, keepdims=keepdims)\n+    y = T.any(x, axis=axis, keepdims=keepdims)\n+    if hasattr(x, '_keras_shape'):\n+        if axis is None:\n+            y._keras_shape = (1,) * len(x._keras_shape) if keepdims else (1,)\n+        else:\n+            if isinstance(axis, int):\n+                axis_list = [axis]\n+            else:\n+                axis_list = list(set(int(a) for a in axis))\n+            keras_shape_list = list(x._keras_shape)\n+            if keepdims:\n+                for a in axis_list:\n+                    keras_shape_list[a] = 1\n+            else:\n+                for a in axis_list[::-1]:\n+                    keras_shape_list.pop(a)\n+                if not keras_shape_list:\n+                    keras_shape_list = (1,)\n+            y._keras_shape = tuple(keras_shape_list)\n+    return y\n \n \n def all(x, axis=None, keepdims=False):\n@@ -671,7 +690,12 @@ def equal(x, y):\n \n \n def not_equal(x, y):\n-    return T.neq(x, y)\n+    z = T.neq(x, y)\n+    if hasattr(x, '_keras_shape'):\n+        z._keras_shape = x._keras_shape\n+    elif hasattr(y, '_keras_shape'):\n+        z._keras_shape = y._keras_shape\n+    return z\n \n \n def greater(x, y):\n@@ -868,8 +892,7 @@ def concatenate(tensors, axis=-1):\n \n def reshape(x, shape):\n     y = T.reshape(x, shape)\n-    if _is_explicit_shape(shape):\n-        shape = tuple(x if x != -1 else None for x in shape)\n+    shape = tuple(x if isinstance(x, int) and x > 0 else None for x in shape)\n     y._keras_shape = shape\n     if hasattr(x, '_uses_learning_phase'):\n         y._uses_learning_phase = x._uses_learning_phase\n\n@@ -60,7 +60,8 @@ class Masking(Layer):\n         self.mask_value = mask_value\n \n     def compute_mask(self, inputs, mask=None):\n-        return K.any(K.not_equal(inputs, self.mask_value), axis=-1)\n+        output_mask = K.any(K.not_equal(inputs, self.mask_value), axis=-1)\n+        return output_mask\n \n     def call(self, inputs):\n         boolean_mask = K.any(K.not_equal(inputs, self.mask_value),\n\n@@ -93,6 +93,7 @@ class Embedding(Layer):\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n         self.embeddings_constraint = constraints.get(embeddings_constraint)\n         self.mask_zero = mask_zero\n+        self.supports_masking = mask_zero\n         self.input_length = input_length\n \n     def build(self, input_shape):\n@@ -108,8 +109,8 @@ class Embedding(Layer):\n     def compute_mask(self, inputs, mask=None):\n         if not self.mask_zero:\n             return None\n-        else:\n-            return K.not_equal(inputs, 0)\n+        output_mask = K.not_equal(inputs, 0)\n+        return output_mask\n \n     def compute_output_shape(self, input_shape):\n         if self.input_length is None:\n\n@@ -160,6 +160,37 @@ class TimeDistributed(Wrapper):\n         super(TimeDistributed, self).__init__(layer, **kwargs)\n         self.supports_masking = True\n \n+    def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n+        \"\"\"Finds non-specific dimensions in the static shapes\n+        and replaces them by the corresponding dynamic shapes of the tensor.\n+\n+        # Arguments\n+            init_tuple: a tuple, the first part of the output shape\n+            tensor: the tensor from which to get the (static and dynamic) shapes\n+                as the last part of the output shape\n+            start_idx: int, which indicate the first dimension to take from\n+                the static shape of the tensor\n+            int_shape: an alternative static shape to take as the last part\n+                of the output shape\n+\n+        # Returns\n+            The new int_shape with the first part from init_tuple\n+            and the last part from either `int_shape` (if provided)\n+            or K.int_shape(tensor), where every `None` is replaced by\n+            the corresponding dimension from K.shape(tensor)\n+        \"\"\"\n+        # replace all None in int_shape by K.shape\n+        if int_shape is None:\n+            int_shape = K.int_shape(tensor)[start_idx:]\n+        if not any(not s for s in int_shape):\n+            return init_tuple + int_shape\n+        tensor_shape = K.shape(tensor)\n+        int_shape = list(int_shape)\n+        for i, s in enumerate(int_shape):\n+            if not s:\n+                int_shape[i] = tensor_shape[start_idx + i]\n+        return init_tuple + tuple(int_shape)\n+\n     def build(self, input_shape):\n         assert len(input_shape) >= 3\n         self.input_spec = InputSpec(shape=input_shape)\n@@ -204,18 +235,24 @@ class TimeDistributed(Wrapper):\n             input_length = input_shape[1]\n             if not input_length:\n                 input_length = K.shape(inputs)[1]\n+            inner_input_shape = self._get_shape_tuple((-1,), inputs, 2)\n             # Shape: (num_samples * timesteps, ...). And track the\n             # transformation in self._input_map.\n             input_uid = object_list_uid(inputs)\n-            inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n+            inputs = K.reshape(inputs, inner_input_shape)\n             self._input_map[input_uid] = inputs\n             # (num_samples * timesteps, ...)\n+            if has_arg(self.layer.call, 'mask') and mask is not None:\n+                inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n+                kwargs['mask'] = K.reshape(mask, inner_mask_shape)\n             y = self.layer.call(inputs, **kwargs)\n             if hasattr(y, '_uses_learning_phase'):\n                 uses_learning_phase = y._uses_learning_phase\n             # Shape: (num_samples, timesteps, ...)\n             output_shape = self.compute_output_shape(input_shape)\n-            y = K.reshape(y, (-1, input_length) + output_shape[2:])\n+            output_shape = self._get_shape_tuple(\n+                (-1, input_length), y, 1, output_shape[2:])\n+            y = K.reshape(y, output_shape)\n \n         # Apply activity regularizer if any:\n         if (hasattr(self.layer, 'activity_regularizer') and\n@@ -227,6 +264,70 @@ class TimeDistributed(Wrapper):\n             y._uses_learning_phase = True\n         return y\n \n+    def compute_mask(self, inputs, mask=None):\n+        \"\"\"Computes an output mask tensor for Embedding layer\n+        based on the inputs, mask, and the inner layer.\n+\n+        If batch size is specified:\n+        Simply return the input `mask`. (An rnn-based implementation with\n+        more than one rnn inputs is required but not supported in Keras yet.)\n+\n+        Otherwise we call `compute_mask` of the inner layer at each time step.\n+        If the output mask at each time step is not `None`:\n+        (E.g., inner layer is Masking or RNN)\n+        Concatenate all of them and return the concatenation.\n+        If the output mask at each time step is `None` and the input mask is not `None`:\n+        (E.g., inner layer is Dense)\n+        Reduce the input_mask to 2 dimensions and return it.\n+        Otherwise (both the output mask and the input mask are `None`):\n+        (E.g., `mask` is not used at all)\n+        Return `None`.\n+\n+        # Arguments\n+            inputs: Tensor\n+            mask: Tensor\n+        # Returns\n+            None or a tensor\n+        \"\"\"\n+        # cases need to call the layer.compute_mask when input_mask is None:\n+        # Masking layer and Embedding layer with mask_zero\n+        input_shape = K.int_shape(inputs)\n+        if input_shape[0]:\n+            # batch size matters, we currently do not handle mask explicitly\n+            return mask\n+        inner_mask = mask\n+        if inner_mask is not None:\n+            inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n+            inner_mask = K.reshape(inner_mask, inner_mask_shape)\n+        input_uid = object_list_uid(inputs)\n+        inner_inputs = self._input_map[input_uid]\n+        output_mask = self.layer.compute_mask(inner_inputs, inner_mask)\n+        if output_mask is None:\n+            if mask is None:\n+                return None\n+            # input_mask is not None, and output_mask is None:\n+            # we should return a not-None mask\n+            output_mask = mask\n+            for _ in range(2, len(K.int_shape(mask))):\n+                output_mask = K.any(output_mask, axis=-1)\n+        else:\n+            # output_mask is not None. We need to reshape it\n+            input_length = input_shape[1]\n+            if not input_length:\n+                input_length = K.shape(inputs)[1]\n+            output_mask_int_shape = K.int_shape(output_mask)\n+            if output_mask_int_shape is None:\n+                # if the output_mask does not have a static shape,\n+                # its shape must be the same as mask's\n+                if mask is not None:\n+                    output_mask_int_shape = K.int_shape(mask)\n+                else:\n+                    output_mask_int_shape = K.compute_output_shape(input_shape)[:-1]\n+            output_mask_shape = self._get_shape_tuple(\n+                (-1, input_length), output_mask, 1, output_mask_int_shape[1:])\n+            output_mask = K.reshape(output_mask, output_mask_shape)\n+        return output_mask\n+\n \n class Bidirectional(Wrapper):\n     \"\"\"Bidirectional wrapper for RNNs.\n\n@@ -156,6 +156,59 @@ def test_TimeDistributed_trainable():\n     assert len(layer.trainable_weights) == 2\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Unknown timestamps for RNN not supported in CNTK.')\n+def test_TimeDistributed_with_masked_embedding_and_unspecified_shape():\n+    # test with unspecified shape and Embeddings with mask_zero\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(layers.Embedding(5, 6, mask_zero=True),\n+                                       input_shape=(None, None)))  # N by t_1 by t_2 by 6\n+    model.add(wrappers.TimeDistributed(layers.SimpleRNN(7, return_sequences=True)))\n+    model.add(wrappers.TimeDistributed(layers.SimpleRNN(8, return_sequences=False)))\n+    model.add(layers.SimpleRNN(1, return_sequences=False))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4), dtype='int32')\n+    for i in range(4):\n+        model_input[i, i:, i:] = 0\n+    model.fit(model_input,\n+              np.random.random((10, 1)), epochs=1, batch_size=10)\n+    mask_outputs = [model.layers[0].compute_mask(model.input)]\n+    for layer in model.layers[1:]:\n+        mask_outputs.append(layer.compute_mask(layer.input, mask_outputs[-1]))\n+    func = K.function([model.input], mask_outputs[:-1])\n+    mask_outputs_val = func([model_input])\n+    ref_mask_val_0 = model_input > 0         # embedding layer\n+    ref_mask_val_1 = ref_mask_val_0          # first RNN layer\n+    ref_mask_val_2 = np.any(ref_mask_val_1, axis=-1)     # second RNN layer\n+    ref_mask_val = [ref_mask_val_0, ref_mask_val_1, ref_mask_val_2]\n+    for i in range(3):\n+        assert np.array_equal(mask_outputs_val[i], ref_mask_val[i])\n+    assert mask_outputs[-1] is None  # final layer\n+\n+\n+@keras_test\n+def test_TimeDistributed_with_masking_layer():\n+    # test with Masking layer\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(layers.Masking(mask_value=0.,),\n+                                       input_shape=(None, 4)))\n+    model.add(wrappers.TimeDistributed(layers.Dense(5)))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4))\n+    for i in range(4):\n+        model_input[i, i:, :] = 0.\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.fit(model_input,\n+              np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n+    mask_outputs = [model.layers[0].compute_mask(model.input)]\n+    mask_outputs += [model.layers[1].compute_mask(model.layers[1].input, mask_outputs[-1])]\n+    func = K.function([model.input], mask_outputs)\n+    mask_outputs_val = func([model_input])\n+    assert np.array_equal(mask_outputs_val[0], np.any(model_input, axis=-1))\n+    assert np.array_equal(mask_outputs_val[1], np.any(model_input, axis=-1))\n+\n+\n @keras_test\n def test_regularizers():\n     model = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9cb6087394656247b13dbcb447135dfcb91685c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 6068 | Files Changed: 16 | Hunks: 44 | Methods Changed: 98 | Complexity Δ (Sum/Max): -597/0 | Churn Δ: 6178 | Churn Cumulative: 18239 | Contributors (this commit): 112 | Commits (past 90d): 81 | Contributors (cumulative): 185 | DMM Complexity: 0.7281453548165924\n\nDIFF:\n@@ -1,3 +1,22 @@\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from .. import backend\n+from .. import engine\n+from .. import layers\n+from .. import models\n+from .. import utils\n+\n+import keras_applications\n+\n+keras_applications.set_keras_submodules(\n+    backend=backend,\n+    engine=engine,\n+    layers=layers,\n+    models=models,\n+    utils=utils)\n+\n from .vgg16 import VGG16\n from .vgg19 import VGG19\n from .resnet50 import ResNet50\n\n@@ -1,335 +1,11 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"DenseNet models for Keras.\n-\n-# Reference paper\n-\n-- [Densely Connected Convolutional Networks]\n-  (https://arxiv.org/abs/1608.06993) (CVPR 2017 Best Paper Award)\n-\n-# Reference implementation\n-\n-- [Torch DenseNets]\n-  (https://github.com/liuzhuang13/DenseNet/blob/master/models/densenet.lua)\n-- [TensorNets]\n-  (https://github.com/taehoonlee/tensornets/blob/master/tensornets/densenets.py)\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n+from keras_applications import densenet\n \n-from .. import backend as K\n-from ..models import Model\n-from ..layers import Activation\n-from ..layers import AveragePooling2D\n-from ..layers import BatchNormalization\n-from ..layers import Concatenate\n-from ..layers import Conv2D\n-from ..layers import Dense\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..layers import Input\n-from ..layers import MaxPooling2D\n-from ..layers import ZeroPadding2D\n-from ..utils.data_utils import get_file\n-from ..engine import get_source_inputs\n-from . import imagenet_utils\n-from .imagenet_utils import decode_predictions\n-from .imagenet_utils import _obtain_input_shape\n-\n-\n-DENSENET121_WEIGHT_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels.h5'\n-DENSENET121_WEIGHT_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-DENSENET169_WEIGHT_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet169_weights_tf_dim_ordering_tf_kernels.h5'\n-DENSENET169_WEIGHT_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-DENSENET201_WEIGHT_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels.h5'\n-DENSENET201_WEIGHT_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-\n-\n-def dense_block(x, blocks, name):\n-    \"\"\"A dense block.\n-\n-    # Arguments\n-        x: input tensor.\n-        blocks: integer, the number of building blocks.\n-        name: string, block label.\n-\n-    # Returns\n-        output tensor for the block.\n-    \"\"\"\n-    for i in range(blocks):\n-        x = conv_block(x, 32, name=name + '_block' + str(i + 1))\n-    return x\n-\n-\n-def transition_block(x, reduction, name):\n-    \"\"\"A transition block.\n-\n-    # Arguments\n-        x: input tensor.\n-        reduction: float, compression rate at transition layers.\n-        name: string, block label.\n-\n-    # Returns\n-        output tensor for the block.\n-    \"\"\"\n-    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n-    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n-                           name=name + '_bn')(x)\n-    x = Activation('relu', name=name + '_relu')(x)\n-    x = Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1, use_bias=False,\n-               name=name + '_conv')(x)\n-    x = AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n-    return x\n-\n-\n-def conv_block(x, growth_rate, name):\n-    \"\"\"A building block for a dense block.\n-\n-    # Arguments\n-        x: input tensor.\n-        growth_rate: float, growth rate at dense layers.\n-        name: string, block label.\n-\n-    # Returns\n-        output tensor for the block.\n-    \"\"\"\n-    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n-    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n-                            name=name + '_0_bn')(x)\n-    x1 = Activation('relu', name=name + '_0_relu')(x1)\n-    x1 = Conv2D(4 * growth_rate, 1, use_bias=False,\n-                name=name + '_1_conv')(x1)\n-    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n-                            name=name + '_1_bn')(x1)\n-    x1 = Activation('relu', name=name + '_1_relu')(x1)\n-    x1 = Conv2D(growth_rate, 3, padding='same', use_bias=False,\n-                name=name + '_2_conv')(x1)\n-    x = Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n-    return x\n-\n-\n-def DenseNet(blocks,\n-             include_top=True,\n-             weights='imagenet',\n-             input_tensor=None,\n-             input_shape=None,\n-             pooling=None,\n-             classes=1000):\n-    \"\"\"Instantiates the DenseNet architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        blocks: numbers of building blocks for the four dense layers.\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(224, 224, 3)` (with `channels_last` data format)\n-            or `(3, 224, 224)` (with `channels_first` data format).\n-            It should have exactly 3 inputs channels.\n-        pooling: optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=224,\n-                                      min_size=221,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=include_top,\n-                                      weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n-\n-    x = ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n-    x = Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv')(x)\n-    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n-                           name='conv1/bn')(x)\n-    x = Activation('relu', name='conv1/relu')(x)\n-    x = ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n-    x = MaxPooling2D(3, strides=2, name='pool1')(x)\n-\n-    x = dense_block(x, blocks[0], name='conv2')\n-    x = transition_block(x, 0.5, name='pool2')\n-    x = dense_block(x, blocks[1], name='conv3')\n-    x = transition_block(x, 0.5, name='pool3')\n-    x = dense_block(x, blocks[2], name='conv4')\n-    x = transition_block(x, 0.5, name='pool4')\n-    x = dense_block(x, blocks[3], name='conv5')\n-\n-    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n-                           name='bn')(x)\n-\n-    if include_top:\n-        x = GlobalAveragePooling2D(name='avg_pool')(x)\n-        x = Dense(classes, activation='softmax', name='fc1000')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D(name='avg_pool')(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D(name='max_pool')(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-\n-    # Create model.\n-    if blocks == [6, 12, 24, 16]:\n-        model = Model(inputs, x, name='densenet121')\n-    elif blocks == [6, 12, 32, 32]:\n-        model = Model(inputs, x, name='densenet169')\n-    elif blocks == [6, 12, 48, 32]:\n-        model = Model(inputs, x, name='densenet201')\n-    else:\n-        model = Model(inputs, x, name='densenet')\n-\n-    # Load weights.\n-    if weights == 'imagenet':\n-        if include_top:\n-            if blocks == [6, 12, 24, 16]:\n-                weights_path = get_file(\n-                    'densenet121_weights_tf_dim_ordering_tf_kernels.h5',\n-                    DENSENET121_WEIGHT_PATH,\n-                    cache_subdir='models',\n-                    file_hash='0962ca643bae20f9b6771cb844dca3b0')\n-            elif blocks == [6, 12, 32, 32]:\n-                weights_path = get_file(\n-                    'densenet169_weights_tf_dim_ordering_tf_kernels.h5',\n-                    DENSENET169_WEIGHT_PATH,\n-                    cache_subdir='models',\n-                    file_hash='bcf9965cf5064a5f9eb6d7dc69386f43')\n-            elif blocks == [6, 12, 48, 32]:\n-                weights_path = get_file(\n-                    'densenet201_weights_tf_dim_ordering_tf_kernels.h5',\n-                    DENSENET201_WEIGHT_PATH,\n-                    cache_subdir='models',\n-                    file_hash='7bb75edd58cb43163be7e0005fbe95ef')\n-        else:\n-            if blocks == [6, 12, 24, 16]:\n-                weights_path = get_file(\n-                    'densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                    DENSENET121_WEIGHT_PATH_NO_TOP,\n-                    cache_subdir='models',\n-                    file_hash='4912a53fbd2a69346e7f2c0b5ec8c6d3')\n-            elif blocks == [6, 12, 32, 32]:\n-                weights_path = get_file(\n-                    'densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                    DENSENET169_WEIGHT_PATH_NO_TOP,\n-                    cache_subdir='models',\n-                    file_hash='50662582284e4cf834ce40ab4dfa58c6')\n-            elif blocks == [6, 12, 48, 32]:\n-                weights_path = get_file(\n-                    'densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                    DENSENET201_WEIGHT_PATH_NO_TOP,\n-                    cache_subdir='models',\n-                    file_hash='1c2de60ee40562448dbac34a0737e798')\n-        model.load_weights(weights_path)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    return model\n-\n-\n-def DenseNet121(include_top=True,\n-                weights='imagenet',\n-                input_tensor=None,\n-                input_shape=None,\n-                pooling=None,\n-                classes=1000):\n-    return DenseNet([6, 12, 24, 16],\n-                    include_top, weights,\n-                    input_tensor, input_shape,\n-                    pooling, classes)\n-\n-\n-def DenseNet169(include_top=True,\n-                weights='imagenet',\n-                input_tensor=None,\n-                input_shape=None,\n-                pooling=None,\n-                classes=1000):\n-    return DenseNet([6, 12, 32, 32],\n-                    include_top, weights,\n-                    input_tensor, input_shape,\n-                    pooling, classes)\n-\n-\n-def DenseNet201(include_top=True,\n-                weights='imagenet',\n-                input_tensor=None,\n-                input_shape=None,\n-                pooling=None,\n-                classes=1000):\n-    return DenseNet([6, 12, 48, 32],\n-                    include_top, weights,\n-                    input_tensor, input_shape,\n-                    pooling, classes)\n-\n-\n-def preprocess_input(x, data_format=None):\n-    \"\"\"Preprocesses a numpy array encoding a batch of images.\n-\n-    # Arguments\n-        x: a 3D or 4D numpy array consists of RGB values within [0, 255].\n-        data_format: data format of the image tensor.\n-\n-    # Returns\n-        Preprocessed array.\n-    \"\"\"\n-    return imagenet_utils.preprocess_input(x, data_format, mode='torch')\n-\n-\n-setattr(DenseNet121, '__doc__', DenseNet.__doc__)\n-setattr(DenseNet169, '__doc__', DenseNet.__doc__)\n-setattr(DenseNet201, '__doc__', DenseNet.__doc__)\n+DenseNet121 = densenet.DenseNet121\n+DenseNet169 = densenet.DenseNet169\n+DenseNet201 = densenet.DenseNet201\n+decode_predictions = densenet.decode_predictions\n+preprocess_input = densenet.preprocess_input\n\n@@ -1,363 +1,9 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"Inception-ResNet V2 model for Keras.\n-\n-Model naming and structure follows TF-slim implementation (which has some additional\n-layers and different number of filters from the original arXiv paper):\n-https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py\n-\n-Pre-trained ImageNet weights are also converted from TF-slim, which can be found in:\n-https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\n-\n-# Reference\n-- [Inception-v4, Inception-ResNet and the Impact of\n-   Residual Connections on Learning](https://arxiv.org/abs/1602.07261)\n-\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import inception_resnet_v2\n \n-from ..models import Model\n-from ..layers import Activation\n-from ..layers import AveragePooling2D\n-from ..layers import BatchNormalization\n-from ..layers import Concatenate\n-from ..layers import Conv2D\n-from ..layers import Dense\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..layers import Input\n-from ..layers import Lambda\n-from ..layers import MaxPooling2D\n-from ..utils.data_utils import get_file\n-from ..engine import get_source_inputs\n-from . import imagenet_utils\n-from .imagenet_utils import _obtain_input_shape\n-from .imagenet_utils import decode_predictions\n-from .. import backend as K\n-\n-\n-BASE_WEIGHT_URL = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.7/'\n-\n-\n-def preprocess_input(x):\n-    \"\"\"Preprocesses a numpy array encoding a batch of images.\n-\n-    # Arguments\n-        x: a 4D numpy array consists of RGB values within [0, 255].\n-\n-    # Returns\n-        Preprocessed array.\n-    \"\"\"\n-    return imagenet_utils.preprocess_input(x, mode='tf')\n-\n-\n-def conv2d_bn(x,\n-              filters,\n-              kernel_size,\n-              strides=1,\n-              padding='same',\n-              activation='relu',\n-              use_bias=False,\n-              name=None):\n-    \"\"\"Utility function to apply conv + BN.\n-\n-    # Arguments\n-        x: input tensor.\n-        filters: filters in `Conv2D`.\n-        kernel_size: kernel size as in `Conv2D`.\n-        strides: strides in `Conv2D`.\n-        padding: padding mode in `Conv2D`.\n-        activation: activation in `Conv2D`.\n-        use_bias: whether to use a bias in `Conv2D`.\n-        name: name of the ops; will become `name + '_ac'` for the activation\n-            and `name + '_bn'` for the batch norm layer.\n-\n-    # Returns\n-        Output tensor after applying `Conv2D` and `BatchNormalization`.\n-    \"\"\"\n-    x = Conv2D(filters,\n-               kernel_size,\n-               strides=strides,\n-               padding=padding,\n-               use_bias=use_bias,\n-               name=name)(x)\n-    if not use_bias:\n-        bn_axis = 1 if K.image_data_format() == 'channels_first' else 3\n-        bn_name = None if name is None else name + '_bn'\n-        x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n-    if activation is not None:\n-        ac_name = None if name is None else name + '_ac'\n-        x = Activation(activation, name=ac_name)(x)\n-    return x\n-\n-\n-def inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n-    \"\"\"Adds a Inception-ResNet block.\n-\n-    This function builds 3 types of Inception-ResNet blocks mentioned\n-    in the paper, controlled by the `block_type` argument (which is the\n-    block name used in the official TF-slim implementation):\n-        - Inception-ResNet-A: `block_type='block35'`\n-        - Inception-ResNet-B: `block_type='block17'`\n-        - Inception-ResNet-C: `block_type='block8'`\n-\n-    # Arguments\n-        x: input tensor.\n-        scale: scaling factor to scale the residuals (i.e., the output of\n-            passing `x` through an inception module) before adding them\n-            to the shortcut branch. Let `r` be the output from the residual branch,\n-            the output of this block will be `x + scale * r`.\n-        block_type: `'block35'`, `'block17'` or `'block8'`, determines\n-            the network structure in the residual branch.\n-        block_idx: an `int` used for generating layer names. The Inception-ResNet blocks\n-            are repeated many times in this network. We use `block_idx` to identify\n-            each of the repetitions. For example, the first Inception-ResNet-A block\n-            will have `block_type='block35', block_idx=0`, ane the layer names will have\n-            a common prefix `'block35_0'`.\n-        activation: activation function to use at the end of the block\n-            (see [activations](../activations.md)).\n-            When `activation=None`, no activation is applied\n-            (i.e., \"linear\" activation: `a(x) = x`).\n-\n-    # Returns\n-        Output tensor for the block.\n-\n-    # Raises\n-        ValueError: if `block_type` is not one of `'block35'`,\n-            `'block17'` or `'block8'`.\n-    \"\"\"\n-    if block_type == 'block35':\n-        branch_0 = conv2d_bn(x, 32, 1)\n-        branch_1 = conv2d_bn(x, 32, 1)\n-        branch_1 = conv2d_bn(branch_1, 32, 3)\n-        branch_2 = conv2d_bn(x, 32, 1)\n-        branch_2 = conv2d_bn(branch_2, 48, 3)\n-        branch_2 = conv2d_bn(branch_2, 64, 3)\n-        branches = [branch_0, branch_1, branch_2]\n-    elif block_type == 'block17':\n-        branch_0 = conv2d_bn(x, 192, 1)\n-        branch_1 = conv2d_bn(x, 128, 1)\n-        branch_1 = conv2d_bn(branch_1, 160, [1, 7])\n-        branch_1 = conv2d_bn(branch_1, 192, [7, 1])\n-        branches = [branch_0, branch_1]\n-    elif block_type == 'block8':\n-        branch_0 = conv2d_bn(x, 192, 1)\n-        branch_1 = conv2d_bn(x, 192, 1)\n-        branch_1 = conv2d_bn(branch_1, 224, [1, 3])\n-        branch_1 = conv2d_bn(branch_1, 256, [3, 1])\n-        branches = [branch_0, branch_1]\n-    else:\n-        raise ValueError('Unknown Inception-ResNet block type. '\n-                         'Expects \"block35\", \"block17\" or \"block8\", '\n-                         'but got: ' + str(block_type))\n-\n-    block_name = block_type + '_' + str(block_idx)\n-    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n-    mixed = Concatenate(axis=channel_axis, name=block_name + '_mixed')(branches)\n-    up = conv2d_bn(mixed,\n-                   K.int_shape(x)[channel_axis],\n-                   1,\n-                   activation=None,\n-                   use_bias=True,\n-                   name=block_name + '_conv')\n-\n-    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n-               output_shape=K.int_shape(x)[1:],\n-               arguments={'scale': scale},\n-               name=block_name)([x, up])\n-    if activation is not None:\n-        x = Activation(activation, name=block_name + '_ac')(x)\n-    return x\n-\n-\n-def InceptionResNetV2(include_top=True,\n-                      weights='imagenet',\n-                      input_tensor=None,\n-                      input_shape=None,\n-                      pooling=None,\n-                      classes=1000):\n-    \"\"\"Instantiates the Inception-ResNet v2 architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is `False` (otherwise the input shape\n-            has to be `(299, 299, 3)` (with `'channels_last'` data format)\n-            or `(3, 299, 299)` (with `'channels_first'` data format).\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 139.\n-            E.g. `(150, 150, 3)` would be one valid value.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the last convolutional layer.\n-            - `'avg'` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `'max'` means that global max pooling will be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is `True`, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras `Model` instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(\n-        input_shape,\n-        default_size=299,\n-        min_size=139,\n-        data_format=K.image_data_format(),\n-        require_flatten=False,\n-        weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    # Stem block: 35 x 35 x 192\n-    x = conv2d_bn(img_input, 32, 3, strides=2, padding='valid')\n-    x = conv2d_bn(x, 32, 3, padding='valid')\n-    x = conv2d_bn(x, 64, 3)\n-    x = MaxPooling2D(3, strides=2)(x)\n-    x = conv2d_bn(x, 80, 1, padding='valid')\n-    x = conv2d_bn(x, 192, 3, padding='valid')\n-    x = MaxPooling2D(3, strides=2)(x)\n-\n-    # Mixed 5b (Inception-A block): 35 x 35 x 320\n-    branch_0 = conv2d_bn(x, 96, 1)\n-    branch_1 = conv2d_bn(x, 48, 1)\n-    branch_1 = conv2d_bn(branch_1, 64, 5)\n-    branch_2 = conv2d_bn(x, 64, 1)\n-    branch_2 = conv2d_bn(branch_2, 96, 3)\n-    branch_2 = conv2d_bn(branch_2, 96, 3)\n-    branch_pool = AveragePooling2D(3, strides=1, padding='same')(x)\n-    branch_pool = conv2d_bn(branch_pool, 64, 1)\n-    branches = [branch_0, branch_1, branch_2, branch_pool]\n-    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n-    x = Concatenate(axis=channel_axis, name='mixed_5b')(branches)\n-\n-    # 10x block35 (Inception-ResNet-A block): 35 x 35 x 320\n-    for block_idx in range(1, 11):\n-        x = inception_resnet_block(x,\n-                                   scale=0.17,\n-                                   block_type='block35',\n-                                   block_idx=block_idx)\n-\n-    # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n-    branch_0 = conv2d_bn(x, 384, 3, strides=2, padding='valid')\n-    branch_1 = conv2d_bn(x, 256, 1)\n-    branch_1 = conv2d_bn(branch_1, 256, 3)\n-    branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding='valid')\n-    branch_pool = MaxPooling2D(3, strides=2, padding='valid')(x)\n-    branches = [branch_0, branch_1, branch_pool]\n-    x = Concatenate(axis=channel_axis, name='mixed_6a')(branches)\n-\n-    # 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n-    for block_idx in range(1, 21):\n-        x = inception_resnet_block(x,\n-                                   scale=0.1,\n-                                   block_type='block17',\n-                                   block_idx=block_idx)\n-\n-    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n-    branch_0 = conv2d_bn(x, 256, 1)\n-    branch_0 = conv2d_bn(branch_0, 384, 3, strides=2, padding='valid')\n-    branch_1 = conv2d_bn(x, 256, 1)\n-    branch_1 = conv2d_bn(branch_1, 288, 3, strides=2, padding='valid')\n-    branch_2 = conv2d_bn(x, 256, 1)\n-    branch_2 = conv2d_bn(branch_2, 288, 3)\n-    branch_2 = conv2d_bn(branch_2, 320, 3, strides=2, padding='valid')\n-    branch_pool = MaxPooling2D(3, strides=2, padding='valid')(x)\n-    branches = [branch_0, branch_1, branch_2, branch_pool]\n-    x = Concatenate(axis=channel_axis, name='mixed_7a')(branches)\n-\n-    # 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n-    for block_idx in range(1, 10):\n-        x = inception_resnet_block(x,\n-                                   scale=0.2,\n-                                   block_type='block8',\n-                                   block_idx=block_idx)\n-    x = inception_resnet_block(x,\n-                               scale=1.,\n-                               activation=None,\n-                               block_type='block8',\n-                               block_idx=10)\n-\n-    # Final convolution block: 8 x 8 x 1536\n-    x = conv2d_bn(x, 1536, 1, name='conv_7b')\n-\n-    if include_top:\n-        # Classification block\n-        x = GlobalAveragePooling2D(name='avg_pool')(x)\n-        x = Dense(classes, activation='softmax', name='predictions')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-\n-    # Create model\n-    model = Model(inputs, x, name='inception_resnet_v2')\n-\n-    # Load weights\n-    if weights == 'imagenet':\n-        if include_top:\n-            fname = 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5'\n-            weights_path = get_file(fname,\n-                                    BASE_WEIGHT_URL + fname,\n-                                    cache_subdir='models',\n-                                    file_hash='e693bd0210a403b3192acc6073ad2e96')\n-        else:\n-            fname = 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-            weights_path = get_file(fname,\n-                                    BASE_WEIGHT_URL + fname,\n-                                    cache_subdir='models',\n-                                    file_hash='d19885ff4a710c122648d3b5c3b684e4')\n-        model.load_weights(weights_path)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    return model\n+InceptionResNetV2 = inception_resnet_v2.InceptionResNetV2\n+decode_predictions = inception_resnet_v2.decode_predictions\n+preprocess_input = inception_resnet_v2.preprocess_input\n\n@@ -1,392 +1,9 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"Inception V3 model for Keras.\n-\n-Note that the input image format for this model is different than for\n-the VGG16 and ResNet models (299x299 instead of 224x224),\n-and that the input preprocessing function is also different (same as Xception).\n-\n-# Reference\n-\n-- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n-\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import inception_v3\n \n-from ..models import Model\n-from .. import layers\n-from ..layers import Activation\n-from ..layers import Dense\n-from ..layers import Input\n-from ..layers import BatchNormalization\n-from ..layers import Conv2D\n-from ..layers import MaxPooling2D\n-from ..layers import AveragePooling2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..engine import get_source_inputs\n-from ..utils.data_utils import get_file\n-from .. import backend as K\n-from . import imagenet_utils\n-from .imagenet_utils import decode_predictions\n-from .imagenet_utils import _obtain_input_shape\n-\n-\n-WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n-WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-\n-\n-def conv2d_bn(x,\n-              filters,\n-              num_row,\n-              num_col,\n-              padding='same',\n-              strides=(1, 1),\n-              name=None):\n-    \"\"\"Utility function to apply conv + BN.\n-\n-    # Arguments\n-        x: input tensor.\n-        filters: filters in `Conv2D`.\n-        num_row: height of the convolution kernel.\n-        num_col: width of the convolution kernel.\n-        padding: padding mode in `Conv2D`.\n-        strides: strides in `Conv2D`.\n-        name: name of the ops; will become `name + '_conv'`\n-            for the convolution and `name + '_bn'` for the\n-            batch norm layer.\n-\n-    # Returns\n-        Output tensor after applying `Conv2D` and `BatchNormalization`.\n-    \"\"\"\n-    if name is not None:\n-        bn_name = name + '_bn'\n-        conv_name = name + '_conv'\n-    else:\n-        bn_name = None\n-        conv_name = None\n-    if K.image_data_format() == 'channels_first':\n-        bn_axis = 1\n-    else:\n-        bn_axis = 3\n-    x = Conv2D(\n-        filters, (num_row, num_col),\n-        strides=strides,\n-        padding=padding,\n-        use_bias=False,\n-        name=conv_name)(x)\n-    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n-    x = Activation('relu', name=name)(x)\n-    return x\n-\n-\n-def InceptionV3(include_top=True,\n-                weights='imagenet',\n-                input_tensor=None,\n-                input_shape=None,\n-                pooling=None,\n-                classes=1000):\n-    \"\"\"Instantiates the Inception v3 architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(299, 299, 3)` (with `channels_last` data format)\n-            or `(3, 299, 299)` (with `channels_first` data format).\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 139.\n-            E.g. `(150, 150, 3)` would be one valid value.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(\n-        input_shape,\n-        default_size=299,\n-        min_size=139,\n-        data_format=K.image_data_format(),\n-        require_flatten=False,\n-        weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    if K.image_data_format() == 'channels_first':\n-        channel_axis = 1\n-    else:\n-        channel_axis = 3\n-\n-    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n-    x = conv2d_bn(x, 32, 3, 3, padding='valid')\n-    x = conv2d_bn(x, 64, 3, 3)\n-    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-\n-    x = conv2d_bn(x, 80, 1, 1, padding='valid')\n-    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n-    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-\n-    # mixed 0, 1, 2: 35 x 35 x 256\n-    branch1x1 = conv2d_bn(x, 64, 1, 1)\n-\n-    branch5x5 = conv2d_bn(x, 48, 1, 1)\n-    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n-\n-    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-\n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n-    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n-    x = layers.concatenate(\n-        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n-        axis=channel_axis,\n-        name='mixed0')\n-\n-    # mixed 1: 35 x 35 x 256\n-    branch1x1 = conv2d_bn(x, 64, 1, 1)\n-\n-    branch5x5 = conv2d_bn(x, 48, 1, 1)\n-    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n-\n-    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-\n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n-    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n-    x = layers.concatenate(\n-        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n-        axis=channel_axis,\n-        name='mixed1')\n-\n-    # mixed 2: 35 x 35 x 256\n-    branch1x1 = conv2d_bn(x, 64, 1, 1)\n-\n-    branch5x5 = conv2d_bn(x, 48, 1, 1)\n-    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n-\n-    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-\n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n-    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n-    x = layers.concatenate(\n-        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n-        axis=channel_axis,\n-        name='mixed2')\n-\n-    # mixed 3: 17 x 17 x 768\n-    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n-\n-    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n-    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n-    branch3x3dbl = conv2d_bn(\n-        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n-\n-    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-    x = layers.concatenate(\n-        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n-\n-    # mixed 4: 17 x 17 x 768\n-    branch1x1 = conv2d_bn(x, 192, 1, 1)\n-\n-    branch7x7 = conv2d_bn(x, 128, 1, 1)\n-    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n-    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n-\n-    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n-\n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n-    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-    x = layers.concatenate(\n-        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n-        axis=channel_axis,\n-        name='mixed4')\n-\n-    # mixed 5, 6: 17 x 17 x 768\n-    for i in range(2):\n-        branch1x1 = conv2d_bn(x, 192, 1, 1)\n-\n-        branch7x7 = conv2d_bn(x, 160, 1, 1)\n-        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n-        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n-\n-        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n-        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n-        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n-        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n-        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n-\n-        branch_pool = AveragePooling2D(\n-            (3, 3), strides=(1, 1), padding='same')(x)\n-        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-        x = layers.concatenate(\n-            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n-            axis=channel_axis,\n-            name='mixed' + str(5 + i))\n-\n-    # mixed 7: 17 x 17 x 768\n-    branch1x1 = conv2d_bn(x, 192, 1, 1)\n-\n-    branch7x7 = conv2d_bn(x, 192, 1, 1)\n-    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n-    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n-\n-    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n-    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n-\n-    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n-    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-    x = layers.concatenate(\n-        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n-        axis=channel_axis,\n-        name='mixed7')\n-\n-    # mixed 8: 8 x 8 x 1280\n-    branch3x3 = conv2d_bn(x, 192, 1, 1)\n-    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n-                          strides=(2, 2), padding='valid')\n-\n-    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n-    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n-    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n-    branch7x7x3 = conv2d_bn(\n-        branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n-\n-    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-    x = layers.concatenate(\n-        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n-\n-    # mixed 9: 8 x 8 x 2048\n-    for i in range(2):\n-        branch1x1 = conv2d_bn(x, 320, 1, 1)\n-\n-        branch3x3 = conv2d_bn(x, 384, 1, 1)\n-        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n-        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n-        branch3x3 = layers.concatenate(\n-            [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n-\n-        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n-        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n-        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n-        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n-        branch3x3dbl = layers.concatenate(\n-            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n-\n-        branch_pool = AveragePooling2D(\n-            (3, 3), strides=(1, 1), padding='same')(x)\n-        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n-        x = layers.concatenate(\n-            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n-            axis=channel_axis,\n-            name='mixed' + str(9 + i))\n-    if include_top:\n-        # Classification block\n-        x = GlobalAveragePooling2D(name='avg_pool')(x)\n-        x = Dense(classes, activation='softmax', name='predictions')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-    # Create model.\n-    model = Model(inputs, x, name='inception_v3')\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if include_top:\n-            weights_path = get_file(\n-                'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n-                WEIGHTS_PATH,\n-                cache_subdir='models',\n-                file_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n-        else:\n-            weights_path = get_file(\n-                'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                WEIGHTS_PATH_NO_TOP,\n-                cache_subdir='models',\n-                file_hash='bcbd6486424b2319ff4ef7d526e38f63')\n-        model.load_weights(weights_path)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    return model\n-\n-\n-def preprocess_input(x):\n-    \"\"\"Preprocesses a numpy array encoding a batch of images.\n-\n-    # Arguments\n-        x: a 4D numpy array consists of RGB values within [0, 255].\n-\n-    # Returns\n-        Preprocessed array.\n-    \"\"\"\n-    return imagenet_utils.preprocess_input(x, mode='tf')\n+InceptionV3 = inception_v3.InceptionV3\n+decode_predictions = inception_v3.decode_predictions\n+preprocess_input = inception_v3.preprocess_input\n\n@@ -1,470 +1,9 @@\n-\"\"\"MobileNet v1 models for Keras.\n-\n-MobileNet is a general architecture and can be used for multiple use cases.\n-Depending on the use case, it can use different input layer size and\n-different width factors. This allows different width models to reduce\n-the number of multiply-adds and thereby\n-reduce inference cost on mobile devices.\n-\n-MobileNets support any input size greater than 32 x 32, with larger image sizes\n-offering better performance.\n-The number of parameters and number of multiply-adds\n-can be modified by using the `alpha` parameter,\n-which increases/decreases the number of filters in each layer.\n-By altering the image size and `alpha` parameter,\n-all 16 models from the paper can be built, with ImageNet weights provided.\n-\n-The paper demonstrates the performance of MobileNets using `alpha` values of\n-1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25.\n-For each of these `alpha` values, weights for 4 different input image sizes\n-are provided (224, 192, 160, 128).\n-\n-The following table describes the size and accuracy of the 100% MobileNet\n-on size 224 x 224:\n-----------------------------------------------------------------------------\n-Width Multiplier (alpha) | ImageNet Acc |  Multiply-Adds (M) |  Params (M)\n-----------------------------------------------------------------------------\n-|   1.0 MobileNet-224    |    70.6 %     |        529        |     4.2     |\n-|   0.75 MobileNet-224   |    68.4 %     |        325        |     2.6     |\n-|   0.50 MobileNet-224   |    63.7 %     |        149        |     1.3     |\n-|   0.25 MobileNet-224   |    50.6 %     |        41         |     0.5     |\n-----------------------------------------------------------------------------\n-\n-The following table describes the performance of\n-the 100 % MobileNet on various input sizes:\n-------------------------------------------------------------------------\n-      Resolution      | ImageNet Acc | Multiply-Adds (M) | Params (M)\n-------------------------------------------------------------------------\n-|  1.0 MobileNet-224  |    70.6 %    |        529        |     4.2     |\n-|  1.0 MobileNet-192  |    69.1 %    |        529        |     4.2     |\n-|  1.0 MobileNet-160  |    67.2 %    |        529        |     4.2     |\n-|  1.0 MobileNet-128  |    64.4 %    |        529        |     4.2     |\n-------------------------------------------------------------------------\n-\n-The weights for all 16 models are obtained and translated\n-from TensorFlow checkpoints found at\n-https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\n-\n-# Reference\n-- [MobileNets: Efficient Convolutional Neural Networks for\n-   Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf))\n-\"\"\"\n-from __future__ import print_function\n from __future__ import absolute_import\n from __future__ import division\n+from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import mobilenet\n \n-from ..models import Model\n-from ..layers import Input\n-from ..layers import Activation\n-from ..layers import Dropout\n-from ..layers import Reshape\n-from ..layers import BatchNormalization\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..layers import ZeroPadding2D\n-from ..layers import Conv2D\n-from ..layers import DepthwiseConv2D\n-from .. import initializers\n-from .. import regularizers\n-from .. import constraints\n-from ..utils import conv_utils\n-from ..utils.data_utils import get_file\n-from ..engine import get_source_inputs\n-from ..engine.base_layer import InputSpec\n-from . import imagenet_utils\n-from .imagenet_utils import _obtain_input_shape\n-from .imagenet_utils import decode_predictions\n-from .. import backend as K\n-\n-\n-BASE_WEIGHT_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.6/'\n-\n-\n-def relu6(x):\n-    return K.relu(x, max_value=6)\n-\n-\n-def preprocess_input(x):\n-    \"\"\"Preprocesses a numpy array encoding a batch of images.\n-\n-    # Arguments\n-        x: a 4D numpy array consists of RGB values within [0, 255].\n-\n-    # Returns\n-        Preprocessed array.\n-    \"\"\"\n-    return imagenet_utils.preprocess_input(x, mode='tf')\n-\n-\n-def MobileNet(input_shape=None,\n-              alpha=1.0,\n-              depth_multiplier=1,\n-              dropout=1e-3,\n-              include_top=True,\n-              weights='imagenet',\n-              input_tensor=None,\n-              pooling=None,\n-              classes=1000):\n-    \"\"\"Instantiates the MobileNet architecture.\n-\n-    To load a MobileNet model via `load_model`, import the custom\n-    objects `relu6` and pass them to the `custom_objects` parameter.\n-    E.g.\n-    model = load_model('mobilenet.h5', custom_objects={\n-                       'relu6': mobilenet.relu6})\n-\n-    # Arguments\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(224, 224, 3)` (with `channels_last` data format)\n-            or (3, 224, 224) (with `channels_first` data format).\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 32.\n-            E.g. `(200, 200, 3)` would be one valid value.\n-        alpha: controls the width of the network.\n-            - If `alpha` < 1.0, proportionally decreases the number\n-                of filters in each layer.\n-            - If `alpha` > 1.0, proportionally increases the number\n-                of filters in each layer.\n-            - If `alpha` = 1, default number of filters from the paper\n-                 are used at each layer.\n-        depth_multiplier: depth multiplier for depthwise convolution\n-            (also called the resolution multiplier)\n-        dropout: dropout rate\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of\n-            `layers.Input()`)\n-            to use as image input for the model.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model\n-                will be the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a\n-                2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-        RuntimeError: If attempting to run this model with a\n-            backend that does not support separable convolutions.\n-    \"\"\"\n-\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as ImageNet with `include_top` '\n-                         'as true, `classes` should be 1000')\n-\n-    # Determine proper input shape and default size.\n-    if input_shape is None:\n-        default_size = 224\n-    else:\n-        if K.image_data_format() == 'channels_first':\n-            rows = input_shape[1]\n-            cols = input_shape[2]\n-        else:\n-            rows = input_shape[0]\n-            cols = input_shape[1]\n-\n-        if rows == cols and rows in [128, 160, 192, 224]:\n-            default_size = rows\n-        else:\n-            default_size = 224\n-\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=default_size,\n-                                      min_size=32,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=include_top,\n-                                      weights=weights)\n-\n-    if K.image_data_format() == 'channels_last':\n-        row_axis, col_axis = (0, 1)\n-    else:\n-        row_axis, col_axis = (1, 2)\n-    rows = input_shape[row_axis]\n-    cols = input_shape[col_axis]\n-\n-    if weights == 'imagenet':\n-        if depth_multiplier != 1:\n-            raise ValueError('If imagenet weights are being loaded, '\n-                             'depth multiplier must be 1')\n-\n-        if alpha not in [0.25, 0.50, 0.75, 1.0]:\n-            raise ValueError('If imagenet weights are being loaded, '\n-                             'alpha can be one of'\n-                             '`0.25`, `0.50`, `0.75` or `1.0` only.')\n-\n-        if rows != cols or rows not in [128, 160, 192, 224]:\n-            if rows is None:\n-                rows = 224\n-                warnings.warn('MobileNet shape is undefined.'\n-                              ' Weights for input shape (224, 224) will be loaded.')\n-            else:\n-                raise ValueError('If imagenet weights are being loaded, '\n-                                 'input must have a static square shape (one of '\n-                                 '(128, 128), (160, 160), (192, 192), or (224, 224)).'\n-                                 ' Input shape provided = %s' % (input_shape,))\n-\n-    if K.image_data_format() != 'channels_last':\n-        warnings.warn('The MobileNet family of models is only available '\n-                      'for the input data format \"channels_last\" '\n-                      '(width, height, channels). '\n-                      'However your settings specify the default '\n-                      'data format \"channels_first\" (channels, width, height).'\n-                      ' You should set `image_data_format=\"channels_last\"` '\n-                      'in your Keras config located at ~/.keras/keras.json. '\n-                      'The model being returned right now will expect inputs '\n-                      'to follow the \"channels_last\" data format.')\n-        K.set_image_data_format('channels_last')\n-        old_data_format = 'channels_first'\n-    else:\n-        old_data_format = None\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n-    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1)\n-\n-    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier,\n-                              strides=(2, 2), block_id=2)\n-    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3)\n-\n-    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier,\n-                              strides=(2, 2), block_id=4)\n-    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5)\n-\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier,\n-                              strides=(2, 2), block_id=6)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11)\n-\n-    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier,\n-                              strides=(2, 2), block_id=12)\n-    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n-\n-    if include_top:\n-        if K.image_data_format() == 'channels_first':\n-            shape = (int(1024 * alpha), 1, 1)\n-        else:\n-            shape = (1, 1, int(1024 * alpha))\n-\n-        x = GlobalAveragePooling2D()(x)\n-        x = Reshape(shape, name='reshape_1')(x)\n-        x = Dropout(dropout, name='dropout')(x)\n-        x = Conv2D(classes, (1, 1),\n-                   padding='same', name='conv_preds')(x)\n-        x = Activation('softmax', name='act_softmax')(x)\n-        x = Reshape((classes,), name='reshape_2')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-\n-    # Create model.\n-    model = Model(inputs, x, name='mobilenet_%0.2f_%s' % (alpha, rows))\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if K.image_data_format() == 'channels_first':\n-            raise ValueError('Weights for \"channels_first\" format '\n-                             'are not available.')\n-        if alpha == 1.0:\n-            alpha_text = '1_0'\n-        elif alpha == 0.75:\n-            alpha_text = '7_5'\n-        elif alpha == 0.50:\n-            alpha_text = '5_0'\n-        else:\n-            alpha_text = '2_5'\n-\n-        if include_top:\n-            model_name = 'mobilenet_%s_%d_tf.h5' % (alpha_text, rows)\n-            weigh_path = BASE_WEIGHT_PATH + model_name\n-            weights_path = get_file(model_name,\n-                                    weigh_path,\n-                                    cache_subdir='models')\n-        else:\n-            model_name = 'mobilenet_%s_%d_tf_no_top.h5' % (alpha_text, rows)\n-            weigh_path = BASE_WEIGHT_PATH + model_name\n-            weights_path = get_file(model_name,\n-                                    weigh_path,\n-                                    cache_subdir='models')\n-        model.load_weights(weights_path)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    if old_data_format:\n-        K.set_image_data_format(old_data_format)\n-    return model\n-\n-\n-def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n-    \"\"\"Adds an initial convolution layer (with batch normalization and relu6).\n-\n-    # Arguments\n-        inputs: Input tensor of shape `(rows, cols, 3)`\n-            (with `channels_last` data format) or\n-            (3, rows, cols) (with `channels_first` data format).\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 32.\n-            E.g. `(224, 224, 3)` would be one valid value.\n-        filters: Integer, the dimensionality of the output space\n-            (i.e. the number of output filters in the convolution).\n-        alpha: controls the width of the network.\n-            - If `alpha` < 1.0, proportionally decreases the number\n-                of filters in each layer.\n-            - If `alpha` > 1.0, proportionally increases the number\n-                of filters in each layer.\n-            - If `alpha` = 1, default number of filters from the paper\n-                 are used at each layer.\n-        kernel: An integer or tuple/list of 2 integers, specifying the\n-            width and height of the 2D convolution window.\n-            Can be a single integer to specify the same value for\n-            all spatial dimensions.\n-        strides: An integer or tuple/list of 2 integers,\n-            specifying the strides of the convolution along the width and height.\n-            Can be a single integer to specify the same value for\n-            all spatial dimensions.\n-            Specifying any stride value != 1 is incompatible with specifying\n-            any `dilation_rate` value != 1.\n-\n-    # Input shape\n-        4D tensor with shape:\n-        `(samples, channels, rows, cols)` if data_format='channels_first'\n-        or 4D tensor with shape:\n-        `(samples, rows, cols, channels)` if data_format='channels_last'.\n-\n-    # Output shape\n-        4D tensor with shape:\n-        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n-        or 4D tensor with shape:\n-        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n-        `rows` and `cols` values might have changed due to stride.\n-\n-    # Returns\n-        Output tensor of block.\n-    \"\"\"\n-    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n-    filters = int(filters * alpha)\n-    x = ZeroPadding2D(padding=(1, 1), name='conv1_pad')(inputs)\n-    x = Conv2D(filters, kernel,\n-               padding='valid',\n-               use_bias=False,\n-               strides=strides,\n-               name='conv1')(x)\n-    x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n-    return Activation(relu6, name='conv1_relu')(x)\n-\n-\n-def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n-                          depth_multiplier=1, strides=(1, 1), block_id=1):\n-    \"\"\"Adds a depthwise convolution block.\n-\n-    A depthwise convolution block consists of a depthwise conv,\n-    batch normalization, relu6, pointwise convolution,\n-    batch normalization and relu6 activation.\n-\n-    # Arguments\n-        inputs: Input tensor of shape `(rows, cols, channels)`\n-            (with `channels_last` data format) or\n-            (channels, rows, cols) (with `channels_first` data format).\n-        pointwise_conv_filters: Integer, the dimensionality of the output space\n-            (i.e. the number of output filters in the pointwise convolution).\n-        alpha: controls the width of the network.\n-            - If `alpha` < 1.0, proportionally decreases the number\n-                of filters in each layer.\n-            - If `alpha` > 1.0, proportionally increases the number\n-                of filters in each layer.\n-            - If `alpha` = 1, default number of filters from the paper\n-                 are used at each layer.\n-        depth_multiplier: The number of depthwise convolution output channels\n-            for each input channel.\n-            The total number of depthwise convolution output\n-            channels will be equal to `filters_in * depth_multiplier`.\n-        strides: An integer or tuple/list of 2 integers,\n-            specifying the strides of the convolution along the width and height.\n-            Can be a single integer to specify the same value for\n-            all spatial dimensions.\n-            Specifying any stride value != 1 is incompatible with specifying\n-            any `dilation_rate` value != 1.\n-        block_id: Integer, a unique identification designating the block number.\n-\n-    # Input shape\n-        4D tensor with shape:\n-        `(batch, channels, rows, cols)` if data_format='channels_first'\n-        or 4D tensor with shape:\n-        `(batch, rows, cols, channels)` if data_format='channels_last'.\n-\n-    # Output shape\n-        4D tensor with shape:\n-        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n-        or 4D tensor with shape:\n-        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n-        `rows` and `cols` values might have changed due to stride.\n-\n-    # Returns\n-        Output tensor of block.\n-    \"\"\"\n-    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n-    pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n-\n-    x = ZeroPadding2D(padding=(1, 1), name='conv_pad_%d' % block_id)(inputs)\n-    x = DepthwiseConv2D((3, 3),\n-                        padding='valid',\n-                        depth_multiplier=depth_multiplier,\n-                        strides=strides,\n-                        use_bias=False,\n-                        name='conv_dw_%d' % block_id)(x)\n-    x = BatchNormalization(\n-        axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n-    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n-\n-    x = Conv2D(pointwise_conv_filters, (1, 1),\n-               padding='same',\n-               use_bias=False,\n-               strides=(1, 1),\n-               name='conv_pw_%d' % block_id)(x)\n-    x = BatchNormalization(\n-        axis=channel_axis, name='conv_pw_%d_bn' % block_id)(x)\n-    return Activation(relu6, name='conv_pw_%d_relu' % block_id)(x)\n+MobileNet = mobilenet.MobileNet\n+decode_predictions = mobilenet.decode_predictions\n+preprocess_input = mobilenet.preprocess_input\n\n@@ -1,505 +1,9 @@\n-\"\"\"MobileNet v2 models for Keras.\n-\n-MobileNetV2 is a general architecture and can be used for multiple use cases.\n-Depending on the use case, it can use different input layer size and\n-different width factors. This allows different width models to reduce\n-the number of multiply-adds and thereby\n-reduce inference cost on mobile devices.\n-\n-MobileNetV2 is very similar to the original MobileNet,\n-except that it uses inverted residual blocks with\n-bottlenecking features. It has a drastically lower\n-parameter count than the original MobileNet.\n-MobileNets support any input size greater\n-than 32 x 32, with larger image sizes\n-offering better performance.\n-\n-The number of parameters and number of multiply-adds\n-can be modified by using the `alpha` parameter,\n-which increases/decreases the number of filters in each layer.\n-By altering the image size and `alpha` parameter,\n-all 22 models from the paper can be built, with ImageNet weights provided.\n-\n-The paper demonstrates the performance of MobileNets using `alpha` values of\n-1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n-\n-For each of these `alpha` values, weights for 5 different input image sizes\n-are provided (224, 192, 160, 128, and 96).\n-\n-\n-The following table describes the performance of\n-MobileNet on various input sizes:\n-------------------------------------------------------------------------\n-MACs stands for Multiply Adds\n-\n- Classification Checkpoint| MACs (M)   | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n---------------------------|------------|---------------|---------|----|-------------\n-| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n-| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n-| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n-| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n-| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n-| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n-| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n-| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n-| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n-| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n-| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n-| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n-| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n-| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n-| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n-| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n-| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n-| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n-| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n-| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n-| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n-| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n-\n-The weights for all 16 models are obtained and translated from the Tensorflow checkpoints\n-from TensorFlow checkpoints found at\n-https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md\n-\n-# Reference\n-This file contains building code for MobileNetV2, based on\n-[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)\n-\n-Tests comparing this model to the existing Tensorflow model can be\n-found at [mobilenet_v2_keras](https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n-\"\"\"\n-from __future__ import print_function\n from __future__ import absolute_import\n from __future__ import division\n+from __future__ import print_function\n \n-import os\n-import warnings\n-import h5py\n-import numpy as np\n+from keras_applications import mobilenet_v2\n \n-from ..models import Model\n-from ..layers import Input\n-from ..layers import Activation\n-from ..layers import Dropout\n-from ..layers import Reshape\n-from ..layers import BatchNormalization\n-from ..layers import Conv2D\n-from ..layers import DepthwiseConv2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..layers import Add\n-from ..layers import Flatten\n-from ..layers import Dense\n-from .. import initializers\n-from .. import regularizers\n-from .. import constraints\n-from ..utils import conv_utils\n-from ..utils.data_utils import get_file\n-from ..engine import get_source_inputs\n-from ..engine.base_layer import InputSpec\n-from . import imagenet_utils\n-from .imagenet_utils import _obtain_input_shape\n-from .imagenet_utils import decode_predictions\n-from .. import backend as K\n-\n-# TODO Change path to v1.1\n-BASE_WEIGHT_PATH = 'https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/'\n-\n-\n-def relu6(x):\n-    return K.relu(x, max_value=6)\n-\n-\n-def preprocess_input(x):\n-    \"\"\"Preprocesses a numpy array encoding a batch of images.\n-\n-    This function applies the \"Inception\" preprocessing which converts\n-    the RGB values from [0, 255] to [-1, 1]. Note that this preprocessing\n-    function is different from `imagenet_utils.preprocess_input()`.\n-\n-    # Arguments\n-        x: a 4D numpy array consists of RGB values within [0, 255].\n-\n-    # Returns\n-        Preprocessed array.\n-    \"\"\"\n-    x /= 128.\n-    x -= 1.\n-    return x.astype(np.float32)\n-\n-\n-# This function is taken from the original tf repo.\n-# It ensures that all layers have a channel number that is divisible by 8\n-# It can be seen here:\n-# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n-\n-\n-def _make_divisible(v, divisor, min_value=None):\n-    if min_value is None:\n-        min_value = divisor\n-    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n-    # Make sure that round down does not go down by more than 10%.\n-    if new_v < 0.9 * v:\n-        new_v += divisor\n-    return new_v\n-\n-\n-def MobileNetV2(input_shape=None,\n-                alpha=1.0,\n-                depth_multiplier=1,\n-                include_top=True,\n-                weights='imagenet',\n-                input_tensor=None,\n-                pooling=None,\n-                classes=1000):\n-    \"\"\"Instantiates the MobileNetV2 architecture.\n-\n-    To load a MobileNetV2 model via `load_model`, import the custom\n-    objects `relu6` and pass them to the `custom_objects` parameter.\n-    E.g.\n-    model = load_model('mobilenet.h5', custom_objects={\n-                       'relu6': mobilenet.relu6})\n-\n-    # Arguments\n-        input_shape: optional shape tuple, to be specified if you would\n-            like to use a model with an input img resolution that is not\n-            (224, 224, 3).\n-            It should have exactly 3 inputs channels (224, 224, 3).\n-            You can also omit this option if you would like\n-            to infer input_shape from an input_tensor.\n-            If you choose to include both input_tensor and input_shape then\n-            input_shape will be used if they match, if the shapes\n-            do not match then we will throw an error.\n-            E.g. `(160, 160, 3)` would be one valid value.\n-        alpha: controls the width of the network. This is known as the\n-        width multiplier in the MobileNetV2 paper.\n-            - If `alpha` < 1.0, proportionally decreases the number\n-                of filters in each layer.\n-            - If `alpha` > 1.0, proportionally increases the number\n-                of filters in each layer.\n-            - If `alpha` = 1, default number of filters from the paper\n-                 are used at each layer.\n-        depth_multiplier: depth multiplier for depthwise convolution\n-            (also called the resolution multiplier)\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of\n-            `layers.Input()`)\n-            to use as image input for the model.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model\n-                will be the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a\n-                2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape or invalid depth_multiplier, alpha,\n-            rows when weights='imagenet'\n-    \"\"\"\n-\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as ImageNet with `include_top` '\n-                         'as true, `classes` should be 1000')\n-\n-    # Determine proper input shape and default size.\n-    # If both input_shape and input_tensor are used, they should match\n-    if input_shape is not None and input_tensor is not None:\n-        try:\n-            is_input_t_tensor = K.is_keras_tensor(input_tensor)\n-        except ValueError:\n-            try:\n-                is_input_t_tensor = K.is_keras_tensor(\n-                    get_source_inputs(input_tensor))\n-            except ValueError:\n-                raise ValueError('input_tensor: ', input_tensor,\n-                                 'is not type input_tensor')\n-        if is_input_t_tensor:\n-            if K.image_data_format == 'channels_first':\n-                if input_tensor._keras_shape[1] != input_shape[1]:\n-                    raise ValueError('input_shape: ', input_shape,\n-                                     'and input_tensor: ', input_tensor,\n-                                     'do not meet the same shape requirements')\n-            else:\n-                if input_tensor._keras_shape[2] != input_shape[1]:\n-                    raise ValueError('input_shape: ', input_shape,\n-                                     'and input_tensor: ', input_tensor,\n-                                     'do not meet the same shape requirements')\n-        else:\n-            raise ValueError('input_tensor specified: ', input_tensor,\n-                             'is not a keras tensor')\n-\n-    # If input_shape is None, infer shape from input_tensor\n-    if input_shape is None and input_tensor is not None:\n-\n-        try:\n-            K.is_keras_tensor(input_tensor)\n-        except ValueError:\n-            raise ValueError('input_tensor: ', input_tensor,\n-                             'is type: ', type(input_tensor),\n-                             'which is not a valid type')\n-\n-        if input_shape is None and not K.is_keras_tensor(input_tensor):\n-            default_size = 224\n-        elif input_shape is None and K.is_keras_tensor(input_tensor):\n-            if K.image_data_format() == 'channels_first':\n-                rows = input_tensor._keras_shape[2]\n-                cols = input_tensor._keras_shape[3]\n-            else:\n-                rows = input_tensor._keras_shape[1]\n-                cols = input_tensor._keras_shape[2]\n-\n-            if rows == cols and rows in [96, 128, 160, 192, 224]:\n-                default_size = rows\n-            else:\n-                default_size = 224\n-\n-    # If input_shape is None and no input_tensor\n-    elif input_shape is None:\n-        default_size = 224\n-\n-    # If input_shape is not None, assume default size\n-    else:\n-        if K.image_data_format() == 'channels_first':\n-            rows = input_shape[1]\n-            cols = input_shape[2]\n-        else:\n-            rows = input_shape[0]\n-            cols = input_shape[1]\n-\n-        if rows == cols and rows in [96, 128, 160, 192, 224]:\n-            default_size = rows\n-        else:\n-            default_size = 224\n-\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=default_size,\n-                                      min_size=32,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=include_top,\n-                                      weights=weights)\n-\n-    if K.image_data_format() == 'channels_last':\n-        row_axis, col_axis = (0, 1)\n-    else:\n-        row_axis, col_axis = (1, 2)\n-    rows = input_shape[row_axis]\n-    cols = input_shape[col_axis]\n-\n-    if weights == 'imagenet':\n-        if depth_multiplier != 1:\n-            raise ValueError('If imagenet weights are being loaded, '\n-                             'depth multiplier must be 1')\n-\n-        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n-            raise ValueError('If imagenet weights are being loaded, '\n-                             'alpha can be one of'\n-                             '`0.25`, `0.50`, `0.75` or `1.0` only.')\n-\n-        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n-            if rows is None:\n-                rows = 224\n-                warnings.warn('MobileNet shape is undefined.'\n-                              ' Weights for input shape'\n-                              '(224, 224) will be loaded.')\n-            else:\n-                raise ValueError('If imagenet weights are being loaded, '\n-                                 'input must have a static square shape'\n-                                 '(one of (96, 96), (128, 128), (160, 160),'\n-                                 '(192, 192), or (224, 224)).'\n-                                 'Input shape provided = %s' % (input_shape,))\n-\n-    if K.image_data_format() != 'channels_last':\n-        warnings.warn('The MobileNet family of models is only available '\n-                      'for the input data format \"channels_last\" '\n-                      '(width, height, channels). '\n-                      'However your settings specify the default '\n-                      'data format \"channels_first\" (channels, width, height).'\n-                      ' You should set `image_data_format=\"channels_last\"` '\n-                      'in your Keras config located at ~/.keras/keras.json. '\n-                      'The model being returned right now will expect inputs '\n-                      'to follow the \"channels_last\" data format.')\n-        K.set_image_data_format('channels_last')\n-        old_data_format = 'channels_first'\n-    else:\n-        old_data_format = None\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    first_block_filters = _make_divisible(32 * alpha, 8)\n-    x = Conv2D(first_block_filters,\n-               kernel_size=3,\n-               strides=(2, 2), padding='same',\n-               use_bias=False, name='Conv1')(img_input)\n-    x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='bn_Conv1')(x)\n-    x = Activation(relu6, name='Conv1_relu')(x)\n-\n-    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n-                            expansion=1, block_id=0)\n-\n-    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n-                            expansion=6, block_id=1)\n-    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n-                            expansion=6, block_id=2)\n-\n-    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n-                            expansion=6, block_id=3)\n-    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n-                            expansion=6, block_id=4)\n-    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n-                            expansion=6, block_id=5)\n-\n-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n-                            expansion=6, block_id=6)\n-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n-                            expansion=6, block_id=7)\n-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n-                            expansion=6, block_id=8)\n-    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n-                            expansion=6, block_id=9)\n-\n-    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n-                            expansion=6, block_id=10)\n-    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n-                            expansion=6, block_id=11)\n-    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n-                            expansion=6, block_id=12)\n-\n-    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n-                            expansion=6, block_id=13)\n-    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n-                            expansion=6, block_id=14)\n-    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n-                            expansion=6, block_id=15)\n-\n-    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n-                            expansion=6, block_id=16)\n-\n-    # no alpha applied to last conv as stated in the paper:\n-    # if the width multiplier is greater than 1 we\n-    # increase the number of output channels\n-    if alpha > 1.0:\n-        last_block_filters = _make_divisible(1280 * alpha, 8)\n-    else:\n-        last_block_filters = 1280\n-\n-    x = Conv2D(last_block_filters,\n-               kernel_size=1,\n-               use_bias=False,\n-               name='Conv_1')(x)\n-    x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='Conv_1_bn')(x)\n-    x = Activation(relu6, name='out_relu')(x)\n-\n-    if include_top:\n-        x = GlobalAveragePooling2D()(x)\n-        x = Dense(classes, activation='softmax',\n-                  use_bias=True, name='Logits')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-\n-    # Create model.\n-    model = Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if K.image_data_format() == 'channels_first':\n-            raise ValueError('Weights for \"channels_first\" format '\n-                             'are not available.')\n-\n-        if include_top:\n-            model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + \\\n-                str(alpha) + '_' + str(rows) + '.h5'\n-            weigh_path = BASE_WEIGHT_PATH + model_name\n-            weights_path = get_file(model_name, weigh_path,\n-                                    cache_subdir='models')\n-        else:\n-            model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + \\\n-                str(alpha) + '_' + str(rows) + '_no_top' + '.h5'\n-            weigh_path = BASE_WEIGHT_PATH + model_name\n-            weights_path = get_file(model_name, weigh_path,\n-                                    cache_subdir='models')\n-        model.load_weights(weights_path)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    if old_data_format:\n-        K.set_image_data_format(old_data_format)\n-    return model\n-\n-\n-def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n-    in_channels = inputs._keras_shape[-1]\n-    pointwise_conv_filters = int(filters * alpha)\n-    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n-    x = inputs\n-    prefix = 'block_{}_'.format(block_id)\n-\n-    if block_id:\n-        # Expand\n-        x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',\n-                   use_bias=False, activation=None,\n-                   name=prefix + 'expand')(x)\n-        x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n-                               name=prefix + 'expand_BN')(x)\n-        x = Activation(relu6, name=prefix + 'expand_relu')(x)\n-    else:\n-        prefix = 'expanded_conv_'\n-\n-    # Depthwise\n-    x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,\n-                        use_bias=False, padding='same',\n-                        name=prefix + 'depthwise')(x)\n-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n-                           name=prefix + 'depthwise_BN')(x)\n-\n-    x = Activation(relu6, name=prefix + 'depthwise_relu')(x)\n-\n-    # Project\n-    x = Conv2D(pointwise_filters,\n-               kernel_size=1, padding='same', use_bias=False, activation=None,\n-               name=prefix + 'project')(x)\n-    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n-                           name=prefix + 'project_BN')(x)\n-\n-    if in_channels == pointwise_filters and stride == 1:\n-        return Add(name=prefix + 'add')([inputs, x])\n-\n-    return x\n+MobileNetV2 = mobilenet_v2.MobileNetV2\n+decode_predictions = mobilenet_v2.decode_predictions\n+preprocess_input = mobilenet_v2.preprocess_input\n\n@@ -1,666 +1,10 @@\n-'''NASNet-A models for Keras.\n-\n-NASNet refers to Neural Architecture Search Network, a family of models\n-that were designed automatically by learning the model architectures\n-directly on the dataset of interest.\n-\n-Here we consider NASNet-A, the highest performance model that was found\n-for the CIFAR-10 dataset, and then extended to ImageNet 2012 dataset,\n-obtaining state of the art performance on CIFAR-10 and ImageNet 2012.\n-Only the NASNet-A models, and their respective weights, which are suited\n-for ImageNet 2012 are provided.\n-\n-The below table describes the performance on ImageNet 2012:\n---------------------------------------------------------------------------------\n-      Architecture       | Top-1 Acc | Top-5 Acc |  Multiply-Adds |  Params (M)\n---------------------------------------------------------------------------------\n-|   NASNet-A (4 @ 1056)  |   74.0 %  |   91.6 %  |       564 M    |     5.3    |\n-|   NASNet-A (6 @ 4032)  |   82.7 %  |   96.2 %  |      23.8 B    |    88.9    |\n---------------------------------------------------------------------------------\n-\n-Weights obtained from the official TensorFlow repository found at\n-https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet\n-\n-# References\n- - [Learning Transferable Architectures for Scalable Image Recognition]\n-    (https://arxiv.org/abs/1707.07012)\n-\n-# Based on the following implementations\n- - [TF Slim Implementation]\n-   (https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/nasnet.py)\n- - [TensorNets implementation]\n-   (https://github.com/taehoonlee/tensornets/blob/master/tensornets/nasnets.py)\n-'''\n-from __future__ import print_function\n from __future__ import absolute_import\n from __future__ import division\n+from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import nasnet\n \n-from ..models import Model\n-from ..layers import Input\n-from ..layers import Activation\n-from ..layers import Dense\n-from ..layers import BatchNormalization\n-from ..layers import MaxPooling2D\n-from ..layers import AveragePooling2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..layers import Conv2D\n-from ..layers import SeparableConv2D\n-from ..layers import ZeroPadding2D\n-from ..layers import Cropping2D\n-from ..layers import concatenate\n-from ..layers import add\n-from ..utils.data_utils import get_file\n-from ..engine import get_source_inputs\n-from ..applications.imagenet_utils import _obtain_input_shape\n-from ..applications.inception_v3 import preprocess_input\n-from ..applications.imagenet_utils import decode_predictions\n-from .. import backend as K\n-\n-NASNET_MOBILE_WEIGHT_PATH = 'https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile.h5'\n-NASNET_MOBILE_WEIGHT_PATH_NO_TOP = 'https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile-no-top.h5'\n-NASNET_LARGE_WEIGHT_PATH = 'https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large.h5'\n-NASNET_LARGE_WEIGHT_PATH_NO_TOP = 'https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large-no-top.h5'\n-\n-\n-def NASNet(input_shape=None,\n-           penultimate_filters=4032,\n-           num_blocks=6,\n-           stem_block_filters=96,\n-           skip_reduction=True,\n-           filter_multiplier=2,\n-           include_top=True,\n-           weights=None,\n-           input_tensor=None,\n-           pooling=None,\n-           classes=1000,\n-           default_size=None):\n-    '''Instantiates a NASNet model.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        input_shape: Optional shape tuple, the input shape\n-            is by default `(331, 331, 3)` for NASNetLarge and\n-            `(224, 224, 3)` for NASNetMobile.\n-            It should have exactly 3 input channels,\n-            and width and height should be no smaller than 32.\n-            E.g. `(224, 224, 3)` would be one valid value.\n-        penultimate_filters: Number of filters in the penultimate layer.\n-            NASNet models use the notation `NASNet (N @ P)`, where:\n-                -   N is the number of blocks\n-                -   P is the number of penultimate filters\n-        num_blocks: Number of repeated blocks of the NASNet model.\n-            NASNet models use the notation `NASNet (N @ P)`, where:\n-                -   N is the number of blocks\n-                -   P is the number of penultimate filters\n-        stem_block_filters: Number of filters in the initial stem block\n-        skip_reduction: Whether to skip the reduction step at the tail\n-            end of the network.\n-        filter_multiplier: Controls the width of the network.\n-            - If `filter_multiplier` < 1.0, proportionally decreases the number\n-                of filters in each layer.\n-            - If `filter_multiplier` > 1.0, proportionally increases the number\n-                of filters in each layer.\n-            - If `filter_multiplier` = 1, default number of filters from the\n-                 paper are used at each layer.\n-        include_top: Whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: `None` (random initialization) or\n-            `imagenet` (ImageNet weights)\n-        input_tensor: Optional Keras tensor (i.e. output of\n-            `layers.Input()`)\n-            to use as image input for the model.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model\n-                will be the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a\n-                2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: Optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-        default_size: Specifies the default image size of the model\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: In case of invalid argument for `weights`,\n-            invalid input shape or invalid `penultimate_filters` value.\n-        RuntimeError: If attempting to run this model with a\n-            backend that does not support separable convolutions.\n-    '''\n-    if K.backend() in ['cntk', 'theano']:\n-        raise RuntimeError('Only TensorFlow backend is currently supported, '\n-                           'as other backends do not support '\n-                           'separable convolution.')\n-\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as ImageNet with `include_top` '\n-                         'as true, `classes` should be 1000')\n-\n-    if (isinstance(input_shape, tuple) and\n-            None in input_shape and\n-            weights == 'imagenet'):\n-        raise ValueError('When specifying the input shape of a NASNet'\n-                         ' and loading `ImageNet` weights, '\n-                         'the input_shape argument must be static '\n-                         '(no None entries). Got: `input_shape=' +\n-                         str(input_shape) + '`.')\n-\n-    if default_size is None:\n-        default_size = 331\n-\n-    # Determine proper input shape and default size.\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=default_size,\n-                                      min_size=32,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=False,\n-                                      weights=weights)\n-\n-    if K.image_data_format() != 'channels_last':\n-        warnings.warn('The NASNet family of models is only available '\n-                      'for the input data format \"channels_last\" '\n-                      '(width, height, channels). '\n-                      'However your settings specify the default '\n-                      'data format \"channels_first\" (channels, width, height).'\n-                      ' You should set `image_data_format=\"channels_last\"` '\n-                      'in your Keras config located at ~/.keras/keras.json. '\n-                      'The model being returned right now will expect inputs '\n-                      'to follow the \"channels_last\" data format.')\n-        K.set_image_data_format('channels_last')\n-        old_data_format = 'channels_first'\n-    else:\n-        old_data_format = None\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    if penultimate_filters % 24 != 0:\n-        raise ValueError(\n-            'For NASNet-A models, the value of `penultimate_filters` '\n-            'needs to be divisible by 24. Current value: %d' %\n-            penultimate_filters)\n-\n-    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n-    filters = penultimate_filters // 24\n-\n-    x = Conv2D(stem_block_filters, (3, 3), strides=(2, 2), padding='valid',\n-               use_bias=False, name='stem_conv1',\n-               kernel_initializer='he_normal')(img_input)\n-\n-    x = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                           epsilon=1e-3, name='stem_bn1')(x)\n-\n-    p = None\n-    x, p = _reduction_a_cell(x, p, filters // (filter_multiplier ** 2),\n-                             block_id='stem_1')\n-    x, p = _reduction_a_cell(x, p, filters // filter_multiplier,\n-                             block_id='stem_2')\n-\n-    for i in range(num_blocks):\n-        x, p = _normal_a_cell(x, p, filters, block_id='%d' % (i))\n-\n-    x, p0 = _reduction_a_cell(x, p, filters * filter_multiplier,\n-                              block_id='reduce_%d' % (num_blocks))\n-\n-    p = p0 if not skip_reduction else p\n-\n-    for i in range(num_blocks):\n-        x, p = _normal_a_cell(x, p, filters * filter_multiplier,\n-                              block_id='%d' % (num_blocks + i + 1))\n-\n-    x, p0 = _reduction_a_cell(x, p, filters * filter_multiplier ** 2,\n-                              block_id='reduce_%d' % (2 * num_blocks))\n-\n-    p = p0 if not skip_reduction else p\n-\n-    for i in range(num_blocks):\n-        x, p = _normal_a_cell(x, p, filters * filter_multiplier ** 2,\n-                              block_id='%d' % (2 * num_blocks + i + 1))\n-\n-    x = Activation('relu')(x)\n-\n-    if include_top:\n-        x = GlobalAveragePooling2D()(x)\n-        x = Dense(classes, activation='softmax', name='predictions')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-\n-    model = Model(inputs, x, name='NASNet')\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if default_size == 224:  # mobile version\n-            if include_top:\n-                weights_path = get_file(\n-                    'nasnet_mobile.h5',\n-                    NASNET_MOBILE_WEIGHT_PATH,\n-                    cache_subdir='models',\n-                    file_hash='020fb642bf7360b370c678b08e0adf61')\n-            else:\n-                weights_path = get_file(\n-                    'nasnet_mobile_no_top.h5',\n-                    NASNET_MOBILE_WEIGHT_PATH_NO_TOP,\n-                    cache_subdir='models',\n-                    file_hash='1ed92395b5b598bdda52abe5c0dbfd63')\n-            model.load_weights(weights_path)\n-        elif default_size == 331:  # large version\n-            if include_top:\n-                weights_path = get_file(\n-                    'nasnet_large.h5',\n-                    NASNET_LARGE_WEIGHT_PATH,\n-                    cache_subdir='models',\n-                    file_hash='11577c9a518f0070763c2b964a382f17')\n-            else:\n-                weights_path = get_file(\n-                    'nasnet_large_no_top.h5',\n-                    NASNET_LARGE_WEIGHT_PATH_NO_TOP,\n-                    cache_subdir='models',\n-                    file_hash='d81d89dc07e6e56530c4e77faddd61b5')\n-            model.load_weights(weights_path)\n-        else:\n-            raise ValueError(\n-                'ImageNet weights can only be loaded with NASNetLarge'\n-                ' or NASNetMobile')\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    if old_data_format:\n-        K.set_image_data_format(old_data_format)\n-\n-    return model\n-\n-\n-def NASNetLarge(input_shape=None,\n-                include_top=True,\n-                weights='imagenet',\n-                input_tensor=None,\n-                pooling=None,\n-                classes=1000):\n-    '''Instantiates a NASNet model in ImageNet mode.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        input_shape: Optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(331, 331, 3)` for NASNetLarge.\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 32.\n-            E.g. `(224, 224, 3)` would be one valid value.\n-        include_top: Whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: `None` (random initialization) or\n-            `imagenet` (ImageNet weights)\n-        input_tensor: Optional Keras tensor (i.e. output of\n-            `layers.Input()`)\n-            to use as image input for the model.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model\n-                will be the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a\n-                2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: Optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-        RuntimeError: If attempting to run this model with a\n-            backend that does not support separable convolutions.\n-    '''\n-    return NASNet(input_shape,\n-                  penultimate_filters=4032,\n-                  num_blocks=6,\n-                  stem_block_filters=96,\n-                  skip_reduction=True,\n-                  filter_multiplier=2,\n-                  include_top=include_top,\n-                  weights=weights,\n-                  input_tensor=input_tensor,\n-                  pooling=pooling,\n-                  classes=classes,\n-                  default_size=331)\n-\n-\n-def NASNetMobile(input_shape=None,\n-                 include_top=True,\n-                 weights='imagenet',\n-                 input_tensor=None,\n-                 pooling=None,\n-                 classes=1000):\n-    '''Instantiates a Mobile NASNet model in ImageNet mode.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        input_shape: Optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(224, 224, 3)` for NASNetMobile\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 32.\n-            E.g. `(224, 224, 3)` would be one valid value.\n-        include_top: Whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: `None` (random initialization) or\n-            `imagenet` (ImageNet weights)\n-        input_tensor: Optional Keras tensor (i.e. output of\n-            `layers.Input()`)\n-            to use as image input for the model.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model\n-                will be the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a\n-                2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: Optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: In case of invalid argument for `weights`,\n-            or invalid input shape.\n-        RuntimeError: If attempting to run this model with a\n-            backend that does not support separable convolutions.\n-    '''\n-    return NASNet(input_shape,\n-                  penultimate_filters=1056,\n-                  num_blocks=4,\n-                  stem_block_filters=32,\n-                  skip_reduction=False,\n-                  filter_multiplier=2,\n-                  include_top=include_top,\n-                  weights=weights,\n-                  input_tensor=input_tensor,\n-                  pooling=pooling,\n-                  classes=classes,\n-                  default_size=224)\n-\n-\n-def _separable_conv_block(ip, filters,\n-                          kernel_size=(3, 3),\n-                          strides=(1, 1),\n-                          block_id=None):\n-    '''Adds 2 blocks of [relu-separable conv-batchnorm].\n-\n-    # Arguments\n-        ip: Input tensor\n-        filters: Number of output filters per layer\n-        kernel_size: Kernel size of separable convolutions\n-        strides: Strided convolution for downsampling\n-        block_id: String block_id\n-\n-    # Returns\n-        A Keras tensor\n-    '''\n-    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n-\n-    with K.name_scope('separable_conv_block_%s' % block_id):\n-        x = Activation('relu')(ip)\n-        x = SeparableConv2D(filters, kernel_size, strides=strides,\n-                            name='separable_conv_1_%s' % block_id,\n-                            padding='same', use_bias=False,\n-                            kernel_initializer='he_normal')(x)\n-        x = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                               epsilon=1e-3,\n-                               name='separable_conv_1_bn_%s' % (block_id))(x)\n-        x = Activation('relu')(x)\n-        x = SeparableConv2D(filters, kernel_size,\n-                            name='separable_conv_2_%s' % block_id,\n-                            padding='same', use_bias=False,\n-                            kernel_initializer='he_normal')(x)\n-        x = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                               epsilon=1e-3,\n-                               name='separable_conv_2_bn_%s' % (block_id))(x)\n-    return x\n-\n-\n-def _adjust_block(p, ip, filters, block_id=None):\n-    '''Adjusts the input `previous path` to match the shape of the `input`.\n-\n-    Used in situations where the output number of filters needs to be changed.\n-\n-    # Arguments\n-        p: Input tensor which needs to be modified\n-        ip: Input tensor whose shape needs to be matched\n-        filters: Number of output filters to be matched\n-        block_id: String block_id\n-\n-    # Returns\n-        Adjusted Keras tensor\n-    '''\n-    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n-    img_dim = 2 if K.image_data_format() == 'channels_first' else -2\n-\n-    ip_shape = K.int_shape(ip)\n-\n-    if p is not None:\n-        p_shape = K.int_shape(p)\n-\n-    with K.name_scope('adjust_block'):\n-        if p is None:\n-            p = ip\n-\n-        elif p_shape[img_dim] != ip_shape[img_dim]:\n-            with K.name_scope('adjust_reduction_block_%s' % block_id):\n-                p = Activation('relu', name='adjust_relu_1_%s' % block_id)(p)\n-\n-                p1 = AveragePooling2D((1, 1), strides=(2, 2), padding='valid',\n-                                      name='adjust_avg_pool_1_%s' % block_id)(p)\n-                p1 = Conv2D(filters // 2, (1, 1), padding='same',\n-                            use_bias=False, name='adjust_conv_1_%s' % block_id,\n-                            kernel_initializer='he_normal')(p1)\n-\n-                p2 = ZeroPadding2D(padding=((0, 1), (0, 1)))(p)\n-                p2 = Cropping2D(cropping=((1, 0), (1, 0)))(p2)\n-                p2 = AveragePooling2D((1, 1), strides=(2, 2), padding='valid',\n-                                      name='adjust_avg_pool_2_%s' % block_id)(\n-                    p2)\n-                p2 = Conv2D(filters // 2, (1, 1), padding='same',\n-                            use_bias=False, name='adjust_conv_2_%s' % block_id,\n-                            kernel_initializer='he_normal')(p2)\n-\n-                p = concatenate([p1, p2], axis=channel_dim)\n-                p = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                                       epsilon=1e-3,\n-                                       name='adjust_bn_%s' % block_id)(p)\n-\n-        elif p_shape[channel_dim] != filters:\n-            with K.name_scope('adjust_projection_block_%s' % block_id):\n-                p = Activation('relu')(p)\n-                p = Conv2D(filters, (1, 1), strides=(1, 1), padding='same',\n-                           name='adjust_conv_projection_%s' % block_id,\n-                           use_bias=False, kernel_initializer='he_normal')(p)\n-                p = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                                       epsilon=1e-3,\n-                                       name='adjust_bn_%s' % block_id)(p)\n-    return p\n-\n-\n-def _normal_a_cell(ip, p, filters, block_id=None):\n-    '''Adds a Normal cell for NASNet-A (Fig. 4 in the paper).\n-\n-    # Arguments\n-        ip: Input tensor `x`\n-        p: Input tensor `p`\n-        filters: Number of output filters\n-        block_id: String block_id\n-\n-    # Returns\n-        A Keras tensor\n-    '''\n-    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n-\n-    with K.name_scope('normal_A_block_%s' % block_id):\n-        p = _adjust_block(p, ip, filters, block_id)\n-\n-        h = Activation('relu')(ip)\n-        h = Conv2D(filters, (1, 1), strides=(1, 1), padding='same',\n-                   name='normal_conv_1_%s' % block_id,\n-                   use_bias=False, kernel_initializer='he_normal')(h)\n-        h = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                               epsilon=1e-3,\n-                               name='normal_bn_1_%s' % block_id)(h)\n-\n-        with K.name_scope('block_1'):\n-            x1_1 = _separable_conv_block(h, filters, kernel_size=(5, 5),\n-                                         block_id='normal_left1_%s' % block_id)\n-            x1_2 = _separable_conv_block(p, filters,\n-                                         block_id='normal_right1_%s' % block_id)\n-            x1 = add([x1_1, x1_2], name='normal_add_1_%s' % block_id)\n-\n-        with K.name_scope('block_2'):\n-            x2_1 = _separable_conv_block(p, filters, (5, 5),\n-                                         block_id='normal_left2_%s' % block_id)\n-            x2_2 = _separable_conv_block(p, filters, (3, 3),\n-                                         block_id='normal_right2_%s' % block_id)\n-            x2 = add([x2_1, x2_2], name='normal_add_2_%s' % block_id)\n-\n-        with K.name_scope('block_3'):\n-            x3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same',\n-                                  name='normal_left3_%s' % (block_id))(h)\n-            x3 = add([x3, p], name='normal_add_3_%s' % block_id)\n-\n-        with K.name_scope('block_4'):\n-            x4_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same',\n-                                    name='normal_left4_%s' % (block_id))(p)\n-            x4_2 = AveragePooling2D((3, 3), strides=(1, 1), padding='same',\n-                                    name='normal_right4_%s' % (block_id))(p)\n-            x4 = add([x4_1, x4_2], name='normal_add_4_%s' % block_id)\n-\n-        with K.name_scope('block_5'):\n-            x5 = _separable_conv_block(h, filters,\n-                                       block_id='normal_left5_%s' % block_id)\n-            x5 = add([x5, h], name='normal_add_5_%s' % block_id)\n-\n-        x = concatenate([p, x1, x2, x3, x4, x5], axis=channel_dim,\n-                        name='normal_concat_%s' % block_id)\n-    return x, ip\n-\n-\n-def _reduction_a_cell(ip, p, filters, block_id=None):\n-    '''Adds a Reduction cell for NASNet-A (Fig. 4 in the paper).\n-\n-    # Arguments\n-        ip: Input tensor `x`\n-        p: Input tensor `p`\n-        filters: Number of output filters\n-        block_id: String block_id\n-\n-    # Returns\n-        A Keras tensor\n-    '''\n-    channel_dim = 1 if K.image_data_format() == 'channels_first' else -1\n-\n-    with K.name_scope('reduction_A_block_%s' % block_id):\n-        p = _adjust_block(p, ip, filters, block_id)\n-\n-        h = Activation('relu')(ip)\n-        h = Conv2D(filters, (1, 1), strides=(1, 1), padding='same',\n-                   name='reduction_conv_1_%s' % block_id,\n-                   use_bias=False, kernel_initializer='he_normal')(h)\n-        h = BatchNormalization(axis=channel_dim, momentum=0.9997,\n-                               epsilon=1e-3,\n-                               name='reduction_bn_1_%s' % block_id)(h)\n-\n-        with K.name_scope('block_1'):\n-            x1_1 = _separable_conv_block(h, filters, (5, 5), strides=(2, 2),\n-                                         block_id='reduction_left1_%s' % block_id)\n-            x1_2 = _separable_conv_block(p, filters, (7, 7), strides=(2, 2),\n-                                         block_id='reduction_right1_%s' % block_id)\n-            x1 = add([x1_1, x1_2], name='reduction_add_1_%s' % block_id)\n-\n-        with K.name_scope('block_2'):\n-            x2_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same',\n-                                name='reduction_left2_%s' % block_id)(h)\n-            x2_2 = _separable_conv_block(p, filters, (7, 7), strides=(2, 2),\n-                                         block_id='reduction_right2_%s' % block_id)\n-            x2 = add([x2_1, x2_2], name='reduction_add_2_%s' % block_id)\n-\n-        with K.name_scope('block_3'):\n-            x3_1 = AveragePooling2D((3, 3), strides=(2, 2), padding='same',\n-                                    name='reduction_left3_%s' % block_id)(h)\n-            x3_2 = _separable_conv_block(p, filters, (5, 5), strides=(2, 2),\n-                                         block_id='reduction_right3_%s' % block_id)\n-            x3 = add([x3_1, x3_2], name='reduction_add3_%s' % block_id)\n-\n-        with K.name_scope('block_4'):\n-            x4 = AveragePooling2D((3, 3), strides=(1, 1), padding='same',\n-                                  name='reduction_left4_%s' % block_id)(x1)\n-            x4 = add([x2, x4])\n-\n-        with K.name_scope('block_5'):\n-            x5_1 = _separable_conv_block(x1, filters, (3, 3),\n-                                         block_id='reduction_left4_%s' % block_id)\n-            x5_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='same',\n-                                name='reduction_right5_%s' % block_id)(h)\n-            x5 = add([x5_1, x5_2], name='reduction_add4_%s' % block_id)\n-\n-        x = concatenate([x2, x3, x4, x5], axis=channel_dim,\n-                        name='reduction_concat_%s' % block_id)\n-        return x, ip\n+NASNetMobile = nasnet.NASNetMobile\n+NASNetLarge = nasnet.NASNetLarge\n+decode_predictions = nasnet.decode_predictions\n+preprocess_input = nasnet.preprocess_input\n\n@@ -1,274 +1,9 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"ResNet50 model for Keras.\n-\n-# Reference:\n-\n-- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n-\n-Adapted from code contributed by BigMoyan.\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import resnet50\n \n-from ..layers import Input\n-from .. import layers\n-from ..layers import Dense\n-from ..layers import Activation\n-from ..layers import Flatten\n-from ..layers import Conv2D\n-from ..layers import MaxPooling2D\n-from ..layers import AveragePooling2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..layers import ZeroPadding2D\n-from ..layers import BatchNormalization\n-from ..models import Model\n-from .. import backend as K\n-from ..engine import get_source_inputs\n-from ..utils import layer_utils\n-from ..utils.data_utils import get_file\n-from .imagenet_utils import decode_predictions\n-from .imagenet_utils import preprocess_input\n-from .imagenet_utils import _obtain_input_shape\n-\n-\n-WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n-WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-\n-\n-def identity_block(input_tensor, kernel_size, filters, stage, block):\n-    \"\"\"The identity block is the block that has no conv layer at shortcut.\n-\n-    # Arguments\n-        input_tensor: input tensor\n-        kernel_size: default 3, the kernel size of middle conv layer at main path\n-        filters: list of integers, the filters of 3 conv layer at main path\n-        stage: integer, current stage label, used for generating layer names\n-        block: 'a','b'..., current block label, used for generating layer names\n-\n-    # Returns\n-        Output tensor for the block.\n-    \"\"\"\n-    filters1, filters2, filters3 = filters\n-    if K.image_data_format() == 'channels_last':\n-        bn_axis = 3\n-    else:\n-        bn_axis = 1\n-    conv_name_base = 'res' + str(stage) + block + '_branch'\n-    bn_name_base = 'bn' + str(stage) + block + '_branch'\n-\n-    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n-    x = Activation('relu')(x)\n-\n-    x = Conv2D(filters2, kernel_size,\n-               padding='same', name=conv_name_base + '2b')(x)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n-    x = Activation('relu')(x)\n-\n-    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n-\n-    x = layers.add([x, input_tensor])\n-    x = Activation('relu')(x)\n-    return x\n-\n-\n-def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n-    \"\"\"A block that has a conv layer at shortcut.\n-\n-    # Arguments\n-        input_tensor: input tensor\n-        kernel_size: default 3, the kernel size of middle conv layer at main path\n-        filters: list of integers, the filters of 3 conv layer at main path\n-        stage: integer, current stage label, used for generating layer names\n-        block: 'a','b'..., current block label, used for generating layer names\n-        strides: Strides for the first conv layer in the block.\n-\n-    # Returns\n-        Output tensor for the block.\n-\n-    Note that from stage 3,\n-    the first conv layer at main path is with strides=(2, 2)\n-    And the shortcut should have strides=(2, 2) as well\n-    \"\"\"\n-    filters1, filters2, filters3 = filters\n-    if K.image_data_format() == 'channels_last':\n-        bn_axis = 3\n-    else:\n-        bn_axis = 1\n-    conv_name_base = 'res' + str(stage) + block + '_branch'\n-    bn_name_base = 'bn' + str(stage) + block + '_branch'\n-\n-    x = Conv2D(filters1, (1, 1), strides=strides,\n-               name=conv_name_base + '2a')(input_tensor)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n-    x = Activation('relu')(x)\n-\n-    x = Conv2D(filters2, kernel_size, padding='same',\n-               name=conv_name_base + '2b')(x)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n-    x = Activation('relu')(x)\n-\n-    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n-    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n-\n-    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n-                      name=conv_name_base + '1')(input_tensor)\n-    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n-\n-    x = layers.add([x, shortcut])\n-    x = Activation('relu')(x)\n-    return x\n-\n-\n-def ResNet50(include_top=True, weights='imagenet',\n-             input_tensor=None, input_shape=None,\n-             pooling=None,\n-             classes=1000):\n-    \"\"\"Instantiates the ResNet50 architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(224, 224, 3)` (with `channels_last` data format)\n-            or `(3, 224, 224)` (with `channels_first` data format).\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 197.\n-            E.g. `(200, 200, 3)` would be one valid value.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=224,\n-                                      min_size=197,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=include_top,\n-                                      weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-    if K.image_data_format() == 'channels_last':\n-        bn_axis = 3\n-    else:\n-        bn_axis = 1\n-\n-    x = ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n-    x = Conv2D(64, (7, 7), strides=(2, 2), padding='valid', name='conv1')(x)\n-    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n-    x = Activation('relu')(x)\n-    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n-\n-    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n-    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n-    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n-\n-    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n-    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n-    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n-    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n-\n-    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n-    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n-    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n-    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n-    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n-    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n-\n-    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n-    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n-    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n-\n-    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n-\n-    if include_top:\n-        x = Flatten()(x)\n-        x = Dense(classes, activation='softmax', name='fc1000')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-    # Create model.\n-    model = Model(inputs, x, name='resnet50')\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if include_top:\n-            weights_path = get_file(\n-                'resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n-                WEIGHTS_PATH,\n-                cache_subdir='models',\n-                md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n-        else:\n-            weights_path = get_file(\n-                'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                WEIGHTS_PATH_NO_TOP,\n-                cache_subdir='models',\n-                md5_hash='a268eb855778b3df3c7506639542a6af')\n-        model.load_weights(weights_path)\n-        if K.backend() == 'theano':\n-            layer_utils.convert_all_kernels_in_model(model)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    return model\n+ResNet50 = resnet50.ResNet50\n+decode_predictions = resnet50.decode_predictions\n+preprocess_input = resnet50.preprocess_input\n\n@@ -1,179 +1,9 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"VGG16 model for Keras.\n-\n-# Reference\n-\n-- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n-\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import vgg16\n \n-from ..models import Model\n-from ..layers import Flatten\n-from ..layers import Dense\n-from ..layers import Input\n-from ..layers import Conv2D\n-from ..layers import MaxPooling2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..engine import get_source_inputs\n-from ..utils import layer_utils\n-from ..utils.data_utils import get_file\n-from .. import backend as K\n-from .imagenet_utils import decode_predictions\n-from .imagenet_utils import preprocess_input\n-from .imagenet_utils import _obtain_input_shape\n-\n-\n-WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n-WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-\n-\n-def VGG16(include_top=True, weights='imagenet',\n-          input_tensor=None, input_shape=None,\n-          pooling=None,\n-          classes=1000):\n-    \"\"\"Instantiates the VGG16 architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        include_top: whether to include the 3 fully-connected\n-            layers at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(224, 224, 3)` (with `channels_last` data format)\n-            or `(3, 224, 224)` (with `channels_first` data format).\n-            It should have exactly 3 input channels,\n-            and width and height should be no smaller than 48.\n-            E.g. `(200, 200, 3)` would be one valid value.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=224,\n-                                      min_size=48,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=include_top,\n-                                      weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-    # Block 1\n-    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n-    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n-\n-    # Block 2\n-    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n-    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n-\n-    # Block 3\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n-\n-    # Block 4\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n-\n-    # Block 5\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n-\n-    if include_top:\n-        # Classification block\n-        x = Flatten(name='flatten')(x)\n-        x = Dense(4096, activation='relu', name='fc1')(x)\n-        x = Dense(4096, activation='relu', name='fc2')(x)\n-        x = Dense(classes, activation='softmax', name='predictions')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-    # Create model.\n-    model = Model(inputs, x, name='vgg16')\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if include_top:\n-            weights_path = get_file(\n-                'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n-                WEIGHTS_PATH,\n-                cache_subdir='models',\n-                file_hash='64373286793e3c8b2b4e3219cbf3544b')\n-        else:\n-            weights_path = get_file(\n-                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                WEIGHTS_PATH_NO_TOP,\n-                cache_subdir='models',\n-                file_hash='6d6bbae143d832006294945121d1f1fc')\n-        model.load_weights(weights_path)\n-        if K.backend() == 'theano':\n-            layer_utils.convert_all_kernels_in_model(model)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    return model\n+VGG16 = vgg16.VGG16\n+decode_predictions = vgg16.decode_predictions\n+preprocess_input = vgg16.preprocess_input\n\n@@ -1,182 +1,9 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"VGG19 model for Keras.\n-\n-# Reference\n-\n-- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n-\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import vgg19\n \n-from ..models import Model\n-from ..layers import Flatten\n-from ..layers import Dense\n-from ..layers import Input\n-from ..layers import Conv2D\n-from ..layers import MaxPooling2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..engine import get_source_inputs\n-from ..utils import layer_utils\n-from ..utils.data_utils import get_file\n-from .. import backend as K\n-from .imagenet_utils import decode_predictions\n-from .imagenet_utils import preprocess_input\n-from .imagenet_utils import _obtain_input_shape\n-\n-\n-WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n-WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-\n-\n-def VGG19(include_top=True, weights='imagenet',\n-          input_tensor=None, input_shape=None,\n-          pooling=None,\n-          classes=1000):\n-    \"\"\"Instantiates the VGG19 architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet.\n-    Note that the data format convention used by the model is\n-    the one specified in your Keras config at `~/.keras/keras.json`.\n-    When using TensorFlow, for best performance you should\n-    set `\"image_data_format\": \"channels_last\"` in the config.\n-\n-    # Arguments\n-        include_top: whether to include the 3 fully-connected\n-            layers at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(224, 224, 3)` (with `channels_last` data format)\n-            or `(3, 224, 224)` (with `channels_first` data format).\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 48.\n-            E.g. `(200, 200, 3)` would be one valid value.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=224,\n-                                      min_size=48,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=include_top,\n-                                      weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-    # Block 1\n-    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n-    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n-\n-    # Block 2\n-    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n-    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n-\n-    # Block 3\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n-    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n-\n-    # Block 4\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n-\n-    # Block 5\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n-    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n-    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n-\n-    if include_top:\n-        # Classification block\n-        x = Flatten(name='flatten')(x)\n-        x = Dense(4096, activation='relu', name='fc1')(x)\n-        x = Dense(4096, activation='relu', name='fc2')(x)\n-        x = Dense(classes, activation='softmax', name='predictions')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-    # Create model.\n-    model = Model(inputs, x, name='vgg19')\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if include_top:\n-            weights_path = get_file(\n-                'vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n-                WEIGHTS_PATH,\n-                cache_subdir='models',\n-                file_hash='cbe5617147190e668d6c5d5026f83318')\n-        else:\n-            weights_path = get_file(\n-                'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                WEIGHTS_PATH_NO_TOP,\n-                cache_subdir='models',\n-                file_hash='253f8cb515780f3b799900260a226db6')\n-        model.load_weights(weights_path)\n-        if K.backend() == 'theano':\n-            layer_utils.convert_all_kernels_in_model(model)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    return model\n+VGG19 = vgg19.VGG19\n+decode_predictions = vgg19.decode_predictions\n+preprocess_input = vgg19.preprocess_input\n\n@@ -1,276 +1,9 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"Xception V1 model for Keras.\n-\n-On ImageNet, this model gets to a top-1 validation accuracy of 0.790\n-and a top-5 validation accuracy of 0.945.\n-\n-Do note that the input image format for this model is different than for\n-the VGG16 and ResNet models (299x299 instead of 224x224),\n-and that the input preprocessing function\n-is also different (same as Inception V3).\n-\n-# Reference\n-\n-- [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)\n-\n-\"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import os\n-import warnings\n+from keras_applications import xception\n \n-from ..models import Model\n-from .. import layers\n-from ..layers import Dense\n-from ..layers import Input\n-from ..layers import BatchNormalization\n-from ..layers import Activation\n-from ..layers import Conv2D\n-from ..layers import SeparableConv2D\n-from ..layers import MaxPooling2D\n-from ..layers import GlobalAveragePooling2D\n-from ..layers import GlobalMaxPooling2D\n-from ..engine import get_source_inputs\n-from ..utils.data_utils import get_file\n-from ..utils import layer_utils\n-from .. import backend as K\n-from . import imagenet_utils\n-from .imagenet_utils import decode_predictions\n-from .imagenet_utils import _obtain_input_shape\n-\n-\n-TF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels.h5'\n-TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n-\n-\n-def Xception(include_top=True, weights='imagenet',\n-             input_tensor=None, input_shape=None,\n-             pooling=None,\n-             classes=1000):\n-    \"\"\"Instantiates the Xception architecture.\n-\n-    Optionally loads weights pre-trained on ImageNet. This model can\n-    only be used with the data format `(width, height, channels)`.\n-    You should set `image_data_format='channels_last'` in your Keras config\n-    located at ~/.keras/keras.json.\n-\n-    Note that the default input image size for this model is 299x299.\n-\n-    # Arguments\n-        include_top: whether to include the fully-connected\n-            layer at the top of the network.\n-        weights: one of `None` (random initialization),\n-              'imagenet' (pre-training on ImageNet),\n-              or the path to the weights file to be loaded.\n-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n-            to use as image input for the model.\n-        input_shape: optional shape tuple, only to be specified\n-            if `include_top` is False (otherwise the input shape\n-            has to be `(299, 299, 3)`.\n-            It should have exactly 3 inputs channels,\n-            and width and height should be no smaller than 71.\n-            E.g. `(150, 150, 3)` would be one valid value.\n-        pooling: Optional pooling mode for feature extraction\n-            when `include_top` is `False`.\n-            - `None` means that the output of the model will be\n-                the 4D tensor output of the\n-                last convolutional layer.\n-            - `avg` means that global average pooling\n-                will be applied to the output of the\n-                last convolutional layer, and thus\n-                the output of the model will be a 2D tensor.\n-            - `max` means that global max pooling will\n-                be applied.\n-        classes: optional number of classes to classify images\n-            into, only to be specified if `include_top` is True, and\n-            if no `weights` argument is specified.\n-\n-    # Returns\n-        A Keras model instance.\n-\n-    # Raises\n-        ValueError: in case of invalid argument for `weights`,\n-            or invalid input shape.\n-        RuntimeError: If attempting to run this model with a\n-            backend that does not support separable convolutions.\n-    \"\"\"\n-    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n-        raise ValueError('The `weights` argument should be either '\n-                         '`None` (random initialization), `imagenet` '\n-                         '(pre-training on ImageNet), '\n-                         'or the path to the weights file to be loaded.')\n-\n-    if weights == 'imagenet' and include_top and classes != 1000:\n-        raise ValueError('If using `weights` as imagenet with `include_top`'\n-                         ' as true, `classes` should be 1000')\n-\n-    if K.image_data_format() != 'channels_last':\n-        warnings.warn('The Xception model is only available for the '\n-                      'input data format \"channels_last\" '\n-                      '(width, height, channels). '\n-                      'However your settings specify the default '\n-                      'data format \"channels_first\" (channels, width, height). '\n-                      'You should set `image_data_format=\"channels_last\"` in your Keras '\n-                      'config located at ~/.keras/keras.json. '\n-                      'The model being returned right now will expect inputs '\n-                      'to follow the \"channels_last\" data format.')\n-        K.set_image_data_format('channels_last')\n-        old_data_format = 'channels_first'\n-    else:\n-        old_data_format = None\n-\n-    # Determine proper input shape\n-    input_shape = _obtain_input_shape(input_shape,\n-                                      default_size=299,\n-                                      min_size=71,\n-                                      data_format=K.image_data_format(),\n-                                      require_flatten=False,\n-                                      weights=weights)\n-\n-    if input_tensor is None:\n-        img_input = Input(shape=input_shape)\n-    else:\n-        if not K.is_keras_tensor(input_tensor):\n-            img_input = Input(tensor=input_tensor, shape=input_shape)\n-        else:\n-            img_input = input_tensor\n-\n-    x = Conv2D(32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')(img_input)\n-    x = BatchNormalization(name='block1_conv1_bn')(x)\n-    x = Activation('relu', name='block1_conv1_act')(x)\n-    x = Conv2D(64, (3, 3), use_bias=False, name='block1_conv2')(x)\n-    x = BatchNormalization(name='block1_conv2_bn')(x)\n-    x = Activation('relu', name='block1_conv2_act')(x)\n-\n-    residual = Conv2D(128, (1, 1), strides=(2, 2),\n-                      padding='same', use_bias=False)(x)\n-    residual = BatchNormalization()(residual)\n-\n-    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)\n-    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n-    x = Activation('relu', name='block2_sepconv2_act')(x)\n-    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)\n-    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n-\n-    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n-    x = layers.add([x, residual])\n-\n-    residual = Conv2D(256, (1, 1), strides=(2, 2),\n-                      padding='same', use_bias=False)(x)\n-    residual = BatchNormalization()(residual)\n-\n-    x = Activation('relu', name='block3_sepconv1_act')(x)\n-    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)\n-    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n-    x = Activation('relu', name='block3_sepconv2_act')(x)\n-    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)\n-    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n-\n-    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n-    x = layers.add([x, residual])\n-\n-    residual = Conv2D(728, (1, 1), strides=(2, 2),\n-                      padding='same', use_bias=False)(x)\n-    residual = BatchNormalization()(residual)\n-\n-    x = Activation('relu', name='block4_sepconv1_act')(x)\n-    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)\n-    x = BatchNormalization(name='block4_sepconv1_bn')(x)\n-    x = Activation('relu', name='block4_sepconv2_act')(x)\n-    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)\n-    x = BatchNormalization(name='block4_sepconv2_bn')(x)\n-\n-    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n-    x = layers.add([x, residual])\n-\n-    for i in range(8):\n-        residual = x\n-        prefix = 'block' + str(i + 5)\n-\n-        x = Activation('relu', name=prefix + '_sepconv1_act')(x)\n-        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')(x)\n-        x = BatchNormalization(name=prefix + '_sepconv1_bn')(x)\n-        x = Activation('relu', name=prefix + '_sepconv2_act')(x)\n-        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')(x)\n-        x = BatchNormalization(name=prefix + '_sepconv2_bn')(x)\n-        x = Activation('relu', name=prefix + '_sepconv3_act')(x)\n-        x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n-        x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n-\n-        x = layers.add([x, residual])\n-\n-    residual = Conv2D(1024, (1, 1), strides=(2, 2),\n-                      padding='same', use_bias=False)(x)\n-    residual = BatchNormalization()(residual)\n-\n-    x = Activation('relu', name='block13_sepconv1_act')(x)\n-    x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')(x)\n-    x = BatchNormalization(name='block13_sepconv1_bn')(x)\n-    x = Activation('relu', name='block13_sepconv2_act')(x)\n-    x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')(x)\n-    x = BatchNormalization(name='block13_sepconv2_bn')(x)\n-\n-    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n-    x = layers.add([x, residual])\n-\n-    x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n-    x = BatchNormalization(name='block14_sepconv1_bn')(x)\n-    x = Activation('relu', name='block14_sepconv1_act')(x)\n-\n-    x = SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n-    x = BatchNormalization(name='block14_sepconv2_bn')(x)\n-    x = Activation('relu', name='block14_sepconv2_act')(x)\n-\n-    if include_top:\n-        x = GlobalAveragePooling2D(name='avg_pool')(x)\n-        x = Dense(classes, activation='softmax', name='predictions')(x)\n-    else:\n-        if pooling == 'avg':\n-            x = GlobalAveragePooling2D()(x)\n-        elif pooling == 'max':\n-            x = GlobalMaxPooling2D()(x)\n-\n-    # Ensure that the model takes into account\n-    # any potential predecessors of `input_tensor`.\n-    if input_tensor is not None:\n-        inputs = get_source_inputs(input_tensor)\n-    else:\n-        inputs = img_input\n-    # Create model.\n-    model = Model(inputs, x, name='xception')\n-\n-    # load weights\n-    if weights == 'imagenet':\n-        if include_top:\n-            weights_path = get_file('xception_weights_tf_dim_ordering_tf_kernels.h5',\n-                                    TF_WEIGHTS_PATH,\n-                                    cache_subdir='models',\n-                                    file_hash='0a58e3b7378bc2990ea3b43d5981f1f6')\n-        else:\n-            weights_path = get_file('xception_weights_tf_dim_ordering_tf_kernels_notop.h5',\n-                                    TF_WEIGHTS_PATH_NO_TOP,\n-                                    cache_subdir='models',\n-                                    file_hash='b0042744bf5b25fce3cb969f33bebb97')\n-        model.load_weights(weights_path)\n-        if K.backend() == 'theano':\n-            layer_utils.convert_all_kernels_in_model(model)\n-    elif weights is not None:\n-        model.load_weights(weights)\n-\n-    if old_data_format:\n-        K.set_image_data_format(old_data_format)\n-    return model\n-\n-\n-def preprocess_input(x):\n-    \"\"\"Preprocesses a numpy array encoding a batch of images.\n-\n-    # Arguments\n-        x: a 4D numpy array consists of RGB values within [0, 255].\n-\n-    # Returns\n-        Preprocessed array.\n-    \"\"\"\n-    return imagenet_utils.preprocess_input(x, mode='tf')\n+Xception = xception.Xception\n+decode_predictions = xception.decode_predictions\n+preprocess_input = xception.preprocess_input\n\n@@ -1,3 +1,14 @@\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from .. import backend\n+from .. import utils\n+\n+import keras_preprocessing\n+\n+keras_preprocessing.set_keras_submodules(backend=backend, utils=utils)\n+\n from . import image\n from . import sequence\n from . import text\n\n@@ -1,1760 +1,25 @@\n-\"\"\"Fairly basic set of tools for real-time data augmentation on image data.\n-\n-Can easily be extended to include new transformations,\n-new preprocessing methods, etc...\n+\"\"\"Utilities for real-time data augmentation on image data.\n \"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import numpy as np\n-import re\n-from scipy import linalg\n-import scipy.ndimage as ndi\n-from six.moves import range\n-import os\n-import threading\n-import warnings\n-import multiprocessing.pool\n-from functools import partial\n-\n-from .. import backend as K\n-from ..utils.data_utils import Sequence\n-\n-try:\n-    from PIL import ImageEnhance\n-    from PIL import Image as pil_image\n-except ImportError:\n-    pil_image = None\n-\n-if pil_image is not None:\n-    _PIL_INTERPOLATION_METHODS = {\n-        'nearest': pil_image.NEAREST,\n-        'bilinear': pil_image.BILINEAR,\n-        'bicubic': pil_image.BICUBIC,\n-    }\n-    # These methods were only introduced in version 3.4.0 (2016).\n-    if hasattr(pil_image, 'HAMMING'):\n-        _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n-    if hasattr(pil_image, 'BOX'):\n-        _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n-    # This method is new in version 1.1.3 (2013).\n-    if hasattr(pil_image, 'LANCZOS'):\n-        _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n-\n-\n-def random_rotation(x, rg, row_axis=1, col_axis=2, channel_axis=0,\n-                    fill_mode='nearest', cval=0.):\n-    \"\"\"Performs a random rotation of a Numpy image tensor.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        rg: Rotation range, in degrees.\n-        row_axis: Index of axis for rows in the input tensor.\n-        col_axis: Index of axis for columns in the input tensor.\n-        channel_axis: Index of axis for channels in the input tensor.\n-        fill_mode: Points outside the boundaries of the input\n-            are filled according to the given mode\n-            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n-        cval: Value used for points outside the boundaries\n-            of the input if `mode='constant'`.\n-\n-    # Returns\n-        Rotated Numpy image tensor.\n-    \"\"\"\n-    theta = np.random.uniform(-rg, rg)\n-    x = apply_affine_transform(x, theta=theta, channel_axis=channel_axis,\n-                               fill_mode=fill_mode, cval=cval)\n-    return x\n-\n-\n-def random_shift(x, wrg, hrg, row_axis=1, col_axis=2, channel_axis=0,\n-                 fill_mode='nearest', cval=0.):\n-    \"\"\"Performs a random spatial shift of a Numpy image tensor.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        wrg: Width shift range, as a float fraction of the width.\n-        hrg: Height shift range, as a float fraction of the height.\n-        row_axis: Index of axis for rows in the input tensor.\n-        col_axis: Index of axis for columns in the input tensor.\n-        channel_axis: Index of axis for channels in the input tensor.\n-        fill_mode: Points outside the boundaries of the input\n-            are filled according to the given mode\n-            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n-        cval: Value used for points outside the boundaries\n-            of the input if `mode='constant'`.\n-\n-    # Returns\n-        Shifted Numpy image tensor.\n-    \"\"\"\n-    h, w = x.shape[row_axis], x.shape[col_axis]\n-    tx = np.random.uniform(-hrg, hrg) * h\n-    ty = np.random.uniform(-wrg, wrg) * w\n-    x = apply_affine_transform(x, tx=tx, ty=ty, channel_axis=channel_axis,\n-                               fill_mode=fill_mode, cval=cval)\n-    return x\n-\n-\n-def random_shear(x, intensity, row_axis=1, col_axis=2, channel_axis=0,\n-                 fill_mode='nearest', cval=0.):\n-    \"\"\"Performs a random spatial shear of a Numpy image tensor.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        intensity: Transformation intensity in degrees.\n-        row_axis: Index of axis for rows in the input tensor.\n-        col_axis: Index of axis for columns in the input tensor.\n-        channel_axis: Index of axis for channels in the input tensor.\n-        fill_mode: Points outside the boundaries of the input\n-            are filled according to the given mode\n-            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n-        cval: Value used for points outside the boundaries\n-            of the input if `mode='constant'`.\n-\n-    # Returns\n-        Sheared Numpy image tensor.\n-    \"\"\"\n-    shear = np.random.uniform(-intensity, intensity)\n-    x = apply_affine_transform(x, shear=shear, channel_axis=channel_axis,\n-                               fill_mode=fill_mode, cval=cval)\n-    return x\n-\n-\n-def random_zoom(x, zoom_range, row_axis=1, col_axis=2, channel_axis=0,\n-                fill_mode='nearest', cval=0.):\n-    \"\"\"Performs a random spatial zoom of a Numpy image tensor.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        zoom_range: Tuple of floats; zoom range for width and height.\n-        row_axis: Index of axis for rows in the input tensor.\n-        col_axis: Index of axis for columns in the input tensor.\n-        channel_axis: Index of axis for channels in the input tensor.\n-        fill_mode: Points outside the boundaries of the input\n-            are filled according to the given mode\n-            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n-        cval: Value used for points outside the boundaries\n-            of the input if `mode='constant'`.\n-\n-    # Returns\n-        Zoomed Numpy image tensor.\n-\n-    # Raises\n-        ValueError: if `zoom_range` isn't a tuple.\n-    \"\"\"\n-    if len(zoom_range) != 2:\n-        raise ValueError('`zoom_range` should be a tuple or list of two'\n-                         ' floats. Received: ', zoom_range)\n-\n-    if zoom_range[0] == 1 and zoom_range[1] == 1:\n-        zx, zy = 1, 1\n-    else:\n-        zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n-    x = apply_affine_transform(x, zx=zx, zy=zy, channel_axis=channel_axis,\n-                               fill_mode=fill_mode, cval=cval)\n-    return x\n-\n-\n-def apply_channel_shift(x, intensity, channel_axis=0):\n-    \"\"\"Performs a channel shift.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        intensity: Transformation intensity.\n-        channel_axis: Index of axis for channels in the input tensor.\n-\n-    # Returns\n-        Numpy image tensor.\n-\n-    \"\"\"\n-    x = np.rollaxis(x, channel_axis, 0)\n-    min_x, max_x = np.min(x), np.max(x)\n-    channel_images = [\n-        np.clip(x_channel + intensity,\n-                min_x,\n-                max_x)\n-        for x_channel in x]\n-    x = np.stack(channel_images, axis=0)\n-    x = np.rollaxis(x, 0, channel_axis + 1)\n-    return x\n-\n-\n-def random_channel_shift(x, intensity_range, channel_axis=0):\n-    \"\"\"Performs a random channel shift.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        intensity_range: Transformation intensity.\n-        channel_axis: Index of axis for channels in the input tensor.\n-\n-    # Returns\n-        Numpy image tensor.\n-    \"\"\"\n-    intensity = np.random.uniform(-intensity_range, intensity_range)\n-    return apply_channel_shift(x, intensity, channel_axis=channel_axis)\n-\n-\n-def apply_brightness_shift(x, brightness):\n-    \"\"\"Performs a brightness shift.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        brightness: Float. The new brightness value.\n-        channel_axis: Index of axis for channels in the input tensor.\n-\n-    # Returns\n-        Numpy image tensor.\n-\n-    # Raises\n-        ValueError if `brightness_range` isn't a tuple.\n-    \"\"\"\n-    x = array_to_img(x)\n-    x = imgenhancer_Brightness = ImageEnhance.Brightness(x)\n-    x = imgenhancer_Brightness.enhance(brightness)\n-    x = img_to_array(x)\n-    return x\n-\n-\n-def random_brightness(x, brightness_range):\n-    \"\"\"Performs a random brightness shift.\n-\n-    # Arguments\n-        x: Input tensor. Must be 3D.\n-        brightness_range: Tuple of floats; brightness range.\n-        channel_axis: Index of axis for channels in the input tensor.\n-\n-    # Returns\n-        Numpy image tensor.\n-\n-    # Raises\n-        ValueError if `brightness_range` isn't a tuple.\n-    \"\"\"\n-    if len(brightness_range) != 2:\n-        raise ValueError(\n-            '`brightness_range should be tuple or list of two floats. '\n-            'Received: %s' % brightness_range)\n-\n-    u = np.random.uniform(brightness_range[0], brightness_range[1])\n-    return apply_brightness_shift(x, u)\n-\n-\n-def transform_matrix_offset_center(matrix, x, y):\n-    o_x = float(x) / 2 + 0.5\n-    o_y = float(y) / 2 + 0.5\n-    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n-    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n-    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n-    return transform_matrix\n-\n-\n-def apply_affine_transform(x, theta=0, tx=0, ty=0, shear=0, zx=1, zy=1,\n-                           row_axis=0, col_axis=1, channel_axis=2,\n-                           fill_mode='nearest', cval=0.):\n-    \"\"\"Applies an affine transformation specified by the parameters given.\n-\n-    # Arguments\n-        x: 2D numpy array, single image.\n-        theta: Rotation angle in degrees.\n-        tx: Width shift.\n-        ty: Heigh shift.\n-        shear: Shear angle in degrees.\n-        zx: Zoom in x direction.\n-        zy: Zoom in y direction\n-        row_axis: Index of axis for rows in the input image.\n-        col_axis: Index of axis for columns in the input image.\n-        channel_axis: Index of axis for channels in the input image.\n-        fill_mode: Points outside the boundaries of the input\n-            are filled according to the given mode\n-            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n-        cval: Value used for points outside the boundaries\n-            of the input if `mode='constant'`.\n-\n-    # Returns\n-        The transformed version of the input.\n-    \"\"\"\n-    transform_matrix = None\n-    if theta != 0:\n-        theta = np.deg2rad(theta)\n-        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n-                                    [np.sin(theta), np.cos(theta), 0],\n-                                    [0, 0, 1]])\n-        transform_matrix = rotation_matrix\n-\n-    if tx != 0 or ty != 0:\n-        shift_matrix = np.array([[1, 0, tx],\n-                                 [0, 1, ty],\n-                                 [0, 0, 1]])\n-        transform_matrix = shift_matrix if transform_matrix is None else np.dot(transform_matrix, shift_matrix)\n-\n-    if shear != 0:\n-        shear = np.deg2rad(shear)\n-        shear_matrix = np.array([[1, -np.sin(shear), 0],\n-                                 [0, np.cos(shear), 0],\n-                                 [0, 0, 1]])\n-        transform_matrix = shear_matrix if transform_matrix is None else np.dot(transform_matrix, shear_matrix)\n-\n-    if zx != 1 or zy != 1:\n-        zoom_matrix = np.array([[zx, 0, 0],\n-                                [0, zy, 0],\n-                                [0, 0, 1]])\n-        transform_matrix = zoom_matrix if transform_matrix is None else np.dot(transform_matrix, zoom_matrix)\n-\n-    if transform_matrix is not None:\n-        h, w = x.shape[row_axis], x.shape[col_axis]\n-        transform_matrix = transform_matrix_offset_center(\n-            transform_matrix, h, w)\n-        x = np.rollaxis(x, channel_axis, 0)\n-        final_affine_matrix = transform_matrix[:2, :2]\n-        final_offset = transform_matrix[:2, 2]\n-\n-        channel_images = [ndi.interpolation.affine_transform(\n-            x_channel,\n-            final_affine_matrix,\n-            final_offset,\n-            order=1,\n-            mode=fill_mode,\n-            cval=cval) for x_channel in x]\n-        x = np.stack(channel_images, axis=0)\n-        x = np.rollaxis(x, 0, channel_axis + 1)\n-    return x\n-\n-\n-def flip_axis(x, axis):\n-    x = np.asarray(x).swapaxes(axis, 0)\n-    x = x[::-1, ...]\n-    x = x.swapaxes(0, axis)\n-    return x\n-\n-\n-def array_to_img(x, data_format=None, scale=True):\n-    \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n-\n-    # Arguments\n-        x: Input Numpy array.\n-        data_format: Image data format.\n-            either \"channels_first\" or \"channels_last\".\n-        scale: Whether to rescale image values\n-            to be within `[0, 255]`.\n-\n-    # Returns\n-        A PIL Image instance.\n-\n-    # Raises\n-        ImportError: if PIL is not available.\n-        ValueError: if invalid `x` or `data_format` is passed.\n-    \"\"\"\n-    if pil_image is None:\n-        raise ImportError('Could not import PIL.Image. '\n-                          'The use of `array_to_img` requires PIL.')\n-    x = np.asarray(x, dtype=K.floatx())\n-    if x.ndim != 3:\n-        raise ValueError('Expected image array to have rank 3 (single image). '\n-                         'Got array with shape:', x.shape)\n-\n-    if data_format is None:\n-        data_format = K.image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Invalid data_format:', data_format)\n-\n-    # Original Numpy array x has format (height, width, channel)\n-    # or (channel, height, width)\n-    # but target PIL image has format (width, height, channel)\n-    if data_format == 'channels_first':\n-        x = x.transpose(1, 2, 0)\n-    if scale:\n-        x = x + max(-np.min(x), 0)\n-        x_max = np.max(x)\n-        if x_max != 0:\n-            x /= x_max\n-        x *= 255\n-    if x.shape[2] == 3:\n-        # RGB\n-        return pil_image.fromarray(x.astype('uint8'), 'RGB')\n-    elif x.shape[2] == 1:\n-        # grayscale\n-        return pil_image.fromarray(x[:, :, 0].astype('uint8'), 'L')\n-    else:\n-        raise ValueError('Unsupported channel number: ', x.shape[2])\n-\n-\n-def img_to_array(img, data_format=None):\n-    \"\"\"Converts a PIL Image instance to a Numpy array.\n-\n-    # Arguments\n-        img: PIL Image instance.\n-        data_format: Image data format,\n-            either \"channels_first\" or \"channels_last\".\n-\n-    # Returns\n-        A 3D Numpy array.\n-\n-    # Raises\n-        ValueError: if invalid `img` or `data_format` is passed.\n-    \"\"\"\n-    if data_format is None:\n-        data_format = K.image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ', data_format)\n-    # Numpy array x has format (height, width, channel)\n-    # or (channel, height, width)\n-    # but original PIL image has format (width, height, channel)\n-    x = np.asarray(img, dtype=K.floatx())\n-    if len(x.shape) == 3:\n-        if data_format == 'channels_first':\n-            x = x.transpose(2, 0, 1)\n-    elif len(x.shape) == 2:\n-        if data_format == 'channels_first':\n-            x = x.reshape((1, x.shape[0], x.shape[1]))\n-        else:\n-            x = x.reshape((x.shape[0], x.shape[1], 1))\n-    else:\n-        raise ValueError('Unsupported image shape: ', x.shape)\n-    return x\n-\n-\n-def save_img(path,\n-             x,\n-             data_format=None,\n-             file_format=None,\n-             scale=True, **kwargs):\n-    \"\"\"Saves an image stored as a Numpy array to a path or file object.\n-\n-    # Arguments\n-        path: Path or file object.\n-        x: Numpy array.\n-        data_format: Image data format,\n-            either \"channels_first\" or \"channels_last\".\n-        file_format: Optional file format override. If omitted, the\n-            format to use is determined from the filename extension.\n-            If a file object was used instead of a filename, this\n-            parameter should always be used.\n-        scale: Whether to rescale image values to be within `[0, 255]`.\n-        **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.\n-    \"\"\"\n-    img = array_to_img(x, data_format=data_format, scale=scale)\n-    img.save(path, format=file_format, **kwargs)\n-\n-\n-def load_img(path, grayscale=False, target_size=None,\n-             interpolation='nearest'):\n-    \"\"\"Loads an image into PIL format.\n-\n-    # Arguments\n-        path: Path to image file.\n-        grayscale: Boolean, whether to load the image as grayscale.\n-        target_size: Either `None` (default to original size)\n-            or tuple of ints `(img_height, img_width)`.\n-        interpolation: Interpolation method used to resample the image if the\n-            target size is different from that of the loaded image.\n-            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n-            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n-            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n-            \"hamming\" are also supported. By default, \"nearest\" is used.\n-\n-    # Returns\n-        A PIL Image instance.\n-\n-    # Raises\n-        ImportError: if PIL is not available.\n-        ValueError: if interpolation method is not supported.\n-    \"\"\"\n-    if pil_image is None:\n-        raise ImportError('Could not import PIL.Image. '\n-                          'The use of `array_to_img` requires PIL.')\n-    img = pil_image.open(path)\n-    if grayscale:\n-        if img.mode != 'L':\n-            img = img.convert('L')\n-    else:\n-        if img.mode != 'RGB':\n-            img = img.convert('RGB')\n-    if target_size is not None:\n-        width_height_tuple = (target_size[1], target_size[0])\n-        if img.size != width_height_tuple:\n-            if interpolation not in _PIL_INTERPOLATION_METHODS:\n-                raise ValueError(\n-                    'Invalid interpolation method {} specified. Supported '\n-                    'methods are {}'.format(\n-                        interpolation,\n-                        \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n-            resample = _PIL_INTERPOLATION_METHODS[interpolation]\n-            img = img.resize(width_height_tuple, resample)\n-    return img\n-\n-\n-def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n-    return [os.path.join(root, f)\n-            for root, _, files in os.walk(directory) for f in files\n-            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]\n-\n-\n-class ImageDataGenerator(object):\n-    \"\"\"Generate batches of tensor image data with real-time data augmentation.\n-     The data will be looped over (in batches).\n-\n-    # Arguments\n-        featurewise_center: Boolean.\n-            Set input mean to 0 over the dataset, feature-wise.\n-        samplewise_center: Boolean. Set each sample mean to 0.\n-        featurewise_std_normalization: Boolean.\n-            Divide inputs by std of the dataset, feature-wise.\n-        samplewise_std_normalization: Boolean. Divide each input by its std.\n-        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n-        zca_whitening: Boolean. Apply ZCA whitening.\n-        rotation_range: Int. Degree range for random rotations.\n-        width_shift_range: Float, 1-D array-like or int\n-            - float: fraction of total width, if < 1, or pixels if >= 1.\n-            - 1-D array-like: random elements from the array.\n-            - int: integer number of pixels from interval\n-                `(-width_shift_range, +width_shift_range)`\n-            - With `width_shift_range=2` possible values\n-                are integers `[-1, 0, +1]`,\n-                same as with `width_shift_range=[-1, 0, +1]`,\n-                while with `width_shift_range=1.0` possible values are floats in\n-                the interval [-1.0, +1.0).\n-        height_shift_range: Float, 1-D array-like or int\n-            - float: fraction of total height, if < 1, or pixels if >= 1.\n-            - 1-D array-like: random elements from the array.\n-            - int: integer number of pixels from interval\n-                `(-height_shift_range, +height_shift_range)`\n-            - With `height_shift_range=2` possible values\n-                are integers `[-1, 0, +1]`,\n-                same as with `height_shift_range=[-1, 0, +1]`,\n-                while with `height_shift_range=1.0` possible values are floats in\n-                the interval [-1.0, +1.0).\n-        shear_range: Float. Shear Intensity\n-            (Shear angle in counter-clockwise direction in degrees)\n-        zoom_range: Float or [lower, upper]. Range for random zoom.\n-            If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n-        channel_shift_range: Float. Range for random channel shifts.\n-        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n-            Default is 'nearest'.\n-            Points outside the boundaries of the input are filled\n-            according to the given mode:\n-            - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n-            - 'nearest':  aaaaaaaa|abcd|dddddddd\n-            - 'reflect':  abcddcba|abcd|dcbaabcd\n-            - 'wrap':  abcdabcd|abcd|abcdabcd\n-        cval: Float or Int.\n-            Value used for points outside the boundaries\n-            when `fill_mode = \"constant\"`.\n-        horizontal_flip: Boolean. Randomly flip inputs horizontally.\n-        vertical_flip: Boolean. Randomly flip inputs vertically.\n-        rescale: rescaling factor. Defaults to None.\n-            If None or 0, no rescaling is applied,\n-            otherwise we multiply the data by the value provided\n-            (before applying any other transformation).\n-        preprocessing_function: function that will be implied on each input.\n-            The function will run after the image is resized and augmented.\n-            The function should take one argument:\n-            one image (Numpy tensor with rank 3),\n-            and should output a Numpy tensor with the same shape.\n-        data_format: Image data format,\n-            either \"channels_first\" or \"channels_last\".\n-            \"channels_last\" mode means that the images should have shape\n-            `(samples, height, width, channels)`,\n-            \"channels_first\" mode means that the images should have shape\n-            `(samples, channels, height, width)`.\n-            It defaults to the `image_data_format` value found in your\n-            Keras config file at `~/.keras/keras.json`.\n-            If you never set it, then it will be \"channels_last\".\n-        validation_split: Float. Fraction of images reserved for validation\n-            (strictly between 0 and 1).\n-\n-    # Examples\n-    Example of using `.flow(x, y)`:\n-\n-    ```python\n-    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-    y_train = np_utils.to_categorical(y_train, num_classes)\n-    y_test = np_utils.to_categorical(y_test, num_classes)\n-\n-    datagen = ImageDataGenerator(\n-        featurewise_center=True,\n-        featurewise_std_normalization=True,\n-        rotation_range=20,\n-        width_shift_range=0.2,\n-        height_shift_range=0.2,\n-        horizontal_flip=True)\n-\n-    # compute quantities required for featurewise normalization\n-    # (std, mean, and principal components if ZCA whitening is applied)\n-    datagen.fit(x_train)\n-\n-    # fits the model on batches with real-time data augmentation:\n-    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n-                        steps_per_epoch=len(x_train) / 32, epochs=epochs)\n-\n-    # here's a more \"manual\" example\n-    for e in range(epochs):\n-        print('Epoch', e)\n-        batches = 0\n-        for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n-            model.fit(x_batch, y_batch)\n-            batches += 1\n-            if batches >= len(x_train) / 32:\n-                # we need to break the loop by hand because\n-                # the generator loops indefinitely\n-                break\n-    ```\n-    Example of using `.flow_from_directory(directory)`:\n-\n-    ```python\n-    train_datagen = ImageDataGenerator(\n-            rescale=1./255,\n-            shear_range=0.2,\n-            zoom_range=0.2,\n-            horizontal_flip=True)\n-\n-    test_datagen = ImageDataGenerator(rescale=1./255)\n-\n-    train_generator = train_datagen.flow_from_directory(\n-            'data/train',\n-            target_size=(150, 150),\n-            batch_size=32,\n-            class_mode='binary')\n-\n-    validation_generator = test_datagen.flow_from_directory(\n-            'data/validation',\n-            target_size=(150, 150),\n-            batch_size=32,\n-            class_mode='binary')\n-\n-    model.fit_generator(\n-            train_generator,\n-            steps_per_epoch=2000,\n-            epochs=50,\n-            validation_data=validation_generator,\n-            validation_steps=800)\n-    ```\n-\n-    Example of transforming images and masks together.\n-\n-    ```python\n-    # we create two instances with the same arguments\n-    data_gen_args = dict(featurewise_center=True,\n-                         featurewise_std_normalization=True,\n-                         rotation_range=90.,\n-                         width_shift_range=0.1,\n-                         height_shift_range=0.1,\n-                         zoom_range=0.2)\n-    image_datagen = ImageDataGenerator(**data_gen_args)\n-    mask_datagen = ImageDataGenerator(**data_gen_args)\n-\n-    # Provide the same seed and keyword arguments to the fit and flow methods\n-    seed = 1\n-    image_datagen.fit(images, augment=True, seed=seed)\n-    mask_datagen.fit(masks, augment=True, seed=seed)\n-\n-    image_generator = image_datagen.flow_from_directory(\n-        'data/images',\n-        class_mode=None,\n-        seed=seed)\n-\n-    mask_generator = mask_datagen.flow_from_directory(\n-        'data/masks',\n-        class_mode=None,\n-        seed=seed)\n-\n-    # combine generators into one which yields image and masks\n-    train_generator = zip(image_generator, mask_generator)\n-\n-    model.fit_generator(\n-        train_generator,\n-        steps_per_epoch=2000,\n-        epochs=50)\n-    ```\n-    \"\"\"\n-\n-    def __init__(self,\n-                 featurewise_center=False,\n-                 samplewise_center=False,\n-                 featurewise_std_normalization=False,\n-                 samplewise_std_normalization=False,\n-                 zca_whitening=False,\n-                 zca_epsilon=1e-6,\n-                 rotation_range=0.,\n-                 width_shift_range=0.,\n-                 height_shift_range=0.,\n-                 brightness_range=None,\n-                 shear_range=0.,\n-                 zoom_range=0.,\n-                 channel_shift_range=0.,\n-                 fill_mode='nearest',\n-                 cval=0.,\n-                 horizontal_flip=False,\n-                 vertical_flip=False,\n-                 rescale=None,\n-                 preprocessing_function=None,\n-                 data_format=None,\n-                 validation_split=0.0):\n-        if data_format is None:\n-            data_format = K.image_data_format()\n-        self.featurewise_center = featurewise_center\n-        self.samplewise_center = samplewise_center\n-        self.featurewise_std_normalization = featurewise_std_normalization\n-        self.samplewise_std_normalization = samplewise_std_normalization\n-        self.zca_whitening = zca_whitening\n-        self.zca_epsilon = zca_epsilon\n-        self.rotation_range = rotation_range\n-        self.width_shift_range = width_shift_range\n-        self.height_shift_range = height_shift_range\n-        self.brightness_range = brightness_range\n-        self.shear_range = shear_range\n-        self.zoom_range = zoom_range\n-        self.channel_shift_range = channel_shift_range\n-        self.fill_mode = fill_mode\n-        self.cval = cval\n-        self.horizontal_flip = horizontal_flip\n-        self.vertical_flip = vertical_flip\n-        self.rescale = rescale\n-        self.preprocessing_function = preprocessing_function\n-\n-        if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError(\n-                '`data_format` should be `\"channels_last\"` '\n-                '(channel after row and column) or '\n-                '`\"channels_first\"` (channel before row and column). '\n-                'Received: %s' % data_format)\n-        self.data_format = data_format\n-        if data_format == 'channels_first':\n-            self.channel_axis = 1\n-            self.row_axis = 2\n-            self.col_axis = 3\n-        if data_format == 'channels_last':\n-            self.channel_axis = 3\n-            self.row_axis = 1\n-            self.col_axis = 2\n-        if validation_split and not 0 < validation_split < 1:\n-            raise ValueError(\n-                '`validation_split` must be strictly between 0 and 1. '\n-                ' Received: %s' % validation_split)\n-        self._validation_split = validation_split\n-\n-        self.mean = None\n-        self.std = None\n-        self.principal_components = None\n-\n-        if np.isscalar(zoom_range):\n-            self.zoom_range = [1 - zoom_range, 1 + zoom_range]\n-        elif len(zoom_range) == 2:\n-            self.zoom_range = [zoom_range[0], zoom_range[1]]\n-        else:\n-            raise ValueError('`zoom_range` should be a float or '\n-                             'a tuple or list of two floats. '\n-                             'Received: %s' % zoom_range)\n-        if zca_whitening:\n-            if not featurewise_center:\n-                self.featurewise_center = True\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`zca_whitening`, which overrides '\n-                              'setting of `featurewise_center`.')\n-            if featurewise_std_normalization:\n-                self.featurewise_std_normalization = False\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`zca_whitening` '\n-                              'which overrides setting of'\n-                              '`featurewise_std_normalization`.')\n-        if featurewise_std_normalization:\n-            if not featurewise_center:\n-                self.featurewise_center = True\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`featurewise_std_normalization`, '\n-                              'which overrides setting of '\n-                              '`featurewise_center`.')\n-        if samplewise_std_normalization:\n-            if not samplewise_center:\n-                self.samplewise_center = True\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`samplewise_std_normalization`, '\n-                              'which overrides setting of '\n-                              '`samplewise_center`.')\n-\n-    def flow(self, x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None,\n-             save_to_dir=None, save_prefix='', save_format='png', subset=None):\n-        \"\"\"Takes numpy data & label arrays, and generates batches of augmented data.\n-\n-        # Arguments\n-            x: Input data. Numpy array of rank 4 or a tuple.\n-                If tuple, the first element\n-                should contain the images and the second element\n-                another numpy array or a list of numpy arrays\n-                that gets passed to the output\n-                without any modifications.\n-                Can be used to feed the model miscellaneous data\n-                along with the images.\n-                In case of grayscale data, the channels axis of the image array\n-                should have value 1, and in case\n-                of RGB data, it should have value 3.\n-            y: Labels.\n-            batch_size: Int (default: 32).\n-            shuffle: Boolean (default: True).\n-            sample_weight: Sample weights.\n-            seed: Int (default: None).\n-            save_to_dir: None or str (default: None).\n-                This allows you to optionally specify a directory\n-                to which to save the augmented pictures being generated\n-                (useful for visualizing what you are doing).\n-            save_prefix: Str (default: `''`).\n-                Prefix to use for filenames of saved pictures\n-                (only relevant if `save_to_dir` is set).\n-                save_format: one of \"png\", \"jpeg\"\n-                (only relevant if `save_to_dir` is set). Default: \"png\".\n-            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n-                `validation_split` is set in `ImageDataGenerator`.\n-\n-        # Returns\n-            An `Iterator` yielding tuples of `(x, y)`\n-                where `x` is a numpy array of image data\n-                (in the case of a single image input) or a list\n-                of numpy arrays (in the case with\n-                additional inputs) and `y` is a numpy array\n-                of corresponding labels. If 'sample_weight' is not None,\n-                the yielded tuples are of the form `(x, y, sample_weight)`.\n-                If `y` is None, only the numpy array `x` is returned.\n-        \"\"\"\n-        return NumpyArrayIterator(\n-            x, y, self,\n-            batch_size=batch_size,\n-            shuffle=shuffle,\n-            sample_weight=sample_weight,\n-            seed=seed,\n-            data_format=self.data_format,\n-            save_to_dir=save_to_dir,\n-            save_prefix=save_prefix,\n-            save_format=save_format,\n-            subset=subset)\n-\n-    def flow_from_directory(self, directory,\n-                            target_size=(256, 256), color_mode='rgb',\n-                            classes=None, class_mode='categorical',\n-                            batch_size=32, shuffle=True, seed=None,\n-                            save_to_dir=None,\n-                            save_prefix='',\n-                            save_format='png',\n-                            follow_links=False,\n-                            subset=None,\n-                            interpolation='nearest'):\n-        \"\"\"Takes the path to a directory & generates batches of augmented data.\n-\n-        # Arguments\n-            directory: Path to the target directory.\n-                It should contain one subdirectory per class.\n-                Any PNG, JPG, BMP, PPM or TIF images\n-                inside each of the subdirectories directory tree\n-                will be included in the generator.\n-                See [this script](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n-                for more details.\n-            target_size: Tuple of integers `(height, width)`,\n-                default: `(256, 256)`.\n-                The dimensions to which all images found will be resized.\n-            color_mode: One of \"grayscale\", \"rbg\". Default: \"rgb\".\n-                Whether the images will be converted to\n-                have 1 or 3 color channels.\n-            classes: Optional list of class subdirectories\n-                (e.g. `['dogs', 'cats']`). Default: None.\n-                If not provided, the list of classes will be automatically\n-                inferred from the subdirectory names/structure\n-                under `directory`, where each subdirectory will\n-                be treated as a different class\n-                (and the order of the classes, which will map to the label\n-                indices, will be alphanumeric).\n-                The dictionary containing the mapping from class names to class\n-                indices can be obtained via the attribute `class_indices`.\n-            class_mode: One of \"categorical\", \"binary\", \"sparse\",\n-                \"input\", or None. Default: \"categorical\".\n-                Determines the type of label arrays that are returned:\n-                - \"categorical\" will be 2D one-hot encoded labels,\n-                - \"binary\" will be 1D binary labels,\n-                    \"sparse\" will be 1D integer labels,\n-                - \"input\" will be images identical\n-                    to input images (mainly used to work with autoencoders).\n-                - If None, no labels are returned\n-                  (the generator will only yield batches of image data,\n-                  which is useful to use with `model.predict_generator()`,\n-                  `model.evaluate_generator()`, etc.).\n-                  Please note that in case of class_mode None,\n-                  the data still needs to reside in a subdirectory\n-                  of `directory` for it to work correctly.\n-            batch_size: Size of the batches of data (default: 32).\n-            shuffle: Whether to shuffle the data (default: True)\n-            seed: Optional random seed for shuffling and transformations.\n-            save_to_dir: None or str (default: None).\n-                This allows you to optionally specify\n-                a directory to which to save\n-                the augmented pictures being generated\n-                (useful for visualizing what you are doing).\n-            save_prefix: Str. Prefix to use for filenames of saved pictures\n-                (only relevant if `save_to_dir` is set).\n-            save_format: One of \"png\", \"jpeg\"\n-                (only relevant if `save_to_dir` is set). Default: \"png\".\n-            follow_links: Whether to follow symlinks inside\n-                class subdirectories (default: False).\n-            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n-                `validation_split` is set in `ImageDataGenerator`.\n-            interpolation: Interpolation method used to\n-                resample the image if the\n-                target size is different from that of the loaded image.\n-                Supported methods are `\"nearest\"`, `\"bilinear\"`,\n-                and `\"bicubic\"`.\n-                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n-                supported. If PIL version 3.4.0 or newer is installed,\n-                `\"box\"` and `\"hamming\"` are also supported.\n-                By default, `\"nearest\"` is used.\n-\n-        # Returns\n-            A `DirectoryIterator` yielding tuples of `(x, y)`\n-                where `x` is a numpy array containing a batch\n-                of images with shape `(batch_size, *target_size, channels)`\n-                and `y` is a numpy array of corresponding labels.\n-        \"\"\"\n-        return DirectoryIterator(\n-            directory, self,\n-            target_size=target_size, color_mode=color_mode,\n-            classes=classes, class_mode=class_mode,\n-            data_format=self.data_format,\n-            batch_size=batch_size, shuffle=shuffle, seed=seed,\n-            save_to_dir=save_to_dir,\n-            save_prefix=save_prefix,\n-            save_format=save_format,\n-            follow_links=follow_links,\n-            subset=subset,\n-            interpolation=interpolation)\n-\n-    def standardize(self, x):\n-        \"\"\"Applies the normalization configuration to a batch of inputs.\n-\n-        # Arguments\n-            x: Batch of inputs to be normalized.\n-\n-        # Returns\n-            The inputs, normalized.\n-        \"\"\"\n-        if self.preprocessing_function:\n-            x = self.preprocessing_function(x)\n-        if self.rescale:\n-            x *= self.rescale\n-        if self.samplewise_center:\n-            x -= np.mean(x, keepdims=True)\n-        if self.samplewise_std_normalization:\n-            x /= (np.std(x, keepdims=True) + K.epsilon())\n-\n-        if self.featurewise_center:\n-            if self.mean is not None:\n-                x -= self.mean\n-            else:\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`featurewise_center`, but it hasn\\'t '\n-                              'been fit on any training data. Fit it '\n-                              'first by calling `.fit(numpy_data)`.')\n-        if self.featurewise_std_normalization:\n-            if self.std is not None:\n-                x /= (self.std + K.epsilon())\n-            else:\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`featurewise_std_normalization`, '\n-                              'but it hasn\\'t '\n-                              'been fit on any training data. Fit it '\n-                              'first by calling `.fit(numpy_data)`.')\n-        if self.zca_whitening:\n-            if self.principal_components is not None:\n-                flatx = np.reshape(x, (-1, np.prod(x.shape[-3:])))\n-                whitex = np.dot(flatx, self.principal_components)\n-                x = np.reshape(whitex, x.shape)\n-            else:\n-                warnings.warn('This ImageDataGenerator specifies '\n-                              '`zca_whitening`, but it hasn\\'t '\n-                              'been fit on any training data. Fit it '\n-                              'first by calling `.fit(numpy_data)`.')\n-        return x\n-\n-    def get_random_transform(self, img_shape, seed=None):\n-        \"\"\"Generates random parameters for a transformation.\n-\n-        # Arguments\n-            seed: Random seed.\n-            img_shape: Tuple of integers. Shape of the image that is transformed.\n-\n-        # Returns\n-            A dictionary containing randomly chosen parameters describing the\n-            transformation.\n-        \"\"\"\n-        img_row_axis = self.row_axis - 1\n-        img_col_axis = self.col_axis - 1\n-\n-        if seed is not None:\n-            np.random.seed(seed)\n-\n-        if self.rotation_range:\n-            theta = np.random.uniform(\n-                -self.rotation_range,\n-                self.rotation_range)\n-        else:\n-            theta = 0\n-\n-        if self.height_shift_range:\n-            try:  # 1-D array-like or int\n-                tx = np.random.choice(self.height_shift_range)\n-                tx *= np.random.choice([-1, 1])\n-            except ValueError:  # floating point\n-                tx = np.random.uniform(-self.height_shift_range,\n-                                       self.height_shift_range)\n-            if np.max(self.height_shift_range) < 1:\n-                tx *= img_shape[img_row_axis]\n-        else:\n-            tx = 0\n-\n-        if self.width_shift_range:\n-            try:  # 1-D array-like or int\n-                ty = np.random.choice(self.width_shift_range)\n-                ty *= np.random.choice([-1, 1])\n-            except ValueError:  # floating point\n-                ty = np.random.uniform(-self.width_shift_range,\n-                                       self.width_shift_range)\n-            if np.max(self.width_shift_range) < 1:\n-                ty *= img_shape[img_col_axis]\n-        else:\n-            ty = 0\n-\n-        if self.shear_range:\n-            shear = np.random.uniform(\n-                -self.shear_range,\n-                self.shear_range)\n-        else:\n-            shear = 0\n-\n-        if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n-            zx, zy = 1, 1\n-        else:\n-            zx, zy = np.random.uniform(\n-                self.zoom_range[0],\n-                self.zoom_range[1],\n-                2)\n-\n-        flip_horizontal = (np.random.random() < 0.5) * self.horizontal_flip\n-        flip_vertical = (np.random.random() < 0.5) * self.vertical_flip\n-\n-        channel_shift_intensity = None\n-        if self.channel_shift_range != 0:\n-            channel_shift_intensity = np.random.uniform(-self.channel_shift_range,\n-                                                        self.channel_shift_range)\n-\n-        brightness = None\n-        if self.brightness_range is not None:\n-            if len(self.brightness_range) != 2:\n-                raise ValueError(\n-                    '`brightness_range should be tuple or list of two floats. '\n-                    'Received: %s' % brightness_range)\n-            brightness = np.random.uniform(self.brightness_range[0],\n-                                           self.brightness_range[1])\n-\n-        transform_parameters = {'theta': theta,\n-                                'tx': tx,\n-                                'ty': ty,\n-                                'shear': shear,\n-                                'zx': zx,\n-                                'zy': zy,\n-                                'flip_horizontal': flip_horizontal,\n-                                'flip_vertical': flip_vertical,\n-                                'channel_shift_intensity': channel_shift_intensity,\n-                                'brightness': brightness}\n-\n-        return transform_parameters\n-\n-    def apply_transform(self, x, transform_parameters):\n-        \"\"\"Applies a transformation to an image according to given parameters.\n-\n-        # Arguments\n-            x: 3D tensor, single image.\n-            transform_parameters: Dictionary with string - parameter pairs\n-                describing the transformation. Currently, the following parameters\n-                from the dictionary are used:\n-                - `'theta'`: Float. Rotation angle in degrees.\n-                - `'tx'`: Float. Shift in the x direction.\n-                - `'ty'`: Float. Shift in the y direction.\n-                - `'shear'`: Float. Shear angle in degrees.\n-                - `'zx'`: Float. Zoom in the x direction.\n-                - `'zy'`: Float. Zoom in the y direction.\n-                - `'flip_horizontal'`: Boolean. Horizontal flip.\n-                - `'flip_vertical'`: Boolean. Vertical flip.\n-                - `'channel_shift_intencity'`: Float. Channel shift intensity.\n-                - `'brightness'`: Float. Brightness shift intensity.\n-\n-        # Returns\n-            A ransformed version of the input (same shape).\n-        \"\"\"\n-        # x is a single image, so it doesn't have image number at index 0\n-        img_row_axis = self.row_axis - 1\n-        img_col_axis = self.col_axis - 1\n-        img_channel_axis = self.channel_axis - 1\n-\n-        x = apply_affine_transform(x, transform_parameters.get('theta', 0),\n-                                   transform_parameters.get('tx', 0),\n-                                   transform_parameters.get('ty', 0),\n-                                   transform_parameters.get('shear', 0),\n-                                   transform_parameters.get('zx', 1),\n-                                   transform_parameters.get('zy', 1),\n-                                   row_axis=img_row_axis, col_axis=img_col_axis,\n-                                   channel_axis=img_channel_axis,\n-                                   fill_mode=self.fill_mode, cval=self.cval)\n-\n-        if transform_parameters.get('channel_shift_intensity') is not None:\n-            x = apply_channel_shift(x,\n-                                    transform_parameters['channel_shift_intensity'],\n-                                    img_channel_axis)\n-\n-        if transform_parameters.get('flip_horizontal', False):\n-            x = flip_axis(x, img_col_axis)\n-\n-        if transform_parameters.get('flip_vertical', False):\n-            x = flip_axis(x, img_row_axis)\n-\n-        if transform_parameters.get('brightness') is not None:\n-            x = apply_brightness_shift(x, transform_parameters['brightness'])\n-\n-        return x\n-\n-    def random_transform(self, x, seed=None):\n-        \"\"\"Applies a random transformation to an image.\n-\n-        # Arguments\n-            x: 3D tensor, single image.\n-            seed: Random seed.\n-\n-        # Returns\n-            A randomly transformed version of the input (same shape).\n-        \"\"\"\n-        params = self.get_random_transform(x.shape, seed)\n-        return self.apply_transform(x, params)\n-\n-    def fit(self, x,\n-            augment=False,\n-            rounds=1,\n-            seed=None):\n-        \"\"\"Computes the internal data stats related to the data-dependent transformations, based on an array of sample data.\n-\n-        Only required if `featurewise_center` or\n-        `featurewise_std_normalization` or `zca_whitening` are set to True.\n-\n-        # Arguments\n-            x: Sample data. Should have rank 4.\n-             In case of grayscale data,\n-             the channels axis should have value 1, and in case\n-             of RGB data, it should have value 3.\n-            augment: Boolean (default: False).\n-                Whether to fit on randomly augmented samples.\n-            rounds: Int (default: 1).\n-                If using data augmentation (`augment=True`),\n-                this is how many augmentation passes over the data to use.\n-            seed: Int (default: None). Random seed.\n-       \"\"\"\n-        x = np.asarray(x, dtype=K.floatx())\n-        if x.ndim != 4:\n-            raise ValueError('Input to `.fit()` should have rank 4. '\n-                             'Got array with shape: ' + str(x.shape))\n-        if x.shape[self.channel_axis] not in {1, 3, 4}:\n-            warnings.warn(\n-                'Expected input to be images (as Numpy array) '\n-                'following the data format convention \"' +\n-                self.data_format + '\" (channels on axis ' +\n-                str(self.channel_axis) + '), i.e. expected '\n-                'either 1, 3 or 4 channels on axis ' +\n-                str(self.channel_axis) + '. '\n-                'However, it was passed an array with shape ' +\n-                str(x.shape) + ' (' + str(x.shape[self.channel_axis]) +\n-                ' channels).')\n-\n-        if seed is not None:\n-            np.random.seed(seed)\n-\n-        x = np.copy(x)\n-        if augment:\n-            ax = np.zeros(\n-                tuple([rounds * x.shape[0]] + list(x.shape)[1:]),\n-                dtype=K.floatx())\n-            for r in range(rounds):\n-                for i in range(x.shape[0]):\n-                    ax[i + r * x.shape[0]] = self.random_transform(x[i])\n-            x = ax\n-\n-        if self.featurewise_center:\n-            self.mean = np.mean(x, axis=(0, self.row_axis, self.col_axis))\n-            broadcast_shape = [1, 1, 1]\n-            broadcast_shape[self.channel_axis - 1] = x.shape[self.channel_axis]\n-            self.mean = np.reshape(self.mean, broadcast_shape)\n-            x -= self.mean\n-\n-        if self.featurewise_std_normalization:\n-            self.std = np.std(x, axis=(0, self.row_axis, self.col_axis))\n-            broadcast_shape = [1, 1, 1]\n-            broadcast_shape[self.channel_axis - 1] = x.shape[self.channel_axis]\n-            self.std = np.reshape(self.std, broadcast_shape)\n-            x /= (self.std + K.epsilon())\n-\n-        if self.zca_whitening:\n-            flat_x = np.reshape(\n-                x, (x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]))\n-            sigma = np.dot(flat_x.T, flat_x) / flat_x.shape[0]\n-            u, s, _ = linalg.svd(sigma)\n-            s_inv = 1. / np.sqrt(s[np.newaxis] + self.zca_epsilon)\n-            self.principal_components = (u * s_inv).dot(u.T)\n-\n-\n-class Iterator(Sequence):\n-    \"\"\"Base class for image data iterators.\n-\n-    Every `Iterator` must implement the `_get_batches_of_transformed_samples`\n-    method.\n-\n-    # Arguments\n-        n: Integer, total number of samples in the dataset to loop over.\n-        batch_size: Integer, size of a batch.\n-        shuffle: Boolean, whether to shuffle the data between epochs.\n-        seed: Random seeding for data shuffling.\n-    \"\"\"\n-\n-    def __init__(self, n, batch_size, shuffle, seed):\n-        self.n = n\n-        self.batch_size = batch_size\n-        self.seed = seed\n-        self.shuffle = shuffle\n-        self.batch_index = 0\n-        self.total_batches_seen = 0\n-        self.lock = threading.Lock()\n-        self.index_array = None\n-        self.index_generator = self._flow_index()\n-\n-    def _set_index_array(self):\n-        self.index_array = np.arange(self.n)\n-        if self.shuffle:\n-            self.index_array = np.random.permutation(self.n)\n-\n-    def __getitem__(self, idx):\n-        if idx >= len(self):\n-            raise ValueError('Asked to retrieve element {idx}, '\n-                             'but the Sequence '\n-                             'has length {length}'.format(idx=idx,\n-                                                          length=len(self)))\n-        if self.seed is not None:\n-            np.random.seed(self.seed + self.total_batches_seen)\n-        self.total_batches_seen += 1\n-        if self.index_array is None:\n-            self._set_index_array()\n-        index_array = self.index_array[self.batch_size * idx:\n-                                       self.batch_size * (idx + 1)]\n-        return self._get_batches_of_transformed_samples(index_array)\n-\n-    def __len__(self):\n-        return (self.n + self.batch_size - 1) // self.batch_size  # round up\n-\n-    def on_epoch_end(self):\n-        self._set_index_array()\n-\n-    def reset(self):\n-        self.batch_index = 0\n-\n-    def _flow_index(self):\n-        # Ensure self.batch_index is 0.\n-        self.reset()\n-        while 1:\n-            if self.seed is not None:\n-                np.random.seed(self.seed + self.total_batches_seen)\n-            if self.batch_index == 0:\n-                self._set_index_array()\n-\n-            current_index = (self.batch_index * self.batch_size) % self.n\n-            if self.n > current_index + self.batch_size:\n-                self.batch_index += 1\n-            else:\n-                self.batch_index = 0\n-            self.total_batches_seen += 1\n-            yield self.index_array[current_index:\n-                                   current_index + self.batch_size]\n-\n-    def __iter__(self):\n-        # Needed if we want to do something like:\n-        # for x, y in data_gen.flow(...):\n-        return self\n-\n-    def __next__(self, *args, **kwargs):\n-        return self.next(*args, **kwargs)\n-\n-    def _get_batches_of_transformed_samples(self, index_array):\n-        \"\"\"Gets a batch of transformed samples.\n-\n-        # Arguments\n-            index_array: Array of sample indices to include in batch.\n-\n-        # Returns\n-            A batch of transformed samples.\n-        \"\"\"\n-        raise NotImplementedError\n-\n-\n-class NumpyArrayIterator(Iterator):\n-    \"\"\"Iterator yielding data from a Numpy array.\n-\n-    # Arguments\n-        x: Numpy array of input data or tuple.\n-            If tuple, the second elements is either\n-            another numpy array or a list of numpy arrays,\n-            each of which gets passed\n-            through as an output without any modifications.\n-        y: Numpy array of targets data.\n-        image_data_generator: Instance of `ImageDataGenerator`\n-            to use for random transformations and normalization.\n-        batch_size: Integer, size of a batch.\n-        shuffle: Boolean, whether to shuffle the data between epochs.\n-        sample_weight: Numpy array of sample weights.\n-        seed: Random seed for data shuffling.\n-        data_format: String, one of `channels_first`, `channels_last`.\n-        save_to_dir: Optional directory where to save the pictures\n-            being yielded, in a viewable format. This is useful\n-            for visualizing the random transformations being\n-            applied, for debugging purposes.\n-        save_prefix: String prefix to use for saving sample\n-            images (if `save_to_dir` is set).\n-        save_format: Format to use for saving sample images\n-            (if `save_to_dir` is set).\n-        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n-            validation_split is set in ImageDataGenerator.\n-    \"\"\"\n-\n-    def __init__(self, x, y, image_data_generator,\n-                 batch_size=32, shuffle=False, sample_weight=None,\n-                 seed=None, data_format=None,\n-                 save_to_dir=None, save_prefix='', save_format='png',\n-                 subset=None):\n-        if (type(x) is tuple) or (type(x) is list):\n-            if type(x[1]) is not list:\n-                x_misc = [np.asarray(x[1])]\n-            else:\n-                x_misc = [np.asarray(xx) for xx in x[1]]\n-            x = x[0]\n-            for xx in x_misc:\n-                if len(x) != len(xx):\n-                    raise ValueError(\n-                        'All of the arrays in `x` '\n-                        'should have the same length. '\n-                        'Found a pair with: len(x[0]) = %s, len(x[?]) = %s' %\n-                        (len(x), len(xx)))\n-        else:\n-            x_misc = []\n-\n-        if y is not None and len(x) != len(y):\n-            raise ValueError('`x` (images tensor) and `y` (labels) '\n-                             'should have the same length. '\n-                             'Found: x.shape = %s, y.shape = %s' %\n-                             (np.asarray(x).shape, np.asarray(y).shape))\n-        if sample_weight is not None and len(x) != len(sample_weight):\n-            raise ValueError('`x` (images tensor) and `sample_weight` '\n-                             'should have the same length. '\n-                             'Found: x.shape = %s, sample_weight.shape = %s' %\n-                             (np.asarray(x).shape, np.asarray(sample_weight).shape))\n-        if subset is not None:\n-            if subset not in {'training', 'validation'}:\n-                raise ValueError('Invalid subset name:', subset,\n-                                 '; expected \"training\" or \"validation\".')\n-            split_idx = int(len(x) * image_data_generator._validation_split)\n-            if subset == 'validation':\n-                x = x[:split_idx]\n-                x_misc = [np.asarray(xx[:split_idx]) for xx in x_misc]\n-                if y is not None:\n-                    y = y[:split_idx]\n-            else:\n-                x = x[split_idx:]\n-                x_misc = [np.asarray(xx[split_idx:]) for xx in x_misc]\n-                if y is not None:\n-                    y = y[split_idx:]\n-        if data_format is None:\n-            data_format = K.image_data_format()\n-        self.x = np.asarray(x, dtype=K.floatx())\n-        self.x_misc = x_misc\n-        if self.x.ndim != 4:\n-            raise ValueError('Input data in `NumpyArrayIterator` '\n-                             'should have rank 4. You passed an array '\n-                             'with shape', self.x.shape)\n-        channels_axis = 3 if data_format == 'channels_last' else 1\n-        if self.x.shape[channels_axis] not in {1, 3, 4}:\n-            warnings.warn('NumpyArrayIterator is set to use the '\n-                          'data format convention \"' + data_format + '\" '\n-                          '(channels on axis ' + str(channels_axis) +\n-                          '), i.e. expected either 1, 3 or 4 '\n-                          'channels on axis ' + str(channels_axis) + '. '\n-                          'However, it was passed an array with shape ' +\n-                          str(self.x.shape) + ' (' +\n-                          str(self.x.shape[channels_axis]) + ' channels).')\n-        if y is not None:\n-            self.y = np.asarray(y)\n-        else:\n-            self.y = None\n-        if sample_weight is not None:\n-            self.sample_weight = np.asarray(sample_weight)\n-        else:\n-            self.sample_weight = None\n-        self.image_data_generator = image_data_generator\n-        self.data_format = data_format\n-        self.save_to_dir = save_to_dir\n-        self.save_prefix = save_prefix\n-        self.save_format = save_format\n-        super(NumpyArrayIterator, self).__init__(x.shape[0],\n-                                                 batch_size,\n-                                                 shuffle,\n-                                                 seed)\n-\n-    def _get_batches_of_transformed_samples(self, index_array):\n-        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n-                           dtype=K.floatx())\n-        for i, j in enumerate(index_array):\n-            x = self.x[j]\n-            params = self.image_data_generator.get_random_transform(x.shape)\n-            x = self.image_data_generator.apply_transform(\n-                x.astype(K.floatx()), params)\n-            x = self.image_data_generator.standardize(x)\n-            batch_x[i] = x\n-\n-        if self.save_to_dir:\n-            for i, j in enumerate(index_array):\n-                img = array_to_img(batch_x[i], self.data_format, scale=True)\n-                fname = '{prefix}_{index}_{hash}.{format}'.format(\n-                    prefix=self.save_prefix,\n-                    index=j,\n-                    hash=np.random.randint(1e4),\n-                    format=self.save_format)\n-                img.save(os.path.join(self.save_to_dir, fname))\n-        batch_x_miscs = [xx[index_array] for xx in self.x_misc]\n-        output = (batch_x if batch_x_miscs == []\n-                  else [batch_x] + batch_x_miscs,)\n-        if self.y is None:\n-            return output[0]\n-        output += (self.y[index_array],)\n-        if self.sample_weight is not None:\n-            output += (self.sample_weight[index_array],)\n-        return output\n-\n-    def next(self):\n-        \"\"\"For python 2.x.\n-\n-        # Returns\n-            The next batch.\n-        \"\"\"\n-        # Keeps under lock only the mechanism which advances\n-        # the indexing of each batch.\n-        with self.lock:\n-            index_array = next(self.index_generator)\n-        # The transformation of images is not under thread lock\n-        # so it can be done in parallel\n-        return self._get_batches_of_transformed_samples(index_array)\n-\n-\n-def _iter_valid_files(directory, white_list_formats, follow_links):\n-    \"\"\"Iterates on files with extension in `white_list_formats` contained in `directory`.\n-\n-    # Arguments\n-        directory: Absolute path to the directory\n-            containing files to be counted\n-        white_list_formats: Set of strings containing allowed extensions for\n-            the files to be counted.\n-        follow_links: Boolean.\n-\n-    # Yields\n-        Tuple of (root, filename) with extension in `white_list_formats`.\n-    \"\"\"\n-    def _recursive_list(subpath):\n-        return sorted(os.walk(subpath, followlinks=follow_links),\n-                      key=lambda x: x[0])\n-\n-    for root, _, files in _recursive_list(directory):\n-        for fname in sorted(files):\n-            for extension in white_list_formats:\n-                if fname.lower().endswith('.tiff'):\n-                    warnings.warn('Using \\'.tiff\\' files with multiple bands '\n-                                  'will cause distortion. '\n-                                  'Please verify your output.')\n-                if fname.lower().endswith('.' + extension):\n-                    yield root, fname\n-\n-\n-def _count_valid_files_in_directory(directory,\n-                                    white_list_formats,\n-                                    split,\n-                                    follow_links):\n-    \"\"\"Counts files with extension in `white_list_formats` contained in `directory`.\n-\n-    # Arguments\n-        directory: absolute path to the directory\n-            containing files to be counted\n-        white_list_formats: set of strings containing allowed extensions for\n-            the files to be counted.\n-        split: tuple of floats (e.g. `(0.2, 0.6)`) to only take into\n-            account a certain fraction of files in each directory.\n-            E.g.: `segment=(0.6, 1.0)` would only account for last 40 percent\n-            of images in each directory.\n-        follow_links: boolean.\n-\n-    # Returns\n-        the count of files with extension in `white_list_formats` contained in\n-        the directory.\n-    \"\"\"\n-    num_files = len(list(\n-        _iter_valid_files(directory, white_list_formats, follow_links)))\n-    if split:\n-        start, stop = int(split[0] * num_files), int(split[1] * num_files)\n-    else:\n-        start, stop = 0, num_files\n-    return stop - start\n-\n-\n-def _list_valid_filenames_in_directory(directory, white_list_formats, split,\n-                                       class_indices, follow_links):\n-    \"\"\"Lists paths of files in `subdir` with extensions in `white_list_formats`.\n-\n-    # Arguments\n-        directory: absolute path to a directory containing the files to list.\n-            The directory name is used as class label\n-            and must be a key of `class_indices`.\n-        white_list_formats: set of strings containing allowed extensions for\n-            the files to be counted.\n-        split: tuple of floats (e.g. `(0.2, 0.6)`) to only take into\n-            account a certain fraction of files in each directory.\n-            E.g.: `segment=(0.6, 1.0)` would only account for last 40 percent\n-            of images in each directory.\n-        class_indices: dictionary mapping a class name to its index.\n-        follow_links: boolean.\n-\n-    # Returns\n-        classes: a list of class indices\n-        filenames: the path of valid files in `directory`, relative from\n-            `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n-            the filenames will be\n-            `[\"class1/file1.jpg\", \"class1/file2.jpg\", ...]`).\n-    \"\"\"\n-    dirname = os.path.basename(directory)\n-    if split:\n-        num_files = len(list(\n-            _iter_valid_files(directory, white_list_formats, follow_links)))\n-        start, stop = int(split[0] * num_files), int(split[1] * num_files)\n-        valid_files = list(\n-            _iter_valid_files(\n-                directory, white_list_formats, follow_links))[start: stop]\n-    else:\n-        valid_files = _iter_valid_files(\n-            directory, white_list_formats, follow_links)\n-\n-    classes = []\n-    filenames = []\n-    for root, fname in valid_files:\n-        classes.append(class_indices[dirname])\n-        absolute_path = os.path.join(root, fname)\n-        relative_path = os.path.join(\n-            dirname, os.path.relpath(absolute_path, directory))\n-        filenames.append(relative_path)\n-\n-    return classes, filenames\n-\n-\n-class DirectoryIterator(Iterator):\n-    \"\"\"Iterator capable of reading images from a directory on disk.\n-\n-    # Arguments\n-        directory: Path to the directory to read images from.\n-            Each subdirectory in this directory will be\n-            considered to contain images from one class,\n-            or alternatively you could specify class subdirectories\n-            via the `classes` argument.\n-        image_data_generator: Instance of `ImageDataGenerator`\n-            to use for random transformations and normalization.\n-        target_size: tuple of integers, dimensions to resize input images to.\n-        color_mode: One of `\"rgb\"`, `\"grayscale\"`. Color mode to read images.\n-        classes: Optional list of strings, names of subdirectories\n-            containing images from each class (e.g. `[\"dogs\", \"cats\"]`).\n-            It will be computed automatically if not set.\n-        class_mode: Mode for yielding the targets:\n-            `\"binary\"`: binary targets (if there are only two classes),\n-            `\"categorical\"`: categorical targets,\n-            `\"sparse\"`: integer targets,\n-            `\"input\"`: targets are images identical to input images (mainly\n-                used to work with autoencoders),\n-            `None`: no targets get yielded (only input images are yielded).\n-        batch_size: Integer, size of a batch.\n-        shuffle: Boolean, whether to shuffle the data between epochs.\n-        seed: Random seed for data shuffling.\n-        data_format: String, one of `channels_first`, `channels_last`.\n-        save_to_dir: Optional directory where to save the pictures\n-            being yielded, in a viewable format. This is useful\n-            for visualizing the random transformations being\n-            applied, for debugging purposes.\n-        save_prefix: String prefix to use for saving sample\n-            images (if `save_to_dir` is set).\n-        save_format: Format to use for saving sample images\n-            (if `save_to_dir` is set).\n-        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n-            validation_split is set in ImageDataGenerator.\n-        interpolation: Interpolation method used to resample the image if the\n-            target size is different from that of the loaded image.\n-            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n-            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n-            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n-            \"hamming\" are also supported. By default, \"nearest\" is used.\n-    \"\"\"\n-\n-    def __init__(self, directory, image_data_generator,\n-                 target_size=(256, 256), color_mode='rgb',\n-                 classes=None, class_mode='categorical',\n-                 batch_size=32, shuffle=True, seed=None,\n-                 data_format=None,\n-                 save_to_dir=None, save_prefix='', save_format='png',\n-                 follow_links=False,\n-                 subset=None,\n-                 interpolation='nearest'):\n-        if data_format is None:\n-            data_format = K.image_data_format()\n-        self.directory = directory\n-        self.image_data_generator = image_data_generator\n-        self.target_size = tuple(target_size)\n-        if color_mode not in {'rgb', 'grayscale'}:\n-            raise ValueError('Invalid color mode:', color_mode,\n-                             '; expected \"rgb\" or \"grayscale\".')\n-        self.color_mode = color_mode\n-        self.data_format = data_format\n-        if self.color_mode == 'rgb':\n-            if self.data_format == 'channels_last':\n-                self.image_shape = self.target_size + (3,)\n-            else:\n-                self.image_shape = (3,) + self.target_size\n-        else:\n-            if self.data_format == 'channels_last':\n-                self.image_shape = self.target_size + (1,)\n-            else:\n-                self.image_shape = (1,) + self.target_size\n-        self.classes = classes\n-        if class_mode not in {'categorical', 'binary', 'sparse',\n-                              'input', None}:\n-            raise ValueError('Invalid class_mode:', class_mode,\n-                             '; expected one of \"categorical\", '\n-                             '\"binary\", \"sparse\", \"input\"'\n-                             ' or None.')\n-        self.class_mode = class_mode\n-        self.save_to_dir = save_to_dir\n-        self.save_prefix = save_prefix\n-        self.save_format = save_format\n-        self.interpolation = interpolation\n-\n-        if subset is not None:\n-            validation_split = self.image_data_generator._validation_split\n-            if subset == 'validation':\n-                split = (0, validation_split)\n-            elif subset == 'training':\n-                split = (validation_split, 1)\n-            else:\n-                raise ValueError('Invalid subset name: ', subset,\n-                                 '; expected \"training\" or \"validation\"')\n-        else:\n-            split = None\n-        self.subset = subset\n-\n-        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp',\n-                              'ppm', 'tif', 'tiff'}\n-        # First, count the number of samples and classes.\n-        self.samples = 0\n-\n-        if not classes:\n-            classes = []\n-            for subdir in sorted(os.listdir(directory)):\n-                if os.path.isdir(os.path.join(directory, subdir)):\n-                    classes.append(subdir)\n-        self.num_classes = len(classes)\n-        self.class_indices = dict(zip(classes, range(len(classes))))\n-\n-        pool = multiprocessing.pool.ThreadPool()\n-        function_partial = partial(_count_valid_files_in_directory,\n-                                   white_list_formats=white_list_formats,\n-                                   follow_links=follow_links,\n-                                   split=split)\n-        self.samples = sum(pool.map(function_partial,\n-                                    (os.path.join(directory, subdir)\n-                                     for subdir in classes)))\n-\n-        print('Found %d images belonging to %d classes.' %\n-              (self.samples, self.num_classes))\n-\n-        # Second, build an index of the images\n-        # in the different class subfolders.\n-        results = []\n-        self.filenames = []\n-        self.classes = np.zeros((self.samples,), dtype='int32')\n-        i = 0\n-        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n-            results.append(\n-                pool.apply_async(_list_valid_filenames_in_directory,\n-                                 (dirpath, white_list_formats, split,\n-                                  self.class_indices, follow_links)))\n-        for res in results:\n-            classes, filenames = res.get()\n-            self.classes[i:i + len(classes)] = classes\n-            self.filenames += filenames\n-            i += len(classes)\n-\n-        pool.close()\n-        pool.join()\n-        super(DirectoryIterator, self).__init__(self.samples,\n-                                                batch_size,\n-                                                shuffle,\n-                                                seed)\n-\n-    def _get_batches_of_transformed_samples(self, index_array):\n-        batch_x = np.zeros(\n-            (len(index_array),) + self.image_shape,\n-            dtype=K.floatx())\n-        grayscale = self.color_mode == 'grayscale'\n-        # build batch of image data\n-        for i, j in enumerate(index_array):\n-            fname = self.filenames[j]\n-            img = load_img(os.path.join(self.directory, fname),\n-                           grayscale=grayscale,\n-                           target_size=self.target_size,\n-                           interpolation=self.interpolation)\n-            x = img_to_array(img, data_format=self.data_format)\n-            params = self.image_data_generator.get_random_transform(x.shape)\n-            x = self.image_data_generator.apply_transform(x, params)\n-            x = self.image_data_generator.standardize(x)\n-            batch_x[i] = x\n-        # optionally save augmented images to disk for debugging purposes\n-        if self.save_to_dir:\n-            for i, j in enumerate(index_array):\n-                img = array_to_img(batch_x[i], self.data_format, scale=True)\n-                fname = '{prefix}_{index}_{hash}.{format}'.format(\n-                    prefix=self.save_prefix,\n-                    index=j,\n-                    hash=np.random.randint(1e7),\n-                    format=self.save_format)\n-                img.save(os.path.join(self.save_to_dir, fname))\n-        # build batch of labels\n-        if self.class_mode == 'input':\n-            batch_y = batch_x.copy()\n-        elif self.class_mode == 'sparse':\n-            batch_y = self.classes[index_array]\n-        elif self.class_mode == 'binary':\n-            batch_y = self.classes[index_array].astype(K.floatx())\n-        elif self.class_mode == 'categorical':\n-            batch_y = np.zeros(\n-                (len(batch_x), self.num_classes),\n-                dtype=K.floatx())\n-            for i, label in enumerate(self.classes[index_array]):\n-                batch_y[i, label] = 1.\n-        else:\n-            return batch_x\n-        return batch_x, batch_y\n-\n-    def next(self):\n-        \"\"\"For python 2.x.\n-\n-        # Returns\n-            The next batch.\n-        \"\"\"\n-        with self.lock:\n-            index_array = next(self.index_generator)\n-        # The transformation of images is not under thread lock\n-        # so it can be done in parallel\n-        return self._get_batches_of_transformed_samples(index_array)\n+from keras_preprocessing import image\n+\n+random_rotation = image.random_rotation\n+random_shift = image.random_shift\n+random_shear = image.random_shear\n+random_zoom = image.random_zoom\n+apply_channel_shift = image.apply_channel_shift\n+random_channel_shift = image.random_channel_shift\n+apply_brightness_shift = image.apply_brightness_shift\n+random_brightness = image.random_brightness\n+apply_affine_transform = image.apply_affine_transform\n+array_to_img = image.array_to_img\n+img_to_array = image.img_to_array\n+save_img = image.save_img\n+load_img = image.load_img\n+ImageDataGenerator = image.ImageDataGenerator\n+Iterator = image.Iterator\n+NumpyArrayIterator = image.NumpyArrayIterator\n+DirectoryIterator = image.DirectoryIterator\n\n@@ -1,365 +1,13 @@\n-# -*- coding: utf-8 -*-\n \"\"\"Utilities for preprocessing sequence data.\n \"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import numpy as np\n-import random\n-from six.moves import range\n+from keras_preprocessing import sequence\n \n-from ..utils.data_utils import Sequence\n-\n-\n-def pad_sequences(sequences, maxlen=None, dtype='int32',\n-                  padding='pre', truncating='pre', value=0.):\n-    \"\"\"Pads sequences to the same length.\n-\n-    This function transforms a list of\n-    `num_samples` sequences (lists of integers)\n-    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n-    `num_timesteps` is either the `maxlen` argument if provided,\n-    or the length of the longest sequence otherwise.\n-\n-    Sequences that are shorter than `num_timesteps`\n-    are padded with `value` at the end.\n-\n-    Sequences longer than `num_timesteps` are truncated\n-    so that they fit the desired length.\n-    The position where padding or truncation happens is determined by\n-    the arguments `padding` and `truncating`, respectively.\n-\n-    Pre-padding is the default.\n-\n-    # Arguments\n-        sequences: List of lists, where each element is a sequence.\n-        maxlen: Int, maximum length of all sequences.\n-        dtype: Type of the output sequences.\n-        padding: String, 'pre' or 'post':\n-            pad either before or after each sequence.\n-        truncating: String, 'pre' or 'post':\n-            remove values from sequences larger than\n-            `maxlen`, either at the beginning or at the end of the sequences.\n-        value: Float, padding value.\n-\n-    # Returns\n-        x: Numpy array with shape `(len(sequences), maxlen)`\n-\n-    # Raises\n-        ValueError: In case of invalid values for `truncating` or `padding`,\n-            or in case of invalid shape for a `sequences` entry.\n-    \"\"\"\n-    if not hasattr(sequences, '__len__'):\n-        raise ValueError('`sequences` must be iterable.')\n-    lengths = []\n-    for x in sequences:\n-        if not hasattr(x, '__len__'):\n-            raise ValueError('`sequences` must be a list of iterables. '\n-                             'Found non-iterable: ' + str(x))\n-        lengths.append(len(x))\n-\n-    num_samples = len(sequences)\n-    if maxlen is None:\n-        maxlen = np.max(lengths)\n-\n-    # take the sample shape from the first non empty sequence\n-    # checking for consistency in the main loop below.\n-    sample_shape = tuple()\n-    for s in sequences:\n-        if len(s) > 0:\n-            sample_shape = np.asarray(s).shape[1:]\n-            break\n-\n-    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n-    for idx, s in enumerate(sequences):\n-        if not len(s):\n-            continue  # empty list/array was found\n-        if truncating == 'pre':\n-            trunc = s[-maxlen:]\n-        elif truncating == 'post':\n-            trunc = s[:maxlen]\n-        else:\n-            raise ValueError('Truncating type \"%s\" '\n-                             'not understood' % truncating)\n-\n-        # check `trunc` has expected shape\n-        trunc = np.asarray(trunc, dtype=dtype)\n-        if trunc.shape[1:] != sample_shape:\n-            raise ValueError('Shape of sample %s of sequence at position %s '\n-                             'is different from expected shape %s' %\n-                             (trunc.shape[1:], idx, sample_shape))\n-\n-        if padding == 'post':\n-            x[idx, :len(trunc)] = trunc\n-        elif padding == 'pre':\n-            x[idx, -len(trunc):] = trunc\n-        else:\n-            raise ValueError('Padding type \"%s\" not understood' % padding)\n-    return x\n-\n-\n-def make_sampling_table(size, sampling_factor=1e-5):\n-    \"\"\"Generates a word rank-based probabilistic sampling table.\n-\n-    Used for generating the `sampling_table` argument for `skipgrams`.\n-    `sampling_table[i]` is the probability of sampling\n-    the word i-th most common word in a dataset\n-    (more common words should be sampled less frequently, for balance).\n-\n-    The sampling probabilities are generated according\n-    to the sampling distribution used in word2vec:\n-\n-    `p(word) = min(1, sqrt(word_frequency / sampling_factor) / (word_frequency / sampling_factor))`\n-\n-    We assume that the word frequencies follow Zipf's law (s=1) to derive\n-    a numerical approximation of frequency(rank):\n-\n-    `frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))`\n-    where `gamma` is the Euler-Mascheroni constant.\n-\n-    # Arguments\n-        size: Int, number of possible words to sample.\n-        sampling_factor: The sampling factor in the word2vec formula.\n-\n-    # Returns\n-        A 1D Numpy array of length `size` where the ith entry\n-        is the probability that a word of rank i should be sampled.\n-    \"\"\"\n-    gamma = 0.577\n-    rank = np.arange(size)\n-    rank[0] = 1\n-    inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1. / (12. * rank)\n-    f = sampling_factor * inv_fq\n-\n-    return np.minimum(1., f / np.sqrt(f))\n-\n-\n-def skipgrams(sequence, vocabulary_size,\n-              window_size=4, negative_samples=1., shuffle=True,\n-              categorical=False, sampling_table=None, seed=None):\n-    \"\"\"Generates skipgram word pairs.\n-\n-    This function transforms a sequence of word indexes (list of integers)\n-    into tuples of words of the form:\n-\n-    - (word, word in the same window), with label 1 (positive samples).\n-    - (word, random word from the vocabulary), with label 0 (negative samples).\n-\n-    Read more about Skipgram in this gnomic paper by Mikolov et al.:\n-    [Efficient Estimation of Word Representations in\n-    Vector Space](http://arxiv.org/pdf/1301.3781v3.pdf)\n-\n-    # Arguments\n-        sequence: A word sequence (sentence), encoded as a list\n-            of word indices (integers). If using a `sampling_table`,\n-            word indices are expected to match the rank\n-            of the words in a reference dataset (e.g. 10 would encode\n-            the 10-th most frequently occurring token).\n-            Note that index 0 is expected to be a non-word and will be skipped.\n-        vocabulary_size: Int, maximum possible word index + 1\n-        window_size: Int, size of sampling windows (technically half-window).\n-            The window of a word `w_i` will be\n-            `[i - window_size, i + window_size+1]`.\n-        negative_samples: Float >= 0. 0 for no negative (i.e. random) samples.\n-            1 for same number as positive samples.\n-        shuffle: Whether to shuffle the word couples before returning them.\n-        categorical: bool. if False, labels will be\n-            integers (eg. `[0, 1, 1 .. ]`),\n-            if `True`, labels will be categorical, e.g.\n-            `[[1,0],[0,1],[0,1] .. ]`.\n-        sampling_table: 1D array of size `vocabulary_size` where the entry i\n-            encodes the probability to sample a word of rank i.\n-        seed: Random seed.\n-\n-    # Returns\n-        couples, labels: where `couples` are int pairs and\n-            `labels` are either 0 or 1.\n-\n-    # Note\n-        By convention, index 0 in the vocabulary is\n-        a non-word and will be skipped.\n-    \"\"\"\n-    couples = []\n-    labels = []\n-    for i, wi in enumerate(sequence):\n-        if not wi:\n-            continue\n-        if sampling_table is not None:\n-            if sampling_table[wi] < random.random():\n-                continue\n-\n-        window_start = max(0, i - window_size)\n-        window_end = min(len(sequence), i + window_size + 1)\n-        for j in range(window_start, window_end):\n-            if j != i:\n-                wj = sequence[j]\n-                if not wj:\n-                    continue\n-                couples.append([wi, wj])\n-                if categorical:\n-                    labels.append([0, 1])\n-                else:\n-                    labels.append(1)\n-\n-    if negative_samples > 0:\n-        num_negative_samples = int(len(labels) * negative_samples)\n-        words = [c[0] for c in couples]\n-        random.shuffle(words)\n-\n-        couples += [[words[i % len(words)],\n-                    random.randint(1, vocabulary_size - 1)]\n-                    for i in range(num_negative_samples)]\n-        if categorical:\n-            labels += [[1, 0]] * num_negative_samples\n-        else:\n-            labels += [0] * num_negative_samples\n-\n-    if shuffle:\n-        if seed is None:\n-            seed = random.randint(0, 10e6)\n-        random.seed(seed)\n-        random.shuffle(couples)\n-        random.seed(seed)\n-        random.shuffle(labels)\n-\n-    return couples, labels\n-\n-\n-def _remove_long_seq(maxlen, seq, label):\n-    \"\"\"Removes sequences that exceed the maximum length.\n-\n-    # Arguments\n-        maxlen: Int, maximum length of the output sequences.\n-        seq: List of lists, where each sublist is a sequence.\n-        label: List where each element is an integer.\n-\n-    # Returns\n-        new_seq, new_label: shortened lists for `seq` and `label`.\n-    \"\"\"\n-    new_seq, new_label = [], []\n-    for x, y in zip(seq, label):\n-        if len(x) < maxlen:\n-            new_seq.append(x)\n-            new_label.append(y)\n-    return new_seq, new_label\n-\n-\n-class TimeseriesGenerator(Sequence):\n-    \"\"\"Utility class for generating batches of temporal data.\n-\n-    This class takes in a sequence of data-points gathered at\n-    equal intervals, along with time series parameters such as\n-    stride, length of history, etc., to produce batches for\n-    training/validation.\n-\n-    # Arguments\n-        data: Indexable generator (such as list or Numpy array)\n-            containing consecutive data points (timesteps).\n-            The data should be at 2D, and axis 0 is expected\n-            to be the time dimension.\n-        targets: Targets corresponding to timesteps in `data`.\n-            It should have same length as `data`.\n-        length: Length of the output sequences (in number of timesteps).\n-        sampling_rate: Period between successive individual timesteps\n-            within sequences. For rate `r`, timesteps\n-            `data[i]`, `data[i-r]`, ... `data[i - length]`\n-            are used for create a sample sequence.\n-        stride: Period between successive output sequences.\n-            For stride `s`, consecutive output samples would\n-            be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.\n-        start_index: Data points earlier than `start_index` will not be used\n-            in the output sequences. This is useful to reserve part of the\n-            data for test or validation.\n-        end_index: Data points later than `end_index` will not be used\n-            in the output sequences. This is useful to reserve part of the\n-            data for test or validation.\n-        shuffle: Whether to shuffle output samples,\n-            or instead draw them in chronological order.\n-        reverse: Boolean: if `true`, timesteps in each output sample will be\n-            in reverse chronological order.\n-        batch_size: Number of timeseries samples in each batch\n-            (except maybe the last one).\n-\n-    # Returns\n-        A [Sequence](/utils/#sequence) instance.\n-\n-    # Examples\n-\n-    ```python\n-    from keras.preprocessing.sequence import TimeseriesGenerator\n-    import numpy as np\n-\n-    data = np.array([[i] for i in range(50)])\n-    targets = np.array([[i] for i in range(50)])\n-\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2,\n-                                   batch_size=2)\n-    assert len(data_gen) == 20\n-\n-    batch_0 = data_gen[0]\n-    x, y = batch_0\n-    assert np.array_equal(x,\n-                          np.array([[[0], [2], [4], [6], [8]],\n-                                    [[1], [3], [5], [7], [9]]]))\n-    assert np.array_equal(y,\n-                          np.array([[10], [11]]))\n-    ```\n-    \"\"\"\n-\n-    def __init__(self, data, targets, length,\n-                 sampling_rate=1,\n-                 stride=1,\n-                 start_index=0,\n-                 end_index=None,\n-                 shuffle=False,\n-                 reverse=False,\n-                 batch_size=128):\n-        self.data = data\n-        self.targets = targets\n-        self.length = length\n-        self.sampling_rate = sampling_rate\n-        self.stride = stride\n-        self.start_index = start_index + length\n-        if end_index is None:\n-            end_index = len(data) - 1\n-        self.end_index = end_index\n-        self.shuffle = shuffle\n-        self.reverse = reverse\n-        self.batch_size = batch_size\n-\n-        if self.start_index > self.end_index:\n-            raise ValueError('`start_index+length=%i > end_index=%i` '\n-                             'is disallowed, as no part of the sequence '\n-                             'would be left to be used as current step.'\n-                             % (self.start_index, self.end_index))\n-\n-    def __len__(self):\n-        return (self.end_index - self.start_index +\n-                self.batch_size * self.stride) // (self.batch_size * self.stride)\n-\n-    def _empty_batch(self, num_rows):\n-        samples_shape = [num_rows, self.length // self.sampling_rate]\n-        samples_shape.extend(self.data.shape[1:])\n-        targets_shape = [num_rows]\n-        targets_shape.extend(self.targets.shape[1:])\n-        return np.empty(samples_shape), np.empty(targets_shape)\n-\n-    def __getitem__(self, index):\n-        if self.shuffle:\n-            rows = np.random.randint(\n-                self.start_index, self.end_index + 1, size=self.batch_size)\n-        else:\n-            i = self.start_index + self.batch_size * self.stride * index\n-            rows = np.arange(i, min(i + self.batch_size *\n-                                    self.stride, self.end_index + 1), self.stride)\n-\n-        samples, targets = self._empty_batch(len(rows))\n-        for j, row in enumerate(rows):\n-            indices = range(rows[j] - self.length, rows[j], self.sampling_rate)\n-            samples[j] = self.data[indices]\n-            targets[j] = self.targets[rows[j]]\n-        if self.reverse:\n-            return samples[:, ::-1, ...], targets\n-        return samples, targets\n+pad_sequences = sequence.pad_sequences\n+make_sampling_table = sequence.make_sampling_table\n+skipgrams = sequence.skipgrams\n+_remove_long_seq = sequence._remove_long_seq  # TODO: make it public?\n+TimeseriesGenerator = sequence.TimeseriesGenerator\n\n@@ -1,377 +1,12 @@\n-# -*- coding: utf-8 -*-\n \"\"\"Utilities for text input preprocessing.\n \"\"\"\n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import string\n-import sys\n-import warnings\n-from collections import OrderedDict\n-from hashlib import md5\n+from keras_preprocessing import text\n \n-import numpy as np\n-from six.moves import range\n-from six.moves import zip\n-\n-if sys.version_info < (3,):\n-    maketrans = string.maketrans\n-else:\n-    maketrans = str.maketrans\n-\n-\n-def text_to_word_sequence(text,\n-                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n-                          lower=True, split=\" \"):\n-    \"\"\"Converts a text to a sequence of words (or tokens).\n-\n-    # Arguments\n-        text: Input text (string).\n-        filters: list (or concatenation) of characters to filter out, such as\n-         punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' , includes\n-         basic punctuation, tabs, and newlines.\n-        lower: boolean. Whether to convert the input to lowercase.\n-        split: str. Separator for word splitting.\n-\n-    # Returns\n-        A list of words (or tokens).\n-    \"\"\"\n-    if lower:\n-        text = text.lower()\n-\n-    if sys.version_info < (3,):\n-        if isinstance(text, unicode):\n-            translate_map = dict((ord(c), unicode(split)) for c in filters)\n-            text = text.translate(translate_map)\n-        elif len(split) == 1:\n-            translate_map = maketrans(filters, split * len(filters))\n-            text = text.translate(translate_map)\n-        else:\n-            for c in filters:\n-                text = text.replace(c, split)\n-    else:\n-        translate_dict = dict((c, split) for c in filters)\n-        translate_map = maketrans(translate_dict)\n-        text = text.translate(translate_map)\n-\n-    seq = text.split(split)\n-    return [i for i in seq if i]\n-\n-\n-def one_hot(text, n,\n-            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n-            lower=True,\n-            split=' '):\n-    \"\"\"One-hot encodes a text into a list of word indexes of size n.\n-\n-    This is a wrapper to the `hashing_trick` function using `hash` as the\n-    hashing function; unicity of word to index mapping non-guaranteed.\n-\n-    # Arguments\n-        text: Input text (string).\n-        n: int. Size of vocabulary.\n-        filters: list (or concatenation) of characters to filter out, such as\n-         punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' , includes\n-         basic punctuation, tabs, and newlines.\n-        lower: boolean. Whether to set the text to lowercase.\n-        split: str. Separator for word splitting.\n-\n-    # Returns\n-        List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).\n-    \"\"\"\n-    return hashing_trick(text, n,\n-                         hash_function=hash,\n-                         filters=filters,\n-                         lower=lower,\n-                         split=split)\n-\n-\n-def hashing_trick(text, n,\n-                  hash_function=None,\n-                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n-                  lower=True,\n-                  split=' '):\n-    \"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n-\n-    # Arguments\n-        text: Input text (string).\n-        n: Dimension of the hashing space.\n-        hash_function: defaults to python `hash` function, can be 'md5' or\n-            any function that takes in input a string and returns a int.\n-            Note that 'hash' is not a stable hashing function, so\n-            it is not consistent across different runs, while 'md5'\n-            is a stable hashing function.\n-        filters: list (or concatenation) of characters to filter out, such as\n-         punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' , includes\n-         basic punctuation, tabs, and newlines.\n-        lower: boolean. Whether to set the text to lowercase.\n-        split: str. Separator for word splitting.\n-\n-    # Returns\n-        A list of integer word indices (unicity non-guaranteed).\n-\n-    `0` is a reserved index that won't be assigned to any word.\n-\n-    Two or more words may be assigned to the same index, due to possible\n-    collisions by the hashing function.\n-    The [probability](https://en.wikipedia.org/wiki/Birthday_problem#Probability_table)\n-    of a collision is in relation to the dimension of the hashing space and\n-    the number of distinct objects.\n-    \"\"\"\n-    if hash_function is None:\n-        hash_function = hash\n-    elif hash_function == 'md5':\n-        hash_function = lambda w: int(md5(w.encode()).hexdigest(), 16)\n-\n-    seq = text_to_word_sequence(text,\n-                                filters=filters,\n-                                lower=lower,\n-                                split=split)\n-    return [(hash_function(w) % (n - 1) + 1) for w in seq]\n-\n-\n-class Tokenizer(object):\n-    \"\"\"Text tokenization utility class.\n-\n-    This class allows to vectorize a text corpus, by turning each\n-    text into either a sequence of integers (each integer being the index\n-    of a token in a dictionary) or into a vector where the coefficient\n-    for each token could be binary, based on word count, based on tf-idf...\n-\n-    # Arguments\n-        num_words: the maximum number of words to keep, based\n-            on word frequency. Only the most common `num_words` words will\n-            be kept.\n-        filters: a string where each element is a character that will be\n-            filtered from the texts. The default is all punctuation, plus\n-            tabs and line breaks, minus the `'` character.\n-        lower: boolean. Whether to convert the texts to lowercase.\n-        split: str. Separator for word splitting.\n-        char_level: if True, every character will be treated as a token.\n-        oov_token: if given, it will be added to word_index and used to\n-            replace out-of-vocabulary words during text_to_sequence calls\n-\n-    By default, all punctuation is removed, turning the texts into\n-    space-separated sequences of words\n-    (words maybe include the `'` character). These sequences are then\n-    split into lists of tokens. They will then be indexed or vectorized.\n-\n-    `0` is a reserved index that won't be assigned to any word.\n-    \"\"\"\n-\n-    def __init__(self, num_words=None,\n-                 filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n-                 lower=True,\n-                 split=' ',\n-                 char_level=False,\n-                 oov_token=None,\n-                 **kwargs):\n-        # Legacy support\n-        if 'nb_words' in kwargs:\n-            warnings.warn('The `nb_words` argument in `Tokenizer` '\n-                          'has been renamed `num_words`.')\n-            num_words = kwargs.pop('nb_words')\n-        if kwargs:\n-            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n-\n-        self.word_counts = OrderedDict()\n-        self.word_docs = {}\n-        self.filters = filters\n-        self.split = split\n-        self.lower = lower\n-        self.num_words = num_words\n-        self.document_count = 0\n-        self.char_level = char_level\n-        self.oov_token = oov_token\n-        self.index_docs = {}\n-\n-    def fit_on_texts(self, texts):\n-        \"\"\"Updates internal vocabulary based on a list of texts.\n-        In the case where texts contains lists, we assume each entry of the lists\n-        to be a token.\n-\n-        Required before using `texts_to_sequences` or `texts_to_matrix`.\n-\n-        # Arguments\n-            texts: can be a list of strings,\n-                a generator of strings (for memory-efficiency),\n-                or a list of list of strings.\n-        \"\"\"\n-        for text in texts:\n-            self.document_count += 1\n-            if self.char_level or isinstance(text, list):\n-                seq = text\n-            else:\n-                seq = text_to_word_sequence(text,\n-                                            self.filters,\n-                                            self.lower,\n-                                            self.split)\n-            for w in seq:\n-                if w in self.word_counts:\n-                    self.word_counts[w] += 1\n-                else:\n-                    self.word_counts[w] = 1\n-            for w in set(seq):\n-                if w in self.word_docs:\n-                    self.word_docs[w] += 1\n-                else:\n-                    self.word_docs[w] = 1\n-\n-        wcounts = list(self.word_counts.items())\n-        wcounts.sort(key=lambda x: x[1], reverse=True)\n-        sorted_voc = [wc[0] for wc in wcounts]\n-        # note that index 0 is reserved, never assigned to an existing word\n-        self.word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n-\n-        if self.oov_token is not None:\n-            i = self.word_index.get(self.oov_token)\n-            if i is None:\n-                self.word_index[self.oov_token] = len(self.word_index) + 1\n-\n-        for w, c in list(self.word_docs.items()):\n-            self.index_docs[self.word_index[w]] = c\n-\n-    def fit_on_sequences(self, sequences):\n-        \"\"\"Updates internal vocabulary based on a list of sequences.\n-\n-        Required before using `sequences_to_matrix`\n-        (if `fit_on_texts` was never called).\n-\n-        # Arguments\n-            sequences: A list of sequence.\n-                A \"sequence\" is a list of integer word indices.\n-        \"\"\"\n-        self.document_count += len(sequences)\n-        for seq in sequences:\n-            seq = set(seq)\n-            for i in seq:\n-                if i not in self.index_docs:\n-                    self.index_docs[i] = 1\n-                else:\n-                    self.index_docs[i] += 1\n-\n-    def texts_to_sequences(self, texts):\n-        \"\"\"Transforms each text in texts in a sequence of integers.\n-\n-        Only top \"num_words\" most frequent words will be taken into account.\n-        Only words known by the tokenizer will be taken into account.\n-\n-        # Arguments\n-            texts: A list of texts (strings).\n-\n-        # Returns\n-            A list of sequences.\n-        \"\"\"\n-        res = []\n-        for vect in self.texts_to_sequences_generator(texts):\n-            res.append(vect)\n-        return res\n-\n-    def texts_to_sequences_generator(self, texts):\n-        \"\"\"Transforms each text in `texts` in a sequence of integers.\n-        Each item in texts can also be a list, in which case we assume each item of that list\n-        to be a token.\n-\n-        Only top \"num_words\" most frequent words will be taken into account.\n-        Only words known by the tokenizer will be taken into account.\n-\n-        # Arguments\n-            texts: A list of texts (strings).\n-\n-        # Yields\n-            Yields individual sequences.\n-        \"\"\"\n-        num_words = self.num_words\n-        for text in texts:\n-            if self.char_level or isinstance(text, list):\n-                seq = text\n-            else:\n-                seq = text_to_word_sequence(text,\n-                                            self.filters,\n-                                            self.lower,\n-                                            self.split)\n-            vect = []\n-            for w in seq:\n-                i = self.word_index.get(w)\n-                if i is not None:\n-                    if num_words and i >= num_words:\n-                        continue\n-                    else:\n-                        vect.append(i)\n-                elif self.oov_token is not None:\n-                    i = self.word_index.get(self.oov_token)\n-                    if i is not None:\n-                        vect.append(i)\n-            yield vect\n-\n-    def texts_to_matrix(self, texts, mode='binary'):\n-        \"\"\"Convert a list of texts to a Numpy matrix.\n-\n-        # Arguments\n-            texts: list of strings.\n-            mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n-\n-        # Returns\n-            A Numpy matrix.\n-        \"\"\"\n-        sequences = self.texts_to_sequences(texts)\n-        return self.sequences_to_matrix(sequences, mode=mode)\n-\n-    def sequences_to_matrix(self, sequences, mode='binary'):\n-        \"\"\"Converts a list of sequences into a Numpy matrix.\n-\n-        # Arguments\n-            sequences: list of sequences\n-                (a sequence is a list of integer word indices).\n-            mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n-\n-        # Returns\n-            A Numpy matrix.\n-\n-        # Raises\n-            ValueError: In case of invalid `mode` argument,\n-                or if the Tokenizer requires to be fit to sample data.\n-        \"\"\"\n-        if not self.num_words:\n-            if self.word_index:\n-                num_words = len(self.word_index) + 1\n-            else:\n-                raise ValueError('Specify a dimension (num_words argument), '\n-                                 'or fit on some text data first.')\n-        else:\n-            num_words = self.num_words\n-\n-        if mode == 'tfidf' and not self.document_count:\n-            raise ValueError('Fit the Tokenizer on some data '\n-                             'before using tfidf mode.')\n-\n-        x = np.zeros((len(sequences), num_words))\n-        for i, seq in enumerate(sequences):\n-            if not seq:\n-                continue\n-            counts = {}\n-            for j in seq:\n-                if j >= num_words:\n-                    continue\n-                if j not in counts:\n-                    counts[j] = 1.\n-                else:\n-                    counts[j] += 1\n-            for j, c in list(counts.items()):\n-                if mode == 'count':\n-                    x[i][j] = c\n-                elif mode == 'freq':\n-                    x[i][j] = c / len(seq)\n-                elif mode == 'binary':\n-                    x[i][j] = 1\n-                elif mode == 'tfidf':\n-                    # Use weighting scheme 2 in\n-                    # https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n-                    tf = 1 + np.log(c)\n-                    idf = np.log(1 + self.document_count /\n-                                 (1 + self.index_docs.get(j, 0)))\n-                    x[i][j] = tf * idf\n-                else:\n-                    raise ValueError('Unknown vectorization mode:', mode)\n-        return x\n+text_to_word_sequence = text.text_to_word_sequence\n+one_hot = text.one_hot\n+hashing_trick = text.hashing_trick\n+Tokenizer = text.Tokenizer\n\n@@ -36,7 +36,9 @@ setup(name='Keras',\n                         'scipy>=0.14',\n                         'six>=1.9.0',\n                         'pyyaml',\n-                        'h5py'],\n+                        'h5py',\n+                        'keras_applications==1.0.1',\n+                        'keras_preprocessing==1.0.1'],\n       extras_require={\n           'visualize': ['pydot>=1.2.4'],\n           'tests': ['pytest',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
