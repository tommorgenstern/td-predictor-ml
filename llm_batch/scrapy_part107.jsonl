{"custom_id": "scrapy#5ffc7650bdd37d68c8a525274a3224c486a8f869", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 590 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -160,10 +160,7 @@ class Scraper(object):\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n         \"\"\"\n-        # TODO: keep closing state internally instead of checking engine\n-        if spider in self.engine.closing:\n-            return\n-        elif isinstance(output, Request):\n+        if isinstance(output, Request):\n             send_catch_log(signal=signals.request_received, request=output, \\\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d1d3493e75e72ac4d02d519094becb96898f592", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 37 | Files Changed: 10 | Hunks: 32 | Methods Changed: 24 | Complexity Δ (Sum/Max): 28/12 | Churn Δ: 129 | Churn Cumulative: 551 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,27 +3,26 @@ DefaultHeaders downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-from scrapy.conf import settings\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy import signals\n+from scrapy import conf\n+from scrapy.utils.python import WeakKeyCache\n \n \n class DefaultHeadersMiddleware(object):\n \n-    def __init__(self):\n+    def __init__(self, settings=conf.settings):\n         self.global_default_headers = settings.get('DEFAULT_REQUEST_HEADERS')\n-        self._default_headers = {}\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+        self._headers = WeakKeyCache(self._default_headers)\n+\n+    def _default_headers(self, spider):\n+        headers = dict(self.global_default_headers)\n+        spider_headers = getattr(spider, 'default_request_headers', None) or {}\n+        for k, v in spider_headers.iteritems():\n+            if v:\n+                headers[k] = v\n+            else:\n+                headers.pop(k, None)\n+        return headers.items()\n \n     def process_request(self, request, spider):\n-        for k, v in self._default_headers[spider].iteritems():\n-            if v:\n+        for k, v in self._headers[spider]:\n             request.headers.setdefault(k, v)\n-\n-    def spider_opened(self, spider):\n-        self._default_headers[spider] = dict(self.global_default_headers,\n-                **getattr(spider, 'default_request_headers', {}))\n-\n-    def spider_closed(self, spider):\n-        self._default_headers.pop(spider)\n\n@@ -4,14 +4,24 @@ HTTP basic auth downloader middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n-from scrapy.utils.request import request_authenticate\n+from scrapy.utils.http import basic_auth_header\n+from scrapy.utils.python import WeakKeyCache\n+\n \n class HttpAuthMiddleware(object):\n-    \"\"\"This middleware allows spiders to use HTTP auth in a cleaner way\n+    \"\"\"Set Basic HTTP Authorization header\n     (http_user and http_pass spider class attributes)\"\"\"\n \n+    def __init__(self):\n+        self._cache = WeakKeyCache(self._authorization)\n+\n+    def _authorization(self, spider):\n+        usr = getattr(spider, 'http_user', '')\n+        pwd = getattr(spider, 'http_pass', '')\n+        if usr or pwd:\n+            return basic_auth_header(usr, pwd)\n+\n     def process_request(self, request, spider):\n-        http_user = getattr(spider, 'http_user', '')\n-        http_pass = getattr(spider, 'http_pass', '')\n-        if http_user or http_pass:\n-            request_authenticate(request, http_user, http_pass)\n+        auth = self._cache[spider]\n+        if auth and 'Authorization' not in request.headers:\n+            request.headers['Authorization'] = auth\n\n@@ -1,14 +1,20 @@\n \"\"\"Set User-Agent header per spider or use a default value from settings\"\"\"\n \n from scrapy.conf import settings\n+from scrapy.utils.python import WeakKeyCache\n \n \n class UserAgentMiddleware(object):\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n-    default_useragent = settings.get('USER_AGENT')\n+    def __init__(self, settings=settings):\n+        self.cache = WeakKeyCache(self._user_agent)\n+        self.default_useragent = settings.get('USER_AGENT')\n+\n+    def _user_agent(self, spider):\n+        return getattr(spider, 'user_agent', None) or self.default_useragent\n \n     def process_request(self, request, spider):\n-        ua = getattr(spider, 'user_agent', None) or self.default_useragent\n+        ua = self.cache[spider]\n         if ua:\n             request.headers.setdefault('User-Agent', ua)\n\n@@ -16,9 +16,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n \n     def test_process_request(self):\n         req = Request('http://www.scrapytest.org')\n-        self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n-        self.mw.spider_closed(self.spider)\n         self.assertEquals(req.headers, self.default_request_headers)\n \n     def test_spider_default_request_headers(self):\n@@ -30,9 +28,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.spider.default_request_headers = spider_headers\n \n         req = Request('http://www.scrapytest.org')\n-        self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n-        self.mw.spider_closed(self.spider)\n         self.assertEquals(req.headers, dict(self.default_request_headers, **spider_headers))\n \n     def test_update_headers(self):\n@@ -40,9 +36,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         req = Request('http://www.scrapytest.org', headers=headers)\n         self.assertEquals(req.headers, headers)\n \n-        self.mw.spider_opened(self.spider)\n         self.mw.process_request(req, self.spider)\n-        self.mw.spider_closed(self.spider)\n         self.default_request_headers.update(headers)\n         self.assertEquals(req.headers, self.default_request_headers)\n \n\n@@ -12,17 +12,21 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n         self.mw = HttpAuthMiddleware()\n+        self.spider = TestSpider('foo')\n \n     def tearDown(self):\n         del self.mw\n \n     def test_auth(self):\n-        self.mw.default_useragent = 'default_useragent'\n-        spider = TestSpider('foo')\n         req = Request('http://scrapytest.org/')\n-        assert self.mw.process_request(req, spider) is None\n+        assert self.mw.process_request(req, self.spider) is None\n         self.assertEquals(req.headers['Authorization'], 'Basic Zm9vOmJhcg==')\n \n+    def test_auth_already_set(self):\n+        req = Request('http://scrapytest.org/', headers=dict(Authorization='Digest 123'))\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers['Authorization'], 'Digest 123')\n+\n \n if __name__ == '__main__':\n     unittest.main()\n\n@@ -0,0 +1,12 @@\n+import unittest\n+from scrapy.utils.http import basic_auth_header\n+\n+\n+class UtilsHttpTestCase(unittest.TestCase):\n+\n+    def test_basic_auth_header(self):\n+        self.assertEqual('Basic c29tZXVzZXI6c29tZXBhc3M=',\n+                basic_auth_header('someuser', 'somepass'))\n+        # Check url unsafe encoded header\n+        self.assertEqual('Basic c29tZXVzZXI6QDx5dTk-Jm8_UQ==',\n+            basic_auth_header('someuser', '@<yu9>&o?Q'))\n\n@@ -1,8 +1,10 @@\n import operator\n import unittest\n+from itertools import count\n \n from scrapy.utils.python import str_to_unicode, unicode_to_str, \\\n-    memoizemethod_noargs, isbinarytext, equal_attributes\n+    memoizemethod_noargs, isbinarytext, equal_attributes, \\\n+    WeakKeyCache\n \n class UtilsPythonTestCase(unittest.TestCase):\n     def test_str_to_unicode(self):\n@@ -108,6 +110,19 @@ class UtilsPythonTestCase(unittest.TestCase):\n         a.meta['z'] = 2\n         self.failIf(equal_attributes(a, b, [compare_z, 'x']))\n \n+    def test_weakkeycache(self):\n+        class _Weakme(object): pass\n+        _values = count()\n+        wk = WeakKeyCache(lambda k: _values.next())\n+        k = _Weakme()\n+        v = wk[k]\n+        self.assertEqual(v, wk[k])\n+        self.assertNotEqual(v, wk[_Weakme()])\n+        self.assertEqual(v, wk[k])\n+        del k\n+        self.assertFalse(len(wk._weakdict))\n+\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,3 +1,4 @@\n+from base64 import urlsafe_b64encode\n \n def headers_raw_to_dict(headers_raw):\n     \"\"\"\n@@ -55,4 +56,6 @@ def headers_dict_to_raw(headers_dict):\n     return '\\r\\n'.join(raw_lines)\n \n \n-\n+def basic_auth_header(username, password):\n+    \"\"\"Return `Authorization` header for HTTP Basic Access Authentication (RFC 2617)\"\"\"\n+    return 'Basic ' + urlsafe_b64encode(\"%s:%s\" % (username, password))\n\n@@ -11,6 +11,7 @@ import weakref\n from functools import wraps\n from sgmllib import SGMLParser\n \n+\n class FixedSGMLParser(SGMLParser):\n     \"\"\"The SGMLParser that comes with Python has a bug in the convert_charref()\n     method. This is the same class with the bug fixed\"\"\"\n@@ -179,3 +180,14 @@ def equal_attributes(obj1, obj2, attributes):\n     # all attributes equal\n     return True\n \n+\n+class WeakKeyCache(object):\n+\n+    def __init__(self, default_factory):\n+        self.default_factory = default_factory\n+        self._weakdict = weakref.WeakKeyDictionary()\n+\n+    def __getitem__(self, key):\n+        if key not in self._weakdict:\n+            self._weakdict[key] = self.default_factory(key)\n+        return self._weakdict[key]\n\n@@ -10,6 +10,7 @@ from urlparse import urlunparse\n \n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.http import basic_auth_header\n \n \n _fingerprint_cache = weakref.WeakKeyDictionary()\n@@ -61,8 +62,7 @@ def request_authenticate(request, username, password):\n     \"\"\"Autenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n-    b64userpass = urlsafe_b64encode(\"%s:%s\" % (username, password))\n-    request.headers['Authorization'] = 'Basic ' + b64userpass\n+    request.headers['Authorization'] = basic_auth_header(username, password)\n \n def request_info(request):\n     \"\"\"Return a short string with request info including method, url and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc72f03e103176db2dd6c3e2c4b3ac391c1d892d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 45 | Churn Cumulative: 326 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,8 @@ from urlparse import urlparse\n from ftplib import FTP\n from shutil import copyfileobj\n \n+from zope.interface import Interface, implements\n+\n from twisted.internet import defer, threads\n from scrapy import log, signals\n from scrapy.xlib.pydispatch import dispatcher\n@@ -20,8 +22,21 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.url import file_uri_to_path\n from scrapy.conf import settings\n \n+\n+class IFeedStorage(Interface):\n+    \"\"\"Interface that all Feed Storages must implement\"\"\"\n+\n+    def __init__(uri):\n+        \"\"\"Initialize the storage with the parameters given in the URI\"\"\"\n+\n+    def store(file, spider):\n+        \"\"\"Store the given file stream\"\"\"\n+\n+\n class BlockingFeedStorage(object):\n \n+    implements(IFeedStorage)\n+\n     def store(self, file, spider):\n         return threads.deferToThread(self._store_in_thread, file, spider)\n \n@@ -31,11 +46,13 @@ class BlockingFeedStorage(object):\n \n class StdoutFeedStorage(object):\n \n-    def __init__(self, uri):\n-        pass\n+    implements(IFeedStorage)\n+\n+    def __init__(self, uri, _stdout=sys.stdout):\n+        self._stdout = _stdout\n \n     def store(self, file, spider):\n-        copyfileobj(file, sys.stdout)\n+        copyfileobj(file, self._stdout)\n \n \n class FileFeedStorage(BlockingFeedStorage):\n\n@@ -1,11 +1,12 @@\n import os, urlparse\n \n+from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n from twisted.internet import defer\n from cStringIO import StringIO\n \n from scrapy.spider import BaseSpider\n-from scrapy.contrib.feedexport import FileFeedStorage, FTPFeedStorage, S3FeedStorage\n+from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage\n from scrapy.utils.url import path_to_file_uri\n from scrapy.utils.test import assert_aws_environ\n \n@@ -20,7 +21,6 @@ class FeedStorageTest(unittest.TestCase):\n         yield storage.store(StringIO(\"new content\"), BaseSpider(\"default\"))\n         self.failUnlessEqual(open(path).read(), \"new content\")\n \n-\n class FileFeedStorageTest(FeedStorageTest):\n \n     def test_store_file_uri(self):\n@@ -42,6 +42,11 @@ class FileFeedStorageTest(FeedStorageTest):\n         path = self.mktemp()\n         return self._assert_stores(FileFeedStorage(path), path)\n \n+    def test_interface(self):\n+        path = self.mktemp()\n+        st = FileFeedStorage(path)\n+        verifyObject(IFeedStorage, st)\n+\n \n class FTPFeedStorageTest(FeedStorageTest):\n \n@@ -50,7 +55,9 @@ class FTPFeedStorageTest(FeedStorageTest):\n         path = os.environ.get('FEEDTEST_FTP_PATH')\n         if not (uri and path):\n             raise unittest.SkipTest(\"No FTP server available for testing\")\n-        return self._assert_stores(FTPFeedStorage(uri), path)\n+        st = FTPFeedStorage(uri)\n+        verifyObject(IFeedStorage, st)\n+        return self._assert_stores(st, path)\n \n \n class S3FeedStorageTest(unittest.TestCase):\n@@ -63,8 +70,17 @@ class S3FeedStorageTest(unittest.TestCase):\n             raise unittest.SkipTest(\"No S3 URI available for testing\")\n         from boto import connect_s3\n         storage = S3FeedStorage(uri)\n+        verifyObject(IFeedStorage, storage)\n         yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n         u = urlparse.urlparse(uri)\n         key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)\n         self.failUnlessEqual(key.get_contents_as_string(), \"content\")\n \n+class StdoutFeedStorageTest(FeedStorageTest):\n+\n+    @defer.inlineCallbacks\n+    def test_store(self):\n+        out = StringIO()\n+        storage = StdoutFeedStorage('stdout:', _stdout=out)\n+        yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n+        self.assertEqual(out.getvalue(), \"content\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f58af2005534738ffa12c5faaf12aefffc3147c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 45 | Files Changed: 2 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 65 | Churn Cumulative: 657 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,7 +61,7 @@ class Scraper(object):\n \n     def __init__(self, engine, settings):\n         self.sites = {}\n-        self.spidermw = SpiderMiddlewareManager()\n+        self.spidermw = SpiderMiddlewareManager.from_settings(settings)\n         itemproc_cls = load_object(settings['ITEM_PROCESSOR'])\n         self.itemproc = itemproc_cls.from_settings(settings)\n         self.concurrent_items = settings.getint('CONCURRENT_ITEMS')\n\n@@ -1,65 +1,40 @@\n \"\"\"\n-This module implements the Spider Middleware manager. For more information see\n-the Spider Middleware doc in:\n-\n-docs/topics/spider-middleware.rst\n+Spider Middleware manager\n \n+See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n-from scrapy import log\n from twisted.python.failure import Failure\n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n-from scrapy.utils.conf import build_component_list\n+from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n-from scrapy.conf import settings\n+from scrapy.utils.conf import build_component_list\n \n def _isiterable(possible_iterator):\n     return hasattr(possible_iterator, '__iter__')\n \n-class SpiderMiddlewareManager(object):\n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-        self.disabled = {}\n-        self.spider_middleware = []\n-        self.result_middleware = []\n-        self.exception_middleware = []\n-        self.load()\n+class SpiderMiddlewareManager(MiddlewareManager):\n+\n+    component_name = 'spider middleware'\n+\n+    @classmethod\n+    def _get_mwlist_from_settings(cls, settings):\n+        return build_component_list(settings['SPIDER_MIDDLEWARES_BASE'], \\\n+            settings['SPIDER_MIDDLEWARES'])\n \n     def _add_middleware(self, mw):\n+        super(SpiderMiddlewareManager, self)._add_middleware(mw)\n         if hasattr(mw, 'process_spider_input'):\n-            self.spider_middleware.append(mw.process_spider_input)\n+            self.methods['process_spider_input'].append(mw.process_spider_input)\n         if hasattr(mw, 'process_spider_output'):\n-            self.result_middleware.insert(0, mw.process_spider_output)\n+            self.methods['process_spider_output'].append(mw.process_spider_output)\n         if hasattr(mw, 'process_spider_exception'):\n-            self.exception_middleware.insert(0, mw.process_spider_exception)\n-\n-    def load(self):\n-        \"\"\"Load middleware defined in settings module\"\"\"\n-        mwlist = build_component_list(settings['SPIDER_MIDDLEWARES_BASE'], \\\n-            settings['SPIDER_MIDDLEWARES'])\n-        self.enabled.clear()\n-        self.disabled.clear()\n-        for mwpath in mwlist:\n-            try:\n-                cls = load_object(mwpath)\n-                mw = cls()\n-                self.enabled[cls.__name__] = mw\n-                self._add_middleware(mw)\n-            except NotConfigured, e:\n-                self.disabled[cls.__name__] = mwpath\n-                if e.args:\n-                    log.msg(e)\n-        log.msg(\"Enabled spider middlewares: %s\" % \", \".join(self.enabled.keys()), \\\n-            level=log.DEBUG)\n-        self.loaded = True\n+            self.methods['process_spider_exception'].append(mw.process_spider_exception)\n \n     def scrape_response(self, scrape_func, response, request, spider):\n         fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)\n \n         def process_spider_input(response):\n-            for method in self.spider_middleware:\n+            for method in self.methods['process_spider_input']:\n                 try:\n                     result = method(response=response, spider=spider)\n                     assert result is None, \\\n@@ -72,7 +47,7 @@ class SpiderMiddlewareManager(object):\n \n         def process_spider_exception(_failure):\n             exception = _failure.value\n-            for method in self.exception_middleware:\n+            for method in self.methods['process_spider_exception']:\n                 result = method(response=response, exception=exception, spider=spider)\n                 assert result is None or _isiterable(result), \\\n                     'Middleware %s must returns None, or an iterable object, got %s ' % \\\n@@ -82,7 +57,7 @@ class SpiderMiddlewareManager(object):\n             return _failure\n \n         def process_spider_output(result):\n-            for method in self.result_middleware:\n+            for method in self.methods['process_spider_output']:\n                 result = method(response=response, result=result, spider=spider)\n                 assert _isiterable(result), \\\n                     'Middleware %s must returns an iterable object, got %s ' % \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a72e5c0512433b8bb8b5c5beb8928bde33a9966", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 66 | Files Changed: 6 | Hunks: 33 | Methods Changed: 19 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 134 | Churn Cumulative: 898 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,7 +24,6 @@ from scrapy.http import Request\n from scrapy import signals\n from scrapy.exceptions import DropItem, NotConfigured, IgnoreRequest\n from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.conf import settings\n \n \n class NoimagesDrop(DropItem):\n@@ -78,8 +77,8 @@ class FSImagesStore(object):\n \n class S3ImagesStore(object):\n \n-    AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n-    AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n+    AWS_ACCESS_KEY_ID = None\n+    AWS_SECRET_ACCESS_KEY = None\n \n     def __init__(self, uri):\n         assert uri.startswith('s3://')\n@@ -139,22 +138,33 @@ class ImagesPipeline(MediaPipeline):\n     \"\"\"\n \n     MEDIA_NAME = 'image'\n-    MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n-    MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n-    EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n-    THUMBS = settings.get('IMAGES_THUMBS', {})\n+    MIN_WIDTH = 0\n+    MIN_HEIGHT = 0\n+    EXPIRES = 90\n+    THUMBS = {}\n     STORE_SCHEMES = {\n             '': FSImagesStore,\n             'file': FSImagesStore,\n             's3': S3ImagesStore,\n             }\n \n-    def __init__(self):\n+    def __init__(self, store_uri):\n+        self.store = self._get_store(store_uri)\n+        super(ImagesPipeline, self).__init__()\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        cls.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n+        cls.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n+        cls.EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n+        cls.THUMBS = settings.get('IMAGES_THUMBS', {})\n+        s3store = cls.STORE_SCHEMES['s3']\n+        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n+        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n         store_uri = settings['IMAGES_STORE']\n         if not store_uri:\n             raise NotConfigured\n-        self.store = self._get_store(store_uri)\n-        super(ImagesPipeline, self).__init__()\n+        return cls(store_uri)\n \n     def _get_store(self, uri):\n         if os.path.isabs(uri): # to support win32 paths like: C:\\\\some\\dir\n\n@@ -6,16 +6,24 @@ See documentation in docs/topics/spider-middleware.rst\n \n from scrapy import log\n from scrapy.http import Request\n-from scrapy.stats import stats\n-from scrapy.conf import settings\n \n class DepthMiddleware(object):\n \n-    def __init__(self):\n-        self.maxdepth = settings.getint('DEPTH_LIMIT')\n-        self.stats = settings.getbool('DEPTH_STATS')\n+    def __init__(self, maxdepth, stats=None):\n+        self.maxdepth = maxdepth\n+        self.stats = stats\n         if self.stats and self.maxdepth:\n-            stats.set_value('envinfo/request_depth_limit', self.maxdepth)\n+            stats.set_value('envinfo/request_depth_limit', maxdepth)\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        maxdepth = settings.getint('DEPTH_LIMIT')\n+        usestats = settings.getbool('DEPTH_STATS')\n+        if usestats:\n+            from scrapy.stats import stats\n+        else:\n+            stats = None\n+        return cls(maxdepth, stats)\n \n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n@@ -27,14 +35,14 @@ class DepthMiddleware(object):\n                         level=log.DEBUG, spider=spider)\n                     return False\n                 elif self.stats:\n-                    stats.inc_value('request_depth_count/%s' % depth, spider=spider)\n-                    if depth > stats.get_value('request_depth_max', 0, spider=spider):\n-                        stats.set_value('request_depth_max', depth, spider=spider)\n+                    self.stats.inc_value('request_depth_count/%s' % depth, spider=spider)\n+                    if depth > self.stats.get_value('request_depth_max', 0, spider=spider):\n+                        self.stats.set_value('request_depth_max', depth, spider=spider)\n             return True\n \n         # base case (depth=0)\n         if self.stats and 'depth' not in response.request.meta: \n             response.request.meta['depth'] = 0\n-            stats.inc_value('request_depth_count/0', spider=spider)\n+            self.stats.inc_value('request_depth_count/0', spider=spider)\n \n         return (r for r in result or () if _filter(r))\n\n@@ -7,13 +7,18 @@ See documentation in docs/topics/spider-middleware.rst\n from scrapy import log\n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n-from scrapy.conf import settings\n \n class UrlLengthMiddleware(object):\n-    def __init__(self):\n-        self.maxlength = settings.getint('URLLENGTH_LIMIT')\n-        if not self.maxlength:\n+\n+    def __init__(self, maxlength):\n+        self.maxlength = maxlength\n+\n+    @classmethod\n+    def from_settings(cls, settings):\n+        maxlength = settings.getint('URLLENGTH_LIMIT')\n+        if not maxlength:\n             raise NotConfigured\n+        return cls(maxlength)\n \n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n\n@@ -1,32 +1,29 @@\n import os\n-from twisted.trial import unittest\n-from scrapy.crawler import Crawler\n-from scrapy.conf import settings\n from tempfile import mkdtemp\n from shutil import rmtree\n \n+from twisted.trial import unittest\n+\n+from scrapy.crawler import Crawler\n+from scrapy.conf import settings\n+\n \n class ImagesPipelineTestCase(unittest.TestCase):\n     def setUp(self):\n-        self.crawler = Crawler(settings)\n-        self.crawler.install()\n         try:\n             import Image\n         except ImportError, e:\n             raise unittest.SkipTest(e)\n-\n         from scrapy.contrib.pipeline.images import ImagesPipeline\n+\n+        self.crawler = Crawler(settings)\n+        self.crawler.install()\n         self.tempdir = mkdtemp()\n-        self.settings_disabled_before = settings.disabled\n-        settings.disabled = False\n-        settings.overrides['IMAGES_STORE'] = self.tempdir\n-        self.pipeline = ImagesPipeline()\n+        self.pipeline = ImagesPipeline(self.tempdir)\n \n     def tearDown(self):\n-        del self.pipeline\n-        rmtree(self.tempdir)\n-        settings.disabled = self.settings_disabled_before\n         self.crawler.uninstall()\n+        rmtree(self.tempdir)\n \n     def test_image_path(self):\n         image_path = self.pipeline.image_key\n\n@@ -1,25 +1,21 @@\n from unittest import TestCase\n \n from scrapy.contrib.spidermiddleware.depth import DepthMiddleware \n-from scrapy.conf import settings\n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n-from scrapy.stats import stats\n+from scrapy.statscol import StatsCollector\n \n \n class TestDepthMiddleware(TestCase):\n \n     def setUp(self):\n-        settings.disabled = False\n-        settings.overrides['DEPTH_LIMIT'] = 1\n-        settings.overrides['DEPTH_STATS'] = True\n-\n         self.spider = BaseSpider('scrapytest.org')\n \n-        stats.open_spider(self.spider)\n+        self.stats = StatsCollector()\n+        self.stats.open_spider(self.spider)\n \n-        self.mw = DepthMiddleware()\n-        self.assertEquals(stats.get_value('envinfo/request_depth_limit'), 1)\n+        self.mw = DepthMiddleware(1, self.stats)\n+        self.assertEquals(self.stats.get_value('envinfo/request_depth_limit'), 1)\n \n     def test_process_spider_output(self):\n         req = Request('http://scrapytest.org')\n@@ -30,7 +26,7 @@ class TestDepthMiddleware(TestCase):\n         out = list(self.mw.process_spider_output(resp, result, self.spider))\n         self.assertEquals(out, result)\n \n-        rdc = stats.get_value('request_depth_count/1', spider=self.spider)\n+        rdc = self.stats.get_value('request_depth_count/1', spider=self.spider)\n         self.assertEquals(rdc, 1)\n \n         req.meta['depth'] = 1\n@@ -38,13 +34,9 @@ class TestDepthMiddleware(TestCase):\n         out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n         self.assertEquals(out2, [])\n \n-        rdm = stats.get_value('request_depth_max', spider=self.spider)\n+        rdm = self.stats.get_value('request_depth_max', spider=self.spider)\n         self.assertEquals(rdm, 1)\n  \n     def tearDown(self):\n-        del settings.overrides['DEPTH_LIMIT']\n-        del settings.overrides['DEPTH_STATS']\n-        settings.disabled = True\n-\n-        stats.close_spider(self.spider, '')\n+        self.stats.close_spider(self.spider, '')\n \n\n@@ -1,6 +1,5 @@\n from unittest import TestCase\n \n-from scrapy.conf import settings\n from scrapy.contrib.spidermiddleware.urllength import UrlLengthMiddleware\n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n@@ -8,13 +7,6 @@ from scrapy.spider import BaseSpider\n \n class TestUrlLengthMiddleware(TestCase):\n \n-    def setUp(self):\n-        settings.disabled = False\n-        settings.overrides['URLLENGTH_LIMIT'] = 25\n-\n-        self.spider = BaseSpider('foo')\n-        self.mw = UrlLengthMiddleware()\n-\n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n \n@@ -22,10 +14,8 @@ class TestUrlLengthMiddleware(TestCase):\n         long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n         reqs = [short_url_req, long_url_req] \n \n-        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n+        mw = UrlLengthMiddleware(maxlength=25)\n+        spider = BaseSpider('foo')\n+        out = list(mw.process_spider_output(res, reqs, spider))\n         self.assertEquals(out, [short_url_req])\n \n-    def tearDown(self):\n-        del settings.overrides['URLLENGTH_LIMIT']\n-        settings.disabled = True\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3414bf13ee10aba4d8871efbabbd063fe15593d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 22 | Files Changed: 5 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 38 | Churn Cumulative: 798 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,9 +2,7 @@\n \n from twisted.internet import reactor\n \n-from scrapy import signals\n from scrapy.exceptions import NotSupported\n-from scrapy.utils.signal import send_catch_log\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n from scrapy import optional_features\n@@ -31,17 +29,8 @@ class HttpDownloadHandler(object):\n         return factory.deferred\n \n     def _create_factory(self, request, spider):\n-        def _download_signals(response):\n-            send_catch_log(signal=signals.request_uploaded, request=request, \\\n-                spider=spider)\n-            send_catch_log(signal=signals.response_downloaded, response=response, \\\n-                spider=spider)\n-            return response\n-\n         timeout = getattr(spider, \"download_timeout\", None) or self.download_timeout\n-        factory = self.httpclientfactory(request, timeout)\n-        factory.deferred.addCallbacks(_download_signals)\n-        return factory\n+        return self.httpclientfactory(request, timeout)\n \n     def _connect(self, factory):\n         host, port = factory.host, factory.port\n\n@@ -11,6 +11,8 @@ from twisted.python.failure import Failure\n from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.signal import send_catch_log\n+from scrapy import signals\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n from .handlers import DownloadHandlers\n@@ -87,10 +89,12 @@ class Downloader(object):\n             raise IgnoreRequest('Cannot fetch on a closing spider')\n \n         site.active.add(request)\n-        def _deactivate(_):\n+        def _deactivate(response):\n+            send_catch_log(signal=signals.response_received, \\\n+                response=response, request=request, spider=spider)\n             site.active.remove(request)\n             self._close_if_idle(spider)\n-            return _\n+            return response\n \n         dfd = self.middleware.download(self.enqueue, request, spider)\n         return dfd.addBoth(_deactivate)\n@@ -100,7 +104,13 @@ class Downloader(object):\n         site = self.sites[spider]\n         if site.closing:\n             raise IgnoreRequest\n-        deferred = defer.Deferred()\n+\n+        def _downloaded(response):\n+            send_catch_log(signal=signals.response_downloaded, \\\n+                    response=response, request=request, spider=spider)\n+            return response\n+\n+        deferred = defer.Deferred().addCallback(_downloaded)\n         site.queue.append((request, deferred))\n         self._process_queue(spider)\n         return deferred\n\n@@ -6,8 +6,6 @@ docs/topics/downloader-middleware.rst\n \n \"\"\"\n \n-from scrapy import signals\n-from scrapy.utils.signal import send_catch_log\n from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n@@ -77,11 +75,7 @@ class DownloaderMiddlewareManager(object):\n                     'Middleware %s.process_response must return Response or Request, got %s' % \\\n                     (method.im_self.__class__.__name__, type(response))\n                 if isinstance(response, Request):\n-                    send_catch_log(signal=signals.response_received, \\\n-                        response=response, spider=spider)\n                     return response\n-            send_catch_log(signal=signals.response_received, \\\n-                response=response, spider=spider)\n             return response\n \n         def process_exception(_failure):\n\n@@ -3,3 +3,5 @@ from scrapy.signals import *\n import warnings\n warnings.warn(\"scrapy.core.signals is deprecated and will be removed in Scrapy 0.11, use scrapy.signals instead\", \\\n     DeprecationWarning, stacklevel=2)\n+\n+request_uploaded = response_downloaded\n\n@@ -11,7 +11,6 @@ spider_opened = object()\n spider_idle = object()\n spider_closed = object()\n request_received = object()\n-request_uploaded = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9158e9d682554474d9e302e5d380f0198f9650a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 35 | Churn Cumulative: 195 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,8 @@\n+from scrapy.utils.misc import load_object\n+from .config import Config\n+\n+def get_application():\n+    config = Config()\n+    apppath = config.get('application', 'scrapyd.app.application')\n+    appfunc = load_object(apppath)\n+    return appfunc()\n\n@@ -1,7 +1,4 @@\n-import sys, os\n-\n-from twisted.internet import reactor\n-from twisted.application.service import Application, Service\n+from twisted.application.service import Application\n from twisted.application.internet import TimerService, TCPServer\n from twisted.web import server\n \n@@ -14,8 +11,8 @@ from .environ import Environment\n from .webservice import Root\n from .config import Config\n \n-def get_application():\n-    app = Application(\"Scrapy\")\n+def application():\n+    app = Application(\"Scrapyd\")\n     config = Config()\n     http_port = config.getint('http_port', 6800)\n \n\n@@ -1,23 +1,27 @@\n+import glob\n import pkgutil\n from cStringIO import StringIO\n from ConfigParser import SafeConfigParser, NoSectionError, NoOptionError\n \n-from scrapy.utils.conf import get_sources\n-\n class Config(object):\n     \"\"\"A ConfigParser wrapper to support defaults when calling instance\n     methods, and also tied to a single section\"\"\"\n \n-    SOURCES = ['scrapyd.cfg', '/etc/scrapyd.cfg']\n     SECTION = 'scrapyd'\n \n     def __init__(self):\n-        sources = self.SOURCES + get_sources()\n-        default_config = pkgutil.get_data(__package__, 'default_scrapyd.cfg')\n+        sources = self._getsources()\n+        default_config = pkgutil.get_data(__package__, 'default_scrapyd.conf')\n         self.cp = SafeConfigParser()\n         self.cp.readfp(StringIO(default_config))\n         self.cp.read(sources)\n \n+    def _getsources(self):\n+        sources = ['/etc/scrapyd/scrapyd.conf', r'c:\\scrapyd\\scrapyd.conf']\n+        sources += sorted(glob.glob('/etc/scrapyd/conf.d/*'))\n+        sources += ['scrapyd.conf']\n+        return sources\n+\n     def _getany(self, method, option, default):\n         try:\n             return method(self.SECTION, option)\n\n@@ -19,8 +19,8 @@ class Launcher(Service):\n     def startService(self):\n         for slot in range(self.max_proc):\n             self._wait_for_project(slot)\n-        log.msg(\"Launcher started: max_proc=%r, egg_runner=%r\" % \\\n-            (self.max_proc, self.egg_runner), system=\"Launcher\")\n+        log.msg(\"%s started: max_proc=%r, egg_runner=%r\" % (self.parent.name, \\\n+            self.max_proc, self.egg_runner), system=\"Launcher\")\n \n     def _wait_for_project(self, slot):\n         poller = self.app.getComponent(IPoller)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0081290f25c283215850642712949b188e1e79f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 133 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,5 +86,8 @@ class ExecutionQueue(object):\n             spider = self._spiders.create(name, **spider_kwargs)\n         except KeyError:\n             log.msg('Unable to find spider: %s' % name, log.ERROR)\n+        except:\n+            log.err(None, 'Error creating spider %r with arguments: %r' % \\\n+                (name, spider_kwargs))\n         else:\n             self.append_spider(spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37f4ce24ea7ba89875d435391b4f26b104c87ee4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,9 +26,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         if hasattr(mw, 'process_spider_input'):\n             self.methods['process_spider_input'].append(mw.process_spider_input)\n         if hasattr(mw, 'process_spider_output'):\n-            self.methods['process_spider_output'].append(mw.process_spider_output)\n+            self.methods['process_spider_output'].insert(0, mw.process_spider_output)\n         if hasattr(mw, 'process_spider_exception'):\n-            self.methods['process_spider_exception'].append(mw.process_spider_exception)\n+            self.methods['process_spider_exception'].insert(0, mw.process_spider_exception)\n \n     def scrape_response(self, scrape_func, response, request, spider):\n         fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a23b6981e8fb1056a3bf39bd062817cc879520fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 44 | Files Changed: 2 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 61 | Churn Cumulative: 623 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -74,7 +74,7 @@ class Downloader(object):\n     def __init__(self):\n         self.sites = {}\n         self.handlers = DownloadHandlers()\n-        self.middleware = DownloaderMiddlewareManager()\n+        self.middleware = DownloaderMiddlewareManager.from_settings(settings)\n         self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n \n     def fetch(self, request, spider):\n\n@@ -1,61 +1,34 @@\n \"\"\"\n-This module implements the Downloader Middleware manager. For more information\n-see the Downloader Middleware doc in:\n-\n-docs/topics/downloader-middleware.rst\n+Downloader Middleware manager\n \n+See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n-from scrapy import log\n from scrapy.http import Request, Response\n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.middleware import MiddlewareManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.conf import build_component_list\n-from scrapy.conf import settings\n \n-class DownloaderMiddlewareManager(object):\n+class DownloaderMiddlewareManager(MiddlewareManager):\n \n-    def __init__(self):\n-        self.loaded = False\n-        self.enabled = {}\n-        self.disabled = {}\n-        self.request_middleware = []\n-        self.response_middleware = []\n-        self.exception_middleware = []\n-        self.load()\n+    component_name = 'downloader middleware'\n+\n+    @classmethod\n+    def _get_mwlist_from_settings(cls, settings):\n+        return build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \\\n+            settings['DOWNLOADER_MIDDLEWARES'])\n \n     def _add_middleware(self, mw):\n         if hasattr(mw, 'process_request'):\n-            self.request_middleware.append(mw.process_request)\n+            self.methods['process_request'].append(mw.process_request)\n         if hasattr(mw, 'process_response'):\n-            self.response_middleware.insert(0, mw.process_response)\n+            self.methods['process_response'].insert(0, mw.process_response)\n         if hasattr(mw, 'process_exception'):\n-            self.exception_middleware.insert(0, mw.process_exception)\n-\n-    def load(self):\n-        \"\"\"Load middleware defined in settings module\"\"\"\n-        mwlist = build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \\\n-            settings['DOWNLOADER_MIDDLEWARES'])\n-        self.enabled.clear()\n-        self.disabled.clear()\n-        for mwpath in mwlist:\n-            try:\n-                cls = load_object(mwpath)\n-                mw = cls()\n-                self.enabled[cls.__name__] = mw\n-                self._add_middleware(mw)\n-            except NotConfigured, e:\n-                self.disabled[cls.__name__] = mwpath\n-                if e.args:\n-                    log.msg(e)\n-        log.msg(\"Enabled downloader middlewares: %s\" % \", \".join(self.enabled.keys()), \\\n-            level=log.DEBUG)\n-        self.loaded = True\n+            self.methods['process_exception'].insert(0, mw.process_exception)\n \n     def download(self, download_func, request, spider):\n         def process_request(request):\n-            for method in self.request_middleware:\n+            for method in self.methods['process_request']:\n                 response = method(request=request, spider=spider)\n                 assert response is None or isinstance(response, (Response, Request)), \\\n                         'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n@@ -69,7 +42,7 @@ class DownloaderMiddlewareManager(object):\n             if isinstance(response, Request):\n                 return response\n \n-            for method in self.response_middleware:\n+            for method in self.methods['process_response']:\n                 response = method(request=request, response=response, spider=spider)\n                 assert isinstance(response, (Response, Request)), \\\n                     'Middleware %s.process_response must return Response or Request, got %s' % \\\n@@ -80,7 +53,7 @@ class DownloaderMiddlewareManager(object):\n \n         def process_exception(_failure):\n             exception = _failure.value\n-            for method in self.exception_middleware:\n+            for method in self.methods['process_exception']:\n                 response = method(request=request, exception=exception, spider=spider)\n                 assert response is None or isinstance(response, (Response, Request)), \\\n                     'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12b04b068f1513da935d14ed58ecc1cf5eb3d263", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 15 | Files Changed: 6 | Hunks: 12 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 79 | Churn Cumulative: 1129 | Contributors (this commit): 6 | Commits (past 90d): 44 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,6 +63,7 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n     'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n     'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n+    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n     'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n     'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n\n@@ -0,0 +1,20 @@\n+\"\"\"\n+Download timeout middleware\n+\n+See documentation in docs/topics/downloader-middleware.rst\n+\"\"\"\n+from scrapy.utils.python import WeakKeyCache\n+\n+\n+class DownloadTimeoutMiddleware(object):\n+\n+    def __init__(self):\n+        self._cache = WeakKeyCache(self._download_timeout)\n+\n+    def _download_timeout(self, spider):\n+        return getattr(spider, \"download_timeout\", None)\n+\n+    def process_request(self, request, spider):\n+        timeout = self._cache[spider]\n+        if timeout:\n+            request.meta.setdefault('download_timeout', timeout)\n\n@@ -12,26 +12,19 @@ if ssl_supported:\n     from twisted.internet.ssl import ClientContextFactory\n \n HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-DOWNLOAD_TIMEOUT = settings.getint('DOWNLOAD_TIMEOUT')\n \n \n class HttpDownloadHandler(object):\n \n-    def __init__(self, httpclientfactory=HTTPClientFactory, \\\n-            download_timeout=DOWNLOAD_TIMEOUT):\n+    def __init__(self, httpclientfactory=HTTPClientFactory):\n         self.httpclientfactory = httpclientfactory\n-        self.download_timeout = download_timeout\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n-        factory = self._create_factory(request, spider)\n+        factory = self.httpclientfactory(request)\n         self._connect(factory)\n         return factory.deferred\n \n-    def _create_factory(self, request, spider):\n-        timeout = getattr(spider, \"download_timeout\", None) or self.download_timeout\n-        return self.httpclientfactory(request, timeout)\n-\n     def _connect(self, factory):\n         host, port = factory.host, factory.port\n         if factory.scheme == 'https':\n\n@@ -8,6 +8,10 @@ from twisted.internet import defer\n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.conf import settings\n+\n+\n+DOWNLOAD_TIMEOUT = settings.getint('DOWNLOAD_TIMEOUT')\n \n \n def _parsed_url_args(parsed):\n@@ -85,13 +89,13 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     followRedirect = False\n     afterFoundGet = False\n \n-    def __init__(self, request, timeout=0):\n+    def __init__(self, request, timeout=DOWNLOAD_TIMEOUT):\n         self.url = urldefrag(request.url)[0]\n         self.method = request.method\n         self.body = request.body or None\n         self.headers = Headers(request.headers)\n         self.response_headers = None\n-        self.timeout = timeout\n+        self.timeout = request.meta.get('download_timeout') or timeout\n         self.deferred = defer.Deferred().addCallback(self._build_response)\n \n         self._set_connection_attributes(request)\n\n@@ -98,10 +98,8 @@ class HttpTestCase(unittest.TestCase):\n         return d\n \n     def test_timeout_download_from_spider(self):\n-        spider = BaseSpider('foo')\n-        spider.download_timeout = 0.000001\n-        request = Request(self.getURL('wait'))\n-        d = self.download_request(request, spider)\n+        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.000001))\n+        d = self.download_request(request, BaseSpider('foo'))\n         return self.assertFailure(d, defer.TimeoutError)\n \n     def test_host_header_not_in_request_headers(self):\n\n@@ -0,0 +1,33 @@\n+import unittest\n+\n+from scrapy.contrib.downloadermiddleware.downloadtimeout import DownloadTimeoutMiddleware\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+\n+\n+class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.mw = DownloadTimeoutMiddleware()\n+        self.spider = BaseSpider('foo')\n+        self.req = Request('http://scrapytest.org/')\n+\n+    def tearDown(self):\n+        del self.mw\n+        del self.spider\n+        del self.req\n+\n+    def test_spider_has_no_download_timeout(self):\n+        assert self.mw.process_request(self.req, self.spider) is None\n+        assert 'download_timeout' not in self.req.meta\n+\n+    def test_spider_has_download_timeout(self):\n+        self.spider.download_timeout = 2\n+        assert self.mw.process_request(self.req, self.spider) is None\n+        self.assertEquals(self.req.meta.get('download_timeout'), 2)\n+\n+    def test_request_has_download_timeout(self):\n+        self.spider.download_timeout = 2\n+        self.req.meta['download_timeout'] = 1\n+        assert self.mw.process_request(self.req, self.spider) is None\n+        self.assertEquals(self.req.meta.get('download_timeout'), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
