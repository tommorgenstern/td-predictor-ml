{"custom_id": "keras#d83002415561c32afc374cef2829aa8fa094c266", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 1834 | Contributors (this commit): 17 | Commits (past 90d): 19 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,8 +23,6 @@ from .losses import sparse_categorical_crossentropy\n from .losses import binary_crossentropy\n from .losses import kullback_leibler_divergence\n from .losses import poisson\n-from .losses import cosine_proximity\n-from .losses import cosine_similarity\n from .utils import losses_utils\n from .utils import metrics_utils\n from .utils.generic_utils import deserialize_keras_object\n@@ -983,6 +981,12 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n                   K.floatx())\n \n \n+def cosine_proximity(y_true, y_pred, axis=-1):\n+    y_true = K.l2_normalize(y_true, axis=axis)\n+    y_pred = K.l2_normalize(y_pred, axis=axis)\n+    return K.sum(y_true * y_pred, axis=axis)\n+\n+\n def clone_metric(metric):\n     \"\"\"Returns a clone of the metric if stateful, otherwise returns it as is.\"\"\"\n     if isinstance(metric, Metric):\n@@ -1005,7 +1009,7 @@ mse = MSE = mean_squared_error\n mae = MAE = mean_absolute_error\n mape = MAPE = mean_absolute_percentage_error\n msle = MSLE = mean_squared_logarithmic_error\n-cosine = cosine_proximity\n+cosine = cosine_similarity = cosine_proximity\n \n \n def serialize(metric):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#088bda5902fe6f663a49461e41bbb57630cad681", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 646 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 6 | Methods Changed: 18 | Complexity Δ (Sum/Max): 67/34 | Churn Δ: 646 | Churn Cumulative: 17301 | Contributors (this commit): 139 | Commits (past 90d): 39 | Contributors (cumulative): 152 | DMM Complexity: 0.7245508982035929\n\nDIFF:\n@@ -2655,6 +2655,7 @@ def tile(x, n):\n         n = tuple(n)\n \n     shape = int_shape(x)\n+    if not is_tensor(n):\n         if len(n) < len(shape):  # Padding the axis\n             n = tuple([1 for _ in range(len(shape) - len(n))]) + n\n         elif len(n) != len(shape):\n\n@@ -941,6 +941,215 @@ class SparseCategoricalCrossentropy(MeanMetricWrapper):\n             axis=axis)\n \n \n+class _ConfusionMatrixConditionCount(Metric):\n+    \"\"\"Calculates the number of the given confusion matrix condition.\n+\n+    # Arguments\n+        confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix` conditions.\n+        thresholds: (Optional) Defaults to 0.5. A float value or a python\n+            list/tuple of float threshold values in [0, 1]. A threshold is compared\n+            with prediction values to determine the truth value of predictions\n+            (i.e., above the threshold is `true`, below is `false`). One metric\n+            value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 confusion_matrix_cond,\n+                 thresholds=None,\n+                 name=None,\n+                 dtype=None):\n+        super(_ConfusionMatrixConditionCount, self).__init__(name=name, dtype=dtype)\n+        self._confusion_matrix_cond = confusion_matrix_cond\n+        self.init_thresholds = thresholds\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=0.5)\n+        self.accumulator = self.add_weight(\n+            'accumulator',\n+            shape=(len(self.thresholds),),\n+            initializer='zeros')\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {self._confusion_matrix_cond: self.accumulator},\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            sample_weight=sample_weight)\n+\n+    def result(self):\n+        if len(self.thresholds) == 1:\n+            return self.accumulator[0]\n+        return self.accumulator\n+\n+    def reset_states(self):\n+        num_thresholds = len(to_list(self.thresholds))\n+        K.batch_set_value(\n+            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+\n+    def get_config(self):\n+        config = {'thresholds': self.init_thresholds}\n+        base_config = super(_ConfusionMatrixConditionCount, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class FalsePositives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of false positives.\n+\n+    For example, if `y_true` is [0, 1, 0, 0] and `y_pred` is [0, 0, 1, 1]\n+    then the false positives value is 2.  If the weights were specified as\n+    [0, 0, 1, 0] then the false positives value would be 1.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    false positives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of false positives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.FalsePositives()])\n+    ```\n+\n+    # Arguments\n+        thresholds: (Optional) Defaults to 0.5. A float value or a python\n+            list/tuple of float threshold values in [0, 1]. A threshold is\n+            compared with prediction values to determine the truth value of\n+            predictions (i.e., above the threshold is `true`, below is `false`).\n+            One metric value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super(FalsePositives, self).__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype)\n+\n+\n+class TruePositives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of true positives.\n+\n+    For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\n+    then the true positives value is 2.  If the weights were specified as\n+    [0, 0, 1, 0] then the true positives value would be 1.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    true positives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of true positives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.TruePositives()])\n+    ```\n+\n+    # Arguments\n+        thresholds: (Optional) Defaults to 0.5. A float value or a python\n+            list/tuple of float threshold values in [0, 1]. A threshold is compared\n+            with prediction values to determine the truth value of predictions\n+            (i.e., above the threshold is `true`, below is `false`). One metric\n+            value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super(TruePositives, self).__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype)\n+\n+\n+class TrueNegatives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of true negatives.\n+\n+    For example, if `y_true` is [0, 1, 0, 0] and `y_pred` is [1, 1, 0, 0]\n+    then the true negatives value is 2.  If the weights were specified as\n+    [0, 0, 1, 0] then the true negatives value would be 1.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    true negatives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of true negatives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.TrueNegatives()])\n+    ```\n+\n+    # Arguments\n+        thresholds: (Optional) Defaults to 0.5. A float value or a python\n+            list/tuple of float threshold values in [0, 1]. A threshold is compared\n+            with prediction values to determine the truth value of predictions\n+            (i.e., above the threshold is `true`, below is `false`). One metric\n+            value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super(TrueNegatives, self).__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype)\n+\n+\n+class FalseNegatives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of false negatives.\n+\n+    For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [0, 1, 0, 0]\n+    then the false negatives value is 2.  If the weights were specified as\n+    [0, 0, 1, 0] then the false negatives value would be 1.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    false negatives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of false negatives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.FalseNegatives()])\n+    ```\n+\n+    # Arguments\n+        thresholds: (Optional) Defaults to 0.5. A float value or a python\n+            list/tuple of float threshold values in [0, 1]. A threshold is compared\n+            with prediction values to determine the truth value of predictions\n+            (i.e., above the threshold is `true`, below is `false`). One metric\n+            value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super(FalseNegatives, self).__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype)\n+\n+\n def accuracy(y_true, y_pred):\n     if not K.is_tensor(y_pred):\n         y_pred = K.constant(y_pred)\n\n@@ -3,7 +3,10 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+from enum import Enum\n+\n from .. import backend as K\n+from . import losses_utils\n \n \n class Reduction(object):\n@@ -60,3 +63,193 @@ def result_wrapper(result_fn):\n         return result_t\n \n     return decorated\n+\n+\n+def to_list(x):\n+    if isinstance(x, list):\n+        return x\n+    return [x]\n+\n+\n+def assert_thresholds_range(thresholds):\n+    if thresholds is not None:\n+        invalid_thresholds = [t for t in thresholds if t is None or t < 0 or t > 1]\n+    if invalid_thresholds:\n+        raise ValueError(\n+            'Threshold values must be in [0, 1]. Invalid values: {}'.format(\n+                invalid_thresholds))\n+\n+\n+def parse_init_thresholds(thresholds, default_threshold=0.5):\n+    if thresholds is not None:\n+        assert_thresholds_range(to_list(thresholds))\n+    thresholds = to_list(default_threshold if thresholds is None else thresholds)\n+    return thresholds\n+\n+\n+class ConfusionMatrix(Enum):\n+    TRUE_POSITIVES = 'tp'\n+    FALSE_POSITIVES = 'fp'\n+    TRUE_NEGATIVES = 'tn'\n+    FALSE_NEGATIVES = 'fn'\n+\n+\n+def weighted_assign_add(label, pred, weights, var):\n+    label = K.expand_dims(label, 0)\n+    pred = K.expand_dims(pred, 0)\n+    are_different = K.concatenate([label, pred], axis=0)\n+    label_and_pred = K.all(are_different, axis=0)\n+\n+    label_and_pred = K.cast(label_and_pred, dtype=K.floatx())\n+    if weights is not None:\n+        label_and_pred *= weights\n+    return var.assign_add(K.sum(label_and_pred, 1))\n+\n+\n+def update_confusion_matrix_variables(variables_to_update,\n+                                      y_true,\n+                                      y_pred,\n+                                      thresholds,\n+                                      top_k=None,\n+                                      class_id=None,\n+                                      sample_weight=None):\n+    \"\"\"Returns op to update the given confusion matrix variables.\n+\n+    For every pair of values in y_true and y_pred:\n+\n+    true_positive: y_true == True and y_pred > thresholds\n+    false_negatives: y_true == True and y_pred <= thresholds\n+    true_negatives: y_true == False and y_pred <= thresholds\n+    false_positive: y_true == False and y_pred > thresholds\n+\n+    The results will be weighted and added together. When multiple thresholds are\n+    provided, we will repeat the same for every threshold.\n+\n+    For estimation of these metrics over a stream of data, the function creates an\n+    `update_op` operation that updates the given variables.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use weights of 0 to mask values.\n+\n+    # Arguments\n+    variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n+      and corresponding variables to update as values.\n+    y_true: A `Tensor` whose shape matches `y_pred`. Will be cast to `bool`.\n+    y_pred: A floating point `Tensor` of arbitrary shape and whose values are in\n+      the range `[0, 1]`.\n+    thresholds: A float value or a python list or tuple of float thresholds in\n+      `[0, 1]`, or NEG_INF (used when top_k is set).\n+    top_k: Optional int, indicates that the positive labels should be limited to\n+      the top k predictions.\n+    class_id: Optional int, limits the prediction and labels to the class\n+      specified by this argument.\n+    sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\n+      `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions must\n+      be either `1`, or the same as the corresponding `y_true` dimension).\n+\n+    # Returns\n+        Update ops.\n+\n+    # Raises\n+        ValueError: If `y_pred` and `y_true` have mismatched shapes, or if\n+            `sample_weight` is not `None` and its shape doesn't match `y_pred`, or if\n+            `variables_to_update` contains invalid keys.\n+    \"\"\"\n+    if variables_to_update is None:\n+        return\n+    y_true = K.cast(y_true, dtype=K.floatx())\n+    y_pred = K.cast(y_pred, dtype=K.floatx())\n+    if sample_weight is not None:\n+        sample_weight = K.cast(sample_weight, dtype=K.floatx())\n+\n+    if not any(key\n+               for key in variables_to_update\n+               if key in list(ConfusionMatrix)):\n+        raise ValueError(\n+            'Please provide at least one valid confusion matrix '\n+            'variable to update. Valid variable key options are: \"{}\". '\n+            'Received: \"{}\"'.format(\n+                list(ConfusionMatrix), variables_to_update.keys()))\n+\n+    invalid_keys = [\n+        key for key in variables_to_update if key not in list(ConfusionMatrix)\n+    ]\n+    if invalid_keys:\n+        raise ValueError(\n+            'Invalid keys: {}. Valid variable key options are: \"{}\"'.format(\n+                invalid_keys, list(ConfusionMatrix)))\n+\n+    if sample_weight is None:\n+        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n+            y_pred, y_true=y_true)\n+    else:\n+        y_pred, y_true, sample_weight = (\n+            losses_utils.squeeze_or_expand_dimensions(\n+                y_pred, y_true=y_true, sample_weight=sample_weight))\n+\n+    if top_k is not None:\n+        y_pred = _filter_top_k(y_pred, top_k)\n+    if class_id is not None:\n+        y_true = y_true[..., class_id]\n+        y_pred = y_pred[..., class_id]\n+\n+    thresholds = to_list(thresholds)\n+    num_thresholds = len(thresholds)\n+    num_predictions = K.size(y_pred)\n+\n+    # Reshape predictions and labels.\n+    predictions_2d = K.reshape(y_pred, [1, -1])\n+    labels_2d = K.reshape(\n+        K.cast(y_true, dtype='bool'), [1, -1])\n+\n+    # Tile the thresholds for every prediction.\n+    thresh_tiled = K.tile(\n+        K.expand_dims(K.constant(thresholds), 1),\n+        K.stack([1, num_predictions]))\n+\n+    # Tile the predictions for every threshold.\n+    preds_tiled = K.tile(predictions_2d, [num_thresholds, 1])\n+\n+    # Compare predictions and threshold.\n+    pred_is_pos = K.greater(preds_tiled, thresh_tiled)\n+    pred_is_neg = K.greater(thresh_tiled, preds_tiled)\n+\n+    # Tile labels by number of thresholds\n+    label_is_pos = K.tile(labels_2d, [num_thresholds, 1])\n+\n+    if sample_weight is not None:\n+        weights = losses_utils.broadcast_weights(\n+            y_pred, K.cast(sample_weight, dtype=K.floatx()))\n+        weights_tiled = K.tile(\n+            K.reshape(weights, [1, -1]), [num_thresholds, 1])\n+\n+        print('label_is_pos: ', label_is_pos)\n+        print('pred_is_pos: ', pred_is_pos)\n+        print('Sample_weight: ', weights_tiled)\n+    else:\n+        weights_tiled = None\n+\n+    update_ops = []\n+    loop_vars = {\n+        ConfusionMatrix.TRUE_POSITIVES: (label_is_pos, pred_is_pos),\n+    }\n+    update_tn = ConfusionMatrix.TRUE_NEGATIVES in variables_to_update\n+    update_fp = ConfusionMatrix.FALSE_POSITIVES in variables_to_update\n+    update_fn = ConfusionMatrix.FALSE_NEGATIVES in variables_to_update\n+\n+    if update_fn or update_tn:\n+        loop_vars[ConfusionMatrix.FALSE_NEGATIVES] = (label_is_pos, pred_is_neg)\n+\n+    if update_fp or update_tn:\n+        label_is_neg = K.equal(\n+            label_is_pos, K.zeros_like(label_is_pos, dtype=label_is_pos.dtype))\n+        loop_vars[ConfusionMatrix.FALSE_POSITIVES] = (label_is_neg, pred_is_pos)\n+        if update_tn:\n+            loop_vars[ConfusionMatrix.TRUE_NEGATIVES] = (label_is_neg, pred_is_neg)\n+\n+    for matrix_cond, (label, pred) in loop_vars.items():\n+        if matrix_cond in variables_to_update:\n+            update_ops.append(\n+                weighted_assign_add(label, pred, weights_tiled,\n+                                    variables_to_update[matrix_cond]))\n+    return update_ops\n\n@@ -0,0 +1,243 @@\n+\"\"\"Tests for Keras confusion matrix metrics classes.\"\"\"\n+import pytest\n+import numpy as np\n+\n+from keras import metrics\n+from keras import backend as K\n+\n+if K.backend() != 'tensorflow':\n+    # Need TensorFlow to use metric.__call__\n+    pytestmark = pytest.mark.skip\n+\n+\n+class TestFalsePositives(object):\n+\n+    def test_config(self):\n+        fp_obj = metrics.FalsePositives(name='my_fp', thresholds=[0.4, 0.9])\n+        assert fp_obj.name == 'my_fp'\n+        assert len(fp_obj.weights) == 1\n+        assert fp_obj.thresholds == [0.4, 0.9]\n+\n+        # Check save and restore config\n+        fp_obj2 = metrics.FalsePositives.from_config(fp_obj.get_config())\n+        assert fp_obj2.name == 'my_fp'\n+        assert len(fp_obj2.weights) == 1\n+        assert fp_obj2.thresholds == [.4, 0.9]\n+\n+    def test_unweighted(self):\n+        fp_obj = metrics.FalsePositives()\n+\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+\n+        result = fp_obj(y_true, y_pred)\n+        assert np.allclose(7., K.eval(result))\n+\n+    def test_weighted(self):\n+        fp_obj = metrics.FalsePositives()\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        sample_weight = (1., 1.5, 2., 2.5)\n+        result = fp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(14., K.eval(result))\n+\n+    def test_unweighted_with_thresholds(self):\n+        fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+\n+        result = fp_obj(y_true, y_pred)\n+        assert np.allclose([7., 4., 2.], K.eval(result))\n+\n+    def test_weighted_with_thresholds(self):\n+        fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        sample_weight = ((1.0, 2.0, 3.0, 5.0), (7.0, 11.0, 13.0, 17.0),\n+                         (19.0, 23.0, 29.0, 31.0), (5.0, 15.0, 10.0, 0))\n+\n+        result = fp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose([125., 42., 12.], K.eval(result))\n+\n+    def test_threshold_limit(self):\n+        with pytest.raises(Exception):\n+            metrics.FalsePositives(thresholds=[-1, 0.5, 2])\n+\n+        with pytest.raises(Exception):\n+            metrics.FalsePositives(thresholds=[None])\n+\n+\n+class TestTruePositives(object):\n+\n+    def test_config(self):\n+        tp_obj = metrics.TruePositives(name='my_tp', thresholds=[0.4, 0.9])\n+        assert tp_obj.name == 'my_tp'\n+        assert len(tp_obj.weights) == 1\n+        assert tp_obj.thresholds == [0.4, 0.9]\n+\n+        # Check save and restore config\n+        tp_obj2 = metrics.TruePositives.from_config(tp_obj.get_config())\n+        assert tp_obj2.name == 'my_tp'\n+        assert len(tp_obj2.weights) == 1\n+        assert tp_obj2.thresholds == [0.4, 0.9]\n+\n+    def test_unweighted(self):\n+        tp_obj = metrics.TruePositives()\n+\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n+                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n+                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+\n+        result = tp_obj(y_true, y_pred)\n+        assert np.allclose(7., K.eval(result))\n+\n+    def test_weighted(self):\n+        tp_obj = metrics.TruePositives()\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n+                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n+                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        sample_weight = (1., 1.5, 2., 2.5)\n+        result = tp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(12., K.eval(result))\n+\n+    def test_unweighted_with_thresholds(self):\n+        tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),\n+                  (1, 1, 1, 1))\n+\n+        result = tp_obj(y_true, y_pred)\n+        assert np.allclose([6., 3., 1.], K.eval(result))\n+\n+    def test_weighted_with_thresholds(self):\n+        tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),\n+                  (1, 1, 1, 1))\n+\n+        result = tp_obj(y_true, y_pred, sample_weight=37.)\n+        assert np.allclose([222., 111., 37.], K.eval(result))\n+\n+\n+class TestTrueNegatives(object):\n+\n+    def test_config(self):\n+        tn_obj = metrics.TrueNegatives(name='my_tn', thresholds=[0.4, 0.9])\n+        assert tn_obj.name == 'my_tn'\n+        assert len(tn_obj.weights) == 1\n+        assert tn_obj.thresholds == [0.4, 0.9]\n+\n+        # Check save and restore config\n+        tn_obj2 = metrics.TrueNegatives.from_config(tn_obj.get_config())\n+        assert tn_obj2.name == 'my_tn'\n+        assert len(tn_obj2.weights) == 1\n+        assert tn_obj2.thresholds == [0.4, 0.9]\n+\n+    def test_unweighted(self):\n+        tn_obj = metrics.TrueNegatives()\n+\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n+                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n+                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+\n+        result = tn_obj(y_true, y_pred)\n+        assert np.allclose(3., K.eval(result))\n+\n+    def test_weighted(self):\n+        tn_obj = metrics.TrueNegatives()\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n+                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n+                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        sample_weight = (1., 1.5, 2., 2.5)\n+        result = tn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(4., K.eval(result))\n+\n+    def test_unweighted_with_thresholds(self):\n+        tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+\n+        result = tn_obj(y_true, y_pred)\n+        assert np.allclose([2., 5., 7.], K.eval(result))\n+\n+    def test_weighted_with_thresholds(self):\n+        tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        sample_weight = ((0.0, 2.0, 3.0, 5.0),)\n+\n+        result = tn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose([5., 15., 23.], K.eval(result))\n+\n+\n+class TestFalseNegatives(object):\n+\n+    def test_config(self):\n+        fn_obj = metrics.FalseNegatives(name='my_fn', thresholds=[0.4, 0.9])\n+        assert fn_obj.name == 'my_fn'\n+        assert len(fn_obj.weights) == 1\n+        assert fn_obj.thresholds == [0.4, 0.9]\n+\n+        # Check save and restore config\n+        fn_obj2 = metrics.FalseNegatives.from_config(fn_obj.get_config())\n+        assert fn_obj2.name == 'my_fn'\n+        assert len(fn_obj2.weights) == 1\n+        assert fn_obj2.thresholds == [0.4, 0.9]\n+\n+    def test_unweighted(self):\n+        fn_obj = metrics.FalseNegatives()\n+\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n+                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n+                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+\n+        result = fn_obj(y_true, y_pred)\n+        assert np.allclose(3., K.eval(result))\n+\n+    def test_weighted(self):\n+        fn_obj = metrics.FalseNegatives()\n+        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n+                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n+                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        sample_weight = (1., 1.5, 2., 2.5)\n+        result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(5., K.eval(result))\n+\n+    def test_unweighted_with_thresholds(self):\n+        fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+\n+        result = fn_obj(y_true, y_pred)\n+        assert np.allclose([1., 4., 6.], K.eval(result))\n+\n+    def test_weighted_with_thresholds(self):\n+        fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n+                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n+        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        sample_weight = ((3.0,), (5.0,), (7.0,), (4.0,))\n+\n+        result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose([4., 16., 23.], K.eval(result))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63c0369b2466cdf6002b821351b3ea0ab42cd132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 722 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 7 | Methods Changed: 30 | Complexity Δ (Sum/Max): 62/33 | Churn Δ: 726 | Churn Cumulative: 3273 | Contributors (this commit): 17 | Commits (past 90d): 31 | Contributors (cumulative): 20 | DMM Complexity: 0.7774798927613941\n\nDIFF:\n@@ -1150,6 +1150,396 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n             dtype=dtype)\n \n \n+class SensitivitySpecificityBase(Metric):\n+    \"\"\"Abstract base class for computing sensitivity and specificity.\n+\n+    For additional information about specificity and sensitivity, see the\n+    following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n+    \"\"\"\n+\n+    def __init__(self, value, num_thresholds=200, name=None, dtype=None):\n+        super(SensitivitySpecificityBase, self).__init__(name=name, dtype=dtype)\n+        if num_thresholds <= 0:\n+            raise ValueError('`num_thresholds` must be > 0.')\n+        self.value = value\n+        self.true_positives = self.add_weight(\n+            'true_positives',\n+            shape=(num_thresholds,),\n+            initializer='zeros')\n+        self.true_negatives = self.add_weight(\n+            'true_negatives',\n+            shape=(num_thresholds,),\n+            initializer='zeros')\n+        self.false_positives = self.add_weight(\n+            'false_positives',\n+            shape=(num_thresholds,),\n+            initializer='zeros')\n+        self.false_negatives = self.add_weight(\n+            'false_negatives',\n+            shape=(num_thresholds,),\n+            initializer='zeros')\n+\n+        # Compute `num_thresholds` thresholds in [0, 1]\n+        if num_thresholds == 1:\n+            self.thresholds = [0.5]\n+        else:\n+            thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n+                          for i in range(num_thresholds - 2)]\n+            self.thresholds = [0.0] + thresholds + [1.0]\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n+                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            sample_weight=sample_weight)\n+\n+    def reset_states(self):\n+        num_thresholds = len(self.thresholds)\n+        K.batch_set_value(\n+            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+\n+\n+class SensitivityAtSpecificity(SensitivitySpecificityBase):\n+    \"\"\"Computes the sensitivity at a given specificity.\n+\n+    `Sensitivity` measures the proportion of actual positives that are correctly\n+    identified as such (tp / (tp + fn)).\n+    `Specificity` measures the proportion of actual negatives that are correctly\n+    identified as such (tn / (tn + fp)).\n+\n+    This metric creates four local variables, `true_positives`, `true_negatives`,\n+    `false_positives` and `false_negatives` that are used to compute the\n+    sensitivity at the given specificity. The threshold for the given specificity\n+    value is computed and used to evaluate the corresponding sensitivity.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    For additional information about specificity and sensitivity, see the\n+    following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        'sgd',\n+        loss='mse',\n+        metrics=[keras.metrics.SensitivityAtSpecificity()])\n+    ```\n+\n+    # Arguments\n+        specificity: A scalar value in range `[0, 1]`.\n+        num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+            use for matching the given specificity.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, specificity, num_thresholds=200, name=None, dtype=None):\n+        if specificity < 0 or specificity > 1:\n+            raise ValueError('`specificity` must be in the range [0, 1].')\n+        self.specificity = specificity\n+        self.num_thresholds = num_thresholds\n+        super(SensitivityAtSpecificity, self).__init__(\n+            specificity, num_thresholds=num_thresholds, name=name, dtype=dtype)\n+\n+    def result(self):\n+        # Calculate specificities at all the thresholds.\n+        specificities = K.switch(\n+            K.greater(self.true_negatives + self.false_positives, 0),\n+            (self.true_negatives / (self.true_negatives + self.false_positives)),\n+            K.zeros_like(self.thresholds))\n+\n+        # Find the index of the threshold where the specificity is closest to the\n+        # given specificity.\n+        min_index = K.argmin(\n+            K.abs(specificities - self.value), axis=0)\n+        min_index = K.cast(min_index, 'int32')\n+\n+        # Compute sensitivity at that index.\n+        return K.switch(\n+            K.greater((self.true_positives[min_index] +\n+                       self.false_negatives[min_index]), 0),\n+            (self.true_positives[min_index] /\n+                (self.true_positives[min_index] + self.false_negatives[min_index])),\n+            K.zeros_like(self.true_positives[min_index]))\n+\n+    def get_config(self):\n+        config = {\n+            'num_thresholds': self.num_thresholds,\n+            'specificity': self.specificity\n+        }\n+        base_config = super(SensitivityAtSpecificity, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class AUC(Metric):\n+    \"\"\"Computes the approximate AUC (Area under the curve) via a Riemann sum.\n+\n+    This metric creates four local variables, `true_positives`, `true_negatives`,\n+    `false_positives` and `false_negatives` that are used to compute the AUC.\n+    To discretize the AUC curve, a linearly spaced set of thresholds is used to\n+    compute pairs of recall and precision values. The area under the ROC-curve is\n+    therefore computed using the height of the recall values by the false positive\n+    rate, while the area under the PR-curve is the computed using the height of\n+    the precision values by the recall.\n+\n+    This value is ultimately returned as `auc`, an idempotent operation that\n+    computes the area under a discretized curve of precision versus recall values\n+    (computed using the aforementioned variables). The `num_thresholds` variable\n+    controls the degree of discretization with larger numbers of thresholds more\n+    closely approximating the true AUC. The quality of the approximation may vary\n+    dramatically depending on `num_thresholds`. The `thresholds` parameter can be\n+    used to manually specify thresholds which split the predictions more evenly.\n+\n+    For best results, `predictions` should be distributed approximately uniformly\n+    in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC\n+    approximation may be poor if this is not the case. Setting `summation_method`\n+    to 'minoring' or 'majoring' can help quantify the error in the approximation\n+    by providing lower or upper bound estimate of the AUC.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.AUC()])\n+    ```\n+\n+    # Arguments\n+        num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+            use when discretizing the roc curve. Values must be > 1.\n+            curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n+            [default] or 'PR' for the Precision-Recall-curve.\n+        summation_method: (Optional) Specifies the Riemann summation method used\n+            (https://en.wikipedia.org/wiki/Riemann_sum): 'interpolation' [default],\n+              applies mid-point summation scheme for `ROC`. For PR-AUC, interpolates\n+              (true/false) positives but not the ratio that is precision (see Davis\n+              & Goadrich 2006 for details); 'minoring' that applies left summation\n+              for increasing intervals and right summation for decreasing intervals;\n+              'majoring' that does the opposite.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        thresholds: (Optional) A list of floating point values to use as the\n+            thresholds for discretizing the curve. If set, the `num_thresholds`\n+            parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n+            equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n+            be automatically included with these to correctly handle predictions\n+            equal to exactly 0 or 1.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 num_thresholds=200,\n+                 curve='ROC',\n+                 summation_method='interpolation',\n+                 name=None,\n+                 dtype=None,\n+                 thresholds=None):\n+        # Validate configurations.\n+        if (isinstance(curve, metrics_utils.AUCCurve) and\n+                curve not in list(metrics_utils.AUCCurve)):\n+            raise ValueError('Invalid curve: \"{}\". Valid options are: \"{}\"'.format(\n+                curve, list(metrics_utils.AUCCurve)))\n+        if isinstance(\n+            summation_method,\n+            metrics_utils.AUCSummationMethod) and summation_method not in list(\n+                metrics_utils.AUCSummationMethod):\n+            raise ValueError(\n+                'Invalid summation method: \"{}\". Valid options are: \"{}\"'.format(\n+                    summation_method, list(metrics_utils.AUCSummationMethod)))\n+\n+        # Update properties.\n+        if thresholds is not None:\n+            # If specified, use the supplied thresholds.\n+            self.num_thresholds = len(thresholds) + 2\n+            thresholds = sorted(thresholds)\n+        else:\n+            if num_thresholds <= 1:\n+                raise ValueError('`num_thresholds` must be > 1.')\n+\n+            # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n+            # (0, 1).\n+            self.num_thresholds = num_thresholds\n+            thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n+                          for i in range(num_thresholds - 2)]\n+\n+        # Add an endpoint \"threshold\" below zero and above one for either\n+        # threshold method to account for floating point imprecisions.\n+        self.thresholds = [0.0 - K.epsilon()] + thresholds + [1.0 + K.epsilon()]\n+\n+        if isinstance(curve, metrics_utils.AUCCurve):\n+            self.curve = curve\n+        else:\n+            self.curve = metrics_utils.AUCCurve.from_str(curve)\n+        if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n+            self.summation_method = summation_method\n+        else:\n+            self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n+                summation_method)\n+        super(AUC, self).__init__(name=name, dtype=dtype)\n+\n+        # Create metric variables\n+        self.true_positives = self.add_weight(\n+            'true_positives',\n+            shape=(self.num_thresholds,),\n+            initializer='zeros')\n+        self.true_negatives = self.add_weight(\n+            'true_negatives',\n+            shape=(self.num_thresholds,),\n+            initializer='zeros')\n+        self.false_positives = self.add_weight(\n+            'false_positives',\n+            shape=(self.num_thresholds,),\n+            initializer='zeros')\n+        self.false_negatives = self.add_weight(\n+            'false_negatives',\n+            shape=(self.num_thresholds,),\n+            initializer='zeros')\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        return metrics_utils.update_confusion_matrix_variables({\n+            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n+            metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,\n+            metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n+            metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,\n+        }, y_true, y_pred, self.thresholds, sample_weight=sample_weight)\n+\n+    def interpolate_pr_auc(self):\n+        \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n+\n+        https://www.biostat.wisc.edu/~page/rocpr.pdf\n+\n+        Note here we derive & use a closed formula not present in the paper\n+        as follows:\n+\n+          Precision = TP / (TP + FP) = TP / P\n+\n+        Modeling all of TP (true positive), FP (false positive) and their sum\n+        P = TP + FP (predicted positive) as varying linearly within each interval\n+        [A, B] between successive thresholds, we get\n+\n+          Precision slope = dTP / dP\n+                          = (TP_B - TP_A) / (P_B - P_A)\n+                          = (TP - TP_A) / (P - P_A)\n+          Precision = (TP_A + slope * (P - P_A)) / P\n+\n+        The area within the interval is (slope / total_pos_weight) times\n+\n+          int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n+          int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n+\n+        where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n+\n+          int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n+\n+        Bringing back the factor (slope / total_pos_weight) we'd put aside, we get\n+\n+          slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n+\n+        where dTP == TP_B - TP_A.\n+\n+        Note that when P_A == 0 the above calculation simplifies into\n+\n+          int_A^B{Precision.dTP} = int_A^B{slope * dTP} = slope * (TP_B - TP_A)\n+\n+        which is really equivalent to imputing constant precision throughout the\n+        first bucket having >0 true positives.\n+\n+        # Returns\n+            pr_auc: an approximation of the area under the P-R curve.\n+        \"\"\"\n+        dtp = self.true_positives[:self.num_thresholds -\n+                                  1] - self.true_positives[1:]\n+        p = self.true_positives + self.false_positives\n+        dp = p[:self.num_thresholds - 1] - p[1:]\n+\n+        prec_slope = dtp / K.maximum(dp, 0)\n+        intercept = self.true_positives[1:] - (prec_slope * p[1:])\n+\n+        # Logical and\n+        pMin = K.expand_dims(p[:self.num_thresholds - 1] > 0, 0)\n+        pMax = K.expand_dims(p[1:] > 0, 0)\n+        are_different = K.concatenate([pMin, pMax], axis=0)\n+        switch_condition = K.all(are_different, axis=0)\n+\n+        safe_p_ratio = K.switch(\n+            switch_condition,\n+            p[:self.num_thresholds - 1] / K.maximum(p[1:], 0),\n+            K.ones_like(p[1:]))\n+\n+        numer = prec_slope * (dtp + intercept * K.log(safe_p_ratio))\n+        denom = K.maximum(self.true_positives[1:] + self.false_negatives[1:], 0)\n+        return K.sum((numer / denom))\n+\n+    def result(self):\n+        if (self.curve == metrics_utils.AUCCurve.PR and\n+                (self.summation_method ==\n+                 metrics_utils.AUCSummationMethod.INTERPOLATION)):\n+            # This use case is different and is handled separately.\n+            return self.interpolate_pr_auc()\n+\n+        # Set `x` and `y` values for the curves based on `curve` config.\n+        recall = K.switch(\n+            K.greater((self.true_positives), 0),\n+            (self.true_positives /\n+                (self.true_positives + self.false_negatives)),\n+            K.zeros_like(self.true_positives))\n+        if self.curve == metrics_utils.AUCCurve.ROC:\n+            fp_rate = K.switch(\n+                K.greater((self.false_positives), 0),\n+                (self.false_positives /\n+                    (self.false_positives + self.true_negatives)),\n+                K.zeros_like(self.false_positives))\n+            x = fp_rate\n+            y = recall\n+        else:  # curve == 'PR'.\n+            precision = K.switch(\n+                K.greater((self.true_positives), 0),\n+                (self.true_positives / (self.true_positives + self.false_positives)),\n+                K.zeros_like(self.true_positives))\n+            x = recall\n+            y = precision\n+\n+        # Find the rectangle heights based on `summation_method`.\n+        if self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION:\n+            # Note: the case ('PR', 'interpolation') has been handled above.\n+            heights = (y[:self.num_thresholds - 1] + y[1:]) / 2.\n+        elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n+            heights = K.minimum(y[:self.num_thresholds - 1], y[1:])\n+        else:  # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n+            heights = K.maximum(y[:self.num_thresholds - 1], y[1:])\n+\n+        # Sum up the areas of all the rectangles.\n+        return K.sum((x[:self.num_thresholds - 1] - x[1:]) * heights)\n+\n+    def reset_states(self):\n+        K.batch_set_value(\n+            [(v, np.zeros((self.num_thresholds,))) for v in self.variables])\n+\n+    def get_config(self):\n+        config = {\n+            'num_thresholds': self.num_thresholds,\n+            'curve': self.curve.value,\n+            'summation_method': self.summation_method.value,\n+            # We remove the endpoint thresholds as an inverse of how the thresholds\n+            # were initialized. This ensures that a metric initialized from this\n+            # config has the same thresholds.\n+            'thresholds': self.thresholds[1:-1],\n+        }\n+        base_config = super(AUC, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n def accuracy(y_true, y_pred):\n     if not K.is_tensor(y_pred):\n         y_pred = K.constant(y_pred)\n\n@@ -9,6 +9,9 @@ from .. import backend as K\n from . import losses_utils\n \n \n+NEG_INF = -1e10\n+\n+\n class Reduction(object):\n     \"\"\"Types of metrics reduction.\n \n@@ -94,7 +97,53 @@ class ConfusionMatrix(Enum):\n     FALSE_NEGATIVES = 'fn'\n \n \n+class AUCCurve(Enum):\n+    \"\"\"Type of AUC Curve (ROC or PR).\"\"\"\n+    ROC = 'ROC'\n+    PR = 'PR'\n+\n+    @staticmethod\n+    def from_str(key):\n+        if key in ('pr', 'PR'):\n+            return AUCCurve.PR\n+        elif key in ('roc', 'ROC'):\n+            return AUCCurve.ROC\n+        else:\n+            raise ValueError('Invalid AUC curve value \"%s\".' % key)\n+\n+\n+class AUCSummationMethod(Enum):\n+    \"\"\"Type of AUC summation method.\n+\n+    https://en.wikipedia.org/wiki/Riemann_sum)\n+\n+    Contains the following values:\n+    * 'interpolation': Applies mid-point summation scheme for `ROC` curve. For\n+    `PR` curve, interpolates (true/false) positives but not the ratio that is\n+    precision (see Davis & Goadrich 2006 for details).\n+    * 'minoring': Applies left summation for increasing intervals and right\n+    summation for decreasing intervals.\n+    * 'majoring': Applies right summation for increasing intervals and left\n+    summation for decreasing intervals.\n+    \"\"\"\n+    INTERPOLATION = 'interpolation'\n+    MAJORING = 'majoring'\n+    MINORING = 'minoring'\n+\n+    @staticmethod\n+    def from_str(key):\n+        if key in ('interpolation', 'Interpolation'):\n+            return AUCSummationMethod.INTERPOLATION\n+        elif key in ('majoring', 'Majoring'):\n+            return AUCSummationMethod.MAJORING\n+        elif key in ('minoring', 'Minoring'):\n+            return AUCSummationMethod.MINORING\n+        else:\n+            raise ValueError('Invalid AUC summation method value \"%s\".' % key)\n+\n+\n def weighted_assign_add(label, pred, weights, var):\n+    # Logical and\n     label = K.expand_dims(label, 0)\n     pred = K.expand_dims(pred, 0)\n     are_different = K.concatenate([label, pred], axis=0)\n@@ -222,10 +271,6 @@ def update_confusion_matrix_variables(variables_to_update,\n             y_pred, K.cast(sample_weight, dtype=K.floatx()))\n         weights_tiled = K.tile(\n             K.reshape(weights, [1, -1]), [num_thresholds, 1])\n-\n-        print('label_is_pos: ', label_is_pos)\n-        print('pred_is_pos: ', pred_is_pos)\n-        print('Sample_weight: ', weights_tiled)\n     else:\n         weights_tiled = None\n \n\n@@ -4,6 +4,7 @@ import numpy as np\n \n from keras import metrics\n from keras import backend as K\n+from keras.utils import metrics_utils\n \n if K.backend() != 'tensorflow':\n     # Need TensorFlow to use metric.__call__\n@@ -241,3 +242,285 @@ class TestFalseNegatives(object):\n \n         result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n         assert np.allclose([4., 16., 23.], K.eval(result))\n+\n+\n+class TestSensitivityAtSpecificity(object):\n+\n+    def test_config(self):\n+        s_obj = metrics.SensitivityAtSpecificity(\n+            0.4, num_thresholds=100, name='sensitivity_at_specificity_1')\n+        assert s_obj.name == 'sensitivity_at_specificity_1'\n+        assert len(s_obj.weights) == 4\n+        assert s_obj.specificity == 0.4\n+        assert s_obj.num_thresholds == 100\n+\n+        # Check save and restore config\n+        s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())\n+        assert s_obj2.name == 'sensitivity_at_specificity_1'\n+        assert len(s_obj2.weights) == 4\n+        assert s_obj2.specificity == 0.4\n+        assert s_obj2.num_thresholds == 100\n+\n+    def test_unweighted_all_correct(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.7)\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = K.constant(inputs, dtype='float32')\n+        y_true = K.constant(inputs)\n+        result = s_obj(y_true, y_pred)\n+        assert np.isclose(1, K.eval(result))\n+\n+    def test_unweighted_high_specificity(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.8)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.1, 0.45, 0.5, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = K.constant(pred_values, dtype='float32')\n+        y_true = K.constant(label_values)\n+        result = s_obj(y_true, y_pred)\n+        assert np.isclose(0.8, K.eval(result))\n+\n+    def test_unweighted_low_specificity(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = K.constant(pred_values, dtype='float32')\n+        y_true = K.constant(label_values)\n+        result = s_obj(y_true, y_pred)\n+        assert np.isclose(0.6, K.eval(result))\n+\n+    def test_weighted(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n+\n+        y_pred = K.constant(pred_values, dtype='float32')\n+        y_true = K.constant(label_values, dtype='float32')\n+        weights = K.constant(weight_values)\n+        result = s_obj(y_true, y_pred, sample_weight=weights)\n+        assert np.isclose(0.675, K.eval(result))\n+\n+    def test_invalid_specificity(self):\n+        with pytest.raises(Exception):\n+            metrics.SensitivityAtSpecificity(-1)\n+\n+    def test_invalid_num_thresholds(self):\n+        with pytest.raises(Exception):\n+            metrics.SensitivityAtSpecificity(0.4, num_thresholds=-1)\n+\n+\n+class TestAUC(object):\n+\n+    def setup(self):\n+        self.num_thresholds = 3\n+        self.y_pred = K.constant([0, 0.5, 0.3, 0.9], dtype='float32')\n+        self.y_true = K.constant([0, 0, 1, 1])\n+        self.sample_weight = [1, 2, 3, 4]\n+\n+        # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n+        # y_pred when threshold = 0 - 1e-7  : [1, 1, 1, 1]\n+        # y_pred when threshold = 0.5       : [0, 0, 0, 1]\n+        # y_pred when threshold = 1 + 1e-7  : [0, 0, 0, 0]\n+\n+        # without sample_weight:\n+        # tp = np.sum([[0, 0, 1, 1], [0, 0, 0, 1], [0, 0, 0, 0]], axis=1)\n+        # fp = np.sum([[1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n+        # fn = np.sum([[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]], axis=1)\n+        # tn = np.sum([[0, 0, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]], axis=1)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+\n+        # with sample_weight:\n+        # tp = np.sum([[0, 0, 3, 4], [0, 0, 0, 4], [0, 0, 0, 0]], axis=1)\n+        # fp = np.sum([[1, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n+        # fn = np.sum([[0, 0, 0, 0], [0, 0, 3, 0], [0, 0, 3, 4]], axis=1)\n+        # tn = np.sum([[0, 0, 0, 0], [1, 2, 0, 0], [1, 2, 0, 0]], axis=1)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+\n+    def test_config(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=100,\n+            curve='PR',\n+            summation_method='majoring',\n+            name='auc_1')\n+        assert auc_obj.name == 'auc_1'\n+        assert len(auc_obj.weights) == 4\n+        assert auc_obj.num_thresholds == 100\n+        assert auc_obj.curve == metrics_utils.AUCCurve.PR\n+        assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n+\n+        # Check save and restore config.\n+        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n+        assert auc_obj2.name == 'auc_1'\n+        assert len(auc_obj2.weights) == 4\n+        assert auc_obj2.num_thresholds == 100\n+        assert auc_obj2.curve == metrics_utils.AUCCurve.PR\n+        assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n+\n+    def test_config_manual_thresholds(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=None,\n+            curve='PR',\n+            summation_method='majoring',\n+            name='auc_1',\n+            thresholds=[0.3, 0.5])\n+        assert auc_obj.name == 'auc_1'\n+        assert len(auc_obj.weights) == 4\n+        assert auc_obj.num_thresholds == 4\n+        assert np.allclose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0], atol=1e-3)\n+        assert auc_obj.curve == metrics_utils.AUCCurve.PR\n+        assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n+\n+        # Check save and restore config.\n+        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n+        assert auc_obj2.name == 'auc_1'\n+        assert len(auc_obj2.weights) == 4\n+        assert auc_obj2.num_thresholds == 4\n+        assert auc_obj2.curve == metrics_utils.AUCCurve.PR\n+        assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n+\n+    def test_unweighted_all_correct(self):\n+        self.setup()\n+        auc_obj = metrics.AUC()\n+        result = auc_obj(self.y_true, self.y_true)\n+        assert K.eval(result) == 1\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n+        result = auc_obj(self.y_true, self.y_pred)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = (0.75 * 1 + 0.25 * 0)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_manual_thresholds(self):\n+        self.setup()\n+        # Verify that when specified, thresholds are used instead of num_thresholds.\n+        auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5])\n+        assert auc_obj.num_thresholds == 3\n+        assert np.allclose(auc_obj.thresholds, [0.0, 0.5, 1.0], atol=1e-3)\n+        result = auc_obj(self.y_true, self.y_pred)\n+\n+        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = (0.75 * 1 + 0.25 * 0)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_roc_interpolation(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [(1 + 0.571)/2, (0.571 + 0)/2] = [0.7855, 0.2855]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = (0.7855 * 1 + 0.2855 * 0)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_roc_majoring(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, summation_method='majoring')\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [max(1, 0.571), max(0.571, 0)] = [1, 0.571]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = (1 * 1 + 0.571 * 0)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_roc_minoring(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, summation_method='minoring')\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n+        # heights = [min(1, 0.571), min(0.571, 0)] = [0.571, 0]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = (0.571 * 1 + 0 * 0)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_pr_majoring(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve='PR',\n+            summation_method='majoring')\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # heights = [max(0.7, 1), max(1, 0)] = [1, 1]\n+        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n+        expected_result = (1 * 0.429 + 1 * 0.571)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_pr_minoring(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve='PR',\n+            summation_method='minoring')\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n+        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n+        # heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]\n+        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n+        expected_result = (0.7 * 0.429 + 0 * 0.571)\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_pr_interpolation(self):\n+        self.setup()\n+        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, curve='PR')\n+        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n+\n+        # auc = (slope / Total Pos) * [dTP - intercept * log(Pb/Pa)]\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # P = tp + fp = [10, 4, 0]\n+        # dTP = [7-4, 4-0] = [3, 4]\n+        # dP = [10-4, 4-0] = [6, 4]\n+        # slope = dTP/dP = [0.5, 1]\n+        # intercept = (TPa+(slope*Pa) = [(4 - 0.5*4), (0 - 1*0)] = [2, 0]\n+        # (Pb/Pa) = (Pb/Pa) if Pb > 0 AND Pa > 0 else 1 = [10/4, 4/0] = [2.5, 1]\n+        # auc * TotalPos = [(0.5 * (3 + 2 * log(2.5))), (1 * (4 + 0))]\n+        #                = [2.416, 4]\n+        # auc = [2.416, 4]/(tp[1:]+fn[1:])\n+        # expected_result = (2.416 / 7 + 4 / 7)\n+        expected_result = 0.345 + 0.571\n+        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n+\n+    def test_invalid_num_thresholds(self):\n+        with pytest.raises(Exception):\n+            metrics.AUC(num_thresholds=-1)\n+\n+        with pytest.raises(Exception):\n+            metrics.AUC(num_thresholds=1)\n+\n+    def test_invalid_curve(self):\n+        with pytest.raises(Exception):\n+            metrics.AUC(curve='Invalid')\n+\n+    def test_invalid_summation_method(self):\n+        with pytest.raises(Exception):\n+            metrics.AUC(summation_method='Invalid')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
