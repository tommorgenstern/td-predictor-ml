{"custom_id": "scrapy#38a96fba2d2ab7c781322bb8ffc1e601907d83b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 93 | Churn Cumulative: 596 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,8 +52,8 @@ class CrawlSpider(Spider):\n             links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n             if links and rule.process_links:\n                 links = rule.process_links(links)\n-            seen = seen.union(links)\n             for link in links:\n+                seen.add(link)\n                 r = Request(url=link.url, callback=self._response_downloaded)\n                 r.meta.update(rule=n, link_text=link.text)\n                 yield rule.process_request(r)\n\n@@ -7,9 +7,11 @@ from scrapy.utils.trackref import object_ref\n from twisted.trial import unittest\n \n from scrapy.spider import Spider, BaseSpider\n-from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse\n+from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.contrib.spiders.init import InitSpider\n-from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider\n+from scrapy.contrib.spiders import CrawlSpider, Rule, XMLFeedSpider, \\\n+    CSVFeedSpider, SitemapSpider\n+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n@@ -101,8 +103,93 @@ class CSVFeedSpiderTest(SpiderTest):\n \n class CrawlSpiderTest(SpiderTest):\n \n+    test_body = \"\"\"<html><head><title>Page title<title>\n+    <body>\n+    <p><a href=\"item/12.html\">Item 12</a></p>\n+    <div class='links'>\n+    <p><a href=\"/about.html\">About us</a></p>\n+    </div>\n+    <div>\n+    <p><a href=\"/nofollow.html\">This shouldn't be followed</a></p>\n+    </div>\n+    </body></html>\"\"\"\n     spider_class = CrawlSpider\n \n+    def test_process_links(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n+            body=self.test_body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(SgmlLinkExtractor(), process_links=\"dummy_process_links\"),\n+            )\n+\n+            def dummy_process_links(self, links):\n+                return links\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEquals([r.url for r in output],\n+                          ['http://example.org/somepage/item/12.html',\n+                           'http://example.org/about.html',\n+                           'http://example.org/nofollow.html'])\n+\n+    def test_process_links_filter(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n+            body=self.test_body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            import re\n+\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(SgmlLinkExtractor(), process_links=\"filter_process_links\"),\n+            )\n+            _test_regex = re.compile('nofollow')\n+            def filter_process_links(self, links):\n+                return [link for link in links\n+                        if not self._test_regex.search(link.url)]\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 2)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEquals([r.url for r in output],\n+                          ['http://example.org/somepage/item/12.html',\n+                           'http://example.org/about.html'])\n+\n+    def test_process_links_generator(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\",\n+            body=self.test_body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name=\"test\"\n+            allowed_domains=['example.org']\n+            rules = (\n+                Rule(SgmlLinkExtractor(), process_links=\"dummy_process_links\"),\n+            )\n+\n+            def dummy_process_links(self, links):\n+                for link in links:\n+                    yield link\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEquals([r.url for r in output],\n+                          ['http://example.org/somepage/item/12.html',\n+                           'http://example.org/about.html',\n+                           'http://example.org/nofollow.html'])\n+\n \n class SitemapSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e31fb49320a5a2af0232b98ed509623db8a9b4c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 164 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,7 +40,7 @@ class Command(ScrapyCommand):\n             help=\"Edit spider after creating it\")\n         parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n             help=\"Dump template to standard output\")\n-        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"crawl\",\n+        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n             help=\"Uses a custom template.\")\n         parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n             help=\"If the spider already exists, overwrite it with the template\")\n\n@@ -3,7 +3,7 @@ import sys\n import string\n import re\n import shutil\n-from os.path import join, exists\n+from os.path import join, exists, abspath\n from shutil import copytree, ignore_patterns\n \n import scrapy\n@@ -53,3 +53,8 @@ class Command(ScrapyCommand):\n                 string.Template(path).substitute(project_name=project_name))\n             render_templatefile(tplfile, project_name=project_name,\n                 ProjectName=string_camelcase(project_name))\n+        print(\"New Scrapy project %r created in:\" % project_name)\n+        print(\"    %s\\n\" % abspath(project_name))\n+        print(\"You can start your first spider with:\")\n+        print(\"    cd %s\" % project_name)\n+        print(\"    scrapy genspider example example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8017d85d8a115a3061ef6951cd0989c87d5e7c92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 513 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,11 +62,12 @@ class HtmlParserLinkExtractor(HTMLParser):\n                     self.current_link = link\n \n     def handle_endtag(self, tag):\n+        if self.scan_tag(tag):\n             self.current_link = None\n \n     def handle_data(self, data):\n-        if self.current_link and not self.current_link.text:\n-            self.current_link.text = data.strip()\n+        if self.current_link:\n+            self.current_link.text = self.current_link.text + data\n \n     def matches(self, url):\n         \"\"\"This extractor matches with any url, since\n\n@@ -317,7 +317,8 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                           Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n                           Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n-                          Link(url='http://www.google.com/something', text=u''),])\n+                          Link(url='http://www.google.com/something', text=u''),\n+                          Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n \n \n class RegexLinkExtractorTestCase(unittest.TestCase):\n@@ -332,7 +333,8 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(self.response),\n                          [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n                           Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-                          Link(url='http://www.google.com/something', text=u''),])\n+                          Link(url='http://www.google.com/something', text=u''),\n+                          Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee896b154cc962c5926ac826f47271a5f2194a8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 727 | Contributors (this commit): 3 | Commits (past 90d): 33 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -145,6 +145,9 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n             class OutdatedUserClass1(DeprecatedName):\n                 pass\n \n+            class OutdatedUserClass1a(DeprecatedName):\n+                pass\n+\n             class UnrelatedClass(object):\n                 pass\n \n@@ -159,6 +162,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         assert not issubclass(UnrelatedClass, DeprecatedName)\n         assert not issubclass(OldStyleClass, DeprecatedName)\n         assert not issubclass(OldStyleClass, DeprecatedName)\n+        assert not issubclass(OutdatedUserClass1, OutdatedUserClass1a)\n+        assert not issubclass(OutdatedUserClass1a, OutdatedUserClass1)\n \n         self.assertRaises(TypeError, issubclass, object(), DeprecatedName)\n \n@@ -175,6 +180,9 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n             class OutdatedUserClass2(DeprecatedName):\n                 pass\n \n+            class OutdatedUserClass2a(DeprecatedName):\n+                pass\n+\n             class UnrelatedClass(object):\n                 pass\n \n@@ -186,6 +194,9 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         assert isinstance(UpdatedUserClass2(), DeprecatedName)\n         assert isinstance(UpdatedUserClass2a(), DeprecatedName)\n         assert isinstance(OutdatedUserClass2(), DeprecatedName)\n+        assert isinstance(OutdatedUserClass2a(), DeprecatedName)\n+        assert not isinstance(OutdatedUserClass2a(), OutdatedUserClass2)\n+        assert not isinstance(OutdatedUserClass2(), OutdatedUserClass2a)\n         assert not isinstance(UnrelatedClass(), DeprecatedName)\n         assert not isinstance(OldStyleClass(), DeprecatedName)\n \n\n@@ -79,12 +79,18 @@ def create_deprecated_class(name, new_class, clsdict=None,\n                        for c in {type(inst), inst.__class__})\n \n         def __subclasscheck__(cls, sub):\n+            if cls is not DeprecatedClass.deprecated_class:\n+                # we should do the magic only if second `issubclass` argument\n+                # is the deprecated class itself - subclasses of the\n+                # deprecated class should not use custom `__subclasscheck__`\n+                # method.\n+                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n+\n             if not inspect.isclass(sub):\n                 raise TypeError(\"issubclass() arg 1 must be a class\")\n \n             mro = getattr(sub, '__mro__', ())\n-            candidates = {cls, new_class}\n-            return any(c in candidates for c in mro)\n+            return any(c in {cls, new_class} for c in mro)\n \n         def __call__(cls, *args, **kwargs):\n             old = DeprecatedClass.deprecated_class\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e95b36a09751e97b9844c6bf00f01a2d32ad61b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 461 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,14 +3,18 @@ from __future__ import absolute_import\n import inspect\n import unittest\n import warnings\n+import mock\n from scrapy.utils.deprecate import create_deprecated_class\n \n+\n class MyWarning(UserWarning):\n     pass\n \n+\n class SomeBaseClass(object):\n     pass\n \n+\n class NewName(SomeBaseClass):\n     pass\n \n@@ -223,3 +227,12 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertIn('foo.Bar', str(w[0].message))\n         self.assertIn('AlsoDeprecated', str(w[1].message))\n         self.assertIn('foo.Bar', str(w[1].message))\n+\n+    def test_inspect_stack(self):\n+        with mock.patch('inspect.stack', side_effect=IndexError):\n+            with warnings.catch_warnings(record=True) as w:\n+                DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n+                class SubClass(DeprecatedName):\n+                    pass\n+\n+        self.assertIn(\"Error detecting parent module\", str(w[0].message))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#877e345d79c6a14b6ab24a3f15653f6373918cfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 378 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,6 @@\n import json\n+import socket\n+import mock\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import get_crawler, get_testlog\n@@ -88,7 +90,9 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_retry_dns_error(self):\n-        spider = SimpleSpider(\"http://localhost666/status?n=503\")\n+        with mock.patch('socket.gethostbyname',\n+                        side_effect=socket.gaierror(-5, 'No address associated with hostname')):\n+            spider = SimpleSpider(\"http://example.com/\")\n             yield docrawl(spider)\n             self._assert_retried()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b58285b635d87b97c4d3f9c2131b47cab994194a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 24 | Files Changed: 3 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/2 | Churn Δ: 57 | Churn Cumulative: 861 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -137,6 +137,8 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n class SingleRequestSpider(MetaSpider):\n \n     seed = None\n+    callback_func = None\n+    errback_func = None\n \n     def start_requests(self):\n         if isinstance(self.seed, Request):\n@@ -146,8 +148,12 @@ class SingleRequestSpider(MetaSpider):\n \n     def parse(self, response):\n         self.meta.setdefault('responses', []).append(response)\n+        if callable(self.callback_func):\n+            return self.callback_func(response)\n         if 'next' in response.meta:\n             return response.meta['next']\n \n     def on_error(self, failure):\n         self.meta['failure'] = failure\n+        if callable(self.errback_func):\n+            return self.errback_func(failure)\n\n@@ -192,3 +192,18 @@ with multiples lines\n         # last request explicitly sets a Referer header\n         echo3 = json.loads(spider.meta['responses'][3].body)\n         self.assertEqual(echo3['headers'].get('Referer'), ['http://example.com'])\n+\n+    @defer.inlineCallbacks\n+    def test_engine_status(self):\n+        from scrapy.utils.engine import get_engine_status\n+        est = []\n+\n+        def cb(response):\n+            est.append(get_engine_status(spider.crawler.engine))\n+\n+        spider = SingleRequestSpider(seed='http://localhost:8998/', callback_func=cb)\n+        yield docrawl(spider)\n+        self.assertEqual(len(est), 1, est)\n+        s = dict(est[0])\n+        self.assertEqual(s['engine.spider.name'], spider.name)\n+        self.assertEqual(s['len(engine.scraper.slot.active)'], 1)\n\n@@ -5,14 +5,13 @@ from time import time # used in global tests code\n \n def get_engine_status(engine):\n     \"\"\"Return a report of the current engine status\"\"\"\n-    global_tests = [\n+    tests = [\n         \"time()-engine.start_time\",\n         \"engine.has_capacity()\",\n         \"len(engine.downloader.active)\",\n         \"engine.scraper.is_idle()\",\n-    ]\n-    spider_tests = [\n-        \"engine.spider_is_idle(spider)\",\n+        \"engine.spider.name\",\n+        \"engine.spider_is_idle(engine.spider)\",\n         \"engine.slot.closing\",\n         \"len(engine.slot.inprogress)\",\n         \"len(engine.slot.scheduler.dqs or [])\",\n@@ -24,34 +23,23 @@ def get_engine_status(engine):\n         \"engine.scraper.slot.needs_backout()\",\n     ]\n \n-    status = {'global': [], 'spiders': {}}\n-    for test in global_tests:\n+    checks = []\n+    for test in tests:\n         try:\n-            status['global'] += [(test, eval(test))]\n+            checks += [(test, eval(test))]\n         except Exception as e:\n-            status['global'] += [(test, \"%s (exception)\" % type(e).__name__)]\n-    for spider in engine.slots.keys():\n-        x = []\n-        for test in spider_tests:\n-            try:\n-                x += [(test, eval(test))]\n-            except Exception as e:\n-                x += [(test, \"%s (exception)\" % type(e).__name__)]\n-            status['spiders'][spider] = x\n-    return status\n+            checks += [(test, \"%s (exception)\" % type(e).__name__)]\n+\n+    return checks\n \n def format_engine_status(engine=None):\n-    status = get_engine_status(engine)\n+    checks = get_engine_status(engine)\n     s = \"Execution engine status\\n\\n\"\n-    for test, result in status['global']:\n+    for test, result in checks:\n         s += \"%-47s : %s\\n\" % (test, result)\n     s += \"\\n\"\n-    for spider, tests in status['spiders'].items():\n-        s += \"Spider: %s\\n\" % spider\n-        for test, result in tests:\n-            s += \"  %-50s : %s\\n\" % (test, result)\n+\n     return s\n \n def print_engine_status(engine):\n     print(format_engine_status(engine))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41765ca18d325caa845b37cf567ac5d73a8f13ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 105 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,17 +28,21 @@ class BaseDupeFilter(object):\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n-    def __init__(self, path=None):\n+    _log_level = log.DEBUG\n+\n+    def __init__(self, path=None, verbose_log=False):\n         self.file = None\n         self.fingerprints = set()\n         self.logdupes = True\n+        self.verbose_log = verbose_log\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n     @classmethod\n     def from_settings(cls, settings):\n-        return cls(job_dir(settings))\n+        verbose_log = settings.getbool('DUPEFILTER_DEBUG')\n+        return cls(job_dir(settings), verbose_log)\n \n     def request_seen(self, request):\n         fp = self.request_fingerprint(request)\n@@ -56,7 +60,14 @@ class RFPDupeFilter(BaseDupeFilter):\n             self.file.close()\n \n     def log(self, request, spider):\n-        if self.logdupes:\n-            fmt = \"Filtered duplicate request: %(request)s - no more duplicates will be shown (see DUPEFILTER_CLASS)\"\n-            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n+        if self.verbose_log:\n+            fmt = \"Filtered duplicate request: %(request)s\"\n+            log.msg(format=fmt, request=request, level=self._log_level, spider=spider)\n+        elif self.logdupes:\n+            fmt = (\"Filtered duplicate request: %(request)s\"\n+                   \" - no more duplicates will be shown\"\n+                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n+            log.msg(format=fmt, request=request, level=self._log_level, spider=spider)\n             self.logdupes = False\n+\n+        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66fb1738a216e9fd0a8f14a78fcabbc4211b8c8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 111 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -28,8 +28,6 @@ class BaseDupeFilter(object):\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n-    _log_level = log.DEBUG\n-\n     def __init__(self, path=None, verbose_log=False):\n         self.file = None\n         self.fingerprints = set()\n@@ -62,12 +60,12 @@ class RFPDupeFilter(BaseDupeFilter):\n     def log(self, request, spider):\n         if self.verbose_log:\n             fmt = \"Filtered duplicate request: %(request)s\"\n-            log.msg(format=fmt, request=request, level=self._log_level, spider=spider)\n+            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n         elif self.logdupes:\n             fmt = (\"Filtered duplicate request: %(request)s\"\n                    \" - no more duplicates will be shown\"\n                    \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n-            log.msg(format=fmt, request=request, level=self._log_level, spider=spider)\n+            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n             self.logdupes = False\n \n         spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a676017fe4632c931fccc5fa9539ed4c23624fd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 289 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,11 +35,11 @@ class MailSender(object):\n             settings['MAIL_PASS'], settings.getint('MAIL_PORT'),\n             settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))\n \n-    def send(self, to, subject, body, cc=None, attachs=(), _callback=None):\n+    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', _callback=None):\n         if attachs:\n             msg = MIMEMultipart()\n         else:\n-            msg = MIMENonMultipart('text', 'plain')\n+            msg = MIMENonMultipart(*mimetype.split('/', 1))\n         msg['From'] = self.mailfrom\n         msg['To'] = COMMASPACE.join(to)\n         msg['Date'] = formatdate(localtime=True)\n\n@@ -1,6 +1,6 @@\n-from cStringIO import StringIO\n import unittest\n \n+from cStringIO import StringIO\n from scrapy.mail import MailSender\n \n class MailSenderTest(unittest.TestCase):\n@@ -19,6 +19,15 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(msg['to'], 'test@scrapy.org')\n         self.assertEqual(msg['subject'], 'subject')\n         self.assertEqual(msg.get_payload(), 'body')\n+        self.assertEqual(msg.get('Content-Type'), 'text/plain')\n+\n+    def test_send_html(self):\n+        mailsender = MailSender(debug=True)\n+        mailsender.send(to=['test@scrapy.org'], subject='subject', body='<p>body</p>', mimetype='text/html', _callback=self._catch_mail_sent)\n+\n+        msg = self.catched_msg['msg']\n+        self.assertEqual(msg.get_payload(), '<p>body</p>')\n+        self.assertEqual(msg.get('Content-Type'), 'text/html')\n \n     def test_send_attach(self):\n         attach = StringIO()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#091c336b5923032362e8942497e8718bb8d1c9ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 121 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -28,19 +28,19 @@ class BaseDupeFilter(object):\n class RFPDupeFilter(BaseDupeFilter):\n     \"\"\"Request Fingerprint duplicates filter\"\"\"\n \n-    def __init__(self, path=None, verbose_log=False):\n+    def __init__(self, path=None, debug=False):\n         self.file = None\n         self.fingerprints = set()\n         self.logdupes = True\n-        self.verbose_log = verbose_log\n+        self.debug = debug\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n     @classmethod\n     def from_settings(cls, settings):\n-        verbose_log = settings.getbool('DUPEFILTER_DEBUG')\n-        return cls(job_dir(settings), verbose_log)\n+        debug = settings.getbool('DUPEFILTER_DEBUG')\n+        return cls(job_dir(settings), debug)\n \n     def request_seen(self, request):\n         fp = self.request_fingerprint(request)\n@@ -58,7 +58,7 @@ class RFPDupeFilter(BaseDupeFilter):\n             self.file.close()\n \n     def log(self, request, spider):\n-        if self.verbose_log:\n+        if self.debug:\n             fmt = \"Filtered duplicate request: %(request)s\"\n             log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)\n         elif self.logdupes:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e62041b57f1ff7d1254271eb2c03f115b0c39ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 229 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -168,7 +168,6 @@ LOG_UNSERIALIZABLE_REQUESTS = False\n \n LOGSTATS_INTERVAL = 60.0\n \n-MAIL_DEBUG = False\n MAIL_HOST = 'localhost'\n MAIL_PORT = 25\n MAIL_FROM = 'scrapy@localhost'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#341c5962320c03ed373c8eb58a5814954593ec95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 90 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,7 @@ HttpError Spider Middleware\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n from scrapy.exceptions import IgnoreRequest\n-\n+from scrapy import log\n \n class HttpError(IgnoreRequest):\n     \"\"\"A non-200 response was filtered\"\"\"\n@@ -38,6 +38,9 @@ class HttpErrorMiddleware(object):\n             allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n         if response.status in allowed_statuses:\n             return\n+        log.msg(format=\"Ignoring HTTP response code: not handled or not allowed: %(status_code)d\",\n+                            level=log.DEBUG, spider=spider,\n+                            status_code=response.status)\n         raise HttpError(response, 'Ignoring non-200 response')\n \n     def process_spider_exception(self, response, exception, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d8b7b29ef9fdd4b4cbb5f525d883feb892637e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 231 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -66,8 +66,6 @@ DOWNLOAD_HANDLERS_BASE = {\n \n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n-DOWNLOADER_DEBUG = False\n-\n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67aa586a90e535a4ce717962acab10c8e20a07e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 400 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -214,19 +214,19 @@ class FilesPipeline(MediaPipeline):\n         referer = request.headers.get('Referer')\n \n         if response.status != 200:\n-            log.msg(format='File (code: %(status)s): Error downloading image from %(request)s referred in <%(referer)s>',\n+            log.msg(format='File (code: %(status)s): Error downloading file from %(request)s referred in <%(referer)s>',\n                     level=log.WARNING, spider=info.spider,\n                     status=response.status, request=request, referer=referer)\n             raise FileException('download-error')\n \n         if not response.body:\n-            log.msg(format='File (empty-content): Empty image from %(request)s referred in <%(referer)s>: no-content',\n+            log.msg(format='File (empty-content): Empty file from %(request)s referred in <%(referer)s>: no-content',\n                     level=log.WARNING, spider=info.spider,\n                     request=request, referer=referer)\n             raise FileException('empty-content')\n \n         status = 'cached' if 'cached' in response.flags else 'downloaded'\n-        log.msg(format='File (%(status)s): Downloaded image from %(request)s referred in <%(referer)s>',\n+        log.msg(format='File (%(status)s): Downloaded file from %(request)s referred in <%(referer)s>',\n                 level=log.DEBUG, spider=info.spider,\n                 status=status, request=request, referer=referer)\n         self.inc_stats(info.spider, status)\n@@ -235,12 +235,12 @@ class FilesPipeline(MediaPipeline):\n             path = self.file_path(request, response=response, info=info)\n             checksum = self.file_downloaded(response, request, info)\n         except FileException as exc:\n-            whyfmt = 'File (error): Error processing image from %(request)s referred in <%(referer)s>: %(errormsg)s'\n+            whyfmt = 'File (error): Error processing file from %(request)s referred in <%(referer)s>: %(errormsg)s'\n             log.msg(format=whyfmt, level=log.WARNING, spider=info.spider,\n                     request=request, referer=referer, errormsg=str(exc))\n             raise\n         except Exception as exc:\n-            whyfmt = 'File (unknown-error): Error processing image from %(request)s referred in <%(referer)s>'\n+            whyfmt = 'File (unknown-error): Error processing file from %(request)s referred in <%(referer)s>'\n             log.err(None, whyfmt % {'request': request, 'referer': referer}, spider=info.spider)\n             raise FileException(str(exc))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4de45c942e93845e2d6588c49a9431c5fd933909", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 18 | Files Changed: 8 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 36 | Churn Cumulative: 5540 | Contributors (this commit): 17 | Commits (past 90d): 12 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -57,12 +57,12 @@ class ReturnsContract(Contract):\n             self.max_bound = float('inf')\n \n     def post_process(self, output):\n-        occurences = 0\n+        occurrences = 0\n         for x in output:\n             if isinstance(x, self.obj_type):\n-                occurences += 1\n+                occurrences += 1\n \n-        assertion = (self.min_bound <= occurences <= self.max_bound)\n+        assertion = (self.min_bound <= occurrences <= self.max_bound)\n \n         if not assertion:\n             if self.min_bound == self.max_bound:\n@@ -71,7 +71,7 @@ class ReturnsContract(Contract):\n                 expected = '%s..%s' % (self.min_bound, self.max_bound)\n \n             raise ContractFail(\"Returned %s %s, expected %s\" % \\\n-                (occurences, self.obj_name, expected))\n+                (occurrences, self.obj_name, expected))\n \n \n class ScrapesContract(Contract):\n\n@@ -45,7 +45,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n     def _process_links(self, links):\n         \"\"\" Normalize and filter extracted links\n \n-        The subclass should override it if neccessary\n+        The subclass should override it if necessary\n         \"\"\"\n         links = unique_list(links, key=lambda link: link.url) if self.unique else links\n         return links\n\n@@ -21,7 +21,7 @@ class MemoryUsage(object):\n         if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n         try:\n-            # stdlib's resource module is only availabe on unix platforms.\n+            # stdlib's resource module is only available on unix platforms.\n             self.resource = import_module('resource')\n         except ImportError:\n             raise NotConfigured\n\n@@ -121,7 +121,7 @@ class Scraper(object):\n         return dfd\n \n     def _scrape2(self, request_result, request, spider):\n-        \"\"\"Handle the diferent cases of request's result been a Response or a\n+        \"\"\"Handle the different cases of request's result been a Response or a\n         Failure\"\"\"\n         if not isinstance(request_result, Failure):\n             return self.spidermw.scrape_response(self.call_spider, \\\n\n@@ -1,6 +1,6 @@\n \"\"\"\n-This module implements a class which returns the appropiate Response class\n-based on different criterias.\n+This module implements a class which returns the appropriate Response class\n+based on different criteria.\n \n \"\"\"\n \n@@ -38,7 +38,7 @@ class ResponseTypes(object):\n             self.classes[mimetype] = load_object(cls)\n \n     def from_mimetype(self, mimetype):\n-        \"\"\"Return the most appropiate Response class for the given mimetype\"\"\"\n+        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n         if mimetype is None:\n             return Response\n         elif mimetype in self.classes:\n@@ -48,7 +48,7 @@ class ResponseTypes(object):\n             return self.classes.get(basetype, Response)\n \n     def from_content_type(self, content_type, content_encoding=None):\n-        \"\"\"Return the most appropiate Response class from an HTTP Content-Type\n+        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n         header \"\"\"\n         if content_encoding:\n             return Response\n@@ -64,7 +64,7 @@ class ResponseTypes(object):\n             return Response\n \n     def from_headers(self, headers):\n-        \"\"\"Return the most appropiate Response class by looking at the HTTP\n+        \"\"\"Return the most appropriate Response class by looking at the HTTP\n         headers\"\"\"\n         cls = Response\n         if 'Content-Type' in headers:\n@@ -75,7 +75,7 @@ class ResponseTypes(object):\n         return cls\n \n     def from_filename(self, filename):\n-        \"\"\"Return the most appropiate Response class from a file name\"\"\"\n+        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n         mimetype, encoding = self.mimetypes.guess_type(filename)\n         if mimetype and not encoding:\n             return self.from_mimetype(mimetype)\n@@ -83,7 +83,7 @@ class ResponseTypes(object):\n             return Response\n \n     def from_body(self, body):\n-        \"\"\"Try to guess the appropiate response based on the body content.\n+        \"\"\"Try to guess the appropriate response based on the body content.\n         This method is a bit magic and could be improved in the future, but\n         it's not meant to be used except for special cases where response types\n         cannot be guess using more straightforward methods.\"\"\"\n@@ -98,7 +98,7 @@ class ResponseTypes(object):\n             return self.from_mimetype('text')\n \n     def from_args(self, headers=None, url=None, filename=None, body=None):\n-        \"\"\"Guess the most appropiate Response class based on the given arguments\"\"\"\n+        \"\"\"Guess the most appropriate Response class based on the given arguments\"\"\"\n         cls = Response\n         if headers is not None:\n             cls = self.from_headers(headers)\n\n@@ -27,7 +27,7 @@ def function(receiver):\n         # an instance-method...\n         return receiver, receiver.im_func.func_code, 1\n     elif not hasattr(receiver, 'func_code'):\n-        raise ValueError('unknown reciever type %s %s'%(receiver, type(receiver)))\n+        raise ValueError('unknown receiver type %s %s'%(receiver, type(receiver)))\n \n     return receiver, receiver.func_code, 0\n \n\n@@ -581,7 +581,7 @@ class Request:\n \n         # In the future, having the protocol version be a parameter to this\n         # method would probably be good.  It would be nice if this method\n-        # weren't limited to issueing HTTP/1.1 requests.\n+        # weren't limited to issuing HTTP/1.1 requests.\n         requestLines = []\n         requestLines.append(\n             '%s %s HTTP/1.1\\r\\n' % (self.method, self.uri))\n\n@@ -849,7 +849,7 @@ class IReactorMulticast(Interface):\n     UDP socket methods that support multicast.\n \n     IMPORTANT: This is an experimental new interface. It may change\n-    without backwards compatability. Suggestions are welcome.\n+    without backwards compatibility. Suggestions are welcome.\n     \"\"\"\n \n     def listenMulticast(port, protocol, interface='', maxPacketSize=8192,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a63b7e1c3e453fd88e280d0877881071697054a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 1220 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -439,6 +439,27 @@ class FormRequestTest(RequestTest):\n         self.assertRaises(ValueError, self.request_class.from_response,\n                           response, clickdata={'nonexistent': 'notme'})\n \n+    def test_from_response_nr_index_clickdata(self):\n+        response = _buildresponse(\n+            \"\"\"<form>\n+            <input type=\"submit\" name=\"clickable1\" value=\"clicked1\">\n+            <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n+            </form>\n+            \"\"\")\n+        req = self.request_class.from_response(response, clickdata={'nr': 1})\n+        fs = _qs(req)\n+        self.assertTrue('clickable2' in fs, fs)\n+        self.assertFalse('clickable1' in fs, fs)\n+\n+    def test_from_response_invalid_nr_index_clickdata(self):\n+        response = _buildresponse(\n+            \"\"\"<form>\n+            <input type=\"submit\" name=\"clickable\" value=\"clicked\">\n+            </form>\n+            \"\"\")\n+        self.assertRaises(ValueError, self.request_class.from_response,\n+                          response, clickdata={'nr': 1})\n+\n     def test_from_response_errors_noform(self):\n         response = _buildresponse(\"\"\"<html></html>\"\"\")\n         self.assertRaises(ValueError, self.request_class.from_response, response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae4eb90cbfe9d54ffefc7f64151e7f6beea94863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1224 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -448,8 +448,8 @@ class FormRequestTest(RequestTest):\n             \"\"\")\n         req = self.request_class.from_response(response, clickdata={'nr': 1})\n         fs = _qs(req)\n-        self.assertTrue('clickable2' in fs, fs)\n-        self.assertFalse('clickable1' in fs, fs)\n+        self.assertIn('clickable2', fs)\n+        self.assertNotIn('clickable1', fs)\n \n     def test_from_response_invalid_nr_index_clickdata(self):\n         response = _buildresponse(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1df15b9cea35c3a215778e57d199977fe3da59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 88 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,10 @@ class DownloadHandlers(object):\n         handlers = crawler.settings.get('DOWNLOAD_HANDLERS_BASE')\n         handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))\n         for scheme, clspath in handlers.iteritems():\n+            # Allow to disable a handler just like any other\n+            # component (extension, middlware, etc).\n+            if clspath is None:\n+                continue\n             cls = load_object(clspath)\n             try:\n                 dh = cls(crawler.settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#efb86f966d8be992b9af06c78ef820fcdc64a6d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 35 | Churn Cumulative: 674 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from twisted.protocols.ftp import FTPClient, ConnectionLost\n from w3lib.url import path_to_file_uri\n \n from scrapy import twisted_version\n+from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n@@ -26,6 +27,40 @@ from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy import optional_features\n+from scrapy.utils.test import get_crawler\n+from scrapy.exceptions import NotConfigured\n+\n+\n+class DummyDH(object):\n+\n+    def __init__(self, settings):\n+        pass\n+\n+\n+class OffDH(object):\n+\n+    def __init__(self, crawler):\n+        raise NotConfigured\n+\n+\n+class LoadTestCase(unittest.TestCase):\n+\n+    def test_enabled_handler(self):\n+        handlers = {'scheme': 'scrapy.tests.test_downloader_handlers.DummyDH'}\n+        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n+        self.assertIn('scheme', dh._handlers)\n+        self.assertNotIn('scheme', dh._notconfigured)\n+\n+    def test_not_configured_handler(self):\n+        handlers = {'scheme': 'scrapy.tests.test_downloader_handlers.OffDH'}\n+        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n+        self.assertNotIn('scheme', dh._handlers)\n+        self.assertIn('scheme', dh._notconfigured)\n+\n+    def test_disabled_handler(self):\n+        handlers = {'scheme': None}\n+        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n+        self.assertNotIn('scheme', dh._handlers)\n \n \n class FileTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65734594816b158bdf08b93244795b5dcc8626ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 90 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ class DownloadHandlers(object):\n         handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))\n         for scheme, clspath in handlers.iteritems():\n             # Allow to disable a handler just like any other\n-            # component (extension, middlware, etc).\n+            # component (extension, middleware, etc).\n             if clspath is None:\n                 continue\n             cls = load_object(clspath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
