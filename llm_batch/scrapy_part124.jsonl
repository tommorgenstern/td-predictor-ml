{"custom_id": "scrapy#cd85c12c3383ea185d24c9023c33076081cce7a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 27 | Files Changed: 4 | Hunks: 16 | Methods Changed: 9 | Complexity Δ (Sum/Max): -3/3 | Churn Δ: 93 | Churn Cumulative: 684 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,14 +3,16 @@ SGMLParser-based Link extractors\n \"\"\"\n \n import re\n+from urlparse import urlparse\n \n from w3lib.url import safe_url_string, urljoin_rfc\n \n from scrapy.selector import HtmlXPathSelector\n from scrapy.link import Link\n+from scrapy.linkextractor import IGNORED_EXTENSIONS\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n-from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n+from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension\n from scrapy.utils.response import get_base_url\n \n class BaseSgmlLinkExtractor(FixedSGMLParser):\n@@ -91,13 +93,17 @@ _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'fil\n class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None):\n+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None,\n+                 deny_extensions=None):\n         self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)]\n         self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)]\n         self.allow_domains = set(arg_to_iter(allow_domains))\n         self.deny_domains = set(arg_to_iter(deny_domains))\n         self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n         self.canonicalize = canonicalize\n+        if deny_extensions is None:\n+            deny_extensions = IGNORED_EXTENSIONS\n+        self.deny_extensions = set(['.' + e for e in deny_extensions])\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n         BaseSgmlLinkExtractor.__init__(self, tag=tag_func, attr=attr_func, \n@@ -118,24 +124,27 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n         return links\n \n     def _process_links(self, links):\n-        links = [link for link in links if _is_valid_url(link.url)]\n-\n-        if self.allow_res:\n-            links = [link for link in links if _matches(link.url, self.allow_res)]\n-        if self.deny_res:\n-            links = [link for link in links if not _matches(link.url, self.deny_res)]\n-        if self.allow_domains:\n-            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]\n-        if self.deny_domains:\n-            links = [link for link in links if not url_is_from_any_domain(link.url, self.deny_domains)]\n-\n-        if self.canonicalize:\n-            for link in links:\n-                link.url = canonicalize_url(link.url)\n-\n+        links = [x for x in links if self._link_allowed(x)]\n         links = BaseSgmlLinkExtractor._process_links(self, links)\n         return links\n \n+    def _link_allowed(self, link):\n+        parsed_url = urlparse(link.url)\n+        allowed = _is_valid_url(link.url)\n+        if self.allow_res:\n+            allowed &= _matches(link.url, self.allow_res)\n+        if self.deny_res:\n+            allowed &= not _matches(link.url, self.deny_res)\n+        if self.allow_domains:\n+            allowed &= url_is_from_any_domain(parsed_url, self.allow_domains)\n+        if self.deny_domains:\n+            allowed &= not url_is_from_any_domain(parsed_url, self.deny_domains)\n+        if self.deny_extensions:\n+            allowed &= not url_has_any_extension(parsed_url, self.deny_extensions)\n+        if allowed and self.canonicalize:\n+            link.url = canonicalize_url(parsed_url)\n+        return allowed\n+\n     def matches(self, url):\n         if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n             return False\n\n@@ -0,0 +1,21 @@\n+\"\"\"\n+Common code and definitions used by Link extractors (located in\n+scrapy.contrib.linkextractor).\n+\"\"\"\n+\n+# common file extensions that are not followed if they occur in links\n+IGNORED_EXTENSIONS = [\n+    # images\n+    'mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif',\n+    'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg',\n+\n+    # audio\n+    'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff',\n+\n+    # video\n+    '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv',\n+    'm4a',\n+\n+    # other\n+    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',\n+]\n\n@@ -135,10 +135,6 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n             [ Link(url='http://www.google.com/something', text=u'') ])\n \n-        lx = SgmlLinkExtractor(tags=('img', ), attrs=('src', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample2.jpg', text=u'') ])\n-\n     def test_extraction_using_single_values(self):\n         '''Test the extractor's behaviour among different situations'''\n \n@@ -211,6 +207,13 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.extract_links(response),\n                          [Link(url='http://example.org/about.html', text=u'About us\\xa3')])\n \n+    def test_deny_extensions(self):\n+        html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n+        response = HtmlResponse(\"http://example.org/\", body=html)\n+        lx = SgmlLinkExtractor()\n+        self.assertEqual(lx.extract_links(response),\n+            [Link(url='http://example.org/page.html', text=u'asd')])\n+\n     def test_process_value(self):\n         \"\"\"Test restrict_xpaths with encodings\"\"\"\n         html = \"\"\"\n\n@@ -6,11 +6,8 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n \n-import os\n-import re\n import urlparse\n import urllib\n-import posixpath\n import cgi\n \n from w3lib.url import *\n@@ -18,7 +15,7 @@ from scrapy.utils.python import unicode_to_str\n \n def url_is_from_any_domain(url, domains):\n     \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n-    host = urlparse.urlparse(url).hostname\n+    host = parse_url(url).hostname\n \n     if host:\n         return any(((host == d) or (host.endswith('.%s' % d)) for d in domains))\n@@ -30,6 +27,9 @@ def url_is_from_spider(url, spider):\n     return url_is_from_any_domain(url, [spider.name] + \\\n         getattr(spider, 'allowed_domains', []))\n \n+def url_has_any_extension(url, extensions):\n+    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n+\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n         encoding=None):\n     \"\"\"Canonicalize the given url by applying the following procedures:\n@@ -48,11 +48,17 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n \n-    url = unicode_to_str(url, encoding)\n-    scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)\n+    scheme, netloc, path, params, query, fragment = parse_url(url)\n     keyvals = cgi.parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urllib.urlencode(keyvals)\n     path = safe_url_string(urllib.unquote(path))\n     fragment = '' if not keep_fragments else fragment\n     return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n+\n+def parse_url(url, encoding=None):\n+    \"\"\"Return urlparsed url from the given argument (which could be an already\n+    parsed url)\n+    \"\"\"\n+    return url if isinstance(url, urlparse.ParseResult) else \\\n+        urlparse.urlparse(unicode_to_str(url, encoding))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a98e9e054bfae3727198df07e9eee00e901cfedf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 40 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,12 +30,11 @@ class CoreStats(object):\n     def stats_spider_opened(self, spider):\n         stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n         stats.set_value('envinfo/host', stats.get_value('envinfo/host'), spider=spider)\n-        stats.inc_value('spider_count/opened')\n \n     def stats_spider_closing(self, spider, reason):\n         stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n         stats.set_value('finish_status', 'OK' if reason == 'finished' else reason, spider=spider)\n-        stats.inc_value('spider_count/%s' % reason, spider=spider)\n+        stats.inc_value('spiders_closed_count/%s' % reason)\n \n     def item_scraped(self, item, spider):\n         stats.inc_value('item_scraped_count', spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9016e7e99353ad7462b334fd5ba77e52fba707d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,6 @@\n+from docutils.parsers.rst.roles import set_classes\n+from docutils import nodes\n+\n def setup(app):\n     app.add_crossref_type(\n         directivename = \"setting\",\n@@ -19,3 +22,10 @@ def setup(app):\n         rolename      = \"reqmeta\",\n         indextemplate = \"pair: %s; reqmeta\",\n     )\n+    app.add_role('source', source_role)\n+\n+def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n+    url = 'http://dev.scrapy.org/browser/' + text\n+    set_classes(options)\n+    node = nodes.reference(rawtext, text, refuri=ref, **options)\n+    return [node], []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#951ba507f94cbdeb7b573020dc6fe730fdc61dbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 17 | Churn Cumulative: 399 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -45,13 +45,7 @@ class DictItem(DictMixin, BaseItem):\n                 self[k] = v\n \n     def __getitem__(self, key):\n-        try:\n         return self._values[key]\n-        except KeyError:\n-            field = self.fields[key]\n-            if 'default' in field:\n-                return field['default']\n-            raise\n \n     def __setitem__(self, key, value):\n         if key in self.fields:\n\n@@ -25,7 +25,7 @@ class NewFieldPersonItem(BasePersonItem):\n \n \n class OverrideFieldPersonItem(BasePersonItem):\n-    age = Field(default=1)\n+    age = Field()\n \n \n class DjangoItemTest(unittest.TestCase):\n@@ -45,7 +45,6 @@ class DjangoItemTest(unittest.TestCase):\n     def test_override_field(self):\n         i = OverrideFieldPersonItem()\n         self.assertEqual(i.fields.keys(), ['age', 'name'])\n-        self.assertEqual(i.fields['age'], {'default': 1})\n \n     def test_save(self):\n         i = BasePersonItem()\n@@ -65,5 +64,4 @@ class DjangoItemTest(unittest.TestCase):\n         person = i.save(commit=False)\n \n         self.assertEqual(person.name, 'John')\n-        self.assertEqual(person.age, 1)\n \n\n@@ -40,13 +40,6 @@ class ItemTest(unittest.TestCase):\n         self.assertRaises(KeyError, i.__setitem__, 'field', 'text')\n         self.assertRaises(KeyError, i.__getitem__, 'field')\n \n-    def test_default_value(self):\n-        class TestItem(Item):\n-            name = Field(default=u'John')\n- \n-        i = TestItem()\n-        self.assertEqual(i['name'], u'John')\n-\n     def test_repr(self):\n         class TestItem(Item):\n             name = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6069b0e5b21b8ff66da9e9d4a7bdc8a4dabee263", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 14 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -62,9 +62,7 @@ class Scheduler(object):\n     def clear_pending_requests(self, spider):\n         \"\"\"Remove all pending requests for the given spider\"\"\"\n         q = self.pending_requests[spider]\n-        while q:\n-            _, dfd = q.pop()[0]\n-            dfd.errback(Failure(IgnoreRequest()))\n+        q.clear()\n \n     def next_request(self, spider):\n         \"\"\"Return the next available request to be downloaded for a spider.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d72d3f4607bed1bd033a3f14e2f7a7e431dd9192", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 24 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,18 +8,21 @@ import signal\n import traceback\n from pdb import Pdb\n \n+from scrapy.utils.engine import print_engine_status\n+\n \n class StackTraceDump(object):\n     def __init__(self):\n         try:\n             signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n+            signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n         except AttributeError:\n             # win32 platforms don't support SIGUSR signals\n             pass\n \n     def dump_stacktrace(self, signum, frame):\n-        print \"Got signal. Dumping stack trace...\"\n         traceback.print_stack(frame)\n+        print_engine_status()\n \n \n class Debugger(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70b0e42ca6bf2cc9914742157a7460e0475913d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,11 +3,9 @@ The Scrapy Scheduler\n \"\"\"\n \n from twisted.internet import defer\n-from twisted.python.failure import Failure\n \n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n from scrapy.core.schedulermw import SchedulerMiddlewareManager\n-from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n class Scheduler(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3897e33612ef9bf75b1220e677118f9f76dfa342", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -58,9 +58,7 @@ class Scheduler(object):\n         return dfd\n \n     def clear_pending_requests(self, spider):\n-        \"\"\"Remove all pending requests for the given spider\"\"\"\n-        q = self.pending_requests[spider]\n-        q.clear()\n+        pass\n \n     def next_request(self, spider):\n         \"\"\"Return the next available request to be downloaded for a spider.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88dbe2ae8785ab96ce0553ed8899c021fb9ac109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 108 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,7 +58,7 @@ class Scheduler(object):\n         return dfd\n \n     def clear_pending_requests(self, spider):\n-        pass\n+        self.pending_requests[spider].clear()\n \n     def next_request(self, spider):\n         \"\"\"Return the next available request to be downloaded for a spider.\n\n@@ -306,6 +306,11 @@ class PriorityQueue(object):\n                     return t\n         raise IndexError(\"pop from an empty queue\")\n     \n+    def clear(self):\n+        self.negitems.clear()\n+        self.pzero.clear()\n+        self.positems.clear()\n+\n     def __len__(self):\n         total = sum(len(v) for v in self.negitems.values()) + \\\n                 len(self.pzero) + \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#110cd0529612edc81fa20c15d6f2d1c98b8c3fb0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 8 | Churn Cumulative: 132 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.utils.python import setattr_default\n from scrapy import signals\n \n class AutoThrottle(object):\n@@ -70,16 +71,23 @@ class AutoThrottle(object):\n         self.last_lat = {}\n \n     def spider_opened(self, spider):\n+        setattr_default(spider, 'dont_throttle', False)\n+        if spider.dont_throttle:\n+            return\n         spider.download_delay = self.START_DELAY\n         spider.max_concurrent_requests = 1\n         self.last_latencies[spider] = [self.START_DELAY]\n         self.last_lat[spider] = self.START_DELAY, 0.0\n \n     def spider_closed(self, spider):\n+        if spider.dont_throttle:\n+            return\n         del self.last_latencies[spider]\n         del self.last_lat[spider]\n \n     def response_received(self, response, spider):\n+        if spider.dont_throttle:\n+            return\n         latency = response.meta.get('download_latency')\n         if not latency:\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcc28b71868e6fb39daf9b5d6f74e241736f8d79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 136 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.python import setattr_default\n+from scrapy.conf import settings\n+from scrapy.exceptions import NotConfigured\n from scrapy import signals\n \n class AutoThrottle(object):\n@@ -64,6 +66,8 @@ class AutoThrottle(object):\n     DEBUG = 0\n \n     def __init__(self):\n+        if not settings.getbool('AUTOTHROTTLE_ENABLED', True):\n+            raise NotConfigured\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.response_received, signal=signals.response_received)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#756bf0cc06ecfb9cc1cf19b6d38ce2b52ea6758f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 153 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ class AutoThrottle(object):\n     DEBUG = 0\n \n     def __init__(self):\n-        if not settings.getbool('AUTOTHROTTLE_ENABLED', True):\n+        if not settings.getbool('AUTOTHROTTLE_ENABLED'):\n             raise NotConfigured\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n\n@@ -126,6 +126,7 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.closespider.CloseSpider': 0,\n     'scrapy.contrib.feedexport.FeedExporter': 0,\n     'scrapy.contrib.spidercontext.SpiderContext': 0,\n+    'scrapy.contrib.throttle.AutoThrottle': 0,\n }\n \n FEED_URI = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2fa0f75f2d3dcd7a64b42b343061f24797626e21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 75 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from collections import defaultdict\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n from scrapy.conf import settings\n@@ -14,6 +15,8 @@ class CookiesMiddleware(object):\n     debug = settings.getbool('COOKIES_DEBUG')\n \n     def __init__(self):\n+        if not settings.getbool('COOKIES_ENABLED'):\n+            raise NotConfigured\n         self.jars = defaultdict(CookieJar)\n         dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n\n@@ -28,6 +28,7 @@ CONCURRENT_ITEMS = 100\n CONCURRENT_REQUESTS_PER_SPIDER = 8\n CONCURRENT_SPIDERS = 8\n \n+COOKIES_ENABLED = True\n COOKIES_DEBUG = False\n \n DEFAULT_ITEM_CLASS = 'scrapy.item.Item'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c369c50ca67f1c4c6f4ffac728e930c336fd7ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 7 | Churn Cumulative: 145 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -75,23 +75,16 @@ class AutoThrottle(object):\n         self.last_lat = {}\n \n     def spider_opened(self, spider):\n-        setattr_default(spider, 'dont_throttle', False)\n-        if spider.dont_throttle:\n-            return\n         spider.download_delay = self.START_DELAY\n         spider.max_concurrent_requests = 1\n         self.last_latencies[spider] = [self.START_DELAY]\n         self.last_lat[spider] = self.START_DELAY, 0.0\n \n     def spider_closed(self, spider):\n-        if spider.dont_throttle:\n-            return\n         del self.last_latencies[spider]\n         del self.last_lat[spider]\n \n     def response_received(self, response, spider):\n-        if spider.dont_throttle:\n-            return\n         latency = response.meta.get('download_latency')\n         if not latency:\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13d806678875cdede987af6b72092357d0577dca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 39 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 39 | Churn Cumulative: 59 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,39 +0,0 @@\n-\"\"\"\n-SpiderCloseDelay is an extension that keeps a idle spiders open until a\n-configurable amount of idle time has elapsed\n-\"\"\"\n-\n-from time import time\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from collections import defaultdict\n-\n-from scrapy import signals\n-from scrapy.project import crawler\n-from scrapy.exceptions import NotConfigured, DontCloseSpider\n-from scrapy.conf import settings\n-\n-\n-class SpiderCloseDelay(object):\n-    def __init__(self):\n-        self.delay = settings.getint('SPIDER_CLOSE_DELAY')\n-        if not self.delay:\n-            raise NotConfigured\n-\n-        self.opened_at = defaultdict(time)\n-        dispatcher.connect(self.spider_idle, signal=signals.spider_idle)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-\n-    def spider_idle(self, spider):\n-        try:\n-            lastseen = crawler.engine.downloader.sites[spider].lastseen\n-        except KeyError:\n-            lastseen = None\n-        if not lastseen:\n-            lastseen = self.opened_at[spider]\n-\n-        if time() < lastseen + self.delay:\n-            raise DontCloseSpider\n-\n-    def spider_closed(self, spider):\n-        self.opened_at.pop(spider, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d98fc8fb5b3a1f359d5ef0ba622c964966ce8d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 635 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -121,7 +121,7 @@ class Scraper(object):\n         assert isinstance(response, (Response, Failure))\n \n         dfd = self._scrape2(response, request, spider) # returns spiders processed output\n-        dfd.addErrback(self.handle_spider_error, request, spider)\n+        dfd.addErrback(self.handle_spider_error, request, response, spider)\n         dfd.addCallback(self.handle_spider_output, request, response, spider)\n         return dfd\n \n@@ -142,18 +142,20 @@ class Scraper(object):\n         dfd.addCallbacks(request.callback or spider.parse, request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n-    def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n+    def handle_spider_error(self, _failure, request, response, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n         msg = \"Spider error processing <%s> (referer: <%s>)\" % \\\n             (request.url, referer)\n         log.err(_failure, msg, spider=spider)\n+        send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \\\n+            spider=spider)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n             spider=spider)\n \n     def handle_spider_output(self, result, request, response, spider):\n         if not result:\n             return defer_succeed(None)\n-        it = iter_errback(result, self.handle_spider_error, request, spider)\n+        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n         dfd = parallel(it, self.concurrent_items,\n             self._process_spidermw_output, request, response, spider)\n         return dfd\n\n@@ -10,6 +10,7 @@ engine_stopped = object()\n spider_opened = object()\n spider_idle = object()\n spider_closed = object()\n+spider_error = object()\n request_received = object()\n response_received = object()\n response_downloaded = object()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1bc2339bb837bbc53b6f0760156df2bc60f17490", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 29 | Files Changed: 7 | Hunks: 17 | Methods Changed: 17 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 48 | Churn Cumulative: 1047 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,7 +34,7 @@ class CloseSpider(object):\n         if self.timeout:\n             dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         if self.itempassed:\n-            dispatcher.connect(self.item_passed, signal=signals.item_passed)\n+            dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n     def catch_log(self, event):\n@@ -55,7 +55,7 @@ class CloseSpider(object):\n             crawler.engine.close_spider, spider=spider, \\\n             reason='closespider_timeout')\n \n-    def item_passed(self, item, spider):\n+    def item_scraped(self, item, spider):\n         self.counts[spider] += 1\n         if self.counts[spider] == self.itempassed:\n             crawler.engine.close_spider(spider, 'closespider_itempassed')\n\n@@ -24,7 +24,6 @@ class CoreStats(object):\n         dispatcher.connect(self.stats_spider_opened, signal=signals.stats_spider_opened)\n         dispatcher.connect(self.stats_spider_closing, signal=signals.stats_spider_closing)\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n \n     def stats_spider_opened(self, spider):\n@@ -40,10 +39,6 @@ class CoreStats(object):\n         stats.inc_value('item_scraped_count', spider=spider)\n         stats.inc_value('item_scraped_count')\n \n-    def item_passed(self, item, spider):\n-        stats.inc_value('item_passed_count', spider=spider)\n-        stats.inc_value('item_passed_count')\n-\n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n         stats.inc_value('item_dropped_count', spider=spider)\n\n@@ -136,7 +136,7 @@ class FeedExporter(object):\n         self.slots = {}\n         dispatcher.connect(self.open_spider, signals.spider_opened)\n         dispatcher.connect(self.close_spider, signals.spider_closed)\n-        dispatcher.connect(self.item_passed, signals.item_passed)\n+        dispatcher.connect(self.item_scraped, signals.item_scraped)\n \n     def open_spider(self, spider):\n         file = TemporaryFile(prefix='feed-')\n@@ -163,7 +163,7 @@ class FeedExporter(object):\n         d.addBoth(lambda _: slot.file.close())\n         return d\n \n-    def item_passed(self, item, spider):\n+    def item_scraped(self, item, spider):\n         slot = self.slots[spider]\n         slot.exporter.export_item(item)\n         slot.itemcount += 1\n\n@@ -169,13 +169,9 @@ class Scraper(object):\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n-            log.msg(log.formatter.scraped(output, request, response, spider), \\\n-                level=log.DEBUG, spider=spider)\n             self.sites[spider].itemproc_size += 1\n-            dfd = send_catch_log_deferred(signal=signals.item_scraped, \\\n-                item=output, spider=spider, response=response)\n-            dfd.addBoth(lambda _: self.itemproc.process_item(output, spider))\n-            dfd.addBoth(self._itemproc_finished, output, spider)\n+            dfd = self.itemproc.process_item(output, spider)\n+            dfd.addBoth(self._itemproc_finished, output, response, spider)\n             return dfd\n         elif output is None:\n             pass\n@@ -198,22 +194,22 @@ class Scraper(object):\n         else:\n             return spider_failure # exceptions raised in the spider code\n \n-    def _itemproc_finished(self, output, item, spider):\n+    def _itemproc_finished(self, output, item, response, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n         self.sites[spider].itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                log.msg(log.formatter.dropped(item, ex, spider), \\\n+                log.msg(log.formatter.dropped(item, ex, response, spider), \\\n                     level=log.WARNING, spider=spider)\n                 return send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n-            log.msg(log.formatter.passed(output, spider), log.INFO, spider=spider)\n-            # TODO: remove item_passed 'output' parameter for Scrapy 0.12\n-            return send_catch_log_deferred(signal=signals.item_passed, \\\n-                item=output, spider=spider, output=output, original_item=item)\n+            log.msg(log.formatter.scraped(output, response, spider), \\\n+                log.DEBUG, spider=spider)\n+            return send_catch_log_deferred(signal=signals.item_scraped, \\\n+                item=output, response=response, spider=spider)\n \n\n@@ -11,11 +11,8 @@ class LogFormatter(object):\n         return \"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n             request, referer, flags)\n \n-    def scraped(self, item, request, response, spider):\n-        return \"Scraped %s in <%s>\" % (item, request.url)\n+    def scraped(self, item, response, spider):\n+        return \"Scraped %s in <%s>\" % (item, response.url)\n \n-    def dropped(self, item, exception, spider):\n+    def dropped(self, item, exception, response, spider):\n         return \"Dropped %s - %s\" % (item, unicode(exception))\n-\n-    def passed(self, item, spider):\n-        return \"Passed %s\" % item\n\n@@ -15,8 +15,9 @@ request_received = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n-item_passed = object()\n item_dropped = object()\n stats_spider_opened = object()\n stats_spider_closing = object()\n stats_spider_closed = object()\n+\n+item_passed = item_scraped # for backwards compatibility\n\n@@ -25,7 +25,8 @@ class LoggingContribTest(unittest.TestCase):\n     def test_dropped(self):\n         item = {}\n         exception = Exception(u\"\\u2018\")\n-        self.assertEqual(self.formatter.dropped(item, exception, self.spider),\n+        response = Response(\"http://www.example.com\")\n+        self.assertEqual(self.formatter.dropped(item, exception, response, self.spider),\n             u\"Dropped {} - \\u2018\")\n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bf733b6f624cc5a3ddf8a7e46337dd3290740b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 434 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n+from pprint import pformat\n from UserDict import DictMixin\n \n from scrapy.utils.trackref import object_ref\n@@ -72,11 +73,7 @@ class DictItem(DictMixin, BaseItem):\n         return self._values.keys()\n \n     def __repr__(self):\n-        \"\"\"Generate a representation of this item that can be used to\n-        reconstruct the item by evaluating it\n-        \"\"\"\n-        values = ', '.join('%s=%r' % field for field in self.iteritems())\n-        return \"%s(%s)\" % (self.__class__.__name__, values)\n+        return pformat(dict(self))\n \n \n class Item(DictItem):\n\n@@ -1,3 +1,4 @@\n+import os\n \n class LogFormatter(object):\n     \"\"\"Class for generating log messages for different actions. All methods\n@@ -12,7 +13,7 @@ class LogFormatter(object):\n             request, referer, flags)\n \n     def scraped(self, item, response, spider):\n-        return \"Scraped %s in <%s>\" % (item, response.url)\n+        return \"Scraped from %s\" % response + os.linesep + str(item)\n \n     def dropped(self, item, exception, response, spider):\n-        return \"Dropped %s - %s\" % (item, unicode(exception))\n+        return \"Dropped: %s\" % unicode(exception) + os.linesep + str(item)\n\n@@ -12,7 +12,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_empty(self):\n         _, out, _ = yield self.execute(['-c', 'item'])\n-        assert 'Item' in out\n+        assert '{}' in out\n \n     @defer.inlineCallbacks\n     def test_response_body(self):\n\n@@ -50,7 +50,7 @@ class ItemTest(unittest.TestCase):\n         i['number'] = 123\n         itemrepr = repr(i)\n         self.assertEqual(itemrepr,\n-                         \"TestItem(name=u'John Doe', number=123)\")\n+                         \"{'name': u'John Doe', 'number': 123}\")\n \n         i2 = eval(itemrepr)\n         self.assertEqual(i2['name'], 'John Doe')\n\n@@ -26,8 +26,8 @@ class LoggingContribTest(unittest.TestCase):\n         item = {}\n         exception = Exception(u\"\\u2018\")\n         response = Response(\"http://www.example.com\")\n-        self.assertEqual(self.formatter.dropped(item, exception, response, self.spider),\n-            u\"Dropped {} - \\u2018\")\n+        lines = self.formatter.dropped(item, exception, response, self.spider).splitlines()\n+        self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03ae481cad04a59ee47676f5d1b62b860bc39462", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 481 | Files Changed: 7 | Hunks: 7 | Methods Changed: 34 | Complexity Δ (Sum/Max): -109/0 | Churn Δ: 481 | Churn Cumulative: 996 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 15 | DMM Complexity: 0.25\n\nDIFF:\n@@ -1,4 +0,0 @@\n-\"\"\"CrawlSpider v2\"\"\"\n-\n-from .rules import Rule\n-from .spider import CrawlSpider\n\n@@ -1,61 +0,0 @@\n-\"\"\"\n-Request/Response Matchers\n-\n-Perform evaluation to Request or Response attributes\n-\"\"\"\n-\n-import re\n-\n-class BaseMatcher(object):\n-    \"\"\"Base matcher. Returns True by default.\"\"\"\n-\n-    def matches_request(self, request):\n-        \"\"\"Performs Request Matching\"\"\"\n-        return True\n-\n-    def matches_response(self, response):\n-        \"\"\"Performs Response Matching\"\"\"\n-        return True\n-\n-\n-class UrlMatcher(BaseMatcher):\n-    \"\"\"Matches URL attribute\"\"\"\n-\n-    def __init__(self, url):\n-        \"\"\"Initialize url attribute\"\"\"\n-        self._url = url\n-\n-    def matches_url(self, url):\n-        \"\"\"Returns True if given url is equal to matcher's url\"\"\"\n-        return self._url == url\n-\n-    def matches_request(self, request):\n-        \"\"\"Returns True if Request's url matches initial url\"\"\"\n-        return self.matches_url(request.url) \n-\n-    def matches_response(self, response):\n-        \"\"\"Returns True if Response's url matches initial url\"\"\"\n-        return self.matches_url(response.url)\n-\n-\n-class UrlRegexMatcher(UrlMatcher):\n-    \"\"\"Matches URL using regular expression\"\"\"\n-\n-    def __init__(self, regex, flags=0):\n-        \"\"\"Initialize regular expression\"\"\"\n-        self._regex = re.compile(regex, flags)\n-\n-    def matches_url(self, url):\n-        \"\"\"Returns True if url matches regular expression\"\"\"\n-        return self._regex.search(url) is not None\n-\n-\n-class UrlListMatcher(UrlMatcher):\n-    \"\"\"Matches if URL is in List\"\"\"\n-\n-    def __init__(self, urls):\n-        self._urls = urls\n-\n-    def matches_url(self, url):\n-        \"\"\"Returns True if url is in urls list\"\"\"\n-        return url in self._urls\n\n@@ -1,113 +0,0 @@\n-\"\"\"Request Extractors\"\"\"\n-from w3lib.url import safe_url_string, urljoin_rfc\n-\n-from scrapy.http import Request\n-from scrapy.selector import HtmlXPathSelector\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import FixedSGMLParser, str_to_unicode\n-\n-from itertools import ifilter\n-\n-\n-class BaseSgmlRequestExtractor(FixedSGMLParser):\n-    \"\"\"Base SGML Request Extractor\"\"\"\n-\n-    def __init__(self, tag='a', attr='href'):\n-        \"\"\"Initialize attributes\"\"\"\n-        FixedSGMLParser.__init__(self)\n-\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.current_request = None\n-\n-    def extract_requests(self, response):\n-        \"\"\"Returns list of requests extracted from response\"\"\"\n-        return self._extract_requests(response.body, response.url,\n-                                  response.encoding)\n-\n-    def _extract_requests(self, response_text, response_url, response_encoding):\n-        \"\"\"Extract requests with absolute urls\"\"\"\n-        self.reset()\n-        self.feed(response_text)\n-        self.close()\n-\n-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n-        self._make_absolute_urls(base_url, response_encoding)\n-        self._fix_link_text_encoding(response_encoding)\n-\n-        return self.requests\n-\n-    def _make_absolute_urls(self, base_url, encoding):\n-        \"\"\"Makes all request's urls absolute\"\"\"\n-        self.requests = [x.replace(url=safe_url_string(urljoin_rfc(base_url, \\\n-            x.url, encoding), encoding)) for x in self.requests]\n-\n-    def _fix_link_text_encoding(self, encoding):\n-        \"\"\"Convert link_text to unicode for each request\"\"\"\n-        for req in self.requests:\n-            req.meta.setdefault('link_text', '')\n-            req.meta['link_text'] = str_to_unicode(req.meta['link_text'],\n-                                                   encoding) \n-\n-    def reset(self):\n-        \"\"\"Reset state\"\"\"\n-        FixedSGMLParser.reset(self)\n-        self.requests = []\n-        self.base_url = None\n-            \n-    def unknown_starttag(self, tag, attrs):\n-        \"\"\"Process unknown start tag\"\"\"\n-        if 'base' == tag:\n-            self.base_url = dict(attrs).get('href')\n-\n-        _matches = lambda (attr, value): self.scan_attr(attr) \\\n-                                        and value is not None\n-        if self.scan_tag(tag):\n-            for attr, value in ifilter(_matches, attrs):\n-                req = Request(url=value)\n-                self.requests.append(req)\n-                self.current_request = req\n-\n-    def unknown_endtag(self, tag):\n-        \"\"\"Process unknown end tag\"\"\"\n-        self.current_request = None\n-\n-    def handle_data(self, data):\n-        \"\"\"Process data\"\"\"\n-        current = self.current_request\n-        if current and not 'link_text' in current.meta:\n-            current.meta['link_text'] = data.strip()\n-\n-\n-class SgmlRequestExtractor(BaseSgmlRequestExtractor):\n-    \"\"\"SGML Request Extractor\"\"\"\n-\n-    def __init__(self, tags=None, attrs=None):\n-        \"\"\"Initialize with custom tag & attribute function checkers\"\"\"\n-        # defaults\n-        tags = tuple(tags) if tags else ('a', 'area')\n-        attrs = tuple(attrs) if attrs else ('href', )\n-\n-        tag_func = lambda x: x in tags\n-        attr_func = lambda x: x in attrs\n-        BaseSgmlRequestExtractor.__init__(self, tag=tag_func, attr=attr_func)\n-\n-# TODO: move to own file\n-class XPathRequestExtractor(SgmlRequestExtractor):\n-    \"\"\"SGML Request Extractor with XPath restriction\"\"\"\n-\n-    def __init__(self, restrict_xpaths, tags=None, attrs=None):\n-        \"\"\"Initialize XPath restrictions\"\"\"\n-        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n-        SgmlRequestExtractor.__init__(self, tags, attrs)\n-\n-    def extract_requests(self, response):\n-        \"\"\"Restrict to XPath regions\"\"\"\n-        hxs = HtmlXPathSelector(response)\n-        fragments = (''.join(\n-                            html_frag for html_frag in hxs.select(xpath).extract()\n-                        ) for xpath in self.restrict_xpaths)\n-        html_slice = ''.join(html_frag for html_frag in fragments)\n-        return self._extract_requests(html_slice, response.url,\n-                                        response.encoding)\n-\n\n@@ -1,27 +0,0 @@\n-\"\"\"Request Generator\"\"\"\n-from itertools import imap\n-\n-class RequestGenerator(object):\n-    \"\"\"Extracto and process requests from response\"\"\"\n-\n-    def __init__(self, req_extractors, req_processors, callback, spider=None):\n-        \"\"\"Initialize attributes\"\"\"\n-        self._request_extractors = req_extractors\n-        self._request_processors = req_processors\n-        #TODO: resolve callback?\n-        self._callback = callback\n-\n-    def generate_requests(self, response):\n-        \"\"\"Extract and process new requests from response.\n-           Attach callback to each request as default callback.\"\"\"\n-        requests = []\n-        for ext in self._request_extractors:\n-            requests.extend(ext.extract_requests(response))\n-\n-        for proc in self._request_processors:\n-            requests = proc(requests)\n-\n-        # return iterator\n-        # @@@ creates new Request object with callback\n-        return imap(lambda r: r.replace(callback=self._callback), requests)\n-\n\n@@ -1,107 +0,0 @@\n-\"\"\"Request Processors\"\"\"\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n-\n-from itertools import ifilter\n-\n-import re\n-\n-class Canonicalize(object):\n-    \"\"\"Canonicalize Request Processor\"\"\"\n-\n-    def __call__(self, requests):\n-        \"\"\"Canonicalize all requests' urls\"\"\"\n-        return (x.replace(url=canonicalize_url(x.url)) for x in requests)\n-        \n-\n-class FilterDupes(object):\n-    \"\"\"Filter duplicate Requests\"\"\"\n-\n-    def __init__(self, *attributes):\n-        \"\"\"Initialize comparison attributes\"\"\"\n-        self._attributes = tuple(attributes) if attributes \\\n-                                             else tuple(['url'])\n-\n-    def _equal_attr(self, obj1, obj2, attr):\n-        return getattr(obj1, attr) == getattr(obj2, attr)\n-\n-    def _requests_equal(self, req1, req2):\n-        \"\"\"Attribute comparison helper\"\"\"\n-        # look for not equal attribute\n-        _not_equal = lambda attr: not self._equal_attr(req1, req2, attr)\n-        for attr in ifilter(_not_equal, self._attributes):\n-            return False\n-        # all attributes equal\n-        return True\n-\n-    def _request_in(self, request, requests_seen):\n-        \"\"\"Check if request is in given requests seen list\"\"\"\n-        _req_seen = lambda r: self._requests_equal(r, request)\n-        for seen in ifilter(_req_seen, requests_seen):\n-            return True\n-        # request not seen\n-        return False\n-\n-    def __call__(self, requests):\n-        \"\"\"Filter seen requests\"\"\"\n-        # per-call duplicates filter\n-        self.requests_seen = set()\n-        _not_seen = lambda r: not self._request_in(r, self.requests_seen)\n-        for req in ifilter(_not_seen, requests):\n-            yield req\n-            # registry seen request\n-            self.requests_seen.add(req)\n-\n-\n-class FilterDomain(object):\n-    \"\"\"Filter request's domain\"\"\"\n-\n-    def __init__(self, allow=(), deny=()):\n-         \"\"\"Initialize allow/deny attributes\"\"\"\n-         self.allow = tuple(arg_to_iter(allow))\n-         self.deny = tuple(arg_to_iter(deny))\n-\n-    def __call__(self, requests):\n-        \"\"\"Filter domains\"\"\"\n-        processed = (req for req in requests)\n-\n-        if self.allow:\n-            processed = (req for req in requests\n-                            if url_is_from_any_domain(req.url, self.allow))\n-        if self.deny:\n-            processed = (req for req in requests\n-                            if not url_is_from_any_domain(req.url, self.deny))\n-\n-        return processed\n-\n-\n-class FilterUrl(object):\n-    \"\"\"Filter request's url\"\"\"\n-\n-    def __init__(self, allow=(), deny=()):\n-        \"\"\"Initialize allow/deny attributes\"\"\"\n-        _re_type = type(re.compile('', 0))\n-\n-        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) \n-                          for x in arg_to_iter(allow)]\n-        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) \n-                         for x in arg_to_iter(deny)]\n-\n-    def __call__(self, requests):\n-        \"\"\"Filter request's url based on allow/deny rules\"\"\"\n-        #TODO: filter valid urls here?\n-        processed = (req for req in requests)\n-\n-        if self.allow_res:\n-            processed = (req for req in requests\n-                            if self._matches(req.url, self.allow_res))\n-        if self.deny_res:\n-            processed = (req for req in requests\n-                            if not self._matches(req.url, self.deny_res))\n-\n-        return processed\n-\n-    def _matches(self, url, regexs):\n-        \"\"\"Returns True if url matches any regex in given list\"\"\"\n-        return any(r.search(url) for r in regexs)\n-\n\n@@ -1,100 +0,0 @@\n-\"\"\"Crawler Rules\"\"\"\n-from scrapy.http import Request\n-from scrapy.http import Response\n-\n-from functools import partial\n-from itertools import ifilter\n-\n-from .matchers import BaseMatcher\n-# default strint-to-matcher class\n-from .matchers import UrlRegexMatcher\n-\n-class CompiledRule(object):\n-    \"\"\"Compiled version of Rule\"\"\"\n-    def __init__(self, matcher, callback=None, follow=False):\n-        \"\"\"Initialize attributes checking type\"\"\"\n-        assert isinstance(matcher, BaseMatcher)\n-        assert callback is None or callable(callback)\n-        assert isinstance(follow, bool)\n-\n-        self.matcher = matcher\n-        self.callback = callback\n-        self.follow = follow\n-\n-\n-class Rule(object):\n-    \"\"\"Crawler Rule\"\"\"\n-    def __init__(self, matcher=None, callback=None, follow=False, **kwargs):\n-        \"\"\"Store attributes\"\"\"\n-        self.matcher = matcher\n-        self.callback = callback\n-        self.cb_kwargs = kwargs if kwargs else {}\n-        self.follow = True if follow else False\n-\n-        if self.callback is None and self.follow is False:\n-            raise ValueError(\"Rule must either have a callback or \"\n-                             \"follow=True: %r\" % self)\n-\n-    def __repr__(self):\n-        return \"Rule(matcher=%r, callback=%r, follow=%r, **%r)\" \\\n-                % (self.matcher, self.callback, self.follow, self.cb_kwargs)\n-\n-\n-class RulesManager(object):\n-    \"\"\"Rules Manager\"\"\"\n-    def __init__(self, rules, spider, default_matcher=UrlRegexMatcher):\n-        \"\"\"Initialize rules using spider and default matcher\"\"\"\n-        self._rules = tuple()\n-\n-        # compile absolute/relative-to-spider callbacks\"\"\"\n-        for rule in rules:\n-            # prepare matcher\n-            if rule.matcher is None:\n-                # instance BaseMatcher by default\n-                matcher = BaseMatcher()\n-            elif isinstance(rule.matcher, BaseMatcher):\n-                matcher = rule.matcher\n-            else:\n-                # matcher not BaseMatcher, check for string\n-                if isinstance(rule.matcher, basestring):\n-                    # instance default matcher\n-                    matcher = default_matcher(rule.matcher)\n-                else:\n-                    raise ValueError('Not valid matcher given %r in %r' \\\n-                                    % (rule.matcher, rule))\n-\n-            # prepare callback\n-            if callable(rule.callback):\n-                callback = rule.callback\n-            elif not rule.callback is None:\n-                # callback from spider\n-                callback = getattr(spider, rule.callback)\n-\n-                if not callable(callback):\n-                    raise AttributeError('Invalid callback %r can not be resolved' \\\n-                                            % callback)\n-            else:\n-                callback = None\n-\n-            if rule.cb_kwargs:\n-                # build partial callback\n-                callback = partial(callback, **rule.cb_kwargs)\n-\n-            # append compiled rule to rules list\n-            crule = CompiledRule(matcher, callback, follow=rule.follow)\n-            self._rules += (crule, )\n-\n-    def get_rule_from_request(self, request):\n-        \"\"\"Returns first rule that matches given Request\"\"\"\n-        _matches = lambda r: r.matcher.matches_request(request)\n-        for rule in ifilter(_matches, self._rules):\n-            # return first match of iterator\n-            return rule\n-        \n-    def get_rule_from_response(self, response):\n-        \"\"\"Returns first rule that matches given Response\"\"\"\n-        _matches = lambda r: r.matcher.matches_response(response)\n-        for rule in ifilter(_matches, self._rules):\n-            # return first match of iterator\n-            return rule\n- \n\n@@ -1,69 +0,0 @@\n-\"\"\"CrawlSpider v2\"\"\"\n-from scrapy.spider import BaseSpider\n-from scrapy.utils.spider import iterate_spider_output\n-\n-from .matchers import UrlListMatcher\n-from .rules import Rule, RulesManager\n-from .reqext import SgmlRequestExtractor\n-from .reqgen import RequestGenerator\n-from .reqproc import Canonicalize, FilterDupes\n-\n-class CrawlSpider(BaseSpider):\n-    \"\"\"CrawlSpider v2\"\"\"\n-\n-    request_extractors = None\n-    request_processors = None\n-    rules = []\n-\n-    def __init__(self, *a, **kw):\n-        \"\"\"Initialize dispatcher\"\"\"\n-        super(CrawlSpider, self).__init__(*a, **kw)\n-\n-        # auto follow start urls\n-        if self.start_urls:\n-            _matcher = UrlListMatcher(self.start_urls)\n-            # append new rule using type from current self.rules\n-            rules = self.rules + type(self.rules)([\n-                            Rule(_matcher, follow=True)\n-                        ])\n-        else:\n-            rules = self.rules\n-\n-        # set defaults if not set\n-        if self.request_extractors is None:\n-            # default link extractor. Extracts all links from response\n-            self.request_extractors = [ SgmlRequestExtractor() ]\n-\n-        if self.request_processors is None:\n-            # default proccessor. Filter duplicates requests\n-            self.request_processors = [ FilterDupes() ]\n-\n-\n-        # wrap rules\n-        self._rulesman = RulesManager(rules, spider=self)\n-        # generates new requests with given callback\n-        self._reqgen = RequestGenerator(self.request_extractors,\n-                                        self.request_processors,\n-                                        callback=self.parse)\n-\n-    def parse(self, response):\n-        \"\"\"Dispatch callback and generate requests\"\"\"\n-        # get rule for response\n-        rule = self._rulesman.get_rule_from_response(response)\n-\n-        if rule:\n-            # dispatch callback if set\n-            if rule.callback:\n-                output = iterate_spider_output(rule.callback(response))\n-                for req_or_item in output:\n-                    yield req_or_item\n-\n-            if rule.follow:\n-                for req in self._reqgen.generate_requests(response):\n-                    # only dispatch request if has matching rule\n-                    if self._rulesman.get_rule_from_request(req):\n-                         yield req\n-        else:\n-             self.log(\"No rule for response %s\" % response, level=log.WARNING)\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
