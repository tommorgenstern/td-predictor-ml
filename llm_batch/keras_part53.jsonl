{"custom_id": "keras#b691579fff3f508cff8faae0e20aed0d65c91636", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 2409 | Contributors (this commit): 11 | Commits (past 90d): 16 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,6 +41,15 @@ class Recurrent(MaskedLayer):\n     def step(self, x, states):\n         raise NotImplementedError\n \n+    def get_initial_states(self, X):\n+        # build an all-zero tensor of shape (samples, output_dim)\n+        initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n+        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n+        reducer = K.zeros((self.input_dim, self.output_dim))\n+        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n+        initial_states = [initial_state for _ in range(len(self.states))]\n+        return initial_states\n+\n     def get_output(self, train=False):\n         # input shape: (nb_samples, time (padded with zeros), input_dim)\n         X = self.get_input(train)\n@@ -64,12 +73,7 @@ class Recurrent(MaskedLayer):\n         if self.stateful:\n             initial_states = self.states\n         else:\n-            # build an all-zero tensor of shape (samples, output_dim)\n-            initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n-            initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n-            reducer = K.zeros((self.input_dim, self.output_dim))\n-            initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n-            initial_states = [initial_state for _ in range(len(self.states))]\n+            initial_states = self.get_initial_states(X)\n \n         last_output, outputs, states = K.rnn(self.step, X, initial_states,\n                                              go_backwards=self.go_backwards,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#beb5151b7a721a0813bee3020ad8665a9cd2336f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 15 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 199 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n from __future__ import print_function\n import numpy as np\n-np.random.seed(1337)\n+import pytest\n \n from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n@@ -8,11 +8,10 @@ from keras.layers.core import Dense, Activation, TimeDistributedDense, Flatten\n from keras.layers.recurrent import GRU\n from keras.layers.convolutional import Convolution2D\n from keras.utils.np_utils import to_categorical\n-import unittest\n \n \n-class TestTasks(unittest.TestCase):\n-    def test_vector_clf(self):\n+def test_vector_clf():\n+    np.random.seed(1337)\n     nb_hidden = 10\n \n     print('vector classification data:')\n@@ -38,9 +37,11 @@ class TestTasks(unittest.TestCase):\n     history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16,\n                         validation_data=(X_test, y_test),\n                         show_accuracy=True, verbose=0)\n-        self.assertTrue(history.history['val_acc'][-1] > 0.8)\n+    assert(history.history['val_acc'][-1] > 0.8)\n \n-    def test_vector_reg(self):\n+\n+def test_vector_reg():\n+    np.random.seed(1337)\n     nb_hidden = 10\n     print('vector regression data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n@@ -60,9 +61,11 @@ class TestTasks(unittest.TestCase):\n     model.compile(loss='hinge', optimizer='adagrad')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=0)\n-        self.assertTrue(history.history['val_loss'][-1] < 0.9)\n+    assert (history.history['val_loss'][-1] < 0.9)\n \n-    def test_temporal_clf(self):\n+\n+def test_temporal_clf():\n+    np.random.seed(1337)\n     print('temporal classification data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n@@ -84,9 +87,11 @@ class TestTasks(unittest.TestCase):\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test),\n                         show_accuracy=True, verbose=0)\n-        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n+    assert(history.history['val_acc'][-1] > 0.9)\n \n-    def test_temporal_reg(self):\n+\n+def test_temporal_reg():\n+    np.random.seed(1337)\n     print('temporal regression data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n@@ -103,9 +108,11 @@ class TestTasks(unittest.TestCase):\n     model.compile(loss='hinge', optimizer='adam')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=0)\n-        self.assertTrue(history.history['val_loss'][-1] < 0.8)\n+    assert(history.history['val_loss'][-1] < 0.8)\n \n-    def test_seq_to_seq(self):\n+\n+def test_seq_to_seq():\n+    np.random.seed(1337)\n     print('sequence to sequence data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n@@ -122,9 +129,11 @@ class TestTasks(unittest.TestCase):\n     model.compile(loss='hinge', optimizer='rmsprop')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=0)\n-        self.assertTrue(history.history['val_loss'][-1] < 0.8)\n+    assert(history.history['val_loss'][-1] < 0.8)\n \n-    def test_img_clf(self):\n+\n+def test_img_clf():\n+    np.random.seed(1337)\n     print('image classification data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n@@ -150,9 +159,9 @@ class TestTasks(unittest.TestCase):\n                         validation_data=(X_test, y_test),\n                         show_accuracy=True, verbose=0)\n     print(history.history['val_acc'][-1])\n-        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n+    assert(history.history['val_acc'][-1] > 0.9)\n \n \n if __name__ == '__main__':\n     print('Test different types of classification and regression tasks')\n-    unittest.main()\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cac308e88d07e06415bff8a41100c235d28b7c02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 59 | Files Changed: 1 | Hunks: 24 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 80 | Churn Cumulative: 279 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,29 +10,22 @@ from keras.layers.convolutional import Convolution2D\n from keras.utils.np_utils import to_categorical\n \n \n-def test_vector_clf():\n+def test_vector_classification():\n     np.random.seed(1337)\n     nb_hidden = 10\n \n-    print('vector classification data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(10,),\n                                                          classification=True,\n                                                          nb_class=2)\n-    print('X_train:', X_train.shape)\n-    print('X_test:', X_test.shape)\n-    print('y_train:', y_train.shape)\n-    print('y_test:', y_test.shape)\n-\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n \n-    model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(y_train.shape[-1]))\n-    model.add(Activation('softmax'))\n+    model = Sequential([\n+        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='relu'),\n+        Dense(y_train.shape[-1], activation='softmax')\n+    ])\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n     history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16,\n                         validation_data=(X_test, y_test),\n@@ -40,49 +33,38 @@ def test_vector_clf():\n     assert(history.history['val_acc'][-1] > 0.8)\n \n \n-def test_vector_reg():\n+def test_vector_regression():\n     np.random.seed(1337)\n     nb_hidden = 10\n-    print('vector regression data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(10,),\n                                                          output_shape=(2,),\n                                                          classification=False)\n-    print('X_train:', X_train.shape)\n-    print('X_test:', X_test.shape)\n-    print('y_train:', y_train.shape)\n-    print('y_test:', y_test.shape)\n \n-    model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n-    model.add(Activation('tanh'))\n-    model.add(Dense(y_train.shape[-1]))\n+    model = Sequential([\n+        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='tanh'),\n+        Dense(y_train.shape[-1])\n+    ])\n+\n     model.compile(loss='hinge', optimizer='adagrad')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=0)\n     assert (history.history['val_loss'][-1] < 0.9)\n \n \n-def test_temporal_clf():\n+def test_temporal_classification():\n     np.random.seed(1337)\n-    print('temporal classification data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 5),\n                                                          classification=True,\n                                                          nb_class=2)\n-    print('X_train:', X_train.shape)\n-    print('X_test:', X_test.shape)\n-    print('y_train:', y_train.shape)\n-    print('y_test:', y_test.shape)\n-\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n \n     model = Sequential()\n-    model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n-    model.add(Activation('softmax'))\n+    model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2]), activation='softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test),\n@@ -90,19 +72,13 @@ def test_temporal_clf():\n     assert(history.history['val_acc'][-1] > 0.9)\n \n \n-def test_temporal_reg():\n+def test_temporal_regression():\n     np.random.seed(1337)\n-    print('temporal regression data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 5),\n                                                          output_shape=(2,),\n                                                          classification=False)\n-    print('X_train:', X_train.shape)\n-    print('X_test:', X_test.shape)\n-    print('y_train:', y_train.shape)\n-    print('y_test:', y_test.shape)\n-\n     model = Sequential()\n     model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n     model.compile(loss='hinge', optimizer='adam')\n@@ -111,18 +87,13 @@ def test_temporal_reg():\n     assert(history.history['val_loss'][-1] < 0.8)\n \n \n-def test_seq_to_seq():\n+def test_sequence_to_sequence():\n     np.random.seed(1337)\n-    print('sequence to sequence data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 5),\n                                                          output_shape=(3, 5),\n                                                          classification=False)\n-    print('X_train:', X_train.shape)\n-    print('X_test:', X_test.shape)\n-    print('y_train:', y_train.shape)\n-    print('y_test:', y_test.shape)\n \n     model = Sequential()\n     model.add(TimeDistributedDense(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n@@ -132,36 +103,27 @@ def test_seq_to_seq():\n     assert(history.history['val_loss'][-1] < 0.8)\n \n \n-def test_img_clf():\n+def test_image_classification():\n     np.random.seed(1337)\n-    print('image classification data:')\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 8, 8),\n                                                          classification=True,\n                                                          nb_class=2)\n-    print('X_train:', X_train.shape)\n-    print('X_test:', X_test.shape)\n-    print('y_train:', y_train.shape)\n-    print('y_test:', y_test.shape)\n-\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n \n-    model = Sequential()\n-    model.add(Convolution2D(8, 8, 8, input_shape=(3, 8, 8)))\n-    model.add(Activation('sigmoid'))\n-    model.add(Flatten())\n-    model.add(Dense(y_test.shape[-1]))\n-    model.add(Activation('softmax'))\n+    model = Sequential([\n+        Convolution2D(8, 8, 8, input_shape=(3, 8, 8), activation='sigmoid'),\n+        Flatten(),\n+        Dense(y_test.shape[-1], activation='softmax')\n+    ])\n     model.compile(loss='categorical_crossentropy', optimizer='sgd')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test),\n                         show_accuracy=True, verbose=0)\n-    print(history.history['val_acc'][-1])\n     assert(history.history['val_acc'][-1] > 0.9)\n \n \n if __name__ == '__main__':\n-    print('Test different types of classification and regression tasks')\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#369fb0543610c8ca974e25fcbd99dd49c6b39e8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 91 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n # encoding: utf-8\n-\"\"\"Test seya.layers.recurrent module\"\"\"\n+\"\"\"Test keras.layers.core.Layer.__call__\"\"\"\n \n from __future__ import print_function\n \n@@ -11,7 +11,6 @@ from numpy.testing import assert_allclose\n from keras import backend as K\n from keras.layers.core import Dense\n from keras.models import Sequential\n-floatX = K.common._FLOATX\n \n \n class TestCall(unittest.TestCase):\n@@ -26,7 +25,7 @@ class TestCall(unittest.TestCase):\n         Y = layer(X)\n         F = K.function([X], [Y])\n \n-        x = np.random.randn(nb_samples, input_dim).astype(floatX)\n+        x = np.random.randn(nb_samples, input_dim).astype(K.floatx())\n         y = F([x])[0]\n         assert_allclose(np.dot(x, W), y)\n \n@@ -41,7 +40,7 @@ class TestCall(unittest.TestCase):\n         Y = model(X)\n         F = K.function([X], [Y])\n \n-        x = np.random.randn(nb_samples, input_dim).astype(floatX)\n+        x = np.random.randn(nb_samples, input_dim).astype(K.floatx())\n         y1 = F([x])[0]\n         y2 = model.predict(x)\n         # results of __call__ should match model.predict\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4aa713b1dc39d1e7df03467357b9d9ba6a8cdacd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 6524 | Contributors (this commit): 34 | Commits (past 90d): 113 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class Sequential(Layer):\n             self.add(layer)\n \n     def __call__(self, X, train=False):\n-        # temporally substitute input of first layer\n+        # set temporary input to first layer\n         tmp = self.layers[0].get_input\n         self.layers[0].get_input = lambda _: X\n         Y = self.get_output(train=train)\n\n@@ -34,7 +34,7 @@ class Layer(object):\n             self.params = []\n \n     def __call__(self, X, train=False):\n-        # temporally substitute input\n+        # set temporary input\n         tmp = self.get_input\n         self.get_input = lambda _: X\n         Y = self.get_output(train=train)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#614e48e6128939e1ba655f431a0ad40cb2b759ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 95 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class TestCall(unittest.TestCase):\n         F = K.function([X], [Y])\n \n         x = np.random.randn(nb_samples, input_dim).astype(K.floatx())\n-        y = F([x])[0]\n+        y = F([x])[0].astype(K.floatx())\n         assert_allclose(np.dot(x, W), y)\n \n     def test_sequential_call(self):\n@@ -41,7 +41,7 @@ class TestCall(unittest.TestCase):\n         F = K.function([X], [Y])\n \n         x = np.random.randn(nb_samples, input_dim).astype(K.floatx())\n-        y1 = F([x])[0]\n+        y1 = F([x])[0].astype(K.floatx())\n         y2 = model.predict(x)\n         # results of __call__ should match model.predict\n         assert_allclose(y1, y2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acbae2cebb13a18412b439ed237fbc23abace69a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 104 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,14 +20,15 @@ class TestCall(unittest.TestCase):\n         \"\"\"Test keras.layers.core.Layer.__call__\"\"\"\n         nb_samples, input_dim, output_dim = 3, 10, 5\n         layer = Dense(output_dim, input_dim=input_dim)\n-        W = np.asarray(K.eval(layer.W))\n+        W = np.asarray(K.eval(layer.W)).astype(K.floatx())\n         X = K.placeholder(ndim=2)\n         Y = layer(X)\n         F = K.function([X], [Y])\n \n-        x = np.random.randn(nb_samples, input_dim).astype(K.floatx())\n+        x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n         y = F([x])[0].astype(K.floatx())\n-        assert_allclose(np.dot(x, W), y)\n+        t = np.dot(x, W).astype(K.floatx())\n+        assert_allclose(t, y, rtol=.2)\n \n     def test_sequential_call(self):\n         \"\"\"Test keras.models.Sequential.__call__\"\"\"\n@@ -40,7 +41,7 @@ class TestCall(unittest.TestCase):\n         Y = model(X)\n         F = K.function([X], [Y])\n \n-        x = np.random.randn(nb_samples, input_dim).astype(K.floatx())\n+        x = np.ones((nb_samples, input_dim)).astype(K.floatx())\n         y1 = F([x])[0].astype(K.floatx())\n         y2 = model.predict(x)\n         # results of __call__ should match model.predict\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7b7fae6542ac27799b71916955bc0100b6bd9f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 263 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -111,11 +111,11 @@ else:\n         progbar = generic_utils.Progbar(X_train.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n             loss = model.train_on_batch(X_batch, Y_batch)\n-            progbar.add(X_batch.shape[0], values=[(\"train loss\", loss)])\n+            progbar.add(X_batch.shape[0], values=[(\"train loss\", loss[0])])\n \n         print(\"Testing...\")\n         # test time!\n         progbar = generic_utils.Progbar(X_test.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n             score = model.test_on_batch(X_batch, Y_batch)\n-            progbar.add(X_batch.shape[0], values=[(\"test loss\", score)])\n+            progbar.add(X_batch.shape[0], values=[(\"test loss\", score[0])])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#14278151373a35f7ad805cbcc3eed2ffb4e995e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 191 | Lines Deleted: 227 | Files Changed: 11 | Hunks: 145 | Methods Changed: 106 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 418 | Churn Cumulative: 2372 | Contributors (this commit): 2 | Commits (past 90d): 31 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -224,9 +224,9 @@ class TestBackend(unittest.TestCase):\n         assert len(new_states) == 1\n         tf_state = KTF.eval(new_states[0])\n \n-        assert_allclose(tf_last_output, th_last_output, atol=1e-05)\n-        assert_allclose(tf_outputs, th_outputs, atol=1e-05)\n-        assert_allclose(tf_state, th_state, atol=1e-05)\n+        assert_allclose(tf_last_output, th_last_output, atol=1e-04)\n+        assert_allclose(tf_outputs, th_outputs, atol=1e-04)\n+        assert_allclose(tf_state, th_state, atol=1e-04)\n \n     def test_switch(self):\n         val = np.random.random()\n\n@@ -1,48 +1,25 @@\n from __future__ import print_function\n-import unittest\n+import pytest\n from keras.datasets import cifar10, cifar100, reuters, imdb, mnist\n \n \n-class TestDatasets(unittest.TestCase):\n-    def test_cifar(self):\n-        print('cifar10')\n+def test_cifar():\n     (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n-\n-        print('cifar100 fine')\n     (X_train, y_train), (X_test, y_test) = cifar100.load_data('fine')\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n-\n-        print('cifar100 coarse')\n     (X_train, y_train), (X_test, y_test) = cifar100.load_data('coarse')\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n \n-    def test_reuters(self):\n-        print('reuters')\n+\n+def test_reuters():\n     (X_train, y_train), (X_test, y_test) = reuters.load_data()\n \n-    def test_mnist(self):\n-        print('mnist')\n-        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n \n-    def test_imdb(self):\n-        print('imdb')\n+def test_mnist():\n+    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+\n+def test_imdb():\n     (X_train, y_train), (X_test, y_test) = imdb.load_data()\n \n \n if __name__ == '__main__':\n-    print('Test datasets')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,5 +1,6 @@\n-import unittest\n+import pytest\n import numpy as np\n+from numpy.testing import assert_allclose\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation, Flatten\n from keras.layers.embeddings import Embedding\n@@ -7,14 +8,13 @@ from keras.constraints import unitnorm\n from keras import backend as K\n \n \n-class TestEmbedding(unittest.TestCase):\n-    def setUp(self):\n-        self.X1 = np.array([[1], [2]], dtype='int32')\n-        self.W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n+X1 = np.array([[1], [2]], dtype='int32')\n+W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n \n-    def test_unitnorm_constraint(self):\n+\n+def test_unitnorm_constraint():\n     lookup = Sequential()\n-        lookup.add(Embedding(3, 2, weights=[self.W1],\n+    lookup.add(Embedding(3, 2, weights=[W1],\n                          W_constraint=unitnorm(),\n                          input_length=1))\n     lookup.add(Flatten())\n@@ -22,9 +22,10 @@ class TestEmbedding(unittest.TestCase):\n     lookup.add(Activation('sigmoid'))\n     lookup.compile(loss='binary_crossentropy', optimizer='sgd',\n                    class_mode='binary')\n-        lookup.train_on_batch(self.X1, np.array([[1], [0]], dtype='int32'))\n+    lookup.train_on_batch(X1, np.array([[1], [0]], dtype='int32'))\n     norm = np.linalg.norm(K.get_value(lookup.params[0]), axis=1)\n-        self.assertTrue(np.allclose(norm, np.ones_like(norm).astype('float32')))\n+    assert_allclose(norm, np.ones_like(norm).astype('float32'))\n+\n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n-import unittest\n+import pytest\n import numpy as np\n+import os\n np.random.seed(1337)\n \n from keras.models import Graph, Sequential\n@@ -25,9 +26,10 @@ y2 = np.random.random((100,))\n                                                          output_shape=(1,))\n \n \n-class TestGraph(unittest.TestCase):\n-    def test_1o_1i(self):\n-        print('test a non-sequential graph with 1 input and 1 output')\n+def test_1o_1i():\n+    # test a non-sequential graph with 1 input and 1 output\n+    np.random.seed(1337)\n+\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n \n@@ -48,7 +50,6 @@ class TestGraph(unittest.TestCase):\n     loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n     loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n     loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-        print(loss)\n     assert(loss < 2.5)\n \n     # test validation split\n@@ -59,8 +60,9 @@ class TestGraph(unittest.TestCase):\n                         validation_data={'input1': X_train, 'output1': y_train},\n                         nb_epoch=1)\n \n-    def test_1o_1i_2(self):\n-        print('test a more complex non-sequential graph with 1 input and 1 output')\n+\n+def test_1o_1i_2():\n+    # test a more complex non-sequential graph with 1 input and 1 output\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n \n@@ -81,15 +83,17 @@ class TestGraph(unittest.TestCase):\n     out = graph.predict({'input1': X_train})\n     assert(type(out == dict))\n     assert(len(out) == 1)\n+\n     loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n     loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n     loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-        print(loss)\n     assert(loss < 2.5)\n+\n     graph.get_config(verbose=1)\n \n-    def test_1o_2i(self):\n-        print('test a non-sequential graph with 2 inputs and 1 output')\n+\n+def test_1o_2i():\n+    # test a non-sequential graph with 2 inputs and 1 output\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n     graph.add_input(name='input2', input_shape=(32,))\n@@ -107,15 +111,17 @@ class TestGraph(unittest.TestCase):\n     out = graph.predict({'input1': X_test, 'input2': X2_test})\n     assert(type(out == dict))\n     assert(len(out) == 1)\n+\n     loss = graph.test_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n     loss = graph.train_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n     loss = graph.evaluate({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-        print(loss)\n     assert(loss < 3.0)\n+\n     graph.get_config(verbose=1)\n \n-    def test_2o_1i_weights(self):\n-        print('test a non-sequential graph with 1 input and 2 outputs')\n+\n+def test_2o_1i_weights():\n+    # test a non-sequential graph with 1 input and 2 outputs\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n \n@@ -135,11 +141,12 @@ class TestGraph(unittest.TestCase):\n     loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n     loss = graph.train_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n     loss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-        print(loss)\n     assert(loss < 4.)\n \n-        print('test weight saving')\n-        graph.save_weights('test_2o_1i_weights_temp.h5', overwrite=True)\n+    # test weight saving\n+    fname = 'test_2o_1i_weights_temp.h5'\n+    graph.save_weights(fname, overwrite=True)\n+\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n     graph.add_node(Dense(16), name='dense1', input='input1')\n@@ -149,12 +156,14 @@ class TestGraph(unittest.TestCase):\n     graph.add_output(name='output2', input='dense3')\n     graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n     graph.load_weights('test_2o_1i_weights_temp.h5')\n+    os.remove(fname)\n+\n     nloss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-        print(nloss)\n     assert(loss == nloss)\n \n-    def test_2o_1i_sample_weights(self):\n-        print('test a non-sequential graph with 1 input and 2 outputs with sample weights')\n+\n+def test_2o_1i_sample_weights():\n+    # test a non-sequential graph with 1 input and 2 outputs with sample weights\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n \n@@ -184,10 +193,10 @@ class TestGraph(unittest.TestCase):\n                                 sample_weight={'output1': weights1, 'output2': weights2})\n     loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n                           sample_weight={'output1': weights1, 'output2': weights2})\n-        print(loss)\n \n-    def test_recursive(self):\n-        print('test layer-like API')\n+\n+def test_recursive():\n+    # test layer-like API\n \n     graph = containers.Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n@@ -206,15 +215,15 @@ class TestGraph(unittest.TestCase):\n \n     history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=10)\n     loss = seq.evaluate(X_test, y_test)\n-        print(loss)\n     assert(loss < 2.5)\n \n     loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n     pred = seq.predict(X_test)\n     seq.get_config(verbose=1)\n \n-    def test_create_output(self):\n-        print('test create_output argument')\n+\n+def test_create_output():\n+    # test create_output argument\n     graph = Graph()\n     graph.add_input(name='input1', input_shape=(32,))\n \n@@ -230,14 +239,15 @@ class TestGraph(unittest.TestCase):\n     out = graph.predict({'input1': X_test})\n     assert(type(out == dict))\n     assert(len(out) == 1)\n+\n     loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n     loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n     loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-        print(loss)\n     assert(loss < 2.5)\n \n-    def test_count_params(self):\n-        print('test count params')\n+\n+def test_count_params():\n+    # test count params\n \n     nb_units = 100\n     nb_classes = 2\n@@ -258,13 +268,12 @@ class TestGraph(unittest.TestCase):\n     n += 32 * nb_classes + nb_classes\n     n += nb_units * nb_classes + nb_classes\n \n-        self.assertEqual(n, graph.count_params())\n+    assert(n == graph.count_params())\n \n     graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n \n-        self.assertEqual(n, graph.count_params())\n+    assert(n == graph.count_params())\n \n \n if __name__ == '__main__':\n-    print('Test graph model')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,18 +1,18 @@\n import numpy as np\n+import pytest\n np.random.seed(1337)\n \n-import unittest\n from keras.models import Sequential, weighted_objective\n from keras.layers.core import TimeDistributedDense, Masking\n from keras import objectives\n from keras import backend as K\n \n \n-class TestMasking(unittest.TestCase):\n-    def test_masking(self):\n+def test_masking():\n     if K._BACKEND == 'tensorflow':\n         # skip this test for TF: not supported yet\n         return\n+\n     X = np.array(\n         [[[1, 1], [2, 1], [3, 1], [5, 5]],\n          [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n@@ -24,7 +24,8 @@ class TestMasking(unittest.TestCase):\n     history = model.fit(X, 4 * y, nb_epoch=1, batch_size=2, verbose=1)\n     assert history.history['loss'][0] == 285.\n \n-    def test_loss_masking(self):\n+\n+def test_loss_masking():\n     weighted_loss = weighted_objective(objectives.get('mae'))\n     shape = (3, 4, 2)\n     X = np.arange(24).reshape(shape)\n@@ -39,9 +40,7 @@ class TestMasking(unittest.TestCase):\n                                K.variable(Y),\n                                K.variable(weights),\n                                K.variable(mask)))\n-        print(out)\n \n \n if __name__ == '__main__':\n-    print('Test masking')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,18 +1,17 @@\n from __future__ import absolute_import\n from __future__ import print_function\n-\n+import pytest\n import numpy as np\n-np.random.seed(1336)  # for reproducibility\n+np.random.seed(1337)\n \n from keras.datasets import mnist\n from keras.models import Sequential, Graph\n from keras.layers.core import Dense, Activation\n from keras.utils import np_utils\n-import unittest\n \n nb_classes = 10\n batch_size = 128\n-nb_epoch = 10\n+nb_epoch = 15\n weighted_class = 9\n standard_weight = 1\n high_weight = 5\n@@ -110,11 +109,8 @@ def _test_weights_graph(model, class_weight=None, sample_weight=None):\n     return score\n \n \n-class TestLossWeighting(unittest.TestCase):\n-    def test_sequential(self):\n+def test_sequential():\n     for loss in ['mae', 'mse']:\n-            print('loss:', loss)\n-            print('sequential')\n         # no weights: reference point\n         model = create_sequential_model()\n         model.compile(loss=loss, optimizer='rmsprop')\n@@ -123,19 +119,16 @@ class TestLossWeighting(unittest.TestCase):\n         model = create_sequential_model()\n         model.compile(loss=loss, optimizer='rmsprop')\n         score = _test_weights_sequential(model, class_weight=class_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n+        assert(score < standard_score)\n         # test sample_weight\n         model = create_sequential_model()\n         model.compile(loss=loss, optimizer='rmsprop')\n         score = _test_weights_sequential(model, sample_weight=sample_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n+        assert(score < standard_score)\n \n-    def test_graph(self):\n+\n+def test_graph():\n     for loss in ['mae', 'mse']:\n-            print('loss:', loss)\n-            print('graph')\n         # no weights: reference point\n         model = create_graph_model()\n         model.compile(loss={'output': loss}, optimizer='rmsprop')\n@@ -144,16 +137,13 @@ class TestLossWeighting(unittest.TestCase):\n         model = create_graph_model()\n         model.compile(loss={'output': loss}, optimizer='rmsprop')\n         score = _test_weights_graph(model, class_weight=class_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n+        assert(score < standard_score)\n         # test sample_weight\n         model = create_graph_model()\n         model.compile(loss={'output': loss}, optimizer='rmsprop')\n         score = _test_weights_graph(model, sample_weight=sample_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n+        assert(score < standard_score)\n \n \n if __name__ == '__main__':\n-    print('Test class_weight and sample_weight')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,17 +1,18 @@\n from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)\n+import pytest\n \n from keras.utils.test_utils import get_test_data\n from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.utils.np_utils import to_categorical\n-import unittest\n \n \n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,),\n-                                                     classification=True, nb_class=2)\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n+                                                     nb_test=200,\n+                                                     input_shape=(10,),\n+                                                     classification=True,\n+                                                     nb_class=2)\n y_train = to_categorical(y_train)\n y_test = to_categorical(y_test)\n \n@@ -28,32 +29,32 @@ def get_model(input_dim, nb_hidden, output_dim):\n def _test_optimizer(optimizer, target=0.9):\n     model = get_model(X_train.shape[1], 10, y_train.shape[1])\n     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n-    history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n+    history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+                        validation_data=(X_test, y_test),\n+                        show_accuracy=True, verbose=2)\n     return history.history['val_acc'][-1] > target\n \n \n-class TestOptimizers(unittest.TestCase):\n-    def test_sgd(self):\n-        print('test SGD')\n+def test_sgd():\n     sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n-        self.assertTrue(_test_optimizer(sgd))\n+    assert(_test_optimizer(sgd))\n \n-    def test_rmsprop(self):\n-        print('test RMSprop')\n-        self.assertTrue(_test_optimizer(RMSprop()))\n \n-    def test_adagrad(self):\n-        print('test Adagrad')\n-        self.assertTrue(_test_optimizer(Adagrad()))\n+def test_rmsprop():\n+    assert(_test_optimizer(RMSprop()))\n \n-    def test_adadelta(self):\n-        print('test Adadelta')\n-        self.assertTrue(_test_optimizer(Adadelta()))\n \n-    def test_adam(self):\n-        print('test Adam')\n-        self.assertTrue(_test_optimizer(Adam()))\n+def test_adagrad():\n+    assert(_test_optimizer(Adagrad()))\n+\n+\n+def test_adadelta():\n+    assert(_test_optimizer(Adadelta()))\n+\n+\n+def test_adam():\n+    assert(_test_optimizer(Adam()))\n+\n \n if __name__ == '__main__':\n-    print('Test optimizers')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,4 +1,4 @@\n-import unittest\n+import pytest\n import numpy as np\n np.random.seed(1337) # for reproducibility\n \n@@ -37,26 +37,29 @@ def create_model(weight_reg=None, activity_reg=None):\n     model = Sequential()\n     model.add(Dense(50, input_shape=(784,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(10, W_regularizer=weight_reg, activity_regularizer=activity_reg))\n+    model.add(Dense(10, W_regularizer=weight_reg,\n+                    activity_regularizer=activity_reg))\n     model.add(Activation('softmax'))\n     return model\n \n \n-class TestRegularizers(unittest.TestCase):\n-    def test_W_reg(self):\n+def test_W_reg():\n     for reg in [regularizers.identity(), regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n         model = create_model(weight_reg=reg)\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch, verbose=0)\n         model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n \n-    def test_A_reg(self):\n+\n+def test_A_reg():\n     for reg in [regularizers.activity_l1(), regularizers.activity_l2()]:\n         model = create_model(activity_reg=reg)\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit(X_train, Y_train, batch_size=batch_size,\n+                  nb_epoch=nb_epoch, verbose=0)\n         model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n \n+\n if __name__ == '__main__':\n-    print('Test weight and activity regularizers')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,6 +1,6 @@\n from __future__ import absolute_import\n from __future__ import print_function\n-import unittest\n+import pytest\n import numpy as np\n np.random.seed(1337)\n \n@@ -9,8 +9,6 @@ from keras.models import Sequential, model_from_json, model_from_yaml\n from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n-import pickle\n-import sys\n import os\n \n input_dim = 32\n@@ -29,13 +27,9 @@ test_samples = 1000\n                                                      nb_class=4)\n y_test = np_utils.to_categorical(y_test)\n y_train = np_utils.to_categorical(y_train)\n-print(X_train.shape)\n-print(y_train.shape)\n \n \n-class TestSequential(unittest.TestCase):\n-    def test_sequential(self):\n-        print('Test sequential')\n+def test_sequential():\n     model = Sequential()\n     model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n@@ -53,15 +47,13 @@ class TestSequential(unittest.TestCase):\n     model.train_on_batch(X_train[:32], y_train[:32])\n \n     loss = model.evaluate(X_train, y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n+    assert(loss < 0.7)\n+\n     model.predict(X_test, verbose=0)\n     model.predict_classes(X_test, verbose=0)\n     model.predict_proba(X_test, verbose=0)\n     model.get_config(verbose=0)\n \n-        print('test weight saving')\n     fname = 'test_sequential_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     model = Sequential()\n@@ -84,8 +76,8 @@ class TestSequential(unittest.TestCase):\n     yaml_data = model.to_yaml()\n     model = model_from_yaml(yaml_data)\n \n-    def test_merge_sum(self):\n-        print('Test merge: sum')\n+\n+def test_merge_sum():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -108,15 +100,14 @@ class TestSequential(unittest.TestCase):\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n+    assert(loss < 0.7)\n+\n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n     model.predict_proba([X_test, X_test], verbose=0)\n     model.get_config(verbose=0)\n \n-        print('test weight saving')\n+    # test weight saving\n     fname = 'test_merge_sum_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     left = Sequential()\n@@ -134,14 +125,13 @@ class TestSequential(unittest.TestCase):\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n     nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print(nloss)\n     assert(loss == nloss)\n \n-    def test_merge_dot1(self):\n+\n+def test_merge_dot():\n     if K._BACKEND == 'tensorflow':\n         return\n \n-        print('Test merge: dot')\n     left = Sequential()\n     left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n     left.add(Activation('relu'))\n@@ -157,11 +147,6 @@ class TestSequential(unittest.TestCase):\n \n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    def test_merge_dot2(self):\n-        if K._BACKEND == 'tensorflow':\n-            return\n-\n-        print('Test merge: dot')\n     left = Sequential()\n     left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n     left.add(Activation('relu'))\n@@ -177,8 +162,8 @@ class TestSequential(unittest.TestCase):\n \n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    def test_merge_concat(self):\n-        print('Test merge: concat')\n+\n+def test_merge_concat():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -201,15 +186,13 @@ class TestSequential(unittest.TestCase):\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n+    assert(loss < 0.7)\n+\n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n     model.predict_proba([X_test, X_test], verbose=0)\n     model.get_config(verbose=0)\n \n-        print('test weight saving')\n     fname = 'test_merge_concat_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     left = Sequential()\n@@ -233,9 +216,8 @@ class TestSequential(unittest.TestCase):\n     nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n     assert(loss == nloss)\n \n-    def test_merge_recursivity(self):\n-        print('Test merge recursivity')\n \n+def test_merge_recursivity():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -267,9 +249,8 @@ class TestSequential(unittest.TestCase):\n     model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n+    assert(loss < 0.7)\n+\n     model.predict([X_test, X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test, X_test], verbose=0)\n     model.predict_proba([X_test, X_test, X_test], verbose=0)\n@@ -281,11 +262,10 @@ class TestSequential(unittest.TestCase):\n     os.remove(fname)\n \n     nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n-        print(nloss)\n     assert(loss == nloss)\n \n-    def test_merge_overlap(self):\n-        print('Test merge overlap')\n+\n+def test_merge_overlap():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -306,9 +286,7 @@ class TestSequential(unittest.TestCase):\n     model.train_on_batch(X_train[:32], y_train[:32])\n \n     loss = model.evaluate(X_train, y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n+    assert(loss < 0.7)\n     model.predict(X_test, verbose=0)\n     model.predict_classes(X_test, verbose=0)\n     model.predict_proba(X_test, verbose=0)\n@@ -320,12 +298,10 @@ class TestSequential(unittest.TestCase):\n     os.remove(fname)\n \n     nloss = model.evaluate(X_train, y_train, verbose=0)\n-        print(nloss)\n     assert(loss == nloss)\n \n-    def test_lambda(self):\n-        print('Test lambda: sum')\n \n+def test_lambda():\n     def func(X):\n         s = X[0]\n         for i in range(1, len(X)):\n@@ -361,15 +337,14 @@ class TestSequential(unittest.TestCase):\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n+    assert(loss < 0.7)\n+\n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n     model.predict_proba([X_test, X_test], verbose=0)\n     model.get_config(verbose=0)\n \n-        print('test weight saving')\n+    # test weight saving\n     fname = 'test_lambda_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     left = Sequential()\n@@ -390,8 +365,8 @@ class TestSequential(unittest.TestCase):\n     nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n     assert(loss == nloss)\n \n-    def test_count_params(self):\n-        print('test count params')\n+\n+def test_count_params():\n     input_dim = 20\n     nb_units = 10\n     nb_classes = 2\n@@ -406,13 +381,12 @@ class TestSequential(unittest.TestCase):\n     model.add(Dense(nb_classes))\n     model.add(Activation('softmax'))\n \n-        self.assertEqual(n, model.count_params())\n+    assert(n == model.count_params())\n \n     model.compile('sgd', 'binary_crossentropy')\n \n-        self.assertEqual(n, model.count_params())\n+    assert(n == model.count_params())\n \n \n if __name__ == '__main__':\n-    print('Test Sequential model')\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,4 +1,4 @@\n-import unittest\n+import pytest\n import numpy as np\n \n from keras import backend as K\n@@ -15,39 +15,43 @@ def check_layer_output_shape(layer, input_data):\n \n     function = K.function([layer.input], [layer.get_output()])\n     output = function([input_data])[0]\n-    assert output.shape[1:] == expected_output_shape, str(output.shape[1:]) + ' != ' + str(expected_output_shape)\n+    assert output.shape[1:] == expected_output_shape\n \n \n-class TestShapeInference(unittest.TestCase):\n-    # ########\n-    # # Core #\n-    # ########\n-    def test_Reshape(self):\n+########\n+# Core #\n+########\n+def test_Reshape():\n     layer = Reshape(dims=(2, 3))\n     input_data = np.random.random((2, 6))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_Permute(self):\n+\n+def test_Permute():\n     layer = Permute(dims=(1, 3, 2))\n     input_data = np.random.random((2, 2, 4, 3))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_Flatten(self):\n+\n+def test_Flatten():\n     layer = Flatten()\n     input_data = np.random.random((2, 2, 3))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_RepeatVector(self):\n+\n+def test_RepeatVector():\n     layer = RepeatVector(2)\n     input_data = np.random.random((2, 2))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_Dense(self):\n+\n+def test_Dense():\n     layer = Dense(3)\n     input_data = np.random.random((2, 2))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_TimeDistributedDense(self):\n+\n+def test_TimeDistributedDense():\n     layer = TimeDistributedDense(2)\n     input_data = np.random.random((2, 2, 3))\n     check_layer_output_shape(layer, input_data)\n@@ -55,7 +59,7 @@ class TestShapeInference(unittest.TestCase):\n #################\n # Convolutional #\n #################\n-    def test_Convolution1D(self):\n+def test_Convolution1D():\n     for border_mode in ['same', 'valid']:\n         for filter_length in [2, 3]:\n             for subsample_length in [1]:\n@@ -69,7 +73,8 @@ class TestShapeInference(unittest.TestCase):\n                     input_data = np.random.random(input_data_shape)\n                     check_layer_output_shape(layer, input_data)\n \n-    def test_Convolution2D(self):\n+\n+def test_Convolution2D():\n     for border_mode in ['same', 'valid']:\n         for nb_row, nb_col in [(2, 2), (3, 3)]:\n             for subsample in [(1, 1), (2, 2)]:\n@@ -93,7 +98,8 @@ class TestShapeInference(unittest.TestCase):\n                     input_data = np.random.random(input_data_shape)\n                     check_layer_output_shape(layer, input_data)\n \n-    def test_MaxPooling1D(self):\n+\n+def test_MaxPooling1D():\n     for ignore_border in [True, False]:\n         for pool_length in [1, 2]:\n             for stride in [1]:\n@@ -104,7 +110,8 @@ class TestShapeInference(unittest.TestCase):\n                     input_data = np.random.random(input_data_shape)\n                     check_layer_output_shape(layer, input_data)\n \n-    def test_MaxPooling2D(self):\n+\n+def test_MaxPooling2D():\n     for ignore_border in [True, False]:\n         for strides in [(1, 1), (2, 2)]:\n             for pool_size in [(2, 2), (3, 3), (4, 4)]:\n@@ -124,12 +131,14 @@ class TestShapeInference(unittest.TestCase):\n                     input_data = np.random.random(input_data_shape)\n                     check_layer_output_shape(layer, input_data)\n \n-    def test_UpSampling1D(self):\n+\n+def test_UpSampling1D():\n     layer = UpSampling1D(length=2)\n     input_data = np.random.random((2, 2, 3))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_UpSampling2D(self):\n+\n+def test_UpSampling2D():\n     layer = UpSampling2D(size=(2, 2), dim_ordering='th')\n     input_data = np.random.random((2, 1, 2, 3))\n     check_layer_output_shape(layer, input_data)\n@@ -138,12 +147,14 @@ class TestShapeInference(unittest.TestCase):\n     input_data = np.random.random((2, 2, 3, 1))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_ZeroPadding1D(self):\n+\n+def test_ZeroPadding1D():\n     layer = ZeroPadding1D(1)\n     input_data = np.random.random((2, 2, 1))\n     check_layer_output_shape(layer, input_data)\n \n-    def test_ZeroPadding2D(self):\n+\n+def test_ZeroPadding2D():\n     layer = ZeroPadding2D((1, 2), dim_ordering='th')\n     input_data = np.random.random((2, 1, 2, 3))\n     check_layer_output_shape(layer, input_data)\n@@ -155,7 +166,7 @@ class TestShapeInference(unittest.TestCase):\n # #############\n # # Recurrent #\n # #############\n-    def test_SimpleRNN(self):\n+def test_SimpleRNN():\n     # all recurrent layers inherit output_shape\n     # from the same base recurrent layer\n     layer = SimpleRNN(2)\n@@ -164,4 +175,4 @@ class TestShapeInference(unittest.TestCase):\n \n \n if __name__ == \"__main__\":\n-    unittest.main()\n+    pytest.main([__file__])\n\n@@ -1,17 +1,17 @@\n from __future__ import print_function\n import numpy as np\n import pytest\n+np.random.seed(1337)\n \n from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, TimeDistributedDense, Flatten\n+from keras.layers.core import Dense, TimeDistributedDense, Flatten\n from keras.layers.recurrent import GRU\n from keras.layers.convolutional import Convolution2D\n from keras.utils.np_utils import to_categorical\n \n \n def test_vector_classification():\n-    np.random.seed(1337)\n     nb_hidden = 10\n \n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n@@ -34,7 +34,6 @@ def test_vector_classification():\n \n \n def test_vector_regression():\n-    np.random.seed(1337)\n     nb_hidden = 10\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n@@ -54,7 +53,6 @@ def test_vector_regression():\n \n \n def test_temporal_classification():\n-    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 5),\n@@ -64,7 +62,9 @@ def test_temporal_classification():\n     y_test = to_categorical(y_test)\n \n     model = Sequential()\n-    model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2]), activation='softmax'))\n+    model.add(GRU(y_train.shape[-1],\n+                  input_shape=(X_train.shape[1], X_train.shape[2]),\n+                  activation='softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test),\n@@ -73,14 +73,14 @@ def test_temporal_classification():\n \n \n def test_temporal_regression():\n-    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 5),\n                                                          output_shape=(2,),\n                                                          classification=False)\n     model = Sequential()\n-    model.add(GRU(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n+    model.add(GRU(y_train.shape[-1],\n+              input_shape=(X_train.shape[1], X_train.shape[2])))\n     model.compile(loss='hinge', optimizer='adam')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=0)\n@@ -88,7 +88,6 @@ def test_temporal_regression():\n \n \n def test_sequence_to_sequence():\n-    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 5),\n@@ -96,7 +95,8 @@ def test_sequence_to_sequence():\n                                                          classification=False)\n \n     model = Sequential()\n-    model.add(TimeDistributedDense(y_train.shape[-1], input_shape=(X_train.shape[1], X_train.shape[2])))\n+    model.add(TimeDistributedDense(y_train.shape[-1],\n+              input_shape=(X_train.shape[1], X_train.shape[2])))\n     model.compile(loss='hinge', optimizer='rmsprop')\n     history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=0)\n@@ -104,7 +104,6 @@ def test_sequence_to_sequence():\n \n \n def test_image_classification():\n-    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n                                                          nb_test=200,\n                                                          input_shape=(3, 8, 8),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
