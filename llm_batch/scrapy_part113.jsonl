{"custom_id": "scrapy#ad439173228ae8780b26de43fffc3a0e6e9a7a57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 257 | Lines Deleted: 97 | Files Changed: 2 | Hunks: 37 | Methods Changed: 44 | Complexity Δ (Sum/Max): 25/24 | Churn Δ: 354 | Churn Cumulative: 699 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n@@ -8,20 +9,22 @@ from scrapy.utils.misc import arg_to_iter\n \n class MediaPipeline(object):\n \n-    DOWNLOAD_PRIORITY = 1000\n     LOG_FAILED_RESULTS = True\n \n     class SpiderInfo(object):\n         def __init__(self, spider):\n             self.spider = spider\n-            self.downloading = {}\n+            self.downloading = set()\n             self.downloaded = {}\n-            self.waiting = {}\n+            self.waiting = defaultdict(list)\n \n-    def __init__(self):\n+    def __init__(self, download_func=None):\n         self.spiderinfo = {}\n+        self._download_func = download_func or self._default_download_func()\n+\n+    def _default_download_func(self):\n         from scrapy.project import crawler\n-        self.crawler = crawler\n+        return crawler.engine.download\n \n     def open_spider(self, spider):\n         self.spiderinfo[spider] = self.SpiderInfo(spider)\n@@ -32,63 +35,53 @@ class MediaPipeline(object):\n     def process_item(self, item, spider):\n         info = self.spiderinfo[spider]\n         requests = arg_to_iter(self.get_media_requests(item, info))\n-        dlist = [self._enqueue(r, info) for r in requests]\n+        dlist = [self._process_request(r, info) for r in requests]\n         dfd = DeferredList(dlist, consumeErrors=1)\n         return dfd.addCallback(self.item_completed, item, info)\n \n-    def _enqueue(self, request, info):\n+    def _process_request(self, request, info):\n         fp = request_fingerprint(request)\n         cb = request.callback or (lambda _: _)\n         eb = request.errback\n \n-        # if already downloaded, return cached result.\n+        # Return cached result if request was already seen\n         if fp in info.downloaded:\n             return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n \n+        # Otherwise, wait for result\n         wad = Deferred().addCallbacks(cb, eb)\n-        # add to pending list for this request, and wait for result like the others.\n-        info.waiting.setdefault(fp, []).append(wad)\n-\n-        # if request is not downloading, download it.\n-        if fp not in info.downloading:\n-            self._download(request, info, fp)\n+        info.waiting[fp].append(wad)\n \n+        # Check if request is downloading right now to avoid doing it twice\n+        if fp in info.downloading:\n             return wad\n \n-    def _download(self, request, info, fp):\n-        def _downloaded(result):\n-            info.downloading.pop(fp)\n-            info.downloaded[fp] = result\n-            for wad in info.waiting.pop(fp): # pass result to each waiting client\n-                defer_result(result).chainDeferred(wad)\n-\n-        def _post_media_to_download(result):\n-            if result is None: # continue with download\n-                dwld = mustbe_deferred(self.download, request, info)\n-                dwld.addCallbacks(\n-                        callback=self.media_downloaded,\n-                        callbackArgs=(request, info),\n-                        errback=self.media_failed,\n-                        errbackArgs=(request, info))\n-            else: # or use media_to_download return value as result\n-                dwld = defer_result(result)\n-\n-            info.downloading[fp] = (request, dwld) # fill downloading state data\n-            dwld.addBoth(_downloaded) # append post-download hook\n-            dwld.addErrback(log.err, spider=info.spider)\n-\n-        # declare request in downloading state (None is used as place holder)\n-        info.downloading[fp] = None\n-\n-        # defer pre-download request processing\n+        # Download request checking media_to_download hook output first\n+        info.downloading.add(fp)\n         dfd = mustbe_deferred(self.media_to_download, request, info)\n-        dfd.addCallback(_post_media_to_download)\n+        dfd.addCallback(self._check_media_to_download, request, info)\n+        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n+        dfd.addErrback(log.err, spider=info.spider)\n+        return dfd.addBoth(lambda _: wad) # it must return wad at last\n+\n+    def _check_media_to_download(self, result, request, info):\n+        if result is not None:\n+            return result\n+        # Download request and process its response\n+        return mustbe_deferred(self.download, request, info).addCallbacks(\n+                callback=self.media_downloaded, callbackArgs=(request, info),\n+                errback=self.media_failed, errbackArgs=(request, info))\n+\n+    def _cache_result_and_execute_waiters(self, result, fp, info):\n+        info.downloading.remove(fp)\n+        info.downloaded[fp] = result # cache result\n+        for wad in info.waiting.pop(fp):\n+            defer_result(result).chainDeferred(wad)\n \n     ### Overradiable Interface\n     def download(self, request, info):\n         \"\"\"Defines how to download the media request\"\"\"\n-        request.priority = self.DOWNLOAD_PRIORITY\n-        return self.crawler.engine.download(request, info.spider)\n+        return self._download_func(request, info.spider)\n \n     def media_to_download(self, request, info):\n         \"\"\"Check request before starting download\"\"\"\n@@ -109,8 +102,8 @@ class MediaPipeline(object):\n     def item_completed(self, results, item, info):\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n         if self.LOG_FAILED_RESULTS:\n-            for success, result in results:\n-                if not success:\n-                    log.err(result, '%s found errors proessing %s' % (self.__class__.__name__, item))\n+            msg = '%s found errors proessing %s' % (self.__class__.__name__, item)\n+            for ok, value in results:\n+                if not ok:\n+                    log.err(value, msg, spider=info.spider)\n         return item\n-\n\n@@ -1,85 +1,252 @@\n from twisted.trial import unittest\n-from twisted.python import failure\n-from twisted.internet import defer, reactor\n+from twisted.python.failure import Failure\n+from twisted.internet import reactor\n+from twisted.internet.defer import Deferred, inlineCallbacks\n+from twisted.python import log as txlog\n \n-from scrapy.settings import Settings\n-from scrapy.crawler import Crawler\n from scrapy.http import Request, Response\n from scrapy.spider import BaseSpider\n from scrapy.utils.request import request_fingerprint\n from scrapy.contrib.pipeline.media import MediaPipeline\n+from scrapy.utils.test import get_crawler\n+from scrapy import log\n \n \n-class _MockedMediaPipeline(MediaPipeline):\n-\n-    def download(self, request, info):\n-        delay = request.meta.get('delay')\n+def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n-        if delay is None:\n-            return response\n-        else:\n-            dfd = defer.Deferred()\n-            reactor.callLater(delay, dfd.callback, None)\n-            return dfd.addCallback(lambda _: response)\n-\n-    def get_media_requests(self, item, info):\n-        return item.get('requests')\n+    return response() if callable(response) else response\n \n \n-class MediaPipelineTestCase(unittest.TestCase):\n+class BaseMediaPipelineTestCase(unittest.TestCase):\n \n-    pipeline_class = _MockedMediaPipeline\n+    pipeline_class = MediaPipeline\n \n     def setUp(self):\n-        self.crawler = Crawler(Settings())\n-        self.crawler.install()\n         self.spider = BaseSpider('media.com')\n-        self.pipe = self.pipeline_class()\n+        self.pipe = self.pipeline_class(download_func=_mocked_download_func)\n         self.pipe.open_spider(self.spider)\n+        self.info = self.pipe.spiderinfo[self.spider]\n \n     def tearDown(self):\n         self.pipe.close_spider(self.spider)\n-        self.crawler.uninstall()\n \n-    @defer.inlineCallbacks\n-    def test_return_item_by_default(self):\n-        item = dict(name='sofa')\n+    def test_default_media_to_download(self):\n+        request = Request('url')\n+        assert self.pipe.media_to_download(request, self.info) is None\n+\n+    def test_default_get_media_requests(self):\n+        item = dict(name='name')\n+        assert self.pipe.get_media_requests(item, self.info) is None\n+\n+    def test_default_media_downloaded(self):\n+        request = Request('url')\n+        response = Response('url', body='')\n+        assert self.pipe.media_downloaded(response, request, self.info) is response\n+\n+    def test_default_media_failed(self):\n+        request = Request('url')\n+        fail = Failure(Exception())\n+        assert self.pipe.media_failed(fail, request, self.info) is fail\n+\n+    def test_default_item_completed(self):\n+        item = dict(name='name')\n+        assert self.pipe.item_completed([], item, self.info) is item\n+\n+        # Check that failures are logged by default\n+        fail = Failure(Exception())\n+        results = [(True, 1), (False, fail)]\n+\n+        events = []\n+        txlog.addObserver(events.append)\n+        new_item = self.pipe.item_completed(results, item, self.info)\n+        txlog.removeObserver(events.append)\n+        self.flushLoggedErrors()\n+\n+        assert new_item is item\n+        assert len(events) == 1\n+        assert events[0]['logLevel'] == log.ERROR\n+        assert events[0]['failure'] is fail\n+\n+        # disable failure logging and check again\n+        self.pipe.LOG_FAILED_RESULTS = False\n+        events = []\n+        txlog.addObserver(events.append)\n+        new_item = self.pipe.item_completed(results, item, self.info)\n+        txlog.removeObserver(events.append)\n+        self.flushLoggedErrors()\n+        assert new_item is item\n+        assert len(events) == 0\n+\n+    @inlineCallbacks\n+    def test_default_process_item(self):\n+        item = dict(name='name')\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n \n-    @defer.inlineCallbacks\n+    def test_default_download_func(self):\n+        crawler = get_crawler()\n+        crawler.install()\n+        crawler.configure()\n+        try:\n+            pipe = MediaPipeline()\n+            assert pipe._download_func == crawler.engine.download\n+        finally:\n+            crawler.uninstall()\n+\n+\n+class MockedMediaPipeline(MediaPipeline):\n+\n+    def __init__(self, *args, **kwargs):\n+        super(MockedMediaPipeline, self).__init__(*args, **kwargs)\n+        self._mockcalled = []\n+\n+    def download(self, request, info):\n+        self._mockcalled.append('download')\n+        return super(MockedMediaPipeline, self).download(request, info)\n+\n+    def media_to_download(self, request, info):\n+        self._mockcalled.append('media_to_download')\n+        return super(MockedMediaPipeline, self).media_to_download(request, info)\n+\n+    def get_media_requests(self, item, info):\n+        self._mockcalled.append('get_media_requests')\n+        return item.get('requests')\n+\n+    def media_downloaded(self, response, request, info):\n+        self._mockcalled.append('media_downloaded')\n+        return super(MockedMediaPipeline, self).media_downloaded(response, request, info)\n+\n+    def media_failed(self, failure, request, info):\n+        self._mockcalled.append('media_failed')\n+        return super(MockedMediaPipeline, self).media_failed(failure, request, info)\n+\n+    def item_completed(self, results, item, info):\n+        self._mockcalled.append('item_completed')\n+        item = super(MockedMediaPipeline, self).item_completed(results, item, info)\n+        item['results'] = results\n+        return item\n+\n+\n+class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n+\n+    pipeline_class = MockedMediaPipeline\n+\n+    @inlineCallbacks\n+    def test_result_succeed(self):\n+        cb = lambda _: self.pipe._mockcalled.append('request_callback') or _\n+        eb = lambda _: self.pipe._mockcalled.append('request_errback') or _\n+        rsp = Response('url1')\n+        req = Request('url1', meta=dict(response=rsp), callback=cb, errback=eb)\n+        item = dict(requests=req)\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertEqual(new_item['results'], [(True, rsp)])\n+        self.assertEqual(self.pipe._mockcalled,\n+                ['get_media_requests', 'media_to_download', 'download',\n+                    'media_downloaded', 'request_callback', 'item_completed'])\n+\n+    @inlineCallbacks\n+    def test_result_failure(self):\n+        self.pipe.LOG_FAILED_RESULTS = False\n+        cb = lambda _: self.pipe._mockcalled.append('request_callback') or _\n+        eb = lambda _: self.pipe._mockcalled.append('request_errback') or _\n+        fail = Failure(Exception())\n+        req = Request('url1', meta=dict(response=fail), callback=cb, errback=eb)\n+        item = dict(requests=req)\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertEqual(new_item['results'], [(False, fail)])\n+        self.assertEqual(self.pipe._mockcalled,\n+                ['get_media_requests', 'media_to_download', 'download',\n+                    'media_failed', 'request_errback', 'item_completed'])\n+\n+    @inlineCallbacks\n+    def test_mix_of_success_and_failure(self):\n+        self.pipe.LOG_FAILED_RESULTS = False\n+        rsp1 = Response('url1')\n+        req1 = Request('url1', meta=dict(response=rsp1))\n+        fail = Failure(Exception())\n+        req2 = Request('url2', meta=dict(response=fail))\n+        item = dict(requests=[req1, req2])\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertEqual(new_item['results'], [(True, rsp1), (False, fail)])\n+        m = self.pipe._mockcalled\n+        # only once\n+        self.assertEqual(m[0], 'get_media_requests') # first hook called\n+        self.assertEqual(m.count('get_media_requests'), 1)\n+        self.assertEqual(m.count('item_completed'), 1)\n+        self.assertEqual(m[-1], 'item_completed') # last hook called\n+        # twice, one per request\n+        self.assertEqual(m.count('media_to_download'), 2)\n+        self.assertEqual(m.count('download'), 2)\n+        # one to handle success and other for failure\n+        self.assertEqual(m.count('media_downloaded'), 1)\n+        self.assertEqual(m.count('media_failed'), 1)\n+\n+    @inlineCallbacks\n     def test_get_media_requests(self):\n         # returns single Request (without callback)\n-        info = self.pipe.spiderinfo[self.spider]\n-        req = Request('http://media.com/2.gif')\n+        req = Request('url')\n         item = dict(requests=req) # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n-        assert request_fingerprint(req) in info.downloaded\n+        assert request_fingerprint(req) in self.info.downloaded\n \n         # returns iterable of Requests\n-        req1 = Request('http://media.com/1.gif')\n-        req2 = Request('http://media.com/1.jpg')\n+        req1 = Request('url1')\n+        req2 = Request('url2')\n         item = dict(requests=iter([req1, req2]))\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n-        assert info.downloaded.get(request_fingerprint(req1)) is None\n-        assert info.downloaded.get(request_fingerprint(req2)) is None\n+        assert request_fingerprint(req1) in self.info.downloaded\n+        assert request_fingerprint(req2) in self.info.downloaded\n \n-    @defer.inlineCallbacks\n-    def test_requets_callback_is_called(self):\n-        collected = []\n-        response = Response('http://media.com/2.gif')\n-        request = Request('http://media.com/2.gif', meta=dict(response=response), callback=collected.append)\n-        item = dict(requests=request) # pass a single item\n-        yield self.pipe.process_item(item, self.spider)\n-        assert collected == [response]\n+    @inlineCallbacks\n+    def test_results_are_cached_across_multiple_items(self):\n+        rsp1 = Response('url1')\n+        req1 = Request('url1', meta=dict(response=rsp1))\n+        item = dict(requests=req1)\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertTrue(new_item is item)\n+        self.assertEqual(new_item['results'], [(True, rsp1)])\n \n-    @defer.inlineCallbacks\n-    def test_requets_errback_is_called(self):\n-        collected = []\n-        fail = failure.Failure(Exception())\n-        req = Request('http://media.com/2.gif', meta=dict(response=fail), callback=lambda _:_, errback=collected.append)\n-        item = dict(requests=req) # pass a single item\n-        yield self.pipe.process_item(item, self.spider)\n-        assert collected == [fail]\n+        # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n+        req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))\n+        item = dict(requests=req2)\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertTrue(new_item is item)\n+        self.assertEqual(request_fingerprint(req1), request_fingerprint(req2))\n+        self.assertEqual(new_item['results'], [(True, rsp1)])\n+\n+    @inlineCallbacks\n+    def test_results_are_cached_for_requests_of_single_item(self):\n+        rsp1 = Response('url1')\n+        req1 = Request('url1', meta=dict(response=rsp1))\n+        req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))\n+        item = dict(requests=[req1, req2])\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertTrue(new_item is item)\n+        self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])\n+\n+    @inlineCallbacks\n+    def test_wait_if_request_is_downloading(self):\n+        def _check_downloading(response):\n+            fp = request_fingerprint(req1)\n+            assert fp in self.info.downloading, self.info.downloading\n+            assert fp in self.info.waiting, self.info.waiting\n+            assert len(self.info.waiting[fp]) == 2, self.info.waiting\n+            assert fp not in self.info.downloaded, self.info.downloaded\n+            return response\n+\n+        rsp1 = Response('url')\n+        def rsp1_func():\n+            dfd = Deferred().addCallback(_check_downloading)\n+            reactor.callLater(.1, dfd.callback, rsp1)\n+            return dfd\n+\n+        def rsp2_func():\n+            assert False, 'This can not be called'\n+\n+        req1 = Request('url', meta=dict(response=rsp1_func))\n+        req2 = Request(req1.url, meta=dict(response=rsp2_func))\n+        item = dict(requests=[req1, req2])\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe1dd3a93d2a942440f9b0b18bce7b495ddd2d32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 21 | Churn Cumulative: 1102 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,9 +148,9 @@ class ImagesPipeline(MediaPipeline):\n             's3': S3ImagesStore,\n             }\n \n-    def __init__(self, store_uri):\n+    def __init__(self, store_uri, download_func=None):\n         self.store = self._get_store(store_uri)\n-        super(ImagesPipeline, self).__init__()\n+        super(ImagesPipeline, self).__init__(download_func=download_func)\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -5,8 +5,6 @@ from shutil import rmtree\n \n from twisted.trial import unittest\n \n-from scrapy.crawler import Crawler\n-from scrapy.conf import settings\n \n try:\n     import Image\n@@ -14,6 +12,10 @@ try:\n except ImportError, e:\n     skip = True\n \n+def _mocked_download_func(request, info):\n+    response = request.meta.get('response')\n+    return response() if callable(response) else response\n+\n \n class ImagesPipelineTestCase(unittest.TestCase):\n \n@@ -21,14 +23,10 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.contrib.pipeline.images import ImagesPipeline\n-\n-        self.crawler = Crawler(settings)\n-        self.crawler.install()\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir)\n+        self.pipeline = ImagesPipeline(self.tempdir, download_func=_mocked_download_func)\n \n     def tearDown(self):\n-        self.crawler.uninstall()\n         rmtree(self.tempdir)\n \n     def test_image_path(self):\n\n@@ -9,6 +9,8 @@ from scrapy.spider import BaseSpider\n from scrapy.utils.request import request_fingerprint\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.utils.test import get_crawler\n+from scrapy.utils.signal import disconnect_all\n+from scrapy import signals\n from scrapy import log\n \n \n@@ -28,6 +30,9 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         self.info = self.pipe.spiderinfo[self.spider]\n \n     def tearDown(self):\n+        for name, signal in vars(signals).items():\n+            if not name.startswith('_'):\n+                disconnect_all(signal)\n         self.pipe.close_spider(self.spider)\n \n     def test_default_media_to_download(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7640e99979b188b9e1ffa83fd777b232e4adcaa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 394 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,6 +111,8 @@ class MockedMediaPipeline(MediaPipeline):\n \n     def media_to_download(self, request, info):\n         self._mockcalled.append('media_to_download')\n+        if 'result' in request.meta:\n+            return request.meta.get('result')\n         return super(MockedMediaPipeline, self).media_to_download(request, info)\n \n     def get_media_requests(self, item, info):\n@@ -235,10 +237,10 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     def test_wait_if_request_is_downloading(self):\n         def _check_downloading(response):\n             fp = request_fingerprint(req1)\n-            assert fp in self.info.downloading, self.info.downloading\n-            assert fp in self.info.waiting, self.info.waiting\n-            assert len(self.info.waiting[fp]) == 2, self.info.waiting\n-            assert fp not in self.info.downloaded, self.info.downloaded\n+            self.assertTrue(fp in self.info.downloading)\n+            self.assertTrue(fp in self.info.waiting)\n+            self.assertTrue(fp not in self.info.downloaded)\n+            self.assertEqual(len(self.info.waiting[fp]), 2)\n             return response\n \n         rsp1 = Response('url')\n@@ -248,10 +250,19 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n             return dfd\n \n         def rsp2_func():\n-            assert False, 'This can not be called'\n+            self.fail('it must cache rsp1 result and must not try to redownload')\n \n         req1 = Request('url', meta=dict(response=rsp1_func))\n         req2 = Request(req1.url, meta=dict(response=rsp2_func))\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])\n+\n+    @inlineCallbacks\n+    def test_use_media_to_download_result(self):\n+        req = Request('url', meta=dict(result='ITSME', response=self.fail))\n+        item = dict(requests=req)\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        self.assertEqual(new_item['results'], [(True, 'ITSME')])\n+        self.assertEqual(self.pipe._mockcalled, \\\n+                ['get_media_requests', 'media_to_download', 'item_completed'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7283ad18e7599ddb9094e40be12a7c9464fe194", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 269 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,16 +1,25 @@\n # TODO: we should merge these tests with test_selector_libxml2.py\n \n import re\n-import unittest\n import weakref\n \n+from twisted.trial import unittest\n+\n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n+nolxml = False\n+try:\n     from scrapy.selector.lxmlsel import XmlXPathSelector, HtmlXPathSelector, \\\n         XPathSelector\n+except ImportError:\n+    nolxml = True\n+\n from scrapy.utils.test import libxml2debug\n \n class XPathSelectorTestCase(unittest.TestCase):\n \n+    if nolxml:\n+        skip = \"lxml not available\"\n+\n     @libxml2debug\n     def test_selector_simple(self):\n         \"\"\"Simple selector tests\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df6357724380993d3931fb014f769c9120f05d57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 138 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -116,7 +116,7 @@ setup_args = {\n \n try:\n     from setuptools import setup\n-    setup_args['install_requires'] = ['Twisted>=2.5']\n+    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml']\n except ImportError:\n     from distutils.core import setup\n  \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99096cdacf897c27278ad4f0e0223539e0a8bb25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 625 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,6 +80,12 @@ class S3ImagesStore(object):\n     AWS_ACCESS_KEY_ID = None\n     AWS_SECRET_ACCESS_KEY = None\n \n+    POLICY = 'public-read'\n+    HEADERS = {\n+            'Cache-Control': 'max-age=172800',\n+            'Content-Type': 'image/jpeg',\n+            }\n+\n     def __init__(self, uri):\n         assert uri.startswith('s3://')\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n@@ -107,15 +113,14 @@ class S3ImagesStore(object):\n     def persist_image(self, key, image, buf, info):\n         \"\"\"Upload image to S3 storage\"\"\"\n         width, height = image.size\n-        headers = {'Cache-Control': 'max-age=172800'} # 2 days of cache\n         b = self._get_boto_bucket()\n         key_name = '%s%s' % (self.prefix, key)\n         k = b.new_key(key_name)\n         k.set_metadata('width', str(width))\n         k.set_metadata('height', str(height))\n         buf.seek(0)\n-        return threads.deferToThread(k.set_contents_from_file, buf, headers, \\\n-            policy='public-read')\n+        return threads.deferToThread(k.set_contents_from_file, buf, \\\n+                headers=self.HEADERS, policy=self.POLICY)\n \n \n class ImagesPipeline(MediaPipeline):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41a85f9d14d3637dbd45214755c5612f096552b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 207 | Lines Deleted: 14 | Files Changed: 5 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 221 | Churn Cumulative: 2685 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 14 | DMM Complexity: 0.8947368421052632\n\nDIFF:\n@@ -194,18 +194,17 @@ class AnnotationTag(object):\n     \"\"\"\n     __slots__ = ('surrounds_attribute', 'start_index', 'end_index',\n             'tag_attributes', 'annotation_text', 'variant_id', \n-            'surrounds_variant','match_common_prefix', 'metadata')\n+            'match_common_prefix', 'metadata')\n     \n     def __init__(self, start_index, end_index, surrounds_attribute=None, \n             annotation_text=None, tag_attributes=None, variant_id=None,\n-            surrounds_variant=None, match_common_prefix=False):\n+            match_common_prefix=False):\n         self.start_index = start_index\n         self.end_index = end_index\n         self.surrounds_attribute = surrounds_attribute\n         self.annotation_text = annotation_text\n         self.tag_attributes = tag_attributes or []\n         self.variant_id = variant_id\n-        self.surrounds_variant = surrounds_variant\n         self.match_common_prefix = match_common_prefix\n         self.metadata = {}\n \n\n@@ -201,10 +201,6 @@ class TemplatePageParser(InstanceLearningParser):\n                 \n         self.extra_required_attrs.extend(jannotation.pop('required', []))\n         \n-        variant_id = jannotation.pop('variant', 0)\n-        if variant_id > 0:\n-            self.variant_stack.append(variant_id)\n-            annotation.surrounds_variant = variant_id\n         attribute_annotations = jannotation.pop('annotations', {}).items()\n         for extract_attribute, tag_value in attribute_annotations:\n             if extract_attribute == 'content':\n@@ -212,18 +208,25 @@ class TemplatePageParser(InstanceLearningParser):\n             else:\n                 annotation.tag_attributes.append((extract_attribute, tag_value))\n  \n+        variant_id = jannotation.pop('variant', 0)\n+        if variant_id > 0:\n+            if annotation.surrounds_attribute is not None:\n+                self.variant_stack.append(variant_id)\n+            else:\n+                annotation.variant_id = variant_id\n+       \n         annotation.metadata = jannotation\n \n         if annotation.annotation_text is None:\n             self.next_tag_index += 1\n-        if self.variant_stack:\n+        if self.variant_stack and annotation.variant_id is None:\n             variant_id = self.variant_stack[-1]\n             if variant_id == '0':\n                 variant_id = None\n             annotation.variant_id = variant_id\n         \n         # look for a closing tag if the content is important\n-        if annotation.surrounds_attribute or annotation.surrounds_variant:\n+        if annotation.surrounds_attribute:\n             self.labelled_tag_stacks[html_tag.tag].append(annotation)\n         else:\n             annotation.end_index = annotation.start_index + 1\n@@ -272,9 +275,9 @@ class TemplatePageParser(InstanceLearningParser):\n                 self.next_tag_index += 1\n             if len(labelled_tags) == 0:\n                 del self.labelled_tag_stacks[html_tag.tag]\n-            if annotation.surrounds_variant and self.variant_stack:\n+            if annotation.variant_id and self.variant_stack:\n                 prev = self.variant_stack.pop()\n-                if prev != annotation.surrounds_variant:\n+                if prev != annotation.variant_id:\n                     raise ValueError(\"unbalanced variant annotation tags\")\n                     \n     def handle_data(self, html_data_fragment):\n\n@@ -25,7 +25,7 @@ def build_extraction_tree(template, type_descriptor, trace=True):\n     extractors = BasicTypeExtractor.create(template.annotations, attribute_map)\n     if trace:\n         extractors = TraceExtractor.apply(template, extractors)\n-    for cls in (RepeatedDataExtractor, AdjacentVariantExtractor, RepeatedDataExtractor,\n+    for cls in (AdjacentVariantExtractor, RepeatedDataExtractor, AdjacentVariantExtractor, RepeatedDataExtractor,\n             RecordExtractor):\n         extractors = cls.apply(template, extractors)\n         if trace:\n@@ -465,9 +465,8 @@ class AdjacentVariantExtractor(RecordExtractor):\n                 continue\n             if vid in adjacent_variants:\n                 adjacent_variants.remove(vid)\n-            elif len(list(egroup)) > 1:\n+            else:\n                 adjacent_variants.add(vid)\n-        \n         new_extractors = []\n         for variant, group_seq in groupby(extractors, variantf):\n             group_seq = list(group_seq)\n\n@@ -633,6 +633,121 @@ EXTRACT_PAGE20 = u\"\"\"\n </body></html>\n \"\"\"\n \n+ANNOTATED_PAGE21 = u\"\"\"\n+<html><body>                                                                                                                                                                       \n+<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n+<p>\n+<table>\n+\n+<tr><td><img src=\"swatch1.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\"></td></tr>\n+\n+<tr><td><img src=\"swatch2.jpg\"></td></tr>\n+\n+<tr><td><img src=\"swatch3.jpg\"></td></tr>\n+\n+<tr><td><img src=\"swatch4.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\"></td></tr>\n+\n+</table>\n+\n+<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n+\n+</body></html>\n+\"\"\"\n+\n+EXTRACT_PAGE21 = u\"\"\"\n+<html><body>\n+<img src=\"image.jpg\">\n+<p>\n+<table>\n+\n+<tr><td><img src=\"swatch1.jpg\"></td></tr>\n+\n+<tr><td><img src=\"swatch2.jpg\"></td></tr>\n+\n+<tr><td><img src=\"swatch3.jpg\"></td></tr>\n+\n+<tr><td><img src=\"swatch4.jpg\"></td></tr>\n+\n+</table>\n+\n+<div>chairs</div>\n+</body></html>\n+\"\"\"\n+\n+ANNOTATED_PAGE22 = u\"\"\"\n+<html><body>                                                                                                                                                                       \n+<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n+<p>\n+<table>\n+\n+<tr><td>\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 1</p>\n+<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$67</b>\n+<img src=\"swatch1.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n+</td></tr>\n+\n+<tr><td>\n+<p>product 2</p>\n+<b>$70</b>\n+<img src=\"swatch2.jpg\">\n+</td></tr>\n+\n+<tr><td>\n+<p>product 3</p>\n+<b>$73</b>\n+<img src=\"swatch3.jpg\">\n+</td></tr>\n+\n+<tr><td>\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 4</p>\n+<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$80</b>\n+<img src=\"swatch4.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n+</td></tr>\n+\n+</table>\n+\n+<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n+\n+</body></html>\n+\"\"\"\n+\n+EXTRACT_PAGE22 = u\"\"\"\n+<html><body>\n+<img src=\"image.jpg\">\n+<p>\n+<table>\n+\n+<tr><td>\n+<p>product 1</p>\n+<b>$70</b>\n+<img src=\"swatch1.jpg\">\n+</td></tr>\n+\n+<tr><td>\n+<p>product 2</p>\n+<b>$80</b>\n+<img src=\"swatch2.jpg\">\n+</td></tr>\n+\n+<tr><td>\n+<p>product 3</p>\n+<b>$90</b>\n+<img src=\"swatch3.jpg\">\n+</td></tr>\n+\n+<tr><td>\n+<p>product 4</p>\n+<b>$100</b>\n+<img src=\"swatch4.jpg\">\n+</td></tr>\n+\n+</table>\n+\n+<div>chairs</div>\n+</body></html>\n+\"\"\"\n+\n+\n SAMPLE_DESCRIPTOR1 = ItemDescriptor('test', 'product test', [\n     A('name', \"Product name\", required=True),\n     A('price', \"Product price, including any discounts and tax or vat\", \n@@ -813,6 +928,35 @@ TEST_DATA = [\n                 {'price': ['330'], 'name': ['Queen']},\n             ]},\n     ),\n+    ('variants with swatches', [ANNOTATED_PAGE21], EXTRACT_PAGE21, None,\n+            {u'category': [u'chairs'],\n+             u'image_urls': [u'image.jpg'],\n+             u'variants': [\n+                {'swatches': ['swatch1.jpg']},\n+                {'swatches': ['swatch2.jpg']},\n+                {'swatches': ['swatch3.jpg']},\n+                {'swatches': ['swatch4.jpg']},\n+             ]\n+            },\n+    ),\n+    ('variants with swatches complete', [ANNOTATED_PAGE22], EXTRACT_PAGE22, None,\n+            {u'category': [u'chairs'],\n+             u'variants': [\n+                 {u'swatches': [u'swatch1.jpg'],\n+                  u'price': [u'$70'],\n+                  u'name': [u'product 1']},\n+                 {u'swatches': [u'swatch2.jpg'],\\\n+                  u'price': [u'$80'],\n+                  u'name': [u'product 2']},\n+                 {u'swatches': [u'swatch3.jpg'],\n+                  u'price': [u'$90'],\n+                  u'name': [u'product 3']},\n+                 {u'swatches': [u'swatch4.jpg'],\n+                  u'price': [u'$100'],\n+                  u'name': [u'product 4']}\n+             ],\n+             u'image_urls': [u'image.jpg']},\n+    ),\n ]\n \n class TestExtraction(TestCase):\n\n@@ -155,6 +155,32 @@ Description\n </body></html>\n \"\"\"\n \n+LABELLED_PAGE9 = u\"\"\"\n+<html><body>\n+<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 1</p>\n+<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$67</b>\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 2</p>\n+<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$70</b>\n+<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n+</body></html>\n+\"\"\"\n+\n+LABELLED_PAGE10 = u\"\"\"\n+<html><body>\n+<img src=\"image.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}\">\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 1</p>\n+<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$67</b>\n+<img src=\"swatch1.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 1, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n+\n+<p data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">product 2</p>\n+<b data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">$70</b>\n+<img src=\"swatch2.jpg\" data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 2, &quot;annotations&quot;: {&quot;src&quot;: &quot;swatches&quot;}}\">\n+\n+<div data-scrapy-annotate=\"{&quot;required&quot;: [], &quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;category&quot;}}\">tables</div>\n+</body></html>\n+\"\"\"\n+\n def _parse_page(parser_class, pagetext):\n     htmlpage = HtmlPage(None, {}, pagetext)\n     parser = parser_class(TokenDict())\n@@ -269,6 +295,28 @@ class TestPageParsing(TestCase):\n         p = _parse_page(TemplatePageParser, LABELLED_PAGE8)\n         self.assertEqual(p.extra_required_attrs, [\"description\"])\n     \n+    def test_variants(self):\n+        \"\"\"Test parsing of variant annotations\"\"\"\n+        annotations = _parse_page(TemplatePageParser, LABELLED_PAGE9).annotations\n+        self.assertEqual(annotations[0].variant_id, None)\n+        self.assertEqual(annotations[1].variant_id, 1)\n+        self.assertEqual(annotations[2].variant_id, 1)\n+        self.assertEqual(annotations[3].variant_id, 2)\n+        self.assertEqual(annotations[4].variant_id, 2)\n+        self.assertEqual(annotations[5].variant_id, None)\n+\n+    def test_variants_in_attributes(self):\n+        \"\"\"Test parsing of variant annotations in attributes\"\"\"\n+        annotations = _parse_page(TemplatePageParser, LABELLED_PAGE10).annotations\n+        self.assertEqual(annotations[0].variant_id, None)\n+        self.assertEqual(annotations[1].variant_id, 1)\n+        self.assertEqual(annotations[2].variant_id, 1)\n+        self.assertEqual(annotations[3].variant_id, 1)\n+        self.assertEqual(annotations[4].variant_id, 2)\n+        self.assertEqual(annotations[5].variant_id, 2)\n+        self.assertEqual(annotations[6].variant_id, 2)\n+        self.assertEqual(annotations[7].variant_id, None)\n+\n     def test_site_pages(self):\n         \"\"\"\n         Tests from real pages. More reliable and easy to build for more complicated structures\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a09dd18bbb3332080d1194679b29b260ad01e5f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 338 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -11,11 +11,11 @@ lxml will be used.\n from scrapy.conf import settings\n \n if settings['SELECTORS_BACKEND'] == 'lxml':\n-    from .lxmlsel import *\n+    from scrapy.selector.lxmlsel import *\n elif settings['SELECTORS_BACKEND'] == 'libxml2':\n-    from .libxml2sel import *\n+    from scrapy.selector.libxml2sel import *\n elif settings['SELECTORS_BACKEND'] == 'dummy':\n-    from .dummysel import *\n+    from scrapy.selector.dummysel import *\n else:\n     try:\n         import libxml2\n@@ -23,8 +23,8 @@ else:\n         try:\n             import lxml\n         except ImportError:\n-            from .dummysel import *\n+            from scrapy.selector.dummysel import *\n         else:\n-            from .lxmlsel import *\n+            from scrapy.selector.lxmlsel import *\n     else:\n-        from .libxml2sel import *\n+        from scrapy.selector.libxml2sel import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3a108dc71188d7f07d2dafc325c69e08d0b2fb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 39 | Churn Cumulative: 222 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ in Python 2.5. The Python 2.6 function is used when available.\n import sys\n import os\n import fnmatch\n+import pkgutil\n from shutil import copy2, copystat\n \n __all__ = ['cpu_count', 'copytree', 'ignore_patterns']\n@@ -105,3 +106,27 @@ except ImportError:\n         import simplejson as json\n     except ImportError:\n         import scrapy.xlib.simplejson as json\n+\n+\n+def _get_data(package, resource):\n+    loader = pkgutil.get_loader(package)\n+    if loader is None or not hasattr(loader, 'get_data'):\n+        return None\n+    mod = sys.modules.get(package) or loader.load_module(package)\n+    if mod is None or not hasattr(mod, '__file__'):\n+        return None\n+\n+    # Modify the resource name to be compatible with the loader.get_data\n+    # signature - an os.path format \"filename\" starting with the dirname of\n+    # the package's __file__\n+    parts = resource.split('/')\n+    parts.insert(0, os.path.dirname(mod.__file__))\n+    resource_name = os.path.join(*parts)\n+    return loader.get_data(resource_name)\n+\n+# pkgutil.get_data() not available in python 2.5\n+# see http://docs.python.org/release/2.5/lib/module-pkgutil.html\n+try:\n+    get_data = pkgutil.get_data\n+except AttributeError:\n+    get_data = _get_data\n\n@@ -1,8 +1,11 @@\n import glob\n-import pkgutil\n from cStringIO import StringIO\n from ConfigParser import SafeConfigParser, NoSectionError, NoOptionError\n \n+from scrapy.utils.py26 import get_data\n+\n+__package__ = 'scrapyd' # required for compatibility with python 2.5\n+\n class Config(object):\n     \"\"\"A ConfigParser wrapper to support defaults when calling instance\n     methods, and also tied to a single section\"\"\"\n@@ -12,7 +15,7 @@ class Config(object):\n     def __init__(self, values=None):\n         if values is None:\n             sources = self._getsources()\n-            default_config = pkgutil.get_data(__package__, 'default_scrapyd.conf')\n+            default_config = get_data(__package__, 'default_scrapyd.conf')\n             self.cp = SafeConfigParser()\n             self.cp.readfp(StringIO(default_config))\n             self.cp.read(sources)\n\n@@ -1,11 +1,14 @@\n-import pkgutil, unittest\n+import unittest\n from cStringIO import StringIO\n \n from scrapyd.eggutils import get_spider_list_from_eggfile\n+from scrapy.utils.py26 import get_data\n+\n+__package__ = 'scrapyd.tests' # required for compatibility with python 2.5\n \n class EggUtilsTest(unittest.TestCase):\n \n     def test_get_spider_list_from_eggfile(self):\n-        eggfile = StringIO(pkgutil.get_data(__package__, 'mybot.egg'))\n+        eggfile = StringIO(get_data(__package__, 'mybot.egg'))\n         spiders = get_spider_list_from_eggfile(eggfile, 'mybot')\n         self.assertEqual(set(spiders), set(['spider1', 'spider2']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ead888db879fa8861875f59ff204b75ae778821", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 9 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 28/28 | Churn Δ: 21 | Churn Cumulative: 1603 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,8 @@ from scrapy.contrib.ibl.extractors import (contains_any_numbers,\n \n try:\n     import numpy\n+    __doctests__ = ['scrapy.contrib.ibl.extraction.%s' % x for x in \\\n+        ['regionextract', 'similarity', 'pageobjects']]\n except ImportError:\n     numpy = None\n \n\n@@ -0,0 +1,5 @@\n+try:\n+    import numpy\n+    __doctests__ = ['scrapy.contrib.ibl.extractors']\n+except ImportError:\n+    pass\n\n@@ -3,6 +3,8 @@ import unittest\n \n from scrapy.utils.datatypes import PriorityQueue, PriorityStack, CaselessDict\n \n+__doctests__ = ['scrapy.utils.datatypes']\n+\n # (ITEM, PRIORITY)\n INPUT = [(1, -5), (30, -1), (80, -3), (4, 1), (6, 3), (20, 0), (50, -1)]\n \n\n@@ -1,6 +1,7 @@\n import unittest\n from scrapy.utils.http import basic_auth_header\n \n+__doctests__ = ['scrapy.utils.http']\n \n class UtilsHttpTestCase(unittest.TestCase):\n \n\n@@ -5,6 +5,8 @@ from cStringIO import StringIO\n \n from scrapy.utils.misc import load_object, arg_to_iter, walk_modules\n \n+__doctests__ = ['scrapy.utils.misc']\n+\n class UtilsMiscTestCase(unittest.TestCase):\n \n     def test_load_object(self):\n\n@@ -6,6 +6,8 @@ from scrapy.utils.python import str_to_unicode, unicode_to_str, \\\n     memoizemethod_noargs, isbinarytext, equal_attributes, \\\n     WeakKeyCache, stringify_dict\n \n+__doctests__ = ['scrapy.utils.python']\n+\n class UtilsPythonTestCase(unittest.TestCase):\n     def test_str_to_unicode(self):\n         # converting an utf-8 encoded string to unicode\n\n@@ -7,6 +7,8 @@ from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, \\\n     response_httprepr, get_cached_beautifulsoup, open_in_browser\n \n+__doctests__ = ['scrapy.utils.response']\n+\n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n \n\n@@ -0,0 +1 @@\n+__doctests__ = ['scrapy.utils.template']\n\n@@ -72,10 +72,10 @@ def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n \n     >>> response_status_message(200)\n-    200 OK\n+    '200 OK'\n \n     >>> response_status_message(404)\n-    404 Not Found\n+    '404 Not Found'\n     \"\"\"\n     return '%s %s' % (status, http.responses.get(int(status)))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
