{"custom_id": "keras#5ef5ab1e2826c05137f50801512ce2267a244a75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): 172/167 | Churn Δ: 110 | Churn Cumulative: 28455 | Contributors (this commit): 148 | Commits (past 90d): 25 | Contributors (cumulative): 163 | DMM Complexity: 0.8955223880597015\n\nDIFF:\n@@ -47,16 +47,9 @@ class MnistTest(test_util.DTensorBaseTest):\n             integration_test_utils.get_all_replicated_layout_map(mesh)\n         )\n \n-        self.assertIsInstance(model._train_counter, dtensor.DVariable)\n-        self.assertIsInstance(model._test_counter, dtensor.DVariable)\n-        self.assertIsInstance(model._predict_counter, dtensor.DVariable)\n-\n         optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n         optimizer.build(model.trainable_variables)\n \n-        model.compile(loss=\"CategoricalCrossentropy\", optimizer=optimizer)\n-        self.assertIsInstance(model._steps_per_execution, dtensor.DVariable)\n-\n         train_losses = integration_test_utils.train_mnist_model_batch_sharded(\n             model,\n             optimizer,\n@@ -68,6 +61,68 @@ class MnistTest(test_util.DTensorBaseTest):\n         # Make sure the losses are decreasing\n         self.assertEqual(train_losses, sorted(train_losses, reverse=True))\n \n+    def test_model_fit(self):\n+        devices = tf.config.list_physical_devices(\"CPU\")\n+        tf.config.set_logical_device_configuration(\n+            devices[0],\n+            [\n+                tf.config.LogicalDeviceConfiguration(),\n+            ]\n+            * 8,\n+        )\n+\n+        mesh = dtensor.create_mesh(\n+            devices=[\"CPU:%d\" % i for i in range(8)], mesh_dims=[(\"batch\", 8)]\n+        )\n+\n+        backend.enable_tf_random_generator()\n+        # Needed by keras initializers.\n+        tf_utils.set_random_seed(1337)\n+\n+        model = integration_test_utils.get_model_with_layout_map(\n+            integration_test_utils.get_all_replicated_layout_map(mesh)\n+        )\n+\n+        optimizer = adam.Adam(learning_rate=0.001, mesh=mesh)\n+        optimizer.build(model.trainable_variables)\n+\n+        global_batch_size = 64\n+        model.compile(\n+            loss=\"CategoricalCrossentropy\", optimizer=optimizer, metrics=\"acc\"\n+        )\n+        train_ds, eval_ds = integration_test_utils.get_mnist_datasets(\n+            integration_test_utils.NUM_CLASS, global_batch_size\n+        )\n+\n+        def distribute_ds(dataset):\n+            dataset = dataset.unbatch()\n+\n+            def _create_batch_layout(tensor_spec):\n+                rank = len(tensor_spec.shape) + 1\n+                return dtensor.Layout.batch_sharded(\n+                    mesh, batch_dim=\"batch\", rank=rank\n+                )\n+\n+            layouts = tf.nest.map_structure(\n+                _create_batch_layout, dataset.element_spec\n+            )\n+\n+            return dtensor.DTensorDataset(\n+                dataset=dataset,\n+                mesh=mesh,\n+                layouts=layouts,\n+                global_batch_size=global_batch_size,\n+                dataset_already_batched=False,\n+                batch_dim=\"batch\",\n+                prefetch=None,\n+                tf_data_service_config=None,\n+            )\n+\n+        train_ds = distribute_ds(train_ds)\n+        eval_ds = distribute_ds(eval_ds)\n+        model.fit(train_ds, steps_per_epoch=10)\n+        model.evaluate(eval_ds, steps=10)\n+\n     def DISABLED_test_mnist_training_tpu(self):\n         # TODO(scottzhu): Enable TPU test once the dtensor_test rule is migrated\n         # out of learning/brain\n\n@@ -31,8 +31,11 @@ from keras.utils import tf_utils\n class Container:\n     \"\"\"Base Container class.\"\"\"\n \n-    def __init__(self, output_names=None):\n+    def __init__(self, output_names=None, mesh=None):\n         self._output_names = output_names\n+        # Used by DTensor layout map use case. Can be removed after DTensor\n+        # based distribution strategy.\n+        self._mesh = mesh\n \n     def build(self, y_pred):\n         if self._output_names is None:\n@@ -115,9 +118,16 @@ class LossesContainer(Container):\n     \"\"\"\n \n     def __init__(\n-        self, losses, loss_weights=None, output_names=None, total_loss_mean=None\n+        self,\n+        losses,\n+        loss_weights=None,\n+        output_names=None,\n+        total_loss_mean=None,\n+        mesh=None,\n     ):\n-        super(LossesContainer, self).__init__(output_names=output_names)\n+        super(LossesContainer, self).__init__(\n+            output_names=output_names, mesh=mesh\n+        )\n \n         # Keep user-supplied values untouched for recompiling and serialization.\n         self._user_losses = losses\n@@ -128,7 +138,9 @@ class LossesContainer(Container):\n         self._per_output_metrics = None  # Per-output losses become metrics.\n \n         # Mean of the total loss.\n-        self._total_loss_mean = total_loss_mean or metrics_mod.Mean(name=\"loss\")\n+        self._total_loss_mean = total_loss_mean or metrics_mod.Mean(\n+            name=\"loss\", mesh=self._mesh\n+        )\n         self._built = False\n \n     def get_config(self):\n@@ -210,7 +222,7 @@ class LossesContainer(Container):\n                     self._per_output_metrics.append(None)\n                 else:\n                     self._per_output_metrics.append(\n-                        metrics_mod.Mean(output_name + \"_loss\")\n+                        metrics_mod.Mean(output_name + \"_loss\", mesh=self._mesh)\n                     )\n \n     def __call__(\n@@ -375,6 +387,7 @@ class MetricsContainer(Container):\n         weighted_metrics=None,\n         output_names=None,\n         from_serialized=False,\n+        mesh=None,\n     ):\n         \"\"\"Initializes a container for metrics.\n \n@@ -387,7 +400,9 @@ class MetricsContainer(Container):\n             model.  Used to avoid redundantly applying pre-processing renaming\n             steps.\n         \"\"\"\n-        super(MetricsContainer, self).__init__(output_names=output_names)\n+        super(MetricsContainer, self).__init__(\n+            output_names=output_names, mesh=mesh\n+        )\n \n         self._check_duplicated_metrics(metrics, weighted_metrics)\n         # Keep user-supplied values untouched for recompiling and serialization.\n@@ -688,9 +703,8 @@ class MetricsContainer(Container):\n                     )\n \n             metric_obj = metrics_mod.MeanMetricWrapper(\n-                metric_obj, name=metric_name\n+                metric_obj, name=metric_name, mesh=self._mesh\n             )\n-\n         return metric_obj\n \n     def _should_broadcast(self, obj):\n\n@@ -777,17 +777,26 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self._run_eagerly = run_eagerly\n \n             self.optimizer = self._get_optimizer(optimizer)\n+\n+            mesh = None\n+            if self._layout_map is not None:\n+                mesh = self._layout_map.get_default_mesh()\n+\n             if isinstance(loss, compile_utils.LossesContainer):\n                 self.compiled_loss = loss\n             else:\n                 self.compiled_loss = compile_utils.LossesContainer(\n-                    loss, loss_weights, output_names=self.output_names\n+                    loss,\n+                    loss_weights,\n+                    output_names=self.output_names,\n+                    mesh=mesh,\n                 )\n             self.compiled_metrics = compile_utils.MetricsContainer(\n                 metrics,\n                 weighted_metrics,\n                 output_names=self.output_names,\n                 from_serialized=from_serialized,\n+                mesh=mesh,\n             )\n \n             self._configure_steps_per_execution(steps_per_execution or 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db293f5690d5f2e64111c81589fddd51a37de49b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1736 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1031,7 +1031,7 @@ class Optimizer(_BaseOptimizer):\n \n     This optimizer class is `tf.distribute.Strategy` aware, which means it\n     automatically sums gradients across all replicas. To aggregate gradients\n-    yourself, call `apply_gradients` with `skip_aggregate_gradients` set to\n+    yourself, call `apply_gradients` with `skip_gradients_aggregation` set to\n     True.  This is useful if you need to process aggregated gradients.\n \n     ```python\n@@ -1046,7 +1046,7 @@ class Optimizer(_BaseOptimizer):\n       # Custom logic to aggregate gradients.\n       gradients = strategy.reduce(\"SUM\", gradients, axis=None)\n       opt.apply_gradients(zip(gradients, model.trainable_variables),\n-          skip_aggregate_gradients=True)\n+          skip_gradients_aggregation=True)\n     ```\n \n     ### Creating a custom optimizer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#693cd31022b3dd65b6a567b034d6a986374cb1c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 756 | Contributors (this commit): 7 | Commits (past 90d): 43 | Contributors (cumulative): 13 | DMM Complexity: 0.75\n\nDIFF:\n@@ -42,6 +42,7 @@ import types\n import warnings\n \n import tensorflow as tf\n+from tensorflow import nest\n \n from keras_core.trainers.data_adapters import array_data_adapter\n from keras_core.trainers.data_adapters import data_adapter_utils\n@@ -67,7 +68,8 @@ class EpochIterator:\n         if steps_per_epoch:\n             self._current_iterator = None\n             self._insufficient_data = False\n-        if isinstance(x, data_adapter_utils.ARRAY_TYPES):\n+        first_element = next(iter(nest.flatten(x)), None)\n+        if isinstance(first_element, data_adapter_utils.ARRAY_TYPES):\n             self.data_adapter = array_data_adapter.ArrayDataAdapter(\n                 x,\n                 y,\n\n@@ -29,7 +29,7 @@ class ExampleModel(layers.Dense, Trainer):\n         Trainer.__init__(self)\n \n \n-class OutputStructModel(layers.Layer, Trainer):\n+class StructModel(layers.Layer, Trainer):\n     def __init__(self, units):\n         layers.Layer.__init__(self)\n         Trainer.__init__(self)\n@@ -46,8 +46,8 @@ class OutputStructModel(layers.Layer, Trainer):\n \n     def call(self, x):\n         return {\n-            \"y_one\": self.dense_1(x),\n-            \"y_two\": self.dense_2(x),\n+            \"y_one\": self.dense_1(x[\"x_one\"]),\n+            \"y_two\": self.dense_2(x[\"x_two\"]),\n         }\n \n \n@@ -193,12 +193,15 @@ class TestTrainer(testing.TestCase):\n         outputs = model.predict(x, batch_size=batch_size)\n         self.assertAllClose(outputs, 4 * np.ones((100, 3)))\n \n-        # Test with output struct\n-        model = OutputStructModel(units=3)\n+        # Test with input/output structs\n+        model = StructModel(units=3)\n         model.run_eagerly = run_eagerly\n         model.jit_compile = jit_compile\n \n-        x = np.ones((100, 4))\n+        x = {\n+            \"x_one\": np.ones((100, 4)),\n+            \"x_two\": np.ones((100, 4)),\n+        }\n         batch_size = 16\n         outputs = model.predict(x, batch_size=batch_size)\n         self.assertTrue(isinstance(outputs, dict))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4fb208c378a15303632a60c0d47b15ab1f189804", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 25 | Churn Cumulative: 334 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -47,6 +47,10 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the uniform distribution.\n \n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n     Args:\n         shape: The shape of the random values to generate.\n         minval: Floats, defaults to 0. Lower bound of the range of\n@@ -77,6 +81,10 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the truncated normal distribution.\n \n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n     Args:\n         shape: The shape of the random values to generate.\n         mean: Floats, defaults to 0. Mean of the random values to generate.\n@@ -95,14 +103,15 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n-    x = torch.empty(shape)\n-    # TODO: setting seed globally via `manual_seed` might create side effects.\n-    if seed is not None:\n-        seed_val, _ = draw_seed(seed)\n-        torch.manual_seed(int(seed_val))\n-    return torch.nn.init.trunc_normal_(\n-        x, mean=mean, std=stddev, a=-stddev * 2, b=stddev * 2\n-    )\n+    # Take a larger standard normal dist, discard values outside 2 * stddev\n+    # Offset by mean and stddev\n+    x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n+    valid = (x > -2) & (x < 2)\n+    indexes = valid.max(-1, keepdim=True)[1]\n+    trunc_x = torch.empty(shape)\n+    trunc_x.data.copy_(x.gather(-1, indexes).squeeze(-1))\n+    trunc_x.data.mul_(stddev).add_(mean)\n+    return trunc_x\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#770cb289f7fa49a61230d6d8e9c875e552adb0bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 110 | Files Changed: 18 | Hunks: 52 | Methods Changed: 31 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 237 | Churn Cumulative: 17220 | Contributors (this commit): 12 | Commits (past 90d): 145 | Contributors (cumulative): 65 | DMM Complexity: 0.7\n\nDIFF:\n@@ -162,13 +162,13 @@ def softmax(x, axis=-1):\n \n \n @keras_core_export(\"keras_core.activations.elu\")\n-def elu(x):\n+def elu(x, alpha=1.0):\n     \"\"\"Exponential Linear Unit.\n \n     The exponential linear unit (ELU) with `alpha > 0` is define as:\n \n     - `x` if `x > 0`\n-    - `exp(x) - 1` if `x < 0`\n+    - alpha * `exp(x) - 1` if `x < 0`\n \n     ELUs have negative values which pushes the mean of the activations\n     closer to zero.\n@@ -186,7 +186,7 @@ def elu(x):\n \n     - [Clevert et al., 2016](https://arxiv.org/abs/1511.07289)\n     \"\"\"\n-    return ops.elu(x)\n+    return ops.elu(x, alpha=alpha)\n \n \n @keras_core_export(\"keras_core.activations.selu\")\n\n@@ -56,8 +56,8 @@ def hard_sigmoid(x):\n     return jnn.hard_sigmoid(x)\n \n \n-def elu(x):\n-    return jnn.elu(x)\n+def elu(x, alpha=1.0):\n+    return jnn.elu(x, alpha=alpha)\n \n \n def selu(x):\n\n@@ -2,8 +2,6 @@ from jax import lax\n from jax import numpy as jnp\n from tensorflow import nest\n \n-from keras_core.backend.common.stateless_scope import StatelessScope\n-\n \n def rnn(\n     step_function,\n@@ -180,10 +178,6 @@ def rnn(\n \n             scan_xs = inputs\n \n-        with StatelessScope():\n-            # We must use a stateless scope because `scan` will involve\n-            # JAX tracing -- any variable update at this stage would\n-            # be a leak.\n         new_states, outputs = lax.scan(\n             f=_step,\n             init=initial_states,\n\n@@ -57,8 +57,12 @@ def hard_sigmoid(x):\n     return tf.clip_by_value(x, 0.0, 1.0)\n \n \n-def elu(x):\n-    return tf.nn.elu(x)\n+def elu(x, alpha=1.0):\n+    res = tf.nn.elu(x)\n+    if alpha == 1:\n+        return res\n+    else:\n+        return tf.where(x > 0, res, alpha * res)\n \n \n def selu(x):\n\n@@ -47,6 +47,10 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the uniform distribution.\n \n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n     Args:\n         shape: The shape of the random values to generate.\n         minval: Floats, defaults to 0. Lower bound of the range of\n@@ -77,6 +81,10 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the truncated normal distribution.\n \n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n     Args:\n         shape: The shape of the random values to generate.\n         mean: Floats, defaults to 0. Mean of the random values to generate.\n@@ -95,14 +103,15 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n-    x = torch.empty(shape)\n-    # TODO: setting seed globally via `manual_seed` might create side effects.\n-    if seed is not None:\n-        seed_val, _ = draw_seed(seed)\n-        torch.manual_seed(int(seed_val))\n-    return torch.nn.init.trunc_normal_(\n-        x, mean=mean, std=stddev, a=-stddev * 2, b=stddev * 2\n-    )\n+    # Take a larger standard normal dist, discard values outside 2 * stddev\n+    # Offset by mean and stddev\n+    x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n+    valid = (x > -2) & (x < 2)\n+    indexes = valid.max(-1, keepdim=True)[1]\n+    trunc_x = torch.empty(shape)\n+    trunc_x.data.copy_(x.gather(-1, indexes).squeeze(-1))\n+    trunc_x.data.mul_(stddev).add_(mean)\n+    return trunc_x\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n\n@@ -8,11 +8,9 @@ class Activation(Layer):\n     \"\"\"Applies an activation function to an output.\n \n     Args:\n-        activation: Activation function. It could be\n-            a callable, or the name of an activation\n-            from the `keras_core.activations` namespace.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        activation: Activation function. It could be a callable, or the name of\n+            an activation from the `keras_core.activations` namespace.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Example:\n \n\n@@ -10,21 +10,22 @@ class ELU(Layer):\n     Formula:\n \n     ```\n-    f(x) = (exp(x) - 1.) for x < 0\n+    f(x) = alpha * (exp(x) - 1.) for x < 0\n     f(x) = x for x >= 0\n     ```\n \n     Args:\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        alpha: float, slope of negative section. Defaults to 1.0.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, **kwargs):\n+    def __init__(self, alpha=1.0, **kwargs):\n         super().__init__(**kwargs)\n+        self.alpha = alpha\n         self.supports_masking = True\n \n     def call(self, inputs):\n-        return activations.elu(inputs)\n+        return activations.elu(inputs, alpha=self.alpha)\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core import testing\n from keras_core.layers.activations import elu\n+import tensorflow as tf\n \n \n class ELUTest(testing.TestCase):\n@@ -17,7 +18,12 @@ class ELUTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-        x = np.random.random((2, 5))\n+    def test_correctness(self):\n+        x = np.random.random((2, 2, 5))\n         elu_layer = elu.ELU()\n-        result = elu_layer(x[np.newaxis, :])[0]\n-        self.assertAllClose(result, x, rtol=1e-05)\n+        tf_elu_layer = tf.keras.layers.ELU()\n+        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n+\n+        elu_layer = elu.ELU(alpha=0.7)\n+        tf_elu_layer = tf.keras.layers.ELU(alpha=0.7)\n+        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n\n@@ -13,46 +13,37 @@ class PReLU(Layer):\n \n     Formula:\n     ``` python\n-    f(x) = negative_slope * x for x < 0\n+    f(x) = alpha * x for x < 0\n     f(x) = x for x >= 0\n     ```\n-    where `negative_slope` is a learned array with the same shape as x.\n+    where `alpha` is a learned array with the same shape as x.\n \n     Args:\n-        negative_slope_initializer: Initializer function for the weights.\n-        negative_slope_regularizer: Regularizer for the weights.\n-        negative_slope_constraint: Constraint for the weights.\n-        shared_axes: The axes along which to share learnable\n-            parameters for the activation function.\n-            For example, if the incoming feature maps\n-            are from a 2D convolution\n-            with output shape `(batch, height, width, channels)`,\n-            and you wish to share parameters across space\n-            so that each filter only has one set of parameters,\n+        alpha_initializer: Initializer function for the weights.\n+        alpha_regularizer: Regularizer for the weights.\n+        alpha_constraint: Constraint for the weights.\n+        shared_axes: The axes along which to share learnable parameters for the\n+            activation function. For example, if the incoming feature maps are\n+            from a 2D convolution with output shape\n+            `(batch, height, width, channels)`, and you wish to share parameters\n+            across space so that each filter only has one set of parameters,\n             set `shared_axes=[1, 2]`.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n     def __init__(\n         self,\n-        negative_slope_initializer=\"Zeros\",\n-        negative_slope_regularizer=None,\n-        negative_slope_constraint=None,\n+        alpha_initializer=\"Zeros\",\n+        alpha_regularizer=None,\n+        alpha_constraint=None,\n         shared_axes=None,\n         **kwargs\n     ):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n-        self.negative_slope_initializer = initializers.get(\n-            negative_slope_initializer\n-        )\n-        self.negative_slope_regularizer = regularizers.get(\n-            negative_slope_regularizer\n-        )\n-        self.negative_slope_constraint = constraints.get(\n-            negative_slope_constraint\n-        )\n+        self.alpha_initializer = initializers.get(alpha_initializer)\n+        self.alpha_regularizer = regularizers.get(alpha_regularizer)\n+        self.alpha_constraint = constraints.get(alpha_constraint)\n         if shared_axes is None:\n             self.shared_axes = None\n         elif not isinstance(shared_axes, (list, tuple)):\n@@ -65,12 +56,12 @@ class PReLU(Layer):\n         if self.shared_axes is not None:\n             for i in self.shared_axes:\n                 param_shape[i - 1] = 1\n-        self.negative_slope = self.add_weight(\n+        self.alpha = self.add_weight(\n             shape=param_shape,\n-            name=\"negative_slope\",\n-            initializer=self.negative_slope_initializer,\n-            regularizer=self.negative_slope_regularizer,\n-            constraint=self.negative_slope_constraint,\n+            name=\"alpha\",\n+            initializer=self.alpha_initializer,\n+            regularizer=self.alpha_regularizer,\n+            constraint=self.alpha_constraint,\n         )\n         # Set input spec\n         axes = {}\n@@ -83,21 +74,21 @@ class PReLU(Layer):\n \n     def call(self, inputs):\n         pos = activations.relu(inputs)\n-        neg = -self.negative_slope * activations.relu(-inputs)\n+        neg = -self.alpha * activations.relu(-inputs)\n         return pos + neg\n \n     def get_config(self):\n         config = super().get_config()\n         config.update(\n             {\n-                \"negative_slope_initializer\": initializers.serialize(\n-                    self.negative_slope_initializer\n+                \"alpha_initializer\": initializers.serialize(\n+                    self.alpha_initializer\n                 ),\n-                \"negative_slope_regularizer\": regularizers.serialize(\n-                    self.negative_slope_regularizer\n+                \"alpha_regularizer\": regularizers.serialize(\n+                    self.alpha_regularizer\n                 ),\n-                \"negative_slope_constraint\": constraints.serialize(\n-                    self.negative_slope_constraint\n+                \"alpha_constraint\": constraints.serialize(\n+                    self.alpha_constraint\n                 ),\n                 \"shared_axes\": self.shared_axes,\n             }\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core import testing\n from keras_core.layers.activations import prelu\n+import tensorflow as tf\n \n \n class PReLUTest(testing.TestCase):\n@@ -9,9 +10,9 @@ class PReLUTest(testing.TestCase):\n         self.run_layer_test(\n             prelu.PReLU,\n             init_kwargs={\n-                \"negative_slope_initializer\": \"zeros\",\n-                \"negative_slope_regularizer\": \"L1\",\n-                \"negative_slope_constraint\": \"MaxNorm\",\n+                \"alpha_initializer\": \"zeros\",\n+                \"alpha_regularizer\": \"L1\",\n+                \"alpha_constraint\": \"MaxNorm\",\n                 \"shared_axes\": 1,\n             },\n             input_shape=(2, 3, 4),\n@@ -19,15 +20,25 @@ class PReLUTest(testing.TestCase):\n         )\n \n     def test_prelu_correctness(self):\n+        inputs = np.random.randn(2, 10, 5, 3)\n         prelu_layer = prelu.PReLU(\n-            negative_slope_initializer=\"glorot_uniform\",\n-            negative_slope_regularizer=\"l1\",\n-            negative_slope_constraint=\"non_neg\",\n-            shared_axes=None,\n+            alpha_initializer=\"glorot_uniform\",\n+            alpha_regularizer=\"l1\",\n+            alpha_constraint=\"non_neg\",\n+            shared_axes=(1, 2),\n         )\n-        test_input = np.random.randn(10, 5)\n-        result = prelu_layer(test_input)\n-        expected_output = np.maximum(\n-            0, test_input\n-        ) + prelu_layer.negative_slope.numpy() * np.minimum(0, test_input)\n-        self.assertAllClose(result, expected_output)\n+        tf_prelu_layer = tf.keras.layers.PReLU(\n+            alpha_initializer=\"glorot_uniform\",\n+            alpha_regularizer=\"l1\",\n+            alpha_constraint=\"non_neg\",\n+            shared_axes=(1, 2),\n+        )\n+\n+        prelu_layer.build(inputs.shape)\n+        tf_prelu_layer.build(inputs.shape)\n+\n+        weights = np.random.random((1, 1, 3))\n+        prelu_layer.alpha.assign(weights)\n+        tf_prelu_layer.alpha.assign(weights)\n+\n+        self.assertAllClose(prelu_layer(inputs), tf_prelu_layer(inputs))\n\n@@ -17,7 +17,11 @@ class ReLU(Layer):\n \n     Example:\n     ``` python\n-    relu_layer = relu.ReLU(max_value=10, negative_slope=0.5, threshold=0)\n+    relu_layer = keras_core.layers.activations.ReLU(\n+        max_value=10,\n+        negative_slope=0.5,\n+        threshold=0,\n+    )\n     input = np.array([-10, -5, 0.0, 5, 10])\n     result = relu_layer(input)\n     # result = [-5. , -2.5,  0. ,  5. , 10.]\n@@ -26,12 +30,10 @@ class ReLU(Layer):\n     Args:\n         max_value: Float >= 0. Maximum activation value. None means unlimited.\n             Defaults to `None`.\n-        negative_slope: Float >= 0. Negative slope coefficient.\n-            Defaults to 0.0.\n+        negative_slope: Float >= 0. Negative slope coefficient. Defaults to 0.0.\n         threshold: Float >= 0. Threshold value for thresholded activation.\n             Defaults to 0.0.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n     def __init__(\n\n@@ -15,7 +15,7 @@ class Softmax(Layer):\n     ```\n \n     Example:\n-    >>>softmax_layer = Softmax()\n+    >>>softmax_layer = keras_core.layers.activations.Softmax()\n     >>>input = np.array([1.0, 2.0, 1.0])\n     >>>result = softmax_layer(input)\n     [0.21194157, 0.5761169, 0.21194157]\n@@ -24,11 +24,10 @@ class Softmax(Layer):\n     Args:\n         axis: Integer, or list of Integers, axis along which the softmax\n             normalization is applied.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Call arguments:\n-        inputs: The inputs, or logits to the softmax layer.\n+        inputs: The inputs (logits) to the softmax layer.\n         mask: A boolean mask of the same shape as `inputs`. The mask\n             specifies 1 to keep and 0 to mask. Defaults to `None`.\n \n\n@@ -44,12 +44,6 @@ class RNNCellWithDropout(layers.Layer, DropoutRNNCell):\n \n \n class DropoutRNNCellTest(testing.TestCase):\n-    def test_seed_tracking(self):\n-        cell = RNNCellWithDropout(3, seed=1337)\n-        self.assertEqual(len(cell.non_trainable_variables), 1)\n-        layer = layers.RNN(cell)\n-        self.assertEqual(len(layer.non_trainable_variables), 1)\n-\n     def test_basics(self):\n         self.run_layer_test(\n             layers.RNN,\n@@ -59,6 +53,5 @@ class DropoutRNNCellTest(testing.TestCase):\n             expected_output_shape=(3, 5),\n             expected_num_trainable_weights=2,\n             expected_num_non_trainable_weights=0,\n-            expected_num_non_trainable_variables=1,\n             supports_masking=True,\n         )\n\n@@ -247,6 +247,9 @@ class GRUCell(Layer, DropoutRNNCell):\n \n             hh = self.activation(x_h + recurrent_h)\n         else:\n+            if 0.0 < self.dropout < 1.0:\n+                inputs = inputs * dp_mask[0]\n+\n             # inputs projected by all gate matrices at once\n             matrix_x = ops.matmul(inputs, self.kernel)\n             if self.use_bias:\n\n@@ -262,6 +262,8 @@ class LSTMCell(Layer, DropoutRNNCell):\n             h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n             c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n         else:\n+            if 0.0 < self.dropout < 1.0:\n+                inputs = inputs * dp_mask[0]\n             z = ops.matmul(inputs, self.kernel)\n \n             z += ops.matmul(h_tm1, self.recurrent_kernel)\n\n@@ -239,8 +239,12 @@ def hard_sigmoid(x):\n \n \n class Elu(Operation):\n+    def __init__(self, alpha=1.0):\n+        super().__init__()\n+        self.alpha = alpha\n+\n     def call(self, x):\n-        return backend.nn.elu(x)\n+        return backend.nn.elu(x, alpha=self.alpha)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -249,10 +253,10 @@ class Elu(Operation):\n @keras_core_export(\n     [\"keras_core.operations.elu\", \"keras_core.operations.nn.elu\"]\n )\n-def elu(x):\n+def elu(x, alpha=1.0):\n     if any_symbolic_tensors((x,)):\n-        return Elu().symbolic_call(x)\n-    return backend.nn.elu(x)\n+        return Elu(alpha).symbolic_call(x)\n+    return backend.nn.elu(x, alpha=alpha)\n \n \n class Selu(Operation):\n\n@@ -637,6 +637,10 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.elu(x),\n             [-0.63212055, 0, 1, 2, 3],\n         )\n+        self.assertAllClose(\n+            knn.elu(x, alpha=0.5),\n+            [-0.31606027, 0, 1, 2, 3],\n+        )\n \n     def test_selu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n\n@@ -19,11 +19,7 @@ class SeedGenerator:\n             return [seed, 0]\n \n         self.state = Variable(\n-            seed_initializer,\n-            shape=(2,),\n-            dtype=\"uint32\",\n-            trainable=False,\n-            name=\"seed_generator_state\",\n+            seed_initializer, shape=(2,), dtype=\"uint32\", trainable=False\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#427c5330057f1a5e4f2f9aaa2a5e47d9ca15d1b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 153 | Files Changed: 31 | Hunks: 88 | Methods Changed: 55 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 307 | Churn Cumulative: 23470 | Contributors (this commit): 15 | Commits (past 90d): 338 | Contributors (cumulative): 115 | DMM Complexity: 0.2\n\nDIFF:\n@@ -162,13 +162,13 @@ def softmax(x, axis=-1):\n \n \n @keras_core_export(\"keras_core.activations.elu\")\n-def elu(x, alpha=1.0):\n+def elu(x):\n     \"\"\"Exponential Linear Unit.\n \n     The exponential linear unit (ELU) with `alpha > 0` is define as:\n \n     - `x` if `x > 0`\n-    - alpha * `exp(x) - 1` if `x < 0`\n+    - `exp(x) - 1` if `x < 0`\n \n     ELUs have negative values which pushes the mean of the activations\n     closer to zero.\n@@ -186,7 +186,7 @@ def elu(x, alpha=1.0):\n \n     - [Clevert et al., 2016](https://arxiv.org/abs/1511.07289)\n     \"\"\"\n-    return ops.elu(x, alpha=alpha)\n+    return ops.elu(x)\n \n \n @keras_core_export(\"keras_core.activations.selu\")\n\n@@ -56,8 +56,8 @@ def hard_sigmoid(x):\n     return jnn.hard_sigmoid(x)\n \n \n-def elu(x, alpha=1.0):\n-    return jnn.elu(x, alpha=alpha)\n+def elu(x):\n+    return jnn.elu(x)\n \n \n def selu(x):\n\n@@ -2,6 +2,8 @@ from jax import lax\n from jax import numpy as jnp\n from tensorflow import nest\n \n+from keras_core.backend.common.stateless_scope import StatelessScope\n+\n \n def rnn(\n     step_function,\n@@ -178,6 +180,10 @@ def rnn(\n \n             scan_xs = inputs\n \n+        with StatelessScope():\n+            # We must use a stateless scope because `scan` will involve\n+            # JAX tracing -- any variable update at this stage would\n+            # be a leak.\n             new_states, outputs = lax.scan(\n                 f=_step,\n                 init=initial_states,\n\n@@ -57,12 +57,8 @@ def hard_sigmoid(x):\n     return tf.clip_by_value(x, 0.0, 1.0)\n \n \n-def elu(x, alpha=1.0):\n-    res = tf.nn.elu(x)\n-    if alpha == 1:\n-        return res\n-    else:\n-        return tf.where(x > 0, res, alpha * res)\n+def elu(x):\n+    return tf.nn.elu(x)\n \n \n def selu(x):\n\n@@ -42,9 +42,10 @@ def max(x, axis=None, keepdims=False, initial=None):\n \n     # TensorFlow returns -inf by default for an empty list, but for consistency\n     # with other backends and the numpy API we want to throw in this case.\n+    size_x = size(x)\n     tf.assert_greater(\n-        size(x),\n-        tf.constant(0, dtype=tf.int64),\n+        size_x,\n+        tf.constant(0, dtype=size_x.dtype),\n         message=\"Cannot compute the max of an empty tensor.\",\n     )\n \n\n@@ -55,7 +55,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self._loss_tracker.update_state(loss)\n \n         # Compute gradients\n-        # TODO: move value conversion to TF\n         if self.trainable_weights:\n             trainable_weights = [v.value for v in self.trainable_weights]\n             gradients = tape.gradient(loss, trainable_weights)\n@@ -88,7 +87,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return y_pred\n \n     def make_train_function(self, force=False):\n-        # TODO: support tf.distribute and steps_per_execution.\n         if self.train_function is not None and not force:\n             return self.train_function\n \n@@ -131,10 +129,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.train_function = train_function\n \n     def make_test_function(self, force=False):\n-        # TODO: support tf.distribute and steps_per_execution.\n         if self.test_function is not None and not force:\n             return self.test_function\n \n+        @tf.autograph.experimental.do_not_convert\n         def one_step_on_data(data):\n             \"\"\"Runs a single test step on a batch of data.\"\"\"\n             return self.test_step(data)\n@@ -173,10 +171,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.test_function = test_function\n \n     def make_predict_function(self, force=False):\n-        # TODO: support tf.distribute and steps_per_execution.\n         if self.predict_function is not None and not force:\n             return self.predict_function\n \n+        @tf.autograph.experimental.do_not_convert\n         def one_step_on_data(data):\n             \"\"\"Runs a predict test step on a batch of data.\"\"\"\n             return self.predict_step(data)\n\n@@ -47,10 +47,6 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the uniform distribution.\n \n-    The generated values follow a uniform distribution in the range\n-    `[minval, maxval)`. The lower bound `minval` is included in the range,\n-    while the upper bound `maxval` is excluded.\n-\n     Args:\n         shape: The shape of the random values to generate.\n         minval: Floats, defaults to 0. Lower bound of the range of\n@@ -81,10 +77,6 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the truncated normal distribution.\n \n-    The values are drawn from a normal distribution with specified mean and\n-    standard deviation, discarding and re-drawing any samples that are more\n-    than two standard deviations from the mean.\n-\n     Args:\n         shape: The shape of the random values to generate.\n         mean: Floats, defaults to 0. Mean of the random values to generate.\n@@ -103,15 +95,14 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n-    # Take a larger standard normal dist, discard values outside 2 * stddev\n-    # Offset by mean and stddev\n-    x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n-    valid = (x > -2) & (x < 2)\n-    indexes = valid.max(-1, keepdim=True)[1]\n-    trunc_x = torch.empty(shape)\n-    trunc_x.data.copy_(x.gather(-1, indexes).squeeze(-1))\n-    trunc_x.data.mul_(stddev).add_(mean)\n-    return trunc_x\n+    x = torch.empty(shape)\n+    # TODO: setting seed globally via `manual_seed` might create side effects.\n+    if seed is not None:\n+        seed_val, _ = draw_seed(seed)\n+        torch.manual_seed(int(seed_val))\n+    return torch.nn.init.trunc_normal_(\n+        x, mean=mean, std=stddev, a=-stddev * 2, b=stddev * 2\n+    )\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n\n@@ -8,9 +8,11 @@ class Activation(Layer):\n     \"\"\"Applies an activation function to an output.\n \n     Args:\n-        activation: Activation function. It could be a callable, or the name of\n-            an activation from the `keras_core.activations` namespace.\n-        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+        activation: Activation function. It could be\n+            a callable, or the name of an activation\n+            from the `keras_core.activations` namespace.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n \n     Example:\n \n\n@@ -10,22 +10,21 @@ class ELU(Layer):\n     Formula:\n \n     ```\n-    f(x) = alpha * (exp(x) - 1.) for x < 0\n+    f(x) = (exp(x) - 1.) for x < 0\n     f(x) = x for x >= 0\n     ```\n \n     Args:\n-        alpha: float, slope of negative section. Defaults to 1.0.\n-        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, alpha=1.0, **kwargs):\n+    def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n-        self.alpha = alpha\n         self.supports_masking = True\n \n     def call(self, inputs):\n-        return activations.elu(inputs, alpha=self.alpha)\n+        return activations.elu(inputs)\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n\n@@ -2,7 +2,6 @@ import numpy as np\n \n from keras_core import testing\n from keras_core.layers.activations import elu\n-import tensorflow as tf\n \n \n class ELUTest(testing.TestCase):\n@@ -18,12 +17,7 @@ class ELUTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    def test_correctness(self):\n-        x = np.random.random((2, 2, 5))\n+        x = np.random.random((2, 5))\n         elu_layer = elu.ELU()\n-        tf_elu_layer = tf.keras.layers.ELU()\n-        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n-\n-        elu_layer = elu.ELU(alpha=0.7)\n-        tf_elu_layer = tf.keras.layers.ELU(alpha=0.7)\n-        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n+        result = elu_layer(x[np.newaxis, :])[0]\n+        self.assertAllClose(result, x, rtol=1e-05)\n\n@@ -13,37 +13,46 @@ class PReLU(Layer):\n \n     Formula:\n     ``` python\n-    f(x) = alpha * x for x < 0\n+    f(x) = negative_slope * x for x < 0\n     f(x) = x for x >= 0\n     ```\n-    where `alpha` is a learned array with the same shape as x.\n+    where `negative_slope` is a learned array with the same shape as x.\n \n     Args:\n-        alpha_initializer: Initializer function for the weights.\n-        alpha_regularizer: Regularizer for the weights.\n-        alpha_constraint: Constraint for the weights.\n-        shared_axes: The axes along which to share learnable parameters for the\n-            activation function. For example, if the incoming feature maps are\n-            from a 2D convolution with output shape\n-            `(batch, height, width, channels)`, and you wish to share parameters\n-            across space so that each filter only has one set of parameters,\n+        negative_slope_initializer: Initializer function for the weights.\n+        negative_slope_regularizer: Regularizer for the weights.\n+        negative_slope_constraint: Constraint for the weights.\n+        shared_axes: The axes along which to share learnable\n+            parameters for the activation function.\n+            For example, if the incoming feature maps\n+            are from a 2D convolution\n+            with output shape `(batch, height, width, channels)`,\n+            and you wish to share parameters across space\n+            so that each filter only has one set of parameters,\n             set `shared_axes=[1, 2]`.\n-        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n     \"\"\"\n \n     def __init__(\n         self,\n-        alpha_initializer=\"Zeros\",\n-        alpha_regularizer=None,\n-        alpha_constraint=None,\n+        negative_slope_initializer=\"Zeros\",\n+        negative_slope_regularizer=None,\n+        negative_slope_constraint=None,\n         shared_axes=None,\n         **kwargs\n     ):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n-        self.alpha_initializer = initializers.get(alpha_initializer)\n-        self.alpha_regularizer = regularizers.get(alpha_regularizer)\n-        self.alpha_constraint = constraints.get(alpha_constraint)\n+        self.negative_slope_initializer = initializers.get(\n+            negative_slope_initializer\n+        )\n+        self.negative_slope_regularizer = regularizers.get(\n+            negative_slope_regularizer\n+        )\n+        self.negative_slope_constraint = constraints.get(\n+            negative_slope_constraint\n+        )\n         if shared_axes is None:\n             self.shared_axes = None\n         elif not isinstance(shared_axes, (list, tuple)):\n@@ -56,12 +65,12 @@ class PReLU(Layer):\n         if self.shared_axes is not None:\n             for i in self.shared_axes:\n                 param_shape[i - 1] = 1\n-        self.alpha = self.add_weight(\n+        self.negative_slope = self.add_weight(\n             shape=param_shape,\n-            name=\"alpha\",\n-            initializer=self.alpha_initializer,\n-            regularizer=self.alpha_regularizer,\n-            constraint=self.alpha_constraint,\n+            name=\"negative_slope\",\n+            initializer=self.negative_slope_initializer,\n+            regularizer=self.negative_slope_regularizer,\n+            constraint=self.negative_slope_constraint,\n         )\n         # Set input spec\n         axes = {}\n@@ -74,21 +83,21 @@ class PReLU(Layer):\n \n     def call(self, inputs):\n         pos = activations.relu(inputs)\n-        neg = -self.alpha * activations.relu(-inputs)\n+        neg = -self.negative_slope * activations.relu(-inputs)\n         return pos + neg\n \n     def get_config(self):\n         config = super().get_config()\n         config.update(\n             {\n-                \"alpha_initializer\": initializers.serialize(\n-                    self.alpha_initializer\n+                \"negative_slope_initializer\": initializers.serialize(\n+                    self.negative_slope_initializer\n                 ),\n-                \"alpha_regularizer\": regularizers.serialize(\n-                    self.alpha_regularizer\n+                \"negative_slope_regularizer\": regularizers.serialize(\n+                    self.negative_slope_regularizer\n                 ),\n-                \"alpha_constraint\": constraints.serialize(\n-                    self.alpha_constraint\n+                \"negative_slope_constraint\": constraints.serialize(\n+                    self.negative_slope_constraint\n                 ),\n                 \"shared_axes\": self.shared_axes,\n             }\n\n@@ -2,7 +2,6 @@ import numpy as np\n \n from keras_core import testing\n from keras_core.layers.activations import prelu\n-import tensorflow as tf\n \n \n class PReLUTest(testing.TestCase):\n@@ -10,9 +9,9 @@ class PReLUTest(testing.TestCase):\n         self.run_layer_test(\n             prelu.PReLU,\n             init_kwargs={\n-                \"alpha_initializer\": \"zeros\",\n-                \"alpha_regularizer\": \"L1\",\n-                \"alpha_constraint\": \"MaxNorm\",\n+                \"negative_slope_initializer\": \"zeros\",\n+                \"negative_slope_regularizer\": \"L1\",\n+                \"negative_slope_constraint\": \"MaxNorm\",\n                 \"shared_axes\": 1,\n             },\n             input_shape=(2, 3, 4),\n@@ -20,25 +19,15 @@ class PReLUTest(testing.TestCase):\n         )\n \n     def test_prelu_correctness(self):\n-        inputs = np.random.randn(2, 10, 5, 3)\n         prelu_layer = prelu.PReLU(\n-            alpha_initializer=\"glorot_uniform\",\n-            alpha_regularizer=\"l1\",\n-            alpha_constraint=\"non_neg\",\n-            shared_axes=(1, 2),\n+            negative_slope_initializer=\"glorot_uniform\",\n+            negative_slope_regularizer=\"l1\",\n+            negative_slope_constraint=\"non_neg\",\n+            shared_axes=None,\n         )\n-        tf_prelu_layer = tf.keras.layers.PReLU(\n-            alpha_initializer=\"glorot_uniform\",\n-            alpha_regularizer=\"l1\",\n-            alpha_constraint=\"non_neg\",\n-            shared_axes=(1, 2),\n-        )\n-\n-        prelu_layer.build(inputs.shape)\n-        tf_prelu_layer.build(inputs.shape)\n-\n-        weights = np.random.random((1, 1, 3))\n-        prelu_layer.alpha.assign(weights)\n-        tf_prelu_layer.alpha.assign(weights)\n-\n-        self.assertAllClose(prelu_layer(inputs), tf_prelu_layer(inputs))\n+        test_input = np.random.randn(10, 5)\n+        result = prelu_layer(test_input)\n+        expected_output = np.maximum(\n+            0, test_input\n+        ) + prelu_layer.negative_slope.numpy() * np.minimum(0, test_input)\n+        self.assertAllClose(result, expected_output)\n\n@@ -17,11 +17,7 @@ class ReLU(Layer):\n \n     Example:\n     ``` python\n-    relu_layer = keras_core.layers.activations.ReLU(\n-        max_value=10,\n-        negative_slope=0.5,\n-        threshold=0,\n-    )\n+    relu_layer = relu.ReLU(max_value=10, negative_slope=0.5, threshold=0)\n     input = np.array([-10, -5, 0.0, 5, 10])\n     result = relu_layer(input)\n     # result = [-5. , -2.5,  0. ,  5. , 10.]\n@@ -30,10 +26,12 @@ class ReLU(Layer):\n     Args:\n         max_value: Float >= 0. Maximum activation value. None means unlimited.\n             Defaults to `None`.\n-        negative_slope: Float >= 0. Negative slope coefficient. Defaults to 0.0.\n+        negative_slope: Float >= 0. Negative slope coefficient.\n+            Defaults to 0.0.\n         threshold: Float >= 0. Threshold value for thresholded activation.\n             Defaults to 0.0.\n-        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n     \"\"\"\n \n     def __init__(\n\n@@ -15,7 +15,7 @@ class Softmax(Layer):\n     ```\n \n     Example:\n-    >>>softmax_layer = keras_core.layers.activations.Softmax()\n+    >>>softmax_layer = Softmax()\n     >>>input = np.array([1.0, 2.0, 1.0])\n     >>>result = softmax_layer(input)\n     [0.21194157, 0.5761169, 0.21194157]\n@@ -24,10 +24,11 @@ class Softmax(Layer):\n     Args:\n         axis: Integer, or list of Integers, axis along which the softmax\n             normalization is applied.\n-        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n \n     Call arguments:\n-        inputs: The inputs (logits) to the softmax layer.\n+        inputs: The inputs, or logits to the softmax layer.\n         mask: A boolean mask of the same shape as `inputs`. The mask\n             specifies 1 to keep and 0 to mask. Defaults to `None`.\n \n\n@@ -20,6 +20,7 @@ class AdditiveAttentionTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n         # Sale.\n         self.run_layer_test(\n@@ -35,6 +36,7 @@ class AdditiveAttentionTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n \n     def test_attention_correctness(self):\n\n@@ -20,6 +20,7 @@ class AttentionTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n         # Sale and concat.\n         self.run_layer_test(\n@@ -36,6 +37,7 @@ class AttentionTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n \n     def test_attention_correctness(self):\n\n@@ -21,6 +21,7 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n \n         self.run_layer_test(\n@@ -39,6 +40,7 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n \n     @parameterized.named_parameters(\n@@ -78,6 +80,7 @@ class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n+            run_training_check=False,\n         )\n \n     @parameterized.named_parameters(\n\n@@ -24,8 +24,8 @@ class LambdaTest(testing.TestCase):\n         self.run_layer_test(\n             layers.Lambda,\n             init_kwargs={\"function\": ops.square, \"mask\": ops.ones((2, 3))},\n-            input_shape=(2, 3),\n-            expected_output_shape=(2, 3),\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 4),\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n             expected_num_seed_generators=0,\n\n@@ -706,7 +706,9 @@ class Layer(Operation):\n                 for x in scope.losses:\n                     if x in self._losses:\n                         scope.losses.remove(x)\n-        self._losses = []\n+        self._losses.clear()\n+        for layer in self._layers:\n+            layer._clear_losses()\n \n     def add_metric(self):\n         # Permanently disabled\n\n@@ -49,8 +49,8 @@ def batch_dot(x, y, axes=None):\n         rank is 1, we reshape it to `(batch_size, 1)`.\n     \"\"\"\n \n-    x_shape = tuple(ops.shape(x))\n-    y_shape = tuple(ops.shape(y))\n+    x_shape = x.shape\n+    y_shape = y.shape\n \n     x_ndim = len(x_shape)\n     y_ndim = len(y_shape)\n@@ -301,8 +301,8 @@ class Dot(Merge):\n         if isinstance(self.axes, int):\n             if self.axes < 0:\n                 axes = [\n-                    self.axes % x1.ndim,\n-                    self.axes % x2.ndim,\n+                    self.axes % len(x1.shape),\n+                    self.axes % len(x2.shape),\n                 ]\n             else:\n                 axes = [self.axes] * 2\n@@ -310,7 +310,7 @@ class Dot(Merge):\n             axes = []\n             for i in range(len(self.axes)):\n                 if self.axes[i] < 0:\n-                    axes.append(self.axes[i] % inputs[i].ndim)\n+                    axes.append(self.axes[i] % len(inputs[i].shape))\n                 else:\n                     axes.append(self.axes[i])\n \n\n@@ -146,19 +146,18 @@ class GroupNormalization(Layer):\n         super().build(input_shape)\n \n     def call(self, inputs):\n-        input_shape = inputs.shape\n-\n         reshaped_inputs = self._reshape_into_groups(inputs)\n-\n         normalized_inputs = self._apply_normalization(\n-            reshaped_inputs, input_shape\n+            reshaped_inputs, inputs.shape\n         )\n-\n-        return ops.reshape(normalized_inputs, input_shape)\n+        return ops.reshape(normalized_inputs, ops.shape(inputs))\n \n     def _reshape_into_groups(self, inputs):\n-        input_shape = inputs.shape\n-        group_shape = [input_shape[i] for i in range(len(input_shape))]\n+        input_shape = ops.shape(inputs)\n+        group_shape = list(inputs.shape)\n+        for i, e in enumerate(group_shape):\n+            if e is None:\n+                group_shape[i] = input_shape[i]\n \n         group_shape[self.axis] = input_shape[self.axis] // self.groups\n         group_shape.insert(self.axis, self.groups)\n\n@@ -215,7 +215,7 @@ class LayerNormalization(Layer):\n         outputs = ops.cast(outputs, input_dtype)\n \n         # If some components of the shape got lost due to adjustments, fix that.\n-        outputs = ops.reshape(outputs, input_shape)\n+        outputs = ops.reshape(outputs, ops.shape(inputs))\n \n         return outputs\n \n\n@@ -19,6 +19,7 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            run_training_check=False,\n         )\n         self.run_layer_test(\n             layers.HashedCrossing,\n@@ -30,6 +31,7 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            run_training_check=False,\n         )\n \n     def test_correctness(self):\n\n@@ -14,6 +14,7 @@ class RandomCropTest(testing.TestCase):\n             },\n             input_shape=(2, 3, 4),\n             supports_masking=False,\n+            run_training_check=False,\n         )\n \n     def test_random_crop_full(self):\n@@ -34,6 +35,7 @@ class RandomCropTest(testing.TestCase):\n             input_shape=(12, 8, 16, 3),\n             expected_output_shape=(12, 8, 8, 3),\n             supports_masking=False,\n+            run_training_check=False,\n         )\n \n     def test_predicting_with_longer_height(self):\n@@ -46,6 +48,7 @@ class RandomCropTest(testing.TestCase):\n             input_shape=(12, 8, 16, 3),\n             expected_output_shape=(12, 10, 8, 3),\n             supports_masking=False,\n+            run_training_check=False,\n         )\n \n     def test_predicting_with_longer_width(self):\n@@ -58,4 +61,5 @@ class RandomCropTest(testing.TestCase):\n             input_shape=(12, 8, 16, 3),\n             expected_output_shape=(12, 8, 18, 3),\n             supports_masking=False,\n+            run_training_check=False,\n         )\n\n@@ -44,6 +44,12 @@ class RNNCellWithDropout(layers.Layer, DropoutRNNCell):\n \n \n class DropoutRNNCellTest(testing.TestCase):\n+    def test_seed_tracking(self):\n+        cell = RNNCellWithDropout(3, seed=1337)\n+        self.assertEqual(len(cell.non_trainable_variables), 1)\n+        layer = layers.RNN(cell)\n+        self.assertEqual(len(layer.non_trainable_variables), 1)\n+\n     def test_basics(self):\n         self.run_layer_test(\n             layers.RNN,\n@@ -53,5 +59,6 @@ class DropoutRNNCellTest(testing.TestCase):\n             expected_output_shape=(3, 5),\n             expected_num_trainable_weights=2,\n             expected_num_non_trainable_weights=0,\n+            expected_num_non_trainable_variables=1,\n             supports_masking=True,\n         )\n\n@@ -247,9 +247,6 @@ class GRUCell(Layer, DropoutRNNCell):\n \n             hh = self.activation(x_h + recurrent_h)\n         else:\n-            if 0.0 < self.dropout < 1.0:\n-                inputs = inputs * dp_mask[0]\n-\n             # inputs projected by all gate matrices at once\n             matrix_x = ops.matmul(inputs, self.kernel)\n             if self.use_bias:\n\n@@ -262,8 +262,6 @@ class LSTMCell(Layer, DropoutRNNCell):\n             h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n             c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n         else:\n-            if 0.0 < self.dropout < 1.0:\n-                inputs = inputs * dp_mask[0]\n             z = ops.matmul(inputs, self.kernel)\n \n             z += ops.matmul(h_tm1, self.recurrent_kernel)\n\n@@ -239,12 +239,8 @@ def hard_sigmoid(x):\n \n \n class Elu(Operation):\n-    def __init__(self, alpha=1.0):\n-        super().__init__()\n-        self.alpha = alpha\n-\n     def call(self, x):\n-        return backend.nn.elu(x, alpha=self.alpha)\n+        return backend.nn.elu(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -253,10 +249,10 @@ class Elu(Operation):\n @keras_core_export(\n     [\"keras_core.operations.elu\", \"keras_core.operations.nn.elu\"]\n )\n-def elu(x, alpha=1.0):\n+def elu(x):\n     if any_symbolic_tensors((x,)):\n-        return Elu(alpha).symbolic_call(x)\n-    return backend.nn.elu(x, alpha=alpha)\n+        return Elu().symbolic_call(x)\n+    return backend.nn.elu(x)\n \n \n class Selu(Operation):\n\n@@ -637,10 +637,6 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.elu(x),\n             [-0.63212055, 0, 1, 2, 3],\n         )\n-        self.assertAllClose(\n-            knn.elu(x, alpha=0.5),\n-            [-0.31606027, 0, 1, 2, 3],\n-        )\n \n     def test_selu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n\n@@ -19,7 +19,11 @@ class SeedGenerator:\n             return [seed, 0]\n \n         self.state = Variable(\n-            seed_initializer, shape=(2,), dtype=\"uint32\", trainable=False\n+            seed_initializer,\n+            shape=(2,),\n+            dtype=\"uint32\",\n+            trainable=False,\n+            name=\"seed_generator_state\",\n         )\n \n \n\n@@ -106,6 +106,7 @@ class TestCase(unittest.TestCase):\n         supports_masking=None,\n         expected_mask_shape=None,\n         custom_objects=None,\n+        run_training_check=True,\n     ):\n         \"\"\"Run basic checks on a layer.\n \n@@ -140,6 +141,8 @@ class TestCase(unittest.TestCase):\n                 returned by compute_mask() (only supports 1 shape).\n             custom_objects: Dict of any custom objects to be\n                 considered during deserialization.\n+            run_training_check: Whether to attempt to train the layer\n+                (if an input shape or input data was provided).\n         \"\"\"\n         if input_shape is not None and input_data is not None:\n             raise ValueError(\n@@ -271,7 +274,9 @@ class TestCase(unittest.TestCase):\n \n             model = TestModel(layer)\n             model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=False)\n-            model.fit(np.array(input_data), np.array(output_data))\n+            input_data = nest.map_structure(lambda x: np.array(x), input_data)\n+            output_data = nest.map_structure(lambda x: np.array(x), output_data)\n+            model.fit(input_data, output_data, verbose=0)\n \n         # Build test.\n         if input_shape is not None:\n@@ -309,8 +314,8 @@ class TestCase(unittest.TestCase):\n                 output_data = layer(input_data, **call_kwargs)\n             run_output_asserts(layer, output_data, eager=True)\n \n-            # # Compiled training step - TODO\n-            # run_training_step(layer, input_data, output_data)\n+            if run_training_check:\n+                run_training_step(layer, input_data, output_data)\n \n \n def create_keras_tensors(input_shape, dtype):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
