{"custom_id": "scrapy#87b97f07a753ac8393d9d9dc8f4041ad7586ec4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,10 @@\n-from django import template\n-from django_website.apps.blog.models import Entry\n import datetime\n \n+from django import template\n+\n+from blog.models import Entry\n+\n+\n class LatestBlogEntriesNode(template.Node):\n     def __init__(self, num, varname):\n         self.num, self.varname = num, varname\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06effebd32301488737c46f7df1d636246344727", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 11 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 39 | Churn Cumulative: 128 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,8 @@ class Article(models.Model):\n     text = models.TextField(_(\"text\"), core=True, help_text=REST_HELP_TEXT)\n     main = models.BooleanField(_(\"main\"), core=True, blank=False,\n                                default=False, help_text=MAIN_HELP_TEXT)\n-    order = models.IntegerField(_(\"order\"), core=True, blank=False, default=0)\n+    position = models.IntegerField(_(\"position\"), core=True, blank=False,\n+                                   default=0)\n \n \n     # automatic dates\n@@ -28,12 +29,12 @@ class Article(models.Model):\n     updated = models.DateTimeField(core=True, editable=False)\n \n \n-    def order_up(self):\n-        self.order += 1\n+    def position_up(self):\n+        self.position += 1\n         self.save()\n \n-    def order_down(self):\n-        self.order -= 1\n+    def position_down(self):\n+        self.position -= 1\n         self.save()\n \n     def save(self):\n@@ -47,18 +48,18 @@ class Article(models.Model):\n         return self.title\n     \n     # ugly, but django-admin isn't very versatile right now\n-    def order_link(self):\n-        return _(\"%(order)s (<a href='/article/%(id)s/order/up/'>Up</a>\" \\\n-               \" | <a href='/article/%(id)s/order/down/'>Down</a>)\") % \\\n-               { \"order\": self.order, \"id\": self.id }\n-    order_link.short_description = u\"order\"\n-    order_link.allow_tags = True\n+    def position_link(self):\n+        return _(\"%(position)s (<a href='/article/%(id)s/position/up/'>Up</a>\" \\\n+               \" | <a href='/article/%(id)s/position/down/'>Down</a>)\") % \\\n+               { \"position\": self.position, \"id\": self.id }\n+    position_link.short_description = u\"position\"\n+    position_link.allow_tags = True\n     \n     class Admin:\n-        list_display = (\"title\", \"main\", \"order_link\", \"updated\")\n+        list_display = (\"title\", \"main\", \"position_link\", \"updated\")\n         list_filter = (\"main\", \"created\")\n \n     class Meta:\n         verbose_name = _(\"article\")\n         verbose_name_plural = _(\"articles\")\n-        ordering = [ \"-order\", ]\n+        ordering = [ \"-position\", ]\n\n@@ -4,6 +4,6 @@ from article.views import *\n \n \n urlpatterns = patterns('',\n-    (r\"^(?P<article_id>\\d+)/order/up/$\", order_up),\n-    (r\"^(?P<article_id>\\d+)/order/down/$\", order_down),\n+    (r\"^(?P<article_id>\\d+)/position/up/$\", position_up),\n+    (r\"^(?P<article_id>\\d+)/position/down/$\", position_down),\n )\n\n@@ -4,13 +4,13 @@ from django.http import HttpResponseRedirect\n from article.models import Article\n \n \n-def order_up(request, article_id):\n+def position_up(request, article_id):\n     article = get_object_or_404(Article, pk=article_id)\n-    article.order_up()\n+    article.position_up()\n     return HttpResponseRedirect(\"/admin/article/article/\")\n \n \n-def order_down(request, article_id):\n+def position_down(request, article_id):\n     article = get_object_or_404(Article, pk=article_id)\n-    article.order_down()\n+    article.position_down()\n     return HttpResponseRedirect(\"/admin/article/article/\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5fe965315920cc10664b3e0ba52bc50bf717374", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ urlpatterns = patterns('',\n     (r\"^$\", direct_to_template, { \"template\": \"home.html\" }),\n     (r\"^article/\", include(\"article.urls\")),\n     (r\"^download/\", include(\"download.urls\")),\n+    (r\"^link/\", include(\"link.urls\")),\n     (r\"^weblog/\", include(\"blog.urls\")),\n \n     (r\"^admin/\", include(\"django.contrib.admin.urls\")),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ac926c5fdba9a41cf7a79d7381a341e748cd837", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 31 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 31 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c3cdbbb1e1c6d455bf96114d02cbf4a867a2af0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 8 | Churn Cumulative: 8 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,6 +67,14 @@ class GroupLink(models.Model):\n     created = models.DateTimeField(core=True, editable=False)\n     updated = models.DateTimeField(core=True, editable=False)\n \n+    @property\n+    def address(self):\n+        return self.link.address\n+\n+    @property\n+    def text(self):\n+        return self.link.text\n+\n     def position_up(self):\n         self.position += 1\n         self.save()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cf813f5462892cd2a515ad3bc1e6835c82161f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 31 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 31 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40c9d60db4f541ff0517f77aa348e493a036bed1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 24 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 24 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f5bb2a7c7e45ae22b185427d67e7bbb1f18e28c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#46300ec596ca16116b785cf0ef6985483226edc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 34 | Files Changed: 13 | Hunks: 25 | Methods Changed: 0 | Complexity Δ (Sum/Max): 36/11 | Churn Δ: 65 | Churn Cumulative: 65 | Contributors (this commit): 1 | Commits (past 90d): 26 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n from django import template\n \n-from lib.templatetags import *\n+from scrapyorg.lib.templatetags import *\n \n-from article.models import Article\n+from scrapyorg.article.models import Article\n \n \n register = template.Library()\n\n@@ -1,6 +1,6 @@\n from django.conf.urls.defaults import *\n \n-from article.views import *\n+from scrapyorg.article.views import *\n \n \n urlpatterns = patterns('',\n\n@@ -1,7 +1,7 @@\n from django.shortcuts import get_object_or_404\n from django.http import HttpResponseRedirect\n \n-from article.models import Article\n+from scrapyorg.article.models import Article\n \n \n def position_up(request, article_id):\n\n@@ -2,7 +2,7 @@ import datetime\n \n from django import template\n \n-from blog.models import Entry\n+from scrapyorg.blog.models import Entry\n \n \n class LatestBlogEntriesNode(template.Node):\n\n@@ -1,5 +1,5 @@\n from django.conf.urls.defaults import *\n-from models import Entry # relative import\n+from scrapyorg.blog.models import Entry # relative import\n \n info_dict = {\n     'queryset': Entry.objects.all(),\n\n@@ -1,8 +1,8 @@\n from django import template\n \n-from lib.templatetags import *\n+from scrapyorg.lib.templatetags import *\n \n-from download.models import DownloadLink\n+from scrapyorg.download.models import DownloadLink\n \n \n register = template.Library()\n\n@@ -1,6 +1,6 @@\n from django.conf.urls.defaults import *\n \n-from download.views import *\n+from scrapyorg.download.views import *\n \n \n urlpatterns = patterns('',\n\n@@ -1,7 +1,7 @@\n from django.shortcuts import get_object_or_404\n from django.http import HttpResponseRedirect\n \n-from download.models import DownloadLink\n+from scrapyorg.download.models import DownloadLink\n \n \n def toggle_public(request, link_id):\n\n@@ -1,8 +1,8 @@\n from django import template\n \n-from lib.templatetags import *\n+from scrapyorg.lib.templatetags import *\n \n-from link.models import GroupLink\n+from scrapyorg.link.models import GroupLink\n \n \n register = template.Library()\n\n@@ -1,6 +1,6 @@\n from django.conf.urls.defaults import *\n \n-from link.views import *\n+from scrapyorg.link.views import *\n \n \n urlpatterns = patterns('',\n\n@@ -1,7 +1,7 @@\n from django.shortcuts import get_object_or_404\n from django.http import HttpResponseRedirect\n \n-from link.models import GroupLink\n+from scrapyorg.link.models import GroupLink\n \n \n def position_up(request, grouplink_id):\n\n@@ -1,11 +1,10 @@\n # -*- coding: utf-8 -*-\n # Django settings for scrapy project.\n \n-from os.path import abspath, dirname, basename, join\n-import sys\n+#from os.path import abspath, dirname, basename, join\n+import os\n \n-PROJECT_ABSOLUTE_DIR = dirname(abspath(__file__))\n-PROJECT_NAME = basename(PROJECT_ABSOLUTE_DIR)\n+PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n \n ADMINS = (\n     # ('Your Name', 'your_email@domain.com'),\n@@ -13,6 +12,8 @@ ADMINS = (\n \n MANAGERS = ADMINS\n \n+DEBUG=False\n+\n DEFAULT_CHARSET = \"utf-8\"\n \n # Local time zone for this installation. Choices can be found here:\n@@ -34,7 +35,7 @@ SITE_ID = 1\n \n # Absolute path to the directory that holds media.\n # Example: \"/home/media/media.lawrence.com/\"\n-MEDIA_ROOT = join(PROJECT_ABSOLUTE_DIR, \"static\")\n+MEDIA_ROOT = os.path.join(PROJECT_ROOT, \"static\")\n \n # URL that handles the media served from MEDIA_ROOT. Make sure to use a\n # trailing slash if there is a path component (optional in other cases).\n@@ -61,14 +62,13 @@ MIDDLEWARE_CLASSES = (\n     'django.contrib.sessions.middleware.SessionMiddleware',\n     'django.contrib.auth.middleware.AuthenticationMiddleware',\n     'django.middleware.doc.XViewMiddleware',\n-    #'%s.middleware.threadlocals.ThreadLocals' % PROJECT_NAME,\n     'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware',\n )\n \n-ROOT_URLCONF = '%s.urls' % PROJECT_NAME\n+ROOT_URLCONF = 'scrapyorg.urls'\n \n TEMPLATE_DIRS = (\n-    join(PROJECT_ABSOLUTE_DIR, \"templates\"),\n+    os.path.join(PROJECT_ROOT, \"templates\"),\n )\n \n INSTALLED_APPS = (\n@@ -79,15 +79,12 @@ INSTALLED_APPS = (\n     'django.contrib.admin',\n     'django.contrib.markup',\n     'django.contrib.flatpages',\n-    'link',\n-    'article',\n-    'download',\n-    'blog',\n+    'scrapyorg.link',\n+    'scrapyorg.article',\n+    'scrapyorg.download',\n+    'scrapyorg.blog',\n )\n \n-# Add apps/ dir to python path.\n-sys.path.append(join(PROJECT_ABSOLUTE_DIR, \"apps\"))\n-\n # Override previous settings with values in local_settings.py settings file.\n try:\n     from local_settings import *\n\n@@ -5,10 +5,10 @@ from django.conf import settings\n \n urlpatterns = patterns('',\n     (r\"^$\", direct_to_template, { \"template\": \"home.html\" }),\n-    (r\"^article/\", include(\"article.urls\")),\n-    (r\"^download/\", include(\"download.urls\")),\n-    (r\"^link/\", include(\"link.urls\")),\n-    (r\"^weblog/\", include(\"blog.urls\")),\n+    (r\"^article/\", include(\"scrapyorg.article.urls\")),\n+    (r\"^download/\", include(\"scrapyorg.download.urls\")),\n+    (r\"^link/\", include(\"scrapyorg.link.urls\")),\n+    (r\"^weblog/\", include(\"scrapyorg.blog.urls\")),\n \n     (r\"^admin/\", include(\"django.contrib.admin.urls\")),\n )\n@@ -16,5 +16,5 @@ urlpatterns = patterns('',\n \n if settings.DEBUG: # devel\n     urlpatterns += patterns('',         \n-        (r\"^site-media/(?P<path>.*)$\", \"django.views.static.serve\", { \"document_root\": settings.MEDIA_ROOT }),\n+        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:], 'django.views.static.serve', {'document_root': settings.MEDIA_ROOT}),\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1638ef2b7ae9350a92d99582f7107783078d17af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 137 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 137 | Churn Cumulative: 274 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,26 +0,0 @@\n-import os\n-import unittest\n-\n-from decobot.utils.link_extraction import extract_urls\n-\n-class LinkExtractionTest(unittest.TestCase):\n-    def setUp(self):\n-        self.datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"sample_data\", \"link_extraction\")\n-\n-    def test_extract_urls(self):\n-        text = open(os.path.join(self.datadir, \"extract_urls.html\")).read()\n-        links_expected = {'/products/eco-mattress-protector.html': 'Eco Mattress Protector',\n-                          '/products.html': 'Our Products', \n-                          '/products/eco-duvet.html': 'Eco Duvet', \n-                          '/terms-and-conditions.html': 'Terms &amp; Conditions', \n-                          '/returns-policy.html': 'Returns Policy', \n-                          '/products/eco-pillow.html': 'Eco Pillow', \n-                          '/': 'Home', \n-                          '/privacy-policy.html': 'Privacy Policy', \n-                          '/eco-friendly.html': 'Eco-Friendly', \n-                          '/faqs.html': 'FAQs', \n-                          '/contact-us.html': 'Contact Us'}\n-        self.assertEqual(links_expected, extract_urls(text))\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,111 +0,0 @@\n-# -*- coding: utf8 -*-\n-import unittest\n-from decimal import Decimal\n-\n-from decobot.utils.text_extraction import clean_markup, unquote_html, read_decimal, read_price\n-\n-class TextExtractionTest(unittest.TestCase):\n-    def test_clean_markup(self):\n-        \n-        #with html tags\n-        \n-        html = u\" <p> \\xa0 <!-- comment --> <span>small</span> test </p> \"\n-\n-        self.assertEqual(clean_markup(html),\n-                         u'small test')\n-        self.assertEqual(clean_markup(html, remove_tags=False, remove_root=False, remove_spaces=False, strip=False),\n-                         u' <p> \\xa0 <!-- comment --> <span>small</span> test </p> ')\n-        self.assertEqual(clean_markup(html, remove_tags=False, remove_root=True, strip=False),\n-                         u' <!-- comment --> <span>small</span> test ')\n-        self.assertEqual(clean_markup(html, remove_tags=False, remove_root=False, remove_spaces=False, strip=True),\n-                         u'<p> \\xa0 <!-- comment --> <span>small</span> test </p>')\n-        self.assertEqual(clean_markup(html, remove_tags=False, remove_spaces=True, strip=False),\n-                         u' <!-- comment --> <span>small</span> test ')\n-        self.assertEqual(clean_markup(html, remove_tags=True, remove_spaces=False, strip=False),\n-                         u'   \\xa0    small  test   ')\n-        self.assertEqual(clean_markup(html, remove_tags=True, remove_spaces=True, strip=False),\n-                         u' small test ')\n-        self.assertEqual(clean_markup(html, remove_tags=True, remove_root=True, remove_spaces=True, strip=True),\n-                         u'small test')\n-                         \n-        #with xml tags\n-                \n-        self.assertEqual(clean_markup(u'<description/>', xml_doc=True),\n-                         u'')\n-                         \n-        self.assertEqual(clean_markup(u'<description/>', xml_doc=True, remove_tags=False),\n-                         u'<description/>')\n-\n-        self.assertEqual(clean_markup(u'<value>http://stephen.digivate2:8080/uploads/suppliers/4/11931_Picture 001.jpg</value>', xml_doc=True),\n-                         u'http://stephen.digivate2:8080/uploads/suppliers/4/11931_Picture 001.jpg')\n-        self.assertEqual(clean_markup(u'<material> wood </material>', xml_doc=True),\n-                         u'wood')\n-        self.assertEqual(clean_markup(u'<material> wood </material>', strip=False, xml_doc=True),\n-                         u' wood ')\n-        self.assertEqual(clean_markup(u'<material><value>wood</value><value>iron</value></material>', xml_doc=True),\n-                        u'wood iron')\n-                        \n-        self.assertEqual(clean_markup(u'<data><description>hi</description><![CDATA[This is a <tag> inside cdata </tag> <g>]]><nothing/><![CDATA[Another CDATA with tags <inside>]]></data>', xml_doc=True),\n-                        u'hi This is a inside cdata Another CDATA with tags')\n-                        \n-        self.assertEqual(clean_markup(u'<data><description>hi</description><![CDATA[This is a <tag> inside cdata </tag> <g>]]><nothing/><![CDATA[Another CDATA with tags <inside>]]></data>', xml_doc=True, remove_tags=False),\n-                        u'<description>hi</description>This is a <tag> inside cdata </tag> <g><nothing/>Another CDATA with tags <inside>')\n-                        \n-        self.assertEqual(clean_markup(u'<data><description>hi</description><![CDATA[This is a <tag> inside cdata </tag> <g>]]><nothing/><![CDATA[Another CDATA with tags <inside>]]></data>', remove_cdata=False, xml_doc=True,remove_tags=False),\n-                        u'<description>hi</description><![CDATA[This is a <tag> inside cdata </tag> <g>]]><nothing/><![CDATA[Another CDATA with tags <inside>]]>')\n-                        \n-        self.assertEqual(clean_markup(u'<data><description>hi</description><![CDATA[This is a <tag> inside cdata </tag> <g>]]><nothing/><![CDATA[Another CDATA with tags <inside>]]></data>', remove_cdata=False, xml_doc=True),\n-                        u'hi <![CDATA[This is a <tag> inside cdata </tag> <g>]]> <![CDATA[Another CDATA with tags <inside>]]>')\n-                        \n-        self.assertEqual(clean_markup(u'<![CDATA[This is a <tag> inside cdata </tag> <g>]]>', remove_cdata=False, xml_doc=True),\n-                        u'<![CDATA[This is a <tag> inside cdata </tag> <g>]]>')\n-\n-    def test_unquote_html(self):\n-        self.assertEqual(unquote_html(u'As low as &#163;534,456.34!'),\n-                         'As low as £534,456.34!')\n-        self.assertEqual(unquote_html('As low as &pound;534,456.34!'),\n-                         'As low as £534,456.34!')\n-\n-    def test_read_decimal(self):\n-        self.assertEqual(read_decimal('asdf 234,234.45sdf '),\n-                         Decimal(\"234234.45\"))\n-        self.assertEqual(read_decimal('asdf 2234 sdf '),\n-                         Decimal(\"2234\"))\n-        self.assertEqual(read_decimal('947'),\n-                         Decimal(\"947\"))\n-        self.assertEqual(read_decimal('adsfg'), \n-                         None)\n-        self.assertEqual(read_decimal('''stained, linseed oil finish, clear glas doors'''),\n-                         None)\n-\n-    def test_read_price(self):\n-        self.assertEqual(read_price(u'£549.97'),\n-                         Decimal(\"549.97\"))\n-        self.assertEqual(read_price(u'£5,499.97'),\n-                         Decimal(\"5499.97\"))\n-        self.assertEqual(read_price(u'Now: £5,499.97 for the last time'),\n-                         Decimal(\"5499.97\"))\n-        self.assertEqual(read_price(u'As low as 534,456.34!'),\n-                         Decimal(\"534456.34\"))\n-        self.assertEqual(read_price(u'As low as &#163;534,456.34!'),\n-                         Decimal(\"534456.34\"))\n-        self.assertEqual(read_price(u'As low as &pound;534,456.34!'),\n-                         Decimal(\"534456.34\"))\n-        self.assertEqual(read_price('asdf asdfg asdfg '),\n-                         None)\n-        self.assertEqual(read_price('wrweq qewr -&pound;12', True),\n-                         Decimal(\"-12\"))\n-        self.assertEqual(read_price('wrweq qewr &pound;12', True),\n-                         Decimal(\"12\"))\n-        self.assertEqual(read_price('qwe ewqeq -12', True),\n-                         Decimal(\"0\"))\n-        self.assertEqual(read_price('qwe ewqeq 12', True),\n-                         Decimal(\"0\"))\n-        self.assertEqual(read_price('qwe ewqeq 12'),\n-                         Decimal(\"12\"))\n-        self.assertEqual(read_price('117.54 £'),\n-                         Decimal(\"117.54\"))\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6cc91df9ed1846835d0a5930cf41161e4337cfa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 28 | Files Changed: 4 | Hunks: 26 | Methods Changed: 16 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 106 | Churn Cumulative: 512 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.8928571428571429\n\nDIFF:\n@@ -5,8 +5,8 @@ import unittest\n import libxml2\n \n from scrapy.http import Response\n-from scrapy.xpath.selector import XPathSelector\n-from scrapy.xpath.constructors import xmlDoc_from_xml\n+from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n+#from scrapy.xpath.constructors import xmlDoc_from_xml, xmlDoc_from_html\n from scrapy.xpath.iterator import XMLNodeIterator\n \n class XPathTestCase(unittest.TestCase):\n@@ -23,7 +23,7 @@ class XPathTestCase(unittest.TestCase):\n         \"\"\"Simple selector tests\"\"\"\n         body = \"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n         response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        xpath = XPathSelector(response)\n+        xpath = HtmlXPathSelector(response)\n \n         xl = xpath.x('//input')\n         self.assertEqual(2, len(xl))\n@@ -59,7 +59,7 @@ class XPathTestCase(unittest.TestCase):\n                   </body>\"\"\"\n \n         response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = XPathSelector(response)\n+        x = HtmlXPathSelector(response)\n \n         divtwo = x.x('//div[@class=\"two\"]')\n         self.assertEqual(divtwo.x(\"//li\").extract(),\n@@ -84,7 +84,7 @@ class XPathTestCase(unittest.TestCase):\n \n                \"\"\"\n         response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = XPathSelector(response)\n+        x = HtmlXPathSelector(response)\n \n         name_re = re.compile(\"Name: (\\w+)\")\n         self.assertEqual(x.x(\"//ul/li\").re(name_re),\n@@ -92,6 +92,20 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(x.x(\"//ul/li\").re(\"Age: (\\d+)\"),\n                          [\"10\", \"20\"])\n \n+    def test_selector_over_text(self):\n+        hxs = HtmlXPathSelector(text='<root>lala</root>')\n+        self.assertEqual(hxs.extract(),\n+                         u'<html><body><root>lala</root></body></html>')\n+\n+        xxs = XmlXPathSelector(text='<root>lala</root>')\n+        self.assertEqual(xxs.extract(),\n+                         u'<root>lala</root>')\n+\n+        xxs = XmlXPathSelector(text='<root>lala</root>')\n+        self.assertEqual(xxs.x('.').extract(),\n+                         [u'<root>lala</root>'])\n+\n+\n     def test_selector_namespaces_simple(self):\n         body = \"\"\"\n         <test xmlns:somens=\"http://scrapy.org\">\n@@ -101,7 +115,7 @@ class XPathTestCase(unittest.TestCase):\n         \"\"\"\n \n         response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = XPathSelector(response, constructor=xmlDoc_from_xml)\n+        x = XmlXPathSelector(response)\n         \n         x.register_namespace(\"somens\", \"http://scrapy.org\")\n         self.assertEqual(x.x(\"//somens:a\").extract(), \n@@ -119,7 +133,7 @@ class XPathTestCase(unittest.TestCase):\n </BrowseNode>\n         \"\"\"\n         response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n-        x = XPathSelector(response, constructor=xmlDoc_from_xml)\n+        x = XmlXPathSelector(response)\n \n         x.register_namespace(\"xmlns\", \"http://webservices.amazon.com/AWSECommerceService/2005-10-05\")\n         x.register_namespace(\"p\", \"http://www.scrapy.org/product\")\n@@ -146,10 +160,19 @@ class XPathTestCase(unittest.TestCase):\n \n         headers = {'Content-Type': ['text/html; charset=utf-8']}\n         response = Response(domain=\"example.com\", url=\"http://example.com\", headers=headers, body=html_utf8)\n-        x = XPathSelector(response)\n+        x = HtmlXPathSelector(response)\n         self.assertEquals(x.x(\"//span[@id='blank']/text()\").extract(),\n                           [u'\\xa3'])\n \n+    def test_null_bytes(self):\n+        hxs = HtmlXPathSelector(text='<root>la\\x00la</root>')\n+        self.assertEqual(hxs.extract(),\n+                         u'<html><body><root>lala</root></body></html>')\n+\n+        xxs = XmlXPathSelector(text='<root>la\\x00la</root>')\n+        self.assertEqual(xxs.extract(),\n+                         u'<root>lala</root>')\n+\n     def test_iterator(self):\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n\n@@ -24,5 +24,8 @@ def xmlDoc_from_html(response):\n \n def xmlDoc_from_xml(response):\n     \"\"\"Return libxml2 doc for XMLs\"\"\"\n-    return libxml2.readDoc(response.body.to_string('utf-8'), response.url, 'utf-8', xml_parser_options)\n-\n+    try:\n+        lxdoc = libxml2.readDoc(response.body.to_string('utf-8'), response.url, 'utf-8', xml_parser_options)\n+    except TypeError:  # libxml2 doesn't parse text with null bytes\n+        lxdoc = libxml2.readDoc(response.body.to_string('utf-8').replace(\"\\x00\", \"\"), response.url, 'utf-8', xml_parser_options)\n+    return lxdoc\n\n@@ -4,8 +4,8 @@ from cStringIO import StringIO\n \n import libxml2\n \n-from scrapy.xpath.constructors import xml_parser_options, xmlDoc_from_xml\n-from scrapy.xpath.selector import XPathSelector\n+from scrapy.xpath.constructors import xml_parser_options\n+from scrapy.xpath.selector import XmlXPathSelector\n \n class XMLNodeIterator(object):\n     \"\"\"XMLNodeIterator provides a way to iterate over all nodes of the same\n@@ -63,7 +63,7 @@ class XMLNodeSAXParser():\n         if name == self.requested_nodename:\n             self.inside_requested_node = False\n             string = ''.join([self.xml_declaration, self.buffer.getvalue()])\n-            selector = XPathSelector(text=string, constructor=xmlDoc_from_xml).x('/' + self.requested_nodename)[0]\n+            selector = XmlXPathSelector(text=string).x('/' + self.requested_nodename)[0]\n             self.selectors.append(selector)\n \n     def characters(self, data):\n\n@@ -2,23 +2,21 @@ import libxml2\n \n from scrapy.http import Response\n from scrapy.xpath.extension import Libxml2Document\n-from scrapy.xpath.constructors import xmlDoc_from_html\n+from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml\n from scrapy.utils.python import flatten\n from scrapy.utils.misc import extract_regex\n \n class XPathSelector(object):\n-    \"\"\"Provides an easy way for selecting document parts using XPaths and\n-    regexs, it also supports nested queries.\n+    \"\"\"The XPathSelector class provides a convenient way for selecting document\n+    parts using XPaths and regexs, with support for nested queries.\n \n-    Usage example (untested code):\n+    Although this is not an abstract class, you usually instantiate one of its\n+    children:\n     \n-    x = XPathSelector(response)\n-    i = ScrapedItem()\n-    i.assign(\"name\", x.x(\"//h2/text()\"))\n-    i.assign(\"features\", x.x(\"//div[@class='features']).x(\"./span/text()\")\n+    - XmlXPathSelector (for XML content)\n+    - HtmlXPathSelector (for HTML content)\n     \"\"\"\n \n-\n     def __init__(self, response=None, text=None, node=None, parent=None, expr=None, constructor=xmlDoc_from_html):\n         if parent:\n             self.doc = parent.doc\n@@ -36,6 +34,8 @@ class XPathSelector(object):\n         self.expr = expr\n \n     def x(self, xpath):\n+        \"\"\"Perform the given XPath query on the current XPathSelector and\n+        return a XPathSelectorList of the result\"\"\"\n         if hasattr(self.xmlNode, 'xpathEval'):\n             self.doc.xpathContext.setContextNode(self.xmlNode)\n             xpath_result = self.doc.xpathContext.xpathEval(xpath)\n@@ -47,13 +47,20 @@ class XPathSelector(object):\n             return XPathSelectorList([])\n \n     def re(self, regex):\n+        \"\"\"Return a list of unicode strings by applying the regex over all\n+        current XPath selections, and flattening the results\"\"\"\n         return extract_regex(regex, self.extract(), 'utf-8')\n \n-    def extract(self, **kwargs): \n+    def extract(self):\n+        \"\"\"Return a unicode string of the content referenced by the XPathSelector\"\"\"\n         if isinstance(self.xmlNode, basestring):\n             text = unicode(self.xmlNode, 'utf-8', errors='ignore')\n-        elif hasattr(self.xmlNode, 'xpathEval'):\n-            if isinstance(self.xmlNode, libxml2.xmlAttr):\n+        elif hasattr(self.xmlNode, 'serialize'):\n+            if isinstance(self.xmlNode, libxml2.xmlDoc):\n+                data = self.xmlNode.getRootElement().serialize('utf-8')\n+                text = unicode(data, 'utf-8', errors='ignore') if data else u''\n+            elif isinstance(self.xmlNode, libxml2.xmlAttr): \n+                # serialization doesn't work sometimes for xmlAttr types\n                 text = unicode(self.xmlNode.content, errors='ignore')\n             else:\n                 data = self.xmlNode.serialize('utf-8')\n@@ -66,6 +73,7 @@ class XPathSelector(object):\n         return text\n \n     def register_namespace(self, prefix, uri):\n+        \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n         self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n \n     def __str__(self):\n@@ -75,13 +83,29 @@ class XPathSelector(object):\n \n \n class XPathSelectorList(list):\n-\n-    def extract(self, **kwargs):\n-        return [x.extract(**kwargs) if isinstance(x, XPathSelector) else x for x in self]\n+    \"\"\"List of XPathSelector objects\"\"\"\n \n     def x(self, xpath):\n+        \"\"\"Perform the given XPath query on each XPathSelector of the list and\n+        return a new (flattened) XPathSelectorList of the results\"\"\"\n         return XPathSelectorList(flatten([x.x(xpath) for x in self]))\n \n     def re(self, regex):\n+        \"\"\"Perform the re() method on each XPathSelector of the list, and\n+        return the result as a flattened list of unicode strings\"\"\"\n         return flatten([x.re(regex) for x in self])\n     \n+    def extract(self):\n+        \"\"\"Return a list of unicode strings with the content referenced by each\n+        XPathSelector of the list\"\"\"\n+        return [x.extract() if isinstance(x, XPathSelector) else x for x in self]\n+\n+class XmlXPathSelector(XPathSelector):\n+    \"\"\"XPathSelector for XML content\"\"\"\n+    def __init__(self, response=None, text=None):\n+        XPathSelector.__init__(self, response=response, text=text, constructor=xmlDoc_from_xml)\n+\n+class HtmlXPathSelector(XPathSelector):\n+    \"\"\"XPathSelector for HTML content\"\"\"\n+    def __init__(self, response=None, text=None):\n+        XPathSelector.__init__(self, response=response, text=text, constructor=xmlDoc_from_html)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9fc8a1b65edde2176b179276d3332663e31a128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 103 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.fetcher import fetch\n from scrapy.spider import spiders\n-from scrapy.xpath import XPathSelector\n+from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_class\n from scrapy.extension import extensions\n from scrapy.conf import settings\n@@ -14,7 +14,8 @@ def load_url(url, response):\n     vars['item'] = item\n     vars['url'] = url\n     vars['response'] = response\n-    vars['xs'] = XPathSelector(response)\n+    vars['xxs'] = XmlXPathSelector(response)\n+    vars['hxs'] = HtmlXPathSelector(response)\n     vars['spider'] = spiders.fromurl(url)\n     return vars\n \n\n@@ -11,5 +11,5 @@ spiders, for convenience.\n \"\"\"\n \n from scrapy.xpath.types import XPath\n-from scrapy.xpath.selector import XPathSelector\n+from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n from scrapy.xpath.iterator import XMLNodeIterator\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4caadf6b67e28aa213b3a8efc29e03fafc5ae73f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 16 | Churn Cumulative: 138 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,3 +114,19 @@ class expat_XMLNodeIterator():\n                 start, end = self._byte_offset_buffer.pop(0)\n                 yield response_body[start:end]\n         self._parser.Parse('', 1)\n+\n+\n+# TESTING (pablo) #\n+# Yet another node iterator: this one is based entirely on regular expressions,\n+# which means it should be faster but needs some profiling to confirm.\n+\n+class re_XMLNodeIterator():\n+\n+    def __init__(self, response, node):\n+        self.response = response\n+        self.node = node\n+        self.re = re.compile(r\"<%s[\\s>].*?</%s>\" % (node, node), re.DOTALL)\n+\n+    def __iter__(self):\n+        for match in self.re.finditer(self.response.body.to_string()):\n+            yield XmlXPathSelector(text=match.group()).x('/' + self.node)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53838d6a7dbdb41f8bf0ed7ff572d7db9344a378", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 184 | Files Changed: 4 | Hunks: 41 | Methods Changed: 31 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 273 | Churn Cumulative: 847 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 0.9629629629629629\n\nDIFF:\n@@ -24,45 +24,67 @@ for val, attr in priorities.items():\n     setattr(sys.modules[__name__], \"PRIORITY_%s\" % attr, val )\n \n class Node:\n-    def __init__(self, remote, status, name):\n+    def __init__(self, remote, status, name, master):\n         self.__remote = remote\n         self._set_status(status)\n         self.name = name\n+        self.master = master\n \n     def _set_status(self, status):\n         if not status:\n             self.available = False\n         else:\n+            print \">----------<\"\n             self.available = True\n             self.running = status['running']\n-            self.pending = status['pending']\n+            self.closing = status['closing']\n             self.maxproc = status['maxproc']\n             self.starttime = status['starttime']\n             self.timestamp = status['timestamp']\n             self.loadavg = status['loadavg']\n             self.logdir = status['logdir']\n-            self.lastcallresponse = status['callresponse']\n+            print \"Running: %s\" % self.running\n+            free_slots = self.maxproc - len(self.running)\n+            while free_slots > 0 and self.master.pending:\n+                print \"Free slots %s\" % free_slots\n+                pending = self.master.pending.pop(0)\n+                self.run(pending)\n+                free_slots -= 1\n \n-    def _remote_call(self, function, *args):\n+    def get_status(self):\n         try:\n-            deferred = self.__remote.callRemote(function, *args)\n+            deferred = self.__remote.callRemote(\"status\")\n         except pb.DeadReferenceError:\n             self._set_status(None)\n             log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n         else:\n             deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n \n-    def get_status(self):\n-        self._remote_call(\"status\")\n+    def stop(self, domain):\n+        try:\n+            deferred = self.__remote.callRemote(\"stop\", domain)\n+        except pb.DeadReferenceError:\n+            self._set_status(None)\n+            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+        else:\n+            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n \n-    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):\n-        self._remote_call(\"schedule\", domains, spider_settings, priority)\n+    def run(self, pending):\n \n-    def stop(self, domains):\n-        self._remote_call(\"stop\", domains)\n+        def _run_callback(status):\n+            if status['callresponse'][0] == 1:\n+                #slots are complete. Reschedule in master. This is a security issue because could happen that the slots were completed since last status update by another cluster (thinking at future with full-distributed worker-master clusters)\n+                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])\n+                log.msg(\"Domain %s rescheduled: no proc space in node.\" % pending['domain'], log.WARNING)\n+            self._set_status(status)\n \n-    def remove(self, domains):\n-        self._remote_call(\"remove\", domains)\n+        try:\n+            deferred = self.__remote.callRemote(\"run\", pending[\"domain\"], pending[\"settings\"])\n+        except pb.DeadReferenceError:\n+            self._set_status(None)\n+            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+        else:\n+            deferred.addCallbacks(callback=_run_callback, errback=lambda reason:log.msg(reason, log.ERROR))\n \n class ClusterMaster(object):\n \n@@ -70,7 +92,7 @@ class ClusterMaster(object):\n         if not settings.getbool('CLUSTER_MASTER_ENABLED'):\n             raise NotConfigured\n         self.nodes = {}\n-        self.queue = []\n+        self.pending = []\n         dispatcher.connect(self._engine_started, signal=signals.engine_started)\n \n     def load_nodes(self):\n@@ -105,7 +127,7 @@ class ClusterMaster(object):\n \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n-        node = Node(cworker, None, name)\n+        node = Node(cworker, None, name, self)\n         node.get_status()\n         self.nodes[name] = node\n         log.msg(\"Added cluster worker %s\" % name)\n@@ -113,11 +135,16 @@ class ClusterMaster(object):\n     def remove_node(self, nodename):\n         raise NotImplemented\n \n-    def schedule(self, domains, spider_settings=None, nodename=None, priority=PRIORITY_NORMAL):\n-        if nodename:\n-            self.nodes[nodename].schedule(domains, spider_settings, priority)\n+    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):\n+        i = 0\n+        for p in self.pending:\n+            if p['priority'] <= priority:\n+                i += 1\n             else:\n-            self._dispatch_domains(domains, spider_settings, priority)\n+                break\n+        for domain in domains:\n+            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'priority': priority})\n+        self.update_nodes()\n \n     def stop(self, domains):\n         to_stop = {}\n@@ -129,19 +156,21 @@ class ClusterMaster(object):\n                 to_stop[node.name].append(domain)\n \n         for nodename, domains in to_stop.iteritems():\n-            self.nodes[nodename].stop(domains)\n+            for domain in domains:\n+                self.nodes[nodename].stop(domain)\n \n     def remove(self, domains):\n-        to_remove = {}\n-        for domain in domains:\n-            node = self.pending.get(domain, None)\n-            if node:\n-                if node.name not in to_remove:\n-                    to_remove[node.name] = []\n-                to_remove[node.name].append(domain)\n+        \"\"\"Remove all scheduled instances of the given domains (if it hasn't\n+        started yet). Otherwise use stop()\"\"\"\n \n-        for nodename, domains in to_remove.iteritems():\n-            self.nodes[nodename].remove(domains)\n+        for domain in domains:\n+            to_remove = []\n+            for p in self.pending:\n+                if p['domain'] == domain:\n+                    to_remove.append(p)\n+    \n+            for p in to_remove:\n+                self.pending.remove(p)\n \n     def discard(self, domains):\n         \"\"\"Stop and remove all running and pending instances of the given\n@@ -158,63 +187,10 @@ class ClusterMaster(object):\n                 d[proc['domain']] = node\n         return d\n \n-    @property\n-    def pending(self):\n-        \"\"\"Return dict of pending domains as domain -> node\"\"\"\n-        d = {}\n-        for node in self.nodes.itervalues():\n-            for p in node.pending:\n-                d[p['domain']] = node\n-        return d\n-\n     @property\n     def available_nodes(self):\n         return (node for node in self.nodes.itervalues() if node.available)\n \n-    def _dispatch_domains(self, domains, spider_settings, priority):\n-        \"\"\"Schedule the given domains in the availables nodes as good as\n-        possible. The algorithm follows the next rules (in order):\n-        \n-        1. search for nodes with available capacity(running < maxproc) and (if\n-        any) schedules the domains there\n-\n-        2. if there isn't any node with available capacity it schedules the\n-        domain in the node with the smallest number of pending spiders\n-        \"\"\"\n-\n-        to_schedule = {}  # domains to schedule per node\n-        pending_node = [] # list of #pending, node\n-\n-        for node in self.available_nodes:\n-            capacity = node.maxproc - len(node.running)\n-            #order nodes in pending_node according to insertion position, calculated from priority comparison, for stage 2.\n-            i = 0\n-            for p in node.pending:\n-                if p['priority'] <= priority:\n-                    i += 1\n-                else:\n-                    break\n-            bisect.insort(pending_node, (i, node))\n-\n-            #stage 1: use available capacity\n-            to_schedule[node.name] = []\n-            while domains and capacity > 0:\n-                to_schedule[node.name].append(domains.pop(0))\n-                capacity -= 1\n-            if not domains:\n-                break\n-\n-        #stage 2: queue in pendings the remaining domains.\n-        # a) pops out minor insertion-point node b) schedules the domain c) reinserts the node in list with insertion-point incremented by one.\n-        for domain in domains:\n-            insert_point, node = pending_node.pop(0)\n-            to_schedule[node.name].append(domain)\n-            bisect.insort(pending_node, (insert_point+1, node))\n-            \n-        for nodename, domains in to_schedule.iteritems():\n-            if domains:\n-                self.nodes[nodename].schedule(domains, spider_settings, priority)\n-\n     def _engine_started(self):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n\n@@ -29,14 +29,14 @@ class ClusterMasterWeb(ClusterMaster):\n         s += \"<h2>Home</h2>\\n\"\n \n         s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>&nbsp;</th><th>Name</th><th>Available</th><th>Running</th><th>Pending</th><th>Load.avg</th></tr>\\n\"\n+        s += \"<tr><th>&nbsp;</th><th>Name</th><th>Available</th><th>Running</th><th>Load.avg</th></tr>\\n\"\n         for node in self.nodes.itervalues():\n             #chkbox = \"<input type='checkbox' name='shutdown' value='%s' />\" % domain if node.status in [\"up\", \"idle\"] else \"&nbsp;\"\n             nodelink = \"<a href='nodes/#%s'>%s</a>\" % (node.name, node.name)\n             chkbox = \"&nbsp;\"\n             loadavg = \"%.2f %.2f %.2f\" % node.loadavg\n-            s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%d/%d</td><td>%d</td><td>%s</td></tr>\\n\" % \\\n-                 (chkbox, nodelink, node.available, len(node.running), node.maxproc, len(node.pending), loadavg)\n+            s += \"<tr><td>%s</td><td>%s</td><td>%s</td><td>%d/%d</td><td>%s</td></tr>\\n\" % \\\n+                 (chkbox, nodelink, node.available, len(node.running), node.maxproc, loadavg)\n         s += \"</table>\\n\"\n \n         s += \"</body>\\n\"\n@@ -51,8 +51,7 @@ class ClusterMasterWeb(ClusterMaster):\n             self.update_nodes()\n \n         if \"schedule\" in args:\n-            node = args[\"node\"][0] if \"node\" in args else None\n-            self.schedule(args[\"schedule\"], nodename=node, priority=eval(args[\"priority\"][0]))\n+            self.schedule(args[\"schedule\"], priority=eval(args[\"priority\"][0]))\n \n         if \"stop\" in args:\n             self.stop(args[\"stop\"])\n@@ -91,20 +90,6 @@ class ClusterMasterWeb(ClusterMaster):\n                 else:\n                     s += \"<p>No running domains on %s</p>\\n\" % node.name\n \n-                # pending domains\n-                s += \"<h3>Pending domains</h3>\\n\"\n-                if node.pending:\n-                    s += \"<form method='post' action='.'>\\n\"\n-                    s += \"<select name='remove' multiple='multiple' size='10'>\\n\"\n-                    for p in node.pending:\n-                        s += \"<option value='%s'>%s (P:%s)</option>\\n\" % (p['domain'], p['domain'],p['priority'])\n-                    s += \"</select>\\n\"\n-                    s += \"<input type='hidden' name='node' value='%s'>\\n\" % node.name\n-                    s += \"<p><input type='submit' value='Remove selected pending domains on %s'></p>\\n\" % node.name\n-                    s += \"</form>\\n\"\n-                else:\n-                    s += \"<p>No pending domains on %s</p>\\n\" % node.name\n-\n         return str(s)\n \n     def render_domains(self, wc_request):\n@@ -113,7 +98,7 @@ class ClusterMasterWeb(ClusterMaster):\n \n         enabled_domains = set(spiders.asdict(include_disabled=False).keys())\n         print \"Enabled domains: %s\" % len(enabled_domains)\n-        inactive_domains = enabled_domains - set(self.running.keys() + self.pending.keys())\n+        inactive_domains = enabled_domains - set(self.running.keys() + [p['domain'] for p in self.pending])\n \n         s = self.render_header()\n \n@@ -126,14 +111,6 @@ class ClusterMasterWeb(ClusterMaster):\n             s += \"<option>%s</option>\\n\" % domain\n         s += \"</select>\\n\"\n         s += \"</br>\\n\"\n-        s += \"Node (only available nodes shown):<br />\\n\"\n-        s += \"<select name='node'>\\n\"\n-        s += \"<option value='' selected='selected'>any</option>\"\n-        for node in self.available_nodes:\n-            domcount = \"%d/%d/%d\" % (len(node.running), node.maxproc, len(node.pending))\n-            loadavg = \"%.2f %.2f %.2f\" % node.loadavg\n-            s += \"<option value='%s'>%s [D: %s | LA: %s]</option>\" % (node.name, node.name, domcount, loadavg)\n-        s += \"</select><br />\\n\"\n         s += \"Priority:<br />\\n\"\n         s += \"<select name='priority'>\\n\"\n         for p, pname in priorities.items():\n@@ -150,9 +127,21 @@ class ClusterMasterWeb(ClusterMaster):\n         s += \"<table border='1'>\\n\"\n         s += \"<tr><th>Domain</th><th>Status</th><th>Node</th></tr>\\n\"\n         s += self._domains_table(self.running, '<b>running</b>')\n-        s += self._domains_table(self.pending, 'pending')\n         s += \"</table>\\n\"\n \n+        # pending domains\n+        s += \"<h3>Pending domains</h3>\\n\"\n+        if self.pending:\n+            s += \"<form method='post' action='.'>\\n\"\n+            s += \"<select name='remove' multiple='multiple' size='10'>\\n\"\n+            for p in self.pending:\n+                s += \"<option value='%s'>%s (P:%s)</option>\\n\" % (p['domain'], p['domain'],p['priority'])\n+            s += \"</select>\\n\"\n+            s += \"<p><input type='submit' value='Remove selected pending domains'></p>\\n\"\n+            s += \"</form>\\n\"\n+        else:\n+            s += \"<p>No pending domains</p>\\n\"\n+\n         return str(s)\n \n     def render_header(self):\n\n@@ -42,7 +42,6 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n     def processEnded(self, status_object):\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         del self.procman.running[self.domain]\n-        self.procman.next_pending()\n \n class ClusterWorker(pb.Root):\n \n@@ -53,92 +52,38 @@ class ClusterWorker(pb.Root):\n         self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n         self.logdir = settings['CLUSTER_WORKER_LOGDIR']\n         self.running = {}\n-        self.pending = []\n         self.starttime = time.time()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n \n-    def remote_schedule(self, domains, spider_settings=None, priority=20):\n-        \"\"\"Schedule new domains to be crawled in a separate processes\"\"\"\n-\n-        responses = []\n-        for domain in domains:\n-            if len(self.running) < self.maxproc and domain not in self.running:\n-                self._run(domain, spider_settings)\n-                responses.append(\"Started %s\" % self.running[domain])\n-            else:\n-                i = 0\n-                for p in self.pending:\n-                    if p['priority'] <= priority:\n-                        i += 1\n-                    else:\n-                        break\n-                self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'priority': priority})\n-                responses.append(\"Scheduled domain %s at position %s in queue\" % (domain, i))\n-        return self.status(responses)\n-\n-    def remote_stop(self, domains):\n-        \"\"\"Stop running domains. For removing pending (not yet started) domains\n-        use remove() instead\"\"\"\n-\n-        responses = []\n-        for domain in domains:\n+    def remote_stop(self, domain):\n+        \"\"\"Stop running domain.\"\"\"\n         if domain in self.running:\n             proc = self.running[domain]\n             log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n             proc.transport.signalProcess('INT')\n             proc.status = \"closing\"\n-                responses.append(\"Stopped process %s\" % proc)\n+            return self.status(0, \"Stopped process %s\" % proc)\n         else:\n-                responses.append(\"%s: domain not running.\" % domain)\n-        return self.status(responses)\n-\n-    def remote_remove(self, domains):\n-        \"\"\"Remove all scheduled instances of the given domains (if it hasn't\n-        started yet). Otherwise use stop()\"\"\"\n-\n-        responses = []\n-        for domain in domains:\n-            to_remove = []\n-            for p in self.pending:\n-                if p['domain'] == domain:\n-                    to_remove.append(p)\n-    \n-            for p in to_remove:\n-                self.pending.remove(p)\n-            responses.append(\"Unscheduled domain %s\" % domain)\n-        return self.status(responses)\n+            return self.status(1, \"%s: domain not running.\" % domain)\n \n     def remote_status(self):\n         return self.status()\n     \n-    def status(self, response=\"Status Response\"):\n+    def status(self, rcode=0, rstring=None):\n         status = {}\n-        status[\"pending\"] = self.pending\n         status[\"running\"] = [ self.running[k].as_dict() for k in self.running.keys() ]\n         status[\"starttime\"] = self.starttime\n         status[\"timestamp\"] = time.time()\n         status[\"maxproc\"] = self.maxproc\n         status[\"loadavg\"] = os.getloadavg()\n         status[\"logdir\"] = self.logdir\n-        status[\"callresponse\"] = response\n+        status[\"callresponse\"] = (rcode, rstring) if rstring else (0, \"Status Response.\")\n         return status\n \n-    def next_pending(self):\n-        \"\"\"Run the next domain in the pending list, which is not already running\"\"\"\n-\n-        if len(self.running) >= self.maxproc:\n-            return\n-        for p in self.pending:\n-            if p['domain'] not in self.running:\n-                self._run(p['domain'], p['settings'])\n-                self.pending.remove(p)\n-                return\n-\n-    def _run(self, domain, spider_settings=None):\n-        \"\"\"Spawn process to run the given domain. Don't call this method\n-        directly. Instead use schedule().\"\"\"\n-\n+    def remote_run(self, domain, spider_settings=None):\n+        \"\"\"Spawn process to run the given domain.\"\"\"\n+        if len(self.running) < self.maxproc:\n             logfile = os.path.join(self.logdir, domain, time.strftime(\"%FT%T.log\"))\n             if not os.path.exists(os.path.dirname(logfile)):\n                 os.makedirs(os.path.dirname(logfile))\n@@ -147,5 +92,6 @@ class ClusterWorker(pb.Root):\n             args = [sys.executable, sys.argv[0], 'crawl', domain]\n             proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)\n             self.running[domain] = scrapy_proc\n-\n+            return self.status(0, \"Started process %s.\" % scrapy_proc)\n+        return self.status(1, \"No free slot to run another process.\")\n \n\n@@ -13,16 +13,10 @@ sys.argv.pop(0)\n \n if not sys.argv:\n     d.addCallback(lambda object: object.callRemote(\"status\"))\n-elif sys.argv[0] == \"-d\":\n-    try:\n-        priority = eval(sys.argv[2])\n-    except:\n-        priority = 20\n-    d.addCallback(lambda object: object.callRemote(\"schedule\", [sys.argv[1]], priority=priority))\n elif sys.argv[0] == \"-s\":\n-    d.addCallback(lambda object: object.callRemote(\"stop\", [sys.argv[1]]))\n+    d.addCallback(lambda object: object.callRemote(\"stop\", sys.argv[1]))\n elif sys.argv[0] == \"-r\":\n-    d.addCallback(lambda object: object.callRemote(\"remove\", [sys.argv[1]]))\n+    d.addCallback(lambda object: object.callRemote(\"run\", sys.argv[1]))\n \n d.addCallbacks(callback = util.println, errback = lambda reason: 'error: '+str(reason.value))\n d.addCallback(lambda _: reactor.stop())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
