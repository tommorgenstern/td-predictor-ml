{"custom_id": "keras#c61857d380d3bf5529421db65cb03a20aca969e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -315,7 +315,6 @@ class Embedding(Layer):\n         embeddings_initializer=\"zeros\",\n         embeddings_scale_initializer=\"ones\",\n     ):\n-        self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n         self._embeddings = self.add_weight(\n             name=\"embeddings\",\n             shape=(self.input_dim, self.output_dim),\n@@ -323,9 +322,12 @@ class Embedding(Layer):\n             dtype=\"int8\",\n             trainable=False,\n         )\n+        # We choose to reduce the axis of `output_dim` because, typically,\n+        # `input_dim` is larger than `output_dim`. This reduces quantization\n+        # error.\n         self.embeddings_scale = self.add_weight(\n             name=\"embeddings_scale\",\n-            shape=(self.output_dim,),\n+            shape=(self.input_dim,),\n             initializer=embeddings_scale_initializer,\n             trainable=False,\n         )\n@@ -345,11 +347,12 @@ class Embedding(Layer):\n         # not needed\n         if backend.standardize_dtype(inputs.dtype) not in (\"int32\", \"int64\"):\n             inputs = ops.cast(inputs, \"int32\")\n+        embeddings_scale = ops.take(self.embeddings_scale, inputs, axis=0)\n         outputs = ops.take(self._embeddings, inputs, axis=0)\n         # De-scale outputs\n-        outputs = ops.cast(outputs, self.compute_dtype)\n         outputs = ops.divide(\n-            outputs, ops.expand_dims(self.embeddings_scale, axis=0)\n+            ops.cast(outputs, dtype=self.compute_dtype),\n+            ops.expand_dims(embeddings_scale, axis=-1),\n         )\n         if self.lora_enabled:\n             lora_outputs = ops.take(self.lora_embeddings_a, inputs, axis=0)\n@@ -379,14 +382,12 @@ class Embedding(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._embeddings` to int8 and compute corresponding\n             # scale\n             embeddings_value, embeddings_scale = quantizers.abs_max_quantize(\n-                self._embeddings, axis=0\n+                self._embeddings, axis=-1\n             )\n-            embeddings_scale = ops.squeeze(embeddings_scale, axis=0)\n+            embeddings_scale = ops.squeeze(embeddings_scale, axis=-1)\n             self._untrack_variable(self._embeddings)\n             del self._embeddings\n             # Utilize a lambda expression as an initializer to prevent adding a\n@@ -412,15 +413,15 @@ class Embedding(Layer):\n                 # Dequantize & quantize to merge lora weights into embeddings\n                 # Note that this is a lossy compression\n                 embeddings_value = ops.divide(\n-                    embeddings_value, embeddings_scale\n+                    embeddings_value, ops.expand_dims(embeddings_scale, axis=-1)\n                 )\n                 embeddings_value = ops.add(\n                     embeddings_value,\n                     ops.matmul(self.lora_embeddings_a, self.lora_embeddings_b),\n                 )\n                 embeddings_value, embeddings_scale = (\n-                    quantizers.abs_max_quantize(embeddings_value, axis=0)\n+                    quantizers.abs_max_quantize(embeddings_value, axis=-1)\n                 )\n-                embeddings_scale = ops.squeeze(embeddings_scale, axis=0)\n+                embeddings_scale = ops.squeeze(embeddings_scale, axis=-1)\n             return embeddings_value, embeddings_scale\n         return self.embeddings, None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e0a920ea106afc36a03158b4f1a8f5a70a8e39c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 148 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.75\n\nDIFF:\n@@ -557,8 +557,6 @@ class Dense(Layer):\n \n         self._tracker.unlock()\n         if mode == \"int8\":\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=0\n\n@@ -684,10 +684,6 @@ class EinsumDense(Layer):\n                 self._custom_gradient_equation,\n                 self._kernel_reverse_transpose_axes,\n             ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n-            # Configure `self.inputs_quantizer`\n-            self.inputs_quantizer = quantizers.AbsMaxQuantizer(\n-                axis=self._input_reduced_axes\n-            )\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n                 self._kernel, axis=self._kernel_reduced_axes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2767762688465216ad507c119e404ec8fd79a3e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 48 | Files Changed: 14 | Hunks: 34 | Methods Changed: 18 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 183 | Churn Cumulative: 597 | Contributors (this commit): 4 | Commits (past 90d): 32 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,6 +36,7 @@ from keras.src.distribution import distribution_lib\n from keras.src.layers import input_spec\n from keras.src.metrics.metric import Metric\n from keras.src.ops.operation import Operation\n+from keras.src.saving.keras_saveable import KerasSaveable\n from keras.src.utils import python_utils\n from keras.src.utils import summary_utils\n from keras.src.utils import traceback_utils\n@@ -56,7 +57,7 @@ else:\n \n \n @keras_export([\"keras.Layer\", \"keras.layers.Layer\"])\n-class Layer(BackendLayer, Operation):\n+class Layer(BackendLayer, Operation, KerasSaveable):\n     \"\"\"This is the class from which all layers inherit.\n \n     A layer is a callable object that takes as input one or more tensors and\n@@ -422,6 +423,9 @@ class Layer(BackendLayer, Operation):\n                 self.build(**config[\"shapes_dict\"])\n             self.built = True\n \n+    def _obj_type(self):\n+        return \"Layer\"\n+\n     def add_variable(\n         self,\n         shape,\n\n@@ -1,3 +1,5 @@\n+import pickle\n+\n import numpy as np\n import pytest\n \n@@ -11,6 +13,7 @@ from keras.src import testing\n \n \n class LayerTest(testing.TestCase):\n+\n     def test_compute_output_spec(self):\n         # Test that implementing compute_output_shape\n         # is enough to make compute_output_spec work.\n@@ -957,3 +960,8 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(layer.dtype_policy.name, \"mixed_float16\")\n         self.assertEqual(layer.dtype_policy.compute_dtype, \"float16\")\n         self.assertEqual(layer.dtype_policy.variable_dtype, \"float32\")\n+\n+    def test_pickle_layer(self):\n+        layer = layers.Dense(2)\n+        reloaded = pickle.loads(pickle.dumps(layer))\n+        self.assertEqual(layer.get_config(), reloaded.get_config())\n\n@@ -2,11 +2,12 @@ from keras.src import backend\n from keras.src import ops\n from keras.src import tree\n from keras.src.api_export import keras_export\n+from keras.src.saving.keras_saveable import KerasSaveable\n from keras.src.utils.naming import auto_name\n \n \n @keras_export([\"keras.Loss\", \"keras.losses.Loss\"])\n-class Loss:\n+class Loss(KerasSaveable):\n     \"\"\"Loss base class.\n \n     To be implemented by subclasses:\n@@ -69,6 +70,9 @@ class Loss:\n     def from_config(cls, config):\n         return cls(**config)\n \n+    def _obj_type(self):\n+        return \"Loss\"\n+\n \n def standardize_reduction(reduction):\n     allowed = {\"sum_over_batch_size\", \"sum\", None, \"none\"}\n\n@@ -1,3 +1,5 @@\n+import pickle\n+\n import numpy as np\n import pytest\n \n@@ -226,6 +228,11 @@ class LossTest(testing.TestCase):\n             loss,\n         )\n \n+    def test_pickle(self):\n+        loss = losses_module.get(\"mse\")\n+        loss = pickle.loads(pickle.dumps(loss))\n+        self.assertEqual(loss, losses_module.mean_squared_error)\n+\n     def test_get_method(self):\n         loss = losses_module.get(\"mse\")\n         self.assertEqual(loss, losses_module.mean_squared_error)\n\n@@ -2,12 +2,13 @@ from keras.src import backend\n from keras.src import initializers\n from keras.src import ops\n from keras.src.api_export import keras_export\n+from keras.src.saving.keras_saveable import KerasSaveable\n from keras.src.utils.naming import auto_name\n from keras.src.utils.tracking import Tracker\n \n \n @keras_export([\"keras.Metric\", \"keras.metrics.Metric\"])\n-class Metric:\n+class Metric(KerasSaveable):\n     \"\"\"Encapsulates metric logic and state.\n \n     Args:\n@@ -179,6 +180,9 @@ class Metric:\n     def dtype(self):\n         return self._dtype\n \n+    def _obj_type(self):\n+        return \"Metric\"\n+\n     def add_variable(\n         self, shape, initializer, dtype=None, aggregation=\"sum\", name=None\n     ):\n\n@@ -1,3 +1,5 @@\n+import pickle\n+\n import numpy as np\n \n from keras.src import backend\n@@ -165,6 +167,11 @@ class MetricTest(testing.TestCase):\n             custom_objects={\"ExampleMetric\": ExampleMetric},\n         )\n \n+    def test_pickle(self):\n+        metric = metrics_module.get(\"mse\")\n+        reloaded = pickle.loads(pickle.dumps(metric))\n+        self.assertIsInstance(reloaded, metrics_module.MeanSquaredError)\n+\n     def test_get_method(self):\n         metric = metrics_module.get(\"mse\")\n         self.assertIsInstance(metric, metrics_module.MeanSquaredError)\n\n@@ -178,6 +178,9 @@ class Functional(Function, Model):\n         # functional DAG.\n         pass\n \n+    def _obj_type(self):\n+        return \"Functional\"\n+\n     @property\n     def layers(self):\n         layers = []\n\n@@ -1,10 +1,8 @@\n import inspect\n-import io\n import json\n import typing\n import warnings\n \n-import keras.src.saving.saving_lib as saving_lib\n from keras.src import backend\n from keras.src import utils\n from keras.src.api_export import keras_export\n@@ -350,30 +348,6 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n             self, filepath, skip_mismatch=skip_mismatch, **kwargs\n         )\n \n-    # Note: renaming this function will cause old pickles to be broken.\n-    # This is probably not a huge deal, as pickle should not be a recommended\n-    # saving format -- it should only be supported for use with distributed\n-    # computing frameworks.\n-    @classmethod\n-    def _unpickle_model(cls, bytesio):\n-        # pickle is not safe regardless of what you do.\n-        return saving_lib._load_model_from_fileobj(\n-            bytesio, custom_objects=None, compile=True, safe_mode=False\n-        )\n-\n-    def __reduce__(self):\n-        \"\"\"__reduce__ is used to customize the behavior of `pickle.pickle()`.\n-\n-        The method returns a tuple of two elements: a function, and a list of\n-        arguments to pass to that function.  In this case we just leverage the\n-        keras saving library.\"\"\"\n-        buf = io.BytesIO()\n-        saving_lib._save_model_to_fileobj(self, buf, \"h5\")\n-        return (\n-            self._unpickle_model,\n-            (buf,),\n-        )\n-\n     def quantize(self, mode):\n         \"\"\"Quantize the weights of the model.\n \n\n@@ -142,6 +142,9 @@ class Sequential(Model):\n         # Unlike other layers, Sequential is mutable after build.\n         pass\n \n+    def _obj_type(self):\n+        return \"Sequential\"\n+\n     def build(self, input_shape=None):\n         if not isinstance(input_shape, (tuple, list)):\n             # Do not attempt to build if the model does not have a single\n\n@@ -1,3 +1,5 @@\n+import pickle\n+\n import numpy as np\n import pytest\n \n@@ -246,6 +248,13 @@ class SequentialTest(testing.TestCase):\n         self.assertEqual(model.input_shape, (None, 2))\n         self.assertEqual(model.output_shape, (None, 4))\n \n+    def test_pickleable(self):\n+        model = Sequential(name=\"seq\")\n+        model.add(layers.Dense(4))\n+\n+        result = pickle.loads(pickle.dumps(model))\n+        assert len(result.layers) == 1\n+\n     def test_bad_layer(self):\n         model = Sequential(name=\"seq\")\n         with self.assertRaisesRegex(ValueError, \"Only instances of\"):\n\n@@ -6,11 +6,12 @@ from keras.src import initializers\n from keras.src import ops\n from keras.src.optimizers.schedules import learning_rate_schedule\n from keras.src.saving import serialization_lib\n+from keras.src.saving.keras_saveable import KerasSaveable\n from keras.src.utils import tracking\n from keras.src.utils.naming import auto_name\n \n \n-class BaseOptimizer:\n+class BaseOptimizer(KerasSaveable):\n     def __init__(\n         self,\n         learning_rate,\n@@ -814,6 +815,9 @@ class BaseOptimizer:\n             # optimizer.\n             self._overwrite_model_variables_with_average_value(var_list)\n \n+    def _obj_type(self):\n+        return \"Optimizer\"\n+\n     def get_config(self):\n         \"\"\"Returns the config of the optimizer.\n \n\n@@ -1,7 +1,9 @@\n import os\n+import pickle\n \n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras.src import backend\n from keras.src import constraints\n@@ -11,7 +13,7 @@ from keras.src import optimizers\n from keras.src import testing\n \n \n-class OptimizerTest(testing.TestCase):\n+class OptimizerTest(testing.TestCase, parameterized.TestCase):\n     def test_iterations_counter(self):\n         v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n         grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n@@ -318,3 +320,24 @@ class OptimizerTest(testing.TestCase):\n             adam.learning_rate, 4\n         )\n         self.assertLen(adam.variables, 1)\n+\n+    @parameterized.parameters(\n+        [\n+            (\"adam\",),\n+            (\"sgd\",),\n+            (\"adamw\",),\n+            (\"adagrad\",),\n+            (\"rmsprop\",),\n+            (\"adadelta\",),\n+            (\"adamax\",),\n+            (\"lion\",),\n+            (\"nadam\",),\n+            (\"ftrl\",),\n+            (\"adafactor\",),\n+        ]\n+    )\n+    def test_pickleable_optimizers(self, optimizer):\n+        optimizer = optimizers.get(optimizer)\n+        reloaded = pickle.loads(pickle.dumps(optimizer))\n+\n+        self.assertEqual(optimizer.get_config(), reloaded.get_config())\n\n@@ -0,0 +1,38 @@\n+import io\n+\n+\n+class KerasSaveable:\n+    # Note: renaming this function will cause old pickles to be broken.\n+    # This is probably not a huge deal, as pickle should not be a recommended\n+    # saving format -- it should only be supported for use with distributed\n+    # computing frameworks.\n+\n+    def _obj_type(self):\n+        raise NotImplementedError(\n+            \"KerasSaveable subclases must provide an \"\n+            \"implementation for `obj_type()`\"\n+        )\n+\n+    @classmethod\n+    def _unpickle_model(cls, bytesio):\n+        import keras.src.saving.saving_lib as saving_lib\n+\n+        # pickle is not safe regardless of what you do.\n+        return saving_lib._load_model_from_fileobj(\n+            bytesio, custom_objects=None, compile=True, safe_mode=False\n+        )\n+\n+    def __reduce__(self):\n+        \"\"\"__reduce__ is used to customize the behavior of `pickle.pickle()`.\n+\n+        The method returns a tuple of two elements: a function, and a list of\n+        arguments to pass to that function.  In this case we just leverage the\n+        keras saving library.\"\"\"\n+        import keras.src.saving.saving_lib as saving_lib\n+\n+        buf = io.BytesIO()\n+        saving_lib._save_model_to_fileobj(self, buf, \"h5\")\n+        return (\n+            self._unpickle_model,\n+            (buf,),\n+        )\n\n@@ -319,23 +319,16 @@ def _name_key(name):\n \n \n def _walk_trackable(trackable):\n-    from keras.src.models import Functional\n-    from keras.src.models import Sequential\n+    from keras.src.saving.keras_saveable import KerasSaveable\n \n-    if isinstance(trackable, Sequential):\n-        obj_type = \"Sequential\"\n-    elif isinstance(trackable, Functional):\n-        obj_type = \"Functional\"\n-    elif isinstance(trackable, Layer):\n-        obj_type = \"Layer\"\n-    elif isinstance(trackable, Optimizer):\n-        obj_type = \"Optimizer\"\n-    elif isinstance(trackable, Metric):\n-        obj_type = \"Metric\"\n-    elif isinstance(trackable, Loss):\n-        obj_type = \"Loss\"\n-    else:\n-        raise ValueError(f\"Invalid obj_type: {obj_type}\")\n+    if not isinstance(trackable, KerasSaveable):\n+        raise ValueError(\n+            \"Expected `trackable` to be an \"\n+            \"instance of `KerasSaveable`, but \"\n+            f\"got {trackable=}.\"\n+        )\n+\n+    obj_type = trackable._obj_type()\n     attr_skiplist = get_attr_skiplist(obj_type)\n \n     # Save all layers directly tracked by Sequential and Functional first.\n@@ -793,7 +786,13 @@ def get_attr_skiplist(obj_type):\n         ref_obj = Loss()\n         skiplist += dir(ref_obj)\n     else:\n-        raise ValueError(f\"Invalid obj_type: {obj_type}\")\n+        raise ValueError(\n+            f\"get_attr_skiplist got invalid {obj_type=}. \"\n+            \"Accepted values for `obj_type` are \"\n+            \"['Layer', 'Functional', 'Sequential', 'Metric', \"\n+            \"'Optimizer', 'Loss']\"\n+        )\n+\n     global_state.set_global_attribute(\n         f\"saving_attr_skiplist_{obj_type}\", skiplist\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3382b31f114049ccf0d01ef1d1acad051a8d1bbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 126 | Files Changed: 3 | Hunks: 76 | Methods Changed: 22 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 249 | Churn Cumulative: 420 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ from keras.src import backend\n from keras.src import utils\n from keras.src.api_export import keras_export\n from keras.src.layers.layer import Layer\n-from keras.src.models.variable_mapping import map_trackable_variables\n+from keras.src.models.variable_mapping import map_saveable_variables\n from keras.src.saving import saving_api\n from keras.src.trainers import trainer as base_trainer\n from keras.src.utils import summary_utils\n@@ -544,7 +544,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n \n     def _get_variable_map(self):\n         store = {}\n-        map_trackable_variables(self, store=store, visited_trackables=set())\n+        map_saveable_variables(self, store=store, visited_saveables=set())\n         return store\n \n \n\n@@ -2,24 +2,25 @@ from keras.src.layers.layer import Layer\n from keras.src.metrics.metric import Metric\n from keras.src.optimizers.optimizer import Optimizer\n from keras.src.saving import saving_lib\n+from keras.src.saving.keras_saveable import KerasSaveable\n \n \n-def map_trackable_variables(trackable, store, visited_trackables):\n-    # If the trackable has already been saved, skip it.\n-    if id(trackable) in visited_trackables:\n+def map_saveable_variables(saveable, store, visited_saveables):\n+    # If the saveable has already been seen, skip it.\n+    if id(saveable) in visited_saveables:\n         return\n \n-    visited_trackables.add(id(trackable))\n+    visited_saveables.add(id(saveable))\n \n     variables = []\n-    if isinstance(trackable, Layer):\n+    if isinstance(saveable, Layer):\n         variables = (\n-            trackable._trainable_variables + trackable._non_trainable_variables\n+            saveable._trainable_variables + saveable._non_trainable_variables\n         )\n-    elif isinstance(trackable, Optimizer):\n-        variables = trackable._variables\n-    elif isinstance(trackable, Metric):\n-        variables = trackable._variables\n+    elif isinstance(saveable, Optimizer):\n+        variables = saveable._variables\n+    elif isinstance(saveable, Metric):\n+        variables = saveable._variables\n     for v in variables:\n         if v.path in store:\n             raise ValueError(\n@@ -31,30 +32,30 @@ def map_trackable_variables(trackable, store, visited_trackables):\n             )\n         store[v.path] = v\n \n-    # Recursively save state of children trackables (layers, optimizers, etc.)\n-    for child_attr, child_obj in saving_lib._walk_trackable(trackable):\n-        if saving_lib._is_keras_trackable(child_obj):\n-            map_trackable_variables(\n+    # Recursively save state of children saveables (layers, optimizers, etc.)\n+    for child_attr, child_obj in saving_lib._walk_saveable(saveable):\n+        if isinstance(child_obj, KerasSaveable):\n+            map_saveable_variables(\n                 child_obj,\n                 store,\n-                visited_trackables=visited_trackables,\n+                visited_saveables=visited_saveables,\n             )\n         elif isinstance(child_obj, (list, dict, tuple, set)):\n             map_container_variables(\n                 child_obj,\n                 store,\n-                visited_trackables=visited_trackables,\n+                visited_saveables=visited_saveables,\n             )\n \n \n-def map_container_variables(container, store, visited_trackables):\n+def map_container_variables(container, store, visited_saveables):\n     if isinstance(container, dict):\n         container = list(container.values())\n \n-    for trackable in container:\n-        if saving_lib._is_keras_trackable(trackable):\n-            map_trackable_variables(\n-                trackable,\n+    for saveable in container:\n+        if isinstance(saveable, KerasSaveable):\n+            map_saveable_variables(\n+                saveable,\n                 store,\n-                visited_trackables=visited_trackables,\n+                visited_saveables=visited_saveables,\n             )\n\n@@ -41,16 +41,16 @@ def save_model(model, filepath, weights_format=\"h5\"):\n     The zip-based archive contains the following structure:\n \n     - JSON-based configuration file (config.json): Records of model, layer, and\n-        other trackables' configuration.\n-    - H5-based trackable state files, found in respective directories, such as\n+        other saveables' configuration.\n+    - H5-based saveable state files, found in respective directories, such as\n         model/states.npz, model/dense_layer/states.npz, etc.\n     - Metadata file.\n \n-    The states of Keras trackables (layers, optimizers, loss, and metrics) are\n+    The states of Keras saveables (layers, optimizers, loss, and metrics) are\n     automatically saved as long as they can be discovered through the attributes\n     returned by `dir(Model)`. Typically, the state includes the variables\n-    associated with the trackable, but some specially purposed layers may\n-    contain more such as the vocabularies stored in the hashmaps. The trackables\n+    associated with the saveable, but some specially purposed layers may\n+    contain more such as the vocabularies stored in the hashmaps. The saveables\n     define how their states are saved by exposing `save_state()` and\n     `load_state()` APIs.\n \n@@ -129,7 +129,7 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n             weights_store=weights_store,\n             assets_store=asset_store,\n             inner_path=\"\",\n-            visited_trackables=set(),\n+            visited_saveables=set(),\n         )\n         weights_store.close()\n         asset_store.close()\n@@ -188,22 +188,22 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n         else:\n             asset_store = None\n \n-        failed_trackables = set()\n+        failed_saveables = set()\n         error_msgs = {}\n         _load_state(\n             model,\n             weights_store=weights_store,\n             assets_store=asset_store,\n             inner_path=\"\",\n-            visited_trackables=set(),\n-            failed_trackables=failed_trackables,\n+            visited_saveables=set(),\n+            failed_saveables=failed_saveables,\n             error_msgs=error_msgs,\n         )\n         weights_store.close()\n         if asset_store:\n             asset_store.close()\n \n-        if failed_trackables:\n+        if failed_saveables:\n             _raise_loading_failure(error_msgs)\n     return model\n \n@@ -223,15 +223,15 @@ def save_weights_only(model, filepath, objects_to_skip=None):\n         )\n     weights_store = H5IOStore(filepath, mode=\"w\")\n     if objects_to_skip is not None:\n-        visited_trackables = set(id(o) for o in objects_to_skip)\n+        visited_saveables = set(id(o) for o in objects_to_skip)\n     else:\n-        visited_trackables = set()\n+        visited_saveables = set()\n     _save_state(\n         model,\n         weights_store=weights_store,\n         assets_store=None,\n         inner_path=\"\",\n-        visited_trackables=visited_trackables,\n+        visited_saveables=visited_saveables,\n     )\n     weights_store.close()\n \n@@ -254,11 +254,11 @@ def load_weights_only(\n             _VARS_FNAME + \".h5\", archive=archive, mode=\"r\"\n         )\n \n-    failed_trackables = set()\n+    failed_saveables = set()\n     if objects_to_skip is not None:\n-        visited_trackables = set(id(o) for o in objects_to_skip)\n+        visited_saveables = set(id(o) for o in objects_to_skip)\n     else:\n-        visited_trackables = set()\n+        visited_saveables = set()\n     error_msgs = {}\n     _load_state(\n         model,\n@@ -266,25 +266,25 @@ def load_weights_only(\n         assets_store=None,\n         inner_path=\"\",\n         skip_mismatch=skip_mismatch,\n-        visited_trackables=visited_trackables,\n-        failed_trackables=failed_trackables,\n+        visited_saveables=visited_saveables,\n+        failed_saveables=failed_saveables,\n         error_msgs=error_msgs,\n     )\n     weights_store.close()\n     if archive:\n         archive.close()\n \n-    if failed_trackables:\n+    if failed_saveables:\n         _raise_loading_failure(error_msgs, warn_only=skip_mismatch)\n \n \n def _raise_loading_failure(error_msgs, warn_only=False):\n     first_key = list(error_msgs.keys())[0]\n-    ex_trackable, ex_error = error_msgs[first_key]\n+    ex_saveable, ex_error = error_msgs[first_key]\n     msg = (\n         f\"A total of {len(error_msgs)} objects could not \"\n         \"be loaded. Example error message for \"\n-        f\"object {ex_trackable}:\\n\\n\"\n+        f\"object {ex_saveable}:\\n\\n\"\n         f\"{ex_error}\\n\\n\"\n         \"List of objects that could not be loaded:\\n\"\n         f\"{[x[0] for x in error_msgs.values()]}\"\n@@ -318,30 +318,30 @@ def _name_key(name):\n     return name\n \n \n-def _walk_trackable(trackable):\n+def _walk_saveable(saveable):\n     from keras.src.saving.keras_saveable import KerasSaveable\n \n-    if not isinstance(trackable, KerasSaveable):\n+    if not isinstance(saveable, KerasSaveable):\n         raise ValueError(\n-            \"Expected `trackable` to be an \"\n+            \"Expected object to be an \"\n             \"instance of `KerasSaveable`, but \"\n-            f\"got {trackable=}.\"\n+            f\"got {saveable} of type {type(saveable)}\"\n         )\n \n-    obj_type = trackable._obj_type()\n+    obj_type = saveable._obj_type()\n     attr_skiplist = get_attr_skiplist(obj_type)\n \n     # Save all layers directly tracked by Sequential and Functional first.\n     # This helps avoid ordering concerns for subclassed Sequential or Functional\n     # models with extra attributes--the internal Keras state take precedence.\n     if obj_type in (\"Sequential\", \"Functional\"):\n-        yield \"layers\", trackable.layers\n+        yield \"layers\", saveable.layers\n \n-    for child_attr in sorted(dir(trackable), key=lambda x: _name_key(x)):\n+    for child_attr in sorted(dir(saveable), key=lambda x: _name_key(x)):\n         if child_attr.startswith(\"__\") or child_attr in attr_skiplist:\n             continue\n         try:\n-            child_obj = getattr(trackable, child_attr)\n+            child_obj = getattr(saveable, child_attr)\n         except Exception:\n             # Avoid raising the exception when visiting the attributes.\n             continue\n@@ -349,26 +349,28 @@ def _walk_trackable(trackable):\n \n \n def _save_state(\n-    trackable,\n+    saveable,\n     weights_store,\n     assets_store,\n     inner_path,\n-    visited_trackables,\n+    visited_saveables,\n ):\n-    # If the trackable has already been saved, skip it.\n-    if id(trackable) in visited_trackables:\n+    from keras.src.saving.keras_saveable import KerasSaveable\n+\n+    # If the saveable has already been saved, skip it.\n+    if id(saveable) in visited_saveables:\n         return\n \n-    if hasattr(trackable, \"save_own_variables\") and weights_store:\n-        trackable.save_own_variables(weights_store.make(inner_path))\n-    if hasattr(trackable, \"save_assets\") and assets_store:\n-        trackable.save_assets(assets_store.make(inner_path))\n+    if hasattr(saveable, \"save_own_variables\") and weights_store:\n+        saveable.save_own_variables(weights_store.make(inner_path))\n+    if hasattr(saveable, \"save_assets\") and assets_store:\n+        saveable.save_assets(assets_store.make(inner_path))\n \n-    visited_trackables.add(id(trackable))\n+    visited_saveables.add(id(saveable))\n \n-    # Recursively save state of children trackables (layers, optimizers, etc.)\n-    for child_attr, child_obj in _walk_trackable(trackable):\n-        if _is_keras_trackable(child_obj):\n+    # Recursively save state of children saveables (layers, optimizers, etc.)\n+    for child_attr, child_obj in _walk_saveable(saveable):\n+        if isinstance(child_obj, KerasSaveable):\n             _save_state(\n                 child_obj,\n                 weights_store,\n@@ -376,7 +378,7 @@ def _save_state(\n                 inner_path=file_utils.join(inner_path, child_attr).replace(\n                     \"\\\\\", \"/\"\n                 ),\n-                visited_trackables=visited_trackables,\n+                visited_saveables=visited_saveables,\n             )\n         elif isinstance(child_obj, (list, dict, tuple, set)):\n             _save_container_state(\n@@ -386,55 +388,57 @@ def _save_state(\n                 inner_path=file_utils.join(inner_path, child_attr).replace(\n                     \"\\\\\", \"/\"\n                 ),\n-                visited_trackables=visited_trackables,\n+                visited_saveables=visited_saveables,\n             )\n \n \n def _load_state(\n-    trackable,\n+    saveable,\n     weights_store,\n     assets_store,\n     inner_path,\n     skip_mismatch=False,\n-    visited_trackables=None,\n-    failed_trackables=None,\n+    visited_saveables=None,\n+    failed_saveables=None,\n     error_msgs=None,\n ):\n-    if visited_trackables and id(trackable) in visited_trackables:\n+    from keras.src.saving.keras_saveable import KerasSaveable\n+\n+    if visited_saveables and id(saveable) in visited_saveables:\n         return\n \n     failure = False\n \n-    if hasattr(trackable, \"load_own_variables\") and weights_store:\n-        if skip_mismatch or failed_trackables is not None:\n+    if hasattr(saveable, \"load_own_variables\") and weights_store:\n+        if skip_mismatch or failed_saveables is not None:\n             try:\n-                trackable.load_own_variables(weights_store.get(inner_path))\n+                saveable.load_own_variables(weights_store.get(inner_path))\n             except Exception as e:\n-                failed_trackables.add(id(trackable))\n-                error_msgs[id(trackable)] = trackable, e\n+                failed_saveables.add(id(saveable))\n+                error_msgs[id(saveable)] = saveable, e\n                 failure = True\n         else:\n-            trackable.load_own_variables(weights_store.get(inner_path))\n+            saveable.load_own_variables(weights_store.get(inner_path))\n \n-    if hasattr(trackable, \"load_assets\") and assets_store:\n-        if skip_mismatch or failed_trackables is not None:\n+    if hasattr(saveable, \"load_assets\") and assets_store:\n+        if skip_mismatch or failed_saveables is not None:\n             try:\n-                trackable.load_assets(assets_store.get(inner_path))\n+                saveable.load_assets(assets_store.get(inner_path))\n             except Exception as e:\n-                failed_trackables.add(id(trackable))\n-                error_msgs[id(trackable)] = trackable, e\n+                failed_saveables.add(id(saveable))\n+                error_msgs[id(saveable)] = saveable, e\n                 failure = True\n         else:\n-            trackable.load_assets(assets_store.get(inner_path))\n+            saveable.load_assets(assets_store.get(inner_path))\n \n-    if failed_trackables is not None:\n-        currently_failed = len(failed_trackables)\n+    if failed_saveables is not None:\n+        currently_failed = len(failed_saveables)\n     else:\n         currently_failed = 0\n \n-    # Recursively load states for Keras trackables such as layers/optimizers.\n-    for child_attr, child_obj in _walk_trackable(trackable):\n-        if _is_keras_trackable(child_obj):\n+    # Recursively load states for Keras saveables such as layers/optimizers.\n+    for child_attr, child_obj in _walk_saveable(saveable):\n+        if isinstance(child_obj, KerasSaveable):\n             _load_state(\n                 child_obj,\n                 weights_store,\n@@ -443,8 +447,8 @@ def _load_state(\n                     \"\\\\\", \"/\"\n                 ),\n                 skip_mismatch=skip_mismatch,\n-                visited_trackables=visited_trackables,\n-                failed_trackables=failed_trackables,\n+                visited_saveables=visited_saveables,\n+                failed_saveables=failed_saveables,\n                 error_msgs=error_msgs,\n             )\n         elif isinstance(child_obj, (list, dict, tuple, set)):\n@@ -456,48 +460,50 @@ def _load_state(\n                     \"\\\\\", \"/\"\n                 ),\n                 skip_mismatch=skip_mismatch,\n-                visited_trackables=visited_trackables,\n-                failed_trackables=failed_trackables,\n+                visited_saveables=visited_saveables,\n+                failed_saveables=failed_saveables,\n                 error_msgs=error_msgs,\n             )\n \n-    if failed_trackables is not None:\n-        newly_failed = len(failed_trackables) - currently_failed\n+    if failed_saveables is not None:\n+        newly_failed = len(failed_saveables) - currently_failed\n     else:\n         newly_failed = 0\n \n     if not failure:\n-        if visited_trackables is not None and newly_failed <= 0:\n-            visited_trackables.add(id(trackable))\n-        if id(trackable) in failed_trackables:\n-            failed_trackables.remove(id(trackable))\n-            error_msgs.pop(id(trackable))\n+        if visited_saveables is not None and newly_failed <= 0:\n+            visited_saveables.add(id(saveable))\n+        if id(saveable) in failed_saveables:\n+            failed_saveables.remove(id(saveable))\n+            error_msgs.pop(id(saveable))\n \n \n def _save_container_state(\n-    container, weights_store, assets_store, inner_path, visited_trackables\n+    container, weights_store, assets_store, inner_path, visited_saveables\n ):\n+    from keras.src.saving.keras_saveable import KerasSaveable\n+\n     used_names = {}\n     if isinstance(container, dict):\n         container = list(container.values())\n \n-    for trackable in container:\n-        if _is_keras_trackable(trackable):\n-            # Do NOT address the trackable via `trackable.name`, since\n+    for saveable in container:\n+        if isinstance(saveable, KerasSaveable):\n+            # Do NOT address the saveable via `saveable.name`, since\n             # names are usually autogenerated and thus not reproducible\n             # (i.e. they may vary across two instances of the same model).\n-            name = naming.to_snake_case(trackable.__class__.__name__)\n+            name = naming.to_snake_case(saveable.__class__.__name__)\n             if name in used_names:\n                 used_names[name] += 1\n                 name = f\"{name}_{used_names[name]}\"\n             else:\n                 used_names[name] = 0\n             _save_state(\n-                trackable,\n+                saveable,\n                 weights_store,\n                 assets_store,\n                 inner_path=file_utils.join(inner_path, name).replace(\"\\\\\", \"/\"),\n-                visited_trackables=visited_trackables,\n+                visited_saveables=visited_saveables,\n             )\n \n \n@@ -507,30 +513,32 @@ def _load_container_state(\n     assets_store,\n     inner_path,\n     skip_mismatch,\n-    visited_trackables,\n-    failed_trackables,\n+    visited_saveables,\n+    failed_saveables,\n     error_msgs,\n ):\n+    from keras.src.saving.keras_saveable import KerasSaveable\n+\n     used_names = {}\n     if isinstance(container, dict):\n         container = list(container.values())\n \n-    for trackable in container:\n-        if _is_keras_trackable(trackable):\n-            name = naming.to_snake_case(trackable.__class__.__name__)\n+    for saveable in container:\n+        if isinstance(saveable, KerasSaveable):\n+            name = naming.to_snake_case(saveable.__class__.__name__)\n             if name in used_names:\n                 used_names[name] += 1\n                 name = f\"{name}_{used_names[name]}\"\n             else:\n                 used_names[name] = 0\n             _load_state(\n-                trackable,\n+                saveable,\n                 weights_store,\n                 assets_store,\n                 inner_path=file_utils.join(inner_path, name).replace(\"\\\\\", \"/\"),\n                 skip_mismatch=skip_mismatch,\n-                visited_trackables=visited_trackables,\n-                failed_trackables=failed_trackables,\n+                visited_saveables=visited_saveables,\n+                failed_saveables=failed_saveables,\n                 error_msgs=error_msgs,\n             )\n \n@@ -797,15 +805,3 @@ def get_attr_skiplist(obj_type):\n         f\"saving_attr_skiplist_{obj_type}\", skiplist\n     )\n     return skiplist\n-\n-\n-def _is_keras_trackable(obj):\n-    return isinstance(\n-        obj,\n-        (\n-            Layer,\n-            Optimizer,\n-            Metric,\n-            Loss,\n-        ),\n-    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e123d898349fa768f515544d0b670d0c3d56bcba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 145 | Files Changed: 6 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 10/5 | Churn Δ: 209 | Churn Cumulative: 1474 | Contributors (this commit): 4 | Commits (past 90d): 51 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1162,3 +1162,8 @@ def correlate(x1, x2, mode=\"valid\"):\n \n def select(condlist, choicelist, default=0):\n     return jnp.select(condlist, choicelist, default=default)\n+\n+\n+def slogdet(x):\n+    x = convert_to_tensor(x)\n+    return tuple(jnp.linalg.slogdet(x))\n\n@@ -1098,3 +1098,7 @@ def correlate(x1, x2, mode=\"valid\"):\n \n def select(condlist, choicelist, default=0):\n     return np.select(condlist, choicelist, default=default)\n+\n+\n+def slogdet(x):\n+    return tuple(np.linalg.slogdet(x))\n\n@@ -2430,3 +2430,8 @@ def correlate(x1, x2, mode=\"valid\"):\n \n def select(condlist, choicelist, default=0):\n     return tf.experimental.numpy.select(condlist, choicelist, default=default)\n+\n+\n+def slogdet(x):\n+    x = convert_to_tensor(x)\n+    return tuple(tf.linalg.slogdet(x))\n\n@@ -1608,3 +1608,8 @@ def select(condlist, choicelist, default=0):\n     for c, v in reversed(list(zip(condlist, choicelist))):\n         out = torch.where(c, v, out)\n     return out\n+\n+\n+def slogdet(x):\n+    x = convert_to_tensor(x)\n+    return tuple(torch.linalg.slogdet(x))\n\n@@ -1,148 +1,5 @@\n-\"\"\"\n-MANIFEST:\n-\n-abs\n-absolute\n-add\n-all\n-amax\n-amin\n-append\n-arange\n-arccos\n-arccosh\n-arcsin\n-arcsinh\n-arctan\n-arctan2\n-arctanh\n-argmax\n-argmin\n-argsort\n-array\n-average\n-bincount\n-broadcast_to\n-ceil\n-clip\n-concatenate\n-conj\n-conjugate\n-copy\n-correlate\n-cos\n-cosh\n-count_nonzero\n-cross\n-cumprod\n-cumsum\n-diag\n-diagonal\n-diff\n-digitize\n-divide\n-dot\n-dtype\n-einsum\n-empty\n-equal\n-exp\n-expand_dims\n-expm1\n-eye\n-flip\n-floor\n-full\n-full_like\n-greater\n-greater_equal\n-hstack\n-identity\n-imag\n-interp\n-isclose\n-isfinite\n-isinf\n-isnan\n-less\n-less_equal\n-linspace\n-log\n-log10\n-log1p\n-log2\n-logaddexp\n-logical_and\n-logical_not\n-logical_or\n-logspace\n-matmul\n-max\n-maximum\n-mean\n-median\n-meshgrid\n-mgrid\n-min\n-minimum\n-mod\n-moveaxis\n-multiply\n-nan_to_num\n-ndim\n-nonzero\n-not_equal\n-ones\n-ones_like\n-outer\n-pad\n-percentile\n-power\n-prod\n-quantile\n-ravel\n-real\n-reciprocal\n-repeat\n-reshape\n-roll\n-round\n-sign\n-sin\n-sinh\n-size\n-sort\n-split\n-sqrt\n-square\n-squeeze\n-stack\n-std\n-subtract\n-sum\n-swapaxes\n-take\n-take_along_axis\n-tan\n-tanh\n-tensordot\n-tile\n-trace\n-transpose\n-tri\n-tril\n-triu\n-true_divide\n-vdot\n-vstack\n-where\n-zeros\n-zeros_like\n-\n-\n-\"\"\"\n-\n import builtins\n+import collections\n import re\n \n import numpy as np\n@@ -6247,7 +6104,7 @@ class Select(Operation):\n         super().__init__()\n \n     def call(self, condlist, choicelist, default=0):\n-        return backend.numpy.correlate(condlist, choicelist, default)\n+        return backend.numpy.select(condlist, choicelist, default)\n \n     def compute_output_spec(self, condlist, choicelist, default=0):\n         first_element = choicelist[0]\n@@ -6303,3 +6160,35 @@ def select(condlist, choicelist, default=0):\n     if any_symbolic_tensors(condlist + choicelist + [default]):\n         return Select().symbolic_call(condlist, choicelist, default)\n     return backend.numpy.select(condlist, choicelist, default)\n+\n+\n+class Slogdet(Operation):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return backend.numpy.slogdet(x)\n+\n+    def compute_output_spec(self, x):\n+        sign = KerasTensor((), dtype=x.dtype)\n+        logabsdet = KerasTensor((), dtype=x.dtype)\n+        return (sign, logabsdet)\n+\n+\n+@keras_export([\"keras.ops.slogdet\", \"keras.ops.numpy.slogdet\"])\n+def slogdet(x):\n+    \"\"\"Compute the sign and natural logarithm of the determinant of a matrix.\n+\n+    Args:\n+        x: Input matrix. It must 2D and square.\n+\n+    Returns:\n+        A tuple `(sign, logabsdet)`. `sign` is a number representing\n+        the sign of the determinant. For a real matrix, this is 1, 0, or -1.\n+        For a complex matrix, this is a complex number with absolute value 1\n+        (i.e., it is on the unit circle), or else 0.\n+        `logabsdet` is the natural log of the absolute value of the determinant.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Slogdet().symbolic_call(x)\n+    return backend.numpy.slogdet(x)\n\n@@ -4250,6 +4250,17 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         y = knp.select(condlist, choicelist, 42)\n         self.assertEqual(y.shape, (6,))\n \n+    def test_slogdet(self):\n+        x = np.ones((4, 4)) * 2.0\n+        out = knp.slogdet(x)\n+        self.assertAllClose(out[0], 0)\n+        self.assertAllClose(out[0], 0)\n+\n+        x = backend.KerasTensor((3, 3))\n+        out = knp.slogdet(x)\n+        self.assertEqual(out[0].shape, ())\n+        self.assertEqual(out[1].shape, ())\n+\n     def test_nan_to_num(self):\n         x = knp.array([1.0, np.nan, np.inf, -np.inf])\n         self.assertAllClose(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3c035d5fc72e37deb1d5fe8c20d8ab4608bdf55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 754 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -196,6 +196,7 @@ from keras.src.ops.numpy import sign\n from keras.src.ops.numpy import sin\n from keras.src.ops.numpy import sinh\n from keras.src.ops.numpy import size\n+from keras.src.ops.numpy import slogdet\n from keras.src.ops.numpy import sort\n from keras.src.ops.numpy import split\n from keras.src.ops.numpy import sqrt\n\n@@ -117,6 +117,7 @@ from keras.src.ops.numpy import sign\n from keras.src.ops.numpy import sin\n from keras.src.ops.numpy import sinh\n from keras.src.ops.numpy import size\n+from keras.src.ops.numpy import slogdet\n from keras.src.ops.numpy import sort\n from keras.src.ops.numpy import split\n from keras.src.ops.numpy import sqrt\n\n@@ -196,6 +196,7 @@ from keras.src.ops.numpy import sign\n from keras.src.ops.numpy import sin\n from keras.src.ops.numpy import sinh\n from keras.src.ops.numpy import size\n+from keras.src.ops.numpy import slogdet\n from keras.src.ops.numpy import sort\n from keras.src.ops.numpy import split\n from keras.src.ops.numpy import sqrt\n\n@@ -117,6 +117,7 @@ from keras.src.ops.numpy import sign\n from keras.src.ops.numpy import sin\n from keras.src.ops.numpy import sinh\n from keras.src.ops.numpy import size\n+from keras.src.ops.numpy import slogdet\n from keras.src.ops.numpy import sort\n from keras.src.ops.numpy import split\n from keras.src.ops.numpy import sqrt\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e22cadb8d75f84dff09e2354496ecae353c7d03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 403 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import builtins\n-import collections\n import re\n \n import numpy as np\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74df92625eb5dc5a403b820a855fa50ede6ad921", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 167 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 17/6 | Churn Δ: 167 | Churn Cumulative: 2439 | Contributors (this commit): 5 | Commits (past 90d): 48 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -72,6 +72,7 @@ from keras.src.ops.nn import moments\n from keras.src.ops.nn import multi_hot\n from keras.src.ops.nn import normalize\n from keras.src.ops.nn import one_hot\n+from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n from keras.src.ops.nn import selu\n\n@@ -26,6 +26,7 @@ from keras.src.ops.nn import moments\n from keras.src.ops.nn import multi_hot\n from keras.src.ops.nn import normalize\n from keras.src.ops.nn import one_hot\n+from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n from keras.src.ops.nn import selu\n\n@@ -72,6 +72,7 @@ from keras.src.ops.nn import moments\n from keras.src.ops.nn import multi_hot\n from keras.src.ops.nn import normalize\n from keras.src.ops.nn import one_hot\n+from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n from keras.src.ops.nn import selu\n\n@@ -26,6 +26,7 @@ from keras.src.ops.nn import moments\n from keras.src.ops.nn import multi_hot\n from keras.src.ops.nn import normalize\n from keras.src.ops.nn import one_hot\n+from keras.src.ops.nn import psnr\n from keras.src.ops.nn import relu\n from keras.src.ops.nn import relu6\n from keras.src.ops.nn import selu\n\n@@ -926,3 +926,16 @@ def ctc_decode(\n             f\"Invalid strategy {strategy}. Supported values are \"\n             \"'greedy' and 'beam_search'.\"\n         )\n+\n+\n+def psnr(x1, x2, max_val):\n+    if x1.shape != x2.shape:\n+        raise ValueError(\n+            f\"Input shapes {x1.shape} and {x2.shape} must \"\n+            \"match for PSNR calculation. \"\n+        )\n+\n+    max_val = convert_to_tensor(max_val, dtype=x2.dtype)\n+    mse = jnp.mean(jnp.square(x1 - x2))\n+    psnr = 20 * jnp.log10(max_val) - 10 * jnp.log10(mse)\n+    return psnr\n\n@@ -967,3 +967,16 @@ def ctc_decode(\n             f\"Invalid strategy {strategy}. Supported values are \"\n             \"'greedy' and 'beam_search'.\"\n         )\n+\n+\n+def psnr(x1, x2, max_val):\n+    if x1.shape != x2.shape:\n+        raise ValueError(\n+            f\"Input shapes {x1.shape} and {x2.shape} must \"\n+            \"match for PSNR calculation. \"\n+        )\n+\n+    max_val = convert_to_tensor(max_val, dtype=x2.dtype)\n+    mse = np.mean(np.square(x1 - x2))\n+    psnr = 20 * np.log10(max_val) - 10 * np.log10(mse)\n+    return psnr\n\n@@ -846,3 +846,18 @@ def ctc_decode(\n     decoded_dense = tf.stack(decoded_dense, axis=0)\n     decoded_dense = tf.cast(decoded_dense, \"int32\")\n     return decoded_dense, scores\n+\n+\n+def psnr(x1, x2, max_val):\n+    from keras.src.backend.tensorflow.numpy import log10\n+\n+    if x1.shape != x2.shape:\n+        raise ValueError(\n+            f\"Input shapes {x1.shape} and {x2.shape} must \"\n+            \"match for PSNR calculation. \"\n+        )\n+\n+    max_val = convert_to_tensor(max_val, dtype=x2.dtype)\n+    mse = tf.reduce_mean(tf.square(x1 - x2))\n+    psnr = 20 * log10(max_val) - 10 * log10(mse)\n+    return psnr\n\n@@ -848,3 +848,20 @@ def ctc_decode(\n             f\"Invalid strategy {strategy}. Supported values are \"\n             \"'greedy' and 'beam_search'.\"\n         )\n+\n+\n+def psnr(x1, x2, max_val):\n+    if x1.shape != x2.shape:\n+        raise ValueError(\n+            f\"Input shapes {x1.shape} and {x2.shape} must \"\n+            \"match for PSNR calculation. \"\n+        )\n+\n+    x1, x2 = (\n+        convert_to_tensor(x1),\n+        convert_to_tensor(x2),\n+    )\n+    max_val = convert_to_tensor(max_val, dtype=x1.dtype)\n+    mse = torch.mean((x1 - x2) ** 2)\n+    psnr = 20 * torch.log10(max_val) - 10 * torch.log10(mse)\n+    return psnr\n\n@@ -2042,3 +2042,77 @@ def _normalize(x, axis=-1, order=2):\n     norm = backend.linalg.norm(x, ord=order, axis=axis, keepdims=True)\n     denom = backend.numpy.maximum(norm, epsilon)\n     return backend.numpy.divide(x, denom)\n+\n+\n+class PSNR(Operation):\n+    def __init__(\n+        self,\n+        max_val,\n+    ):\n+        super().__init__()\n+        self.max_val = max_val\n+\n+    def call(self, x1, x2):\n+        return backend.nn.psnr(\n+            x1=x1,\n+            x2=x2,\n+            max_val=self.max_val,\n+        )\n+\n+    def compute_output_spec(self, x1, x2):\n+        if len(x1.shape) != len(x2.shape):\n+            raise ValueError(\"Inputs must have the same rank\")\n+\n+        return KerasTensor(shape=())\n+\n+\n+@keras_export(\n+    [\n+        \"keras.ops.psnr\",\n+        \"keras.ops.nn.psnr\",\n+    ]\n+)\n+def psnr(\n+    x1,\n+    x2,\n+    max_val,\n+):\n+    \"\"\"Peak Signal-to-Noise Ratio (PSNR) calculation.\n+\n+    This function calculates the Peak Signal-to-Noise Ratio between two signals,\n+    `x1` and `x2`. PSNR is a measure of the quality of a reconstructed signal.\n+    The higher the PSNR, the closer the reconstructed signal is to the original\n+    signal.\n+\n+    Args:\n+        x1: The first input signal.\n+        x2: The second input signal. Must have the same shape as `x1`.\n+        max_val: The maximum possible value in the signals.\n+\n+    Returns:\n+        float: The PSNR value between `x1` and `x2`.\n+\n+    Examples:\n+        >>> import numpy as np\n+        >>> from keras import ops\n+        >>> x = np.random.random((2, 4, 4, 3))\n+        >>> y = np.random.random((2, 4, 4, 3))\n+        >>> max_val = 1.0\n+        >>> psnr_value = ops.nn.psnr(x, y, max_val)\n+        >>> psnr_value\n+        20.0\n+    \"\"\"\n+    if any_symbolic_tensors(\n+        (\n+            x1,\n+            x2,\n+        )\n+    ):\n+        return PSNR(\n+            max_val,\n+        ).symbolic_call(x1, x2)\n+    return backend.nn.psnr(\n+        x1,\n+        x2,\n+        max_val,\n+    )\n\n@@ -654,6 +654,12 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.normalize(x).shape, (None, 2, 3))\n \n+    def test_psnr(self):\n+        x1 = KerasTensor([None, 2, 3])\n+        x2 = KerasTensor([None, 5, 6])\n+        out = knn.psnr(x1, x2, max_val=224)\n+        self.assertEqual(out.shape, ())\n+\n \n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n@@ -1114,6 +1120,12 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.normalize(x).shape, (1, 2, 3))\n \n+    def test_psnr(self):\n+        x1 = KerasTensor([1, 2, 3])\n+        x2 = KerasTensor([5, 6, 7])\n+        out = knn.psnr(x1, x2, max_val=224)\n+        self.assertEqual(out.shape, ())\n+\n \n class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_relu(self):\n@@ -2032,6 +2044,25 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             ],\n         )\n \n+    def test_psnr(self):\n+        x1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n+        x2 = np.array([[0.2, 0.2, 0.3], [0.4, 0.6, 0.6]])\n+        max_val = 1.0\n+        expected_psnr_1 = 20 * np.log10(max_val) - 10 * np.log10(\n+            np.mean(np.square(x1 - x2))\n+        )\n+        psnr_1 = knn.psnr(x1, x2, max_val)\n+        self.assertAlmostEqual(psnr_1, expected_psnr_1)\n+\n+        x3 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n+        x4 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n+        max_val = 1.0\n+        expected_psnr_2 = 20 * np.log10(max_val) - 10 * np.log10(\n+            np.mean(np.square(x3 - x4))\n+        )\n+        psnr_2 = knn.psnr(x3, x4, max_val)\n+        self.assertAlmostEqual(psnr_2, expected_psnr_2)\n+\n \n class NNOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n     \"\"\"Test the dtype to verify that the behavior matches JAX.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
