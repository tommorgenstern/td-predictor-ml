{"custom_id": "keras#e95dc8efc9bd350db79f81a4208033c5f7685196", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 710 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -83,15 +83,24 @@ def to_categorical(y, num_classes=None, dtype='float32'):\n \n @keras_export('keras.utils.normalize')\n def normalize(x, axis=-1, order=2):\n-  \"\"\"Normalizes a Numpy array.\n+  \"\"\"Normalizes a NumPy array.\n \n   Args:\n-      x: Numpy array to normalize.\n-      axis: axis along which to normalize.\n-      order: Normalization order (e.g. `order=2` for L2 norm).\n+      x: NumPy array to normalize.\n+      axis: Axis along which to normalize. For instance, `axis=-1` corresponds\n+        to feature-wise normalization.\n+      order: Normalization order (e.g. `order=2` for the L2 norm).\n \n   Returns:\n       A normalized copy of the array.\n+\n+  Example:\n+\n+  >>> array = np.random.random(size=(32, 3))\n+  >>> normalized_array = tf.keras.utils.normalize(array, axis=-1)\n+  >>> # Every element in the batch has now a unit norm\n+  >>> for i in range(32):\n+  >>>     np.testing.assert_allclose(np.square(normalized_array[i, :]).sum(), 1)\n   \"\"\"\n   l2 = np.atleast_1d(np.linalg.norm(x, order, axis))\n   l2[l2 == 0] = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca9dcdd437e608caf09930ea143d4a7b01a6c006", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 19 | Churn Cumulative: 1333 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,14 +22,10 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.framework import test_util\n from keras import combinations\n from keras.optimizer_v2 import adadelta\n \n-_DATA_TYPES = [tf.half, tf.float32, tf.float64]\n-# TODO(b/143684500): Eigen to support complex sqrt\n-if not test_util.IsBuiltWithNvcc():\n-  _DATA_TYPES += [tf.complex64, tf.complex128]\n+_DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n \n \n class AdadeltaOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -24,15 +24,11 @@ import copy\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.framework import test_util\n from keras import combinations\n from keras.optimizer_v2 import adagrad\n from keras.optimizer_v2 import learning_rate_schedule\n \n-_DATA_TYPES = [tf.half, tf.float32, tf.float64]\n-# TODO(b/143684500): Eigen to support complex sqrt\n-if not test_util.IsBuiltWithNvcc():\n-  _DATA_TYPES += [tf.complex64, tf.complex128]\n+_DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n \n \n def adagrad_update_numpy(param, accum, g_t, lr=0.001, epsilon=1e-7):\n\n@@ -32,10 +32,7 @@ from keras import testing_utils\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.optimizer_v2 import rmsprop\n \n-_DATA_TYPES = [tf.half, tf.float32, tf.float64]\n-# TODO(b/143684500): Eigen to support complex sqrt\n-if not test_util.IsBuiltWithNvcc():\n-  _DATA_TYPES += [tf.complex64, tf.complex128]\n+_DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n \n _TEST_PARAM_VALUES = [\n     # learning_rate, rho, momentum, epsilon, centered\n@@ -346,8 +343,6 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     with tf.Graph().as_default():\n       for dtype in _DATA_TYPES:\n-        if test_util.is_xla_enabled() and dtype.is_complex:\n-          self.skipTest(\"b/143578550\")\n         var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n         x = tf.constant([[4.0], [5.0]], dtype=dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2321565d6904eab2b2898e7d5d2ab6f698b25611", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 6740 | Contributors (this commit): 89 | Commits (past 90d): 12 | Contributors (cumulative): 89 | DMM Complexity: None\n\nDIFF:\n@@ -1086,22 +1086,33 @@ class ProgbarLogger(Callback):\n class History(Callback):\n   \"\"\"Callback that records events into a `History` object.\n \n-  This callback is automatically applied to\n-  every Keras model. The `History` object\n-  gets returned by the `fit` method of models.\n+  This callback gets added by default to\n+  every call of `model.fit()`. That's how Keras keeps track of model training\n+  history. You should never have to add this callback manually.\n+\n+  The `History` callback instance that is automatically added to `fit()`\n+  gets returned at the end of `fit()`.\n+  This `History` instance contains a `.history` attribute, which is a dict\n+  that stores the logs of the model, such as the loss values for each\n+  training epoch.\n \n   Example:\n \n   >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n-  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n-  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n-  ...                     epochs=10)\n+  >>> model.compile(optimizer='sgd', loss='mse')\n+  >>> history = model.fit(np.zeros((32, 5)), np.zeros((32, 10)),\n+  ...                     epochs=5)\n+  >>> # Inspect training parameters\n   >>> print(history.params)\n-  {'verbose': 1, 'epochs': 10, 'steps': 1}\n-  >>> # check the keys of history object\n+  {'verbose': 1, 'epochs': 5, 'steps': 1}\n+  >>> # Inspect the keys of the `history` dict\n   >>> print(history.history.keys())\n   dict_keys(['loss'])\n-\n+  >>> print(history.history.keys())\n+  dict_keys(['loss'])\n+  >>> # This is the loss history over the 5 training epochs\n+  >>> history.history['loss']\n+  [0.0, 0.0, 0.0, 0.0, 0.0]\n   \"\"\"\n \n   def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fc995e61bfacce25d29eb039c55fa3096e61ef3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1436 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -25,11 +25,11 @@ import types\n \n from google.protobuf import message\n \n-from tensorflow.core.framework import versions_pb2\n from keras import backend\n from keras import regularizers\n from keras.engine import input_spec\n from keras.protobuf import saved_metadata_pb2\n+from keras.protobuf import versions_pb2\n from keras.saving import saving_utils\n from keras.saving.saved_model import constants\n from keras.saving.saved_model import json_utils\n\n@@ -21,9 +21,9 @@ import tensorflow.compat.v2 as tf\n \n import os\n \n-from tensorflow.core.framework import versions_pb2\n from keras import backend as K\n from keras.protobuf import saved_metadata_pb2\n+from keras.protobuf import versions_pb2\n from keras.saving import saving_utils\n from keras.saving.saved_model import constants\n from keras.saving.saved_model import save_impl\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f1cc1032ea51e5762ea4ed24e33deb33bc37075", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 984 | Lines Deleted: 471 | Files Changed: 62 | Hunks: 357 | Methods Changed: 163 | Complexity Δ (Sum/Max): -15/5 | Churn Δ: 1455 | Churn Cumulative: 114694 | Contributors (this commit): 332 | Commits (past 90d): 266 | Contributors (cumulative): 627 | DMM Complexity: 0.6857142857142857\n\nDIFF:\n@@ -739,6 +739,23 @@ EfficientNetB7.__doc__ = BASE_DOCSTRING.format(name='EfficientNetB7')\n \n @keras_export('keras.applications.efficientnet.preprocess_input')\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+  \"\"\"A placeholder method for backward compatibility.\n+\n+  The preprocessing logic has been included in the efficientnet model\n+  implementation. Users are no longer required to call this method to normalize\n+  the input data. This method does nothing and only kept as a placeholder to\n+  align the API surface between old and new version of model.\n+\n+  Args:\n+    x: A floating point `numpy.array` or a `tf.Tensor`.\n+    data_format: Optional data format of the image tensor/array. Defaults to\n+      None, in which case the global setting\n+      `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+      it defaults to \"channels_last\").{mode}\n+\n+  Returns:\n+    Unchanged `numpy.array` or `tf.Tensor`.\n+  \"\"\"\n   return x\n \n \n\n@@ -592,6 +592,27 @@ class CallbackList(object):\n class Callback(object):\n   \"\"\"Abstract base class used to build new callbacks.\n \n+  Callbacks can be passed to keras methods such as `fit`, `evaluate`, and\n+  `predict` in order to hook into the various stages of the model training and\n+  inference lifecycle.\n+\n+  To create a custom callback, subclass `keras.callbacks.Callback` and override\n+  the method associated with the stage of interest. See\n+  https://www.tensorflow.org/guide/keras/custom_callback for more information.\n+\n+  Example:\n+\n+  >>> training_finished = False\n+  >>> class MyCallback(tf.keras.callbacks.Callback):\n+  ...   def on_train_end(self, logs=None):\n+  ...     global training_finished\n+  ...     training_finished = True\n+  >>> model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n+  >>> model.compile(loss='mean_squared_error')\n+  >>> model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]),\n+  ...           callbacks=[MyCallback()])\n+  >>> assert training_finished == True\n+\n   Attributes:\n       params: Dict. Training parameters\n           (eg. verbosity, batch size, number of epochs...).\n\n@@ -31,11 +31,53 @@ from tensorflow.tools.docs import doc_controls\n \n @keras_export('keras.constraints.Constraint')\n class Constraint(object):\n+  \"\"\"Base class for weight constraints.\n+\n+  A `Constraint` instance works like a stateless function.\n+  Users who subclass this\n+  class should override the `__call__` method, which takes a single\n+  weight parameter and return a projected version of that parameter\n+  (e.g. normalized or clipped). Constraints can be used with various Keras\n+  layers via the `kernel_constraint` or `bias_constraint` arguments.\n+\n+  Here's a simple example of a non-negative weight constraint:\n+\n+  >>> class NonNegative(tf.keras.constraints.Constraint):\n+  ...\n+  ...  def __call__(self, w):\n+  ...    return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n+\n+  >>> weight = tf.constant((-1.0, 1.0))\n+  >>> NonNegative()(weight)\n+  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.,  1.], dtype=float32)>\n+\n+  >>> tf.keras.layers.Dense(4, kernel_constraint=NonNegative())\n+  \"\"\"\n \n   def __call__(self, w):\n+    \"\"\"Applies the constraint to the input weight variable.\n+\n+    By default, the inputs weight variable is not modified.\n+    Users should override this method to implement their own projection\n+    function.\n+\n+    Args:\n+      w: Input weight variable.\n+\n+    Returns:\n+      Projected variable (by default, returns unmodified inputs).\n+    \"\"\"\n     return w\n \n   def get_config(self):\n+    \"\"\"Returns a Python dict of the object config.\n+\n+    A constraint config is a Python dictionary (JSON-serializable) that can\n+    be used to reinstantiate the same object.\n+\n+    Returns:\n+      Python dict containing the configuration of the constraint object.\n+    \"\"\"\n     return {}\n \n \n\n@@ -120,7 +120,7 @@ class MirroredStrategyDefunTest(tf.test.TestCase, parameterized.TestCase):\n       num_epochs = 4\n       num_steps = 7\n       for _ in range(num_epochs):\n-        accuracy.reset_states()\n+        accuracy.reset_state()\n         for _ in range(num_steps):\n           train_step(distributed_iterator)\n \n\n@@ -292,7 +292,7 @@ class KPLMultiWorkerTest(tf.test.TestCase,\n       num_epochs = 4\n       num_steps = 7\n       for _ in range(num_epochs):\n-        accuracy.reset_states()\n+        accuracy.reset_state()\n         for _ in range(num_steps):\n           train_step(distributed_iterator)\n \n\n@@ -167,7 +167,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n     distributed_dataset = self.coordinator.create_per_worker_dataset(dataset_fn)\n     distributed_iterator = iter(distributed_dataset)\n     for _ in range(4):\n-      accuracy.reset_states()\n+      accuracy.reset_state()\n       for _ in range(7):\n         self.coordinator.schedule(worker_fn, args=(distributed_iterator,))\n       self.coordinator.join()\n\n@@ -1683,40 +1683,40 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           update()  # pylint: disable=not-callable\n \n   def set_weights(self, weights):\n-    \"\"\"Sets the weights of the layer, from Numpy arrays.\n+    \"\"\"Sets the weights of the layer, from NumPy arrays.\n \n     The weights of a layer represent the state of the layer. This function\n     sets the weight values from numpy arrays. The weight values should be\n     passed in the order they are created by the layer. Note that the layer's\n-    weights must be instantiated before calling this function by calling\n+    weights must be instantiated before calling this function, by calling\n     the layer.\n \n-    For example, a Dense layer returns a list of two values-- per-output\n-    weights and the bias value. These can be used to set the weights of another\n-    Dense layer:\n+    For example, a `Dense` layer returns a list of two values: the kernel matrix\n+    and the bias vector. These can be used to set the weights of another\n+    `Dense` layer:\n \n-    >>> a = tf.keras.layers.Dense(1,\n+    >>> layer_a = tf.keras.layers.Dense(1,\n     ...   kernel_initializer=tf.constant_initializer(1.))\n-    >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n-    >>> a.get_weights()\n+    >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n+    >>> layer_a.get_weights()\n     [array([[1.],\n            [1.],\n            [1.]], dtype=float32), array([0.], dtype=float32)]\n-    >>> b = tf.keras.layers.Dense(1,\n+    >>> layer_b = tf.keras.layers.Dense(1,\n     ...   kernel_initializer=tf.constant_initializer(2.))\n-    >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n-    >>> b.get_weights()\n+    >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n+    >>> layer_b.get_weights()\n     [array([[2.],\n            [2.],\n            [2.]], dtype=float32), array([0.], dtype=float32)]\n-    >>> b.set_weights(a.get_weights())\n-    >>> b.get_weights()\n+    >>> layer_b.set_weights(layer_a.get_weights())\n+    >>> layer_b.get_weights()\n     [array([[1.],\n            [1.],\n            [1.]], dtype=float32), array([0.], dtype=float32)]\n \n     Args:\n-        weights: a list of Numpy arrays. The number\n+      weights: a list of NumPy arrays. The number\n         of arrays and their shape must match\n         number of the dimensions of the weights\n         of the layer (i.e. it should match the\n@@ -1763,39 +1763,39 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     backend.batch_set_value(weight_value_tuples)\n \n   def get_weights(self):\n-    \"\"\"Returns the current weights of the layer.\n+    \"\"\"Returns the current weights of the layer, as NumPy arrays.\n \n     The weights of a layer represent the state of the layer. This function\n     returns both trainable and non-trainable weight values associated with this\n-    layer as a list of Numpy arrays, which can in turn be used to load state\n+    layer as a list of NumPy arrays, which can in turn be used to load state\n     into similarly parameterized layers.\n \n-    For example, a Dense layer returns a list of two values-- per-output\n-    weights and the bias value. These can be used to set the weights of another\n-    Dense layer:\n+    For example, a `Dense` layer returns a list of two values: the kernel matrix\n+    and the bias vector. These can be used to set the weights of another\n+    `Dense` layer:\n \n-    >>> a = tf.keras.layers.Dense(1,\n+    >>> layer_a = tf.keras.layers.Dense(1,\n     ...   kernel_initializer=tf.constant_initializer(1.))\n-    >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n-    >>> a.get_weights()\n+    >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n+    >>> layer_a.get_weights()\n     [array([[1.],\n            [1.],\n            [1.]], dtype=float32), array([0.], dtype=float32)]\n-    >>> b = tf.keras.layers.Dense(1,\n+    >>> layer_b = tf.keras.layers.Dense(1,\n     ...   kernel_initializer=tf.constant_initializer(2.))\n-    >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n-    >>> b.get_weights()\n+    >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n+    >>> layer_b.get_weights()\n     [array([[2.],\n            [2.],\n            [2.]], dtype=float32), array([0.], dtype=float32)]\n-    >>> b.set_weights(a.get_weights())\n-    >>> b.get_weights()\n+    >>> layer_b.set_weights(layer_a.get_weights())\n+    >>> layer_b.get_weights()\n     [array([[1.],\n            [1.],\n            [1.]], dtype=float32), array([0.], dtype=float32)]\n \n     Returns:\n-        Weights values as a list of numpy arrays.\n+        Weights values as a list of NumPy arrays.\n     \"\"\"\n     weights = self.weights\n     output_weights = []\n\n@@ -45,6 +45,18 @@ keras_kpl_gauge = tf.__internal__.monitoring.BoolGauge(\n class PreprocessingLayer(Layer):\n   \"\"\"Base class for Preprocessing Layers.\n \n+  **Don't use this class directly: it's an abstract base class!** You may\n+  be looking for one of the many built-in\n+  [preprocessing layers](https://keras.io/guides/preprocessing_layers/)\n+  instead.\n+\n+  Preprocessing layers are layers whose state gets computed before model\n+  training starts. They do not get updated during training.\n+  Most preprocessing layers implement an `adapt()` method for state computation.\n+\n+  The `PreprocessingLayer` class is the base class you would subclass to\n+  implement your own preprocessing layers.\n+\n   Attributes:\n     stateful: Whether the layer contains state that needs to be adapted via\n       `PreprocessingLayer.adapt`.\n\n@@ -251,14 +251,14 @@ class LossesContainer(Container):\n       # Ok for a model to have no compiled loss.\n       return tf.zeros(shape=())\n \n-  def reset_states(self):\n+  def reset_state(self):\n     \"\"\"Resets the state of loss metrics.\"\"\"\n     if not self._built:\n       return\n     metrics = [self._loss_metric] + tf.nest.flatten(self._per_output_metrics)\n     for metric_obj in metrics:\n       if metric_obj is not None:\n-        metric_obj.reset_states()\n+        metric_obj.reset_state()\n \n   def _get_loss_object(self, loss):\n     \"\"\"Returns a `Loss` object.\n@@ -466,7 +466,7 @@ class MetricsContainer(Container):\n           continue\n         weighted_metric_obj.update_state(y_t, y_p, sample_weight=sw)\n \n-  def reset_states(self):\n+  def reset_state(self):\n     \"\"\"Resets the state of all `Metric`s in this container.\"\"\"\n     if self._built:\n       metrics = self._metrics_in_order\n@@ -479,7 +479,7 @@ class MetricsContainer(Container):\n \n     for metric_obj in metrics:\n       if isinstance(metric_obj, metrics_mod.Metric):\n-        metric_obj.reset_states()\n+        metric_obj.reset_state()\n \n   def _get_metric_objects(self, metrics, y_t, y_p):\n     \"\"\"Convert user-supplied metrics to `Metric` objects.\"\"\"\n\n@@ -42,7 +42,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(loss_metric.name, 'loss')\n     self.assertEqual(loss_metric.result().numpy(), 1.)\n \n-    loss_container.reset_states()\n+    loss_container.reset_state()\n     self.assertEqual(loss_metric.result().numpy(), 0.)\n \n   def test_loss_list(self):\n@@ -71,7 +71,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(output_2_metric.name, 'output_2_loss')\n     self.assertEqual(output_2_metric.result().numpy(), 0.5)\n \n-    loss_container.reset_states()\n+    loss_container.reset_state()\n     self.assertEqual(loss_metric.result().numpy(), 0)\n     self.assertEqual(output_1_metric.result().numpy(), 0)\n     self.assertEqual(output_2_metric.result().numpy(), 0)\n@@ -108,7 +108,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(out2_metric.name, 'out2_loss')\n     self.assertEqual(out2_metric.result().numpy(), 0.5)\n \n-    loss_container.reset_states()\n+    loss_container.reset_state()\n     self.assertEqual(loss_metric.result().numpy(), 0)\n     self.assertEqual(out1_metric.result().numpy(), 0)\n     self.assertEqual(out2_metric.result().numpy(), 0)\n@@ -405,7 +405,7 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(metric.name, 'mse')\n     self.assertEqual(metric.result().numpy(), 1.)\n \n-    metric_container.reset_states()\n+    metric_container.reset_state()\n     self.assertEqual(metric.result().numpy(), 0.)\n \n   def test_list_of_metrics_one_output(self):\n@@ -422,7 +422,7 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(mae_metric.name, 'mae')\n     self.assertEqual(mae_metric.result().numpy(), 2.)\n \n-    metric_container.reset_states()\n+    metric_container.reset_state()\n     self.assertEqual(mse_metric.result().numpy(), 0.)\n     self.assertEqual(mae_metric.result().numpy(), 0.)\n \n@@ -507,7 +507,7 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(weighted_mae_metric.name, 'out2_weighted_mae')\n     self.assertEqual(weighted_mae_metric.result().numpy(), 2.)\n \n-    metric_container.reset_states()\n+    metric_container.reset_state()\n     self.assertEqual(mse_metric.result().numpy(), 0.)\n     self.assertEqual(weighted_mse_metric.result().numpy(), 0.)\n     self.assertEqual(mae_metric.result().numpy(), 0.)\n@@ -782,13 +782,13 @@ class MetricsContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(metric_container.metrics[0].name, 'custom_metric_fn')\n     self.assertEqual(metric_container.metrics[1].name, 'custom_metric_class')\n \n-  def test_reset_states_existing_metric_before_built(self):\n+  def test_reset_state_existing_metric_before_built(self):\n     metric = metrics_mod.Mean()\n     metric.update_state([2.0, 4.0])\n     self.assertEqual(metric.result().numpy(), 3.0)\n \n     metric_container = compile_utils.MetricsContainer(metric)\n-    metric_container.reset_states()\n+    metric_container.reset_state()\n     self.assertEqual(metric.result().numpy(), 0.0)\n \n \n\n@@ -1704,7 +1704,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     \"\"\"\n     for m in self.metrics:\n-      m.reset_states()\n+      m.reset_state()\n \n   def train_on_batch(self,\n                      x,\n\n@@ -985,7 +985,7 @@ class Model(training_lib.Model):\n     \"\"\"Resets the state of metrics.\"\"\"\n     metrics = self._get_training_eval_metrics()\n     for m in metrics:\n-      m.reset_states()\n+      m.reset_state()\n \n     # Reset metrics on all the distributed (cloned) models.\n     if self._distribution_strategy:\n\n@@ -21,8 +21,6 @@ from __future__ import print_function\n import tensorflow.compat.v2 as tf\n \n import json\n-\n-from tensorflow.python.feature_column import feature_column_v2 as fc\n from keras import backend\n from keras.feature_column import base_feature_layer as kfc\n from keras.saving.saved_model import json_utils\n@@ -159,7 +157,7 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n     if not isinstance(features, dict):\n       raise ValueError('We expected a dictionary here. Instead we got: ',\n                        features)\n-    transformation_cache = fc.FeatureTransformationCache(features)\n+    transformation_cache = tf.__internal__.feature_column.FeatureTransformationCache(features)\n     output_tensors = []\n     for column in self._feature_columns:\n       with backend.name_scope(column.name):\n\n@@ -22,8 +22,6 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n-\n-from tensorflow.python.feature_column import feature_column_v2 as fc\n from keras import backend\n from keras.feature_column import base_feature_layer as kfc\n from tensorflow.python.util.tf_export import keras_export\n@@ -145,7 +143,7 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n                        features)\n     if training is None:\n       training = backend.learning_phase()\n-    transformation_cache = fc.FeatureTransformationCache(features)\n+    transformation_cache = tf.__internal__.feature_column.FeatureTransformationCache(features)\n     output_tensors = []\n     sequence_lengths = []\n \n\n@@ -150,6 +150,38 @@ def deserialize(config, custom_objects=None):\n \n @keras_export('keras.initializers.get')\n def get(identifier):\n+  \"\"\"Retrieve a Keras initializer by the identifier.\n+\n+  The `identifier` may be the string name of a initializers function or class (\n+  case-sensitively).\n+\n+  >>> identifier = 'Ones'\n+  >>> tf.keras.initializers.deserialize(identifier)\n+  <...tensorflow.python.keras.initializers.initializers_v2.Ones...>\n+\n+  You can also specify `config` of the initializer to this function by passing\n+  dict containing `class_name` and `config` as an identifier. Also note that the\n+  `class_name` must map to a `Initializer` class.\n+\n+  >>> cfg = {'class_name': 'Ones', 'config': {}}\n+  >>> tf.keras.initializers.deserialize(cfg)\n+  <...tensorflow.python.keras.initializers.initializers_v2.Ones...>\n+\n+  In the case that the `identifier` is a class, this method will return a new\n+  instance of the class by its constructor.\n+\n+  Args:\n+    identifier: String or dict that contains the initializer name or\n+      configurations.\n+\n+  Returns:\n+    Initializer instance base on the input identifier.\n+\n+  Raises:\n+    ValueError: If the input identifier is not a supported type or in a bad\n+      format.\n+  \"\"\"\n+\n   if identifier is None:\n     return None\n   if isinstance(identifier, dict):\n\n@@ -74,7 +74,7 @@ class CentralStorageStrategyTest(tf.test.TestCase, parameterized.TestCase):\n       num_epochs = 4\n       num_steps = 7\n       for _ in range(num_epochs):\n-        accuracy.reset_states()\n+        accuracy.reset_state()\n         for _ in range(num_steps):\n           train_step(distributed_iterator)\n \n\n@@ -317,7 +317,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n           logging.info('Epoch: %d, accuracy: %f, train_loss: %f.',\n                        epoch.numpy(), train_accuracy.result(), train_loss)\n \n-          train_accuracy.reset_states()\n+          train_accuracy.reset_state()\n \n           checkpoint_manager.save()\n           if not _is_chief(task_type, task_id):\n\n@@ -186,7 +186,7 @@ class TpuStrategyTest(tf.test.TestCase):\n       num_epochs = 4\n       num_steps = 7\n       for _ in range(num_epochs):\n-        accuracy.reset_states()\n+        accuracy.reset_state()\n         for _ in range(num_steps):\n           train_step(distributed_iterator)\n \n\n@@ -2257,11 +2257,22 @@ class SeparableConv2D(SeparableConv):\n \n @keras_export('keras.layers.DepthwiseConv2D')\n class DepthwiseConv2D(Conv2D):\n-  \"\"\"Depthwise separable 2D convolution.\n+  \"\"\"Depthwise 2D convolution.\n+\n+  Depthwise convolution is a type of convolution in which a single convolutional\n+  filter is apply to each input channel (i.e. in a depthwise way).\n+  You can understand depthwise convolution as being\n+  the first step in a depthwise separable convolution.\n+\n+  It is implemented via the following steps:\n+\n+  - Split the input into individual channels.\n+  - Convolve each input with the layer's kernel (called a depthwise kernel).\n+  - Stack the convolved outputs together (along the channels axis).\n+\n+  Unlike a regular 2D convolution, depthwise convolution does not mix\n+  information across different input channels.\n \n-  Depthwise Separable convolutions consist of performing\n-  just the first step in a depthwise spatial convolution\n-  (which acts on each input channel separately).\n   The `depth_multiplier` argument controls how many\n   output channels are generated per input channel in the depthwise step.\n \n@@ -2329,10 +2340,11 @@ class DepthwiseConv2D(Conv2D):\n \n   Output shape:\n     4D tensor with shape:\n-    `[batch_size, filters, new_rows, new_cols]` if data_format='channels_first'\n-    or 4D tensor with shape:\n-    `[batch_size, new_rows, new_cols, filters]` if data_format='channels_last'.\n-    `rows` and `cols` values might have changed due to padding.\n+    `[batch_size, channels * depth_multiplier, new_rows, new_cols]` if\n+    data_format='channels_first' or 4D tensor with shape:\n+    `[batch_size, new_rows, new_cols, channels * depth_multiplier]` if\n+    data_format='channels_last'. `rows` and `cols` values might have\n+    changed due to padding.\n \n   Returns:\n     A tensor of rank 4 representing\n\n@@ -699,10 +699,16 @@ class ConvLSTM2DCell(DropoutRNNCellMixin, Layer):\n \n @keras_export('keras.layers.ConvLSTM2D')\n class ConvLSTM2D(ConvRNN2D):\n-  \"\"\"Convolutional LSTM.\n+  \"\"\"2D Convolutional LSTM layer.\n \n-  It is similar to an LSTM layer, but the input transformations\n-  and recurrent transformations are both convolutional.\n+  A convolutional LSTM is similar to an LSTM, but the input transformations\n+  and recurrent transformations are both convolutional. This layer is typically\n+  used to process timeseries of images (i.e. video-like data).\n+\n+  It is known to perform well for weather data forecasting,\n+  using inputs that are timeseries of 2D grids of sensor values.\n+  It isn't usually applied to regular video data, due to its high computational\n+  cost.\n \n   Args:\n     filters: Integer, the dimensionality of the output space\n@@ -776,7 +782,7 @@ class ConvLSTM2D(ConvRNN2D):\n       the linear transformation of the recurrent state.\n \n   Call arguments:\n-    inputs: A 5D tensor.\n+    inputs: A 5D float tensor (see input shape description below).\n     mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n       a given timestep should be masked.\n     training: Python boolean indicating whether the layer should behave in\n@@ -824,6 +830,20 @@ class ConvLSTM2D(ConvRNN2D):\n     - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)\n     (the current implementation does not include the feedback loop on the\n     cells output).\n+\n+  Example:\n+\n+  ```python\n+  steps = 10\n+  height = 32\n+  width = 32\n+  input_channels = 3\n+  output_channels = 6\n+\n+  inputs = tf.keras.Input(shape=(steps, height, width, input_channels))\n+  layer = tf.keras.layers.ConvLSTM2D(filters=output_channels, kernel_size=3)\n+  outputs = layer(inputs)\n+  ```\n   \"\"\"\n \n   def __init__(self,\n\n@@ -202,6 +202,9 @@ class ConvLSTMTest(keras_parameterized.TestCase):\n       outputs = clone.predict(test_inputs)\n       self.assertAllClose(reference_outputs, outputs, atol=1e-5)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping the test as OOM occurred with 1 GB budget.')\n   def test_conv_lstm_with_initial_state(self):\n     num_samples = 32\n     sequence_len = 5\n\n@@ -46,10 +46,11 @@ class GRULayerTest(keras_parameterized.TestCase):\n                 'return_sequences': True},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Double type is not yet supported in ROCm')\n   @testing_utils.run_v2_only\n   def test_float64_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Double type is yet not supported in ROCm')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -132,9 +133,10 @@ class GRULayerTest(keras_parameterized.TestCase):\n     gru_model.fit(x_train, y_train)\n     gru_model.predict(x_train)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='MIOpen only supports packed input output')\n   def test_with_masking_layer_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('MIOpen only supports packed input output')\n     layer_class = keras.layers.GRU\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n@@ -148,9 +150,10 @@ class GRULayerTest(keras_parameterized.TestCase):\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='MIOpen only supports packed input output')\n   def test_statefulness_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('MIOpen only supports packed input output')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n\n@@ -136,12 +136,11 @@ class GRUV2Test(keras_parameterized.TestCase):\n       l2 = layer_class.from_config(l1.get_config())\n       assert l1.get_config() == l2.get_config()\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @testing_utils.run_v2_only\n   def test_gru_v2_feature_parity_with_canonical_gru(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     input_shape = 10\n     rnn_state_size = 8\n     timestep = 4\n@@ -309,11 +308,10 @@ class GRUV2Test(keras_parameterized.TestCase):\n \n     self.assertAllClose(y, y_ref)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_with_masking_layer_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     layer_class = rnn.GRU\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n@@ -325,11 +323,10 @@ class GRUV2Test(keras_parameterized.TestCase):\n                   optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.001))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_masking_with_stacking_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n@@ -353,11 +350,11 @@ class GRUV2Test(keras_parameterized.TestCase):\n                 'return_sequences': True},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Double type is not yet supported in ROCm')\n   @testing_utils.run_v2_only\n   def test_float64_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Double type is yet not supported in ROCm')\n-\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -370,11 +367,10 @@ class GRUV2Test(keras_parameterized.TestCase):\n         input_shape=(num_samples, timesteps, embedding_dim),\n         input_dtype='float64')\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_return_states_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     layer_class = rnn.GRU\n     x = np.random.random((2, 3, 4))\n     y = np.abs(np.random.random((2, 5)))\n@@ -454,11 +450,10 @@ class GRUV2Test(keras_parameterized.TestCase):\n     else:\n       self.assertEqual(len(layer.get_losses_for(x)), 1)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_statefulness_GRU(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -553,12 +548,11 @@ class GRUV2Test(keras_parameterized.TestCase):\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.fit(x, y, epochs=1, shuffle=False)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @testing_utils.run_v2_only\n   def test_explicit_device_with_go_backward_and_mask(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     batch_size = 8\n     timestep = 7\n     masksteps = 5\n@@ -735,12 +729,11 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n     model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])\n     self._test_runtime_with_model(model)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @testing_utils.run_v2_only\n   def test_GRU_runtime_with_mask(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n     layer = rnn.GRU(self.rnn_state_size, return_runtime=True)\n\n@@ -44,10 +44,11 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n                 'return_sequences': True},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Double type is yet not supported in ROCm')\n   @testing_utils.run_v2_only\n   def test_float64_LSTM(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Double type is yet not supported in ROCm')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -141,10 +142,10 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(layer.cell.bias.constraint, b_constraint)\n \n   @parameterized.parameters([True, False])\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input.')\n   def test_with_masking_layer_LSTM(self, unroll):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest(\n-          'Skipping the test as ROCm MIOpen does not support padded input.')\n     layer_class = keras.layers.LSTM\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n@@ -383,10 +384,10 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     else:\n       self.assertEqual(len(layer.get_losses_for(x)), 1)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input.')\n   def test_statefulness_LSTM(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest(\n-          'Skipping the test as ROCm MIOpen does not support padded input.')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n\n@@ -240,10 +240,10 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     targets = np.random.random((num_samples, units))\n     model.train_on_batch([inputs] + initial_state, targets)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_return_state(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n     num_states = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -310,11 +310,11 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     targets = np.random.random((num_samples, units))\n     model.train_on_batch([main_inputs] + initial_state, targets)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @testing_utils.run_v2_only\n   def test_lstm_v2_feature_parity_with_canonical_lstm(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n     input_shape = 10\n     rnn_state_size = 8\n     timestep = 4\n@@ -358,10 +358,10 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     self.assertAllClose(y_2, y_4, rtol=1e-5, atol=2e-5)\n \n   @parameterized.named_parameters(('v0', 0), ('v1', 1), ('v2', 2))\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_implementation_mode_LSTM(self, implementation_mode):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -403,10 +403,10 @@ class LSTMV2Test(keras_parameterized.TestCase):\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_masking_with_stacking_LSTM(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n     inputs = np.random.random((2, 3, 4))\n     targets = np.abs(np.random.random((2, 3, 5)))\n     targets /= targets.sum(axis=-1, keepdims=True)\n@@ -578,11 +578,11 @@ class LSTMV2Test(keras_parameterized.TestCase):\n         },\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support float64 yet.')\n   @testing_utils.run_v2_only\n   def test_float64_LSTM(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support float64 yet.')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -618,10 +618,10 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     else:\n       self.assertEqual(len(layer.get_losses_for(x)), 1)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_statefulness_LSTM(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n@@ -753,12 +753,11 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     model.evaluate(x, y)\n     model.predict(x)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @testing_utils.run_v2_only\n   def test_explicit_device_with_go_backward_and_mask(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     batch_size = 8\n     timestep = 7\n     masksteps = 5\n@@ -908,12 +907,11 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n     model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])\n     self._test_runtime_with_model(model)\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   @testing_utils.run_v2_only\n   def test_LSTM_runtime_with_mask(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Skipping the test as ROCm MIOpen does not '\n-                    'support padded input yet.')\n-\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n     layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)\n\n@@ -803,6 +803,23 @@ def subtract(inputs, **kwargs):\n def multiply(inputs, **kwargs):\n   \"\"\"Functional interface to the `Multiply` layer.\n \n+  Example:\n+\n+  >>> x1 = np.arange(3.0)\n+  >>> x2 = np.arange(3.0)\n+  >>> tf.keras.layers.multiply([x1, x2])\n+  <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>\n+\n+  Usage in a functional model:\n+\n+  >>> input1 = tf.keras.layers.Input(shape=(16,))\n+  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n+  >>> input2 = tf.keras.layers.Input(shape=(32,))\n+  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n+  >>> out = tf.keras.layers.multiply([x1,x2]) #shape=(None, 8)\n+  >>> out = tf.keras.layers.Dense(4)(out)\n+  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n   Args:\n       inputs: A list of input tensors (at least 2).\n       **kwargs: Standard layer keyword arguments.\n\n@@ -1261,15 +1261,12 @@ class LayerNormalization(Layer):\n \n       inputs = tf.reshape(inputs, squeezed_shape)\n \n-      def _set_const_tensor(val, dtype, shape):\n-        return tf.fill(shape, tf.constant(val, dtype=dtype))\n-\n       # self.gamma and self.beta have the wrong shape for fused_batch_norm, so\n       # we cannot pass them as the scale and offset parameters. Therefore, we\n       # create two constant tensors in correct shapes for fused_batch_norm and\n       # later construct a separate calculation on the scale and offset.\n-      scale = _set_const_tensor(1.0, self.dtype, [pre_dim])\n-      offset = _set_const_tensor(0.0, self.dtype, [pre_dim])\n+      scale = tf.ones([pre_dim], dtype=self.dtype)\n+      offset = tf.zeros([pre_dim], dtype=self.dtype)\n \n       # Compute layer normalization using the fused_batch_norm function.\n       outputs, _, _ = tf.compat.v1.nn.fused_batch_norm(\n\n@@ -201,6 +201,73 @@ class MaxPooling1D(Pooling1D):\n class AveragePooling1D(Pooling1D):\n   \"\"\"Average pooling for temporal data.\n \n+  Downsamples the input representation by taking the average value over the\n+  window defined by `pool_size`. The window is shifted by `strides`.  The\n+  resulting output when using \"valid\" padding option has a shape of:\n+  `output_shape = (input_shape - pool_size + 1) / strides)`\n+\n+  The resulting output shape when using the \"same\" padding option is:\n+  `output_shape = input_shape / strides`\n+\n+  For example, for strides=1 and padding=\"valid\":\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> x\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+    array([[[1.],\n+            [2.],\n+            [3.],\n+            [4.],\n+            [5.]], dtype=float32)>\n+  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n+  ...    strides=1, padding='valid')\n+  >>> avg_pool_1d(x)\n+  <tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=\n+  array([[[1.5],\n+          [2.5],\n+          [3.5],\n+          [4.5]]], dtype=float32)>\n+\n+  For example, for strides=2 and padding=\"valid\":\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> x\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+    array([[[1.],\n+            [2.],\n+            [3.],\n+            [4.],\n+            [5.]], dtype=float32)>\n+  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n+  ...    strides=2, padding='valid')\n+  >>> avg_pool_1d(x)\n+  <tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n+  array([[[1.5],\n+          [3.5]]], dtype=float32)>\n+\n+  For example, for strides=1 and padding=\"same\":\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> x\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+    array([[[1.],\n+            [2.],\n+            [3.],\n+            [4.],\n+            [5.]], dtype=float32)>\n+  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n+  ...    strides=1, padding='same')\n+  >>> avg_pool_1d(x)\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+  array([[[1.5],\n+          [2.5],\n+          [3.5],\n+          [4.5],\n+          [5.]]], dtype=float32)>\n+\n   Args:\n     pool_size: Integer, size of the average pooling windows.\n     strides: Integer, or None. Factor by which to downscale.\n@@ -464,6 +531,64 @@ class MaxPooling2D(Pooling2D):\n class AveragePooling2D(Pooling2D):\n   \"\"\"Average pooling operation for spatial data.\n \n+  Downsamples the input representation by taking the average value over the\n+  window defined by `pool_size` for each dimension along the features axis.\n+  The window is shifted by `strides` in each dimension.  The resulting output\n+  when using \"valid\" padding option has a shape(number of rows or columns) of:\n+  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+  (when input_shape >= pool_size)\n+\n+  The resulting output shape when using the \"same\" padding option is:\n+  `output_shape = math.floor((input_shape - 1) / strides) + 1`\n+\n+  For example, for stride=(1,1) and padding=\"valid\":\n+\n+  >>> x = tf.constant([[1., 2., 3.],\n+  ...                  [4., 5., 6.],\n+  ...                  [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [1, 3, 3, 1])\n+  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n+  ...    strides=(1, 1), padding='valid')\n+  >>> avg_pool_2d(x)\n+  <tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n+    array([[[[3.],\n+             [4.]],\n+            [[6.],\n+             [7.]]]], dtype=float32)>\n+\n+  For example, for stride=(2,2) and padding=\"valid\":\n+\n+  >>> x = tf.constant([[1., 2., 3., 4.],\n+  ...                  [5., 6., 7., 8.],\n+  ...                  [9., 10., 11., 12.]])\n+  >>> x = tf.reshape(x, [1, 3, 4, 1])\n+  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n+  ...    strides=(2, 2), padding='valid')\n+  >>> avg_pool_2d(x)\n+  <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=\n+    array([[[[3.5],\n+             [5.5]]]], dtype=float32)>\n+\n+  For example, for stride=(1,1) and padding=\"same\":\n+\n+  >>> x = tf.constant([[1., 2., 3.],\n+  ...                  [4., 5., 6.],\n+  ...                  [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [1, 3, 3, 1])\n+  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n+  ...    strides=(1, 1), padding='same')\n+  >>> avg_pool_2d(x)\n+  <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n+    array([[[[3.],\n+             [4.],\n+             [4.5]],\n+            [[6.],\n+             [7.],\n+             [7.5]],\n+            [[7.5],\n+             [8.5],\n+             [9.]]]], dtype=float32)>\n+\n   Args:\n     pool_size: integer or tuple of 2 integers,\n       factors by which to downscale (vertical, horizontal).\n\n@@ -20,7 +20,6 @@ import numpy as np\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n@@ -52,7 +51,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data.to_tensor(default_value=0.0)}\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import category_crossing\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n@@ -54,7 +53,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n@@ -49,7 +48,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n@@ -51,7 +50,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data}\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import string_lookup\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n@@ -48,7 +47,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import string_lookup\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n@@ -54,7 +53,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import string_lookup\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n@@ -56,7 +55,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data}\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing import string_lookup\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n@@ -50,7 +49,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data}\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n # This is required as of 3/2021 because otherwise we drop into graph mode.\n@@ -51,7 +50,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data.to_tensor(default_value=0)}\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n # This is required as of 3/2021 because otherwise we drop into graph mode.\n@@ -52,7 +51,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data}\n\n@@ -18,7 +18,6 @@ import tensorflow as tf\n \n import keras\n from tensorflow.python.eager.def_function import function as tf_function\n-from tensorflow.python.feature_column import feature_column_v2 as fcv2\n from keras.layers.preprocessing.benchmarks import feature_column_benchmark as fc_bm\n \n # This is required as of 3/2021 because otherwise we drop into graph mode.\n@@ -59,7 +58,7 @@ def embedding_varlen(batch_size, max_length):\n   # Wrap the FC implementation in a tf.function for a fair comparison\n   @tf_function()\n   def fc_fn(tensors):\n-    fc.transform_feature(fcv2.FeatureTransformationCache(tensors), None)\n+    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)\n \n   # Benchmark runs\n   keras_data = {\"data\": data, \"weight\": weight}\n\n@@ -238,44 +238,39 @@ class Discretization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n   def call(self, inputs):\n-    def _bucketize_op(bins):\n-      bins = [tf.cast(bins, tf.float32)]\n-      return lambda inputs: tf.raw_ops.BoostedTreesBucketize(  # pylint: disable=g-long-lambda\n+    bins = [tf.cast(tf.compat.v1.squeeze(self.bins), tf.float32)]\n+\n+    def _bucketize_fn(inputs):\n+      return tf.raw_ops.BoostedTreesBucketize(\n           float_values=[tf.cast(inputs, tf.float32)],\n           bucket_boundaries=bins)[0]\n \n     if tf_utils.is_ragged(inputs):\n       integer_buckets = tf.ragged.map_flat_values(\n-          _bucketize_op(tf.compat.v1.squeeze(self.bins)),\n-          inputs)\n+          _bucketize_fn, inputs)\n       # Ragged map_flat_values doesn't touch the non-values tensors in the\n       # ragged composite tensor. If this op is the only op a Keras model,\n       # this can cause errors in Graph mode, so wrap the tensor in an identity.\n       return tf.identity(integer_buckets)\n     elif isinstance(inputs, tf.SparseTensor):\n-      integer_buckets = tf.raw_ops.BoostedTreesBucketize(\n-          float_values=[tf.cast(inputs.values, tf.float32)],\n-          bucket_boundaries=[tf.cast(tf.compat.v1.squeeze(self.bins),\n-                                           tf.float32)])[0]\n       return tf.SparseTensor(\n           indices=tf.identity(inputs.indices),\n-          values=integer_buckets,\n+          values=_bucketize_fn(inputs.values),\n           dense_shape=tf.identity(inputs.dense_shape))\n     else:\n-      input_shape = inputs.get_shape()\n-      if any(dim is None for dim in input_shape.as_list()[1:]):\n+      static_shape = inputs.get_shape()\n+      if any(dim is None for dim in static_shape.as_list()[1:]):\n         raise NotImplementedError(\n             \"Discretization Layer requires known non-batch shape,\"\n-            \"found {}\".format(input_shape))\n-\n-      reshaped = tf.reshape(\n-          inputs,\n-          [-1, tf.raw_ops.Prod(input=input_shape.as_list()[1:], axis=0)])\n+            \"found {}\".format(static_shape))\n \n+      dynamic_shape = tf.shape(inputs)\n+      # BoostedTreesBucketize only handles rank 1 inputs. We need to flatten our\n+      # inputs after batch size and vectorized_map over each sample.\n+      reshaped = tf.reshape(inputs, [dynamic_shape[0], -1])\n       return tf.reshape(\n-          tf.vectorized_map(\n-              _bucketize_op(tf.compat.v1.squeeze(self.bins)), reshaped),\n-          tf.constant([-1] + input_shape.as_list()[1:]))\n+          tf.vectorized_map(_bucketize_fn, reshaped),\n+          dynamic_shape)\n \n   class DiscretizingCombiner(Combiner):\n     \"\"\"Combiner for the Discretization preprocessing layer.\n\n@@ -31,7 +31,7 @@ from keras.layers.preprocessing import preprocessing_test_utils\n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n-        distribution=strategy_combinations.strategies_minus_tpu,\n+        distribution=strategy_combinations.all_strategies,\n         mode=[\"eager\", \"graph\"]))\n class DiscretizationDistributionTest(\n     keras_parameterized.TestCase,\n\n@@ -124,6 +124,12 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     self.assertAllEqual(indices, output_dataset.indices)\n     self.assertAllEqual(expected_output, output_dataset.values)\n \n+  def test_output_shape(self):\n+    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n+    output = layer(input_data)\n+    self.assertAllEqual(output.shape.as_list(), [16, 4])\n+\n   def test_num_bins_negative_fails(self):\n     with self.assertRaisesRegex(ValueError, \"`num_bins` must be.*num_bins=-7\"):\n       _ = discretization.Discretization(num_bins=-7)\n\n@@ -22,76 +22,80 @@ import tensorflow.compat.v2 as tf\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import index_lookup\n from keras.layers.preprocessing import table_utils\n+from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export(\"keras.layers.experimental.preprocessing.IntegerLookup\", v1=[])\n class IntegerLookup(index_lookup.IndexLookup):\n-  \"\"\"Maps integers from a vocabulary to integer indices.\n+  \"\"\"Reindex integer inputs to be in a contiguous range, via a dict lookup.\n \n-  This layer translates a set of arbitrary integers into an integer output via a\n-  table-based vocabulary lookup.\n+  This layer maps a set of arbitrary integer input tokens into indexed\n+  integer output via a table-based vocabulary lookup. The layer's output indices\n+  will be contiguously arranged up to the maximum vocab size, even if the input\n+  tokens are non-continguous or unbounded. The layer supports multiple options\n+  for encoding the output via `output_mode`, and has optional support for\n+  out-of-vocabulary (OOV) tokens and masking.\n \n   The vocabulary for the layer can be supplied on construction or learned via\n   `adapt()`. During `adapt()`, the layer will analyze a data set, determine the\n-  frequency of individual integer values, and create a vocabulary from them. If\n-  the vocabulary is capped in size, the most frequent values will be used to\n-  create the vocabulary and all others will be treated as out-of-vocabulary\n-  (OOV).\n+  frequency of individual integer tokens, and create a vocabulary from them. If\n+  the vocabulary is capped in size, the most frequent tokens will be used to\n+  create the vocabulary and all others will be treated as OOV.\n \n   There are two possible output modes for the layer.\n-  When `output_mode` is \"int\", input values are converted to their index in the\n-  vocabulary (an integer).\n-  When `output_mode` is \"binary\", \"count\", or \"tf-idf\", input strings\n+  When `output_mode` is \"int\",\n+  input integers are converted to their index in the vocabulary (an integer).\n+  When `output_mode` is \"binary\", \"count\", or \"tf-idf\", input integers\n   are encoded into an array where each dimension corresponds to an element in\n   the vocabulary.\n \n-  The vocabulary can optionally contain a mask value as well as an OOV value\n+  The vocabulary can optionally contain a mask token as well as an OOV token\n   (which can optionally occupy multiple indices in the vocabulary, as set\n   by `num_oov_indices`).\n-  The position of these values in the vocabulary is fixed. When `output_mode` is\n-  \"int\", the vocabulary will begin with the mask value at index 0, followed by\n+  The position of these tokens in the vocabulary is fixed. When `output_mode` is\n+  \"int\", the vocabulary will begin with the mask token at index 0, followed by\n   OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n   \"binary\", \"count\", or \"tf-idf\" the vocabulary will begin with OOV indices and\n-  instances of the mask value will be dropped.\n+  instances of the mask token will be dropped.\n \n   Args:\n-    max_values: The maximum size of the vocabulary for this layer. If None,\n+    max_tokens: The maximum size of the vocabulary for this layer. If None,\n       there is no cap on the size of the vocabulary. Note that this size\n-      includes the OOV and mask values. Default to None.\n-    num_oov_indices: The number of out-of-vocabulary values to use. If this\n+      includes the OOV and mask tokens. Default to None.\n+    num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n       value is more than 1, OOV inputs are modulated to determine their OOV\n       value. If this value is 0, OOV inputs will map to -1 when `output_mode` is\n       \"int\" and are dropped otherwise. Defaults to 1.\n-    mask_value: A value that represents masked inputs. When `output_mode` is\n-      \"int\", the value is included in vocabulary and mapped to index 0. In other\n-      output modes, the value will not appear in the vocabulary and instances\n-      of the mask value in the input will be dropped. If set to None, no mask\n-      term will be added. Defaults to 0.\n-    oov_value: Only used when `invert` is True. The value to return for OOV\n+    mask_token: An integer token that represents masked inputs. When\n+      `output_mode` is \"int\", the token is included in vocabulary and mapped to\n+      index 0. In other output modes, the token will not appear in the\n+      vocabulary and instances of the mask token in the input will be dropped.\n+      If set to None, no mask term will be added. Defaults to 0.\n+    oov_token: Only used when `invert` is True. The token to return for OOV\n       indices. Defaults to -1.\n-    vocabulary: An optional list of values, or a path to a text file containing\n-      a vocabulary to load into this layer. The file should contain one value\n-      per line. If the list or file contains the same value multiple times, an\n-      error will be thrown.\n+    vocabulary: An optional list of integer tokens, or a path to a text file\n+      containing a vocabulary to load into this layer. The file should contain\n+      one integer token per line. If the list or file contains the same token\n+      multiple times, an error will be thrown.\n     invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n       indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n     output_mode: Specification for the output of the layer. Defaults to \"int\".\n       Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n       as follows:\n-        \"int\": Return the raw integer indices of the input values.\n-        \"binary\": Outputs a single int array per sample, of either vocab_size or\n-          max_values size, containing 1s in all elements where the value mapped\n-          to that index exists at least once in the sample.\n+        \"int\": Return the vocabulary indices of the input tokens.\n+        \"binary\": Outputs a single int array per sample, of either vocabulary\n+          size or `max_tokens` size, containing 1s in all elements where the\n+          token mapped to that index exists at least once in the sample.\n         \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the value at that index appeared in the sample.\n+          of times the token at that index appeared in the sample.\n         \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each value slot.\n-    pad_to_max_values: Only applicable when `output_mode` is \"binary\", \"count\",\n+          value in each token slot.\n+    pad_to_max_tokens: Only applicable when `output_mode` is \"binary\", \"count\",\n       or \"tf-idf\". If True, the output will have its feature axis padded to\n-      `max_values` even if the number of unique values in the vocabulary is less\n-      than max_values, resulting in a tensor of shape [batch_size, max_values]\n+      `max_tokens` even if the number of unique tokens in the vocabulary is less\n+      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n       regardless of vocabulary size. Defaults to False.\n     sparse: Boolean. Only applicable when `output_mode` is \"binary\", \"count\",\n       or \"tf-idf\". If True, returns a `SparseTensor` instead of a dense\n@@ -104,7 +108,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   This example creates a lookup layer with a pre-existing vocabulary.\n \n   >>> vocab = [12, 36, 1138, 42]\n-  >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])  # Note OOV values\n+  >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])  # Note OOV tokens\n   >>> layer = IntegerLookup(vocabulary=vocab)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n@@ -122,8 +126,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> layer.get_vocabulary()\n   [0, -1, 42, 1138, 1000, 36, 12]\n \n-  Note how the mask value 0 and the OOV value -1 have been added to the\n-  vocabulary. The remaining values are sorted by frequency (1138, which has\n+  Note how the mask token 0 and the OOV token -1 have been added to the\n+  vocabulary. The remaining tokens are sorted by frequency (1138, which has\n   2 occurrences, is first) then by inverse sort order.\n \n   >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])\n@@ -138,8 +142,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n   **Lookups with multiple OOV indices**\n \n   This example demonstrates how to use a lookup layer with multiple OOV indices.\n-  When a layer is created with more than one OOV index, any OOV values are\n-  hashed into the number of OOV buckets, distributing OOV values in a\n+  When a layer is created with more than one OOV index, any OOV tokens are\n+  hashed into the number of OOV buckets, distributing OOV tokens in a\n   deterministic fashion across the set.\n \n   >>> vocab = [12, 36, 1138, 42]\n@@ -150,31 +154,31 @@ class IntegerLookup(index_lookup.IndexLookup):\n   array([[3, 5, 6],\n          [2, 1, 4]])>\n \n-  Note that the output for OOV value 37 is 2, while the output for OOV value\n+  Note that the output for OOV token 37 is 2, while the output for OOV token\n   1000 is 1. The in-vocab terms have their output index increased by 1 from\n   earlier examples (12 maps to 3, etc) in order to make space for the extra OOV\n-  value.\n+  token.\n \n   **Multi-hot output**\n \n   Configure the layer with `output_mode='binary'`. Note that the first\n-  `num_oov_indices` dimensions in the binary encoding represent OOV values\n+  `num_oov_indices` dimensions in the binary encoding represent OOV tokens\n \n   >>> vocab = [12, 36, 1138, 42]\n-  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n+  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n   >>> layer = IntegerLookup(vocabulary=vocab, output_mode='binary')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 1.],\n            [1., 0., 1., 0., 1.]], dtype=float32)>\n \n-  **Value count output**\n+  **Token count output**\n \n   Configure the layer with `output_mode='count'`. As with binary output, the\n-  first `num_oov_indices` dimensions in the output represent OOV values.\n+  first `num_oov_indices` dimensions in the output represent OOV tokens.\n \n   >>> vocab = [12, 36, 1138, 42]\n-  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n+  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n   >>> layer = IntegerLookup(vocabulary=vocab, output_mode='count')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -184,16 +188,16 @@ class IntegerLookup(index_lookup.IndexLookup):\n   **TF-IDF output**\n \n   Configure the layer with `output_mode='tf-idf'`. As with binary output, the\n-  first `num_oov_indices` dimensions in the output represent OOV values.\n+  first `num_oov_indices` dimensions in the output represent OOV tokens.\n \n-  Each value bin will output `value_count * idf_weight`, where the idf weights\n-  are the inverse document frequency weights per value. These should be provided\n-  along with the vocabulary. Note that the `idf_weight` for OOV values will\n+  Each token bin will output `token_count * idf_weight`, where the idf weights\n+  are the inverse document frequency weights per token. These should be provided\n+  along with the vocabulary. Note that the `idf_weight` for OOV tokens will\n   default to the average of all idf weights passed in.\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n-  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n+  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n   >>> layer = IntegerLookup(output_mode='tf-idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n@@ -201,12 +205,12 @@ class IntegerLookup(index_lookup.IndexLookup):\n     array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n            [1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-  To specify the idf weights for oov values, you will need to pass the entire\n-  vocabularly including the leading oov value.\n+  To specify the idf weights for oov tokens, you will need to pass the entire\n+  vocabularly including the leading oov token.\n \n   >>> vocab = [-1, 12, 36, 1138, 42]\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n-  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV values\n+  >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n   >>> layer = IntegerLookup(output_mode='tf-idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n@@ -215,12 +219,12 @@ class IntegerLookup(index_lookup.IndexLookup):\n            [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n   When adapting the layer in tf-idf mode, each input sample will be considered a\n-  document, and idf weight per value will be calculated as\n-  `log(1 + num_documents / (1 + value_document_count))`.\n+  document, and idf weight per token will be calculated as\n+  `log(1 + num_documents / (1 + token_document_count))`.\n \n   **Inverse lookup**\n \n-  This example demonstrates how to map indices to values using this layer. (You\n+  This example demonstrates how to map indices to tokens using this layer. (You\n   can also use adapt() with inverse=True, but for simplicity we'll pass the\n   vocab in this example.)\n \n@@ -232,8 +236,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n   array([[  12, 1138,   42],\n          [  42,   -1,   36]])>\n \n-  Note that the first two indices correspond to the mask and oov value by\n-  default. This behavior can be disabled by setting `mask_value=None` and\n+  Note that the first two indices correspond to the mask and oov token by\n+  default. This behavior can be disabled by setting `mask_token=None` and\n   `num_oov_indices=0`.\n \n \n@@ -252,26 +256,40 @@ class IntegerLookup(index_lookup.IndexLookup):\n   array([[  12, 1138,   42],\n          [  42,   -1,   36]])>\n \n-  In this example, the input value 1000 resulted in an output of -1, since\n+  In this example, the input token 1000 resulted in an output of -1, since\n   1000 was not in the vocabulary - it got represented as an OOV, and all OOV\n-  values are returned as -1 in the inverse layer. Also, note that for the\n+  tokens are returned as -1 in the inverse layer. Also, note that for the\n   inverse to work, you must have already set the forward layer vocabulary\n   either directly or via `fit()` before calling `get_vocabulary()`.\n   \"\"\"\n \n   def __init__(self,\n-               max_values=None,\n+               max_tokens=None,\n                num_oov_indices=1,\n-               mask_value=0,\n-               oov_value=-1,\n+               mask_token=0,\n+               oov_token=-1,\n                vocabulary=None,\n                invert=False,\n                output_mode=index_lookup.INT,\n                sparse=False,\n-               pad_to_max_values=False,\n+               pad_to_max_tokens=False,\n                **kwargs):\n     allowed_dtypes = [tf.int64]\n \n+    # Support deprecated args for this layer.\n+    if \"max_values\" in kwargs:\n+      logging.warning(\"max_values is deprecated, use max_tokens instead.\")\n+      max_tokens = kwargs[\"max_values\"]\n+      del kwargs[\"max_values\"]\n+    if \"mask_value\" in kwargs:\n+      logging.warning(\"mask_value is deprecated, use mask_token instead.\")\n+      mask_token = kwargs[\"mask_value\"]\n+      del kwargs[\"mask_value\"]\n+    if \"oov_value\" in kwargs:\n+      logging.warning(\"oov_value is deprecated, use oov_token instead.\")\n+      oov_token = kwargs[\"oov_value\"]\n+      del kwargs[\"oov_value\"]\n+\n     if \"dtype\" in kwargs and kwargs[\"dtype\"] not in allowed_dtypes:\n       raise ValueError(\"The value of the dtype argument for IntegerLookup may \"\n                        \"only be one of %s.\" % (allowed_dtypes,))\n@@ -279,11 +297,11 @@ class IntegerLookup(index_lookup.IndexLookup):\n     if \"dtype\" not in kwargs:\n       kwargs[\"dtype\"] = tf.int64\n \n-    # If max_values is set, the value must be greater than 1 - otherwise we\n+    # If max_tokens is set, the token must be greater than 1 - otherwise we\n     # are creating a 0-element vocab, which doesn't make sense.\n-    if max_values is not None and max_values <= 1:\n-      raise ValueError(\"If set, max_values must be greater than 1. \"\n-                       \"You passed %s\" % (max_values,))\n+    if max_tokens is not None and max_tokens <= 1:\n+      raise ValueError(\"If set, max_tokens must be greater than 1. \"\n+                       \"You passed %s\" % (max_tokens,))\n \n     if num_oov_indices < 0:\n       raise ValueError(\n@@ -296,36 +314,18 @@ class IntegerLookup(index_lookup.IndexLookup):\n         vocabulary = [int(v) for v in vocabulary]\n \n     super(IntegerLookup, self).__init__(\n-        max_tokens=max_values,\n+        max_tokens=max_tokens,\n         num_oov_indices=num_oov_indices,\n-        mask_token=mask_value,\n-        oov_token=oov_value,\n+        mask_token=mask_token,\n+        oov_token=oov_token,\n         vocabulary=vocabulary,\n         invert=invert,\n         output_mode=output_mode,\n         sparse=sparse,\n-        pad_to_max_tokens=pad_to_max_values,\n+        pad_to_max_tokens=pad_to_max_tokens,\n         **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"IntegerLookup\").set(True)\n \n-  def get_config(self):\n-    base_config = super(IntegerLookup, self).get_config()\n-    # Because the super config has a bunch of args we're also passing,\n-    # we need to rename and remove them from the config dict.\n-    base_config[\"max_values\"] = base_config[\"max_tokens\"]\n-    del base_config[\"max_tokens\"]\n-\n-    base_config[\"mask_value\"] = base_config[\"mask_token\"]\n-    del base_config[\"mask_token\"]\n-\n-    base_config[\"oov_value\"] = base_config[\"oov_token\"]\n-    del base_config[\"oov_token\"]\n-\n-    base_config[\"pad_to_max_values\"] = base_config[\"pad_to_max_tokens\"]\n-    del base_config[\"pad_to_max_tokens\"]\n-\n-    return base_config\n-\n   def set_vocabulary(self, vocabulary, idf_weights=None):\n     if isinstance(vocabulary, str):\n       if self.output_mode == index_lookup.TFIDF:\n\n@@ -52,7 +52,7 @@ def _get_end_to_end_test_cases():\n               np.array([[1138], [1729], [725], [42], [42], [725], [1138], [4]],\n                        dtype=np.int64),\n           \"kwargs\": {\n-              \"max_values\": None,\n+              \"max_tokens\": None,\n               \"dtype\": tf.int64,\n           },\n           \"expected_output\": [[2], [3], [4], [5], [5], [4], [2], [1]],\n@@ -133,7 +133,7 @@ class CategoricalEncodingInputTest(\n     expected_dense_shape = [3, 4]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n-    layer = integer_lookup.IntegerLookup(max_values=None)\n+    layer = integer_lookup.IntegerLookup(max_tokens=None)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -149,7 +149,7 @@ class CategoricalEncodingInputTest(\n     expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n-    layer = integer_lookup.IntegerLookup(max_values=None)\n+    layer = integer_lookup.IntegerLookup(max_tokens=None)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -175,11 +175,11 @@ class CategoricalEncodingMultiOOVTest(\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n     layer = integer_lookup.IntegerLookup(\n-        max_values=None,\n+        max_tokens=None,\n         dtype=tf.int64,\n         num_oov_indices=2,\n-        mask_value=0,\n-        oov_value=-1)\n+        mask_token=0,\n+        oov_token=-1)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -195,7 +195,7 @@ class CategoricalEncodingMultiOOVTest(\n     expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n-    layer = integer_lookup.IntegerLookup(max_values=None, num_oov_indices=2)\n+    layer = integer_lookup.IntegerLookup(max_tokens=None, num_oov_indices=2)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -240,7 +240,7 @@ class CategoricalEncodingAdaptTest(\n     batched_ds = ds.take(2)\n     input_t = keras.Input(shape=(), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n-        max_values=10, num_oov_indices=0, mask_value=None, oov_value=None)\n+        max_tokens=10, num_oov_indices=0, mask_token=None, oov_token=None)\n     _ = layer(input_t)\n     layer.adapt(batched_ds)\n \n@@ -264,7 +264,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n \n   def test_output_shape(self):\n     input_data = keras.Input(shape=(4,), dtype=tf.int64)\n-    layer = integer_lookup.IntegerLookup(max_values=2, num_oov_indices=1)\n+    layer = integer_lookup.IntegerLookup(max_tokens=2, num_oov_indices=1)\n     int_data = layer(input_data)\n     self.assertAllEqual(int_data.shape[1:], input_data.shape[1:])\n \n@@ -274,7 +274,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = integer_lookup.IntegerLookup(max_values=None, mask_value=None)\n+    layer = integer_lookup.IntegerLookup(max_tokens=None, mask_token=None)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -289,7 +289,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n         vocabulary=vocab_data,\n-        max_values=None,\n+        max_tokens=None,\n     )\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -304,7 +304,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n         vocabulary=vocab_data,\n-        max_values=None,\n+        max_tokens=None,\n     )\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -479,14 +479,14 @@ class IntegerLookupErrorTest(keras_parameterized.TestCase,\n   def test_too_long_vocab_fails_in_single_setting(self):\n     vocab_data = [42, 1138, 725, 1729]\n \n-    layer = integer_lookup.IntegerLookup(max_values=4, num_oov_indices=1)\n+    layer = integer_lookup.IntegerLookup(max_tokens=4, num_oov_indices=1)\n     with self.assertRaisesRegex(ValueError,\n                                 \"vocabulary larger than the maximum vocab.*\"):\n       layer.set_vocabulary(vocab_data)\n \n-  def test_zero_max_values_fails(self):\n-    with self.assertRaisesRegex(ValueError, \".*max_values.*\"):\n-      _ = integer_lookup.IntegerLookup(max_values=0, num_oov_indices=1)\n+  def test_zero_max_tokens_fails(self):\n+    with self.assertRaisesRegex(ValueError, \".*max_tokens.*\"):\n+      _ = integer_lookup.IntegerLookup(max_tokens=0, num_oov_indices=1)\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -505,7 +505,7 @@ class IntegerLookupSavingTest(keras_parameterized.TestCase,\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = integer_lookup.IntegerLookup(max_values=None, num_oov_indices=1)\n+    layer = integer_lookup.IntegerLookup(max_tokens=None, num_oov_indices=1)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n\n@@ -1022,13 +1022,10 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n           input_layer.compute_output_shape([None, 2, 4]).as_list(),\n           [None, 2, 16])\n \n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_Bidirectional_last_output_with_masking(self):\n-    if tf.test.is_built_with_rocm():\n-      # testcase uses input and/or output sequences which require padding\n-      # leading to the following error on ROCm platform\n-      # ROCm MIOpen only supports packed input output\n-      # Skip this subtest for now\n-      self.skipTest('Test not supported on the ROCm platform')\n     rnn = keras.layers.LSTM\n     samples = 2\n     dim = 5\n@@ -1055,13 +1052,10 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(y[0], np.concatenate([y[1], y[3]], axis=1))\n \n   @parameterized.parameters([keras.layers.LSTM, keras.layers.GRU])\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n   def test_Bidirectional_sequence_output_with_masking(self, rnn):\n-    if tf.test.is_built_with_rocm():\n-      # testcase uses input and/or output sequences which require padding\n-      # leading to the following error on ROCm platform\n-      # ROCm MIOpen only supports packed input output\n-      # Skip this subtest for now\n-      self.skipTest('Test not supported on the ROCm platform')\n     samples = 2\n     dim = 5\n     timesteps = 3\n@@ -1263,10 +1257,10 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         batch_size=10)\n \n   @parameterized.parameters(['ave', 'concat', 'mul'])\n+  @tf.test.disable_with_predicate(\n+      pred=tf.test.is_built_with_rocm,\n+      skip_message='Skipping as ROCm RNN does not support ragged tensors yet.')\n   def test_Bidirectional_ragged_input(self, merge_mode):\n-    if tf.test.is_built_with_rocm():\n-      # ragged tenors are not supported in ROCM RNN implementation\n-      self.skipTest('Test not supported on the ROCm platform')\n     np.random.seed(100)\n     rnn = keras.layers.LSTM\n     units = 3\n\n@@ -14,6 +14,7 @@\n # ==============================================================================\n # pylint: disable=unused-import\n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-doc-return-or-yield\n \"\"\"Built-in metrics.\n \"\"\"\n from __future__ import absolute_import\n@@ -24,10 +25,10 @@ import tensorflow.compat.v2 as tf\n \n import abc\n import types\n+import warnings\n \n import numpy as np\n import six\n-from tensorflow.python.eager import def_function\n from keras import activations\n from keras import backend as K\n from keras.engine import base_layer\n@@ -47,6 +48,7 @@ from keras.losses import poisson\n from keras.losses import sparse_categorical_crossentropy\n from keras.losses import squared_hinge\n from keras.saving.saved_model import metric_serialization\n+from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import metrics_utils\n from keras.utils import tf_inspect\n@@ -157,7 +159,7 @@ class Metric(base_layer.Layer):\n         ag_update_state = tf.__internal__.autograph.tf_convert(obj_update_state, control_status)\n         return ag_update_state(*args, **kwargs)\n     else:\n-      if isinstance(obj.update_state, def_function.Function):\n+      if isinstance(obj.update_state, tf.__internal__.function.Function):\n         update_state_fn = obj.update_state\n       else:\n         update_state_fn = tf.function(obj.update_state)\n@@ -225,12 +227,19 @@ class Metric(base_layer.Layer):\n     \"\"\"Returns the serializable config of the metric.\"\"\"\n     return {'name': self.name, 'dtype': self.dtype}\n \n-  def reset_states(self):\n+  def reset_state(self):\n     \"\"\"Resets all of the metric state variables.\n \n     This function is called between epochs/steps,\n     when a metric is evaluated during training.\n     \"\"\"\n+    if not generic_utils.is_default(self.reset_states):\n+      warnings.warn('Metric %s implements a `reset_states()` method; rename it '\n+                    'to `reset_state()` (without the final \"s\"). The name '\n+                    '`reset_states()` has been deprecated to improve API '\n+                    'consistency.' % (self.__class__.__name__,))\n+      return self.reset_states()\n+    else:\n       K.batch_set_value([(v, 0) for v in self.variables])\n \n   @abc.abstractmethod\n@@ -323,6 +332,13 @@ class Metric(base_layer.Layer):\n   def _trackable_saved_model_saver(self):\n     return metric_serialization.MetricSavedModelSaver(self)\n \n+  @generic_utils.default\n+  @doc_controls.do_not_generate_docs\n+  def reset_states(self):\n+    # Backwards compatibility alias of `reset_state`. New classes should\n+    # only implement `reset_state`.\n+    return self.reset_state()\n+\n \n class Reduce(Metric):\n   \"\"\"Encapsulates metrics that perform a reduce operation on the values.\n@@ -475,7 +491,7 @@ class Mean(Reduce):\n   >>> m.update_state([1, 3, 5, 7])\n   >>> m.result().numpy()\n   4.0\n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])\n   >>> m.result().numpy()\n   2.0\n@@ -669,7 +685,7 @@ class Accuracy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.75\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]],\n   ...                sample_weight=[1, 1, 0, 0])\n   >>> m.result().numpy()\n@@ -713,7 +729,7 @@ class BinaryAccuracy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.75\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],\n   ...                sample_weight=[1, 0, 0, 1])\n   >>> m.result().numpy()\n@@ -763,7 +779,7 @@ class CategoricalAccuracy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],\n   ...                 [0.05, 0.95, 0]],\n   ...                sample_weight=[0.7, 0.3])\n@@ -815,7 +831,7 @@ class SparseCategoricalAccuracy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]],\n   ...                sample_weight=[0.7, 0.3])\n   >>> m.result().numpy()\n@@ -854,7 +870,7 @@ class TopKCategoricalAccuracy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 0, 1], [0, 1, 0]],\n   ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],\n   ...                sample_weight=[0.7, 0.3])\n@@ -892,7 +908,7 @@ class SparseTopKCategoricalAccuracy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],\n   ...                sample_weight=[0.7, 0.3])\n   >>> m.result().numpy()\n@@ -969,7 +985,7 @@ class _ConfusionMatrixConditionCount(Metric):\n       result = self.accumulator\n     return tf.convert_to_tensor(result)\n \n-  def reset_states(self):\n+  def reset_state(self):\n     num_thresholds = len(to_list(self.thresholds))\n     K.batch_set_value(\n         [(v, np.zeros((num_thresholds,))) for v in self.variables])\n@@ -1007,7 +1023,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n   >>> m.result().numpy()\n   2.0\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n   >>> m.result().numpy()\n   1.0\n@@ -1056,7 +1072,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n   >>> m.result().numpy()\n   2.0\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n   >>> m.result().numpy()\n   1.0\n@@ -1105,7 +1121,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n   >>> m.result().numpy()\n   2.0\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n   >>> m.result().numpy()\n   1.0\n@@ -1154,7 +1170,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n   >>> m.result().numpy()\n   2.0\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n   >>> m.result().numpy()\n   1.0\n@@ -1219,7 +1235,7 @@ class Precision(Metric):\n   >>> m.result().numpy()\n   0.6666667\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n   >>> m.result().numpy()\n   1.0\n@@ -1299,7 +1315,7 @@ class Precision(Metric):\n                                  self.true_positives + self.false_positives)\n     return result[0] if len(self.thresholds) == 1 else result\n \n-  def reset_states(self):\n+  def reset_state(self):\n     num_thresholds = len(to_list(self.thresholds))\n     K.batch_set_value(\n         [(v, np.zeros((num_thresholds,))) for v in self.variables])\n@@ -1356,7 +1372,7 @@ class Recall(Metric):\n   >>> m.result().numpy()\n   0.6666667\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n   >>> m.result().numpy()\n   1.0\n@@ -1424,7 +1440,7 @@ class Recall(Metric):\n                                  self.true_positives + self.false_negatives)\n     return result[0] if len(self.thresholds) == 1 else result\n \n-  def reset_states(self):\n+  def reset_state(self):\n     num_thresholds = len(to_list(self.thresholds))\n     K.batch_set_value(\n         [(v, np.zeros((num_thresholds,))) for v in self.variables])\n@@ -1447,11 +1463,17 @@ class SensitivitySpecificityBase(Metric):\n   [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n   \"\"\"\n \n-  def __init__(self, value, num_thresholds=200, name=None, dtype=None):\n+  def __init__(self,\n+               value,\n+               num_thresholds=200,\n+               class_id=None,\n+               name=None,\n+               dtype=None):\n     super(SensitivitySpecificityBase, self).__init__(name=name, dtype=dtype)\n     if num_thresholds <= 0:\n       raise ValueError('`num_thresholds` must be > 0.')\n     self.value = value\n+    self.class_id = class_id\n     self.true_positives = self.add_weight(\n         'true_positives',\n         shape=(num_thresholds,),\n@@ -1500,13 +1522,19 @@ class SensitivitySpecificityBase(Metric):\n         y_true,\n         y_pred,\n         thresholds=self.thresholds,\n+        class_id=self.class_id,\n         sample_weight=sample_weight)\n \n-  def reset_states(self):\n+  def reset_state(self):\n     num_thresholds = len(self.thresholds)\n     K.batch_set_value(\n         [(v, np.zeros((num_thresholds,))) for v in self.variables])\n \n+  def get_config(self):\n+    config = {'class_id': self.class_id}\n+    base_config = super(SensitivitySpecificityBase, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n   def _find_max_under_constraint(self, constrained, dependent, predicate):\n     \"\"\"Returns the maximum of dependent_statistic that satisfies the constraint.\n \n@@ -1550,6 +1578,11 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n   If `sample_weight` is `None`, weights default to 1.\n   Use `sample_weight` of 0 to mask values.\n \n+  If `class_id` is specified, we calculate precision by considering only the\n+  entries in the batch for which `class_id` is above the threshold predictions,\n+  and computing the fraction of them for which `class_id` is indeed a correct\n+  label.\n+\n   For additional information about specificity and sensitivity, see\n   [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n \n@@ -1557,6 +1590,9 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n     specificity: A scalar value in range `[0, 1]`.\n     num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n       use for matching the given specificity.\n+    class_id: (Optional) Integer class ID for which we want binary metrics.\n+      This must be in the half-open interval `[0, num_classes)`, where\n+      `num_classes` is the last dimension of predictions.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -1567,7 +1603,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n   ...                sample_weight=[1, 1, 2, 2, 1])\n   >>> m.result().numpy()\n@@ -1583,13 +1619,22 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  def __init__(self, specificity, num_thresholds=200, name=None, dtype=None):\n+  def __init__(self,\n+               specificity,\n+               num_thresholds=200,\n+               class_id=None,\n+               name=None,\n+               dtype=None):\n     if specificity < 0 or specificity > 1:\n       raise ValueError('`specificity` must be in the range [0, 1].')\n     self.specificity = specificity\n     self.num_thresholds = num_thresholds\n     super(SensitivityAtSpecificity, self).__init__(\n-        specificity, num_thresholds=num_thresholds, name=name, dtype=dtype)\n+        specificity,\n+        num_thresholds=num_thresholds,\n+        class_id=class_id,\n+        name=name,\n+        dtype=dtype)\n \n   def result(self):\n     specificities = tf.math.divide_no_nan(\n@@ -1625,6 +1670,11 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n   If `sample_weight` is `None`, weights default to 1.\n   Use `sample_weight` of 0 to mask values.\n \n+  If `class_id` is specified, we calculate precision by considering only the\n+  entries in the batch for which `class_id` is above the threshold predictions,\n+  and computing the fraction of them for which `class_id` is indeed a correct\n+  label.\n+\n   For additional information about specificity and sensitivity, see\n   [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n \n@@ -1632,6 +1682,9 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n     sensitivity: A scalar value in range `[0, 1]`.\n     num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n       use for matching the given sensitivity.\n+    class_id: (Optional) Integer class ID for which we want binary metrics.\n+      This must be in the half-open interval `[0, num_classes)`, where\n+      `num_classes` is the last dimension of predictions.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -1642,7 +1695,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n   >>> m.result().numpy()\n   0.66666667\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n   ...                sample_weight=[1, 1, 2, 2, 2])\n   >>> m.result().numpy()\n@@ -1658,13 +1711,22 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  def __init__(self, sensitivity, num_thresholds=200, name=None, dtype=None):\n+  def __init__(self,\n+               sensitivity,\n+               num_thresholds=200,\n+               class_id=None,\n+               name=None,\n+               dtype=None):\n     if sensitivity < 0 or sensitivity > 1:\n       raise ValueError('`sensitivity` must be in the range [0, 1].')\n     self.sensitivity = sensitivity\n     self.num_thresholds = num_thresholds\n     super(SpecificityAtSensitivity, self).__init__(\n-        sensitivity, num_thresholds=num_thresholds, name=name, dtype=dtype)\n+        sensitivity,\n+        num_thresholds=num_thresholds,\n+        class_id=class_id,\n+        name=name,\n+        dtype=dtype)\n \n   def result(self):\n     sensitivities = tf.math.divide_no_nan(\n@@ -1695,10 +1757,18 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n   If `sample_weight` is `None`, weights default to 1.\n   Use `sample_weight` of 0 to mask values.\n \n+  If `class_id` is specified, we calculate precision by considering only the\n+  entries in the batch for which `class_id` is above the threshold predictions,\n+  and computing the fraction of them for which `class_id` is indeed a correct\n+  label.\n+\n   Args:\n     recall: A scalar value in range `[0, 1]`.\n     num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n       use for matching the given recall.\n+    class_id: (Optional) Integer class ID for which we want binary metrics.\n+      This must be in the half-open interval `[0, num_classes)`, where\n+      `num_classes` is the last dimension of predictions.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -1709,7 +1779,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n   ...                sample_weight=[2, 2, 2, 1, 1])\n   >>> m.result().numpy()\n@@ -1725,7 +1795,12 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  def __init__(self, recall, num_thresholds=200, name=None, dtype=None):\n+  def __init__(self,\n+               recall,\n+               num_thresholds=200,\n+               class_id=None,\n+               name=None,\n+               dtype=None):\n     if recall < 0 or recall > 1:\n       raise ValueError('`recall` must be in the range [0, 1].')\n     self.recall = recall\n@@ -1733,6 +1808,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n     super(PrecisionAtRecall, self).__init__(\n         value=recall,\n         num_thresholds=num_thresholds,\n+        class_id=class_id,\n         name=name,\n         dtype=dtype)\n \n@@ -1765,10 +1841,18 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   If `sample_weight` is `None`, weights default to 1.\n   Use `sample_weight` of 0 to mask values.\n \n+  If `class_id` is specified, we calculate precision by considering only the\n+  entries in the batch for which `class_id` is above the threshold predictions,\n+  and computing the fraction of them for which `class_id` is indeed a correct\n+  label.\n+\n   Args:\n     precision: A scalar value in range `[0, 1]`.\n     num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n       use for matching the given precision.\n+    class_id: (Optional) Integer class ID for which we want binary metrics.\n+      This must be in the half-open interval `[0, num_classes)`, where\n+      `num_classes` is the last dimension of predictions.\n     name: (Optional) string name of the metric instance.\n     dtype: (Optional) data type of the metric result.\n \n@@ -1779,7 +1863,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n   ...                sample_weight=[1, 0, 0, 1])\n   >>> m.result().numpy()\n@@ -1795,7 +1879,12 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  def __init__(self, precision, num_thresholds=200, name=None, dtype=None):\n+  def __init__(self,\n+               precision,\n+               num_thresholds=200,\n+               class_id=None,\n+               name=None,\n+               dtype=None):\n     if precision < 0 or precision > 1:\n       raise ValueError('`precision` must be in the range [0, 1].')\n     self.precision = precision\n@@ -1803,6 +1892,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n     super(RecallAtPrecision, self).__init__(\n         value=precision,\n         num_thresholds=num_thresholds,\n+        class_id=class_id,\n         name=name,\n         dtype=dtype)\n \n@@ -1917,7 +2007,7 @@ class AUC(Metric):\n   >>> m.result().numpy()\n   0.75\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n   ...                sample_weight=[1, 0, 0, 1])\n   >>> m.result().numpy()\n@@ -2265,7 +2355,7 @@ class AUC(Metric):\n           tf.multiply(x[:self.num_thresholds - 1] - x[1:], heights),\n           name=self.name)\n \n-  def reset_states(self):\n+  def reset_state(self):\n     if self.multi_label:\n       K.batch_set_value([(v, np.zeros((self.num_thresholds, self._num_labels)))\n                          for v in self.variables])\n@@ -2323,7 +2413,7 @@ class CosineSimilarity(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.49999997\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],\n   ...                sample_weight=[0.3, 0.7])\n   >>> m.result().numpy()\n@@ -2359,7 +2449,7 @@ class MeanAbsoluteError(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.25\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2395,7 +2485,7 @@ class MeanAbsolutePercentageError(MeanMetricWrapper):\n   >>> m.result().numpy()\n   250000000.0\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2431,7 +2521,7 @@ class MeanSquaredError(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.25\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2467,7 +2557,7 @@ class MeanSquaredLogarithmicError(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.12011322\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2506,7 +2596,7 @@ class Hinge(MeanMetricWrapper):\n   >>> m.result().numpy()\n   1.3\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2541,7 +2631,7 @@ class SquaredHinge(MeanMetricWrapper):\n   >>> m.result().numpy()\n   1.86\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2576,7 +2666,7 @@ class CategoricalHinge(MeanMetricWrapper):\n   >>> m.result().numpy()\n   1.4000001\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2607,7 +2697,7 @@ class RootMeanSquaredError(Mean):\n   >>> m.result().numpy()\n   0.5\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2668,7 +2758,7 @@ class LogCoshError(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.10844523\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2704,7 +2794,7 @@ class Poisson(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.49999997\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2740,7 +2830,7 @@ class KLDivergence(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.45814306\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -2793,7 +2883,7 @@ class MeanIoU(Metric):\n   >>> m.result().numpy()\n   0.33333334\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n   ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n   >>> m.result().numpy()\n@@ -2881,7 +2971,7 @@ class MeanIoU(Metric):\n     return tf.math.divide_no_nan(\n         tf.reduce_sum(iou, name='mean_iou'), num_valid_entries)\n \n-  def reset_states(self):\n+  def reset_state(self):\n     K.set_value(self.total_cm, np.zeros((self.num_classes, self.num_classes)))\n \n   def get_config(self):\n@@ -3008,7 +3098,7 @@ class MeanTensor(Metric):\n           )\n     return tf.math.divide_no_nan(self.total, self.count)\n \n-  def reset_states(self):\n+  def reset_state(self):\n     if self._built:\n       K.batch_set_value(\n           [(v, np.zeros(self._shape.as_list())) for v in self.variables])\n@@ -3038,7 +3128,7 @@ class BinaryCrossentropy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   0.81492424\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n   ...                sample_weight=[1, 0])\n   >>> m.result().numpy()\n@@ -3101,7 +3191,7 @@ class CategoricalCrossentropy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   1.1769392\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([[0, 1, 0], [0, 0, 1]],\n   ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],\n   ...                sample_weight=tf.constant([0.3, 0.7]))\n@@ -3172,7 +3262,7 @@ class SparseCategoricalCrossentropy(MeanMetricWrapper):\n   >>> m.result().numpy()\n   1.1769392\n \n-  >>> m.reset_states()\n+  >>> m.reset_state()\n   >>> m.update_state([1, 2],\n   ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],\n   ...                sample_weight=tf.constant([0.3, 0.7]))\n\n@@ -24,12 +24,12 @@ import json\n \n from absl.testing import parameterized\n import numpy as np\n-from scipy.special import expit\n from keras import combinations\n from keras import layers\n from keras import metrics\n from keras import models\n from keras.utils import metrics_utils\n+from tensorflow.python.platform import tf_logging\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -729,11 +729,15 @@ class SensitivityAtSpecificityTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n     s_obj = metrics.SensitivityAtSpecificity(\n-        0.4, num_thresholds=100, name='sensitivity_at_specificity_1')\n+        0.4,\n+        num_thresholds=100,\n+        class_id=12,\n+        name='sensitivity_at_specificity_1')\n     self.assertEqual(s_obj.name, 'sensitivity_at_specificity_1')\n     self.assertLen(s_obj.variables, 4)\n     self.assertEqual(s_obj.specificity, 0.4)\n     self.assertEqual(s_obj.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n     # Check save and restore config\n     s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())\n@@ -741,6 +745,7 @@ class SensitivityAtSpecificityTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertLen(s_obj2.variables, 4)\n     self.assertEqual(s_obj2.specificity, 0.4)\n     self.assertEqual(s_obj2.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n   def test_value_is_idempotent(self):\n     s_obj = metrics.SensitivityAtSpecificity(0.7)\n@@ -797,6 +802,17 @@ class SensitivityAtSpecificityTest(tf.test.TestCase, parameterized.TestCase):\n     result = s_obj(y_true, y_pred)\n     self.assertAlmostEqual(0.6, self.evaluate(result))\n \n+  def test_unweighted_class_id(self):\n+    s_obj = metrics.SpecificityAtSensitivity(0.4, class_id=2)\n+    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+    label_values = [0, 0, 0, 0, 0, 2, 2, 2, 2, 2]\n+\n+    y_pred = tf.compat.v1.transpose([pred_values] * 3)\n+    y_true = tf.one_hot(label_values, depth=3)\n+    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n+    result = s_obj(y_true, y_pred)\n+    self.assertAlmostEqual(0.6, self.evaluate(result))\n+\n   @parameterized.parameters([tf.bool, tf.int32, tf.float32])\n   def test_weighted(self, label_dtype):\n     s_obj = metrics.SensitivityAtSpecificity(0.4)\n@@ -826,11 +842,15 @@ class SpecificityAtSensitivityTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n     s_obj = metrics.SpecificityAtSensitivity(\n-        0.4, num_thresholds=100, name='specificity_at_sensitivity_1')\n+        0.4,\n+        num_thresholds=100,\n+        class_id=12,\n+        name='specificity_at_sensitivity_1')\n     self.assertEqual(s_obj.name, 'specificity_at_sensitivity_1')\n     self.assertLen(s_obj.variables, 4)\n     self.assertEqual(s_obj.sensitivity, 0.4)\n     self.assertEqual(s_obj.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n     # Check save and restore config\n     s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())\n@@ -838,6 +858,7 @@ class SpecificityAtSensitivityTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertLen(s_obj2.variables, 4)\n     self.assertEqual(s_obj2.sensitivity, 0.4)\n     self.assertEqual(s_obj2.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n   def test_value_is_idempotent(self):\n     s_obj = metrics.SpecificityAtSensitivity(0.7)\n@@ -893,6 +914,17 @@ class SpecificityAtSensitivityTest(tf.test.TestCase, parameterized.TestCase):\n     result = s_obj(y_true, y_pred)\n     self.assertAlmostEqual(0.6, self.evaluate(result))\n \n+  def test_unweighted_class_id(self):\n+    s_obj = metrics.SpecificityAtSensitivity(0.4, class_id=2)\n+    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+    label_values = [0, 0, 0, 0, 0, 2, 2, 2, 2, 2]\n+\n+    y_pred = tf.compat.v1.transpose([pred_values] * 3)\n+    y_true = tf.one_hot(label_values, depth=3)\n+    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n+    result = s_obj(y_true, y_pred)\n+    self.assertAlmostEqual(0.6, self.evaluate(result))\n+\n   @parameterized.parameters([tf.bool, tf.int32, tf.float32])\n   def test_weighted(self, label_dtype):\n     s_obj = metrics.SpecificityAtSensitivity(0.4)\n@@ -922,11 +954,12 @@ class PrecisionAtRecallTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n     s_obj = metrics.PrecisionAtRecall(\n-        0.4, num_thresholds=100, name='precision_at_recall_1')\n+        0.4, num_thresholds=100, class_id=12, name='precision_at_recall_1')\n     self.assertEqual(s_obj.name, 'precision_at_recall_1')\n     self.assertLen(s_obj.variables, 4)\n     self.assertEqual(s_obj.recall, 0.4)\n     self.assertEqual(s_obj.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n     # Check save and restore config\n     s_obj2 = metrics.PrecisionAtRecall.from_config(s_obj.get_config())\n@@ -934,6 +967,7 @@ class PrecisionAtRecallTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertLen(s_obj2.variables, 4)\n     self.assertEqual(s_obj2.recall, 0.4)\n     self.assertEqual(s_obj2.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n   def test_value_is_idempotent(self):\n     s_obj = metrics.PrecisionAtRecall(0.7)\n@@ -991,6 +1025,18 @@ class PrecisionAtRecallTest(tf.test.TestCase, parameterized.TestCase):\n     # For 0.2 < decision threshold < 0.5.\n     self.assertAlmostEqual(0.75, self.evaluate(result))\n \n+  def test_unweighted_class_id(self):\n+    s_obj = metrics.PrecisionAtRecall(0.6, class_id=2)\n+    pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]\n+    label_values = [0, 0, 0, 0, 0, 2, 2, 2, 2, 2]\n+\n+    y_pred = tf.compat.v1.transpose([pred_values] * 3)\n+    y_true = tf.one_hot(label_values, depth=3)\n+    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n+    result = s_obj(y_true, y_pred)\n+    # For 0.2 < decision threshold < 0.5.\n+    self.assertAlmostEqual(0.75, self.evaluate(result))\n+\n   @parameterized.parameters([tf.bool, tf.int32, tf.float32])\n   def test_weighted(self, label_dtype):\n     s_obj = metrics.PrecisionAtRecall(7.0/8)\n@@ -1021,11 +1067,12 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n     s_obj = metrics.RecallAtPrecision(\n-        0.4, num_thresholds=100, name='recall_at_precision_1')\n+        0.4, num_thresholds=100, class_id=12, name='recall_at_precision_1')\n     self.assertEqual(s_obj.name, 'recall_at_precision_1')\n     self.assertLen(s_obj.variables, 4)\n     self.assertEqual(s_obj.precision, 0.4)\n     self.assertEqual(s_obj.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n     # Check save and restore config\n     s_obj2 = metrics.RecallAtPrecision.from_config(s_obj.get_config())\n@@ -1033,6 +1080,7 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertLen(s_obj2.variables, 4)\n     self.assertEqual(s_obj2.precision, 0.4)\n     self.assertEqual(s_obj2.num_thresholds, 100)\n+    self.assertEqual(s_obj.class_id, 12)\n \n   def test_value_is_idempotent(self):\n     s_obj = metrics.RecallAtPrecision(0.7)\n@@ -1096,6 +1144,21 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n     # The precision 5/7 can be reached at thresholds 00.3<=t<0.35.\n     self.assertAlmostEqual(5. / 6, self.evaluate(result))\n \n+  def test_unweighted_class_id(self):\n+    s_obj = metrics.RecallAtPrecision(2.0 / 3, class_id=2)\n+    pred_values = [\n+        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95\n+    ]\n+    label_values = [0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2]\n+    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].\n+    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].\n+    y_pred = tf.compat.v1.transpose([pred_values] * 3)\n+    y_true = tf.one_hot(label_values, depth=3)\n+    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))\n+    result = s_obj(y_true, y_pred)\n+    # The precision 5/7 can be reached at thresholds 00.3<=t<0.35.\n+    self.assertAlmostEqual(5. / 6, self.evaluate(result))\n+\n   @parameterized.parameters([tf.bool, tf.int32, tf.float32])\n   def test_weighted(self, label_dtype):\n     s_obj = metrics.RecallAtPrecision(0.75)\n@@ -1409,16 +1472,20 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.AUC(summation_method='Invalid')\n \n   def test_extra_dims(self):\n+    try:\n+      from scipy import special  # pylint: disable=g-import-not-at-top\n       self.setup()\n-    logits = expit(-np.array([[[-10., 10., -10.], [10., -10., 10.]],\n+      logits = special.expit(-np.array([[[-10., 10., -10.], [10., -10., 10.]],\n                                         [[-12., 12., -12.], [12., -12., 12.]]],\n                                        dtype=np.float32))\n-    labels = np.array([[[1, 0, 0], [1, 0, 0]],\n-                       [[0, 1, 1], [0, 1, 1]]], dtype=np.int64)\n+      labels = np.array([[[1, 0, 0], [1, 0, 0]], [[0, 1, 1], [0, 1, 1]]],\n+                        dtype=np.int64)\n       auc_obj = metrics.AUC()\n       self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n       result = auc_obj(labels, logits)\n       self.assertEqual(self.evaluate(result), 0.5)\n+    except ImportError as e:\n+      tf_logging.warn('Cannot test special functions: %s' % str(e))\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -1696,14 +1763,14 @@ class MultiAUCTest(tf.test.TestCase, parameterized.TestCase):\n         metrics=[metrics.AUC(multi_label=True)]\n     )\n \n-  def test_reset_states(self):\n+  def test_reset_state(self):\n     with self.test_session():\n       self.setup()\n       auc_obj = metrics.AUC(num_thresholds=self.num_thresholds,\n                             multi_label=True)\n       self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n       auc_obj(self.y_true_good, self.y_pred)\n-      auc_obj.reset_states()\n+      auc_obj.reset_state()\n       self.assertAllEqual(auc_obj.true_positives, np.zeros((5, 2)))\n \n \n\n@@ -64,8 +64,8 @@ class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAlmostEqual(self.evaluate(m.result()), 106)\n       self.assertEqual(self.evaluate(m.total), 106)  # 100 + 1 + 5\n \n-      # check reset_states()\n-      m.reset_states()\n+      # check reset_state()\n+      m.reset_state()\n       self.assertEqual(self.evaluate(m.total), 0)\n \n   def test_sum_with_sample_weight(self):\n@@ -189,8 +189,8 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(m.total), 106)  # 100 + 1 + 5\n     self.assertEqual(self.evaluate(m.count), 3)\n \n-    # check reset_states()\n-    m.reset_states()\n+    # check reset_state()\n+    m.reset_state()\n     self.assertEqual(self.evaluate(m.total), 0)\n     self.assertEqual(self.evaluate(m.count), 0)\n \n@@ -205,10 +205,10 @@ class MeanTest(keras_parameterized.TestCase):\n   def test_function_wrapped_reset_state(self):\n     m = metrics.Mean(name='my_mean')\n \n-    # check reset_states in function.\n+    # check reset_state in function.\n     @tf.function\n     def reset_in_fn():\n-      m.reset_states()\n+      m.reset_state()\n       return m.update_state(100)\n \n     for _ in range(5):\n@@ -1415,8 +1415,8 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(self.evaluate(m.total), [101, 45])\n       self.assertAllClose(self.evaluate(m.count), [2, 2])\n \n-      # check reset_states()\n-      m.reset_states()\n+      # check reset_state()\n+      m.reset_state()\n       self.assertAllClose(self.evaluate(m.total), [0, 0])\n       self.assertAllClose(self.evaluate(m.count), [0, 0])\n \n@@ -2116,7 +2116,7 @@ def _get_model(compile_metrics):\n @keras_parameterized.run_all_keras_modes\n class ResetStatesTest(keras_parameterized.TestCase):\n \n-  def test_reset_states_false_positives(self):\n+  def test_reset_state_false_positives(self):\n     fp_obj = metrics.FalsePositives()\n     model = _get_model([fp_obj])\n     x = np.ones((100, 4))\n@@ -2126,7 +2126,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     model.evaluate(x, y)\n     self.assertEqual(self.evaluate(fp_obj.accumulator), 100.)\n \n-  def test_reset_states_false_negatives(self):\n+  def test_reset_state_false_negatives(self):\n     fn_obj = metrics.FalseNegatives()\n     model = _get_model([fn_obj])\n     x = np.zeros((100, 4))\n@@ -2136,7 +2136,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     model.evaluate(x, y)\n     self.assertEqual(self.evaluate(fn_obj.accumulator), 100.)\n \n-  def test_reset_states_true_negatives(self):\n+  def test_reset_state_true_negatives(self):\n     tn_obj = metrics.TrueNegatives()\n     model = _get_model([tn_obj])\n     x = np.zeros((100, 4))\n@@ -2146,7 +2146,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     model.evaluate(x, y)\n     self.assertEqual(self.evaluate(tn_obj.accumulator), 100.)\n \n-  def test_reset_states_true_positives(self):\n+  def test_reset_state_true_positives(self):\n     tp_obj = metrics.TruePositives()\n     model = _get_model([tp_obj])\n     x = np.ones((100, 4))\n@@ -2156,7 +2156,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     model.evaluate(x, y)\n     self.assertEqual(self.evaluate(tp_obj.accumulator), 100.)\n \n-  def test_reset_states_precision(self):\n+  def test_reset_state_precision(self):\n     p_obj = metrics.Precision()\n     model = _get_model([p_obj])\n     x = np.concatenate((np.ones((50, 4)), np.ones((50, 4))))\n@@ -2168,7 +2168,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(p_obj.true_positives), 50.)\n     self.assertEqual(self.evaluate(p_obj.false_positives), 50.)\n \n-  def test_reset_states_recall(self):\n+  def test_reset_state_recall(self):\n     r_obj = metrics.Recall()\n     model = _get_model([r_obj])\n     x = np.concatenate((np.ones((50, 4)), np.zeros((50, 4))))\n@@ -2180,7 +2180,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(r_obj.true_positives), 50.)\n     self.assertEqual(self.evaluate(r_obj.false_negatives), 50.)\n \n-  def test_reset_states_sensitivity_at_specificity(self):\n+  def test_reset_state_sensitivity_at_specificity(self):\n     s_obj = metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)\n     model = _get_model([s_obj])\n     x = np.concatenate((np.ones((25, 4)), np.zeros((25, 4)), np.zeros((25, 4)),\n@@ -2195,7 +2195,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(s_obj.false_negatives), 25.)\n       self.assertEqual(self.evaluate(s_obj.true_negatives), 25.)\n \n-  def test_reset_states_specificity_at_sensitivity(self):\n+  def test_reset_state_specificity_at_sensitivity(self):\n     s_obj = metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)\n     model = _get_model([s_obj])\n     x = np.concatenate((np.ones((25, 4)), np.zeros((25, 4)), np.zeros((25, 4)),\n@@ -2210,7 +2210,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(s_obj.false_negatives), 25.)\n       self.assertEqual(self.evaluate(s_obj.true_negatives), 25.)\n \n-  def test_reset_states_precision_at_recall(self):\n+  def test_reset_state_precision_at_recall(self):\n     s_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n     model = _get_model([s_obj])\n     x = np.concatenate((np.ones((25, 4)), np.zeros((25, 4)), np.zeros((25, 4)),\n@@ -2225,7 +2225,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(s_obj.false_negatives), 25.)\n       self.assertEqual(self.evaluate(s_obj.true_negatives), 25.)\n \n-  def test_reset_states_recall_at_precision(self):\n+  def test_reset_state_recall_at_precision(self):\n     s_obj = metrics.RecallAtPrecision(precision=0.5, num_thresholds=1)\n     model = _get_model([s_obj])\n     x = np.concatenate((np.ones((25, 4)), np.zeros((25, 4)), np.zeros((25, 4)),\n@@ -2240,7 +2240,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(s_obj.false_negatives), 25.)\n       self.assertEqual(self.evaluate(s_obj.true_negatives), 25.)\n \n-  def test_reset_states_auc(self):\n+  def test_reset_state_auc(self):\n     auc_obj = metrics.AUC(num_thresholds=3)\n     model = _get_model([auc_obj])\n     x = np.concatenate((np.ones((25, 4)), np.zeros((25, 4)), np.zeros((25, 4)),\n@@ -2255,7 +2255,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.)\n       self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.)\n \n-  def test_reset_states_auc_from_logits(self):\n+  def test_reset_state_auc_from_logits(self):\n     auc_obj = metrics.AUC(num_thresholds=3, from_logits=True)\n \n     model_layers = [layers.Dense(1, kernel_initializer='ones', use_bias=False)]\n@@ -2278,7 +2278,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.)\n       self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.)\n \n-  def test_reset_states_auc_manual_thresholds(self):\n+  def test_reset_state_auc_manual_thresholds(self):\n     auc_obj = metrics.AUC(thresholds=[0.5])\n     model = _get_model([auc_obj])\n     x = np.concatenate((np.ones((25, 4)), np.zeros((25, 4)), np.zeros((25, 4)),\n@@ -2293,7 +2293,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.)\n       self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.)\n \n-  def test_reset_states_mean_iou(self):\n+  def test_reset_state_mean_iou(self):\n     m_obj = metrics.MeanIoU(num_classes=2)\n     model = _get_model([m_obj])\n     x = np.asarray([[0, 0, 0, 0], [1, 1, 1, 1], [1, 0, 1, 0], [0, 1, 0, 1]],\n@@ -2306,7 +2306,7 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     self.assertArrayNear(self.evaluate(m_obj.total_cm)[0], [1, 0], 1e-1)\n     self.assertArrayNear(self.evaluate(m_obj.total_cm)[1], [3, 0], 1e-1)\n \n-  def test_reset_states_recall_float64(self):\n+  def test_reset_state_recall_float64(self):\n     # Test case for GitHub issue 36790.\n     try:\n       backend.set_floatx('float64')\n\n@@ -386,22 +386,23 @@ def _clone_sequential_model(model, input_tensors=None, layer_fn=_clone_layer):\n \n @keras_export('keras.models.clone_model')\n def clone_model(model, input_tensors=None, clone_function=None):\n-  \"\"\"Clone any `Model` instance.\n+  \"\"\"Clone a Functional or Sequential `Model` instance.\n \n   Model cloning is similar to calling a model on new inputs,\n   except that it creates new layers (and thus new weights) instead\n   of sharing the weights of the existing layers.\n \n+  Note that\n   `clone_model` will not preserve the uniqueness of shared objects within the\n   model (e.g. a single variable attached to two distinct layers will be\n   restored as two separate variables).\n \n   Args:\n       model: Instance of `Model`\n-          (could be a functional model or a Sequential model).\n+          (could be a Functional model or a Sequential model).\n       input_tensors: optional list of input tensors or InputLayer objects\n           to build the model upon. If not provided,\n-          placeholders will be created.\n+          new `Input` objects will be created.\n       clone_function: Callable to be used to clone each layer in the target\n           model (except `InputLayer` instances). It takes as argument the layer\n           instance to be cloned, and returns the corresponding layer instance to\n@@ -416,12 +417,32 @@ def clone_model(model, input_tensors=None, clone_function=None):\n   Returns:\n     An instance of `Model` reproducing the behavior\n     of the original model, on top of new inputs tensors,\n-      using newly instantiated weights. The cloned model might behave\n-      differently from the original model if a custom clone_function\n+    using newly instantiated weights. The cloned model may behave\n+    differently from the original model if a custom `clone_function`\n     modifies the layer.\n \n-  Raises:\n-      ValueError: in case of invalid `model` argument value.\n+  Example:\n+\n+  ```python\n+  # Create a test Sequential model.\n+  model = keras.Sequential([\n+      keras.Input(shape=(728,)),\n+      keras.layers.Dense(32, activation='relu'),\n+      keras.layers.Dense(1, activation='sigmoid'),\n+  ])\n+  # Create a copy of the test model (with freshly initialized weights).\n+  new_model = clone_model(model)\n+  ```\n+\n+  Note that subclassed models cannot be cloned, since their internal\n+  layer structure is not known. To achieve equivalent functionality\n+  as `clone_model` in the case of a subclassed model, simply make sure\n+  that the model class implements `get_config()`\n+  (and optionally `from_config()`), and call:\n+\n+  ```python\n+  new_model = model.__class__.from_config(model.get_config())\n+  ```\n   \"\"\"\n   with generic_utils.DisableSharedObjectScope():\n     if clone_function is None:\n\n@@ -33,42 +33,34 @@ class Adadelta(optimizer_v2.OptimizerV2):\n   Adadelta optimization is a stochastic gradient descent method that is based on\n   adaptive learning rate per dimension to address two drawbacks:\n \n-  - The continual decay of learning rates throughout training\n-  - The need for a manually selected global learning rate\n+  - The continual decay of learning rates throughout training.\n+  - The need for a manually selected global learning rate.\n \n   Adadelta is a more robust extension of Adagrad that adapts learning rates\n   based on a moving window of gradient updates, instead of accumulating all\n   past gradients. This way, Adadelta continues learning even when many updates\n   have been done. Compared to Adagrad, in the original version of Adadelta you\n-  don't have to set an initial learning rate. In this version, initial\n+  don't have to set an initial learning rate. In this version, the initial\n   learning rate can be set, as in most other Keras optimizers.\n \n-  According to section 4.3 (\"Effective Learning rates\"), near the end of\n-  training step sizes converge to 1 which is effectively a high learning\n-  rate which would cause divergence. This occurs only near the end of the\n-  training as gradients and step sizes are small, and the epsilon constant\n-  in the numerator and denominator dominate past gradients and parameter\n-  updates which converge the learning rate to 1.\n-\n-  According to section 4.4(\"Speech Data\"),where a large neural network with\n-  4 hidden layers was trained on a corpus of US English data, ADADELTA was\n-  used with 100 network replicas.The epsilon used is 1e-6 with rho=0.95\n-  which converged faster than ADAGRAD, by the following construction:\n-  def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, decay=0., **kwargs):\n-\n   Args:\n-    learning_rate: A `Tensor`, floating point value, or a schedule that is a\n-      `tf.keras.optimizers.schedules.LearningRateSchedule`. The learning rate.\n-      To match the exact form in the original paper use 1.0.\n+    learning_rate: Initial value for the learning rate:\n+      either a floating point value,\n+      or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n+      Defaults to 0.001.\n+      Note that `Adadelta` tends to benefit from higher initial learning rate\n+      values compared to other optimizers.\n+      To match the exact form in the original paper, use 1.0.\n     rho: A `Tensor` or a floating point value. The decay rate.\n-    epsilon: A `Tensor` or a floating point value.  A constant epsilon used\n-             to better conditioning the grad update.\n+    epsilon: Small floating point value used to maintain numerical stability.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adadelta\"`.\n     **kwargs: Keyword arguments. Allowed to be one of\n       `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+      `\"clipnorm\"` (float) clips gradients by norm and represents\n+      the maximum norm of each parameter;\n+      `\"clipvalue\"` (float) clips gradient by value and represents the\n+      maximum absolute value of each parameter.\n \n   Reference:\n     - [Zeiler, 2012](http://arxiv.org/abs/1212.5701)\n\n@@ -25,7 +25,10 @@ import numpy as np\n from keras import combinations\n from keras.optimizer_v2 import adadelta\n \n-_DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n+_DATA_TYPES = [\n+    tf.half, tf.float32, tf.float64, tf.complex64,\n+    tf.complex128\n+]\n \n \n class AdadeltaOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -36,17 +36,25 @@ class Adagrad(optimizer_v2.OptimizerV2):\n   the smaller the updates.\n \n   Args:\n-    learning_rate: A `Tensor`, floating point value, or a schedule that is a\n-      `tf.keras.optimizers.schedules.LearningRateSchedule`. The learning rate.\n-    initial_accumulator_value: A floating point value.\n-      Starting value for the accumulators, must be non-negative.\n-    epsilon: A small floating point value to avoid zero denominator.\n+    learning_rate: Initial value for the learning rate:\n+      either a floating point value,\n+      or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n+      Defaults to 0.001.\n+      Note that `Adagrad` tends to benefit from higher initial learning rate\n+      values compared to other optimizers.\n+      To match the exact form in the original paper, use 1.0.\n+    initial_accumulator_value: Floating point value.\n+      Starting value for the accumulators (per-parameter momentum values).\n+      Must be non-negative.\n+    epsilon: Small floating point value used to maintain numerical stability.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adagrad\"`.\n     **kwargs: Keyword arguments. Allowed to be one of\n       `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+      `\"clipnorm\"` (float) clips gradients by norm and represents\n+      the maximum L2 norm of each weight variable;\n+      `\"clipvalue\"` (float) clips gradient by value and represents the\n+      maximum absolute value of each weight variable.\n \n   Reference:\n     - [Duchi et al., 2011](\n\n@@ -28,7 +28,10 @@ from keras import combinations\n from keras.optimizer_v2 import adagrad\n from keras.optimizer_v2 import learning_rate_schedule\n \n-_DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n+_DATA_TYPES = [\n+    tf.half, tf.float32, tf.float64, tf.complex64,\n+    tf.complex128\n+]\n \n \n def adagrad_update_numpy(param, accum, g_t, lr=0.001, epsilon=1e-7):\n\n@@ -27,12 +27,46 @@ from tensorflow.python.util.tf_export import keras_export\n \n @keras_export(\"keras.optimizers.schedules.LearningRateSchedule\")\n class LearningRateSchedule(object):\n-  \"\"\"A serializable learning rate decay schedule.\n+  \"\"\"The learning rate schedule base class.\n \n-  `LearningRateSchedule`s can be passed in as the learning rate of optimizers in\n-  `tf.keras.optimizers`. They can be serialized and deserialized using\n-  `tf.keras.optimizers.schedules.serialize` and\n-  `tf.keras.optimizers.schedules.deserialize`.\n+  You can use a learning rate schedule to modulate how the learning rate\n+  of your optimizer changes over time.\n+\n+  Several built-in learning rate schedules are available, such as\n+  `tf.keras.optimizers.schedules.ExponentialDecay` or\n+  `tf.keras.optimizers.schedules.PiecewiseConstantDecay`:\n+\n+  ```python\n+  lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n+      initial_learning_rate=1e-2,\n+      decay_steps=10000,\n+      decay_rate=0.9)\n+  optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n+  ```\n+\n+  A `LearningRateSchedule` instance can be passed in as the `learning_rate`\n+  argument of any optimizer.\n+\n+  To implement your own schedule object, you should implement the `__call__`\n+  method, which takes a `step` argument (scalar integer tensor, the\n+  current training step count).\n+  Like for any other Keras object, you can also optionally\n+  make your object serializable by implementing the `get_config`\n+  and `from_config` methods.\n+\n+  Example:\n+\n+  ```python\n+  class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n+\n+    def __init__(self, initial_learning_rate):\n+      self.initial_learning_rate = initial_learning_rate\n+\n+    def __call__(self, step):\n+       return self.initial_learning_rate / (step + 1)\n+\n+  optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n+  ```\n   \"\"\"\n \n   @abc.abstractmethod\n\n@@ -32,7 +32,10 @@ from keras import testing_utils\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.optimizer_v2 import rmsprop\n \n-_DATA_TYPES = [tf.half, tf.float32, tf.float64, tf.complex64, tf.complex128]\n+_DATA_TYPES = [\n+    tf.half, tf.float32, tf.float64, tf.complex64,\n+    tf.complex128\n+]\n \n _TEST_PARAM_VALUES = [\n     # learning_rate, rho, momentum, epsilon, centered\n\n@@ -44,6 +44,22 @@ from tensorflow.python.util.tf_export import keras_export\n \n @keras_export('keras.optimizers.serialize')\n def serialize(optimizer):\n+  \"\"\"Serialize the optimizer configuration to JSON compatible python dict.\n+\n+  The configuration can be used for persistence and reconstruct the `Optimizer`\n+  instance again.\n+\n+  >>> tf.keras.optimizers.serialize(tf.keras.optimizers.SGD())\n+  {'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,\n+                                   'decay': 0.0, 'momentum': 0.0,\n+                                   'nesterov': False}}\n+\n+  Args:\n+    optimizer: An `Optimizer` instance to serialize.\n+\n+  Returns:\n+    Python dict which contains the configuration of the input optimizer.\n+  \"\"\"\n   return serialize_keras_object(optimizer)\n \n \n\n@@ -40,7 +40,6 @@ from keras.utils import metrics_utils\n from keras.utils.generic_utils import LazyLoader\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.saved_model import loader_impl\n-from tensorflow.python.saved_model import nested_structure_coder\n from tensorflow.python.saved_model import revived_types\n from tensorflow.python.training.tracking import base as trackable\n \n@@ -749,7 +748,7 @@ class KerasObjectLoader(object):\n \n   def _infer_inputs(self, layer_node_id, convert_to_shapes=False):\n     \"\"\"Infers input shape of layer from SavedModel functions.\"\"\"\n-    coder = nested_structure_coder.StructureCoder()\n+    coder = tf.__internal__.saved_model.StructureCoder()\n     call_fn_id = self._search_for_child_node(\n         layer_node_id, ['call_and_return_all_conditional_losses'])\n     if call_fn_id is None:\n\n@@ -26,8 +26,6 @@ import tensorflow.compat.v2 as tf\n import functools\n import threading\n import weakref\n-\n-from tensorflow.python.eager import def_function\n from keras import backend as K\n from keras.engine import base_layer_utils\n from keras.engine import input_spec\n@@ -420,7 +418,7 @@ class LayerCallCollection(object):\n       List of possibly nested TensorSpecs of the layer call function inputs.\n       The list does not contain the `training` argument.\n     \"\"\"\n-    if (isinstance(layer.call, def_function.Function) and\n+    if (isinstance(layer.call, tf.__internal__.function.Function) and\n         layer.call.input_signature is not None):\n       return layer.call.input_signature\n     elif isinstance(layer, training_lib.Model):\n@@ -687,19 +685,19 @@ def _wrap_unconditional_loss(loss_fn, index):\n   \"\"\"Wraps callable/unconditional loss, returning a serializable function.\"\"\"\n   # Extract original loss function from partial function\n   fn = loss_fn.args[0] if isinstance(loss_fn, functools.partial) else loss_fn\n-  if isinstance(fn, def_function.Function):\n+  if isinstance(fn, tf.__internal__.function.Function):\n     return fn\n   else:\n-    return def_function.Function(\n+    return tf.__internal__.function.Function(\n         fn, 'loss_fn_{}'.format(index), input_signature=[])\n \n \n def _wrap_activity_regularizer(layer):\n   \"\"\"Wraps the activity regularizer.\"\"\"\n   # pylint: disable=protected-access\n-  if isinstance(layer._activity_regularizer, def_function.Function):\n+  if isinstance(layer._activity_regularizer, tf.__internal__.function.Function):\n     return layer._activity_regularizer\n-  return def_function.Function(\n+  return tf.__internal__.function.Function(\n       layer._activity_regularizer,\n       '{}_activity_regularizer'.format(layer.name),\n       input_signature=[\n@@ -709,6 +707,6 @@ def _wrap_activity_regularizer(layer):\n \n \n def _get_layer_call_method(layer):\n-  if isinstance(layer.call, (def_function.Function)):\n+  if isinstance(layer.call, (tf.__internal__.function.Function)):\n     return layer.call.python_function\n   return layer.call\n\n@@ -19,8 +19,6 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n-\n-from tensorflow.python.eager import def_function\n from keras.saving.saved_model import constants\n from keras.saving.saved_model import save_impl\n from keras.utils.generic_utils import LazyLoader\n@@ -196,7 +194,7 @@ class SerializedAttributes(object):\n       if key in function_dict:\n         if (function_dict[key] is not None and  # Not all functions are required\n             not isinstance(function_dict[key],\n-                           (def_function.Function, save_impl.LayerCall))):\n+                           (tf.__internal__.function.Function, save_impl.LayerCall))):\n           raise ValueError(\n               'Function dictionary contained a non-function object: {} (for key'\n               ' {})'.format(function_dict[key], key))\n\n@@ -23,8 +23,6 @@ import collections.abc as collections_abc\n import copy\n import os\n import six\n-\n-from tensorflow.python.eager import def_function\n from keras import backend as K\n from keras import losses\n from keras import optimizer_v1\n@@ -114,7 +112,7 @@ def trace_model_call(model, input_signature=None):\n     ValueError: if input signature cannot be inferred from the model.\n   \"\"\"\n   if input_signature is None:\n-    if isinstance(model.call, def_function.Function):\n+    if isinstance(model.call, tf.__internal__.function.Function):\n       input_signature = model.call.input_signature\n \n   if input_signature is None:\n\n@@ -19,14 +19,17 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n \n \n+@keras_export('keras.utils.experimental.DatasetCreator', v1=[])\n class DatasetCreator(object):\n   \"\"\"Object that returns a `tf.data.Dataset` upon invoking.\n \n-  `DatasetCreator` is designated as a supported type for `x`, or the input, in\n-  `tf.keras.Model.fit`. Pass an instance of this class to `fit` when using a\n-  callable (with a `input_context` argument) that returns a `tf.data.Dataset`.\n+  `tf.keras.utils.experimental.DatasetCreator` is designated as a supported type\n+  for `x`, or the input, in `tf.keras.Model.fit`. Pass an instance of this class\n+  to `fit` when using a callable (with a `input_context` argument) that returns\n+  a `tf.data.Dataset`.\n \n   ```python\n   model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n@@ -45,6 +48,22 @@ class DatasetCreator(object):\n   model.fit(DatasetCreator(dataset_fn), epochs=10, steps_per_epoch=10)\n   ```\n \n+  `Model.fit` usage with `DatasetCreator` is intended to work across all\n+  `tf.distribute.Strategy`s, as long as `Strategy.scope` is used at model\n+  creation:\n+\n+  ```python\n+  strategy = tf.distribute.experimental.ParameterServerStrategy(\n+      cluster_resolver)\n+  with strategy.scope():\n+    model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n+  model.compile(tf.keras.optimizers.SGD(), loss=\"mse\")\n+  ...\n+  ```\n+\n+  Note: When using `DatasetCreator`, `steps_per_epoch` argument in `Model.fit`\n+  must be provided as the cardinality of such input cannot be inferred.\n+\n   Args:\n     dataset_fn: A callable that takes a single argument of type\n       `tf.distribute.InputContext`, which is used for batch size calculation and\n\n@@ -83,24 +83,15 @@ def to_categorical(y, num_classes=None, dtype='float32'):\n \n @keras_export('keras.utils.normalize')\n def normalize(x, axis=-1, order=2):\n-  \"\"\"Normalizes a NumPy array.\n+  \"\"\"Normalizes a Numpy array.\n \n   Args:\n-      x: NumPy array to normalize.\n-      axis: Axis along which to normalize. For instance, `axis=-1` corresponds\n-        to feature-wise normalization.\n-      order: Normalization order (e.g. `order=2` for the L2 norm).\n+      x: Numpy array to normalize.\n+      axis: axis along which to normalize.\n+      order: Normalization order (e.g. `order=2` for L2 norm).\n \n   Returns:\n       A normalized copy of the array.\n-\n-  Example:\n-\n-  >>> array = np.random.random(size=(32, 3))\n-  >>> normalized_array = tf.keras.utils.normalize(array, axis=-1)\n-  >>> # Every element in the batch has now a unit norm\n-  >>> for i in range(32):\n-  >>>     np.testing.assert_allclose(np.square(normalized_array[i, :]).sum(), 1)\n   \"\"\"\n   l2 = np.atleast_1d(np.linalg.norm(x, order, axis))\n   l2[l2 == 0] = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
