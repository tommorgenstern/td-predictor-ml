{"custom_id": "scrapy#d68615a5af6df2f972c7d46ab3fb8950f15747e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 69 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,10 +7,12 @@ from twisted.web import server, resource, static, util\n class SiteTest(object):\n \n     def setUp(self):\n+        super(SiteTest, self).setUp()\n         self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n         self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port\n \n     def tearDown(self):\n+        super(SiteTest, self).tearDown()\n         self.site.stopListening()\n \n     def url(self, path):\n\n@@ -8,9 +8,13 @@ from shutil import rmtree\n from tempfile import mkdtemp\n \n from twisted.trial import unittest\n+from twisted.internet import defer\n \n from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.test import get_testenv\n+from scrapy.utils.testsite import SiteTest\n+from scrapy.utils.testproc import ProcessTest\n+\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -177,7 +181,9 @@ from scrapy.spider import Spider\n         self.assert_(\"Unable to load\" in log)\n \n \n-class ParseCommandTest(CommandTest):\n+class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n+\n+    command = 'parse'\n \n     def setUp(self):\n         super(ParseCommandTest, self).setUp()\n@@ -217,17 +223,21 @@ class MyPipeline(object):\n ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n \"\"\" % self.project_name)\n \n+    @defer.inlineCallbacks\n     def test_spider_arguments(self):\n-        p = self.proc('parse', '--spider', self.spider_name, '-a', 'test_arg=1',\n-                '-c', 'parse', 'http://scrapinghub.com')\n-        log = p.stderr.read()\n-        self.assert_(\"[parse_spider] DEBUG: It Works!\" in log, log)\n+        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n+                                           '-a', 'test_arg=1',\n+                                           '-c', 'parse',\n+                                           self.url('/html')])\n+        self.assert_(\"[parse_spider] DEBUG: It Works!\" in stderr, stderr)\n \n+    @defer.inlineCallbacks\n     def test_pipelines(self):\n-        p = self.proc('parse', '--spider', self.spider_name, '--pipelines',\n-                '-c', 'parse', 'http://scrapinghub.com')\n-        log = p.stderr.read()\n-        self.assert_(\"[scrapy] INFO: It Works!\" in log, log)\n+        _, _, stderr = yield self.execute(['--spider', self.spider_name,\n+                                           '--pipelines',\n+                                           '-c', 'parse',\n+                                           self.url('/html')])\n+        self.assert_(\"[scrapy] INFO: It Works!\" in stderr, stderr)\n \n \n class BenchCommandTest(CommandTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd5d99a2d2dd61868d8c69acf410f27238443abb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 27 | Churn Cumulative: 1070 | Contributors (this commit): 19 | Commits (past 90d): 5 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from __future__ import print_function\n import os\n+import gzip\n from six.moves import cPickle as pickle\n from importlib import import_module\n from time import time\n@@ -220,6 +221,8 @@ class FilesystemCacheStorage(object):\n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n+        self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n+        self._open = gzip.open if self.use_gzip else open\n \n     def open_spider(self, spider):\n         pass\n@@ -233,9 +236,9 @@ class FilesystemCacheStorage(object):\n         if metadata is None:\n             return  # not cached\n         rpath = self._get_request_path(spider, request)\n-        with open(os.path.join(rpath, 'response_body'), 'rb') as f:\n+        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n             body = f.read()\n-        with open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n+        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n             rawheaders = f.read()\n         url = metadata.get('response_url')\n         status = metadata['status']\n@@ -256,17 +259,17 @@ class FilesystemCacheStorage(object):\n             'response_url': response.url,\n             'timestamp': time(),\n         }\n-        with open(os.path.join(rpath, 'meta'), 'wb') as f:\n+        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n             f.write(repr(metadata))\n-        with open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n+        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n             pickle.dump(metadata, f, protocol=2)\n-        with open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n+        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n             f.write(headers_dict_to_raw(response.headers))\n-        with open(os.path.join(rpath, 'response_body'), 'wb') as f:\n+        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n             f.write(response.body)\n-        with open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n+        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n             f.write(headers_dict_to_raw(request.headers))\n-        with open(os.path.join(rpath, 'request_body'), 'wb') as f:\n+        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n             f.write(request.body)\n \n     def _get_request_path(self, spider, request):\n@@ -281,7 +284,7 @@ class FilesystemCacheStorage(object):\n         mtime = os.stat(rpath).st_mtime\n         if 0 < self.expiration_secs < time() - mtime:\n             return  # expired\n-        with open(metapath, 'rb') as f:\n+        with self._open(metapath, 'rb') as f:\n             return pickle.load(f)\n \n \n\n@@ -154,6 +154,7 @@ HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_DBM_MODULE = 'anydbm'\n HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.DummyPolicy'\n+HTTPCACHE_GZIP = False\n \n ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n \n\n@@ -136,6 +136,11 @@ class FilesystemStorageTest(DefaultStorageTest):\n \n     storage_class = 'scrapy.contrib.httpcache.FilesystemCacheStorage'\n \n+class FilesystemStorageGzipTest(FilesystemStorageTest):\n+\n+    def _get_settings(self, **new_settings):\n+        new_settings.setdefault('HTTPCACHE_GZIP', True)\n+        return super(FilesystemStorageTest, self)._get_settings(**new_settings)\n \n class LeveldbStorageTest(DefaultStorageTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#721d8d5daedf543a51a074e5d8691312e63acbd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 656 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-This module implements the FormRequest class which is a more covenient class\n+This module implements the FormRequest class which is a more convenient class\n (than Request) to generate Requests based on form data.\n \n See documentation in docs/topics/request-response.rst\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ea4a72b8eecdae6a2ed29adbbc96df9987509ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 17 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -9,9 +9,10 @@ if 'django' not in optional_features:\n     collect_ignore.append(\"tests/test_djangoitem/models.py\")\n \n if six.PY3:\n-    for fn in open('tests/py3-ignores.txt'):\n-        if fn.strip():\n-            collect_ignore.append(fn.strip())\n+    for line in open('tests/py3-ignores.txt'):\n+        filePath = line.strip()\n+        if len(filePath) > 0 and filePath[0] != '#':    \n+            collect_ignore.append(filePath)\n \n class LogObservers:\n     \"\"\"Class for keeping track of log observers across test modules\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0299d97f56d1d034ac5178d1c1ef9da57aa0cde", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 23 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -10,9 +10,9 @@ if 'django' not in optional_features:\n \n if six.PY3:\n     for line in open('tests/py3-ignores.txt'):\n-        filePath = line.strip()\n-        if len(filePath) > 0 and filePath[0] != '#':    \n-            collect_ignore.append(filePath)\n+        file_path = line.strip()\n+        if len(file_path) > 0 and file_path[0] != '#':\n+            collect_ignore.append(file_path)\n \n class LogObservers:\n     \"\"\"Class for keeping track of log observers across test modules\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d67ca77e61020802c593c8b60a977e26bebfd7c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 1132 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,7 +19,7 @@ from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n-from scrapy import log\n+from scrapy import log, twisted_version\n \n \n class HTTP11DownloadHandler(object):\n@@ -142,12 +142,21 @@ class TunnelingAgent(Agent):\n         self._proxyConf = proxyConf\n         self._contextFactory = contextFactory\n \n+    if twisted_version >= (15, 0, 0):\n+        def _getEndpoint(self, uri):\n+            return TunnelingTCP4ClientEndpoint(\n+                self._reactor, uri.host, uri.port, self._proxyConf,\n+                self._contextFactory, self._endpointFactory._connectTimeout,\n+                self._endpointFactory._bindAddress)\n+    else:\n         def _getEndpoint(self, scheme, host, port):\n-        return TunnelingTCP4ClientEndpoint(self._reactor, host, port,\n-            self._proxyConf, self._contextFactory, self._connectTimeout,\n+            return TunnelingTCP4ClientEndpoint(\n+                self._reactor, host, port, self._proxyConf,\n+                self._contextFactory, self._connectTimeout,\n                 self._bindAddress)\n \n \n+\n class ScrapyAgent(object):\n \n     _Agent = Agent\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#839ffba971c29b8aa5875ca60d5ea280acd95ae3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 67/65 | Churn Δ: 49 | Churn Cumulative: 109 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.datatypes import MergeDict\n from .common import wrap_loader_context\n \n+\n class MapCompose(object):\n \n     def __init__(self, *functions, **default_loader_context):\n@@ -63,6 +64,26 @@ class Identity(object):\n         return values\n \n \n+class SelectJmes(object):\n+    \"\"\"\n+        Query the input string for the jmespath (given at instantiation),\n+        and return the answer\n+        Requires : jmespath(https://github.com/jmespath/jmespath)\n+        Note: SelectJmes accepts only one input element at a time.\n+    \"\"\"\n+    def __init__(self, json_path):\n+        self.json_path = json_path\n+        import jmespath\n+        self.compiled_path = jmespath.compile(self.json_path)\n+\n+    def __call__(self, value):\n+        \"\"\"Query value for the jmespath query and return answer\n+        :param str value: a string with JSON data to extract from\n+        :return: Element extracted according to jmespath query\n+        \"\"\"\n+        return self.compiled_path.search(value)\n+\n+\n class Join(object):\n \n     def __init__(self, separator=u' '):\n\n@@ -3,12 +3,11 @@ from functools import partial\n \n from scrapy.contrib.loader import ItemLoader\n from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \\\n-    Compose, MapCompose\n+    Compose, MapCompose, SelectJmes\n from scrapy.item import Item, Field\n from scrapy.selector import Selector\n from scrapy.http import HtmlResponse\n \n-\n # test items\n class NameItem(Item):\n     name = Field()\n@@ -579,5 +578,30 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value('url'), [u'scrapy.org'])\n \n \n+class SelectJmesTestCase(unittest.TestCase):\n+        test_list_equals = {\n+            'simple': ('foo.bar', {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n+            'invalid': ('foo.bar.baz', {\"foo\": {\"bar\": \"baz\"}}, None),\n+            'top_level': ('foo', {\"foo\": {\"bar\": \"baz\"}}, {\"bar\": \"baz\"}),\n+            'double_vs_single_quote_string': ('foo.bar', {\"foo\": {\"bar\": \"baz\"}}, \"baz\"),\n+            'dict': (\n+                'foo.bar[*].name',\n+                {\"foo\": {\"bar\": [{\"name\": \"one\"}, {\"name\": \"two\"}]}},\n+                ['one', 'two']\n+            ),\n+            'list': ('[1]', [1, 2], 2)\n+        }\n+\n+        def test_output(self):\n+            for l in self.test_list_equals:\n+                expr, test_list, expected = self.test_list_equals[l]\n+                test = SelectJmes(expr)(test_list)\n+                self.assertEqual(\n+                    test,\n+                    expected,\n+                    msg='test \"{}\" got {} expected {}'.format(l, test, expected)\n+                )\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31e5f164d41ad4781bb610db53f7ab7bb5ddc4f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 7 | Hunks: 13 | Methods Changed: 0 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 26 | Churn Cumulative: 371 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,11 @@ see http://doc.scrapy.org/en/latest/contributing.html#running-tests\n \n import os\n \n+try:\n+    import unittest.mock as mock\n+except ImportError:\n+    import mock\n+\n tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data')\n \n def get_testdata(*paths):\n\n@@ -1,9 +1,9 @@\n import json\n import socket\n-import mock\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import get_crawler, get_testlog\n+from tests import mock\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from tests.mockserver import MockServer\n\n@@ -1,6 +1,5 @@\n from __future__ import absolute_import\n import re\n-import mock\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n@@ -8,6 +7,7 @@ from scrapy.contrib.downloadermiddleware.robotstxt import RobotsTxtMiddleware\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n+from tests import mock\n \n \n class RobotsTxtMiddlewareTest(unittest.TestCase):\n\n@@ -1,4 +1,3 @@\n-import mock\n import os\n import time\n import hashlib\n@@ -14,6 +13,8 @@ from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n \n+from tests import mock\n+\n \n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n\n@@ -1,12 +1,9 @@\n import six\n import unittest\n import warnings\n-try:\n-    from unittest import mock\n-except ImportError:\n-    import mock\n \n from scrapy.settings import Settings, SettingsAttribute, CrawlerSettings\n+from tests import mock\n from . import default_settings\n \n \n\n@@ -3,10 +3,6 @@ import inspect\n import warnings\n from io import BytesIO\n from twisted.trial import unittest\n-try:\n-    from unittest import mock\n-except ImportError:\n-    import mock\n \n from scrapy import signals\n from scrapy.spider import Spider, BaseSpider\n@@ -20,6 +16,8 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.test import get_crawler\n \n+from tests import mock\n+\n \n class SpiderTest(unittest.TestCase):\n \n\n@@ -3,9 +3,10 @@ from __future__ import absolute_import\n import inspect\n import unittest\n import warnings\n-import mock\n from scrapy.utils.deprecate import create_deprecated_class\n \n+from tests import mock\n+\n \n class MyWarning(UserWarning):\n     pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c86e1bebec9d1e994ca4afdc13610c0fb59d6c53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 48 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -33,6 +33,9 @@ class SettingsAttributeTest(unittest.TestCase):\n \n class SettingsTest(unittest.TestCase):\n \n+    if six.PY3:\n+        assertItemsEqual = unittest.TestCase.assertCountEqual\n+\n     def setUp(self):\n         self.settings = Settings()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c13e23641bbc66c5d1a760310c6eb97e35c12c11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 18 | Churn Cumulative: 1274 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,6 +28,9 @@ class HttpCacheMiddleware(object):\n         self.storage.close_spider(spider)\n \n     def process_request(self, request, spider):\n+        if request.meta.get('dont_cache', False):\n+            return\n+\n         # Skip uncacheable requests\n         if not self.policy.should_cache_request(request):\n             request.meta['_dont_cache'] = True  # flag as uncacheable\n@@ -53,6 +56,9 @@ class HttpCacheMiddleware(object):\n         request.meta['cached_response'] = cachedresponse\n \n     def process_response(self, request, response, spider):\n+        if request.meta.get('dont_cache', False):\n+            return response\n+\n         # Skip cached responses and uncacheable requests\n         if 'cached' in response.flags or '_dont_cache' in request.meta:\n             request.meta.pop('_dont_cache', None)\n\n@@ -89,6 +89,18 @@ class _BaseTest(unittest.TestCase):\n         assert any(h in request2.headers for h in ('If-None-Match', 'If-Modified-Since'))\n         self.assertEqual(request1.body, request2.body)\n \n+    def test_dont_cache(self):\n+        with self._middleware() as mw:\n+            self.request.meta['dont_cache'] = True\n+            mw.process_response(self.request, self.response, self.spider)\n+            self.assertEqual(mw.storage.retrieve_response(self.spider, self.request), None)\n+\n+        with self._middleware() as mw:\n+            self.request.meta['dont_cache'] = False\n+            mw.process_response(self.request, self.response, self.spider)\n+            if mw.policy.should_cache_response(self.response, self.request):\n+                self.assertIsInstance(mw.storage.retrieve_response(self.spider, self.request), self.response.__class__)\n+\n \n class DefaultStorageTest(_BaseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05cb31d3060c543e4266f0e3b67eb332ac4abe0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 13 | Files Changed: 5 | Hunks: 43 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 64 | Churn Cumulative: 136 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -33,11 +33,14 @@ __author__ = \"Patrick K. O'Brien <pobrien@orbtech.com>\"\n __cvsid__ = \"$Id: dispatcher.py,v 1.1.1.1 2006/07/07 15:59:38 mcfletch Exp $\"\n __version__ = \"$Revision: 1.1.1.1 $\"[11:-2]\n \n+\n class _Parameter:\n     \"\"\"Used to represent default parameter values.\"\"\"\n+\n     def __repr__(self):\n         return self.__class__.__name__\n \n+\n class _Any(_Parameter):\n     \"\"\"Singleton used to signal either \"Any Sender\" or \"Any Signal\"\n \n@@ -46,8 +49,11 @@ class _Any(_Parameter):\n     Any should react to all senders/signals, not just\n     a particular sender/signal.\n     \"\"\"\n+\n+\n Any = _Any()\n \n+\n class _Anonymous(_Parameter):\n     \"\"\"Singleton used to signal \"Anonymous Sender\"\n \n@@ -66,6 +72,8 @@ class _Anonymous(_Parameter):\n         as though there was a single sender (Anonymous)\n         being used everywhere.\n     \"\"\"\n+\n+\n Anonymous = _Anonymous()\n \n WEAKREF_TYPES = (weakref.ReferenceType, saferef.BoundMethodWeakref)\n@@ -129,7 +137,8 @@ def connect(receiver, signal=Any, sender=Any, weak=True):\n     \"\"\"\n     if signal is None:\n         raise errors.DispatcherTypeError(\n-\t\t\t'Signal cannot be None (receiver=%r sender=%r)'%( receiver,sender)\n+            'Signal cannot be None (receiver=%r sender=%r)' % (\n+                receiver, sender)\n         )\n     if weak:\n         receiver = saferef.safeRef(receiver, onDelete=_removeReceiver)\n@@ -143,6 +152,7 @@ def connect(receiver, signal=Any, sender=Any, weak=True):\n     if sender not in (None, Anonymous, Any):\n         def remove(object, senderkey=senderkey):\n             _removeSender(senderkey=senderkey)\n+\n         # Skip objects that can not be weakly referenced, which means\n         # they won't be automatically cleaned up, but that's too bad.\n         try:\n@@ -171,7 +181,6 @@ def connect(receiver, signal=Any, sender=Any, weak=True):\n     receivers.append(receiver)\n \n \n-\n def disconnect(receiver, signal=Any, sender=Any, weak=True):\n     \"\"\"Disconnect receiver from sender for signal\n \n@@ -200,7 +209,8 @@ def disconnect(receiver, signal=Any, sender=Any, weak=True):\n     \"\"\"\n     if signal is None:\n         raise errors.DispatcherTypeError(\n-\t\t\t'Signal cannot be None (receiver=%r sender=%r)'%( receiver,sender)\n+            'Signal cannot be None (receiver=%r sender=%r)' % (\n+                receiver, sender)\n         )\n     if weak: receiver = saferef.safeRef(receiver)\n     senderkey = id(sender)\n@@ -227,6 +237,7 @@ def disconnect(receiver, signal=Any, sender=Any, weak=True):\n         )\n     _cleanupConnections(senderkey, signal)\n \n+\n def getReceivers(sender=Any, signal=Any):\n     \"\"\"Get list of receivers from global tables\n \n@@ -250,6 +261,7 @@ def getReceivers( sender = Any, signal = Any ):\n     except KeyError:\n         return []\n \n+\n def liveReceivers(receivers):\n     \"\"\"Filter sequence of receivers to get resolved, live receivers\n \n@@ -268,7 +280,6 @@ def liveReceivers(receivers):\n             yield receiver\n \n \n-\n def getAllReceivers(sender=Any, signal=Any):\n     \"\"\"Get list of all receivers from global tables\n \n@@ -297,6 +308,7 @@ def getAllReceivers( sender = Any, signal = Any ):\n                     # dead weakrefs raise TypeError on hash...\n                     pass\n \n+\n def send(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Send signal from sender to all connected receivers.\n \n@@ -344,6 +356,8 @@ def send(signal=Any, sender=Anonymous, *arguments, **named):\n         )\n         responses.append((receiver, response))\n     return responses\n+\n+\n def sendExact(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Send signal only to those receivers registered for exact message\n \n@@ -394,6 +408,7 @@ def _removeReceiver(receiver):\n                             pass\n                     _cleanupConnections(senderkey, signal)\n \n+\n def _cleanupConnections(senderkey, signal):\n     \"\"\"Delete any empty signals for senderkey. Delete senderkey if empty.\"\"\"\n     try:\n@@ -413,6 +428,7 @@ def _cleanupConnections(senderkey, signal):\n                     # No more signal connections. Therefore, remove the sender.\n                     _removeSender(senderkey)\n \n+\n def _removeSender(senderkey):\n     \"\"\"Remove senderkey from connections.\"\"\"\n     _removeBackrefs(senderkey)\n@@ -436,13 +452,16 @@ def _removeBackrefs( senderkey):\n         signals = None\n     else:\n         items = signals.items()\n+\n         def allReceivers():\n             for signal, set in items:\n                 for item in set:\n                     yield item\n+\n         for receiver in allReceivers():\n             _killBackref(receiver, senderkey)\n \n+\n def _removeOldBackRefs(senderkey, signal, receiver, receivers):\n     \"\"\"Kill old sendersBack references from receiver\n \n\n@@ -1,10 +1,15 @@\n \"\"\"Error types for dispatcher mechanism\n \"\"\"\n \n+\n class DispatcherError(Exception):\n     \"\"\"Base class for all Dispatcher errors\"\"\"\n+\n+\n class DispatcherKeyError(KeyError, DispatcherError):\n     \"\"\"Error raised when unknown (sender,signal) set specified\"\"\"\n+\n+\n class DispatcherTypeError(TypeError, DispatcherError):\n     \"\"\"Error raised when inappropriate signal-type specified (None)\"\"\"\n \n\n@@ -2,11 +2,8 @@\n from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers\n from scrapy.xlib.pydispatch.robustapply import robustApply\n \n-def sendRobust(\n-\tsignal=Any, \n-\tsender=Anonymous, \n-\t*arguments, **named\n-):\n+\n+def sendRobust(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Send signal from sender to all connected receivers catching errors\n \n     signal -- (hashable) signal value, see connect for details\n\n@@ -5,9 +5,9 @@ what arguments a given callable object can take,\n and subset the given arguments to match only\n those which are acceptable.\n \"\"\"\n-\n import inspect\n \n+\n def function(receiver):\n     \"\"\"Get function-like callable object for given receiver\n \n@@ -27,15 +27,18 @@ def function(receiver):\n         # an instance-method...\n         return receiver, receiver.im_func.func_code, 1\n     elif not hasattr(receiver, 'func_code'):\n-        raise ValueError('unknown receiver type %s %s'%(receiver, type(receiver)))\n+        raise ValueError(\n+            'unknown receiver type %s %s' % (receiver, type(receiver)))\n \n     return receiver, receiver.func_code, 0\n \n+\n def robustApply(receiver, *arguments, **named):\n     \"\"\"Call receiver with arguments and an appropriate subset of named\n     \"\"\"\n     receiver, codeObject, startIndex = function(receiver)\n-    acceptable = codeObject.co_varnames[startIndex+len(arguments):codeObject.co_argcount]\n+    acceptable = codeObject.co_varnames[\n+                 startIndex + len(arguments):codeObject.co_argcount]\n     for name in codeObject.co_varnames[startIndex:startIndex + len(arguments)]:\n         if name in named:\n             raise TypeError(\n\n@@ -2,6 +2,7 @@\n from __future__ import print_function\n import weakref, traceback\n \n+\n def safeRef(target, onDelete=None):\n     \"\"\"Return a *safe* weak reference to a callable target\n \n@@ -28,6 +29,7 @@ def safeRef(target, onDelete = None):\n     else:\n         return weakref.ref(target)\n \n+\n class BoundMethodWeakref(object):\n     \"\"\"'Safe' and reusable weak references to instance methods\n \n@@ -62,6 +64,7 @@ class BoundMethodWeakref(object):\n \n     \"\"\"\n     _allInstances = weakref.WeakValueDictionary()\n+\n     def __new__(cls, target, onDelete=None, *arguments, **named):\n         \"\"\"Create new instance or return current instance\n \n@@ -84,6 +87,7 @@ class BoundMethodWeakref(object):\n             cls._allInstances[key] = base\n             base.__init__(target, onDelete, *arguments, **named)\n             return base\n+\n     def __init__(self, target, onDelete=None):\n         \"\"\"Return a weak-reference-like instance for a bound method\n \n@@ -98,6 +102,7 @@ class BoundMethodWeakref(object):\n             collected).  Should take a single argument,\n             which will be passed a pointer to this object.\n         \"\"\"\n+\n         def remove(weak, self=self):\n             \"\"\"Set self.isDead to true when method or instance is destroyed\"\"\"\n             methods = self.deletionMethods[:]\n@@ -114,15 +119,18 @@ class BoundMethodWeakref(object):\n                     try:\n                         traceback.print_exc()\n                     except AttributeError as err:\n-\t\t\t\t\t\tprint('''Exception during saferef %s cleanup function %s: %s'''%(\n+                        print(\n+                            '''Exception during saferef %s cleanup function %s: %s''' % (\n                                 self, function, e\n                             ))\n+\n         self.deletionMethods = [onDelete]\n         self.key = self.calculateKey(target)\n         self.weakSelf = weakref.ref(target.im_self, remove)\n         self.weakFunc = weakref.ref(target.im_func, remove)\n         self.selfName = target.im_self.__class__.__name__\n         self.funcName = str(target.im_func.__name__)\n+\n     def calculateKey(cls, target):\n         \"\"\"Calculate the reference key for this reference\n \n@@ -130,7 +138,9 @@ class BoundMethodWeakref(object):\n         target object and the target function respectively.\n         \"\"\"\n         return (id(target.im_self), id(target.im_func))\n+\n     calculateKey = classmethod(calculateKey)\n+\n     def __str__(self):\n         \"\"\"Give a friendly representation of the object\"\"\"\n         return \"\"\"%s( %s.%s )\"\"\" % (\n@@ -138,15 +148,19 @@ class BoundMethodWeakref(object):\n             self.selfName,\n             self.funcName,\n         )\n+\n     __repr__ = __str__\n+\n     def __nonzero__(self):\n         \"\"\"Whether we are still a valid reference\"\"\"\n         return self() is not None\n+\n     def __cmp__(self, other):\n         \"\"\"Compare with another reference\"\"\"\n         if not isinstance(other, self.__class__):\n             return cmp(self.__class__, type(other))\n         return cmp(self.key, other.key)\n+\n     def __call__(self):\n         \"\"\"Return a strong reference to the bound method\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ce4ad06151273fe67c28a95ec59269439d95025", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 89 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -3,13 +3,13 @@ import inspect\n import six\n \n from scrapy import log\n-from scrapy.item import BaseItem\n from scrapy.spider import Spider\n from scrapy.utils.misc import  arg_to_iter\n \n \n def iterate_spider_output(result):\n-    return [result] if isinstance(result, BaseItem) else arg_to_iter(result)\n+    return arg_to_iter(result)\n+\n \n def iter_spider_classes(module):\n     \"\"\"Return an iterator over all spider classes defined in the given module\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6974902323f4b4cecbe663e6e0ff4c67c0557029", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 9 | Churn Cumulative: 104 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,8 +5,6 @@ for scraping from an XML feed.\n See documentation in docs/topics/spiders.rst\n \"\"\"\n from scrapy.spider import Spider\n-from scrapy.item import BaseItem\n-from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.selector import Selector\n@@ -92,6 +90,7 @@ class XMLFeedSpider(Spider):\n         for (prefix, uri) in self.namespaces:\n             selector.register_namespace(prefix, uri)\n \n+\n class CSVFeedSpider(Spider):\n     \"\"\"Spider for parsing CSV feeds.\n     It receives a CSV file in a response; iterates through each of its rows,\n@@ -125,11 +124,7 @@ class CSVFeedSpider(Spider):\n         \"\"\"\n \n         for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n-            ret = self.parse_row(response, row)\n-            if isinstance(ret, (BaseItem, Request)):\n-                ret = [ret]\n-            if not isinstance(ret, (list, tuple)):\n-                raise TypeError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n+            ret = iterate_spider_output(self.parse_row(response, row))\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da90449edfa13b5be1550b3acc212dbf3a8c6e69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 418 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class MediaPipeline(object):\n     def item_completed(self, results, item, info):\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n         if self.LOG_FAILED_RESULTS:\n-            msg = '%s found errors proessing %s' % (self.__class__.__name__, item)\n+            msg = '%s found errors processing %s' % (self.__class__.__name__, item)\n             for ok, value in results:\n                 if not ok:\n                     log.err(value, msg, spider=info.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39635e5f55bfbbc88d051778db6ba6f391630619", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 268 | Lines Deleted: 130 | Files Changed: 13 | Hunks: 86 | Methods Changed: 47 | Complexity Δ (Sum/Max): 111/63 | Churn Δ: 398 | Churn Cumulative: 4084 | Contributors (this commit): 26 | Commits (past 90d): 15 | Contributors (cumulative): 63 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,7 +107,7 @@ class Command(ScrapyCommand):\n         items, requests = [], []\n \n         for x in iterate_spider_output(cb(response)):\n-            if isinstance(x, BaseItem):\n+            if isinstance(x, (BaseItem, dict)):\n                 items.append(x)\n             elif isinstance(x, Request):\n                 requests.append(x)\n\n@@ -35,8 +35,8 @@ class ReturnsContract(Contract):\n     objects = {\n         'request': Request,\n         'requests': Request,\n-        'item': BaseItem,\n-        'items': BaseItem,\n+        'item': (BaseItem, dict),\n+        'items': (BaseItem, dict),\n     }\n \n     def __init__(self, *args, **kwargs):\n@@ -83,7 +83,7 @@ class ScrapesContract(Contract):\n \n     def post_process(self, output):\n         for x in output:\n-            if isinstance(x, BaseItem):\n+            if isinstance(x, (BaseItem, dict)):\n                 for arg in self.args:\n                     if not arg in x:\n                         raise ContractFail(\"'%s' field is missing\" % arg)\n\n@@ -9,6 +9,7 @@ import marshal\n import six\n from six.moves import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n+\n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.item import BaseItem\n \n@@ -50,13 +51,13 @@ class BaseItemExporter(object):\n         return value.encode(self.encoding) if isinstance(value, unicode) else value\n \n     def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n-        \"\"\"Return the fields to export as an iterable of tuples (name,\n-        serialized_value)\n+        \"\"\"Return the fields to export as an iterable of tuples\n+        (name, serialized_value)\n         \"\"\"\n         if include_empty is None:\n             include_empty = self.export_empty_fields\n         if self.fields_to_export is None:\n-            if include_empty:\n+            if include_empty and not isinstance(item, dict):\n                 field_iter = six.iterkeys(item.fields)\n             else:\n                 field_iter = six.iterkeys(item)\n@@ -64,12 +65,11 @@ class BaseItemExporter(object):\n             if include_empty:\n                 field_iter = self.fields_to_export\n             else:\n-                nonempty_fields = set(item.keys())\n-                field_iter = (x for x in self.fields_to_export if x in\n-                              nonempty_fields)\n+                field_iter = (x for x in self.fields_to_export if x in item)\n+\n         for field_name in field_iter:\n             if field_name in item:\n-                field = item.fields[field_name]\n+                field = {} if isinstance(item, dict) else item.fields[field_name]\n                 value = self.serialize_field(field, field_name, item[field_name])\n             else:\n                 value = default_value\n@@ -191,7 +191,12 @@ class CsvItemExporter(BaseItemExporter):\n     def _write_headers_and_set_fields_to_export(self, item):\n         if self.include_headers_line:\n             if not self.fields_to_export:\n-                self.fields_to_export = item.fields.keys()\n+                if isinstance(item, dict):\n+                    # for dicts try using fields of the first item\n+                    self.fields_to_export = list(item.keys())\n+                else:\n+                    # use fields declared in Item\n+                    self.fields_to_export = list(item.fields.keys())\n             self.csv_writer.writerow(self.fields_to_export)\n \n \n\n@@ -267,7 +267,7 @@ class FilesPipeline(MediaPipeline):\n         return checksum\n \n     def item_completed(self, results, item, info):\n-        if self.FILES_RESULT_FIELD in item.fields:\n+        if isinstance(item, dict) or self.FILES_RESULT_FIELD in item.fields:\n             item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]\n         return item\n \n\n@@ -109,7 +109,7 @@ class ImagesPipeline(FilesPipeline):\n         return [Request(x) for x in item.get(self.IMAGES_URLS_FIELD, [])]\n \n     def item_completed(self, results, item, info):\n-        if self.IMAGES_RESULT_FIELD in item.fields:\n+        if isinstance(item, dict) or self.IMAGES_RESULT_FIELD in item.fields:\n             item[self.IMAGES_RESULT_FIELD] = [x for ok, x in results if ok]\n         return item\n \n\n@@ -174,7 +174,7 @@ class Scraper(object):\n         \"\"\"\n         if isinstance(output, Request):\n             self.crawler.engine.crawl(request=output, spider=spider)\n-        elif isinstance(output, BaseItem):\n+        elif isinstance(output, (BaseItem, dict)):\n             self.slot.itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n             dfd.addBoth(self._itemproc_finished, output, response, spider)\n@@ -183,7 +183,7 @@ class Scraper(object):\n             pass\n         else:\n             typename = type(output).__name__\n-            log.msg(format='Spider must return Request, BaseItem or None, '\n+            log.msg(format='Spider must return Request, BaseItem, dict or None, '\n                            'got %(typename)r in %(request)s',\n                     level=log.ERROR, spider=spider, request=request, typename=typename)\n \n\n@@ -85,6 +85,7 @@ class ItemSpider(FollowAllSpider):\n         for request in super(ItemSpider, self).parse(response):\n             yield request\n             yield Item()\n+            yield {}\n \n \n class DefaultError(Exception):\n\n@@ -127,6 +127,7 @@ class MiscCommandsTest(CommandTest):\n     def test_list(self):\n         self.assertEqual(0, self.call('list'))\n \n+\n class RunSpiderCommandTest(CommandTest):\n \n     def test_runspider(self):\n@@ -135,10 +136,10 @@ class RunSpiderCommandTest(CommandTest):\n         fname = abspath(join(tmpdir, 'myspider.py'))\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n+import scrapy\n from scrapy import log\n-from scrapy.spider import Spider\n \n-class MySpider(Spider):\n+class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n     def start_requests(self):\n@@ -192,16 +193,15 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n from scrapy import log\n-from scrapy.spider import Spider\n-from scrapy.item import Item\n+import scrapy\n \n-class MySpider(Spider):\n+class MySpider(scrapy.Spider):\n     name = '{0}'\n \n     def parse(self, response):\n         if getattr(self, 'test_arg', None):\n             self.log('It Works!')\n-        return [Item()]\n+        return [scrapy.Item(), dict(foo='bar')]\n \"\"\".format(self.spider_name))\n \n         fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n@@ -239,6 +239,14 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            self.url('/html')])\n         self.assert_(\"[scrapy] INFO: It Works!\" in stderr, stderr)\n \n+    @defer.inlineCallbacks\n+    def test_parse_items(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", out)\n+\n+\n \n class BenchCommandTest(CommandTest):\n \n\n@@ -39,6 +39,13 @@ class TestSpider(Spider):\n         \"\"\"\n         return TestItem(url=response.url)\n \n+    def returns_dict_item(self, response):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        \"\"\"\n+        return {\"url\": response.url}\n+\n     def returns_fail(self, response):\n         \"\"\" method which returns item\n         @url http://scrapy.org\n@@ -46,6 +53,13 @@ class TestSpider(Spider):\n         \"\"\"\n         return TestItem(url=response.url)\n \n+    def returns_dict_fail(self, response):\n+        \"\"\" method which returns item\n+        @url http://scrapy.org\n+        @returns items 0 0\n+        \"\"\"\n+        return {'url': response.url}\n+\n     def scrapes_item_ok(self, response):\n         \"\"\" returns item with name and url\n         @url http://scrapy.org\n@@ -54,6 +68,14 @@ class TestSpider(Spider):\n         \"\"\"\n         return TestItem(name='test', url=response.url)\n \n+    def scrapes_dict_item_ok(self, response):\n+        \"\"\" returns item with name and url\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        @scrapes name url\n+        \"\"\"\n+        return {'name': 'test', 'url': response.url}\n+\n     def scrapes_item_fail(self, response):\n         \"\"\" returns item with no name\n         @url http://scrapy.org\n@@ -62,6 +84,14 @@ class TestSpider(Spider):\n         \"\"\"\n         return TestItem(url=response.url)\n \n+    def scrapes_dict_item_fail(self, response):\n+        \"\"\" returns item with no name\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        @scrapes name url\n+        \"\"\"\n+        return {'url': response.url}\n+\n     def parse_no_url(self, response):\n         \"\"\" method with no url\n         @returns items 1 1\n@@ -110,6 +140,11 @@ class ContractsManagerTest(unittest.TestCase):\n         request.callback(response)\n         self.should_succeed()\n \n+        # returns_dict_item\n+        request = self.conman.from_method(spider.returns_dict_item, self.results)\n+        request.callback(response)\n+        self.should_succeed()\n+\n         # returns_request\n         request = self.conman.from_method(spider.returns_request, self.results)\n         request.callback(response)\n@@ -120,6 +155,11 @@ class ContractsManagerTest(unittest.TestCase):\n         request.callback(response)\n         self.should_fail()\n \n+        # returns_dict_fail\n+        request = self.conman.from_method(spider.returns_dict_fail, self.results)\n+        request.callback(response)\n+        self.should_fail()\n+\n     def test_scrapes(self):\n         spider = TestSpider()\n         response = ResponseMock()\n@@ -129,8 +169,19 @@ class ContractsManagerTest(unittest.TestCase):\n         request.callback(response)\n         self.should_succeed()\n \n+        # scrapes_dict_item_ok\n+        request = self.conman.from_method(spider.scrapes_dict_item_ok, self.results)\n+        request.callback(response)\n+        self.should_succeed()\n+\n         # scrapes_item_fail\n         request = self.conman.from_method(spider.scrapes_item_fail,\n                 self.results)\n         request.callback(response)\n         self.should_fail()\n+\n+        # scrapes_dict_item_fail\n+        request = self.conman.from_method(spider.scrapes_dict_item_fail,\n+                self.results)\n+        request.callback(response)\n+        self.should_fail()\n\n@@ -1,14 +1,19 @@\n-import unittest, json\n+from __future__ import absolute_import\n+import re\n+import json\n+import unittest\n from io import BytesIO\n from six.moves import cPickle as pickle\n+\n import lxml.etree\n-import re\n \n from scrapy.item import Item, Field\n from scrapy.utils.python import str_to_unicode\n-from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n-    PickleItemExporter, CsvItemExporter, XmlItemExporter, JsonLinesItemExporter, \\\n-    JsonItemExporter, PythonItemExporter\n+from scrapy.contrib.exporter import (\n+    BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,\n+    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter, PythonItemExporter\n+)\n+\n \n class TestItem(Item):\n     name = Field()\n@@ -33,21 +38,28 @@ class BaseItemExporterTest(unittest.TestCase):\n             exported_dict[k] = str_to_unicode(v)\n         self.assertEqual(self.i, exported_dict)\n \n-    def test_export_item(self):\n+    def assertItemExportWorks(self, item):\n         self.ie.start_exporting()\n         try:\n-            self.ie.export_item(self.i)\n+            self.ie.export_item(item)\n         except NotImplementedError:\n             if self.ie.__class__ is not BaseItemExporter:\n                 raise\n         self.ie.finish_exporting()\n         self._check_output()\n \n+    def test_export_item(self):\n+        self.assertItemExportWorks(self.i)\n+\n+    def test_export_dict_item(self):\n+        self.assertItemExportWorks(dict(self.i))\n+\n     def test_serialize_field(self):\n-        self.assertEqual(self.ie.serialize_field( \\\n-            self.i.fields['name'], 'name', self.i['name']), 'John\\xc2\\xa3')\n-        self.assertEqual( \\\n-            self.ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '22')\n+        res = self.ie.serialize_field(self.i.fields['name'], 'name', self.i['name'])\n+        self.assertEqual(res, 'John\\xc2\\xa3')\n+\n+        res = self.ie.serialize_field(self.i.fields['age'], 'age', self.i['age'])\n+        self.assertEqual(res, '22')\n \n     def test_fields_to_export(self):\n         ie = self._get_exporter(fields_to_export=['name'])\n@@ -72,13 +84,14 @@ class BaseItemExporterTest(unittest.TestCase):\n         self.assertEqual(ie.serialize_field(i.fields['name'], 'name', i['name']), 'John\\xc2\\xa3')\n         self.assertEqual(ie.serialize_field(i.fields['age'], 'age', i['age']), '24')\n \n+\n class PythonItemExporterTest(BaseItemExporterTest):\n     def _get_exporter(self, **kwargs):\n         return PythonItemExporter(**kwargs)\n \n     def test_nested_item(self):\n         i1 = TestItem(name=u'Joseph', age='22')\n-        i2 = TestItem(name=u'Maria', age=i1)\n+        i2 = dict(name=u'Maria', age=i1)\n         i3 = TestItem(name=u'Jesus', age=i2)\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -107,6 +120,7 @@ class PythonItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(type(exported['age'][0]), dict)\n         self.assertEqual(type(exported['age'][0]['age'][0]), dict)\n \n+\n class PprintItemExporterTest(BaseItemExporterTest):\n \n     def _get_exporter(self, **kwargs):\n@@ -115,6 +129,7 @@ class PprintItemExporterTest(BaseItemExporterTest):\n     def _check_output(self):\n         self._assert_expected_item(eval(self.output.getvalue()))\n \n+\n class PickleItemExporterTest(BaseItemExporterTest):\n \n     def _get_exporter(self, **kwargs):\n@@ -150,48 +165,65 @@ class CsvItemExporterTest(BaseItemExporterTest):\n     def _check_output(self):\n         self.assertCsvEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n-    def test_header(self):\n-        output = BytesIO()\n-        ie = CsvItemExporter(output, fields_to_export=self.i.fields.keys())\n+    def assertExportResult(self, item, expected, **kwargs):\n+        fp = BytesIO()\n+        ie = CsvItemExporter(fp, **kwargs)\n         ie.start_exporting()\n-        ie.export_item(self.i)\n+        ie.export_item(item)\n         ie.finish_exporting()\n-        self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n+        self.assertCsvEqual(fp.getvalue(), expected)\n \n-        output = BytesIO()\n-        ie = CsvItemExporter(output, fields_to_export=['age'])\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n-        self.assertCsvEqual(output.getvalue(), 'age\\r\\n22\\r\\n')\n+    def test_header_export_all(self):\n+        self.assertExportResult(\n+            item=self.i,\n+            fields_to_export=self.i.fields.keys(),\n+            expected='age,name\\r\\n22,John\\xc2\\xa3\\r\\n',\n+        )\n \n+    def test_header_export_all_dict(self):\n+        self.assertExportResult(\n+            item=dict(self.i),\n+            expected='age,name\\r\\n22,John\\xc2\\xa3\\r\\n',\n+        )\n+\n+    def test_header_export_single_field(self):\n+        for item in [self.i, dict(self.i)]:\n+            self.assertExportResult(\n+                item=item,\n+                fields_to_export=['age'],\n+                expected='age\\r\\n22\\r\\n',\n+            )\n+\n+    def test_header_export_two_items(self):\n+        for item in [self.i, dict(self.i)]:\n             output = BytesIO()\n             ie = CsvItemExporter(output)\n             ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.export_item(self.i)\n+            ie.export_item(item)\n+            ie.export_item(item)\n             ie.finish_exporting()\n             self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n-        output = BytesIO()\n-        ie = CsvItemExporter(output, include_headers_line=False)\n-        ie.start_exporting()\n-        ie.export_item(self.i)\n-        ie.finish_exporting()\n-        self.assertCsvEqual(output.getvalue(), '22,John\\xc2\\xa3\\r\\n')\n+    def test_header_no_header_line(self):\n+        for item in [self.i, dict(self.i)]:\n+            self.assertExportResult(\n+                item=item,\n+                include_headers_line=False,\n+                expected='22,John\\xc2\\xa3\\r\\n',\n+            )\n \n     def test_join_multivalue(self):\n         class TestItem2(Item):\n             name = Field()\n             friends = Field()\n \n-        i = TestItem2(name='John', friends=['Mary', 'Paul'])\n-        output = BytesIO()\n-        ie = CsvItemExporter(output, include_headers_line=False)\n-        ie.start_exporting()\n-        ie.export_item(i)\n-        ie.finish_exporting()\n-        self.assertCsvEqual(output.getvalue(), '\"Mary,Paul\",John\\r\\n')\n+        for cls in TestItem2, dict:\n+            self.assertExportResult(\n+                item=cls(name='John', friends=['Mary', 'Paul']),\n+                include_headers_line=False,\n+                expected='\"Mary,Paul\",John\\r\\n',\n+            )\n+\n \n class XmlItemExporterTest(BaseItemExporterTest):\n \n@@ -211,60 +243,62 @@ class XmlItemExporterTest(BaseItemExporterTest):\n             return xmltuple(doc)\n         return self.assertEqual(xmlsplit(first), xmlsplit(second), msg)\n \n+    def assertExportResult(self, item, expected_value):\n+        fp = BytesIO()\n+        ie = XmlItemExporter(fp)\n+        ie.start_exporting()\n+        ie.export_item(item)\n+        ie.finish_exporting()\n+        self.assertXmlEquivalent(fp.getvalue(), expected_value)\n+\n     def _check_output(self):\n         expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><age>22</age><name>John\\xc2\\xa3</name></item></items>'\n         self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n \n     def test_multivalued_fields(self):\n-        output = BytesIO()\n-        item = TestItem(name=[u'John\\xa3', u'Doe'])\n-        ie = XmlItemExporter(output)\n-        ie.start_exporting()\n-        ie.export_item(item)\n-        ie.finish_exporting()\n-        expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><name><value>John\\xc2\\xa3</value><value>Doe</value></name></item></items>'\n-        self.assertXmlEquivalent(output.getvalue(), expected_value)\n+        self.assertExportResult(\n+            TestItem(name=[u'John\\xa3', u'Doe']),\n+            '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><name><value>John\\xc2\\xa3</value><value>Doe</value></name></item></items>'\n+        )\n \n     def test_nested_item(self):\n-        output = BytesIO()\n         i1 = TestItem(name=u'foo\\xa3hoo', age='22')\n-        i2 = TestItem(name=u'bar', age=i1)\n+        i2 = dict(name=u'bar', age=i1)\n         i3 = TestItem(name=u'buz', age=i2)\n-        ie = XmlItemExporter(output)\n-        ie.start_exporting()\n-        ie.export_item(i3)\n-        ie.finish_exporting()\n-        expected_value = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\\\n-                '<items><item>'\\\n-                    '<age>'\\\n-                        '<age>'\\\n-                            '<age>22</age>'\\\n-                            '<name>foo\\xc2\\xa3hoo</name>'\\\n-                        '</age>'\\\n-                        '<name>bar</name>'\\\n-                    '</age>'\\\n-                    '<name>buz</name>'\\\n-                '</item></items>'\n-        self.assertXmlEquivalent(output.getvalue(), expected_value)\n+\n+        self.assertExportResult(i3,\n+            '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+            '<items>'\n+                '<item>'\n+                    '<age>'\n+                        '<age>'\n+                            '<age>22</age>'\n+                            '<name>foo\\xc2\\xa3hoo</name>'\n+                        '</age>'\n+                        '<name>bar</name>'\n+                    '</age>'\n+                    '<name>buz</name>'\n+                '</item>'\n+            '</items>'\n+        )\n \n     def test_nested_list_item(self):\n-        output = BytesIO()\n         i1 = TestItem(name=u'foo')\n-        i2 = TestItem(name=u'bar')\n+        i2 = dict(name=u'bar', v2={\"egg\": [\"spam\"]})\n         i3 = TestItem(name=u'buz', age=[i1, i2])\n-        ie = XmlItemExporter(output)\n-        ie.start_exporting()\n-        ie.export_item(i3)\n-        ie.finish_exporting()\n-        expected_value =  '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\\\n-                '<items><item>'\\\n-                    '<age>'\\\n-                        '<value><name>foo</name></value>'\\\n-                        '<value><name>bar</name></value>'\\\n-                    '</age>'\\\n-                    '<name>buz</name>'\\\n-                '</item></items>'\n-        self.assertXmlEquivalent(output.getvalue(), expected_value)\n+\n+        self.assertExportResult(i3,\n+            '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+            '<items>'\n+                '<item>'\n+                    '<age>'\n+                        '<value><name>foo</name></value>'\n+                        '<value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>'\n+                    '</age>'\n+                    '<name>buz</name>'\n+                '</item>'\n+            '</items>'\n+        )\n \n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n@@ -280,7 +314,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n     def test_nested_item(self):\n         i1 = TestItem(name=u'Joseph', age='22')\n-        i2 = TestItem(name=u'Maria', age=i1)\n+        i2 = dict(name=u'Maria', age=i1)\n         i3 = TestItem(name=u'Jesus', age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n@@ -306,13 +340,19 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         exported = json.loads(self.output.getvalue().strip())\n         self.assertEqual(exported, [dict(self.i)])\n \n-    def test_two_items(self):\n+    def assertTwoItemsExported(self, item):\n         self.ie.start_exporting()\n-        self.ie.export_item(self.i)\n-        self.ie.export_item(self.i)\n+        self.ie.export_item(item)\n+        self.ie.export_item(item)\n         self.ie.finish_exporting()\n         exported = json.loads(self.output.getvalue())\n-        self.assertEqual(exported, [dict(self.i), dict(self.i)])\n+        self.assertEqual(exported, [dict(item), dict(item)])\n+\n+    def test_two_items(self):\n+        self.assertTwoItemsExported(self.i)\n+\n+    def test_two_dict_items(self):\n+        self.assertTwoItemsExported(dict(self.i))\n \n     def test_nested_item(self):\n         i1 = TestItem(name=u'Joseph\\xa3', age='22')\n@@ -325,6 +365,18 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         expected = {'name': u'Jesus', 'age': {'name': 'Maria', 'age': dict(i1)}}\n         self.assertEqual(exported, [expected])\n \n+    def test_nested_dict_item(self):\n+        i1 = dict(name=u'Joseph\\xa3', age='22')\n+        i2 = TestItem(name=u'Maria', age=i1)\n+        i3 = dict(name=u'Jesus', age=i2)\n+        self.ie.start_exporting()\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(self.output.getvalue())\n+        expected = {'name': u'Jesus', 'age': {'name': 'Maria', 'age': i1}}\n+        self.assertEqual(exported, [expected])\n+\n+\n class CustomItemExporterTest(unittest.TestCase):\n \n     def test_exporter_custom_serializer(self):\n@@ -333,16 +385,17 @@ class CustomItemExporterTest(unittest.TestCase):\n                 if name == 'age':\n                     return str(int(value) + 1)\n                 else:\n-                    return super(CustomItemExporter, self).serialize_field(field, \\\n-                        name, value)\n+                    return super(CustomItemExporter, self).serialize_field(field, name, value)\n \n         i = TestItem(name=u'John', age='22')\n         ie = CustomItemExporter()\n \n-        self.assertEqual( \\\n-            ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')\n-        self.assertEqual(\n-            ie.serialize_field(i.fields['age'], 'age', i['age']), '23')\n+        self.assertEqual(ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')\n+        self.assertEqual(ie.serialize_field(i.fields['age'], 'age', i['age']), '23')\n+\n+        i2 = {'name': u'John', 'age': '22'}\n+        self.assertEqual(ie.serialize_field({}, 'name', i2['name']), 'John')\n+        self.assertEqual(ie.serialize_field({}, 'age', i2['age']), '23')\n \n \n if __name__ == '__main__':\n\n@@ -28,11 +28,13 @@ from scrapy.contrib.linkextractors import LinkExtractor\n from scrapy.http import Request\n from scrapy.utils.signal import disconnect_all\n \n+\n class TestItem(Item):\n     name = Field()\n     url = Field()\n     price = Field()\n \n+\n class TestSpider(Spider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n@@ -41,6 +43,8 @@ class TestSpider(Spider):\n     name_re = re.compile(\"<h1>(.*?)</h1>\", re.M)\n     price_re = re.compile(\">Price: \\$(.*?)<\", re.M)\n \n+    item_cls = TestItem\n+\n     def parse(self, response):\n         xlink = LinkExtractor()\n         itemre = re.compile(self.itemurl_re)\n@@ -49,7 +53,7 @@ class TestSpider(Spider):\n                 yield Request(url=link.url, callback=self.parse_item)\n \n     def parse_item(self, response):\n-        item = TestItem()\n+        item = self.item_cls()\n         m = self.name_re.search(response.body)\n         if m:\n             item['name'] = m.group(1)\n@@ -65,6 +69,10 @@ class TestDupeFilterSpider(TestSpider):\n         return Request(url)  # dont_filter=False\n \n \n+class DictItemsSpider(TestSpider):\n+    item_cls = dict\n+\n+\n def start_test_site(debug=False):\n     root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n@@ -81,15 +89,14 @@ def start_test_site(debug=False):\n class CrawlerRun(object):\n     \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n \n-    def __init__(self, with_dupefilter=False):\n+    def __init__(self, spider_class):\n         self.spider = None\n         self.respplug = []\n         self.reqplug = []\n         self.reqdropped = []\n         self.itemresp = []\n         self.signals_catched = {}\n-        self.spider_class = TestSpider if not with_dupefilter else \\\n-            TestDupeFilterSpider\n+        self.spider_class = spider_class\n \n     def run(self):\n         self.port = start_test_site()\n@@ -152,14 +159,17 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawler(self):\n-        self.run = CrawlerRun()\n+\n+        for spider in TestSpider, DictItemsSpider:\n+            self.run = CrawlerRun(spider)\n             yield self.run.run()\n             self._assert_visited_urls()\n             self._assert_scheduled_requests(urls_to_visit=8)\n             self._assert_downloaded_responses()\n             self._assert_scraped_items()\n             self._assert_signals_catched()\n-        self.run = CrawlerRun(with_dupefilter=True)\n+\n+        self.run = CrawlerRun(TestDupeFilterSpider)\n         yield self.run.run()\n         self._assert_scheduled_requests(urls_to_visit=7)\n         self._assert_dropped_requests()\n\n@@ -142,13 +142,14 @@ class DeprecatedFilesPipelineTestCase(unittest.TestCase):\n class FilesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n-        from scrapy.contrib.pipeline.files import FilesPipeline\n         class TestItem(Item):\n             name = Field()\n             file_urls = Field()\n             files = Field()\n+\n+        for cls in TestItem, dict:\n             url = 'http://www.example.com/files/1.txt'\n-        item = TestItem({'name': 'item1', 'file_urls': [url]})\n+            item = cls({'name': 'item1', 'file_urls': [url]})\n             pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/'}))\n             requests = list(pipeline.get_media_requests(item, None))\n             self.assertEqual(requests[0].url, url)\n@@ -157,15 +158,19 @@ class FilesPipelineTestCaseFields(unittest.TestCase):\n             self.assertEqual(item['files'], [results[0][1]])\n \n     def test_item_fields_override_settings(self):\n-        from scrapy.contrib.pipeline.files import FilesPipeline\n         class TestItem(Item):\n             name = Field()\n             files = Field()\n             stored_file = Field()\n+\n+        for cls in TestItem, dict:\n             url = 'http://www.example.com/files/1.txt'\n-        item = TestItem({'name': 'item1', 'files': [url]})\n-        pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/',\n-                'FILES_URLS_FIELD': 'files', 'FILES_RESULT_FIELD': 'stored_file'}))\n+            item = cls({'name': 'item1', 'files': [url]})\n+            pipeline = FilesPipeline.from_settings(Settings({\n+                'FILES_STORE': 's3://example/files/',\n+                'FILES_URLS_FIELD': 'files',\n+                'FILES_RESULT_FIELD': 'stored_file'\n+            }))\n             requests = list(pipeline.get_media_requests(item, None))\n             self.assertEqual(requests[0].url, url)\n             results = [(True, {'url': url})]\n\n@@ -168,13 +168,14 @@ class DeprecatedImagesPipelineTestCase(unittest.TestCase):\n class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n-        from scrapy.contrib.pipeline.images import ImagesPipeline\n         class TestItem(Item):\n             name = Field()\n             image_urls = Field()\n             images = Field()\n+\n+        for cls in TestItem, dict:\n             url = 'http://www.example.com/images/1.jpg'\n-        item = TestItem({'name': 'item1', 'image_urls': [url]})\n+            item = cls({'name': 'item1', 'image_urls': [url]})\n             pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/'}))\n             requests = list(pipeline.get_media_requests(item, None))\n             self.assertEqual(requests[0].url, url)\n@@ -183,15 +184,19 @@ class ImagesPipelineTestCaseFields(unittest.TestCase):\n             self.assertEqual(item['images'], [results[0][1]])\n \n     def test_item_fields_override_settings(self):\n-        from scrapy.contrib.pipeline.images import ImagesPipeline\n         class TestItem(Item):\n             name = Field()\n             image = Field()\n             stored_image = Field()\n+\n+        for cls in TestItem, dict:\n             url = 'http://www.example.com/images/1.jpg'\n-        item = TestItem({'name': 'item1', 'image': [url]})\n-        pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/',\n-                'IMAGES_URLS_FIELD': 'image', 'IMAGES_RESULT_FIELD': 'stored_image'}))\n+            item = cls({'name': 'item1', 'image': [url]})\n+            pipeline = ImagesPipeline.from_settings(Settings({\n+                'IMAGES_STORE': 's3://example/images/',\n+                'IMAGES_URLS_FIELD': 'image',\n+                'IMAGES_RESULT_FIELD': 'stored_image'\n+            }))\n             requests = list(pipeline.get_media_requests(item, None))\n             self.assertEqual(requests[0].url, url)\n             results = [(True, {'url': url})]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
