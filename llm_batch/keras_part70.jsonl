{"custom_id": "keras#9720db95669abdca9f57438be708b3edf37c5485", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7880 | Contributors (this commit): 37 | Commits (past 90d): 88 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -1183,7 +1183,7 @@ class AutoEncoder(Layer):\n     autoencoder.fit(X_test, representations, nb_epoch=1)  # in this case the loss will be 0, so it's useless\n \n     # to keep training against the original inputs, just switch back output_reconstruction to True:\n-    autoencoder.output_reconstruction = False\n+    autoencoder.output_reconstruction = True\n     autoencoder.compile(optimizer='sgd', loss='mse')\n     autoencoder.fit(X_train, X_train, nb_epoch=10)\n     ```\n\n@@ -139,7 +139,7 @@ def test_autoencoder_advanced():\n     autoencoder.fit(X_test, representations, nb_epoch=1, batch_size=32)\n \n     # to keep training against the original inputs, just switch back output_reconstruction to True:\n-    autoencoder.output_reconstruction = False\n+    autoencoder.output_reconstruction = True\n     autoencoder.compile(optimizer='sgd', loss='mse')\n     autoencoder.fit(X_train, X_train, nb_epoch=1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d85402ca548957030685c451c96b05342454906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 52 | Churn Cumulative: 7932 | Contributors (this commit): 37 | Commits (past 90d): 90 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1166,26 +1166,26 @@ class AutoEncoder(Layer):\n     encoder = containers.Sequential([Dense(16, input_dim=32), Dense(8)])\n     decoder = containers.Sequential([Dense(16, input_dim=8), Dense(32)])\n \n-    autoencoder = Sequential()\n-    autoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder,\n-                                output_reconstruction=True))\n+    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=True)\n+    model = Sequential()\n+    model.add(autoencoder)\n \n     # training the autoencoder:\n-    autoencoder.compile(optimizer='sgd', loss='mse')\n-    autoencoder.fit(X_train, X_train, nb_epoch=10)\n+    model.compile(optimizer='sgd', loss='mse')\n+    model.fit(X_train, X_train, nb_epoch=10)\n \n     # predicting compressed representations of inputs:\n-    autoencoder.output_reconstruction = False  # the autoencoder has to be recompiled after modifying this property\n-    autoencoder.compile(optimizer='sgd', loss='mse')\n-    representations = autoencoder.predict(X_test)\n+    autoencoder.output_reconstruction = False  # the model has to be recompiled after modifying this property\n+    model.compile(optimizer='sgd', loss='mse')\n+    representations = model.predict(X_test)\n \n     # the model is still trainable, although it now expects compressed representations as targets:\n-    autoencoder.fit(X_test, representations, nb_epoch=1)  # in this case the loss will be 0, so it's useless\n+    model.fit(X_test, representations, nb_epoch=1)  # in this case the loss will be 0, so it's useless\n \n     # to keep training against the original inputs, just switch back output_reconstruction to True:\n     autoencoder.output_reconstruction = True\n-    autoencoder.compile(optimizer='sgd', loss='mse')\n-    autoencoder.fit(X_train, X_train, nb_epoch=10)\n+    model.compile(optimizer='sgd', loss='mse')\n+    model.fit(X_train, X_train, nb_epoch=10)\n     ```\n     '''\n     def __init__(self, encoder, decoder, output_reconstruction=True,\n\n@@ -121,27 +121,35 @@ def test_autoencoder_advanced():\n     X_train = np.random.random((100, 10))\n     X_test = np.random.random((100, 10))\n \n-    autoencoder = Sequential()\n-    autoencoder.add(core.Dense(output_dim=10, input_dim=10))\n-    autoencoder.add(core.AutoEncoder(encoder=encoder, decoder=decoder,\n-                                     output_reconstruction=True))\n+    model = Sequential()\n+    model.add(core.Dense(output_dim=10, input_dim=10))\n+    autoencoder = core.AutoEncoder(encoder=encoder, decoder=decoder,\n+                                   output_reconstruction=True)\n+    model.add(autoencoder)\n \n     # training the autoencoder:\n-    autoencoder.compile(optimizer='sgd', loss='mse')\n-    autoencoder.fit(X_train, X_train, nb_epoch=1, batch_size=32)\n+    model.compile(optimizer='sgd', loss='mse')\n+    assert autoencoder.output_reconstruction\n+\n+    model.fit(X_train, X_train, nb_epoch=1, batch_size=32)\n \n     # predicting compressed representations of inputs:\n     autoencoder.output_reconstruction = False  # the autoencoder has to be recompiled after modifying this property\n-    autoencoder.compile(optimizer='sgd', loss='mse')\n-    representations = autoencoder.predict(X_test)\n+    assert not autoencoder.output_reconstruction\n+    model.compile(optimizer='sgd', loss='mse')\n+    representations = model.predict(X_test)\n+    assert representations.shape == (100, 5)\n \n     # the model is still trainable, although it now expects compressed representations as targets:\n-    autoencoder.fit(X_test, representations, nb_epoch=1, batch_size=32)\n+    model.fit(X_test, representations, nb_epoch=1, batch_size=32)\n \n     # to keep training against the original inputs, just switch back output_reconstruction to True:\n     autoencoder.output_reconstruction = True\n-    autoencoder.compile(optimizer='sgd', loss='mse')\n-    autoencoder.fit(X_train, X_train, nb_epoch=1)\n+    model.compile(optimizer='sgd', loss='mse')\n+    model.fit(X_train, X_train, nb_epoch=1)\n+\n+    reconstructions = model.predict(X_test)\n+    assert reconstructions.shape == (100, 10)\n \n \n def test_maxout_dense():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e33b516ef0d6e4f3c698676874edb8e4ebf7356", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 75 | Churn Cumulative: 239 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from os.path import isfile, join\n import random\n import math\n from six.moves import range\n+import threading\n \n '''Fairly basic set of tools for realtime data augmentation on image data.\n Can easily be extended to include new transformations, new preprocessing methods, etc...\n@@ -144,11 +145,9 @@ class ImageDataGenerator(object):\n         self.mean = None\n         self.std = None\n         self.principal_components = None\n+        self.lock = threading.Lock()\n \n-    def flow(self, X, y, batch_size=32, shuffle=False, seed=None,\n-             save_to_dir=None, save_prefix=\"\", save_format=\"jpeg\"):\n-        assert len(X) == len(y)\n-\n+    def _flow_index(self, N, batch_size=32, shuffle=False, seed=None):\n         b = 0\n         total_b = 0\n         while 1:\n@@ -157,31 +156,61 @@ class ImageDataGenerator(object):\n                     np.random.seed(seed + total_b)\n \n                 if shuffle:\n-                    index_array = np.random.permutation(X.shape[0])\n+                    index_array = np.random.permutation(N)\n                 else:\n-                    index_array = np.arange(X.shape[0])\n+                    index_array = np.arange(N)\n \n-            current_index = (b * batch_size) % X.shape[0]\n-            if X.shape[0] >= current_index + batch_size:\n+            current_index = (b * batch_size) % N\n+            if N >= current_index + batch_size:\n                 current_batch_size = batch_size\n             else:\n-                current_batch_size = X.shape[0] - current_index\n-            bX = np.zeros(tuple([current_batch_size] + list(X.shape)[1:]))\n-            for i in range(current_batch_size):\n-                x = X[index_array[current_index + i]]\n-                x = self.random_transform(x.astype(\"float32\"))\n-                x = self.standardize(x)\n-                bX[i] = x\n-            if save_to_dir:\n-                for i in range(current_batch_size):\n-                    img = array_to_img(bX[i], scale=True)\n-                    img.save(save_to_dir + \"/\" + save_prefix + \"_\" + str(current_index + i) + \".\" + save_format)\n+                current_batch_size = N - current_index\n+\n             if current_batch_size == batch_size:\n                 b += 1\n             else:\n                 b = 0\n             total_b += 1\n-            yield bX, y[index_array[current_index: current_index + batch_size]]\n+            yield index_array[current_index: current_index + current_batch_size], current_index, current_batch_size\n+\n+    def flow(self, X, y, batch_size=32, shuffle=False, seed=None,\n+             save_to_dir=None, save_prefix=\"\", save_format=\"jpeg\"):\n+        assert len(X) == len(y)\n+        self.X = X\n+        self.y = y\n+        self.save_to_dir = save_to_dir\n+        self.save_prefix = save_prefix\n+        self.save_format = save_format\n+        self.flow_generator = self._flow_index(X.shape[0], batch_size, shuffle, seed)\n+        return self\n+\n+    def __iter__(self):\n+        # needed if we want to do something like for x,y in data_gen.flow(...):\n+        return self\n+\n+    def next(self):\n+        # for python 2.x\n+        # Keep under lock only the mechainsem which advance the indexing of each batch\n+        # see # http://anandology.com/blog/using-iterators-and-generators/\n+        with self.lock:\n+            index_array, current_index, current_batch_size = next(self.flow_generator)\n+        # The transformation of images is not under thread lock so it can be done in parallel\n+        bX = np.zeros(tuple([current_batch_size] + list(self.X.shape)[1:]))\n+        for i, j in enumerate(index_array):\n+            x = self.X[j]\n+            x = self.random_transform(x.astype(\"float32\"))\n+            x = self.standardize(x)\n+            bX[i] = x\n+        if self.save_to_dir:\n+            for i in range(current_batch_size):\n+                img = array_to_img(bX[i], scale=True)\n+                img.save(self.save_to_dir + \"/\" + self.save_prefix + \"_\" + str(current_index + i) + \".\" + self.save_format)\n+        bY = self.y[index_array]\n+        return bX, bY\n+\n+    def __next__(self):\n+        # for python 3.x\n+        return self.next()\n \n     def standardize(self, x):\n         if self.featurewise_center:\n@@ -250,3 +279,9 @@ class ImageDataGenerator(object):\n             sigma = np.dot(flatX.T, flatX) / flatX.shape[1]\n             U, S, V = linalg.svd(sigma)\n             self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + fudge))), U.T)\n+\n+class GraphImageDataGenerator(ImageDataGenerator):\n+    \"\"\"example of how to build a generator for a graph model\"\"\"\n+    def next(self):\n+        bX, bY = super(GraphImageDataGenerator,self).next()\n+        return {'input': bX, 'output': bY}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04d75045375e4c531c5d9eb2bf73af03190b2249", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 258 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,8 +39,10 @@ def random_shift(x, wrg, hrg, fill_mode=\"nearest\", cval=0.):\n         crop = random.uniform(0., hrg)\n         split = random.uniform(0, 1)\n         crop_top_pixels = int(split*crop*x.shape[2])\n-    x = ndimage.interpolation.shift(x, (0, crop_left_pixels, crop_top_pixels), order=0,\n-                                    mode=fill_mode, cval=cval)\n+    x = ndimage.interpolation.shift(x, (0, crop_left_pixels, crop_top_pixels),\n+                                    order=0,\n+                                    mode=fill_mode,\n+                                    cval=cval)\n     return x\n \n \n@@ -67,9 +69,12 @@ def random_shear(x, intensity, fill_mode=\"nearest\", cval=0.):\n                             [0.0, math.cos(shear), 0.0],\n                             [0.0, 0.0, 1.0]])\n     x = ndimage.interpolation.affine_transform(x, shear_matrix,\n-                                               mode=fill_mode, order=3, cval=cval)\n+                                               mode=fill_mode,\n+                                               order=3,\n+                                               cval=cval)\n     return x\n \n+\n def random_channel_shift(x, rg):\n     # TODO\n     pass\n@@ -79,7 +84,8 @@ def random_zoom(x, rg, fill_mode=\"nearest\", cval=0.):\n     zoom_w = random.uniform(1.-rg, 1.)\n     zoom_h = random.uniform(1.-rg, 1.)\n     x = ndimage.interpolation.zoom(x, zoom=(1., zoom_w, zoom_h),\n-                                   mode=fill_mode, cval=cval)\n+                                   mode=fill_mode,\n+                                   cval=cval)\n     return x  # shape of result will be different from shape of input!\n \n \n@@ -280,8 +286,11 @@ class ImageDataGenerator(object):\n             U, S, V = linalg.svd(sigma)\n             self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + fudge))), U.T)\n \n+\n class GraphImageDataGenerator(ImageDataGenerator):\n-    \"\"\"example of how to build a generator for a graph model\"\"\"\n+    '''Example of how to build a generator for a Graph model\n+    '''\n+\n     def next(self):\n         bX, bY = super(GraphImageDataGenerator, self).next()\n         return {'input': bX, 'output': bY}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b688192cbd74495c9907b6dcfce47e9a582ae1af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 920 | Contributors (this commit): 6 | Commits (past 90d): 30 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -429,20 +429,6 @@ def rnn(step_function, inputs, initial_states,\n     axes = [1, 0] + list(range(2, ndim))\n     inputs = tf.transpose(inputs, (axes))\n     input_list = tf.unpack(inputs)\n-    if mask is None:\n-        mask = ones_like(tf.slice(inputs, [0, 0, 0], [-1, -1, 1]))\n-        inputs_shape = inputs.get_shape()\n-\n-        # TODO: the mask's shape should be automatically inferred, by\n-        # tensorflow yet for some reason it fails to in some test-cases. This\n-        # fixes the issue, but should be removed in future.\n-        mask.set_shape([inputs_shape[0].value, inputs_shape[1].value, 1])\n-        mask = tf.cast(mask, tf.bool)\n-    else:\n-        # Transpose not supported by bool tensor types, hence round-trip to uint8.\n-        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)\n-\n-    mask_list = tf.unpack(mask)\n \n     states = initial_states\n     successive_states = []\n@@ -450,6 +436,11 @@ def rnn(step_function, inputs, initial_states,\n     if go_backwards:\n         input_list.reverse()\n \n+    if mask is not None:\n+        # Transpose not supported by bool tensor types, hence round-trip to uint8.\n+        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)\n+        mask_list = tf.unpack(mask)\n+\n         for input, mask_t in zip(input_list, mask_list):\n             output, new_states = step_function(input, states)\n \n@@ -477,6 +468,11 @@ def rnn(step_function, inputs, initial_states,\n             states = return_states\n             successive_outputs.append(output)\n             successive_states.append(states)\n+    else:\n+        for input in input_list:\n+            output, states = step_function(input, states)\n+            successive_outputs.append(output)\n+            successive_states.append(states)\n \n     last_output = successive_outputs[-1]\n     outputs = tf.pack(successive_outputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c94cf4b32a22504a01953c917402ab27b926875f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 25 | Files Changed: 9 | Hunks: 21 | Methods Changed: 14 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 89 | Churn Cumulative: 10910 | Contributors (this commit): 47 | Commits (past 90d): 192 | Contributors (cumulative): 83 | DMM Complexity: 0.875\n\nDIFF:\n@@ -154,12 +154,12 @@ def mean(x, axis=None, keepdims=False):\n def any(x, axis=None, keepdims=False):\n     '''Bitwise reduction (logical OR).\n \n-    Return array of int8 (0s and 1s).\n+    Return array of uint8 (0s and 1s).\n     '''\n     axis = normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\n-    return tf.cast(x, tf.int8)\n+    return tf.cast(x, tf.uint8)\n \n \n def argmax(x, axis=-1):\n@@ -438,7 +438,10 @@ def rnn(step_function, inputs, initial_states,\n \n     if mask is not None:\n         # Transpose not supported by bool tensor types, hence round-trip to uint8.\n-        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)\n+        mask = tf.cast(mask, tf.uint8)\n+        if len(mask.get_shape()) == ndim-1:\n+            mask = expand_dims(mask)\n+        mask = tf.cast(tf.transpose(mask, axes), tf.bool)\n         mask_list = tf.unpack(mask)\n \n         for input, mask_t in zip(input_list, mask_list):\n\n@@ -427,7 +427,7 @@ def rnn(step_function, inputs, initial_states,\n         the step function.\n     go_backwards: boolean. If True, do the iteration over\n         the time dimension in reverse order.\n-    mask: binary tensor with shape (samples, time, 1),\n+    mask: binary tensor with shape (samples, time),\n         with a zero for every element that is masked.\n \n     Returns\n@@ -447,6 +447,9 @@ def rnn(step_function, inputs, initial_states,\n     if mask is None:\n         mask = expand_dims(ones_like(T.sum(inputs, axis=-1)))\n     else:\n+        if mask.ndim == ndim-1:\n+            mask = expand_dims(mask)\n+        assert mask.ndim == ndim\n         mask = mask.dimshuffle(axes)\n \n     def _step(input, mask, output_tm1, *states):\n@@ -665,6 +668,7 @@ def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n         pool_out = pool_out.dimshuffle((0, 2, 3, 1))\n     return pool_out\n \n+\n # RANDOMNESS\n \n \n\n@@ -319,17 +319,16 @@ class Masking(MaskedLayer):\n     def __init__(self, mask_value=0., **kwargs):\n         super(Masking, self).__init__(**kwargs)\n         self.mask_value = mask_value\n+        if (not hasattr(self, 'input')):\n             self.input = K.placeholder(ndim=3)\n \n     def get_output_mask(self, train=False):\n         X = self.get_input(train)\n-        return K.any(K.ones_like(X) * (1. - K.equal(X, self.mask_value)),\n-                     axis=-1)\n+        return K.any(K.not_equal(X, self.mask_value), axis=-1)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        return X * K.any((1. - K.equal(X, self.mask_value)),\n-                         axis=-1, keepdims=True)\n+        return X * K.cast(K.any(K.not_equal(X, self.mask_value), axis=-1, keepdims=True), K.floatx())\n \n     def get_config(self):\n         config = {'name': self.__class__.__name__,\n\n@@ -89,7 +89,7 @@ class Embedding(Layer):\n         if not self.mask_zero:\n             return None\n         else:\n-            return K.expand_dims(K.not_equal(X, 0))\n+            return K.not_equal(X, 0)\n \n     @property\n     def output_shape(self):\n\n@@ -37,7 +37,7 @@ def hinge(y_true, y_pred):\n def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes.\n     '''\n-    return K.mean(K.categorical_crossentropy(y_pred, y_true), axis=-1)\n+    return K.categorical_crossentropy(y_pred, y_true)\n \n \n def binary_crossentropy(y_true, y_pred):\n@@ -49,11 +49,9 @@ def poisson(y_true, y_pred):\n \n \n def cosine_proximity(y_true, y_pred):\n-    assert K.ndim(y_true) == 2\n-    assert K.ndim(y_pred) == 2\n-    y_true = K.l2_normalize(y_true, axis=1)\n-    y_pred = K.l2_normalize(y_pred, axis=1)\n-    return -K.mean(y_true * y_pred, axis=1)\n+    y_true = K.l2_normalize(y_true, axis=-1)\n+    y_pred = K.l2_normalize(y_pred, axis=-1)\n+    return -K.mean(y_true * y_pred, axis=-1)\n \n \n # aliases\n\n@@ -172,13 +172,11 @@ def test_naming():\n     model.train_on_batch(np.random.random((2, 2)), np.random.random((2, 2)))\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_sequences():\n     '''Test masking sequences with zeroes as padding'''\n     # integer inputs, one per timestep, like embeddings\n     layer = core.Masking()\n-    func = K.function([layer.input], [layer.get_output_mask()])\n+    func = K.function([layer.get_input(True)], [layer.get_output_mask()])\n     input_data = np.array([[[1], [2], [3], [0]],\n                            [[0], [4], [5], [0]]], dtype=np.int32)\n \n@@ -190,8 +188,6 @@ def test_sequences():\n     assert np.all(output == expected), 'Output not as expected'\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_non_zero():\n     '''Test masking with non-zero mask value'''\n     layer = core.Masking(5)\n@@ -204,8 +200,6 @@ def test_non_zero():\n     assert np.all(output == expected), 'Output not as expected'\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_non_zero_output():\n     '''Test output of masking layer with non-zero mask value'''\n     layer = core.Masking(5)\n\n@@ -3,6 +3,9 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.layers import recurrent, embeddings\n+from keras.models import Sequential\n+from keras.layers.core import Masking\n+\n from keras import backend as K\n from keras.models import Sequential\n \n@@ -100,5 +103,20 @@ def test_LSTM():\n     _runner(recurrent.LSTM)\n \n \n+def test_masking_layer():\n+    ''' This test based on a previously failing issue here:\n+    https://github.com/fchollet/keras/issues/1567\n+\n+    '''\n+    model = Sequential()\n+    model.add(Masking(input_shape=(3, 4)))\n+    model.add(recurrent.LSTM(output_dim=5, return_sequences=True))\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+    I = np.random.random((6, 3, 4))\n+    V = np.abs(np.random.random((6, 3, 5)))\n+    V /= V.sum(axis=-1, keepdims=True)\n+    model.fit(I, V, nb_epoch=1, batch_size=100, verbose=1)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -0,0 +1,25 @@\n+import numpy as np\n+\n+from keras import objectives\n+from keras import backend as K\n+\n+\n+allobj = [objectives.mean_squared_error, objectives.root_mean_squared_error,\n+          objectives.mean_absolute_error, objectives.mean_absolute_percentage_error,\n+          objectives.mean_squared_logarithmic_error, objectives.squared_hinge,\n+          objectives.hinge, objectives.categorical_crossentropy, objectives.binary_crossentropy, objectives.poisson,\n+          objectives.cosine_proximity]\n+\n+def test_objective_shapes_3d():\n+    y_a = K.variable(np.random.random((5, 6, 7)))\n+    y_b = K.variable(np.random.random((5, 6, 7)))\n+    for obj in allobj:\n+        objective_output = obj(y_a, y_b)\n+        assert K.eval(objective_output).shape == (5, 6)\n+\n+def test_objective_shapes_2d():\n+    y_a = K.variable(np.random.random((6, 7)))\n+    y_b = K.variable(np.random.random((6, 7)))\n+    for obj in allobj:\n+        objective_output = obj(y_a, y_b)\n+        assert K.eval(objective_output).shape == (6,)\n\n@@ -7,15 +7,13 @@ from keras import objectives\n from keras import backend as K\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n def test_masking():\n     np.random.seed(1337)\n     X = np.array(\n         [[[1, 1], [2, 1], [3, 1], [5, 5]],\n          [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n     model = Sequential()\n-    model.add(Masking(mask_value=0, input_shape=(None, 2)))\n+    model.add(Masking(mask_value=0, input_shape=(4, 2)))\n     model.add(TimeDistributedDense(1, init='one'))\n     model.compile(loss='mse', optimizer='sgd')\n     y = model.predict(X)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#827ec65111d90d9e6ab4aa6a94c0701afdf15f98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 7561 | Contributors (this commit): 38 | Commits (past 90d): 82 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -244,7 +244,11 @@ class Layer(object):\n         '''\n         config = {'name': self.__class__.__name__}\n         if hasattr(self, '_input_shape'):\n-            config['input_shape'] = self._input_shape[1:]\n+            input_shape = self._input_shape\n+            if input_shape[0]:\n+                config['batch_input_shape'] = input_shape[:]\n+            else:\n+                config['input_shape'] = input_shape[1:]\n         if hasattr(self, '_trainable'):\n             config['trainable'] = self._trainable\n         config['cache_enabled'] = self.cache_enabled\n\n@@ -4,7 +4,7 @@ from numpy.testing import assert_allclose\n \n from keras.layers import recurrent, embeddings\n from keras import backend as K\n-from keras.models import Sequential\n+from keras.models import Sequential, model_from_json\n \n nb_samples, timesteps, embedding_dim, output_dim = 3, 5, 10, 5\n embedding_num = 12\n@@ -100,5 +100,16 @@ def test_LSTM():\n     _runner(recurrent.LSTM)\n \n \n+def test_batch_input_shape_serialization():\n+    model = Sequential()\n+    model.add(embeddings.Embedding(2, 2,\n+                                   mask_zero=True,\n+                                   input_length=2,\n+                                   batch_input_shape=(2, 2)))\n+    json_data = model.to_json()\n+    reconstructed_model = model_from_json(json_data)\n+    assert(reconstructed_model.input_shape == (2, 2))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#de8a0133f01b4e9204eb81eb85ff4f9684aeb98f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2848 | Contributors (this commit): 22 | Commits (past 90d): 78 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -289,6 +289,7 @@ def repeat(x, n):\n     if x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim)\n     '''\n+    assert x.ndim == 2\n     tensors = [x] * n\n     stacked = tf.pack(tensors)\n     return tf.transpose(stacked, (1, 0, 2))\n\n@@ -281,9 +281,9 @@ def repeat(x, n):\n     If x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim).\n     '''\n-    tensors = [x] * n\n-    stacked = T.stack(*tensors)\n-    return stacked.dimshuffle((1, 0, 2))\n+    assert x.ndim == 2\n+    x = x.dimshuffle((0, 'x', 1))\n+    return T.extra_ops.repeat(x, n, axis=1)\n \n \n def tile(x, n):\n\n@@ -275,6 +275,7 @@ class Adam(Optimizer):\n                 \"beta_2\": float(K.get_value(self.beta_2)),\n                 \"epsilon\": self.epsilon}\n \n+\n class Adamax(Optimizer):\n     '''Adamax optimizer from Adam paper's Section 7. It is a variant\n      of Adam based on the infinity norm.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef22fcf548814b37e794073068b422581f4a1881", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 932 | Contributors (this commit): 6 | Commits (past 90d): 33 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -289,7 +289,7 @@ def repeat(x, n):\n     if x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim)\n     '''\n-    assert x.ndim == 2\n+    assert ndim(x) == 2\n     tensors = [x] * n\n     stacked = tf.pack(tensors)\n     return tf.transpose(stacked, (1, 0, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b789c7fa0a545d0791df0e3f014437f43352f16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 21 | Churn Cumulative: 11206 | Contributors (this commit): 48 | Commits (past 90d): 130 | Contributors (cumulative): 72 | DMM Complexity: 0.5\n\nDIFF:\n@@ -320,9 +320,12 @@ class Graph(Layer):\n             else:\n                 raise Exception('Type \"int\" can only be used with ndim==2 (Embedding).')\n         self.inputs[name] = layer\n-        self.input_config.append({'name': name,\n-                                  'input_shape': input_shape,\n-                                  'dtype': dtype})\n+        config = {'name': name, 'dtype': dtype}\n+        if batch_input_shape:\n+            config['batch_input_shape'] = batch_input_shape\n+        else:\n+            config['input_shape'] = input_shape\n+        self.input_config.append(config)\n \n     def add_node(self, layer, name, input=None, inputs=[],\n                  merge_mode='concat', concat_axis=-1, dot_axes=-1,\n\n@@ -41,10 +41,10 @@ class Layer(object):\n         for kwarg in kwargs:\n             assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n \n-        if 'input_shape' in kwargs:\n-            self.set_input_shape((None,) + tuple(kwargs['input_shape']))\n         if 'batch_input_shape' in kwargs:\n             self.set_input_shape(tuple(kwargs['batch_input_shape']))\n+        elif 'input_shape' in kwargs:\n+            self.set_input_shape((None,) + tuple(kwargs['input_shape']))\n         self.trainable = True\n         if 'trainable' in kwargs:\n             self.trainable = kwargs['trainable']\n\n@@ -161,10 +161,14 @@ class Recurrent(MaskedLayer):\n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n                   \"return_sequences\": self.return_sequences,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length,\n                   \"go_backwards\": self.go_backwards,\n                   \"stateful\": self.stateful}\n+        if self.stateful:\n+            config['batch_input_shape'] = self.input_shape\n+        else:\n+            config['input_dim'] = self.input_dim\n+            config['input_length'] = self.input_length\n+            \n         base_config = super(Recurrent, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff4a9b7b24011e5eec09bbf03c7b3afe8f1a958c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 33 | Churn Cumulative: 8971 | Contributors (this commit): 43 | Commits (past 90d): 120 | Contributors (cumulative): 64 | DMM Complexity: 0.0\n\nDIFF:\n@@ -155,7 +155,7 @@ class Sequential(Layer):\n \n     def set_weights(self, weights):\n         for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params)\n+            nb_param = len(self.layers[i].params) + len(self.layers[i].non_trainable_weights)\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n\n@@ -33,6 +33,11 @@ class Layer(object):\n             (e.g. `(32, 100)` for a batch of 32 100-dimensional inputs).\n     '''\n     def __init__(self, **kwargs):\n+        if not hasattr(self, 'params'):\n+            self.params = []\n+        if not hasattr(self, 'non_trainable_weights'):\n+            self.non_trainable_weights = []\n+            \n         allowed_kwargs = {'input_shape',\n                           'trainable',\n                           'batch_input_shape',\n@@ -40,7 +45,6 @@ class Layer(object):\n                           'name'}\n         for kwarg in kwargs:\n             assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n-\n         if 'batch_input_shape' in kwargs:\n             self.set_input_shape(tuple(kwargs['batch_input_shape']))\n         elif 'input_shape' in kwargs:\n@@ -51,8 +55,6 @@ class Layer(object):\n         self.name = self.__class__.__name__.lower()\n         if 'name' in kwargs:\n             self.name = kwargs['name']\n-        if not hasattr(self, 'params'):\n-            self.params = []\n         self.cache_enabled = True\n         if 'cache_enabled' in kwargs:\n             self.cache_enabled = kwargs['cache_enabled']\n@@ -222,10 +224,11 @@ class Layer(object):\n             of the layer (i.e. it should match the\n             output of `get_weights`).\n         '''\n-        assert len(self.params) == len(weights), ('Provided weight array does not match layer weights (' +\n-                                                  str(len(self.params)) + ' layer params vs. ' +\n+        params = self.params + self.non_trainable_weights\n+        assert len(params) == len(weights), ('Provided weight array does not match layer weights (' +\n+                                             str(len(params)) + ' layer params vs. ' +\n                                              str(len(weights)) + ' provided weights)')\n-        for p, w in zip(self.params, weights):\n+        for p, w in zip(params, weights):\n             if K.get_value(p).shape != w.shape:\n                 raise Exception('Layer shape %s not compatible with weight shape %s.' % (K.get_value(p).shape, w.shape))\n             K.set_value(p, w)\n@@ -234,8 +237,9 @@ class Layer(object):\n         '''Return the weights of the layer,\n         as a list of numpy arrays.\n         '''\n+        params = self.params + self.non_trainable_weights\n         weights = []\n-        for p in self.params:\n+        for p in params:\n             weights.append(K.get_value(p))\n         return weights\n \n\n@@ -57,25 +57,16 @@ class BatchNormalization(Layer):\n \n         self.gamma = self.init(shape)\n         self.beta = K.zeros(shape)\n-\n         self.params = [self.gamma, self.beta]\n+        \n         self.running_mean = K.zeros(shape)\n         self.running_std = K.ones(shape)\n+        self.non_trainable_weights = [self.running_mean, self.running_std]\n         \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n-    def get_weights(self):\n-        super_weights = super(BatchNormalization, self).get_weights()\n-        return super_weights + [K.get_value(self.running_mean),\n-                                K.get_value(self.running_std)]\n-\n-    def set_weights(self, weights):\n-        K.set_value(self.running_mean, weights[-2])\n-        K.set_value(self.running_std, weights[-1])\n-        super(BatchNormalization, self).set_weights(weights[:-2])\n-\n     def get_output(self, train):\n         X = self.get_input(train)\n         if self.mode == 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cd2e36392c93f86ade4bad04a8fb339546ac8ae8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 57 | Files Changed: 9 | Hunks: 48 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 113 | Churn Cumulative: 19279 | Contributors (this commit): 79 | Commits (past 90d): 220 | Contributors (cumulative): 152 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ class PReLU(MaskedLayer):\n     def build(self):\n         input_shape = self.input_shape[1:]\n         self.alphas = self.init(input_shape)\n-        self.params = [self.alphas]\n+        self.trainable_weights = [self.alphas]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -142,7 +142,7 @@ class ParametricSoftplus(MaskedLayer):\n         input_shape = self.input_shape[1:]\n         self.alphas = K.variable(self.alpha_init * np.ones(input_shape))\n         self.betas = K.variable(self.beta_init * np.ones(input_shape))\n-        self.params = [self.alphas, self.betas]\n+        self.trainable_weights = [self.alphas, self.betas]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n\n@@ -70,12 +70,12 @@ class Sequential(Layer):\n                 self.set_input()\n \n     @property\n-    def params(self):\n-        params = []\n+    def trainable_weights(self):\n+        weights = []\n         for l in self.layers:\n             if l.trainable:\n-                params += l.get_params()[0]\n-        return params\n+                weights += l.get_params()[0]\n+        return weights\n \n     @property\n     def regularizers(self):\n@@ -155,7 +155,7 @@ class Sequential(Layer):\n \n     def set_weights(self, weights):\n         for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params) + len(self.layers[i].non_trainable_weights)\n+            nb_param = len(self.layers[i].trainable_weights) + len(self.layers[i].non_trainable_weights)\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n@@ -198,12 +198,12 @@ class Graph(Layer):\n         return len(self.outputs)\n \n     @property\n-    def params(self):\n-        params = []\n+    def trainable_weights(self):\n+        weights = []\n         for l in self.nodes.values():\n             if l.trainable:\n-                params += l.get_params()[0]\n-        return params\n+                weights += l.get_params()[0]\n+        return weights\n \n     @property\n     def regularizers(self):\n\n@@ -109,7 +109,7 @@ class Convolution1D(Layer):\n         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)\n         self.W = self.init(self.W_shape)\n         self.b = K.zeros((self.nb_filter,))\n-        self.params = [self.W, self.b]\n+        self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n         if self.W_regularizer:\n@@ -265,7 +265,7 @@ class Convolution2D(Layer):\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         self.W = self.init(self.W_shape)\n         self.b = K.zeros((self.nb_filter,))\n-        self.params = [self.W, self.b]\n+        self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n         if self.W_regularizer:\n\n@@ -33,8 +33,8 @@ class Layer(object):\n             (e.g. `(32, 100)` for a batch of 32 100-dimensional inputs).\n     '''\n     def __init__(self, **kwargs):\n-        if not hasattr(self, 'params'):\n-            self.params = []\n+        if not hasattr(self, 'trainable_weights'):\n+            self.trainable_weights = []\n         if not hasattr(self, 'non_trainable_weights'):\n             self.non_trainable_weights = []\n \n@@ -224,7 +224,7 @@ class Layer(object):\n             of the layer (i.e. it should match the\n             output of `get_weights`).\n         '''\n-        params = self.params + self.non_trainable_weights\n+        params = self.trainable_weights + self.non_trainable_weights\n         assert len(params) == len(weights), ('Provided weight array does not match layer weights (' +\n                                              str(len(params)) + ' layer params vs. ' +\n                                              str(len(weights)) + ' provided weights)')\n@@ -237,7 +237,7 @@ class Layer(object):\n         '''Return the weights of the layer,\n         as a list of numpy arrays.\n         '''\n-        params = self.params + self.non_trainable_weights\n+        params = self.trainable_weights + self.non_trainable_weights\n         weights = []\n         for p in params:\n             weights.append(K.get_value(p))\n@@ -268,27 +268,27 @@ class Layer(object):\n         else:\n             regularizers = []\n \n-        if hasattr(self, 'constraints') and len(self.constraints) == len(self.params):\n+        if hasattr(self, 'constraints') and len(self.constraints) == len(self.trainable_weights):\n             for c in self.constraints:\n                 if c:\n                     consts.append(c)\n                 else:\n                     consts.append(constraints.identity())\n         elif hasattr(self, 'constraint') and self.constraint:\n-            consts += [self.constraint for _ in range(len(self.params))]\n+            consts += [self.constraint for _ in range(len(self.trainable_weights))]\n         else:\n-            consts += [constraints.identity() for _ in range(len(self.params))]\n+            consts += [constraints.identity() for _ in range(len(self.trainable_weights))]\n \n         if hasattr(self, 'updates') and self.updates:\n             updates += self.updates\n \n-        return self.params, regularizers, consts, updates\n+        return self.trainable_weights, regularizers, consts, updates\n \n     def count_params(self):\n         '''Return the total number of floats (or ints)\n         composing the weights of the layer.\n         '''\n-        return sum([K.count_params(p) for p in self.params])\n+        return sum([K.count_params(p) for p in self.trainable_weights])\n \n \n class MaskedLayer(Layer):\n@@ -362,7 +362,7 @@ class TimeDistributedMerge(Layer):\n     def __init__(self, mode='sum', **kwargs):\n         super(TimeDistributedMerge, self).__init__(**kwargs)\n         self.mode = mode\n-        self.params = []\n+        self.trainable_weights = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n@@ -487,7 +487,7 @@ class Merge(Layer):\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n         self.layers = layers\n-        self.params = []\n+        self.trainable_weights = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n@@ -497,8 +497,8 @@ class Merge(Layer):\n             self.updates += updates\n             # params and constraints have the same size\n             for p, c in zip(params, consts):\n-                if p not in self.params:\n-                    self.params.append(p)\n+                if p not in self.trainable_weights:\n+                    self.trainable_weights.append(p)\n                     self.constraints.append(c)\n         super(Merge, self).__init__()\n \n@@ -532,7 +532,7 @@ class Merge(Layer):\n             return (input_shapes[0][0], 1)\n \n     def get_params(self):\n-        return self.params, self.regularizers, self.constraints, self.updates\n+        return self.trainable_weights, self.regularizers, self.constraints, self.updates\n \n     def get_output(self, train=False):\n         if self.mode == 'sum' or self.mode == 'ave':\n@@ -611,7 +611,7 @@ class Merge(Layer):\n \n     def set_weights(self, weights):\n         for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params)\n+            nb_param = len(self.layers[i].trainable_weights)\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n@@ -942,7 +942,7 @@ class Dense(Layer):\n         self.W = self.init((input_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n \n-        self.params = [self.W, self.b]\n+        self.trainable_weights = [self.W, self.b]\n \n         self.regularizers = []\n         if self.W_regularizer:\n@@ -1093,7 +1093,7 @@ class TimeDistributedDense(MaskedLayer):\n         self.W = self.init((input_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n \n-        self.params = [self.W, self.b]\n+        self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n         if self.W_regularizer:\n@@ -1222,7 +1222,7 @@ class AutoEncoder(Layer):\n         self.build()\n \n     def build(self):\n-        self.params = []\n+        self.trainable_weights = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n@@ -1235,8 +1235,8 @@ class AutoEncoder(Layer):\n             self.regularizers += regularizers\n             self.updates += updates\n             for p, c in zip(params, constraints):\n-                if p not in self.params:\n-                    self.params.append(p)\n+                if p not in self.trainable_weights:\n+                    self.trainable_weights.append(p)\n                     self.constraints.append(c)\n \n     def set_previous(self, node):\n@@ -1249,7 +1249,7 @@ class AutoEncoder(Layer):\n         return weights\n \n     def set_weights(self, weights):\n-        nb_param = len(self.encoder.params)\n+        nb_param = len(self.encoder.trainable_weights)\n         self.encoder.set_weights(weights[:nb_param])\n         self.decoder.set_weights(weights[nb_param:])\n \n@@ -1337,7 +1337,7 @@ class MaxoutDense(Layer):\n         self.W = self.init((self.nb_feature, input_dim, self.output_dim))\n         self.b = K.zeros((self.nb_feature, self.output_dim))\n \n-        self.params = [self.W, self.b]\n+        self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n         if self.W_regularizer:\n@@ -1463,7 +1463,7 @@ class LambdaMerge(Lambda):\n             raise Exception('Please specify two or more input layers '\n                             '(or containers) to merge.')\n         self.layers = layers\n-        self.params = []\n+        self.trainable_weights = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n@@ -1473,8 +1473,8 @@ class LambdaMerge(Lambda):\n             self.updates += updates\n             # params and constraints have the same size\n             for p, c in zip(params, consts):\n-                if p not in self.params:\n-                    self.params.append(p)\n+                if p not in self.trainable_weights:\n+                    self.trainable_weights.append(p)\n                     self.constraints.append(c)\n         py3 = sys.version_info[0] == 3\n         if py3:\n@@ -1508,7 +1508,7 @@ class LambdaMerge(Lambda):\n             return tuple(shape)\n \n     def get_params(self):\n-        return self.params, self.regularizers, self.constraints, self.updates\n+        return self.trainable_weights, self.regularizers, self.constraints, self.updates\n \n     def get_output(self, train=False):\n         func = marshal.loads(self.function)\n@@ -1545,7 +1545,7 @@ class LambdaMerge(Lambda):\n \n     def set_weights(self, weights):\n         for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params)\n+            nb_param = len(self.layers[i].trainable_weights) + len(self.non_trainable_weights)\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n@@ -1594,7 +1594,7 @@ class Siamese(Layer):\n         self.merge_mode = merge_mode\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n-        self.params = []\n+        self.trainable_weights = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n@@ -1607,8 +1607,8 @@ class Siamese(Layer):\n             self.updates += updates\n             # params and constraints have the same size\n             for p, c in zip(params, consts):\n-                if p not in self.params:\n-                    self.params.append(p)\n+                if p not in self.trainable_weights:\n+                    self.trainable_weights.append(p)\n                     self.constraints.append(c)\n         super(Siamese, self).__init__()\n \n@@ -1646,7 +1646,7 @@ class Siamese(Layer):\n             return (input_shapes[0][0], 1)\n \n     def get_params(self):\n-        return self.params, self.regularizers, self.constraints, self.updates\n+        return self.trainable_weights, self.regularizers, self.constraints, self.updates\n \n     def set_layer_input(self, head):\n         layer = self.layer\n@@ -1766,12 +1766,12 @@ class Siamese(Layer):\n         return weights\n \n     def set_weights(self, weights):\n-        nb_param = len(self.layer.params)\n+        nb_param = len(self.layer.trainable_weights)\n         self.layer.set_weights(weights[:nb_param])\n         weights = weights[nb_param:]\n         if self.merge_mode and not self.is_graph:\n             for i in range(len(self.inputs)):\n-                nb_param = len(self.inputs[i].params)\n+                nb_param = len(self.inputs[i].trainable_weights)\n                 self.inputs[i].set_weights(weights[:nb_param])\n                 weights = weights[nb_param:]\n \n@@ -1800,7 +1800,7 @@ class SiameseHead(Layer):\n     '''\n     def __init__(self, head):\n         self.head = head\n-        self.params = []\n+        self.trainable_weights = []\n         super(SiameseHead, self).__init__()\n \n     def get_output(self, train=False):\n@@ -1914,7 +1914,7 @@ class Highway(Layer):\n         # initialize with a vector of values `transform_bias`\n         self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias)\n \n-        self.params = [self.W, self.b, self.W_carry, self.b_carry]\n+        self.trainable_weights = [self.W, self.b, self.W_carry, self.b_carry]\n \n         self.regularizers = []\n         if self.W_regularizer:\n\n@@ -71,7 +71,7 @@ class Embedding(Layer):\n         self.input = K.placeholder(shape=(self.input_shape[0], self.input_length),\n                                    dtype='int32')\n         self.W = self.init((self.input_dim, self.output_dim))\n-        self.params = [self.W]\n+        self.trainable_weights = [self.W]\n         self.regularizers = []\n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n\n@@ -57,7 +57,7 @@ class BatchNormalization(Layer):\n \n         self.gamma = self.init(shape)\n         self.beta = K.zeros(shape)\n-        self.params = [self.gamma, self.beta]\n+        self.trainable_weights = [self.gamma, self.beta]\n \n         self.running_mean = K.zeros(shape)\n         self.running_std = K.ones(shape)\n\n@@ -208,7 +208,7 @@ class SimpleRNN(Recurrent):\n         self.W = self.init((input_dim, self.output_dim))\n         self.U = self.inner_init((self.output_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n-        self.params = [self.W, self.U, self.b]\n+        self.trainable_weights = [self.W, self.U, self.b]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -291,10 +291,9 @@ class GRU(Recurrent):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = K.zeros((self.output_dim,))\n \n-        self.params = [self.W_z, self.U_z, self.b_z,\n+        self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n                                   self.W_r, self.U_r, self.b_r,\n                                   self.W_h, self.U_h, self.b_h]\n-\n         if self.stateful:\n             self.reset_states()\n         else:\n@@ -407,7 +406,7 @@ class LSTM(Recurrent):\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n         self.b_o = K.zeros((self.output_dim,))\n \n-        self.params = [self.W_i, self.U_i, self.b_i,\n+        self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                   self.W_c, self.U_c, self.b_c,\n                                   self.W_f, self.U_f, self.b_f,\n                                   self.W_o, self.U_o, self.b_o]\n\n@@ -496,7 +496,7 @@ class Sequential(Model, containers.Sequential):\n \n         for r in self.regularizers:\n             train_loss = r(train_loss)\n-        updates = self.optimizer.get_updates(self.params,\n+        updates = self.optimizer.get_updates(self.trainable_weights,\n                                              self.constraints,\n                                              train_loss)\n         updates += self.updates\n@@ -1125,7 +1125,7 @@ class Graph(Model, containers.Graph):\n         for r in self.regularizers:\n             train_loss = r(train_loss)\n         self.optimizer = optimizers.get(optimizer)\n-        updates = self.optimizer.get_updates(self.params,\n+        updates = self.optimizer.get_updates(self.trainable_weights,\n                                              self.constraints,\n                                              train_loss)\n         updates += self.updates\n\n@@ -23,7 +23,7 @@ def test_unitnorm_constraint():\n     lookup.compile(loss='binary_crossentropy', optimizer='sgd',\n                    class_mode='binary')\n     lookup.train_on_batch(X1, np.array([[1], [0]], dtype='int32'))\n-    norm = np.linalg.norm(K.get_value(lookup.params[0]), axis=0)\n+    norm = np.linalg.norm(K.get_value(lookup.trainable_weights[0]), axis=0)\n     assert_allclose(norm, np.ones_like(norm).astype('float32'), rtol=1e-05)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
