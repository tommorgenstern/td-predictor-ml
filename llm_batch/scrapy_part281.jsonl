{"custom_id": "scrapy#3263441fbcec8f46d363926d9106572cb0ecac5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 32 | Churn Cumulative: 218 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,7 +49,7 @@ class RFPDupeFilter(BaseDupeFilter):\n             return True\n         self.fingerprints.add(fp)\n         if self.file:\n-            self.file.write(fp + os.linesep)\n+            self.file.write(fp + '\\n')\n \n     def request_fingerprint(self, request):\n         return request_fingerprint(request)\n\n@@ -2,6 +2,8 @@ import hashlib\n import tempfile\n import unittest\n import shutil\n+import os\n+import sys\n from testfixtures import LogCapture\n \n from scrapy.dupefilters import RFPDupeFilter\n@@ -84,16 +86,20 @@ class RFPDupeFilterTest(unittest.TestCase):\n         path = tempfile.mkdtemp()\n         try:\n             df = RFPDupeFilter(path)\n+            try:\n                 df.open()\n                 assert not df.request_seen(r1)\n                 assert df.request_seen(r1)\n+            finally:\n                 df.close('finished')\n \n             df2 = RFPDupeFilter(path)\n+            try:\n                 df2.open()\n                 assert df2.request_seen(r1)\n                 assert not df2.request_seen(r2)\n                 assert df2.request_seen(r2)\n+            finally:\n                 df2.close('finished')\n         finally:\n             shutil.rmtree(path)\n@@ -129,6 +135,30 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n         case_insensitive_dupefilter.close('finished')\n \n+    def test_seenreq_newlines(self):\n+        \"\"\" Checks against adding duplicate \\r to\n+        line endings on Windows platforms. \"\"\"\n+\n+        r1 = Request('http://scrapytest.org/1')\n+\n+        path = tempfile.mkdtemp()\n+        try:\n+            df = RFPDupeFilter(path)\n+            df.open()\n+            df.request_seen(r1)\n+            df.close('finished')\n+\n+            with open(os.path.join(path, 'requests.seen'), 'rb') as seen_file:\n+                line = next(seen_file).decode()\n+                assert not line.endswith('\\r\\r\\n')\n+                if sys.platform == 'win32':\n+                    assert line.endswith('\\r\\n')\n+                else:\n+                    assert line.endswith('\\n')\n+\n+        finally:\n+            shutil.rmtree(path)\n+\n     def test_log(self):\n         with LogCapture() as l:\n             settings = {'DUPEFILTER_DEBUG': False,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31f6c7112fe8efce2105983b8350b1dabdce7a1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 32 | Churn Cumulative: 984 | Contributors (this commit): 13 | Commits (past 90d): 15 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -117,6 +117,24 @@ class AsyncDefAsyncioReturnSpider(SimpleSpider):\n         return [{'id': 1}, {'id': 2}]\n \n \n+class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_reqs_return'\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.2)\n+        req_id = response.meta.get('req_id', 0)\n+        status = await get_from_asyncio_queue(response.status)\n+        self.logger.info(\"Got response %d, req_id %d\" % (status, req_id))\n+        if req_id > 0:\n+            return\n+        reqs = []\n+        for i in range(1, 3):\n+            req = Request(self.start_urls[0], dont_filter=True, meta={'req_id': i})\n+            reqs.append(req)\n+        return reqs\n+\n+\n class ItemSpider(FollowAllSpider):\n \n     name = 'item'\n\n@@ -13,7 +13,8 @@ from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n from tests.spiders import (FollowAllSpider, DelaySpider, SimpleSpider, BrokenStartRequestsSpider,\n                            SingleRequestSpider, DuplicateStartRequestsSpider, CrawlSpiderWithErrback,\n-                           AsyncDefSpider, AsyncDefAsyncioSpider, AsyncDefAsyncioReturnSpider)\n+                           AsyncDefSpider, AsyncDefAsyncioSpider, AsyncDefAsyncioReturnSpider,\n+                           AsyncDefAsyncioReqsReturnSpider)\n \n \n class CrawlTestCase(TestCase):\n@@ -330,7 +331,7 @@ with multiples lines\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n-    def test_async_def_asyncio_parse_list(self):\n+    def test_async_def_asyncio_parse_items_list(self):\n         items = []\n \n         def _on_item_scraped(item):\n@@ -343,3 +344,12 @@ with multiples lines\n         self.assertIn(\"Got response 200\", str(log))\n         self.assertIn({'id': 1}, items)\n         self.assertIn({'id': 2}, items)\n+\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncio_parse_reqs_list(self):\n+        crawler = self.runner.create_crawler(AsyncDefAsyncioReqsReturnSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        for req_id in range(3):\n+            self.assertIn(\"Got response 200, req_id %d\" % req_id, str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7323780c97e69b560bf9a4bd7e6ccd60fb2b8f13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 1 | Files Changed: 5 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 19/9 | Churn Δ: 123 | Churn Cumulative: 1096 | Contributors (this commit): 19 | Commits (past 90d): 26 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,9 @@ collect_ignore = [\n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n     # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n-    *_py_files(\"tests/CrawlerProcess\")\n+    *_py_files(\"tests/CrawlerProcess\"),\n+    # Py36-only parts of respective tests\n+    *_py_files(\"tests/py36\"),\n ]\n \n for line in open('tests/ignores.txt'):\n\n@@ -0,0 +1,10 @@\n+\"\"\"\n+Helpers using Python 3.6+ syntax (ignore SyntaxError on import).\n+\"\"\"\n+\n+\n+async def collect_asyncgen(result):\n+    results = []\n+    async for x in result:\n+        results.append(x)\n+    return results\n\n@@ -4,12 +4,20 @@ import inspect\n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n+try:\n+    from scrapy.utils.py36 import collect_asyncgen\n+except SyntaxError:\n+    collect_asyncgen = None\n \n \n logger = logging.getLogger(__name__)\n \n \n def iterate_spider_output(result):\n+    if collect_asyncgen and hasattr(inspect, 'isasyncgen') and inspect.isasyncgen(result):\n+        d = deferred_from_coro(collect_asyncgen(result))\n+        d.addCallback(iterate_spider_output)\n+        return d\n     return arg_to_iter(deferred_from_coro(result))\n \n \n\n@@ -0,0 +1,50 @@\n+import asyncio\n+\n+from scrapy import Request\n+from tests.spiders import SimpleSpider\n+\n+\n+class AsyncDefAsyncioGenSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_gen'\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.2)\n+        yield {'foo': 42}\n+        self.logger.info(\"Got response %d\" % response.status)\n+\n+\n+class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_gen_loop'\n+\n+    async def parse(self, response):\n+        for i in range(10):\n+            await asyncio.sleep(0.1)\n+            yield {'foo': i}\n+        self.logger.info(\"Got response %d\" % response.status)\n+\n+\n+class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_gen_complex'\n+    initial_reqs = 4\n+    following_reqs = 3\n+    depth = 2\n+\n+    def _get_req(self, index):\n+        return Request(self.mockserver.url(\"/status?n=200&request=%d\" % index),\n+                       meta={'index': index})\n+\n+    def start_requests(self):\n+        for i in range(self.initial_reqs):\n+            yield self._get_req(i)\n+\n+    async def parse(self, response):\n+        index = response.meta['index']\n+        yield {'index': index}\n+        if index < 10 ** self.depth:\n+            for new_index in range(10 * index, 10 * index + self.following_reqs):\n+                yield self._get_req(new_index)\n+        await asyncio.sleep(0.1)\n+        yield {'index': index + 5}\n\n@@ -1,5 +1,6 @@\n import json\n import logging\n+import sys\n \n from pytest import mark\n from testfixtures import LogCapture\n@@ -343,3 +344,53 @@ with multiples lines\n         self.assertIn(\"Got response 200\", str(log))\n         self.assertIn({'id': 1}, items)\n         self.assertIn({'id': 2}, items)\n+\n+    @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncgen_parse(self):\n+        from tests.py36._test_crawl import AsyncDefAsyncioGenSpider\n+        crawler = self.runner.create_crawler(AsyncDefAsyncioGenSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        self.assertIn(\"Got response 200\", str(log))\n+        itemcount = crawler.stats.get_value('item_scraped_count')\n+        self.assertEqual(itemcount, 1)\n+\n+    @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncgen_parse_loop(self):\n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n+        from tests.py36._test_crawl import AsyncDefAsyncioGenLoopSpider\n+        crawler = self.runner.create_crawler(AsyncDefAsyncioGenLoopSpider)\n+        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n+        with LogCapture() as log:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        self.assertIn(\"Got response 200\", str(log))\n+        itemcount = crawler.stats.get_value('item_scraped_count')\n+        self.assertEqual(itemcount, 10)\n+        for i in range(10):\n+            self.assertIn({'foo': i}, items)\n+\n+    @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n+    @mark.only_asyncio()\n+    @defer.inlineCallbacks\n+    def test_async_def_asyncgen_parse_complex(self):\n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n+        from tests.py36._test_crawl import AsyncDefAsyncioGenComplexSpider\n+        crawler = self.runner.create_crawler(AsyncDefAsyncioGenComplexSpider)\n+        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        itemcount = crawler.stats.get_value('item_scraped_count')\n+        self.assertEqual(itemcount, 80)\n+        for i in [0, 3, 21, 22, 207, 311]:  # some random items\n+            self.assertIn({'index': i}, items)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35723d76c0c07575309810e776305e9ea22fc18d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 653 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from w3lib.url import canonicalize_url\n \n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n-from scrapy.utils.python import unique as unique_list, to_unicode\n+from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n from scrapy.linkextractors import FilteringLinkExtractor\n \n@@ -66,7 +66,7 @@ class LxmlParserLinkExtractor(object):\n                 url = self.process_attr(attr_val)\n                 if url is None:\n                     continue\n-            url = to_unicode(url, encoding=response_encoding)\n+            url = canonicalize_url(url, encoding=response_encoding)\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n             link = Link(url, _collect_string_content(el) or u'',\n\n@@ -2,8 +2,6 @@ import re\n import unittest\n from warnings import catch_warnings\n \n-import pytest\n-\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n@@ -214,7 +212,7 @@ class Base:\n             response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html, encoding='iso8859-15')\n             links = self.extractor_cls(restrict_xpaths='//p').extract_links(response)\n             self.assertEqual(links,\n-                             [Link(url='http://example.org/%E2%99%A5/you?c=%E2%82%AC', text=u'text')])\n+                             [Link(url='http://example.org/%E2%99%A5/you?c=%A4', text=u'text')])\n \n         def test_restrict_xpaths_concat_in_handle_data(self):\n             \"\"\"html entities cause SGMLParser to call handle_data hook twice\"\"\"\n@@ -506,7 +504,6 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n             Link(url='http://example.org/item2.html', text=u'Pic of a dog', nofollow=False),\n         ])\n \n-    @pytest.mark.xfail\n     def test_restrict_xpaths_with_html_entities(self):\n         super(LxmlLinkExtractorTestCase, self).test_restrict_xpaths_with_html_entities()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a64fa2f0866c10594f1e5cf00a0161f9fea1eb62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1926 | Contributors (this commit): 21 | Commits (past 90d): 9 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -417,14 +417,14 @@ class ScrapyAgent:\n         d = defer.Deferred(_cancel)\n         txresponse.deliverBody(\n             _ResponseReader(\n-                d,\n-                txresponse,\n-                request,\n-                maxsize,\n-                warnsize,\n-                fail_on_dataloss,\n-                self._crawler,\n-                self._source,\n+                finished=d,\n+                txresponse=txresponse,\n+                request=request,\n+                maxsize=maxsize,\n+                warnsize=warnsize,\n+                fail_on_dataloss=fail_on_dataloss,\n+                crawler=self._crawler,\n+                source=self._source,\n             )\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#122ce6d6fb3861d99ba2f2810b2370056bae1190", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 331 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -12,7 +12,6 @@ module with the ``runserver`` argument::\n \n import os\n import re\n-import string\n import sys\n from collections import defaultdict\n from urllib.parse import urlparse\n@@ -91,7 +90,8 @@ def start_test_site(debug=False):\n     r = static.File(root_dir)\n     r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n     r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n-    r.putChild(b\"random\", static.Data(string.ascii_letters.encode(\"utf8\") * 2**14, \"text/plain\"))\n+    numbers = [str(x).encode(\"utf8\") for x in range(2**14)]\n+    r.putChild(b\"numbers\", static.Data(b\"\".join(numbers), \"text/plain\"))\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n@@ -124,7 +124,7 @@ class CrawlerRun(object):\n             self.geturl(\"/\"),\n             self.geturl(\"/redirect\"),\n             self.geturl(\"/redirect\"),  # duplicate\n-            self.geturl(\"/random\"),\n+            self.geturl(\"/numbers\"),\n         ]\n \n         for name, signal in vars(signals).items():\n@@ -315,8 +315,12 @@ class EngineTest(unittest.TestCase):\n                     b\"  </body>\\n\"\n                     b\"</html>\\n\"\n                 )\n-            elif self.run.getpath(request.url) == \"/random\":\n-                self.assertTrue(len(data) > 1)  # signal was fired multiple times\n+            elif self.run.getpath(request.url) == \"/numbers\":\n+                # signal was fired multiple times\n+                self.assertTrue(len(data) > 1)\n+                # bytes were received in order\n+                numbers = [str(x).encode(\"utf8\") for x in range(2**14)]\n+                self.assertEqual(joined_data, b\"\".join(numbers))\n \n     def _assert_signals_caught(self):\n         assert signals.engine_started in self.run.signals_caught\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4626e90df8ba4a945bb9cd6be47a915788e76f23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 44 | Churn Cumulative: 1887 | Contributors (this commit): 16 | Commits (past 90d): 10 | Contributors (cumulative): 35 | DMM Complexity: 0.96875\n\nDIFF:\n@@ -107,7 +107,7 @@ class Response(object_ref):\n \n     def follow(self, url, callback=None, method='GET', headers=None, body=None,\n                cookies=None, meta=None, encoding='utf-8', priority=0,\n-               dont_filter=False, errback=None, cb_kwargs=None):\n+               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n         # type: (...) -> Request\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -124,6 +124,7 @@ class Response(object_ref):\n         elif url is None:\n             raise ValueError(\"url can't be None\")\n         url = self.urljoin(url)\n+\n         return Request(\n             url=url,\n             callback=callback,\n@@ -137,11 +138,12 @@ class Response(object_ref):\n             dont_filter=dont_filter,\n             errback=errback,\n             cb_kwargs=cb_kwargs,\n+            flags=flags,\n         )\n \n     def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,\n                    cookies=None, meta=None, encoding='utf-8', priority=0,\n-                   dont_filter=False, errback=None, cb_kwargs=None):\n+                   dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n         # type: (...) -> Generator[Request, None, None]\n         \"\"\"\n         Return an iterable of :class:`~.Request` instances to follow all links\n@@ -169,6 +171,7 @@ class Response(object_ref):\n                 dont_filter=dont_filter,\n                 errback=errback,\n                 cb_kwargs=cb_kwargs,\n+                flags=flags,\n             )\n             for url in urls\n         )\n\n@@ -121,7 +121,7 @@ class TextResponse(Response):\n \n     def follow(self, url, callback=None, method='GET', headers=None, body=None,\n                cookies=None, meta=None, encoding=None, priority=0,\n-               dont_filter=False, errback=None, cb_kwargs=None):\n+               dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n         # type: (...) -> Request\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -157,11 +157,12 @@ class TextResponse(Response):\n             dont_filter=dont_filter,\n             errback=errback,\n             cb_kwargs=cb_kwargs,\n+            flags=flags,\n         )\n \n     def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,\n                    cookies=None, meta=None, encoding=None, priority=0,\n-                   dont_filter=False, errback=None, cb_kwargs=None,\n+                   dont_filter=False, errback=None, cb_kwargs=None, flags=None,\n                    css=None, xpath=None):\n         # type: (...) -> Generator[Request, None, None]\n         \"\"\"\n@@ -214,6 +215,7 @@ class TextResponse(Response):\n             dont_filter=dont_filter,\n             errback=errback,\n             cb_kwargs=cb_kwargs,\n+            flags=flags,\n         )\n \n \n\n@@ -166,6 +166,10 @@ class BaseResponseTest(unittest.TestCase):\n     def test_follow_whitespace_link(self):\n         self._assert_followed_url(Link('http://example.com/foo '),\n                                   'http://example.com/foo%20')\n+    def test_follow_flags(self):\n+        res = self.response_class('http://example.com/')\n+        fol = res.follow('http://example.com/', flags=['cached', 'allowed'])\n+        self.assertEqual(fol.flags, ['cached', 'allowed'])\n \n     # Response.follow_all\n \n@@ -232,6 +236,17 @@ class BaseResponseTest(unittest.TestCase):\n         expected = [u.replace(' ', '%20') for u in absolute]\n         self._assert_followed_all_urls(links, expected)\n \n+    def test_follow_all_flags(self):\n+        re = self.response_class('http://www.example.com/')\n+        urls = [\n+            'http://www.example.com/',\n+            'http://www.example.com/2',\n+            'http://www.example.com/foo',\n+        ]\n+        fol = re.follow_all(urls, flags=['cached', 'allowed'])\n+        for req in fol:\n+            self.assertEqual(req.flags, ['cached', 'allowed'])\n+\n     def _assert_followed_url(self, follow_obj, target_url, response=None):\n         if response is None:\n             response = self._links_response()\n@@ -562,6 +577,22 @@ class TextResponseTest(BaseResponseTest):\n         )\n         self.assertEqual(req.encoding, 'cp1251')\n \n+    def test_follow_flags(self):\n+        res = self.response_class('http://example.com/')\n+        fol = res.follow('http://example.com/', flags=['cached', 'allowed'])\n+        self.assertEqual(fol.flags, ['cached', 'allowed'])\n+\n+    def test_follow_all_flags(self):\n+        re = self.response_class('http://www.example.com/')\n+        urls = [\n+            'http://www.example.com/',\n+            'http://www.example.com/2',\n+            'http://www.example.com/foo',\n+        ]\n+        fol = re.follow_all(urls, flags=['cached', 'allowed'])\n+        for req in fol:\n+            self.assertEqual(req.flags, ['cached', 'allowed'])\n+\n     def test_follow_all_css(self):\n         expected = [\n             'http://example.com/sample3.html',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36dcf901849014d7db00a0294ed86c6cc79b5cc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 20 | Churn Cumulative: 797 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,12 +32,14 @@ class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n     following_reqs = 3\n     depth = 2\n \n-    def _get_req(self, index):\n+    def _get_req(self, index, cb=None):\n         return Request(self.mockserver.url(\"/status?n=200&request=%d\" % index),\n-                       meta={'index': index})\n+                       meta={'index': index},\n+                       dont_filter=True,\n+                       callback=cb)\n \n     def start_requests(self):\n-        for i in range(self.initial_reqs):\n+        for i in range(1, self.initial_reqs + 1):\n             yield self._get_req(i)\n \n     async def parse(self, response):\n@@ -46,5 +48,10 @@ class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n         if index < 10 ** self.depth:\n             for new_index in range(10 * index, 10 * index + self.following_reqs):\n                 yield self._get_req(new_index)\n+        yield self._get_req(index, cb=self.parse2)\n         await asyncio.sleep(0.1)\n         yield {'index': index + 5}\n+\n+    async def parse2(self, response):\n+        await asyncio.sleep(0.1)\n+        yield {'index2': response.meta['index']}\n\n@@ -392,9 +392,12 @@ with multiples lines\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         yield crawler.crawl(mockserver=self.mockserver)\n         itemcount = crawler.stats.get_value('item_scraped_count')\n-        self.assertEqual(itemcount, 80)\n-        for i in [0, 3, 21, 22, 207, 311]:  # some random items\n+        self.assertEqual(itemcount, 156)\n+        # some random items\n+        for i in [1, 4, 21, 22, 207, 311]:\n             self.assertIn({'index': i}, items)\n+        for i in [10, 30, 122]:\n+            self.assertIn({'index2': i}, items)\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f0f52cbf7bdc9f11f7b83c482ad52ad7ad32ba0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n+from pytest import mark\n from twisted.internet import defer\n from twisted.trial import unittest\n \n from scrapy import signals, Request, Spider\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n \n from tests.mockserver import MockServer\n \n@@ -11,12 +12,12 @@ class ItemSpider(Spider):\n     name = 'itemspider'\n \n     def start_requests(self):\n-        for _ in range(10):\n-            yield Request(self.mockserver.url('/status?n=200'),\n-                          dont_filter=True)\n+        for index in range(10):\n+            yield Request(self.mockserver.url('/status?n=200&id=%d' % index),\n+                          meta={'index': index})\n \n     def parse(self, response):\n-        return {'field': 42}\n+        return {'index': response.meta['index']}\n \n \n class AsyncSignalTestCase(unittest.TestCase):\n@@ -29,11 +30,15 @@ class AsyncSignalTestCase(unittest.TestCase):\n         self.mockserver.__exit__(None, None, None)\n \n     async def _on_item_scraped(self, item):\n+        item = await get_from_asyncio_queue(item)\n         self.items.append(item)\n \n+    @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_simple_pipeline(self):\n         crawler = get_crawler(ItemSpider)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 10)\n+        for index in range(10):\n+            self.assertIn({'index': index}, self.items)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61e74bac765de0f786d2125e876e4d7934f1722b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 127 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from urllib.parse import urljoin\n \n import lxml.etree as etree\n from w3lib.html import strip_html5_whitespace\n-from w3lib.url import canonicalize_url\n+from w3lib.url import canonicalize_url, safe_url_string\n \n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n@@ -66,7 +66,7 @@ class LxmlParserLinkExtractor(object):\n                 url = self.process_attr(attr_val)\n                 if url is None:\n                     continue\n-            url = canonicalize_url(url, encoding=response_encoding)\n+            url = safe_url_string(url, encoding=response_encoding)\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n             link = Link(url, _collect_string_content(el) or u'',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d6d4fb2335ef24b1efca67dcedf5d264a642e0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 251 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -68,7 +68,6 @@ def get_project_settings():\n     if settings_module_path:\n         settings.setmodule(settings_module_path, priority='project')\n \n-    # XXX: remove this hack\n     pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n     if pickled_settings:\n         warnings.warn(\"Use of environment variable \"\n@@ -76,10 +75,9 @@ def get_project_settings():\n                       \"is deprecated.\", ScrapyDeprecationWarning)\n         settings.setdict(pickle.loads(pickled_settings), priority='project')\n \n-    # XXX: deprecate and remove this functionality\n     env_overrides = {k[7:]: v for k, v in os.environ.items() if\n                      k.startswith('SCRAPY_')}\n     if env_overrides:\n+        warnings.warn(\"Use of 'SCRAPY_'-prefixed environment variables to override settings is deprecated.\", ScrapyDeprecationWarning)\n         settings.setdict(env_overrides, priority='project')\n-\n     return settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#73e88d036c72aa628af50371aace4cfce7e286e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 335 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -2,14 +2,13 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n-from __future__ import absolute_import\n-from posixpath import split\n import asyncio\n import os\n+from posixpath import split\n+from unittest import mock\n \n from importlib import import_module\n from twisted.trial.unittest import SkipTest\n-from tests import mock\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.boto import is_botocore\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1be078eaa17a8df72716932fde07e225f79745f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 21 | Churn Cumulative: 1592 | Contributors (this commit): 24 | Commits (past 90d): 8 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -120,12 +120,14 @@ def assert_samelines(testcase, text1, text2, msg=None):\n     \"\"\"\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n \n+\n def get_from_asyncio_queue(value):\n     q = asyncio.Queue()\n     getter = q.get()\n     q.put_nowait(value)\n     return getter\n \n+\n def mock_google_cloud_storage():\n     \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n     classes and set their proper return values.\n\n@@ -25,9 +25,13 @@ from scrapy.extensions.feedexport import (\n     IFeedStorage, FileFeedStorage, FTPFeedStorage, GCSFeedStorage,\n     S3FeedStorage, StdoutFeedStorage,\n     BlockingFeedStorage)\n-from scrapy.utils.test import (assert_aws_environ, get_s3_content_and_delete,\n-    get_crawler, mock_google_cloud_storage)\n from scrapy.utils.python import to_unicode\n+from scrapy.utils.test import (\n+    assert_aws_environ,\n+    get_s3_content_and_delete,\n+    get_crawler,\n+    mock_google_cloud_storage,\n+)\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -362,11 +366,9 @@ class S3FeedStorageTest(unittest.TestCase):\n \n class GCSFeedStorageTest(unittest.TestCase):\n \n-    @mock.patch('scrapy.conf.settings',\n-                new={'GCS_PROJECT_ID': 'conf_id', 'FEED_STORAGE_GCS_ACL': None }, create=True)\n     def test_parse_settings(self):\n         try:\n-            from google.cloud.storage import Client\n+            from google.cloud.storage import Client  # noqa\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n@@ -378,11 +380,9 @@ class GCSFeedStorageTest(unittest.TestCase):\n         assert storage.bucket_name == 'mybucket'\n         assert storage.blob_name == 'export.csv'\n \n-    @mock.patch('scrapy.conf.settings',\n-                new={'GCS_PROJECT_ID': 'conf_id', 'FEED_STORAGE_GCS_ACL': '' }, create=True)\n     def test_parse_empty_acl(self):\n         try:\n-            from google.cloud.storage import Client\n+            from google.cloud.storage import Client  # noqa\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n@@ -399,7 +399,7 @@ class GCSFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_store(self):\n         try:\n-            from google.cloud.storage import Client\n+            from google.cloud.storage import Client  # noqa\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n@@ -420,6 +420,7 @@ class GCSFeedStorageTest(unittest.TestCase):\n             bucket_mock.blob.assert_called_once_with('export.csv')\n             blob_mock.upload_from_file.assert_called_once_with(f, predefined_acl=acl)\n \n+\n class StdoutFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b4958358e89b6611f7dd852684ba6d599831fe27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 7 | Churn Cumulative: 537 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,7 +14,6 @@ from tests import get_testdata\n class Base:\n     class LinkExtractorTestCase(unittest.TestCase):\n         extractor_cls = None\n-        escapes_whitespace = False\n \n         def setUp(self):\n             body = get_testdata('link_extractor', 'linkextractor.html')\n@@ -28,10 +27,7 @@ class Base:\n \n         def test_extract_all_links(self):\n             lx = self.extractor_cls()\n-            if self.escapes_whitespace:\n             page4_url = 'http://example.com/page%204.html'\n-            else:\n-                page4_url = 'http://example.com/page 4.html'\n \n             self.assertEqual([link for link in lx.extract_links(self.response)], [\n                 Link(url='http://example.com/sample1.html', text=u''),\n@@ -308,10 +304,7 @@ class Base:\n \n         def test_attrs(self):\n             lx = self.extractor_cls(attrs=\"href\")\n-            if self.escapes_whitespace:\n             page4_url = 'http://example.com/page%204.html'\n-            else:\n-                page4_url = 'http://example.com/page 4.html'\n \n             self.assertEqual(lx.extract_links(self.response), [\n                 Link(url='http://example.com/sample1.html', text=u''),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df937d8280fe0781f6cf1715a3a0cd28c6e94eae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 376 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,16 @@ class Response(object_ref):\n         self.request = request\n         self.flags = [] if flags is None else list(flags)\n \n+    @property\n+    def cb_kwargs(self):\n+        try:\n+            return self.request.cb_kwargs\n+        except AttributeError:\n+            raise AttributeError(\n+                \"Response.cb_kwargs not available, this response \"\n+                \"is not tied to any request\"\n+            )\n+\n     @property\n     def meta(self):\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ff9eb90ea9d533d3f960db75071f0fe638503ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -28/0 | Churn Δ: 6 | Churn Cumulative: 824 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -72,6 +72,12 @@ class BaseResponseTest(unittest.TestCase):\n         r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\", request=req)\n         assert r1.meta is req.meta\n \n+    def test_copy_cb_kwargs(self):\n+        req = Request(\"http://www.example.com\")\n+        req.cb_kwargs['foo'] = 'bar'\n+        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\", request=req)\n+        assert r1.cb_kwargs is req.cb_kwargs\n+\n     def test_copy_inherited_classes(self):\n         \"\"\"Test Response children copies preserve their class\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43b43654a1dacae7b63fc067dc69929c262d9a15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 834 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,6 +78,16 @@ class BaseResponseTest(unittest.TestCase):\n         r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\", request=req)\n         assert r1.cb_kwargs is req.cb_kwargs\n \n+    def test_unavailable_meta(self):\n+        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n+        with self.assertRaisesRegex(AttributeError, r'Response\\.meta not available'):\n+            r1.meta\n+\n+    def test_unavailable_cb_kwargs(self):\n+        r1 = self.response_class(\"http://www.example.com\", body=b\"Some body\")\n+        with self.assertRaisesRegex(AttributeError, r'Response\\.cb_kwargs not available'):\n+            r1.cb_kwargs\n+\n     def test_copy_inherited_classes(self):\n         \"\"\"Test Response children copies preserve their class\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5834088e670d93b2a63ad8afb258e687af0a9b88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 58 | Files Changed: 2 | Hunks: 24 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 114 | Churn Cumulative: 2326 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -207,8 +207,7 @@ class BaseSettings(MutableMapping):\n         If it is a string it will be evaluated as JSON, or as a comma-separated\n         list of strings as a fallback.\n \n-        For example, settings populated through environment variables will\n-        return:\n+        For example, settings populated from the command line will return:\n \n         -   ``OrdetedDict([('key1', 'value1'), ('key2', 'value2')])`` if set to\n             ``'{\"key1\": \"value1\", \"key2\": \"value2\"}'``\n@@ -223,7 +222,7 @@ class BaseSettings(MutableMapping):\n         \"\"\"\n         value = self.get(name, default)\n         if value is None:\n-            return {}\n+            return OrderedDict()\n         if isinstance(value, str):\n             try:\n                 return json.loads(value, object_pairs_hook=OrderedDict)\n\n@@ -5,6 +5,7 @@ import warnings\n import tempfile\n import shutil\n import string\n+import sys\n from collections import OrderedDict\n from io import BytesIO\n from pathlib import Path\n@@ -12,6 +13,7 @@ from unittest import mock\n from urllib.parse import urljoin, urlparse, quote\n from urllib.request import pathname2url\n \n+import pytest\n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n from twisted.internet import defer\n@@ -590,78 +592,75 @@ class FeedExportTest(unittest.TestCase):\n         yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n-        # edge case: FEED_EXPORT_FIELDS==[] means the same as default None\n+    @defer.inlineCallbacks\n+    def test_export_items_empty_field_list(self):\n+        # FEED_EXPORT_FIELDS==[] means the same as default None\n+        items = [{'foo': 'bar'}]\n+        header = [\"foo\"]\n+        rows = [{'foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': []}\n-        yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n-        yield self.assertExportedJsonLines(items, rows_jl, settings)\n+        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+        yield self.assertExportedJsonLines(items, rows, settings)\n \n-        # it is possible to override fields using FEED_EXPORT_FIELDS\n-        header = [\"foo\", \"baz\", \"hello\"]\n+    @defer.inlineCallbacks\n+    def test_export_items_field_list(self):\n+        items = [{'foo': 'bar'}]\n+        header = [\"foo\", \"baz\"]\n+        rows = [{'foo': 'bar', 'baz': ''}]\n         settings = {'FEED_EXPORT_FIELDS': header}\n-        rows = [\n-            {'foo': 'bar1', 'baz': '',      'hello': ''},\n-            {'foo': 'bar2', 'baz': '',      'hello': 'world2'},\n-            {'foo': 'bar3', 'baz': 'quux3', 'hello': ''},\n-            {'foo': '',     'baz': '',      'hello': 'world4'},\n-        ]\n-        yield self.assertExported(items, header, rows,\n-                                  settings=settings, ordered=True)\n+        yield self.assertExported(items, header, rows, settings=settings)\n \n-        # fields may be defined as a comma-separated list\n-        header = [\"foo\", \"baz\", \"hello\"]\n+    @defer.inlineCallbacks\n+    def test_export_items_comma_separated_field_list(self):\n+        items = [{'foo': 'bar'}]\n+        header = [\"foo\", \"baz\"]\n+        rows = [{'foo': 'bar', 'baz': ''}]\n         settings = {'FEED_EXPORT_FIELDS': \",\".join(header)}\n-        rows = [\n-            {'foo': 'bar1', 'baz': '',      'hello': ''},\n-            {'foo': 'bar2', 'baz': '',      'hello': 'world2'},\n-            {'foo': 'bar3', 'baz': 'quux3', 'hello': ''},\n-            {'foo': '',     'baz': '',      'hello': 'world4'},\n-        ]\n-        yield self.assertExported(items, header, rows,\n-                                  settings=settings, ordered=True)\n+        yield self.assertExported(items, header, rows, settings=settings)\n \n-        # fields may also be defined as a JSON array\n-        header = [\"foo\", \"baz\", \"hello\"]\n+    @defer.inlineCallbacks\n+    def test_export_items_json_field_list(self):\n+        items = [{'foo': 'bar'}]\n+        header = [\"foo\", \"baz\"]\n+        rows = [{'foo': 'bar', 'baz': ''}]\n         settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}\n-        rows = [\n-            {'foo': 'bar1', 'baz': '',      'hello': ''},\n-            {'foo': 'bar2', 'baz': '',      'hello': 'world2'},\n-            {'foo': 'bar3', 'baz': 'quux3', 'hello': ''},\n-            {'foo': '',     'baz': '',      'hello': 'world4'},\n-        ]\n-        yield self.assertExported(items, header, rows,\n-                                  settings=settings, ordered=True)\n+        yield self.assertExported(items, header, rows, settings=settings)\n \n-        # custom output field names can be specified\n+    @defer.inlineCallbacks\n+    def test_export_items_field_names(self):\n+        items = [{'foo': 'bar'}]\n         header = OrderedDict((\n             (\"foo\", \"Foo\"),\n-            (\"baz\", \"Baz\"),\n-            (\"hello\", \"Hello\"),\n         ))\n+        rows = [{'Foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': header}\n-        rows = [\n-            {'Foo': 'bar1', 'Baz': '',      'Hello': ''},\n-            {'Foo': 'bar2', 'Baz': '',      'Hello': 'world2'},\n-            {'Foo': 'bar3', 'Baz': 'quux3', 'Hello': ''},\n-            {'Foo': '',     'Baz': '',      'Hello': 'world4'},\n-        ]\n         yield self.assertExported(items, list(header.values()), rows,\n-                                  settings=settings, ordered=True)\n+                                  settings=settings)\n \n-        # custom output field names can be specified as a JSON object\n+    @pytest.mark.skipif(sys.version_info < (3, 7),\n+                        reason='Only official in Python 3.7+')\n+    @defer.inlineCallbacks\n+    def test_export_items_dict_field_names(self):\n+        items = [{'foo': 'bar'}]\n+        header = {\n+            'baz': 'Baz',\n+            'foo': 'Foo',\n+        }\n+        rows = [{'Baz': '', 'Foo': 'bar'}]\n+        settings = {'FEED_EXPORT_FIELDS': header}\n+        yield self.assertExported(items, ['Baz', 'Foo'], rows,\n+                                  settings=settings)\n+\n+    @defer.inlineCallbacks\n+    def test_export_items_json_field_names(self):\n+        items = [{'foo': 'bar'}]\n         header = OrderedDict((\n             (\"foo\", \"Foo\"),\n-            (\"baz\", \"Baz\"),\n-            (\"hello\", \"Hello\"),\n         ))\n+        rows = [{'Foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}\n-        rows = [\n-            {'Foo': 'bar1', 'Baz': '',      'Hello': ''},\n-            {'Foo': 'bar2', 'Baz': '',      'Hello': 'world2'},\n-            {'Foo': 'bar3', 'Baz': 'quux3', 'Hello': ''},\n-            {'Foo': '',     'Baz': '',      'Hello': 'world4'},\n-        ]\n         yield self.assertExported(items, list(header.values()), rows,\n-                                  settings=settings, ordered=True)\n+                                  settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_dicts(self):\n@@ -697,7 +696,7 @@ class FeedExportTest(unittest.TestCase):\n                 {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n             ]\n             yield self.assertExported(items, ['foo', 'baz', 'egg'], rows,\n-                                      settings=settings, ordered=True)\n+                                      settings=settings)\n \n             # export a subset of columns\n             settings = {'FEED_EXPORT_FIELDS': 'egg,baz'}\n@@ -706,7 +705,7 @@ class FeedExportTest(unittest.TestCase):\n                 {'egg': 'spam2', 'baz': 'quux2'}\n             ]\n             yield self.assertExported(items, ['egg', 'baz'], rows,\n-                                      settings=settings, ordered=True)\n+                                      settings=settings)\n \n     @defer.inlineCallbacks\n     def test_export_encoding(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
