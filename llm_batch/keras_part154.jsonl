{"custom_id": "keras#011c1faeb4429cd9ae4332a32be662254b706688", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 81 | Files Changed: 5 | Hunks: 43 | Methods Changed: 17 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 144 | Churn Cumulative: 26647 | Contributors (this commit): 114 | Commits (past 90d): 69 | Contributors (cumulative): 158 | DMM Complexity: 0.0\n\nDIFF:\n@@ -288,7 +288,6 @@ class Layer(object):\n \n     def __init__(self, **kwargs):\n         self.input_spec = None\n-        self.uses_learning_phase = False\n         self.supports_masking = False\n \n         # These properties will be set upon call of self.build()\n@@ -303,15 +302,16 @@ class Layer(object):\n         self.outbound_nodes = []\n \n         # These properties should be set by the user via keyword arguments.\n-        # note that 'input_dtype', 'input_shape' and 'batch_input_shape'\n+        # note that 'dtype', 'input_shape' and 'batch_input_shape'\n         # are only applicable to input layers: do not pass these keywords\n         # to non-input layers.\n         allowed_kwargs = {'input_shape',\n                           'batch_input_shape',\n                           'batch_size',\n-                          'input_dtype',\n+                          'dtype',\n                           'name',\n-                          'trainable'}\n+                          'trainable',\n+                          'weights'}\n         for kwarg in kwargs.keys():\n             if kwarg not in allowed_kwargs:\n                 raise TypeError('Keyword argument not understood:', kwarg)\n@@ -334,8 +334,12 @@ class Layer(object):\n                     batch_size = None\n                 batch_input_shape = (batch_size,) + tuple(kwargs['input_shape'])\n             self.batch_input_shape = batch_input_shape\n-            input_dtype = kwargs.get('input_dtype', K.floatx())\n-            self.input_dtype = input_dtype\n+            dtype = kwargs.get('dtype', K.floatx())\n+            self.dtype = dtype\n+        if 'weights' in kwargs:\n+            self._initial_weights = kwargs['weights']\n+        else:\n+            self._initial_weights = None\n \n     @property\n     def trainable_weights(self):\n@@ -361,25 +365,6 @@ class Layer(object):\n     def non_trainable_weights(self, weights):\n         self._non_trainable_weights = weights\n \n-    def create_input_layer(self, batch_input_shape,\n-                           input_dtype=None, name=None):\n-        if not name:\n-            prefix = _to_snake_case(self.__class__.__name__) + '_input_'\n-            name = prefix + str(K.get_uid(prefix))\n-        if not input_dtype:\n-            input_dtype = K.floatx()\n-\n-        self.batch_input_shape = batch_input_shape\n-        self.input_dtype = input_dtype\n-\n-        # Instantiate the input layer.\n-        x = Input(batch_shape=batch_input_shape,\n-                  dtype=input_dtype, name=name)\n-        # This will build the current layer\n-        # and create the node connecting the current layer\n-        # to the input layer we just created.\n-        self(x)\n-\n     def add_weight(self, shape, initializer,\n                    name=None,\n                    trainable=True,\n@@ -529,6 +514,10 @@ class Layer(object):\n                 self.build(input_shapes)\n             self.built = True\n \n+            # Load weights that were specified at layer instantiation.\n+            if self._initial_weights is not None:\n+                self.set_weights(self._initial_weights)\n+\n         # Raise exceptions in case the input is not compatible\n         # with the input_spec set at build time.\n         self.assert_input_compatibility(x)\n@@ -610,9 +599,8 @@ class Layer(object):\n         # Update tensor history, _keras_shape and _uses_learning_phase.\n         for i in range(len(output_tensors)):\n             output_tensors[i]._keras_shape = output_shapes[i]\n-            uses_lp = (any([x._uses_learning_phase for x in input_tensors]) or\n-                       self.uses_learning_phase)\n-            output_tensors[i]._uses_learning_phase = uses_lp\n+            uses_lp = any([x._uses_learning_phase for x in input_tensors])\n+            output_tensors[i]._uses_learning_phase = getattr(output_tensors[i], '_uses_learning_phase', False) or uses_lp\n             output_tensors[i]._keras_history = (self,\n                                                 len(self.inbound_nodes) - 1,\n                                                 i)\n@@ -1026,8 +1014,8 @@ class Layer(object):\n                   'trainable': self.trainable}\n         if hasattr(self, 'batch_input_shape'):\n             config['batch_input_shape'] = self.batch_input_shape\n-        if hasattr(self, 'input_dtype'):\n-            config['input_dtype'] = self.input_dtype\n+        if hasattr(self, 'dtype'):\n+            config['dtype'] = self.dtype\n         return config\n \n     @classmethod\n@@ -1070,12 +1058,12 @@ class InputLayer(Layer):\n \n     It can either wrap an existing tensor (pass an `input_tensor` argument)\n     or create its a placeholder tensor (pass arguments `input_shape`\n-    or `batch_input_shape` as well as `input_dtype`).\n+    or `batch_input_shape` as well as `dtype`).\n \n     # Arguments\n         input_shape: Shape tuple, not including the batch axis.\n         batch_input_shape: Shape tuple, including the batch axis.\n-        input_dtype: Datatype of the input.\n+        dtype: Datatype of the input.\n         input_tensor: Optional tensor to use as layer input\n             instead of creating a placeholder.\n         sparse: Boolean, whether the placeholder created\n@@ -1084,10 +1072,9 @@ class InputLayer(Layer):\n     \"\"\"\n \n     def __init__(self, input_shape=None, batch_input_shape=None,\n-                 input_dtype=None, input_tensor=None, sparse=False, name=None):\n+                 dtype=None, input_tensor=None, sparse=False, name=None):\n         # TODO: call parent's __init__ instead.\n         self.input_spec = None\n-        self.uses_learning_phase = False\n         self.trainable = False\n         self.built = True\n         self._trainable_weights = []\n@@ -1127,18 +1114,18 @@ class InputLayer(Layer):\n         else:\n             batch_input_shape = tuple(batch_input_shape)\n \n-        if not input_dtype:\n+        if not dtype:\n             if input_tensor is None:\n-                input_dtype = K.floatx()\n+                dtype = K.floatx()\n             else:\n-                input_dtype = K.dtype(input_tensor)\n+                dtype = K.dtype(input_tensor)\n \n         self.batch_input_shape = batch_input_shape\n-        self.input_dtype = input_dtype\n+        self.dtype = dtype\n \n         if input_tensor is None:\n             input_tensor = K.placeholder(shape=batch_input_shape,\n-                                         dtype=input_dtype,\n+                                         dtype=dtype,\n                                          sparse=self.sparse,\n                                          name=self.name)\n         else:\n@@ -1160,7 +1147,7 @@ class InputLayer(Layer):\n \n     def get_config(self):\n         config = {'batch_input_shape': self.batch_input_shape,\n-                  'input_dtype': self.input_dtype,\n+                  'dtype': self.dtype,\n                   'sparse': self.sparse,\n                   'name': self.name}\n         return config\n@@ -1220,7 +1207,7 @@ def Input(shape=None, batch_shape=None,\n     if shape and not batch_shape:\n         batch_shape = (None,) + tuple(shape)\n     input_layer = InputLayer(batch_input_shape=batch_shape,\n-                             name=name, input_dtype=dtype,\n+                             name=name, dtype=dtype,\n                              sparse=sparse,\n                              input_tensor=tensor)\n     # Return tensor including _keras_shape and _keras_history.\n@@ -1305,7 +1292,6 @@ class Merge(Layer):\n         self.constraints = {}\n         self._trainable_weights = []\n         self._non_trainable_weights = []\n-        self.uses_learning_phase = False\n         self.input_spec = None  # Compatible with anything.\n         if not name:\n             prefix = self.__class__.__name__.lower()\n@@ -2136,6 +2122,10 @@ class Container(Layer):\n                     losses += layer.get_losses_for(None)\n         return losses\n \n+    @property\n+    def uses_learning_phase(self):\n+        return any([x._uses_learning_phase for x in self.outputs])\n+\n     @property\n     def stateful(self):\n         return any([(hasattr(layer, 'stateful') and layer.stateful) for layer in self.layers])\n@@ -2231,13 +2221,6 @@ class Container(Layer):\n                 specs += layer.input_spec\n         return specs\n \n-    @property\n-    def uses_learning_phase(self):\n-        \"\"\"True if any layer in the graph uses it.\n-        \"\"\"\n-        layers_learning_phase = any([layer.uses_learning_phase for layer in self.layers])\n-        return layers_learning_phase\n-\n     def call(self, input, mask=None):\n         \"\"\"`call` just reapplies all ops in the graph to the new inputs\n         (e.g. build a new computational graph from the provided inputs).\n@@ -2437,13 +2420,13 @@ class Container(Layer):\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n                         if len(computed_tensors) == 1:\n                             shapes = to_list(layer.get_output_shape_for(computed_tensors[0]._keras_shape))\n-                            uses_learning_phase = computed_tensors[0]._uses_learning_phase or layer.uses_learning_phase\n+                            uses_learning_phase = computed_tensors[0]._uses_learning_phase\n                         else:\n                             shapes = to_list(layer.get_output_shape_for([x._keras_shape for x in computed_tensors]))\n-                            uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors]) or layer.uses_learning_phase\n+                            uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n                         for x, s in zip(output_tensors, shapes):\n                             x._keras_shape = s\n-                            x._uses_learning_phase = uses_learning_phase\n+                            x._uses_learning_phase = getattr(x, '_uses_learning_phase', False) or uses_learning_phase\n \n                     # Update tensor_map.\n                     for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):\n\n@@ -93,8 +93,6 @@ class Dropout(Layer):\n         self.rate = rate\n         self.noise_shape = noise_shape\n         self.seed = seed\n-        if 0. < self.rate < 1.:\n-            self.uses_learning_phase = True\n         self.supports_masking = True\n \n     def _get_noise_shape(self, _):\n\n@@ -72,7 +72,7 @@ class Embedding(Layer):\n                  mask_zero=False,\n                  input_length=None,\n                  **kwargs):\n-        kwargs['input_dtype'] = 'int32'\n+        kwargs['dtype'] = 'int32'\n         super(Embedding, self).__init__(**kwargs)\n \n         self.input_dim = input_dim\n\n@@ -9,7 +9,7 @@ from . import backend as K\n from . import optimizers\n from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n-from .engine.topology import get_source_inputs, Node, Layer, Merge\n+from .engine.topology import get_source_inputs, Node, Layer, Merge, Input\n \n \n def save_model(model, filepath, overwrite=True):\n@@ -290,12 +290,13 @@ class Sequential(Model):\n                                      'Sequential model must '\n                                      'get an `input_shape` or '\n                                      '`batch_input_shape` argument.')\n-                batch_input_shape = layer.batch_input_shape\n-                if hasattr(layer, 'input_dtype'):\n-                    input_dtype = layer.input_dtype\n-                else:\n-                    input_dtype = None\n-                layer.create_input_layer(batch_input_shape, input_dtype)\n+                # Instantiate the input layer.\n+                x = Input(batch_shape=layer.batch_input_shape,\n+                          dtype=layer.dtype, name=layer.name + '_input')\n+                # This will build the current layer\n+                # and create the node connecting the current layer\n+                # to the input layer we just created.\n+                layer(x)\n \n             if len(layer.inbound_nodes) != 1:\n                 raise ValueError('A layer added to a Sequential model must '\n\n@@ -10,35 +10,35 @@ from ..models import model_from_json\n from .. import backend as K\n \n \n-def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n+def get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n                   output_shape=(2,),\n-                  classification=True, nb_class=2):\n+                  classification=True, num_class=2):\n     \"\"\"Generates test data to train a model on.\n \n     classification=True overrides output_shape\n     (i.e. output_shape is set to (1,)) and the output\n-    consists in integers in [0, nb_class-1].\n+    consists in integers in [0, num_class-1].\n \n     Otherwise: float output with shape output_shape.\n     \"\"\"\n-    nb_sample = nb_train + nb_test\n+    num_sample = num_train + num_test\n     if classification:\n-        y = np.random.randint(0, nb_class, size=(nb_sample,))\n-        X = np.zeros((nb_sample,) + input_shape)\n-        for i in range(nb_sample):\n+        y = np.random.randint(0, num_class, size=(num_sample,))\n+        X = np.zeros((num_sample,) + input_shape)\n+        for i in range(num_sample):\n             X[i] = np.random.normal(loc=y[i], scale=0.7, size=input_shape)\n     else:\n-        y_loc = np.random.random((nb_sample,))\n-        X = np.zeros((nb_sample,) + input_shape)\n-        y = np.zeros((nb_sample,) + output_shape)\n-        for i in range(nb_sample):\n+        y_loc = np.random.random((num_sample,))\n+        X = np.zeros((num_sample,) + input_shape)\n+        y = np.zeros((num_sample,) + output_shape)\n+        for i in range(num_sample):\n             X[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=input_shape)\n             y[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=output_shape)\n \n-    return (X[:nb_train], y[:nb_train]), (X[nb_train:], y[nb_train:])\n+    return (X[:num_train], y[:num_train]), (X[num_train:], y[num_train:])\n \n \n-def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n+def layer_test(layer_cls, kwargs={}, input_shape=None, dtype=None,\n                input_data=None, expected_output=None,\n                expected_output_dtype=None, fixed_batch_size=False):\n     \"\"\"Test routine for a layer with a single input tensor\n@@ -46,19 +46,19 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     \"\"\"\n     if input_data is None:\n         assert input_shape\n-        if not input_dtype:\n-            input_dtype = K.floatx()\n+        if not dtype:\n+            dtype = K.floatx()\n         input_data_shape = list(input_shape)\n         for i, e in enumerate(input_data_shape):\n             if e is None:\n                 input_data_shape[i] = np.random.randint(1, 4)\n         input_data = (10 * np.random.random(input_data_shape))\n-        input_data = input_data.astype(input_dtype)\n+        input_data = input_data.astype(dtype)\n     elif input_shape is None:\n         input_shape = input_data.shape\n \n     if expected_output_dtype is None:\n-        expected_output_dtype = input_dtype\n+        expected_output_dtype = dtype\n \n     # instantiation\n     layer = layer_cls(**kwargs)\n@@ -74,9 +74,9 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     # test in functional API\n     if fixed_batch_size:\n-        x = Input(batch_shape=input_shape, dtype=input_dtype)\n+        x = Input(batch_shape=input_shape, dtype=dtype)\n     else:\n-        x = Input(shape=input_shape[1:], dtype=input_dtype)\n+        x = Input(shape=input_shape[1:], dtype=dtype)\n     y = layer(x)\n     assert K.dtype(y) == expected_output_dtype\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca23406974546c5214a3ec73eb903952c14ab2f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 68 | Files Changed: 2 | Hunks: 25 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 121 | Churn Cumulative: 423 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -9,74 +9,62 @@ from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n+import keras\n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Convolution2D, MaxPooling2D\n-from keras.utils import np_utils\n+from keras.layers import Dense, Dropout, Flatten\n+from keras.layers import Conv2D, MaxPooling2D\n from keras import backend as K\n \n batch_size = 128\n-nb_classes = 10\n-nb_epoch = 12\n+num_classes = 10\n+num_epoch = 12\n \n # input image dimensions\n img_rows, img_cols = 28, 28\n-# number of convolutional filters to use\n-nb_filters = 32\n-# size of pooling area for max pooling\n-pool_size = (2, 2)\n-# convolution kernel size\n-kernel_size = (3, 3)\n \n # the data, shuffled and split between train and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n if K.image_data_format() == 'channels_first':\n-    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n-    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n+    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n+    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n     input_shape = (1, img_rows, img_cols)\n else:\n-    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n-    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n+    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n+    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n     input_shape = (img_rows, img_cols, 1)\n \n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n-print('X_train shape:', X_train.shape)\n-print(X_train.shape[0], 'train samples')\n-print(X_test.shape[0], 'test samples')\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+print('x_train shape:', x_train.shape)\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n-\n-model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n-                        border_mode='valid',\n+model.add(Conv2D(32, kernel_size=(3, 3),\n+                 activation='relu',\n                  input_shape=input_shape))\n-model.add(Activation('relu'))\n-model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=pool_size))\n+model.add(Conv2D(64, (3, 3), activation='relu'))\n+model.add(MaxPooling2D(pool_size=(2, 2)))\n model.add(Dropout(0.25))\n-\n model.add(Flatten())\n-model.add(Dense(128))\n-model.add(Activation('relu'))\n+model.add(Dense(128, activation='relu'))\n model.add(Dropout(0.5))\n-model.add(Dense(nb_classes))\n-model.add(Activation('softmax'))\n+model.add(Dense(num_classes, activation='softmax'))\n \n-model.compile(loss='categorical_crossentropy',\n-              optimizer='adadelta',\n+model.compile(loss=keras.losses.categorical_crossentropy,\n+              optimizer=keras.optimizers.Adadelta(),\n               metrics=['accuracy'])\n \n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n-          verbose=1, validation_data=(X_test, Y_test))\n-score = model.evaluate(X_test, Y_test, verbose=0)\n-print('Test score:', score[0])\n+model.fit(x_train, y_train, batch_size=batch_size, num_epoch=num_epoch,\n+          verbose=1, validation_data=(x_test, y_test))\n+score = model.evaluate(x_test, y_test, verbose=0)\n+print('Test loss:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -11,40 +11,37 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n+from keras.layers import Dense, Dropout, Activation\n from keras.optimizers import RMSprop\n from keras.utils import np_utils\n \n \n batch_size = 128\n-nb_classes = 10\n-nb_epoch = 20\n+num_classes = 10\n+num_epoch = 20\n \n # the data, shuffled and split between train and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n-X_train = X_train.reshape(60000, 784)\n-X_test = X_test.reshape(10000, 784)\n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n-print(X_train.shape[0], 'train samples')\n-print(X_test.shape[0], 'test samples')\n+x_train = x_train.reshape(60000, 784)\n+x_test = x_test.reshape(10000, 784)\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+y_train = np_utils.to_categorical(y_train, num_classes)\n+y_test = np_utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n-model.add(Dense(512, input_shape=(784,)))\n-model.add(Activation('relu'))\n+model.add(Dense(512, activation='relu', input_shape=(784,)))\n model.add(Dropout(0.2))\n-model.add(Dense(512))\n-model.add(Activation('relu'))\n+model.add(Dense(512, activation='relu'))\n model.add(Dropout(0.2))\n-model.add(Dense(10))\n-model.add(Activation('softmax'))\n+model.add(Dense(10, activation='softmax'))\n \n model.summary()\n \n@@ -52,9 +49,9 @@ model.compile(loss='categorical_crossentropy',\n               optimizer=RMSprop(),\n               metrics=['accuracy'])\n \n-history = model.fit(X_train, Y_train,\n-                    batch_size=batch_size, nb_epoch=nb_epoch,\n-                    verbose=1, validation_data=(X_test, Y_test))\n-score = model.evaluate(X_test, Y_test, verbose=0)\n-print('Test score:', score[0])\n+history = model.fit(x_train, y_train,\n+                    batch_size=batch_size, num_epoch=num_epoch,\n+                    verbose=1, validation_data=(x_test, y_test))\n+score = model.evaluate(x_test, y_test, verbose=0)\n+print('Test loss:', score[0])\n print('Test accuracy:', score[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc11aa99c0c1427f0bdaf7a5434d4e272ec7fa66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 6 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,6 @@\n+from __future__ import absolute_import\n+from . import np_utils\n+from . import conv_utils\n+from . import data_utils\n+from . import generic_utils\n+from . import io_utils\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1a5fbf7f67b1052e4f8b945c87d90ab59404a48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 9317 | Contributors (this commit): 33 | Commits (past 90d): 23 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1118,7 +1118,8 @@ class UpSampling2D(Layer):\n                                self.data_format)\n \n     def get_config(self):\n-        config = {'size': self.size}\n+        config = {'size': self.size,\n+                  'data_format': self.data_format}\n         base_config = super(UpSampling2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1191,7 +1192,8 @@ class UpSampling3D(Layer):\n                                 self.data_format)\n \n     def get_config(self):\n-        config = {'size': self.size}\n+        config = {'size': self.size,\n+                  'data_format': self.data_format}\n         base_config = super(UpSampling3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1291,7 +1293,8 @@ class ZeroPadding2D(Layer):\n             self.padding = ((padding, padding), (padding, padding))\n         elif hasattr(padding, '__len__'):\n             if len(padding) != 2:\n-                raise ValueError('TODO')\n+                raise ValueError('`padding` should have two elements. '\n+                                 'Found: ' + str(padding))\n             height_padding = conv_utils.normalize_tuple(padding[0], 2,\n                                                         '1st entry of padding')\n             width_padding = conv_utils.normalize_tuple(padding[1], 2,\n@@ -1333,7 +1336,8 @@ class ZeroPadding2D(Layer):\n                                                data_format=self.data_format)\n \n     def get_config(self):\n-        config = {'padding': self.padding}\n+        config = {'padding': self.padding,\n+                  'data_format': self.data_format}\n         base_config = super(ZeroPadding2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1385,7 +1389,8 @@ class ZeroPadding3D(Layer):\n             self.padding = ((padding, padding), (padding, padding), (padding, padding))\n         elif hasattr(padding, '__len__'):\n             if len(padding) != 3:\n-                raise ValueError('TODO')\n+                raise ValueError('`padding` should have 3 elements. '\n+                                 'Found: ' + str(padding))\n             dim1_padding = conv_utils.normalize_tuple(padding[0], 2,\n                                                       '1st entry of padding')\n             dim2_padding = conv_utils.normalize_tuple(padding[1], 2,\n@@ -1431,7 +1436,8 @@ class ZeroPadding3D(Layer):\n                                     data_format=self.data_format)\n \n     def get_config(self):\n-        config = {'padding': self.padding}\n+        config = {'padding': self.padding,\n+                  'data_format': self.data_format}\n         base_config = super(ZeroPadding3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1548,7 +1554,8 @@ class Cropping2D(Layer):\n             self.cropping = ((cropping, cropping), (cropping, cropping))\n         elif hasattr(cropping, '__len__'):\n             if len(cropping) != 2:\n-                raise ValueError('TODO')\n+                raise ValueError('`cropping` should have two elements. '\n+                                 'Found: ' + str(padding))\n             height_cropping = conv_utils.normalize_tuple(\n                 cropping[0], 2,\n                 '1st entry of cropping')\n@@ -1622,7 +1629,8 @@ class Cropping2D(Layer):\n                      :]\n \n     def get_config(self):\n-        config = {'cropping': self.cropping}\n+        config = {'cropping': self.cropping,\n+                  'data_format': self.data_format}\n         base_config = super(Cropping2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1677,7 +1685,8 @@ class Cropping3D(Layer):\n                              (cropping, cropping))\n         elif hasattr(cropping, '__len__'):\n             if len(cropping) != 3:\n-                raise ValueError('TODO')\n+                raise ValueError('`cropping` should have 3 elements. '\n+                                 'Found: ' + str(padding))\n             dim1_cropping = conv_utils.normalize_tuple(cropping[0], 2,\n                                                        '1st entry of cropping')\n             dim2_cropping = conv_utils.normalize_tuple(cropping[1], 2,\n@@ -1767,6 +1776,7 @@ class Cropping3D(Layer):\n                      self.cropping[0][0]:-self.cropping[0][1],\n                      self.cropping[1][0]:-self.cropping[1][1],\n                      self.cropping[2][0]:-self.cropping[2][1]]\n+\n         elif self.data_format == 'channels_last':\n             if self.cropping[0][1] == self.cropping[1][1] == self.cropping[2][1] == 0:\n                 return x[:,\n@@ -1817,7 +1827,8 @@ class Cropping3D(Layer):\n                      :]\n \n     def get_config(self):\n-        config = {'cropping': self.cropping}\n+        config = {'cropping': self.cropping,\n+                  'data_format': self.data_format}\n         base_config = super(Cropping3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10bfb1c565582803982e2205d41b095bc929d4a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1081 | Lines Deleted: 1031 | Files Changed: 83 | Hunks: 758 | Methods Changed: 219 | Complexity Δ (Sum/Max): -2/2 | Churn Δ: 2112 | Churn Cumulative: 92251 | Contributors (this commit): 306 | Commits (past 90d): 562 | Contributors (cumulative): 824 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,7 +152,7 @@ print('Build model...')\n model = Sequential()\n # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n # Note: In a situation where your input sequences have a variable length,\n-# use input_shape=(None, nb_feature).\n+# use input_shape=(None, num_feature).\n model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n # As the decoder RNN's input, repeatedly provide with the last hidden state of\n # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n@@ -161,7 +161,7 @@ model.add(RepeatVector(DIGITS + 1))\n # The decoder RNN could be multiple layers stacked or a single layer.\n for _ in range(LAYERS):\n     # By setting return_sequences to True, return not only the last output but\n-    # all the outputs so far in the form of (nb_samples, timesteps,\n+    # all the outputs so far in the form of (num_samples, timesteps,\n     # output_dim). This is necessary as TimeDistributed in the below expects\n     # the first dimension to be the timesteps.\n     model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n@@ -181,7 +181,7 @@ for iteration in range(1, 200):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1,\n+    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1,\n               validation_data=(X_val, y_val))\n     # Select 10 samples from the validation set at random so we can visualize\n     # errors.\n\n@@ -46,7 +46,7 @@ class Antirectifier(Layer):\n         classification accuracy as an equivalent ReLU-based network.\n     '''\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         shape = list(input_shape)\n         assert len(shape) == 2  # only valid for 2D tensors\n         shape[-1] *= 2\n@@ -61,8 +61,8 @@ class Antirectifier(Layer):\n \n # global parameters\n batch_size = 128\n-nb_classes = 10\n-nb_epoch = 40\n+num_classes = 10\n+epochs = 40\n \n # the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n@@ -77,8 +77,8 @@ print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+Y_train = np_utils.to_categorical(y_train, num_classes)\n+Y_test = np_utils.to_categorical(y_test, num_classes)\n \n # build the model\n model = Sequential()\n@@ -98,7 +98,7 @@ model.compile(loss='categorical_crossentropy',\n \n # train the model\n model.fit(X_train, Y_train,\n-          batch_size=batch_size, nb_epoch=nb_epoch,\n+          batch_size=batch_size, epochs=epochs,\n           verbose=1, validation_data=(X_test, Y_test))\n \n # next, compare with an equivalent network\n\n@@ -206,5 +206,5 @@ answer.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n # Note: you could use a Graph model to avoid repeat the input twice\n answer.fit([inputs_train, queries_train, inputs_train], answers_train,\n            batch_size=32,\n-           nb_epoch=120,\n+           epochs=120,\n            validation_data=([inputs_test, queries_test, inputs_test], answers_test))\n\n@@ -206,6 +206,6 @@ model.compile(optimizer='adam',\n               metrics=['accuracy'])\n \n print('Training')\n-model.fit([X, Xq], Y, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, validation_split=0.05)\n+model.fit([X, Xq], Y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.05)\n loss, acc = model.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE)\n print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n\n@@ -16,8 +16,8 @@ from keras.layers import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n \n batch_size = 32\n-nb_classes = 10\n-nb_epoch = 200\n+num_classes = 10\n+epochs = 200\n data_augmentation = True\n \n # input image dimensions\n@@ -32,8 +32,8 @@ print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n # Convert class vectors to binary class matrices.\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+Y_train = np_utils.to_categorical(y_train, num_classes)\n+Y_test = np_utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n \n@@ -56,7 +56,7 @@ model.add(Flatten())\n model.add(Dense(512))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n-model.add(Dense(nb_classes))\n+model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n \n # Let's train the model using RMSprop\n@@ -73,7 +73,7 @@ if not data_augmentation:\n     print('Not using data augmentation.')\n     model.fit(X_train, Y_train,\n               batch_size=batch_size,\n-              nb_epoch=nb_epoch,\n+              epochs=epochs,\n               validation_data=(X_test, Y_test),\n               shuffle=True)\n else:\n@@ -99,5 +99,5 @@ else:\n     model.fit_generator(datagen.flow(X_train, Y_train,\n                                      batch_size=batch_size),\n                         samples_per_epoch=X_train.shape[0],\n-                        nb_epoch=nb_epoch,\n+                        epochs=epochs,\n                         validation_data=(X_test, Y_test))\n\n@@ -14,24 +14,24 @@ import pylab as plt\n # of identical shape.\n \n seq = Sequential()\n-seq.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n+seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n                    input_shape=(None, 40, 40, 1),\n                    border_mode='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n+seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n                    border_mode='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n+seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n                    border_mode='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n+seq.add(ConvLSTM2D(filters=40, num_row=3, num_col=3,\n                    border_mode='same', return_sequences=True))\n seq.add(BatchNormalization())\n \n-seq.add(Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3,\n+seq.add(Convolution3D(filters=1, kernel_dim1=1, kernel_dim2=3,\n                       kernel_dim3=3, activation='sigmoid',\n                       border_mode='same', data_format='channels_last'))\n \n@@ -101,7 +101,7 @@ def generate_movies(n_samples=1200, n_frames=15):\n # Train the network\n noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n seq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n-        nb_epoch=300, validation_split=0.05)\n+        epochs=300, validation_split=0.05)\n \n # Testing the network on one movie\n # feed it with the first 7 positions and then\n\n@@ -403,7 +403,7 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n     val_words = int(words_per_epoch * (val_split))\n \n     # Network parameters\n-    conv_num_filters = 16\n+    conv_filterss = 16\n     filter_size = 3\n     pool_size = 2\n     time_dense_size = 32\n@@ -427,14 +427,14 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n                                  )\n     act = 'relu'\n     input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n-    inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n+    inner = Convolution2D(conv_filterss, filter_size, filter_size, border_mode='same',\n                           activation=act, init='he_normal', name='conv1')(input_data)\n     inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n-    inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n+    inner = Convolution2D(conv_filterss, filter_size, filter_size, border_mode='same',\n                           activation=act, init='he_normal', name='conv2')(inner)\n     inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n \n-    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_num_filters)\n+    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filterss)\n     inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n \n     # cuts down input size going into RNN:\n@@ -477,7 +477,7 @@ def train(run_name, start_epoch, stop_epoch, img_w):\n     viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n \n     model.fit_generator(generator=img_gen.next_train(), samples_per_epoch=(words_per_epoch - val_words),\n-                        nb_epoch=stop_epoch, validation_data=img_gen.next_val(), nb_val_samples=val_words,\n+                        epochs=stop_epoch, validation_data=img_gen.next_val(), num_val_samples=val_words,\n                         callbacks=[viz_cb, img_gen], initial_epoch=start_epoch)\n \n \n\n@@ -19,7 +19,7 @@ maxlen = 100  # cut texts after this number of words (among top max_features mos\n batch_size = 32\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n@@ -43,5 +43,5 @@ model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n print('Train...')\n model.fit(X_train, y_train,\n           batch_size=batch_size,\n-          nb_epoch=4,\n+          epochs=4,\n           validation_data=[X_test, y_test])\n\n@@ -23,13 +23,13 @@ max_features = 5000\n maxlen = 400\n batch_size = 32\n embedding_dims = 50\n-nb_filter = 250\n+filters = 250\n filter_length = 3\n hidden_dims = 250\n-nb_epoch = 2\n+epochs = 2\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n@@ -49,9 +49,9 @@ model.add(Embedding(max_features,\n                     input_length=maxlen,\n                     dropout=0.2))\n \n-# we add a Convolution1D, which will learn nb_filter\n+# we add a Convolution1D, which will learn filters\n # word group filters of size filter_length:\n-model.add(Convolution1D(nb_filter=nb_filter,\n+model.add(Convolution1D(filters=filters,\n                         filter_length=filter_length,\n                         border_mode='valid',\n                         activation='relu',\n@@ -73,5 +73,5 @@ model.compile(loss='binary_crossentropy',\n               metrics=['accuracy'])\n model.fit(X_train, y_train,\n           batch_size=batch_size,\n-          nb_epoch=nb_epoch,\n+          epochs=epochs,\n           validation_data=(X_test, y_test))\n\n@@ -23,7 +23,7 @@ embedding_size = 128\n \n # Convolution\n filter_length = 5\n-nb_filter = 64\n+filters = 64\n pool_length = 4\n \n # LSTM\n@@ -31,7 +31,7 @@ lstm_output_size = 70\n \n # Training\n batch_size = 30\n-nb_epoch = 2\n+epochs = 2\n \n '''\n Note:\n@@ -40,7 +40,7 @@ Only 2 epochs are needed as the dataset is very small.\n '''\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n@@ -55,7 +55,7 @@ print('Build model...')\n model = Sequential()\n model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n model.add(Dropout(0.25))\n-model.add(Convolution1D(nb_filter=nb_filter,\n+model.add(Convolution1D(filters=filters,\n                         filter_length=filter_length,\n                         border_mode='valid',\n                         activation='relu',\n@@ -70,7 +70,7 @@ model.compile(loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n print('Train...')\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n+model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n           validation_data=(X_test, y_test))\n score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n print('Test score:', score)\n\n@@ -70,10 +70,10 @@ max_features = 20000\n maxlen = 400\n batch_size = 32\n embedding_dims = 50\n-nb_epoch = 5\n+epochs = 5\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n print('Average train sequence length: {}'.format(np.mean(list(map(len, X_train)), dtype=int)))\n@@ -132,5 +132,5 @@ model.compile(loss='binary_crossentropy',\n \n model.fit(X_train, y_train,\n           batch_size=batch_size,\n-          nb_epoch=nb_epoch,\n+          epochs=epochs,\n           validation_data=(X_test, y_test))\n\n@@ -25,7 +25,7 @@ maxlen = 80  # cut texts after this number of words (among top max_features most\n batch_size = 32\n \n print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = imdb.load_data(nb_words=max_features)\n+(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n print(len(x_train), 'train sequences')\n print(len(x_test), 'test sequences')\n \n@@ -48,7 +48,7 @@ model.compile(loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n print('Train...')\n-model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=15,\n+model.fit(x_train, y_train, batch_size=batch_size, epochs=15,\n           validation_data=(x_test, y_test))\n score, acc = model.evaluate(x_test, y_test,\n                             batch_size=batch_size)\n\n@@ -33,7 +33,7 @@ epochs = 10\n modes = ['cpu', 'mem', 'gpu']\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n X_train = sequence.pad_sequences(X_train, max_length)\n X_test = sequence.pad_sequences(X_test, max_length)\n \n@@ -53,7 +53,7 @@ for mode in modes:\n     start_time = time.time()\n     history = model.fit(X_train, y_train,\n                         batch_size=batch_size,\n-                        nb_epoch=epochs,\n+                        epochs=epochs,\n                         validation_data=(X_test, y_test))\n     average_time_per_epoch = (time.time() - start_time) / epochs\n \n\n@@ -73,7 +73,7 @@ for iteration in range(1, 60):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n-    model.fit(X, y, batch_size=128, nb_epoch=1)\n+    model.fit(X, y, batch_size=128, epochs=1)\n \n     start_index = random.randint(0, len(text) - maxlen - 1)\n \n\n@@ -127,7 +127,7 @@ def build_discriminator():\n if __name__ == '__main__':\n \n     # batch and latent size taken from the paper\n-    nb_epochs = 50\n+    epochss = 50\n     batch_size = 100\n     latent_size = 100\n \n@@ -172,21 +172,21 @@ if __name__ == '__main__':\n     X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n     X_test = np.expand_dims(X_test, axis=1)\n \n-    nb_train, nb_test = X_train.shape[0], X_test.shape[0]\n+    num_train, num_test = X_train.shape[0], X_test.shape[0]\n \n     train_history = defaultdict(list)\n     test_history = defaultdict(list)\n \n-    for epoch in range(nb_epochs):\n-        print('Epoch {} of {}'.format(epoch + 1, nb_epochs))\n+    for epoch in range(epochss):\n+        print('Epoch {} of {}'.format(epoch + 1, epochss))\n \n-        nb_batches = int(X_train.shape[0] / batch_size)\n-        progress_bar = Progbar(target=nb_batches)\n+        num_batches = int(X_train.shape[0] / batch_size)\n+        progress_bar = Progbar(target=num_batches)\n \n         epoch_gen_loss = []\n         epoch_disc_loss = []\n \n-        for index in range(nb_batches):\n+        for index in range(num_batches):\n             progress_bar.update(index)\n             # generate a new batch of noise\n             noise = np.random.uniform(-1, 1, (batch_size, latent_size))\n@@ -231,15 +231,15 @@ if __name__ == '__main__':\n         # evaluate the testing loss here\n \n         # generate a new batch of noise\n-        noise = np.random.uniform(-1, 1, (nb_test, latent_size))\n+        noise = np.random.uniform(-1, 1, (num_test, latent_size))\n \n         # sample some labels from p_c and generate images from them\n-        sampled_labels = np.random.randint(0, 10, nb_test)\n+        sampled_labels = np.random.randint(0, 10, num_test)\n         generated_images = generator.predict(\n             [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n \n         X = np.concatenate((X_test, generated_images))\n-        y = np.array([1] * nb_test + [0] * nb_test)\n+        y = np.array([1] * num_test + [0] * num_test)\n         aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n \n         # see if the discriminator can figure itself out...\n@@ -249,10 +249,10 @@ if __name__ == '__main__':\n         discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n \n         # make new noise\n-        noise = np.random.uniform(-1, 1, (2 * nb_test, latent_size))\n-        sampled_labels = np.random.randint(0, 10, 2 * nb_test)\n+        noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n+        sampled_labels = np.random.randint(0, 10, 2 * num_test)\n \n-        trick = np.ones(2 * nb_test)\n+        trick = np.ones(2 * num_test)\n \n         generator_test_loss = combined.evaluate(\n             [noise, sampled_labels.reshape((-1, 1))],\n\n@@ -18,7 +18,7 @@ from keras import backend as K\n \n batch_size = 128\n num_classes = 10\n-num_epoch = 12\n+epochs = 12\n \n # input image dimensions\n img_rows, img_cols = 28, 28\n@@ -63,7 +63,7 @@ model.compile(loss=keras.losses.categorical_crossentropy,\n               optimizer=keras.optimizers.Adadelta(),\n               metrics=['accuracy'])\n \n-model.fit(x_train, y_train, batch_size=batch_size, num_epoch=num_epoch,\n+model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n           verbose=1, validation_data=(x_test, y_test))\n score = model.evaluate(x_test, y_test, verbose=0)\n print('Test loss:', score[0])\n\n@@ -34,8 +34,8 @@ from keras.utils import np_utils\n \n # Training parameters.\n batch_size = 32\n-nb_classes = 10\n-nb_epochs = 5\n+num_classes = 10\n+epochss = 5\n \n # Embedding dimensions.\n row_hidden = 128\n@@ -56,8 +56,8 @@ print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n # Converts class vectors to binary class matrices.\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+Y_train = np_utils.to_categorical(y_train, num_classes)\n+Y_test = np_utils.to_categorical(y_test, num_classes)\n \n row, col, pixel = X_train.shape[1:]\n \n@@ -71,14 +71,14 @@ encoded_rows = TimeDistributed(LSTM(output_dim=row_hidden))(x)\n encoded_columns = LSTM(col_hidden)(encoded_rows)\n \n # Final predictions and model.\n-prediction = Dense(nb_classes, activation='softmax')(encoded_columns)\n+prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n model = Model(input=x, output=prediction)\n model.compile(loss='categorical_crossentropy',\n               optimizer='rmsprop',\n               metrics=['accuracy'])\n \n # Training.\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n+model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochss,\n           verbose=1, validation_data=(X_test, Y_test))\n \n # Evaluation.\n\n@@ -24,8 +24,8 @@ from keras.optimizers import RMSprop\n from keras.utils import np_utils\n \n batch_size = 32\n-nb_classes = 10\n-nb_epochs = 200\n+num_classes = 10\n+epochss = 200\n hidden_units = 100\n \n learning_rate = 1e-6\n@@ -45,8 +45,8 @@ print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+Y_train = np_utils.to_categorical(y_train, num_classes)\n+Y_test = np_utils.to_categorical(y_test, num_classes)\n \n print('Evaluate IRNN...')\n model = Sequential()\n@@ -55,14 +55,14 @@ model.add(SimpleRNN(output_dim=hidden_units,\n                     inner_init=initializers.Identity(gain=1.0),\n                     activation='relu',\n                     input_shape=X_train.shape[1:]))\n-model.add(Dense(nb_classes))\n+model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n model.compile(loss='categorical_crossentropy',\n               optimizer=rmsprop,\n               metrics=['accuracy'])\n \n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n+model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochss,\n           verbose=1, validation_data=(X_test, Y_test))\n \n scores = model.evaluate(X_test, Y_test, verbose=0)\n\n@@ -18,7 +18,7 @@ from keras.utils import np_utils\n \n batch_size = 128\n num_classes = 10\n-num_epoch = 20\n+epochs = 20\n \n # the data, shuffled and split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n@@ -50,7 +50,7 @@ model.compile(loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n history = model.fit(x_train, y_train,\n-                    batch_size=batch_size, num_epoch=num_epoch,\n+                    batch_size=batch_size, epochs=epochs,\n                     verbose=1, validation_data=(x_test, y_test))\n score = model.evaluate(x_test, y_test, verbose=0)\n print('Test loss:', score[0])\n\n@@ -66,7 +66,7 @@ from keras.utils import np_utils\n from keras.datasets import mnist\n \n input_shape = (1, 28, 28)  # image shape\n-nb_class = 10  # number of class\n+num_class = 10  # number of class\n \n \n # load and pre-process data\n@@ -88,17 +88,17 @@ print('validation_x shape:', validation_x.shape,\n \n # knowledge transfer algorithms\n def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n-    '''Get initial weights for a wider conv2d layer with a bigger nb_filter,\n+    '''Get initial weights for a wider conv2d layer with a bigger filters,\n     by 'random-padding' or 'net2wider'.\n \n     # Arguments\n         teacher_w1: `weight` of conv2d layer to become wider,\n-          of shape (nb_filter1, nb_channel1, kh1, kw1)\n+          of shape (filters1, num_channel1, kh1, kw1)\n         teacher_b1: `bias` of conv2d layer to become wider,\n-          of shape (nb_filter1, )\n+          of shape (filters1, )\n         teacher_w2: `weight` of next connected conv2d layer,\n-          of shape (nb_filter2, nb_channel2, kh2, kw2)\n-        new_width: new `nb_filter` for the wider conv2d layer\n+          of shape (filters2, num_channel2, kh2, kw2)\n+        new_width: new `filters` for the wider conv2d layer\n         init: initialization algorithm for new weights,\n           either 'random-pad' or 'net2wider'\n     '''\n@@ -107,7 +107,7 @@ def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n     assert teacher_w1.shape[0] == teacher_b1.shape[0], (\n         'weight and bias from same layer should have compatible shapes')\n     assert new_width > teacher_w1.shape[0], (\n-        'new width (nb_filter) should be bigger than the existing one')\n+        'new width (filters) should be bigger than the existing one')\n \n     n = new_width - teacher_w1.shape[0]\n     if init == 'random-pad':\n@@ -193,13 +193,13 @@ def deeper2net_conv2d(teacher_w):\n \n     # Arguments\n         teacher_w: `weight` of previous conv2d layer,\n-          of shape (nb_filter, nb_channel, kh, kw)\n+          of shape (filters, num_channel, kh, kw)\n     '''\n-    nb_filter, nb_channel, kh, kw = teacher_w.shape\n-    student_w = np.zeros((nb_filter, nb_filter, kh, kw))\n-    for i in xrange(nb_filter):\n+    filters, num_channel, kh, kw = teacher_w.shape\n+    student_w = np.zeros((filters, filters, kh, kw))\n+    for i in xrange(filters):\n         student_w[i, i, (kh - 1) / 2, (kw - 1) / 2] = 1.\n-    student_b = np.zeros(nb_filter)\n+    student_b = np.zeros(filters)\n     return student_w, student_b\n \n \n@@ -213,7 +213,7 @@ def copy_weights(teacher_model, student_model, layer_names):\n \n \n # methods to construct teacher_model and student_models\n-def make_teacher_model(train_data, validation_data, nb_epoch=3):\n+def make_teacher_model(train_data, validation_data, epochs=3):\n     '''Train a simple CNN as teacher model.\n     '''\n     model = Sequential()\n@@ -224,19 +224,19 @@ def make_teacher_model(train_data, validation_data, nb_epoch=3):\n     model.add(MaxPooling2D(name='pool2'))\n     model.add(Flatten(name='flatten'))\n     model.add(Dense(64, activation='relu', name='fc1'))\n-    model.add(Dense(nb_class, activation='softmax', name='fc2'))\n+    model.add(Dense(num_class, activation='softmax', name='fc2'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer=SGD(lr=0.01, momentum=0.9),\n                   metrics=['accuracy'])\n \n     train_x, train_y = train_data\n-    history = model.fit(train_x, train_y, nb_epoch=nb_epoch,\n+    history = model.fit(train_x, train_y, epochs=epochs,\n                         validation_data=validation_data)\n     return model, history\n \n \n def make_wider_student_model(teacher_model, train_data,\n-                             validation_data, init, nb_epoch=3):\n+                             validation_data, init, epochs=3):\n     '''Train a wider student model based on teacher_model,\n        with either 'random-pad' (baseline) or 'net2wider'\n     '''\n@@ -253,7 +253,7 @@ def make_wider_student_model(teacher_model, train_data,\n     model.add(Flatten(name='flatten'))\n     # a wider fc1 compared to teacher model\n     model.add(Dense(new_fc1_width, activation='relu', name='fc1'))\n-    model.add(Dense(nb_class, activation='softmax', name='fc2'))\n+    model.add(Dense(num_class, activation='softmax', name='fc2'))\n \n     # The weights for other layers need to be copied from teacher_model\n     # to student_model, except for widened layers\n@@ -279,13 +279,13 @@ def make_wider_student_model(teacher_model, train_data,\n                   metrics=['accuracy'])\n \n     train_x, train_y = train_data\n-    history = model.fit(train_x, train_y, nb_epoch=nb_epoch,\n+    history = model.fit(train_x, train_y, epochs=epochs,\n                         validation_data=validation_data)\n     return model, history\n \n \n def make_deeper_student_model(teacher_model, train_data,\n-                              validation_data, init, nb_epoch=3):\n+                              validation_data, init, epochs=3):\n     '''Train a deeper student model based on teacher_model,\n        with either 'random-init' (baseline) or 'net2deeper'\n     '''\n@@ -316,7 +316,7 @@ def make_deeper_student_model(teacher_model, train_data,\n         model.add(Dense(64, activation='relu', name='fc1-deeper'))\n     else:\n         raise ValueError('Unsupported weight initializer: %s' % init)\n-    model.add(Dense(nb_class, activation='softmax', name='fc2'))\n+    model.add(Dense(num_class, activation='softmax', name='fc2'))\n \n     # copy weights for other layers\n     copy_weights(teacher_model, model, layer_names=[\n@@ -327,7 +327,7 @@ def make_deeper_student_model(teacher_model, train_data,\n                   metrics=['accuracy'])\n \n     train_x, train_y = train_data\n-    history = model.fit(train_x, train_y, nb_epoch=nb_epoch,\n+    history = model.fit(train_x, train_y, epochs=epochs,\n                         validation_data=validation_data)\n     return model, history\n \n@@ -345,16 +345,16 @@ def net2wider_experiment():\n     print('\\nbuilding teacher model ...')\n     teacher_model, _ = make_teacher_model(train_data,\n                                           validation_data,\n-                                          nb_epoch=3)\n+                                          epochs=3)\n \n     print('\\nbuilding wider student model by random padding ...')\n     make_wider_student_model(teacher_model, train_data,\n                              validation_data, 'random-pad',\n-                             nb_epoch=3)\n+                             epochs=3)\n     print('\\nbuilding wider student model by net2wider ...')\n     make_wider_student_model(teacher_model, train_data,\n                              validation_data, 'net2wider',\n-                             nb_epoch=3)\n+                             epochs=3)\n \n \n def net2deeper_experiment():\n@@ -369,16 +369,16 @@ def net2deeper_experiment():\n     print('\\nbuilding teacher model ...')\n     teacher_model, _ = make_teacher_model(train_data,\n                                           validation_data,\n-                                          nb_epoch=3)\n+                                          epochs=3)\n \n     print('\\nbuilding deeper student model by random init ...')\n     make_deeper_student_model(teacher_model, train_data,\n                               validation_data, 'random-init',\n-                              nb_epoch=3)\n+                              epochs=3)\n     print('\\nbuilding deeper student model by net2deeper ...')\n     make_deeper_student_model(teacher_model, train_data,\n                               validation_data, 'net2deeper',\n-                              nb_epoch=3)\n+                              epochs=3)\n \n # run the experiments\n net2wider_experiment()\n\n@@ -87,7 +87,7 @@ X_test = X_test.astype('float32')\n X_train /= 255\n X_test /= 255\n input_dim = 784\n-nb_epoch = 20\n+epochs = 20\n \n # create training+test positive and negative pairs\n digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n@@ -118,7 +118,7 @@ model.compile(loss=contrastive_loss, optimizer=rms)\n model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n           batch_size=128,\n-          nb_epoch=nb_epoch)\n+          epochs=epochs)\n \n # compute final accuracy on training and test sets\n pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n\n@@ -17,7 +17,7 @@ from keras import backend as K\n from sklearn.grid_search import GridSearchCV\n \n \n-nb_classes = 10\n+num_classes = 10\n \n # input image dimensions\n img_rows, img_cols = 28, 28\n@@ -40,28 +40,28 @@ X_train /= 255\n X_test /= 255\n \n # convert class vectors to binary class matrices\n-y_train = np_utils.to_categorical(y_train, nb_classes)\n-y_test = np_utils.to_categorical(y_test, nb_classes)\n+y_train = np_utils.to_categorical(y_train, num_classes)\n+y_test = np_utils.to_categorical(y_test, num_classes)\n \n \n-def make_model(dense_layer_sizes, nb_filters, nb_conv, nb_pool):\n+def make_model(dense_layer_sizes, filterss, num_conv, num_pool):\n     '''Creates model comprised of 2 convolutional layers followed by dense layers\n \n     dense_layer_sizes: List of layer sizes. This list has one number for each layer\n-    nb_filters: Number of convolutional filters in each convolutional layer\n-    nb_conv: Convolutional kernel size\n-    nb_pool: Size of pooling area for max pooling\n+    filterss: Number of convolutional filters in each convolutional layer\n+    num_conv: Convolutional kernel size\n+    num_pool: Size of pooling area for max pooling\n     '''\n \n     model = Sequential()\n \n-    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n+    model.add(Convolution2D(filterss, num_conv, num_conv,\n                             border_mode='valid',\n                             input_shape=input_shape))\n     model.add(Activation('relu'))\n-    model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n+    model.add(Convolution2D(filterss, num_conv, num_conv))\n     model.add(Activation('relu'))\n-    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n+    model.add(MaxPooling2D(pool_size=(num_pool, num_pool)))\n     model.add(Dropout(0.25))\n \n     model.add(Flatten())\n@@ -69,7 +69,7 @@ def make_model(dense_layer_sizes, nb_filters, nb_conv, nb_pool):\n         model.add(Dense(layer_size))\n     model.add(Activation('relu'))\n     model.add(Dropout(0.5))\n-    model.add(Dense(nb_classes))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n \n     model.compile(loss='categorical_crossentropy',\n@@ -82,12 +82,12 @@ dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n my_classifier = KerasClassifier(make_model, batch_size=32)\n validator = GridSearchCV(my_classifier,\n                          param_grid={'dense_layer_sizes': dense_size_candidates,\n-                                     # nb_epoch is avail for tuning even when not\n+                                     # epochs is avail for tuning even when not\n                                      # an argument to model building function\n-                                     'nb_epoch': [3, 6],\n-                                     'nb_filters': [8],\n-                                     'nb_conv': [3],\n-                                     'nb_pool': [2]},\n+                                     'epochs': [3, 6],\n+                                     'filterss': [8],\n+                                     'num_conv': [3],\n+                                     'num_pool': [2]},\n                          scoring='log_loss',\n                          n_jobs=1)\n validator.fit(X_train, y_train)\n\n@@ -110,7 +110,7 @@ pool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size\n # The convolution kernel size\r\n ksize = 3\r\n # Number of epochs to train for\r\n-nb_epoch = 5\r\n+epochs = 5\r\n # Batch size during training\r\n batch_size = 128\r\n \r\n@@ -162,7 +162,7 @@ model.compile('adam', 'mse')\n \r\n # Fit the model\r\n model.fit(X_train, X_train, validation_data=(X_test, X_test),\r\n-          batch_size=batch_size, nb_epoch=nb_epoch)\r\n+          batch_size=batch_size, epochs=epochs)\r\n \r\n # Plot\r\n X_recon = model.predict(X_test[:25])\r\n\n@@ -27,13 +27,13 @@ from keras import backend as K\n now = datetime.datetime.now\n \n batch_size = 128\n-nb_classes = 5\n-nb_epoch = 5\n+num_classes = 5\n+epochs = 5\n \n # input image dimensions\n img_rows, img_cols = 28, 28\n # number of convolutional filters to use\n-nb_filters = 32\n+filterss = 32\n # size of pooling area for max pooling\n pool_size = 2\n # convolution kernel size\n@@ -45,7 +45,7 @@ else:\n     input_shape = (img_rows, img_cols, 1)\n \n \n-def train_model(model, train, test, nb_classes):\n+def train_model(model, train, test, num_classes):\n     X_train = train[0].reshape((train[0].shape[0],) + input_shape)\n     X_test = test[0].reshape((test[0].shape[0],) + input_shape)\n     X_train = X_train.astype('float32')\n@@ -57,8 +57,8 @@ def train_model(model, train, test, nb_classes):\n     print(X_test.shape[0], 'test samples')\n \n     # convert class vectors to binary class matrices\n-    Y_train = np_utils.to_categorical(train[1], nb_classes)\n-    Y_test = np_utils.to_categorical(test[1], nb_classes)\n+    Y_train = np_utils.to_categorical(train[1], num_classes)\n+    Y_test = np_utils.to_categorical(test[1], num_classes)\n \n     model.compile(loss='categorical_crossentropy',\n                   optimizer='adadelta',\n@@ -66,7 +66,7 @@ def train_model(model, train, test, nb_classes):\n \n     t = now()\n     model.fit(X_train, Y_train,\n-              batch_size=batch_size, nb_epoch=nb_epoch,\n+              batch_size=batch_size, epochs=epochs,\n               verbose=1,\n               validation_data=(X_test, Y_test))\n     print('Training time: %s' % (now() - t))\n@@ -91,11 +91,11 @@ y_test_gte5 = y_test[y_test >= 5] - 5\n \n # define two groups of layers: feature (convolutions) and classification (dense)\n feature_layers = [\n-    Convolution2D(nb_filters, kernel_size, kernel_size,\n+    Convolution2D(filterss, kernel_size, kernel_size,\n                   border_mode='valid',\n                   input_shape=input_shape),\n     Activation('relu'),\n-    Convolution2D(nb_filters, kernel_size, kernel_size),\n+    Convolution2D(filterss, kernel_size, kernel_size),\n     Activation('relu'),\n     MaxPooling2D(pool_size=(pool_size, pool_size)),\n     Dropout(0.25),\n@@ -105,7 +105,7 @@ classification_layers = [\n     Dense(128),\n     Activation('relu'),\n     Dropout(0.5),\n-    Dense(nb_classes),\n+    Dense(num_classes),\n     Activation('softmax')\n ]\n \n@@ -115,7 +115,7 @@ model = Sequential(feature_layers + classification_layers)\n # train model for 5-digit classification [0..4]\n train_model(model,\n             (X_train_lt5, y_train_lt5),\n-            (X_test_lt5, y_test_lt5), nb_classes)\n+            (X_test_lt5, y_test_lt5), num_classes)\n \n # freeze feature layers and rebuild model\n for l in feature_layers:\n@@ -124,4 +124,4 @@ for l in feature_layers:\n # transfer: train dense layers for new classification task [5..9]\n train_model(model,\n             (X_train_gte5, y_train_gte5),\n-            (X_test_gte5, y_test_gte5), nb_classes)\n+            (X_test_gte5, y_test_gte5), num_classes)\n\n@@ -77,8 +77,8 @@ content_img_path = args.content_image\n target_img_prefix = args.target_image_prefix\r\n use_content_img = content_img_path is not None\r\n \r\n-nb_labels = args.nlabels\r\n-nb_colors = 3  # RGB\r\n+num_labels = args.nlabels\r\n+num_colors = 3  # RGB\r\n # determine image sizes based on target_mask\r\n ref_img = imread(target_mask_path)\r\n img_nrows, img_ncols = ref_img.shape[:2]\r\n@@ -147,16 +147,16 @@ def load_mask_labels():\n         mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)),\r\n                                target_mask_img.reshape((-1, 3))])\r\n \r\n-    labels = kmeans(mask_vecs, nb_labels)\r\n+    labels = kmeans(mask_vecs, num_labels)\r\n     style_mask_label = labels[:img_nrows *\r\n                               img_ncols].reshape((img_nrows, img_ncols))\r\n     target_mask_label = labels[img_nrows *\r\n                                img_ncols:].reshape((img_nrows, img_ncols))\r\n \r\n     stack_axis = 0 if K.image_data_format() == 'channels_first' else -1\r\n-    style_mask = np.stack([style_mask_label == r for r in xrange(nb_labels)],\r\n+    style_mask = np.stack([style_mask_label == r for r in xrange(num_labels)],\r\n                           axis=stack_axis)\r\n-    target_mask = np.stack([target_mask_label == r for r in xrange(nb_labels)],\r\n+    target_mask = np.stack([target_mask_label == r for r in xrange(num_labels)],\r\n                            axis=stack_axis)\r\n \r\n     return (np.expand_dims(style_mask, axis=0),\r\n@@ -164,9 +164,9 @@ def load_mask_labels():\n \r\n # Create tensor variables for images\r\n if K.image_data_format() == 'channels_first':\r\n-    shape = (1, nb_colors, img_nrows, img_ncols)\r\n+    shape = (1, num_colors, img_nrows, img_ncols)\r\n else:\r\n-    shape = (1, img_nrows, img_ncols, nb_colors)\r\n+    shape = (1, img_nrows, img_ncols, num_colors)\r\n \r\n style_image = K.variable(preprocess_image(style_img_path))\r\n target_image = K.placeholder(shape=shape)\r\n@@ -231,15 +231,15 @@ def region_style_loss(style_image, target_image, style_mask, target_mask):\n     if K.image_data_format() == 'channels_first':\r\n         masked_style = style_image * style_mask\r\n         masked_target = target_image * target_mask\r\n-        nb_channels = K.shape(style_image)[0]\r\n+        num_channels = K.shape(style_image)[0]\r\n     else:\r\n         masked_style = K.permute_dimensions(\r\n             style_image, (2, 0, 1)) * style_mask\r\n         masked_target = K.permute_dimensions(\r\n             target_image, (2, 0, 1)) * target_mask\r\n-        nb_channels = K.shape(style_image)[-1]\r\n-    s = gram_matrix(masked_style) / K.mean(style_mask) / nb_channels\r\n-    c = gram_matrix(masked_target) / K.mean(target_mask) / nb_channels\r\n+        num_channels = K.shape(style_image)[-1]\r\n+    s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels\r\n+    c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels\r\n     return K.mean(K.square(s - c))\r\n \r\n \r\n@@ -250,7 +250,7 @@ def style_loss(style_image, target_image, style_masks, target_masks):\n     assert 3 == K.ndim(style_image) == K.ndim(target_image)\r\n     assert 3 == K.ndim(style_masks) == K.ndim(target_masks)\r\n     loss = K.variable(0)\r\n-    for i in xrange(nb_labels):\r\n+    for i in xrange(num_labels):\r\n         if K.image_data_format() == 'channels_first':\r\n             style_mask = style_masks[i, :, :]\r\n             target_mask = target_masks[i, :, :]\r\n\n@@ -73,7 +73,7 @@ for name in sorted(os.listdir(TEXT_DATA_DIR)):\n print('Found %s texts.' % len(texts))\n \n # finally, vectorize the text samples into a 2D integer tensor\n-tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n+tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n tokenizer.fit_on_texts(texts)\n sequences = tokenizer.texts_to_sequences(texts)\n \n@@ -91,18 +91,18 @@ indices = np.arange(data.shape[0])\n np.random.shuffle(indices)\n data = data[indices]\n labels = labels[indices]\n-nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n+num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n \n-x_train = data[:-nb_validation_samples]\n-y_train = labels[:-nb_validation_samples]\n-x_val = data[-nb_validation_samples:]\n-y_val = labels[-nb_validation_samples:]\n+x_train = data[:-num_validation_samples]\n+y_train = labels[:-num_validation_samples]\n+x_val = data[-num_validation_samples:]\n+y_val = labels[-num_validation_samples:]\n \n print('Preparing embedding matrix.')\n \n # prepare embedding matrix\n-nb_words = min(MAX_NB_WORDS, len(word_index))\n-embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n+num_words = min(MAX_NB_WORDS, len(word_index))\n+embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n for word, i in word_index.items():\n     if i >= MAX_NB_WORDS:\n         continue\n@@ -113,7 +113,7 @@ for word, i in word_index.items():\n \n # load pre-trained word embeddings into an Embedding layer\n # note that we set trainable = False so as to keep the embeddings fixed\n-embedding_layer = Embedding(nb_words,\n+embedding_layer = Embedding(num_words,\n                             EMBEDDING_DIM,\n                             weights=[embedding_matrix],\n                             input_length=MAX_SEQUENCE_LENGTH,\n@@ -141,4 +141,4 @@ model.compile(loss='categorical_crossentropy',\n \n # happy learning!\n model.fit(x_train, y_train, validation_data=(x_val, y_val),\n-          nb_epoch=2, batch_size=128)\n+          epochs=2, batch_size=128)\n\n@@ -14,26 +14,26 @@ from keras.preprocessing.text import Tokenizer\n \n max_words = 1000\n batch_size = 32\n-nb_epoch = 5\n+epochs = 5\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_words, test_split=0.2)\n+(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)\n print(len(X_train), 'train sequences')\n print(len(X_test), 'test sequences')\n \n-nb_classes = np.max(y_train) + 1\n-print(nb_classes, 'classes')\n+num_classes = np.max(y_train) + 1\n+print(num_classes, 'classes')\n \n print('Vectorizing sequence data...')\n-tokenizer = Tokenizer(nb_words=max_words)\n+tokenizer = Tokenizer(num_words=max_words)\n X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n print('X_train shape:', X_train.shape)\n print('X_test shape:', X_test.shape)\n \n print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+Y_train = np_utils.to_categorical(y_train, num_classes)\n+Y_test = np_utils.to_categorical(y_test, num_classes)\n print('Y_train shape:', Y_train.shape)\n print('Y_test shape:', Y_test.shape)\n \n@@ -42,7 +42,7 @@ model = Sequential()\n model.add(Dense(512, input_shape=(max_words,)))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n-model.add(Dense(nb_classes))\n+model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy',\n@@ -50,7 +50,7 @@ model.compile(loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n history = model.fit(X_train, Y_train,\n-                    nb_epoch=nb_epoch, batch_size=batch_size,\n+                    epochs=epochs, batch_size=batch_size,\n                     verbose=1, validation_split=0.1)\n score = model.evaluate(X_test, Y_test,\n                        batch_size=batch_size, verbose=1)\n\n@@ -66,7 +66,7 @@ for i in range(epochs):\n               expected_output,\n               batch_size=batch_size,\n               verbose=1,\n-              nb_epoch=1,\n+              epochs=1,\n               shuffle=False)\n     model.reset_states()\n \n\n@@ -16,7 +16,7 @@ batch_size = 100\n original_dim = 784\n latent_dim = 2\n intermediate_dim = 256\n-nb_epoch = 50\n+epochs = 50\n epsilon_std = 1.0\n \n x = Input(batch_shape=(batch_size, original_dim))\n@@ -59,7 +59,7 @@ x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n \n vae.fit(x_train, x_train,\n         shuffle=True,\n-        nb_epoch=nb_epoch,\n+        epochs=epochs,\n         batch_size=batch_size,\n         validation_data=(x_test, x_test))\n \n\n@@ -17,9 +17,9 @@ from keras.datasets import mnist\n # input image dimensions\n img_rows, img_cols, img_chns = 28, 28, 1\n # number of convolutional filters to use\n-nb_filters = 64\n+filterss = 64\n # convolution kernel size\n-nb_conv = 3\n+num_conv = 3\n \n batch_size = 100\n if K.image_data_format() == 'channels_first':\n@@ -29,17 +29,17 @@ else:\n latent_dim = 2\n intermediate_dim = 128\n epsilon_std = 1.0\n-nb_epoch = 5\n+epochs = 5\n \n x = Input(batch_shape=(batch_size,) + original_img_size)\n conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n-conv_2 = Convolution2D(nb_filters, 2, 2,\n+conv_2 = Convolution2D(filterss, 2, 2,\n                        border_mode='same', activation='relu',\n                        subsample=(2, 2))(conv_1)\n-conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n+conv_3 = Convolution2D(filterss, num_conv, num_conv,\n                        border_mode='same', activation='relu',\n                        subsample=(1, 1))(conv_2)\n-conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n+conv_4 = Convolution2D(filterss, num_conv, num_conv,\n                        border_mode='same', activation='relu',\n                        subsample=(1, 1))(conv_3)\n flat = Flatten()(conv_4)\n@@ -61,29 +61,29 @@ z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n \n # we instantiate these layers separately so as to reuse them later\n decoder_hid = Dense(intermediate_dim, activation='relu')\n-decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n+decoder_upsample = Dense(filterss * 14 * 14, activation='relu')\n \n if K.image_data_format() == 'channels_first':\n-    output_shape = (batch_size, nb_filters, 14, 14)\n+    output_shape = (batch_size, filterss, 14, 14)\n else:\n-    output_shape = (batch_size, 14, 14, nb_filters)\n+    output_shape = (batch_size, 14, 14, filterss)\n \n decoder_reshape = Reshape(output_shape[1:])\n-decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n+decoder_deconv_1 = Deconvolution2D(filterss, num_conv, num_conv,\n                                    output_shape,\n                                    border_mode='same',\n                                    subsample=(1, 1),\n                                    activation='relu')\n-decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n+decoder_deconv_2 = Deconvolution2D(filterss, num_conv, num_conv,\n                                    output_shape,\n                                    border_mode='same',\n                                    subsample=(1, 1),\n                                    activation='relu')\n if K.image_data_format() == 'channels_first':\n-    output_shape = (batch_size, nb_filters, 29, 29)\n+    output_shape = (batch_size, filterss, 29, 29)\n else:\n-    output_shape = (batch_size, 29, 29, nb_filters)\n-decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2,\n+    output_shape = (batch_size, 29, 29, filterss)\n+decoder_deconv_3_upsamp = Deconvolution2D(filterss, 2, 2,\n                                           output_shape,\n                                           border_mode='valid',\n                                           subsample=(2, 2),\n@@ -126,7 +126,7 @@ print('x_train.shape:', x_train.shape)\n \n vae.fit(x_train, x_train,\n         shuffle=True,\n-        nb_epoch=nb_epoch,\n+        epochs=epochs,\n         batch_size=batch_size,\n         validation_data=(x_test, x_test))\n \n\n@@ -36,7 +36,7 @@ TH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/relea\n TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n \n \n-def conv2d_bn(x, nb_filter, nb_row, nb_col,\n+def conv2d_bn(x, filters, num_row, num_col,\n               border_mode='same', subsample=(1, 1),\n               name=None):\n     \"\"\"Utility function to apply conv + BN.\n@@ -51,7 +51,7 @@ def conv2d_bn(x, nb_filter, nb_row, nb_col,\n         bn_axis = 1\n     else:\n         bn_axis = 3\n-    x = Convolution2D(nb_filter, nb_row, nb_col,\n+    x = Convolution2D(filters, num_row, num_col,\n                       subsample=subsample,\n                       activation='relu',\n                       border_mode=border_mode,\n\n@@ -36,11 +36,11 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n     # Arguments\n         input_tensor: input tensor\n         kernel_size: defualt 3, the kernel size of middle conv layer at main path\n-        filters: list of integers, the nb_filters of 3 conv layer at main path\n+        filters: list of integers, the filterss of 3 conv layer at main path\n         stage: integer, current stage label, used for generating layer names\n         block: 'a','b'..., current block label, used for generating layer names\n     \"\"\"\n-    nb_filter1, nb_filter2, nb_filter3 = filters\n+    filters1, filters2, filters3 = filters\n     if K.image_data_format() == 'channels_last':\n         bn_axis = 3\n     else:\n@@ -48,16 +48,16 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n     conv_name_base = 'res' + str(stage) + block + '_branch'\n     bn_name_base = 'bn' + str(stage) + block + '_branch'\n \n-    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n+    x = Convolution2D(filters1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n+    x = Convolution2D(filters2, kernel_size, kernel_size,\n                       border_mode='same', name=conv_name_base + '2b')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n+    x = Convolution2D(filters3, 1, 1, name=conv_name_base + '2c')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n \n     x = merge([x, input_tensor], mode='sum')\n@@ -71,14 +71,14 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n     # Arguments\n         input_tensor: input tensor\n         kernel_size: defualt 3, the kernel size of middle conv layer at main path\n-        filters: list of integers, the nb_filters of 3 conv layer at main path\n+        filters: list of integers, the filterss of 3 conv layer at main path\n         stage: integer, current stage label, used for generating layer names\n         block: 'a','b'..., current block label, used for generating layer names\n \n     Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n     And the shortcut should have subsample=(2,2) as well\n     \"\"\"\n-    nb_filter1, nb_filter2, nb_filter3 = filters\n+    filters1, filters2, filters3 = filters\n     if K.image_data_format() == 'channels_last':\n         bn_axis = 3\n     else:\n@@ -86,20 +86,20 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n     conv_name_base = 'res' + str(stage) + block + '_branch'\n     bn_name_base = 'bn' + str(stage) + block + '_branch'\n \n-    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n+    x = Convolution2D(filters1, 1, 1, subsample=strides,\n                       name=conv_name_base + '2a')(input_tensor)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode='same',\n+    x = Convolution2D(filters2, kernel_size, kernel_size, border_mode='same',\n                       name=conv_name_base + '2b')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n     x = Activation('relu')(x)\n \n-    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n+    x = Convolution2D(filters3, 1, 1, name=conv_name_base + '2c')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n \n-    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n+    shortcut = Convolution2D(filters3, 1, 1, subsample=strides,\n                              name=conv_name_base + '1')(input_tensor)\n     shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n \n\n@@ -139,8 +139,8 @@ def get_session():\n             if not os.environ.get('OMP_NUM_THREADS'):\n                 config = tf.ConfigProto(allow_soft_placement=True)\n             else:\n-                nb_thread = int(os.environ.get('OMP_NUM_THREADS'))\n-                config = tf.ConfigProto(intra_op_parallelism_threads=nb_thread,\n+                num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n+                config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n                                         allow_soft_placement=True)\n             _SESSION = tf.Session(config=config)\n         session = _SESSION\n@@ -1814,15 +1814,15 @@ def stack(x):\n         return tf.pack(x)\n \n \n-def one_hot(indices, nb_classes):\n+def one_hot(indices, num_classes):\n     \"\"\"Input: nD integer tensor of shape `(batch_size, dim1, dim2, ... dim(n-1))`\n     Output: (n + 1)D one hot representation of the input\n-    with shape `(batch_size, dim1, dim2, ... dim(n-1), nb_classes)`\n+    with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n \n     # Returns\n         The one-hot tensor.\n     \"\"\"\n-    return tf.one_hot(indices, depth=nb_classes, axis=-1)\n+    return tf.one_hot(indices, depth=num_classes, axis=-1)\n \n \n def reverse(x, axes):\n\n@@ -921,15 +921,15 @@ def stack(x):\n     return T.stack(*x)\n \n \n-def one_hot(indices, nb_classes):\n+def one_hot(indices, num_classes):\n     \"\"\"Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n     Output: (n + 1)D one hot representation of the input\n-    with shape (batch_size, dim1, dim2, ... dim(n-1), nb_classes)\n+    with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)\n     \"\"\"\n     input_shape = tuple((indices.shape[i] for i in range(indices.ndim)))\n     indices = T.flatten(indices)\n-    oh = T.extra_ops.to_one_hot(indices, nb_classes)\n-    oh = T.reshape(oh, input_shape + (nb_classes,))\n+    oh = T.extra_ops.to_one_hot(indices, num_classes)\n+    oh = T.reshape(oh, input_shape + (num_classes,))\n     return oh\n \n \n@@ -1306,7 +1306,7 @@ def categorical_crossentropy(output, target, from_logits=False):\n \n def sparse_categorical_crossentropy(output, target, from_logits=False):\n     target = T.cast(T.flatten(target), 'int32')\n-    target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n+    target = T.extra_ops.to_one_hot(target, num_classes=output.shape[-1])\n     target = reshape(target, shape(output))\n     return categorical_crossentropy(output, target, from_logits)\n \n\n@@ -229,17 +229,17 @@ class ProgbarLogger(Callback):\n \n     def on_train_begin(self, logs=None):\n         self.verbose = self.params['verbose']\n-        self.nb_epoch = self.params['nb_epoch']\n+        self.epochs = self.params['epochs']\n \n     def on_epoch_begin(self, epoch, logs=None):\n         if self.verbose:\n-            print('Epoch %d/%d' % (epoch + 1, self.nb_epoch))\n-            self.progbar = Progbar(target=self.params['nb_sample'],\n+            print('Epoch %d/%d' % (epoch + 1, self.epochs))\n+            self.progbar = Progbar(target=self.params['samples'],\n                                    verbose=self.verbose)\n         self.seen = 0\n \n     def on_batch_begin(self, batch, logs=None):\n-        if self.seen < self.params['nb_sample']:\n+        if self.seen < self.params['samples']:\n             self.log_values = []\n \n     def on_batch_end(self, batch, logs=None):\n@@ -253,7 +253,7 @@ class ProgbarLogger(Callback):\n \n         # Skip progbar update for the last batch;\n         # will be handled by on_epoch_end.\n-        if self.verbose and self.seen < self.params['nb_sample']:\n+        if self.verbose and self.seen < self.params['samples']:\n             self.progbar.update(self.seen, self.log_values)\n \n     def on_epoch_end(self, epoch, logs=None):\n\n@@ -16,10 +16,10 @@ def load_data():\n     origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n     path = get_file(dirname, origin=origin, untar=True)\n \n-    nb_train_samples = 50000\n+    num_train_samples = 50000\n \n-    x_train = np.zeros((nb_train_samples, 3, 32, 32), dtype='uint8')\n-    y_train = np.zeros((nb_train_samples,), dtype='uint8')\n+    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype='uint8')\n+    y_train = np.zeros((num_train_samples,), dtype='uint8')\n \n     for i in range(1, 6):\n         fpath = os.path.join(path, 'data_batch_' + str(i))\n\n@@ -7,14 +7,14 @@ import numpy as np\n import sys\n \n \n-def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n+def load_data(path='imdb_full.pkl', num_words=None, skip_top=0,\n               maxlen=None, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n     \"\"\"Loads the IMDB dataset.\n \n     # Arguments\n         path: where to cache the data (relative to `~/.keras/dataset`).\n-        nb_words: max number of words to include. Words are ranked\n+        num_words: max number of words to include. Words are ranked\n             by how often they occur (in the training set) and only\n             the most frequent words are kept\n         skip_top: skip the top N most frequently occuring words\n@@ -23,7 +23,7 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n         seed: random seed for sample shuffling.\n         start_char: The start of a sequence will be marked with this character.\n             Set to 1 because 0 is usually the padding character.\n-        oov_char: words that were cut out because of the `nb_words`\n+        oov_char: words that were cut out because of the `num_words`\n             or `skip_top` limit will be replaced with this character.\n         index_from: index actual words with this index and higher.\n \n@@ -36,7 +36,7 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n \n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n-    because they're not making the `nb_words` cut here.\n+    because they're not making the `num_words` cut here.\n     Words that were not seen in the trining set but are in the test set\n     have simply been skipped.\n     \"\"\"\n@@ -83,20 +83,20 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n         raise ValueError('After filtering for sequences shorter than maxlen=' +\n                          str(maxlen) + ', no sequence was kept. '\n                          'Increase maxlen.')\n-    if not nb_words:\n-        nb_words = max([max(x) for x in xs])\n+    if not num_words:\n+        num_words = max([max(x) for x in xs])\n \n     # by convention, use 2 as OOV word\n     # reserve 'index_from' (=3 by default) characters:\n     # 0 (padding), 1 (start), 2 (OOV)\n     if oov_char is not None:\n-        xs = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in xs]\n+        xs = [[oov_char if (w >= num_words or w < skip_top) else w for w in x] for x in xs]\n     else:\n         new_xs = []\n         for x in xs:\n             nx = []\n             for w in x:\n-                if w >= nb_words or w < skip_top:\n+                if w >= num_words or w < skip_top:\n                     nx.append(w)\n             new_xs.append(nx)\n         xs = new_xs\n\n@@ -7,14 +7,14 @@ import numpy as np\n import sys\n \n \n-def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n+def load_data(path='reuters.pkl', num_words=None, skip_top=0,\n               maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n     \"\"\"Loads the Reuters newswire classification dataset.\n \n     # Arguments\n         path: where to cache the data (relative to `~/.keras/dataset`).\n-        nb_words: max number of words to include. Words are ranked\n+        num_words: max number of words to include. Words are ranked\n             by how often they occur (in the training set) and only\n             the most frequent words are kept\n         skip_top: skip the top N most frequently occuring words\n@@ -24,7 +24,7 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n         seed: random seed for sample shuffling.\n         start_char: The start of a sequence will be marked with this character.\n             Set to 1 because 0 is usually the padding character.\n-        oov_char: words that were cut out because of the `nb_words`\n+        oov_char: words that were cut out because of the `num_words`\n             or `skip_top` limit will be replaced with this character.\n         index_from: index actual words with this index and higher.\n \n@@ -33,7 +33,7 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n \n     Note that the 'out of vocabulary' character is only used for\n     words that were present in the training set but are not included\n-    because they're not making the `nb_words` cut here.\n+    because they're not making the `num_words` cut here.\n     Words that were not seen in the trining set but are in the test set\n     have simply been skipped.\n     \"\"\"\n@@ -63,20 +63,20 @@ def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n         xs = new_xs\n         labels = new_labels\n \n-    if not nb_words:\n-        nb_words = max([max(x) for x in xs])\n+    if not num_words:\n+        num_words = max([max(x) for x in xs])\n \n     # by convention, use 2 as OOV word\n     # reserve 'index_from' (=3 by default) characters:\n     # 0 (padding), 1 (start), 2 (OOV)\n     if oov_char is not None:\n-        xs = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in xs]\n+        xs = [[oov_char if (w >= num_words or w < skip_top) else w for w in x] for x in xs]\n     else:\n         new_xs = []\n         for x in xs:\n             nx = []\n             for w in x:\n-                if w >= nb_words or w < skip_top:\n+                if w >= num_words or w < skip_top:\n                     nx.append(w)\n             new_xs.append(nx)\n         xs = new_xs\n\n@@ -4,7 +4,5 @@ from .topology import InputSpec\n from .topology import Input\n from .topology import InputLayer\n from .topology import Layer\n-from .topology import Merge\n-from .topology import merge\n from .topology import get_source_inputs\n from .training import Model\n\n@@ -193,7 +193,7 @@ class Layer(object):\n         set_weights(weights)\n         get_config()\n         count_params()\n-        get_output_shape_for(input_shape)\n+        compute_output_shape(input_shape)\n         compute_mask(x, mask)\n         get_input_at(node_index)\n         get_output_at(node_index)\n@@ -377,7 +377,7 @@ class Layer(object):\n                 except TypeError:\n                     continue\n                 for axis, value in spec.axes.items():\n-                    if value is not None and x_shape[int(axis)] != value:\n+                    if value is not None and x_shape[int(axis)] not in {value, None}:\n                         raise ValueError('Input ' + str(input_index) +\n                                          ' is incompatible with layer ' +\n                                          self.name + ': expected axis ' +\n@@ -391,7 +391,7 @@ class Layer(object):\n                 except TypeError:\n                     continue\n                 for spec_dim, dim in zip(spec.shape, x_shape):\n-                    if spec_dim is not None:\n+                    if spec_dim is not None and dim is not None:\n                         if spec_dim != dim:\n                             raise ValueError(\n                                 'Input ' + str(input_index) +\n@@ -480,7 +480,7 @@ class Layer(object):\n         output = self.call(x, **kwargs)\n         output_mask = self.compute_mask(x, previous_mask)\n         # Infering the output shape is only relevant for Theano.\n-        output_shape = self.get_output_shape_for(input_shape)\n+        output_shape = self.compute_output_shape(input_shape)\n \n         # Add an inbound node to the layer, so that it keeps track\n         # of the call and of all new variables created during the call.\n@@ -547,7 +547,7 @@ class Layer(object):\n                                                 len(self.inbound_nodes) - 1,\n                                                 i)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         \"\"\"Computes the output shape of the layer given\n         an input shape (assumes that the layer will be built\n         to match that input shape).\n@@ -560,7 +560,7 @@ class Layer(object):\n         \"\"\"\n         return input_shape\n \n-    def compute_mask(self, input, input_mask=None):\n+    def compute_mask(self, input, mask=None):\n         \"\"\"Computes an output masking tensor, given an input tensor\n         (or list thereof) and an input mask (or list thereof).\n \n@@ -573,23 +573,23 @@ class Layer(object):\n                 one per output tensor of the layer).\n         \"\"\"\n         if not self.supports_masking:\n-            if input_mask is not None:\n-                if isinstance(input_mask, list):\n-                    if any(mask is not None for mask in input_mask):\n+            if mask is not None:\n+                if isinstance(mask, list):\n+                    if any(m is not None for m in mask):\n                         raise TypeError('Layer ' + self.name +\n                                         ' does not support masking, '\n                                         'but was passed an input_mask: ' +\n-                                        str(input_mask))\n+                                        str(mask))\n                 else:\n                     raise TypeError('Layer ' + self.name +\n                                     ' does not support masking, '\n                                     'but was passed an input_mask: ' +\n-                                    str(input_mask))\n+                                    str(mask))\n             # masking not explicitly supported: return None as mask\n             return None\n         # if masking is explictly supported, by default\n         # carry over the input mask\n-        return input_mask\n+        return mask\n \n     def build(self, input_shape):\n         \"\"\"Creates the layer weights.\n@@ -1656,11 +1656,11 @@ class Container(Layer):\n         \"\"\"\n         tuples = []\n         for layer in self.layers:\n-            nb_param = len(layer.weights)\n-            layer_weights = weights[:nb_param]\n+            num_param = len(layer.weights)\n+            layer_weights = weights[:num_param]\n             for sw, w in zip(layer.weights, layer_weights):\n                 tuples.append((sw, w))\n-            weights = weights[nb_param:]\n+            weights = weights[num_param:]\n         K.batch_set_value(tuples)\n \n     @property\n@@ -1721,7 +1721,7 @@ class Container(Layer):\n             output_tensors, output_masks, output_shapes = self.run_internal_graph(inputs, masks)\n             return output_masks\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         input_shapes = _to_list(input_shape)\n         if len(input_shapes) != len(self.input_layers):\n             raise ValueError('Invalid input_shape argument ' +\n@@ -1770,9 +1770,9 @@ class Container(Layer):\n                             input_shapes.append(input_shape)\n \n                         if len(input_shapes) == 1:\n-                            output_shape = layer.get_output_shape_for(input_shapes[0])\n+                            output_shape = layer.compute_output_shape(input_shapes[0])\n                         else:\n-                            output_shape = layer.get_output_shape_for(input_shapes)\n+                            output_shape = layer.compute_output_shape(input_shapes)\n \n                         output_shapes = _to_list(output_shape)\n                         node_index = layer.inbound_nodes.index(node)\n@@ -1877,10 +1877,10 @@ class Container(Layer):\n                     # Update _keras_shape.\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n                         if len(computed_tensors) == 1:\n-                            shapes = _to_list(layer.get_output_shape_for(computed_tensors[0]._keras_shape))\n+                            shapes = _to_list(layer.compute_output_shape(computed_tensors[0]._keras_shape))\n                             uses_learning_phase = computed_tensors[0]._uses_learning_phase\n                         else:\n-                            shapes = _to_list(layer.get_output_shape_for([x._keras_shape for x in computed_tensors]))\n+                            shapes = _to_list(layer.compute_output_shape([x._keras_shape for x in computed_tensors]))\n                             uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n                         for x, s in zip(output_tensors, shapes):\n                             x._keras_shape = s\n@@ -2056,7 +2056,7 @@ class Container(Layer):\n             layer = created_layers[layer_name]\n             layer_output_tensors = layer.inbound_nodes[node_index].output_tensors\n             output_tensors.append(layer_output_tensors[tensor_index])\n-        return cls(input=input_tensors, output=output_tensors, name=name)\n+        return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n \n     def save(self, filepath, overwrite=True):\n         \"\"\"Save into a single HDF5 file:\n\n@@ -272,9 +272,9 @@ def batch_shuffle(index_array, batch_size):\n def make_batches(size, batch_size):\n     \"\"\"Returns a list of batch indices (tuples of indices).\n     \"\"\"\n-    nb_batch = int(np.ceil(size / float(batch_size)))\n+    num_batches = int(np.ceil(size / float(batch_size)))\n     return [(i * batch_size, min(size, (i + 1) * batch_size))\n-            for i in range(0, nb_batch)]\n+            for i in range(0, num_batches)]\n \n \n def slice_X(X, start=None, stop=None):\n@@ -413,11 +413,11 @@ class GeneratorEnqueuer(object):\n \n         self.queue = None\n \n-    def start(self, nb_worker=1, max_q_size=10, wait_time=0.05):\n+    def start(self, workers=1, max_q_size=10, wait_time=0.05):\n         \"\"\"Kick off threads which add data from the generator into the queue.\n \n         # Arguments\n-            nb_worker: number of worker threads\n+            workers: number of worker threads\n             max_q_size: queue size (when full, threads could block on put())\n             wait_time: time to sleep in-between calls to put()\n         \"\"\"\n@@ -442,7 +442,7 @@ class GeneratorEnqueuer(object):\n                 self.queue = queue.Queue()\n                 self._stop_event = threading.Event()\n \n-            for i in range(nb_worker):\n+            for i in range(workers):\n                 if self._pickle_safe:\n                     # Reset random seed else all children processes\n                     # share the same seed\n@@ -798,7 +798,7 @@ class Model(Container):\n                                                **kwargs)\n \n     def _fit_loop(self, f, ins, out_labels=None, batch_size=32,\n-                  nb_epoch=100, verbose=1, callbacks=None,\n+                  epochs=100, verbose=1, callbacks=None,\n                   val_f=None, val_ins=None, shuffle=True,\n                   callback_metrics=None, initial_epoch=0):\n         \"\"\"Abstract fit function for f(ins).\n@@ -810,7 +810,7 @@ class Model(Container):\n             out_labels: list of strings, display names of\n                 the outputs of `f`\n             batch_size: integer batch size\n-            nb_epoch: number of times to iterate over the data\n+            epochs: number of times to iterate over the data\n             verbose: verbosity mode, 0, 1 or 2\n             callbacks: list of callbacks to be called during training\n             val_f: Keras function to call for validation\n@@ -833,8 +833,8 @@ class Model(Container):\n                 print('Train on %d samples, validate on %d samples' %\n                       (ins[0].shape[0], val_ins[0].shape[0]))\n \n-        nb_train_sample = ins[0].shape[0]\n-        index_array = np.arange(nb_train_sample)\n+        num_train_samples = ins[0].shape[0]\n+        index_array = np.arange(num_train_samples)\n \n         self.history = cbks.History()\n         callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]\n@@ -853,8 +853,8 @@ class Model(Container):\n         callbacks.set_model(callback_model)\n         callbacks.set_params({\n             'batch_size': batch_size,\n-            'nb_epoch': nb_epoch,\n-            'nb_sample': nb_train_sample,\n+            'epochs': epochs,\n+            'samples': num_train_samples,\n             'verbose': verbose,\n             'do_validation': do_validation,\n             'metrics': callback_metrics or [],\n@@ -863,14 +863,14 @@ class Model(Container):\n         callback_model.stop_training = False\n         self.validation_data = val_ins\n \n-        for epoch in range(initial_epoch, nb_epoch):\n+        for epoch in range(initial_epoch, epochs):\n             callbacks.on_epoch_begin(epoch)\n             if shuffle == 'batch':\n                 index_array = batch_shuffle(index_array, batch_size)\n             elif shuffle:\n                 np.random.shuffle(index_array)\n \n-            batches = make_batches(nb_train_sample, batch_size)\n+            batches = make_batches(num_train_samples, batch_size)\n             epoch_logs = {}\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n@@ -928,12 +928,12 @@ class Model(Container):\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n         \"\"\"\n-        nb_sample = ins[0].shape[0]\n+        samples = ins[0].shape[0]\n         outs = []\n         if verbose == 1:\n-            progbar = Progbar(target=nb_sample)\n-        batches = make_batches(nb_sample, batch_size)\n-        index_array = np.arange(nb_sample)\n+            progbar = Progbar(target=samples)\n+        batches = make_batches(samples, batch_size)\n+        index_array = np.arange(samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n             if isinstance(ins[-1], float):\n@@ -947,7 +947,7 @@ class Model(Container):\n                 batch_outs = [batch_outs]\n             if batch_index == 0:\n                 for batch_out in batch_outs:\n-                    shape = (nb_sample,) + batch_out.shape[1:]\n+                    shape = (samples,) + batch_out.shape[1:]\n                     outs.append(np.zeros(shape, dtype=K.floatx()))\n \n             for i, batch_out in enumerate(batch_outs):\n@@ -973,12 +973,12 @@ class Model(Container):\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         \"\"\"\n-        nb_sample = ins[0].shape[0]\n+        samples = ins[0].shape[0]\n         outs = []\n         if verbose == 1:\n-            progbar = Progbar(target=nb_sample)\n-        batches = make_batches(nb_sample, batch_size)\n-        index_array = np.arange(nb_sample)\n+            progbar = Progbar(target=samples)\n+        batches = make_batches(samples, batch_size)\n+        index_array = np.arange(samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n             if isinstance(ins[-1], float):\n@@ -1002,7 +1002,7 @@ class Model(Container):\n             if verbose == 1:\n                 progbar.update(batch_end)\n         for i, out in enumerate(outs):\n-            outs[i] /= nb_sample\n+            outs[i] /= samples\n         if len(outs) == 1:\n             return outs[0]\n         return outs\n@@ -1049,7 +1049,7 @@ class Model(Container):\n                                  str(x[0].shape[0]) + ' samples')\n         return x, y, sample_weights\n \n-    def fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None,\n+    def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,\n             validation_split=0., validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0):\n         \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n@@ -1066,7 +1066,7 @@ class Model(Container):\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n             batch_size: integer. Number of samples per gradient update.\n-            nb_epoch: integer, the number of times to iterate\n+            epochs: integer, the number of times to iterate\n                 over the training data arrays.\n                 verbose: 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = verbose, 2 = one log line per epoch.\n@@ -1189,7 +1189,7 @@ class Model(Container):\n \n         # delegate logic to _fit_loop\n         return self._fit_loop(f, ins, out_labels=out_labels,\n-                              batch_size=batch_size, nb_epoch=nb_epoch,\n+                              batch_size=batch_size, epochs=epochs,\n                               verbose=verbose, callbacks=callbacks,\n                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n                               callback_metrics=callback_metrics,\n@@ -1379,11 +1379,11 @@ class Model(Container):\n             return outputs[0]\n         return outputs\n \n-    def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n+    def fit_generator(self, generator, samples_per_epoch, epochs,\n                       verbose=1, callbacks=None,\n-                      validation_data=None, nb_val_samples=None,\n+                      validation_data=None, num_val_samples=None,\n                       class_weight=None,\n-                      max_q_size=10, nb_worker=1, pickle_safe=False,\n+                      max_q_size=10, workers=1, pickle_safe=False,\n                       initial_epoch=0):\n         \"\"\"Fits the model on data generated batch-by-batch by\n         a Python generator.\n@@ -1402,20 +1402,20 @@ class Model(Container):\n                 samples have been seen by the model.\n             samples_per_epoch: integer, number of samples to process before\n                 going to the next epoch.\n-            nb_epoch: integer, total number of iterations on the data.\n+            epochs: integer, total number of iterations on the data.\n             verbose: verbosity mode, 0, 1, or 2.\n             callbacks: list of callbacks to be called during training.\n             validation_data: this can be either\n                 - a generator for the validation data\n                 - a tuple (inputs, targets)\n                 - a tuple (inputs, targets, sample_weights).\n-            nb_val_samples: only relevant if `validation_data` is a generator.\n+            num_val_samples: only relevant if `validation_data` is a generator.\n                 number of samples to use from validation generator\n                 at the end of every epoch.\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             max_q_size: maximum size for the generator queue\n-            nb_worker: maximum number of processes to spin up\n+            workers: maximum number of processes to spin up\n                 when using process based threading\n             pickle_safe: if True, use process based threading.\n                 Note that because\n@@ -1444,7 +1444,7 @@ class Model(Container):\n                     f.close()\n \n             model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n-                                samples_per_epoch=10000, nb_epoch=10)\n+                                samples_per_epoch=10000, epochs=10)\n         ```\n         \"\"\"\n         wait_time = 0.01  # in seconds\n@@ -1459,9 +1459,9 @@ class Model(Container):\n         # avoid any explicit version checks\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__'))\n-        if val_gen and not nb_val_samples:\n+        if val_gen and not num_val_samples:\n             raise ValueError('When using a generator for validation data, '\n-                             'you must specify a value for \"nb_val_samples\".')\n+                             'you must specify a value for \"num_val_samples\".')\n \n         out_labels = self.metrics_names\n         callback_metrics = out_labels + ['val_' + n for n in out_labels]\n@@ -1480,8 +1480,8 @@ class Model(Container):\n             callback_model = self\n         callbacks.set_model(callback_model)\n         callbacks.set_params({\n-            'nb_epoch': nb_epoch,\n-            'nb_sample': samples_per_epoch,\n+            'epochs': epochs,\n+            'samples': samples_per_epoch,\n             'verbose': verbose,\n             'do_validation': do_validation,\n             'metrics': callback_metrics,\n@@ -1509,10 +1509,10 @@ class Model(Container):\n \n         try:\n             enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n-            enqueuer.start(max_q_size=max_q_size, nb_worker=nb_worker)\n+            enqueuer.start(max_q_size=max_q_size, workers=workers)\n \n             callback_model.stop_training = False\n-            while epoch < nb_epoch:\n+            while epoch < epochs:\n                 callbacks.on_epoch_begin(epoch)\n                 samples_seen = 0\n                 batch_index = 0\n@@ -1579,9 +1579,9 @@ class Model(Container):\n                         if val_gen:\n                             val_outs = self.evaluate_generator(\n                                 validation_data,\n-                                nb_val_samples,\n+                                num_val_samples,\n                                 max_q_size=max_q_size,\n-                                nb_worker=nb_worker,\n+                                workers=workers,\n                                 pickle_safe=pickle_safe)\n                         else:\n                             # no need for try/except because\n@@ -1610,7 +1610,7 @@ class Model(Container):\n         return self.history\n \n     def evaluate_generator(self, generator, val_samples,\n-                           max_q_size=10, nb_worker=1, pickle_safe=False):\n+                           max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n \n@@ -1622,7 +1622,7 @@ class Model(Container):\n                 total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n-            nb_worker: maximum number of processes to spin up\n+            workers: maximum number of processes to spin up\n                 when using process based threading\n             pickle_safe: if True, use process based threading.\n                 Note that because\n@@ -1649,7 +1649,7 @@ class Model(Container):\n \n         try:\n             enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n-            enqueuer.start(nb_worker=nb_worker, max_q_size=max_q_size)\n+            enqueuer.start(workers=workers, max_q_size=max_q_size)\n \n             while processed_samples < val_samples:\n                 generator_output = None\n@@ -1677,15 +1677,15 @@ class Model(Container):\n                 outs = self.test_on_batch(x, y, sample_weight=sample_weight)\n \n                 if isinstance(x, list):\n-                    nb_samples = len(x[0])\n+                    sampless = len(x[0])\n                 elif isinstance(x, dict):\n-                    nb_samples = len(list(x.values())[0])\n+                    sampless = len(list(x.values())[0])\n                 else:\n-                    nb_samples = len(x)\n+                    sampless = len(x)\n                 all_outs.append(outs)\n \n-                processed_samples += nb_samples\n-                weights.append(nb_samples)\n+                processed_samples += sampless\n+                weights.append(sampless)\n \n         finally:\n             if enqueuer is not None:\n@@ -1702,7 +1702,7 @@ class Model(Container):\n             return averages\n \n     def predict_generator(self, generator, val_samples,\n-                          max_q_size=10, nb_worker=1, pickle_safe=False):\n+                          max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n@@ -1712,7 +1712,7 @@ class Model(Container):\n             val_samples: total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n-            nb_worker: maximum number of processes to spin up\n+            workers: maximum number of processes to spin up\n                 when using process based threading\n             pickle_safe: if True, use process based threading.\n                 Note that because\n@@ -1735,7 +1735,7 @@ class Model(Container):\n \n         try:\n             enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n-            enqueuer.start(nb_worker=nb_worker, max_q_size=max_q_size)\n+            enqueuer.start(workers=workers, max_q_size=max_q_size)\n \n             while processed_samples < val_samples:\n                 generator_output = None\n@@ -1763,11 +1763,11 @@ class Model(Container):\n                 outs = self.predict_on_batch(x)\n \n                 if isinstance(x, list):\n-                    nb_samples = len(x[0])\n+                    sampless = len(x[0])\n                 elif isinstance(x, dict):\n-                    nb_samples = len(list(x.values())[0])\n+                    sampless = len(list(x.values())[0])\n                 else:\n-                    nb_samples = len(x)\n+                    sampless = len(x)\n \n                 if not isinstance(outs, list):\n                     outs = [outs]\n@@ -1778,8 +1778,8 @@ class Model(Container):\n                         all_outs.append(np.zeros(shape, dtype=K.floatx()))\n \n                 for i, out in enumerate(outs):\n-                    all_outs[i][processed_samples:(processed_samples + nb_samples)] = out\n-                processed_samples += nb_samples\n+                    all_outs[i][processed_samples:(processed_samples + sampless)] = out\n+                processed_samples += sampless\n \n         finally:\n             if enqueuer is not None:\n\n@@ -1,5 +1,10 @@\n from __future__ import absolute_import\n-from ..engine import Layer, Input, InputLayer, Merge, merge, InputSpec\n+\n+from ..engine import Layer\n+from ..engine import Input\n+from ..engine import InputLayer\n+from ..engine import InputSpec\n+from .merge import *\n from .core import *\n from .convolutional import *\n from .pooling import *\n\n@@ -176,7 +176,7 @@ class _Conv(Layer):\n             return self.activation(outputs)\n         return outputs\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_last':\n             space = input_shape[1:-1]\n             new_space = []\n@@ -281,7 +281,7 @@ class Conv1D(_Conv):\n         3D tensor with shape: `(batch_size, steps, input_dim)`\n \n     # Output shape\n-        3D tensor with shape: `(batch_size, new_steps, nb_filter)`\n+        3D tensor with shape: `(batch_size, new_steps, filters)`\n         `steps` value might have changed due to padding or strides.\n     \"\"\"\n \n@@ -402,9 +402,9 @@ class Conv2D(_Conv):\n \n     # Output shape\n         4D tensor with shape:\n-        `(samples, nb_filter, new_rows, new_cols)` if data_format='channels_first'\n+        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n         or 4D tensor with shape:\n-        `(samples, new_rows, new_cols, nb_filter)` if data_format='channels_last'.\n+        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n         `rows` and `cols` values might have changed due to padding.\n     \"\"\"\n \n@@ -526,9 +526,9 @@ class Conv3D(_Conv):\n \n     # Output shape\n         5D tensor with shape:\n-        `(samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if data_format='channels_first'\n+        `(samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if data_format='channels_first'\n         or 5D tensor with shape:\n-        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)` if data_format='channels_last'.\n+        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)` if data_format='channels_last'.\n         `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n     \"\"\"\n \n@@ -650,9 +650,9 @@ class Conv2DTranspose(Conv2D):\n \n     # Output shape\n         4D tensor with shape:\n-        `(batch, nb_filter, new_rows, new_cols)` if data_format='channels_first'\n+        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n         or 4D tensor with shape:\n-        `(batch, new_rows, new_cols, nb_filter)` if data_format='channels_last'.\n+        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n         `rows` and `cols` values might have changed due to padding.\n \n     # References\n@@ -766,7 +766,7 @@ class Conv2DTranspose(Conv2D):\n             return self.activation(outputs)\n         return outputs\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n         if self.data_format == 'channels_first':\n             c_axis, h_axis, w_axis = 1, 2, 3\n@@ -837,7 +837,7 @@ class SeparableConv2D(Conv2D):\n         depth_multiplier: The number of depthwise convolution output channels\n             for each input channel.\n             The total number of depthwise convolution output\n-            channels will be equal to `num_filters_in * depth_multiplier`.\n+            channels will be equal to `filterss_in * depth_multiplier`.\n         activation: Activation function to use\n             (see [activations](../activations.md)).\n             If you don't specify anything, no activation is applied\n@@ -877,9 +877,9 @@ class SeparableConv2D(Conv2D):\n \n     # Output shape\n         4D tensor with shape:\n-        `(batch, nb_filter, new_rows, new_cols)` if data_format='channels_first'\n+        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n         or 4D tensor with shape:\n-        `(batch, new_rows, new_cols, nb_filter)` if data_format='channels_last'.\n+        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n         `rows` and `cols` values might have changed due to padding.\n     \"\"\"\n \n@@ -989,7 +989,7 @@ class SeparableConv2D(Conv2D):\n             return self.activation(outputs)\n         return outputs\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             rows = input_shape[2]\n             cols = input_shape[3]\n@@ -1045,7 +1045,7 @@ class UpSampling1D(Layer):\n         self.size = int(size)\n         self.input_spec = InputSpec(ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         size = self.size * input_shape[1] if input_shape[1] is not None else None\n         return (input_shape[0], size, input_shape[2])\n \n@@ -1100,7 +1100,7 @@ class UpSampling2D(Layer):\n         self.size = conv_utils.normalize_tuple(size, 2, 'size')\n         self.input_spec = InputSpec(ndim=4)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             width = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n             height = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n@@ -1168,7 +1168,7 @@ class UpSampling3D(Layer):\n         self.input_spec = InputSpec(ndim=5)\n         super(UpSampling3D, self).__init__(**kwargs)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             dim1 = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n             dim2 = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n@@ -1228,7 +1228,7 @@ class ZeroPadding1D(Layer):\n         self.right_pad = self.padding[1]\n         self.input_spec = InputSpec(ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         length = input_shape[1] + self.left_pad + self.right_pad if input_shape[1] is not None else None\n         return (input_shape[0],\n                 length,\n@@ -1314,7 +1314,7 @@ class ZeroPadding2D(Layer):\n                              'Found: ' + str(padding))\n         self.input_spec = InputSpec(ndim=4)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             rows = input_shape[2] + self.padding[0][0] + self.padding[0][1] if input_shape[2] is not None else None\n             cols = input_shape[3] + self.padding[1][0] + self.padding[1][1] if input_shape[3] is not None else None\n@@ -1414,7 +1414,7 @@ class ZeroPadding3D(Layer):\n                              'Found: ' + str(padding))\n         self.input_spec = InputSpec(ndim=5)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             dim1 = input_shape[2] + 2 * self.padding[0][0] if input_shape[2] is not None else None\n             dim2 = input_shape[3] + 2 * self.padding[1][0] if input_shape[3] is not None else None\n@@ -1472,7 +1472,7 @@ class Cropping1D(Layer):\n         self.cropping = conv_utils.normalize_tuple(cropping, 2, 'cropping')\n         self.input_spec = InputSpec(ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if input_shape[1] is not None:\n             length = input_shape[1] - self.cropping[0] - self.cropping[1]\n         else:\n@@ -1574,7 +1574,7 @@ class Cropping2D(Layer):\n                              'Found: ' + str(cropping))\n         self.input_spec = InputSpec(ndim=4)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             return (input_shape[0],\n                     input_shape[1],\n@@ -1707,7 +1707,7 @@ class Cropping3D(Layer):\n                              'Found: ' + str(cropping))\n         self.input_spec = InputSpec(ndim=5)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             dim1 = input_shape[2] - self.cropping[0][0] - self.cropping[0][1] if input_shape[2] is not None else None\n             dim2 = input_shape[3] - self.cropping[1][0] - self.cropping[1][1] if input_shape[3] is not None else None\n\n@@ -51,12 +51,12 @@ class ConvRecurrent2D(Recurrent):\n             state for the sample of index i in the following batch.\n \n     # Input shape\n-        5D tensor with shape `(nb_samples, timesteps, channels, rows, cols)`.\n+        5D tensor with shape `(num_samples, timesteps, channels, rows, cols)`.\n \n     # Output shape\n         - if `return_sequences`: 5D tensor with shape\n-            `(nb_samples, timesteps, channels, rows, cols)`.\n-        - else, 4D tensor with shape `(nb_samples, channels, rows, cols)`.\n+            `(num_samples, timesteps, channels, rows, cols)`.\n+        - else, 4D tensor with shape `(num_samples, channels, rows, cols)`.\n \n     # Masking\n         This layer supports masking for input data with a variable number\n@@ -106,7 +106,7 @@ class ConvRecurrent2D(Recurrent):\n         self.stateful = stateful\n         self.input_spec = InputSpec(ndim=5)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             rows = input_shape[3]\n             cols = input_shape[4]\n@@ -408,7 +408,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n         if not self.stateful:\n             raise RuntimeError('Layer must be stateful.')\n         input_shape = self.input_spec[0].shape\n-        output_shape = self.get_output_shape_for(input_shape)\n+        output_shape = self.compute_output_shape(input_shape)\n         if not input_shape[0]:\n             raise ValueError('If a RNN is stateful, a complete ' +\n                              'input_shape must be provided ' +\n\n@@ -53,7 +53,7 @@ class Masking(Layer):\n         self.supports_masking = True\n         self.mask_value = mask_value\n \n-    def compute_mask(self, inputs, input_mask=None):\n+    def compute_mask(self, inputs, mask=None):\n         return K.any(K.not_equal(inputs, self.mask_value), axis=-1)\n \n     def call(self, inputs):\n@@ -376,7 +376,7 @@ class Reshape(Layer):\n \n         return tuple(output_shape)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         return (input_shape[0],) + self._fix_unknown_dimension(\n             input_shape[1:], self.target_shape)\n \n@@ -395,7 +395,7 @@ class Reshape(Layer):\n             except TypeError:\n                 pass\n             if input_shape is not None:\n-                target_shape = self.get_output_shape_for(input_shape)[1:]\n+                target_shape = self.compute_output_shape(input_shape)[1:]\n         return K.reshape(inputs, (-1,) + target_shape)\n \n     def get_config(self):\n@@ -439,7 +439,7 @@ class Permute(Layer):\n         self.dims = tuple(dims)\n         self.input_spec = InputSpec(ndim=len(self.dims) + 1)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         input_shape = list(input_shape)\n         output_shape = copy.copy(input_shape)\n         for i, dim in enumerate(self.dims):\n@@ -477,7 +477,7 @@ class Flatten(Layer):\n         super(Flatten, self).__init__(**kwargs)\n         self.input_spec = InputSpec(min_ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if not all(input_shape[1:]):\n             raise ValueError('The shape of the input to \"Flatten\" '\n                              'is not fully defined '\n@@ -510,10 +510,10 @@ class RepeatVector(Layer):\n         n: integer, repetition factor.\n \n     # Input shape\n-        2D tensor of shape `(nb_samples, features)`.\n+        2D tensor of shape `(num_samples, features)`.\n \n     # Output shape\n-        3D tensor of shape `(nb_samples, n, features)`.\n+        3D tensor of shape `(num_samples, n, features)`.\n     \"\"\"\n \n     def __init__(self, n, **kwargs):\n@@ -521,7 +521,7 @@ class RepeatVector(Layer):\n         self.n = n\n         self.input_spec = InputSpec(ndim=2)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         return (input_shape[0], self.n, input_shape[1])\n \n     def call(self, inputs):\n@@ -607,7 +607,7 @@ class Lambda(Layer):\n                                 'must be a list, a tuple, or a function.')\n             self._output_shape = output_shape\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self._output_shape is None:\n             # With TensorFlow, we can infer the output shape directly:\n             if K.backend() == 'tensorflow':\n@@ -633,10 +633,10 @@ class Lambda(Layer):\n             return input_shape\n         elif isinstance(self._output_shape, (tuple, list)):\n             if isinstance(input_shape, list):\n-                nb_samples = input_shape[0][0]\n+                num_samples = input_shape[0][0]\n             else:\n-                nb_samples = input_shape[0] if input_shape else None\n-            return (nb_samples,) + tuple(self._output_shape)\n+                num_samples = input_shape[0] if input_shape else None\n+            return (num_samples,) + tuple(self._output_shape)\n         else:\n             shape = self._output_shape(input_shape)\n             if not isinstance(shape, (list, tuple)):\n@@ -825,7 +825,7 @@ class Dense(Layer):\n             output = self.activation(output)\n         return output\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         assert input_shape and len(input_shape) >= 2\n         assert input_shape[-1]\n         output_shape = list(input_shape)\n\n@@ -73,6 +73,11 @@ class Embedding(Layer):\n                  input_length=None,\n                  **kwargs):\n         kwargs['dtype'] = 'int32'\n+        if 'input_shape' not in kwargs:\n+            if input_length:\n+                kwargs['input_shape'] = (input_length,)\n+            else:\n+                kwargs['input_shape'] = (None,)\n         super(Embedding, self).__init__(**kwargs)\n \n         self.input_dim = input_dim\n@@ -92,13 +97,13 @@ class Embedding(Layer):\n             regularizer=self.embeddings_regularizer,\n             constraint=self.embeddings_constraint)\n \n-    def compute_mask(self, inputs):\n+    def compute_mask(self, inputs, mask=None):\n         if not self.mask_zero:\n             return None\n         else:\n             return K.not_equal(inputs, 0)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if not self.input_length:\n             input_length = input_shape[1]\n         else:\n\n@@ -67,7 +67,7 @@ class LocallyConnected1D(Layer):\n         3D tensor with shape: `(batch_size, steps, input_dim)`\n \n     # Output shape\n-        3D tensor with shape: `(batch_size, new_steps, nb_filter)`\n+        3D tensor with shape: `(batch_size, new_steps, filters)`\n         `steps` value might have changed due to padding or strides.\n     \"\"\"\n \n@@ -108,7 +108,7 @@ class LocallyConnected1D(Layer):\n         if input_dim is None:\n             raise ValueError('Axis 2 of input should be fully-defined. '\n                              'Found shape:', input_shape)\n-        _, output_length, filters = self.get_output_shape_for(input_shape)\n+        _, output_length, filters = self.compute_output_shape(input_shape)\n         self.kernel_shape = (output_length,\n                              self.kernel_size[0] * input_dim,\n                              filters)\n@@ -129,7 +129,7 @@ class LocallyConnected1D(Layer):\n             self.bias = None\n         self.input_spec = InputSpec(ndim=3, axes={1: input_dim})\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         length = conv_utils.conv_output_length(input_shape[1],\n                                                self.kernel_size[0],\n                                                self.padding,\n@@ -255,9 +255,9 @@ class LocallyConnected2D(Layer):\n \n     # Output shape\n         4D tensor with shape:\n-        `(samples, nb_filter, new_rows, new_cols)` if data_format='channels_first'\n+        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n         or 4D tensor with shape:\n-        `(samples, new_rows, new_cols, nb_filter)` if data_format='channels_last'.\n+        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n         `rows` and `cols` values might have changed due to padding.\n     \"\"\"\n \n@@ -304,7 +304,7 @@ class LocallyConnected2D(Layer):\n                              'should be fully-defined, but layer received '\n                              'the inputs shape ' + str(input_shape))\n \n-        output_shape = self.get_output_shape_for(input_shape)\n+        output_shape = self.compute_output_shape(input_shape)\n         if self.data_format == 'channels_first':\n             _, filters, output_row, output_col = output_shape\n             input_filter = input_shape[1]\n@@ -335,7 +335,7 @@ class LocallyConnected2D(Layer):\n         else:\n             self.input_spec = InputSpec(ndim=4, axes={-1: input_filter})\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             rows = input_shape[2]\n             cols = input_shape[3]\n\n@@ -120,7 +120,7 @@ class Concatenate(_Merge):\n                              'on a list of 2 inputs.')\n         return K.concatenate(inputs, axis=self.axis)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if not isinstance(input_shape, list) or len(input_shape) != 2:\n             raise ValueError('A `Concatenate` layer should be called '\n                              'on a list of 2 inputs.')\n@@ -224,7 +224,7 @@ class Dot(_Merge):\n         output = K.batch_dot(x1, x2, axes)\n         return output\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if not isinstance(input_shape, list) or len(input_shape) != 2:\n             raise ValueError('A `Dot` layer should be called '\n                              'on a list of 2 inputs.')\n\n@@ -21,7 +21,7 @@ class _Pooling1D(Layer):\n         self.padding = conv_utils.normalize_padding(padding)\n         self.input_spec = InputSpec(ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         length = conv_utils.conv_output_length(input_shape[1], self.pool_size,\n                                                self.padding, self.strides)\n         return (input_shape[0], length, input_shape[2])\n@@ -121,7 +121,7 @@ class _Pooling2D(Layer):\n         self.data_format = conv_utils.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=4)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             rows = input_shape[2]\n             cols = input_shape[3]\n@@ -280,7 +280,7 @@ class _Pooling3D(Layer):\n         self.data_format = conv_utils.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=5)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_first':\n             len_dim1 = input_shape[2]\n             len_dim2 = input_shape[3]\n@@ -432,7 +432,7 @@ class _GlobalPooling1D(Layer):\n         super(_GlobalPooling1D, self).__init__(**kwargs)\n         self.input_spec = InputSpec(ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         return (input_shape[0], input_shape[2])\n \n     def call(self, inputs):\n@@ -478,7 +478,7 @@ class _GlobalPooling2D(Layer):\n         self.data_format = conv_utils.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=4)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_last':\n             return (input_shape[0], input_shape[3])\n         else:\n@@ -572,7 +572,7 @@ class _GlobalPooling3D(Layer):\n         self.data_format = conv_utils.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=5)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.data_format == 'channels_last':\n             return (input_shape[0], input_shape[4])\n         else:\n\n@@ -178,7 +178,7 @@ class Recurrent(Layer):\n         self.supports_masking = True\n         self.input_spec = InputSpec(ndim=3)\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.return_sequences:\n             return (input_shape[0], input_shape[1], self.units)\n         else:\n@@ -713,16 +713,16 @@ class GRU(Recurrent):\n \n             x_z = matrix_x[:, :self.units]\n             x_r = matrix_x[:, self.units: 2 * self.units]\n-            inner_z = matrix_inner[:, :self.units]\n-            inner_r = matrix_inner[:, self.units: 2 * self.units]\n+            recurrent_z = matrix_inner[:, :self.units]\n+            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n \n-            z = self.recurrent_activation(x_z + inner_z)\n-            r = self.recurrent_activation(x_r + inner_r)\n+            z = self.recurrent_activation(x_z + recurrent_z)\n+            r = self.recurrent_activation(x_r + recurrent_r)\n \n             x_h = matrix_x[:, 2 * self.units:]\n-            inner_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n+            recurrent_h = K.dot(r * h_tm1 * rec_dp_mask[0],\n                             self.recurrent_kernel[:, 2 * self.units:])\n-            hh = self.activation(x_h + inner_h)\n+            hh = self.activation(x_h + recurrent_h)\n         else:\n             if self.implementation == 0:\n                 x_z = inputs[:, :self.units]\n@@ -756,9 +756,9 @@ class GRU(Recurrent):\n                   'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                   'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n-                  'kernel_regularizer': regularizers.serialize(kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(recurrent_regularizer),\n-                  'bias_regularizer': regularizers.serialize(bias_regularizer),\n+                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                   'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n@@ -899,9 +899,9 @@ class LSTM(Recurrent):\n                                         regularizer=self.bias_regularizer,\n                                         constraint=self.bias_constraint)\n             if self.unit_forget_bias:\n-                self.bias += K.concatenate(K.zeros((self.units,)),\n+                self.bias += K.concatenate([K.zeros((self.units,)),\n                                            K.ones((self.units,)),\n-                                           K.zeros((self.units * 2,)))\n+                                           K.zeros((self.units * 2,))])\n         else:\n             self.bias = None\n \n@@ -1012,10 +1012,10 @@ class LSTM(Recurrent):\n             z2 = z[:, 2 * self.units: 3 * self.units]\n             z3 = z[:, 3 * self.units:]\n \n-            i = self.inner_activation(z0)\n-            f = self.inner_activation(z1)\n+            i = self.recurrent_activation(z0)\n+            f = self.recurrent_activation(z1)\n             c = f * c_tm1 + i * self.activation(z2)\n-            o = self.inner_activation(z3)\n+            o = self.recurrent_activation(z3)\n         else:\n             if self.implementation == 0:\n                 x_i = inputs[:, :self.units]\n@@ -1030,13 +1030,13 @@ class LSTM(Recurrent):\n             else:\n                 raise ValueError('Unknown `implementation` mode.')\n \n-            i = self.inner_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n+            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],\n                                                       self.recurrent_kernel_i))\n-            f = self.inner_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n+            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],\n                                                       self.recurrent_kernel_f))\n             c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],\n                                                             self.recurrent_kernel_c))\n-            o = self.inner_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n+            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],\n                                                       self.recurrent_kernel_o))\n         h = o * self.activation(c)\n         return h, [h, c]\n\n@@ -13,7 +13,6 @@ class Wrapper(Layer):\n \n     def __init__(self, layer, **kwargs):\n         self.layer = layer\n-        self.uses_learning_phase = layer.uses_learning_phase\n         super(Wrapper, self).__init__(**kwargs)\n \n     def build(self, input_shape=None):\n@@ -98,9 +97,9 @@ class TimeDistributed(Wrapper):\n             self.layer.built = True\n         super(TimeDistributed, self).build()\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         child_input_shape = (input_shape[0],) + input_shape[2:]\n-        child_output_shape = self.layer.get_output_shape_for(child_input_shape)\n+        child_output_shape = self.layer.compute_output_shape(child_input_shape)\n         timesteps = input_shape[1]\n         return (child_output_shape[0], timesteps) + child_output_shape[1:]\n \n@@ -124,11 +123,11 @@ class TimeDistributed(Wrapper):\n             input_length = input_shape[1]\n             if not input_length:\n                 input_length = K.shape(inputs)[1]\n-            # (nb_samples * timesteps, ...)\n+            # (num_samples * timesteps, ...)\n             inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n-            y = self.layer.call(inputs)  # (nb_samples * timesteps, ...)\n-            # (nb_samples, timesteps, ...)\n-            output_shape = self.get_output_shape_for(input_shape)\n+            y = self.layer.call(inputs)  # (num_samples * timesteps, ...)\n+            # (num_samples, timesteps, ...)\n+            output_shape = self.compute_output_shape(input_shape)\n             y = K.reshape(y, (-1, input_length) + output_shape[2:])\n \n         # Apply activity regularizer if any:\n@@ -191,15 +190,15 @@ class Bidirectional(Wrapper):\n         self.forward_layer.set_weights(weights[:nw // 2])\n         self.backward_layer.set_weights(weights[nw // 2:])\n \n-    def get_output_shape_for(self, input_shape):\n+    def compute_output_shape(self, input_shape):\n         if self.merge_mode in ['sum', 'ave', 'mul']:\n-            return self.forward_layer.get_output_shape_for(input_shape)\n+            return self.forward_layer.compute_output_shape(input_shape)\n         elif self.merge_mode == 'concat':\n-            shape = list(self.forward_layer.get_output_shape_for(input_shape))\n+            shape = list(self.forward_layer.compute_output_shape(input_shape))\n             shape[-1] *= 2\n             return tuple(shape)\n         elif self.merge_mode is None:\n-            return [self.forward_layer.get_output_shape_for(input_shape)] * 2\n+            return [self.forward_layer.compute_output_shape(input_shape)] * 2\n \n     def call(self, inputs, mask=None):\n         y = self.forward_layer.call(inputs, mask)\n\n@@ -12,7 +12,8 @@ from . import backend as K\n from . import optimizers\n from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n-from .engine.topology import get_source_inputs, Node, Layer, Merge, Input\n+from .engine.topology import get_source_inputs, Node, Layer, Input\n+from .legacy import layers as legacy_layers\n \n \n def save_model(model, filepath, overwrite=True):\n@@ -426,32 +427,7 @@ class Sequential(Model):\n \n     @property\n     def flattened_layers(self):\n-        if self._flattened_layers is not None:\n-            return self._flattened_layers\n-        layers = []\n-        if self.layers:\n-            if isinstance(self.layers[0], Merge):\n-                merge = self.layers[0]\n-                for layer in merge.layers:\n-                    if hasattr(layer, 'flattened_layers'):\n-                        for sublayer in layer.flattened_layers:\n-                            if sublayer not in layers:\n-                                layers.append(sublayer)\n-                    elif hasattr(layer, 'layers'):\n-                        for sublayer in layer.layers:\n-                            if sublayer not in layers:\n-                                layers.append(sublayer)\n-                    else:\n-                        if layer not in layers:\n-                            layers.append(layer)\n-            else:\n-                if self.layers[0] not in layers:\n-                    layers.append(self.layers[0])\n-            for layer in self.layers[1:]:\n-                if layer not in layers:\n-                    layers.append(layer)\n-        self._flattened_layers = layers\n-        return layers\n+        return self.model.flattened_layers\n \n     def _gather_list_attr(self, attr):\n         all_attrs = []\n@@ -540,9 +516,9 @@ class Sequential(Model):\n         \"\"\"\n         # support for legacy behavior\n         for layer in self.flattened_layers:\n-            nb_param = len(layer.weights)\n-            layer.set_weights(weights[:nb_param])\n-            weights = weights[nb_param:]\n+            num_param = len(layer.weights)\n+            layer.set_weights(weights[:num_param])\n+            weights = weights[num_param:]\n \n     @property\n     def validation_data(self):\n@@ -603,7 +579,7 @@ class Sequential(Model):\n         self.metrics_names = self.model.metrics_names\n         self.sample_weight_mode = self.model.sample_weight_mode\n \n-    def fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None,\n+    def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,\n             validation_split=0., validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0, **kwargs):\n         \"\"\"Trains the model for a fixed number of epochs.\n@@ -613,7 +589,7 @@ class Sequential(Model):\n                 (if the model has multiple inputs).\n             y: labels, as a Numpy array.\n             batch_size: integer. Number of samples per gradient update.\n-            nb_epoch: integer, the number of epochs to train the model.\n+            epochs: integer, the number of epochs to train the model.\n             verbose: 0 for no logging to stdout,\n                 1 for progress bar logging, 2 for one log line per epoch.\n             callbacks: list of `keras.callbacks.Callback` instances.\n@@ -664,7 +640,7 @@ class Sequential(Model):\n                             str(kwargs))\n         return self.model.fit(x, y,\n                               batch_size=batch_size,\n-                              nb_epoch=nb_epoch,\n+                              epochs=epochs,\n                               verbose=verbose,\n                               callbacks=callbacks,\n                               validation_split=validation_split,\n@@ -840,10 +816,10 @@ class Sequential(Model):\n         else:\n             return (proba > 0.5).astype('int32')\n \n-    def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n+    def fit_generator(self, generator, samples_per_epoch, epochs,\n                       verbose=1, callbacks=None,\n-                      validation_data=None, nb_val_samples=None,\n-                      class_weight=None, max_q_size=10, nb_worker=1,\n+                      validation_data=None, num_val_samples=None,\n+                      class_weight=None, max_q_size=10, workers=1,\n                       pickle_safe=False, initial_epoch=0, **kwargs):\n         \"\"\"Fits the model on data generated batch-by-batch by\n         a Python generator.\n@@ -862,20 +838,20 @@ class Sequential(Model):\n                 samples have been seen by the model.\n             samples_per_epoch: integer, number of samples to process before\n                 going to the next epoch.\n-            nb_epoch: integer, total number of iterations on the data.\n+            epochs: integer, total number of iterations on the data.\n             verbose: verbosity mode, 0, 1, or 2.\n             callbacks: list of callbacks to be called during training.\n             validation_data: this can be either\n                 - a generator for the validation data\n                 - a tuple (inputs, targets)\n                 - a tuple (inputs, targets, sample_weights).\n-            nb_val_samples: only relevant if `validation_data` is a generator.\n+            num_val_samples: only relevant if `validation_data` is a generator.\n                 number of samples to use from validation generator\n                 at the end of every epoch.\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             max_q_size: maximum size for the generator queue\n-            nb_worker: maximum number of processes to spin up\n+            workers: maximum number of processes to spin up\n             pickle_safe: if True, use process based threading. Note that because\n                 this implementation relies on multiprocessing, you should not pass\n                 non picklable arguments to the generator as they can't be passed\n@@ -900,16 +876,16 @@ class Sequential(Model):\n                     f.close()\n \n             model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n-                                samples_per_epoch=10000, nb_epoch=10)\n+                                samples_per_epoch=10000, epochs=10)\n         ```\n         \"\"\"\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if nb_worker > 1 and not pickle_safe:\n-            warnings.warn('The \"nb_worker\" argument is deprecated '\n+        if workers > 1 and not pickle_safe:\n+            warnings.warn('The \"workers\" argument is deprecated '\n                           'when pickle_safe is False')\n-            nb_worker = 1  # For backward compatibility\n+            workers = 1  # For backward compatibility\n         if 'show_accuracy' in kwargs:\n             kwargs.pop('show_accuracy')\n             warnings.warn('The \"show_accuracy\" argument is deprecated, '\n@@ -917,28 +893,28 @@ class Sequential(Model):\n                           'the model at compile time:\\n'\n                           '`model.compile(optimizer, loss, '\n                           'metrics=[\"accuracy\"])`')\n-        if 'nb_val_worker' in kwargs:\n-            kwargs.pop('nb_val_worker')\n-            warnings.warn('The \"nb_val_worker\" argument is deprecated, '\n+        if 'num_val_worker' in kwargs:\n+            kwargs.pop('num_val_worker')\n+            warnings.warn('The \"num_val_worker\" argument is deprecated, '\n                           'please remove it from your code.')\n         if kwargs:\n             raise TypeError('Received unknown keyword arguments: ' +\n                             str(kwargs))\n         return self.model.fit_generator(generator,\n                                         samples_per_epoch,\n-                                        nb_epoch,\n+                                        epochs,\n                                         verbose=verbose,\n                                         callbacks=callbacks,\n                                         validation_data=validation_data,\n-                                        nb_val_samples=nb_val_samples,\n+                                        num_val_samples=num_val_samples,\n                                         class_weight=class_weight,\n                                         max_q_size=max_q_size,\n-                                        nb_worker=nb_worker,\n+                                        workers=workers,\n                                         pickle_safe=pickle_safe,\n                                         initial_epoch=initial_epoch)\n \n     def evaluate_generator(self, generator, val_samples,\n-                           max_q_size=10, nb_worker=1,\n+                           max_q_size=10, workers=1,\n                            pickle_safe=False, **kwargs):\n         \"\"\"Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n@@ -951,7 +927,7 @@ class Sequential(Model):\n                 total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n-            nb_worker: maximum number of processes to spin up\n+            workers: maximum number of processes to spin up\n             pickle_safe: if True, use process based threading. Note that because\n                 this implementation relies on multiprocessing, you should not pass non\n                 non picklable arguments to the generator as they can't be passed\n@@ -960,10 +936,10 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if nb_worker > 1 and not pickle_safe:\n-            warnings.warn('The \"nb_worker\" argument is deprecated '\n+        if workers > 1 and not pickle_safe:\n+            warnings.warn('The \"workers\" argument is deprecated '\n                           'when pickle_safe is False')\n-            nb_worker = 1  # For backward compatibility\n+            workers = 1  # For backward compatibility\n         if 'show_accuracy' in kwargs:\n             kwargs.pop('show_accuracy')\n             warnings.warn('The \"show_accuracy\" argument is deprecated, '\n@@ -980,11 +956,11 @@ class Sequential(Model):\n         return self.model.evaluate_generator(generator,\n                                              val_samples,\n                                              max_q_size=max_q_size,\n-                                             nb_worker=nb_worker,\n+                                             workers=workers,\n                                              pickle_safe=pickle_safe)\n \n     def predict_generator(self, generator, val_samples,\n-                          max_q_size=10, nb_worker=1, pickle_safe=False):\n+                          max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n@@ -994,7 +970,7 @@ class Sequential(Model):\n             val_samples: total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n-            nb_worker: maximum number of processes to spin up\n+            workers: maximum number of processes to spin up\n             pickle_safe: if True, use process based threading. Note that because\n                 this implementation relies on multiprocessing, you should not pass non\n                 non picklable arguments to the generator as they can't be passed\n@@ -1005,21 +981,45 @@ class Sequential(Model):\n         \"\"\"\n         if self.model is None:\n             self.build()\n-        if nb_worker > 1 and not pickle_safe:\n-            warnings.warn('The \"nb_worker\" argument is deprecated '\n+        if workers > 1 and not pickle_safe:\n+            warnings.warn('The \"workers\" argument is deprecated '\n                           'when pickle_safe is False')\n-            nb_worker = 1  # For backward compatibility\n+            workers = 1  # For backward compatibility\n         return self.model.predict_generator(generator, val_samples,\n                                             max_q_size=max_q_size,\n-                                            nb_worker=nb_worker,\n+                                            workers=workers,\n                                             pickle_safe=pickle_safe)\n \n     def get_config(self):\n-        \"\"\"Returns the model configuration\n-        as a Python list.\n+        \"\"\"Returns the model configuration as a Python list.\n+        \"\"\"\n+        if isinstance(self.layers[0], legacy_layers.Merge):\n+            return self.legacy_get_config()\n+\n+        config = []\n+        for layer in self.layers:\n+            config.append({'class_name': layer.__class__.__name__,\n+                           'config': layer.get_config()})\n+        return copy.deepcopy(config)\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        from keras.utils.layer_utils import layer_from_config\n+\n+        if 'class_name' not in config[0] or config[0]['class_name'] == 'Merge':\n+            return cls.legacy_from_config(config)\n+\n+        model = cls()\n+        for conf in config:\n+            layer = layer_from_config(conf)\n+            model.add(layer)\n+        return model\n+\n+    def legacy_get_config(self):\n+        \"\"\"Returns the model configuration as a Python list.\n         \"\"\"\n         config = []\n-        if isinstance(self.layers[0], Merge):\n+        if isinstance(self.layers[0], legacy_layers.Merge):\n             assert hasattr(self.layers[0], 'layers')\n             layers = []\n             for layer in self.layers[0].layers:\n@@ -1038,11 +1038,10 @@ class Sequential(Model):\n         return copy.deepcopy(config)\n \n     @classmethod\n-    def from_config(cls, config, layer_cache=None):\n-        \"\"\"Supports legacy formats\n+    def legacy_from_config(cls, config, layer_cache=None):\n+        \"\"\"Supports legacy formats.\n         \"\"\"\n         from keras.utils.layer_utils import layer_from_config\n-        from keras.layers import Merge\n \n         if not layer_cache:\n             layer_cache = {}\n@@ -1082,7 +1081,7 @@ class Sequential(Model):\n                 merge_input = layer_from_config(merge_input_config)\n                 merge_inputs.append(merge_input)\n             first_layer_config['layers'] = merge_inputs\n-            merge = Merge.from_config(first_layer_config)\n+            merge = legacy_layers.Merge.from_config(first_layer_config)\n             model.add(merge)\n         else:\n             layer = get_or_create_layer(first_layer)\n\n@@ -632,7 +632,8 @@ def deserialize(config, custom_objects=None):\n         'nadam': Nadam,\n         'tfoptimizer': TFOptimizer,\n     }\n-    deserialize_keras_object(config,\n+    config['class_name'] = config['class_name'].lower()\n+    return deserialize_keras_object(config,\n                                     module_objects=all_classes,\n                                     custom_objects=custom_objects,\n                                     printable_module_name='optimizer')\n\n@@ -772,14 +772,14 @@ class DirectoryIterator(Iterator):\n         white_list_formats = {'png', 'jpg', 'jpeg', 'bmp'}\n \n         # first, count the number of samples and classes\n-        self.nb_sample = 0\n+        self.samples = 0\n \n         if not classes:\n             classes = []\n             for subdir in sorted(os.listdir(directory)):\n                 if os.path.isdir(os.path.join(directory, subdir)):\n                     classes.append(subdir)\n-        self.nb_class = len(classes)\n+        self.num_class = len(classes)\n         self.class_indices = dict(zip(classes, range(len(classes))))\n \n         def _recursive_list(subpath):\n@@ -795,12 +795,12 @@ class DirectoryIterator(Iterator):\n                             is_valid = True\n                             break\n                     if is_valid:\n-                        self.nb_sample += 1\n-        print('Found %d images belonging to %d classes.' % (self.nb_sample, self.nb_class))\n+                        self.samples += 1\n+        print('Found %d images belonging to %d classes.' % (self.samples, self.num_class))\n \n         # second, build an index of the images in the different class subfolders\n         self.filenames = []\n-        self.classes = np.zeros((self.nb_sample,), dtype='int32')\n+        self.classes = np.zeros((self.samples,), dtype='int32')\n         i = 0\n         for subdir in classes:\n             subpath = os.path.join(directory, subdir)\n@@ -817,7 +817,7 @@ class DirectoryIterator(Iterator):\n                         # add filename relative to directory\n                         absolute_path = os.path.join(root, fname)\n                         self.filenames.append(os.path.relpath(absolute_path, directory))\n-        super(DirectoryIterator, self).__init__(self.nb_sample, batch_size, shuffle, seed)\n+        super(DirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n \n     def next(self):\n         with self.lock:\n@@ -851,7 +851,7 @@ class DirectoryIterator(Iterator):\n         elif self.class_mode == 'binary':\n             batch_y = self.classes[index_array].astype(K.floatx())\n         elif self.class_mode == 'categorical':\n-            batch_y = np.zeros((len(batch_x), self.nb_class), dtype=K.floatx())\n+            batch_y = np.zeros((len(batch_x), self.num_class), dtype=K.floatx())\n             for i, label in enumerate(self.classes[index_array]):\n                 batch_y[i, label] = 1.\n         else:\n\n@@ -35,7 +35,7 @@ def pad_sequences(sequences, maxlen=None, dtype='int32',\n     \"\"\"\n     lengths = [len(s) for s in sequences]\n \n-    nb_samples = len(sequences)\n+    num_samples = len(sequences)\n     if maxlen is None:\n         maxlen = np.max(lengths)\n \n@@ -47,7 +47,7 @@ def pad_sequences(sequences, maxlen=None, dtype='int32',\n             sample_shape = np.asarray(s).shape[1:]\n             break\n \n-    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n+    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n     for idx, s in enumerate(sequences):\n         if not len(s):\n             continue  # empty list/array was found\n@@ -165,16 +165,16 @@ def skipgrams(sequence, vocabulary_size,\n                     labels.append(1)\n \n     if negative_samples > 0:\n-        nb_negative_samples = int(len(labels) * negative_samples)\n+        num_negative_samples = int(len(labels) * negative_samples)\n         words = [c[0] for c in couples]\n         random.shuffle(words)\n \n         couples += [[words[i % len(words)],\n-                    random.randint(1, vocabulary_size - 1)] for i in range(nb_negative_samples)]\n+                    random.randint(1, vocabulary_size - 1)] for i in range(num_negative_samples)]\n         if categorical:\n-            labels += [[1, 0]] * nb_negative_samples\n+            labels += [[1, 0]] * num_negative_samples\n         else:\n-            labels += [0] * nb_negative_samples\n+            labels += [0] * num_negative_samples\n \n     if shuffle:\n         seed = random.randint(0, 10e6)\n\n@@ -59,8 +59,8 @@ class Tokenizer(object):\n     for each token could be binary, based on word count, based on tf-idf...\n \n     # Arguments\n-        nb_words: the maximum number of words to keep, based\n-            on word frequency. Only the most common `nb_words` words will\n+        num_words: the maximum number of words to keep, based\n+            on word frequency. Only the most common `num_words` words will\n             be kept.\n         filters: a string where each element is a character that will be\n             filtered from the texts. The default is all punctuation, plus\n@@ -77,7 +77,7 @@ class Tokenizer(object):\n     `0` is a reserved index that won't be assigned to any word.\n     \"\"\"\n \n-    def __init__(self, nb_words=None,\n+    def __init__(self, num_words=None,\n                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                  lower=True,\n                  split=' ',\n@@ -87,7 +87,7 @@ class Tokenizer(object):\n         self.filters = filters\n         self.split = split\n         self.lower = lower\n-        self.nb_words = nb_words\n+        self.num_words = num_words\n         self.document_count = 0\n         self.char_level = char_level\n \n@@ -151,7 +151,7 @@ class Tokenizer(object):\n     def texts_to_sequences(self, texts):\n         \"\"\"Transforms each text in texts in a sequence of integers.\n \n-        Only top \"nb_words\" most frequent words will be taken into account.\n+        Only top \"num_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n \n         # Arguments\n@@ -168,7 +168,7 @@ class Tokenizer(object):\n     def texts_to_sequences_generator(self, texts):\n         \"\"\"Transforms each text in texts in a sequence of integers.\n \n-        Only top \"nb_words\" most frequent words will be taken into account.\n+        Only top \"num_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n \n         # Arguments\n@@ -177,7 +177,7 @@ class Tokenizer(object):\n         # Yields\n             Yields individual sequences.\n         \"\"\"\n-        nb_words = self.nb_words\n+        num_words = self.num_words\n         for text in texts:\n             seq = text if self.char_level else text_to_word_sequence(text,\n                                                                      self.filters,\n@@ -187,7 +187,7 @@ class Tokenizer(object):\n             for w in seq:\n                 i = self.word_index.get(w)\n                 if i is not None:\n-                    if nb_words and i >= nb_words:\n+                    if num_words and i >= num_words:\n                         continue\n                     else:\n                         vect.append(i)\n@@ -221,26 +221,26 @@ class Tokenizer(object):\n             ValueError: In case of invalid `mode` argument,\n                 or if the Tokenizer requires to be fit to sample data.\n         \"\"\"\n-        if not self.nb_words:\n+        if not self.num_words:\n             if self.word_index:\n-                nb_words = len(self.word_index) + 1\n+                num_words = len(self.word_index) + 1\n             else:\n-                raise ValueError('Specify a dimension (nb_words argument), '\n+                raise ValueError('Specify a dimension (num_words argument), '\n                                  'or fit on some text data first.')\n         else:\n-            nb_words = self.nb_words\n+            num_words = self.num_words\n \n         if mode == 'tfidf' and not self.document_count:\n             raise ValueError('Fit the Tokenizer on some data '\n                              'before using tfidf mode.')\n \n-        x = np.zeros((len(sequences), nb_words))\n+        x = np.zeros((len(sequences), num_words))\n         for i, seq in enumerate(sequences):\n             if not seq:\n                 continue\n             counts = {}\n             for j in seq:\n-                if j >= nb_words:\n+                if j >= num_words:\n                     continue\n                 if j not in counts:\n                     counts[j] = 1.\n\n@@ -111,7 +111,7 @@ def deserialize_keras_object(identifier, module_objects=None,\n         # In this case we are dealing with a Keras config dictionary.\n         config = identifier\n         if 'class_name' not in config or 'config' not in config:\n-            raise ValueError('Improper config format:', config)\n+            raise ValueError('Improper config format: ' + str(config))\n         class_name = config['class_name']\n         if custom_objects and class_name in custom_objects:\n             cls = custom_objects[class_name]\n@@ -121,8 +121,8 @@ def deserialize_keras_object(identifier, module_objects=None,\n             module_objects = module_objects or {}\n             cls = module_objects.get(class_name)\n             if cls is None:\n-                raise ValueError('Unknown ' + printable_module_name,\n-                                 ':', class_name)\n+                raise ValueError('Unknown ' + printable_module_name +\n+                                 ': ' + class_name)\n         if hasattr(cls, 'from_config'):\n             arg_spec = inspect.getargspec(cls.from_config)\n             if 'custom_objects' in arg_spec.args:\n@@ -149,8 +149,8 @@ def deserialize_keras_object(identifier, module_objects=None,\n                                  ':', class_name)\n         return fn\n     else:\n-        raise ValueError('Could not interpret serialized',\n-                         printable_module_name, ':', identifier)\n+        raise ValueError('Could not interpret serialized ' +\n+                         printable_module_name + ': ' + identifier)\n \n \n def make_tuple(*args):\n\n@@ -4,6 +4,7 @@ import inspect\n from .generic_utils import get_custom_objects\n from .generic_utils import deserialize_keras_object\n from .conv_utils import convert_kernel\n+py_sum = sum\n from ..layers import *\n from ..models import Model, Sequential\n from .. import backend as K\n@@ -137,8 +138,8 @@ def count_total_params(layers, layer_set=None):\n             trainable_count += t\n             non_trainable_count += nt\n         else:\n-            trainable_count += sum([K.count_params(p) for p in layer.trainable_weights])\n-            non_trainable_count += sum([K.count_params(p) for p in layer.non_trainable_weights])\n+            trainable_count += py_sum([K.count_params(p) for p in layer.trainable_weights])\n+            non_trainable_count += py_sum([K.count_params(p) for p in layer.non_trainable_weights])\n     return trainable_count, non_trainable_count\n \n \n\n@@ -5,24 +5,24 @@ import numpy as np\n from six.moves import zip\n \n \n-def to_categorical(y, nb_classes=None):\n+def to_categorical(y, num_classes=None):\n     \"\"\"Converts a class vector (integers) to binary class matrix.\n \n     E.g. for use with categorical_crossentropy.\n \n     # Arguments\n         y: class vector to be converted into a matrix\n-            (integers from 0 to nb_classes).\n-        nb_classes: total number of classes.\n+            (integers from 0 to num_classes).\n+        num_classes: total number of classes.\n \n     # Returns\n         A binary matrix representation of the input.\n     \"\"\"\n     y = np.array(y, dtype='int').ravel()\n-    if not nb_classes:\n-        nb_classes = np.max(y) + 1\n+    if not num_classes:\n+        num_classes = np.max(y) + 1\n     n = y.shape[0]\n-    categorical = np.zeros((n, nb_classes))\n+    categorical = np.zeros((n, num_classes))\n     categorical[np.arange(n), y] = 1\n     return categorical\n \n\n@@ -12,7 +12,7 @@ from .. import backend as K\n \n def get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n                   output_shape=(2,),\n-                  classification=True, num_class=2):\n+                  classification=True, num_classes=2):\n     \"\"\"Generates test data to train a model on.\n \n     classification=True overrides output_shape\n@@ -21,17 +21,17 @@ def get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n \n     Otherwise: float output with shape output_shape.\n     \"\"\"\n-    num_sample = num_train + num_test\n+    samples = num_train + num_test\n     if classification:\n-        y = np.random.randint(0, num_class, size=(num_sample,))\n-        X = np.zeros((num_sample,) + input_shape)\n-        for i in range(num_sample):\n+        y = np.random.randint(0, num_classes, size=(samples,))\n+        X = np.zeros((samples,) + input_shape)\n+        for i in range(samples):\n             X[i] = np.random.normal(loc=y[i], scale=0.7, size=input_shape)\n     else:\n-        y_loc = np.random.random((num_sample,))\n-        X = np.zeros((num_sample,) + input_shape)\n-        y = np.zeros((num_sample,) + output_shape)\n-        for i in range(num_sample):\n+        y_loc = np.random.random((samples,))\n+        X = np.zeros((samples,) + input_shape)\n+        y = np.zeros((samples,) + output_shape)\n+        for i in range(samples):\n             X[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=input_shape)\n             y[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=output_shape)\n \n@@ -83,7 +83,7 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, dtype=None,\n     model = Model(input=x, output=y)\n     model.compile('rmsprop', 'mse')\n \n-    expected_output_shape = layer.get_output_shape_for(input_shape)\n+    expected_output_shape = layer.compute_output_shape(input_shape)\n     actual_output = model.predict(input_data)\n     actual_output_shape = actual_output.shape\n     for expected_dim, actual_dim in zip(expected_output_shape,\n\n@@ -36,7 +36,7 @@ class BaseWrapper(object):\n     values to `sk_params`.\n \n     `sk_params` could also accept parameters for calling `fit`, `predict`,\n-    `predict_proba`, and `score` methods (e.g., `nb_epoch`, `batch_size`).\n+    `predict_proba`, and `score` methods (e.g., `epochs`, `batch_size`).\n     fitting (predicting) parameters are selected in the following order:\n \n     1. Values passed to the dictionary arguments of\n@@ -48,7 +48,7 @@ class BaseWrapper(object):\n     When using scikit-learn's `grid_search` API, legal tunable parameters are\n     those you could pass to `sk_params`, including fitting parameters.\n     In other words, you could use `grid_search` to search for the best\n-    `batch_size` or `nb_epoch` as well as the model parameters.\n+    `batch_size` or `epochs` as well as the model parameters.\n     \"\"\"\n \n     def __init__(self, build_fn=None, **sk_params):\n\n@@ -4,45 +4,41 @@ import pytest\n \n from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import Sequential\n-from keras.layers.core import Dense, Flatten, Activation\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras import layers\n from keras.utils.np_utils import to_categorical\n \n \n @keras_test\n def test_image_classification():\n-    '''\n-    Classify random 16x16 color images into several classes using logistic regression\n-    with convolutional hidden layer.\n-    '''\n-    np.random.seed(1337)\n     input_shape = (16, 16, 3)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n+                                                         num_test=200,\n                                                          input_shape=input_shape,\n                                                          classification=True,\n-                                                         nb_class=4)\n+                                                         num_classes=4)\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n-    # convolution kernel size\n-    nb_conv = 3\n-    # size of pooling area for max pooling\n-    nb_pool = 2\n \n     model = Sequential([\n-        Convolution2D(nb_filter=8, nb_row=nb_conv, nb_col=nb_conv, input_shape=input_shape),\n-        MaxPooling2D(pool_size=(nb_pool, nb_pool)),\n-        Flatten(),\n-        Activation('relu'),\n-        Dense(y_test.shape[-1], activation='softmax')\n+        layers.Conv2D(filters=8, kernel_size=3,\n+                      activation='relu',\n+                      input_shape=input_shape),\n+        layers.MaxPooling2D(pool_size=2),\n+        layers.Conv2D(filters=4, kernel_size=(3, 3),\n+                      activation='relu', padding='same'),\n+        layers.GlobalAveragePooling2D(),\n+        layers.Dense(y_test.shape[-1], activation='softmax')\n     ])\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n-    history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16,\n-                        validation_data=(X_test, y_test),\n+    model.summary()\n+    history = model.fit(x_train, y_train, epochs=10, batch_size=16,\n+                        validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.85)\n+    assert history.history['val_acc'][-1] > 0.8\n+    config = model.get_config()\n+    model = Sequential.from_config(config)\n \n \n if __name__ == '__main__':\n\n@@ -1,18 +1,13 @@\n from __future__ import print_function\n import numpy as np\n-np.random.seed(1337)\n import pytest\n import string\n \n from keras.utils.test_utils import get_test_data, keras_test\n from keras.utils.np_utils import to_categorical\n from keras.models import Sequential\n-from keras.layers import TimeDistributedDense\n-from keras.layers import Dense\n-from keras.layers import Activation\n-from keras.layers import GRU\n-from keras.layers import LSTM\n-from keras.layers import Embedding\n+from keras import layers\n+import keras\n \n \n @keras_test\n@@ -23,25 +18,59 @@ def test_temporal_classification():\n     single layer of GRU units and softmax applied\n     to the last activations of the units\n     '''\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=500,\n-                                                         input_shape=(3, 5),\n+    np.random.seed(1337)\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n+                                                         num_test=20,\n+                                                         input_shape=(3, 4),\n                                                          classification=True,\n-                                                         nb_class=2)\n+                                                         num_classes=2)\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n \n     model = Sequential()\n-    model.add(GRU(y_train.shape[-1],\n-                  input_shape=(X_train.shape[1], X_train.shape[2]),\n-                  activation='softmax'))\n+    model.add(layers.GRU(8,\n+                         input_shape=(x_train.shape[1], x_train.shape[2])))\n+    model.add(layers.Dense(y_train.shape[-1], activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n-                  optimizer='adagrad',\n+                  optimizer='rmsprop',\n                   metrics=['accuracy'])\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=32,\n-                        validation_data=(X_test, y_test),\n+    model.summary()\n+    history = model.fit(x_train, y_train, epochs=4, batch_size=10,\n+                        validation_data=(x_test, y_test),\n                         verbose=0)\n-    assert(history.history['val_acc'][-1] >= 0.8)\n+    assert(history.history['acc'][-1] >= 0.8)\n+    config = model.get_config()\n+    model = Sequential.from_config(config)\n+\n+\n+@keras_test\n+def test_temporal_classification_functional():\n+    '''\n+    Classify temporal sequences of float numbers\n+    of length 3 into 2 classes using\n+    single layer of GRU units and softmax applied\n+    to the last activations of the units\n+    '''\n+    np.random.seed(1337)\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n+                                                         num_test=20,\n+                                                         input_shape=(3, 4),\n+                                                         classification=True,\n+                                                         num_classes=2)\n+    y_train = to_categorical(y_train)\n+    y_test = to_categorical(y_test)\n+\n+    inputs = layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n+    x = layers.SimpleRNN(8)(inputs)\n+    outputs = layers.Dense(y_train.shape[-1], activation='softmax')(x)\n+    model = keras.models.Model(inputs, outputs)\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='rmsprop',\n+                  metrics=['accuracy'])\n+    history = model.fit(x_train, y_train, epochs=4, batch_size=10,\n+                        validation_data=(x_test, y_test),\n+                        verbose=0)\n+    assert(history.history['acc'][-1] >= 0.8)\n \n \n @keras_test\n@@ -50,41 +79,43 @@ def test_temporal_regression():\n     Predict float numbers (regression) based on sequences\n     of float numbers of length 3 using a single layer of GRU units\n     '''\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=400,\n+    np.random.seed(1337)\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n+                                                         num_test=20,\n                                                          input_shape=(3, 5),\n                                                          output_shape=(2,),\n                                                          classification=False)\n     model = Sequential()\n-    model.add(GRU(y_train.shape[-1],\n-                  input_shape=(X_train.shape[1], X_train.shape[2])))\n+    model.add(layers.LSTM(y_train.shape[-1],\n+                          input_shape=(x_train.shape[1], x_train.shape[2])))\n     model.compile(loss='hinge', optimizer='adam')\n-    history = model.fit(X_train, y_train, nb_epoch=5, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=0)\n-    assert(history.history['val_loss'][-1] < 1.)\n+    history = model.fit(x_train, y_train, epochs=5, batch_size=16,\n+                        validation_data=(x_test, y_test), verbose=0)\n+    assert(history.history['loss'][-1] < 1.)\n \n \n @keras_test\n-def test_sequence_to_sequence():\n+def test_3d_to_3d():\n     '''\n     Apply a same Dense layer for each element of time dimension of the input\n     and make predictions of the output sequence elements.\n     This does not make use of the temporal structure of the sequence\n     (see TimeDistributedDense for more details)\n     '''\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n+    np.random.seed(1337)\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=100,\n+                                                         num_test=20,\n                                                          input_shape=(3, 5),\n                                                          output_shape=(3, 5),\n                                                          classification=False)\n \n     model = Sequential()\n-    model.add(TimeDistributedDense(y_train.shape[-1],\n-                                   input_shape=(X_train.shape[1], X_train.shape[2])))\n+    model.add(layers.TimeDistributed(\n+        layers.Dense(y_train.shape[-1]), input_shape=(x_train.shape[1], x_train.shape[2])))\n     model.compile(loss='hinge', optimizer='rmsprop')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=0)\n-    assert(history.history['val_loss'][-1] < 0.8)\n+    history = model.fit(x_train, y_train, epochs=20, batch_size=16,\n+                        validation_data=(x_test, y_test), verbose=0)\n+    assert(history.history['loss'][-1] < 1.)\n \n \n @keras_test\n@@ -104,21 +135,21 @@ def test_stacked_lstm_char_prediction():\n     next_chars = [alphabet[i + sequence_length] for i in range(len(alphabet) - sequence_length)]\n \n     # Transform sequences and labels into 'one-hot' encoding\n-    X = np.zeros((len(sentences), sequence_length, number_of_chars), dtype=np.bool)\n+    x = np.zeros((len(sentences), sequence_length, number_of_chars), dtype=np.bool)\n     y = np.zeros((len(sentences), number_of_chars), dtype=np.bool)\n     for i, sentence in enumerate(sentences):\n         for t, char in enumerate(sentence):\n-            X[i, t, ord(char) - ord('a')] = 1\n+            x[i, t, ord(char) - ord('a')] = 1\n         y[i, ord(next_chars[i]) - ord('a')] = 1\n \n     # learn the alphabet with stacked LSTM\n     model = Sequential([\n-        LSTM(16, return_sequences=True, input_shape=(sequence_length, number_of_chars)),\n-        LSTM(16, return_sequences=False),\n-        Dense(number_of_chars, activation='softmax')\n+        layers.LSTM(16, return_sequences=True, input_shape=(sequence_length, number_of_chars)),\n+        layers.LSTM(16, return_sequences=False),\n+        layers.Dense(number_of_chars, activation='softmax')\n     ])\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(X, y, batch_size=1, nb_epoch=60, verbose=1)\n+    model.fit(x, y, batch_size=1, epochs=60, verbose=1)\n \n     # prime the model with 'ab' sequence and let it generate the learned alphabet\n     sentence = alphabet[:sequence_length]\n@@ -150,37 +181,28 @@ def test_masked_temporal():\n \n     '''\n     model = Sequential()\n-    model.add(Embedding(10, 20, mask_zero=True, input_length=20))\n-    model.add(TimeDistributedDense(10))\n-    model.add(Activation('softmax'))\n+    model.add(layers.Embedding(10, 10, mask_zero=True))\n+    model.add(layers.Activation('softmax'))\n     model.compile(loss='categorical_crossentropy',\n-                  optimizer='adam',\n-                  sample_weight_mode='temporal')\n+                  optimizer='adam')\n \n-    X = np.random.random_integers(1, 9, (50000, 20))\n-    for rowi in range(X.shape[0]):\n-        padding = np.random.random_integers(X.shape[1] / 2)\n-        X[rowi, :padding] = 0\n+    x = np.random.random_integers(1, 9, (20000, 10))\n+    for rowi in range(x.shape[0]):\n+        padding = np.random.random_integers(x.shape[1] / 2)\n+        x[rowi, :padding] = 0\n \n     # 50% of the time the correct output is the input.\n     # The other 50% of the time it's 2 * input % 10\n-    y = (X * np.random.random_integers(1, 2, X.shape)) % 10\n-    Y = np.zeros((y.size, 10), dtype='int32')\n+    y = (x * np.random.random_integers(1, 2, x.shape)) % 10\n+    ys = np.zeros((y.size, 10), dtype='int32')\n     for i, target in enumerate(y.flat):\n-        Y[i, target] = 1\n-    Y = Y.reshape(y.shape + (10,))\n+        ys[i, target] = 1\n+    ys = ys.reshape(y.shape + (10,))\n \n-    # Mask 50% of the outputs via sample weights\n-    sample_weight = np.random.random_integers(0, 1, y.shape)\n-    print('X shape:', X.shape)\n-    print('Y shape:', Y.shape)\n-    print('sample_weight shape:', Y.shape)\n-\n-    history = model.fit(X, Y, validation_split=0.05,\n-                        sample_weight=None,\n-                        verbose=1, nb_epoch=2)\n+    history = model.fit(x, ys, validation_split=0.05, batch_size=10,\n+                        verbose=0, epochs=3)\n     ground_truth = -np.log(0.5)\n-    assert(np.abs(history.history['val_loss'][-1] - ground_truth) < 0.06)\n+    assert(np.abs(history.history['loss'][-1] - ground_truth) < 0.06)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -1,10 +1,10 @@\n from __future__ import print_function\n-import numpy as np\n import pytest\n \n from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import Sequential\n-from keras.layers.core import Dense\n+from keras import layers\n+import keras\n from keras.utils.np_utils import to_categorical\n \n \n@@ -14,26 +14,52 @@ def test_vector_classification():\n     Classify random float vectors into 2 classes with logistic regression\n     using 2 layer neural network with ReLU hidden units.\n     '''\n-    np.random.seed(1337)\n-    nb_hidden = 10\n-\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n+                                                         num_test=200,\n                                                          input_shape=(20,),\n                                                          classification=True,\n-                                                         nb_class=2)\n+                                                         num_classes=2)\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n \n+    # Test with Sequential API\n     model = Sequential([\n-        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='relu'),\n-        Dense(y_train.shape[-1], activation='softmax')\n+        layers.Dense(16, input_shape=(x_train.shape[-1],), activation='relu'),\n+        layers.Dense(8),\n+        layers.Activation('relu'),\n+        layers.Dense(y_train.shape[-1], activation='softmax')\n     ])\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n-    history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16,\n-                        validation_data=(X_test, y_test),\n+    model.summary()\n+    history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n+                        validation_data=(x_test, y_test),\n+                        verbose=0)\n+    assert(history.history['val_acc'][-1] > 0.8)\n+    config = model.get_config()\n+    model = Sequential.from_config(config)\n+\n+\n+@keras_test\n+def test_vector_classification_functional():\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n+                                                         num_test=200,\n+                                                         input_shape=(20,),\n+                                                         classification=True,\n+                                                         num_classes=2)\n+    # Test with functional API\n+    inputs = layers.Input(shape=(x_train.shape[-1],))\n+    x = layers.Dense(16, activation=keras.activations.relu)(inputs)\n+    x = layers.Dense(8)(x)\n+    x = layers.Activation('relu')(x)\n+    outputs = layers.Dense(y_train.shape[-1], activation='softmax')(x)\n+    model = keras.models.Model(inputs, outputs)\n+    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n+                  optimizer=keras.optimizers.RMSprop(),\n+                  metrics=['acc'])\n+    history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n+                        validation_data=(x_test, y_test),\n                         verbose=0)\n     assert(history.history['val_acc'][-1] > 0.8)\n \n@@ -44,22 +70,20 @@ def test_vector_regression():\n     Perform float data prediction (regression) using 2 layer MLP\n     with tanh and sigmoid activations.\n     '''\n-    np.random.seed(1337)\n-    nb_hidden = 10\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n+                                                         num_test=200,\n                                                          input_shape=(20,),\n                                                          output_shape=(2,),\n                                                          classification=False)\n \n     model = Sequential([\n-        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='tanh'),\n-        Dense(y_train.shape[-1])\n+        layers.Dense(16, input_shape=(x_train.shape[-1],), activation='tanh'),\n+        layers.Dense(y_train.shape[-1])\n     ])\n \n     model.compile(loss='hinge', optimizer='adagrad')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=0)\n+    history = model.fit(x_train, y_train, epochs=20, batch_size=16,\n+                        validation_data=(x_test, y_test), verbose=0)\n     assert (history.history['val_loss'][-1] < 0.9)\n \n \n\n@@ -843,12 +843,12 @@ class TestBackend(object):\n \n     def test_one_hot(self):\n         input_length = 10\n-        nb_classes = 20\n+        num_classes = 20\n         batch_size = 30\n-        indices = np.random.randint(0, nb_classes, size=(batch_size, input_length))\n-        oh = np.eye(nb_classes)[indices]\n+        indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n+        oh = np.eye(num_classes)[indices]\n         for K in [KTH, KTF]:\n-            koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), nb_classes))\n+            koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n             assert np.all(koh == oh)\n \n     def test_sparse_dot(self):\n\n@@ -230,13 +230,13 @@ def test_multi_input_layer():\n     print('model.input_layers_tensor_indices:', model.input_layers_tensor_indices)\n     print('model.output_layers', model.output_layers)\n \n-    print('output_shape:', model.get_output_shape_for([(None, 32), (None, 32)]))\n-    assert model.get_output_shape_for([(None, 32), (None, 32)]) == [(None, 64), (None, 5)]\n+    print('output_shape:', model.compute_output_shape([(None, 32), (None, 32)]))\n+    assert model.compute_output_shape([(None, 32), (None, 32)]) == [(None, 64), (None, 5)]\n \n     assert model.compute_mask([a, b], [None, None]) == [None, None]\n \n-    print('output_shape:', model.get_output_shape_for([(None, 32), (None, 32)]))\n-    assert model.get_output_shape_for([(None, 32), (None, 32)]) == [(None, 64), (None, 5)]\n+    print('output_shape:', model.compute_output_shape([(None, 32), (None, 32)]))\n+    assert model.compute_output_shape([(None, 32), (None, 32)]) == [(None, 64), (None, 5)]\n \n     # we don't check names of first 2 layers (inputs) because\n     # ordering of same-level layers is not fixed\n@@ -320,8 +320,8 @@ def test_recursion():\n     print(model.compute_mask([e, f], [None, None]))\n     assert model.compute_mask([e, f], [None, None]) == [None, None]\n \n-    print(final_model.get_output_shape_for([(10, 32), (10, 32)]))\n-    assert final_model.get_output_shape_for([(10, 32), (10, 32)]) == [(10, 7), (10, 64)]\n+    print(final_model.compute_output_shape([(10, 32), (10, 32)]))\n+    assert final_model.compute_output_shape([(10, 32), (10, 32)]) == [(10, 7), (10, 64)]\n \n     # run recursive model\n     fn = K.function(final_model.inputs, final_model.outputs)\n@@ -496,7 +496,7 @@ def test_recursion():\n #     X_train = np.random.random((100, 784))\n #     Y_train = np.random.random((100, 10))\n \n-#     model.fit(X_train, Y_train, nb_epoch=2, batch_size=128)\n+#     model.fit(X_train, Y_train, epochs=2, batch_size=128)\n \n #     assert model.inputs == [inputs]\n #     assert model.outputs == [predictions]\n@@ -546,7 +546,7 @@ def test_recursion():\n #     data_a = np.random.random((1000, 4, 25))\n #     data_b = np.random.random((1000, 4, 25))\n #     labels = np.random.random((1000,))\n-#     model.fit([data_a, data_b], labels, nb_epoch=1)\n+#     model.fit([data_a, data_b], labels, epochs=1)\n \n #     model.summary()\n #     assert model.inputs == [tweet_a, tweet_b]\n@@ -617,7 +617,7 @@ def test_sequential_regression():\n     y = np.random.random((100, 8))\n     z = np.random.random((100, 6))\n     labels = np.random.random((100, 16))\n-    model.fit([x, y, z], labels, nb_epoch=1)\n+    model.fit([x, y, z], labels, epochs=1)\n \n     # test if Sequential can be called in the functional API\n \n@@ -630,7 +630,7 @@ def test_sequential_regression():\n     outer_model.compile(optimizer='rmsprop',\n                         loss='categorical_crossentropy',\n                         metrics=['accuracy'])\n-    outer_model.fit([x, y, z], labels, nb_epoch=1)\n+    outer_model.fit([x, y, z], labels, epochs=1)\n \n     # test serialization\n     config = outer_model.get_config()\n@@ -638,7 +638,7 @@ def test_sequential_regression():\n     outer_model.compile(optimizer='rmsprop',\n                         loss='categorical_crossentropy',\n                         metrics=['accuracy'])\n-    outer_model.fit([x, y, z], labels, nb_epoch=1)\n+    outer_model.fit([x, y, z], labels, epochs=1)\n \n \n if __name__ == \"__main__\":\n\n@@ -44,36 +44,36 @@ def test_model_methods():\n \n     # test fit\n     out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np], nb_epoch=1, batch_size=4)\n+                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    [output_a_np, output_b_np], nb_epoch=1, batch_size=4)\n+                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    nb_epoch=1, batch_size=4)\n+                    epochs=1, batch_size=4)\n \n     # test validation_split\n     out = model.fit([input_a_np, input_b_np],\n                     [output_a_np, output_b_np],\n-                    nb_epoch=1, batch_size=4, validation_split=0.5)\n+                    epochs=1, batch_size=4, validation_split=0.5)\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     [output_a_np, output_b_np],\n-                    nb_epoch=1, batch_size=4, validation_split=0.5)\n+                    epochs=1, batch_size=4, validation_split=0.5)\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    nb_epoch=1, batch_size=4, validation_split=0.5)\n+                    epochs=1, batch_size=4, validation_split=0.5)\n \n     # test validation data\n     out = model.fit([input_a_np, input_b_np],\n                     [output_a_np, output_b_np],\n-                    nb_epoch=1, batch_size=4,\n+                    epochs=1, batch_size=4,\n                     validation_data=([input_a_np, input_b_np], [output_a_np, output_b_np]))\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     [output_a_np, output_b_np],\n-                    nb_epoch=1, batch_size=4, validation_split=0.5,\n+                    epochs=1, batch_size=4, validation_split=0.5,\n                     validation_data=({'input_a': input_a_np, 'input_b': input_b_np}, [output_a_np, output_b_np]))\n     out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n                     {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    nb_epoch=1, batch_size=4, validation_split=0.5,\n+                    epochs=1, batch_size=4, validation_split=0.5,\n                     validation_data=({'input_a': input_a_np, 'input_b': input_b_np}, {'dense_1': output_a_np, 'dropout': output_b_np}))\n \n     # test_on_batch\n@@ -154,7 +154,7 @@ def test_model_methods():\n         trained_epochs.append(epoch)\n     tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin)\n     out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np], nb_epoch=5, batch_size=4,\n+                    [output_a_np, output_b_np], epochs=5, batch_size=4,\n                     initial_epoch=2, callbacks=[tracker_cb])\n     assert trained_epochs == [2, 3, 4]\n \n@@ -165,7 +165,7 @@ def test_model_methods():\n         while True:\n             yield ([np.random.random((batch_sz, 3)), np.random.random((batch_sz, 3))],\n                    [np.random.random((batch_sz, 4)), np.random.random((batch_sz, 3))])\n-    out = model.fit_generator(gen_data(4), samples_per_epoch=10, nb_epoch=5,\n+    out = model.fit_generator(gen_data(4), samples_per_epoch=10, epochs=5,\n                               initial_epoch=2, callbacks=[tracker_cb])\n     assert trained_epochs == [2, 3, 4]\n \n@@ -196,7 +196,7 @@ def test_model_methods():\n     output_a_np = np.random.random((10, 4))\n     output_b_np = np.random.random((10, 3))\n \n-    out = model.fit([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4, nb_epoch=1)\n+    out = model.fit([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4, epochs=1)\n     out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4)\n     out = model.predict([input_a_np, input_b_np], batch_size=4)\n \n\n@@ -66,11 +66,11 @@ def test_averagepooling_1d():\n \n @keras_test\n def test_convolution_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     filters = 2\n     stack_size = 3\n-    nb_row = 10\n-    nb_col = 6\n+    num_row = 10\n+    num_col = 6\n \n     for padding in _convolution_paddings:\n         for subsample in [(1, 1), (2, 2)]:\n@@ -79,55 +79,55 @@ def test_convolution_2d():\n \n             layer_test(convolutional.Convolution2D,\n                        kwargs={'filters': filters,\n-                               'nb_row': 3,\n-                               'nb_col': 3,\n+                               'num_row': 3,\n+                               'num_col': 3,\n                                'padding': padding,\n                                'subsample': subsample},\n-                       input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                       input_shape=(num_samples, num_row, num_col, stack_size))\n \n             layer_test(convolutional.Convolution2D,\n                        kwargs={'filters': filters,\n-                               'nb_row': 3,\n-                               'nb_col': 3,\n+                               'num_row': 3,\n+                               'num_col': 3,\n                                'padding': padding,\n                                'W_regularizer': 'l2',\n                                'b_regularizer': 'l2',\n                                'activity_regularizer': 'activity_l2',\n                                'subsample': subsample},\n-                       input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                       input_shape=(num_samples, num_row, num_col, stack_size))\n \n \n @keras_test\n def test_deconvolution_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     filters = 2\n     stack_size = 3\n-    nb_row = 10\n-    nb_col = 6\n+    num_row = 10\n+    num_col = 6\n \n-    for batch_size in [None, nb_samples]:\n+    for batch_size in [None, num_samples]:\n         for padding in _convolution_paddings:\n             for subsample in [(1, 1), (2, 2)]:\n                 if padding == 'same' and subsample != (1, 1):\n                     continue\n \n-                rows = conv_input_length(nb_row, 3, padding, subsample[0])\n-                cols = conv_input_length(nb_col, 3, padding, subsample[1])\n+                rows = conv_input_length(num_row, 3, padding, subsample[0])\n+                cols = conv_input_length(num_col, 3, padding, subsample[1])\n                 layer_test(convolutional.Deconvolution2D,\n                            kwargs={'filters': filters,\n-                                   'nb_row': 3,\n-                                   'nb_col': 3,\n+                                   'num_row': 3,\n+                                   'num_col': 3,\n                                    'output_shape': (batch_size, filters, rows, cols),\n                                    'padding': padding,\n                                    'subsample': subsample,\n                                    'data_format': 'channels_first'},\n-                           input_shape=(nb_samples, stack_size, nb_row, nb_col),\n+                           input_shape=(num_samples, stack_size, num_row, num_col),\n                            fixed_batch_size=True)\n \n                 layer_test(convolutional.Deconvolution2D,\n                            kwargs={'filters': filters,\n-                                   'nb_row': 3,\n-                                   'nb_col': 3,\n+                                   'num_row': 3,\n+                                   'num_col': 3,\n                                    'output_shape': (batch_size, filters, rows, cols),\n                                    'padding': padding,\n                                    'data_format': 'channels_first',\n@@ -135,17 +135,17 @@ def test_deconvolution_2d():\n                                    'b_regularizer': 'l2',\n                                    'activity_regularizer': 'activity_l2',\n                                    'subsample': subsample},\n-                           input_shape=(nb_samples, stack_size, nb_row, nb_col),\n+                           input_shape=(num_samples, stack_size, num_row, num_col),\n                            fixed_batch_size=True)\n \n \n @keras_test\n def test_atrous_conv_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     filters = 2\n     stack_size = 3\n-    nb_row = 10\n-    nb_col = 6\n+    num_row = 10\n+    num_col = 6\n \n     for padding in _convolution_paddings:\n         for subsample in [(1, 1), (2, 2)]:\n@@ -157,34 +157,34 @@ def test_atrous_conv_2d():\n \n                 layer_test(convolutional.AtrousConv2D,\n                            kwargs={'filters': filters,\n-                                   'nb_row': 3,\n-                                   'nb_col': 3,\n+                                   'num_row': 3,\n+                                   'num_col': 3,\n                                    'padding': padding,\n                                    'subsample': subsample,\n                                    'atrous_rate': atrous_rate},\n-                           input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                           input_shape=(num_samples, num_row, num_col, stack_size))\n \n                 layer_test(convolutional.AtrousConv2D,\n                            kwargs={'filters': filters,\n-                                   'nb_row': 3,\n-                                   'nb_col': 3,\n+                                   'num_row': 3,\n+                                   'num_col': 3,\n                                    'padding': padding,\n                                    'W_regularizer': 'l2',\n                                    'b_regularizer': 'l2',\n                                    'activity_regularizer': 'activity_l2',\n                                    'subsample': subsample,\n                                    'atrous_rate': atrous_rate},\n-                           input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                           input_shape=(num_samples, num_row, num_col, stack_size))\n \n \n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n @keras_test\n def test_separable_conv_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     filters = 6\n     stack_size = 3\n-    nb_row = 10\n-    nb_col = 6\n+    num_row = 10\n+    num_col = 6\n \n     for padding in _convolution_paddings:\n         for subsample in [(1, 1), (2, 2)]:\n@@ -194,17 +194,17 @@ def test_separable_conv_2d():\n \n                 layer_test(convolutional.SeparableConv2D,\n                            kwargs={'filters': filters,\n-                                   'nb_row': 3,\n-                                   'nb_col': 3,\n+                                   'num_row': 3,\n+                                   'num_col': 3,\n                                    'padding': padding,\n                                    'subsample': subsample,\n                                    'depth_multiplier': multiplier},\n-                           input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                           input_shape=(num_samples, num_row, num_col, stack_size))\n \n                 layer_test(convolutional.SeparableConv2D,\n                            kwargs={'filters': filters,\n-                                   'nb_row': 3,\n-                                   'nb_col': 3,\n+                                   'num_row': 3,\n+                                   'num_col': 3,\n                                    'padding': padding,\n                                    'depthwise_regularizer': 'l2',\n                                    'pointwise_regularizer': 'l2',\n@@ -214,7 +214,7 @@ def test_separable_conv_2d():\n                                    'depthwise_constraint': 'unitnorm',\n                                    'subsample': subsample,\n                                    'depth_multiplier': multiplier},\n-                           input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                           input_shape=(num_samples, num_row, num_col, stack_size))\n \n \n @keras_test\n@@ -283,7 +283,7 @@ def test_averagepooling_2d():\n \n @keras_test\n def test_convolution_3d():\n-    nb_samples = 2\n+    num_samples = 2\n     filters = 2\n     stack_size = 3\n     kernel_dim1 = 2\n@@ -306,7 +306,7 @@ def test_convolution_3d():\n                                'kernel_dim3': kernel_dim3,\n                                'padding': padding,\n                                'subsample': subsample},\n-                       input_shape=(nb_samples,\n+                       input_shape=(num_samples,\n                                     input_len_dim1, input_len_dim2, input_len_dim3,\n                                     stack_size))\n \n@@ -320,7 +320,7 @@ def test_convolution_3d():\n                                'b_regularizer': 'l2',\n                                'activity_regularizer': 'activity_l2',\n                                'subsample': subsample},\n-                       input_shape=(nb_samples,\n+                       input_shape=(num_samples,\n                                     input_len_dim1, input_len_dim2, input_len_dim3,\n                                     stack_size))\n \n@@ -351,10 +351,10 @@ def test_averagepooling_3d():\n \n @keras_test\n def test_zero_padding_1d():\n-    nb_samples = 2\n+    num_samples = 2\n     input_dim = 2\n-    nb_steps = 5\n-    shape = (nb_samples, nb_steps, input_dim)\n+    num_steps = 5\n+    shape = (num_samples, num_steps, input_dim)\n     input = np.ones(shape)\n \n     # basic test\n@@ -391,17 +391,17 @@ def test_zero_padding_1d():\n \n @keras_test\n def test_zero_padding_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     stack_size = 2\n-    input_nb_row = 4\n-    input_nb_col = 5\n+    input_num_row = 4\n+    input_num_col = 5\n     data_format = K.image_data_format()\n     assert data_format in {'channels_last', 'channels_first'}, 'data_format must be in {\"channels_last\", \"channels_first\"}'\n \n     if data_format == 'channels_last':\n-        input = np.ones((nb_samples, input_nb_row, input_nb_col, stack_size))\n+        input = np.ones((num_samples, input_num_row, input_num_col, stack_size))\n     elif data_format == 'channels_first':\n-        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n+        input = np.ones((num_samples, stack_size, input_num_row, input_num_col))\n \n     # basic test\n     layer_test(convolutional.ZeroPadding2D,\n@@ -458,13 +458,13 @@ def test_zero_padding_2d():\n \n \n def test_zero_padding_3d():\n-    nb_samples = 2\n+    num_samples = 2\n     stack_size = 2\n     input_len_dim1 = 4\n     input_len_dim2 = 5\n     input_len_dim3 = 3\n \n-    input = np.ones((nb_samples,\n+    input = np.ones((num_samples,\n                      input_len_dim1, input_len_dim2, input_len_dim3,\n                      stack_size))\n \n@@ -495,17 +495,17 @@ def test_upsampling_1d():\n \n @keras_test\n def test_upsampling_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     stack_size = 2\n-    input_nb_row = 11\n-    input_nb_col = 12\n+    input_num_row = 11\n+    input_num_col = 12\n \n     for data_format in ['channels_first', 'channels_last']:\n         if data_format == 'channels_first':\n-            input = np.random.rand(nb_samples, stack_size, input_nb_row,\n-                                   input_nb_col)\n+            input = np.random.rand(num_samples, stack_size, input_num_row,\n+                                   input_num_col)\n         else:  # tf\n-            input = np.random.rand(nb_samples, input_nb_row, input_nb_col,\n+            input = np.random.rand(num_samples, input_num_row, input_num_col,\n                                    stack_size)\n \n         for length_row in [2, 3, 9]:\n@@ -517,11 +517,11 @@ def test_upsampling_2d():\n                 output = layer(K.variable(input))\n                 np_output = K.eval(output)\n                 if data_format == 'channels_first':\n-                    assert np_output.shape[2] == length_row * input_nb_row\n-                    assert np_output.shape[3] == length_col * input_nb_col\n+                    assert np_output.shape[2] == length_row * input_num_row\n+                    assert np_output.shape[3] == length_col * input_num_col\n                 else:  # tf\n-                    assert np_output.shape[1] == length_row * input_nb_row\n-                    assert np_output.shape[2] == length_col * input_nb_col\n+                    assert np_output.shape[1] == length_row * input_num_row\n+                    assert np_output.shape[2] == length_col * input_num_col\n \n                 # compare with numpy\n                 if data_format == 'channels_first':\n@@ -535,7 +535,7 @@ def test_upsampling_2d():\n \n \n def test_upsampling_3d():\n-    nb_samples = 2\n+    num_samples = 2\n     stack_size = 2\n     input_len_dim1 = 10\n     input_len_dim2 = 11\n@@ -543,10 +543,10 @@ def test_upsampling_3d():\n \n     for data_format in ['channels_first', 'channels_last']:\n         if data_format == 'channels_first':\n-            input = np.random.rand(nb_samples, stack_size, input_len_dim1, input_len_dim2,\n+            input = np.random.rand(num_samples, stack_size, input_len_dim1, input_len_dim2,\n                                    input_len_dim3)\n         else:  # tf\n-            input = np.random.rand(nb_samples, input_len_dim1, input_len_dim2, input_len_dim3,\n+            input = np.random.rand(num_samples, input_len_dim1, input_len_dim2, input_len_dim3,\n                                    stack_size)\n         for length_dim1 in [2, 3, 9]:\n             for length_dim2 in [2, 3, 9]:\n@@ -581,10 +581,10 @@ def test_upsampling_3d():\n \n @keras_test\n def test_cropping_1d():\n-    nb_samples = 2\n+    num_samples = 2\n     time_length = 4\n     input_len_dim1 = 2\n-    input = np.random.rand(nb_samples, time_length, input_len_dim1)\n+    input = np.random.rand(num_samples, time_length, input_len_dim1)\n \n     layer_test(convolutional.Cropping1D,\n                kwargs={'cropping': (2, 2)},\n@@ -592,7 +592,7 @@ def test_cropping_1d():\n \n \n def test_cropping_2d():\n-    nb_samples = 2\n+    num_samples = 2\n     stack_size = 2\n     input_len_dim1 = 8\n     input_len_dim2 = 8\n@@ -600,10 +600,10 @@ def test_cropping_2d():\n     data_format = K.image_data_format()\n \n     if data_format == 'channels_first':\n-        input = np.random.rand(nb_samples, stack_size,\n+        input = np.random.rand(num_samples, stack_size,\n                                input_len_dim1, input_len_dim2)\n     else:\n-        input = np.random.rand(nb_samples,\n+        input = np.random.rand(num_samples,\n                                input_len_dim1, input_len_dim2,\n                                stack_size)\n     # basic test\n@@ -641,7 +641,7 @@ def test_cropping_2d():\n \n \n def test_cropping_3d():\n-    nb_samples = 2\n+    num_samples = 2\n     stack_size = 2\n     input_len_dim1 = 8\n     input_len_dim2 = 8\n@@ -650,10 +650,10 @@ def test_cropping_3d():\n     data_format = K.image_data_format()\n \n     if data_format == 'channels_first':\n-        input = np.random.rand(nb_samples, stack_size,\n+        input = np.random.rand(num_samples, stack_size,\n                                input_len_dim1, input_len_dim2, input_len_dim3)\n     else:\n-        input = np.random.rand(nb_samples,\n+        input = np.random.rand(num_samples,\n                                input_len_dim1, input_len_dim2,\n                                input_len_dim3, stack_size)\n     # basic test\n\n@@ -10,23 +10,23 @@ from keras import regularizers\n \n \n def test_recurrent_convolutional():\n-    nb_row = 3\n-    nb_col = 3\n-    nb_filter = 5\n-    nb_samples = 2\n+    num_row = 3\n+    num_col = 3\n+    filters = 5\n+    num_samples = 2\n     input_channel = 2\n-    input_nb_row = 5\n-    input_nb_col = 5\n+    input_num_row = 5\n+    input_num_col = 5\n     sequence_len = 2\n     for data_format in ['channels_first', 'channels_last']:\n \n         if data_format == 'channels_first':\n-            input = np.random.rand(nb_samples, sequence_len,\n+            input = np.random.rand(num_samples, sequence_len,\n                                    input_channel,\n-                                   input_nb_row, input_nb_col)\n+                                   input_num_row, input_num_col)\n         else:  # tf\n-            input = np.random.rand(nb_samples, sequence_len,\n-                                   input_nb_row, input_nb_col,\n+            input = np.random.rand(num_samples, sequence_len,\n+                                   input_num_row, input_num_col,\n                                    input_channel)\n \n         for return_sequences in [True, False]:\n@@ -34,18 +34,18 @@ def test_recurrent_convolutional():\n             output = layer_test(convolutional_recurrent.ConvLSTM2D,\n                                 kwargs={'data_format': data_format,\n                                         'return_sequences': return_sequences,\n-                                        'nb_filter': nb_filter,\n-                                        'nb_row': nb_row,\n-                                        'nb_col': nb_col,\n+                                        'filters': filters,\n+                                        'num_row': num_row,\n+                                        'num_col': num_col,\n                                         'border_mode': \"same\"},\n                                 input_shape=input.shape)\n \n-            output_shape = [nb_samples, input_nb_row, input_nb_col]\n+            output_shape = [num_samples, input_num_row, input_num_col]\n \n             if data_format == 'channels_first':\n-                output_shape.insert(1, nb_filter)\n+                output_shape.insert(1, filters)\n             else:\n-                output_shape.insert(3, nb_filter)\n+                output_shape.insert(3, filters)\n \n             if return_sequences:\n                 output_shape.insert(1, sequence_len)\n@@ -60,9 +60,9 @@ def test_recurrent_convolutional():\n             model = Sequential()\n             kwargs = {'data_format': data_format,\n                       'return_sequences': return_sequences,\n-                      'nb_filter': nb_filter,\n-                      'nb_row': nb_row,\n-                      'nb_col': nb_col,\n+                      'filters': filters,\n+                      'num_row': num_row,\n+                      'num_col': num_col,\n                       'stateful': True,\n                       'batch_input_shape': input.shape,\n                       'border_mode': \"same\"}\n@@ -99,9 +99,9 @@ def test_recurrent_convolutional():\n             # check regularizers\n             kwargs = {'data_format': data_format,\n                       'return_sequences': return_sequences,\n-                      'nb_filter': nb_filter,\n-                      'nb_row': nb_row,\n-                      'nb_col': nb_col,\n+                      'filters': filters,\n+                      'num_row': num_row,\n+                      'num_col': num_col,\n                       'stateful': True,\n                       'batch_input_shape': input.shape,\n                       'W_regularizer': regularizers.WeightRegularizer(l1=0.01),\n@@ -118,9 +118,9 @@ def test_recurrent_convolutional():\n             layer_test(convolutional_recurrent.ConvLSTM2D,\n                        kwargs={'data_format': data_format,\n                                'return_sequences': return_sequences,\n-                               'nb_filter': nb_filter,\n-                               'nb_row': nb_row,\n-                               'nb_col': nb_col,\n+                               'filters': filters,\n+                               'num_row': num_row,\n+                               'num_col': num_col,\n                                'border_mode': \"same\",\n                                'dropout_W': 0.1,\n                                'dropout_U': 0.1},\n\n@@ -31,7 +31,7 @@ def test_merge():\n         model = Model([input_a, input_b], merged)\n         model.compile('rmsprop', 'mse')\n \n-        expected_output_shape = model.get_output_shape_for(input_shapes)\n+        expected_output_shape = model.compute_output_shape(input_shapes)\n         actual_output_shape = model.predict(inputs).shape\n         assert expected_output_shape == actual_output_shape\n \n@@ -44,7 +44,7 @@ def test_merge():\n         model = Model([input_a, input_b], merged)\n         model.compile('rmsprop', 'mse')\n \n-        expected_output_shape = model.get_output_shape_for(input_shapes)\n+        expected_output_shape = model.compute_output_shape(input_shapes)\n         actual_output_shape = model.predict(inputs).shape\n         assert expected_output_shape == actual_output_shape\n \n@@ -55,7 +55,7 @@ def test_merge():\n                    mode=lambda tup: K.concatenate([tup[0], tup[1]]),\n                    output_shape=lambda tup: tup[0][:-1] + (tup[0][-1] + tup[1][-1],))\n     model = Model([input_a, input_b], merged)\n-    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n@@ -78,7 +78,7 @@ def test_merge():\n                    mode=fn_mode,\n                    output_shape=fn_output_shape)\n     model = Model([input_a, input_b], merged)\n-    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n@@ -101,7 +101,7 @@ def test_merge():\n     b = Masking()(input_b)\n     merged = merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n     model = Model([input_a, input_b], merged)\n-    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n@@ -124,7 +124,7 @@ def test_merge():\n                    output_shape=lambda tup: (tup[0][0], tup[0][1] + tup[1][1]) + tup[0][2:],\n                    output_mask=lambda tup: K.concatenate([tup[0], tup[1]]))\n     model = Model([input_a, input_b], merged)\n-    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n@@ -178,17 +178,17 @@ def test_merge_mask_2d():\n     # test sum\n     model_sum = Model([input_a, input_b], [merged_sum])\n     model_sum.compile(loss='mse', optimizer='sgd')\n-    model_sum.fit([rand(2, 3), rand(2, 3)], [rand(2, 3)], nb_epoch=1)\n+    model_sum.fit([rand(2, 3), rand(2, 3)], [rand(2, 3)], epochs=1)\n \n     # test concatenation\n     model_concat = Model([input_a, input_b], [merged_concat])\n     model_concat.compile(loss='mse', optimizer='sgd')\n-    model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], nb_epoch=1)\n+    model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], epochs=1)\n \n     # test concatenation with masked and non-masked inputs\n     model_concat = Model([input_a, input_b], [merged_concat_mixed])\n     model_concat.compile(loss='mse', optimizer='sgd')\n-    model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], nb_epoch=1)\n+    model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], epochs=1)\n \n \n @keras_test\n\n@@ -6,41 +6,41 @@ from keras.layers import local\n \n @keras_test\n def test_locallyconnected_1d():\n-    nb_samples = 2\n-    nb_steps = 8\n+    num_samples = 2\n+    num_steps = 8\n     input_dim = 5\n     filter_length = 3\n-    nb_filter = 4\n+    filters = 4\n \n     for border_mode in ['valid']:\n         for subsample_length in [1]:\n             if border_mode == 'same' and subsample_length != 1:\n                 continue\n             layer_test(local.LocallyConnected1D,\n-                       kwargs={'nb_filter': nb_filter,\n+                       kwargs={'filters': filters,\n                                'filter_length': filter_length,\n                                'border_mode': border_mode,\n                                'subsample_length': subsample_length},\n-                       input_shape=(nb_samples, nb_steps, input_dim))\n+                       input_shape=(num_samples, num_steps, input_dim))\n \n             layer_test(local.LocallyConnected1D,\n-                       kwargs={'nb_filter': nb_filter,\n+                       kwargs={'filters': filters,\n                                'filter_length': filter_length,\n                                'border_mode': border_mode,\n                                'W_regularizer': 'l2',\n                                'b_regularizer': 'l2',\n                                'activity_regularizer': 'activity_l2',\n                                'subsample_length': subsample_length},\n-                       input_shape=(nb_samples, nb_steps, input_dim))\n+                       input_shape=(num_samples, num_steps, input_dim))\n \n \n @keras_test\n def test_locallyconnected_2d():\n-    nb_samples = 8\n-    nb_filter = 3\n+    num_samples = 8\n+    filters = 3\n     stack_size = 4\n-    nb_row = 6\n-    nb_col = 10\n+    num_row = 6\n+    num_col = 10\n \n     for border_mode in ['valid']:\n         for subsample in [(1, 1), (2, 2)]:\n@@ -48,28 +48,28 @@ def test_locallyconnected_2d():\n                 continue\n \n             layer_test(local.LocallyConnected2D,\n-                       kwargs={'nb_filter': nb_filter,\n-                               'nb_row': 3,\n-                               'nb_col': 3,\n+                       kwargs={'filters': filters,\n+                               'num_row': 3,\n+                               'num_col': 3,\n                                'border_mode': border_mode,\n                                'W_regularizer': 'l2',\n                                'b_regularizer': 'l2',\n                                'activity_regularizer': 'activity_l2',\n                                'subsample': subsample,\n                                'data_format': 'channels_last'},\n-                       input_shape=(nb_samples, nb_row, nb_col, stack_size))\n+                       input_shape=(num_samples, num_row, num_col, stack_size))\n \n             layer_test(local.LocallyConnected2D,\n-                       kwargs={'nb_filter': nb_filter,\n-                               'nb_row': 3,\n-                               'nb_col': 3,\n+                       kwargs={'filters': filters,\n+                               'num_row': 3,\n+                               'num_col': 3,\n                                'border_mode': border_mode,\n                                'W_regularizer': 'l2',\n                                'b_regularizer': 'l2',\n                                'activity_regularizer': 'activity_l2',\n                                'subsample': subsample,\n                                'data_format': 'channels_first'},\n-                       input_shape=(nb_samples, stack_size, nb_row, nb_col))\n+                       input_shape=(num_samples, stack_size, num_row, num_col))\n \n \n if __name__ == '__main__':\n\n@@ -37,7 +37,7 @@ def test_batchnorm_mode_0_or_2():\n \n         # centered on 5.0, variance 10.0\n         X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n-        model.fit(X, X, nb_epoch=4, verbose=0)\n+        model.fit(X, X, epochs=4, verbose=0)\n         out = model.predict(X)\n         out -= K.eval(norm_m0.beta)\n         out /= K.eval(norm_m0.gamma)\n@@ -56,7 +56,7 @@ def test_batchnorm_mode_0_or_2_twice():\n     model.compile(loss='mse', optimizer='sgd')\n \n     X = np.random.normal(loc=5.0, scale=10.0, size=(20, 10, 5, 5))\n-    model.fit(X, X, nb_epoch=1, verbose=0)\n+    model.fit(X, X, epochs=1, verbose=0)\n     model.predict(X)\n \n \n@@ -69,7 +69,7 @@ def test_batchnorm_mode_0_convnet():\n \n     # centered on 5.0, variance 10.0\n     X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n-    model.fit(X, X, nb_epoch=4, verbose=0)\n+    model.fit(X, X, epochs=4, verbose=0)\n     out = model.predict(X)\n     out -= np.reshape(K.eval(norm_m0.beta), (1, 3, 1, 1))\n     out /= np.reshape(K.eval(norm_m0.gamma), (1, 3, 1, 1))\n\n@@ -11,7 +11,7 @@ from keras.utils.test_utils import keras_test\n \n from keras import backend as K\n \n-nb_samples, timesteps, embedding_dim, output_dim = 2, 5, 4, 3\n+num_samples, timesteps, embedding_dim, output_dim = 2, 5, 4, 3\n embedding_num = 12\n \n \n@@ -33,7 +33,7 @@ def test_return_sequences(layer_class):\n     layer_test(layer_class,\n                kwargs={'output_dim': output_dim,\n                        'return_sequences': True},\n-               input_shape=(nb_samples, timesteps, embedding_dim))\n+               input_shape=(num_samples, timesteps, embedding_dim))\n \n \n @rnn_test\n@@ -42,8 +42,8 @@ def test_dynamic_behavior(layer_class):\n     model = Sequential()\n     model.add(layer)\n     model.compile('sgd', 'mse')\n-    x = np.random.random((nb_samples, timesteps, embedding_dim))\n-    y = np.random.random((nb_samples, output_dim))\n+    x = np.random.random((num_samples, timesteps, embedding_dim))\n+    y = np.random.random((num_samples, output_dim))\n     model.train_on_batch(x, y)\n \n \n@@ -53,7 +53,7 @@ def test_dropout(layer_class):\n                kwargs={'output_dim': output_dim,\n                        'dropout_U': 0.1,\n                        'dropout_W': 0.1},\n-               input_shape=(nb_samples, timesteps, embedding_dim))\n+               input_shape=(num_samples, timesteps, embedding_dim))\n \n \n @rnn_test\n@@ -62,7 +62,7 @@ def test_implementation_mode(layer_class):\n         layer_test(layer_class,\n                    kwargs={'output_dim': output_dim,\n                            'consume_less': mode},\n-                   input_shape=(nb_samples, timesteps, embedding_dim))\n+                   input_shape=(num_samples, timesteps, embedding_dim))\n \n \n @rnn_test\n@@ -71,19 +71,19 @@ def test_statefulness(layer_class):\n     model.add(embeddings.Embedding(embedding_num, embedding_dim,\n                                    mask_zero=True,\n                                    input_length=timesteps,\n-                                   batch_input_shape=(nb_samples, timesteps)))\n+                                   batch_input_shape=(num_samples, timesteps)))\n     layer = layer_class(output_dim, return_sequences=False,\n                         stateful=True,\n                         weights=None)\n     model.add(layer)\n     model.compile(optimizer='sgd', loss='mse')\n-    out1 = model.predict(np.ones((nb_samples, timesteps)))\n-    assert(out1.shape == (nb_samples, output_dim))\n+    out1 = model.predict(np.ones((num_samples, timesteps)))\n+    assert(out1.shape == (num_samples, output_dim))\n \n     # train once so that the states change\n-    model.train_on_batch(np.ones((nb_samples, timesteps)),\n-                         np.ones((nb_samples, output_dim)))\n-    out2 = model.predict(np.ones((nb_samples, timesteps)))\n+    model.train_on_batch(np.ones((num_samples, timesteps)),\n+                         np.ones((num_samples, output_dim)))\n+    out2 = model.predict(np.ones((num_samples, timesteps)))\n \n     # if the state is not reset, output should be different\n     assert(out1.max() != out2.max())\n@@ -91,29 +91,29 @@ def test_statefulness(layer_class):\n     # check that output changes after states are reset\n     # (even though the model itself didn't change)\n     layer.reset_states()\n-    out3 = model.predict(np.ones((nb_samples, timesteps)))\n+    out3 = model.predict(np.ones((num_samples, timesteps)))\n     assert(out2.max() != out3.max())\n \n     # check that container-level reset_states() works\n     model.reset_states()\n-    out4 = model.predict(np.ones((nb_samples, timesteps)))\n+    out4 = model.predict(np.ones((num_samples, timesteps)))\n     assert_allclose(out3, out4, atol=1e-5)\n \n     # check that the call to `predict` updated the states\n-    out5 = model.predict(np.ones((nb_samples, timesteps)))\n+    out5 = model.predict(np.ones((num_samples, timesteps)))\n     assert(out4.max() != out5.max())\n \n     # Check masking\n     layer.reset_states()\n \n-    left_padded_input = np.ones((nb_samples, timesteps))\n+    left_padded_input = np.ones((num_samples, timesteps))\n     left_padded_input[0, :1] = 0\n     left_padded_input[1, :2] = 0\n     out6 = model.predict(left_padded_input)\n \n     layer.reset_states()\n \n-    right_padded_input = np.ones((nb_samples, timesteps))\n+    right_padded_input = np.ones((num_samples, timesteps))\n     right_padded_input[0, -1:] = 0\n     right_padded_input[1, -2:] = 0\n     out7 = model.predict(right_padded_input)\n@@ -124,11 +124,11 @@ def test_statefulness(layer_class):\n @rnn_test\n def test_regularizer(layer_class):\n     layer = layer_class(output_dim, return_sequences=False, weights=None,\n-                        batch_input_shape=(nb_samples, timesteps, embedding_dim),\n+                        batch_input_shape=(num_samples, timesteps, embedding_dim),\n                         W_regularizer=regularizers.WeightRegularizer(l1=0.01),\n                         U_regularizer=regularizers.WeightRegularizer(l1=0.01),\n                         b_regularizer='l2')\n-    shape = (nb_samples, timesteps, embedding_dim)\n+    shape = (num_samples, timesteps, embedding_dim)\n     layer.build(shape)\n     output = layer(K.variable(np.ones(shape)))\n     K.eval(output)\n@@ -154,13 +154,13 @@ def test_masking_layer():\n     model.add(Masking(input_shape=(3, 4)))\n     model.add(recurrent.LSTM(output_dim=5, return_sequences=True, unroll=False))\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(I, V, nb_epoch=1, batch_size=100, verbose=1)\n+    model.fit(I, V, epochs=1, batch_size=100, verbose=1)\n \n     model = Sequential()\n     model.add(Masking(input_shape=(3, 4)))\n     model.add(recurrent.LSTM(output_dim=5, return_sequences=True, unroll=True))\n     model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(I, V, nb_epoch=1, batch_size=100, verbose=1)\n+    model.fit(I, V, epochs=1, batch_size=100, verbose=1)\n \n \n @rnn_test\n\n@@ -14,7 +14,7 @@ def test_TimeDistributed():\n     model.add(wrappers.TimeDistributed(core.Dense(2), input_shape=(3, 4)))\n     model.add(core.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)), nb_epoch=1, batch_size=10)\n+    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)), epochs=1, batch_size=10)\n \n     # test config\n     model.get_config()\n@@ -58,7 +58,7 @@ def test_TimeDistributed():\n     model.add(core.Activation('relu'))\n     model.compile(optimizer='rmsprop', loss='mse')\n \n-    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n+    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n \n     # test wrapping Sequential model\n     model = Sequential()\n@@ -66,14 +66,14 @@ def test_TimeDistributed():\n     outer_model = Sequential()\n     outer_model.add(wrappers.TimeDistributed(model, input_shape=(3, 2)))\n     outer_model.compile(optimizer='rmsprop', loss='mse')\n-    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n+    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n \n     # test with functional API\n     x = Input(shape=(3, 2))\n     y = wrappers.TimeDistributed(model)(x)\n     outer_model = Model(x, y)\n     outer_model.compile(optimizer='rmsprop', loss='mse')\n-    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n+    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), epochs=1, batch_size=10)\n \n \n @keras_test\n@@ -88,21 +88,21 @@ def test_regularizers():\n @keras_test\n def test_Bidirectional():\n     rnn = recurrent.SimpleRNN\n-    nb_sample = 2\n+    samples = 2\n     dim = 2\n     timesteps = 2\n     output_dim = 2\n     for mode in ['sum', 'concat']:\n-        x = np.random.random((nb_sample, timesteps, dim))\n+        x = np.random.random((samples, timesteps, dim))\n         target_dim = 2 * output_dim if mode == 'concat' else output_dim\n-        y = np.random.random((nb_sample, target_dim))\n+        y = np.random.random((samples, target_dim))\n \n         # test with Sequential model\n         model = Sequential()\n         model.add(wrappers.Bidirectional(rnn(output_dim),\n                                          merge_mode=mode, input_shape=(timesteps, dim)))\n         model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, nb_epoch=1, batch_size=1)\n+        model.fit(x, y, epochs=1, batch_size=1)\n \n         # test config\n         model.get_config()\n@@ -115,21 +115,21 @@ def test_Bidirectional():\n                                          merge_mode=mode, input_shape=(timesteps, dim)))\n         model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n         model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, nb_epoch=1, batch_size=1)\n+        model.fit(x, y, epochs=1, batch_size=1)\n \n         # test with functional API\n         input = Input((timesteps, dim))\n         output = wrappers.Bidirectional(rnn(output_dim), merge_mode=mode)(input)\n         model = Model(input, output)\n         model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, nb_epoch=1, batch_size=1)\n+        model.fit(x, y, epochs=1, batch_size=1)\n \n         # Bidirectional and stateful\n         input = Input(batch_shape=(1, timesteps, dim))\n         output = wrappers.Bidirectional(rnn(output_dim, stateful=True), merge_mode=mode)(input)\n         model = Model(input, output)\n         model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, nb_epoch=1, batch_size=1)\n+        model.fit(x, y, epochs=1, batch_size=1)\n \n \n if __name__ == '__main__':\n\n@@ -15,7 +15,7 @@ def test_tokenizer():\n     texts = ['The cat sat on the mat.',\n              'The dog sat on the log.',\n              'Dogs and cats living together.']\n-    tokenizer = Tokenizer(nb_words=10)\n+    tokenizer = Tokenizer(num_words=10)\n     tokenizer.fit_on_texts(texts)\n \n     sequences = []\n\n@@ -17,8 +17,8 @@ from keras import backend as K\n from keras.utils import np_utils\n \n input_dim = 2\n-nb_hidden = 4\n-nb_class = 2\n+num_hidden = 4\n+num_class = 2\n batch_size = 5\n train_samples = 20\n test_samples = 20\n@@ -26,11 +26,11 @@ test_samples = 20\n \n def test_ModelCheckpoint():\n     filepath = 'checkpoint.h5'\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     # case 1\n@@ -39,8 +39,8 @@ def test_ModelCheckpoint():\n     mode = 'auto'\n \n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(nb_class, activation='softmax'))\n+    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n@@ -48,7 +48,7 @@ def test_ModelCheckpoint():\n     cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n     assert os.path.exists(filepath)\n     os.remove(filepath)\n \n@@ -57,7 +57,7 @@ def test_ModelCheckpoint():\n     cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n     assert os.path.exists(filepath)\n     os.remove(filepath)\n \n@@ -67,7 +67,7 @@ def test_ModelCheckpoint():\n     cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n     assert os.path.exists(filepath)\n     os.remove(filepath)\n \n@@ -76,7 +76,7 @@ def test_ModelCheckpoint():\n     cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                       save_best_only=save_best_only, mode=mode)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n     assert os.path.exists(filepath)\n     os.remove(filepath)\n \n@@ -89,7 +89,7 @@ def test_ModelCheckpoint():\n                                       save_best_only=save_best_only, mode=mode,\n                                       period=period)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=4)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=4)\n     assert os.path.exists(filepath.format(epoch=1))\n     assert os.path.exists(filepath.format(epoch=3))\n     assert not os.path.exists(filepath.format(epoch=0))\n@@ -99,16 +99,16 @@ def test_ModelCheckpoint():\n \n \n def test_EarlyStopping():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(nb_class, activation='softmax'))\n+    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n@@ -117,14 +117,14 @@ def test_EarlyStopping():\n     patience = 0\n     cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n     history = model.fit(X_train, y_train, batch_size=batch_size,\n-                        validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=20)\n+                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n \n     mode = 'auto'\n     monitor = 'val_acc'\n     patience = 2\n     cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n     history = model.fit(X_train, y_train, batch_size=batch_size,\n-                        validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=20)\n+                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n \n \n def test_EarlyStopping_reuse():\n@@ -149,40 +149,40 @@ def test_EarlyStopping_reuse():\n \n \n def test_LearningRateScheduler():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(nb_class, activation='softmax'))\n+    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n \n     cbks = [callbacks.LearningRateScheduler(lambda x: 1. / (1. + x))]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n     assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n \n \n def test_ReduceLROnPlateau():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n     def make_model():\n         np.random.seed(1337)\n         model = Sequential()\n-        model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(nb_class, activation='softmax'))\n+        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+        model.add(Dense(num_class, activation='softmax'))\n \n         model.compile(loss='categorical_crossentropy',\n                       optimizer=optimizers.SGD(lr=0.1),\n@@ -194,32 +194,32 @@ def test_ReduceLROnPlateau():\n     # This should reduce the LR after the first epoch (due to high epsilon).\n     cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=10, patience=1, cooldown=5)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5, verbose=2)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n     assert np.allclose(float(K.get_value(model.optimizer.lr)), 0.01, atol=K.epsilon())\n \n     model = make_model()\n     cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=0, patience=1, cooldown=5)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5, verbose=2)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n     assert np.allclose(float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())\n \n \n def test_CSVLogger():\n     filepath = 'log.tsv'\n     sep = '\\t'\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n     def make_model():\n         np.random.seed(1337)\n         model = Sequential()\n-        model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(nb_class, activation='softmax'))\n+        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+        model.add(Dense(num_class, activation='softmax'))\n \n         model.compile(loss='categorical_crossentropy',\n                       optimizer=optimizers.SGD(lr=0.1),\n@@ -230,7 +230,7 @@ def test_CSVLogger():\n     model = make_model()\n     cbks = [callbacks.CSVLogger(filepath, separator=sep)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n \n     assert os.path.exists(filepath)\n     with open(filepath) as csvfile:\n@@ -243,11 +243,11 @@ def test_CSVLogger():\n     model = make_model()\n     cbks = [callbacks.CSVLogger(filepath, separator=sep, append=True)]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n \n     # case 3, reuse of CSVLogger object\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n \n     import re\n     with open(filepath) as csvfile:\n@@ -264,11 +264,11 @@ def test_TensorBoard():\n \n     filepath = './logs'\n     (X_train, y_train), (X_test, y_test) = get_test_data(\n-        nb_train=train_samples,\n-        nb_test=test_samples,\n+        num_train=train_samples,\n+        num_test=test_samples,\n         input_shape=(input_dim,),\n         classification=True,\n-        nb_class=nb_class)\n+        num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -297,8 +297,8 @@ def test_TensorBoard():\n \n     # case 1 Sequential\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(nb_class, activation='softmax'))\n+    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n@@ -308,28 +308,28 @@ def test_TensorBoard():\n \n     # fit with validation data\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=3)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=3)\n \n     # fit with validation data and accuracy\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n \n     # fit generator with validation data\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         validation_data=(X_test, y_test),\n                         callbacks=cbks)\n \n     # fit generator without validation data\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         callbacks=cbks)\n \n     # fit generator with validation data and accuracy\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         validation_data=(X_test, y_test),\n                         callbacks=cbks)\n \n     # fit generator without validation data and accuracy\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                         callbacks=cbks)\n \n     assert os.path.exists(filepath)\n@@ -337,16 +337,16 @@ def test_TensorBoard():\n \n \n def test_LambdaCallback():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(nb_class, activation='softmax'))\n+    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n@@ -362,7 +362,7 @@ def test_LambdaCallback():\n \n     cbks = [cleanup_callback]\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n     p.join()\n     assert not p.is_alive()\n \n@@ -372,17 +372,17 @@ def test_LambdaCallback():\n def test_TensorBoard_with_ReduceLROnPlateau():\n     import shutil\n     filepath = './logs'\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=nb_class)\n+                                                         num_classes=num_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(nb_class, activation='softmax'))\n+    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(num_class, activation='softmax'))\n     model.compile(loss='binary_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n@@ -397,7 +397,7 @@ def test_TensorBoard_with_ReduceLROnPlateau():\n             log_dir=filepath)]\n \n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n \n     assert os.path.exists(filepath)\n     shutil.rmtree(filepath)\n\n@@ -34,15 +34,15 @@ def test_multiprocessing_training():\n \n     model.fit_generator(myGenerator(),\n                         samples_per_epoch=320,\n-                        nb_epoch=1,\n+                        epochs=1,\n                         verbose=1,\n                         max_q_size=10,\n-                        nb_worker=4,\n+                        workers=4,\n                         pickle_safe=True)\n \n     model.fit_generator(myGenerator(),\n                         samples_per_epoch=320,\n-                        nb_epoch=1,\n+                        epochs=1,\n                         verbose=1,\n                         max_q_size=10,\n                         pickle_safe=False)\n@@ -83,15 +83,15 @@ def test_multiprocessing_training_fromfile():\n \n     model.fit_generator(myGenerator(),\n                         samples_per_epoch=320,\n-                        nb_epoch=1,\n+                        epochs=1,\n                         verbose=1,\n                         max_q_size=10,\n-                        nb_worker=2,\n+                        workers=2,\n                         pickle_safe=True)\n \n     model.fit_generator(myGenerator(),\n                         samples_per_epoch=320,\n-                        nb_epoch=1,\n+                        epochs=1,\n                         verbose=1,\n                         max_q_size=10,\n                         pickle_safe=False)\n@@ -126,7 +126,7 @@ def test_multiprocessing_predicting():\n     model.predict_generator(myGenerator(),\n                             val_samples=320,\n                             max_q_size=10,\n-                            nb_worker=2,\n+                            workers=2,\n                             pickle_safe=True)\n     model.predict_generator(myGenerator(),\n                             val_samples=320,\n@@ -166,7 +166,7 @@ def test_multiprocessing_evaluating():\n     model.evaluate_generator(myGenerator(),\n                              val_samples=320,\n                              max_q_size=10,\n-                             nb_worker=2,\n+                             workers=2,\n                              pickle_safe=True)\n     model.evaluate_generator(myGenerator(),\n                              val_samples=320,\n@@ -199,7 +199,7 @@ def test_multiprocessing_fit_error():\n     with pytest.raises(Exception):\n         model.fit_generator(\n             myGenerator(), samples, 1,\n-            nb_worker=4, pickle_safe=True,\n+            workers=4, pickle_safe=True,\n         )\n \n     with pytest.raises(Exception):\n@@ -231,7 +231,7 @@ def test_multiprocessing_evaluate_error():\n     with pytest.raises(Exception):\n         model.evaluate_generator(\n             myGenerator(), samples, 1,\n-            nb_worker=4, pickle_safe=True,\n+            workers=4, pickle_safe=True,\n         )\n \n     with pytest.raises(Exception):\n@@ -263,7 +263,7 @@ def test_multiprocessing_predict_error():\n     with pytest.raises(Exception):\n         model.predict_generator(\n             myGenerator(), samples, 1,\n-            nb_worker=4, pickle_safe=True,\n+            workers=4, pickle_safe=True,\n         )\n \n     with pytest.raises(Exception):\n\n@@ -10,18 +10,18 @@ from keras.layers.core import Dense, Activation\n from keras.utils.np_utils import to_categorical\n \n \n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n-                                                     nb_test=200,\n+(X_train, y_train), (X_test, y_test) = get_test_data(num_train=1000,\n+                                                     num_test=200,\n                                                      input_shape=(10,),\n                                                      classification=True,\n-                                                     nb_class=2)\n+                                                     num_classes=2)\n y_train = to_categorical(y_train)\n y_test = to_categorical(y_test)\n \n \n-def get_model(input_dim, nb_hidden, output_dim):\n+def get_model(input_dim, num_hidden, output_dim):\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n     model.add(Dense(output_dim))\n     model.add(Activation('softmax'))\n@@ -33,7 +33,7 @@ def _test_optimizer(optimizer, target=0.89):\n     model.compile(loss='categorical_crossentropy',\n                   optimizer=optimizer,\n                   metrics=['accuracy'])\n-    history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16,\n+    history = model.fit(X_train, y_train, epochs=12, batch_size=16,\n                         validation_data=(X_test, y_test), verbose=2)\n     config = optimizer.get_config()\n     assert type(config) == dict\n\n@@ -13,9 +13,9 @@ from keras.datasets import mnist\n from keras.utils import np_utils\n from keras import regularizers\n \n-nb_classes = 10\n+num_classes = 10\n batch_size = 128\n-nb_epoch = 5\n+epochs = 5\n weighted_class = 9\n standard_weight = 1\n high_weight = 5\n@@ -34,8 +34,8 @@ def get_data():\n     # convert class vectors to binary class matrices\n     y_train = y_train[:max_train_samples]\n     y_test = y_test[:max_test_samples]\n-    Y_train = np_utils.to_categorical(y_train, nb_classes)\n-    Y_test = np_utils.to_categorical(y_test, nb_classes)\n+    Y_train = np_utils.to_categorical(y_train, num_classes)\n+    Y_test = np_utils.to_categorical(y_test, num_classes)\n     test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n     return (X_train, Y_train), (X_test, Y_test), test_ids\n@@ -56,7 +56,7 @@ def test_Eigenvalue_reg():\n     reg = regularizers.EigenvalueRegularizer(0.01)\n     model = create_model(weight_reg=reg)\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n     model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n \n \n@@ -69,7 +69,7 @@ def test_W_reg():\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n         assert len(model.losses) == 1\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch, verbose=0)\n+                  epochs=epochs, verbose=0)\n         model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n \n \n@@ -80,7 +80,7 @@ def test_A_reg():\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n         assert len(model.losses) == 1\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch, verbose=0)\n+                  epochs=epochs, verbose=0)\n         model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n \n \n\n@@ -16,27 +16,27 @@ from keras.engine.training import make_batches\n \n \n input_dim = 16\n-nb_hidden = 8\n-nb_class = 4\n+num_hidden = 8\n+num_class = 4\n batch_size = 32\n-nb_epoch = 1\n+epochs = 1\n \n \n @keras_test\n def test_sequential_pop():\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_dim=input_dim))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_hidden, input_dim=input_dim))\n+    model.add(Dense(num_class))\n     model.compile(loss='mse', optimizer='sgd')\n     x = np.random.random((batch_size, input_dim))\n-    y = np.random.random((batch_size, nb_class))\n-    model.fit(x, y, nb_epoch=1)\n+    y = np.random.random((batch_size, num_class))\n+    model.fit(x, y, epochs=1)\n     model.pop()\n     assert len(model.layers) == 1\n-    assert model.output_shape == (None, nb_hidden)\n+    assert model.output_shape == (None, num_hidden)\n     model.compile(loss='mse', optimizer='sgd')\n-    y = np.random.random((batch_size, nb_hidden))\n-    model.fit(x, y, nb_epoch=1)\n+    y = np.random.random((batch_size, num_hidden))\n+    model.fit(x, y, epochs=1)\n \n \n def _get_test_data():\n@@ -45,11 +45,11 @@ def _get_test_data():\n     train_samples = 100\n     test_samples = 50\n \n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                         nb_test=test_samples,\n+    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                         num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         nb_class=4)\n+                                                         num_classes=4)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     return (X_train, y_train), (X_test, y_test)\n@@ -74,19 +74,19 @@ def test_sequential_fit_generator():\n             i = i % max_batch_index\n \n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.pop()\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch)\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch, validation_data=(X_test, y_test))\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch,\n-                        validation_data=data_generator(False), nb_val_samples=batch_size * 3)\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch, max_q_size=2)\n+    model.fit_generator(data_generator(True), len(X_train), epochs)\n+    model.fit_generator(data_generator(True), len(X_train), epochs, validation_data=(X_test, y_test))\n+    model.fit_generator(data_generator(True), len(X_train), epochs,\n+                        validation_data=data_generator(False), num_val_samples=batch_size * 3)\n+    model.fit_generator(data_generator(True), len(X_train), epochs, max_q_size=2)\n     model.evaluate(X_train, y_train)\n \n \n@@ -106,16 +106,16 @@ def test_sequential():\n                 yield (x_batch, y_batch)\n \n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n     model.train_on_batch(X_train[:32], y_train[:32])\n \n@@ -135,9 +135,9 @@ def test_sequential():\n     fname = 'test_sequential_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n     model.load_weights(fname)\n@@ -163,9 +163,9 @@ def test_nested_sequential():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n     inner = Sequential()\n-    inner.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n     inner.add(Activation('relu'))\n-    inner.add(Dense(nb_class))\n+    inner.add(Dense(num_class))\n \n     middle = Sequential()\n     middle.add(inner)\n@@ -175,10 +175,10 @@ def test_nested_sequential():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n     model.train_on_batch(X_train[:32], y_train[:32])\n \n@@ -192,9 +192,9 @@ def test_nested_sequential():\n     model.save_weights(fname, overwrite=True)\n \n     inner = Sequential()\n-    inner.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n     inner.add(Activation('relu'))\n-    inner.add(Dense(nb_class))\n+    inner.add(Dense(num_class))\n \n     middle = Sequential()\n     middle.add(inner)\n@@ -225,23 +225,23 @@ def test_nested_sequential():\n def test_merge_sum():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n \n     right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n     right.add(Activation('relu'))\n \n     model = Sequential()\n     model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n \n@@ -253,14 +253,14 @@ def test_merge_sum():\n     fname = 'test_merge_sum_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n     right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n     right.add(Activation('relu'))\n     model = Sequential()\n     model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.load_weights(fname)\n     os.remove(fname)\n@@ -286,31 +286,31 @@ def test_merge_dot():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n     left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n     left.add(Activation('relu'))\n \n     right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n     right.add(Activation('relu'))\n \n     model = Sequential()\n     model.add(Merge([left, right], mode='dot', dot_axes=1))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n \n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n     left = Sequential()\n-    left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n     left.add(Activation('relu'))\n \n     right = Sequential()\n-    right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n+    right.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n     right.add(Activation('relu'))\n \n     model = Sequential()\n     model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n \n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -321,23 +321,23 @@ def test_merge_concat():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n     left = Sequential(name='branch_1')\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,), name='dense_1'))\n+    left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n     left.add(Activation('relu', name='relu_1'))\n \n     right = Sequential(name='branch_2')\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,), name='dense_2'))\n+    right.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_2'))\n     right.add(Activation('relu', name='relu_2'))\n \n     model = Sequential(name='merged_branches')\n     model.add(Merge([left, right], mode='concat', name='merge'))\n-    model.add(Dense(nb_class, name='final_dense'))\n+    model.add(Dense(num_class, name='final_dense'))\n     model.add(Activation('softmax', name='softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n \n@@ -348,7 +348,7 @@ def test_merge_concat():\n \n     fname = 'test_merge_concat_temp.h5'\n     model.save_weights(fname, overwrite=True)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n     model.load_weights(fname)\n     os.remove(fname)\n \n@@ -360,32 +360,32 @@ def test_merge_concat():\n def test_merge_recursivity():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n \n     right = Sequential()\n-    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n     right.add(Activation('relu'))\n \n     righter = Sequential()\n-    righter.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    righter.add(Dense(num_hidden, input_shape=(input_dim,)))\n     righter.add(Activation('relu'))\n \n     intermediate = Sequential()\n     intermediate.add(Merge([left, right], mode='sum'))\n-    intermediate.add(Dense(nb_hidden))\n+    intermediate.add(Dense(num_hidden))\n     intermediate.add(Activation('relu'))\n \n     model = Sequential()\n     model.add(Merge([intermediate, righter], mode='sum'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test, X_test], y_test, verbose=0)\n \n@@ -417,19 +417,19 @@ def test_merge_recursivity():\n def test_merge_overlap():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n-    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n \n     model = Sequential()\n     model.add(Merge([left, left], mode='sum'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n     model.train_on_batch(X_train[:32], y_train[:32])\n \n@@ -464,17 +464,17 @@ def test_merge_overlap():\n @keras_test\n def test_sequential_count_params():\n     input_dim = 20\n-    nb_units = 10\n-    nb_classes = 2\n+    num_units = 10\n+    num_classes = 2\n \n-    n = input_dim * nb_units + nb_units\n-    n += nb_units * nb_units + nb_units\n-    n += nb_units * nb_classes + nb_classes\n+    n = input_dim * num_units + num_units\n+    n += num_units * num_units + num_units\n+    n += num_units * num_classes + num_classes\n \n     model = Sequential()\n-    model.add(Dense(nb_units, input_shape=(input_dim,)))\n-    model.add(Dense(nb_units))\n-    model.add(Dense(nb_classes))\n+    model.add(Dense(num_units, input_shape=(input_dim,)))\n+    model.add(Dense(num_units))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.build()\n \n\n@@ -15,10 +15,10 @@ np.random.seed(1337)\n \r\n \r\n input_dim = 16\r\n-nb_hidden = 8\r\n-nb_class = 4\r\n+num_hidden = 8\r\n+num_class = 4\r\n batch_size = 32\r\n-nb_epoch = 1\r\n+epochs = 1\r\n \r\n \r\n def do_sparse():\r\n@@ -31,11 +31,11 @@ def test_sparse_mlp():\n         return\r\n \r\n     input = Input(batch_shape=(None, input_dim), sparse=True)\r\n-    hidden = Dense(nb_hidden, activation='relu')(input)\r\n-    hidden = Dense(nb_hidden, activation='relu')(hidden)\r\n-    predictions = Dense(nb_class, activation='sigmoid')(hidden)\r\n+    hidden = Dense(num_hidden, activation='relu')(input)\r\n+    hidden = Dense(num_hidden, activation='relu')(hidden)\r\n+    predictions = Dense(num_class, activation='sigmoid')(hidden)\r\n     model = Model(input=[input], output=predictions)\r\n     model.compile(loss='mse', optimizer='sgd')\r\n     x = sparse.rand(batch_size, input_dim, density=0.1, format='csr')\r\n-    y = np.random.random((batch_size, nb_class))\r\n-    model.fit(x, y, nb_epoch=1)\r\n+    y = np.random.random((batch_size, num_class))\r\n+    model.fit(x, y, epochs=1)\r\n\n@@ -11,19 +11,19 @@ from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n \n input_dim = 5\n hidden_dims = 5\n-nb_train = 100\n-nb_test = 50\n-nb_class = 3\n+num_train = 100\n+num_test = 50\n+num_class = 3\n batch_size = 32\n-nb_epoch = 1\n+epochs = 1\n verbosity = 0\n optim = 'adam'\n loss = 'categorical_crossentropy'\n \n np.random.seed(42)\n (X_train, y_train), (X_test, y_test) = get_test_data(\n-    nb_train=nb_train, nb_test=nb_test, input_shape=(input_dim,),\n-    classification=True, nb_class=nb_class)\n+    num_train=num_train, num_test=num_test, input_shape=(input_dim,),\n+    classification=True, num_classes=num_class)\n \n \n def build_fn_clf(hidden_dims):\n@@ -32,7 +32,7 @@ def build_fn_clf(hidden_dims):\n     model.add(Activation('relu'))\n     model.add(Dense(hidden_dims))\n     model.add(Activation('relu'))\n-    model.add(Dense(nb_class))\n+    model.add(Dense(num_class))\n     model.add(Activation('softmax'))\n     model.compile(optimizer='sgd', loss='categorical_crossentropy',\n                   metrics=['accuracy'])\n@@ -42,7 +42,7 @@ def build_fn_clf(hidden_dims):\n def test_clasify_build_fn():\n     clf = KerasClassifier(\n         build_fn=build_fn_clf, hidden_dims=hidden_dims,\n-        batch_size=batch_size, nb_epoch=nb_epoch)\n+        batch_size=batch_size, epochs=epochs)\n \n     assert_classification_works(clf)\n \n@@ -55,7 +55,7 @@ def test_clasify_class_build_fn():\n \n     clf = KerasClassifier(\n         build_fn=ClassBuildFnClf(), hidden_dims=hidden_dims,\n-        batch_size=batch_size, nb_epoch=nb_epoch)\n+        batch_size=batch_size, epochs=epochs)\n \n     assert_classification_works(clf)\n \n@@ -68,25 +68,25 @@ def test_clasify_inherit_class_build_fn():\n \n     clf = InheritClassBuildFnClf(\n         build_fn=None, hidden_dims=hidden_dims,\n-        batch_size=batch_size, nb_epoch=nb_epoch)\n+        batch_size=batch_size, epochs=epochs)\n \n     assert_classification_works(clf)\n \n \n def assert_classification_works(clf):\n-    clf.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+    clf.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n \n     score = clf.score(X_train, y_train, batch_size=batch_size)\n     assert np.isscalar(score) and np.isfinite(score)\n \n     preds = clf.predict(X_test, batch_size=batch_size)\n-    assert preds.shape == (nb_test, )\n+    assert preds.shape == (num_test, )\n     for prediction in np.unique(preds):\n-        assert prediction in range(nb_class)\n+        assert prediction in range(num_class)\n \n     proba = clf.predict_proba(X_test, batch_size=batch_size)\n-    assert proba.shape == (nb_test, nb_class)\n-    assert np.allclose(np.sum(proba, axis=1), np.ones(nb_test))\n+    assert proba.shape == (num_test, num_class)\n+    assert np.allclose(np.sum(proba, axis=1), np.ones(num_test))\n \n \n def build_fn_reg(hidden_dims=50):\n@@ -105,7 +105,7 @@ def build_fn_reg(hidden_dims=50):\n def test_regression_build_fn():\n     reg = KerasRegressor(\n         build_fn=build_fn_reg, hidden_dims=hidden_dims,\n-        batch_size=batch_size, nb_epoch=nb_epoch)\n+        batch_size=batch_size, epochs=epochs)\n \n     assert_regression_works(reg)\n \n@@ -118,7 +118,7 @@ def test_regression_class_build_fn():\n \n     reg = KerasRegressor(\n         build_fn=ClassBuildFnReg(), hidden_dims=hidden_dims,\n-        batch_size=batch_size, nb_epoch=nb_epoch)\n+        batch_size=batch_size, epochs=epochs)\n \n     assert_regression_works(reg)\n \n@@ -131,19 +131,19 @@ def test_regression_inherit_class_build_fn():\n \n     reg = InheritClassBuildFnReg(\n         build_fn=None, hidden_dims=hidden_dims,\n-        batch_size=batch_size, nb_epoch=nb_epoch)\n+        batch_size=batch_size, epochs=epochs)\n \n     assert_regression_works(reg)\n \n \n def assert_regression_works(reg):\n-    reg.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+    reg.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n \n     score = reg.score(X_train, y_train, batch_size=batch_size)\n     assert np.isscalar(score) and np.isfinite(score)\n \n     preds = reg.predict(X_test, batch_size=batch_size)\n-    assert preds.shape == (nb_test, )\n+    assert preds.shape == (num_test, )\n \n \n if __name__ == '__main__':\n@@ -151,11 +151,11 @@ if __name__ == '__main__':\n \n # Usage of sklearn's grid_search\n # from sklearn import grid_search\n-# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128], nb_epoch=[2], verbose=[0])\n+# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128], epochs=[2], verbose=[0])\n # classifier = Inherit_class_build_fn_clf()\n # clf = grid_search.GridSearchCV(classifier, parameters)\n # clf.fit(X_train, y_train)\n-# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128], nb_epoch=[2], verbose=[0])\n+# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128], epochs=[2], verbose=[0])\n # regressor = Inherit_class_build_fn_reg()\n # reg = grid_search.GridSearchCV(regressor, parameters, scoring='mean_squared_error', n_jobs=1, cv=2, verbose=2)\n # reg.fit(X_train_reg, y_train_reg)\n\n@@ -10,9 +10,9 @@ from keras.layers import Dense, Activation, RepeatVector, TimeDistributedDense,\n from keras.utils import np_utils\n from keras.utils.test_utils import keras_test\n \n-nb_classes = 10\n+num_classes = 10\n batch_size = 128\n-nb_epoch = 15\n+epochs = 15\n weighted_class = 5\n standard_weight = 1\n high_weight = 10\n@@ -22,18 +22,18 @@ timesteps = 3\n input_dim = 10\n loss = 'mse'\n \n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                     nb_test=test_samples,\n+(X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+                                                     num_test=test_samples,\n                                                      input_shape=(input_dim,),\n                                                      classification=True,\n-                                                     nb_class=nb_classes)\n+                                                     num_classes=num_classes)\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n+Y_train = np_utils.to_categorical(y_train, num_classes)\n+Y_test = np_utils.to_categorical(y_test, num_classes)\n test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n-class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n+class_weight = dict([(i, standard_weight) for i in range(num_classes)])\n class_weight[weighted_class] = high_weight\n \n sample_weight = np.ones((y_train.shape[0])) * standard_weight\n@@ -57,7 +57,7 @@ def create_sequential_model():\n     model = Sequential()\n     model.add(Dense(32, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(nb_classes))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     return model\n \n@@ -65,7 +65,7 @@ def create_sequential_model():\n def create_temporal_sequential_model():\n     model = Sequential()\n     model.add(GRU(32, input_shape=(timesteps, input_dim), return_sequences=True))\n-    model.add(TimeDistributedDense(nb_classes))\n+    model.add(TimeDistributedDense(num_classes))\n     model.add(Activation('softmax'))\n     return model\n \n@@ -76,22 +76,22 @@ def _test_weights_sequential(model, class_weight=None, sample_weight=None,\n                              X_test=X_test, Y_test=Y_test):\n     if sample_weight is not None:\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  epochs=epochs // 3, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight)\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  epochs=epochs // 3, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight,\n                   validation_split=0.1)\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 3, verbose=0,\n+                  epochs=epochs // 3, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight,\n                   validation_data=(X_train, Y_train, sample_weight))\n     else:\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 2, verbose=0,\n+                  epochs=epochs // 2, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight)\n         model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 2, verbose=0,\n+                  epochs=epochs // 2, verbose=0,\n                   class_weight=class_weight, sample_weight=sample_weight,\n                   validation_split=0.1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
