{"custom_id": "scrapy#928e7f2924811574e788e7083ce5339a5f83379f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 384 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -84,6 +84,6 @@ def _has_ajaxcrawlable_meta(text):\n \n     text = _script_re.sub(u'', text)\n     text = _noscript_re.sub(u'', text)\n-    text = html.remove_comments(html.remove_entities(text))\n+    text = html.remove_comments(html.replace_entities(text))\n     return _ajax_crawlable_re.search(text) is not None\n \n\n@@ -1,7 +1,7 @@\n import re\n from six.moves.urllib.parse import urljoin\n \n-from w3lib.html import remove_tags, remove_entities, replace_escape_chars\n+from w3lib.html import remove_tags, replace_entities, replace_escape_chars\n \n from scrapy.link import Link\n from .sgml import SgmlLinkExtractor\n@@ -21,7 +21,7 @@ class RegexLinkExtractor(SgmlLinkExtractor):\n         if base_url is None:\n             base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n \n-        clean_url = lambda u: urljoin(base_url, remove_entities(clean_link(u.decode(response_encoding))))\n+        clean_url = lambda u: urljoin(base_url, replace_entities(clean_link(u.decode(response_encoding))))\n         clean_text = lambda t: replace_escape_chars(remove_tags(t.decode(response_encoding))).strip()\n \n         links_text = linkre.findall(response_text)\n\n@@ -5,7 +5,7 @@ from importlib import import_module\n from pkgutil import iter_modules\n \n import six\n-from w3lib.html import remove_entities\n+from w3lib.html import replace_entities\n \n from scrapy.utils.python import flatten\n from scrapy.item import BaseItem\n@@ -94,9 +94,9 @@ def extract_regex(regex, text, encoding='utf-8'):\n     strings = flatten(strings)\n \n     if isinstance(text, unicode):\n-        return [remove_entities(s, keep=['lt', 'amp']) for s in strings]\n+        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n     else:\n-        return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n+        return [replace_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n \n def md5sum(file):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#480cfa199475d867cb51cb04ce76ed54809514f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 2 | Churn Cumulative: 394 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -124,7 +124,7 @@ except ImportError:\n else:\n     setup_args['install_requires'] = [\n         'Twisted>=10.0.0',\n-        'w3lib>=1.2',\n+        'w3lib>=1.8.0',\n         'queuelib',\n         'lxml',\n         'pyOpenSSL',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#029c51acef74e980114c22ef65efbf58c22e2397", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 1 | Churn Cumulative: 395 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -23,6 +23,7 @@ setup(\n         'console_scripts': ['scrapy = scrapy.cmdline:execute']\n     },\n     classifiers=[\n+        'Framework :: Scrapy',\n         'Development Status :: 5 - Production/Stable',\n         'Environment :: Console',\n         'Intended Audience :: Developers',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84fa004793cb1be07c7a3d0ac6fd80a83b4e8487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 63 | Churn Cumulative: 137 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,11 +3,14 @@ Base class for Scrapy spiders\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n+import warnings\n+\n from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n from scrapy.utils.deprecate import create_deprecated_class\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n class Spider(object_ref):\n@@ -32,18 +35,24 @@ class Spider(object_ref):\n         \"\"\"\n         log.msg(message, spider=self, level=level, **kw)\n \n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = cls(*args, **kwargs)\n+        spider._set_crawler(crawler)\n+        return spider\n+\n     def set_crawler(self, crawler):\n-        assert not hasattr(self, '_crawler'), \"Spider already bounded to %s\" % crawler\n-        self._crawler = crawler\n+        warnings.warn(\"set_crawler is deprecated, instantiate and bound the \"\n+                      \"spider to this crawler with from_crawler method \"\n+                      \"instead.\",\n+                      category=ScrapyDeprecationWarning, stacklevel=2)\n+        assert not hasattr(self, 'crawler'), \"Spider already bounded to a \" \\\n+                                             \"crawler\"\n+        self._set_crawler(crawler)\n \n-    @property\n-    def crawler(self):\n-        assert hasattr(self, '_crawler'), \"Spider not bounded to any crawler\"\n-        return self._crawler\n-\n-    @property\n-    def settings(self):\n-        return self.crawler.settings\n+    def _set_crawler(self, crawler):\n+        self.crawler = crawler\n+        self.settings = crawler.settings\n \n     def start_requests(self):\n         for url in self.start_urls:\n\n@@ -1,10 +1,12 @@\n import gzip\n import inspect\n import warnings\n-from scrapy.utils.trackref import object_ref\n from io import BytesIO\n-\n from twisted.trial import unittest\n+try:\n+    from unittest import mock\n+except ImportError:\n+    import mock\n \n from scrapy.spider import Spider, BaseSpider\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n@@ -13,6 +15,8 @@ from scrapy.contrib.spiders import CrawlSpider, Rule, XMLFeedSpider, \\\n     CSVFeedSpider, SitemapSpider\n from scrapy.contrib.linkextractors import LinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.trackref import object_ref\n+from scrapy.utils.test import get_crawler\n \n \n class SpiderTest(unittest.TestCase):\n@@ -46,6 +50,32 @@ class SpiderTest(unittest.TestCase):\n         self.assertRaises(ValueError, self.spider_class)\n         self.assertRaises(ValueError, self.spider_class, somearg='foo')\n \n+    def test_deprecated_set_crawler_method(self):\n+        spider = self.spider_class('example.com')\n+        crawler = get_crawler()\n+        with warnings.catch_warnings(record=True) as w:\n+            spider.set_crawler(crawler)\n+            self.assertIn(\"set_crawler\", str(w[0].message))\n+            self.assertTrue(hasattr(spider, 'crawler'))\n+            self.assertIs(spider.crawler, crawler)\n+            self.assertTrue(hasattr(spider, 'settings'))\n+            self.assertIs(spider.settings, crawler.settings)\n+\n+    def test_from_crawler_crawler_and_settings_population(self):\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, 'example.com')\n+        self.assertTrue(hasattr(spider, 'crawler'))\n+        self.assertIs(spider.crawler, crawler)\n+        self.assertTrue(hasattr(spider, 'settings'))\n+        self.assertIs(spider.settings, crawler.settings)\n+\n+    def test_from_crawler_init_call(self):\n+        with mock.patch.object(self.spider_class, '__init__',\n+                               return_value=None) as mock_init:\n+            self.spider_class.from_crawler(get_crawler(), 'example.com',\n+                                           foo='bar')\n+            mock_init.assert_called_once_with('example.com', foo='bar')\n+\n \n class InitSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb0253e5301ea54d4f37da8974d0dac295ebe871", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 31 | Churn Cumulative: 208 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,6 +86,13 @@ class CrawlSpider(Spider):\n             rule.process_links = get_method(rule.process_links)\n             rule.process_request = get_method(rule.process_request)\n \n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)\n+        spider._follow_links = crawler.settings.getbool(\n+            'CRAWLSPIDER_FOLLOW_LINKS', True)\n+        return spider\n+\n     def set_crawler(self, crawler):\n         super(CrawlSpider, self).set_crawler(crawler)\n         self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n\n@@ -220,6 +220,30 @@ class CrawlSpiderTest(SpiderTest):\n                            'http://example.org/about.html',\n                            'http://example.org/nofollow.html'])\n \n+    def test_follow_links_attribute_population(self):\n+        crawler = get_crawler()\n+        spider = self.spider_class.from_crawler(crawler, 'example.com')\n+        self.assertTrue(hasattr(spider, '_follow_links'))\n+        self.assertTrue(spider._follow_links)\n+\n+        crawler.settings.set('CRAWLSPIDER_FOLLOW_LINKS', False)\n+        spider = self.spider_class.from_crawler(crawler, 'example.com')\n+        self.assertTrue(hasattr(spider, '_follow_links'))\n+        self.assertFalse(spider._follow_links)\n+\n+    def test_follow_links_attribute_deprecated_population(self):\n+        spider = self.spider_class('example.com')\n+        self.assertFalse(hasattr(spider, '_follow_links'))\n+\n+        spider.set_crawler(get_crawler())\n+        self.assertTrue(hasattr(spider, '_follow_links'))\n+        self.assertTrue(spider._follow_links)\n+\n+        spider = self.spider_class('example.com')\n+        spider.set_crawler(get_crawler({'CRAWLSPIDER_FOLLOW_LINKS': False}))\n+        self.assertTrue(hasattr(spider, '_follow_links'))\n+        self.assertFalse(spider._follow_links)\n+\n \n class SitemapSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a995727117d10133a20553a648e85970fc6a6543", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 24 | Churn Cumulative: 185 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/spiders.rst\n import warnings\n \n from scrapy import log\n+from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n@@ -53,6 +54,7 @@ class Spider(object_ref):\n     def _set_crawler(self, crawler):\n         self.crawler = crawler\n         self.settings = crawler.settings\n+        crawler.signals.connect(self.close, signals.spider_closed)\n \n     def start_requests(self):\n         for url in self.start_urls:\n@@ -68,6 +70,12 @@ class Spider(object_ref):\n     def handles_request(cls, request):\n         return url_is_from_spider(request.url, cls)\n \n+    @staticmethod\n+    def close(spider, reason):\n+        closed = getattr(spider, 'closed', None)\n+        if callable(closed):\n+            return closed(reason)\n+\n     def __str__(self):\n         return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))\n \n\n@@ -8,6 +8,7 @@ try:\n except ImportError:\n     import mock\n \n+from scrapy import signals\n from scrapy.spider import Spider, BaseSpider\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.contrib.spiders.init import InitSpider\n@@ -76,6 +77,21 @@ class SpiderTest(unittest.TestCase):\n                                            foo='bar')\n             mock_init.assert_called_once_with('example.com', foo='bar')\n \n+    def test_closed_signal_call(self):\n+        class TestSpider(self.spider_class):\n+            closed_called = False\n+\n+            def closed(self, reason):\n+                self.closed_called = True\n+\n+        crawler = get_crawler()\n+        spider = TestSpider.from_crawler(crawler, 'example.com')\n+        crawler.signals.send_catch_log(signal=signals.spider_opened,\n+                                       spider=spider)\n+        crawler.signals.send_catch_log(signal=signals.spider_closed,\n+                                       spider=spider, reason=None)\n+        self.assertTrue(spider.closed_called)\n+\n \n class InitSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ae971468ff3a6712aa47ad3a5d2b9f0c9663b60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 52 | Churn Cumulative: 353 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import six\n import json\n+import copy\n import warnings\n from collections import MutableMapping\n from importlib import import_module\n@@ -46,6 +47,7 @@ class SettingsAttribute(object):\n class Settings(object):\n \n     def __init__(self, values=None, priority='project'):\n+        self.frozen = False\n         self.attributes = {}\n         self.setmodule(default_settings, priority='default')\n         if values is not None:\n@@ -93,6 +95,7 @@ class Settings(object):\n         raise ValueError(\"Cannot convert value for setting '%s' to dict: '%s'\" % (name, value))\n \n     def set(self, name, value, priority='project'):\n+        assert not self.frozen, \"Trying to modify an immutable Settings object\"\n         if isinstance(priority, six.string_types):\n             priority = SETTINGS_PRIORITIES[priority]\n         if name not in self.attributes:\n@@ -101,16 +104,29 @@ class Settings(object):\n             self.attributes[name].set(value, priority)\n \n     def setdict(self, values, priority='project'):\n+        assert not self.frozen, \"Trying to modify an immutable Settings object\"\n         for name, value in six.iteritems(values):\n             self.set(name, value, priority)\n \n     def setmodule(self, module, priority='project'):\n+        assert not self.frozen, \"Trying to modify an immutable Settings object\"\n         if isinstance(module, six.string_types):\n             module = import_module(module)\n         for key in dir(module):\n             if key.isupper():\n                 self.set(key, getattr(module, key), priority)\n \n+    def copy(self):\n+        return copy.deepcopy(self)\n+\n+    def freeze(self):\n+        self.frozen = True\n+\n+    def frozencopy(self):\n+        copy = self.copy()\n+        copy.freeze()\n+        return copy\n+\n     @property\n     def overrides(self):\n         warnings.warn(\"`Settings.overrides` attribute is deprecated and won't \"\n\n@@ -190,6 +190,42 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})\n         self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')\n \n+    def test_copy(self):\n+        values = {\n+            'TEST_BOOL': True,\n+            'TEST_LIST': ['one', 'two'],\n+            'TEST_LIST_OF_LISTS': [['first_one', 'first_two'],\n+                                   ['second_one', 'second_two']]\n+        }\n+        self.settings.setdict(values)\n+        copy = self.settings.copy()\n+        self.settings.set('TEST_BOOL', False)\n+        self.assertTrue(copy.get('TEST_BOOL'))\n+\n+        test_list = self.settings.get('TEST_LIST')\n+        test_list.append('three')\n+        self.assertListEqual(copy.get('TEST_LIST'), ['one', 'two'])\n+\n+        test_list_of_lists = self.settings.get('TEST_LIST_OF_LISTS')\n+        test_list_of_lists[0].append('first_three')\n+        self.assertListEqual(copy.get('TEST_LIST_OF_LISTS')[0],\n+                             ['first_one', 'first_two'])\n+\n+    def test_freeze(self):\n+        self.settings.freeze()\n+        with self.assertRaises(AssertionError) as cm:\n+            self.settings.set('TEST_BOOL', False)\n+            self.assertEqual(str(cm.exception),\n+                             \"Trying to modify an immutable Settings object\")\n+\n+    def test_frozencopy(self):\n+        with mock.patch.object(self.settings, 'copy') as mock_copy:\n+            with mock.patch.object(mock_copy, 'freeze') as mock_freeze:\n+                mock_object = self.settings.frozencopy()\n+                mock_copy.assert_call_once()\n+                mock_freeze.assert_call_once()\n+                self.assertEqual(mock_object, mock_copy.return_value)\n+\n     def test_deprecated_attribute_overrides(self):\n         self.settings.set('BAR', 'fuz', priority='cmdline')\n         with warnings.catch_warnings(record=True) as w:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39c6a80f9db6ec04cac59f116ee9620c3d540be0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 19 | Churn Cumulative: 334 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -76,23 +76,16 @@ class Settings(object):\n         return float(self.get(name, default))\n \n     def getlist(self, name, default=None):\n-        value = self.get(name)\n-        if value is None:\n-            return default or []\n-        elif hasattr(value, '__iter__'):\n-            return value\n-        else:\n-            return str(value).split(',')\n+        value = self.get(name, default or [])\n+        if isinstance(value, six.string_types):\n+            value = value.split(',')\n+        return list(value)\n \n     def getdict(self, name, default=None):\n-        value = self.get(name)\n-        if value is None:\n-            return default or {}\n+        value = self.get(name, default or {})\n         if isinstance(value, six.string_types):\n             value = json.loads(value)\n-        if isinstance(value, dict):\n-            return value\n-        raise ValueError(\"Cannot convert value for setting '%s' to dict: '%s'\" % (name, value))\n+        return dict(value)\n \n     def set(self, name, value, priority='project'):\n         assert not self.frozen, \"Trying to modify an immutable Settings object\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7038b2a136a2b79df9fb16d7b3327cb29f9c46f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 52 | Files Changed: 4 | Hunks: 16 | Methods Changed: 19 | Complexity Δ (Sum/Max): -5/1 | Churn Δ: 78 | Churn Cumulative: 282 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,10 +2,12 @@ from zope.interface import Interface\n \n class ISpiderManager(Interface):\n \n-    def create(spider_name, **spider_args):\n-        \"\"\"Returns a new Spider instance for the given spider name, and using\n-        the given spider arguments. If the spider name is not found, it must\n-        raise a KeyError.\"\"\"\n+    def from_settings(settings):\n+        \"\"\"Returns an instance of the class for the given settings\"\"\"\n+\n+    def load(spider_name):\n+        \"\"\"Returns the Spider class for the given spider name. If the spider\n+        name is not found, it must raise a KeyError.\"\"\"\n \n     def list():\n         \"\"\"Return a list with the names of all spiders available in the\n\n@@ -6,7 +6,6 @@ spiders\n from zope.interface import implementer\n import six\n \n-from scrapy import signals\n from scrapy.interfaces import ISpiderManager\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n@@ -15,8 +14,8 @@ from scrapy.utils.spider import iter_spider_classes\n @implementer(ISpiderManager)\n class SpiderManager(object):\n \n-    def __init__(self, spider_modules):\n-        self.spider_modules = spider_modules\n+    def __init__(self, settings):\n+        self.spider_modules = settings['SPIDER_MODULES']\n         self._spiders = {}\n         for name in self.spider_modules:\n             for module in walk_modules(name):\n@@ -28,33 +27,17 @@ class SpiderManager(object):\n \n     @classmethod\n     def from_settings(cls, settings):\n-        return cls(settings.getlist('SPIDER_MODULES'))\n+        return cls(settings)\n \n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        sm = cls.from_settings(crawler.settings)\n-        sm.crawler = crawler\n-        crawler.signals.connect(sm.close_spider, signals.spider_closed)\n-        return sm\n-\n-    def create(self, spider_name, **spider_kwargs):\n+    def load(self, spider_name):\n         try:\n-            spcls = self._spiders[spider_name]\n+            return self._spiders[spider_name]\n         except KeyError:\n-            raise KeyError(\"Spider not found: %s\" % spider_name)\n-        if hasattr(self, 'crawler') and hasattr(spcls, 'from_crawler'):\n-            return spcls.from_crawler(self.crawler, **spider_kwargs)\n-        else:\n-            return spcls(**spider_kwargs)\n+            raise KeyError(\"Spider not found: {}\".format(spider_name))\n \n     def find_by_request(self, request):\n         return [name for name, cls in six.iteritems(self._spiders)\n             if cls.handles_request(request)]\n \n     def list(self):\n-        return self._spiders.keys()\n-\n-    def close_spider(self, spider, reason):\n-        closed = getattr(spider, 'closed', None)\n-        if callable(closed):\n-            return closed(reason)\n+        return list(self._spiders.keys())\n\n@@ -10,6 +10,7 @@ from twisted.trial import unittest\n # alone\n from scrapy.interfaces import ISpiderManager\n from scrapy.spidermanager import SpiderManager\n+from scrapy.settings import Settings\n from scrapy.http import Request\n \n module_dir = os.path.dirname(os.path.abspath(__file__))\n@@ -23,7 +24,8 @@ class SpiderManagerTest(unittest.TestCase):\n         self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n         shutil.copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n-        self.spiderman = SpiderManager(['test_spiders_xxx'])\n+        settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n+        self.spiderman = SpiderManager.from_settings(settings)\n \n     def tearDown(self):\n         del self.spiderman\n@@ -35,14 +37,11 @@ class SpiderManagerTest(unittest.TestCase):\n \n     def test_list(self):\n         self.assertEqual(set(self.spiderman.list()),\n-            set(['spider1', 'spider2', 'spider3', 'spider4']))\n+            set(['spider1', 'spider2', 'spider3']))\n \n-    def test_create(self):\n-        spider1 = self.spiderman.create(\"spider1\")\n-        self.assertEqual(spider1.__class__.__name__, 'Spider1')\n-        spider2 = self.spiderman.create(\"spider2\", foo=\"bar\")\n-        self.assertEqual(spider2.__class__.__name__, 'Spider2')\n-        self.assertEqual(spider2.foo, 'bar')\n+    def test_load(self):\n+        spider1 = self.spiderman.load(\"spider1\")\n+        self.assertEqual(spider1.__name__, 'Spider1')\n \n     def test_find_by_request(self):\n         self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy1.org/test')),\n@@ -59,13 +58,13 @@ class SpiderManagerTest(unittest.TestCase):\n             ['spider3'])\n \n     def test_load_spider_module(self):\n-        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider1'])\n+        module = 'tests.test_spidermanager.test_spiders.spider1'\n+        settings = Settings({'SPIDER_MODULES': [module]})\n+        self.spiderman = SpiderManager.from_settings(settings)\n         assert len(self.spiderman._spiders) == 1\n \n     def test_load_base_spider(self):\n-        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider0'])\n+        module = 'tests.test_spidermanager.test_spiders.spider0'\n+        settings = Settings({'SPIDER_MODULES': [module]})\n+        self.spiderman = SpiderManager.from_settings(settings)\n         assert len(self.spiderman._spiders) == 0\n-\n-    def test_load_from_crawler(self):\n-        spider = self.spiderman.create('spider4', a='OK')\n-        self.assertEqual(spider.a, 'OK')\n\n@@ -1,10 +0,0 @@\n-from scrapy.spider import Spider\n-\n-class Spider4(Spider):\n-    name = \"spider4\"\n-\n-    @classmethod\n-    def from_crawler(cls, crawler, **kwargs):\n-        o = cls(**kwargs)\n-        o.crawler = crawler\n-        return o\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#980e30a18758f21f416036c2076bf98630c70193", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 35 | Files Changed: 1 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 68 | Churn Cumulative: 872 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,16 +13,22 @@ from scrapy import log, signals\n \n class Crawler(object):\n \n-    def __init__(self, settings):\n-        self.configured = False\n+    def __init__(self, spidercls, settings):\n+        self.spidercls = spidercls\n         self.settings = settings\n         self.signals = SignalManager(self)\n-        self.stats = load_object(settings['STATS_CLASS'])(self)\n-        self._start_requests = lambda: ()\n-        self._spider = None\n-        # TODO: move SpiderManager to CrawlerProcess\n+        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n+        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n+        self.logformatter = lf_cls.from_crawler(self)\n+        self.extensions = ExtensionManager.from_crawler(self)\n+\n+        # Attribute kept for backward compatibility (Use CrawlerRunner.spiders)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n-        self.spiders = spman_cls.from_crawler(self)\n+        self.spiders = spman_cls.from_settings(self.settings)\n+\n+        self.crawling = False\n+        self.spider = None\n+        self.engine = None\n \n     def install(self):\n         # TODO: remove together with scrapy.project.crawler usage\n@@ -36,39 +42,31 @@ class Crawler(object):\n         assert hasattr(scrapy.project, 'crawler'), \"crawler not installed\"\n         del scrapy.project.crawler\n \n-    def configure(self):\n-        if self.configured:\n-            return\n-\n-        self.configured = True\n-        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n-        self.logformatter = lf_cls.from_crawler(self)\n-        self.extensions = ExtensionManager.from_crawler(self)\n-        self.engine = ExecutionEngine(self, self._spider_closed)\n-\n-    def crawl(self, spider, requests=None):\n-        assert self._spider is None, 'Spider already attached'\n-        self._spider = spider\n-        spider.set_crawler(self)\n-        if requests is None:\n-            self._start_requests = spider.start_requests\n-        else:\n-            self._start_requests = lambda: requests\n-\n-    def _spider_closed(self, spider=None):\n-        if not self.engine.open_spiders:\n-            self.stop()\n-\n     @defer.inlineCallbacks\n-    def start(self):\n-        yield defer.maybeDeferred(self.configure)\n-        if self._spider:\n-            yield self.engine.open_spider(self._spider, self._start_requests())\n+    def crawl(self, *args, **kwargs):\n+        assert not self.crawling, \"Crawling already taking place\"\n+        self.crawling = True\n+\n+        try:\n+            self.spider = self._create_spider(*args, **kwargs)\n+            self.engine = self._create_engine()\n+            start_requests = iter(self.spider.start_requests())\n+            yield self.engine.open_spider(self.spider, start_requests)\n             yield defer.maybeDeferred(self.engine.start)\n+        except Exception:\n+            self.crawling = False\n+            raise\n+\n+    def _create_spider(self, *args, **kwargs):\n+        return self.spidercls.from_crawler(self, *args, **kwargs)\n+\n+    def _create_engine(self):\n+        return ExecutionEngine(self, lambda _: self.stop())\n \n     @defer.inlineCallbacks\n     def stop(self):\n-        if self.configured and self.engine.running:\n+        if self.crawling:\n+            self.crawling = False\n             yield defer.maybeDeferred(self.engine.stop)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d40273561dad76a409f847b5f8ce1daafdb1dc7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 48 | Files Changed: 1 | Hunks: 12 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 98 | Churn Cumulative: 970 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import six\n import signal\n \n from twisted.internet import reactor, defer\n@@ -70,31 +71,50 @@ class Crawler(object):\n             yield defer.maybeDeferred(self.engine.stop)\n \n \n-class CrawlerProcess(object):\n-    \"\"\" A class to run multiple scrapy crawlers in a process sequentially\"\"\"\n+class CrawlerRunner(object):\n \n     def __init__(self, settings):\n-        install_shutdown_handlers(self._signal_shutdown)\n         self.settings = settings\n-        self.crawlers = {}\n-        self.stopping = False\n-        self._started = None\n+        smcls = load_object(settings['SPIDER_MANAGER_CLASS'])\n+        self.spiders = smcls.from_settings(settings.frozencopy())\n+        self.crawlers = set()\n+        self.crawl_deferreds = set()\n \n-    def create_crawler(self, name=None):\n-        if name not in self.crawlers:\n-            self.crawlers[name] = Crawler(self.settings)\n+    def crawl(self, spidercls, *args, **kwargs):\n+        crawler = self._create_logged_crawler(spidercls)\n+        self.crawlers.add(crawler)\n \n-        return self.crawlers[name]\n+        crawler.install()\n+        crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n \n-    def start(self):\n-        if self.start_crawling():\n-            self.start_reactor()\n+        d = crawler.crawl(*args, **kwargs)\n+        self.crawl_deferreds.add(d)\n+        return d\n+\n+    def _create_logged_crawler(self, spidercls):\n+        crawler = self._create_crawler(spidercls)\n+        log_observer = log.start_from_crawler(crawler)\n+        if log_observer:\n+            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n+        return crawler\n+\n+    def _create_crawler(self, spidercls):\n+        if isinstance(spidercls, six.string_types):\n+            spidercls = self.spiders.load(spidercls)\n+        crawler = Crawler(spidercls, self.settings.frozencopy())\n+        return crawler\n \n-    @defer.inlineCallbacks\n     def stop(self):\n-        self.stopping = True\n-        if self._active_crawler:\n-            yield self._active_crawler.stop()\n+        return defer.DeferredList(c.stop() for c in self.crawlers)\n+\n+\n+class CrawlerProcess(CrawlerRunner):\n+    \"\"\"A class to run multiple scrapy crawlers in a process simultaneously\"\"\"\n+\n+    def __init__(self, settings):\n+        super(CrawlerProcess, self).__init__(settings)\n+        install_shutdown_handlers(self._signal_shutdown)\n+        self.stopping = False\n \n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n@@ -110,43 +130,25 @@ class CrawlerProcess(object):\n                 level=log.INFO, signame=signame)\n         reactor.callFromThread(self._stop_reactor)\n \n-    # ------------------------------------------------------------------------#\n-    # The following public methods can't be considered stable and may change at\n-    # any moment.\n-    #\n-    # start_crawling and start_reactor are called from scrapy.commands.shell\n-    # They are splitted because reactor is started on a different thread than IPython shell.\n-    #\n-    def start_crawling(self):\n-        log.scrapy_info(self.settings)\n-        return self._start_crawler() is not None\n+    def start(self, stop_after_crawl=True):\n+        self._start_logging()\n+        self._start_reactor(stop_after_crawl)\n \n-    def start_reactor(self):\n+    def _start_logging(self):\n+        log.scrapy_info(self.settings)\n+\n+    def _start_reactor(self, stop_after_crawl=True):\n+        if stop_after_crawl:\n+            d = defer.DeferredList(self.crawl_deferreds)\n+            if d.called:\n+                # Don't start the reactor if the deferreds are already fired\n+                return\n+            d.addBoth(lambda _: self._stop_reactor())\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n-    def _start_crawler(self):\n-        if not self.crawlers or self.stopping:\n-            return\n-\n-        name, crawler = self.crawlers.popitem()\n-        self._active_crawler = crawler\n-        log_observer = log.start_from_crawler(crawler)\n-        crawler.configure()\n-        crawler.install()\n-        crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n-        if log_observer:\n-            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n-        crawler.signals.connect(self._check_done, signals.engine_stopped)\n-        crawler.start()\n-        return name, crawler\n-\n-    def _check_done(self, **kwargs):\n-        if not self._start_crawler():\n-            self._stop_reactor()\n-\n     def _stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#870438e5f4665de836f7ff423055895b305a4e7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 155 | Lines Deleted: 166 | Files Changed: 20 | Hunks: 82 | Methods Changed: 52 | Complexity Δ (Sum/Max): 73/19 | Churn Δ: 321 | Churn Cumulative: 592 | Contributors (this commit): 7 | Commits (past 90d): 46 | Contributors (cumulative): 46 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,15 +20,17 @@ def assert_aws_environ():\n     if 'AWS_ACCESS_KEY_ID' not in os.environ:\n         raise SkipTest(\"AWS keys not found\")\n \n-def get_crawler(settings_dict=None):\n+def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n     priority.\n     \"\"\"\n-    from scrapy.crawler import Crawler\n+    from scrapy.crawler import CrawlerRunner\n     from scrapy.settings import Settings\n+    from scrapy.spider import Spider\n \n-    return Crawler(Settings(settings_dict))\n+    runner = CrawlerRunner(Settings(settings_dict))\n+    return runner._create_crawler(spidercls or Spider)\n \n def get_pythonpath():\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n@@ -62,10 +64,3 @@ def assert_samelines(testcase, text1, text2, msg=None):\n     line endings between platforms\n     \"\"\"\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n-\n-def docrawl(spider, settings=None):\n-    \"\"\"Configure and start Crawler; return the result of crawler.start()\"\"\"\n-    crawler = get_crawler(settings)\n-    crawler.configure()\n-    crawler.crawl(spider)\n-    return crawler.start()\n\n@@ -1,6 +1,6 @@\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import docrawl\n+from scrapy.utils.test import get_crawler\n from tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider\n from tests.mockserver import MockServer\n \n@@ -16,45 +16,45 @@ class TestCloseSpider(TestCase):\n \n     @defer.inlineCallbacks\n     def test_closespider_itemcount(self):\n-        spider = ItemSpider()\n         close_on = 5\n-        yield docrawl(spider, {'CLOSESPIDER_ITEMCOUNT': close_on})\n-        reason = spider.meta['close_reason']\n+        crawler = get_crawler(ItemSpider, {'CLOSESPIDER_ITEMCOUNT': close_on})\n+        yield crawler.crawl()\n+        reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_itemcount')\n-        itemcount = spider.crawler.stats.get_value('item_scraped_count')\n+        itemcount = crawler.stats.get_value('item_scraped_count')\n         self.assertTrue(itemcount >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_pagecount(self):\n-        spider = FollowAllSpider()\n         close_on = 5\n-        yield docrawl(spider, {'CLOSESPIDER_PAGECOUNT': close_on})\n-        reason = spider.meta['close_reason']\n+        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_PAGECOUNT': close_on})\n+        yield crawler.crawl()\n+        reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_pagecount')\n-        pagecount = spider.crawler.stats.get_value('response_received_count')\n+        pagecount = crawler.stats.get_value('response_received_count')\n         self.assertTrue(pagecount >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n-        spider = ErrorSpider(total=1000000)\n         close_on = 5\n-        yield docrawl(spider, {'CLOSESPIDER_ERRORCOUNT': close_on})\n-        self.flushLoggedErrors(spider.exception_cls)\n-        reason = spider.meta['close_reason']\n+        crawler = get_crawler(ErrorSpider, {'CLOSESPIDER_ERRORCOUNT': close_on})\n+        yield crawler.crawl(total=1000000)\n+        self.flushLoggedErrors(crawler.spider.exception_cls)\n+        reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_errorcount')\n         key = 'spider_exceptions/{name}'\\\n-                .format(name=spider.exception_cls.__name__)\n-        errorcount = spider.crawler.stats.get_value(key)\n+                .format(name=crawler.spider.exception_cls.__name__)\n+        errorcount = crawler.stats.get_value(key)\n         self.assertTrue(errorcount >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_timeout(self):\n-        spider = FollowAllSpider(total=1000000)\n         close_on = 0.1\n-        yield docrawl(spider, {'CLOSESPIDER_TIMEOUT': close_on})\n-        reason = spider.meta['close_reason']\n+        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_TIMEOUT': close_on})\n+        yield crawler.crawl(total=1000000)\n+        reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_timeout')\n-        stats = spider.crawler.stats\n+        stats = crawler.stats\n         start = stats.get_value('start_time')\n         stop = stats.get_value('finish_time')\n         diff = stop - start\n\n@@ -3,7 +3,7 @@ import socket\n import mock\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import docrawl, get_testlog\n+from scrapy.utils.test import get_crawler, get_testlog\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from tests.mockserver import MockServer\n@@ -21,9 +21,9 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n-        spider = FollowAllSpider()\n-        yield docrawl(spider)\n-        self.assertEqual(len(spider.urls_visited), 11)  # 10 + start_url\n+        crawler = get_crawler(FollowAllSpider)\n+        yield crawler.crawl()\n+        self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n \n     @defer.inlineCallbacks\n     def test_delay(self):\n@@ -37,9 +37,9 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def _test_delay(self, delay, randomize):\n         settings = {\"DOWNLOAD_DELAY\": delay, 'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n-        spider = FollowAllSpider(maxlatency=delay * 2)\n-        yield docrawl(spider, settings)\n-        t = spider.times\n+        crawler = get_crawler(FollowAllSpider, settings)\n+        yield crawler.crawl(maxlatency=delay * 2)\n+        t = crawler.spider.times\n         totaltime = t[-1] - t[0]\n         avgd = totaltime / (len(t) - 1)\n         tolerance = 0.6 if randomize else 0.2\n@@ -48,85 +48,79 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n-        spider = DelaySpider(n=0.5)\n-        yield docrawl(spider)\n-        self.assertTrue(spider.t1 > 0)\n-        self.assertTrue(spider.t2 > 0)\n-        self.assertTrue(spider.t2 > spider.t1)\n+        crawler = get_crawler(DelaySpider)\n+        yield crawler.crawl(n=0.5)\n+        self.assertTrue(crawler.spider.t1 > 0)\n+        self.assertTrue(crawler.spider.t2 > 0)\n+        self.assertTrue(crawler.spider.t2 > crawler.spider.t1)\n \n     @defer.inlineCallbacks\n     def test_timeout_failure(self):\n-        spider = DelaySpider(n=0.5)\n-        yield docrawl(spider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n-        self.assertTrue(spider.t1 > 0)\n-        self.assertTrue(spider.t2 == 0)\n-        self.assertTrue(spider.t2_err > 0)\n-        self.assertTrue(spider.t2_err > spider.t1)\n+        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n+        yield crawler.crawl(n=0.5)\n+        self.assertTrue(crawler.spider.t1 > 0)\n+        self.assertTrue(crawler.spider.t2 == 0)\n+        self.assertTrue(crawler.spider.t2_err > 0)\n+        self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n         # server hangs after receiving response headers\n-        spider = DelaySpider(n=0.5, b=1)\n-        yield docrawl(spider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n-        self.assertTrue(spider.t1 > 0)\n-        self.assertTrue(spider.t2 == 0)\n-        self.assertTrue(spider.t2_err > 0)\n-        self.assertTrue(spider.t2_err > spider.t1)\n+        yield crawler.crawl(n=0.5, b=1)\n+        self.assertTrue(crawler.spider.t1 > 0)\n+        self.assertTrue(crawler.spider.t2 == 0)\n+        self.assertTrue(crawler.spider.t2_err > 0)\n+        self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n \n     @defer.inlineCallbacks\n     def test_retry_503(self):\n-        spider = SimpleSpider(\"http://localhost:8998/status?n=503\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"http://localhost:8998/status?n=503\")\n         self._assert_retried()\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n-        spider = SimpleSpider(\"http://localhost:65432/status?n=503\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"http://localhost:65432/status?n=503\")\n         self._assert_retried()\n \n     @defer.inlineCallbacks\n     def test_retry_dns_error(self):\n         with mock.patch('socket.gethostbyname',\n                         side_effect=socket.gaierror(-5, 'No address associated with hostname')):\n-            spider = SimpleSpider(\"http://example.com/\")\n-            yield docrawl(spider)\n+            crawler = get_crawler(SimpleSpider)\n+            yield crawler.crawl(\"http://example.com/\")\n             self._assert_retried()\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n-        spider = BrokenStartRequestsSpider(fail_before_yield=1)\n-        yield docrawl(spider)\n+        crawler = get_crawler(BrokenStartRequestsSpider)\n+        yield crawler.crawl(fail_before_yield=1)\n         errors = self.flushLoggedErrors(ZeroDivisionError)\n         self.assertEqual(len(errors), 1)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n-        spider = BrokenStartRequestsSpider(fail_yielding=1)\n-        yield docrawl(spider)\n+        crawler = get_crawler(BrokenStartRequestsSpider)\n+        yield crawler.crawl(fail_yielding=1)\n         errors = self.flushLoggedErrors(ZeroDivisionError)\n         self.assertEqual(len(errors), 1)\n \n     @defer.inlineCallbacks\n     def test_start_requests_lazyness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        spider = BrokenStartRequestsSpider()\n-        yield docrawl(spider, settings)\n-        #self.assertTrue(False, spider.seedsseen)\n-        #self.assertTrue(spider.seedsseen.index(None) < spider.seedsseen.index(99),\n-        #                spider.seedsseen)\n+        crawler = get_crawler(BrokenStartRequestsSpider, settings)\n+        yield crawler.crawl()\n+        #self.assertTrue(False, crawler.spider.seedsseen)\n+        #self.assertTrue(crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n+        #                crawler.spider.seedsseen)\n \n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        spider = DuplicateStartRequestsSpider(dont_filter=True,\n-                                              distinct_urls=2,\n-                                              dupe_factor=3)\n-        yield docrawl(spider, settings)\n-        self.assertEqual(spider.visited, 6)\n+        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n+        yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3)\n+        self.assertEqual(crawler.spider.visited, 6)\n \n-        spider = DuplicateStartRequestsSpider(dont_filter=False,\n-                                              distinct_urls=3,\n-                                              dupe_factor=4)\n-        yield docrawl(spider, settings)\n-        self.assertEqual(spider.visited, 3)\n+        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4)\n+        self.assertEqual(crawler.spider.visited, 3)\n \n     @defer.inlineCallbacks\n     def test_unbounded_response(self):\n@@ -150,23 +144,23 @@ Connection: close\n foo body\n with multiples lines\n '''})\n-        spider = SimpleSpider(\"http://localhost:8998/raw?{0}\".format(query))\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"http://localhost:8998/raw?{0}\".format(query))\n         log = get_testlog()\n         self.assertEqual(log.count(\"Got response 200\"), 1)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n-        spider = SimpleSpider(\"http://localhost:8998/drop?abort=0\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"http://localhost:8998/drop?abort=0\")\n         self._assert_retried()\n \n     @defer.inlineCallbacks\n     def test_retry_conn_aborted(self):\n         # connection lost before receiving data\n-        spider = SimpleSpider(\"http://localhost:8998/drop?abort=1\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"http://localhost:8998/drop?abort=1\")\n         self._assert_retried()\n \n     def _assert_retried(self):\n@@ -184,22 +178,22 @@ with multiples lines\n         req0.meta['next'] = req1\n         req1.meta['next'] = req2\n         req2.meta['next'] = req3\n-        spider = SingleRequestSpider(seed=req0)\n-        yield docrawl(spider)\n+        crawler = get_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=req0)\n         # basic asserts in case of weird communication errors\n-        self.assertIn('responses', spider.meta)\n-        self.assertNotIn('failures', spider.meta)\n+        self.assertIn('responses', crawler.spider.meta)\n+        self.assertNotIn('failures', crawler.spider.meta)\n         # start requests doesn't set Referer header\n-        echo0 = json.loads(spider.meta['responses'][2].body)\n+        echo0 = json.loads(crawler.spider.meta['responses'][2].body)\n         self.assertNotIn('Referer', echo0['headers'])\n         # following request sets Referer to start request url\n-        echo1 = json.loads(spider.meta['responses'][1].body)\n+        echo1 = json.loads(crawler.spider.meta['responses'][1].body)\n         self.assertEqual(echo1['headers'].get('Referer'), [req0.url])\n         # next request avoids Referer header\n-        echo2 = json.loads(spider.meta['responses'][2].body)\n+        echo2 = json.loads(crawler.spider.meta['responses'][2].body)\n         self.assertNotIn('Referer', echo2['headers'])\n         # last request explicitly sets a Referer header\n-        echo3 = json.loads(spider.meta['responses'][3].body)\n+        echo3 = json.loads(crawler.spider.meta['responses'][3].body)\n         self.assertEqual(echo3['headers'].get('Referer'), ['http://example.com'])\n \n     @defer.inlineCallbacks\n@@ -208,11 +202,11 @@ with multiples lines\n         est = []\n \n         def cb(response):\n-            est.append(get_engine_status(spider.crawler.engine))\n+            est.append(get_engine_status(crawler.engine))\n \n-        spider = SingleRequestSpider(seed='http://localhost:8998/', callback_func=cb)\n-        yield docrawl(spider)\n+        crawler = get_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed='http://localhost:8998/', callback_func=cb)\n         self.assertEqual(len(est), 1, est)\n         s = dict(est[0])\n-        self.assertEqual(s['engine.spider.name'], spider.name)\n+        self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n         self.assertEqual(s['len(engine.scraper.slot.active)'], 1)\n\n@@ -47,19 +47,22 @@ class LoadTestCase(unittest.TestCase):\n \n     def test_enabled_handler(self):\n         handlers = {'scheme': 'tests.test_downloader_handlers.DummyDH'}\n-        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        dh = DownloadHandlers(crawler)\n         self.assertIn('scheme', dh._handlers)\n         self.assertNotIn('scheme', dh._notconfigured)\n \n     def test_not_configured_handler(self):\n         handlers = {'scheme': 'tests.test_downloader_handlers.OffDH'}\n-        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        dh = DownloadHandlers(crawler)\n         self.assertNotIn('scheme', dh._handlers)\n         self.assertIn('scheme', dh._notconfigured)\n \n     def test_disabled_handler(self):\n         handlers = {'scheme': None}\n-        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n+        dh = DownloadHandlers(crawler)\n         self.assertNotIn('scheme', dh._handlers)\n         self.assertNotIn('scheme', dh._notconfigured)\n \n\n@@ -12,9 +12,8 @@ class ManagerTestCase(TestCase):\n     settings_dict = None\n \n     def setUp(self):\n-        self.crawler = get_crawler(self.settings_dict)\n-        self.spider = Spider('foo')\n-        self.spider.set_crawler(self.crawler)\n+        self.crawler = get_crawler(Spider, self.settings_dict)\n+        self.spider = self.crawler._create_spider('foo')\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n         # some mw depends on stats collector\n         self.crawler.stats.open_spider(self.spider)\n\n@@ -9,8 +9,8 @@ __doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawl']\n \n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n-        self.spider = Spider('foo')\n-        crawler = get_crawler({'AJAXCRAWL_ENABLED': True})\n+        crawler = get_crawler(Spider, {'AJAXCRAWL_ENABLED': True})\n+        self.spider = crawler._create_spider('foo')\n         self.mw = AjaxCrawlMiddleware.from_crawler(crawler)\n \n     def _ajaxcrawlable_body(self):\n\n@@ -10,9 +10,8 @@ from scrapy.utils.test import get_crawler\n class TestDefaultHeadersMiddleware(TestCase):\n \n     def get_defaults_spider_mw(self):\n-        crawler = get_crawler()\n-        spider = Spider('foo')\n-        spider.set_crawler(crawler)\n+        crawler = get_crawler(Spider)\n+        spider = crawler._create_spider('foo')\n         defaults = dict([(k, [v]) for k, v in \\\n             six.iteritems(crawler.settings.get('DEFAULT_REQUEST_HEADERS'))])\n         return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)\n\n@@ -9,9 +9,8 @@ from scrapy.utils.test import get_crawler\n class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n \n     def get_request_spider_mw(self):\n-        crawler = get_crawler()\n-        spider = Spider('foo')\n-        spider.set_crawler(crawler)\n+        crawler = get_crawler(Spider)\n+        spider = crawler._create_spider('foo')\n         request = Request('http://scrapytest.org/')\n         return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n \n\n@@ -24,8 +24,8 @@ class _BaseTest(unittest.TestCase):\n         self.yesterday = email.utils.formatdate(time.time() - 86400)\n         self.today = email.utils.formatdate()\n         self.tomorrow = email.utils.formatdate(time.time() + 86400)\n-        self.crawler = get_crawler()\n-        self.spider = Spider('example.com')\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('example.com')\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request('http://www.example.com',\n                                headers={'User-Agent': 'test'})\n\n@@ -10,8 +10,8 @@ from scrapy.utils.test import get_crawler\n class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        crawler = get_crawler()\n-        self.spider = Spider('foo')\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider('foo')\n         self.mw = RedirectMiddleware.from_crawler(crawler)\n \n     def test_priority_adjust(self):\n@@ -123,8 +123,8 @@ class RedirectMiddlewareTest(unittest.TestCase):\n class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        crawler = get_crawler()\n-        self.spider = Spider('foo')\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider('foo')\n         self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n \n     def _body(self, interval=5, url='http://example.org/newpage'):\n\n@@ -14,8 +14,8 @@ from scrapy.utils.test import get_crawler\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n-        crawler = get_crawler()\n-        self.spider = Spider('foo')\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider('foo')\n         self.mw = RetryMiddleware.from_crawler(crawler)\n         self.mw.max_retry_times = 2\n \n\n@@ -9,8 +9,8 @@ from scrapy.utils.test import get_crawler\n class TestDownloaderStats(TestCase):\n \n     def setUp(self):\n-        self.crawler = get_crawler()\n-        self.spider = Spider('scrapytest.org')\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('scrapytest.org')\n         self.mw = DownloaderStats(self.crawler.stats)\n \n         self.crawler.stats.open_spider(self.spider)\n\n@@ -9,9 +9,8 @@ from scrapy.utils.test import get_crawler\n class UserAgentMiddlewareTest(TestCase):\n \n     def get_spider_and_mw(self, default_useragent):\n-        crawler = get_crawler({'USER_AGENT': default_useragent})\n-        spider = Spider('foo')\n-        spider.set_crawler(crawler)\n+        crawler = get_crawler(Spider, {'USER_AGENT': default_useragent})\n+        spider = crawler._create_spider('foo')\n         return spider, UserAgentMiddleware.from_crawler(crawler)\n \n     def test_default_agent(self):\n\n@@ -87,20 +87,18 @@ class CrawlerRun(object):\n         self.portno = self.port.getHost().port\n \n         start_urls = [self.geturl(\"/\"), self.geturl(\"/redirect\")]\n-        self.spider = TestSpider(start_urls=start_urls)\n \n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n                 dispatcher.connect(self.record_signal, signal)\n \n-        self.crawler = get_crawler()\n+        self.crawler = get_crawler(TestSpider)\n         self.crawler.install()\n-        self.crawler.configure()\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n-        self.crawler.crawl(self.spider)\n-        self.crawler.start()\n+        self.crawler.crawl(start_urls=start_urls)\n+        self.spider = self.crawler.spider\n \n         self.deferred = defer.Deferred()\n         dispatcher.connect(self.stop, signals.engine_stopped)\n\n@@ -8,7 +8,7 @@ from netlib import http_auth\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_testlog, docrawl\n+from scrapy.utils.test import get_testlog, get_crawler\n from scrapy.http import Request\n from tests.spiders import SimpleSpider, SingleRequestSpider\n from tests.mockserver import MockServer\n@@ -49,29 +49,29 @@ class ProxyConnectTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_https_connect_tunnel(self):\n-        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n         self._assert_got_response_code(200)\n \n     @defer.inlineCallbacks\n     def test_https_noconnect(self):\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888?noconnect'\n-        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n         self._assert_got_response_code(200)\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n-        spider = SimpleSpider(\"https://localhost:99999/status?n=200\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n         self._assert_got_tunnel_error()\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n         os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888'\n-        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n         # The proxy returns a 407 error code but it does not reach the client;\n         # he just sees a TunnelError.\n         self._assert_got_tunnel_error()\n@@ -80,17 +80,17 @@ class ProxyConnectTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_https_tunnel_without_leak_proxy_authorization_header(self):\n         request = Request(\"https://localhost:8999/echo\")\n-        spider = SingleRequestSpider(seed=request)\n-        yield docrawl(spider)\n+        crawler = get_crawler(SingleRequestSpider)\n+        yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200)\n-        echo = json.loads(spider.meta['responses'][0].body)\n+        echo = json.loads(crawler.spider.meta['responses'][0].body)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n     @defer.inlineCallbacks\n     def test_https_noconnect_auth_error(self):\n         os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n-        spider = SimpleSpider(\"https://localhost:8999/status?n=200\")\n-        yield docrawl(spider)\n+        crawler = get_crawler(SimpleSpider)\n+        yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n         self._assert_got_response_code(407)\n \n     def _assert_got_response_code(self, code):\n\n@@ -242,7 +242,8 @@ class CrawlSpiderTest(SpiderTest):\n         self.assertTrue(hasattr(spider, '_follow_links'))\n         self.assertTrue(spider._follow_links)\n \n-        crawler.settings.set('CRAWLSPIDER_FOLLOW_LINKS', False)\n+        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}\n+        crawler = get_crawler(settings_dict=settings_dict)\n         spider = self.spider_class.from_crawler(crawler, 'example.com')\n         self.assertTrue(hasattr(spider, '_follow_links'))\n         self.assertFalse(spider._follow_links)\n@@ -256,7 +257,8 @@ class CrawlSpiderTest(SpiderTest):\n         self.assertTrue(spider._follow_links)\n \n         spider = self.spider_class('example.com')\n-        spider.set_crawler(get_crawler({'CRAWLSPIDER_FOLLOW_LINKS': False}))\n+        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}\n+        spider.set_crawler(get_crawler(settings_dict=settings_dict))\n         self.assertTrue(hasattr(spider, '_follow_links'))\n         self.assertFalse(spider._follow_links)\n \n\n@@ -10,9 +10,10 @@ from scrapy.utils.test import get_crawler\n class TestDepthMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = Spider('scrapytest.org')\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider('scrapytest.org')\n \n-        self.stats = StatsCollector(get_crawler())\n+        self.stats = StatsCollector(crawler)\n         self.stats.open_spider(self.spider)\n \n         self.mw = DepthMiddleware(1, self.stats, True)\n\n@@ -3,7 +3,7 @@ from unittest import TestCase\n from twisted.trial.unittest import TestCase as TrialTestCase\n from twisted.internet import defer\n \n-from scrapy.utils.test import docrawl, get_testlog\n+from scrapy.utils.test import get_crawler, get_testlog\n from tests.mockserver import MockServer\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n@@ -165,20 +165,20 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n \n     @defer.inlineCallbacks\n     def test_middleware_works(self):\n-        spider = _HttpErrorSpider()\n-        yield docrawl(spider)\n-        assert not spider.skipped, spider.skipped\n-        self.assertEqual(spider.parsed, {'200'})\n-        self.assertEqual(spider.failed, {'404', '402', '500'})\n+        crawler = get_crawler(_HttpErrorSpider)\n+        yield crawler.crawl()\n+        assert not crawler.spider.skipped, crawler.spider.skipped\n+        self.assertEqual(crawler.spider.parsed, {'200'})\n+        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n \n     @defer.inlineCallbacks\n     def test_logging(self):\n-        spider = _HttpErrorSpider(bypass_status_codes={402})\n-        yield docrawl(spider)\n+        crawler = get_crawler(_HttpErrorSpider)\n+        yield crawler.crawl(bypass_status_codes={402})\n         # print(get_testlog())\n-        self.assertEqual(spider.parsed, {'200', '402'})\n-        self.assertEqual(spider.skipped, {'402'})\n-        self.assertEqual(spider.failed, {'404', '500'})\n+        self.assertEqual(crawler.spider.parsed, {'200', '402'})\n+        self.assertEqual(crawler.spider.skipped, {'402'})\n+        self.assertEqual(crawler.spider.failed, {'404', '500'})\n \n         log = get_testlog()\n         self.assertIn('Ignoring response <404', log)\n\n@@ -10,13 +10,13 @@ from scrapy.utils.test import get_crawler\n class TestOffsiteMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = self._get_spider()\n-        crawler = get_crawler()\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider(**self._get_spiderargs())\n         self.mw = OffsiteMiddleware.from_crawler(crawler)\n         self.mw.spider_opened(self.spider)\n \n-    def _get_spider(self):\n-        return Spider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])\n+    def _get_spiderargs(self):\n+        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org'])\n \n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n@@ -39,8 +39,8 @@ class TestOffsiteMiddleware(TestCase):\n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n \n-    def _get_spider(self):\n-        return Spider('foo', allowed_domains=None)\n+    def _get_spiderargs(self):\n+        return dict(name='foo', allowed_domains=None)\n \n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n@@ -58,7 +58,7 @@ class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n \n     def _get_spider(self):\n       bad_hostname = urlparse('http:////scrapytest.org').hostname\n-      return Spider('foo', allowed_domains=['scrapytest.org', None, bad_hostname])\n+      return dict(name='foo', allowed_domains=['scrapytest.org', None, bad_hostname])\n \n     def test_process_spider_output(self):\n       res = Response('http://scrapytest.org')\n\n@@ -7,8 +7,8 @@ from scrapy.utils.test import get_crawler\n class StatsCollectorTest(unittest.TestCase):\n \n     def setUp(self):\n-        self.crawler = get_crawler()\n-        self.spider = Spider('foo')\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('foo')\n \n     def test_collector(self):\n         stats = StatsCollector(self.crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
