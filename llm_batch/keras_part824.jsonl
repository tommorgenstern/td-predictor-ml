{"custom_id": "keras#9cf4e9481d41608ea399a17ffa4f99e4b84d8ded", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 105 | Churn Cumulative: 392 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: 0.03278688524590164\n\nDIFF:\n@@ -3,6 +3,7 @@\n import datetime\n import io\n import json\n+import pathlib\n import tempfile\n import warnings\n import zipfile\n@@ -32,6 +33,8 @@ except ImportError:\n _CONFIG_FILENAME = \"config.json\"\n _METADATA_FILENAME = \"metadata.json\"\n _VARS_FNAME = \"model.weights\"  # Will become e.g. \"model.weights.h5\"\n+_VARS_FNAME_H5 = _VARS_FNAME + \".h5\"\n+_VARS_FNAME_NPZ = _VARS_FNAME + \".npz\"\n _ASSETS_DIRNAME = \"assets\"\n \n \n@@ -109,11 +112,24 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n         with zf.open(_CONFIG_FILENAME, \"w\") as f:\n             f.write(config_json.encode())\n \n+        weights_file_path = None\n+        try:\n             if weights_format == \"h5\":\n-            weights_store = H5IOStore(_VARS_FNAME + \".h5\", archive=zf, mode=\"w\")\n+                if isinstance(fileobj, io.BufferedWriter):\n+                    # First, open the .h5 file, then write it to `zf` at the end\n+                    # of the function call.\n+                    working_dir = pathlib.Path(fileobj.name).parent\n+                    weights_file_path = working_dir / _VARS_FNAME_H5\n+                    weights_store = H5IOStore(weights_file_path, mode=\"w\")\n+                else:\n+                    # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n+                    # this usage is for pickling.\n+                    weights_store = H5IOStore(\n+                        _VARS_FNAME_H5, archive=zf, mode=\"w\"\n+                    )\n             elif weights_format == \"npz\":\n                 weights_store = NpzIOStore(\n-                _VARS_FNAME + \".npz\", archive=zf, mode=\"w\"\n+                    _VARS_FNAME_NPZ, archive=zf, mode=\"w\"\n                 )\n             else:\n                 raise ValueError(\n@@ -131,8 +147,15 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n                 inner_path=\"\",\n                 visited_saveables=set(),\n             )\n+        except:\n+            # Skip the final `zf.write` if any exception is raised\n+            weights_file_path = None\n+            raise\n+        finally:\n             weights_store.close()\n             asset_store.close()\n+            if weights_file_path:\n+                zf.write(weights_file_path, weights_file_path.name)\n \n \n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n@@ -172,15 +195,25 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n             )\n \n         all_filenames = zf.namelist()\n-        if _VARS_FNAME + \".h5\" in all_filenames:\n-            weights_store = H5IOStore(_VARS_FNAME + \".h5\", archive=zf, mode=\"r\")\n-        elif _VARS_FNAME + \".npz\" in all_filenames:\n-            weights_store = NpzIOStore(\n-                _VARS_FNAME + \".npz\", archive=zf, mode=\"r\"\n-            )\n+        weights_file_path = None\n+        try:\n+            if _VARS_FNAME_H5 in all_filenames:\n+                if isinstance(fileobj, io.BufferedReader):\n+                    # First, extract the model.weights.h5 file, then load it\n+                    # using h5py.\n+                    working_dir = pathlib.Path(fileobj.name).parent\n+                    zf.extract(_VARS_FNAME_H5, working_dir)\n+                    weights_file_path = working_dir / _VARS_FNAME_H5\n+                    weights_store = H5IOStore(weights_file_path, mode=\"r\")\n+                else:\n+                    # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n+                    # this usage is for pickling.\n+                    weights_store = H5IOStore(_VARS_FNAME_H5, zf, mode=\"r\")\n+            elif _VARS_FNAME_NPZ in all_filenames:\n+                weights_store = NpzIOStore(_VARS_FNAME_NPZ, zf, mode=\"r\")\n             else:\n                 raise ValueError(\n-                f\"Expected a {_VARS_FNAME}.h5 or {_VARS_FNAME}.npz file.\"\n+                    f\"Expected a {_VARS_FNAME_H5} or {_VARS_FNAME_NPZ} file.\"\n                 )\n \n             if len(all_filenames) > 3:\n@@ -199,9 +232,12 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n                 failed_saveables=failed_saveables,\n                 error_msgs=error_msgs,\n             )\n+        finally:\n             weights_store.close()\n             if asset_store:\n                 asset_store.close()\n+            if weights_file_path:\n+                weights_file_path.unlink()\n \n         if failed_saveables:\n             _raise_loading_failure(error_msgs)\n@@ -250,9 +286,7 @@ def load_weights_only(\n         weights_store = H5IOStore(filepath, mode=\"r\")\n     elif filepath.endswith(\".keras\"):\n         archive = zipfile.ZipFile(filepath, \"r\")\n-        weights_store = H5IOStore(\n-            _VARS_FNAME + \".h5\", archive=archive, mode=\"r\"\n-        )\n+        weights_store = H5IOStore(_VARS_FNAME_H5, archive=archive, mode=\"r\")\n \n     failed_saveables = set()\n     if objects_to_skip is not None:\n\n@@ -612,6 +612,53 @@ class SavingTest(testing.TestCase):\n \n         self.assertAllClose(pred1, pred2, atol=1e-5)\n \n+    def test_save_model_exception_raised(self):\n+        # Assume we have an error in `save_own_variables`.\n+        class RaiseErrorLayer(keras.layers.Layer):\n+            def __init__(self, **kwargs):\n+                super().__init__(**kwargs)\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+            def save_own_variables(self, store):\n+                raise ValueError\n+\n+        model = keras.Sequential([keras.Input([1]), RaiseErrorLayer()])\n+        filepath = f\"{self.get_temp_dir()}/model.keras\"\n+        with self.assertRaises(ValueError):\n+            saving_lib.save_model(model, filepath)\n+\n+        # Ensure we don't have a bad \"model.weights.h5\" inside the zip file.\n+        self.assertTrue(Path(filepath).exists())\n+        with zipfile.ZipFile(filepath) as zf:\n+            all_filenames = zf.namelist()\n+            self.assertNotIn(\"model.weights.h5\", all_filenames)\n+\n+    def test_load_model_exception_raised(self):\n+        # Assume we have an error in `load_own_variables`.\n+        class RaiseErrorLayer(keras.layers.Layer):\n+            def __init__(self, **kwargs):\n+                super().__init__(**kwargs)\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+            def load_own_variables(self, store):\n+                raise ValueError\n+\n+        model = keras.Sequential([keras.Input([1]), RaiseErrorLayer()])\n+        filepath = f\"{self.get_temp_dir()}/model.keras\"\n+        saving_lib.save_model(model, filepath)\n+        with self.assertRaises(ValueError):\n+            saving_lib.load_model(\n+                filepath, custom_objects={\"RaiseErrorLayer\": RaiseErrorLayer}\n+            )\n+\n+        # Ensure we don't leave a bad \"model.weights.h5\" on the filesystem.\n+        self.assertTrue(Path(filepath).exists())\n+        self.assertFalse(Path(filepath).with_name(\"model.weights.h5\").exists())\n+\n \n @pytest.mark.requires_trainable_backend\n class SavingAPITest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#727b7ae4e1d366f52c7aaf189207c18f35387176", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 32 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class HashedCrossing(Layer):\n \n     This layer performs crosses of categorical features using the \"hashing\n     trick\". Conceptually, the transformation can be thought of as:\n-    `hash(concatenate(features)) % num_bins.\n+    `hash(concatenate(features)) % num_bins`.\n \n     This layer currently only performs crosses of scalar inputs and batches of\n     scalar inputs. Valid input shapes are `(batch_size, 1)`, `(batch_size,)` and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45e1175b785514593092ba0b197a562df512017b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 577 | Contributors (this commit): 26 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -60,5 +60,8 @@ setup(\n         \"Topic :: Scientific/Engineering\",\n         \"Topic :: Software Development\",\n     ],\n-    packages=find_packages(exclude=(\"*_test.py\",)),\n+    packages=find_packages(\n+        include=(\"keras\", \"keras.*\"),\n+        exclude=(\"*_test.py\", \"benchmarks\"),\n+    ),\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ee33b5a8c2b1eaeabe539c6f81ef5ac7f6d1bb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 67 | Churn Cumulative: 87 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -520,9 +520,11 @@ class ModelParallel(Distribution):\n     layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n     layout_map['conv2d.*bias'] = ('model',)\n \n-    distribution = ModelParallel(device_mesh=device_mesh,\n+    distribution = ModelParallel(\n         layout_map=layout_map,\n-                                 batch_dim_name='batch')\n+        batch_dim_name='batch',\n+    )\n+\n     # Set the global distribution, or via `with distribution.scope():`\n     set_distribution(distribution)\n \n@@ -533,12 +535,16 @@ class ModelParallel(Distribution):\n \n     You can quickly update the device mesh shape to change the sharding factor\n     of the variables. E.g.\n-    ```\n+\n+    ```python\n     # With only the shape change for the device mesh, the variables will be\n     # sharded across 8 devices instead of 4, which further reduces the memory\n     # footprint of variables on each of the device.\n-    device_mesh = DeviceMesh(shape=(1, 8), axis_names=('batch', 'model'),\n-                             devices=devices)\n+    device_mesh = DeviceMesh(\n+        shape=(1, 8),\n+        axis_names=('batch', 'model'),\n+        devices=devices,\n+    )\n     ```\n \n     To figure out a proper layout mapping rule for all the model variables, you\n@@ -546,25 +552,32 @@ class ModelParallel(Distribution):\n     key to map the variables to `TensorLayout`.\n \n     e.g.\n-    ```\n+\n+    ```python\n     model = create_model()\n     for v in model.variables:\n         print(v.path)\n     ```\n \n     Args:\n-        device_mesh: `DeviceMesh` instance for physical device and its\n-            logical mapping.\n         layout_map: `LayoutMap` instance which map the variable path to the\n-            corresponding `TensorLayout`. The axis names of the\n-            `TensorLayout`s should match to the axis names in the\n-            device_mesh, or exception will be raised.\n-        batch_dim_name: optional string, the axis name in the `device_mesh`\n+            corresponding tensor layout.\n+        batch_dim_name: Optional string, the axis name in the device mesh\n+            (of the `layout_map` object)\n             that will be used to distribute data. If unspecified, the\n-            first axis from the `device_mesh` will be used.\n+            first axis from the device mesh will be used.\n     \"\"\"\n \n-    def __init__(self, device_mesh, layout_map, batch_dim_name=None):\n+    def __init__(self, *, layout_map=None, batch_dim_name=None, **kwargs):\n+        kwargs.pop(\"device_mesh\", None)\n+        if layout_map is None:\n+            raise ValueError(\"You must specify a layout_map argument.\")\n+        if not isinstance(layout_map, LayoutMap):\n+            raise ValueError(\n+                \"Argument `layout_map` must be a `LayoutMap` instance. \"\n+                f\"Received: layout_map={layout_map}\"\n+            )\n+        device_mesh = layout_map.device_mesh\n         super().__init__(device_mesh)\n         self._layout_map = layout_map\n         self._batch_dim_name = batch_dim_name or self.device_mesh.axis_names[0]\n@@ -693,11 +706,11 @@ class LayoutMap(collections.abc.MutableMapping):\n     as value, and will be converted to `TensorLayout`.\n \n     ```python\n-    layout_map = LayoutMap(device_mesh=None)\n-    layout_map['dense.*kernel'] = (None, 'model')         # layout_2d\n-    layout_map['dense.*bias'] = ('model',)                # layout_1d\n-    layout_map['conv2d.*kernel'] = TensorLayout((None, None, None, 'model'))\n-    layout_map['conv2d.*bias'] = TensorLayout(('model',))  # layout_1d\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['dense.*kernel'] = (None, 'model')\n+    layout_map['dense.*bias'] = ('model',)\n+    layout_map['conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['conv2d.*bias'] = ('model',)\n \n     layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n     layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n@@ -710,11 +723,10 @@ class LayoutMap(collections.abc.MutableMapping):\n     ```\n \n     Args:\n-        device_mesh: An optional `DeviceMesh` that can be used to populate the\n-            `TensorLayout.device_mesh` if `TensorLayout.device_mesh` is not set.\n+        device_mesh: `keras.distribution.DeviceMesh` instance.\n     \"\"\"\n \n-    def __init__(self, device_mesh=None):\n+    def __init__(self, device_mesh):\n         self._layout_map = collections.OrderedDict()\n         self._device_mesh = device_mesh\n \n\n@@ -273,7 +273,7 @@ class ModelParallelDistributionTest(testing.TestCase):\n         layout_map[\".*bias\"] = distribution_lib.TensorLayout([\"model\"])\n \n         distribution = distribution_lib.ModelParallel(\n-            self.device_mesh, layout_map, batch_dim_name=\"data\"\n+            layout_map=layout_map, batch_dim_name=\"data\"\n         )\n         kernel = backend.Variable(initializer=np.arange(8, 4), name=\"kernel\")\n         bias = backend.Variable(initializer=np.arange(4), name=\"bias\")\n@@ -294,7 +294,7 @@ class ModelParallelDistributionTest(testing.TestCase):\n     def test_distribute_data(self):\n         layout_map = distribution_lib.LayoutMap(self.device_mesh)\n         distribution = distribution_lib.ModelParallel(\n-            self.device_mesh, layout_map, batch_dim_name=\"data\"\n+            layout_map=layout_map, batch_dim_name=\"data\"\n         )\n \n         data = np.arange(16).reshape((4, 2, 2))\n@@ -309,7 +309,7 @@ class ModelParallelDistributionTest(testing.TestCase):\n         layout_map[\"/model/layer/tensor\"] = (\"data\", None)\n \n         distribution = distribution_lib.ModelParallel(\n-            self.device_mesh, layout_map, batch_dim_name=\"data\"\n+            layout_map=layout_map, batch_dim_name=\"data\"\n         )\n         layout = distribution.get_tensor_layout(\"/model/layer/tensor\")\n         self.assertIs(layout.device_mesh, self.device_mesh)\n@@ -321,8 +321,9 @@ class ModelParallelDistributionTest(testing.TestCase):\n     def test_distribute_dataset(self):\n         # We can only verify the single worker/process case in OSS for now.\n         dataset = tf.data.Dataset.range(8)\n-        distribution = distribution = distribution_lib.ModelParallel(\n-            self.device_mesh, {}, batch_dim_name=\"data\"\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        distribution = distribution_lib.ModelParallel(\n+            layout_map=layout_map, batch_dim_name=\"data\"\n         )\n         distributed_dataset = distribution.distribute_dataset(dataset)\n         self.assertIs(dataset, distributed_dataset)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0eaac473791f0481c2b292492a4af81eef14153", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 167 | Lines Deleted: 22 | Files Changed: 4 | Hunks: 22 | Methods Changed: 15 | Complexity Δ (Sum/Max): 27/22 | Churn Δ: 189 | Churn Cumulative: 657 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 14 | DMM Complexity: 0.6363636363636364\n\nDIFF:\n@@ -265,18 +265,19 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         )\n \n     @traceback_utils.filter_traceback\n-    def save(self, filepath, overwrite=True, **kwargs):\n+    def save(self, filepath, overwrite=True, zipped=True, **kwargs):\n         \"\"\"Saves a model as a `.keras` file.\n \n         Args:\n             filepath: `str` or `pathlib.Path` object.\n-                The path where to save the model. Must end in `.keras`.\n+                The path where to save the model. Must end in `.keras`\n+                (unless saving the model as an unzipped directory\n+                via `zipped=False`).\n             overwrite: Whether we should overwrite any existing model at\n                 the target location, or instead ask the user via\n                 an interactive prompt.\n-            save_format: The `save_format` argument is deprecated in Keras 3.\n-                Format to use, as a string. Only the `\"keras\"` format is\n-                supported at this time.\n+            zipped: Whether to save the model as a zipped `.keras`\n+                archive (default), or as an unzipped directory.\n \n         Example:\n \n@@ -303,7 +304,9 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n \n         Thus models can be reinstantiated in the exact same state.\n         \"\"\"\n-        return saving_api.save_model(self, filepath, overwrite, **kwargs)\n+        return saving_api.save_model(\n+            self, filepath, overwrite=overwrite, zipped=zipped, **kwargs\n+        )\n \n     @traceback_utils.filter_traceback\n     def save_weights(self, filepath, overwrite=True):\n\n@@ -16,7 +16,7 @@ except ImportError:\n \n \n @keras_export([\"keras.saving.save_model\", \"keras.models.save_model\"])\n-def save_model(model, filepath, overwrite=True, **kwargs):\n+def save_model(model, filepath, overwrite=True, zipped=True, **kwargs):\n     \"\"\"Saves a model as a `.keras` file.\n \n     Args:\n@@ -24,6 +24,8 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n         filepath: `str` or `pathlib.Path` object. Path where to save the model.\n         overwrite: Whether we should overwrite any existing model at the target\n             location, or instead ask the user via an interactive prompt.\n+        zipped: Whether to save the model as a zipped `.keras`\n+            archive (default), or as an unzipped directory.\n \n     Example:\n \n@@ -42,7 +44,7 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n \n     Note that `model.save()` is an alias for `keras.saving.save_model()`.\n \n-    The saved `.keras` file contains:\n+    The saved `.keras` file is a `zip` archive that contains:\n \n     - The model's configuration (architecture)\n     - The model's weights\n@@ -96,13 +98,14 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n         if not proceed:\n             return\n \n-    if str(filepath).endswith(\".keras\"):\n-        saving_lib.save_model(model, filepath)\n-    elif str(filepath).endswith((\".h5\", \".hdf5\")):\n-        legacy_h5_format.save_model_to_hdf5(\n+    if zipped and str(filepath).endswith(\".keras\"):\n+        return saving_lib.save_model(model, filepath)\n+    if not zipped:\n+        return saving_lib.save_model(model, filepath, zipped=False)\n+    if str(filepath).endswith((\".h5\", \".hdf5\")):\n+        return legacy_h5_format.save_model_to_hdf5(\n             model, filepath, overwrite, include_optimizer\n         )\n-    else:\n     raise ValueError(\n         \"Invalid filepath extension for saving. \"\n         \"Please add either a `.keras` extension for the native Keras \"\n@@ -153,6 +156,9 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     is_keras_zip = str(filepath).endswith(\".keras\") and zipfile.is_zipfile(\n         filepath\n     )\n+    is_keras_dir = file_utils.isdir(filepath) and file_utils.exists(\n+        file_utils.join(filepath, \"config.json\")\n+    )\n \n     # Support for remote zip files\n     if (\n@@ -160,7 +166,7 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n         and not file_utils.isdir(filepath)\n         and not is_keras_zip\n     ):\n-        local_path = os.path.join(\n+        local_path = file_utils.join(\n             saving_lib.get_temp_dir(), os.path.basename(filepath)\n         )\n \n@@ -172,7 +178,7 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n             filepath = local_path\n             is_keras_zip = True\n \n-    if is_keras_zip:\n+    if is_keras_zip or is_keras_dir:\n         return saving_lib.load_model(\n             filepath,\n             custom_objects=custom_objects,\n\n@@ -167,6 +167,29 @@ class LoadModelTests(test_case.TestCase, parameterized.TestCase):\n         self.assertIsInstance(loaded_model.layers[0], CustomLayer)\n         os.remove(filepath)\n \n+    def test_save_unzipped(self):\n+        \"\"\"Test saving/loading an unzipped model dir.\"\"\"\n+        model = self.get_model()\n+\n+        # Test error with keras extension\n+        bad_filepath = os.path.join(self.get_temp_dir(), \"test_model.keras\")\n+        with self.assertRaisesRegex(ValueError, \"should not end in\"):\n+            saving_api.save_model(model, bad_filepath, zipped=False)\n+\n+        filepath = os.path.join(self.get_temp_dir(), \"test_model_dir\")\n+        saving_api.save_model(model, filepath, zipped=False)\n+\n+        self.assertTrue(os.path.exists(filepath))\n+        self.assertTrue(os.path.isdir(filepath))\n+        config_filepath = os.path.join(filepath, \"config.json\")\n+        weights_filepath = os.path.join(filepath, \"model.weights.h5\")\n+        self.assertTrue(os.path.exists(config_filepath))\n+        self.assertTrue(os.path.exists(weights_filepath))\n+\n+        loaded_model = saving_api.load_model(filepath)\n+        x = np.random.uniform(size=(10, 3))\n+        self.assertTrue(np.allclose(model.predict(x), loaded_model.predict(x)))\n+\n \n class LoadWeightsTests(test_case.TestCase, parameterized.TestCase):\n     def get_model(self, dtype=None):\n\n@@ -38,7 +38,7 @@ _VARS_FNAME_NPZ = _VARS_FNAME + \".npz\"\n _ASSETS_DIRNAME = \"assets\"\n \n \n-def save_model(model, filepath, weights_format=\"h5\"):\n+def save_model(model, filepath, weights_format=\"h5\", zipped=True):\n     \"\"\"Save a zip-archive representing a Keras model to the given file or path.\n \n     The zip-based archive contains the following structure:\n@@ -79,11 +79,19 @@ def save_model(model, filepath, weights_format=\"h5\"):\n         return\n \n     filepath = str(filepath)\n-    if not filepath.endswith(\".keras\"):\n+    if zipped and not filepath.endswith(\".keras\"):\n         raise ValueError(\n             \"Invalid `filepath` argument: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n+    if not zipped and filepath.endswith(\".keras\"):\n+        raise ValueError(\n+            \"When using `zipped=False`, the `filepath` argument should not \"\n+            f\"end in `.keras`. Received: filepath={filepath}\"\n+        )\n+    if not zipped:\n+        _save_model_to_dir(model, filepath, weights_format)\n+    else:\n         if file_utils.is_remote_path(filepath):\n             # Remote path. Zip to local memory byte io and copy to remote\n             zip_filepath = io.BytesIO()\n@@ -95,7 +103,7 @@ def save_model(model, filepath, weights_format=\"h5\"):\n                 _save_model_to_fileobj(model, f, weights_format)\n \n \n-def _save_model_to_fileobj(model, fileobj, weights_format):\n+def _serialize_model_as_json(model):\n     with ObjectSharingScope():\n         serialized_model_dict = serialize_keras_object(model)\n     config_json = json.dumps(serialized_model_dict)\n@@ -105,6 +113,45 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n             \"date_saved\": datetime.datetime.now().strftime(\"%Y-%m-%d@%H:%M:%S\"),\n         }\n     )\n+    return config_json, metadata_json\n+\n+\n+def _save_model_to_dir(model, dirpath, weights_format):\n+    if not file_utils.exists(dirpath):\n+        file_utils.makedirs(dirpath)\n+    config_json, metadata_json = _serialize_model_as_json(model)\n+    with open(file_utils.join(dirpath, _METADATA_FILENAME), \"w\") as f:\n+        f.write(metadata_json)\n+    with open(file_utils.join(dirpath, _CONFIG_FILENAME), \"w\") as f:\n+        f.write(config_json)\n+    weights_filepath = file_utils.join(dirpath, _VARS_FNAME_H5)\n+    assert_dirpath = file_utils.join(dirpath, _ASSETS_DIRNAME)\n+    try:\n+        if weights_format == \"h5\":\n+            weights_store = H5IOStore(weights_filepath, mode=\"w\")\n+        elif weights_format == \"npz\":\n+            weights_store = NpzIOStore(weights_filepath, mode=\"w\")\n+        else:\n+            raise ValueError(\n+                \"Unknown `weights_format` argument. \"\n+                \"Expected 'h5' or 'npz'. \"\n+                f\"Received: weights_format={weights_format}\"\n+            )\n+        asset_store = DiskIOStore(assert_dirpath, mode=\"w\")\n+        _save_state(\n+            model,\n+            weights_store=weights_store,\n+            assets_store=asset_store,\n+            inner_path=\"\",\n+            visited_saveables=set(),\n+        )\n+    finally:\n+        weights_store.close()\n+        asset_store.close()\n+\n+\n+def _save_model_to_fileobj(model, fileobj, weights_format):\n+    config_json, metadata_json = _serialize_model_as_json(model)\n \n     with zipfile.ZipFile(fileobj, \"w\") as zf:\n         with zf.open(_METADATA_FILENAME, \"w\") as f:\n@@ -167,6 +214,13 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     else:\n         filepath = str(filepath)\n         if not filepath.endswith(\".keras\"):\n+            is_keras_dir = file_utils.isdir(filepath) and file_utils.exists(\n+                file_utils.join(filepath, \"config.json\")\n+            )\n+            if is_keras_dir:\n+                return _load_model_from_dir(\n+                    filepath, custom_objects, compile, safe_mode\n+                )\n             raise ValueError(\n                 \"Invalid filename: expected a `.keras` extension. \"\n                 f\"Received: filepath={filepath}\"\n@@ -177,11 +231,59 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n             )\n \n \n-def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n-    with zipfile.ZipFile(fileobj, \"r\") as zf:\n-        with zf.open(_CONFIG_FILENAME, \"r\") as f:\n-            config_json = f.read()\n+def _load_model_from_dir(dirpath, custom_objects, compile, safe_mode):\n+    if not file_utils.exists(dirpath):\n+        raise ValueError(f\"Directory doesn't exist: {dirpath}\")\n+    if not file_utils.isdir(dirpath):\n+        raise ValueError(f\"Path isn't a directory: {dirpath}\")\n \n+    with open(file_utils.join(dirpath, _CONFIG_FILENAME), \"r\") as f:\n+        config_json = f.read()\n+    model = _model_from_config(config_json, custom_objects, compile, safe_mode)\n+\n+    all_filenames = file_utils.listdir(dirpath)\n+    try:\n+        if _VARS_FNAME_H5 in all_filenames:\n+            weights_file_path = file_utils.join(dirpath, _VARS_FNAME_H5)\n+            weights_store = H5IOStore(weights_file_path, mode=\"r\")\n+        elif _VARS_FNAME_NPZ in all_filenames:\n+            weights_file_path = file_utils.join(dirpath, _VARS_FNAME_NPZ)\n+            weights_store = NpzIOStore(weights_file_path, mode=\"r\")\n+        else:\n+            raise ValueError(\n+                f\"Expected a {_VARS_FNAME_H5} or {_VARS_FNAME_NPZ} file.\"\n+            )\n+        if len(all_filenames) > 3:\n+            asset_store = DiskIOStore(\n+                file_utils.join(dirpath, _ASSETS_DIRNAME), mode=\"r\"\n+            )\n+\n+        else:\n+            asset_store = None\n+\n+        failed_saveables = set()\n+        error_msgs = {}\n+        _load_state(\n+            model,\n+            weights_store=weights_store,\n+            assets_store=asset_store,\n+            inner_path=\"\",\n+            visited_saveables=set(),\n+            failed_saveables=failed_saveables,\n+            error_msgs=error_msgs,\n+        )\n+\n+    finally:\n+        weights_store.close()\n+        if asset_store:\n+            asset_store.close()\n+\n+    if failed_saveables:\n+        _raise_loading_failure(error_msgs)\n+    return model\n+\n+\n+def _model_from_config(config_json, custom_objects, compile, safe_mode):\n     # Note: we should NOT use a custom JSON decoder. Anything that\n     # needs custom decoding must be handled in deserialize_keras_object.\n     config_dict = json.loads(config_json)\n@@ -193,6 +295,17 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n         model = deserialize_keras_object(\n             config_dict, custom_objects, safe_mode=safe_mode\n         )\n+    return model\n+\n+\n+def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n+    with zipfile.ZipFile(fileobj, \"r\") as zf:\n+        with zf.open(_CONFIG_FILENAME, \"r\") as f:\n+            config_json = f.read()\n+\n+        model = _model_from_config(\n+            config_json, custom_objects, compile, safe_mode\n+        )\n \n         all_filenames = zf.namelist()\n         weights_file_path = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6cf6a0e77dd504cfc35dd499dd8694b0b80b4ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -179,7 +179,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         layout_map[\".*dense.*bias\"] = distribution_lib.TensorLayout([\"model\"])\n \n         distribution = distribution_lib.ModelParallel(\n-            device_mesh, layout_map, batch_dim_name=\"batch\"\n+            layout_map=layout_map, batch_dim_name=\"batch\"\n         )\n \n         with distribution.scope():\n@@ -242,7 +242,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         layout_map[\".*dense.*bias\"] = distribution_lib.TensorLayout([\"model\"])\n \n         distribution = distribution_lib.ModelParallel(\n-            device_mesh, layout_map, batch_dim_name=\"batch\"\n+            layout_map=layout_map, batch_dim_name=\"batch\"\n         )\n         with distribution.scope():\n             inputs = layers.Input(shape=[28, 28, 1])\n@@ -284,7 +284,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         layout_map[\".*dense.*output\"] = (\"batch\", None)\n \n         distribution = distribution_lib.ModelParallel(\n-            device_mesh, layout_map, batch_dim_name=\"batch\"\n+            layout_map=layout_map, batch_dim_name=\"batch\"\n         )\n         sharding_capture = ShardingCaptureLayer()\n         with distribution.scope():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c250afad33a58faff9e6012c8022065babf0b79d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 50 | Churn Cumulative: 70 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.425\n\nDIFF:\n@@ -76,18 +76,32 @@ def bold_text(x, color=None):\n \n \n def format_layer_shape(layer):\n-    if not layer._inbound_nodes:\n+    if not layer._inbound_nodes and not layer._build_shapes_dict:\n         return \"?\"\n \n     def format_shape(shape):\n         highlighted = [highlight_number(x) for x in shape]\n         return \"(\" + \", \".join(highlighted) + \")\"\n \n+    # There are 2 approaches to get output shapes:\n+    # 1. Using `layer._inbound_nodes`, which is possible if the model is a\n+    # Sequential or Functional.\n+    # 2. Using `layer._build_shapes_dict`, which is possible if users manually\n+    # build the layer.\n+    if len(layer._inbound_nodes) > 0:\n         for i in range(len(layer._inbound_nodes)):\n             outputs = layer._inbound_nodes[i].output_tensors\n             output_shapes = tree.map_structure(\n                 lambda x: format_shape(x.shape), outputs\n             )\n+    else:\n+        try:\n+            outputs = layer.compute_output_shape(**layer._build_shapes_dict)\n+        except NotImplementedError:\n+            return \"?\"\n+        output_shapes = tree.map_shape_structure(\n+            lambda x: format_shape(x), outputs\n+        )\n     if len(output_shapes) == 1:\n         return output_shapes[0]\n     out = str(output_shapes)\n\n@@ -40,3 +40,37 @@ class SummaryUtilsTest(testing.TestCase, parameterized.TestCase):\n                 self.assertNotIn(\"Optimizer params\", summary_content)\n         except ImportError:\n             pass\n+\n+    def test_print_model_summary_custom_build(self):\n+        class MyModel(models.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.dense1 = layers.Dense(4, activation=\"relu\")\n+                self.dense2 = layers.Dense(2, activation=\"softmax\")\n+                self.unbuilt_dense = layers.Dense(1)\n+\n+            def build(self, input_shape):\n+                self.dense1.build(input_shape)\n+                input_shape = self.dense1.compute_output_shape(input_shape)\n+                self.dense2.build(input_shape)\n+\n+            def call(self, inputs):\n+                x = self.dense1(inputs)\n+                return self.dense2(x)\n+\n+        model = MyModel()\n+        model.build((None, 2))\n+\n+        summary_content = []\n+\n+        def print_to_variable(text, line_break=False):\n+            summary_content.append(text)\n+\n+        summary_utils.print_summary(model, print_fn=print_to_variable)\n+        summary_content = \"\\n\".join(summary_content)\n+        self.assertIn(\"(None, 4)\", summary_content)  # dense1\n+        self.assertIn(\"(None, 2)\", summary_content)  # dense2\n+        self.assertIn(\"?\", summary_content)  # unbuilt_dense\n+        self.assertIn(\"Total params: 22\", summary_content)\n+        self.assertIn(\"Trainable params: 22\", summary_content)\n+        self.assertIn(\"Non-trainable params: 0\", summary_content)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#707c2448e2af285d41909953e7f20dd667c9a221", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 45 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 120 | Churn Cumulative: 1045 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ Run via `./shell/api_gen.sh`.\n It generates API and formats user and generated APIs.\n \"\"\"\n \n+import importlib\n import os\n import re\n import shutil\n@@ -13,6 +14,7 @@ import shutil\n import namex\n \n package = \"keras\"\n+BUILD_DIR_NAME = \"tmp_build_dir\"\n \n \n def ignore_files(_, filenames):\n@@ -21,7 +23,7 @@ def ignore_files(_, filenames):\n \n def copy_source_to_build_directory(root_path):\n     # Copy sources (`keras/` directory and setup files) to build dir\n-    build_dir = os.path.join(root_path, \"tmp_build_dir\")\n+    build_dir = os.path.join(root_path, BUILD_DIR_NAME)\n     if os.path.exists(build_dir):\n         shutil.rmtree(build_dir)\n     os.mkdir(build_dir)\n@@ -140,38 +142,22 @@ def export_version_string(api_init_fname):\n         f.write(contents)\n \n \n-def update_package_init(init_fname):\n-    contents = \"\"\"\n-# Import everything from /api/ into keras.\n-from keras.api import *  # noqa: F403\n-from keras.api import __version__  # Import * ignores names start with \"_\".\n-\n-import os\n-\n-# Add everything in /api/ to the module search path.\n-__path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n-\n-# Don't pollute namespace.\n-del os\n-\n-# Never autocomplete `.src` or `.api` on an imported keras object.\n-def __dir__():\n-    keys = dict.fromkeys((globals().keys()))\n-    keys.pop(\"src\")\n-    keys.pop(\"api\")\n-    return list(keys)\n-\n-\n-# Don't import `.src` or `.api` during `from keras import *`.\n-__all__ = [\n-    name\n-    for name in globals().keys()\n-    if not (name.startswith(\"_\") or name in (\"src\", \"api\"))\n-]\"\"\"\n-    with open(init_fname) as f:\n-        init_contents = f.read()\n-    with open(init_fname, \"w\") as f:\n-        f.write(init_contents.replace(\"\\nfrom keras import api\", contents))\n+def update_package_init(template_fname, dest_fname, api_module):\n+    with open(template_fname) as template_file:\n+        with open(dest_fname, \"w\") as dest_file:\n+            for line in template_file:\n+                if \"# DO NOT EDIT.\" in line:\n+                    dest_file.write(line)\n+                    # Import all symbols from `api/`.\n+                    for symbol in api_module.__dict__.keys():\n+                        if symbol.startswith(\"__\") and symbol != \"__version__\":\n+                            continue\n+                        dest_file.write(f\"from keras.api import {symbol}\\n\")\n+                    # Skip the previous autogenerated block.\n+                    for line in template_file:\n+                        if \"# END DO NOT EDIT.\" in line:\n+                            break\n+                dest_file.write(line)\n \n \n def build():\n@@ -195,12 +181,13 @@ def build():\n         namex.generate_api_files(\n             \"keras\", code_directory=\"src\", target_directory=\"api\"\n         )\n-        # Creates `keras/__init__.py` importing from `keras/api`\n-        update_package_init(build_init_fname)\n-        # Add __version__ to keras package\n+        # Add __version__ to `api/`.\n         export_version_string(build_api_init_fname)\n         # Creates `_tf_keras` with full keras API\n         create_legacy_directory(package_dir=os.path.join(build_dir, package))\n+        # Update toplevel init with all `api/` imports.\n+        api_module = importlib.import_module(f\"{BUILD_DIR_NAME}.keras.api\")\n+        update_package_init(code_init_fname, build_init_fname, api_module)\n         # Copy back the keras/api and keras/__init__.py from build directory\n         if os.path.exists(code_api_dir):\n             shutil.rmtree(code_api_dir)\n\n@@ -1,14 +1,57 @@\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n import os\n \n-# Import everything from /api/ into keras.\n-from keras.api import *  # noqa: F403\n-from keras.api import __version__  # Import * ignores names start with \"_\".\n+# DO NOT EDIT. Generated by api_gen.sh\n+from keras.api import DTypePolicy\n+from keras.api import FloatDTypePolicy\n+from keras.api import Function\n+from keras.api import Initializer\n+from keras.api import Input\n+from keras.api import InputSpec\n+from keras.api import KerasTensor\n+from keras.api import Layer\n+from keras.api import Loss\n+from keras.api import Metric\n+from keras.api import Model\n+from keras.api import Operation\n+from keras.api import Optimizer\n+from keras.api import Quantizer\n+from keras.api import Regularizer\n+from keras.api import Sequential\n+from keras.api import StatelessScope\n+from keras.api import Variable\n+from keras.api import __version__\n+from keras.api import _tf_keras\n+from keras.api import activations\n+from keras.api import applications\n+from keras.api import backend\n+from keras.api import callbacks\n+from keras.api import config\n+from keras.api import constraints\n+from keras.api import datasets\n+from keras.api import device\n+from keras.api import distribution\n+from keras.api import dtype_policies\n+from keras.api import export\n+from keras.api import initializers\n+from keras.api import layers\n+from keras.api import legacy\n+from keras.api import losses\n+from keras.api import metrics\n+from keras.api import mixed_precision\n+from keras.api import models\n+from keras.api import name_scope\n+from keras.api import ops\n+from keras.api import optimizers\n+from keras.api import preprocessing\n+from keras.api import quantizers\n+from keras.api import random\n+from keras.api import regularizers\n+from keras.api import saving\n+from keras.api import tree\n+from keras.api import utils\n+from keras.api import version\n+\n+# END DO NOT EDIT.\n \n # Add everything in /api/ to the module search path.\n __path__.append(os.path.join(os.path.dirname(__file__), \"api\"))  # noqa: F405\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef50ceb0f05df7ba2bd406f935a75fad6a13255d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 9 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,6 @@ class UpSampling1D(Layer):\n       [ 0.  1.  2.]\n       [ 3.  4.  5.]\n       [ 3.  4.  5.]]\n-\n      [[ 6.  7.  8.]\n       [ 6.  7.  8.]\n       [ 9. 10. 11.]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54141df6546fb917b632f2d3be8b482bdf6c9ab3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 7 | Churn Cumulative: 1096 | Contributors (this commit): 8 | Commits (past 90d): 51 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1359,6 +1359,8 @@ def round(x, decimals=0):\n def tile(x, repeats):\n     if is_tensor(repeats):\n         repeats = tuple(repeats.int().numpy())\n+    if isinstance(repeats, int):\n+        repeats = (repeats,)\n     x = convert_to_tensor(x)\n     return torch.tile(x, dims=repeats)\n \n\n@@ -5047,6 +5047,8 @@ class Tile(Operation):\n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n         repeats = self.repeats\n+        if isinstance(repeats, int):\n+            repeats = [repeats]\n         if len(x_shape) > len(repeats):\n             repeats = [1] * (len(x_shape) - len(repeats)) + repeats\n         else:\n\n@@ -1445,6 +1445,7 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n \n     def test_tile(self):\n         x = KerasTensor((None, 3))\n+        self.assertEqual(knp.tile(x, 2).shape, (None, 6))\n         self.assertEqual(knp.tile(x, [2]).shape, (None, 6))\n         self.assertEqual(knp.tile(x, [1, 2]).shape, (None, 6))\n         self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, None, 6))\n@@ -1969,6 +1970,7 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n \n     def test_tile(self):\n         x = KerasTensor((2, 3))\n+        self.assertEqual(knp.tile(x, 2).shape, (2, 6))\n         self.assertEqual(knp.tile(x, [2]).shape, (2, 6))\n         self.assertEqual(knp.tile(x, [1, 2]).shape, (2, 6))\n         self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, 2, 6))\n@@ -4119,6 +4121,7 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n     def test_tile(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(knp.tile(x, 2), np.tile(x, 2))\n         self.assertAllClose(knp.tile(x, [2, 3]), np.tile(x, [2, 3]))\n         self.assertAllClose(knp.Tile([2, 3])(x), np.tile(x, [2, 3]))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f846b0cfeafbe9004e727c571f0e496057201b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1050 | Contributors (this commit): 12 | Commits (past 90d): 12 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -148,9 +148,9 @@ def update_package_init(template_fname, dest_fname, api_module):\n             for line in template_file:\n                 if \"# DO NOT EDIT.\" in line:\n                     dest_file.write(line)\n-                    # Import all symbols from `api/`.\n+                    # Import all public symbols from `api/` and `__version__`.\n                     for symbol in api_module.__dict__.keys():\n-                        if symbol.startswith(\"__\") and symbol != \"__version__\":\n+                        if symbol.startswith(\"_\") and symbol != \"__version__\":\n                             continue\n                         dest_file.write(f\"from keras.api import {symbol}\\n\")\n                     # Skip the previous autogenerated block.\n\n@@ -20,7 +20,6 @@ from keras.api import Sequential\n from keras.api import StatelessScope\n from keras.api import Variable\n from keras.api import __version__\n-from keras.api import _tf_keras\n from keras.api import activations\n from keras.api import applications\n from keras.api import backend\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ea70698b98015dd41fab28e273e6ce9638f3597", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 89 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,7 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n             )\n         # Broadcast sample_weight. It doesn't change the multiplication below\n         # but changes the sample_weight reduction applied later.\n-        sample_weight = ops.broadcast_to(sample_weight, values.shape)\n+        sample_weight = ops.broadcast_to(sample_weight, ops.shape(values))\n         values = values * sample_weight\n         if weight_ndim > 1:\n             sample_weight = reduce_fn(\n\n@@ -1,6 +1,8 @@\n import numpy as np\n \n+from keras.src import backend\n from keras.src import testing\n+from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.metrics import reduction_metrics\n from keras.src.saving import register_keras_serializable\n \n@@ -86,6 +88,13 @@ class MeanTest(testing.TestCase):\n         result = mean_obj.result()\n         self.assertAllClose(result, 3.0, atol=1e-3)\n \n+    def test_weighted_dynamic_shapes(self):\n+        mean_obj = reduction_metrics.Mean(name=\"mean\", dtype=\"float32\")\n+        result = backend.compute_output_spec(\n+            mean_obj, KerasTensor((None, 2)), KerasTensor((None, 2))\n+        )\n+        self.assertAllEqual(result.shape, ())\n+\n \n # How users would register a custom function or class to use with\n # MeanMetricWrapper.\n@@ -153,3 +162,15 @@ class MetricWrapperTest(testing.TestCase):\n         sample_weight = np.array([[1.0, 0.0, 0.5, 0.0, 1.0]])\n         result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(0.45, result, atol=1e-5)\n+\n+    def test_weighted_dynamic_shape(self):\n+        mse_obj = reduction_metrics.MeanMetricWrapper(\n+            fn=mse, name=\"mse\", dtype=\"float32\"\n+        )\n+        result = backend.compute_output_spec(\n+            mse_obj,\n+            KerasTensor((None, 5)),\n+            KerasTensor((None, 5)),\n+            KerasTensor((None, 5)),\n+        )\n+        self.assertAllEqual(result.shape, ())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
