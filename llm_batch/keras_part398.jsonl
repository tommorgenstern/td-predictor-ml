{"custom_id": "keras#df108c516b8946474f0c7513749b28620bf798e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1757 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -411,6 +411,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   @property\n   def global_clipnorm(self):\n     \"\"\"`float` or `None`. If set, clips gradients to a maximum norm.\n+    \n     Check `tf.clip_by_global_norm` for more details.\"\"\"\n     return self._global_clipnorm\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2049ff503d9aab18de2479a772c4e6c6e769b986", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 196 | Files Changed: 1 | Hunks: 78 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 290 | Churn Cumulative: 2462 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,8 +31,8 @@ ALL_LOSSES = [\n     losses.mean_absolute_percentage_error,\n     losses.mean_squared_logarithmic_error, losses.squared_hinge, losses.hinge,\n     losses.categorical_crossentropy, losses.binary_crossentropy,\n-    losses.kl_divergence, losses.poisson,\n-    losses.cosine_similarity, losses.log_cosh, losses.categorical_hinge\n+    losses.kl_divergence, losses.poisson, losses.cosine_similarity,\n+    losses.log_cosh, losses.categorical_hinge\n ]\n \n \n@@ -99,10 +99,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     p = backend.placeholder()\n     o = losses.categorical_crossentropy(t, p)\n \n-    t_val = tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.],\n-                                                    [0., 0., 1.]])\n-    p_val = tf.convert_to_tensor([[.9, .05, .05],\n-                                                    [.05, .89, .06],\n+    t_val = tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n+    p_val = tf.convert_to_tensor([[.9, .05, .05], [.05, .89, .06],\n                                   [.05, .01, .94]])\n     f = backend.function([t, p], o)\n \n@@ -110,8 +108,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertArrayNear(result, [.105, .116, .062], 1e-3)\n \n     # from logits\n-    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.],\n-                                                    [2., 3., 5.]])\n+    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n     o = losses.categorical_crossentropy(t, p, from_logits=True)\n     f = backend.function([t, p], o)\n \n@@ -141,8 +138,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     o = losses.sparse_categorical_crossentropy(t, p)\n \n     t_val = tf.convert_to_tensor([0, 1, 2])\n-    p_val = tf.convert_to_tensor([[.9, .05, .05],\n-                                                    [.05, .89, .06],\n+    p_val = tf.convert_to_tensor([[.9, .05, .05], [.05, .89, .06],\n                                   [.05, .01, .94]])\n     f = backend.function([t, p], o)\n \n@@ -150,8 +146,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertArrayNear(result, [.105, .116, .062], 1e-3)\n \n     # from logits\n-    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.],\n-                                                    [2., 3., 5.]])\n+    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n     o = losses.sparse_categorical_crossentropy(t, p, from_logits=True)\n     f = backend.function([t, p], o)\n \n@@ -331,7 +326,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n       assert False, 'Function should not be autographed.'\n       return fn\n \n-    with tf.compat.v1.test.mock.patch.object(autograph, 'tf_convert', tf_convert):\n+    with tf.compat.v1.test.mock.patch.object(autograph, 'tf_convert',\n+                                             tf_convert):\n       loss(y_true, y_pred)\n \n \n@@ -353,27 +349,21 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n   def test_unweighted(self):\n     mse_obj = losses.MeanSquaredError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mse_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), 49.5, 3)\n \n   def test_scalar_weighted(self):\n     mse_obj = losses.MeanSquaredError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 113.85, 3)\n \n   def test_sample_weighted(self):\n     mse_obj = losses.MeanSquaredError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n     loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 767.8 / 6, 3)\n@@ -395,9 +385,7 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     mse_obj = losses.MeanSquaredError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3, 1),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1), dtype=tf.float32)\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n     loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 587 / 6, 3)\n@@ -405,9 +393,7 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     mse_obj = losses.MeanSquaredError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mse_obj(y_true, y_pred, sample_weight=0)\n     self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n \n@@ -424,9 +410,7 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n   def test_no_reduction(self):\n     mse_obj = losses.MeanSquaredError(reduction=losses_utils.ReductionV2.NONE)\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n     loss = self.evaluate(loss)\n     self.assertArrayNear(loss, [84.3333, 143.3666], 1e-3)\n@@ -434,9 +418,7 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n   def test_sum_reduction(self):\n     mse_obj = losses.MeanSquaredError(reduction=losses_utils.ReductionV2.SUM)\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 227.69998, 3)\n \n@@ -459,27 +441,21 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n   def test_unweighted(self):\n     mae_obj = losses.MeanAbsoluteError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mae_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), 5.5, 3)\n \n   def test_scalar_weighted(self):\n     mae_obj = losses.MeanAbsoluteError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 12.65, 3)\n \n   def test_sample_weighted(self):\n     mae_obj = losses.MeanAbsoluteError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n     loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 81.4 / 6, 3)\n@@ -487,9 +463,7 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     mae_obj = losses.MeanAbsoluteError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3, 1),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1), dtype=tf.float32)\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n     loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 83 / 6, 3)\n@@ -497,9 +471,7 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     mae_obj = losses.MeanAbsoluteError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mae_obj(y_true, y_pred, sample_weight=0)\n     self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n \n@@ -516,9 +488,7 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n   def test_no_reduction(self):\n     mae_obj = losses.MeanAbsoluteError(reduction=losses_utils.ReductionV2.NONE)\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n     loss = self.evaluate(loss)\n     self.assertArrayNear(loss, [10.7333, 14.5666], 1e-3)\n@@ -526,18 +496,14 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n   def test_sum_reduction(self):\n     mae_obj = losses.MeanAbsoluteError(reduction=losses_utils.ReductionV2.SUM)\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 25.29999, 3)\n \n   def test_ragged_tensor(self):\n     mae_obj = losses.MeanAbsoluteError()\n-    y_true = tf.ragged.constant([[1, 9, 2], [-5, -2]],\n-                                         dtype=tf.float32)\n-    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]],\n-                                         dtype=tf.float32)\n+    y_true = tf.ragged.constant([[1, 9, 2], [-5, -2]], dtype=tf.float32)\n+    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]], dtype=tf.float32)\n     # loss = [14/3, 16/2]\n     sample_weight = tf.constant([1.2, 1.0], shape=(2, 1))\n     loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n@@ -555,36 +521,28 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n \n   def test_all_correct_unweighted(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n-    y_true = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_true = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mape_obj(y_true, y_true)\n     self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n \n   def test_unweighted(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mape_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), 211.8518, 3)\n \n   def test_scalar_weighted(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 487.259, 3)\n \n   def test_sample_weighted(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n     loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 422.8888, 3)\n@@ -592,8 +550,7 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n   def test_ragged_tensors(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n     y_true = tf.ragged.constant([[1, 9, 2], [-5, -2]])\n-    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]],\n-                                         dtype=tf.float32)\n+    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]], dtype=tf.float32)\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n     loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 510.7222, 3)\n@@ -601,9 +558,7 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3, 1),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1), dtype=tf.float32)\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n     loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 694.4445, 3)\n@@ -611,9 +566,7 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     mape_obj = losses.MeanAbsolutePercentageError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mape_obj(y_true, y_pred, sample_weight=0)\n     self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n \n@@ -621,9 +574,7 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n     mape_obj = losses.MeanAbsolutePercentageError(\n         reduction=losses_utils.ReductionV2.NONE)\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n     loss = self.evaluate(loss)\n     self.assertArrayNear(loss, [621.8518, 352.6666], 1e-3)\n@@ -641,27 +592,21 @@ class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n   def test_unweighted(self):\n     msle_obj = losses.MeanSquaredLogarithmicError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = msle_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), 1.4370, 3)\n \n   def test_scalar_weighted(self):\n     msle_obj = losses.MeanSquaredLogarithmicError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = msle_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 3.3051, 3)\n \n   def test_sample_weighted(self):\n     msle_obj = losses.MeanSquaredLogarithmicError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n     loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 3.7856, 3)\n@@ -669,9 +614,7 @@ class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     msle_obj = losses.MeanSquaredLogarithmicError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3, 1),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1), dtype=tf.float32)\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n     loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 2.6473, 3)\n@@ -679,9 +622,7 @@ class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     msle_obj = losses.MeanSquaredLogarithmicError()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = msle_obj(y_true, y_pred, sample_weight=0)\n     self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n \n@@ -689,8 +630,7 @@ class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n     msle_obj = losses.MeanSquaredLogarithmicError()\n     y_true = tf.ragged.constant([[1, 9, 2], [-5, -2]])\n     # log(max(y_true, 0) + 1): [[0.69314, 2.3025, 1.0986], [0., 0.]]\n-    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]],\n-                                         dtype=tf.float32)\n+    y_pred = tf.ragged.constant([[4, 8, 12], [8, 1]], dtype=tf.float32)\n     # log(max(y_pred, 0) + 1): [[1.6094, 2.1972, 2.5649], [2.1972, 0.6932]]\n     # per batch loss: [1.0002, 2.6541]\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n@@ -744,9 +684,7 @@ class CosineSimilarityTest(tf.test.TestCase):\n     cosine_obj = losses.CosineSimilarity()\n     sample_weight = np.asarray([1.2, 3.4])\n     loss = cosine_obj(\n-        self.y_true,\n-        self.y_pred,\n-        sample_weight=tf.constant(sample_weight))\n+        self.y_true, self.y_pred, sample_weight=tf.constant(sample_weight))\n     expected_loss = -np.mean(self.expected_loss * sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n \n@@ -763,8 +701,7 @@ class CosineSimilarityTest(tf.test.TestCase):\n \n     y_true = tf.constant(np_y_true)\n     y_pred = tf.constant(np_y_pred)\n-    loss = cosine_obj(\n-        y_true, y_pred, sample_weight=tf.constant(sample_weight))\n+    loss = cosine_obj(y_true, y_pred, sample_weight=tf.constant(sample_weight))\n \n     expected_loss = -np.mean(expected_loss * sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n@@ -793,15 +730,13 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n     self.assertEqual(bce_obj.reduction, losses_utils.ReductionV2.SUM)\n \n   def test_all_correct_unweighted(self):\n-    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n-                                  dtype=tf.float32)\n+    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n     bce_obj = losses.BinaryCrossentropy()\n     loss = bce_obj(y_true, y_true)\n     self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n \n     # Test with logits.\n-    logits = tf.constant([[100.0, -100.0, -100.0],\n-                                   [-100.0, 100.0, -100.0],\n+    logits = tf.constant([[100.0, -100.0, -100.0], [-100.0, 100.0, -100.0],\n                           [-100.0, -100.0, 100.0]])\n     bce_obj = losses.BinaryCrossentropy(from_logits=True)\n     loss = bce_obj(y_true, logits)\n@@ -827,8 +762,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n \n     # Test with logits.\n     y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n-    logits = tf.constant([[100.0, -100.0, 100.0],\n-                                   [100.0, 100.0, -100.0]])\n+    logits = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n     bce_obj = losses.BinaryCrossentropy(from_logits=True)\n     loss = bce_obj(y_true, logits)\n \n@@ -866,8 +800,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n \n     # Test with logits.\n     y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n-    logits = tf.constant([[100.0, -100.0, 100.0],\n-                                   [100.0, 100.0, -100.0]])\n+    logits = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n     bce_obj = losses.BinaryCrossentropy(from_logits=True)\n     loss = bce_obj(y_true, logits, sample_weight=2.3)\n \n@@ -900,8 +833,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n \n     # Test with logits.\n     y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n-    logits = tf.constant([[100.0, -100.0, 100.0],\n-                                   [100.0, 100.0, -100.0]])\n+    logits = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n     weights = tf.constant([4, 3])\n     bce_obj = losses.BinaryCrossentropy(from_logits=True)\n     loss = bce_obj(y_true, logits, sample_weight=weights)\n@@ -916,8 +848,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n \n   def test_no_reduction(self):\n     y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n-    logits = tf.constant([[100.0, -100.0, 100.0],\n-                                   [100.0, 100.0, -100.0]])\n+    logits = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n     bce_obj = losses.BinaryCrossentropy(\n         from_logits=True, reduction=losses_utils.ReductionV2.NONE)\n     loss = bce_obj(y_true, logits)\n@@ -983,8 +914,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n \n     # Test with logits.\n     y_true = tf.ragged.constant([[1, 0, 1], [0, 1]])\n-    logits = tf.ragged.constant([[100.0, -100.0, 100.0],\n-                                          [100.0, 100.0]])\n+    logits = tf.ragged.constant([[100.0, -100.0, 100.0], [100.0, 100.0]])\n     weights = tf.constant([4, 3])\n     bce_obj = losses.BinaryCrossentropy(from_logits=True)\n     loss = bce_obj(y_true, logits, sample_weight=weights)\n@@ -1191,8 +1121,7 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertEqual(cce_obj.reduction, losses_utils.ReductionV2.SUM)\n \n   def test_all_correct_unweighted(self):\n-    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n-                                  dtype=tf.int64)\n+    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.int64)\n     y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\n                          dtype=tf.float32)\n     cce_obj = losses.CategoricalCrossentropy()\n@@ -1208,8 +1137,8 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n   def test_unweighted(self):\n     cce_obj = losses.CategoricalCrossentropy()\n     y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-    y_pred = tf.constant(\n-        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],\n+                         dtype=tf.float32)\n     loss = cce_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), .3239, 3)\n \n@@ -1222,8 +1151,8 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n   def test_scalar_weighted(self):\n     cce_obj = losses.CategoricalCrossentropy()\n     y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-    y_pred = tf.constant(\n-        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],\n+                         dtype=tf.float32)\n     loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), .7449, 3)\n \n@@ -1236,8 +1165,8 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n   def test_sample_weighted(self):\n     cce_obj = losses.CategoricalCrossentropy()\n     y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-    y_pred = tf.constant(\n-        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],\n+                         dtype=tf.float32)\n     sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))\n     loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)\n@@ -1302,8 +1231,7 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n \n   def test_shape_mismatch(self):\n     y_true = tf.constant([[0], [1], [2]])\n-    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6],\n-                                   [.05, .01, .94]])\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n \n     cce_obj = losses.CategoricalCrossentropy()\n     with self.assertRaisesRegex(ValueError, 'Shapes .+ are incompatible'):\n@@ -1313,8 +1241,7 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     cce_obj = losses.CategoricalCrossentropy()\n     y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])\n     y_pred = tf.ragged.constant(\n-        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],\n-        dtype=tf.float32)\n+        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]], dtype=tf.float32)\n     # batch losses [[0.1054, 0.8047], [0.0619]]\n     sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))\n     loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n@@ -1322,8 +1249,7 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)\n \n     # Test with logits.\n-    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],\n-                                          [[2., 3., 5.]]])\n+    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]])\n     cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n     # batch losses [[0.0018, 0.0004], [0.1698]]\n     loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n@@ -1377,8 +1303,8 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n   def test_unweighted(self):\n     cce_obj = losses.SparseCategoricalCrossentropy()\n     y_true = tf.constant([0, 1, 2])\n-    y_pred = tf.constant(\n-        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],\n+                         dtype=tf.float32)\n     loss = cce_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), .3239, 3)\n \n@@ -1391,8 +1317,8 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n   def test_scalar_weighted(self):\n     cce_obj = losses.SparseCategoricalCrossentropy()\n     y_true = tf.constant([[0], [1], [2]])\n-    y_pred = tf.constant(\n-        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],\n+                         dtype=tf.float32)\n     loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), .7449, 3)\n \n@@ -1405,8 +1331,8 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n   def test_sample_weighted(self):\n     cce_obj = losses.SparseCategoricalCrossentropy()\n     y_true = tf.constant([[0], [1], [2]])\n-    y_pred = tf.constant(\n-        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)\n+    y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]],\n+                         dtype=tf.float32)\n     sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))\n     loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)\n@@ -1437,8 +1363,7 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n     cce_obj = losses.SparseCategoricalCrossentropy()\n     y_true = tf.ragged.constant([[0, 1], [2]])\n     y_pred = tf.ragged.constant(\n-        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],\n-        dtype=tf.float32)\n+        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]], dtype=tf.float32)\n     # batch losses [[0.1054, 0.8047], [0.0619]]\n     sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))\n     loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n@@ -1446,8 +1371,7 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)\n \n     # Test with logits.\n-    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],\n-                                          [[2., 3., 5.]]])\n+    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]])\n     cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n     # batch losses [[0.0018, 0.0004], [0.1698]]\n     loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n@@ -1467,8 +1391,8 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)\n \n     # Test with logits.\n-    logits = tf.ragged.constant(\n-        [[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]], ragged_rank=1)\n+    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]],\n+                                ragged_rank=1)\n     cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n     # batch losses [[0.0018, 0.0004], [0.1698]]\n     loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n@@ -1478,8 +1402,7 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n     # shape [2, 1, None]\n     y_true = tf.ragged.constant([[[1, 1]], [[0]]])\n     # shape [2, 1, None, 2]\n-    y_pred = tf.ragged.constant([[[[0.1, 0.9], [0.1, 0.9]]],\n-                                          [[[0.9, 0.1]]]])\n+    y_pred = tf.ragged.constant([[[[0.1, 0.9], [0.1, 0.9]]], [[[0.9, 0.1]]]])\n     cce_obj = losses.SparseCategoricalCrossentropy()\n     loss = cce_obj(y_true, y_pred)\n     self.assertAlmostEqual(self.evaluate(loss), 0.1054, 3)\n@@ -1497,8 +1420,7 @@ class HingeTest(tf.test.TestCase):\n   def test_unweighted(self):\n     hinge_obj = losses.Hinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n \n@@ -1515,8 +1437,7 @@ class HingeTest(tf.test.TestCase):\n   def test_scalar_weighted(self):\n     hinge_obj = losses.Hinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n \n@@ -1538,8 +1459,7 @@ class HingeTest(tf.test.TestCase):\n   def test_sample_weighted(self):\n     hinge_obj = losses.Hinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n \n@@ -1558,8 +1478,8 @@ class HingeTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     hinge_obj = losses.Hinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]], shape=(2, 4, 1))\n-    y_pred = tf.constant(\n-        [[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]], shape=(2, 4, 1))\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]],\n+                         shape=(2, 4, 1))\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2, 1, 3], shape=(2, 4))\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n@@ -1579,8 +1499,7 @@ class HingeTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     hinge_obj = losses.Hinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n     loss = hinge_obj(y_true, y_pred, sample_weight=0)\n     self.assertAllClose(self.evaluate(loss), 0., 1e-3)\n \n@@ -1597,8 +1516,7 @@ class SquaredHingeTest(tf.test.TestCase):\n   def test_unweighted(self):\n     sq_hinge_obj = losses.SquaredHinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n \n@@ -1618,8 +1536,7 @@ class SquaredHingeTest(tf.test.TestCase):\n   def test_scalar_weighted(self):\n     sq_hinge_obj = losses.SquaredHinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n \n@@ -1644,8 +1561,7 @@ class SquaredHingeTest(tf.test.TestCase):\n   def test_sample_weighted(self):\n     sq_hinge_obj = losses.SquaredHinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n \n@@ -1667,8 +1583,8 @@ class SquaredHingeTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     sq_hinge_obj = losses.SquaredHinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]], shape=(2, 4, 1))\n-    y_pred = tf.constant(\n-        [[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]], shape=(2, 4, 1))\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]],\n+                         shape=(2, 4, 1))\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2, 1, 3], shape=(2, 4))\n \n     # loss = max(0, 1-y_true * y_pred), where y_true is -1/1\n@@ -1688,8 +1604,7 @@ class SquaredHingeTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     sq_hinge_obj = losses.SquaredHinge()\n     y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6],\n-                                   [-0.25, -1., 0.5, 0.6]])\n+    y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1., 0.5, 0.6]])\n     loss = sq_hinge_obj(y_true, y_pred, sample_weight=0)\n     self.assertAllClose(self.evaluate(loss), 0., 1e-3)\n \n@@ -1706,9 +1621,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n   def test_unweighted(self):\n     cat_hinge_obj = losses.CategoricalHinge()\n     y_true = tf.constant([1, 9, 2, -5], shape=(2, 2))\n-    y_pred = tf.constant([4, 8, 12, 8],\n-                                  shape=(2, 2),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8], shape=(2, 2), dtype=tf.float32)\n     loss = cat_hinge_obj(y_true, y_pred)\n \n     # pos = reduce_sum(y_true * y_pred) = [1*4+8*9, 12*2+8*-5] = [76, -16]\n@@ -1720,9 +1633,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n   def test_scalar_weighted(self):\n     cat_hinge_obj = losses.CategoricalHinge()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = cat_hinge_obj(y_true, y_pred, sample_weight=2.3)\n     self.assertAlmostEqual(self.evaluate(loss), 83.95, 3)\n \n@@ -1733,9 +1644,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n   def test_sample_weighted(self):\n     cat_hinge_obj = losses.CategoricalHinge()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n     loss = cat_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 124.1, 3)\n@@ -1743,9 +1652,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n   def test_timestep_weighted(self):\n     cat_hinge_obj = losses.CategoricalHinge()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3, 1),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1), dtype=tf.float32)\n     sample_weight = tf.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n     loss = cat_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAlmostEqual(self.evaluate(loss), 4.0, 3)\n@@ -1753,9 +1660,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n   def test_zero_weighted(self):\n     cat_hinge_obj = losses.CategoricalHinge()\n     y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-    y_pred = tf.constant([4, 8, 12, 8, 1, 3],\n-                                  shape=(2, 3),\n-                                  dtype=tf.float32)\n+    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.float32)\n     loss = cat_hinge_obj(y_true, y_pred, sample_weight=0)\n     self.assertAlmostEqual(self.evaluate(loss), 0., 3)\n \n@@ -1827,9 +1732,7 @@ class LogCoshTest(tf.test.TestCase):\n     y_pred = tf.constant(y_pred, dtype=tf.float32)\n     y_true = tf.constant(y_true)\n     loss = logcosh_obj(\n-        y_true,\n-        y_pred,\n-        sample_weight=tf.constant(sample_weight, shape=(2, 3)))\n+        y_true, y_pred, sample_weight=tf.constant(sample_weight, shape=(2, 3)))\n     expected_loss = np.sum(expected_losses * sample_weight) / self.batch_size\n     self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n \n@@ -1909,9 +1812,7 @@ class PoissonTest(tf.test.TestCase):\n     y_true = tf.constant(y_true)\n \n     loss = poisson_obj(\n-        y_true,\n-        y_pred,\n-        sample_weight=tf.constant(sample_weight, shape=(2, 3)))\n+        y_true, y_pred, sample_weight=tf.constant(sample_weight, shape=(2, 3)))\n     expected_loss = np.sum(expected_losses * sample_weight) / self.batch_size\n     self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n \n@@ -1987,8 +1888,7 @@ class KLDivergenceTest(tf.test.TestCase):\n \n     y_pred = tf.constant(y_pred, dtype=tf.float32)\n     y_true = tf.constant(y_true)\n-    loss = k_obj(\n-        y_true, y_pred, sample_weight=tf.constant(sample_weight))\n+    loss = k_obj(y_true, y_pred, sample_weight=tf.constant(sample_weight))\n \n     num_timesteps = 3\n     expected_loss = np.sum(expected_losses * sample_weight) / (\n@@ -2079,9 +1979,7 @@ class HuberLossTest(tf.test.TestCase):\n     y_true = tf.constant(y_true)\n     sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3, 1))\n     loss = h_obj(\n-        y_true,\n-        y_pred,\n-        sample_weight=tf.constant(sample_weight, shape=(2, 3)))\n+        y_true, y_pred, sample_weight=tf.constant(sample_weight, shape=(2, 3)))\n     actual_loss = np.multiply(expected_losses, sample_weight)\n     actual_loss = np.sum(actual_loss) / self.batch_size\n     self.assertAlmostEqual(self.evaluate(loss), actual_loss, 3)\n@@ -2135,10 +2033,10 @@ class BinaryTruePositivesViaControlFlow(losses.Loss):\n class CustomLossTest(tf.test.TestCase):\n \n   def test_autograph(self):\n-    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n-                                   [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n-    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n-                                   [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n+    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1], [1, 1, 1, 1, 0],\n+                          [0, 0, 0, 0, 1.5]])\n+    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0],\n+                          [1, 10, 1, 1, 1]])\n \n     @tf.function\n     def loss_fn(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e812c17f6ab32e6cc8229d502a6b13c4eedd08c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 5291 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -253,7 +253,7 @@ class RandomCrop(base_layer.BaseRandomLayer):\n   def call(self, inputs, training=True):\n     if training is None:\n       training = backend.learning_phase()\n-    inputs = utils.ensure_tensor(inputs)\n+    inputs = utils.ensure_tensor(inputs, dtype=self.compute_dtype)\n     input_shape = tf.shape(inputs)\n     h_diff = input_shape[H_AXIS] - self.height\n     w_diff = input_shape[W_AXIS] - self.width\n@@ -266,11 +266,15 @@ class RandomCrop(base_layer.BaseRandomLayer):\n       return tf.image.crop_to_bounding_box(inputs, h_start, w_start,\n                                            self.height, self.width)\n \n-    outputs = tf.cond(\n-        tf.reduce_all((training, h_diff >= 0, w_diff >= 0)), random_crop,\n-        lambda: smart_resize(inputs, [self.height, self.width]))\n+    def resize():\n+      outputs = smart_resize(inputs, [self.height, self.width])\n+      # smart_resize will always output float32, so we need to re-cast.\n       return tf.cast(outputs, self.compute_dtype)\n \n+    return tf.cond(\n+        tf.reduce_all((training, h_diff >= 0, w_diff >= 0)), random_crop,\n+        resize)\n+\n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n     input_shape[H_AXIS] = self.height\n\n@@ -17,6 +17,7 @@\n import functools\n from absl.testing import parameterized\n \n+import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import sequential\n@@ -398,6 +399,12 @@ class RandomCropTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n+  @testing_utils.run_v2_only\n+  def test_uint8_input(self):\n+    inputs = keras.Input((128, 128, 3), batch_size=2, dtype=tf.uint8)\n+    layer = image_preprocessing.RandomCrop(64, 64)\n+    self.assertAllEqual(layer(inputs).dtype, 'float32')\n+\n   @testing_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#540d94baf15836cf729ce6b311e43af31639000e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 393 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,6 @@ from keras.engine.training import Model\n \n from tensorflow.python.util.tf_export import keras_export\n \n-__version__ = '2.8.0'\n+__version__ = '2.9.0'\n \n keras_export('keras.__version__').export_constant(__name__, '__version__')\n\n@@ -30,7 +30,7 @@ DOCLINES = __doc__.split('\\n')\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = '2.8.0'\n+_VERSION = '2.9.0'\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#099a48f78f1e9fde735cd357cd08f0135a9e8a7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 32 | Churn Cumulative: 3361 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -262,6 +262,25 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     # reduced_weighted_mse = (6 + 26) / 2 =\n     self.assertAllClose(self.evaluate(loss), 16, 1e-2)\n \n+  def test_loss_wrapper_dtype(self):\n+    # Make sure the loss wrapper doesn't cause any numerical precision loss\n+    # during calculation. See https://github.com/keras-team/keras/issues/15791\n+    x = tf.convert_to_tensor([[2.1]], dtype=tf.float64)\n+    y_true = tf.square(x)\n+    y_pred = tf.convert_to_tensor([[3.68]], dtype=tf.float64)\n+\n+    # TF loss\n+    loss = losses.MeanSquaredError()\n+    tf_loss = loss(y_pred, y_true)\n+\n+    # manually computed loss in 64-bit\n+    man_loss64 = tf.squeeze(tf.square(y_pred - y_true))\n+\n+    self.assertEqual(tf_loss.dtype, tf.float64)\n+    # Make a smaller atol to ensure the float64 precision is hold.\n+    self.assertAllClose(self.evaluate(tf_loss), self.evaluate(man_loss64),\n+                        atol=1e-8)\n+\n   def test_invalid_reduction(self):\n     with self.assertRaisesRegex(ValueError, 'Invalid Reduction Key: Foo.'):\n       losses.MeanSquaredError(reduction='Foo')\n\n@@ -307,16 +307,20 @@ def compute_weighted_loss(losses,\n     if not isinstance(losses,\n                       (keras_tensor.KerasTensor, tf.RaggedTensor)):\n       losses = tf.convert_to_tensor(losses)\n-    input_dtype = losses.dtype\n \n     if not isinstance(sample_weight,\n                       (keras_tensor.KerasTensor, tf.RaggedTensor)):\n       sample_weight = tf.convert_to_tensor(sample_weight)\n \n-    # TODO(psv): Handle casting here in a better way, eg. if losses is float64\n-    # we do not want to lose precision.\n+    # Convert any non float dtypes to floats, to avoid it loss any precision for\n+    # dtype like int or bool.\n+    if not losses.dtype.is_floating:\n+      input_dtype = losses.dtype\n       losses = tf.cast(losses, 'float32')\n-    sample_weight = tf.cast(sample_weight, 'float32')\n+      input_casted = True\n+    else:\n+      input_casted = False\n+    sample_weight = tf.cast(sample_weight, losses.dtype)\n     # Update dimensions of `sample_weight` to match with `losses` if possible.\n     losses, _, sample_weight = squeeze_or_expand_dimensions(  # pylint: disable=unbalanced-tuple-unpacking\n         losses, None, sample_weight)\n@@ -324,6 +328,7 @@ def compute_weighted_loss(losses,\n \n     # Apply reduction function to the individual weighted losses.\n     loss = reduce_weighted_loss(weighted_losses, reduction)\n+    if input_casted:\n       # Convert the result back to the input type.\n       loss = tf.cast(loss, input_dtype)\n     return loss\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
