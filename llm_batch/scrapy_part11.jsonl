{"custom_id": "scrapy#09b11a3a7b28a0a2f06da4b2277fc37d8bfa3e38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 20 | Methods Changed: 12 | Complexity Δ (Sum/Max): 17/10 | Churn Δ: 89 | Churn Cumulative: 232 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.5217391304347826\n\nDIFF:\n@@ -5,6 +5,7 @@ SimpageMiddleware is a middleware for detecting similar page layouts\n import sys\n import datetime\n import pprint\n+import pickle\n \n from pydispatch import dispatcher\n \n@@ -13,7 +14,10 @@ from scrapy.http import Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n-from .metrics import tagdepth\n+from scrapy.contrib.rulengine.responseWrapper import ResponseWrapper\n+from scrapy.contrib.rulengine.pipeline import RulesPipeline\n+\n+from .metrics import tagdepth, histogram\n \n class SimpagesMiddleware(object):\n     \n@@ -25,41 +29,53 @@ class SimpagesMiddleware(object):\n             raise NotConfigured\n         repfilename = settings.get('SIMPAGES_REPORT_FILE')\n         self.reportfile = open(repfilename, \"a\") if repfilename else None        \n+        persistence_filename = open(repfilename + '.pickle', 'w') if repfilename else None\n+        self.persistent_simgroup = pickle.Pickler(persistence_filename) if persistence_filename else None\n         self.sim_groups = {}\n         self.last_group = 0\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+        #Rules \n+        RulesPipeline.loadRules()\n \n     def process_response(self, request, response, spider):\n         if isinstance(response, Response):\n-            group, simrate = self.get_similarity_group(response)\n+            group, simrate, simhash = self.get_similarity_group(response)\n             if group:\n-                self.sim_groups[group]['similar_urls'].append((response.url, simrate))\n+                wres = ResponseWrapper(response)\n+                rp = RulesPipeline(wres)\n+                rules_info = rp.execute()\n+                self.sim_groups[group]['similar_urls'].append((response.url, simrate, simhash, rules_info))\n             else:\n                 self.create_similarity_group(response)\n         return response\n \n     def get_similarity_group(self, response):\n-        sh = self.metric.simhash(response)\n+        sh = self.metric.simhash(response, symnumbers=True)\n         for group, data in self.sim_groups.iteritems():\n             simrate = self.metric.compare(sh, data['simhash'])\n             if simrate > self.threshold:\n-                return (group, simrate)\n-        return (None, 0)\n+                return (group, simrate, data['simhash'])\n+        return (None, 0, set())\n \n     def create_similarity_group(self, response):\n         self.last_group += 1\n         data = {}\n-        data['simhash'] = self.metric.simhash(response)\n+        data['simhash'] = self.metric.simhash(response, symnumbers=True)\n         data['first_url'] = response.url\n+        wres = ResponseWrapper(response)\n+        rp = RulesPipeline(wres)\n+        data['rules_info'] = rp.execute()\n         data['similar_urls'] = []\n         self.sim_groups[self.last_group] = data\n \n     def get_report(self):\n+        data_hist = dict( [(k, len(v['similar_urls'])) for k, v in self.sim_groups.items()] )\n         r =  \"Page similarity results\\n\"\n         r += \"=======================\\n\\n\"\n         r += \"Datetime : %s\\n\" % datetime.datetime.now()\n         r += \"Metric   : %s\\n\" % self.metric.__name__\n         r += \"Threshold: %s\\n\" % self.threshold\n+        r += \"Distribution per group:\\n%s\\n\" % histogram.plot(data_hist)\n         r += \"Results  :\\n\"\n         r += pprint.pformat(self.sim_groups)\n         r += \"\\n\\n\"\n@@ -71,3 +87,5 @@ class SimpagesMiddleware(object):\n             self.reportfile.write(rep)\n         else:\n             print rep\n+        if self.persistent_simgroup:\n+            self.persistent_simgroup.dump(self.sim_groups)\n\n@@ -4,7 +4,7 @@ SimpagesMiddleware.\n \n A metric must implement two functions:\n \n-1. simhash(response)\n+1. simhash(response, *args)\n \n Receives a response and returns a simhash of that response. A simhash can be\n an object of any type and its only purpose is to provide a fast way for\n\n@@ -0,0 +1,39 @@\n+import math\n+\n+def plot(data):\n+    \"\"\"\n+    data is a dict of tuples of the form: {key: quantity, ...}.\n+    Make the histogram of the data dict.\n+    \"\"\"\n+    maxv = max(data.values())\n+    minv = min(data.values())\n+    step = (maxv - minv) * 0.1 if (maxv - minv) != 0 else 1\n+    s = []\n+    for key, q in data.items():\n+        s1 = \"%s%s: \" % (key, blanks(6 - len(str(key))))\n+        for i in xrange(1, int(math.ceil(q/step)+1)):\n+            s1+= \"=\"\n+        if s1[len(s1)-1] == '=':\n+            s1+= \" \" \n+        s1 += str(q)\n+        s.append(s1)\n+    maxl = len(max(s, key=lambda x:len(x)))\n+    s2 = ''\n+    for i in xrange(1,maxl+1):\n+        s2 += '-'\n+\n+    r = \"\\tgroup | quantities\\n\"\n+    r += \"\\t%s\" % s2\n+    for x in s:\n+        r += \"\\n\\t%s\" % x\n+    return r\n+\n+def blanks(n):\n+    return ''.join([' ' for x in range(1,n+1)])\n+\n+def print_plot(data):\n+    print plot(data)\n+\n+\n+\n+\n\n@@ -11,7 +11,7 @@ from __future__ import division\n \n from BeautifulSoup import Tag\n \n-relevant_tags = set(['div', 'table', 'td', 'tr', 'h1'])\n+relevant_tags = set(['div', 'table', 'td', 'tr', 'h1','p'])\n \n def get_symbol_dict(node, tags=(), depth=1):\n     symdict = {}\n@@ -22,10 +22,18 @@ def get_symbol_dict(node, tags=(), depth=1):\n             symdict.update(get_symbol_dict(tag, tags, depth+1))\n     return symdict\n \n-def simhash(response):\n+def simhash(response, symnumbers=False):\n     soup = response.soup\n-    symdict = get_symbol_dict(response.soup.find('body'), relevant_tags)\n-    return set(symdict.keys())\n+    symdict = get_symbol_dict(soup.find('body'), relevant_tags)\n+    if symnumbers:\n+        s = set([k+str(v) for k,v in symdict.items()])\n+    else:\n+        s = set(symdict.keys())\n+    return s\n \n def compare(sh1, sh2):\n+    if sh1 == sh2:\n+        return 1.0\n+    else:\n         return len(sh1 & sh2) / len(sh1 | sh2)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b09be8601b0be7e0aa7f6af995dd1d0a4fde1e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 36 | Files Changed: 2 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): -5/1 | Churn Δ: 52 | Churn Cumulative: 334 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 0.8666666666666667\n\nDIFF:\n@@ -9,26 +9,24 @@ class Adaptor(object):\n     Adaptors instances should be instantiated and used only\n     inside the AdaptorPipe.\n     \"\"\"\n-    def __init__(self, function, name, attribute_re=None, attribute_list=None, negative_match=False):\n+    def __init__(self, function, name, match_function):\n         self.name = name\n         self.basefunction = function\n-        self.attribute_re = re.compile(attribute_re) if attribute_re else None\n-        self.attribute_list = attribute_list or []\n-        self.negative_match = negative_match\n-    def function(self, value, **pipeargs):\n-        return self.basefunction(value, **pipeargs)\n+        self.match_function = match_function\n+    def function(self, value):\n+        return self.basefunction(value)\n         \n class AdaptorPipe:\n \n     def __init__(self, adaptors=None, adaptorclass=None):\n         \"\"\"\n         If \"adaptors\" is given, constructs pipeline from this.\n-        \"define_from\" is an ordered tuple of 4-elements tuples, each of which\n+        \"adaptors\" is an ordered tuple of 3-elements tuples, each of which\n         has the same parameters you give to the insertadaptor method, except \n         'after' and 'before', because you define the adaptors order in the tuple.\n         . Example:\n         (\n-          (\"my_function\", \"my_function\", None, \"name\")\n+          (my_function, \"my_function\", lambda x: x in my_list)\n           ...\n         )\n         \"\"\"\n@@ -45,23 +43,18 @@ class AdaptorPipe:\n             _adaptors.append(a.name)\n         return _adaptors\n     \n-    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, negative_match=False, after=None, before=None):\n+    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None):\n         \"\"\"\n-        Inserts a \"function\" as an adaptor that will apply for attribute names\n-        which matches regex given in \"attrs_re\" (None matches all), or are included in \"attrs_list\" list.\n-        If both, attrs_re and attrs_list are given, apply both. Else if only one is given, apply those.\n-        Else, all attributes will match.\n-        If negative_match is True, the adaptor will be applied only if there is no match. By default,\n-        negative_match is False (i.e. match -> apply)\n+        Inserts a \"function\" as an adaptor that will apply when match_function returns True (by\n+        default always apply)\n         If \"after\" is given, inserts the adaptor after the already inserted adaptor\n         of the name given in this parameter, If \"before\" is given, inserts it before\n-        the adaptor of the given name. The \"function\" must always have a **keyword\n-        argument to ignore unused keywords. \"name\" is the name of the adaptor.\n+        the adaptor of the given name. \"name\" is the name of the adaptor.\n         \"\"\"\n         if name in self.adaptors_names:\n             raise DuplicatedAdaptorName(name)\n         else:\n-            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list, negative_match)\n+            adaptor = self.__adaptorclass(function, name, match_function)\n             #by default append adaptor at end of pipe\n             pos = len(self.adaptors_names)\n             if after:\n@@ -71,30 +64,17 @@ class AdaptorPipe:\n             self.__adaptorspipe.insert(pos, adaptor)\n             return pos\n \n-    def execute(self, attrname, value, debug=False, **pipeargs):\n+    def execute(self, match_condition, value, debug=False):\n         \"\"\"\n         Execute pipeline for attribute name \"attrname\" and value \"value\".\n         Pass the given pipeargs to each adaptor function in the pipe.\n         \"\"\"\n         for adaptor in self.__adaptorspipe:\n-            match = False\n-            if adaptor.attribute_re:\n-                if adaptor.attribute_re.search(attrname):\n-                    if adaptor.attribute_list:\n-                        match = attrname in adaptor.attribute_list\n-                    else:\n-                        match = True\n-            elif adaptor.attribute_list:\n-                match = attrname in adaptor.attribute_list\n-            else:\n-                match = True\n-            adapt = match ^ adaptor.negative_match\n-            if adapt:\n+            if adaptor.match_function(match_condition):\n                 try:\n                     if debug:\n-                        print \"pipeargs: %s\" % repr(pipeargs)\n                         print \"  %07s | input >\" % adaptor.name, repr(value)\n-                    value = adaptor.function(value, **pipeargs)\n+                    value = adaptor.function(value)\n                     if debug:\n                         print \"  %07s | output>\" % adaptor.name, repr(value)\n \n\n@@ -10,7 +10,7 @@ class ScrapedItem(object):\n     \"\"\"\n     adaptors_pipe = AdaptorPipe()\n \n-    def attribute(self, attrname, value, **pipeargs):\n-        value = self.adaptors_pipe.execute(attrname, value, **pipeargs)\n+    def attribute(self, attrname, value, match_condition=None):\n+        value = self.adaptors_pipe.execute(match_condition or attrname, value)\n         if not hasattr(self, attrname):\n             setattr(self, attrname, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fe36e77d2216d6a55cc719ffa2ee676154cc45e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 26 | Churn Cumulative: 1344 | Contributors (this commit): 2 | Commits (past 90d): 45 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,14 +11,7 @@ from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n-priorities = { 20:'NORMAL',\n-               10:'QUICK',\n-                0:'NOW',\n-}\n-\n-# Set priorities module attributes\n-for val, attr in priorities.items():\n-    setattr(sys.modules[__name__], \"PRIORITY_%s\" % attr, val )\n+DEFAULT_PRIORITY = settings.getint(\"DEFAULT_PRIORITY\") or 20\n \n def my_import(name):\n     mod = __import__(name)\n@@ -244,7 +237,7 @@ class ClusterMaster:\n     def remove_node(self, nodename):\n         raise NotImplemented\n \n-    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):\n+    def schedule(self, domains, spider_settings=None, priority=DEFAULT_PRIORITY):\n         i = 0\n         for p in self.pending:\n             if p['priority'] <= priority:\n\n@@ -4,7 +4,7 @@ from pydispatch import dispatcher\n \n from scrapy.spider import spiders \n from scrapy.management.web import banner, webconsole_discover_module\n-from scrapy.contrib.pbcluster.master.manager import *\n+from scrapy.contrib.pbcluster.master.manager import ClusterMaster, DEFAULT_PRIORITY\n from scrapy.utils.serialization import serialize\n \n class ClusterMasterWeb(ClusterMaster):\n@@ -61,7 +61,7 @@ class ClusterMasterWeb(ClusterMaster):\n             else:\n                 sep = \"\\r\"\n                 domains = args[\"schedule\"]\n-            priority = int(args.get(\"priority\", [\"PRIORITY_NORMAL\"])[0])\n+            priority = int(args.get(\"priority\", [DEFAULT_PRIORITY])[0])\n             \n             #spider settings\n             slist = args.get(\"settings\", [\"\"])[0].split(sep)\n@@ -162,14 +162,9 @@ class ClusterMasterWeb(ClusterMaster):\n             s += \"<option>%s</option>\\n\" % domain\n         s += \"</select>\\n\"\n         s += \"<br />\\n\"\n+        \n         s += \"Priority:<br />\\n\"\n-        s += \"<select name='priority'>\\n\"\n-        for p, pname in priorities.items():\n-            if pname == \"NORMAL\":\n-                s += \"<option value='%s' selected>NORMAL</option>\" % p\n-            else:\n-                s += \"<option value='%s'>%s</option>\" % (p, pname)\n-        s += \"</select>\\n\"\n+        s += \"<input type='text' name='priority'>%s</input>\" % DEFAULT_PRIORITY\n         s += \"<br />\\n\"\n         \n         #spider settings\n\n@@ -47,7 +47,7 @@ def main():\n     elif opts.schedule and domains:\n         post[\"schedule\"] = domains\n         if opts.now:\n-            post[\"priority\"] = \"PRIORITY_NOW\"\n+            post[\"priority\"] = \"0\"\n             post[\"settings\"] = \"UNAVAILABLES_NOTIFY=2\"\n     elif opts.remove and domains:\n         post[\"remove\"] = domains\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a2981801c4ef644a809bd218400c9ad0c028e57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 732 | Contributors (this commit): 2 | Commits (past 90d): 28 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n-DEFAULT_PRIORITY = settings.getint(\"DEFAULT_PRIORITY\") or 20\n+DEFAULT_PRIORITY = settings.getint(\"DEFAULT_PRIORITY\", 20)\n \n def my_import(name):\n     mod = __import__(name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7df63477bff7ccae129fef87d4bf150b6222ba0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 17/10 | Churn Δ: 98 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.48214285714285715\n\nDIFF:\n@@ -0,0 +1,45 @@\n+import os\n+from unittest import TestCase, main\n+from scrapy.http import Response, ResponseBody\n+from scrapy.utils.decompressor import Decompressor\n+\n+class ScrapyDecompressTest(TestCase):\n+    uncompressed_body = ''\n+    test_responses = {}\n+    decompressor = Decompressor()\n+    \n+    def setUp(self):\n+        self.datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n+        formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n+        uncompressed_fd = open(os.path.join(self.datadir, 'feed-sample1.xml'), 'r')\n+        self.uncompressed_body = uncompressed_fd.read()\n+        uncompressed_fd.close()\n+        \n+        for format in formats:\n+            fd = open(os.path.join(self.datadir, 'feed-sample1.' + format), 'r')\n+            body = ResponseBody(fd.read())\n+            fd.close()\n+            self.test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n+    \n+    def test_tar(self):\n+        ret = self.decompressor.extract(self.test_responses['tar'])\n+        if ret:\n+            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+        \n+    def test_zip(self):\n+        ret = self.decompressor.extract(self.test_responses['zip'])\n+        if ret:\n+            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+    \n+    def test_gz(self):\n+        ret = self.decompressor.extract(self.test_responses['xml.gz'])\n+        if ret:\n+            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+\n+    def test_bz2(self):\n+        ret = self.decompressor.extract(self.test_responses['xml.bz2'])\n+        if ret:\n+            self.assertEqual(ret.body.to_string(), self.uncompressed_body)\n+        \n+if __name__ == '__main__':\n+    main()\n\n@@ -0,0 +1,53 @@\n+\"\"\"\n+Utility for autodetecting and decompressing responses\n+\"\"\"\n+\n+import zipfile\n+import tarfile\n+import gzip\n+import bz2\n+from tempfile import NamedTemporaryFile\n+from scrapy.http import ResponseBody\n+\n+class Decompressor(object):\n+    class ArchiveIsEmpty(Exception):\n+        pass\n+    \n+    def extract(self, response):\n+        temp = NamedTemporaryFile()\n+        temp.file.write(response.body.to_string())\n+        temp.file.seek(0)\n+        \n+        if tarfile.is_tarfile(temp.name):\n+            tar = tarfile.open(temp.name)\n+            if tar.members:\n+                return response.replace(body=ResponseBody(tar.extractfile(tar.members[0]).read()))\n+            else:\n+                raise self.ArchiveIsEmpty\n+           \n+        elif zipfile.is_zipfile(temp.name):\n+            zipf = zipfile.ZipFile(temp.name, 'r')\n+            namelist = zipf.namelist()\n+            if namelist:\n+                return response.replace(body=ResponseBody(zipf.read(namelist[0])))\n+            else:\n+                raise self.ArchiveIsEmpty\n+           \n+        else:\n+            # It's neither a tar or a zip, so we try to decompress using Gzip now\n+            try:\n+                gzip_file = gzip.GzipFile(temp.name)\n+                return response.replace(body=ResponseBody(gzip_file.read()))\n+            except IOError:\n+                pass\n+      \n+            # Finally, we try with Bzip2\n+            try:\n+                bzip_file = bz2.BZ2File(temp.name)\n+                return response.replace(body=ResponseBody(bzip_file.read()))\n+            except IOError:\n+                pass\n+        \n+            # We couldn't decompress the file, so we return the same response\n+            return response\n+    \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98ac66f8213fa59161ff3960ec7f1ed4a6f8377b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 66 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,13 @@ from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n \n class Command(ScrapyCommand):\n+\n+    \"\"\" Default template file name \"\"\"\n+    template_name = 'spider.tmpl'\n+\n+    \"\"\" Childs can define custom tvars \"\"\"\n+    custom_tvars = {}\n+\n     def syntax(self):\n         return \"<spider_name> <spider_domain_name>\"\n \n@@ -31,13 +38,14 @@ class Command(ScrapyCommand):\n             'site': site,\n             'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('-')])\n         }\n+        tvars.update(self.custom_tvars)\n \n         spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n         spidersdir = os.path.abspath(os.path.dirname(spiders_module.__file__))\n         if name[0] not in string.letters: # must start with a letter, for valid python modules\n             name = \"a\" + name\n         name = name.replace('-', '_') # - are replaced by _, for valid python modules\n-        self._genfiles('spider.tmpl', '%s/%s.py' % (spidersdir, name), tvars)\n+        self._genfiles(self.template_name, '%s/%s.py' % (spidersdir, name), tvars)\n \n     def _genfiles(self, template_name, source_name, tvars):\n         \"\"\" Generate source from template, substitute variables \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b363f9085d2bf7e11eb656b25b490972dd06bd48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 62 | Churn Cumulative: 115 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import zipfile\n import tarfile\n import gzip\n import bz2\n+from cStringIO import StringIO\n from tempfile import NamedTemporaryFile\n from scrapy.http import ResponseBody\n \n@@ -13,41 +14,54 @@ class Decompressor(object):\n     class ArchiveIsEmpty(Exception):\n         pass\n     \n-    def extract(self, response):\n-        temp = NamedTemporaryFile()\n-        temp.file.write(response.body.to_string())\n-        temp.file.seek(0)\n-        \n-        if tarfile.is_tarfile(temp.name):\n-            tar = tarfile.open(temp.name)\n-            if tar.members:\n-                return response.replace(body=ResponseBody(tar.extractfile(tar.members[0]).read()))\n+    def __init__(self):\n+        self.decompressors = [self.is_tar, self.is_zip,\n+                              self.is_gzip, self.is_bzip2]        \n+    def is_tar(self, response):\n+        try:\n+            tar_file = tarfile.open(fileobj=self.archive)\n+        except tarfile.ReadError:\n+            return False\n+        if tar_file.members:\n+            return response.replace(body=ResponseBody(tar_file.extractfile(tar_file.members[0]).read()))\n         else:\n             raise self.ArchiveIsEmpty\n     \n-        elif zipfile.is_zipfile(temp.name):\n-            zipf = zipfile.ZipFile(temp.name, 'r')\n-            namelist = zipf.namelist()\n+    def is_zip(self, response):\n+        try:\n+            zip_file = zipfile.ZipFile(self.archive)\n+        except zipfile.BadZipfile:\n+            return False\n+        namelist = zip_file.namelist()\n         if namelist:\n-                return response.replace(body=ResponseBody(zipf.read(namelist[0])))\n+            return response.replace(body=ResponseBody(zip_file.read(namelist[0])))\n         else:\n             raise self.ArchiveIsEmpty\n             \n-        else:\n-            # It's neither a tar or a zip, so we try to decompress using Gzip now\n+    def is_gzip(self, response):\n         try:\n-                gzip_file = gzip.GzipFile(temp.name)\n-                return response.replace(body=ResponseBody(gzip_file.read()))\n+            gzip_file = gzip.GzipFile(fileobj=self.archive)\n+            decompressed_body = gzip_file.read()\n         except IOError:\n-                pass\n+            return False\n+        return response.replace(body=decompressed_body)\n             \n-            # Finally, we try with Bzip2\n+    def is_bzip2(self, response):\n         try:\n-                bzip_file = bz2.BZ2File(temp.name)\n-                return response.replace(body=ResponseBody(bzip_file.read()))\n+            decompressed_body = bz2.decompress(self.body)\n         except IOError:\n-                pass\n+            return False\n+        return response.replace(body=ResponseBody(decompressed_body))\n             \n-            # We couldn't decompress the file, so we return the same response\n+    def extract(self, response):\n+        self.body = response.body.to_string()\n+        self.archive = StringIO()\n+        self.archive.write(self.body)\n+        self.archive.seek(0)\n+\n+        for decompressor in self.decompressors:\n+            self.archive.seek(0)\n+            ret = decompressor(response)\n+            if ret:\n+                return ret\n         return response\n-    \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#543b9de9739e7371d4c8ec6263ea7c4a51352887", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 117 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,6 @@ import tarfile\n import gzip\n import bz2\n from cStringIO import StringIO\n-from tempfile import NamedTemporaryFile\n from scrapy.http import ResponseBody\n \n class Decompressor(object):\n@@ -57,7 +56,6 @@ class Decompressor(object):\n         self.body = response.body.to_string()\n         self.archive = StringIO()\n         self.archive.write(self.body)\n-        self.archive.seek(0)\n \n         for decompressor in self.decompressors:\n             self.archive.seek(0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec710702ce1b646012ccf6d83fe80c322fc4eef1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 52 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from scrapy.core.manager import scrapymanager\n from scrapy.http import Request, Response\n from scrapy.core.downloader.handlers import download_any\n from scrapy.fetcher import get_or_create_spider\n+from scrapy.utils.decompressor import Decompressor\n \n #This code comes from twisted 8. We define here while\n #using old twisted version.\n@@ -57,7 +58,7 @@ class Command(ScrapyCommand):\n         \"\"\" You can use this function to update the local variables that will be available in the scrape console \"\"\"\n         pass\n \n-    def get_url(self, url):\n+    def get_url(self, url, decompress=False):\n         #def _get_callback(_response):\n             #print \"done\"\n             #if not _response:\n@@ -71,13 +72,17 @@ class Command(ScrapyCommand):\n             #print _failure\n \n         print \"Downloading URL...           \",\n-        url = url.strip()\n         r = Request(url)\n         spider = get_or_create_spider(url)\n         try:\n             result = blockingCallFromThread(reactor, download_any, r, spider)\n             if isinstance(result, Response):\n                 print \"Done.\"\n+                if decompress:\n+                    print \"Decompressing response...\",\n+                    d = Decompressor()\n+                    result = d.extract(result)\n+                    print \"Done.\"\n                 self.generate_vars(url, result)\n                 return True\n         except Exception, e:\n@@ -107,6 +112,7 @@ class Command(ScrapyCommand):\n                 print \"   %s: %s\" % (key, val.__class__)\n         print \"Available commands:\"\n         print \"   get <url>: Fetches an url and updates all variables.\"\n+        print \"   getd <url>: Similar to get, but filter with decompress.\"\n         print \"   scrapehelp: Prints this help.\"\n         print '-' * 78\n     \n@@ -125,10 +131,12 @@ class Command(ScrapyCommand):\n \n         def _console_thread():\n             \n-            def _get_magic(shell, url):\n-                self.get_url(url)\n+            def _get_magic(shell, arg):\n+                self.get_url(arg.strip())\n             def _help_magic(shell, _):\n                 self.print_vars()\n+            def _getd_magic(shell, arg):\n+                self.get_url(arg.strip(), decompress=True)\n                 \n             if url:\n                 result = self.get_url(url)\n@@ -141,6 +149,7 @@ class Command(ScrapyCommand):\n                 shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n                 ip = shell.IP.getapi()\n                 ip.expose_magic(\"get\", _get_magic)\n+                ip.expose_magic(\"getd\", _getd_magic)\n                 ip.expose_magic(\"scrapehelp\", _help_magic)\n                 shell.mainloop()\n                 reactor.callFromThread(scrapymanager.stop)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#789d8d3531b1f4915465ff3718468a1d8941a613", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 125 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -6,8 +6,11 @@ import zipfile\n import tarfile\n import gzip\n import bz2\n-from cStringIO import StringIO\n from scrapy.http import ResponseBody\n+try:\n+    from cStringIO import StringIO\n+except:\n+    from StringIO import StringIO\n \n class Decompressor(object):\n     class ArchiveIsEmpty(Exception):\n@@ -16,9 +19,10 @@ class Decompressor(object):\n     def __init__(self):\n         self.decompressors = [self.is_tar, self.is_zip,\n                               self.is_gzip, self.is_bzip2]        \n+    \n     def is_tar(self, response):\n         try:\n-            tar_file = tarfile.open(fileobj=self.archive)\n+            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)\n         except tarfile.ReadError:\n             return False\n         if tar_file.members:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da7b87962a8f08482a96e2e033e39326ce9412a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 6/7 | Churn Δ: 44 | Churn Cumulative: 378 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,26 +15,31 @@ class Adaptor(object):\n         self.match_function = match_function\n     def function(self, value):\n         return self.basefunction(value)\n+    def __repr__(self):\n+        return \"'%s' Adaptor\" %self.name\n         \n class AdaptorPipe:\n \n-    def __init__(self, adaptors=None, adaptorclass=None):\n+    def __init__(self, attribute_names, adaptors=None, adaptorclass=None):\n         \"\"\"\n         If \"adaptors\" is given, constructs pipeline from this.\n         \"adaptors\" is an ordered tuple of 3-elements tuples, each of which\n         has the same parameters you give to the insertadaptor method, except \n         'after' and 'before', because you define the adaptors order in the tuple.\n-        . Example:\n+        Example:\n         (\n           (my_function, \"my_function\", lambda x: x in my_list)\n           ...\n         )\n         \"\"\"\n+        self.__attribute_names = [ n for n in attribute_names ]\n         self.__adaptorspipe = []\n         self.__adaptorclass = adaptorclass or Adaptor\n+        self.pipes = {}\n         if adaptors:\n             for entry in adaptors:\n-                self.insertadaptor(*entry)\n+                self.insertadaptor(compile_pipe=False, *entry)\n+            self._compile_pipe()\n \n     @property\n     def adaptors_names(self):\n@@ -43,7 +48,7 @@ class AdaptorPipe:\n             _adaptors.append(a.name)\n         return _adaptors\n     \n-    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None):\n+    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None, compile_pipe=True):\n         \"\"\"\n         Inserts a \"function\" as an adaptor that will apply when match_function returns True (by\n         default always apply)\n@@ -62,15 +67,28 @@ class AdaptorPipe:\n             elif before:\n                 pos = self.adaptors_names.index(before)\n             self.__adaptorspipe.insert(pos, adaptor)\n+            if compile_pipe:\n+                self._compile_pipe()\n             return pos\n \n-    def execute(self, match_condition, value, debug=False):\n+    def removeadaptor(self, adaptorname):\n+        pos = self.adaptors_names.index(adaptorname)\n+        self.__adaptorspipe.pop(pos)\n+        self._compile_pipe()\n+\n+    def _compile_pipe(self):\n+        for attrname in self.__attribute_names:\n+            adaptors_pipe = []\n+            for adaptor in self.__adaptorspipe:\n+                if adaptor.match_function(attrname):\n+                    adaptors_pipe.append(adaptor)\n+            self.pipes[attrname] = adaptors_pipe\n+            \n+    def execute(self, attrname, value, debug=False):\n         \"\"\"\n         Execute pipeline for attribute name \"attrname\" and value \"value\".\n-        Pass the given pipeargs to each adaptor function in the pipe.\n         \"\"\"\n-        for adaptor in self.__adaptorspipe:\n-            if adaptor.match_function(match_condition):\n+        for adaptor in self.pipes.get(attrname, []):\n             try:\n                 if debug:\n                     print \"  %07s | input >\" % adaptor.name, repr(value)\n\n@@ -3,14 +3,10 @@ from scrapy.item.adaptors import AdaptorPipe\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n-\n-    The only required attributes are:\n-    * guid (unique global indentifier)\n-    * url (URL where that item was scraped from)\n     \"\"\"\n-    adaptors_pipe = AdaptorPipe()\n+    adaptors_pipe = AdaptorPipe([])\n \n-    def attribute(self, attrname, value, match_condition=None):\n-        value = self.adaptors_pipe.execute(match_condition or attrname, value)\n+    def attribute(self, attrname, value):\n+        value = self.adaptors_pipe.execute(attrname, value)\n         if not hasattr(self, attrname):\n             setattr(self, attrname, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8b7f41ee50539e174f09e8275bce2e9bf565ce1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 40 | Churn Cumulative: 481 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,5 +191,35 @@ class XPathTestCase(unittest.TestCase):\n         self.assertEqual(xxs.extract(),\n                          u'<root>lala</root>')\n \n+    def test_unquote(self):\n+        xmldoc='\\n'.join((\n+            '<root>',\n+            '  lala',\n+            '  <node>',\n+            '    blabla&amp;more<!--comment-->a<b>test</b>oh',\n+            '    <![CDATA[lalalal&ppppp<b>PPPP</b>ppp&amp;la]]>',\n+            '  </node>',\n+            '  pff',\n+            '</root>'))\n+        xxs = XmlXPathSelector(text=xmldoc)\n+\n+        self.assertEqual(xxs.extract_unquoted(), u'')\n+\n+        self.assertEqual(xxs.x('/root').extract_unquoted(), [u''])\n+        self.assertEqual(xxs.x('/root/text()').extract_unquoted(), [\n+            u'\\n  lala\\n  ',\n+            u'\\n  pff\\n'])\n+\n+        self.assertEqual(xxs.x('//*').extract_unquoted(), [u'', u'', u''])\n+        self.assertEqual(xxs.x('//text()').extract_unquoted(), [\n+            u'\\n  lala\\n  ',\n+            u'\\n    blabla&more',\n+            u'a',\n+            u'test',\n+            u'oh\\n    ',\n+            u'lalalal&ppppp<b>PPPP</b>ppp&amp;la',\n+            u'\\n  ',\n+            u'\\n  pff\\n'])\n+\n     if __name__ == \"__main__\":\n         unittest.main()   \n\n@@ -73,6 +73,13 @@ class XPathSelector(object):\n                 text = unicode(self.xmlNode)\n         return text\n \n+    def extract_unquoted(self):\n+        \"\"\"Get unescaped contents from the text node (no entities, no CDATA)\"\"\"\n+        if self.x('self::text()'):\n+            return unicode(self.xmlNode.getContent(), errors='ignore')\n+        else:\n+            return u''\n+\n     def register_namespace(self, prefix, uri):\n         \"\"\"Register namespace so that it can be used in XPath queries\"\"\"\n         self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n@@ -104,6 +111,9 @@ class XPathSelectorList(list):\n         XPathSelector of the list\"\"\"\n         return [x.extract() if isinstance(x, XPathSelector) else x for x in self]\n \n+    def extract_unquoted(self):\n+        return [x.extract_unquoted() if isinstance(x, XPathSelector) else x for x in self]\n+\n \n class XmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for XML content\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06964cf36b6da18fc606a02bbbe5d3e57b6f3252", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 297 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,13 @@\n-import os\n import re\n import unittest\n \n import libxml2\n \n from scrapy.http import Response\n-from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n \n class XPathTestCase(unittest.TestCase):\n \n@@ -221,5 +223,3 @@ class XPathTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n-    if __name__ == \"__main__\":\n-        unittest.main()   \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b6bcd61dffae28986ab778e55ed48354893a7ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 303 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,9 +6,6 @@ import libxml2\n from scrapy.http import Response\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n \n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n class XPathTestCase(unittest.TestCase):\n \n     def setUp(self):\n@@ -223,3 +220,6 @@ class XPathTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23814537b32b009c6464f6dff145d9102f6735af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 199 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--target\", dest=\"targets\", action=\"append\", help=\"crawl TARGET instead of recorded urls/domains. can be passed multiple times\")\n         # adding option to update \n         parser.add_option(\"--pages\", dest=\"pages\", action=\"store_true\", help=\"update all the pages in the replay file, recording it again.\")\n+        parser.add_option(\"-q\", \"--quiet\", dest=\"quiet\", action=\"store_true\", help=\"no verbose mode in option diff.\")\n \n     def process_options(self, args, opts):\n         if args:\n@@ -81,7 +82,7 @@ class Command(ScrapyCommand):\n     def action_update(self, opts):\n         self.replay.update(args=opts.targets, opts=opts.__dict__)\n         if (opts.pages):\n-            args = ['decobot-ctl.py', 'crawl']\n+            args = ['scrapy-crawl', 'crawl']\n             args.extend(self.replay.options['args'])\n             for k in self.replay.options['opts']:                \n                 if self.replay.options['opts'][k]:                   \n@@ -119,9 +120,12 @@ class Command(ScrapyCommand):\n \n         chcount, chreport = self._report_differences(self.before_db, self.now_db, guids_both)\n \n+        if chcount == 0 and opts.quiet:\n+            s = \"\"\n+        else:\n             s = \"CRAWLING DIFFERENCES REPORT\\n\\n\"\n \n-        s += \"Total items     : %d\\n\" % (len(guids_both) - chcount)\n+            s += \"Total items     : %d\\n\" % (len(guids_both) + len(guids_new) + len(guids_missing))\n             s += \"  Items OK      : %d\\n\" % (len(guids_both) - chcount)\n             s += \"  New items     : %d\\n\" % len(guids_new)\n             s += \"  Missing items : %d\\n\" % len(guids_missing)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cffc039ff3218afad21e181ca30548704d87efa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 84 | Files Changed: 2 | Hunks: 10 | Methods Changed: 18 | Complexity Δ (Sum/Max): 2/20 | Churn Δ: 177 | Churn Cumulative: 441 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,85 @@\n+from scrapy.item.adaptors import AdaptorPipe as BaseAdaptorPipe\n+from scrapy.utils.serialization import serialize\n+\n+class DuplicatedAdaptorName(Exception): pass\n+\n+class _Adaptor(object):\n+    \"\"\"\n+    Adaptors instances should be instantiated and used only\n+    inside the AdaptorPipe.\n+    \"\"\"\n+    def __init__(self, function, match_function):\n+        self.basefunction = function\n+        self.match_function = match_function\n+    def __repr__(self):\n+        return self.basefunction.func_name\n+    def __call__(self, *args):\n+        return self.basefunction(*args)\n+        \n+class AdaptorPipe(BaseAdaptorPipe):\n+\n+    def __init__(self, attribute_names, adaptors=None):\n+        \"\"\"\n+        If \"adaptors\" is given, constructs pipeline from this.\n+        \"adaptors\" is an ordered tuple of 2-elements tuples, each of which\n+        has the same parameters you give to the insertadaptor method, except \n+        'after' and 'before', because you define the adaptors order in the tuple.\n+        Example:\n+        (\n+          (my_function, lambda x: x in my_list)\n+          ...\n+        )\n+        \"\"\"\n+        self.__attribute_names = [ n for n in attribute_names ]\n+        self.__adaptorspipe = []\n+        self.pipes = {}\n+        if adaptors:\n+            for entry in adaptors:\n+                self.insertadaptor(compile_pipe=False, *entry)\n+            self._compile_pipe()\n+\n+    @property\n+    def adaptors_names(self):\n+        _adaptors = []\n+        for a in self.__adaptorspipe:\n+            _adaptors.append(a.basefunction.func_name)\n+        return _adaptors\n+    \n+    def insertadaptor(self, function, match_function=lambda x: True, compile_pipe=True, after=None, before=None):\n+        \"\"\"\n+        Inserts a \"function\" as an adaptor that will apply when match_function returns True (by\n+        default always apply)\n+        If \"after\" is given, inserts the adaptor after the already inserted adaptor\n+        of the name given in this parameter, If \"before\" is given, inserts it before\n+        the adaptor of the given name. \"name\" is the name of the adaptor.\n+        \"\"\"\n+        if function.func_name in self.adaptors_names:\n+            raise DuplicatedAdaptorName(function.func_name)\n+        else:\n+            adaptor = _Adaptor(function, match_function)\n+            #by default append adaptor at end of pipe\n+            pos = len(self.adaptors_names)\n+            if after:\n+                pos = self.adaptors_names.index(after) + 1\n+            elif before:\n+                pos = self.adaptors_names.index(before)\n+            self.__adaptorspipe.insert(pos, adaptor)\n+            if compile_pipe:\n+                self._compile_pipe()\n+            return pos\n+\n+    def removeadaptor(self, adaptorname):\n+        pos = self.adaptors_names.index(adaptorname)\n+        self.__adaptorspipe.pop(pos)\n+        self._compile_pipe()\n+\n+    def _compile_pipe(self):\n+        for attrname in self.__attribute_names:\n+            adaptors_pipe = []\n+            for adaptor in self.__adaptorspipe:\n+                if adaptor.match_function(attrname):\n+                    adaptors_pipe.append(adaptor)\n+            self.pipes[attrname] = adaptors_pipe\n+\n+    def __repr__(self):\n+        return serialize(self.pipes, \"pprint\")\n\\ No newline at end of file\n\n@@ -1,103 +1,27 @@\n-import re\n from traceback import format_exc\n-from scrapy.core import log\n-\n-class DuplicatedAdaptorName(Exception): pass\n-\n-class Adaptor(object):\n-    \"\"\"\n-    Adaptors instances should be instantiated and used only\n-    inside the AdaptorPipe.\n-    \"\"\"\n-    def __init__(self, function, name, match_function):\n-        self.name = name\n-        self.basefunction = function\n-        self.match_function = match_function\n-    def function(self, value):\n-        return self.basefunction(value)\n-    def __repr__(self):\n-        return \"'%s' Adaptor\" %self.name\n \n class AdaptorPipe:\n \n-    def __init__(self, attribute_names, adaptors=None, adaptorclass=None):\n+    def __init__(self, adaptors_dict=None):\n         \"\"\"\n-        If \"adaptors\" is given, constructs pipeline from this.\n-        \"adaptors\" is an ordered tuple of 3-elements tuples, each of which\n-        has the same parameters you give to the insertadaptor method, except \n-        'after' and 'before', because you define the adaptors order in the tuple.\n-        Example:\n-        (\n-          (my_function, \"my_function\", lambda x: x in my_list)\n-          ...\n-        )\n+        Receives a dictionary that maps attribute_name to a list of adaptor functions\n         \"\"\"\n-        self.__attribute_names = [ n for n in attribute_names ]\n-        self.__adaptorspipe = []\n-        self.__adaptorclass = adaptorclass or Adaptor\n-        self.pipes = {}\n-        if adaptors:\n-            for entry in adaptors:\n-                self.insertadaptor(compile_pipe=False, *entry)\n-            self._compile_pipe()\n-\n-    @property\n-    def adaptors_names(self):\n-        _adaptors = []\n-        for a in self.__adaptorspipe:\n-            _adaptors.append(a.name)\n-        return _adaptors\n-    \n-    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None, compile_pipe=True):\n-        \"\"\"\n-        Inserts a \"function\" as an adaptor that will apply when match_function returns True (by\n-        default always apply)\n-        If \"after\" is given, inserts the adaptor after the already inserted adaptor\n-        of the name given in this parameter, If \"before\" is given, inserts it before\n-        the adaptor of the given name. \"name\" is the name of the adaptor.\n-        \"\"\"\n-        if name in self.adaptors_names:\n-            raise DuplicatedAdaptorName(name)\n-        else:\n-            adaptor = self.__adaptorclass(function, name, match_function)\n-            #by default append adaptor at end of pipe\n-            pos = len(self.adaptors_names)\n-            if after:\n-                pos = self.adaptors_names.index(after) + 1\n-            elif before:\n-                pos = self.adaptors_names.index(before)\n-            self.__adaptorspipe.insert(pos, adaptor)\n-            if compile_pipe:\n-                self._compile_pipe()\n-            return pos\n-\n-    def removeadaptor(self, adaptorname):\n-        pos = self.adaptors_names.index(adaptorname)\n-        self.__adaptorspipe.pop(pos)\n-        self._compile_pipe()\n-\n-    def _compile_pipe(self):\n-        for attrname in self.__attribute_names:\n-            adaptors_pipe = []\n-            for adaptor in self.__adaptorspipe:\n-                if adaptor.match_function(attrname):\n-                    adaptors_pipe.append(adaptor)\n-            self.pipes[attrname] = adaptors_pipe\n+        self.pipes = adaptors_dict or {}\n \n     def execute(self, attrname, value, debug=False):\n         \"\"\"\n         Execute pipeline for attribute name \"attrname\" and value \"value\".\n         \"\"\"\n-        for adaptor in self.pipes.get(attrname, []):\n+        for function in self.pipes.get(attrname, []):\n             try:\n                 if debug:\n-                    print \"  %07s | input >\" % adaptor.name, repr(value)\n-                value = adaptor.function(value)\n+                    print \"  %07s | input >\" % function.func_name, repr(value)\n+                value = function(value)\n                 if debug:\n-                    print \"  %07s | output>\" % adaptor.name, repr(value)\n+                    print \"  %07s | output>\" % function.func_name, repr(value)\n \n             except Exception, e:\n-                print \"Error in '%s' adaptor. Traceback text:\" % adaptor.name\n+                print \"Error in '%s' adaptor. Traceback text:\" % function.func_name\n                 print format_exc()\n                 return\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
