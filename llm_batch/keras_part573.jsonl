{"custom_id": "keras#eb934b8c179ed5fea635715ad962d1b4446ae96a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 2759 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ class Relu(Operation):\n         return backend.nn.relu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def relu(x):\n@@ -60,7 +60,7 @@ class Relu6(Operation):\n         return backend.nn.relu6(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def relu6(x):\n@@ -74,7 +74,7 @@ class Sigmoid(Operation):\n         return backend.nn.sigmoid(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def sigmoid(x):\n@@ -88,7 +88,7 @@ class Tanh(Operation):\n         return backend.nn.tanh(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def tanh(x):\n@@ -102,7 +102,7 @@ class Softplus(Operation):\n         return backend.nn.softplus(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def softplus(x):\n@@ -116,7 +116,7 @@ class Softsign(Operation):\n         return backend.nn.softsign(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def softsign(x):\n@@ -130,7 +130,7 @@ class Silu(Operation):\n         return backend.nn.silu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def silu(x):\n@@ -144,7 +144,7 @@ class Swish(Operation):\n         return backend.nn.swish(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def swish(x):\n@@ -158,7 +158,7 @@ class LogSigmoid(Operation):\n         return backend.nn.log_sigmoid(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def log_sigmoid(x):\n@@ -176,7 +176,7 @@ class LeakyRelu(Operation):\n         return backend.nn.leaky_relu(x, self.negative_slope)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def leaky_relu(x, negative_slope=0.2):\n@@ -190,7 +190,7 @@ class HardSigmoid(Operation):\n         return backend.nn.hard_sigmoid(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def hard_sigmoid(x):\n@@ -204,7 +204,7 @@ class Elu(Operation):\n         return backend.nn.elu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def elu(x):\n@@ -218,7 +218,7 @@ class Selu(Operation):\n         return backend.nn.selu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def selu(x):\n@@ -236,7 +236,7 @@ class Gelu(Operation):\n         return backend.nn.gelu(x, self.approximate)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def gelu(x, approximate=True):\n@@ -254,7 +254,7 @@ class Softmax(Operation):\n         return backend.nn.softmax(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def softmax(x, axis=None):\n@@ -272,7 +272,7 @@ class LogSoftmax(Operation):\n         return backend.nn.log_softmax(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, x.dtype)\n+        return KerasTensor(x.shape, dtype=x.dtype)\n \n \n def log_softmax(x, axis=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80d0009eb5765dedfe0435032402136a065e5340", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 78 | Churn Cumulative: 207 | Contributors (this commit): 4 | Commits (past 90d): 25 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras_core.layers.activations.activation import Activation\n+from keras_core.layers.core.activation import Activation\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.embedding import Embedding\n from keras_core.layers.core.input_layer import Input\n\n@@ -0,0 +1,44 @@\n+from keras_core import activations\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Activation\")\n+class Activation(Layer):\n+    \"\"\"Applies an activation function to an input.\n+\n+    Args:\n+        activation: Activation function, such as\n+            `keras_core.operations.relu`, or string name of\n+            built-in activation function, such as `\"relu\"`.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+\n+    Examples:\n+\n+    >>> layer = keras_core.layers.Activation('relu')\n+    >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+    >>> output\n+    [0.0, 0.0, 0.0, 2.0]\n+    >>> layer = keras_core.layers.Activation(tf.nn.relu)\n+    >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n+    >>> output\n+    [0.0, 0.0, 0.0, 2.0]\n+    \"\"\"\n+\n+    def __init__(self, activation, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.activation = activations.get(activation)\n+\n+    def call(self, inputs):\n+        return self.activation(inputs)\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\"activation\": activations.serialize(self.activation)}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,33 @@\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class BatchNormalizationTest(testing.TestCase):\n+    def test_activation_basics(self):\n+        self.run_layer_test(\n+            layers.Activation,\n+            init_kwargs={\n+                \"activation\": ops.relu,\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+        self.run_layer_test(\n+            layers.Activation,\n+            init_kwargs={\n+                \"activation\": \"softmax\",\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b7d35c359c1b1beb62b618d7ce7e1582bb99cd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 78 | Files Changed: 4 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/4 | Churn Δ: 80 | Churn Cumulative: 287 | Contributors (this commit): 4 | Commits (past 90d): 30 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n from keras_core.layers.activations.activation import Activation\n-from keras_core.layers.core.activation import Activation\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.embedding import Embedding\n from keras_core.layers.core.input_layer import Input\n\n@@ -11,6 +11,8 @@ class Activation(Layer):\n         activation: Activation function. It could be\n             a callable, or the name of an activation\n             from the `keras_core.activations` namespace.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n \n     Example:\n \n\n@@ -1,44 +0,0 @@\n-from keras_core import activations\n-from keras_core import operations as ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-\n-\n-@keras_core_export(\"keras_core.layers.Activation\")\n-class Activation(Layer):\n-    \"\"\"Applies an activation function to an input.\n-\n-    Args:\n-        activation: Activation function, such as\n-            `keras_core.operations.relu`, or string name of\n-            built-in activation function, such as `\"relu\"`.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n-\n-    Examples:\n-\n-    >>> layer = keras_core.layers.Activation('relu')\n-    >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-    >>> output\n-    [0.0, 0.0, 0.0, 2.0]\n-    >>> layer = keras_core.layers.Activation(tf.nn.relu)\n-    >>> output = layer([-3.0, -1.0, 0.0, 2.0])\n-    >>> output\n-    [0.0, 0.0, 0.0, 2.0]\n-    \"\"\"\n-\n-    def __init__(self, activation, **kwargs):\n-        super().__init__(**kwargs)\n-        self.supports_masking = True\n-        self.activation = activations.get(activation)\n-\n-    def call(self, inputs):\n-        return self.activation(inputs)\n-\n-    def compute_output_shape(self, input_shape):\n-        return input_shape\n-\n-    def get_config(self):\n-        config = {\"activation\": activations.serialize(self.activation)}\n-        base_config = super().get_config()\n-        return {**base_config, **config}\n\n@@ -1,33 +0,0 @@\n-from keras_core import layers\n-from keras_core import operations as ops\n-from keras_core import testing\n-\n-\n-class BatchNormalizationTest(testing.TestCase):\n-    def test_activation_basics(self):\n-        self.run_layer_test(\n-            layers.Activation,\n-            init_kwargs={\n-                \"activation\": ops.relu,\n-            },\n-            input_shape=(2, 3),\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n-        self.run_layer_test(\n-            layers.Activation,\n-            init_kwargs={\n-                \"activation\": \"softmax\",\n-            },\n-            input_shape=(2, 3),\n-            expected_output_shape=(2, 3),\n-            expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=0,\n-            expected_num_seed_generators=0,\n-            expected_num_losses=0,\n-            supports_masking=True,\n-        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6a93bc013c875de7003217a795c2a2679c00c57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 37 | Files Changed: 27 | Hunks: 26 | Methods Changed: 5 | Complexity Δ (Sum/Max): 22/13 | Churn Δ: 138 | Churn Cumulative: 12273 | Contributors (this commit): 10 | Commits (past 90d): 256 | Contributors (cumulative): 71 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,12 +13,12 @@ from keras_core.backend.config import image_data_format\n from keras_core.backend.config import set_epsilon\n from keras_core.backend.config import set_floatx\n from keras_core.backend.config import set_image_data_format\n-from keras_core.backend.keras_tensor import KerasTensor\n-from keras_core.backend.keras_tensor import any_symbolic_tensors\n-from keras_core.backend.keras_tensor import is_keras_tensor\n-from keras_core.backend.stateless_scope import StatelessScope\n-from keras_core.backend.stateless_scope import get_stateless_scope\n-from keras_core.backend.stateless_scope import in_stateless_scope\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n+from keras_core.backend.common.keras_tensor import is_keras_tensor\n+from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import get_stateless_scope\n+from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.utils.io_utils import print_msg\n \n # Import backend functions.\n\n\n@@ -1,4 +1,4 @@\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n from keras_core.testing import test_case\n from keras_core.utils.naming import auto_name\n \n\n\n@@ -4,7 +4,7 @@ from jax import numpy as jnp\n \n from keras_core import operations as ops\n from keras_core import testing\n-from keras_core.backend import keras_tensor\n+from keras_core.backend.common import keras_tensor\n \n \n class KerasTensorTest(testing.TestCase):\n\n@@ -1,4 +1,4 @@\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n from keras_core.backend.common.variables import KerasVariable\n from keras_core.backend.common.variables import initialize_all_variables\n \n\n@@ -3,7 +3,7 @@ import numpy as np\n from keras_core import backend\n from keras_core import operations as ops\n from keras_core import testing\n-from keras_core.backend.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import StatelessScope\n \n \n class TestStatelessScope(testing.TestCase):\n\n@@ -1,4 +1,4 @@\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n from keras_core.backend.config import floatx\n from keras_core.utils.naming import auto_name\n \n@@ -20,7 +20,7 @@ class KerasVariable:\n                     f\"Received: initializer={initializer} \"\n                     f\"and shape={shape}\"\n                 )\n-        from keras_core.backend.stateless_scope import in_stateless_scope\n+        from keras_core.backend.common.stateless_scope import in_stateless_scope\n \n         if in_stateless_scope():\n             if callable(initializer):\n@@ -51,7 +51,7 @@ class KerasVariable:\n     def _deferred_initialize(self):\n         if self._value is not None:\n             raise ValueError(f\"Variable {self.name} is already initialized.\")\n-        from keras_core.backend.stateless_scope import in_stateless_scope\n+        from keras_core.backend.common.stateless_scope import in_stateless_scope\n \n         if in_stateless_scope():\n             raise ValueError(\n\n@@ -11,10 +11,10 @@ from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n-from keras_core.backend.keras_tensor import KerasTensor\n-from keras_core.backend.stateless_scope import StatelessScope\n-from keras_core.backend.stateless_scope import get_stateless_scope\n-from keras_core.backend.stateless_scope import in_stateless_scope\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import get_stateless_scope\n+from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = False  # Dynamic shapes NG\n\n\n@@ -4,10 +4,10 @@ from tensorflow.experimental import numpy as tfnp\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.keras_tensor import KerasTensor\n-from keras_core.backend.stateless_scope import StatelessScope\n-from keras_core.backend.stateless_scope import get_stateless_scope\n-from keras_core.backend.stateless_scope import in_stateless_scope\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import get_stateless_scope\n+from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.backend.tensorflow import math\n from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n\n@@ -0,0 +1,64 @@\n+\"\"\"\n+Upload to GCS:\n+\n+```\n+gcloud config set project tensorflow\n+gcloud storage cp /Users/fchollet/Downloads/CaliforniaHousing/california_housing.npz gs://tensorflow/tf-keras-datasets/california_housing.npz\n+```\n+\"\"\"\n+import numpy as np\n+\n+fpath = \"/Users/fchollet/Downloads/CaliforniaHousing/cal_housing.data\"\n+target_fpath = (\n+    \"/Users/fchollet/Downloads/CaliforniaHousing/california_housing.npz\"\n+)\n+header = [\n+    \"MedInc\",\n+    \"HouseAge\",\n+    \"AveRooms\",\n+    \"AveBedrms\",\n+    \"Population\",\n+    \"AveOccup\",\n+    \"Latitude\",\n+    \"Longitude\",\n+    \"MedHouseVal\",\n+]\n+\n+\n+def process_dataset():\n+    f = open(fpath)\n+    contents = f.read()\n+    f.close()\n+    lines = contents.split(\"\\n\")\n+    samples = []\n+    targets = []\n+    for line in lines[:-1]:\n+        values = line.split(\",\")\n+        values = [float(v) for v in values]\n+        values, target = values[:-1], values[-1]\n+        samples.append(values)\n+        targets.append(target)\n+    samples = np.array(samples, dtype=\"float32\")\n+    targets = np.array(targets, dtype=\"float32\")\n+    print(samples.shape)\n+    print(targets.shape)\n+    np.savez(target_fpath, x=samples, y=targets)\n+\n+\n+def test_dataset():\n+    with np.load(target_fpath, allow_pickle=True) as f:\n+        x = f[\"x\"]\n+        y = f[\"y\"]\n+    print(x.shape)\n+    print(y.shape)\n+\n+    print(\"Lowest y\", sorted(y)[:10])\n+    print(\"Highest y\", sorted(y, reverse=True)[:10])\n+\n+    from keras_core.utils.file_utils import hash_file\n+\n+    print(hash_file(target_fpath))\n+\n+\n+# process_dataset()\n+test_dataset()\n\n@@ -2,7 +2,7 @@ import numpy as np\n \n from keras_core import layers\n from keras_core import testing\n-from keras_core.backend import keras_tensor\n+from keras_core.backend.common import keras_tensor\n \n \n class DenseTest(testing.TestCase):\n\n@@ -29,7 +29,7 @@ from keras_core import regularizers\n from keras_core import utils\n from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n from keras_core.layers import input_spec\n from keras_core.metrics.metric import Metric\n from keras_core.operations.operation import Operation\n\n@@ -1,6 +1,6 @@\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n \n \n @keras_core_export(\"keras_core.mixed_precision.DTypePolicy\")\n\n@@ -3,7 +3,7 @@ import pytest\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.backend import keras_tensor\n+from keras_core.backend.common import keras_tensor\n from keras_core.operations import function\n from keras_core.operations import numpy as knp\n \n\n@@ -3,7 +3,7 @@ import pytest\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.backend.keras_tensor import KerasTensor\n+from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.operations import math as kmath\n \n \n\n@@ -4,7 +4,7 @@ import tensorflow as tf\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.backend.keras_tensor import KerasTensor\n+from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.operations import nn as knn\n \n \n\n@@ -4,7 +4,7 @@ from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.backend.keras_tensor import KerasTensor\n+from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.operations import numpy as knp\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n\n@@ -4,7 +4,7 @@ import textwrap\n from tensorflow import nest\n \n from keras_core import backend\n-from keras_core.backend.keras_tensor import any_symbolic_tensors\n+from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n from keras_core.operations.node import Node\n from keras_core.saving import serialization_lib\n from keras_core.utils import python_utils\n\n@@ -2,7 +2,7 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import testing\n-from keras_core.backend import keras_tensor\n+from keras_core.backend.common import keras_tensor\n from keras_core.operations import numpy as knp\n from keras_core.operations import operation\n \n\n@@ -1,7 +1,7 @@\n import inspect\n \n from keras_core.api_export import keras_core_export\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n \n GLOBAL_CUSTOM_OBJECTS = {}\n GLOBAL_CUSTOM_NAMES = {}\n\n@@ -12,7 +12,7 @@ import tensorflow as tf\n from keras_core import api_export\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n from keras_core.saving import object_registration\n from keras_core.utils import python_utils\n \n\n@@ -220,7 +220,7 @@ class TestCase(unittest.TestCase):\n \n \n def create_keras_tensors(input_shape, dtype):\n-    from keras_core.backend import keras_tensor\n+    from keras_core.backend.common import keras_tensor\n \n     if isinstance(input_shape, tuple):\n         return keras_tensor.KerasTensor(input_shape, dtype=dtype)\n\n@@ -3,7 +3,7 @@ import sys\n from absl import logging\n \n from keras_core.api_export import keras_core_export\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n \n \n @keras_core_export(\"keras_core.utils.enable_interactive_logging\")\n\n@@ -2,7 +2,7 @@ import collections\n import re\n \n from keras_core.api_export import keras_core_export\n-from keras_core.backend import global_state\n+from keras_core.backend.common import global_state\n \n \n def auto_name(prefix):\n\n@@ -1,5 +1,5 @@\n-from keras_core.backend.global_state import get_global_attribute\n-from keras_core.backend.global_state import set_global_attribute\n+from keras_core.backend.common.global_state import get_global_attribute\n+from keras_core.backend.common.global_state import set_global_attribute\n \n \n class DotNotTrackScope:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc1301f9d5abc6695447c4a59ca711111a62147c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 1021 | Contributors (this commit): 5 | Commits (past 90d): 38 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,12 @@\n import json\n import os\n \n+from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n+from keras_core.backend.common.keras_tensor import is_keras_tensor\n+from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras_core.backend.common.stateless_scope import get_stateless_scope\n+from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.backend.common.variables import AutocastScope\n from keras_core.backend.common.variables import get_autocast_scope\n from keras_core.backend.common.variables import is_float_dtype\n@@ -13,12 +19,6 @@ from keras_core.backend.config import image_data_format\n from keras_core.backend.config import set_epsilon\n from keras_core.backend.config import set_floatx\n from keras_core.backend.config import set_image_data_format\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n-from keras_core.backend.common.keras_tensor import is_keras_tensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.common.stateless_scope import get_stateless_scope\n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.utils.io_utils import print_msg\n \n # Import backend functions.\n\n@@ -7,14 +7,14 @@ from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import get_autocast_scope\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common import standardize_shape\n-from keras_core.backend.jax import math\n-from keras_core.backend.jax import nn\n-from keras_core.backend.jax import numpy\n-from keras_core.backend.jax import random\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.common.stateless_scope import get_stateless_scope\n from keras_core.backend.common.stateless_scope import in_stateless_scope\n+from keras_core.backend.jax import math\n+from keras_core.backend.jax import nn\n+from keras_core.backend.jax import numpy\n+from keras_core.backend.jax import random\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = False  # Dynamic shapes NG\n\n@@ -7,6 +7,7 @@ from absl.testing import parameterized\n \n from keras_core import testing\n from keras_core.trainers.data_adapters import py_dataset_adapter\n+from keras_core.utils.rng_utils import set_random_seed\n \n \n class ExamplePyDataset(py_dataset_adapter.PyDataset):\n@@ -52,6 +53,7 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n     def test_basic_flow(\n         self, shuffle, workers, use_multiprocessing, max_queue_size\n     ):\n+        set_random_seed(1337)\n         x = np.random.random((64, 4))\n         y = np.array([[i, i] for i in range(64)], dtype=\"float64\")\n         py_dataset = ExamplePyDataset(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10ef809a452e0e0f7d828588ca7e0f5bbb531abb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 889 | Contributors (this commit): 7 | Commits (past 90d): 25 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -61,11 +61,8 @@ class Optimizer:\n             )\n \n         self.built = False\n-        # NOTE: the below will not work in a stateless scope.\n-        # optimizers should be created outside of any stateless scope,\n-        # at this time.\n         self.iterations = backend.Variable(\n-            0, name=\"iteration\", dtype=\"int32\", trainable=False\n+            0, name=\"iteration\", dtype=\"int64\", trainable=False\n         )\n         if isinstance(\n             learning_rate, learning_rate_schedule.LearningRateSchedule\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f14fb4097fd221287c33321acc752456090a9aa3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 235 | Lines Deleted: 3 | Files Changed: 11 | Hunks: 15 | Methods Changed: 22 | Complexity Δ (Sum/Max): 26/6 | Churn Δ: 238 | Churn Cumulative: 11150 | Contributors (this commit): 9 | Commits (past 90d): 143 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,6 +49,10 @@ def all(x, axis=None, keepdims=False):\n     return jnp.all(x, axis=axis, keepdims=keepdims)\n \n \n+def any(x, axis=None, keepdims=False):\n+    return jnp.any(x, axis=axis, keepdims=keepdims)\n+\n+\n def amax(x, axis=None, keepdims=False):\n     return jnp.amax(x, axis=axis, keepdims=keepdims)\n \n\n@@ -50,6 +50,10 @@ def all(x, axis=None, keepdims=False):\n     return tfnp.all(x, axis=axis, keepdims=keepdims)\n \n \n+def any(x, axis=None, keepdims=False):\n+    return tfnp.any(x, axis=axis, keepdims=keepdims)\n+\n+\n def amax(x, axis=None, keepdims=False):\n     return tfnp.amax(x, axis=axis, keepdims=keepdims)\n \n\n@@ -1,8 +1,10 @@\n from keras_core.layers.activations.activation import Activation\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.embedding import Embedding\n+from keras_core.layers.core.identity import Identity\n from keras_core.layers.core.input_layer import Input\n from keras_core.layers.core.input_layer import InputLayer\n+from keras_core.layers.core.masking import Masking\n from keras_core.layers.layer import Layer\n from keras_core.layers.merging.add import Add\n from keras_core.layers.merging.add import add\n\n@@ -0,0 +1,18 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Identity\")\n+class Identity(Layer):\n+    \"\"\"Identity layer.\n+\n+    This layer should be used as a placeholder when no operation is to be\n+    performed. The layer just returns its `inputs` argument as output.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+\n+    def call(self, inputs):\n+        return inputs\n\n@@ -0,0 +1,17 @@\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class IdentityTest(testing.TestCase):\n+    def test_identity_basics(self):\n+        self.run_layer_test(\n+            layers.Identity,\n+            init_kwargs={},\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n\n@@ -0,0 +1,70 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Masking\")\n+class Masking(Layer):\n+    \"\"\"Masks a sequence by using a mask value to skip timesteps.\n+\n+    For each timestep in the input tensor (dimension #1 in the tensor),\n+    if all values in the input tensor at that timestep\n+    are equal to `mask_value`, then the timestep will be masked (skipped)\n+    in all downstream layers (as long as they support masking).\n+\n+    If any downstream layer does not support masking yet receives such\n+    an input mask, an exception will be raised.\n+\n+    Example:\n+\n+    Consider a NumPy data array `x` of shape `(samples, timesteps, features)`,\n+    to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you\n+    lack data for these timesteps. You can:\n+\n+    - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.`\n+    - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:\n+\n+    ```python\n+    samples, timesteps, features = 32, 10, 8\n+    inputs = np.random.random([samples, timesteps, features]).astype(np.float32)\n+    inputs[:, 3, :] = 0.\n+    inputs[:, 5, :] = 0.\n+\n+    model = keras_core.models.Sequential()\n+    model.add(keras_core.layers.Masking(mask_value=0.)\n+    model.add(keras_core.layers.LSTM(32))\n+    output = model(inputs)\n+    # The time step 3 and 5 will be skipped from LSTM calculation.\n+    ```\n+\n+    Note: in the Keras masking convention, a masked timestep is denoted by\n+    a mask value of `False`, while a non-masked (i.e. usable) timestep\n+    is denoted by a mask value of `True`.\n+    \"\"\"\n+\n+    def __init__(self, mask_value=0.0, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.mask_value = mask_value\n+\n+    def compute_mask(self, inputs, mask=None):\n+        return ops.any(ops.not_equal(inputs, self.mask_value), axis=-1)\n+\n+    def call(self, inputs):\n+        boolean_mask = ops.any(\n+            ops.not_equal(inputs, self.mask_value), axis=-1, keepdims=True\n+        )\n+        # Set masked outputs to 0\n+        outputs = inputs * backend.cast(boolean_mask, dtype=inputs.dtype)\n+        # Compute the mask and outputs simultaneously.\n+        outputs._keras_mask = ops.squeeze(boolean_mask, axis=-1)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\"mask_value\": self.mask_value}\n+        return {**base_config, **config}\n\n@@ -0,0 +1,53 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import models\n+from keras_core import testing\n+\n+\n+class MaskingTest(testing.TestCase):\n+    def test_masking_basics(self):\n+        self.run_layer_test(\n+            layers.Masking,\n+            init_kwargs={\"mask_value\": 0.0},\n+            input_shape=(2, 3, 2),\n+            expected_output_shape=(2, 3, 2),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_masking_correctness(self):\n+        x = np.array(\n+            [\n+                [[0.0, 0.0], [1.0, 2.0], [0.0, 0.0]],\n+                [[2.0, 2.0], [0.0, 0.0], [2.0, 1.0]],\n+            ]\n+        )\n+        expected_mask = [[False, True, False], [True, False, True]]\n+\n+        layer = layers.Masking(mask_value=0.0)\n+        self.assertAllClose(layer.compute_mask(x), expected_mask)\n+\n+        class TestLayer(layers.Layer):\n+            def __init__(self, **kwargs):\n+                super().__init__(**kwargs)\n+                self.supports_masking = True\n+\n+            def compute_output_shape(self, input_shape):\n+                return input_shape\n+\n+            def call(self, inputs, mask=None):\n+                assert mask is not None\n+                np.testing.assert_allclose(mask, expected_mask)\n+                return inputs\n+\n+        model = models.Sequential(\n+            [\n+                layers.Masking(mask_value=0.0),\n+                TestLayer(),\n+            ]\n+        )\n+        model(x)\n\n@@ -877,6 +877,9 @@ def get_shapes_dict(call_spec):\n     \"\"\"\n     shapes_dict = {}\n     for k, v in call_spec.tensor_arguments_dict.items():\n+        if k == \"mask\" or k.startswith(\"mask_\"):\n+            # Do not include mask tensors in shapes dict\n+            continue\n         if k in call_spec.nested_tensor_argument_names:\n             shapes_dict[f\"{k}_shape\"] = nest.map_structure(\n                 lambda x: backend.standardize_shape(x.shape), v\n\n@@ -283,7 +283,7 @@ class All(Operation):\n                 axis=self.axis,\n                 keepdims=self.keepdims,\n             ),\n-            dtype=x.dtype,\n+            dtype=\"bool\",\n         )\n \n \n@@ -293,6 +293,39 @@ def all(x, axis=None, keepdims=False):\n     return backend.numpy.all(x, axis=axis, keepdims=keepdims)\n \n \n+class Any(Operation):\n+    def __init__(self, axis=None, keepdims=False):\n+        super().__init__()\n+        if isinstance(axis, int):\n+            self.axis = [axis]\n+        else:\n+            self.axis = axis\n+        self.keepdims = keepdims\n+\n+    def call(self, x):\n+        return backend.numpy.any(\n+            x,\n+            axis=self.axis,\n+            keepdims=self.keepdims,\n+        )\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(\n+            reduce_shape(\n+                x.shape,\n+                axis=self.axis,\n+                keepdims=self.keepdims,\n+            ),\n+            dtype=\"bool\",\n+        )\n+\n+\n+def any(x, axis=None, keepdims=False):\n+    if any_symbolic_tensors((x,)):\n+        return Any(axis=axis, keepdims=keepdims).symbolic_call(x)\n+    return backend.numpy.any(x, axis=axis, keepdims=keepdims)\n+\n+\n class Amax(Operation):\n     def __init__(self, axis=None, keepdims=False):\n         super().__init__()\n\n@@ -644,6 +644,14 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knp.all(x, axis=1).shape, (None, 3))\n         self.assertEqual(knp.all(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n+    def test_any(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.any(x).shape, ())\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.any(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.any(x, axis=1, keepdims=True).shape, (None, 1, 3))\n+\n     def test_var(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.var(x).shape, ())\n@@ -1111,6 +1119,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.all(x).shape, ())\n \n+    def test_any(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.any(x).shape, ())\n+\n     def test_var(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.var(x).shape, ())\n@@ -2085,6 +2097,22 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.all(x, axis=1, keepdims=True),\n         )\n \n+    def test_any(self):\n+        x = np.array([[True, False, True], [True, True, True]])\n+        self.assertAllClose(np.array(knp.any(x)), np.any(x))\n+        self.assertAllClose(np.array(knp.any(x, axis=1)), np.any(x, axis=1))\n+        self.assertAllClose(\n+            np.array(knp.any(x, axis=1, keepdims=True)),\n+            np.any(x, axis=1, keepdims=True),\n+        )\n+\n+        self.assertAllClose(np.array(knp.Any()(x)), np.any(x))\n+        self.assertAllClose(np.array(knp.Any(axis=1)(x)), np.any(x, axis=1))\n+        self.assertAllClose(\n+            np.array(knp.Any(axis=1, keepdims=True)(x)),\n+            np.any(x, axis=1, keepdims=True),\n+        )\n+\n     def test_var(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(np.array(knp.var(x)), np.var(x))\n\n@@ -52,8 +52,8 @@ class Operation:\n         except Exception as e:\n             raise RuntimeError(\n                 \"Could not automatically infer the output shape / dtype of \"\n-                \"this operation. \"\n-                \"Please implement the `compute_output_spec` method \"\n+                f\"operation '{self.name}'. \"\n+                \"Please implement the `compute_output_spec()` method \"\n                 f\"on your object ({self.__class__.__name__}). \"\n                 f\"Error encountered: {e}\"\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70a217de5c0235f5a7bb490951916c3a89db6ad9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 800 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,6 +42,10 @@ try:\n except ImportError:\n     h5py = None\n \n+keras_saving_gauge = tf.__internal__.monitoring.BoolGauge(\n+    \"/tensorflow/api/keras/saving\", \"keras saving usage\", \"method\"\n+)\n+\n # isort: off\n \n _CONFIG_FILENAME = \"config.json\"\n@@ -129,7 +133,7 @@ def save_model(model, filepath, weights_format=\"h5\"):\n     \"\"\"\n \n     # API usage tracking for Keras V3 saving\n-    base_layer.keras_api_gauge.get_cell(\"save_model_v3\").set(True)\n+    keras_saving_gauge.get_cell(\"save_model_v3\").set(True)\n \n     filepath = str(filepath)\n     if not filepath.endswith(\".keras\"):\n@@ -292,7 +296,7 @@ def save_weights_only(model, filepath):\n     # then upload it\n \n     # API usage tracking for Keras V3 saving\n-    base_layer.keras_api_gauge.get_cell(\"save_weights_v3\").set(True)\n+    keras_saving_gauge.get_cell(\"save_weights_v3\").set(True)\n \n     filepath = str(filepath)\n     if not filepath.endswith(\".weights.h5\"):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbcc7c0aaf498b80903aeda48db88cf913e0c49f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 64 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 64 | Churn Cumulative: 128 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,64 +0,0 @@\n-\"\"\"\n-Upload to GCS:\n-\n-```\n-gcloud config set project tensorflow\n-gcloud storage cp /Users/fchollet/Downloads/CaliforniaHousing/california_housing.npz gs://tensorflow/tf-keras-datasets/california_housing.npz\n-```\n-\"\"\"\n-import numpy as np\n-\n-fpath = \"/Users/fchollet/Downloads/CaliforniaHousing/cal_housing.data\"\n-target_fpath = (\n-    \"/Users/fchollet/Downloads/CaliforniaHousing/california_housing.npz\"\n-)\n-header = [\n-    \"MedInc\",\n-    \"HouseAge\",\n-    \"AveRooms\",\n-    \"AveBedrms\",\n-    \"Population\",\n-    \"AveOccup\",\n-    \"Latitude\",\n-    \"Longitude\",\n-    \"MedHouseVal\",\n-]\n-\n-\n-def process_dataset():\n-    f = open(fpath)\n-    contents = f.read()\n-    f.close()\n-    lines = contents.split(\"\\n\")\n-    samples = []\n-    targets = []\n-    for line in lines[:-1]:\n-        values = line.split(\",\")\n-        values = [float(v) for v in values]\n-        values, target = values[:-1], values[-1]\n-        samples.append(values)\n-        targets.append(target)\n-    samples = np.array(samples, dtype=\"float32\")\n-    targets = np.array(targets, dtype=\"float32\")\n-    print(samples.shape)\n-    print(targets.shape)\n-    np.savez(target_fpath, x=samples, y=targets)\n-\n-\n-def test_dataset():\n-    with np.load(target_fpath, allow_pickle=True) as f:\n-        x = f[\"x\"]\n-        y = f[\"y\"]\n-    print(x.shape)\n-    print(y.shape)\n-\n-    print(\"Lowest y\", sorted(y)[:10])\n-    print(\"Highest y\", sorted(y, reverse=True)[:10])\n-\n-    from keras_core.utils.file_utils import hash_file\n-\n-    print(hash_file(target_fpath))\n-\n-\n-# process_dataset()\n-test_dataset()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c74ae46cfdd7da8e74e580ee6e849e0ad836255", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 554 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 17 | Complexity Δ (Sum/Max): 68/61 | Churn Δ: 554 | Churn Cumulative: 1334 | Contributors (this commit): 4 | Commits (past 90d): 31 | Contributors (cumulative): 7 | DMM Complexity: 0.3959537572254335\n\nDIFF:\n@@ -12,6 +12,8 @@ from keras_core.layers.merging.average import Average\n from keras_core.layers.merging.average import average\n from keras_core.layers.merging.concatenate import Concatenate\n from keras_core.layers.merging.concatenate import concatenate\n+from keras_core.layers.merging.dot import Dot\n+from keras_core.layers.merging.dot import dot\n from keras_core.layers.merging.maximum import Maximum\n from keras_core.layers.merging.maximum import maximum\n from keras_core.layers.merging.minimum import Minimum\n\n@@ -38,6 +38,7 @@ class Concatenate(Merge):\n         self._reshape_required = False\n \n     def build(self, input_shape):\n+        super().build(input_shape)\n         # Used purely for shape validation.\n         if len(input_shape) < 1 or not isinstance(input_shape[0], tuple):\n             raise ValueError(\n\n@@ -0,0 +1,399 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+def batch_dot(x, y, axes=None):\n+    \"\"\"Batchwise dot product.\n+\n+    `batch_dot` is used to compute dot product of `x` and `y` when\n+    `x` and `y` are data in batch, i.e. in a shape of `(batch_size, :)`.\n+    `batch_dot` results in a tensor or variable with less dimensions\n+    than the input. If the number of dimensions is reduced to 1,\n+    we use `expand_dims` to make sure that ndim is at least 2.\n+\n+    Shape inference:\n+\n+    Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n+    If `axes` is (1, 2), to find the output shape of resultant tensor,\n+    loop through each dimension in `x`'s shape and `y`'s shape:\n+\n+    * `x.shape[0]` : 100 : append to output shape\n+    * `x.shape[1]` : 20 : do not append to output shape, dimension 1 of\n+        `x` has been summed over. (`dot_axes[0]` = 1)\n+    * `y.shape[0]` : 100 : do not append to output shape, always ignore\n+        first dimension of `y`\n+    * `y.shape[1]` : 30 : append to output shape\n+    * `y.shape[2]` : 20 : do not append to output shape, dimension 2 of\n+        `y` has been summed over.\n+        (`dot_axes[1]` = 2) `output_shape` = `(100, 30)`\n+\n+    Examples:\n+\n+    >>> x_batch = np.ones(shape=(32, 20, 1))\n+    >>> y_batch = np.ones(shape=(32, 30, 20))\n+    >>> xy_batch_dot = batch_dot(x_batch, y_batch, axes=(1, 2))\n+\n+    Args:\n+        x: Keras tensor or variable with `ndim >= 2`.\n+        y: Keras tensor or variable with `ndim >= 2`.\n+        axes: Tuple or list of integers with target dimensions, or single\n+            integer. The sizes of `x.shape[axes[0]]` and `y.shape[axes[1]]`\n+            should be equal.\n+\n+    Returns:\n+        A tensor with shape equal to the concatenation of `x`'s shape\n+        (less the dimension that was summed over) and `y`'s shape (less the\n+        batch dimension and the dimension that was summed over). If the final\n+        rank is 1, we reshape it to `(batch_size, 1)`.\n+    \"\"\"\n+\n+    x_shape = tuple(ops.shape(x))\n+    y_shape = tuple(ops.shape(y))\n+\n+    x_ndim = len(x_shape)\n+    y_ndim = len(y_shape)\n+\n+    if x_ndim < 2 or y_ndim < 2:\n+        raise ValueError(\n+            f\"Cannot do batch_dot on inputs \"\n+            f\"with rank < 2. \"\n+            f\"Received inputs with shapes \"\n+            f\"{x_shape} and {y_shape}.\"\n+        )\n+\n+    x_batch_size = x_shape[0]\n+    y_batch_size = y_shape[0]\n+\n+    if x_batch_size is not None and y_batch_size is not None:\n+        if x_batch_size != y_batch_size:\n+            raise ValueError(\n+                f\"Cannot do batch_dot on inputs \"\n+                f\"with different batch sizes. \"\n+                f\"Received inputs with shapes \"\n+                f\"{x_shape} and {y_shape}.\"\n+            )\n+    if isinstance(axes, int):\n+        axes = [axes, axes]\n+\n+    if axes is None:\n+        if y_ndim == 2:\n+            axes = [x_ndim - 1, y_ndim - 1]\n+        else:\n+            axes = [x_ndim - 1, y_ndim - 2]\n+\n+    if any(isinstance(a, (list, tuple)) for a in axes):\n+        raise ValueError(\n+            f\"Multiple target dimensions are not supported. \"\n+            f\"Expected: None, int, (int, int), \"\n+            f\"Provided: {axes} \"\n+        )\n+\n+    # if tuple, convert to list.\n+    axes = list(axes)\n+\n+    # convert negative indices.\n+    if axes[0] < 0:\n+        axes[0] += x_ndim\n+    if axes[1] < 0:\n+        axes[1] += y_ndim\n+\n+    # sanity checks\n+    if 0 in axes:\n+        raise ValueError(\n+            \"Cannot perform batch_dot over axis 0. \"\n+            \"If your inputs are not batched, \"\n+            \"add a dummy batch dimension to your \"\n+            \"inputs using keras_core.operations.expand_dims(x, 0)\"\n+        )\n+    a0, a1 = axes\n+    d1 = x_shape[a0]\n+    d2 = y_shape[a1]\n+\n+    if d1 is not None and d2 is not None and d1 != d2:\n+        raise ValueError(\n+            f\"Cannot do batch_dot on inputs with shapes \"\n+            f\"{x_shape} and {y_shape} with axes={axes}. \"\n+            f\"x.shape[{axes[0]}] != y.shape[{axes[1]}] ({d1} != {d2}).\"\n+        )\n+\n+    # backup ndims. Need them later.\n+    orig_x_ndim = x_ndim\n+    orig_y_ndim = y_ndim\n+\n+    # if rank is 2, expand to 3.\n+    if x_ndim == 2:\n+        x = ops.expand_dims(x, 1)\n+        a0 += 1\n+        x_ndim += 1\n+    if y_ndim == 2:\n+        y = ops.expand_dims(y, 2)\n+        y_ndim += 1\n+\n+    # bring x's dimension to be reduced to last axis.\n+    if a0 != x_ndim - 1:\n+        pattern = list(range(x_ndim))\n+        for i in range(a0, x_ndim - 1):\n+            pattern[i] = pattern[i + 1]\n+        pattern[-1] = a0\n+        x = ops.transpose(x, pattern)\n+\n+    # bring y's dimension to be reduced to axis 1.\n+    if a1 != 1:\n+        pattern = list(range(y_ndim))\n+        for i in range(a1, 1, -1):\n+            pattern[i] = pattern[i - 1]\n+        pattern[1] = a1\n+        y = ops.transpose(y, pattern)\n+\n+    # normalize both inputs to rank 3.\n+    if x_ndim > 3:\n+        # squash middle dimensions of x.\n+        x_shape = list(ops.shape(x))\n+        x_mid_dims = x_shape[1:-1]\n+        x_squashed_shape = ops.stack([x_shape[0], -1, x_shape[-1]])\n+        x = ops.reshape(x, x_squashed_shape)\n+        x_squashed = True\n+    else:\n+        x_squashed = False\n+\n+    if y_ndim > 3:\n+        # squash trailing dimensions of y.\n+        y_shape = list(ops.shape(y))\n+        y_trail_dims = y_shape[2:]\n+        y_squashed_shape = ops.stack([y_shape[0], y_shape[1], -1])\n+        y = ops.reshape(y, y_squashed_shape)\n+        y_squashed = True\n+    else:\n+        y_squashed = False\n+\n+    result = ops.matmul(x, y)\n+\n+    # if inputs were squashed, we have to reshape the matmul output.\n+    output_shape = ops.shape(result)\n+    do_reshape = False\n+\n+    if x_squashed:\n+        output_shape = ops.concat(\n+            [output_shape[:1], x_mid_dims, output_shape[-1:]], 0\n+        )\n+        do_reshape = True\n+\n+    if y_squashed:\n+        output_shape = ops.concat([output_shape[:-1], y_trail_dims], 0)\n+        do_reshape = True\n+\n+    if do_reshape:\n+        result = ops.reshape(result, output_shape)\n+\n+    # if the inputs were originally rank 2, we remove the added 1 dim.\n+    if orig_x_ndim == 2:\n+        result = ops.squeeze(result, 1)\n+    elif orig_y_ndim == 2:\n+        result = ops.squeeze(result, -1)\n+\n+    return result\n+\n+\n+def l2_normalize(x, axis=None, epsilon=1e-9):\n+    \"\"\"Normalizes along dimension `axis` using an L2 norm.\n+\n+    For a 1-D tensor with `axis = 0`, computes\n+\n+    `output = x / sqrt(max(sum(x**2), epsilon))`\n+\n+    For `x` with more dimensions, independently normalizes each\n+    1-D slice along dimension `axis`.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Dimension along which to normalize. A scalar or a\n+            vector of integers.\n+        epsilon: A lower bound value for the norm. Will use `sqrt(epsilon)`\n+            as the divisor if `norm < sqrt(epsilon)`.\n+\n+    Returns:\n+        A normalized tensor with the same shape as input.\n+    \"\"\"\n+\n+    square_sum = ops.sum(ops.square(x), axis=axis, keepdims=True)\n+    x_inv_norm = 1.0 / ops.sqrt(ops.maximum(square_sum, epsilon))\n+    return ops.multiply(x, x_inv_norm)\n+\n+\n+@keras_core_export(\"keras_core.layers.Dot\")\n+class Dot(Merge):\n+    \"\"\"Computes element-wise dot product of two tensors.\n+\n+    It takes a list of inputs of size 2, and the axes\n+    corresponding to each input along with the dot product\n+    is to be performed.\n+\n+    Let's say `x` and `y` are the two input tensors with shapes\n+    `(2, 3, 5)` and `(2, 10, 3)`. The batch dimension should be\n+    of same size for both the inputs, and `axes` should correspond\n+    to the dimensions that have the same size in the corresponding\n+    inputs. e.g. with `axes=[1, 2]`, the dot product of `x`, and `y`\n+    will result in a tensor with shape `(2, 5, 10)`\n+\n+    Examples:\n+\n+    >>> x = np.arange(10).reshape(1, 5, 2)\n+    >>> y = np.arange(10, 20).reshape(1, 2, 5)\n+    >>> keras_core.layers.Dot(axes=(1, 2))([x, y])\n+\n+    Usage in a Keras model:\n+\n+    >>> x1 = keras_core.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+    >>> x2 = keras_core.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+    >>> y = keras_core.layers.Dot(axes=1)([x1, x2])\n+\n+    Args:\n+        axes: Integer or tuple of integers, axis or axes along which to\n+            take the dot product. If a tuple, should be two integers\n+            corresponding to the desired axis from the first input and the\n+            desired axis from the second input, respectively. Note that the\n+            size of the two selected axes must match.\n+        normalize: Whether to L2-normalize samples along the dot product axis\n+            before taking the dot product. If set to True, then the output of\n+            the dot product is the cosine proximity between the two samples.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor, the dot product of the samples from the inputs.\n+    \"\"\"\n+\n+    def __init__(self, axes, normalize=False, **kwargs):\n+        super().__init__(**kwargs)\n+        if not isinstance(axes, int):\n+            if not isinstance(axes, (list, tuple)):\n+                raise TypeError(\n+                    f\"Invalid type for argument `axes`: it should be \"\n+                    f\"a list or an int. Received: axes={axes}\"\n+                )\n+            if len(axes) != 2:\n+                raise ValueError(\n+                    f\"Invalid format for argument `axes`: it should contain \"\n+                    f\"two elements. Received: axes={axes}\"\n+                )\n+            if not isinstance(axes[0], int) or not isinstance(axes[1], int):\n+                raise ValueError(\n+                    f\"Invalid format for argument `axes`: list elements should \"\n+                    f\"be integers. Received: axes={axes}\"\n+                )\n+        self.axes = axes\n+        self.normalize = normalize\n+        self.supports_masking = True\n+        self._reshape_required = False\n+\n+    def build(self, input_shape):\n+        super().build(input_shape)\n+        # Used purely for shape validation.\n+        if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n+            raise ValueError(\n+                f\"A `Dot` layer should be called on a list of 2 inputs. \"\n+                f\"Received: input_shape={input_shape}\"\n+            )\n+        shape1 = input_shape[0]\n+        shape2 = input_shape[1]\n+        if shape1 is None or shape2 is None:\n+            return\n+        if isinstance(self.axes, int):\n+            if self.axes < 0:\n+                axes = [self.axes % len(shape1), self.axes % len(shape2)]\n+            else:\n+                axes = [self.axes] * 2\n+        else:\n+            axes = self.axes\n+        if shape1[axes[0]] != shape2[axes[1]]:\n+            raise ValueError(\n+                f\"Incompatible input shapes: \"\n+                f\"axis values {shape1[axes[0]]} (at axis {axes[0]}) != \"\n+                f\"{shape2[axes[1]]} (at axis {axes[1]}). \"\n+                f\"Full input shapes: {shape1}, {shape2}\"\n+            )\n+\n+    def _merge_function(self, inputs):\n+        if len(inputs) != 2:\n+            raise ValueError(\n+                f\"A `Dot` layer should be called on exactly 2 inputs. \"\n+                f\"Received: inputs={inputs}\"\n+            )\n+        x1 = inputs[0]\n+        x2 = inputs[1]\n+\n+        if isinstance(self.axes, int):\n+            if self.axes < 0:\n+                axes = [\n+                    self.axes % x1.ndim,\n+                    self.axes % x2.ndim,\n+                ]\n+            else:\n+                axes = [self.axes] * 2\n+        else:\n+            axes = []\n+            for i in range(len(self.axes)):\n+                if self.axes[i] < 0:\n+                    axes.append(self.axes[i] % inputs[i].ndim)\n+                else:\n+                    axes.append(self.axes[i])\n+\n+        if self.normalize:\n+            x1 = l2_normalize(x1, axis=axes[0])\n+            x2 = l2_normalize(x2, axis=axes[1])\n+        output = batch_dot(x1, x2, axes)\n+        return output\n+\n+    def compute_output_shape(self, input_shape):\n+        if not isinstance(input_shape, (tuple, list)) or len(input_shape) != 2:\n+            raise ValueError(\n+                f\"A `Dot` layer should be called on a list of 2 inputs. \"\n+                f\"Received: input_shape={input_shape}\"\n+            )\n+        shape1 = list(input_shape[0])\n+        shape2 = list(input_shape[1])\n+        if isinstance(self.axes, int):\n+            if self.axes < 0:\n+                axes = [self.axes % len(shape1), self.axes % len(shape2)]\n+            else:\n+                axes = [self.axes] * 2\n+        else:\n+            axes = self.axes\n+        shape1.pop(axes[0])\n+        shape2.pop(axes[1])\n+        shape2.pop(0)\n+        output_shape = shape1 + shape2\n+        if len(output_shape) == 1:\n+            output_shape += [1]\n+        return tuple(output_shape)\n+\n+    def compute_mask(self, inputs, mask=None):\n+        return None\n+\n+    def get_config(self):\n+        config = {\n+            \"axes\": self.axes,\n+            \"normalize\": self.normalize,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.layers.dot\")\n+def dot(inputs, axes=-1, **kwargs):\n+    \"\"\"Functional interface to the `Dot` layer.\n+\n+    Args:\n+        inputs: A list of input tensors (at least 2).\n+        axes: Integer or tuple of integers,\n+            axis or axes along which to take the dot product.\n+        normalize: Whether to L2-normalize samples along the\n+            dot product axis before taking the dot product.\n+            If set to True, then the output of the dot product\n+            is the cosine proximity between the two samples.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor, the dot product of the samples from the inputs.\n+    \"\"\"\n+    return Dot(axes=axes, **kwargs)(inputs)\n\n@@ -563,3 +563,155 @@ class MergingLayersTest(testing.TestCase):\n             ValueError, \" should have the same length.\"\n         ):\n             merge_layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_concatenate_basic(self):\n+        self.run_layer_test(\n+            layers.Concatenate,\n+            init_kwargs={\"axis\": 1},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 6),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_concatenate_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        axis = 1\n+\n+        x3 = ops.concatenate([x1, x2], axis=axis)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        merge_layer = layers.Concatenate(axis=axis)\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 8, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_concatenate_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        axis = 1\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.concatenate([x1, x2], axis=axis)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Concatenate(axis=axis)\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, 8, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [x1, x2],\n+                )\n+            )\n+        )\n+\n+    def test_concatenate_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        axis = 1\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Concatenate(axis=axis)\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            merge_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            merge_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            merge_layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_dot_basic(self):\n+        self.run_layer_test(\n+            layers.Dot,\n+            init_kwargs={\"axes\": -1},\n+            input_shape=[(4, 3), (4, 3)],\n+            expected_output_shape=(4, 1),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=None,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_dot_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4)\n+        x2 = np.random.rand(2, 4)\n+        axes = 1\n+\n+        expected = np.zeros((2, 1))\n+        expected[0, 0] = np.dot(x1[0], x2[0])\n+        expected[1, 0] = np.dot(x1[1], x2[1])\n+\n+        input_1 = layers.Input(shape=(4,))\n+        input_2 = layers.Input(shape=(4,))\n+        merge_layer = layers.Dot(axes=axes)\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 1))\n+        self.assertAllClose(res, expected, atol=1e-4)\n+\n+    def test_dot_correctness_static(self):\n+        batch_size = 2\n+        shape = (4,)\n+        axes = 1\n+\n+        x1 = np.random.rand(2, 4)\n+        x2 = np.random.rand(2, 4)\n+        expected = np.zeros((2, 1))\n+        expected[0, 0] = np.dot(x1[0], x2[0])\n+        expected[1, 0] = np.dot(x1[1], x2[1])\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Dot(axes=axes)\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 1))\n+        self.assertAllClose(res, expected, atol=1e-4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
