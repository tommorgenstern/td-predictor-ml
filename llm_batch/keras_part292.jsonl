{"custom_id": "keras#665b0076e089dbe60ecadc34bf2cbff9d7042482", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 228 | Files Changed: 2 | Hunks: 83 | Methods Changed: 23 | Complexity Δ (Sum/Max): -16/0 | Churn Δ: 351 | Churn Cumulative: 3445 | Contributors (this commit): 41 | Commits (past 90d): 13 | Contributors (cumulative): 49 | DMM Complexity: 0.5393258426966292\n\nDIFF:\n@@ -6,7 +6,6 @@ from __future__ import print_function\n \n import six\n import copy\n-import numpy as np\n from six.moves import zip\n \n from . import backend as K\n@@ -162,24 +161,23 @@ class SGD(Optimizer):\n     learning rate decay, and Nesterov momentum.\n \n     # Arguments\n-        learning_rate: float >= 0. Learning rate.\n+        lr: float >= 0. Learning rate.\n         momentum: float >= 0. Parameter that accelerates SGD\n             in the relevant direction and dampens oscillations.\n+        decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.01, momentum=0.,\n+    def __init__(self, lr=0.01, momentum=0., decay=0.,\n                  nesterov=False, **kwargs):\n-        self.initial_decay = kwargs.pop('decay', 0.0)\n+        super(SGD, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n+            self.lr = K.variable(lr, name='lr')\n             self.momentum = K.variable(momentum, name='momentum')\n-            self.decay = K.variable(self.initial_decay, name='decay')\n-        self._momentum = True if momentum > 0. else False\n+            self.decay = K.variable(decay, name='decay')\n+        self.initial_decay = decay\n         self.nesterov = nesterov\n-        super(SGD, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -190,35 +188,28 @@ class SGD(Optimizer):\n         if self.initial_decay > 0:\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n-        self.weights = [self.iterations]\n-        if self._momentum:\n+        # momentum\n         shapes = [K.int_shape(p) for p in params]\n         moments = [K.zeros(shape) for shape in shapes]\n-            self.weights.extend([K.zeros(shape) for shape in shapes])\n-        else:\n-            moments = [None] * len(params)\n+        self.weights = [self.iterations] + moments\n         for p, g, m in zip(params, grads, moments):\n-            if self._momentum:\n-                # momentum optimizer\n             v = self.momentum * m - lr * g  # velocity\n             self.updates.append(K.update(m, v))\n+\n             if self.nesterov:\n                 new_p = p + self.momentum * v - lr * g\n             else:\n                 new_p = p + v\n-            else:\n-                # sgd optimizer.\n-                new_p = p - lr * g\n+\n             # Apply constraints.\n             if getattr(p, 'constraint', None) is not None:\n                 new_p = p.constraint(new_p)\n \n             self.updates.append(K.update(p, new_p))\n-\n         return self.updates\n \n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'momentum': float(K.get_value(self.momentum)),\n                   'decay': float(K.get_value(self.decay)),\n                   'nesterov': self.nesterov}\n@@ -234,41 +225,34 @@ class RMSprop(Optimizer):\n     (except the learning rate, which can be freely tuned).\n \n     # Arguments\n-        learning_rate: float >= 0. Learning rate.\n+        lr: float >= 0. Learning rate.\n         rho: float >= 0.\n-        momentum: float hyperparameter >= 0 that accelerates RMSprop in the relevant\n-            direction and dampens oscillations.\n-        centered: If True, gradients are normalized by the estimated variance of\n-            the gradient; if False, by the uncentered second moment. Setting this to\n-            True may help with training, but is slightly more expensive in terms of\n-            computation and memory. Defaults to False.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [rmsprop: Divide the gradient by a running average of its recent magnitude\n            ](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, rho=0.9, momentum=0., centered=False,\n+    def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n                  **kwargs):\n-        self.initial_decay = kwargs.pop('decay', 0.0)\n-        self._momentum = True if momentum > 0. else False\n-        with K.name_scope(self.__class__.__name__):\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n-            self.rho = K.variable(rho, name='rho')\n-            self.decay = K.variable(self.initial_decay, name='decay')\n-            self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            if self._momentum:\n-                self.momentum = K.variable(momentum, name='momentum')\n-        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n-        self.centered = centered\n         super(RMSprop, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n+            self.lr = K.variable(lr, name='lr')\n+            self.rho = K.variable(rho, name='rho')\n+            self.decay = K.variable(decay, name='decay')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-        self.weights = [self.iterations] + accumulators\n+        self.weights = accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n@@ -276,52 +260,21 @@ class RMSprop(Optimizer):\n             lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                       K.dtype(self.decay))))\n \n-        if self._momentum:\n-            ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-            self.weights.extend(ms)\n-            if self.centered:\n-                mgs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-                self.weights.extend(mgs)\n-            else:\n-                mgs = [None] * len(params)\n-        else:\n-            ms = [None] * len(params)\n-            mgs = [None] * len(params)\n-\n-        for p, g, a, m, mg in zip(params, grads, accumulators, ms, mgs):\n+        for p, g, a in zip(params, grads, accumulators):\n+            # update accumulator\n             new_a = self.rho * a + (1. - self.rho) * K.square(g)\n             self.updates.append(K.update(a, new_a))\n-            if m:\n-                if mg:\n-                    new_mg = self.rho * mg + (1. - self.rho) * g\n-                    new_m = self.momentum * m + lr * g / K.sqrt(\n-                        new_a - K.square(new_mg) + self.epsilon)\n-                    self.updates.append(K.update(mg, new_mg))\n-                else:\n-                    new_m = self.momentum * m + lr * g / K.sqrt(new_a + self.epsilon)\n-                self.updates.append(K.update(m, new_m))\n-                new_p = p - new_m\n-            else:\n             new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n+\n             # Apply constraints.\n             if getattr(p, 'constraint', None) is not None:\n                 new_p = p.constraint(new_p)\n \n             self.updates.append(K.update(p, new_p))\n-\n         return self.updates\n \n-    def set_weights(self, weights):\n-        params = self.weights\n-        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n-        # since it does not include iteration at head of the weight list. Set\n-        # iteration to 0.\n-        if len(params) == len(weights) + 1:\n-            weights = [np.array(0)] + weights\n-        super(RMSprop, self).set_weights(weights)\n-\n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'rho': float(K.get_value(self.rho)),\n                   'decay': float(K.get_value(self.decay)),\n                   'epsilon': self.epsilon}\n@@ -341,33 +294,32 @@ class Adagrad(Optimizer):\n     at their default values.\n \n     # Arguments\n-        learning_rate: float >= 0. Initial learning rate.\n-        initial_accumulator_value: A floating point value.\n-            Starting value for the accumulators, must be positive.\n+        lr: float >= 0. Initial learning rate.\n+        epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic\n            Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, initial_accumulator_value=0.1, **kwargs):\n-        self.initial_decay = kwargs.pop('decay', 0.0)\n-        with K.name_scope(self.__class__.__name__):\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n-            self.decay = K.variable(self.initial_decay, name='decay')\n-            self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n-        self._initial_accumulator_value = initial_accumulator_value\n+    def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n+            self.lr = K.variable(lr, name='lr')\n+            self.decay = K.variable(decay, name='decay')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         shapes = [K.int_shape(p) for p in params]\n-        accumulators = [K.variable(np.full(shape, self._initial_accumulator_value))\n-                        for shape in shapes]\n-        self.weights = [self.iterations] + accumulators\n+        accumulators = [K.zeros(shape) for shape in shapes]\n+        self.weights = accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n@@ -387,19 +339,10 @@ class Adagrad(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n-    def set_weights(self, weights):\n-        params = self.weights\n-        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n-        # since it does not include iteration at head of the weight list. Set\n-        # iteration to 0.\n-        if len(params) == len(weights) + 1:\n-            weights = [np.array(0)] + weights\n-        super(Adagrad, self).set_weights(weights)\n-\n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'decay': float(K.get_value(self.decay)),\n-                  'initial_accumulator_value': self._initial_accumulator_value}\n+                  'epsilon': self.epsilon}\n         base_config = super(Adagrad, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -419,26 +362,30 @@ class Adadelta(Optimizer):\n     at their default values.\n \n     # Arguments\n-        learning_rate: float >= 0. Initial learning rate, defaults to 1.\n+        lr: float >= 0. Initial learning rate, defaults to 1.\n             It is recommended to leave it at the default value.\n         rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n             gradient to keep at each time step.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n+        decay: float >= 0. Initial learning rate decay.\n \n     # References\n         - [Adadelta - an adaptive learning rate method](\n            https://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, rho=0.95, **kwargs):\n-        self.initial_decay = kwargs.pop('decay', 0.0)\n-        with K.name_scope(self.__class__.__name__):\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n-            self.decay = K.variable(self.initial_decay, name='decay')\n-            self.iterations = K.variable(0, dtype='int64', name='iterations')\n-        self.rho = rho\n-        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+    def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n+                 **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n+        with K.name_scope(self.__class__.__name__):\n+            self.lr = K.variable(lr, name='lr')\n+            self.decay = K.variable(decay, name='decay')\n+            self.iterations = K.variable(0, dtype='int64', name='iterations')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n+        self.rho = rho\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -446,7 +393,7 @@ class Adadelta(Optimizer):\n         shapes = [K.int_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         delta_accumulators = [K.zeros(shape) for shape in shapes]\n-        self.weights = [self.iterations] + accumulators + delta_accumulators\n+        self.weights = accumulators + delta_accumulators\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         lr = self.lr\n@@ -474,19 +421,11 @@ class Adadelta(Optimizer):\n             self.updates.append(K.update(d_a, new_d_a))\n         return self.updates\n \n-    def set_weights(self, weights):\n-        params = self.weights\n-        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n-        # since it does not include iteration at head of the weight list. Set\n-        # iteration to 0.\n-        if len(params) == len(weights) + 1:\n-            weights = [np.array(0)] + weights\n-        super(Adadelta, self).set_weights(weights)\n-\n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'rho': self.rho,\n-                  'decay': float(K.get_value(self.decay))}\n+                  'decay': float(K.get_value(self.decay)),\n+                  'epsilon': self.epsilon}\n         base_config = super(Adadelta, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -497,9 +436,11 @@ class Adam(Optimizer):\n     Default parameters follow those provided in the original paper.\n \n     # Arguments\n-        learning_rate: float >= 0. Learning rate.\n+        lr: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n+        decay: float >= 0. Learning rate decay over each update.\n         amsgrad: boolean. Whether to apply the AMSGrad variant of this\n             algorithm from the paper \"On the Convergence of Adam and\n             Beyond\".\n@@ -511,19 +452,20 @@ class Adam(Optimizer):\n            https://openreview.net/forum?id=ryQu7f-RZ)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n-                 amsgrad=False, **kwargs):\n-        self.initial_decay = kwargs.pop('decay', 0.0)\n+    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n+                 epsilon=None, decay=0., amsgrad=False, **kwargs):\n+        super(Adam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n+            self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-            self.decay = K.variable(self.initial_decay, name='decay')\n-        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n+            self.decay = K.variable(decay, name='decay')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n         self.amsgrad = amsgrad\n-        super(Adam, self).__init__(**kwargs)\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -541,46 +483,39 @@ class Adam(Optimizer):\n \n         ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n         vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-        self.weights = [self.iterations] + ms + vs\n         if self.amsgrad:\n             vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n-            self.weights.extend(vhats)\n         else:\n-            vhats = [None] * len(params)\n+            vhats = [K.zeros(1) for _ in params]\n+        self.weights = [self.iterations] + ms + vs + vhats\n+\n         for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n             m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n             v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n-            self.updates.append(K.update(m, m_t))\n-            self.updates.append(K.update(v, v_t))\n             if self.amsgrad:\n                 vhat_t = K.maximum(vhat, v_t)\n                 p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n                 self.updates.append(K.update(vhat, vhat_t))\n             else:\n                 p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n+\n+            self.updates.append(K.update(m, m_t))\n+            self.updates.append(K.update(v, v_t))\n             new_p = p_t\n+\n             # Apply constraints.\n             if getattr(p, 'constraint', None) is not None:\n                 new_p = p.constraint(new_p)\n-            self.updates.append(K.update(p, new_p))\n \n+            self.updates.append(K.update(p, new_p))\n         return self.updates\n \n-    def set_weights(self, weights):\n-        params = self.weights\n-        # If the weights are generated by Keras V1 optimizer, it includes vhats\n-        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n-        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n-        num_vars = int((len(params) - 1) / 2)\n-        if len(weights) == 3 * num_vars + 1:\n-            weights = weights[:len(params)]\n-        super(Adam, self).set_weights(weights)\n-\n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n                   'decay': float(K.get_value(self.decay)),\n+                  'epsilon': self.epsilon,\n                   'amsgrad': self.amsgrad}\n         base_config = super(Adam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -593,26 +528,30 @@ class Adamax(Optimizer):\n     Default parameters follow those provided in the paper.\n \n     # Arguments\n-        learning_rate: float >= 0. Learning rate.\n+        lr: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n+        decay: float >= 0. Learning rate decay over each update.\n \n     # References\n         - [Adam - A Method for Stochastic Optimization](\n            https://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, **kwargs):\n-        self.initial_decay = kwargs.pop('decay', 0.0)\n+    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n+                 epsilon=None, decay=0., **kwargs):\n+        super(Adamax, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n+            self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-            self.decay = K.variable(self.initial_decay, name='decay')\n-        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n-        super(Adamax, self).__init__(**kwargs)\n+            self.decay = K.variable(decay, name='decay')\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n+        self.epsilon = epsilon\n+        self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -652,10 +591,11 @@ class Adamax(Optimizer):\n         return self.updates\n \n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n-                  'decay': float(K.get_value(self.decay))}\n+                  'decay': float(K.get_value(self.decay)),\n+                  'epsilon': self.epsilon}\n         base_config = super(Adamax, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -671,9 +611,11 @@ class Nadam(Optimizer):\n     at their default values.\n \n     # Arguments\n-        learning_rate: float >= 0. Learning rate.\n+        lr: float >= 0. Learning rate.\n         beta_1: float, 0 < beta < 1. Generally close to 1.\n         beta_2: float, 0 < beta < 1. Generally close to 1.\n+        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n+        schedule_decay: float, 0 < schedule_decay < 1.\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n@@ -681,17 +623,19 @@ class Nadam(Optimizer):\n            http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n \n-    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, **kwargs):\n-        self.schedule_decay = kwargs.pop('schedule_decay', 0.004)\n+    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n+                 epsilon=None, schedule_decay=0.004, **kwargs):\n+        super(Nadam, self).__init__(**kwargs)\n         with K.name_scope(self.__class__.__name__):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n             self.m_schedule = K.variable(1., name='m_schedule')\n-            learning_rate = kwargs.pop('lr', learning_rate)\n-            self.lr = K.variable(learning_rate, name='learning_rate')\n+            self.lr = K.variable(lr, name='lr')\n             self.beta_1 = K.variable(beta_1, name='beta_1')\n             self.beta_2 = K.variable(beta_2, name='beta_2')\n-        self.epsilon = kwargs.pop('epsilon', K.epsilon())\n-        super(Nadam, self).__init__(**kwargs)\n+        if epsilon is None:\n+            epsilon = K.epsilon()\n+        self.epsilon = epsilon\n+        self.schedule_decay = schedule_decay\n \n     @interfaces.legacy_get_updates_support\n     def get_updates(self, loss, params):\n@@ -713,7 +657,7 @@ class Nadam(Optimizer):\n         ms = [K.zeros(shape) for shape in shapes]\n         vs = [K.zeros(shape) for shape in shapes]\n \n-        self.weights = [self.iterations, self.m_schedule] + ms + vs\n+        self.weights = [self.iterations] + ms + vs\n \n         for p, g, m, v in zip(params, grads, ms, vs):\n             # the following equations given in [1]\n@@ -738,19 +682,11 @@ class Nadam(Optimizer):\n             self.updates.append(K.update(p, new_p))\n         return self.updates\n \n-    def set_weights(self, weights):\n-        params = self.weights\n-        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer\n-        # since it does not include iteration at head of the weight list. Set\n-        # iteration to 0.\n-        if len(params) == len(weights) + 1:\n-            weights = [weights[0]] + [np.array(1.)] + weights[1:]\n-        super(Nadam, self).set_weights(weights)\n-\n     def get_config(self):\n-        config = {'learning_rate': float(K.get_value(self.lr)),\n+        config = {'lr': float(K.get_value(self.lr)),\n                   'beta_1': float(K.get_value(self.beta_1)),\n                   'beta_2': float(K.get_value(self.beta_2)),\n+                  'epsilon': self.epsilon,\n                   'schedule_decay': self.schedule_decay}\n         base_config = super(Nadam, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -81,76 +81,35 @@ def test_no_grad():\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n-    sgd = optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.1, nesterov=True)\n-    _test_optimizer(sgd)\n \n \n def test_rmsprop():\n     _test_optimizer(optimizers.RMSprop())\n     _test_optimizer(optimizers.RMSprop(decay=1e-3))\n-    _test_optimizer(optimizers.RMSprop(learning_rate=0.1, epsilon=0.01, decay=0.1))\n-    _test_optimizer(optimizers.RMSprop(learning_rate=0.1, momentum=0.1))\n-    _test_optimizer(optimizers.RMSprop(learning_rate=0.1, momentum=0.1,\n-                                       centered=True))\n \n \n def test_adagrad():\n-    adagrad = optimizers.Adagrad(lr=0.01)\n-    _test_optimizer(adagrad)\n-    _test_optimizer(optimizers.Adagrad(lr=0.01, decay=1e-3))\n-    new_adagrad = optimizers.Adagrad(learning_rate=0.01,\n-                                     initial_accumulator_value=0.001,\n-                                     epsilon=0.001, decay=0.001)\n-    _test_optimizer(new_adagrad)\n-    # test backward compatibility for adding opt.iterations.\n-    assert len(adagrad.get_weights()) == len(new_adagrad.get_weights())\n-    new_adagrad.set_weights(adagrad.get_weights()[1:])\n-    _test_optimizer(new_adagrad)\n+    _test_optimizer(optimizers.Adagrad())\n+    _test_optimizer(optimizers.Adagrad(decay=1e-3))\n \n \n def test_adadelta():\n-    adadelta = optimizers.Adadelta(lr=1.0)\n-    _test_optimizer(adadelta, target=0.6)\n-    _test_optimizer(optimizers.Adadelta(lr=1.0, decay=1e-3), target=0.6)\n-    new_adadelta = optimizers.Adadelta(learning_rate=1.0,\n-                                       epsilon=0.001, decay=0.001)\n-    _test_optimizer(new_adadelta, target=0.6)\n-    # test backward compatibility for adding opt.iterations.\n-    assert len(adadelta.get_weights()) == len(new_adadelta.get_weights())\n-    new_adadelta.set_weights(adadelta.get_weights()[1:])\n-    _test_optimizer(new_adadelta, target=0.6)\n+    _test_optimizer(optimizers.Adadelta(), target=0.6)\n+    _test_optimizer(optimizers.Adadelta(decay=1e-3), target=0.6)\n \n \n def test_adam():\n-    adam = optimizers.Adam()\n-    _test_optimizer(adam)\n+    _test_optimizer(optimizers.Adam())\n     _test_optimizer(optimizers.Adam(decay=1e-3))\n-    new_adam = optimizers.Adam(learning_rate=0.1, epsilon=0.01, decay=0.1)\n-    _test_optimizer(new_adam)\n-    num_vars = int((len(adam.weights) - 1) / 2)\n-    ms = adam.weights[1:(num_vars + 1)]\n-    adam.weights.extend([K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in ms])\n-    new_adam.set_weights(adam.get_weights())\n-    _test_optimizer(new_adam)\n \n \n def test_adamax():\n-    _test_optimizer(optimizers.Adamax(lr=0.002))\n-    _test_optimizer(optimizers.Adamax(lr=0.002, decay=1e-3))\n-    _test_optimizer(optimizers.Adamax(learning_rate=0.002, epsilon=0.001,\n-                                      decay=0.001))\n+    _test_optimizer(optimizers.Adamax())\n+    _test_optimizer(optimizers.Adamax(decay=1e-3))\n \n \n def test_nadam():\n-    nadam = optimizers.Nadam()\n-    _test_optimizer(nadam)\n-    new_nadam = optimizers.Nadam(learning_rate=0.1, epsilon=0.01, schedule_decay=0.1)\n-    _test_optimizer(new_nadam)\n-    # test backward compatibility for adding opt.m_schedule.\n-    assert len(nadam.get_weights()) == len(new_nadam.get_weights())\n-    weights = nadam.get_weights()\n-    new_nadam.set_weights([weights[0]] + weights[1:])\n-    _test_optimizer(new_nadam)\n+    _test_optimizer(optimizers.Nadam())\n \n \n def test_adam_amsgrad():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eab1b5bcdf105746ede02d2eb8a5cb3ca359b1b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2065 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -840,7 +840,7 @@ def test_constant_initializer_with_numpy():\n     model_from_yaml(yaml_str).summary()\n \n \n-@pytest.mark.skipif(K.backend() == 'ctnk',\n+@pytest.mark.skipif(K.backend() == 'cntk',\n                     reason='Float64 not supported with CNTK.')\n def test_initialization_dtype():\n     class TestLayer(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d48e278f8470f7f6fb9aead5521684f618ebc07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 519 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 8 | Methods Changed: 22 | Complexity Δ (Sum/Max): 41/31 | Churn Δ: 522 | Churn Cumulative: 1036 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -172,15 +172,176 @@ class MeanSquaredError(LossFunctionWrapper):\n             mean_squared_error, name=name, reduction=reduction)\n \n \n+class MeanAbsoluteError(LossFunctionWrapper):\n+    \"\"\"Computes the mean of absolute difference between labels and predictions.\n+\n+    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n+    then the mean absolute error value is 3/4 (0.75).\n+\n+    Standalone usage:\n+\n+    ```python\n+    mae = keras.losses.MeanAbsoluteError()\n+    loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])\n+    ```\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.MeanAbsoluteError())\n+    ```\n+\n+    # Arguments\n+        reduction: (Optional) Type of loss reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: (Optional) name for the loss.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='mean_absolute_error'):\n+        super(MeanAbsoluteError, self).__init__(\n+            mean_absolute_error, name=name, reduction=reduction)\n+\n+\n+class MeanAbsolutePercentageError(LossFunctionWrapper):\n+    \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n+\n+    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n+    then the mean absolute percentage error value is 5e+08.\n+\n+    Standalone usage:\n+\n+    ```python\n+    mape = keras.losses.MeanAbsolutePercentageError()\n+    loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])\n+    ```\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.MeanAbsolutePercentageError())\n+    ```\n+\n+    # Arguments\n+        reduction: (Optional) Type of loss reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: (Optional) name for the loss.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='mean_absolute_percentage_error'):\n+        super(MeanAbsolutePercentageError, self).__init__(\n+            mean_absolute_percentage_error, name=name, reduction=reduction)\n+\n+\n+class MeanSquaredLogarithmicError(LossFunctionWrapper):\n+    \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n+\n+    For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n+    then the mean squared logarithmic error value is 0.36034.\n+\n+    Standalone usage:\n+\n+    ```python\n+    msle = keras.losses.MeanSquaredLogarithmicError()\n+    loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])\n+    ```\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.MeanSquaredLogarithmicError())\n+    ```\n+\n+    # Arguments\n+        reduction: (Optional) Type of loss reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: (Optional) name for the loss.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='mean_squared_logarithmic_error'):\n+        super(MeanSquaredLogarithmicError, self).__init__(\n+            mean_squared_logarithmic_error, name=name, reduction=reduction)\n+\n+\n+class BinaryCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the cross-entropy loss between true labels and predicted labels.\n+\n+    Use this cross-entropy loss when there are only two label classes (assumed to\n+    be 0 and 1). For each example, there should be a single floating-point value\n+    per prediction.\n+\n+    In the snippet below, each of the four examples has only a single\n+    floating-pointing value, and both `y_pred` and `y_true` have the shape\n+    `[batch_size]`.\n+\n+    Standalone usage:\n+\n+    ```python\n+    bce = keras.losses.BinaryCrossentropy()\n+    loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])\n+    ```\n+\n+    Usage with the `compile` API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss=keras.losses.BinaryCrossentropy())\n+    ```\n+\n+    # Arguments\n+        from_logits: Whether to interpret `y_pred` as a tensor of\n+            [logit](https://en.wikipedia.org/wiki/Logit) values. By default,\n+            we assume that `y_pred` contains probabilities\n+            (i.e., values in [0, 1]).\n+        label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When > 0, we\n+            compute the loss between the predicted labels and a smoothed version of\n+            the true labels, where the smoothing squeezes the labels towards 0.5.\n+            Larger values of `label_smoothing` correspond to heavier smoothing.\n+        reduction: (Optional) Type of loss reduction to apply to loss.\n+            Default value is `SUM_OVER_BATCH_SIZE`.\n+        name: (Optional) Name for the op.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 from_logits=False,\n+                 label_smoothing=0,\n+                 reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                 name='binary_crossentropy'):\n+        super(BinaryCrossentropy, self).__init__(\n+            binary_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing)\n+        self.from_logits = from_logits\n+\n+\n def mean_squared_error(y_true, y_pred):\n+    if not K.is_tensor(y_pred):\n+        y_pred = K.constant(y_pred)\n+    y_true = K.cast(y_true, y_pred.dtype)\n     return K.mean(K.square(y_pred - y_true), axis=-1)\n \n \n def mean_absolute_error(y_true, y_pred):\n+    if not K.is_tensor(y_pred):\n+        y_pred = K.constant(y_pred)\n+    y_true = K.cast(y_true, y_pred.dtype)\n     return K.mean(K.abs(y_pred - y_true), axis=-1)\n \n \n def mean_absolute_percentage_error(y_true, y_pred):\n+    if not K.is_tensor(y_pred):\n+        y_pred = K.constant(y_pred)\n+    y_true = K.cast(y_true, y_pred.dtype)\n     diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                             K.epsilon(),\n                                             None))\n@@ -188,6 +349,9 @@ def mean_absolute_percentage_error(y_true, y_pred):\n \n \n def mean_squared_logarithmic_error(y_true, y_pred):\n+    if not K.is_tensor(y_pred):\n+        y_pred = K.constant(y_pred)\n+    y_true = K.cast(y_true, y_pred.dtype)\n     first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n     second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n     return K.mean(K.square(first_log - second_log), axis=-1)\n@@ -235,8 +399,18 @@ def sparse_categorical_crossentropy(y_true, y_pred):\n     return K.sparse_categorical_crossentropy(y_true, y_pred)\n \n \n-def binary_crossentropy(y_true, y_pred):\n-    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n+def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):\n+    if not K.is_tensor(y_pred):\n+        y_pred = K.constant(y_pred)\n+    y_true = K.cast(y_true, y_pred.dtype)\n+    label_smoothing = K.cast_to_floatx(label_smoothing)\n+\n+    def _smooth_labels():\n+        return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n+\n+    y_true = K.switch(K.greater(label_smoothing, 0), _smooth_labels, lambda: y_true)\n+    return K.mean(\n+        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n \n \n def kullback_leibler_divergence(y_true, y_pred):\n\n@@ -167,7 +167,7 @@ class TestMeanSquaredError:\n         y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n         sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n         loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 587 / 6, rtol=1e-3)\n+        assert np.isclose(K.eval(loss), 97.833, rtol=1e-3)\n \n     def test_zero_weighted(self):\n         mse_obj = losses.MeanSquaredError()\n@@ -201,5 +201,347 @@ class TestMeanSquaredError:\n         assert np.isclose(K.eval(loss), 227.69998, rtol=1e-3)\n \n \n+class TestMeanAbsoluteError:\n+\n+    def test_config(self):\n+        mae_obj = losses.MeanAbsoluteError(\n+            reduction=losses_utils.Reduction.SUM, name='mae_1')\n+        assert mae_obj.name == 'mae_1'\n+        assert mae_obj.reduction == losses_utils.Reduction.SUM\n+\n+    def test_all_correct_unweighted(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mae_obj(y_true, y_true)\n+        assert np.isclose(K.eval(loss), 0.0, rtol=1e-3)\n+\n+    def test_unweighted(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mae_obj(y_true, y_pred)\n+        assert np.isclose(K.eval(loss), 5.5, rtol=1e-3)\n+\n+    def test_scalar_weighted(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.isclose(K.eval(loss), 12.65, rtol=1e-3)\n+\n+    def test_sample_weighted(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n+        loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.isclose(K.eval(loss), 81.4 / 6, rtol=1e-3)\n+\n+    def test_timestep_weighted(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n+        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n+        loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.isclose(K.eval(loss), 13.833, rtol=1e-3)\n+\n+    def test_zero_weighted(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mae_obj(y_true, y_pred, sample_weight=0)\n+        assert np.isclose(K.eval(loss), 0.0, rtol=1e-3)\n+\n+    def test_invalid_sample_weight(self):\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n+        sample_weight = K.constant([3, 6, 5, 0], shape=(2, 2))\n+        with pytest.raises(Exception):\n+            mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+    def test_no_reduction(self):\n+        mae_obj = losses.MeanAbsoluteError(\n+            reduction=losses_utils.Reduction.NONE)\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.allclose(K.eval(loss), [10.7333, 14.5666], rtol=1e-3)\n+\n+    def test_sum_reduction(self):\n+        mae_obj = losses.MeanAbsoluteError(\n+            reduction=losses_utils.Reduction.SUM)\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.isclose(K.eval(loss), 25.29999, rtol=1e-3)\n+\n+\n+class TestMeanAbsolutePercentageError:\n+\n+    def test_config(self):\n+        mape_obj = losses.MeanAbsolutePercentageError(\n+            reduction=losses_utils.Reduction.SUM, name='mape_1')\n+        assert mape_obj.name == 'mape_1'\n+        assert mape_obj.reduction == losses_utils.Reduction.SUM\n+\n+    def test_all_correct_unweighted(self):\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mape_obj(y_true, y_true)\n+        assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n+\n+    def test_unweighted(self):\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mape_obj(y_true, y_pred)\n+        assert np.allclose(K.eval(loss), 211.8518, rtol=1e-3)\n+\n+    def test_scalar_weighted(self):\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.allclose(K.eval(loss), 487.259, rtol=1e-3)\n+\n+    def test_sample_weighted(self):\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n+        loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(K.eval(loss), 422.8888, rtol=1e-3)\n+\n+    def test_timestep_weighted(self):\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n+        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n+        loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(K.eval(loss), 694.4445, rtol=1e-3)\n+\n+    def test_zero_weighted(self):\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mape_obj(y_true, y_pred, sample_weight=0)\n+        assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n+\n+    def test_no_reduction(self):\n+        mape_obj = losses.MeanAbsolutePercentageError(\n+            reduction=losses_utils.Reduction.NONE)\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.allclose(K.eval(loss), [621.8518, 352.6666], rtol=1e-3)\n+\n+\n+class TestMeanSquaredLogarithmicError:\n+\n+    def test_config(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError(\n+            reduction=losses_utils.Reduction .SUM, name='mape_1')\n+        assert msle_obj.name == 'mape_1'\n+        assert msle_obj.reduction == losses_utils.Reduction .SUM\n+\n+    def test_unweighted(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = msle_obj(y_true, y_pred)\n+        assert np.allclose(K.eval(loss), 1.4370, rtol=1e-3)\n+\n+    def test_scalar_weighted(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = msle_obj(y_true, y_pred, sample_weight=2.3)\n+        assert np.allclose(K.eval(loss), 3.3051, rtol=1e-3)\n+\n+    def test_sample_weighted(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n+        loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(K.eval(loss), 3.7856, rtol=1e-3)\n+\n+    def test_timestep_weighted(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n+        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n+        loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(K.eval(loss), 2.6473, rtol=1e-3)\n+\n+    def test_zero_weighted(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError()\n+        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n+        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n+        loss = msle_obj(y_true, y_pred, sample_weight=0)\n+        assert np.allclose(K.eval(loss), 0.0, rtol=1e-3)\n+\n+\n+class TestBinaryCrossentropy:\n+\n+    def test_config(self):\n+        bce_obj = losses.BinaryCrossentropy(\n+            reduction=losses_utils.Reduction.SUM, name='bce_1')\n+        assert bce_obj.name == 'bce_1'\n+        assert bce_obj.reduction == losses_utils.Reduction.SUM\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n+        bce_obj = losses.BinaryCrossentropy()\n+        loss = bce_obj(y_true, y_true)\n+        assert np.isclose(K.eval(loss), 0.0, rtol=1e-3)\n+\n+        # Test with logits.\n+        logits = K.constant([[100.0, -100.0, -100.0],\n+                             [-100.0, 100.0, -100.0],\n+                             [-100.0, -100.0, 100.0]])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits)\n+        assert np.isclose(K.eval(loss), 0.0, 3)\n+\n+    def test_unweighted(self):\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        bce_obj = losses.BinaryCrossentropy()\n+        loss = bce_obj(y_true, y_pred)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n+        # y` = clip(output, EPSILON, 1. - EPSILON)\n+        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n+\n+        # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n+        #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n+        #         -log(Y_MAX + EPSILON), -log(1)]\n+        #      = [0, 15.33, 0, 0]\n+        # Reduced loss = 15.33 / 4\n+\n+        assert np.isclose(K.eval(loss), 3.833, rtol=1e-3)\n+\n+        # Test with logits.\n+        y_true = K.constant([[1., 0., 1.], [0., 1., 1.]])\n+        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits)\n+\n+        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #            (where x = logits and z = y_true)\n+        #      = [((100 - 100 * 1 + log(1 + exp(-100))) +\n+        #          (0 + 100 * 0 + log(1 + exp(-100))) +\n+        #          (100 - 100 * 1 + log(1 + exp(-100))),\n+        #         ((100 - 100 * 0 + log(1 + exp(-100))) +\n+        #          (100 - 100 * 1 + log(1 + exp(-100))) +\n+        #          (0 + 100 * 1 + log(1 + exp(-100))))]\n+        #      = [(0 + 0 + 0) / 3, 200 / 3]\n+        # Reduced loss = (0 + 66.666) / 2\n+\n+        assert np.isclose(K.eval(loss), 33.333, rtol=1e-3)\n+\n+    def test_scalar_weighted(self):\n+        bce_obj = losses.BinaryCrossentropy()\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        loss = bce_obj(y_true, y_pred, sample_weight=2.3)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n+        # y` = clip(output, EPSILON, 1. - EPSILON)\n+        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n+\n+        # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n+        #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n+        #         -log(Y_MAX + EPSILON), -log(1)]\n+        #      = [0, 15.33, 0, 0]\n+        # Weighted loss = [0, 15.33 * 2.3, 0, 0]\n+        # Reduced loss = 15.33 * 2.3 / 4\n+\n+        assert np.isclose(K.eval(loss), 8.817, rtol=1e-3)\n+\n+        # Test with logits.\n+        y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n+        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits, sample_weight=2.3)\n+\n+        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #            (where x = logits and z = y_true)\n+        # Loss = [(0 + 0 + 0) / 3, 200 / 3]\n+        # Weighted loss = [0 * 2.3, 66.666 * 2.3]\n+        # Reduced loss = (0 + 66.666 * 2.3) / 2\n+\n+        assert np.isclose(K.eval(loss), 76.667, rtol=1e-3)\n+\n+    def test_sample_weighted(self):\n+        bce_obj = losses.BinaryCrossentropy()\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n+        loss = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n+        # y` = clip(output, EPSILON, 1. - EPSILON)\n+        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n+\n+        # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n+        #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n+        #         -log(Y_MAX + EPSILON), -log(1)]\n+        #      = [0, 15.33, 0, 0]\n+        # Reduced loss = 15.33 * 1.2 / 4\n+\n+        assert np.isclose(K.eval(loss), 4.6, rtol=1e-3)\n+\n+        # Test with logits.\n+        y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n+        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n+        weights = K.constant([4, 3])\n+        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n+        loss = bce_obj(y_true, logits, sample_weight=weights)\n+\n+        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #            (where x = logits and z = y_true)\n+        # Loss = [(0 + 0 + 0)/3, 200 / 3]\n+        # Weighted loss = [0 * 4, 66.666 * 3]\n+        # Reduced loss = (0 + 66.666 * 3) / 2\n+\n+        assert np.isclose(K.eval(loss), 100, rtol=1e-3)\n+\n+    def test_no_reduction(self):\n+        y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n+        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n+        bce_obj = losses.BinaryCrossentropy(\n+            from_logits=True, reduction=losses_utils.Reduction.NONE)\n+        loss = bce_obj(y_true, logits)\n+\n+        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #            (where x = logits and z = y_true)\n+        # Loss = [(0 + 0 + 0)/3, (200)/3]\n+\n+        assert np.allclose(K.eval(loss), (0., 66.6666), rtol=1e-3)\n+\n+    def test_label_smoothing(self):\n+        logits = K.constant([[100.0, -100.0, -100.0]])\n+        y_true = K.constant([[1, 0, 1]])\n+        label_smoothing = 0.1\n+        # Loss: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #            (where x = logits and z = y_true)\n+        # Label smoothing: z' = z * (1 - L) + 0.5L\n+        #                  1  = 1 - 0.5L\n+        #                  0  = 0.5L\n+        # Applying the above two fns to the given input:\n+        # (100 - 100 * (1 - 0.5 L)  + 0 +\n+        #  0   + 100 * (0.5 L)      + 0 +\n+        #  0   + 100 * (1 - 0.5 L)  + 0) * (1/3)\n+        #  = (100 + 50L) * 1/3\n+        bce_obj = losses.BinaryCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing)\n+        loss = bce_obj(y_true, logits)\n+        expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n+        assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
