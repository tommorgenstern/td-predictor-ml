{"custom_id": "scrapy#f796d8780c75543eadb6cf3689c7d0ca02896f0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 19 | Complexity Δ (Sum/Max): 19/12 | Churn Δ: 161 | Churn Cumulative: 2679 | Contributors (this commit): 33 | Commits (past 90d): 7 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import dataclasses\n import os\n import random\n import time\n+import warnings\n from datetime import datetime\n from io import BytesIO\n from pathlib import Path\n@@ -16,6 +17,7 @@ from itemadapter import ItemAdapter\n from twisted.internet import defer\n from twisted.trial import unittest\n \n+from scrapy import Spider\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n from scrapy.pipelines.files import (\n@@ -687,3 +689,63 @@ def _prepare_request_object(item_url, flags=None):\n         item_url,\n         meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n     )\n+\n+\n+# this is separate from the one in test_pipeline_media.py to specifically test FilesPipeline subclasses\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.crawler = get_crawler(Spider, {\"FILES_STORE\": self.tempdir})\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_simple(self):\n+        class Pipeline(FilesPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(FilesPipeline):\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = super().from_settings(settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+            assert pipe._from_settings_called\n+\n+    @pytest.mark.xfail(\n+        reason=\"No way to override MediaPipeline.from_crawler having non-trivial __init__\"\n+    )\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(FilesPipeline):\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                # you can either call super().from_crawler() or cls.__init__() but you need both\n+                o = cls(store_uri, settings=settings)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+            assert pipe._from_crawler_called\n\n@@ -1,5 +1,8 @@\n from __future__ import annotations\n \n+import warnings\n+\n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -410,3 +413,99 @@ class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n         )\n+\n+\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(Spider, {\"FILES_STORE\": \"/foo\"})\n+\n+    def test_simple(self):\n+        class Pipeline(UserDefinedPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(UserDefinedPipeline):\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = cls()\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_settings_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            def __init__(self, store_uri, settings):\n+                super().__init__()\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_settings_called\n+            assert pipe._init_called\n+\n+    @pytest.mark.xfail(\n+        reason=\"No way to override MediaPipeline.from_crawler having non-trivial __init__\"\n+    )\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            def __init__(self, store_uri, settings):\n+                super().__init__()\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                # you can either call super().from_crawler() or cls.__init__() but you need both\n+                o = cls(store_uri, settings=settings)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as super().from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler(self):\n+        class Pipeline(UserDefinedPipeline):\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                o = super().from_crawler(crawler)\n+                o._from_crawler_called = True\n+                o.store_uri = settings[\"FILES_STORE\"]\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#499e7e8aa685b2c8ba60576707e6fbf8ed8180ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 29 | Files Changed: 8 | Hunks: 23 | Methods Changed: 18 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 119 | Churn Cumulative: 2929 | Contributors (this commit): 45 | Commits (past 90d): 21 | Contributors (cumulative): 102 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,7 @@ from scrapy.core.downloader.tls import (\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -69,6 +70,31 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         method: int = SSL.SSLv23_METHOD,\n         *args: Any,\n         **kwargs: Any,\n+    ) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Self:\n+        return cls._from_settings(crawler.settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n     ) -> Self:\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\n\n@@ -1,9 +1,11 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -26,6 +28,15 @@ if TYPE_CHECKING:\n class BaseDupeFilter:\n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls()\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -72,17 +83,31 @@ class RFPDupeFilter(BaseDupeFilter):\n         *,\n         fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> Self:\n-        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n-        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, fingerprinter=fingerprinter)\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.request_fingerprinter\n-        return cls.from_settings(\n+        return cls._from_settings(\n             crawler.settings,\n             fingerprinter=crawler.request_fingerprinter,\n         )\n \n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        *,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n+    ) -> Self:\n+        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n+        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+\n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\n\n@@ -48,7 +48,7 @@ class MemoryUsage:\n         self.check_interval: float = crawler.settings.getfloat(\n             \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n         )\n-        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n+        self.mail: MailSender = MailSender.from_crawler(crawler)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n\n@@ -33,7 +33,7 @@ class StatsMailer:\n         recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        mail: MailSender = MailSender.from_settings(crawler.settings)\n+        mail: MailSender = MailSender.from_crawler(crawler)\n         assert crawler.stats\n         o = cls(crawler.stats, recipients, mail)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n\n@@ -7,6 +7,7 @@ See documentation in docs/topics/email.rst\n from __future__ import annotations\n \n import logging\n+import warnings\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n from email.mime.multipart import MIMEMultipart\n@@ -19,6 +20,7 @@ from typing import IO, TYPE_CHECKING, Any\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n@@ -32,6 +34,7 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -72,6 +75,19 @@ class MailSender:\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(\n             smtphost=settings[\"MAIL_HOST\"],\n             mailfrom=settings[\"MAIL_FROM\"],\n\n@@ -7,9 +7,10 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n+import warnings\n from typing import TYPE_CHECKING, Any\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n@@ -19,6 +20,7 @@ if TYPE_CHECKING:\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -31,6 +33,19 @@ class UrlLengthMiddleware:\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\n\n@@ -33,14 +33,6 @@ class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n         return df\n \n \n-class FromSettingsRFPDupeFilter(RFPDupeFilter):\n-    @classmethod\n-    def from_settings(cls, settings, *, fingerprinter=None):\n-        df = super().from_settings(settings, fingerprinter=fingerprinter)\n-        df.method = \"from_settings\"\n-        return df\n-\n-\n class DirectDupeFilter:\n     method = \"n/a\"\n \n@@ -56,16 +48,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n-    def test_df_from_settings_scheduler(self):\n-        settings = {\n-            \"DUPEFILTER_DEBUG\": True,\n-            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n-        }\n-        crawler = get_crawler(settings_dict=settings)\n-        scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, \"from_settings\")\n-\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\n\n@@ -3,7 +3,6 @@ from unittest import TestCase\n from testfixtures import LogCapture\n \n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n@@ -12,12 +11,10 @@ from scrapy.utils.test import get_crawler\n class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         self.maxlength = 25\n-        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n-\n-        crawler = get_crawler(Spider)\n+        crawler = get_crawler(Spider, {\"URLLENGTH_LIMIT\": self.maxlength})\n         self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n-        self.mw = UrlLengthMiddleware.from_settings(settings)\n+        self.mw = UrlLengthMiddleware.from_crawler(crawler)\n \n         self.response = Response(\"http://scrapytest.org\")\n         self.short_url_req = Request(\"http://scrapytest.org/\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eda1a8a7c5b3b61aedd5fcc2e3950b7af28a2926", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 448 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -65,13 +65,19 @@ class MiddlewareManager:\n \n     @classmethod\n     def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n-        if crawler is None:\n         warnings.warn(\n-                \"Calling MiddlewareManager.from_settings() without a Crawler instance is deprecated.\"\n-                \" As this method will be deprecated in the future, please switch to from_crawler().\",\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n             category=ScrapyDeprecationWarning,\n             stacklevel=2,\n         )\n+        return cls._from_settings(settings, crawler)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -102,10 +108,6 @@ class MiddlewareManager:\n         )\n         return cls(*middlewares)\n \n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler) -> Self:\n-        return cls.from_settings(crawler.settings, crawler)\n-\n     def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"open_spider\"):\n             self.methods[\"open_spider\"].append(mw.open_spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8700a5b7a92582fb1dc2a8fad7e41aa79948258b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 3093 | Contributors (this commit): 46 | Commits (past 90d): 16 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,6 +54,14 @@ class MiddlewareManager:\n     @staticmethod\n     def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n         if hasattr(objcls, \"from_settings\"):\n+            warnings.warn(\n+                f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n+                \" This is deprecated and calling from_settings() will be removed in a future\"\n+                \" Scrapy version. You can implement a simple from_crawler() that calls\"\n+                \" from_settings() with crawler.settings.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n             instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n             method_name = \"from_settings\"\n         else:\n\n@@ -185,6 +185,14 @@ def build_from_crawler(\n         instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_crawler\"\n     elif hasattr(objcls, \"from_settings\"):\n+        warnings.warn(\n+            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n+            \" This is deprecated and calling from_settings() will be removed in a future\"\n+            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n+            \" from_settings() with crawler.settings.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n\n@@ -8,6 +8,7 @@ from weakref import WeakKeyDictionary\n \n import pytest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -384,6 +385,8 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n             crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6aa4d2b4ab28b8f657645de613295ea2498e8cee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 16 | Files Changed: 6 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 8/3 | Churn Δ: 61 | Churn Cumulative: 8237 | Contributors (this commit): 55 | Commits (past 90d): 34 | Contributors (cumulative): 153 | DMM Complexity: 0.0\n\nDIFF:\n@@ -453,7 +453,17 @@ class FilesPipeline(MediaPipeline):\n         if not store_uri:\n             raise NotConfigured\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {self.__class__.__qualname__}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         cls_name = \"FilesPipeline\"\n         self.store: FilesStoreProtocol = self._get_store(store_uri)\n@@ -473,7 +483,9 @@ class FilesPipeline(MediaPipeline):\n         )\n \n         super().__init__(\n-            download_func=download_func, settings=settings, crawler=crawler\n+            download_func=download_func,\n+            settings=settings if not crawler else None,\n+            crawler=crawler,\n         )\n \n     @classmethod\n@@ -526,7 +538,7 @@ class FilesPipeline(MediaPipeline):\n \n         store_uri = settings[\"FILES_STORE\"]\n         if \"crawler\" in get_func_args(cls.__init__):\n-            o = cls(store_uri, settings=settings, crawler=crawler)\n+            o = cls(store_uri, crawler=crawler)\n         else:\n             o = cls(store_uri, settings=settings)\n             if crawler:\n\n@@ -79,10 +79,23 @@ class ImagesPipeline(FilesPipeline):\n             )\n \n         super().__init__(\n-            store_uri, settings=settings, download_func=download_func, crawler=crawler\n+            store_uri,\n+            settings=settings if not crawler else None,\n+            download_func=download_func,\n+            crawler=crawler,\n         )\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {self.__class__.__qualname__}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n         resolve = functools.partial(\n@@ -140,7 +153,7 @@ class ImagesPipeline(FilesPipeline):\n \n         store_uri = settings[\"IMAGES_STORE\"]\n         if \"crawler\" in get_func_args(cls.__init__):\n-            o = cls(store_uri, settings=settings, crawler=crawler)\n+            o = cls(store_uri, crawler=crawler)\n         else:\n             o = cls(store_uri, settings=settings)\n             if crawler:\n\n@@ -81,7 +81,17 @@ class MediaPipeline(ABC):\n     ):\n         self.download_func = download_func\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {self.__class__.__qualname__}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n@@ -92,7 +102,6 @@ class MediaPipeline(ABC):\n         self._handle_statuses(self.allow_redirects)\n \n         if crawler:\n-            # TODO use crawler.settings\n             self._finish_init(crawler)\n             self._modern_init = True\n         else:\n\n@@ -755,7 +755,7 @@ class BuildFromCrawlerTestCase(unittest.TestCase):\n             def from_crawler(cls, crawler):\n                 settings = crawler.settings\n                 store_uri = settings[\"FILES_STORE\"]\n-                o = cls(store_uri, settings=settings, crawler=crawler)\n+                o = cls(store_uri, crawler=crawler)\n                 o._from_crawler_called = True\n                 return o\n \n\n@@ -13,7 +13,6 @@ from twisted.trial import unittest\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n from scrapy.pipelines.images import ImageException, ImagesPipeline\n-from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler\n \n skip_pillow: str | None\n@@ -392,10 +391,7 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         have different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        default_settings = Settings()\n-        default_sts_pipe = ImagesPipeline(\n-            self.tempdir, settings=default_settings, crawler=get_crawler(None)  # TODO\n-        )\n+        default_sts_pipe = ImagesPipeline(self.tempdir, crawler=get_crawler(None))\n         user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n\n@@ -378,8 +378,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n \n     def _assert_request_no3xx(self, pipeline_class, settings):\n-        crawler = get_crawler(None, settings)\n-        pipe = pipeline_class(settings=settings, crawler=crawler)  # TODO\n+        pipe = pipeline_class(crawler=get_crawler(None, settings))\n         request = Request(\"http://url\")\n         pipe._modify_media_request(request)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ad5f0c12bfafc66fda6eb71790f447d2f7b8c13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 47 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 68 | Churn Cumulative: 2916 | Contributors (this commit): 45 | Commits (past 90d): 13 | Contributors (cumulative): 62 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,7 +29,7 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWar\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\n-from scrapy.settings import Settings\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n from scrapy.utils.deprecate import method_is_overridden\n@@ -513,6 +513,23 @@ class FilesPipeline(MediaPipeline):\n \n     @classmethod\n     def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n+        store_uri = settings[\"FILES_STORE\"]\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{cls.__qualname__}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n+\n+    @classmethod\n+    def _update_stores(cls, settings: BaseSettings) -> None:\n         s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -536,20 +553,6 @@ class FilesPipeline(MediaPipeline):\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings[\"FILES_STORE\"]\n-        if \"crawler\" in get_func_args(cls.__init__):\n-            o = cls(store_uri, crawler=crawler)\n-        else:\n-            o = cls(store_uri, settings=settings)\n-            if crawler:\n-                o._finish_init(crawler)\n-            warnings.warn(\n-                f\"{cls.__qualname__}.__init__() doesn't take a crawler argument.\"\n-                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n-                category=ScrapyDeprecationWarning,\n-            )\n-        return o\n-\n     def _get_store(self, uri: str) -> FilesStoreProtocol:\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = \"file\"\n\n@@ -11,21 +11,14 @@ import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, cast\n+from typing import TYPE_CHECKING, Any\n \n from itemadapter import ItemAdapter\n \n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import (\n-    FileException,\n-    FilesPipeline,\n-    FTPFilesStore,\n-    GCSFilesStore,\n-    S3FilesStore,\n-    _md5sum,\n-)\n+from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n from scrapy.settings import Settings\n from scrapy.utils.python import get_func_args, to_bytes\n \n@@ -128,29 +121,7 @@ class ImagesPipeline(FilesPipeline):\n \n     @classmethod\n     def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n-        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n-        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n-        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n-        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n-        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n-        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n-        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n-        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n-        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n-\n-        gcs_store: type[GCSFilesStore] = cast(\n-            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n-        )\n-        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n-        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n-\n-        ftp_store: type[FTPFilesStore] = cast(\n-            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n-        )\n-        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n-        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n-\n+        cls._update_stores(settings)\n         store_uri = settings[\"IMAGES_STORE\"]\n         if \"crawler\" in get_func_args(cls.__init__):\n             o = cls(store_uri, crawler=crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#929d665a74333434c9cede7133ea4f0707dbf9a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 4244 | Contributors (this commit): 57 | Commits (past 90d): 29 | Contributors (cumulative): 97 | DMM Complexity: 0.0\n\nDIFF:\n@@ -54,14 +54,6 @@ class MiddlewareManager:\n     @staticmethod\n     def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n         if hasattr(objcls, \"from_settings\"):\n-            warnings.warn(\n-                f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n-                \" This is deprecated and calling from_settings() will be removed in a future\"\n-                \" Scrapy version. You can implement a simple from_crawler() that calls\"\n-                \" from_settings() with crawler.settings.\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n             instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n             method_name = \"from_settings\"\n         else:\n\n@@ -35,7 +35,7 @@ from scrapy.utils.datatypes import CaseInsensitiveDict\n from scrapy.utils.deprecate import method_is_overridden\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import get_func_args, to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n@@ -457,7 +457,7 @@ class FilesPipeline(MediaPipeline):\n             if settings is not None:\n                 warnings.warn(\n                     f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n-                    f\" when creating {self.__class__.__qualname__}. The settings instance will be ignored\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n                     f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n                     category=ScrapyDeprecationWarning,\n                     stacklevel=2,\n@@ -501,7 +501,7 @@ class FilesPipeline(MediaPipeline):\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         if method_is_overridden(cls, FilesPipeline, \"from_settings\"):\n             warnings.warn(\n-                f\"{cls.__name__} overrides FilesPipeline.from_settings().\"\n+                f\"{global_object_name(cls)} overrides FilesPipeline.from_settings().\"\n                 f\" This method is deprecated and won't be called in future Scrapy versions,\"\n                 f\" please update your code so that it overrides from_crawler() instead.\",\n                 category=ScrapyDeprecationWarning,\n@@ -522,7 +522,7 @@ class FilesPipeline(MediaPipeline):\n             if crawler:\n                 o._finish_init(crawler)\n             warnings.warn(\n-                f\"{cls.__qualname__}.__init__() doesn't take a crawler argument.\"\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n                 \" This is deprecated and the argument will be required in future Scrapy versions.\",\n                 category=ScrapyDeprecationWarning,\n             )\n\n@@ -20,7 +20,7 @@ from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n from scrapy.settings import Settings\n-from scrapy.utils.python import get_func_args, to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -82,7 +82,7 @@ class ImagesPipeline(FilesPipeline):\n             if settings is not None:\n                 warnings.warn(\n                     f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n-                    f\" when creating {self.__class__.__qualname__}. The settings instance will be ignored\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n                     f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n                     category=ScrapyDeprecationWarning,\n                     stacklevel=2,\n@@ -130,7 +130,7 @@ class ImagesPipeline(FilesPipeline):\n             if crawler:\n                 o._finish_init(crawler)\n             warnings.warn(\n-                f\"{cls.__qualname__}.__init__() doesn't take a crawler argument.\"\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n                 \" This is deprecated and the argument will be required in future Scrapy versions.\",\n                 category=ScrapyDeprecationWarning,\n             )\n\n@@ -28,7 +28,7 @@ from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import get_func_args\n+from scrapy.utils.python import get_func_args, global_object_name\n \n if TYPE_CHECKING:\n     from collections.abc import Callable\n@@ -85,7 +85,7 @@ class MediaPipeline(ABC):\n             if settings is not None:\n                 warnings.warn(\n                     f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n-                    f\" when creating {self.__class__.__qualname__}. The settings instance will be ignored\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n                     f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n                     category=ScrapyDeprecationWarning,\n                     stacklevel=2,\n@@ -107,7 +107,7 @@ class MediaPipeline(ABC):\n         else:\n             warnings.warn(\n                 f\"MediaPipeline.__init__() was called without the crawler argument\"\n-                f\" when creating {self.__class__.__qualname__}.\"\n+                f\" when creating {global_object_name(self.__class__)}.\"\n                 f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n                 category=ScrapyDeprecationWarning,\n                 stacklevel=2,\n@@ -154,7 +154,7 @@ class MediaPipeline(ABC):\n         else:\n             pipe = cls()\n             warnings.warn(\n-                f\"{cls.__qualname__}.__init__() doesn't take a crawler argument.\"\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n                 \" This is deprecated and the argument will be required in future Scrapy versions.\",\n                 category=ScrapyDeprecationWarning,\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bfcee452b0f90dc3c642604bb77cd37f22ac0af1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/7 | Churn Δ: 74 | Churn Cumulative: 1438 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,9 +38,7 @@ class Contract:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -51,13 +49,10 @@ class Contract:\n                     results.addError(self.testcase_pre, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_pre)\n-                finally:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(  # pylint: disable=return-in-finally\n-                        cast(Iterable[Any], iterate_spider_output(cb_result))\n-                    )\n+                return list(cast(Iterable[Any], iterate_spider_output(cb_result)))\n \n             request.callback = wrapper\n \n@@ -69,9 +64,7 @@ class Contract:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -86,8 +79,7 @@ class Contract:\n                     results.addError(self.testcase_post, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_post)\n-                finally:\n-                    return output  # pylint: disable=return-in-finally\n+                return output\n \n             request.callback = wrapper\n \n\n@@ -556,3 +556,61 @@ class ContractsManagerTest(unittest.TestCase):\n \n         requests = self.conman.from_spider(spider, self.results)\n         self.assertTrue(requests)\n+\n+\n+class CustomFailContractPreProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def pre_process(self, response):\n+        raise KeyboardInterrupt(\"Pre-process exception\")\n+\n+\n+class CustomFailContractPostProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def post_process(self, response):\n+        raise KeyboardInterrupt(\"Post-process exception\")\n+\n+\n+class CustomContractPrePostProcess(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n+\n+    def test_pre_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPreProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_pre_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Pre-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def test_post_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPostProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_post_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Post-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10089c6fe2028b879f9f60e9598fa580ef6a3e33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 14 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 96 | Churn Cumulative: 9844 | Contributors (this commit): 99 | Commits (past 90d): 39 | Contributors (cumulative): 190 | DMM Complexity: 1.0\n\nDIFF:\n@@ -184,9 +184,23 @@ class Crawler:\n         return None\n \n     def get_addon(self, cls: type[_T]) -> _T | None:\n+        \"\"\"Return the run-time instance of an :ref:`add-on <topics-addons>` of\n+        the specified class or a subclass, or ``None`` if none is found.\n+\n+        .. versionadded:: 2.12\n+        \"\"\"\n         return self._get_component(cls, self.addons.addons)\n \n     def get_downloader_middleware(self, cls: type[_T]) -> _T | None:\n+        \"\"\"Return the run-time instance of a :ref:`downloader middleware\n+        <topics-downloader-middleware>` of the specified class or a subclass,\n+        or ``None`` if none is found.\n+\n+        .. versionadded:: 2.12\n+\n+        This method can only be called after the crawl engine has been created,\n+        e.g. at signals :signal:`engine_started` or :signal:`spider_opened`.\n+        \"\"\"\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_downloader_middleware() can only be called after \"\n@@ -195,6 +209,16 @@ class Crawler:\n         return self._get_component(cls, self.engine.downloader.middleware.middlewares)\n \n     def get_extension(self, cls: type[_T]) -> _T | None:\n+        \"\"\"Return the run-time instance of an :ref:`extension\n+        <topics-extensions>` of the specified class or a subclass,\n+        or ``None`` if none is found.\n+\n+        .. versionadded:: 2.12\n+\n+        This method can only be called after the extension manager has been\n+        created, e.g. at signals :signal:`engine_started` or\n+        :signal:`spider_opened`.\n+        \"\"\"\n         if not self.extensions:\n             raise RuntimeError(\n                 \"Crawler.get_extension() can only be called after the \"\n@@ -203,6 +227,15 @@ class Crawler:\n         return self._get_component(cls, self.extensions.middlewares)\n \n     def get_item_pipeline(self, cls: type[_T]) -> _T | None:\n+        \"\"\"Return the run-time instance of a :ref:`item pipeline\n+        <topics-item-pipeline>` of the specified class or a subclass, or\n+        ``None`` if none is found.\n+\n+        .. versionadded:: 2.12\n+\n+        This method can only be called after the crawl engine has been created,\n+        e.g. at signals :signal:`engine_started` or :signal:`spider_opened`.\n+        \"\"\"\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_item_pipeline() can only be called after the \"\n@@ -211,6 +244,15 @@ class Crawler:\n         return self._get_component(cls, self.engine.scraper.itemproc.middlewares)\n \n     def get_spider_middleware(self, cls: type[_T]) -> _T | None:\n+        \"\"\"Return the run-time instance of a :ref:`spider middleware\n+        <topics-spider-middleware>` of the specified class or a subclass, or\n+        ``None`` if none is found.\n+\n+        .. versionadded:: 2.12\n+\n+        This method can only be called after the crawl engine has been created,\n+        e.g. at signals :signal:`engine_started` or :signal:`spider_opened`.\n+        \"\"\"\n         if not self.engine:\n             raise RuntimeError(\n                 \"Crawler.get_spider_middleware() can only be called after the \"\n\n@@ -692,8 +692,10 @@ class FeedExporter:\n     def _get_storage(\n         self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n-        feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n-        return build_from_crawler(feedcls, self.crawler, uri, feed_options=feed_options)\n+        \"\"\"Build a storage object for the specified *uri* with the specified\n+        *feed_options*.\"\"\"\n+        cls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n+        return build_from_crawler(cls, self.crawler, uri, feed_options=feed_options)\n \n     def _get_uri_params(\n         self,\n\n@@ -149,6 +149,15 @@ class MediaPipeline(ABC):\n         pipe: Self\n         if hasattr(cls, \"from_settings\"):\n             pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n+            warnings.warn(\n+                f\"{global_object_name(cls)} has from_settings() and either doesn't have\"\n+                \" from_crawler() or calls MediaPipeline.from_crawler() from it,\"\n+                \" so from_settings() was used to create the instance of it.\"\n+                \" This is deprecated and calling from_settings() will be removed\"\n+                \" in a future Scrapy version. Please move the initialization code into\"\n+                \" from_crawler() or __init__().\",\n+                category=ScrapyDeprecationWarning,\n+            )\n         elif \"crawler\" in get_func_args(cls.__init__):\n             pipe = cls(crawler=crawler)\n         else:\n@@ -249,7 +258,7 @@ class MediaPipeline(ABC):\n             # minimize cached information for failure\n             result.cleanFailure()\n             result.frames = []\n-            if twisted_version <= Version(\"twisted\", 24, 10, 0):\n+            if twisted_version < Version(\"twisted\", 24, 10, 0):\n                 result.stack = []  # type: ignore[method-assign]\n             # This code fixes a memory leak by avoiding to keep references to\n             # the Request and Response objects on the Media Pipeline cache.\n@@ -269,9 +278,6 @@ class MediaPipeline(ABC):\n             # To avoid keeping references to the Response and therefore Request\n             # objects on the Media Pipeline cache, we should wipe the context of\n             # the encapsulated exception when it is a StopIteration instance\n-            #\n-            # This problem does not occur in Python 2.7 since we don't have\n-            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n             context = getattr(result.value, \"__context__\", None)\n             if isinstance(context, StopIteration):\n                 result.value.__context__ = None\n\n@@ -177,6 +177,8 @@ def build_from_crawler(\n ) -> T:\n     \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n \n+    .. versionadded:: 2.12\n+\n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n \n     Raises ``TypeError`` if the resulting instance is ``None``.\n\n@@ -328,8 +328,7 @@ def without_none_values(\n \n \n def global_object_name(obj: Any) -> str:\n-    \"\"\"\n-    Return full name of a global object.\n+    \"\"\"Return the full import path of the given class.\n \n     >>> from scrapy import Request\n     >>> global_object_name(Request)\n\n@@ -454,9 +454,34 @@ class BuildFromCrawlerTestCase(unittest.TestCase):\n             pipe = Pipeline.from_crawler(self.crawler)\n             assert pipe.crawler == self.crawler\n             assert pipe._fingerprinter\n-            self.assertEqual(len(w), 1)\n+            self.assertEqual(len(w), 2)\n             assert pipe._from_settings_called\n \n+    def test_has_from_settings_and_from_crawler(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = cls()\n+                o._from_settings_called = True\n+                return o\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                o = super().from_crawler(crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._from_settings_called\n+            assert pipe._from_crawler_called\n+\n     def test_has_from_settings_and_init(self):\n         class Pipeline(UserDefinedPipeline):\n             _from_settings_called = False\n@@ -476,7 +501,7 @@ class BuildFromCrawlerTestCase(unittest.TestCase):\n             pipe = Pipeline.from_crawler(self.crawler)\n             assert pipe.crawler == self.crawler\n             assert pipe._fingerprinter\n-            self.assertEqual(len(w), 1)\n+            self.assertEqual(len(w), 2)\n             assert pipe._from_settings_called\n             assert pipe._init_called\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
