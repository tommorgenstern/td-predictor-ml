{"custom_id": "keras#8bb22223f23650a4e7983a3760e6a8c92c294919", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 11 | Files Changed: 7 | Hunks: 12 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 2562 | Contributors (this commit): 13 | Commits (past 90d): 14 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,7 @@ ds_combinations = tf.__internal__.distribute.combinations\n # TODO(b/188763034): Proceed to export the strategy combinations as public APIs.\n STRATEGIES = [\n     ds_combinations.default_strategy,\n-    ds_combinations.mirrored_strategy_with_cpu_1_and_2,\n+    ds_combinations.mirrored_strategy_with_two_cpus,\n     ds_combinations.mirrored_strategy_with_two_gpus,\n     ds_combinations.tpu_strategy,\n     ds_combinations.cloud_tpu_strategy,\n\n@@ -29,7 +29,7 @@ test_combinations = tf.__internal__.test.combinations\n # to API changes and backward-compatibility is not guaranteed.\n STRATEGIES = [\n     ds_combinations.default_strategy,\n-    ds_combinations.mirrored_strategy_with_cpu_1_and_2,\n+    ds_combinations.mirrored_strategy_with_two_cpus,\n     ds_combinations.mirrored_strategy_with_two_gpus,\n     ds_combinations.tpu_strategy,\n     ds_combinations.cloud_tpu_strategy,\n\n@@ -31,7 +31,7 @@ test_combinations = tf.__internal__.test.combinations\n # a DatasetCreator when training on a tf.data.Dataset.\n STRATEGIES = [\n     ds_combinations.default_strategy,\n-    ds_combinations.mirrored_strategy_with_cpu_1_and_2,\n+    ds_combinations.mirrored_strategy_with_two_cpus,\n     ds_combinations.mirrored_strategy_with_two_gpus,\n     ds_combinations.tpu_strategy,\n     ds_combinations.cloud_tpu_strategy,\n\n@@ -29,7 +29,7 @@ test_combinations = tf.__internal__.test.combinations\n # to API changes and backward-compatibility is not guaranteed.\n STRATEGIES = [\n     ds_combinations.default_strategy,\n-    ds_combinations.mirrored_strategy_with_cpu_1_and_2,\n+    ds_combinations.mirrored_strategy_with_two_cpus,\n     ds_combinations.mirrored_strategy_with_two_gpus,\n     # TODO(b/183044870) TPU strategies with soft placement do not yet work.\n     # ds_combinations.tpu_strategy,\n\n@@ -35,7 +35,7 @@ from keras.optimizers.legacy import rmsprop\n maybe_distribute = tf.__internal__.test.combinations.combine(\n     distribution=[\n         tf.__internal__.distribute.combinations.default_strategy,\n-        tf.__internal__.distribute.combinations.mirrored_strategy_with_cpu_1_and_2,  # noqa: E501\n+        tf.__internal__.distribute.combinations.mirrored_strategy_with_two_cpus,  # noqa: E501\n     ]\n )\n \n\n@@ -78,6 +78,7 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n                 tf.config.LogicalDeviceConfiguration(),\n             ],\n         )\n+        self.strategy = create_mirrored_strategy()\n \n     def _create_model_from_layer(self, layer, input_shapes):\n         inputs = [layers.Input(batch_input_shape=s) for s in input_shapes]\n@@ -269,7 +270,6 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n             input_shapes = [input_shape]\n         else:\n             input_shapes = input_shape\n-        strategy = create_mirrored_strategy()\n         f32_layer = f32_layer_fn()\n \n         # Create the layers\n@@ -281,12 +281,13 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n \n         # Compute per_replica_input_shapes for the distributed model\n         global_batch_size = input_shapes[0][0]\n-        assert global_batch_size % strategy.num_replicas_in_sync == 0, (\n+        assert global_batch_size % self.strategy.num_replicas_in_sync == 0, (\n             \"The number of replicas, %d, does not divide the global batch \"\n-            \"size of %d\" % (strategy.num_replicas_in_sync, global_batch_size)\n+            \"size of %d\"\n+            % (self.strategy.num_replicas_in_sync, global_batch_size)\n         )\n         per_replica_batch_size = (\n-            global_batch_size // strategy.num_replicas_in_sync\n+            global_batch_size // self.strategy.num_replicas_in_sync\n         )\n         per_replica_input_shapes = [\n             (per_replica_batch_size,) + s[1:] for s in input_shapes\n@@ -295,7 +296,7 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n         # Create the models\n         f32_model = self._create_model_from_layer(f32_layer, input_shapes)\n         mp_model = self._create_model_from_layer(mp_layer, input_shapes)\n-        with strategy.scope():\n+        with self.strategy.scope():\n             distributed_mp_model = self._create_model_from_layer(\n                 distributed_mp_layer, per_replica_input_shapes\n             )\n\n@@ -36,7 +36,7 @@ ds_combinations = tf.__internal__.distribute.combinations\n STRATEGIES = [\n     # TODO(b/202992598): Add PSS strategy once the XLA issues is resolved.\n     ds_combinations.one_device_strategy,\n-    ds_combinations.mirrored_strategy_with_cpu_1_and_2,\n+    ds_combinations.mirrored_strategy_with_two_cpus,\n     ds_combinations.mirrored_strategy_with_two_gpus,\n     ds_combinations.tpu_strategy,\n     ds_combinations.cloud_tpu_strategy,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d4444be69df71089cc6ca38c62325d7a079787e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 234 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -134,7 +134,7 @@ def maybe_preemption_handler_scope(model):\n \n     if getattr(model, \"_preemption_handler\", None):\n         preemption_checkpoint_scope = (\n-            model._preemption_handler._watch_error_scope()\n+            model._preemption_handler.watch_preemption_scope()\n         )\n     else:\n         preemption_checkpoint_scope = contextlib.nullcontext()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#118f27d3ed7bf776c9f6126543e9048416a77eae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 987 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@\n \"\"\"Contains AutoCastVariable, a variable which automatically casts itself.\"\"\"\n \n import threading\n+from typing import Optional\n \n import tensorflow.compat.v2 as tf\n \n@@ -184,6 +185,13 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         )\n         return tf.cast(val, self._cast_dtype)\n \n+    def __tf_tensor__(\n+        self,\n+        dtype: Optional[tf.dtypes.DType] = None,\n+        name: Optional[str] = None,\n+    ) -> tf.Tensor:\n+        return self._dense_var_to_tensor(dtype=dtype, name=name)\n+\n     def _should_act_as_resource_variable(self):\n         \"\"\"Pass resource_variable_ops.is_resource_variable check.\"\"\"\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e78b4ab9487a88a65163ac4e37418cccbbbc21e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 409 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -170,7 +170,7 @@ def has_fully_masked_sequence(mask):\n \n def is_cudnn_supported_inputs(mask, time_major, sequence_lengths):\n     if tf.sysconfig.get_build_info()[\"is_rocm_build\"]:\n-        if not time_major:\n+        if (not time_major) and (sequence_lengths is not None):\n             return False\n         if mask is not None:\n             return tf.reduce_all(mask)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2ce1cd802e93676d07be55fb78d5d324b1efbd2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 589 | Lines Deleted: 402 | Files Changed: 5 | Hunks: 3 | Methods Changed: 16 | Complexity Δ (Sum/Max): 9/71 | Churn Δ: 991 | Churn Cumulative: 1674 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 0.512396694214876\n\nDIFF:\n@@ -1,402 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-r\"\"\"Utils for EfficientNet models for Keras.\n-\n-Write weights from  ckpt file as in original repo\n-(https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)\n-to h5 file for keras implementation of the models.\n-\n-Usage:\n-\n-# use checkpoint efficientnet-b0/model.ckpt (can be downloaded from\n-# https://storage.googleapis.com/cloud-tpu-checkpoints/\n-#     efficientnet/ckptsaug/efficientnet-b0.tar.gz)\n-# to update weight without top layers, saving to efficientnetb0_notop.h5\n-python efficientnet_weight_update_util.py --model b0 --notop \\\n-    --ckpt efficientnet-b0/model.ckpt --o efficientnetb0_notop.h5\n-\n-# use checkpoint noisy_student_efficientnet-b3/model.ckpt (providing\n-# improved result for b3, can be downloaded from\n-# https://storage.googleapis.com/cloud-tpu-checkpoints/\n-#     efficientnet/noisystudent/noisy_student_efficientnet-b3.tar.gz)\n-# to update weight with top layers, saving to efficientnetb3_new.h5\n-python efficientnet_weight_update_util.py --model b3 --notop \\\n-    --ckpt noisy_student_efficientnet-b3/model.ckpt --o efficientnetb3_new.h5\n-\"\"\"\n-\n-import argparse\n-import warnings\n-\n-import tensorflow.compat.v2 as tf\n-from tensorflow.keras.applications import efficientnet\n-\n-from keras.utils import io_utils\n-\n-\n-def write_ckpt_to_h5(path_h5, path_ckpt, keras_model, use_ema=True):\n-    \"\"\"Map the weights in checkpoint file (tf) to h5 file (keras).\n-\n-    Args:\n-      path_h5: str, path to output hdf5 file to write weights loaded from ckpt\n-        files.\n-      path_ckpt: str, path to the ckpt files (e.g. 'efficientnet-b0/model.ckpt')\n-        that records efficientnet weights from original repo\n-        https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\n-      keras_model: keras model, built from keras.applications efficientnet\n-        functions (e.g. EfficientNetB0)\n-      use_ema: Bool, whether to use ExponentialMovingAverage result or not\n-    \"\"\"\n-    model_name_keras = keras_model.name\n-    model_name_tf = model_name_keras.replace(\"efficientnet\", \"efficientnet-\")\n-\n-    keras_weight_names = [w.name for w in keras_model.weights]\n-    tf_weight_names = get_variable_names_from_ckpt(path_ckpt)\n-\n-    keras_blocks = get_keras_blocks(keras_weight_names)\n-    tf_blocks = get_tf_blocks(tf_weight_names)\n-\n-    io_utils.print_msg(\"check variables match in each block\")\n-    for keras_block, tf_block in zip(keras_blocks, tf_blocks):\n-        check_match(\n-            keras_block,\n-            tf_block,\n-            keras_weight_names,\n-            tf_weight_names,\n-            model_name_tf,\n-        )\n-        io_utils.print_msg(f\"{tf_block} and {keras_block} match.\")\n-\n-    block_mapping = {x[0]: x[1] for x in zip(keras_blocks, tf_blocks)}\n-\n-    changed_weights = 0\n-    for w in keras_model.weights:\n-        if \"block\" in w.name:\n-            # example: 'block1a_dwconv/depthwise_kernel:0' -> 'block1a'\n-            keras_block = w.name.split(\"/\")[0].split(\"_\")[0]\n-            tf_block = block_mapping[keras_block]\n-            tf_name = keras_name_to_tf_name_block(\n-                w.name,\n-                keras_block=keras_block,\n-                tf_block=tf_block,\n-                use_ema=use_ema,\n-                model_name_tf=model_name_tf,\n-            )\n-        elif any(\n-            [x in w.name for x in [\"stem\", \"top\", \"predictions\", \"probs\"]]\n-        ):\n-            tf_name = keras_name_to_tf_name_stem_top(\n-                w.name, use_ema=use_ema, model_name_tf=model_name_tf\n-            )\n-        elif \"normalization\" in w.name:\n-            io_utils.print_msg(\n-                f\"Skipping variable {w.name}: normalization is a Keras \"\n-                \"preprocessing layer, which does not exist in the TF ckpt.\"\n-            )\n-            continue\n-        else:\n-            raise ValueError(f\"{w.name} failed to parse.\")\n-\n-        try:\n-            w_tf = tf.train.load_variable(path_ckpt, tf_name)\n-            if (w.value().numpy() != w_tf).any():\n-                w.assign(w_tf)\n-                changed_weights += 1\n-        except ValueError as e:\n-            if any([x in w.name for x in [\"top\", \"predictions\", \"probs\"]]):\n-                warnings.warn(\n-                    \"Fail to load top layer variable {}\"\n-                    \"from {} because of {}.\".format(w.name, tf_name, e),\n-                    stacklevel=2,\n-                )\n-            else:\n-                raise ValueError(f\"Fail to load {w.name} from {tf_name}\")\n-\n-    total_weights = len(keras_model.weights)\n-    io_utils.print_msg(f\"{changed_weights}/{total_weights} weights updated\")\n-    keras_model.save_weights(path_h5)\n-\n-\n-def get_variable_names_from_ckpt(path_ckpt, use_ema=True):\n-    \"\"\"Get list of tensor names from checkpoint.\n-\n-    Args:\n-      path_ckpt: str, path to the ckpt files\n-      use_ema: Bool, whether to use ExponentialMovingAverage result or not.\n-    Returns:\n-      List of variable names from checkpoint.\n-    \"\"\"\n-    v_all = tf.train.list_variables(path_ckpt)\n-\n-    # keep name only\n-    v_name_all = [x[0] for x in v_all]\n-\n-    if use_ema:\n-        v_name_all = [x for x in v_name_all if \"ExponentialMovingAverage\" in x]\n-    else:\n-        v_name_all = [\n-            x for x in v_name_all if \"ExponentialMovingAverage\" not in x\n-        ]\n-\n-    # remove util variables used for RMSprop\n-    v_name_all = [x for x in v_name_all if \"RMS\" not in x]\n-    return v_name_all\n-\n-\n-def get_tf_blocks(tf_weight_names):\n-    \"\"\"Extract the block names from list of full weight names.\"\"\"\n-    # Example: 'efficientnet-b0/blocks_0/conv2d/kernel' -> 'blocks_0'\n-    tf_blocks = {x.split(\"/\")[1] for x in tf_weight_names if \"block\" in x}\n-    # sort by number\n-    tf_blocks = sorted(tf_blocks, key=lambda x: int(x.split(\"_\")[1]))\n-    return tf_blocks\n-\n-\n-def get_keras_blocks(keras_weight_names):\n-    \"\"\"Extract the block names from list of full weight names.\"\"\"\n-    # example: 'block1a_dwconv/depthwise_kernel:0' -> 'block1a'\n-    keras_blocks = {x.split(\"_\")[0] for x in keras_weight_names if \"block\" in x}\n-    return sorted(keras_blocks)\n-\n-\n-def keras_name_to_tf_name_stem_top(\n-    keras_name, use_ema=True, model_name_tf=\"efficientnet-b0\"\n-):\n-    \"\"\"Mapping name in h5 to ckpt that is in stem or top (head).\n-\n-    we map name keras_name that points to a weight in h5 file\n-    to a name of weight in ckpt file.\n-\n-    Args:\n-      keras_name: str, the name of weight in the h5 file of keras implementation\n-      use_ema: Bool, use the ExponentialMovingAverage resuolt in ckpt or not\n-      model_name_tf: str, the name of model in ckpt.\n-\n-    Returns:\n-      String for the name of weight as in ckpt file.\n-\n-    Raises:\n-      KeyError: if we cannot parse the keras_name.\n-    \"\"\"\n-    if use_ema:\n-        ema = \"/ExponentialMovingAverage\"\n-    else:\n-        ema = \"\"\n-\n-    stem_top_dict = {\n-        \"probs/bias:0\": \"{}/head/dense/bias{}\",\n-        \"probs/kernel:0\": \"{}/head/dense/kernel{}\",\n-        \"predictions/bias:0\": \"{}/head/dense/bias{}\",\n-        \"predictions/kernel:0\": \"{}/head/dense/kernel{}\",\n-        \"stem_conv/kernel:0\": \"{}/stem/conv2d/kernel{}\",\n-        \"top_conv/kernel:0\": \"{}/head/conv2d/kernel{}\",\n-    }\n-    for x in stem_top_dict:\n-        stem_top_dict[x] = stem_top_dict[x].format(model_name_tf, ema)\n-\n-    # stem batch normalization\n-    for bn_weights in [\"beta\", \"gamma\", \"moving_mean\", \"moving_variance\"]:\n-        tf_name = \"{}/stem/tpu_batch_normalization/{}{}\".format(\n-            model_name_tf, bn_weights, ema\n-        )\n-        stem_top_dict[f\"stem_bn/{bn_weights}:0\"] = tf_name\n-\n-    # top / head batch normalization\n-    for bn_weights in [\"beta\", \"gamma\", \"moving_mean\", \"moving_variance\"]:\n-        tf_name = \"{}/head/tpu_batch_normalization/{}{}\".format(\n-            model_name_tf, bn_weights, ema\n-        )\n-        stem_top_dict[f\"top_bn/{bn_weights}:0\"] = tf_name\n-\n-    if keras_name in stem_top_dict:\n-        return stem_top_dict[keras_name]\n-    raise KeyError(f\"{keras_name} from h5 file cannot be parsed\")\n-\n-\n-def keras_name_to_tf_name_block(\n-    keras_name,\n-    keras_block=\"block1a\",\n-    tf_block=\"blocks_0\",\n-    use_ema=True,\n-    model_name_tf=\"efficientnet-b0\",\n-):\n-    \"\"\"Mapping name in h5 to ckpt that belongs to a block.\n-\n-    we map name keras_name that points to a weight in h5 file\n-    to a name of weight in ckpt file.\n-\n-    Args:\n-      keras_name: str, the name of weight in the h5 file of keras implementation\n-      keras_block: str, the block name for keras implementation (e.g. 'block1a')\n-      tf_block: str, the block name for tf implementation (e.g. 'blocks_0')\n-      use_ema: Bool, use the ExponentialMovingAverage resuolt in ckpt or not\n-      model_name_tf: str, the name of model in ckpt.\n-\n-    Returns:\n-      String for the name of weight as in ckpt file.\n-\n-    Raises:\n-      ValueError if keras_block does not show up in keras_name\n-    \"\"\"\n-\n-    if keras_block not in keras_name:\n-        raise ValueError(f\"block name {keras_block} not found in {keras_name}\")\n-\n-    # all blocks in the first group will not have expand conv and bn\n-    is_first_blocks = keras_block[5] == \"1\"\n-\n-    tf_name = [model_name_tf, tf_block]\n-\n-    # depthwide conv\n-    if \"dwconv\" in keras_name:\n-        tf_name.append(\"depthwise_conv2d\")\n-        tf_name.append(\"depthwise_kernel\")\n-\n-    # conv layers\n-    if is_first_blocks:\n-        # first blocks only have one conv2d\n-        if \"project_conv\" in keras_name:\n-            tf_name.append(\"conv2d\")\n-            tf_name.append(\"kernel\")\n-    else:\n-        if \"project_conv\" in keras_name:\n-            tf_name.append(\"conv2d_1\")\n-            tf_name.append(\"kernel\")\n-        elif \"expand_conv\" in keras_name:\n-            tf_name.append(\"conv2d\")\n-            tf_name.append(\"kernel\")\n-\n-    # squeeze expansion layers\n-    if \"_se_\" in keras_name:\n-        if \"reduce\" in keras_name:\n-            tf_name.append(\"se/conv2d\")\n-        elif \"expand\" in keras_name:\n-            tf_name.append(\"se/conv2d_1\")\n-\n-        if \"kernel\" in keras_name:\n-            tf_name.append(\"kernel\")\n-        elif \"bias\" in keras_name:\n-            tf_name.append(\"bias\")\n-\n-    # batch normalization layers\n-    if \"bn\" in keras_name:\n-        if is_first_blocks:\n-            if \"project\" in keras_name:\n-                tf_name.append(\"tpu_batch_normalization_1\")\n-            else:\n-                tf_name.append(\"tpu_batch_normalization\")\n-        else:\n-            if \"project\" in keras_name:\n-                tf_name.append(\"tpu_batch_normalization_2\")\n-            elif \"expand\" in keras_name:\n-                tf_name.append(\"tpu_batch_normalization\")\n-            else:\n-                tf_name.append(\"tpu_batch_normalization_1\")\n-\n-        for x in [\"moving_mean\", \"moving_variance\", \"beta\", \"gamma\"]:\n-            if x in keras_name:\n-                tf_name.append(x)\n-    if use_ema:\n-        tf_name.append(\"ExponentialMovingAverage\")\n-    return \"/\".join(tf_name)\n-\n-\n-def check_match(\n-    keras_block, tf_block, keras_weight_names, tf_weight_names, model_name_tf\n-):\n-    \"\"\"Check if the weights in h5 and ckpt match.\n-\n-    we match each name from keras_weight_names that is in keras_block\n-    and check if there is 1-1 correspondence to names from tf_weight_names\n-    that is in tf_block\n-\n-    Args:\n-      keras_block: str, the block name for keras implementation (e.g. 'block1a')\n-      tf_block: str, the block name for tf implementation (e.g. 'blocks_0')\n-      keras_weight_names: list of str, weight names in keras implementation\n-      tf_weight_names: list of str, weight names in tf implementation\n-      model_name_tf: str, the name of model in ckpt.\n-    \"\"\"\n-    names_from_keras = set()\n-    for x in keras_weight_names:\n-        if keras_block in x:\n-            y = keras_name_to_tf_name_block(\n-                x,\n-                keras_block=keras_block,\n-                tf_block=tf_block,\n-                model_name_tf=model_name_tf,\n-            )\n-            names_from_keras.add(y)\n-\n-    names_from_tf = set()\n-    for x in tf_weight_names:\n-        if tf_block in x and x.split(\"/\")[1].endswith(tf_block):\n-            names_from_tf.add(x)\n-\n-    names_missing = names_from_keras - names_from_tf\n-    if names_missing:\n-        raise ValueError(\n-            \"{} variables not found in checkpoint file: {}\".format(\n-                len(names_missing), names_missing\n-            )\n-        )\n-\n-    names_unused = names_from_tf - names_from_keras\n-    if names_unused:\n-        warnings.warn(\n-            \"{} variables from checkpoint file are not used: {}\".format(\n-                len(names_unused), names_unused\n-            ),\n-            stacklevel=2,\n-        )\n-\n-\n-if __name__ == \"__main__\":\n-    arg_to_model = {\n-        \"b0\": efficientnet.EfficientNetB0,\n-        \"b1\": efficientnet.EfficientNetB1,\n-        \"b2\": efficientnet.EfficientNetB2,\n-        \"b3\": efficientnet.EfficientNetB3,\n-        \"b4\": efficientnet.EfficientNetB4,\n-        \"b5\": efficientnet.EfficientNetB5,\n-        \"b6\": efficientnet.EfficientNetB6,\n-        \"b7\": efficientnet.EfficientNetB7,\n-    }\n-\n-    p = argparse.ArgumentParser(\n-        description=\"write weights from checkpoint to h5\"\n-    )\n-    p.add_argument(\n-        \"--model\",\n-        required=True,\n-        type=str,\n-        help=\"name of efficient model\",\n-        choices=arg_to_model.keys(),\n-    )\n-    p.add_argument(\n-        \"--notop\",\n-        action=\"store_true\",\n-        help=\"do not include top layers\",\n-        default=False,\n-    )\n-    p.add_argument(\"--ckpt\", required=True, type=str, help=\"checkpoint path\")\n-    p.add_argument(\n-        \"--output\", \"-o\", required=True, type=str, help=\"output (h5) file path\"\n-    )\n-    args = p.parse_args()\n-\n-    include_top = not args.notop\n-\n-    model = arg_to_model[args.model](include_top=include_top)\n-    write_ckpt_to_h5(args.output, args.ckpt, keras_model=model)\n\n\n\n@@ -0,0 +1,92 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Setup script for the Keras pip package.\"\"\"\n+\n+import os\n+\n+import setuptools\n+\n+DESCRIPTION = \"\"\"Keras is a deep learning API written in Python,\n+running on top of the machine learning platform TensorFlow.\n+\n+It was developed with a focus on enabling fast experimentation and\n+providing a delightful developer experience.\n+The purpose of Keras is to give an *unfair advantage* to any developer\n+looking to ship ML-powered apps.\n+\n+Keras is:\n+\n+-   **Simple** -- but not simplistic. Keras reduces developer *cognitive load*\n+    to free you to focus on the parts of the problem that really matter.\n+    Keras focuses on ease of use, debugging speed, code elegance & conciseness,\n+    maintainability, and deployability (via TFServing, TFLite, TF.js).\n+-   **Flexible** -- Keras adopts the principle of *progressive disclosure of\n+    complexity*: simple workflows should be quick and easy, while arbitrarily\n+    advanced workflows should be *possible* via a clear path that builds upon\n+    what you've already learned.\n+-   **Powerful** -- Keras provides industry-strength performance and\n+    scalability: it is used by organizations and companies including NASA,\n+    YouTube, and Waymo. That's right -- your YouTube recommendations are\n+    powered by Keras, and so is the world's most advanced driverless vehicle.\n+\"\"\"\n+\n+with open(os.path.abspath(__file__)) as f:\n+    contents = f.read()\n+    if contents.count(\"{PACKAGE}\") > 1 or contents.count(\"{VERSION}\") > 1:\n+        raise ValueError(\n+            \"You must fill the 'PACKAGE' and 'VERSION' \"\n+            \"tags before running setup.py. If you are trying to \"\n+            \"build a fresh package, you should be using \"\n+            \"`pip_build.py` instead of `setup.py`.\"\n+        )\n+\n+setuptools.setup(\n+    name=\"{{PACKAGE}}\",\n+    # Version strings with `-` characters are semver compatible,\n+    # but incompatible with pip. For pip, we will remove all `-`` characters.\n+    version=\"{{VERSION}}\",\n+    description=\"Deep learning for humans.\",\n+    long_description=DESCRIPTION,\n+    url=\"https://keras.io/\",\n+    download_url=\"https://github.com/keras-team/keras/tags\",\n+    author=\"Keras team\",\n+    author_email=\"keras-users@googlegroups.com\",\n+    packages=setuptools.find_packages(),\n+    install_requires=[],\n+    # Supported Python versions\n+    python_requires=\">=3.8\",\n+    # PyPI package information.\n+    classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n+        \"Intended Audience :: Developers\",\n+        \"Intended Audience :: Education\",\n+        \"Intended Audience :: Science/Research\",\n+        \"License :: OSI Approved :: Apache Software License\",\n+        \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: 3.8\",\n+        \"Programming Language :: Python :: 3.9\",\n+        \"Programming Language :: Python :: 3.10\",\n+        \"Programming Language :: Python :: 3.11\",\n+        \"Programming Language :: Python :: 3 :: Only\",\n+        \"Topic :: Scientific/Engineering\",\n+        \"Topic :: Scientific/Engineering :: Mathematics\",\n+        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n+        \"Topic :: Software Development\",\n+        \"Topic :: Software Development :: Libraries\",\n+        \"Topic :: Software Development :: Libraries :: Python Modules\",\n+    ],\n+    license=\"Apache 2.0\",\n+    keywords=[\"keras\", \"tensorflow\", \"machine learning\", \"deep learning\"],\n+)\n\n@@ -0,0 +1,497 @@\n+\"\"\"Build the Keras pip package.\n+\n+The steps are as follows:\n+\n+0. Run bazel build in the Keras root directory to obtain protobuf Python files.\n+1. Create a temporary build directory (e.g. `/tmp/keras_build`)\n+2. Copy the Keras codebase to it (to `/tmp/keras_build/keras/src`)\n+  and rewrite internal imports so that they refer to `keras.src` rather than\n+  just `keras`.\n+3. Also copy `setup.py` to the build directory.\n+4. List and import every file in the codebase (in `/tmp/keras_build/keras/src`),\n+  so we can inspect the symbols the codebase contains.\n+5. Use the annotations left by the `keras_export` decorator to filter the\n+  symbols that should be exported, as well as their export path (default one\n+  and v1 one).\n+6. Use this information to generate `__init__.py` files in\n+  `tmp/keras_build/keras/`.\n+7. Run the setup script to write out build artifacts to `tmp/keras_build/dist`.\n+8. Copy the artifacts out. This is what should be uploaded to PyPI.\n+\n+This script borrows heavily from Namex (https://github.com/fchollet/namex).\n+\n+Notes:\n+\n+* This script should be run on the Keras codebase as obtained from GitHub\n+  (OSS-facing), not the Google-internal one. The files are expect to be already\n+  converted to their public form.\n+* This script only targets Linux x86 64. It could be adapted to MacOS\n+  relatively easily by changing requirements.txt and the bazel build script.\n+* This script should be run from an environment that has all Keras dependencies\n+  installed. Note that their specific version is not important; the only\n+  thing that matters is that we should be able to import the Keras codebase\n+  in its current state (so we can perform step 4). If you install the\n+  dependencies used by the latest TF-nightly you should be good.\n+\"\"\"\n+\n+import argparse\n+import datetime\n+import glob\n+import importlib\n+import inspect\n+import os\n+import pathlib\n+import shutil\n+import subprocess\n+import sys\n+import tempfile\n+\n+PACKAGE_NAME = \"keras\"\n+DIST_DIRNAME = \"dist\"\n+SRC_DIRNAME = \"src\"\n+TMP_BUILD_DIRNAME = \"keras_build\"\n+TMP_TEST_DIRNAME = \"keras_test\"\n+VERBOSE = True\n+INIT_FILE_HEADER = \"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\n+# These are symbols that have export issues and that we skip for now.\n+SYMBOLS_TO_SKIP = [\"layer_test\"]\n+\n+\n+def copy_keras_codebase(source_dir, target_dir):\n+    disallowed = [\n+        \"benchmarks\",\n+        \"tools\",\n+        \"tests\",\n+        \"integration_test\",\n+    ]\n+\n+    def ignore(path, names):\n+        to_ignore = []\n+        for name in names:\n+            if name.endswith(\"_test.py\"):\n+                to_ignore.append(name)\n+            elif name in disallowed:\n+                to_ignore.append(name)\n+        return to_ignore\n+\n+    shutil.copytree(source_dir, target_dir, ignore=ignore)\n+\n+\n+def convert_keras_imports(src_directory):\n+    def _convert_line(line):\n+        if \"import keras.protobuf\" in line or \"from keras.protobuf\" in line:\n+            return line\n+        # Imports starting from `root_name`.\n+        if line.strip() == f\"import {PACKAGE_NAME}\":\n+            line = line.replace(\n+                f\"import {PACKAGE_NAME}\",\n+                f\"import {PACKAGE_NAME}.{SRC_DIRNAME} as {PACKAGE_NAME}\",\n+            )\n+            return line\n+\n+        line = line.replace(\n+            f\"import {PACKAGE_NAME}.\",\n+            f\"import {PACKAGE_NAME}.{SRC_DIRNAME}.\",\n+        )\n+        line = line.replace(\n+            f\"from {PACKAGE_NAME}.\",\n+            f\"from {PACKAGE_NAME}.{SRC_DIRNAME}.\",\n+        )\n+        line = line.replace(\n+            f\"from {PACKAGE_NAME} import\",\n+            f\"from {PACKAGE_NAME}.{SRC_DIRNAME} import\",\n+        )\n+        # A way to catch LazyLoader calls. Hacky.\n+        line = line.replace('globals(), \"keras.', 'globals(), \"keras.src.')\n+        return line\n+\n+    for root, _, files in os.walk(src_directory):\n+        for fname in files:\n+            if fname.endswith(\".py\") and not fname.endswith(\"_pb2.py\"):\n+                fpath = os.path.join(root, fname)\n+                if VERBOSE:\n+                    print(f\"...processing {fpath}\")\n+                with open(fpath) as f:\n+                    contents = f.read()\n+                lines = contents.split(\"\\n\")\n+                in_string = False\n+                new_lines = []\n+                for line in lines:\n+                    if line.strip().startswith('\"\"\"') or line.strip().endswith(\n+                        '\"\"\"'\n+                    ):\n+                        if line.count('\"') % 2 == 1:\n+                            in_string = not in_string\n+                    else:\n+                        line = _convert_line(line)\n+                    new_lines.append(line)\n+\n+                with open(fpath, \"w\") as f:\n+                    f.write(\"\\n\".join(new_lines) + \"\\n\")\n+\n+\n+def generate_keras_api_files(package_directory, src_directory):\n+    if VERBOSE:\n+        print(\"# Compiling codebase entry points.\")\n+\n+    codebase_walk_entry_points = []\n+    for root, _, files in os.walk(src_directory):\n+        for fname in files:\n+            parts = root.split(\"/\")\n+            parts = parts[parts.index(\"keras\") :]\n+            base_entry_point = \".\".join(parts)\n+            if fname == \"__init__.py\":\n+                codebase_walk_entry_points.append(base_entry_point)\n+            elif fname.endswith(\".py\") and not fname.endswith(\"_test.py\"):\n+                module_name = fname[:-3]\n+                codebase_walk_entry_points.append(\n+                    base_entry_point + \".\" + module_name\n+                )\n+\n+    # Import all Python modules found in the code directory.\n+    modules = []\n+    sys.path.insert(0, os.getcwd())\n+    for entry_point in codebase_walk_entry_points:\n+        if VERBOSE:\n+            print(f\"Load entry point: {entry_point}\")\n+        mod = importlib.import_module(entry_point, package=\".\")\n+        modules.append(mod)\n+\n+    if VERBOSE:\n+        print(\"# Compiling list of symbols to export.\")\n+\n+    # Populate list of all symbols to register.\n+    all_symbols = set()\n+    processed = set()\n+    from tensorflow.python.util import tf_decorator\n+\n+    for module in modules:\n+        for name in dir(module):\n+            if name in SYMBOLS_TO_SKIP:\n+                continue\n+            symbol = getattr(module, name)\n+\n+            # Get the real symbol behind any TF decorator\n+            try:\n+                _, symbol = tf_decorator.unwrap(symbol)\n+            except ModuleNotFoundError:\n+                # unwrap will not work on a ModuleSpec (which can't be\n+                # an API symbol anyway)\n+                continue\n+\n+            # Skip if already seen\n+            if id(symbol) in processed:\n+                continue\n+            processed.add(id(symbol))\n+\n+            try:\n+                if not hasattr(symbol, \"_keras_api_names\"):\n+                    continue\n+            except:  # noqa: E722\n+                if VERBOSE:\n+                    print(\n+                        f\"[!] Could not inspect symbol '{name}' from {module}.\"\n+                    )\n+                continue\n+            # If the symbol is a subclass of a non-registered symbol, skip it.\n+            skip = False\n+            try:\n+                classes = inspect.getmro(symbol)\n+                if len(classes) >= 2:\n+                    parents = classes[1:]\n+                    for p in parents:\n+                        if (\n+                            hasattr(p, \"_keras_api_names\")\n+                            and p._keras_api_names == symbol._keras_api_names\n+                        ):\n+                            skip = True\n+            except AttributeError:\n+                # getmro will error out on a non-class\n+                # (in which case there can be no subclassing issues).\n+                pass\n+            if not skip:\n+                all_symbols.add(symbol)\n+\n+    # Generate __init__ files content.\n+    if VERBOSE:\n+        print(\"# Processing export path data for each symbol.\")\n+    init_files_content = grab_symbol_metadata(all_symbols, is_v1=False)\n+    init_files_content_v1 = grab_symbol_metadata(all_symbols, is_v1=True)\n+\n+    if VERBOSE:\n+        print(\"# Writing out API files.\")\n+    write_out_api_files(\n+        init_files_content,\n+        target_dir=pathlib.Path(package_directory).parent.resolve(),\n+    )\n+    v1_path = os.path.join(package_directory, \"api\", \"_v1\")\n+    v2_path = os.path.join(package_directory, \"api\", \"_v2\")\n+    write_out_api_files(\n+        init_files_content,\n+        target_dir=v2_path,\n+        root_offset=[\"api\", \"_v2\", \"keras\"],\n+    )\n+    write_out_api_files(\n+        init_files_content_v1,\n+        target_dir=v1_path,\n+        root_offset=[\"api\", \"_v1\", \"keras\"],\n+    )\n+    # Add missing __init__ files in api dirs.\n+    with open(os.path.join(package_directory, \"api\", \"__init__.py\"), \"w\") as f:\n+        pass\n+    with open(os.path.join(v1_path, \"__init__.py\"), \"w\") as f:\n+        pass\n+    with open(os.path.join(v2_path, \"__init__.py\"), \"w\") as f:\n+        pass\n+\n+\n+def grab_symbol_metadata(all_symbols, is_v1=False):\n+    # init_files_content is a dict mapping a directory path to a list of\n+    # symbol metadata entries to populate the __init__ file for the directory.\n+    # Each entry is a dict with keys 'symbol' and 'export_name'.\n+    init_files_content = {}\n+    for symbol in all_symbols:\n+        if VERBOSE:\n+            print(f\"...processing symbol '{symbol.__name__}'\")\n+        if is_v1:\n+            api_names = symbol._keras_api_names_v1\n+        else:\n+            api_names = symbol._keras_api_names\n+        for export_path in api_names:\n+            export_modules = export_path.split(\".\")\n+            export_name = export_modules[-1]\n+            parent_path = os.path.join(*export_modules[:-1])\n+            if parent_path not in init_files_content:\n+                init_files_content[parent_path] = []\n+            init_files_content[parent_path].append(\n+                {\"symbol\": symbol, \"export_name\": export_name}\n+            )\n+            for i in range(1, len(export_modules[:-1])):\n+                intermediate_path = os.path.join(*export_modules[:i])\n+                if intermediate_path not in init_files_content:\n+                    init_files_content[intermediate_path] = []\n+                init_files_content[intermediate_path].append(\n+                    {\n+                        \"module\": export_modules[i],\n+                        \"location\": \".\".join(export_modules[:i]),\n+                    }\n+                )\n+    return init_files_content\n+\n+\n+def write_out_api_files(init_files_content, target_dir, root_offset=None):\n+    # Go over init_files_content, make dirs,\n+    # create __init__.py file, populate file with public symbol imports.\n+    root_offset = root_offset or []\n+    for path, contents in init_files_content.items():\n+        os.makedirs(os.path.join(target_dir, path), exist_ok=True)\n+        init_file_lines = []\n+        modules_included = set()\n+        for symbol_metadata in contents:\n+            if \"symbol\" in symbol_metadata:\n+                symbol = symbol_metadata[\"symbol\"]\n+                name = symbol_metadata[\"export_name\"]\n+                if name == symbol.__name__:\n+                    init_file_lines.append(\n+                        f\"from {symbol.__module__} import {symbol.__name__}\"\n+                    )\n+                else:\n+                    init_file_lines.append(\n+                        f\"from {symbol.__module__} \"\n+                        f\"import {symbol.__name__} as {name}\"\n+                    )\n+            elif \"module\" in symbol_metadata:\n+                if symbol_metadata[\"module\"] not in modules_included:\n+                    parts = path.split(\"/\")\n+                    parts = [parts[0]] + root_offset + parts[1:]\n+                    module_location = \".\".join(parts)\n+                    init_file_lines.append(\n+                        f\"from {module_location} \"\n+                        f\"import {symbol_metadata['module']}\"\n+                    )\n+                    modules_included.add(symbol_metadata[\"module\"])\n+\n+        init_path = os.path.join(target_dir, path, \"__init__.py\")\n+        if VERBOSE:\n+            print(f\"...writing {init_path}\")\n+        init_file_lines = sorted(init_file_lines)\n+        with open(init_path, \"w\") as f:\n+            contents = (\n+                f'\"\"\"{INIT_FILE_HEADER}\"\"\"\\n\\n'\n+                + \"\\n\".join(init_file_lines)\n+                + \"\\n\"\n+            )\n+            f.write(contents)\n+\n+\n+def build_pip_package(\n+    keras_root_directory,\n+    build_directory,\n+    package_directory,\n+    src_directory,\n+    dist_directory,\n+    is_nightly=False,\n+):\n+    # Build Keras with Bazel to get the protobuf .py files\n+    os.chdir(keras_root_directory)\n+    os.system(f\"sh {os.path.join('keras', 'tools', 'bazel_build.sh')}\")\n+    os.chdir(build_directory)\n+\n+    # Copy sources (`keras/` directory and setup files) to build directory\n+    copy_keras_codebase(\n+        os.path.join(keras_root_directory, \"keras\"), src_directory\n+    )\n+    shutil.copy(\n+        os.path.join(keras_root_directory, \"oss_setup.py\"),\n+        os.path.join(build_directory, \"setup.py\"),\n+    )\n+\n+    # Add blank __init__.py file at package root\n+    # to make the package directory importable.\n+    with open(os.path.join(package_directory, \"__init__.py\"), \"w\") as f:\n+        pass\n+\n+    # Move protobuf .py files to package root.\n+    shutil.rmtree(os.path.join(src_directory, \"protobuf\"))\n+    shutil.move(\n+        os.path.join(keras_root_directory, \"bazel-bin\", \"keras\", \"protobuf\"),\n+        package_directory,\n+    )\n+    # Add blank __init__.py file in protobuf dir.\n+    with open(\n+        os.path.join(package_directory, \"protobuf\", \"__init__.py\"), \"w\"\n+    ) as f:\n+        pass\n+\n+    # Convert imports from `keras.xyz` to `keras.src.xyz`.\n+    convert_keras_imports(src_directory)\n+\n+    # Generate API __init__.py files in `keras/`\n+    generate_keras_api_files(package_directory, src_directory)\n+\n+    # Make sure to export the __version__ string\n+    version = getattr(\n+        importlib.import_module(\"keras.src\", package=\".\"), \"__version__\"\n+    )\n+    if is_nightly:\n+        date = datetime.datetime.now()\n+        version += f\".dev{date.strftime('%Y%m%d%H')}\"\n+    with open(os.path.join(package_directory, \"__init__.py\")) as f:\n+        init_contents = f.read()\n+    with open(os.path.join(package_directory, \"__init__.py\"), \"w\") as f:\n+        f.write(init_contents + \"\\n\\n\" + f'__version__ = \"{version}\"\\n')\n+\n+    # Insert {{PACKAGE}} and {{VERSION}} strings in setup.py\n+    if is_nightly:\n+        package = PACKAGE_NAME + \"-nightly\"\n+    else:\n+        package = PACKAGE_NAME\n+    with open(os.path.join(build_directory, \"setup.py\")) as f:\n+        setup_contents = f.read()\n+    with open(os.path.join(build_directory, \"setup.py\"), \"w\") as f:\n+        setup_contents = setup_contents.replace(\"{{VERSION}}\", version)\n+        setup_contents = setup_contents.replace(\"{{PACKAGE}}\", package)\n+        f.write(setup_contents)\n+\n+    # Build the package\n+    os.system(\"python3 -m build\")\n+\n+    # Save the dist files generated by the build process\n+    saved_filenames = []\n+    for filename in glob.glob(os.path.join(build_directory, \"dist\", \"*.*\")):\n+        if VERBOSE:\n+            print(f\"Saving build artifact {filename}\")\n+        shutil.copy(filename, dist_directory)\n+        saved_filenames.append(filename)\n+    if VERBOSE:\n+        print(f\"Saved artifacts to {dist_directory}\")\n+    return saved_filenames, version\n+\n+\n+def test_wheel(wheel_path, expected_version, requirements_path):\n+    test_directory = os.path.join(tempfile.gettempdir(), TMP_TEST_DIRNAME)\n+    os.mkdir(test_directory)\n+    os.chdir(test_directory)\n+    symbols_to_check = [\n+        \"keras.layers\",\n+        \"keras.Input\",\n+        \"keras.__internal__\",\n+        \"keras.experimental\",\n+    ]\n+    checks = \";\".join(symbols_to_check)\n+    script = (\n+        \"#!/bin/bash\\n\"\n+        \"virtualenv kenv\\n\"\n+        f\"source {os.path.join('kenv', 'bin', 'activate')}\\n\"\n+        f\"pip3 install -r {requirements_path}\\n\"\n+        f\"pip3 install {wheel_path} --force-reinstall\\n\"\n+        f\"python3 -c 'import keras;{checks};print(keras.__version__)'\\n\"\n+    )\n+    try:\n+        # Check version is correct\n+        output = subprocess.check_output(script.encode(), shell=True)\n+        output = output.decode().rstrip().split(\"\\n\")[-1].strip()\n+        if not output == expected_version:\n+            raise ValueError(\n+                \"Incorrect version; expected \"\n+                f\"{expected_version} but received {output}\"\n+            )\n+    finally:\n+        shutil.rmtree(test_directory)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--nightly\", default=False, help=\"Whether this is for keras-nightly\"\n+    )\n+    args = parser.parse_args()\n+    is_nightly = args.nightly\n+\n+    build_directory = os.path.join(tempfile.gettempdir(), TMP_BUILD_DIRNAME)\n+    keras_root_directory = pathlib.Path(__file__).parent.resolve()\n+    dist_directory = os.path.join(keras_root_directory, DIST_DIRNAME)\n+    package_directory = os.path.join(build_directory, PACKAGE_NAME)\n+    src_directory = os.path.join(build_directory, PACKAGE_NAME, SRC_DIRNAME)\n+    if VERBOSE:\n+        print(\n+            \"Using:\\n\"\n+            f\"build_directory={build_directory}\\n\"\n+            f\"keras_root_directory={keras_root_directory}\\n\"\n+            f\"dist_directory={dist_directory}\\n\"\n+            f\"package_directory={package_directory}\\n\"\n+            f\"src_directory={src_directory}\\n\"\n+            f\"is_nightly={is_nightly}\"\n+        )\n+    if os.path.exists(build_directory):\n+        raise ValueError(f\"Directory already exists: {build_directory}\")\n+    os.mkdir(build_directory)\n+    os.mkdir(package_directory)\n+    if not os.path.exists(dist_directory):\n+        os.mkdir(dist_directory)\n+    try:\n+        saved_filenames, version = build_pip_package(\n+            keras_root_directory,\n+            build_directory,\n+            package_directory,\n+            src_directory,\n+            dist_directory,\n+            is_nightly,\n+        )\n+        wheel_filename = [f for f in saved_filenames if f.endswith(\".whl\")][0]\n+        if VERBOSE:\n+            print(\"Testing wheel artifact.\")\n+        test_wheel(\n+            wheel_path=os.path.join(dist_directory, wheel_filename),\n+            expected_version=version,\n+            requirements_path=os.path.join(\n+                keras_root_directory, \"requirements.txt\"\n+            ),\n+        )\n+        if VERBOSE:\n+            print(\"Test successful.\")\n+    finally:\n+        # Clean up: remove the build directory (no longer needed)\n+        if VERBOSE:\n+            print(f\"Deleting temp build directory at {build_directory}...\")\n+        shutil.rmtree(build_directory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dbe2925976c74eeac9447ce0c391e329f06f9ec9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 71 | Churn Cumulative: 306 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,11 +14,18 @@\n # ==============================================================================\n \"\"\"Tests for initializers.\"\"\"\n \n+import os\n+\n import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n+from keras import backend\n+from keras import layers\n+from keras import losses\n+from keras import models\n from keras.dtensor import dtensor_api as dtensor\n+from keras.dtensor import layout_map\n from keras.dtensor import optimizers\n from keras.dtensor import test_util\n \n@@ -26,6 +33,7 @@ from keras.dtensor import test_util\n class OptimizersTest(test_util.DTensorBaseTest):\n     def setUp(self):\n         super().setUp()\n+\n         global_ids = test_util.create_device_ids_array((2, 2))\n         local_device_ids = np.ravel(global_ids).tolist()\n         mesh_dict = {\n@@ -134,6 +142,69 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         all_names = [var._shared_name for var in optimizer_variables]\n         self.assertCountEqual(all_names, expect_variable_names)\n \n+    def test_embedding_lookup_backward_path(self):\n+        # See b/265441685 for more context.\n+        backend.enable_tf_random_generator()\n+        os.environ[\n+            \"DTENSOR_ENABLE_REPLICATED_SPMD_AS_DEFAULT_TF.RESOURCESCATTERADD\"\n+        ] = \"1\"\n+        # Build a small functional model with embedding layer, it contains\n+        # tf.gather ops which will trigger the _deduplicate_sparse_grad() code\n+        # path. tf.unique op will have a shape mismatch issue for dtensor.\n+        batch_size = 16\n+        seq_length = 10\n+        vocab_size = 100\n+        output_size = 8\n+\n+        def produce_data():\n+            inputs = tf.random.uniform(\n+                maxval=vocab_size,\n+                shape=(batch_size, seq_length),\n+                dtype=tf.int32,\n+            )\n+            label = tf.random.uniform(\n+                maxval=output_size, shape=(batch_size,), dtype=tf.int32\n+            )\n+            inputs = dtensor.copy_to_mesh(\n+                inputs, layout=dtensor.Layout.replicated(self.mesh, rank=2)\n+            )\n+            inputs = dtensor.relayout(\n+                inputs, dtensor.Layout.batch_sharded(self.mesh, \"X\", 2)\n+            )\n+            label = dtensor.copy_to_mesh(\n+                label, layout=dtensor.Layout.replicated(self.mesh, rank=1)\n+            )\n+            label = dtensor.relayout(\n+                label, dtensor.Layout.batch_sharded(self.mesh, \"X\", 1)\n+            )\n+            return inputs, label\n+\n+        with layout_map.LayoutMap(self.mesh).scope():\n+            inputs = layers.Input(shape=(seq_length,))\n+            x = layers.Embedding(vocab_size, 64)(inputs)\n+            x = layers.GlobalAveragePooling1D()(x)\n+            preds = layers.Dense(output_size, activation=\"softmax\")(x)\n+            model = models.Model(inputs, preds)\n+\n+        optimizer = optimizers.AdamW(mesh=self.mesh)\n+\n+        @tf.function\n+        def train_func(model, inputs, label, optimizer):\n+            with tf.GradientTape() as tape:\n+                output = model(inputs)\n+                loss = losses.sparse_categorical_crossentropy(label, output)\n+            optimizer.minimize(loss, model.variables, tape)\n+            return loss\n+\n+        # The error only happens across the batch, where the value of\n+        # tf.unique are different.\n+        input1, label1 = produce_data()\n+        train_func(model, input1, label1, optimizer)\n+        input2, label2 = produce_data()\n+        train_func(model, input2, label2, optimizer)\n+        # Assert nothing here, and expect the train_func can run properly with\n+        # different inputs.\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
