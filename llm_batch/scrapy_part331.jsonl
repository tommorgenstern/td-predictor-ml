{"custom_id": "scrapy#4bdaa54af4470582845206a9fa21779f80a4b123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 43 | Churn Cumulative: 494 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,22 @@\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.python import global_object_name, to_bytes\n from scrapy.utils.request import request_httprepr\n-from scrapy.utils.response import response_httprepr\n-from scrapy.utils.python import global_object_name\n+\n+from twisted.web import http\n+\n+\n+def get_header_size(headers):\n+    size = 0\n+    for key, value in headers.items():\n+        if isinstance(value, (list, tuple)):\n+            for v in value:\n+                size += len(b\": \") + len(key) + len(v)\n+    return size + len(b'\\r\\n') * (len(headers.keys()) - 1)\n+\n+\n+def get_status_size(response_status):\n+    return len(to_bytes(http.RESPONSES.get(response_status, b''))) + 15\n+    # resp.status + b\"\\r\\n\" + b\"HTTP/1.1 <100-599> \"\n \n \n class DownloaderStats:\n@@ -24,7 +39,8 @@ class DownloaderStats:\n     def process_response(self, request, response, spider):\n         self.stats.inc_value('downloader/response_count', spider=spider)\n         self.stats.inc_value(f'downloader/response_status_count/{response.status}', spider=spider)\n-        reslen = len(response_httprepr(response))\n+        reslen = len(response.body) + get_header_size(response.headers) + get_status_size(response.status) + 4\n+        # response.body + b\"\\r\\n\"+ response.header + b\"\\r\\n\" + response.status\n         self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n         return response\n \n\n@@ -14,6 +14,7 @@ from scrapy.http.response import Response\n \n from twisted.web import http\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.decorators import deprecated\n from w3lib import html\n \n \n@@ -51,6 +52,7 @@ def response_status_message(status: Union[bytes, float, int, str]) -> str:\n     return f'{status_int} {to_unicode(message)}'\n \n \n+@deprecated\n def response_httprepr(response: Response) -> bytes:\n     \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n     is provided only for reference, since it's not the exact stream of bytes\n\n@@ -1,8 +1,10 @@\n+from itertools import product\n from unittest import TestCase\n \n from scrapy.downloadermiddlewares.stats import DownloaderStats\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n+from scrapy.utils.response import response_httprepr\n from scrapy.utils.test import get_crawler\n \n \n@@ -37,6 +39,23 @@ class TestDownloaderStats(TestCase):\n         self.mw.process_response(self.req, self.res, self.spider)\n         self.assertStatsEqual('downloader/response_count', 1)\n \n+    def test_response_len(self):\n+        body = (b'', b'not_empty')  # empty/notempty body\n+        headers = ({}, {'lang': 'en'}, {'lang': 'en', 'User-Agent': 'scrapy'})  # 0 headers, 1h and 2h\n+        test_responses = [  # form test responses with all combinations of body/headers\n+            Response(\n+                url='scrapytest.org',\n+                status=200,\n+                body=r[0],\n+                headers=r[1]\n+            )\n+            for r in product(body, headers)\n+        ]\n+        for test_response in test_responses:\n+            self.crawler.stats.set_value('downloader/response_bytes', 0)\n+            self.mw.process_response(self.req, test_response, self.spider)\n+            self.assertStatsEqual('downloader/response_bytes', len(response_httprepr(test_response)))\n+\n     def test_process_exception(self):\n         self.mw.process_exception(self.req, MyException(), self.spider)\n         self.assertStatsEqual('downloader/exception_count', 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30d5779ea94ed1e9343a4590895a3f5e65e444b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 183 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 12 | Complexity Δ (Sum/Max): 27/24 | Churn Δ: 189 | Churn Cumulative: 5465 | Contributors (this commit): 38 | Commits (past 90d): 2 | Contributors (cumulative): 63 | DMM Complexity: 0.8376623376623377\n\nDIFF:\n@@ -11,14 +11,14 @@ import sys\n import warnings\n from datetime import datetime\n from tempfile import NamedTemporaryFile\n-from typing import Any, Optional, Tuple\n+from typing import Any, Callable, Optional, Tuple, Union\n from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n from zope.interface import implementer, Interface\n \n-from scrapy import signals\n+from scrapy import signals, Spider\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.utils.boto import is_botocore_available\n@@ -524,7 +524,12 @@ class FeedExporter:\n             raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n         return instance\n \n-    def _get_uri_params(self, spider, uri_params, slot=None):\n+    def _get_uri_params(\n+        self,\n+        spider: Spider,\n+        uri_params_function: Optional[Union[str, Callable[[dict, Spider], dict]]],\n+        slot: Optional[_FeedSlot] = None,\n+    ) -> dict:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n@@ -532,9 +537,18 @@ class FeedExporter:\n         params['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')\n         params['batch_time'] = utc_now.isoformat().replace(':', '-')\n         params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n-        uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n-        uripar_function(params, spider)\n-        return params\n+        original_params = params.copy()\n+        uripar_function = load_object(uri_params_function) if uri_params_function else lambda params, _: params\n+        new_params = uripar_function(params, spider)\n+        if new_params is None or original_params != params:\n+            warnings.warn(\n+                'Modifying the params dictionary in-place in the function defined in '\n+                'the FEED_URI_PARAMS setting or in the uri_params key of the FEEDS '\n+                'setting is deprecated. The function must return a new dictionary '\n+                'instead.',\n+                category=ScrapyDeprecationWarning\n+            )\n+        return new_params if new_params is not None else params\n \n     def _load_filter(self, feed_options):\n         # load the item filter if declared else load the default filter class\n\n@@ -2608,3 +2608,166 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n                     ),\n                 )\n             )\n+\n+\n+class URIParamsTest:\n+\n+    spider_name = \"uri_params_spider\"\n+\n+    def build_settings(self, uri='file:///tmp/foobar', uri_params=None):\n+        raise NotImplementedError\n+\n+    def test_default(self):\n+        settings = self.build_settings(\n+            uri='file:///tmp/%(name)s',\n+        )\n+        crawler = get_crawler(settings_dict=settings)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(self.spider_name)\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(\n+                str(item.message) for item in w\n+                if item.category is ScrapyDeprecationWarning\n+            )\n+            self.assertEqual(messages, tuple())\n+\n+        self.assertEqual(\n+            feed_exporter.slots[0].uri,\n+            f'file:///tmp/{self.spider_name}'\n+        )\n+\n+    def test_none(self):\n+        def uri_params(params, spider):\n+            pass\n+\n+        settings = self.build_settings(\n+            uri='file:///tmp/%(name)s',\n+            uri_params=uri_params,\n+        )\n+        crawler = get_crawler(settings_dict=settings)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(self.spider_name)\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(\n+                str(item.message) for item in w\n+                if item.category is ScrapyDeprecationWarning\n+            )\n+            self.assertEqual(\n+                messages,\n+                (\n+                    (\n+                        'Modifying the params dictionary in-place in the '\n+                        'function defined in the FEED_URI_PARAMS setting or '\n+                        'in the uri_params key of the FEEDS setting is '\n+                        'deprecated. The function must return a new '\n+                        'dictionary instead.'\n+                    ),\n+                )\n+            )\n+\n+        self.assertEqual(\n+            feed_exporter.slots[0].uri,\n+            f'file:///tmp/{self.spider_name}'\n+        )\n+\n+    def test_empty_dict(self):\n+        def uri_params(params, spider):\n+            return {}\n+\n+        settings = self.build_settings(\n+            uri='file:///tmp/%(name)s',\n+            uri_params=uri_params,\n+        )\n+        crawler = get_crawler(settings_dict=settings)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(self.spider_name)\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            with self.assertRaises(KeyError):\n+                feed_exporter.open_spider(spider)\n+            messages = tuple(\n+                str(item.message) for item in w\n+                if item.category is ScrapyDeprecationWarning\n+            )\n+            self.assertEqual(messages, tuple())\n+\n+    def test_params_as_is(self):\n+        def uri_params(params, spider):\n+            return params\n+\n+        settings = self.build_settings(\n+            uri='file:///tmp/%(name)s',\n+            uri_params=uri_params,\n+        )\n+        crawler = get_crawler(settings_dict=settings)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(self.spider_name)\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(\n+                str(item.message) for item in w\n+                if item.category is ScrapyDeprecationWarning\n+            )\n+            self.assertEqual(messages, tuple())\n+\n+        self.assertEqual(\n+            feed_exporter.slots[0].uri,\n+            f'file:///tmp/{self.spider_name}'\n+        )\n+\n+    def test_custom_param(self):\n+        def uri_params(params, spider):\n+            return {**params, 'foo': self.spider_name}\n+\n+        settings = self.build_settings(\n+            uri='file:///tmp/%(foo)s',\n+            uri_params=uri_params,\n+        )\n+        crawler = get_crawler(settings_dict=settings)\n+        feed_exporter = FeedExporter.from_crawler(crawler)\n+        spider = scrapy.Spider(self.spider_name)\n+        spider.crawler = crawler\n+        with warnings.catch_warnings(record=True) as w:\n+            feed_exporter.open_spider(spider)\n+            messages = tuple(\n+                str(item.message) for item in w\n+                if item.category is ScrapyDeprecationWarning\n+            )\n+            self.assertEqual(messages, tuple())\n+\n+        self.assertEqual(\n+            feed_exporter.slots[0].uri,\n+            f'file:///tmp/{self.spider_name}'\n+        )\n+\n+\n+class URIParamsSettingTest(URIParamsTest, unittest.TestCase):\n+\n+    def build_settings(self, uri='file:///tmp/foobar', uri_params=None):\n+        extra_settings = {}\n+        if uri_params:\n+            extra_settings['FEED_URI_PARAMS'] = uri_params\n+        return {\n+            'FEED_URI': uri,\n+            **extra_settings,\n+        }\n+\n+\n+class URIParamsFeedOptionTest(URIParamsTest, unittest.TestCase):\n+\n+    def build_settings(self, uri='file:///tmp/foobar', uri_params=None):\n+        options = {\n+            'format': 'jl',\n+        }\n+        if uri_params:\n+            options['uri_params'] = uri_params\n+        return {\n+            'FEEDS': {\n+                uri: options,\n+            },\n+        }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe43411bc4d0164a0f0ecc596c23b59c99d31f17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 19 | Churn Cumulative: 1110 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -375,9 +375,13 @@ class BaseSettings(MutableMapping):\n         return len(self.attributes)\n \n     def _to_dict(self):\n-        return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n+        return {self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n                 for k, v in self.items()}\n \n+    def _get_key(self, key_value):\n+        return (key_value if isinstance(key_value, (bool, float, int, str, type(None)))\n+                else str(key_value))\n+\n     def copy_to_dict(self):\n         \"\"\"\n         Make a copy of current settings and convert to a dict.\n\n@@ -64,3 +64,7 @@ class CmdlineTest(unittest.TestCase):\n         settingsdict = json.loads(settingsstr)\n         self.assertCountEqual(settingsdict.keys(), EXTENSIONS.keys())\n         self.assertEqual(200, settingsdict[EXT_PATH])\n+\n+    def test_pathlib_path_as_feeds_key(self):\n+        self.assertEqual(self._execute('settings', '--get', 'FEEDS'),\n+                         json.dumps({\"items.csv\": {\"format\": \"csv\", \"fields\": [\"price\", \"name\"]}}))\n\n@@ -1,5 +1,14 @@\n+from pathlib import Path\n+\n EXTENSIONS = {\n     'tests.test_cmdline.extensions.TestExtension': 0,\n }\n \n TEST1 = 'default'\n+\n+FEEDS = {\n+    Path('items.csv'): {\n+        'format': 'csv',\n+        'fields': ['price', 'name'],\n+    },\n+}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#55ae2109c95e497d4a730afeb9caf71aa78a7723", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 27 | Churn Cumulative: 1857 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,7 +6,6 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import json\n-import warnings\n from contextlib import suppress\n from typing import Generator, Tuple\n from urllib.parse import urljoin\n@@ -16,7 +15,6 @@ from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n                             http_content_type_encoding, resolve_encoding)\n from w3lib.html import strip_html5_whitespace\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n@@ -66,13 +64,6 @@ class TextResponse(Response):\n             or self._body_declared_encoding()\n         )\n \n-    def body_as_unicode(self):\n-        \"\"\"Return body as unicode\"\"\"\n-        warnings.warn('Response.body_as_unicode() is deprecated, '\n-                      'please use Response.text instead.',\n-                      ScrapyDeprecationWarning, stacklevel=2)\n-        return self.text\n-\n     def json(self):\n         \"\"\"\n         .. versionadded:: 2.2\n\n@@ -1,10 +1,8 @@\n import unittest\n from unittest import mock\n-from warnings import catch_warnings, filterwarnings\n \n from w3lib.encoding import resolve_encoding\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n                          XmlResponse, Headers)\n from scrapy.selector import Selector\n@@ -134,9 +132,6 @@ class BaseResponseTest(unittest.TestCase):\n         assert isinstance(response.text, str)\n         self._assert_response_encoding(response, encoding)\n         self.assertEqual(response.body, body_bytes)\n-        with catch_warnings():\n-            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n-            self.assertEqual(response.body_as_unicode(), body_unicode)\n         self.assertEqual(response.text, body_unicode)\n \n     def _assert_response_encoding(self, response, encoding):\n@@ -346,12 +341,6 @@ class TextResponseTest(BaseResponseTest):\n         original_string = unicode_string.encode('cp1251')\n         r1 = self.response_class('http://www.example.com', body=original_string, encoding='cp1251')\n \n-        # check body_as_unicode\n-        with catch_warnings():\n-            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n-            self.assertTrue(isinstance(r1.body_as_unicode(), str))\n-            self.assertEqual(r1.body_as_unicode(), unicode_string)\n-\n         # check response.text\n         self.assertTrue(isinstance(r1.text, str))\n         self.assertEqual(r1.text, unicode_string)\n@@ -683,13 +672,6 @@ class TextResponseTest(BaseResponseTest):\n         with self.assertRaises(ValueError):\n             response.follow_all(css='a[href*=\"example.com\"]', xpath='//a[contains(@href, \"example.com\")]')\n \n-    def test_body_as_unicode_deprecation_warning(self):\n-        with catch_warnings(record=True) as warnings:\n-            r1 = self.response_class(\"http://www.example.com\", body='Hello', encoding='utf-8')\n-            self.assertEqual(r1.body_as_unicode(), 'Hello')\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-\n     def test_json_response(self):\n         json_body = b\"\"\"{\"ip\": \"109.187.217.200\"}\"\"\"\n         json_response = self.response_class(\"http://www.example.com\", body=json_body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bbfa185664cef79299b48cb0ae22065439bb07fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 109 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 117 | Churn Cumulative: 2041 | Contributors (this commit): 47 | Commits (past 90d): 4 | Contributors (cumulative): 71 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,7 +13,7 @@ from xml.sax.saxutils import XMLGenerator\n from itemadapter import is_item, ItemAdapter\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.item import _BaseItem\n+from scrapy.item import Item\n from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n@@ -315,7 +315,7 @@ class PythonItemExporter(BaseItemExporter):\n         return serializer(value)\n \n     def _serialize_value(self, value):\n-        if isinstance(value, _BaseItem):\n+        if isinstance(value, Item):\n             return self.export_item(value)\n         elif is_item(value):\n             return dict(self._serialize_item(value))\n\n@@ -15,39 +15,11 @@ from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n \n-class _BaseItem(object_ref):\n-    \"\"\"\n-    Temporary class used internally to avoid the deprecation\n-    warning raised by isinstance checks using BaseItem.\n-    \"\"\"\n-    pass\n-\n-\n-class _BaseItemMeta(ABCMeta):\n-    def __instancecheck__(cls, instance):\n-        if cls is BaseItem:\n-            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n-                 ScrapyDeprecationWarning, stacklevel=2)\n-        return super().__instancecheck__(instance)\n-\n-\n-class BaseItem(_BaseItem, metaclass=_BaseItemMeta):\n-    \"\"\"\n-    Deprecated, please use :class:`scrapy.item.Item` instead\n-    \"\"\"\n-\n-    def __new__(cls, *args, **kwargs):\n-        if issubclass(cls, BaseItem) and not issubclass(cls, (Item, DictItem)):\n-            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n-                 ScrapyDeprecationWarning, stacklevel=2)\n-        return super().__new__(cls, *args, **kwargs)\n-\n-\n class Field(dict):\n     \"\"\"Container of field metadata\"\"\"\n \n \n-class ItemMeta(_BaseItemMeta):\n+class ItemMeta(ABCMeta):\n     \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n \n     .. _metaclass: https://realpython.com/python-metaclasses\n@@ -74,7 +46,7 @@ class ItemMeta(_BaseItemMeta):\n         return super().__new__(mcs, class_name, bases, new_attrs)\n \n \n-class DictItem(MutableMapping, BaseItem):\n+class DictItem(MutableMapping, object_ref):\n \n     fields: Dict[str, Field] = {}\n \n@@ -118,7 +90,7 @@ class DictItem(MutableMapping, BaseItem):\n     def __iter__(self):\n         return iter(self._values)\n \n-    __hash__ = BaseItem.__hash__\n+    __hash__ = object_ref.__hash__\n \n     def keys(self):\n         return self._values.keys()\n\n@@ -14,11 +14,11 @@ from w3lib.html import replace_entities\n \n from scrapy.utils.datatypes import LocalWeakReferencedCache\n from scrapy.utils.python import flatten, to_unicode\n-from scrapy.item import _BaseItem\n+from scrapy.item import Item\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n \n \n-_ITERABLE_SINGLE_VALUES = dict, _BaseItem, str, bytes\n+_ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n \n \n def arg_to_iter(arg):\n\n@@ -3,7 +3,7 @@ from unittest import mock\n from warnings import catch_warnings, filterwarnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.item import ABCMeta, _BaseItem, BaseItem, DictItem, Field, Item, ItemMeta\n+from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n \n \n class ItemTest(unittest.TestCase):\n@@ -318,79 +318,6 @@ class DictItemTest(unittest.TestCase):\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n \n-class BaseItemTest(unittest.TestCase):\n-\n-    def test_isinstance_check(self):\n-\n-        class SubclassedBaseItem(BaseItem):\n-            pass\n-\n-        class SubclassedItem(Item):\n-            pass\n-\n-        with catch_warnings():\n-            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n-            self.assertTrue(isinstance(BaseItem(), BaseItem))\n-            self.assertTrue(isinstance(SubclassedBaseItem(), BaseItem))\n-            self.assertTrue(isinstance(Item(), BaseItem))\n-            self.assertTrue(isinstance(SubclassedItem(), BaseItem))\n-\n-            # make sure internal checks using private _BaseItem class succeed\n-            self.assertTrue(isinstance(BaseItem(), _BaseItem))\n-            self.assertTrue(isinstance(SubclassedBaseItem(), _BaseItem))\n-            self.assertTrue(isinstance(Item(), _BaseItem))\n-            self.assertTrue(isinstance(SubclassedItem(), _BaseItem))\n-\n-    def test_deprecation_warning(self):\n-        \"\"\"\n-        Make sure deprecation warnings are logged whenever BaseItem is used,\n-        either instantiated or in an isinstance check\n-        \"\"\"\n-        with catch_warnings(record=True) as warnings:\n-            BaseItem()\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-\n-        with catch_warnings(record=True) as warnings:\n-\n-            class SubclassedBaseItem(BaseItem):\n-                pass\n-\n-            SubclassedBaseItem()\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-\n-        with catch_warnings(record=True) as warnings:\n-            self.assertFalse(isinstance(\"foo\", BaseItem))\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-\n-        with catch_warnings(record=True) as warnings:\n-            self.assertTrue(isinstance(BaseItem(), BaseItem))\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-\n-\n-class ItemNoDeprecationWarningTest(unittest.TestCase):\n-    def test_no_deprecation_warning(self):\n-        \"\"\"\n-        Make sure deprecation warnings are NOT logged whenever BaseItem subclasses are used.\n-        \"\"\"\n-        class SubclassedItem(Item):\n-            pass\n-\n-        with catch_warnings(record=True) as warnings:\n-            Item()\n-            SubclassedItem()\n-            _BaseItem()\n-            self.assertFalse(isinstance(\"foo\", _BaseItem))\n-            self.assertFalse(isinstance(\"foo\", Item))\n-            self.assertFalse(isinstance(\"foo\", SubclassedItem))\n-            self.assertTrue(isinstance(_BaseItem(), _BaseItem))\n-            self.assertTrue(isinstance(Item(), Item))\n-            self.assertTrue(isinstance(SubclassedItem(), SubclassedItem))\n-            self.assertEqual(len(warnings), 0)\n-\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c8c1edd43b04ce7a2d9b1da198af26ba271cb1d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 500 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n import unittest\n from unittest import mock\n-from warnings import catch_warnings, filterwarnings\n+from warnings import catch_warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n@@ -318,6 +318,5 @@ class DictItemTest(unittest.TestCase):\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n \n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fca49cca929de035fb5d179d83f7f79da22fd205", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 64 | Files Changed: 3 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 87 | Churn Cumulative: 1312 | Contributors (this commit): 30 | Commits (past 90d): 6 | Contributors (cumulative): 45 | DMM Complexity: 0.0\n\nDIFF:\n@@ -272,7 +272,6 @@ coverage_ignore_pyobjects = [\n     r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]',  # helper functions\n \n     # Never documented before, and deprecated now.\n-    r'^scrapy\\.item\\.DictItem$',\n     r'^scrapy\\.linkextractors\\.FilteringLinkExtractor$',\n \n     # Implementation detail of LxmlLinkExtractor\n\n@@ -9,9 +9,7 @@ from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n from typing import Dict\n-from warnings import warn\n \n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n \n@@ -46,15 +44,30 @@ class ItemMeta(ABCMeta):\n         return super().__new__(mcs, class_name, bases, new_attrs)\n \n \n-class DictItem(MutableMapping, object_ref):\n+class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n+    \"\"\"\n+    Base class for scraped items.\n \n-    fields: Dict[str, Field] = {}\n+    In Scrapy, an object is considered an ``item`` if it is an instance of either\n+    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n+    spider callback is evaluated, only instances of :class:`Item` or\n+    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n \n-    def __new__(cls, *args, **kwargs):\n-        if issubclass(cls, DictItem) and not issubclass(cls, Item):\n-            warn('scrapy.item.DictItem is deprecated, please use scrapy.item.Item instead',\n-                 ScrapyDeprecationWarning, stacklevel=2)\n-        return super().__new__(cls, *args, **kwargs)\n+    If you need instances of a custom class to be considered items by Scrapy,\n+    you must inherit from either :class:`Item` or :class:`dict`.\n+\n+    Items must declare :class:`Field` attributes, which are processed and stored\n+    in the ``fields`` attribute. This restricts the set of allowed field names\n+    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n+    Additionally, fields can be used to define metadata and control the way\n+    data is processed internally. Please refer to the :ref:`documentation\n+    about fields <topics-items-fields>` for additional information.\n+\n+    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n+    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n+    \"\"\"\n+\n+    fields: Dict[str, Field]\n \n     def __init__(self, *args, **kwargs):\n         self._values = {}\n@@ -105,27 +118,3 @@ class DictItem(MutableMapping, object_ref):\n         \"\"\"Return a :func:`~copy.deepcopy` of this item.\n         \"\"\"\n         return deepcopy(self)\n-\n-\n-class Item(DictItem, metaclass=ItemMeta):\n-    \"\"\"\n-    Base class for scraped items.\n-\n-    In Scrapy, an object is considered an ``item`` if it is an instance of either\n-    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n-    spider callback is evaluated, only instances of :class:`Item` or\n-    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n-\n-    If you need instances of a custom class to be considered items by Scrapy,\n-    you must inherit from either :class:`Item` or :class:`dict`.\n-\n-    Items must declare :class:`Field` attributes, which are processed and stored\n-    in the ``fields`` attribute. This restricts the set of allowed field names\n-    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n-    Additionally, fields can be used to define metadata and control the way\n-    data is processed internally. Please refer to the :ref:`documentation\n-    about fields <topics-items-fields>` for additional information.\n-\n-    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n-    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n-    \"\"\"\n\n@@ -1,9 +1,7 @@\n import unittest\n from unittest import mock\n-from warnings import catch_warnings\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n+from scrapy.item import ABCMeta, Field, Item, ItemMeta\n \n \n class ItemTest(unittest.TestCase):\n@@ -254,18 +252,6 @@ class ItemTest(unittest.TestCase):\n         item['tags'].append('tag2')\n         assert item['tags'] != copied_item['tags']\n \n-    def test_dictitem_deprecation_warning(self):\n-        \"\"\"Make sure the DictItem deprecation warning is not issued for\n-        Item\"\"\"\n-        with catch_warnings(record=True) as warnings:\n-            Item()\n-            self.assertEqual(len(warnings), 0)\n-\n-            class SubclassedItem(Item):\n-                pass\n-            SubclassedItem()\n-            self.assertEqual(len(warnings), 0)\n-\n \n class ItemMetaTest(unittest.TestCase):\n \n@@ -303,20 +289,5 @@ class ItemMetaClassCellRegression(unittest.TestCase):\n                 super().__init__(*args, **kwargs)\n \n \n-class DictItemTest(unittest.TestCase):\n-\n-    def test_deprecation_warning(self):\n-        with catch_warnings(record=True) as warnings:\n-            DictItem()\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-        with catch_warnings(record=True) as warnings:\n-            class SubclassedDictItem(DictItem):\n-                pass\n-            SubclassedDictItem()\n-            self.assertEqual(len(warnings), 1)\n-            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-\n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4bda0976b28917405f54c781afda5ea55b65b16b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 735 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,7 +123,7 @@ class CSVFeedSpider(Spider):\n         process_results methods for pre and post-processing purposes.\n         \"\"\"\n \n-        for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n+        for row in csviter(response, self.delimiter, self.headers, quotechar=self.quotechar):\n             ret = iterate_spider_output(self.parse_row(response, row))\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n\n@@ -21,6 +21,7 @@ from scrapy.spiders import (\n )\n from scrapy.linkextractors import LinkExtractor\n from scrapy.utils.test import get_crawler\n+from tests import get_testdata\n \n \n class SpiderTest(unittest.TestCase):\n@@ -167,6 +168,23 @@ class CSVFeedSpiderTest(SpiderTest):\n \n     spider_class = CSVFeedSpider\n \n+    def test_parse_rows(self):\n+        body = get_testdata('feeds', 'feed-sample6.csv')\n+        response = Response(\"http://example.org/dummy.csv\", body=body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name = \"test\"\n+            delimiter = \",\"\n+            quotechar = \"'\"\n+\n+            def parse_row(self, response, row):\n+                return row\n+\n+        spider = _CrawlSpider()\n+        rows = list(spider.parse_rows(response))\n+        assert rows[0] == {'id': '1', 'name': 'alpha', 'value': 'foobar'}\n+        assert len(rows) == 4\n+\n \n class CrawlSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e1cfc26dbdde5fc3035169884c4d1218844d0de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 104 | Churn Cumulative: 3078 | Contributors (this commit): 29 | Commits (past 90d): 4 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,10 +14,9 @@ from twisted.internet import defer, reactor, ssl\n from twisted.internet.task import deferLater\n from twisted.names import dns, error\n from twisted.names.server import DNSServerFactory\n-from twisted.web.resource import EncodingResourceWrapper, Resource\n+from twisted.web import resource, server\n from twisted.web.server import GzipEncoderFactory, NOT_DONE_YET, Site\n from twisted.web.static import File\n-from twisted.web.test.test_webclient import PayloadResource\n from twisted.web.util import redirectTo\n \n from scrapy.utils.python import to_bytes, to_unicode\n@@ -35,7 +34,70 @@ def getarg(request, name, default=None, type=None):\n         return default\n \n \n-class LeafResource(Resource):\n+# most of the following resources are copied from twisted.web.test.test_webclient\n+class ForeverTakingResource(resource.Resource):\n+    \"\"\"\n+    L{ForeverTakingResource} is a resource which never finishes responding\n+    to requests.\n+    \"\"\"\n+\n+    def __init__(self, write=False):\n+        resource.Resource.__init__(self)\n+        self._write = write\n+\n+    def render(self, request):\n+        if self._write:\n+            request.write(b\"some bytes\")\n+        return server.NOT_DONE_YET\n+\n+\n+class ErrorResource(resource.Resource):\n+    def render(self, request):\n+        request.setResponseCode(401)\n+        if request.args.get(b\"showlength\"):\n+            request.setHeader(b\"content-length\", b\"0\")\n+        return b\"\"\n+\n+\n+class NoLengthResource(resource.Resource):\n+    def render(self, request):\n+        return b\"nolength\"\n+\n+\n+class HostHeaderResource(resource.Resource):\n+    \"\"\"\n+    A testing resource which renders itself as the value of the host header\n+    from the request.\n+    \"\"\"\n+\n+    def render(self, request):\n+        return request.requestHeaders.getRawHeaders(b\"host\")[0]\n+\n+\n+class PayloadResource(resource.Resource):\n+    \"\"\"\n+    A testing resource which renders itself as the contents of the request body\n+    as long as the request body is 100 bytes long, otherwise which renders\n+    itself as C{\"ERROR\"}.\n+    \"\"\"\n+\n+    def render(self, request):\n+        data = request.content.read()\n+        contentLength = request.requestHeaders.getRawHeaders(b\"content-length\")[0]\n+        if len(data) != 100 or int(contentLength) != 100:\n+            return b\"ERROR\"\n+        return data\n+\n+\n+class BrokenDownloadResource(resource.Resource):\n+    def render(self, request):\n+        # only sends 3 bytes even though it claims to send 5\n+        request.setHeader(b\"content-length\", b\"5\")\n+        request.write(b\"abc\")\n+        return b\"\"\n+\n+\n+class LeafResource(resource.Resource):\n \n     isLeaf = True\n \n@@ -175,10 +237,10 @@ class ArbitraryLengthPayloadResource(LeafResource):\n         return request.content.read()\n \n \n-class Root(Resource):\n+class Root(resource.Resource):\n \n     def __init__(self):\n-        Resource.__init__(self)\n+        resource.Resource.__init__(self)\n         self.putChild(b\"status\", Status())\n         self.putChild(b\"follow\", Follow())\n         self.putChild(b\"delay\", Delay())\n@@ -187,7 +249,7 @@ class Root(Resource):\n         self.putChild(b\"raw\", Raw())\n         self.putChild(b\"echo\", Echo())\n         self.putChild(b\"payload\", PayloadResource())\n-        self.putChild(b\"xpayload\", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n+        self.putChild(b\"xpayload\", resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))\n         self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n         try:\n             from tests import tests_datadir\n\n@@ -15,8 +15,6 @@ from twisted.trial import unittest\n from twisted.web import resource, server, static, util\n from twisted.web._newclient import ResponseFailed\n from twisted.web.http import _DataLoss\n-from twisted.web.test.test_webclient import (ForeverTakingResource, HostHeaderResource,\n-                                             NoLengthResource, PayloadResource)\n from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.handlers import DownloadHandlers\n@@ -34,7 +32,15 @@ from scrapy.spiders import Spider\n from scrapy.utils.misc import create_instance\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler, skip_if_no_boto\n-from tests.mockserver import MockServer, ssl_context_factory, Echo\n+from tests.mockserver import (\n+    Echo,\n+    ForeverTakingResource,\n+    HostHeaderResource,\n+    MockServer,\n+    NoLengthResource,\n+    PayloadResource,\n+    ssl_context_factory,\n+)\n from tests.spiders import SingleRequestSpider\n \n \n\n@@ -21,14 +21,6 @@ except ImportError:\n from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n from twisted.internet.defer import inlineCallbacks\n-from twisted.web.test.test_webclient import (\n-    ForeverTakingResource,\n-    ErrorResource,\n-    NoLengthResource,\n-    HostHeaderResource,\n-    PayloadResource,\n-    BrokenDownloadResource,\n-)\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n@@ -36,7 +28,15 @@ from scrapy.http import Request, Headers\n from scrapy.settings import Settings\n from scrapy.utils.misc import create_instance\n from scrapy.utils.python import to_bytes, to_unicode\n-from tests.mockserver import ssl_context_factory\n+from tests.mockserver import (\n+    BrokenDownloadResource,\n+    ErrorResource,\n+    ForeverTakingResource,\n+    HostHeaderResource,\n+    NoLengthResource,\n+    PayloadResource,\n+    ssl_context_factory,\n+)\n \n \n def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2e2ffd0d162cfed5a2e82e9fb9472dbf233c919", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 129 | Lines Deleted: 60 | Files Changed: 12 | Hunks: 58 | Methods Changed: 18 | Complexity Δ (Sum/Max): 12/9 | Churn Δ: 189 | Churn Cumulative: 4878 | Contributors (this commit): 53 | Commits (past 90d): 16 | Contributors (cumulative): 166 | DMM Complexity: 0.7959183673469388\n\nDIFF:\n@@ -1,13 +1,13 @@\n import sys\n import os\n-import optparse\n+import argparse\n import cProfile\n import inspect\n import pkg_resources\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from scrapy.commands import ScrapyCommand\n+from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n@@ -123,8 +123,6 @@ def execute(argv=None, settings=None):\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n-    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),\n-                                   conflict_handler='resolve')\n     if not cmdname:\n         _print_commands(settings, inproject)\n         sys.exit(0)\n@@ -133,12 +131,14 @@ def execute(argv=None, settings=None):\n         sys.exit(2)\n \n     cmd = cmds[cmdname]\n-    parser.usage = f\"scrapy {cmdname} {cmd.syntax()}\"\n-    parser.description = cmd.long_desc()\n+    parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,\n+                                     usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n+                                     conflict_handler='resolve',\n+                                     description=cmd.long_desc())\n     settings.setdict(cmd.default_settings, priority='command')\n     cmd.settings = settings\n     cmd.add_options(parser)\n-    opts, args = parser.parse_args(args=argv[1:])\n+    opts, args = parser.parse_known_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n     cmd.crawler_process = CrawlerProcess(settings)\n\n@@ -2,7 +2,7 @@\n Base class for Scrapy commands\n \"\"\"\n import os\n-from optparse import OptionGroup\n+import argparse\n from typing import Any, Dict\n \n from twisted.python import failure\n@@ -59,22 +59,20 @@ class ScrapyCommand:\n         \"\"\"\n         Populate option parse with options available for this command\n         \"\"\"\n-        group = OptionGroup(parser, \"Global Options\")\n-        group.add_option(\"--logfile\", metavar=\"FILE\",\n+        group = parser.add_argument_group(title='Global Options')\n+        group.add_argument(\"--logfile\", metavar=\"FILE\",\n                            help=\"log file. if omitted stderr will be used\")\n-        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n+        group.add_argument(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n                            help=f\"log level (default: {self.settings['LOG_LEVEL']})\")\n-        group.add_option(\"--nolog\", action=\"store_true\",\n+        group.add_argument(\"--nolog\", action=\"store_true\",\n                            help=\"disable logging completely\")\n-        group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n+        group.add_argument(\"--profile\", metavar=\"FILE\", default=None,\n                            help=\"write python cProfile stats to FILE\")\n-        group.add_option(\"--pidfile\", metavar=\"FILE\",\n+        group.add_argument(\"--pidfile\", metavar=\"FILE\",\n                            help=\"write process ID to FILE\")\n-        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n+        group.add_argument(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                            help=\"set/override setting (may be repeated)\")\n-        group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n-\n-        parser.add_option_group(group)\n+        group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n \n     def process_options(self, args, opts):\n         try:\n@@ -114,13 +112,13 @@ class BaseRunSpiderCommand(ScrapyCommand):\n     \"\"\"\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n+        parser.add_argument(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                             help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n+        parser.add_argument(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n                             help=\"append scraped items to the end of FILE (use - for stdout)\")\n-        parser.add_option(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n+        parser.add_argument(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n                             help=\"dump scraped items into FILE, overwriting any existing file\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n+        parser.add_argument(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                             help=\"format to use for dumping items\")\n \n     def process_options(self, args, opts):\n@@ -137,3 +135,30 @@ class BaseRunSpiderCommand(ScrapyCommand):\n                 opts.overwrite_output,\n             )\n             self.settings.set('FEEDS', feeds, priority='cmdline')\n+\n+\n+class ScrapyHelpFormatter(argparse.HelpFormatter):\n+    \"\"\"\n+    Help Formatter for scrapy command line help messages.\n+    \"\"\"\n+    def __init__(self, prog, indent_increment=2, max_help_position=24, width=None):\n+        super().__init__(prog, indent_increment=indent_increment,\n+                         max_help_position=max_help_position, width=width)\n+\n+    def _join_parts(self, part_strings):\n+        parts = self.format_part_strings(part_strings)\n+        return super()._join_parts(parts)\n+\n+    def format_part_strings(self, part_strings):\n+        \"\"\"\n+        Underline and title case command line help message headers.\n+        \"\"\"\n+        if part_strings and part_strings[0].startswith(\"usage: \"):\n+            part_strings[0] = \"Usage\\n=====\\n  \" + part_strings[0][len('usage: '):]\n+        headings = [i for i in range(len(part_strings)) if part_strings[i].endswith(':\\n')]\n+        for index in headings[::-1]:\n+            char = '-' if \"Global Options\" in part_strings[index] else '='\n+            part_strings[index] = part_strings[index][:-2].title()\n+            underline = ''.join([\"\\n\", (char * len(part_strings[index])), \"\\n\"])\n+            part_strings.insert(index + 1, underline)\n+        return part_strings\n\n@@ -49,9 +49,9 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n+        parser.add_argument(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n                             help=\"only list contracts, without checking them\")\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n+        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", default=False, action='store_true',\n                             help=\"print contract tests for all spiders\")\n \n     def run(self, args, opts):\n\n@@ -26,10 +26,10 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\", help=\"use this spider\")\n-        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\",\n+        parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n+        parser.add_argument(\"--headers\", dest=\"headers\", action=\"store_true\",\n                             help=\"print response HTTP headers instead of body\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n+        parser.add_argument(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n                             help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def _print_headers(self, headers, prefix):\n\n@@ -44,15 +44,15 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n+        parser.add_argument(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n                             help=\"List available templates\")\n-        parser.add_option(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n+        parser.add_argument(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n                             help=\"Edit spider after creating it\")\n-        parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n+        parser.add_argument(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n                             help=\"Dump template to standard output\")\n-        parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n+        parser.add_argument(\"-t\", \"--template\", dest=\"template\", default=\"basic\",\n                             help=\"Uses a custom template.\")\n-        parser.add_option(\"--force\", dest=\"force\", action=\"store_true\",\n+        parser.add_argument(\"--force\", dest=\"force\", action=\"store_true\",\n                             help=\"If the spider already exists, overwrite it with the template\")\n \n     def run(self, args, opts):\n\n@@ -32,27 +32,27 @@ class Command(BaseRunSpiderCommand):\n \n     def add_options(self, parser):\n         BaseRunSpiderCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None,\n+        parser.add_argument(\"--spider\", dest=\"spider\", default=None,\n                             help=\"use this spider without looking for one\")\n-        parser.add_option(\"--pipelines\", action=\"store_true\",\n+        parser.add_argument(\"--pipelines\", action=\"store_true\",\n                             help=\"process items through pipelines\")\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n+        parser.add_argument(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n                             help=\"don't show links to follow (extracted requests)\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n+        parser.add_argument(\"--noitems\", dest=\"noitems\", action=\"store_true\",\n                             help=\"don't show scraped items\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n+        parser.add_argument(\"--nocolour\", dest=\"nocolour\", action=\"store_true\",\n                             help=\"avoid using pygments to colorize the output\")\n-        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n+        parser.add_argument(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\",\n                             help=\"use CrawlSpider rules to discover the callback\")\n-        parser.add_option(\"-c\", \"--callback\", dest=\"callback\",\n+        parser.add_argument(\"-c\", \"--callback\", dest=\"callback\",\n                             help=\"use this callback for parsing, instead looking for a callback\")\n-        parser.add_option(\"-m\", \"--meta\", dest=\"meta\",\n+        parser.add_argument(\"-m\", \"--meta\", dest=\"meta\",\n                             help=\"inject extra meta into the Request, it must be a valid raw json string\")\n-        parser.add_option(\"--cbkwargs\", dest=\"cbkwargs\",\n+        parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n                             help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n-        parser.add_option(\"-d\", \"--depth\", dest=\"depth\", type=\"int\", default=1,\n+        parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n                             help=\"maximum depth for parsing requests [default: %default]\")\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n+        parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                             help=\"print each depth level one by one\")\n \n     @property\n\n@@ -18,15 +18,15 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\",\n+        parser.add_argument(\"--get\", dest=\"get\", metavar=\"SETTING\",\n                             help=\"print raw setting value\")\n-        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n+        parser.add_argument(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n                             help=\"print setting value, interpreted as a boolean\")\n-        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n+        parser.add_argument(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n                             help=\"print setting value, interpreted as an integer\")\n-        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n+        parser.add_argument(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n                             help=\"print setting value, interpreted as a float\")\n-        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n+        parser.add_argument(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n                             help=\"print setting value, interpreted as a list\")\n \n     def run(self, args, opts):\n\n@@ -33,11 +33,11 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-c\", dest=\"code\",\n+        parser.add_argument(\"-c\", dest=\"code\",\n                             help=\"evaluate the code in the shell, print the result and exit\")\n-        parser.add_option(\"--spider\", dest=\"spider\",\n+        parser.add_argument(\"--spider\", dest=\"spider\",\n                             help=\"use this spider\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n+        parser.add_argument(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", default=False,\n                             help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def update_vars(self, vars):\n\n@@ -16,7 +16,7 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n+        parser.add_argument(\"--verbose\", \"-v\", dest=\"verbose\", action=\"store_true\",\n                             help=\"also display twisted/python/platform info (useful for bug reports)\")\n \n     def run(self, args, opts):\n\n@@ -1,3 +1,4 @@\n+import argparse\n from scrapy.commands import fetch\n from scrapy.utils.response import open_in_browser\n \n@@ -12,7 +13,7 @@ class Command(fetch.Command):\n \n     def add_options(self, parser):\n         super().add_options(parser)\n-        parser.remove_option(\"--headers\")\n+        parser.add_argument('--headers', help=argparse.SUPPRESS)\n \n     def _print_response(self, response, opts):\n         open_in_browser(response)\n\n@@ -1,6 +1,9 @@\n import os\n+import argparse\n from os.path import join, abspath, isfile, exists\n from twisted.internet import defer\n+from scrapy.commands import parse\n+from scrapy.settings import Settings\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.python import to_unicode\n@@ -239,3 +242,19 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n         with open(file_path, 'r') as f:\n             self.assertEqual(f.read(), content)\n+\n+    def test_parse_add_options(self):\n+        command = parse.Command()\n+        command.settings = Settings()\n+        parser = argparse.ArgumentParser(\n+            prog='scrapy', formatter_class=argparse.HelpFormatter,\n+            conflict_handler='resolve', prefix_chars='-'\n+        )\n+        command.add_options(parser)\n+        namespace = parser.parse_args(\n+            ['--verbose', '--nolinks', '-d', '2', '--spider', self.spider_name]\n+        )\n+        self.assertTrue(namespace.nolinks)\n+        self.assertEqual(namespace.depth, 2)\n+        self.assertEqual(namespace.spider, self.spider_name)\n+        self.assertTrue(namespace.verbose)\n\n@@ -1,6 +1,6 @@\n import inspect\n import json\n-import optparse\n+import argparse\n import os\n import platform\n import re\n@@ -23,7 +23,7 @@ from twisted.python.versions import Version\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.commands import ScrapyCommand\n+from scrapy.commands import view, ScrapyCommand, ScrapyHelpFormatter\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n@@ -37,19 +37,28 @@ class CommandSettings(unittest.TestCase):\n     def setUp(self):\n         self.command = ScrapyCommand()\n         self.command.settings = Settings()\n-        self.parser = optparse.OptionParser(\n-            formatter=optparse.TitledHelpFormatter(),\n-            conflict_handler='resolve',\n-        )\n+        self.parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,\n+                                              conflict_handler='resolve')\n         self.command.add_options(self.parser)\n \n     def test_settings_json_string(self):\n         feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n-        opts, args = self.parser.parse_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])\n+        opts, args = self.parser.parse_known_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])\n         self.command.process_options(args, opts)\n         self.assertIsInstance(self.command.settings['FEEDS'], scrapy.settings.BaseSettings)\n         self.assertEqual(dict(self.command.settings['FEEDS']), json.loads(feeds_json))\n \n+    def test_help_formatter(self):\n+        formatter = ScrapyHelpFormatter(prog='scrapy')\n+        part_strings = ['usage: scrapy genspider [options] <name> <domain>\\n\\n',\n+                        '\\n', 'optional arguments:\\n', '\\n', 'Global Options:\\n']\n+        self.assertEqual(\n+            formatter._join_parts(part_strings),\n+            ('Usage\\n=====\\n  scrapy genspider [options] <name> <domain>\\n\\n\\n'\n+             'Optional Arguments\\n==================\\n\\n'\n+             'Global Options\\n--------------\\n')\n+        )\n+\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -812,6 +821,21 @@ class BenchCommandTest(CommandTest):\n         self.assertNotIn('Unhandled Error', log)\n \n \n+class ViewCommandTest(CommandTest):\n+\n+    def test_methods(self):\n+        command = view.Command()\n+        command.settings = Settings()\n+        parser = argparse.ArgumentParser(prog='scrapy', prefix_chars='-',\n+                                         formatter_class=ScrapyHelpFormatter,\n+                                         conflict_handler='resolve')\n+        command.add_options(parser)\n+        self.assertEqual(command.short_desc(),\n+                         \"Open URL in browser, as seen by Scrapy\")\n+        self.assertIn(\"URL using the Scrapy downloader and show its\",\n+                      command.long_desc())\n+\n+\n class CrawlCommandTest(CommandTest):\n \n     def crawl(self, code, args=()):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
