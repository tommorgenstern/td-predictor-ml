{"custom_id": "scrapy#de0d57cd9b392480e0000df1d1ceec2595dc8067", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 19 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,4 +59,8 @@ def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n     message = message + '\\n' + format_exc()\n     msg(message, level, component, domain)\n \n-err = log.err\n+def err(*args, **kwargs):\n+    domain = kwargs.pop('domain', '')\n+    component = kwargs.pop('component', BOT_NAME)\n+    kwargs['system'] = \"%s/%s\" % (BOT_NAME, domain) if domain else component\n+    log.err(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d59bb315315e2e4012191918b33922f0ee02069", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 29 | Churn Cumulative: 853 | Contributors (this commit): 4 | Commits (past 90d): 26 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list, arg_to_iter\n+from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_iter\n from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n@@ -61,19 +61,6 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n         self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n \n-    def test_arg_to_list(self):\n-        self.assertEqual(arg_to_list(None), [])\n-        self.assertEqual(arg_to_list('lala'), ['lala'])\n-        self.assertEqual(arg_to_list(100), [100])\n-        self.assertEqual(arg_to_list(('lala', 'poo')), ['lala', 'poo'])\n-\n-    def test_arg_to_list(self):\n-        self.assertEqual(arg_to_list(None), [])\n-        self.assertEqual(arg_to_list('lala'), ['lala'])\n-        self.assertEqual(arg_to_list(100), [100])\n-        self.assertEqual(arg_to_list(('lala', 'poo')), ['lala', 'poo'])\n-        self.assertEqual(arg_to_list(s for s in ['lala', 'poo']), ['lala', 'poo'])\n-\n     def test_arg_to_iter(self):\n         assert hasattr(arg_to_iter(None), '__iter__')\n         assert hasattr(arg_to_iter(100), '__iter__')\n\n@@ -14,20 +14,6 @@ from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.markup import remove_entities\n from scrapy.utils.defer import defer_succeed\n \n-def arg_to_list(arg):\n-    \"\"\"Convert an argument to list, so that it can be iterated. The argument\n-    can be a None, single value, or an iterable.\n-\n-    This is meant to be used in functions for arguments which can be either\n-    None, single valued or multi-valued.\n-    \"\"\"\n-    if arg is None:\n-        return []\n-    elif hasattr(arg, '__iter__'):\n-        return list(arg)\n-    else:\n-        return [arg]\n-\n def arg_to_iter(arg):\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n     value, or an iterable.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#828c3e09889eac8b6445eac24460b4041a44117f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 10 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 75 | Churn Cumulative: 800 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,7 +22,7 @@ from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipelineManager\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n-from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate\n+from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_imap\n from scrapy.utils.request import request_info\n \n class ExecutionEngine(object):\n@@ -255,12 +255,8 @@ class ExecutionEngine(object):\n                 else:\n                     log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" % (type(item).__name__, request), log.WARNING, domain=domain)\n \n-            class _ResultContainer(object):\n-                def append(self, item):\n-                    _onsuccess_per_item(item)\n-\n             def _onsuccess(result):\n-                return deferred_degenerate(result, _ResultContainer())\n+                return deferred_imap(_onsuccess_per_item, result)\n \n             def _onerror(_failure):\n                 if not isinstance(_failure.value, IgnoreRequest):\n\n@@ -0,0 +1,37 @@\n+from itertools import imap\n+from twisted.trial import unittest\n+\n+from scrapy.utils.defer import deferred_imap\n+\n+\n+class DeferTest(unittest.TestCase):\n+    def test_deferred_imap_1(self):\n+        \"\"\"deferred_imap storing results\"\"\"\n+        seq = [1, 2, 3]\n+        output = list(imap(None, seq))\n+\n+        dfd = deferred_imap(None, seq)\n+        dfd.addCallback(self.assertEqual, output)\n+        return dfd\n+\n+    def test_deferred_imap_2(self):\n+        \"\"\"deferred_imap not storing results\"\"\"\n+        seq = [1, 2, 3]\n+        output = list(imap(None, seq, seq))\n+\n+        dfd = deferred_imap(None, seq, seq)\n+        dfd.addCallback(self.assertEqual, output)\n+        return dfd\n+\n+    def test_deferred_imap_3(self):\n+        \"\"\"deferred_imap not storing results\"\"\"\n+        seq = [1, 2, 3]\n+        output = []\n+        function = lambda v: output.append(v)\n+\n+        dfd = deferred_imap(function, seq, store_results=False)\n+        dfd.addCallback(self.assertEqual, [])\n+        dfd.addCallback(lambda _: self.assertEqual(output, seq))\n+        return dfd\n+\n+\n\n@@ -2,6 +2,7 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n \n+from itertools import imap\n from twisted.internet import defer, reactor\n from twisted.python import failure\n \n@@ -63,19 +64,38 @@ def lambda_deferred(func):\n         return d\n     return deferred.addCallbacks(_success, _fail)\n \n-def deferred_degenerate(generator, container=None, next_delay=0):\n-    generator = iter(generator or [])\n+def deferred_imap(function, *sequences, **kwargs):\n+    \"\"\"Analog to itertools.imap python function but friendly iterable evaluation\n+    taking in count cooperative multitasking.\n+\n+    It returns a Deferred object that is fired when StopIteration is reached or\n+    when any exception is raised when calling function.\n+\n+    By default the output of the evaluation is collected into a list and\n+    returned as deferred result when iterable finished. But it can be disabled\n+    (to save memory) using `store_results` parameter.\n+\n+    \"\"\"\n+\n+    next_delay = kwargs.pop('next_delay', 0)\n+    store_results = kwargs.pop('store_results', True)\n+\n     deferred = defer.Deferred()\n-    container = container or []\n+    container = []\n+\n+    iterator = imap(function, *sequences)\n+\n     def _next():\n         try:\n-            container.append(generator.next())\n+            value = iterator.next()\n+            if store_results:\n+                container.append(value)\n         except StopIteration:\n             reactor.callLater(0, deferred.callback, container)\n         except:\n             reactor.callLater(0, deferred.errback, failure.Failure())\n         else:\n             reactor.callLater(next_delay, _next)\n+\n     _next()\n     return deferred\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70a686c02b2be82fb25bc2076003a34a46e3f14a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 64 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,8 @@ from scrapy.utils.defer import chain_deferred\n class Request(object):\n \n     def __init__(self, url, callback=None, method='GET', headers=None, body=None, \n-                 cookies=None, meta=None, encoding='utf-8', dont_filter=False):\n+                 cookies=None, meta=None, encoding='utf-8', dont_filter=False,\n+                 errback=None):\n \n         self._encoding = encoding  # this one has to be set first\n         self.method = method.upper()\n@@ -26,7 +27,7 @@ class Request(object):\n         self.set_body(body)\n \n         if callable(callback):\n-            callback = defer.Deferred().addCallback(callback)\n+            callback = defer.Deferred().addCallbacks(callback, errback)\n         self.deferred = callback or defer.Deferred()\n \n         self.cookies = cookies or {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b7cfb31845dab3d4b537ed09eadd01cd49feb86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 246 | Lines Deleted: 235 | Files Changed: 3 | Hunks: 43 | Methods Changed: 36 | Complexity Δ (Sum/Max): -9/6 | Churn Δ: 481 | Churn Cumulative: 1969 | Contributors (this commit): 4 | Commits (past 90d): 25 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,23 +1,19 @@\n-import re\n import os\n import time\n import hashlib\n-import urllib\n-import urlparse\n+import Image\n from cStringIO import StringIO\n \n-import Image\n+from twisted.internet import defer\n \n from scrapy import log\n from scrapy.stats import stats\n+from scrapy.utils.misc import md5sum\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.core.exceptions import HttpException, IgnoreRequest\n from scrapy.conf import settings\n-\n from scrapy.contrib.pipeline.media import MediaPipeline\n \n-# the age at which we download images again\n-IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n \n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n@@ -27,10 +23,48 @@ class ImageException(Exception):\n \n \n class BaseImagesPipeline(MediaPipeline):\n-    MEDIA_TYPE = 'image'\n+    \"\"\"Abstract pipeline that implement the image downloading and thumbnail generation logic\n+\n+    This pipeline tries to minimize network transfers and image processing,\n+    doing stat of the images and determining if image is new, uptodate or\n+    expired.\n+\n+    `new` images are those that pipeline never processed and needs to be\n+        downloaded from supplier site the first time.\n+\n+    `uptodate` images are the ones that the pipeline processed and are still\n+        valid images.\n+\n+    `expired` images are those that pipeline already processed but the last\n+        modification was made long time ago, so a reprocessing is recommended to\n+        refresh it in case of change.\n+\n+    IMAGES_EXPIRES setting controls the maximun days since an image was modified\n+    to consider it uptodate.\n+\n+    THUMBS is a tuple of tuples, each sub-tuple is a pair of thumb_id string\n+    and a compatible python image library size (a tuple).\n+    See thumbnail method at http://www.pythonware.com/library/pil/handbook/image.htm\n+\n+    Downloaded images are skipped if sizes aren't greater than MIN_WIDTH and\n+    MIN_HEIGHT limit. A proper log messages will be printed.\n+\n+    \"\"\"\n+\n+    MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n+    MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n+    IMAGES_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n+    MEDIA_NAME = 'image'\n+    THUMBS = (\n+#             ('50', (50, 50)),\n+#             ('110', (110, 110)),\n+#             ('270', (270, 270))\n+     )\n+\n+\n \n     def media_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n+        mtype = self.MEDIA_NAME\n         referer = request.headers.get('Referer')\n \n         if not response or not response.body:\n@@ -43,7 +77,18 @@ class BaseImagesPipeline(MediaPipeline):\n         msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)\n         log.msg(msg, level=log.DEBUG, domain=info.domain)\n         self.inc_stats(info.domain, status)\n-        return self.image_downloaded(response, request, info)\n+\n+        try:\n+            key = self.image_key(request.url)\n+            checksum = self.image_downloaded(response, request, info)\n+        except ImageException, ex:\n+            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n+            raise ex\n+        except Exception, ex:\n+            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n+            raise ex\n+\n+        return '%s#%s' % (key, checksum) if checksum else key\n \n     def media_failed(self, failure, request, info):\n         referer = request.headers.get('Referer')\n@@ -53,31 +98,133 @@ class BaseImagesPipeline(MediaPipeline):\n             errmsg = str(failure)\n \n         msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' \\\n-                % (self.MEDIA_TYPE, request, referer, errmsg)\n+                % (self.MEDIA_NAME, request, referer, errmsg)\n         log.msg(msg, level=log.WARNING, domain=info.domain)\n         raise ImageException(msg)\n \n+    def media_to_download(self, request, info):\n+        def _onsuccess(result):\n+            last_modified = result.get('last_modified', None)\n+            if not last_modified:\n+                return # returning None force download\n+\n+            age_seconds = time.time() - last_modified\n+            age_days = age_seconds / 60 / 60 / 24\n+            if age_days > self.IMAGES_EXPIRES:\n+                return # returning None force download\n+\n+            referer = request.headers.get('Referer')\n+            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n+                    (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, domain=info.domain)\n+            self.inc_stats(info.domain, 'uptodate')\n+\n+            checksum = result.get('checksum', None)\n+            return '%s#%s' % (key, checksum) if checksum else key\n+\n+        key = self.image_key(request.url)\n+        dfd = defer.maybeDeferred(self.stat_key, key, info)\n+        dfd.addCallbacks(_onsuccess, lambda _:None)\n+        dfd.addErrback(log.err, self.__class__.__name__ + '.stat_key')\n+        return dfd\n+\n+    def image_downloaded(self, response, request, info):\n+        first_buf = None\n+        for key, image, buf in self.get_images(response, request, info):\n+            self.store_image(key, image, buf, info)\n+            if first_buf is None:\n+                first_buf = buf\n+        return md5sum(first_buf)\n+\n+    def get_images(self, response, request, info):\n+        key = self.image_key(request.url)\n+        orig_image = Image.open(StringIO(response.body))\n+\n+        width, height = orig_image.size\n+        if width < self.MIN_WIDTH or height < self.MIN_HEIGHT:\n+            raise ImageException(\"Image too small (%dx%d < %dx%d): %s\" % \\\n+                    (width, height, self.MIN_WIDTH, self.MIN_HEIGHT, response.url))\n+\n+        image, buf = self.convert_image(orig_image)\n+        yield key, image, buf\n+\n+        for thumb_id, size in self.THUMBS or []:\n+            thumb_key = self.thumb_key(request.url, thumb_id)\n+            thumb_image, thumb_buf = self.convert_image(image, size)\n+            yield thumb_key, thumb_image, thumb_buf\n+\n     def inc_stats(self, domain, status):\n         stats.incpath('%s/image_count' % domain)\n         stats.incpath('%s/image_status_count/%s' % (domain, status))\n \n-    def image_downloaded(self, response, request, info):\n+    def convert_image(self, image, size=None):\n+        if image.mode != 'RGB':\n+            image = image.convert('RGB')\n+\n+        if size:\n+            image = image.copy()\n+            image.thumbnail(size, Image.ANTIALIAS)\n+\n+        buf = StringIO()\n+        try:\n+            image.save(buf, 'JPEG')\n+        except Exception, ex:\n+            raise ImageException(\"Cannot process image. Error: %s\" % ex)\n+\n+        return image, buf\n+\n+    def image_key(self, url):\n+        image_guid = hashlib.sha1(url).hexdigest()\n+        return 'full/%s.jpg' % (image_guid)\n+\n+    def thumb_key(self, url, thumb_id):\n+        image_guid = hashlib.sha1(url).hexdigest()\n+        return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n+\n+    # Required overradiable interface\n+    def store_image(self, key, image, buf, info):\n+        \"\"\"Override this method with specific code to persist an image\n+\n+        This method is used to persist the full image and any defined\n+        thumbnail, one a time.\n+\n+        Return value is ignored.\n+\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def stat_key(self, key, info):\n+        \"\"\"Override this method with specific code to stat an image\n+\n+        this method should return and dictionary with two parameters:\n+         * last_modified: the last modification time in seconds since the epoch\n+         * checksum: the md5sum of the content of the stored image if found\n+\n+         If an exception is raised or last_modified is None, then the image\n+         will be re-downloaded.\n+\n+         If the difference in days between last_modified and now is greater than\n+         IMAGES_EXPIRES settings, then the image will be re-downloaded\n+\n+         The checksum value is appended to returned image path after a hash\n+         sign (#), if checksum is None, then nothing is appended including the\n+         hash sign.\n+\n+        \"\"\"\n         raise NotImplementedError\n \n \n class ImagesPipeline(BaseImagesPipeline):\n-    MIN_WIDTH = 0\n-    MIN_HEIGHT = 0\n-    THUMBS = (\n-#             ('50', (50, 50)),\n-#             ('110', (110, 110)),\n-#             ('270', (270, 270))\n-     )\n+    \"\"\"Images pipeline with filesystem support as image's store backend\n+\n+    If IMAGES_DIR setting has a valid value, this pipeline is enabled and use\n+    path defined at setting as dirname for storing images.\n+\n+    \"\"\"\n \n     class DomainInfo(BaseImagesPipeline.DomainInfo):\n         def __init__(self, domain):\n-            super(ImagesPipeline.DomainInfo, self).__init__(domain)\n             self.created_directories = set()\n+            super(ImagesPipeline.DomainInfo, self).__init__(domain)\n \n     def __init__(self):\n         if not settings['IMAGES_DIR']:\n@@ -85,44 +232,27 @@ class ImagesPipeline(BaseImagesPipeline):\n \n         self.BASEDIRNAME = settings['IMAGES_DIR']\n         self.mkdir(self.BASEDIRNAME)\n-\n-        self.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n-        self.MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n         super(ImagesPipeline, self).__init__()\n \n-    def media_to_download(self, request, info):\n-        relative, absolute = self._get_paths(request)\n-        if not should_download(absolute):\n-            self.inc_stats(info.domain, 'uptodate')\n-            referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate): Downloaded %s from %s referred in <%s>' % \\\n-                    (self.MEDIA_TYPE, request, referer), level=log.DEBUG, domain=info.domain)\n-            return relative\n-\n-    def image_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_TYPE\n-        relpath, abspath = self._get_paths(request)\n-        dirname = os.path.dirname(abspath)\n-        self.mkdir(dirname, info)\n+    def store_image(self, key, image, buf, info):\n+        absolute_path = self.get_filesystem_path(key)\n+        self.mkdir(os.path.dirname(absolute_path), info)\n+        image.save(absolute_path)\n \n+    def stat_key(self, key, info):\n+        absolute_path = self.get_filesystem_path(key)\n         try:\n-            save_image_with_thumbnails(response, abspath, self.THUMBS, self.MIN_WIDTH, self.MIN_HEIGHT)\n-        except ImageException, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n-            raise ex\n-        except Exception, ex:\n-            referer = request.headers.get('Referer')\n-            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' \\\n-                    % (mtype, request, referer, ex)\n-            log.msg(msg, level=log.WARNING, domain=info.domain)\n-            raise ImageException(msg)\n+            last_modified = os.path.getmtime(absolute_path)\n+        except:\n+            return {}\n \n-        return relpath # success value sent as input result for item_media_downloaded\n+        with open(absolute_path, 'rb') as imagefile:\n+            checksum = md5sum(imagefile)\n \n-    def _get_paths(self, request):\n-        relative = image_path(request.url)\n-        absolute = os.path.join(self.BASEDIRNAME, relative)\n-        return relative, absolute\n+        return {'last_modified': last_modified, 'checksum': checksum}\n+\n+    def get_filesystem_path(self, key):\n+        return os.path.join(self.BASEDIRNAME, key)\n \n     def mkdir(self, dirname, info=None):\n         already_created = info.created_directories if info else set()\n@@ -130,66 +260,3 @@ class ImagesPipeline(BaseImagesPipeline):\n             if not os.path.exists(dirname):\n                 os.makedirs(dirname)\n             already_created.add(dirname)\n-\n-\n-def should_download(path):\n-    \"\"\"Should the image downloader download the image to the location specified\n-    \"\"\"\n-    try:\n-        mtime = os.path.getmtime(path)\n-        age_seconds = time.time() - mtime\n-        age_days = age_seconds / 60 / 60 / 24\n-        return age_days > IMAGE_EXPIRES\n-    except:\n-        return True\n-\n-_MULTIPLE_SLASHES_REGEXP = re.compile(r\"\\/{2,}\")\n-_FINAL_SLASH_REGEXP = re.compile(r\"\\/$\")\n-def image_path(url):\n-    \"\"\"Return the relative path on the target filesystem for an image to be\n-    downloaded to.\n-    \"\"\"\n-    _, netloc, urlpath, query, _ = urlparse.urlsplit(url)\n-    urlpath = _MULTIPLE_SLASHES_REGEXP.sub('/', urlpath)\n-    urlpath = _FINAL_SLASH_REGEXP.sub('.jpg', urlpath)\n-    if os.sep != '/':\n-        urlpath.replace('/', os.sep)\n-    if query:\n-        img_path = os.path.join(netloc, hashlib.sha1(url).hexdigest())\n-    else:\n-        img_path = os.path.join(netloc, urlpath[1:])\n-    return urllib.unquote(img_path)\n-\n-\n-def thumbnail_name(image, sizestr):\n-    \"\"\"Get the name of a thumbnail image given the name of the original file.\n-\n-    There will can be many types of thumbnails, so we will have a \"name\" for\n-    each type.\n-    \"\"\"\n-    return os.path.splitext(image)[0] + '_' + sizestr + '.jpg'\n-\n-def save_scaled_image(image, img_path, name, size):\n-    thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n-    thumb.thumbnail(size, Image.ANTIALIAS)\n-    filename = thumbnail_name(img_path, name)\n-    thumb.save(filename, 'JPEG')\n-\n-def save_image_with_thumbnails(response, path, thumbsizes, min_width=0, min_height=0):\n-    memoryfile = StringIO(response.body)\n-    im = Image.open(memoryfile)\n-    if im.mode != 'RGB':\n-        log.msg(\"Found non-RGB image during scraping %s\" % path, level=log.WARNING)\n-    for name, size in thumbsizes or []:\n-        save_scaled_image(im, path, name, size)\n-    try:\n-        im.save(path)\n-    except Exception, ex:\n-        log.msg(\"Image (processing-error): cannot process %s, so writing direct file: Error: %s\" % (path, ex))\n-        f = open(path, 'wb')\n-        f.write(response.body)\n-        f.close()\n-    width, height = im.size\n-    if width < min_width or height < min_height:\n-        raise ImageException(\"Image too small (%dx%d < %dx%d): %s\" % (width, height, min_width, min_height, response.url))\n-\n\n@@ -1,150 +1,77 @@\n-import time\n-import hashlib\n import rfc822\n-from cStringIO import StringIO\n \n-import Image\n-\n-from scrapy import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.contrib.aws import sign_request\n+from scrapy.contrib.pipeline.images import BaseImagesPipeline, md5sum\n from scrapy.conf import settings\n \n-from .images import BaseImagesPipeline, ImageException\n-\n-\n-def md5sum(buffer):\n-    m = hashlib.md5()\n-    buffer.seek(0)\n-    while 1:\n-        d = buffer.read(8096)\n-        if not d: break\n-        m.update(d)\n-    return m.hexdigest()\n-\n \n class S3ImagesPipeline(BaseImagesPipeline):\n-    MEDIA_TYPE = 'image'\n-    THUMBS = (\n-#             ('50', (50, 50)),\n-#             ('110', (110, 110)),\n-#             ('270', (270, 270))\n-    )\n+    \"\"\"Images pipeline with amazon S3 support as image's store backend\n \n-    # Automatically sign requests with AWS authorization header,\n-    # alternative we can do this using scrapy.contrib.aws.AWSMiddleware\n-    sign_requests = True\n+    This pipeline tries to minimize the PUT requests made to amazon doing a\n+    HEAD per full image, if HEAD returns a successfully response, then the\n+    Last-Modified header is compared to current timestamp and if the difference\n+    in days are greater that IMAGE_EXPIRES setting, then the image is\n+    downloaded, reprocessed and uploaded to S3 again including its thumbnails.\n \n-    s3_custom_spider = None\n+    It is recommended to add an spider with domain_name 's3.amazonaws.com',\n+    doing that you will overcome the limit of request per spider. The following\n+    is the minimal code for this spider:\n+\n+        from scrapy.spider import BaseSpider\n+\n+        class S3AmazonAWSSpider(BaseSpider):\n+            domain_name = \"s3.amazonaws.com\"\n+            max_concurrent_requests = 100\n+            start_urls = ('http://s3.amazonaws.com/',)\n+\n+        SPIDER = S3AmazonAWSSpider()\n+\n+    Commonly uploading images to S3 requires requests to be signed, the\n+    recommended way is to enable scrapy.contrib.aws.AWSMiddleware downloader\n+    middleware and configure AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n+    settings\n+\n+    More info about amazon S3 at http://docs.amazonwebservices.com/AmazonS3/2006-03-01/\n+\n+    \"\"\"\n+\n+    # amazon s3 bucket name to put images\n+    bucket_name = settings.get('S3_BUCKET')\n+\n+    # prefix to prepend to image keys\n+    key_prefix = settings.get('S3_PREFIX', '')\n+\n+    # Optional spider to use for image uploading\n+    AmazonS3Spider = None\n \n     def __init__(self):\n         if not settings['S3_IMAGES']:\n             raise NotConfigured\n-\n-        self.bucket_name = settings['S3_BUCKET']\n-        self.prefix = settings['S3_PREFIX']\n-        self.access_key = settings['AWS_ACCESS_KEY_ID']\n-        self.secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        self.image_refresh_days = settings.getint('IMAGES_EXPIRES', 90)\n-        MediaPipeline.__init__(self)\n+        super(S3ImagesPipeline, self).__init__()\n \n     def s3_request(self, key, method, body=None, headers=None):\n-        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.prefix, key)\n+        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.key_prefix, key)\n         req = Request(url, method=method, body=body, headers=headers)\n-        if self.sign_requests:\n-            sign_request(req, self.access_key, self.secret_key)\n         return req\n \n-    def image_downloaded(self, response, request, info):\n-        try:\n-            key = self.s3_image_key(request.url)\n-            etag = self.s3_store_image(response, request.url, info)\n-        except ImageException, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n-            raise ex\n-        except Exception, ex:\n-            log.msg(str(ex), level=log.WARNING, domain=info.domain)\n-            raise ex\n-\n-        return '%s#%s' % (key, etag) # success value sent as input result for item_media_downloaded\n-\n-    def s3_image_key(self, url):\n-        \"\"\"Return the relative path on the target filesystem for an image to be\n-        downloaded to.\n-        \"\"\"\n-        image_guid = hashlib.sha1(url).hexdigest()\n-        return 'full/%s.jpg' % (image_guid)\n-\n-    def s3_thumb_key(self, url, thumb_id):\n-        \"\"\"Return the relative path on the target filesystem for an image to be\n-        downloaded to.\n-        \"\"\"\n-        image_guid = hashlib.sha1(url).hexdigest()\n-        return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n-\n-    def media_to_download(self, request, info):\n-        \"\"\"Return if the image should be downloaded by checking if it's already in\n-        the S3 storage and not too old\"\"\"\n-\n+    def stat_key(self, key, info):\n         def _onsuccess(response):\n-            if 'Last-Modified' not in response.headers:\n-                return # returning None force download\n-\n-            # check if last modified date did not expires\n+            checksum = response.headers['Etag'].strip('\"')\n             last_modified = response.headers['Last-Modified']\n             modified_tuple = rfc822.parsedate_tz(last_modified)\n             modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n-            age_seconds = time.time() - modified_stamp\n-            age_days = age_seconds / 60 / 60 / 24\n+            return {'checksum': checksum, 'last_modified': modified_stamp}\n \n-            if age_days > self.image_refresh_days:\n-                return # returning None force download\n-\n-            etag = response.headers['Etag'].strip('\"')\n-            referer = request.headers.get('Referer')\n-            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n-                    (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n-\n-            self.inc_stats(info.domain, 'uptodate')\n-            return '%s#%s' % (key, etag)\n-\n-        key = self.s3_image_key(request.url)\n         req = self.s3_request(key, method='HEAD')\n         dfd = self.s3_download(req, info)\n-        dfd.addCallbacks(_onsuccess, lambda _:None)\n-        dfd.addErrback(log.err, 'S3ImagesPipeline.media_to_download')\n+        dfd.addCallback(_onsuccess)\n         return dfd\n \n-    def s3_store_image(self, response, url, info):\n+    def store_image(self, key, image, buf, info):\n         \"\"\"Upload image to S3 storage\"\"\"\n-        buf = StringIO(response.body)\n-        image = Image.open(buf)\n-        key = self.s3_image_key(url)\n-        _, jpegbuf = self._s3_put_image(image, key, info)\n-        self.s3_store_thumbnails(image, url, info)\n-        return md5sum(jpegbuf) # Etag\n-\n-    def s3_store_thumbnails(self, image, url, info):\n-        \"\"\"Upload image thumbnails to S3 storage\"\"\"\n-        for thumb_id, size in self.THUMBS or []:\n-            thumb = image.copy() if image.mode == 'RGB' else image.convert('RGB')\n-            thumb.thumbnail(size, Image.ANTIALIAS)\n-            key = self.s3_thumb_key(url, thumb_id)\n-            self._s3_put_image(thumb, key, info)\n-\n-    def _s3_put_image(self, image, key, info):\n-        if image.mode != 'RGB':\n-            image = image.convert('RGB')\n-\n-        buf = StringIO()\n-        try:\n-            image.save(buf, 'JPEG')\n-        except Exception, ex:\n-            raise ImageException(\"Cannot process image. Error: %s\" % ex)\n-\n         width, height = image.size\n         headers = {\n                 'Content-Type': 'image/jpeg',\n@@ -156,7 +83,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n \n         buf.seek(0)\n         req = self.s3_request(key, method='PUT', body=buf.read(), headers=headers)\n-        return self.s3_download(req, info), buf\n+        self.s3_download(req, info)\n \n     def s3_download(self, request, info):\n         \"\"\"This method is used for HEAD and PUT requests sent to amazon S3\n@@ -165,8 +92,8 @@ class S3ImagesPipeline(BaseImagesPipeline):\n         to current domain spider.\n \n         \"\"\"\n-        if self.s3_custom_spider:\n-            return scrapyengine.schedule(request, self.s3_custom_spider)\n+        if self.AmazonS3Spider:\n+            return scrapyengine.schedule(request, self.AmazonS3Spider)\n         return self.download(request, info)\n \n \n\n@@ -185,3 +185,20 @@ def string_camelcase(string):\n     return CAMELCASE_INVALID_CHARS.sub('', string.title())\n \n \n+def md5sum(buffer):\n+    \"\"\"Calculate the md5 checksum of a file\n+\n+    >>> from StringIO import StringIO\n+    >>> md5sum(StringIO('file content to hash'))\n+    '784406af91dd5a54fbb9c84c2236595a'\n+\n+    \"\"\"\n+    m = hashlib.md5()\n+    buffer.seek(0)\n+    while 1:\n+        d = buffer.read(8096)\n+        if not d: break\n+        m.update(d)\n+    return m.hexdigest()\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4fa7e6cac4816c9532bc3a2fcc5871282c283ca9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 14 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 30 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,31 +1,39 @@\n import unittest\n-from scrapy.contrib.pipeline.images import thumbnail_name, image_path\n+from scrapy.contrib.pipeline.images import BaseImagesPipeline\n \n class ImagesPipelineTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.pipeline = BaseImagesPipeline()\n+\n+    def tearDown(self):\n+        del self.pipeline\n+\n     def test_image_path(self):\n+        image_path = self.pipeline.image_key\n         self.assertEqual(image_path(\"https://dev.mydeco.com/mydeco.gif\"),\n-                         'dev.mydeco.com/mydeco.gif')\n+                         'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n         self.assertEqual(image_path(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\"),\n-                         'www.maddiebrown.co.uk/catalogue-items/image_54642_12175_95307.jpg')\n+                         'full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg')\n         self.assertEqual(image_path(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\"),\n-                         'dev.mydeco.com/two/dirs/with spaces+signs.gif')\n+                         'full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg')\n         self.assertEqual(image_path(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"),\n-                         'www.dfsonline.co.uk/4507be485f38b0da8a0be9eb2e1dfab8a19223f2')\n+                         'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n         self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532/\"),\n-                         'www.dorma.co.uk/images/product_details/2532.jpg')\n+                         'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')\n         self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532\"),\n-                         'www.dorma.co.uk/images/product_details/2532')\n+                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n \n     def test_thumbnail_name(self):\n+        thumbnail_name = self.pipeline.thumb_key\n         name = '50'\n         self.assertEqual(thumbnail_name(\"/tmp/foo.jpg\", name),  \n-                         '/tmp/foo_50.jpg')\n+                         'thumbs/50/271f172bb4727281011c80fe763e93a47bb6b3fe.jpg')\n         self.assertEqual(thumbnail_name(\"foo.png\", name),\n-                         'foo_50.jpg')\n+                         'thumbs/50/0945c699b5580b99e4f40dffc009699b2b6830a7.jpg')\n         self.assertEqual(thumbnail_name(\"/tmp/foo\", name),\n-                         '/tmp/foo_50.jpg')\n+                         'thumbs/50/469150566bd728fc90b4adf6495202fd70ec3537.jpg')\n         self.assertEqual(thumbnail_name(\"/tmp/some.name/foo\", name),\n-                         '/tmp/some.name/foo_50.jpg')\n+                         'thumbs/50/92dac2a6a2072c5695a5dff1f865b3cb70c657bb.jpg')\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d0e2d12027079fa3b147e0a51c3afe6f207a41f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 33 | Churn Cumulative: 857 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ import re\n from scrapy.link import LinkExtractor\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n from scrapy.xpath import HtmlXPathSelector\n+from scrapy.utils.misc import arg_to_iter\n \n _re_type = type(re.compile(\"\", 0))\n \n@@ -42,11 +43,11 @@ class RegexLinkExtractor(LinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n                  tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):\n-        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in allow]\n-        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in deny]\n-        self.allow_domains = set(allow_domains)\n-        self.deny_domains = set(deny_domains)\n-        self.restrict_xpaths = restrict_xpaths\n+        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)]\n+        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)]\n+        self.allow_domains = set(arg_to_iter(allow_domains))\n+        self.deny_domains = set(arg_to_iter(deny_domains))\n+        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n         self.canonicalize = canonicalize\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n\n@@ -115,6 +115,28 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n #            [ Link(url='http://example.com/sample1.html', text=u''),\n #              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n \n+    def test_extraction_using_single_values(self):\n+        '''Test the extractor's behaviour among different situations'''\n+\n+        lx = RegexLinkExtractor(allow='sample')\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3 text') ])\n+\n+        lx = RegexLinkExtractor(allow='sample', deny='3')\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n+\n+        lx = RegexLinkExtractor(allow_domains='google.com')\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://www.google.com/something', text=u'') ])\n+\n+        lx = RegexLinkExtractor(deny_domains='example.com')\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://www.google.com/something', text=u'') ])\n+\n     def test_matches(self):\n         url1 = 'http://lotsofstuff.com/stuff1/index'\n         url2 = 'http://evenmorestuff.com/uglystuff/index'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7227b65de1ee8bfcab22bc49acf06150a1d3ff42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 164 | Lines Deleted: 130 | Files Changed: 5 | Hunks: 10 | Methods Changed: 33 | Complexity Δ (Sum/Max): 7/14 | Churn Δ: 294 | Churn Cumulative: 1770 | Contributors (this commit): 3 | Commits (past 90d): 24 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,7 +51,7 @@ class Headers(CaselessDict):\n         self.setdefault(key, default_list)\n \n     def appendlist(self, key, value):\n-        self.getlist(key).append(self.normvalue(value))\n+        self.getlist(key).extend(self.normvalue(value))\n \n     def items(self):\n         return list(self.iteritems())\n@@ -68,3 +68,8 @@ class Headers(CaselessDict):\n     def to_string(self):\n         return headers_dict_to_raw(self)\n \n+    def __copy__(self):\n+        return self.__class__(self.lists())\n+    copy = __copy__\n+\n+\n\n@@ -1,121 +0,0 @@\n-import unittest\n-\n-from scrapy.utils.datatypes import CaselessDict\n-\n-\n-class CaselessDictTest(unittest.TestCase):\n-\n-    def test_init(self):\n-        seq = {'red': 1, 'black': 3}\n-        d = CaselessDict(seq)\n-        self.assertEqual(d['red'], 1)\n-        self.assertEqual(d['black'], 3)\n-\n-        seq = (('red', 1), ('black', 3))\n-        d = CaselessDict(seq)\n-        self.assertEqual(d['red'], 1)\n-        self.assertEqual(d['black'], 3)\n-\n-    def test_caseless(self):\n-        d = CaselessDict()\n-        d['key_Lower'] = 1\n-        self.assertEqual(d['KEy_loWer'], 1)\n-        self.assertEqual(d.get('KEy_loWer'), 1)\n-\n-        d['KEY_LOWER'] = 3\n-        self.assertEqual(d['key_Lower'], 3)\n-        self.assertEqual(d.get('key_Lower'), 3)\n-\n-    def test_delete(self):\n-        d = CaselessDict({'key_lower': 1})\n-        del d['key_LOWER']\n-        self.assertRaises(KeyError, d.__getitem__, 'key_LOWER')\n-        self.assertRaises(KeyError, d.__getitem__, 'key_lower')\n-\n-    def test_getdefault(self):\n-        d = CaselessDict()\n-        self.assertEqual(d.get('c', 5), 5)\n-        d['c'] = 10\n-        self.assertEqual(d.get('c', 5), 10)\n-\n-    def test_setdefault(self):\n-        d = CaselessDict({'a': 1, 'b': 2})\n-\n-        r = d.setdefault('A', 5)\n-        self.assertEqual(r, 1)\n-        self.assertEqual(d['A'], 1)\n-\n-        r = d.setdefault('c', 5)\n-        self.assertEqual(r, 5)\n-        self.assertEqual(d['C'], 5)\n-\n-    def test_fromkeys(self):\n-        keys = ('a', 'b')\n-\n-        d = CaselessDict.fromkeys(keys)\n-        self.assertEqual(d['A'], None)\n-        self.assertEqual(d['B'], None)\n-\n-        d = CaselessDict.fromkeys(keys, 1)\n-        self.assertEqual(d['A'], 1)\n-        self.assertEqual(d['B'], 1)\n-\n-        instance = CaselessDict()\n-        d = instance.fromkeys(keys)\n-        self.assertEqual(d['A'], None)\n-        self.assertEqual(d['B'], None)\n-\n-        d = instance.fromkeys(keys, 1)\n-        self.assertEqual(d['A'], 1)\n-        self.assertEqual(d['B'], 1)\n-\n-    def test_contains(self):\n-        d = CaselessDict()\n-        d['a'] = 1\n-        assert 'a' in d\n-        assert d.has_key('a')\n-\n-    def test_pop(self):\n-        d = CaselessDict()\n-        d['a'] = 1\n-        self.assertEqual(d.pop('A'), 1)\n-        self.assertRaises(KeyError, d.pop, 'A')\n-\n-    def test_normkey(self):\n-        class MyDict(CaselessDict):\n-            def normkey(self, key):\n-                return key.title()\n-\n-        d = MyDict()\n-        d['key-one'] = 2\n-        self.assertEqual(list(d.keys()), ['Key-One'])\n-\n-    def test_normvalue(self):\n-        class MyDict(CaselessDict):\n-            def normvalue(self, value):\n-                if value is not None:\n-                    return value + 1\n-\n-        d = MyDict({'key': 1})\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n-\n-        d = MyDict()\n-        d['key'] = 1\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n-\n-        d = MyDict()\n-        d.setdefault('key', 1)\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n-\n-        d = MyDict()\n-        d.update({'key': 1})\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n-\n-        d = MyDict.fromkeys(('key',), 1)\n-        self.assertEqual(d['key'], 2)\n-        self.assertEqual(d.get('key'), 2)\n-\n\n@@ -74,10 +74,29 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h.getlist('Content-Type'), ['text/html'])\n         self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1', 'ip2'])\n \n-    #def test_copy(self):\n-    #    h1 = Headers({'header1': ['value1', 'value2']})\n-    #    h2 = copy.copy(h1)\n-    #    self.assertEqual(h1, h2)\n-    #    self.assertEqual(h1.getlist('header1'), h2.getlist('header1'))\n-    #    assert h1.getlist('header1') is not h2.getlist('header1')\n+    def test_copy(self):\n+        h1 = Headers({'header1': ['value1', 'value2']})\n+        h2 = copy.copy(h1)\n+        self.assertEqual(h1, h2)\n+        self.assertEqual(h1.getlist('header1'), h2.getlist('header1'))\n+        assert h1.getlist('header1') is not h2.getlist('header1')\n+        assert isinstance(h2, Headers)\n+\n+    def test_appendlist(self):\n+        h1 = Headers({'header1': 'value1'})\n+        h1.appendlist('header1', 'value3')\n+        self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])\n+\n+    def test_setlist(self):\n+        h1 = Headers({'header1': 'value1'})\n+        self.assertEqual(h1.getlist('header1'), ['value1'])\n+        h1.setlist('header1', ['value2', 'value3'])\n+        self.assertEqual(h1.getlist('header1'), ['value2', 'value3'])\n+\n+    def test_setlistdefault(self):\n+        h1 = Headers({'header1': 'value1'})\n+        h1.setlistdefault('header1', ['value2', 'value3'])\n+        h1.setlistdefault('header2', ['value2', 'value3'])\n+        self.assertEqual(h1.getlist('header1'), ['value1'])\n+        self.assertEqual(h1.getlist('header2'), ['value2', 'value3'])\n \n\n@@ -1,6 +1,7 @@\n+import copy\n import unittest\n \n-from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n+from scrapy.utils.datatypes import PriorityQueue, PriorityStack, CaselessDict\n \n # (ITEM, PRIORITY)\n INPUT = [(1, -5), (30, -1), (80, -3), (4, 1), (6, 3), (20, 0), (50, -1)]\n@@ -72,5 +73,131 @@ class PriorityStackTestCase(unittest.TestCase):\n         result = [x for x in pq]\n         self.assertEquals(result, self.output)\n \n+\n+class CaselessDictTest(unittest.TestCase):\n+\n+    def test_init(self):\n+        seq = {'red': 1, 'black': 3}\n+        d = CaselessDict(seq)\n+        self.assertEqual(d['red'], 1)\n+        self.assertEqual(d['black'], 3)\n+\n+        seq = (('red', 1), ('black', 3))\n+        d = CaselessDict(seq)\n+        self.assertEqual(d['red'], 1)\n+        self.assertEqual(d['black'], 3)\n+\n+    def test_caseless(self):\n+        d = CaselessDict()\n+        d['key_Lower'] = 1\n+        self.assertEqual(d['KEy_loWer'], 1)\n+        self.assertEqual(d.get('KEy_loWer'), 1)\n+\n+        d['KEY_LOWER'] = 3\n+        self.assertEqual(d['key_Lower'], 3)\n+        self.assertEqual(d.get('key_Lower'), 3)\n+\n+    def test_delete(self):\n+        d = CaselessDict({'key_lower': 1})\n+        del d['key_LOWER']\n+        self.assertRaises(KeyError, d.__getitem__, 'key_LOWER')\n+        self.assertRaises(KeyError, d.__getitem__, 'key_lower')\n+\n+    def test_getdefault(self):\n+        d = CaselessDict()\n+        self.assertEqual(d.get('c', 5), 5)\n+        d['c'] = 10\n+        self.assertEqual(d.get('c', 5), 10)\n+\n+    def test_setdefault(self):\n+        d = CaselessDict({'a': 1, 'b': 2})\n+\n+        r = d.setdefault('A', 5)\n+        self.assertEqual(r, 1)\n+        self.assertEqual(d['A'], 1)\n+\n+        r = d.setdefault('c', 5)\n+        self.assertEqual(r, 5)\n+        self.assertEqual(d['C'], 5)\n+\n+    def test_fromkeys(self):\n+        keys = ('a', 'b')\n+\n+        d = CaselessDict.fromkeys(keys)\n+        self.assertEqual(d['A'], None)\n+        self.assertEqual(d['B'], None)\n+\n+        d = CaselessDict.fromkeys(keys, 1)\n+        self.assertEqual(d['A'], 1)\n+        self.assertEqual(d['B'], 1)\n+\n+        instance = CaselessDict()\n+        d = instance.fromkeys(keys)\n+        self.assertEqual(d['A'], None)\n+        self.assertEqual(d['B'], None)\n+\n+        d = instance.fromkeys(keys, 1)\n+        self.assertEqual(d['A'], 1)\n+        self.assertEqual(d['B'], 1)\n+\n+    def test_contains(self):\n+        d = CaselessDict()\n+        d['a'] = 1\n+        assert 'a' in d\n+        assert d.has_key('a')\n+\n+    def test_pop(self):\n+        d = CaselessDict()\n+        d['a'] = 1\n+        self.assertEqual(d.pop('A'), 1)\n+        self.assertRaises(KeyError, d.pop, 'A')\n+\n+    def test_normkey(self):\n+        class MyDict(CaselessDict):\n+            def normkey(self, key):\n+                return key.title()\n+\n+        d = MyDict()\n+        d['key-one'] = 2\n+        self.assertEqual(list(d.keys()), ['Key-One'])\n+\n+    def test_normvalue(self):\n+        class MyDict(CaselessDict):\n+            def normvalue(self, value):\n+                if value is not None:\n+                    return value + 1\n+\n+        d = MyDict({'key': 1})\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict()\n+        d['key'] = 1\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict()\n+        d.setdefault('key', 1)\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict()\n+        d.update({'key': 1})\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict.fromkeys(('key',), 1)\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+    def test_copy(self):\n+        h1 = CaselessDict({'header1': 'value'})\n+        h2 = copy.copy(h1)\n+        self.assertEqual(h1, h2)\n+        self.assertEqual(h1.get('header1'), h2.get('header1'))\n+        assert isinstance(h2, CaselessDict)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n+\n\n@@ -186,7 +186,7 @@ class SiteNode(object):\n \n class CaselessDict(dict):\n     def __init__(self, seq=None):\n-        dict.__init__(self)\n+        super(CaselessDict, self).__init__()\n         if seq:\n             self.update(seq)\n \n@@ -203,6 +203,10 @@ class CaselessDict(dict):\n         return dict.__contains__(self, self.normkey(key))\n     has_key = __contains__\n \n+    def __copy__(self):\n+        return self.__class__(self)\n+    copy = __copy__\n+\n     def normkey(self, key):\n         \"\"\"Method to normalize dictionary key access\"\"\"\n         return key.lower()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
