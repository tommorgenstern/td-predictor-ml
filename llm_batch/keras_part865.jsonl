{"custom_id": "keras#154f0b30d6048c5dcda1d65fbb872dc0639358c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -270,7 +270,6 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n         axis: (Optional) Defaults to `-1`. The dimension along which the cosine\n             similarity is computed.\n \n-    Example:\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a591869bdbe4ebc02a21efdf52007f68a78d659", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 165 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 20/12 | Churn Δ: 165 | Churn Cumulative: 4636 | Contributors (this commit): 21 | Commits (past 90d): 94 | Contributors (cumulative): 92 | DMM Complexity: 0.7818181818181819\n\nDIFF:\n@@ -146,6 +146,7 @@ from keras.src.ops.numpy import cross\n from keras.src.ops.numpy import cumprod\n from keras.src.ops.numpy import cumsum\n from keras.src.ops.numpy import diag\n+from keras.src.ops.numpy import diagflat\n from keras.src.ops.numpy import diagonal\n from keras.src.ops.numpy import diff\n from keras.src.ops.numpy import digitize\n\n@@ -49,6 +49,7 @@ from keras.src.ops.numpy import cross\n from keras.src.ops.numpy import cumprod\n from keras.src.ops.numpy import cumsum\n from keras.src.ops.numpy import diag\n+from keras.src.ops.numpy import diagflat\n from keras.src.ops.numpy import diagonal\n from keras.src.ops.numpy import diff\n from keras.src.ops.numpy import digitize\n\n@@ -146,6 +146,7 @@ from keras.src.ops.numpy import cross\n from keras.src.ops.numpy import cumprod\n from keras.src.ops.numpy import cumsum\n from keras.src.ops.numpy import diag\n+from keras.src.ops.numpy import diagflat\n from keras.src.ops.numpy import diagonal\n from keras.src.ops.numpy import diff\n from keras.src.ops.numpy import digitize\n\n@@ -49,6 +49,7 @@ from keras.src.ops.numpy import cross\n from keras.src.ops.numpy import cumprod\n from keras.src.ops.numpy import cumsum\n from keras.src.ops.numpy import diag\n+from keras.src.ops.numpy import diagflat\n from keras.src.ops.numpy import diagonal\n from keras.src.ops.numpy import diff\n from keras.src.ops.numpy import digitize\n\n@@ -525,6 +525,11 @@ def diag(x, k=0):\n     return jnp.diag(x, k=k)\n \n \n+def diagflat(x, k=0):\n+    x = convert_to_tensor(x)\n+    return jnp.diagflat(x, k=k)\n+\n+\n def diagonal(x, offset=0, axis1=0, axis2=1):\n     x = convert_to_tensor(x)\n     return jnp.diagonal(\n\n@@ -449,6 +449,10 @@ def diag(x, k=0):\n     return np.diag(x, k=k)\n \n \n+def diagflat(x, k=0):\n+    return np.diagflat(x, k=k)\n+\n+\n def diagonal(x, offset=0, axis1=0, axis2=1):\n     axis1 = standardize_axis_for_numpy(axis1)\n     axis2 = standardize_axis_for_numpy(axis2)\n\n@@ -1107,6 +1107,11 @@ def diag(x, k=0):\n         raise ValueError(f\"`x` must be 1d or 2d. Received: x.shape={x.shape}\")\n \n \n+def diagflat(x, k=0):\n+    x = convert_to_tensor(x)\n+    return diag(tf.reshape(x, [-1]), k)\n+\n+\n def diagonal(x, offset=0, axis1=0, axis2=1):\n     x = convert_to_tensor(x)\n     x_rank = x.ndim\n\n@@ -598,6 +598,11 @@ def diag(x, k=0):\n     return torch.diag(x, diagonal=k)\n \n \n+def diagflat(x, k=0):\n+    x = convert_to_tensor(x)\n+    return torch.diagflat(x, offset=k)\n+\n+\n def diagonal(x, offset=0, axis1=0, axis2=1):\n     x = convert_to_tensor(x)\n     return torch.diagonal(\n\n@@ -2062,6 +2062,62 @@ def diag(x, k=0):\n     return backend.numpy.diag(x, k=k)\n \n \n+class Diagflat(Operation):\n+    def __init__(self, k=0):\n+        super().__init__()\n+        self.k = k\n+\n+    def call(self, x):\n+        return backend.numpy.diagflat(x, k=self.k)\n+\n+    def compute_output_spec(self, x):\n+        x_shape = x.shape\n+\n+        if len(x_shape) == 0:\n+            flat_size = 1\n+        elif len(x_shape) == 1:\n+            flat_size = x_shape[0] if x_shape[0] is not None else None\n+        else:\n+            flat_size = None\n+            for s in x_shape:\n+                if s is None:\n+                    flat_size = None\n+                    break\n+                elif flat_size is None:\n+                    flat_size = s\n+                else:\n+                    flat_size *= s\n+\n+        if flat_size is None:\n+            output_shape = [None, None]\n+        else:\n+            output_shape = [\n+                flat_size + int(np.abs(self.k)),\n+                flat_size + int(np.abs(self.k)),\n+            ]\n+\n+        return KerasTensor(output_shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.diagflat\", \"keras.ops.numpy.diagflat\"])\n+def diagflat(x, k=0):\n+    \"\"\"Create a two-dimensional array with the flattened input on\n+       the k-th diagonal.\n+\n+    Args:\n+        x: Input tensor to be flattened and placed on the diagonal.\n+        k: The diagonal to place the flattened input. Defaults to `0`.\n+           Use `k > 0` for diagonals above the main diagonal,\n+           and `k < 0` for diagonals below the main diagonal.\n+\n+    Returns:\n+        A 2-D tensor with the flattened input on the specified diagonal.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Diagflat(k=k).symbolic_call(x)\n+    return backend.numpy.diagflat(x, k=k)\n+\n+\n class Diagonal(Operation):\n     def __init__(self, offset=0, axis1=0, axis2=1):\n         super().__init__()\n\n@@ -1233,6 +1233,19 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n             x = KerasTensor((2, 3, 4))\n             knp.diag(x)\n \n+    def test_diagflat(self):\n+        x = KerasTensor((3,))\n+        self.assertEqual(knp.diagflat(x).shape, (3, 3))\n+        self.assertEqual(knp.diagflat(x, k=1).shape, (4, 4))\n+        self.assertEqual(knp.diagflat(x, k=-1).shape, (4, 4))\n+\n+        x = KerasTensor((2, 3))\n+        self.assertEqual(knp.diagflat(x).shape, (6, 6))\n+        self.assertEqual(knp.diagflat(x, k=2).shape, (8, 8))\n+\n+        x = KerasTensor((None, 3))\n+        self.assertEqual(knp.diagflat(x).shape, (None, None))\n+\n     def test_diagonal(self):\n         x = KerasTensor((None, 3, 3))\n         self.assertEqual(knp.diagonal(x).shape, (3, None))\n@@ -1772,6 +1785,23 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n             x = KerasTensor((2, 3, 4))\n             knp.diag(x)\n \n+    def test_diagflat(self):\n+        x = KerasTensor((3,))\n+        self.assertEqual(knp.diagflat(x).shape, (3, 3))\n+        self.assertEqual(knp.diagflat(x, k=1).shape, (4, 4))\n+        self.assertEqual(knp.diagflat(x, k=-1).shape, (4, 4))\n+\n+        x = KerasTensor((2, 3))\n+        self.assertEqual(knp.diagflat(x).shape, (6, 6))\n+        self.assertEqual(knp.diagflat(x, k=1).shape, (7, 7))\n+        self.assertEqual(knp.diagflat(x, k=-1).shape, (7, 7))\n+\n+        x = KerasTensor((None, 3))\n+        self.assertEqual(knp.diagflat(x).shape, (None, None))\n+\n+        x = KerasTensor(())\n+        self.assertEqual(knp.diagflat(x).shape, (1, 1))\n+\n     def test_diagonal(self):\n         x = KerasTensor((3, 3))\n         self.assertEqual(knp.diagonal(x).shape, (3,))\n@@ -3637,6 +3667,33 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.Diag(k=1)(x), np.diag(x, k=1))\n         self.assertAllClose(knp.Diag(k=-1)(x), np.diag(x, k=-1))\n \n+    def test_diagflat(self):\n+        x = np.array([1, 2, 3])\n+        self.assertAllClose(knp.diagflat(x), np.diagflat(x))\n+        self.assertAllClose(knp.diagflat(x, k=1), np.diagflat(x, k=1))\n+        self.assertAllClose(knp.diagflat(x, k=-1), np.diagflat(x, k=-1))\n+\n+        x = np.array([[1, 2], [3, 4]])\n+        self.assertAllClose(knp.diagflat(x), np.diagflat(x))\n+        self.assertAllClose(knp.diagflat(x, k=1), np.diagflat(x, k=1))\n+        self.assertAllClose(knp.diagflat(x, k=-1), np.diagflat(x, k=-1))\n+\n+        x = np.array([1, 2, 3, 4])\n+        self.assertAllClose(knp.diagflat(x), np.diagflat(x))\n+        self.assertAllClose(knp.diagflat(x, k=2), np.diagflat(x, k=2))\n+        self.assertAllClose(knp.diagflat(x, k=-2), np.diagflat(x, k=-2))\n+\n+        x_float = np.array([1.1, 2.2, 3.3])\n+        self.assertAllClose(knp.diagflat(x_float), np.diagflat(x_float))\n+\n+        x_complex = np.array([1 + 1j, 2 + 2j, 3 + 3j])\n+        self.assertAllClose(knp.diagflat(x_complex), np.diagflat(x_complex))\n+\n+        x = np.array([1, 2, 3])\n+        self.assertAllClose(knp.Diagflat()(x), np.diagflat(x))\n+        self.assertAllClose(knp.Diagflat(k=1)(x), np.diagflat(x, k=1))\n+        self.assertAllClose(knp.Diagflat(k=-1)(x), np.diagflat(x, k=-1))\n+\n     def test_diagonal(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.diagonal(x), np.diagonal(x))\n@@ -6273,6 +6330,35 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_diagflat(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.diagflat(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.diagflat(x).dtype), expected_dtype\n+        )\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.Diagflat().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n+        x_2d = knp.ones((1, 1), dtype=dtype)\n+        x_jax_2d = jnp.ones((1, 1), dtype=dtype)\n+        expected_dtype_2d = standardize_dtype(jnp.diagflat(x_jax_2d).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.diagflat(x_2d).dtype), expected_dtype_2d\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Diagflat().symbolic_call(x_2d).dtype),\n+            expected_dtype_2d,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_diagonal(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1d423c6bd3dca1815a87505c5cf8bdb81b587d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 139 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 12 | Complexity Δ (Sum/Max): 15/4 | Churn Δ: 139 | Churn Cumulative: 5393 | Contributors (this commit): 23 | Commits (past 90d): 188 | Contributors (cumulative): 106 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,6 +32,7 @@ from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n+from keras.src.activations.activations import sparse_plus\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n\n@@ -99,6 +99,7 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n\n@@ -44,5 +44,6 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -32,6 +32,7 @@ from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n+from keras.src.activations.activations import sparse_plus\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n\n@@ -99,6 +99,7 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n\n@@ -44,5 +44,6 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -23,6 +23,7 @@ from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n+from keras.src.activations.activations import sparse_plus\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n@@ -42,6 +43,7 @@ ALL_OBJECTS = {\n     softsign,\n     squareplus,\n     soft_shrink,\n+    sparse_plus,\n     silu,\n     gelu,\n     glu,\n\n@@ -277,6 +277,23 @@ def soft_shrink(x, threshold=0.5):\n     return ops.soft_shrink(x, threshold=threshold)\n \n \n+@keras_export(\"keras.activations.sparse_plus\")\n+def sparse_plus(x):\n+    \"\"\"SparsePlus activation function.\n+\n+    SparsePlus is defined as:\n+\n+    `sparse_plus(x) = 0` for `x <= -1`.\n+    `sparse_plus(x) = (1/4) * (x + 1)^2` for `-1 < x < 1`.\n+    `sparse_plus(x) = x` for `x >= 1`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    \"\"\"\n+    return ops.sparse_plus(x)\n+\n+\n @keras_export([\"keras.activations.silu\", \"keras.activations.swish\"])\n def silu(x):\n     \"\"\"Swish (or Silu) activation function.\n\n@@ -706,6 +706,19 @@ class ActivationsTest(testing.TestCase):\n         expected = soft_shrink(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_sparse_plus(self):\n+        def sparse_plus(x):\n+            return np.where(\n+                x <= -1,\n+                np.zeros_like(x),\n+                np.where(x < 1, (1 / 4) * (x + 1) ** 2, x),\n+            )\n+\n+        x = np.random.random((2, 5))\n+        result = activations.sparse_plus(x[np.newaxis, :])[0]\n+        expected = sparse_plus(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_elu(self):\n         x = np.random.random((2, 5))\n         result = activations.elu(x[np.newaxis, :])[0]\n\n@@ -62,6 +62,11 @@ def soft_shrink(x, threshold=0.5):\n     )\n \n \n+def sparse_plus(x):\n+    x = convert_to_tensor(x)\n+    return jnn.sparse_plus(x)\n+\n+\n def silu(x):\n     x = convert_to_tensor(x)\n     return jnn.silu(x)\n\n@@ -62,6 +62,14 @@ def soft_shrink(x, threshold=0.5):\n     )\n \n \n+def sparse_plus(x):\n+    return np.where(\n+        x <= -1,\n+        np.zeros_like(x, dtype=x.dtype),\n+        np.where(x < 1, np.array((1 / 4) * (x + 1) ** 2, dtype=x.dtype), x),\n+    )\n+\n+\n def silu(x):\n     x = convert_to_tensor(x)\n     return x * sigmoid(x)\n\n@@ -50,6 +50,14 @@ def soft_shrink(x, threshold=0.5):\n     )\n \n \n+def sparse_plus(x):\n+    return tf.where(\n+        x <= -1,\n+        tf.zeros_like(x),\n+        tf.where(x < 1, (1 / 4) * tf.pow(x + 1, 2), x),\n+    )\n+\n+\n def silu(x):\n     return tf.nn.silu(x)\n \n\n@@ -55,6 +55,15 @@ def soft_shrink(x, threshold=0.5):\n     return tnn.softshrink(x, lambd=threshold)\n \n \n+def sparse_plus(x):\n+    x = convert_to_tensor(x)\n+    return torch.where(\n+        x <= -1,\n+        torch.zeros_like(x),\n+        torch.where(x < 1, (1 / 4) * (x + 1) ** 2, x),\n+    )\n+\n+\n def silu(x):\n     x = convert_to_tensor(x)\n     return tnn.silu(x)\n\n@@ -216,6 +216,44 @@ def soft_shrink(x, threshold=0.5):\n     return backend.nn.soft_shrink(x, threshold)\n \n \n+class SparsePlus(Operation):\n+    def call(self, x):\n+        return backend.nn.sparse_plus(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.sparse_plus\", \"keras.ops.nn.sparse_plus\"])\n+def sparse_plus(x):\n+    \"\"\"SparsePlus activation function.\n+\n+    It is defined as\n+\n+    `f(x) = 0` for `x <= -1`.\n+    `f(x) = (1/4) * (x + 1)^2` for `-1 < x < 1`.\n+    `f(x) = x` for `x >= 1`.\n+\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1.0, 0.0, 1.0])\n+    >>> x_sparse_plus = keras.ops.sparse_plus(x)\n+    >>> print(x_sparse_plus)\n+    Array([0.   0.25 1.  ], shape=(3,), dtype=float32)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return SparsePlus().symbolic_call(x)\n+    return backend.nn.sparse_plus(x)\n+\n+\n class Silu(Operation):\n     def call(self, x):\n         return backend.nn.silu(x)\n\n@@ -168,6 +168,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.soft_shrink(x).shape, (None, 2, 3))\n \n+    def test_sparse_plus(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.sparse_plus(x).shape, (None, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (None, 2, 3))\n@@ -841,6 +845,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.soft_shrink(x).shape, (1, 2, 3))\n \n+    def test_sparse_plus(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.sparse_plus(x).shape, (1, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (1, 2, 3))\n@@ -1396,6 +1404,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [0.0, 0.0, 0.5, 1.5, 2.5],\n         )\n \n+    def test_sparse_plus(self):\n+        x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.sparse_plus(x),\n+            [0.0625, 0.25, 1.0, 2.0, 3.0],\n+        )\n+\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n@@ -2572,6 +2587,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_sparse_plus(self, dtype):\n+        import jax.nn as jnn\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnn.sparse_plus(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.sparse_plus(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.SparsePlus().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_glu(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0f61ee57624700e5bfc6eb917a1d3b17653a778", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 75 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -278,7 +278,7 @@ class Normalization(TFDataLayer):\n                 ) * batch_weight\n                 total_mean = new_total_mean\n         else:\n-            raise NotImplementedError(type(data))\n+            raise NotImplementedError(f\"Unsupported data type: {type(data)}\")\n \n         self.adapt_mean.assign(total_mean)\n         self.adapt_variance.assign(total_var)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8d7e0dc533b22f412b94c0a9cae9c4c6cb964b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 39 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -74,13 +74,14 @@ class ModelCheckpoint(Callback):\n             which will be filled the value of `epoch` and keys in `logs`\n             (passed in `on_epoch_end`).\n             The `filepath` name needs to end with `\".weights.h5\"` when\n-            `save_weights_only=True` or should end with `\".keras\"` when\n-            checkpoint saving the whole model (default).\n+            `save_weights_only=True` or should end with `\".keras\"` or `\".h5\"`\n+            when checkpoint saving the whole model (default).\n             For example:\n-            if `filepath` is `\"{epoch:02d}-{val_loss:.2f}.keras\"`, then the\n-            model checkpoints will be saved with the epoch number and the\n-            validation loss in the filename. The directory of the filepath\n-            should not be reused by any other callbacks to avoid conflicts.\n+            if `filepath` is `\"{epoch:02d}-{val_loss:.2f}.keras\"` or\n+            \"{epoch:02d}-{val_loss:.2f}.h5\"`, then the model checkpoints\n+            will be saved with the epoch number and the validation loss in\n+            the filename. The directory of the filepath should not be reused\n+            by any other callbacks to avoid conflicts.\n         monitor: The metric name to monitor. Typically the metrics are set by\n             the `Model.compile` method. Note:\n             * Prefix the name with `\"val_\"` to monitor validation metrics.\n@@ -187,10 +188,12 @@ class ModelCheckpoint(Callback):\n                     f\"filepath={self.filepath}\"\n                 )\n         else:\n-            if not self.filepath.endswith(\".keras\"):\n+            if not any(\n+                self.filepath.endswith(ext) for ext in (\".keras\", \".h5\")\n+            ):\n                 raise ValueError(\n                     \"The filepath provided must end in `.keras` \"\n-                    \"(Keras model format). Received: \"\n+                    \"(Keras model format) or `.h5` (HDF5 format). Received: \"\n                     f\"filepath={self.filepath}\"\n                 )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#75522e404a919e97d5f63b4b540d9505855385ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 49 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -78,10 +78,10 @@ class ModelCheckpoint(Callback):\n             when checkpoint saving the whole model (default).\n             For example:\n             if `filepath` is `\"{epoch:02d}-{val_loss:.2f}.keras\"` or\n-            \"{epoch:02d}-{val_loss:.2f}.h5\"`, then the model checkpoints\n-            will be saved with the epoch number and the validation loss in\n-            the filename. The directory of the filepath should not be reused\n-            by any other callbacks to avoid conflicts.\n+            \"{epoch:02d}-{val_loss:.2f}.weights.h5\"`, then the model\n+            checkpoints will be saved with the epoch number and the validation\n+            loss in the filename. The directory of the filepath\n+            should not be reused by any other callbacks to avoid conflicts.\n         monitor: The metric name to monitor. Typically the metrics are set by\n             the `Model.compile` method. Note:\n             * Prefix the name with `\"val_\"` to monitor validation metrics.\n@@ -193,7 +193,7 @@ class ModelCheckpoint(Callback):\n             ):\n                 raise ValueError(\n                     \"The filepath provided must end in `.keras` \"\n-                    \"(Keras model format) or `.h5` (HDF5 format). Received: \"\n+                    \"(Keras model format). Received: \"\n                     f\"filepath={self.filepath}\"\n                 )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3a368d77eaaebef6dbc4dbf37e4c9b020a80a7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 215 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 18 | Methods Changed: 11 | Complexity Δ (Sum/Max): 14/5 | Churn Δ: 215 | Churn Cumulative: 5608 | Contributors (this commit): 24 | Commits (past 90d): 202 | Contributors (cumulative): 121 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import sparse_plus\n+from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n\n@@ -100,6 +100,7 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n\n@@ -45,5 +45,6 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -33,6 +33,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import sparse_plus\n+from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n\n@@ -100,6 +100,7 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n\n@@ -45,5 +45,6 @@ from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n from keras.src.ops.nn import sparse_plus\n+from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -24,6 +24,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import sparse_plus\n+from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n@@ -59,6 +60,7 @@ ALL_OBJECTS = {\n     mish,\n     log_softmax,\n     log_sigmoid,\n+    sparsemax,\n }\n \n ALL_OBJECTS_DICT = {fn.__name__: fn for fn in ALL_OBJECTS}\n\n@@ -617,3 +617,28 @@ def log_softmax(x, axis=-1):\n         axis: Integer, axis along which the softmax is applied.\n     \"\"\"\n     return ops.log_softmax(x, axis=axis)\n+\n+\n+@keras_export([\"keras.activations.sparsemax\"])\n+def sparsemax(x, axis=-1):\n+    \"\"\"Sparsemax activation function.\n+\n+    For each batch `i`, and class `j`,\n+    sparsemax activation function is defined as:\n+\n+    `sparsemax(x)[i, j] = max(x[i, j] - τ(x[i, :]), 0).`\n+\n+    Args:\n+        x: Input tensor.\n+        axis: `int`, axis along which the sparsemax operation is applied.\n+\n+    Returns:\n+        A tensor, output of sparsemax transformation. Has the same type and\n+        shape as `x`.\n+\n+    Reference:\n+\n+    - [Martins et.al., 2016](https://arxiv.org/abs/1602.02068)\n+    \"\"\"\n+    x = backend.convert_to_tensor(x)\n+    return ops.sparsemax(x, axis)\n\n@@ -896,6 +896,55 @@ class ActivationsTest(testing.TestCase):\n         x_int32 = np.random.randint(-10, 10, (10, 5)).astype(np.int32)\n         self.assertAllClose(x_int32, activations.linear(x_int32))\n \n+    def test_sparsemax(self):\n+        # result check with 1d\n+        x_1d = np.linspace(1, 12, num=12)\n+        expected_result = np.zeros_like(x_1d)\n+        expected_result[-1] = 1.0\n+        self.assertAllClose(expected_result, activations.sparsemax(x_1d))\n+\n+        # result check with 2d\n+        x_2d = np.linspace(1, 12, num=12).reshape(-1, 2)\n+        expected_result = np.zeros_like(x_2d)\n+        expected_result[:, -1] = 1.0\n+        self.assertAllClose(expected_result, activations.sparsemax(x_2d))\n+\n+        # result check with 3d\n+        x_3d = np.linspace(1, 12, num=12).reshape(-1, 1, 3)\n+        expected_result = np.zeros_like(x_3d)\n+        expected_result[:, :, -1] = 1.0\n+        self.assertAllClose(expected_result, activations.sparsemax(x_3d))\n+\n+        # result check with axis=-2 with 2d input\n+        x_2d = np.linspace(1, 12, num=12).reshape(-1, 2)\n+        expected_result = np.zeros_like(x_2d)\n+        expected_result[-1, :] = 1.0\n+        self.assertAllClose(\n+            expected_result, activations.sparsemax(x_2d, axis=-2)\n+        )\n+\n+        # result check with axis=-2 with 3d input\n+        x_3d = np.linspace(1, 12, num=12).reshape(-1, 1, 3)\n+        expected_result = np.ones_like(x_3d)\n+        self.assertAllClose(\n+            expected_result, activations.sparsemax(x_3d, axis=-2)\n+        )\n+\n+        # result check with axis=-3 with 3d input\n+        x_3d = np.linspace(1, 12, num=12).reshape(-1, 1, 3)\n+        expected_result = np.zeros_like(x_3d)\n+        expected_result[-1, :, :] = 1.0\n+        self.assertAllClose(\n+            expected_result, activations.sparsemax(x_3d, axis=-3)\n+        )\n+\n+        # result check with axis=-3 with 4d input\n+        x_4d = np.linspace(1, 12, num=12).reshape(-1, 1, 1, 2)\n+        expected_result = np.ones_like(x_4d)\n+        self.assertAllClose(\n+            expected_result, activations.sparsemax(x_4d, axis=-3)\n+        )\n+\n     def test_get_method(self):\n         obj = activations.get(\"relu\")\n         self.assertEqual(obj, activations.relu)\n\n@@ -142,6 +142,24 @@ def log_softmax(x, axis=-1):\n     return jnn.log_softmax(x, axis=axis)\n \n \n+def sparsemax(logits, axis=-1):\n+    # Sort logits along the specified axis in descending order\n+    logits = convert_to_tensor(logits)\n+    logits_sorted = -1.0 * jnp.sort(logits * -1.0, axis=axis)\n+    logits_cumsum = jnp.cumsum(logits_sorted, axis=axis)  # find cumulative sum\n+    r = jnp.arange(1, logits.shape[axis] + 1)  # Determine the sparsity\n+    r_shape = [1] * logits.ndim\n+    r_shape[axis] = -1  # Broadcast to match the target axis\n+    r = r.reshape(r_shape)\n+    support = logits_sorted - (logits_cumsum - 1) / r > 0\n+    # Find the threshold\n+    k = jnp.sum(support, axis=axis, keepdims=True)\n+    logits_cumsum_safe = jnp.where(support, logits_cumsum, 0.0)\n+    tau = (jnp.sum(logits_cumsum_safe, axis=axis, keepdims=True) - 1) / k\n+    output = jnp.maximum(logits - tau, 0.0)\n+    return output\n+\n+\n def _convert_to_spatial_operand(\n     x,\n     num_spatial_dims,\n\n@@ -191,6 +191,24 @@ def log_softmax(x, axis=None):\n     return x - max_x - logsumexp\n \n \n+def sparsemax(logits, axis=-1):\n+    # Sort logits along the specified axis in descending order\n+    logits = convert_to_tensor(logits)\n+    logits_sorted = -1.0 * np.sort(-1.0 * logits, axis=axis)\n+    logits_cumsum = np.cumsum(logits_sorted, axis=axis)\n+    r = np.arange(1, logits.shape[axis] + 1)\n+    r_shape = [1] * logits.ndim\n+    r_shape[axis] = -1  # Broadcast to match the target axis\n+    r = r.reshape(r_shape)\n+    support = logits_sorted - (logits_cumsum - 1) / r > 0\n+    # Find the threshold\n+    k = np.sum(support, axis=axis, keepdims=True)\n+    logits_cumsum_safe = np.where(support, logits_cumsum, 0.0)\n+    tau = (np.sum(logits_cumsum_safe, axis=axis, keepdims=True) - 1) / k\n+    output = np.maximum(logits - tau, 0.0)\n+    return output\n+\n+\n def _convert_to_spatial_operand(\n     x,\n     num_spatial_dims,\n\n@@ -151,6 +151,24 @@ def log_softmax(x, axis=-1):\n     return tf.nn.log_softmax(x, axis=axis)\n \n \n+def sparsemax(logits, axis=-1):\n+    # Sort logits along the specified axis in descending order\n+    logits = convert_to_tensor(logits)\n+    logits_sorted = tf.sort(logits, direction=\"DESCENDING\", axis=axis)\n+    logits_cumsum = tf.cumsum(logits_sorted, axis=axis)\n+    r = tf.range(1, tf.shape(logits)[axis] + 1, dtype=logits.dtype)\n+    r_shape = [1] * len(logits.shape)\n+    r_shape[axis] = -1  # Broadcast to match the target axis\n+    r = tf.reshape(r, r_shape)  # Reshape for broadcasting\n+    support = logits_sorted - (logits_cumsum - 1) / r > 0\n+    # Find the threshold\n+    logits_cumsum_safe = tf.where(support, logits_cumsum, 0.0)\n+    k = tf.reduce_sum(tf.cast(support, logits.dtype), axis=axis, keepdims=True)\n+    tau = (tf.reduce_sum(logits_cumsum_safe, axis=axis, keepdims=True) - 1) / k\n+    output = tf.maximum(logits - tau, 0.0)\n+    return output\n+\n+\n def _transpose_spatial_inputs(inputs):\n     num_spatial_dims = len(inputs.shape) - 2\n     # Tensorflow pooling does not support `channels_first` format, so\n\n@@ -174,6 +174,28 @@ def log_softmax(x, axis=-1):\n     return cast(output, dtype)\n \n \n+def sparsemax(logits, axis=-1):\n+    # Sort logits along the specified axis in descending order\n+    logits = convert_to_tensor(logits)\n+    logits_sorted, _ = torch.sort(logits, dim=axis, descending=True)\n+    logits_cumsum = torch.cumsum(logits_sorted, dim=axis)\n+    r = torch.arange(\n+        1, logits.size(axis) + 1, device=logits.device, dtype=logits.dtype\n+    )\n+    r_shape = [1] * logits.ndim\n+    r_shape[axis] = -1  # Broadcast to match the target axis\n+    r = r.view(r_shape)\n+    support = logits_sorted - (logits_cumsum - 1) / r > 0\n+    # Find the threshold\n+    k = torch.sum(support, dim=axis, keepdim=True)\n+    logits_cumsum_safe = torch.where(\n+        support, logits_cumsum, torch.tensor(0.0, device=logits.device)\n+    )\n+    tau = (torch.sum(logits_cumsum_safe, dim=axis, keepdim=True) - 1) / k\n+    output = torch.clamp(logits - tau, min=0.0)\n+    return output\n+\n+\n def _compute_padding_length(\n     input_length, kernel_length, stride, dilation_rate=1\n ):\n\n@@ -951,6 +951,48 @@ def log_softmax(x, axis=-1):\n         return backend.nn.log_softmax(x, axis=axis)\n \n \n+class Sparsemax(Operation):\n+    def __init__(self, axis=-1):\n+        super().__init__()\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.nn.sparsemax(x, axis=self.axis)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.sparsemax\", \"keras.ops.nn.sparsemax\"])\n+def sparsemax(x, axis=-1):\n+    \"\"\"Sparsemax activation function.\n+\n+    For each batch `i`, and class `j`,\n+    sparsemax activation function is defined as:\n+\n+    `sparsemax(x)[i, j] = max(x[i, j] - τ(x[i, :]), 0).`\n+\n+    Args:\n+        x: Input tensor.\n+        axis: `int`, axis along which the sparsemax operation is applied.\n+\n+    Returns:\n+        A tensor, output of sparsemax transformation. Has the same type and\n+        shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1., 0., 1.])\n+    >>> x_sparsemax = keras.ops.sparsemax(x)\n+    >>> print(x_sparsemax)\n+    array([0., 0., 1.], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Sparsemax(axis).symbolic_call(x)\n+    return backend.nn.sparsemax(x, axis=axis)\n+\n+\n class MaxPool(Operation):\n     def __init__(\n         self,\n\n@@ -200,6 +200,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knn.log_softmax(x, axis=1).shape, (None, 2, 3))\n         self.assertEqual(knn.log_softmax(x, axis=-1).shape, (None, 2, 3))\n \n+    def test_sparsemax(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.sparsemax(x).shape, (None, 2, 3))\n+\n     def test_max_pool(self):\n         data_format = backend.config.image_data_format()\n         if data_format == \"channels_last\":\n@@ -861,6 +865,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knn.log_softmax(x, axis=1).shape, (1, 2, 3))\n         self.assertEqual(knn.log_softmax(x, axis=-1).shape, (1, 2, 3))\n \n+    def test_sparsemax(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.sparsemax(x).shape, (1, 2, 3))\n+\n     def test_max_pool(self):\n         data_format = backend.config.image_data_format()\n         if data_format == \"channels_last\":\n@@ -1487,6 +1495,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             )\n             self.assertAllClose(normalized_sum_by_axis, 1.0)\n \n+    def test_sparsemax(self):\n+        x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.sparsemax(x),\n+            [0.0, 0.0, 0.0, 0.0, 1.0],\n+        )\n+\n     def test_max_pool(self):\n         data_format = backend.config.image_data_format()\n         # Test 1D max pooling.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d5774e14bf1f9c93f80525ea70823d38f2f2fb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 43 | Churn Cumulative: 1526 | Contributors (this commit): 10 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1388,6 +1388,7 @@ class Tversky(LossFunctionWrapper):\n         beta=0.5,\n         reduction=\"sum_over_batch_size\",\n         name=\"tversky\",\n+        axis=None,\n         dtype=None,\n     ):\n         super().__init__(\n@@ -1397,13 +1398,17 @@ class Tversky(LossFunctionWrapper):\n             dtype=dtype,\n             alpha=alpha,\n             beta=beta,\n+            axis=axis,\n         )\n         self.alpha = alpha\n         self.beta = beta\n+        self.axis = axis\n \n     def get_config(self):\n         config = Loss.get_config(self)\n-        config.update({\"alpha\": self.alpha, \"beta\": self.beta})\n+        config.update(\n+            {\"alpha\": self.alpha, \"beta\": self.beta, \"axis\": self.axis}\n+        )\n         return config\n \n \n@@ -2465,7 +2470,7 @@ def dice(y_true, y_pred, axis=None):\n \n \n @keras_export(\"keras.losses.tversky\")\n-def tversky(y_true, y_pred, alpha=0.5, beta=0.5):\n+def tversky(y_true, y_pred, alpha=0.5, beta=0.5, axis=None):\n     \"\"\"Computes the Tversky loss value between `y_true` and `y_pred`.\n \n     This loss function is weighted by the alpha and beta coefficients\n@@ -2479,6 +2484,7 @@ def tversky(y_true, y_pred, alpha=0.5, beta=0.5):\n         y_pred: tensor of predicted targets.\n         alpha: coefficient controlling incidence of false positives.\n         beta: coefficient controlling incidence of false negatives.\n+        axis: tuple for which dimensions the loss is calculated.\n \n     Returns:\n         Tversky loss value.\n@@ -2490,12 +2496,13 @@ def tversky(y_true, y_pred, alpha=0.5, beta=0.5):\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n \n-    inputs = ops.reshape(y_true, [-1])\n-    targets = ops.reshape(y_pred, [-1])\n+    inputs = y_true\n+    targets = y_pred\n+\n+    intersection = ops.sum(inputs * targets, axis=axis)\n+    fp = ops.sum((1 - targets) * inputs, axis=axis)\n+    fn = ops.sum(targets * (1 - inputs), axis=axis)\n \n-    intersection = ops.sum(inputs * targets)\n-    fp = ops.sum((1 - targets) * inputs)\n-    fn = ops.sum(targets * (1 - inputs))\n     tversky = ops.divide(\n         intersection,\n         intersection + fp * alpha + fn * beta + backend.epsilon(),\n\n@@ -1630,6 +1630,16 @@ class TverskyTest(testing.TestCase):\n         output = losses.Tversky()(y_true, y_pred)\n         self.assertAllClose(output, 0.77777773)\n \n+    def test_binary_segmentation_with_axis(self):\n+        y_true = np.array(\n+            [[[[1.0], [1.0]], [[0.0], [0.0]]], [[[1.0], [1.0]], [[0.0], [0.0]]]]\n+        )\n+        y_pred = np.array(\n+            [[[[0.0], [1.0]], [[0.0], [1.0]]], [[[0.4], [0.0]], [[0.0], [0.9]]]]\n+        )\n+        output = losses.Tversky(axis=(1, 2, 3), reduction=None)(y_true, y_pred)\n+        self.assertAllClose(output, [0.5, 0.75757575])\n+\n     def test_binary_segmentation_custom_coefficients(self):\n         y_true = np.array(\n             ([[1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1]])\n@@ -1640,6 +1650,18 @@ class TverskyTest(testing.TestCase):\n         output = losses.Tversky(alpha=0.2, beta=0.8)(y_true, y_pred)\n         self.assertAllClose(output, 0.7916667)\n \n+    def test_binary_segmentation_custom_coefficients_with_axis(self):\n+        y_true = np.array(\n+            [[[[1.0], [1.0]], [[0.0], [0.0]]], [[[1.0], [1.0]], [[0.0], [0.0]]]]\n+        )\n+        y_pred = np.array(\n+            [[[[0.0], [1.0]], [[0.0], [1.0]]], [[[0.4], [0.0]], [[0.0], [0.9]]]]\n+        )\n+        output = losses.Tversky(\n+            alpha=0.2, beta=0.8, axis=(1, 2, 3), reduction=None\n+        )(y_true, y_pred)\n+        self.assertAllClose(output, [0.5, 0.7222222])\n+\n     def test_dtype_arg(self):\n         y_true = np.array(([[1, 2], [1, 2]]))\n         y_pred = np.array(([[4, 1], [6, 1]]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#461fbf3f5e727b130da77babe978963c9a1741bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 49 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,6 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n         dtype: (Optional) data type of the metric result.\n \n     Example:\n-\n     >>> m = keras.metrics.MeanSquaredError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n@@ -64,6 +63,7 @@ class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.25\n+\n     >>> m.reset_state()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n     ...                sample_weight=[1, 0])\n@@ -103,14 +103,12 @@ class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n-    Example:\n-\n-    Example:\n-\n+    Examples:\n     >>> m = keras.metrics.MeanAbsolutePercentageError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     250000000.0\n+\n     >>> m.reset_state()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n     ...                sample_weight=[1, 0])\n@@ -150,14 +148,13 @@ class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n-    Example:\n-\n-    Example:\n+    Examples:\n \n     >>> m = keras.metrics.MeanSquaredLogarithmicError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.12011322\n+\n     >>> m.reset_state()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n     ...                sample_weight=[1, 0])\n@@ -197,9 +194,7 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n-    Example:\n-\n-    Example:\n+    Examples:\n \n     >>> m = keras.metrics.RootMeanSquaredError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n@@ -270,8 +265,7 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n         axis: (Optional) Defaults to `-1`. The dimension along which the cosine\n             similarity is computed.\n \n-\n-    Example:\n+    Examples:\n \n     >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n     >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n@@ -282,6 +276,7 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n     >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n     >>> m.result()\n     0.49999997\n+\n     >>> m.reset_state()\n     >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],\n     ...                sample_weight=[0.3, 0.7])\n@@ -322,14 +317,13 @@ class LogCoshError(reduction_metrics.MeanMetricWrapper):\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n-    Example:\n-\n-    Example:\n+    Examples:\n \n     >>> m = keras.metrics.LogCoshError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.10844523\n+\n     >>> m.reset_state()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n     ...                sample_weight=[1, 0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c05c2c45293a4f6a734fc855d37c1fbb2b1b260", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 27 | Methods Changed: 24 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 76 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import os\n \n import numpy as np\n import pytest\n+import tf_keras\n \n import keras\n from keras.src import layers\n@@ -16,9 +17,6 @@ from keras.src.saving import serialization_lib\n # on exact weight ordering for each layer, so we need\n # to test across all types of layers.\n \n-# TODO: re-enable tests after tf_keras is available.\n-tf_keras = None\n-\n \n def get_sequential_model(keras):\n     return keras.Sequential(\n@@ -77,19 +75,19 @@ class LegacyH5WeightsTest(testing.TestCase):\n         output = model(ref_input)\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n-    def DISABLED_test_sequential_model_weights(self):\n+    def test_sequential_model_weights(self):\n         model = get_sequential_model(keras)\n         tf_keras_model = get_sequential_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_weights(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_functional_model_weights(self):\n+    def test_functional_model_weights(self):\n         model = get_functional_model(keras)\n         tf_keras_model = get_functional_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_weights(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_subclassed_model_weights(self):\n+    def test_subclassed_model_weights(self):\n         model = get_subclassed_model(keras)\n         tf_keras_model = get_subclassed_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n@@ -107,27 +105,27 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         output = loaded(ref_input)\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n-    def DISABLED_test_sequential_model(self):\n+    def test_sequential_model(self):\n         model = get_sequential_model(keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model)\n \n-    def DISABLED_test_functional_model(self):\n+    def test_functional_model(self):\n         model = get_functional_model(keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model)\n \n-    def DISABLED_test_compiled_model_with_various_layers(self):\n+    def test_compiled_model_with_various_layers(self):\n         model = models.Sequential()\n         model.add(layers.Dense(2, input_shape=(3,)))\n         model.add(layers.RepeatVector(3))\n         model.add(layers.TimeDistributed(layers.Dense(3)))\n \n-        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+        model.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n         ref_input = np.random.random((1, 3))\n         self._check_reloading_model(ref_input, model)\n \n-    def DISABLED_test_saving_lambda(self):\n+    def test_saving_lambda(self):\n         mean = ops.random.uniform((4, 2, 3))\n         std = ops.abs(ops.random.uniform((4, 2, 3))) + 1e-5\n         inputs = layers.Input(shape=(4, 2, 3))\n@@ -136,7 +134,9 @@ class LegacyH5WholeModelTest(testing.TestCase):\n             arguments={\"mu\": mean, \"std\": std},\n         )(inputs)\n         model = models.Model(inputs, output)\n-        model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"acc\"])\n+        model.compile(\n+            loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=[\"acc\"]\n+        )\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n         legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n@@ -145,10 +145,10 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         self.assertAllClose(mean, loaded.layers[1].arguments[\"mu\"])\n         self.assertAllClose(std, loaded.layers[1].arguments[\"std\"])\n \n-    def DISABLED_test_saving_include_optimizer_false(self):\n+    def test_saving_include_optimizer_false(self):\n         model = models.Sequential()\n         model.add(layers.Dense(1))\n-        model.compile(\"adam\", loss=\"mse\")\n+        model.compile(\"adam\", loss=\"mean_squared_error\")\n         x, y = np.ones((10, 10)), np.ones((10, 1))\n         model.fit(x, y)\n         ref_output = model(x)\n@@ -167,7 +167,7 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         # Compare output\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n-    def DISABLED_test_custom_sequential_registered_no_scope(self):\n+    def test_custom_sequential_registered_no_scope(self):\n         @object_registration.register_keras_serializable(package=\"my_package\")\n         class MyDense(layers.Dense):\n             def __init__(self, units, **kwargs):\n@@ -180,7 +180,7 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         ref_input = np.array([5])\n         self._check_reloading_model(ref_input, model)\n \n-    def DISABLED_test_custom_functional_registered_no_scope(self):\n+    def test_custom_functional_registered_no_scope(self):\n         @object_registration.register_keras_serializable(package=\"my_package\")\n         class MyDense(layers.Dense):\n             def __init__(self, units, **kwargs):\n@@ -193,7 +193,7 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         ref_input = np.array([5])\n         self._check_reloading_model(ref_input, model)\n \n-    def DISABLED_test_nested_layers(self):\n+    def test_nested_layers(self):\n         class MyLayer(layers.Layer):\n             def __init__(self, sublayers, **kwargs):\n                 super().__init__(**kwargs)\n@@ -273,19 +273,19 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         output = loaded(ref_input)\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n-    def DISABLED_test_sequential_model(self):\n+    def test_sequential_model(self):\n         model = get_sequential_model(keras)\n         tf_keras_model = get_sequential_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_functional_model(self):\n+    def test_functional_model(self):\n         tf_keras_model = get_functional_model(tf_keras)\n         model = get_functional_model(keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_compiled_model_with_various_layers(self):\n+    def test_compiled_model_with_various_layers(self):\n         model = models.Sequential()\n         model.add(layers.Dense(2, input_shape=(3,)))\n         model.add(layers.RepeatVector(3))\n@@ -298,12 +298,12 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         tf_keras_model.add(\n             tf_keras.layers.TimeDistributed(tf_keras.layers.Dense(3))\n         )\n-        tf_keras_model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+        tf_keras_model.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n \n         ref_input = np.random.random((1, 3))\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_saving_lambda(self):\n+    def test_saving_lambda(self):\n         mean = np.random.random((4, 2, 3))\n         std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n         inputs = tf_keras.layers.Input(shape=(4, 2, 3))\n@@ -313,7 +313,9 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n             output_shape=inputs.shape,\n         )(inputs)\n         tf_keras_model = tf_keras.Model(inputs, output)\n-        tf_keras_model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"acc\"])\n+        tf_keras_model.compile(\n+            loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=[\"acc\"]\n+        )\n \n         temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n         tf_keras_model.save(temp_filepath)\n@@ -322,7 +324,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         self.assertAllClose(mean, loaded.layers[1].arguments[\"mu\"])\n         self.assertAllClose(std, loaded.layers[1].arguments[\"std\"])\n \n-    def DISABLED_test_saving_include_optimizer_false(self):\n+    def test_saving_include_optimizer_false(self):\n         tf_keras_model = tf_keras.Sequential()\n         tf_keras_model.add(tf_keras.layers.Dense(1))\n         tf_keras_model.compile(\"adam\", loss=\"mse\")\n@@ -342,7 +344,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         # Compare output\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n-    def DISABLED_test_custom_sequential_registered_no_scope(self):\n+    def test_custom_sequential_registered_no_scope(self):\n         @tf_keras.saving.register_keras_serializable(package=\"my_package\")\n         class MyDense(tf_keras.layers.Dense):\n             def __init__(self, units, **kwargs):\n@@ -365,7 +367,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         ref_input = np.array([5])\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_custom_functional_registered_no_scope(self):\n+    def test_custom_functional_registered_no_scope(self):\n         @tf_keras.saving.register_keras_serializable(package=\"my_package\")\n         class MyDense(tf_keras.layers.Dense):\n             def __init__(self, units, **kwargs):\n@@ -388,7 +390,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         ref_input = np.array([5])\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n-    def DISABLED_test_nested_layers(self):\n+    def test_nested_layers(self):\n         class MyLayer(tf_keras.layers.Layer):\n             def __init__(self, sublayers, **kwargs):\n                 super().__init__(**kwargs)\n@@ -487,7 +489,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n \n @pytest.mark.requires_trainable_backend\n class DirectoryCreationTest(testing.TestCase):\n-    def DISABLED_test_directory_creation_on_save(self):\n+    def test_directory_creation_on_save(self):\n         \"\"\"Test if directory is created on model save.\"\"\"\n         model = get_sequential_model(keras)\n         nested_dirpath = os.path.join(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
