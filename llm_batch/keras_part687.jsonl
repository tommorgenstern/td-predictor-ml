{"custom_id": "keras#c9399c9d9d66ad2af3e71639590ab1a8be6c98bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 921 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -118,11 +118,11 @@ class RandomBrightness(TFDataLayer):\n     def call(self, inputs, training=True):\n         inputs = self.backend.cast(inputs, self.compute_dtype)\n         if training:\n-            return self._brightness_adjust(inputs)\n+            return self._randomly_adjust_brightness(inputs)\n         else:\n             return inputs\n \n-    def _brightness_adjust(self, images):\n+    def _randomly_adjust_brightness(self, images):\n         rank = len(images.shape)\n         if rank == 3:\n             rgb_delta_shape = (1, 1, 1)\n\n@@ -39,10 +39,8 @@ class RandomFlip(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, mode=HORIZONTAL_AND_VERTICAL, seed=None, name=None, **kwargs\n-    ):\n-        super().__init__(name=name)\n+    def __init__(self, mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs):\n+        super().__init__(**kwargs)\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n         self.mode = mode\n@@ -50,10 +48,9 @@ class RandomFlip(TFDataLayer):\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n-    def call(self, inputs, training=True):\n-        def random_flipped_inputs(inputs):\n+    def _randomly_flip_inputs(self, inputs):\n         flipped_outputs = inputs\n-            seed_generator = self._get_seed_generator()\n+        seed_generator = self._get_seed_generator(self.backend._backend)\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n             flipped_outputs = self.backend.cond(\n                 self.backend.random.uniform(shape=(), seed=seed_generator)\n@@ -70,9 +67,10 @@ class RandomFlip(TFDataLayer):\n             )\n         return flipped_outputs\n \n+    def call(self, inputs, training=True):\n         inputs = self.backend.cast(inputs, self.compute_dtype)\n         if training:\n-            return random_flipped_inputs(inputs)\n+            return self._randomly_flip_inputs(inputs)\n         else:\n             return inputs\n \n\n@@ -120,7 +120,6 @@ class RandomTranslation(TFDataLayer):\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n         self.data_format = backend.standardize_data_format(data_format)\n-\n         self.supports_jit = False\n \n     def _set_factor(self, factor, factor_name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f7d0bc8b282d6f2b49259c75074524bdde36fff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 827 | Contributors (this commit): 6 | Commits (past 90d): 15 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -514,6 +514,37 @@ class Amin(Operation):\n \n @keras_core_export([\"keras_core.ops.amin\", \"keras_core.ops.numpy.amin\"])\n def amin(x, axis=None, keepdims=False):\n+    \"\"\"Returns the minimum of an array or minimum value along an axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis along which to compute the minimum.\n+            By default (`axis=None`), find the minimum value in all the\n+            dimensions of the input array.\n+        keep_dims: If `True`, axes which are reduced are left in the result as\n+            dimensions that are broadcast to the size of the original\n+            input tensor. Defaults to `False`.\n+\n+    Returns:\n+        An array with the minimum value. If `axis=None`, the result is a scalar\n+        value representing the minimum element in the entire array. If `axis` is\n+        given, the result is an array with the minimum values along\n+        the specified axis.\n+\n+    Examples:\n+\n+    >>> x = keras_core.ops.convert_to_tensor([1, 3, 5, 2, 3, 6])\n+    >>> keras_core.ops.amin(x)\n+    array(1, dtype=int32)\n+\n+    >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [7, 5, 3]])\n+    >>> keras_core.ops.amin(x, axis=0)\n+    array([1,5,3], dtype=int32)\n+\n+    >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [7, 5, 3]])\n+    >>> keras_core.ops.amin(x, axis=1, keepdims=True)\n+    array([[1],[3]], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Amin(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.amin(x, axis=axis, keepdims=keepdims)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5bea3f6bfd83551529e0c56416ecb963e5a9691e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 79 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ def serialize(constraint):\n     return serialization_lib.serialize_keras_object(constraint)\n \n \n-@keras_core_export(\"keras_core.initializers.deserialize\")\n+@keras_core_export(\"keras_core.constraints.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Return a Keras constraint object via its config.\"\"\"\n     return serialization_lib.deserialize_keras_object(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7414812aed9eadfae923fb431195faac7f37470d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 300 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -1,7 +1,7 @@\n import numpy as np\n import tensorflow as tf\n-from jax import numpy as jnp\n \n+from keras_core import backend\n from keras_core import ops\n from keras_core import testing\n from keras_core.backend.common import keras_tensor\n@@ -37,6 +37,9 @@ class KerasTensorTest(testing.TestCase):\n         ):\n             np.array(x)\n \n+        if backend.backend() == \"jax\":\n+            from jax import numpy as jnp\n+\n             with self.assertRaisesRegex(\n                 ValueError, \"cannot be used as input to a JAX function\"\n             ):\n\n@@ -1,8 +1,6 @@\n-import jax\n import numpy as np\n import pandas\n import tensorflow as tf\n-import torch\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -11,9 +9,7 @@ from keras_core.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters(\n-        [(\"np\",), (\"tf\",), (\"jax\",), (\"torch\",), (\"pandas\")]\n-    )\n+    @parameterized.parameters([(\"np\",), (\"tf\",), (\"backend\",), (\"pandas\",)])\n     def test_basic_flow(self, array_type):\n         if array_type == \"np\":\n             x = np.random.random((34, 4))\n@@ -21,12 +17,19 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n         elif array_type == \"tf\":\n             x = tf.random.normal((34, 4))\n             y = tf.random.normal((34, 2))\n-        elif array_type == \"jax\":\n+        elif array_type == \"backend\":\n+            if backend.backend() == \"jax\":\n+                import jax\n+\n                 x = jax.numpy.ones((34, 4))\n                 y = jax.numpy.ones((34, 2))\n-        elif array_type == \"torch\":\n+            elif backend.backend() == \"torch\":\n+                import torch\n+\n                 x = torch.ones((34, 4))\n                 y = torch.ones((34, 2))\n+            else:\n+                return  # skip TF already addressed\n         elif array_type == \"pandas\":\n             x = pandas.DataFrame(np.random.random((34, 4)))\n             y = pandas.DataFrame(np.random.random((34, 2)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ca4bfc8c2a9eba21f5dd0271faac97ac906ae49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 238 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 29/17 | Churn Δ: 238 | Churn Cumulative: 568 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 0.44970414201183434\n\nDIFF:\n@@ -3,6 +3,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n from keras_core.backend import any_symbolic_tensors\n from keras_core.ops.operation import Operation\n+from keras_core.ops.operation_utils import compute_conv_output_shape\n \n \n class Resize(Operation):\n@@ -244,3 +245,159 @@ def affine_transform(\n         fill_value=fill_value,\n         data_format=data_format,\n     )\n+\n+\n+class ExtractPatches(Operation):\n+    def __init__(\n+        self,\n+        size,\n+        strides=None,\n+        dilation_rate=1,\n+        padding=\"valid\",\n+        data_format=\"channels_last\",\n+    ):\n+        super().__init__()\n+        self.size = size\n+        self.strides = strides\n+        self.dilation_rate = dilation_rate\n+        self.padding = padding\n+        self.data_format = data_format\n+\n+    def call(self, image):\n+        return _extract_patches(\n+            image=image,\n+            size=self.size,\n+            strides=self.strides,\n+            dilation_rate=self.dilation_rate,\n+            padding=self.padding,\n+            data_format=self.data_format,\n+        )\n+\n+    def compute_output_spec(self, image):\n+        image_shape = image.shape\n+        if not self.strides:\n+            strides = (self.size[0], self.size[1])\n+        if self.data_format == \"channels_last\":\n+            channels_in = image.shape[-1]\n+        else:\n+            channels_in = image.shape[-3]\n+        if len(image.shape) == 3:\n+            image_shape = (1,) + image_shape\n+        filters = self.size[0] * self.size[1] * channels_in\n+        kernel_size = (self.size[0], self.size[1])\n+        out_shape = compute_conv_output_shape(\n+            image_shape,\n+            filters,\n+            kernel_size,\n+            strides=strides,\n+            padding=self.padding,\n+            data_format=self.data_format,\n+            dilation_rate=self.dilation_rate,\n+        )\n+        if len(image.shape) == 3:\n+            out_shape = out_shape[1:]\n+        return KerasTensor(shape=out_shape, dtype=image.dtype)\n+\n+\n+@keras_core_export(\"keras_core.ops.image.extract_patches\")\n+def extract_patches(\n+    image,\n+    size,\n+    strides=None,\n+    dilation_rate=1,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+):\n+    \"\"\"Extracts patches from the image(s).\n+\n+    Args:\n+        image: Input image or batch of images. Must be 3D or 4D.\n+        size: Patch size int or tuple (patch_height, patch_widht)\n+        strides: strides along height and width. if not specified or\n+                None it is same patch_size\n+        dilation_rate: This is the input stride, specifying how far two\n+            consecutive patch samples are in the input. For value other than 1,\n+            strides must be 1. NOTE: `strides > 1` not supported in\n+            conjunction with `dilation_rate > 1`\n+        padding: The type of padding algorithm to use.\n+            'same' or 'valid'.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, weight)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+\n+    Returns:\n+        Extracted patches 3D (if not batched) or 4D (if batched)\n+\n+    Examples:\n+\n+    >>> image = np.random.random((1, 20, 20, 3)) # batch of 2 RGB images\n+    >>> patches = keras_core.ops.image.extract_patches(image, (5, 5))\n+    >>> patches.shape\n+    (1, 4, 4, 75)\n+    >>> image = np.random.random((20, 20, 3)) # batch of 2 RGB images\n+    >>> patches = keras_core.ops.image.extract_patches(image, (3, 3), (1, 1))\n+    >>> patches.shape\n+    (4, 4, 75)\n+    \"\"\"\n+    if any_symbolic_tensors((image,)):\n+        return ExtractPatches(\n+            size=size,\n+            strides=strides,\n+            dilation_rate=dilation_rate,\n+            padding=padding,\n+            data_format=data_format,\n+        ).symbolic_call(image)\n+\n+    return _extract_patches(\n+        image, size, strides, dilation_rate, padding, data_format=data_format\n+    )\n+\n+\n+def _extract_patches(\n+    image,\n+    size,\n+    strides=None,\n+    dilation_rate=1,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+):\n+    if isinstance(size, int):\n+        patch_h = patch_w = size\n+    elif len(size) == 2:\n+        patch_h, patch_w = size[0], size[1]\n+    else:\n+        raise TypeError(\n+            \"invalid size argument \"\n+            f\"int or tuple of length 2. Received {size}\"\n+        )\n+    if data_format == \"channels_last\":\n+        channels_in = image.shape[-1]\n+    elif data_format == \"channels_first\":\n+        channels_in = image.shape[-3]\n+    if not strides:\n+        strides = size\n+    out_dim = patch_h * patch_w * channels_in\n+    kernel = backend.numpy.eye(out_dim)\n+    kernel = backend.numpy.reshape(\n+        kernel, (patch_h, patch_w, channels_in, out_dim)\n+    )\n+    _unbatched = False\n+    if len(image.shape) == 3:\n+        _unbatched = True\n+        image = backend.numpy.expand_dims(image, axis=0)\n+    patches = backend.nn.conv(\n+        inputs=image,\n+        kernel=kernel,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+    )\n+    if _unbatched:\n+        patches = backend.numpy.squeeze(patches, axis=0)\n+    return patches\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import tensorflow as tf\n from absl.testing import parameterized\n+import pytest\n \n from keras_core import backend\n from keras_core import testing\n@@ -24,6 +25,12 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.affine_transform(x, transform)\n         self.assertEqual(out.shape, (None, 20, 20, 3))\n \n+    def test_extract_patches(self):\n+        x = KerasTensor([None, 20, 20, 3])\n+        p_h, p_w = 5, 5\n+        out = kimage.extract_patches(x, (p_h, p_w))\n+        self.assertEqual(out.shape, (None, 4, 4, 75))\n+\n \n class ImageOpsStaticShapeTest(testing.TestCase):\n     def test_resize(self):\n@@ -37,6 +44,12 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         out = kimage.affine_transform(x, transform)\n         self.assertEqual(out.shape, (20, 20, 3))\n \n+    def test_extract_patches(self):\n+        x = KerasTensor([20, 20, 3])\n+        p_h, p_w = 5, 5\n+        out = kimage.extract_patches(x, (p_h, p_w))\n+        self.assertEqual(out.shape, (4, 4, 75))\n+\n \n class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n@@ -208,3 +221,71 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(ref_out, out, atol=0.3)\n         else:\n             self.assertAllClose(ref_out, out, atol=0.3)\n+\n+    @parameterized.parameters(\n+        [\n+            ((5, 5), None, 1, \"valid\", \"channels_last\"),\n+            ((3, 3), (2, 2), 1, \"valid\", \"channels_last\"),\n+            ((5, 5), None, 1, \"valid\", \"channels_first\"),\n+            ((3, 3), (2, 2), 1, \"valid\", \"channels_first\"),\n+            ((5, 5), None, 1, \"same\", \"channels_last\"),\n+            ((3, 3), (2, 2), 1, \"same\", \"channels_last\"),\n+            ((5, 5), None, 1, \"same\", \"channels_first\"),\n+            ((3, 3), (2, 2), 1, \"same\", \"channels_first\"),\n+            ((5, 5), (1, 1), 3, \"same\", \"channels_first\"),\n+            ((5, 5), (2, 2), 3, \"same\", \"channels_first\"),\n+            ((5, 5), (2, 2), 3, \"same\", \"channels_last\"),\n+        ]\n+    )\n+    def test_extract_patches(\n+        self, size, strides, dilation_rate, padding, data_format\n+    ):\n+        if (\n+            data_format == \"channels_first\"\n+            and backend.backend() == \"tensorflow\"\n+        ):\n+            pytest.skip(\"channels_first unsupported on CPU with TF\")\n+\n+        if (\n+            isinstance(strides, tuple)\n+            and backend.backend() == \"tensorflow\"\n+            and dilation_rate > 1\n+        ):\n+            pytest.skip(\n+                \"dilation_rate>1 with strides>1 than not supported with TF\"\n+            )\n+        if data_format == \"channels_first\":\n+            image = np.random.uniform(size=(1, 3, 20, 20))\n+        else:\n+            image = np.random.uniform(size=(1, 20, 20, 3))\n+        patch_h, patch_w = size[0], size[1]\n+        if strides is None:\n+            strides_h, strides_w = patch_h, patch_w\n+        else:\n+            strides_h, strides_w = strides[0], strides[1]\n+\n+        patches_out = kimage.extract_patches(\n+            backend.convert_to_tensor(image, dtype=\"float32\"),\n+            size=size,\n+            strides=strides,\n+            dilation_rate=dilation_rate,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        if data_format == \"channels_first\":\n+            patches_out = backend.numpy.transpose(\n+                patches_out, axes=[0, 2, 3, 1]\n+            )\n+        if data_format == \"channels_first\":\n+            image = np.transpose(image, [0, 2, 3, 1])\n+        patches_ref = tf.image.extract_patches(\n+            image,\n+            sizes=(1, patch_h, patch_w, 1),\n+            strides=(1, strides_h, strides_w, 1),\n+            rates=(1, dilation_rate, dilation_rate, 1),\n+            padding=padding.upper(),\n+        )\n+        self.assertEqual(tuple(patches_out.shape), tuple(patches_ref.shape))\n+        self.assertAllClose(\n+            patches_ref.numpy(), backend.convert_to_numpy(patches_out), atol=0.3\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f34d716c4ca5d680c5ddf5ab9cf868fa07e34d5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 10 | Files Changed: 5 | Hunks: 12 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 1854 | Contributors (this commit): 12 | Commits (past 90d): 34 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -167,7 +167,12 @@ def set_dtype_policy(policy):\n     global_state.set_global_setting(\"dtype_policy\", policy)\n \n \n-@keras_core_export(\"keras_core.mixed_precision.dtype_policy\")\n+@keras_core_export(\n+    [\n+        \"keras_core.mixed_precision.dtype_policy\",\n+        \"keras_core.mixed_precision.global_policy\",\n+    ]\n+)\n def dtype_policy():\n     \"\"\"Returns the current default dtype policy object.\"\"\"\n     policy = global_state.get_global_setting(\"dtype_policy\", None)\n\n@@ -313,14 +313,13 @@ def extract_patches(\n     Args:\n         image: Input image or batch of images. Must be 3D or 4D.\n         size: Patch size int or tuple (patch_height, patch_widht)\n-        strides: strides along height and width. if not specified or\n-                None it is same patch_size\n+        strides: strides along height and width. If not specified, or\n+            if `None`, it defaults to the same value as `size`.\n         dilation_rate: This is the input stride, specifying how far two\n             consecutive patch samples are in the input. For value other than 1,\n-            strides must be 1. NOTE: `strides > 1` not supported in\n+            strides must be 1. NOTE: `strides > 1` is not supported in\n             conjunction with `dilation_rate > 1`\n-        padding: The type of padding algorithm to use.\n-            'same' or 'valid'.\n+        padding: The type of padding algorithm to use: `\"same\"` or `\"valid\"`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, height, width, channels)`\n@@ -372,8 +371,8 @@ def _extract_patches(\n         patch_h, patch_w = size[0], size[1]\n     else:\n         raise TypeError(\n-            \"invalid size argument \"\n-            f\"int or tuple of length 2. Received {size}\"\n+            \"Invalid `size` argument. Expected an \"\n+            f\"int or a tuple of length 2. Received: size={size}\"\n         )\n     if data_format == \"channels_last\":\n         channels_in = image.shape[-1]\n\n@@ -1,7 +1,7 @@\n import numpy as np\n+import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n-import pytest\n \n from keras_core import backend\n from keras_core import testing\n\n@@ -3,6 +3,8 @@ import math\n import numpy as np\n import tree\n \n+from keras_core.api_export import keras_core_export\n+\n \n def compute_pooling_output_shape(\n     input_shape,\n@@ -236,6 +238,7 @@ def reduce_shape(shape, axis=None, keepdims=False):\n         return output_shape\n \n \n+@keras_core_export(\"keras_core.utils.get_source_inputs\")\n def get_source_inputs(tensor):\n     \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n \n\n@@ -112,7 +112,12 @@ def record_object_after_deserialization(obj, obj_id):\n     id_to_obj_map[obj_id] = obj\n \n \n-@keras_core_export(\"keras_core.saving.serialize_keras_object\")\n+@keras_core_export(\n+    [\n+        \"keras_core.saving.serialize_keras_object\",\n+        \"keras_core.utils.serialize_keras_object\",\n+    ]\n+)\n def serialize_keras_object(obj):\n     \"\"\"Retrieve the config dict by serializing the Keras object.\n \n@@ -368,7 +373,10 @@ def serialize_dict(obj):\n \n \n @keras_core_export(\n+    [\n         \"keras_core.saving.deserialize_keras_object\",\n+        \"keras_core.utils.deserialize_keras_object\",\n+    ]\n )\n def deserialize_keras_object(\n     config, custom_objects=None, safe_mode=True, **kwargs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74a2358697067ae18bcf1621481474b5920f1214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 140 | Churn Cumulative: 2770 | Contributors (this commit): 13 | Commits (past 90d): 53 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,14 @@\n-import numpy as np\n-\n from keras_core import backend\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras_core.utils import image_utils\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,41 +52,121 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer RandomCrop requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n-            )\n-\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.seed_generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.numpy.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            input_height, input_width = (\n+                input_shape[self.height_axis],\n+                input_shape[self.width_axis],\n+            )\n+\n+            h_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_height - self.height + 1),\n+                    dtype=inputs.dtype,\n+                    seed=self.seed_generator,\n+                ),\n+                \"int32\",\n+            )\n+            w_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_width - self.width + 1),\n+                    dtype=inputs.dtype,\n+                    seed=self.seed_generator,\n+                ),\n+                \"int32\",\n+            )\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n+        outputs = (\n+            self.backend.numpy.squeeze(outputs, axis=0)\n+            if not is_batched\n+            else outputs\n+        )\n+\n+        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3419fc3ac7e60ff545c4c40c6d3c3cf1a638a8b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 356 | Lines Deleted: 110 | Files Changed: 2 | Hunks: 12 | Methods Changed: 16 | Complexity Δ (Sum/Max): 6/11 | Churn Δ: 466 | Churn Cumulative: 3236 | Contributors (this commit): 13 | Commits (past 90d): 55 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,326 @@\n+\"\"\"\n+Title: Image classification with EANet (External Attention Transformer)\n+Author: [ZhiYong Chang](https://github.com/czy00000)\n+Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/10/19\n+Last modified: 2023/07/18\n+Description: Image classification with a Transformer that leverages external attention.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example implements the [EANet](https://arxiv.org/abs/2105.02358)\n+model for image classification, and demonstrates it on the CIFAR-100 dataset.\n+EANet introduces a novel attention mechanism\n+named ***external attention***, based on two external, small, learnable, and\n+shared memories, which can be implemented easily by simply using two cascaded\n+linear layers and two normalization layers. It conveniently replaces self-attention\n+as used in existing architectures. External attention has linear complexity, as it only\n+implicitly considers the correlations between all samples.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+\n+import matplotlib.pyplot as plt\n+\n+\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+num_classes = 100\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\"\"\"\n+## Configure the hyperparameters\n+\"\"\"\n+\n+weight_decay = 0.0001\n+learning_rate = 0.001\n+label_smoothing = 0.1\n+validation_split = 0.2\n+batch_size = 128\n+num_epochs = 50\n+patch_size = 2  # Size of the patches to be extracted from the input images.\n+num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n+embedding_dim = 64  # Number of hidden units.\n+mlp_dim = 64\n+dim_coefficient = 4\n+num_heads = 4\n+attention_dropout = 0.2\n+projection_dropout = 0.2\n+num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n+\n+print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n+print(f\"Patches per image: {num_patches}\")\n+\n+\n+\"\"\"\n+## Use data augmentation\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Normalization(),\n+        layers.RandomFlip(\"horizontal\"),\n+        layers.RandomRotation(factor=0.1),\n+        layers.RandomContrast(factor=0.1),\n+        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+# Compute the mean and the variance of the training data for normalization.\n+data_augmentation.layers[0].adapt(x_train)\n+\n+\"\"\"\n+## Implement the patch extraction and encoding layer\n+\"\"\"\n+\n+\n+class PatchExtract(layers.Layer):\n+    def __init__(self, patch_size, **kwargs):\n+        super().__init__(**kwargs)\n+        self.patch_size = patch_size\n+\n+    def call(self, x):\n+        B, C = ops.shape(x)[0], ops.shape(x)[-1]\n+        x = ops.image.extract_patches(x, self.patch_size)\n+        x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n+        return x\n+\n+\n+class PatchEmbedding(layers.Layer):\n+    def __init__(self, num_patch, embed_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.num_patch = num_patch\n+        self.proj = layers.Dense(embed_dim)\n+        self.pos_embed = layers.Embedding(\n+            input_dim=num_patch, output_dim=embed_dim\n+        )\n+\n+    def call(self, patch):\n+        pos = ops.arange(start=0, stop=self.num_patch, step=1)\n+        return self.proj(patch) + self.pos_embed(pos)\n+\n+\n+\"\"\"\n+## Implement the external attention block\n+\"\"\"\n+\n+\n+def external_attention(\n+    x,\n+    dim,\n+    num_heads,\n+    dim_coefficient=4,\n+    attention_dropout=0,\n+    projection_dropout=0,\n+):\n+    _, num_patch, channel = x.shape\n+    assert dim % num_heads == 0\n+    num_heads = num_heads * dim_coefficient\n+\n+    x = layers.Dense(dim * dim_coefficient)(x)\n+    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n+    x = ops.reshape(\n+        x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads)\n+    )\n+    x = ops.transpose(x, axes=[0, 2, 1, 3])\n+    # a linear layer M_k\n+    attn = layers.Dense(dim // dim_coefficient)(x)\n+    # normalize attention map\n+    attn = layers.Softmax(axis=2)(attn)\n+    # dobule-normalization\n+    attn = layers.Lambda(\n+        lambda attn: ops.divide(\n+            attn,\n+            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n+        )\n+    )(attn)\n+    attn = layers.Dropout(attention_dropout)(attn)\n+    # a linear layer M_v\n+    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n+    x = ops.transpose(x, axes=[0, 2, 1, 3])\n+    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n+    # a linear layer to project original dim\n+    x = layers.Dense(dim)(x)\n+    x = layers.Dropout(projection_dropout)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Implement the MLP block\n+\"\"\"\n+\n+\n+def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n+    x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n+    x = layers.Dropout(drop_rate)(x)\n+    x = layers.Dense(embedding_dim)(x)\n+    x = layers.Dropout(drop_rate)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Implement the Transformer block\n+\"\"\"\n+\n+\n+def transformer_encoder(\n+    x,\n+    embedding_dim,\n+    mlp_dim,\n+    num_heads,\n+    dim_coefficient,\n+    attention_dropout,\n+    projection_dropout,\n+    attention_type=\"external_attention\",\n+):\n+    residual_1 = x\n+    x = layers.LayerNormalization(epsilon=1e-5)(x)\n+    if attention_type == \"external_attention\":\n+        x = external_attention(\n+            x,\n+            embedding_dim,\n+            num_heads,\n+            dim_coefficient,\n+            attention_dropout,\n+            projection_dropout,\n+        )\n+    elif attention_type == \"self_attention\":\n+        x = layers.MultiHeadAttention(\n+            num_heads=num_heads,\n+            key_dim=embedding_dim,\n+            dropout=attention_dropout,\n+        )(x, x)\n+    x = layers.add([x, residual_1])\n+    residual_2 = x\n+    x = layers.LayerNormalization(epsilon=1e-5)(x)\n+    x = mlp(x, embedding_dim, mlp_dim)\n+    x = layers.add([x, residual_2])\n+    return x\n+\n+\n+\"\"\"\n+## Implement the EANet model\n+\"\"\"\n+\n+\"\"\"\n+The EANet model leverages external attention.\n+The computational complexity of traditional self attention is `O(d * N ** 2)`,\n+where `d` is the embedding size, and `N` is the number of patch.\n+the authors find that most pixels are closely related to just a few other\n+pixels, and an `N`-to-`N` attention matrix may be redundant.\n+So, they propose as an alternative an external\n+attention module where the computational complexity of external attention is `O(d * S * N)`.\n+As `d` and `S` are hyper-parameters,\n+the proposed algorithm is linear in the number of pixels. In fact, this is equivalent\n+to a drop patch operation, because a lot of information contained in a patch\n+in an image is redundant and unimportant.\n+\"\"\"\n+\n+\n+def get_model(attention_type=\"external_attention\"):\n+    inputs = layers.Input(shape=input_shape)\n+    # Image augment\n+    x = data_augmentation(inputs)\n+    # Extract patches.\n+    x = PatchExtract(patch_size)(x)\n+    # Create patch embedding.\n+    x = PatchEmbedding(num_patches, embedding_dim)(x)\n+    # Create Transformer block.\n+    for _ in range(num_transformer_blocks):\n+        x = transformer_encoder(\n+            x,\n+            embedding_dim,\n+            mlp_dim,\n+            num_heads,\n+            dim_coefficient,\n+            attention_dropout,\n+            projection_dropout,\n+            attention_type,\n+        )\n+\n+    x = layers.GlobalAveragePooling1D()(x)\n+    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+\"\"\"\n+## Train on CIFAR-100\n+\n+\"\"\"\n+\n+\n+model = get_model(attention_type=\"external_attention\")\n+\n+model.compile(\n+    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n+    optimizer=keras.optimizers.AdamW(\n+        learning_rate=learning_rate, weight_decay=weight_decay\n+    ),\n+    metrics=[\n+        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n+        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+    ],\n+)\n+\n+history = model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=batch_size,\n+    epochs=num_epochs,\n+    validation_split=validation_split,\n+)\n+\n+\"\"\"\n+### Let's visualize the training progress of the model.\n+\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"], label=\"train_loss\")\n+plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n+plt.xlabel(\"Epochs\")\n+plt.ylabel(\"Loss\")\n+plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n+plt.legend()\n+plt.grid()\n+plt.show()\n+\n+\"\"\"\n+### Let's display the final results of the test on CIFAR-100.\n+\n+\"\"\"\n+\n+loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n+print(f\"Test loss: {round(loss, 2)}\")\n+print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+\"\"\"\n+EANet just replaces self attention in Vit with external attention.\n+The traditional Vit achieved a ~73% test top-5 accuracy and ~41 top-1 accuracy after\n+training 50 epochs, but with 0.6M parameters. Under the same experimental environment\n+and the same hyperparameters, The EANet model we just trained has just 0.3M parameters,\n+and it gets us to ~73% test top-5 accuracy and ~43% top-1 accuracy. This fully demonstrates the\n+effectiveness of external attention.\n+\n+We only show the training\n+process of EANet, you can train Vit under the same experimental conditions and observe\n+the test results.\n+\"\"\"\n\n@@ -1,14 +1,14 @@\n+import numpy as np\n+\n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n+from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,121 +52,41 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+        if not tf.available:\n+            raise ImportError(\n+                \"Layer RandomCrop requires TensorFlow. \"\n+                \"Install it via `pip install tensorflow`.\"\n+            )\n+\n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.numpy.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            input_height, input_width = (\n-                input_shape[self.height_axis],\n-                input_shape[self.width_axis],\n-            )\n-\n-            h_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_height - self.height + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                \"int32\",\n-            )\n-            w_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_width - self.width + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                \"int32\",\n-            )\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n-        outputs = (\n-            self.backend.numpy.squeeze(outputs, axis=0)\n-            if not is_batched\n-            else outputs\n-        )\n-\n-        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n-            outputs = self.backend.convert_to_tensor(outputs)\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n+        if (\n+            backend.backend() != \"tensorflow\"\n+            and not backend_utils.in_tf_graph()\n+        ):\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a04c7076dba9983044c9b14ea6a83e9d59b04ea3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 356 | Files Changed: 2 | Hunks: 12 | Methods Changed: 16 | Complexity Δ (Sum/Max): -6/5 | Churn Δ: 467 | Churn Cumulative: 3703 | Contributors (this commit): 13 | Commits (past 90d): 57 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,326 +0,0 @@\n-\"\"\"\n-Title: Image classification with EANet (External Attention Transformer)\n-Author: [ZhiYong Chang](https://github.com/czy00000)\n-Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n-Date created: 2021/10/19\n-Last modified: 2023/07/18\n-Description: Image classification with a Transformer that leverages external attention.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example implements the [EANet](https://arxiv.org/abs/2105.02358)\n-model for image classification, and demonstrates it on the CIFAR-100 dataset.\n-EANet introduces a novel attention mechanism\n-named ***external attention***, based on two external, small, learnable, and\n-shared memories, which can be implemented easily by simply using two cascaded\n-linear layers and two normalization layers. It conveniently replaces self-attention\n-as used in existing architectures. External attention has linear complexity, as it only\n-implicitly considers the correlations between all samples.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n-\n-import matplotlib.pyplot as plt\n-\n-\n-\"\"\"\n-## Prepare the data\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\"\"\"\n-\n-weight_decay = 0.0001\n-learning_rate = 0.001\n-label_smoothing = 0.1\n-validation_split = 0.2\n-batch_size = 128\n-num_epochs = 50\n-patch_size = 2  # Size of the patches to be extracted from the input images.\n-num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n-embedding_dim = 64  # Number of hidden units.\n-mlp_dim = 64\n-dim_coefficient = 4\n-num_heads = 4\n-attention_dropout = 0.2\n-projection_dropout = 0.2\n-num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n-\n-print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n-print(f\"Patches per image: {num_patches}\")\n-\n-\n-\"\"\"\n-## Use data augmentation\n-\"\"\"\n-\n-data_augmentation = keras.Sequential(\n-    [\n-        layers.Normalization(),\n-        layers.RandomFlip(\"horizontal\"),\n-        layers.RandomRotation(factor=0.1),\n-        layers.RandomContrast(factor=0.1),\n-        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-    ],\n-    name=\"data_augmentation\",\n-)\n-# Compute the mean and the variance of the training data for normalization.\n-data_augmentation.layers[0].adapt(x_train)\n-\n-\"\"\"\n-## Implement the patch extraction and encoding layer\n-\"\"\"\n-\n-\n-class PatchExtract(layers.Layer):\n-    def __init__(self, patch_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.patch_size = patch_size\n-\n-    def call(self, x):\n-        B, C = ops.shape(x)[0], ops.shape(x)[-1]\n-        x = ops.image.extract_patches(x, self.patch_size)\n-        x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n-        return x\n-\n-\n-class PatchEmbedding(layers.Layer):\n-    def __init__(self, num_patch, embed_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_patch = num_patch\n-        self.proj = layers.Dense(embed_dim)\n-        self.pos_embed = layers.Embedding(\n-            input_dim=num_patch, output_dim=embed_dim\n-        )\n-\n-    def call(self, patch):\n-        pos = ops.arange(start=0, stop=self.num_patch, step=1)\n-        return self.proj(patch) + self.pos_embed(pos)\n-\n-\n-\"\"\"\n-## Implement the external attention block\n-\"\"\"\n-\n-\n-def external_attention(\n-    x,\n-    dim,\n-    num_heads,\n-    dim_coefficient=4,\n-    attention_dropout=0,\n-    projection_dropout=0,\n-):\n-    _, num_patch, channel = x.shape\n-    assert dim % num_heads == 0\n-    num_heads = num_heads * dim_coefficient\n-\n-    x = layers.Dense(dim * dim_coefficient)(x)\n-    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n-    x = ops.reshape(\n-        x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads)\n-    )\n-    x = ops.transpose(x, axes=[0, 2, 1, 3])\n-    # a linear layer M_k\n-    attn = layers.Dense(dim // dim_coefficient)(x)\n-    # normalize attention map\n-    attn = layers.Softmax(axis=2)(attn)\n-    # dobule-normalization\n-    attn = layers.Lambda(\n-        lambda attn: ops.divide(\n-            attn,\n-            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n-        )\n-    )(attn)\n-    attn = layers.Dropout(attention_dropout)(attn)\n-    # a linear layer M_v\n-    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n-    x = ops.transpose(x, axes=[0, 2, 1, 3])\n-    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n-    # a linear layer to project original dim\n-    x = layers.Dense(dim)(x)\n-    x = layers.Dropout(projection_dropout)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement the MLP block\n-\"\"\"\n-\n-\n-def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n-    x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n-    x = layers.Dropout(drop_rate)(x)\n-    x = layers.Dense(embedding_dim)(x)\n-    x = layers.Dropout(drop_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement the Transformer block\n-\"\"\"\n-\n-\n-def transformer_encoder(\n-    x,\n-    embedding_dim,\n-    mlp_dim,\n-    num_heads,\n-    dim_coefficient,\n-    attention_dropout,\n-    projection_dropout,\n-    attention_type=\"external_attention\",\n-):\n-    residual_1 = x\n-    x = layers.LayerNormalization(epsilon=1e-5)(x)\n-    if attention_type == \"external_attention\":\n-        x = external_attention(\n-            x,\n-            embedding_dim,\n-            num_heads,\n-            dim_coefficient,\n-            attention_dropout,\n-            projection_dropout,\n-        )\n-    elif attention_type == \"self_attention\":\n-        x = layers.MultiHeadAttention(\n-            num_heads=num_heads,\n-            key_dim=embedding_dim,\n-            dropout=attention_dropout,\n-        )(x, x)\n-    x = layers.add([x, residual_1])\n-    residual_2 = x\n-    x = layers.LayerNormalization(epsilon=1e-5)(x)\n-    x = mlp(x, embedding_dim, mlp_dim)\n-    x = layers.add([x, residual_2])\n-    return x\n-\n-\n-\"\"\"\n-## Implement the EANet model\n-\"\"\"\n-\n-\"\"\"\n-The EANet model leverages external attention.\n-The computational complexity of traditional self attention is `O(d * N ** 2)`,\n-where `d` is the embedding size, and `N` is the number of patch.\n-the authors find that most pixels are closely related to just a few other\n-pixels, and an `N`-to-`N` attention matrix may be redundant.\n-So, they propose as an alternative an external\n-attention module where the computational complexity of external attention is `O(d * S * N)`.\n-As `d` and `S` are hyper-parameters,\n-the proposed algorithm is linear in the number of pixels. In fact, this is equivalent\n-to a drop patch operation, because a lot of information contained in a patch\n-in an image is redundant and unimportant.\n-\"\"\"\n-\n-\n-def get_model(attention_type=\"external_attention\"):\n-    inputs = layers.Input(shape=input_shape)\n-    # Image augment\n-    x = data_augmentation(inputs)\n-    # Extract patches.\n-    x = PatchExtract(patch_size)(x)\n-    # Create patch embedding.\n-    x = PatchEmbedding(num_patches, embedding_dim)(x)\n-    # Create Transformer block.\n-    for _ in range(num_transformer_blocks):\n-        x = transformer_encoder(\n-            x,\n-            embedding_dim,\n-            mlp_dim,\n-            num_heads,\n-            dim_coefficient,\n-            attention_dropout,\n-            projection_dropout,\n-            attention_type,\n-        )\n-\n-    x = layers.GlobalAveragePooling1D()(x)\n-    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-\"\"\"\n-## Train on CIFAR-100\n-\n-\"\"\"\n-\n-\n-model = get_model(attention_type=\"external_attention\")\n-\n-model.compile(\n-    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n-    optimizer=keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    ),\n-    metrics=[\n-        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n-        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n-    ],\n-)\n-\n-history = model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=num_epochs,\n-    validation_split=validation_split,\n-)\n-\n-\"\"\"\n-### Let's visualize the training progress of the model.\n-\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"], label=\"train_loss\")\n-plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n-plt.xlabel(\"Epochs\")\n-plt.ylabel(\"Loss\")\n-plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n-plt.legend()\n-plt.grid()\n-plt.show()\n-\n-\"\"\"\n-### Let's display the final results of the test on CIFAR-100.\n-\n-\"\"\"\n-\n-loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-print(f\"Test loss: {round(loss, 2)}\")\n-print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-\"\"\"\n-EANet just replaces self attention in Vit with external attention.\n-The traditional Vit achieved a ~73% test top-5 accuracy and ~41 top-1 accuracy after\n-training 50 epochs, but with 0.6M parameters. Under the same experimental environment\n-and the same hyperparameters, The EANet model we just trained has just 0.3M parameters,\n-and it gets us to ~73% test top-5 accuracy and ~43% top-1 accuracy. This fully demonstrates the\n-effectiveness of external attention.\n-\n-We only show the training\n-process of EANet, you can train Vit under the same experimental conditions and observe\n-the test results.\n-\"\"\"\n\n@@ -1,14 +1,14 @@\n-import numpy as np\n-\n from keras_core import backend\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras_core.utils import image_utils\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,41 +52,122 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer RandomCrop requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n-            )\n-\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.numpy.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            input_height, input_width = (\n+                input_shape[self.height_axis],\n+                input_shape[self.width_axis],\n+            )\n+\n+            seed_generator = self._get_seed_generator(self.backend._backend)\n+            h_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_height - self.height + 1),\n+                    dtype=inputs.dtype,\n+                    seed=seed_generator,\n+                ),\n+                \"int32\",\n+            )\n+            w_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_width - self.width + 1),\n+                    dtype=inputs.dtype,\n+                    seed=seed_generator,\n+                ),\n+                \"int32\",\n+            )\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n+        outputs = (\n+            self.backend.numpy.squeeze(outputs, axis=0)\n+            if not is_batched\n+            else outputs\n+        )\n+\n+        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
