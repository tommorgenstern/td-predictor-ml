{"custom_id": "keras#99cc39cd12057ad6765fcd3db355777e5bf4213c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 20 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 65 | Churn Cumulative: 636 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ Author: [fchollet](https://twitter.com/fchollet)\n Date created: 2023/07/11\n Last modified: 2023/07/11\n Description: Guide to multi-GPU/TPU training for Keras models with JAX.\n-Accelerator: GPU or TPU\n+Accelerator: GPU\n \"\"\"\n \"\"\"\n ## Introduction\n@@ -58,9 +58,9 @@ def get_model():\n     # Make a simple convnet with batch normalization and dropout.\n     inputs = keras.Input(shape=(28, 28, 1))\n     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n-    x = keras.layers.Conv2D(\n-        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n-    )(x)\n+    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n+        x\n+    )\n     x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n     x = keras.layers.ReLU()(x)\n     x = keras.layers.Conv2D(\n@@ -174,7 +174,8 @@ optimizer.build(model.trainable_variables)\n # Keras provides a pure functional forward pass: model.stateless_call\n def compute_loss(trainable_variables, non_trainable_variables, x, y):\n     y_pred, updated_non_trainable_variables = model.stateless_call(\n-        trainable_variables, non_trainable_variables, x)\n+        trainable_variables, non_trainable_variables, x\n+    )\n     loss_value = loss(y, y_pred)\n     return loss_value, updated_non_trainable_variables\n \n@@ -195,19 +196,25 @@ def train_step(train_state, x, y):\n         optimizer_variables, grads, trainable_variables\n     )\n \n-    return loss_value, (trainable_variables, non_trainable_variables, optimizer_variables)\n+    return loss_value, (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+    )\n \n \n # Replicate the model and optimizer variable on all devices\n def get_replicated_train_state(devices):\n     # All variables will be replicated on all devices\n-    var_mesh = Mesh(devices, axis_names=('_'))\n+    var_mesh = Mesh(devices, axis_names=(\"_\"))\n     # In NamedSharding, axes not mentioned are replicated (all axes here)\n     var_replication = NamedSharding(var_mesh, P())\n \n     # Apply the distribution settings to the model variables\n     trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n-    non_trainable_variables = jax.device_put(model.non_trainable_variables, var_replication)\n+    non_trainable_variables = jax.device_put(\n+        model.non_trainable_variables, var_replication\n+    )\n     optimizer_variables = jax.device_put(optimizer.variables, var_replication)\n \n     # Combine all state in a tuple\n@@ -219,8 +226,13 @@ print(f\"Running on {num_devices} devices: {jax.local_devices()}\")\n devices = mesh_utils.create_device_mesh((num_devices,))\n \n # Data will be split along the batch axis\n-data_mesh = Mesh(devices, axis_names=('batch',)) # naming axes of the mesh\n-data_sharding = NamedSharding(data_mesh, P('batch',)) # naming axes of the sharded partition\n+data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\n+data_sharding = NamedSharding(\n+    data_mesh,\n+    P(\n+        \"batch\",\n+    ),\n+)  # naming axes of the sharded partition\n \n # Display data sharding\n x, y = next(iter(train_data))\n@@ -243,9 +255,7 @@ for epoch in range(num_epochs):\n trainable_variables, non_trainable_variables, optimizer_variables = train_state\n for variable, value in zip(model.trainable_variables, trainable_variables):\n     variable.assign(value)\n-for variable, value in zip(\n-    model.non_trainable_variables, non_trainable_variables\n-):\n+for variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n     variable.assign(value)\n \n \"\"\"\n\n@@ -53,9 +53,9 @@ def get_model():\n     # Make a simple convnet with batch normalization and dropout.\n     inputs = keras.Input(shape=(28, 28, 1))\n     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n-    x = keras.layers.Conv2D(\n-        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n-    )(x)\n+    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n+        x\n+    )\n     x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n     x = keras.layers.ReLU()(x)\n     x = keras.layers.Conv2D(\n@@ -94,8 +94,6 @@ def get_dataset():\n     x_train = np.expand_dims(x_train, -1)\n     x_test = np.expand_dims(x_test, -1)\n     print(\"x_train shape:\", x_train.shape)\n-    print(x_train.shape[0], \"train samples\")\n-    print(x_test.shape[0], \"test samples\")\n \n     # Create a TensorDataset\n     dataset = torch.utils.data.TensorDataset(\n@@ -113,6 +111,7 @@ a GPU (note the calls to `.cuda()`).\n def train_model(model, dataloader, num_epochs, optimizer, loss_fn):\n     for epoch in range(num_epochs):\n         running_loss = 0.0\n+        running_loss_count = 0\n         for batch_idx, (inputs, targets) in enumerate(dataloader):\n             inputs = inputs.cuda(non_blocking=True)\n             targets = targets.cuda(non_blocking=True)\n@@ -127,15 +126,13 @@ def train_model(model, dataloader, num_epochs, optimizer, loss_fn):\n             optimizer.step()\n \n             running_loss += loss.item()\n+            running_loss_count += 1\n \n         # Print loss statistics\n-            if (batch_idx + 1) % 10 == 0:\n         print(\n             f\"Epoch {epoch + 1}/{num_epochs}, \"\n-                    f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n-                    f\"Loss: {running_loss / 10}\"\n+            f\"Loss: {running_loss / running_loss_count}\"\n         )\n-                running_loss = 0.0\n \n \n \"\"\"\n@@ -169,7 +166,7 @@ To do single-host, multi-device synchronous training with a Keras model, you wou\n the `torch.nn.parallel.DistributedDataParallel` module wrapper.\n Here's how it works:\n \n-- We use `torch.multiprocessing.spawn` to spawn multiple Python processes, one\n+- We use `torch.multiprocessing.start_processes` to start multiple Python processes, one\n per device. Each process will run the `per_device_launch_fn` function.\n - The `per_device_launch_fn` function does the following:\n     - It uses `torch.distributed.init_process_group` and `torch.cuda.set_device`\n@@ -234,9 +231,7 @@ def per_device_launch_fn(current_gpu_index, num_gpu):\n     model = get_model()\n \n     # prepare the dataloader\n-    dataloader = prepare_dataloader(\n-        dataset, current_gpu_index, num_gpu, batch_size\n-    )\n+    dataloader = prepare_dataloader(dataset, current_gpu_index, num_gpu, batch_size)\n \n     # Instantiate the torch optimizer\n     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n@@ -256,15 +251,17 @@ def per_device_launch_fn(current_gpu_index, num_gpu):\n \n \n \"\"\"\n-Time to spawn:\n+Time to start multiple processes:\n \"\"\"\n \n-if __name__ == '__main__':\n-    torch.multiprocessing.spawn(\n+if __name__ == \"__main__\":\n+    # We use the \"fork\" method rather than \"spawn\" to support notebooks\n+    torch.multiprocessing.start_processes(\n         per_device_launch_fn,\n         args=(num_gpu,),\n         nprocs=num_gpu,\n         join=True,\n+        start_method=\"fork\",\n     )\n \n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4bcacdaa8f11abf6e95916d05c8be2d85cd0ab6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 48 | Churn Cumulative: 2080 | Contributors (this commit): 12 | Commits (past 90d): 38 | Contributors (cumulative): 12 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -4,6 +4,8 @@ import warnings\n import numpy as np\n import tensorflow as tf\n import tree\n+\n+from packaging.version import Version\n from tensorflow.python.eager import context as tf_context\n \n from keras_core import callbacks as callbacks_module\n@@ -22,6 +24,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n+        if Version(tf.__version__) >= Version('2.9.0'):\n+            self._support_reduce_retracing = True\n+        else:\n+            self._support_reduce_retracing = False\n+\n         # Model must be created under scope of DistStrat it will be trained\n         # with.\n         if tf.distribute.has_strategy():\n@@ -99,11 +106,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return self.train_step(data)\n \n         if not self.run_eagerly:\n-            one_step_on_data = tf.function(\n-                one_step_on_data,\n-                jit_compile=self.jit_compile,\n-                reduce_retracing=True,\n-            )\n+            kwargs = {'jit_compile': self.jit_compile}\n+            if self._support_reduce_retracing:\n+                kwargs.update({'reduce_retracing': True})\n+            one_step_on_data = tf.function(one_step_on_data, **kwargs)\n \n         @tf.autograph.experimental.do_not_convert\n         def one_step_on_iterator(iterator):\n@@ -131,7 +137,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             train_function = one_step_on_iterator\n \n         if not self.run_eagerly:\n-            train_function = tf.function(train_function, reduce_retracing=True)\n+            kwargs = {}\n+            if self._support_reduce_retracing:\n+                kwargs.update({'reduce_retracing': True})\n+            train_function = tf.function(train_function, **kwargs)\n \n         self.train_function = train_function\n \n@@ -145,9 +154,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return self.test_step(data)\n \n         if not self.run_eagerly and self.jit_compile:\n-            one_step_on_data = tf.function(\n-                one_step_on_data, jit_compile=True, reduce_retracing=True\n-            )\n+            kwargs = {'jit_compile': True}\n+            if self._support_reduce_retracing:\n+                kwargs.update({'reduce_retracing': True})\n+            one_step_on_data = tf.function(one_step_on_data, **kwargs)\n \n         @tf.autograph.experimental.do_not_convert\n         def one_step_on_iterator(iterator):\n@@ -175,7 +185,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             test_function = one_step_on_iterator\n \n         if not self.run_eagerly:\n-            test_function = tf.function(test_function, reduce_retracing=True)\n+            kwargs = {}\n+            if self._support_reduce_retracing:\n+                kwargs.update({'reduce_retracing': True})\n+            test_function = tf.function(test_function, **kwargs)\n \n         self.test_function = test_function\n \n@@ -189,9 +202,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             return self.predict_step(data)\n \n         if not self.run_eagerly and self.jit_compile:\n-            one_step_on_data = tf.function(\n-                one_step_on_data, jit_compile=True, reduce_retracing=True\n-            )\n+            kwargs = {'jit_compile': True}\n+            if self._support_reduce_retracing:\n+                kwargs.update({'reduce_retracing': True})\n+            one_step_on_data = tf.function(one_step_on_data, **kwargs)\n \n         @tf.autograph.experimental.do_not_convert\n         def one_step_on_data_distributed(data):\n@@ -222,9 +236,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             predict_function = one_step_on_data_distributed\n \n         if not self.run_eagerly:\n-            predict_function = tf.function(\n-                predict_function, reduce_retracing=True\n-            )\n+            kwargs = {}\n+            if self._support_reduce_retracing:\n+                kwargs.update({'reduce_retracing': True})\n+\n+            predict_function = tf.function(predict_function, **kwargs)\n \n         self.predict_function = predict_function\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32758f4511bc3840ee17d87673221863b472515c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 443 | Lines Deleted: 158 | Files Changed: 7 | Hunks: 48 | Methods Changed: 20 | Complexity Δ (Sum/Max): 28/12 | Churn Δ: 601 | Churn Cumulative: 2451 | Contributors (this commit): 7 | Commits (past 90d): 43 | Contributors (cumulative): 24 | DMM Complexity: 0.8413461538461539\n\nDIFF:\n@@ -86,8 +86,8 @@ AFFINE_TRANSFORM_FILL_MODES = {\n     \"constant\": \"zeros\",\n     \"nearest\": \"border\",\n     # \"wrap\",  not supported by torch\n-    # \"mirror\",  not supported by torch\n-    \"reflect\": \"reflection\",\n+    \"mirror\": \"reflection\",  # torch's reflection is mirror in other backends\n+    \"reflect\": \"reflection\",  # if fill_mode==reflect, redirect to mirror\n }\n \n \n@@ -122,7 +122,7 @@ def _apply_grid_transform(\n         grid,\n         mode=interpolation,\n         padding_mode=fill_mode,\n-        align_corners=False,\n+        align_corners=True,\n     )\n     # Fill with required color\n     if fill_value is not None:\n@@ -187,9 +187,9 @@ def affine_transform(\n             f\"transform.shape={transform.shape}\"\n         )\n \n-    if fill_mode != \"constant\":\n+    # the default fill_value of tnn.grid_sample is \"zeros\"\n+    if fill_mode != \"constant\" or (fill_mode == \"constant\" and fill_value == 0):\n         fill_value = None\n-    fill_mode = AFFINE_TRANSFORM_FILL_MODES[fill_mode]\n \n     # unbatched case\n     need_squeeze = False\n@@ -202,23 +202,60 @@ def affine_transform(\n     if data_format == \"channels_last\":\n         image = image.permute((0, 3, 1, 2))\n \n-    # deal with transform\n-    h, w = image.shape[2], image.shape[3]\n-    theta = torch.zeros((image.shape[0], 2, 3)).to(transform)\n-    theta[:, 0, 0] = transform[:, 0]\n-    theta[:, 0, 1] = transform[:, 1] * h / w\n-    theta[:, 0, 2] = (\n-        transform[:, 2] * 2 / w + theta[:, 0, 0] + theta[:, 0, 1] - 1\n+    batch_size = image.shape[0]\n+    h, w, c = image.shape[-2], image.shape[-1], image.shape[-3]\n+\n+    # get indices\n+    shape = [h, w, c]  # (H, W, C)\n+    meshgrid = torch.meshgrid(\n+        *[torch.arange(size) for size in shape], indexing=\"ij\"\n     )\n-    theta[:, 1, 0] = transform[:, 3] * w / h\n-    theta[:, 1, 1] = transform[:, 4]\n-    theta[:, 1, 2] = (\n-        transform[:, 5] * 2 / h + theta[:, 1, 0] + theta[:, 1, 1] - 1\n+    indices = torch.concatenate(\n+        [torch.unsqueeze(x, dim=-1) for x in meshgrid], dim=-1\n+    )\n+    indices = torch.tile(indices, (batch_size, 1, 1, 1, 1))\n+    indices = indices.to(transform)\n+\n+    # swap the values\n+    a0 = transform[:, 0].clone()\n+    a2 = transform[:, 2].clone()\n+    b1 = transform[:, 4].clone()\n+    b2 = transform[:, 5].clone()\n+    transform[:, 0] = b1\n+    transform[:, 2] = b2\n+    transform[:, 4] = a0\n+    transform[:, 5] = a2\n+\n+    # deal with transform\n+    transform = torch.nn.functional.pad(\n+        transform, pad=[0, 1, 0, 0], mode=\"constant\", value=1\n+    )\n+    transform = torch.reshape(transform, (batch_size, 3, 3))\n+    offset = transform[:, 0:2, 2].clone()\n+    offset = torch.nn.functional.pad(offset, pad=[0, 1, 0, 0])\n+    transform[:, 0:2, 2] = 0\n+\n+    # transform the indices\n+    coordinates = torch.einsum(\"Bhwij, Bjk -> Bhwik\", indices, transform)\n+    coordinates = torch.moveaxis(coordinates, source=-1, destination=1)\n+    coordinates += torch.reshape(a=offset, shape=(*offset.shape, 1, 1, 1))\n+    coordinates = coordinates[:, 0:2, ..., 0]\n+    coordinates = coordinates.permute((0, 2, 3, 1))\n+\n+    # normalize coordinates\n+    coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / (w - 1) * 2.0 - 1.0\n+    coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / (h - 1) * 2.0 - 1.0\n+    grid = torch.stack(\n+        [coordinates[:, :, :, 1], coordinates[:, :, :, 0]], dim=-1\n     )\n \n-    grid = tnn.affine_grid(theta, image.shape)\n     affined = _apply_grid_transform(\n-        image, grid, interpolation, fill_mode, fill_value\n+        image,\n+        grid,\n+        interpolation=interpolation,\n+        # if fill_mode==reflect, redirect to mirror\n+        fill_mode=AFFINE_TRANSFORM_FILL_MODES[fill_mode],\n+        fill_value=fill_value,\n     )\n \n     if data_format == \"channels_last\":\n\n@@ -48,17 +48,23 @@ class RandomTranslation(TFDataLayer):\n             left by 20%, and shifted right by 30%. `width_factor=0.2` results\n             in an output height shifted left or right by 20%.\n         fill_mode: Points outside the boundaries of the input are filled\n-            according to the given mode\n-            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n-            - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended\n-                by reflecting about the edge of the last pixel.\n-            - *constant*: `(k k k k | a b c d | k k k k)` The input is extended\n-                by filling all values beyond the edge with the same constant\n-                value k = 0.\n-            - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n-                wrapping around to the opposite edge.\n-            - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended\n-                by the nearest pixel.\n+            according to the given mode. Available methods are `\"constant\"`,\n+            `\"nearest\"`, `\"wrap\"` and `\"reflect\"`. Defaults to `\"constant\"`.\n+            - `\"reflect\"`: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about the edge of the last\n+                pixel.\n+            - `\"constant\"`: `(k k k k | a b c d | k k k k)`\n+                The input is extended by filling all values beyond\n+                the edge with the same constant value k specified by\n+                `fill_value`.\n+            - `\"wrap\"`: `(a b c d | a b c d | a b c d)`\n+                The input is extended by wrapping around to the opposite edge.\n+            - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n+            Note that when using torch backend, `\"reflect\"` is redirected to\n+            `\"mirror\"` `(c d c b | a b c d | c b a b)` because torch does not\n+            support `\"reflect\"`.\n+            Note that torch backend does not support `\"wrap\"`.\n         interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n             `\"bilinear\"`.\n         seed: Integer. Used to create a random seed.\n\n@@ -58,6 +58,18 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters([\"channels_first\", \"channels_last\"])\n     def test_random_translation_up_numeric_reflect(self, data_format):\n         input_image = np.arange(0, 25)\n+        if backend.backend() == \"torch\":\n+            # redirect fill_mode=reflect to fill_mode=mirror\n+            expected_output = np.asarray(\n+                [\n+                    [5, 6, 7, 8, 9],\n+                    [10, 11, 12, 13, 14],\n+                    [15, 16, 17, 18, 19],\n+                    [20, 21, 22, 23, 24],\n+                    [15, 16, 17, 18, 19],\n+                ]\n+            )\n+        else:\n             expected_output = np.asarray(\n                 [\n                     [5, 6, 7, 8, 9],\n@@ -133,6 +145,18 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     def test_random_translation_down_numeric_reflect(self, data_format):\n         input_image = np.arange(0, 25)\n         # Shifting by .2 * 5 = 1 pixel.\n+        if backend.backend() == \"torch\":\n+            # redirect fill_mode=reflect to fill_mode=mirror\n+            expected_output = np.asarray(\n+                [\n+                    [5, 6, 7, 8, 9],\n+                    [0, 1, 2, 3, 4],\n+                    [5, 6, 7, 8, 9],\n+                    [10, 11, 12, 13, 14],\n+                    [15, 16, 17, 18, 19],\n+                ]\n+            )\n+        else:\n             expected_output = np.asarray(\n                 [\n                     [0, 1, 2, 3, 4],\n@@ -172,6 +196,21 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     ):\n         input_image = np.arange(0, 16)\n         # Shifting by .2 * 5 = 1 pixel.\n+        if backend.backend() == \"torch\":\n+            # redirect fill_mode=reflect to fill_mode=mirror\n+            expected_output = np.asarray(\n+                [\n+                    [8, 9],\n+                    [6, 7],\n+                    [4, 5],\n+                    [2, 3],\n+                    [0, 1],\n+                    [2, 3],\n+                    [4, 5],\n+                    [6, 7],\n+                ]\n+            )\n+        else:\n             expected_output = np.asarray(\n                 [\n                     [6, 7],\n@@ -251,6 +290,18 @@ class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n     def test_random_translation_left_numeric_reflect(self, data_format):\n         input_image = np.arange(0, 25)\n         # Shifting by .2 * 5 = 1 pixel.\n+        if backend.backend() == \"torch\":\n+            # redirect fill_mode=reflect to fill_mode=mirror\n+            expected_output = np.asarray(\n+                [\n+                    [1, 2, 3, 4, 3],\n+                    [6, 7, 8, 9, 8],\n+                    [11, 12, 13, 14, 13],\n+                    [16, 17, 18, 19, 18],\n+                    [21, 22, 23, 24, 23],\n+                ]\n+            )\n+        else:\n             expected_output = np.asarray(\n                 [\n                     [1, 2, 3, 4, 4],\n\n@@ -1,14 +1,11 @@\n-import numpy as np\n-\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n \n \n @keras_core_export(\"keras_core.layers.RandomZoom\")\n-class RandomZoom(Layer):\n+class RandomZoom(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly zooms images during training.\n \n     This layer will randomly zoom in or out on each axis of an image\n@@ -18,75 +15,85 @@ class RandomZoom(Layer):\n     of integer or floating point dtype.\n     By default, the layer will output floats.\n \n-    **Note:** This layer wraps `tf.keras.layers.RandomZoom`. It cannot\n-    be used as part of the compiled computation graph of a model with\n-    any backend other than TensorFlow.\n-    It can however be used with any backend when running eagerly.\n-    It can also always be used as part of an input preprocessing pipeline\n-    with any backend (outside the model itself), which is how we recommend\n-    to use this layer.\n+    Input shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format,\n+        or `(..., channels, height, width)`, in `\"channels_first\"` format.\n+\n+    Output shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., target_height, target_width, channels)`,\n+        or `(..., channels, target_height, target_width)`,\n+        in `\"channels_first\"` format.\n \n     **Note:** This layer is safe to use inside a `tf.data` pipeline\n     (independently of which backend you're using).\n \n     Args:\n-        height_factor: a float represented as fraction of value,\n-            or a tuple of size 2 representing lower and upper bound\n-            for zooming vertically. When represented as a single float,\n-            this value is used for both the upper and\n-            lower bound. A positive value means zooming out,\n-            while a negative value\n-            means zooming in. For instance, `height_factor=(0.2, 0.3)`\n-            result in an output zoomed out by a random amount\n-            in the range `[+20%, +30%]`.\n-            `height_factor=(-0.3, -0.2)` result in an output zoomed\n-            in by a random amount in the range `[+20%, +30%]`.\n-        width_factor: a float represented as fraction of value,\n-            or a tuple of size 2 representing lower and upper bound\n-            for zooming horizontally. When\n-            represented as a single float, this value is used\n-            for both the upper and\n-            lower bound. For instance, `width_factor=(0.2, 0.3)`\n-            result in an output\n-            zooming out between 20% to 30%.\n-            `width_factor=(-0.3, -0.2)` result in an\n-            output zooming in between 20% to 30%. `None` means\n-            i.e., zooming vertical and horizontal directions\n-            by preserving the aspect ratio. Defaults to `None`.\n-        fill_mode: Points outside the boundaries of the input are\n-            filled according to the given mode\n-            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n-            - *reflect*: `(d c b a | a b c d | d c b a)`\n-                The input is extended by reflecting about\n-                the edge of the last pixel.\n-            - *constant*: `(k k k k | a b c d | k k k k)`\n+        height_factor: a float represented as fraction of value, or a tuple of\n+            size 2 representing lower and upper bound for zooming vertically.\n+            When represented as a single float, this value is used for both the\n+            upper and lower bound. A positive value means zooming out, while a\n+            negative value means zooming in. For instance,\n+            `height_factor=(0.2, 0.3)` result in an output zoomed out by a\n+            random amount in the range `[+20%, +30%]`.\n+            `height_factor=(-0.3, -0.2)` result in an output zoomed in by a\n+            random amount in the range `[+20%, +30%]`.\n+        width_factor: a float represented as fraction of value, or a tuple of\n+            size 2 representing lower and upper bound for zooming horizontally.\n+            When represented as a single float, this value is used for both the\n+            upper and lower bound. For instance, `width_factor=(0.2, 0.3)`\n+            result in an output zooming out between 20% to 30%.\n+            `width_factor=(-0.3, -0.2)` result in an output zooming in between\n+            20% to 30%. `None` means i.e., zooming vertical and horizontal\n+            directions by preserving the aspect ratio. Defaults to `None`.\n+        fill_mode: Points outside the boundaries of the input are filled\n+            according to the given mode. Available methods are `\"constant\"`,\n+            `\"nearest\"`, `\"wrap\"` and `\"reflect\"`. Defaults to `\"constant\"`.\n+            - `\"reflect\"`: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about the edge of the last\n+                pixel.\n+            - `\"constant\"`: `(k k k k | a b c d | k k k k)`\n                 The input is extended by filling all values beyond\n-                the edge with the same constant value k = 0.\n-            - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n-                wrapping around to the opposite edge.\n-            - *nearest*: `(a a a a | a b c d | d d d d)`\n+                the edge with the same constant value k specified by\n+                `fill_value`.\n+            - `\"wrap\"`: `(a b c d | a b c d | a b c d)`\n+                The input is extended by wrapping around to the opposite edge.\n+            - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n                 The input is extended by the nearest pixel.\n+            Note that when using torch backend, `\"reflect\"` is redirected to\n+            `\"mirror\"` `(c d c b | a b c d | c b a b)` because torch does not\n+            support `\"reflect\"`.\n+            Note that torch backend does not support `\"wrap\"`.\n         interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n             `\"bilinear\"`.\n         seed: Integer. Used to create a random seed.\n         fill_value: a float represents the value to be filled outside\n             the boundaries when `fill_mode=\"constant\"`.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, width)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Example:\n \n     >>> input_img = np.random.random((32, 224, 224, 3))\n     >>> layer = keras_core.layers.RandomZoom(.5, .2)\n     >>> out_img = layer(input_img)\n-\n-    Input shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., height, width, channels)`, in `\"channels_last\"` format.\n-\n-    Output shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., height, width, channels)`, in `\"channels_last\"` format.\n     \"\"\"\n \n+    _FACTOR_VALIDATION_ERROR = (\n+        \"The `factor` argument should be a number (or a list of two numbers) \"\n+        \"in the range [-1.0, 1.0]. \"\n+    )\n+    _SUPPORTED_FILL_MODE = (\"reflect\", \"wrap\", \"constant\", \"nearest\")\n+    _SUPPORTED_INTERPOLATION = (\"nearest\", \"bilinear\")\n+\n     def __init__(\n         self,\n         height_factor,\n@@ -95,46 +102,156 @@ class RandomZoom(Layer):\n         interpolation=\"bilinear\",\n         seed=None,\n         fill_value=0.0,\n-        name=None,\n+        data_format=None,\n         **kwargs,\n     ):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer RandomZoom requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n+        super().__init__(**kwargs)\n+        self.height_factor = height_factor\n+        self.height_lower, self.height_upper = self._set_factor(\n+            height_factor, \"height_factor\"\n+        )\n+        self.width_factor = width_factor\n+        if width_factor is not None:\n+            self.width_lower, self.width_upper = self._set_factor(\n+                width_factor, \"width_factor\"\n+            )\n+        if fill_mode not in self._SUPPORTED_FILL_MODE:\n+            raise NotImplementedError(\n+                f\"Unknown `fill_mode` {fill_mode}. Expected of one \"\n+                f\"{self._SUPPORTED_FILL_MODE}.\"\n+            )\n+        if interpolation not in self._SUPPORTED_INTERPOLATION:\n+            raise NotImplementedError(\n+                f\"Unknown `interpolation` {interpolation}. Expected of one \"\n+                f\"{self._SUPPORTED_INTERPOLATION}.\"\n             )\n \n-        super().__init__(name=name, **kwargs)\n-        self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomZoom(\n-            height_factor=height_factor,\n-            width_factor=width_factor,\n-            fill_mode=fill_mode,\n-            interpolation=interpolation,\n-            seed=self.seed,\n-            name=name,\n-            fill_value=fill_value,\n-            **kwargs,\n-        )\n-        self._allow_non_tensor_positional_args = True\n-        self._convert_input_args = False\n+        self.fill_mode = fill_mode\n+        self.fill_value = fill_value\n+        self.interpolation = interpolation\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n         self.supports_jit = False\n \n+    def _set_factor(self, factor, factor_name):\n+        if isinstance(factor, (tuple, list)):\n+            if len(factor) != 2:\n+                raise ValueError(\n+                    self._FACTOR_VALIDATION_ERROR\n+                    + f\"Received: {factor_name}={factor}\"\n+                )\n+            self._check_factor_range(factor[0])\n+            self._check_factor_range(factor[1])\n+            lower, upper = sorted(factor)\n+        elif isinstance(factor, (int, float)):\n+            self._check_factor_range(factor)\n+            factor = abs(factor)\n+            lower, upper = [-factor, factor]\n+        else:\n+            raise ValueError(\n+                self._FACTOR_VALIDATION_ERROR\n+                + f\"Received: {factor_name}={factor}\"\n+            )\n+        return lower, upper\n+\n+    def _check_factor_range(self, input_number):\n+        if input_number > 1.0 or input_number < -1.0:\n+            raise ValueError(\n+                self._FACTOR_VALIDATION_ERROR\n+                + f\"Received: input_number={input_number}\"\n+            )\n+\n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        if training:\n+            return self._randomly_zoom_inputs(inputs)\n+        else:\n+            return inputs\n+\n+    def _randomly_zoom_inputs(self, inputs):\n+        unbatched = len(inputs.shape) == 3\n+        if unbatched:\n+            inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+\n+        batch_size = self.backend.shape(inputs)[0]\n+        if self.data_format == \"channels_first\":\n+            height = inputs.shape[-2]\n+            width = inputs.shape[-1]\n+        else:\n+            height = inputs.shape[-3]\n+            width = inputs.shape[-2]\n+\n+        seed_generator = self._get_seed_generator(self.backend._backend)\n+        height_zoom = self.backend.random.uniform(\n+            minval=1.0 + self.height_lower,\n+            maxval=1.0 + self.height_upper,\n+            shape=[batch_size, 1],\n+            seed=seed_generator,\n+        )\n+        if self.width_factor is not None:\n+            width_zoom = self.backend.random.uniform(\n+                minval=1.0 + self.width_lower,\n+                maxval=1.0 + self.width_upper,\n+                shape=[batch_size, 1],\n+                seed=seed_generator,\n+            )\n+        else:\n+            width_zoom = height_zoom\n+        zooms = self.backend.cast(\n+            self.backend.numpy.concatenate([width_zoom, height_zoom], axis=1),\n+            dtype=\"float32\",\n+        )\n+\n+        outputs = self.backend.image.affine_transform(\n+            inputs,\n+            transform=self._get_zoom_matrix(zooms, height, width),\n+            interpolation=self.interpolation,\n+            fill_mode=self.fill_mode,\n+            fill_value=self.fill_value,\n+            data_format=self.data_format,\n+        )\n+\n+        if unbatched:\n+            outputs = self.backend.numpy.squeeze(outputs, axis=0)\n         return outputs\n \n+    def _get_zoom_matrix(self, zooms, image_height, image_width):\n+        num_zooms = self.backend.shape(zooms)[0]\n+        # The zoom matrix looks like:\n+        #     [[zx 0 0]\n+        #      [0 zy 0]\n+        #      [0 0 1]]\n+        # where the last entry is implicit.\n+        # zoom matrices are always float32.\n+        x_offset = ((image_width - 1.0) / 2.0) * (1.0 - zooms[:, 0:1])\n+        y_offset = ((image_height - 1.0) / 2.0) * (1.0 - zooms[:, 1:])\n+        return self.backend.numpy.concatenate(\n+            [\n+                zooms[:, 0:1],\n+                self.backend.numpy.zeros((num_zooms, 1)),\n+                x_offset,\n+                self.backend.numpy.zeros((num_zooms, 1)),\n+                zooms[:, 1:],\n+                y_offset,\n+                self.backend.numpy.zeros((num_zooms, 2)),\n+            ],\n+            axis=1,\n+        )\n+\n     def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+        return input_shape\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n-        return config\n+        base_config = super().get_config()\n+        config = {\n+            \"height_factor\": self.height_factor,\n+            \"width_factor\": self.width_factor,\n+            \"fill_mode\": self.fill_mode,\n+            \"interpolation\": self.interpolation,\n+            \"seed\": self.seed,\n+            \"fill_value\": self.fill_value,\n+            \"data_format\": self.data_format,\n+        }\n+        return {**base_config, **config}\n\n@@ -34,9 +34,9 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n         expected_output = np.asarray(\n             [\n                 [0, 0, 0, 0, 0],\n-                [0, 5, 7, 9, 0],\n-                [0, 10, 12, 14, 0],\n-                [0, 20, 22, 24, 0],\n+                [0, 2.7, 4.5, 6.3, 0],\n+                [0, 10.2, 12.0, 13.8, 0],\n+                [0, 17.7, 19.5, 21.3, 0],\n                 [0, 0, 0, 0, 0],\n             ]\n         )\n@@ -48,7 +48,7 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n             init_kwargs={\n                 \"height_factor\": (0.5, 0.5),\n                 \"width_factor\": (0.8, 0.8),\n-                \"interpolation\": \"nearest\",\n+                \"interpolation\": \"bilinear\",\n                 \"fill_mode\": \"constant\",\n             },\n             input_shape=None,\n@@ -62,11 +62,11 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n         expected_output = np.asarray(\n             [\n-                [6, 7, 7, 8, 8],\n-                [11, 12, 12, 13, 13],\n-                [11, 12, 12, 13, 13],\n-                [16, 17, 17, 18, 18],\n-                [16, 17, 17, 18, 18],\n+                [6.0, 6.5, 7.0, 7.5, 8.0],\n+                [8.5, 9.0, 9.5, 10.0, 10.5],\n+                [11.0, 11.5, 12.0, 12.5, 13.0],\n+                [13.5, 14.0, 14.5, 15.0, 15.5],\n+                [16.0, 16.5, 17.0, 17.5, 18.0],\n             ]\n         )\n         expected_output = backend.convert_to_tensor(\n@@ -77,7 +77,8 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n             init_kwargs={\n                 \"height_factor\": (-0.5, -0.5),\n                 \"width_factor\": (-0.5, -0.5),\n-                \"interpolation\": \"nearest\",\n+                \"interpolation\": \"bilinear\",\n+                \"fill_mode\": \"constant\",\n             },\n             input_shape=None,\n             input_data=input_image,\n\n@@ -184,7 +184,21 @@ def affine_transform(\n         fill_mode: Points outside the boundaries of the input are filled\n             according to the given mode. Available methods are `\"constant\"`,\n             `\"nearest\"`, `\"wrap\"` and `\"reflect\"`. Defaults to `\"constant\"`.\n-            Note that `\"wrap\"` is not supported by Torch backend.\n+            - `\"reflect\"`: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about the edge of the last\n+                pixel.\n+            - `\"constant\"`: `(k k k k | a b c d | k k k k)`\n+                The input is extended by filling all values beyond\n+                the edge with the same constant value k specified by\n+                `fill_value`.\n+            - `\"wrap\"`: `(a b c d | a b c d | a b c d)`\n+                The input is extended by wrapping around to the opposite edge.\n+            - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n+            Note that when using torch backend, `\"reflect\"` is redirected to\n+            `\"mirror\"` `(c d c b | a b c d | c b a b)` because torch does not\n+            support `\"reflect\"`.\n+            Note that torch backend does not support `\"wrap\"`.\n         fill_value: Value used for points outside the boundaries of the input if\n             `fill_mode=\"constant\"`. Defaults to `0`.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pytest\n+import scipy.ndimage\n import tensorflow as tf\n from absl.testing import parameterized\n \n@@ -51,6 +52,57 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(out.shape, (4, 4, 75))\n \n \n+AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n+    \"nearest\": 0,\n+    \"bilinear\": 1,\n+}\n+AFFINE_TRANSFORM_FILL_MODES = {\n+    \"constant\": \"grid-constant\",\n+    \"nearest\": \"nearest\",\n+    \"wrap\": \"grid-wrap\",\n+    \"mirror\": \"mirror\",\n+    \"reflect\": \"reflect\",\n+}\n+\n+\n+def _compute_affine_transform_coordinates(image, transform):\n+    need_squeeze = False\n+    if len(image.shape) == 3:  # unbatched\n+        need_squeeze = True\n+        image = np.expand_dims(image, axis=0)\n+        transform = np.expand_dims(transform, axis=0)\n+    batch_size = image.shape[0]\n+    # get indices\n+    meshgrid = np.meshgrid(\n+        *[np.arange(size) for size in image.shape[1:]], indexing=\"ij\"\n+    )\n+    indices = np.concatenate(\n+        [np.expand_dims(x, axis=-1) for x in meshgrid], axis=-1\n+    )\n+    indices = np.tile(indices, (batch_size, 1, 1, 1, 1))\n+    # swap the values\n+    transform[:, 4], transform[:, 0] = (\n+        transform[:, 0].copy(),\n+        transform[:, 4].copy(),\n+    )\n+    transform[:, 5], transform[:, 2] = (\n+        transform[:, 2].copy(),\n+        transform[:, 5].copy(),\n+    )\n+    # deal with transform\n+    transform = np.pad(transform, pad_width=[[0, 0], [0, 1]], constant_values=1)\n+    transform = np.reshape(transform, (batch_size, 3, 3))\n+    offset = np.pad(transform[:, 0:2, 2], pad_width=[[0, 0], [0, 1]])\n+    transform[:, 0:2, 2] = 0\n+    # transform the indices\n+    coordinates = np.einsum(\"Bhwij, Bjk -> Bhwik\", indices, transform)\n+    coordinates = np.moveaxis(coordinates, source=-1, destination=1)\n+    coordinates += np.reshape(a=offset, newshape=(*offset.shape, 1, 1, 1))\n+    if need_squeeze:\n+        coordinates = np.squeeze(coordinates, axis=0)\n+    return coordinates\n+\n+\n class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n         [\n@@ -135,22 +187,34 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             (\"nearest\", \"nearest\", \"channels_last\"),\n             (\"bilinear\", \"wrap\", \"channels_last\"),\n             (\"nearest\", \"wrap\", \"channels_last\"),\n+            (\"bilinear\", \"mirror\", \"channels_last\"),\n+            (\"nearest\", \"mirror\", \"channels_last\"),\n             (\"bilinear\", \"reflect\", \"channels_last\"),\n             (\"nearest\", \"reflect\", \"channels_last\"),\n             (\"bilinear\", \"constant\", \"channels_first\"),\n         ]\n     )\n     def test_affine_transform(self, interpolation, fill_mode, data_format):\n-        if fill_mode == \"wrap\" and backend.backend() == \"torch\":\n+        if backend.backend() == \"torch\" and fill_mode == \"wrap\":\n             self.skipTest(\n-                \"Applying affine transform with fill_mode=wrap is not support\"\n-                \" in torch backend\"\n+                \"In torch backend, applying affine_transform with \"\n+                \"fill_mode=wrap is not supported\"\n             )\n-        if fill_mode == \"wrap\" and backend.backend() in (\"jax\", \"numpy\"):\n+        if backend.backend() == \"torch\" and fill_mode == \"reflect\":\n             self.skipTest(\n-                \"The numerical results of applying affine transform with \"\n-                \"fill_mode=wrap in tensorflow is inconsistent with jax and \"\n-                \"numpy backends\"\n+                \"In torch backend, applying affine_transform with \"\n+                \"fill_mode=reflect is redirected to fill_mode=mirror\"\n+            )\n+        if backend.backend() == \"tensorflow\" and fill_mode == \"mirror\":\n+            self.skipTest(\n+                \"In tensorflow backend, applying affine_transform with \"\n+                \"fill_mode=mirror is not supported\"\n+            )\n+        if backend.backend() == \"tensorflow\" and fill_mode == \"wrap\":\n+            self.skipTest(\n+                \"In tensorflow backend, the numerical results of applying \"\n+                \"affine_transform with fill_mode=wrap is inconsistent with\"\n+                \"scipy\"\n             )\n \n         # Unbatched case\n@@ -169,23 +233,17 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n         if data_format == \"channels_first\":\n             x = np.transpose(x, (1, 2, 0))\n-        ref_out = tf.raw_ops.ImageProjectiveTransformV3(\n-            images=tf.expand_dims(x, axis=0),\n-            transforms=tf.cast(tf.expand_dims(transform, axis=0), tf.float32),\n-            output_shape=tf.shape(x)[:-1],\n-            fill_value=0,\n-            interpolation=interpolation.upper(),\n-            fill_mode=fill_mode.upper(),\n+        coordinates = _compute_affine_transform_coordinates(x, transform)\n+        ref_out = scipy.ndimage.map_coordinates(\n+            x,\n+            coordinates,\n+            order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+            mode=AFFINE_TRANSFORM_FILL_MODES[fill_mode],\n+            prefilter=False,\n         )\n-        ref_out = ref_out[0]\n         if data_format == \"channels_first\":\n             ref_out = np.transpose(ref_out, (2, 0, 1))\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n-        if backend.backend() == \"torch\":\n-            # TODO: cannot pass with torch backend\n-            with self.assertRaises(AssertionError):\n-                self.assertAllClose(ref_out, out, atol=0.3)\n-        else:\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n         # Batched case\n@@ -204,22 +262,23 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n         if data_format == \"channels_first\":\n             x = np.transpose(x, (0, 2, 3, 1))\n-        ref_out = tf.raw_ops.ImageProjectiveTransformV3(\n-            images=x,\n-            transforms=tf.cast(transform, tf.float32),\n-            output_shape=tf.shape(x)[1:-1],\n-            fill_value=0,\n-            interpolation=interpolation.upper(),\n-            fill_mode=fill_mode.upper(),\n+        coordinates = _compute_affine_transform_coordinates(x, transform)\n+        ref_out = np.stack(\n+            [\n+                scipy.ndimage.map_coordinates(\n+                    x[i],\n+                    coordinates[i],\n+                    order=AFFINE_TRANSFORM_INTERPOLATIONS[interpolation],\n+                    mode=AFFINE_TRANSFORM_FILL_MODES[fill_mode],\n+                    prefilter=False,\n+                )\n+                for i in range(x.shape[0])\n+            ],\n+            axis=0,\n         )\n         if data_format == \"channels_first\":\n             ref_out = np.transpose(ref_out, (0, 3, 1, 2))\n         self.assertEqual(tuple(out.shape), tuple(ref_out.shape))\n-        if backend.backend() == \"torch\":\n-            # TODO: cannot pass with torch backend\n-            with self.assertRaises(AssertionError):\n-                self.assertAllClose(ref_out, out, atol=0.3)\n-        else:\n         self.assertAllClose(ref_out, out, atol=0.3)\n \n     @parameterized.parameters(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e5ccb82c8ad6fe19e5780eb588af6d8cdb594f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 21/9 | Churn Δ: 106 | Churn Cumulative: 2632 | Contributors (this commit): 15 | Commits (past 90d): 124 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -287,3 +287,10 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(variable):\n     return jax.lax.stop_gradient(variable)\n+\n+\n+def unstack(x, num=None, axis=0):\n+    return [\n+        jax.lax.index_in_dim(x, i, axis, keepdims=False)\n+        for i in range(x.shape[axis])\n+    ]\n\n@@ -217,3 +217,8 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(x):\n     pass\n+\n+\n+def unstack(x, num=None, axis=0):\n+    x = np.moveaxis(x, axis, 0)\n+    return [x[i] for i in range(x.shape[0])]\n\n@@ -183,3 +183,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def stop_gradient(variable):\n     return tf.stop_gradient(variable)\n+\n+\n+def unstack(x, num=None, axis=0):\n+    return tf.unstack(x, num=num, axis=axis)\n\n@@ -353,3 +353,7 @@ def stop_gradient(variable):\n     # We can't use `.requires_grad_(False)` here since it only\n     # works when the tensor is a leaf node in the graph.\n     return variable.detach()\n+\n+\n+def unstack(x, num=None, axis=0):\n+    return x.unbind(axis)\n\n@@ -344,6 +344,60 @@ def fori_loop(lower, upper, body_fun, init_val):\n     return backend.core.fori_loop(lower, upper, body_fun, init_val)\n \n \n+class Unstack(Operation):\n+    def __init__(self, num=None, axis=0):\n+        super().__init__()\n+        self.num = num\n+        self.axis = axis\n+\n+    def call(self, x):\n+        return backend.core.unstack(x, self.num, self.axis)\n+\n+    def compute_output_spec(self, x):\n+        axis = self.axis\n+        if axis < 0:\n+            axis = len(x.shape) + axis\n+        output_shapes = x.shape[:axis] + x.shape[axis + 1 :]\n+        num = self.num\n+        if num is None:\n+            num = x.shape[axis]\n+        if num is None:\n+            raise ValueError(\n+                \"Cannot infer argument `num` from shape \"\n+                f\"{x.shape}. Either provide a tensor with a \"\n+                \"concrete shape in the `axis` dimension or \"\n+                \"explicitly pass the `num` argument.\"\n+            )\n+        output = [\n+            KerasTensor(shape=output_shapes, dtype=x.dtype) for _ in range(num)\n+        ]\n+        return output\n+\n+\n+@keras_core_export(\"keras_core.ops.unstack\")\n+def unstack(x, num=None, axis=0):\n+    \"\"\"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n+\n+    Args:\n+        x: The input tensor.\n+        num: The length of the dimension axis. Automatically inferred\n+            if `None`.\n+        axis: The axis along which to unpack.\n+\n+    Returns:\n+        A list of tensors unpacked along the given axis.\n+\n+    Example:\n+\n+    >>> x = keras_core.ops.array([[1, 2], [3, 4]])\n+    >>> keras_core.ops.unstack(x, axis=0)\n+    [array([1, 2]), array([3, 4])]\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Unstack(num, axis).symbolic_call(x)\n+    return backend.core.unstack(x, num=num, axis=axis)\n+\n+\n @keras_core_export(\"keras_core.ops.shape\")\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n\n@@ -56,6 +56,26 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n         result = core.fori_loop(0, 10, body_fun, initial_value)\n         self.assertEqual(result.shape, (3, 5, 7))\n \n+    def test_unstack(self):\n+        x = KerasTensor((2, 3, 4))\n+        axis = 1\n+        out = core.unstack(x, axis=axis)\n+        self.assertEqual(len(out), 3)\n+        for o in out:\n+            self.assertEqual(o.shape, (2, 4))\n+\n+        x = KerasTensor((2, None, None))\n+        axis, num = 1, 3\n+        out = core.unstack(x, num=num, axis=axis)\n+        self.assertEqual(len(out), 3)\n+        for o in out:\n+            self.assertEqual(o.shape, (2, None))\n+\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Cannot infer argument `num` from shape\"\n+        ):\n+            core.unstack(x, axis=axis)\n+\n \n class CoreOpsCorrectnessTest(testing.TestCase):\n     def test_scatter(self):\n@@ -298,3 +318,15 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n                 lambda: KerasTensor((3,)),\n                 lambda: KerasTensor((4,)),\n             )\n+\n+    def test_unstack(self):\n+        rng = np.random.default_rng(0)\n+        x = rng.uniform(size=(2, 3, 4))\n+        x_tensor = ops.convert_to_tensor(x)\n+        axis = 1\n+        out = ops.unstack(x_tensor, axis=axis)\n+        out_ex = [x[:, i, :] for i in range(x.shape[axis])]\n+        self.assertEqual(len(out), len(out_ex))\n+        for o, o_e in zip(out, out_ex):\n+            o = ops.convert_to_numpy(o)\n+            self.assertAllClose(o, o_e)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
