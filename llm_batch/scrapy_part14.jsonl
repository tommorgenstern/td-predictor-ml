{"custom_id": "scrapy#fbec2b3a439b76f87b08815d3662417adf6bb346", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 241 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -10,8 +10,8 @@ from pydispatch import dispatcher\n \n from scrapy.core import signals, log\n from scrapy.core.engine import scrapyengine\n-from scrapy.core.mail import MailSender\n from scrapy.core.exceptions import NotConfigured\n+from scrapy.mail import MailSender\n from scrapy.stats import stats\n from scrapy.conf import settings\n \n\n@@ -7,7 +7,7 @@ from pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.core.mail import MailSender\n+from scrapy.mail import MailSender\n from scrapy.stats import stats\n from scrapy.extension import extensions\n from scrapy.conf import settings\n\n@@ -9,7 +9,7 @@ from scrapy.core import log, signals\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.core.mail import MailSender\n+from scrapy.mail import MailSender\n from scrapy.stats import stats\n from scrapy.conf import settings\n \n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79be2ca97d6cfc563f66e03372f934481f0f9eda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 41 | Files Changed: 40 | Hunks: 42 | Methods Changed: 0 | Complexity Δ (Sum/Max): 21/12 | Churn Δ: 94 | Churn Cumulative: 5552 | Contributors (this commit): 4 | Commits (past 90d): 212 | Contributors (cumulative): 67 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ import os\n import optparse\n \n import scrapy\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.spider import spiders\n from scrapy.conf import settings\n \n\n@@ -8,7 +8,8 @@ import pprint\n \n from pydispatch import dispatcher\n \n-from scrapy.core import signals, log\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.mail import MailSender\n\n@@ -9,7 +9,8 @@ import cPickle as pickle\n from pydispatch import dispatcher\n from twisted.internet import defer\n \n-from scrapy.core import signals, log\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.http import Response, Headers\n from scrapy.http.headers import headers_dict_to_raw\n\n@@ -1,4 +1,4 @@\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.conf import settings\n \n class CrawlDebug(object):\n\n@@ -1,6 +1,6 @@\n import re\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import HttpException\n from scrapy.utils.url import urljoin_rfc as urljoin\n\n@@ -27,7 +27,7 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookup\n                                    ConnectionRefusedError, ConnectionDone, ConnectError\n from twisted.internet.defer import TimeoutError as UserTimeoutError \n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import HttpException\n from scrapy.conf import settings\n \n\n@@ -15,7 +15,8 @@ import robotparser\n \n from pydispatch import dispatcher\n \n-from scrapy.core import log, signals\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n\n@@ -4,7 +4,8 @@ from datetime import datetime\n from pydispatch import dispatcher\n \n from scrapy.utils.misc import load_class\n-from scrapy.core import log, signals\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.conf import settings\n \n\n@@ -2,7 +2,7 @@ from datetime import datetime\n \n from twisted.internet import defer\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.scheduler import Scheduler\n from scrapy.core.exceptions import IgnoreRequest\n \n\n@@ -4,7 +4,7 @@ SQLHistoryStore\n Persistent history storage using relational database\n \"\"\"\n from scrapy.contrib.history.history import URLHistory\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.conf import settings\n \n class SQLHistoryStore(object) :\n\n@@ -5,7 +5,8 @@ import socket\n \n from pydispatch import dispatcher\n \n-from scrapy.core import log, signals\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n\n@@ -4,7 +4,7 @@ from twisted.spread import pb\n from twisted.internet import reactor\n \n from scrapy.conf import settings\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured\n \n\n@@ -6,7 +6,8 @@ from pydispatch import dispatcher\n from twisted.spread import pb\n from twisted.internet import reactor\n \n-from scrapy.core import log, signals\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n\n@@ -3,7 +3,7 @@ import sys, os, time, datetime, pickle\n from twisted.internet import protocol, reactor\n from twisted.spread import pb\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n from scrapy.core.engine import scrapyengine\n\n@@ -10,7 +10,7 @@ from cStringIO import StringIO\n \n import Image\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.stats import stats\n from scrapy.http import Request\n from scrapy.core.exceptions import DropItem, NotConfigured\n\n@@ -1,7 +1,7 @@\n from twisted.internet import defer\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.http import Request\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Pipeline to print Items\n \"\"\"\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n\n@@ -3,7 +3,7 @@ Here put the rules that the rules engine / pipline will take to process it.\n \"\"\"\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n-from scrapy.core import log \n+from scrapy import log \n from scrapy.conf import settings\n \n class Rule(object):\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Represent the pipepline of execution to the rules engine.\n \"\"\"\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.contrib.rulengine.exceptions import RulesNotLoaded\n \n from scrapy.contrib.rulengine import rules\n\n@@ -5,7 +5,8 @@ This is to release any resources held on to by scraping spiders.\n \"\"\"\n import sys\n from pydispatch import dispatcher\n-from scrapy.core import log, signals\n+from scrapy.core import signals\n+from scrapy import log\n \n class SpiderReloader(object):\n \n\n@@ -4,7 +4,7 @@ Request inside the site being scraped. It can be used to limit the maximum\n depth to scrape or things like that\n \"\"\"\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.http import Request\n from scrapy.stats import stats\n from scrapy.conf import settings\n\n@@ -12,7 +12,7 @@ will be applied. If given a value of 0, no limit will be applied.\n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n from scrapy.http import Request\n-from scrapy.core import log\n+from scrapy import log\n \n class RequestLimitMiddleware(object):\n     #_last_queue_size = 0\n\n@@ -3,7 +3,7 @@ OffsiteMiddleware: Filters out Requests for URLs outside the domains covered by\n the spider.\n \"\"\"\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.url import url_is_from_spider\n \n\n@@ -3,7 +3,6 @@ RefererMiddleware: populates Request referer field, based on the Response which\n originated it.\n \"\"\"\n \n-from scrapy.core import log\n from scrapy.http import Request\n \n class RefererMiddleware(object):\n\n@@ -2,7 +2,7 @@\n UrlLengthMiddleware: Filters out requests with URLs longer than URLLENGTH_LIMIT\n \"\"\"\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.http import Request\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n\n@@ -10,7 +10,7 @@ from scrapy.core.exceptions import IgnoreRequest\n from scrapy.spider import spiders\n from scrapy.core.downloader.handlers import download_any\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.defer import chain_deferred, mustbe_deferred\n \n\n@@ -1,7 +1,8 @@\n \"\"\"\n request-response middleware extension\n \"\"\"\n-from scrapy.core import signals, log\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n\n@@ -8,7 +8,8 @@ from twisted.internet import defer, reactor, task\n from twisted.python.failure import Failure\n from pydispatch import dispatcher\n \n-from scrapy.core import signals, log\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.scheduler import Scheduler\n from scrapy.core.downloader import Downloader\n from scrapy.http import Response, Request\n\n@@ -1,11 +1,11 @@\n import signal\n \n from scrapy.extension import extensions\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.utils.defer import load_class\n+from scrapy.utils.misc import load_class\n from scrapy.utils.url import is_url\n from scrapy.conf import settings\n \n\n@@ -5,7 +5,7 @@ The Scrapy Scheduler\n from twisted.internet import defer\n \n from scrapy.core.scheduler.filter import GroupFilter\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n from scrapy.utils.defer import defer_fail\n\n@@ -3,7 +3,7 @@ Scrapy core signals\n \"\"\"\n from pydispatch import dispatcher\n \n-from scrapy.core import log\n+from scrapy import log\n \n # After the execution engine has started\n # args: None\n\n@@ -5,7 +5,7 @@ ExtensionManager (extensions) to be used as singleton.\n \"\"\"\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.conf import settings\n \n class ExtensionManager(object):\n\n@@ -1,4 +1,4 @@\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import DropItem, NotConfigured\n from scrapy.item import ScrapedItem\n from scrapy.utils.misc import load_class\n\n@@ -1,8 +1,10 @@\n \"\"\" \n Crawler logging functionality\n \"\"\"\n-from twisted.python import log\n import sys\n+\n+from twisted.python import log\n+\n from scrapy.conf import settings\n  \n # Logging levels\n\n@@ -6,7 +6,7 @@ from email.MIMEText import MIMEText\n from email.Utils import COMMASPACE, formatdate\n from email import Encoders\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n\n@@ -8,7 +8,8 @@ import tarfile\n \n from pydispatch import dispatcher\n \n-from scrapy.core import signals, log\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.core.manager import scrapymanager\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n\n@@ -9,7 +9,7 @@ import urlparse\n from twisted.plugin import getCache\n \n from scrapy.spider.models import ISpider\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.url import url_is_from_spider\n \n\n@@ -5,7 +5,7 @@ Spider middleware manager\n \n from twisted.python.failure import Failure\n \n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class\n from scrapy.utils.defer import mustbe_deferred\n\n@@ -5,7 +5,8 @@ import pprint\n \n from pydispatch import dispatcher\n \n-from scrapy.core import signals, log\n+from scrapy.core import signals\n+from scrapy import log\n from scrapy.utils.misc import stats_getpath\n from scrapy.conf import settings\n \n\n@@ -3,7 +3,7 @@ Function for dealing with databases\n \"\"\"\n import re\n from scrapy.conf import settings\n-from scrapy.core import log\n+from scrapy import log\n from scrapy.core.engine import scrapyengine\n \n def mysql_connect(db_uri, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e7e539b037ead9852b8965649f08a5abaa9dac8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 171 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ Functions for dealing with markup text\n import re\n import htmlentitydefs\n \n-_ent_re = re.compile(r'&(#?)(.+?);')\n+_ent_re = re.compile(r'&(#?)([^&;]+);')\n _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n \n def remove_entities(text, keep=(), remove_illegal=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4b709e03f2cf080b04f5426fb8903f320ed38bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,8 @@ class UtilsMarkupTest(unittest.TestCase):\n                          u'As low as \\xa3100!')\n         self.assertEqual(remove_entities('As low as &pound;100!'),\n                          u'As low as \\xa3100!')\n-\n+        self.assertEqual(remove_entities('redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold &frac12;oz solid crucifix pendant'),\n+                         u'redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold \\xbdoz solid crucifix pendant')\n         # keep some entities\n         self.assertEqual(remove_entities('<b>Low &lt; High &amp; Medium &pound; six</b>', keep=['lt', 'amp']),\n                          u'<b>Low &lt; High &amp; Medium \\xa3 six</b>')\n@@ -25,6 +26,8 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_entities('a &lt; b &illegal; c &#12345678; six', remove_illegal=True),\n                          u'a < b  c  six')\n \n+\n+\n     def test_remove_tags(self):\n         # make sure it always return uncode\n         assert isinstance(replace_tags('no entities'), unicode)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79485ceb5cb680721388743c09eafb579e20e7de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 590 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -106,7 +106,16 @@ class ResponseBody(object):\n     This handles conversion to unicode and various character encodings.\n     \"\"\"\n \n-    CHARSET_RE = re.compile(r'<meta\\s+http-equiv\\s*=\\s*[\\\"\\']?\\s*Content-Type\\s*[\\\"\\']?\\s+content\\s*=(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)', re.I)\n+    _template = r'%s\\s*=\\s*[\\\"\\']?\\s*%s\\s*[\\\"\\']?'\n+\n+    _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n+    _content_re   = _template % ('content', r'(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)')\n+    _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n+\n+    XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n+\n+    METATAG_RE  = re.compile(r'<meta\\s+%s\\s+%s' % (_httpequiv_re, _content_re), re.I)\n+    METATAG_RE2 = re.compile(r'<meta\\s+%s\\s+%s' % (_content_re, _httpequiv_re), re.I)\n \n     def __init__(self, content, declared_encoding=None):\n         self._content = content\n@@ -163,7 +172,8 @@ class ResponseBody(object):\n             return self._expected_encoding\n         proposed = self.declared_encoding\n         if not proposed:\n-            match = self.CHARSET_RE.search(self._content[:5000])\n+            chunk = self._content[:5000]\n+            match = self.XMLDECL_RE.search(chunk) or self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)\n             if match:\n                 proposed = match.group(\"charset\")\n         self._expected_encoding = proposed\n\n@@ -79,7 +79,7 @@ def extract_regex(regex, text, encoding):\n     \n     * if the regex contains a named group called \"extract\" that will be returned\n     * if the regex contains multiple numbered groups, all those will be returned (flattened)\n-    * if the refex doesn't contain any group the entire regex matching is returned\n+    * if the regex doesn't contain any group the entire regex matching is returned\n     \"\"\"\n \n     if isinstance(regex, basestring):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c73cbdd9b1db0f993dcd451cd8cab0b1ffff95e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 123 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,15 @@ from scrapy.item.adaptors import AdaptorPipe\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n+    'guid' attribute is required, and is an attribute\n+    that identifies uniquely the given scraped item.\n     \"\"\"\n-    adaptors_pipe = AdaptorPipe([])\n+    adaptors_pipe = AdaptorPipe()\n+\n+    def __init__(self, adaptors_pipe=None):\n+        \"\"\"If an adaptors_pipe is given, overrides class adaptors_pipe\"\"\"\n+        if adaptors_pipe:\n+            self.adaptors_pipe = adaptors_pipe\n \n     def attribute(self, attrname, value):\n         value = self.adaptors_pipe.execute(attrname, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#555cb0940be5a363a0c734addf61ccaee3b6a2e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 187 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class ImagesPipeline(MediaPipeline):\n \n     def media_failed(self, failure, request, info):\n         referer = request.headers.get('Referer')\n-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else failure.getTraceback()\n+        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n         msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' % (self.MEDIA_TYPE, request, referer, errmsg)\n         log.msg(msg, level=log.WARNING, domain=info.domain)\n         raise ImageException(msg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4aa1e7e8ba502c64428f8bb3aa8ff823c711e8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 217 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -106,7 +106,7 @@ class ResponseBody(object):\n     This handles conversion to unicode and various character encodings.\n     \"\"\"\n \n-    _template = r'%s\\s*=\\s*[\\\"\\']?\\s*%s\\s*[\\\"\\']?'\n+    _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n \n     _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n     _content_re   = _template % ('content', r'(?P<mime>[^;]+);\\s*charset=(?P<charset>[\\w-]+)')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad00d5e6321bf67b800762a500994a61f20f05a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 175 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ Functions for dealing with markup text\n import re\n import htmlentitydefs\n \n-_ent_re = re.compile(r'&(#?)([^&;]+);')\n+ent_re = re.compile(r'&(#?)([^&;]+);')\n _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n \n def remove_entities(text, keep=(), remove_illegal=True):\n@@ -45,7 +45,7 @@ def remove_entities(text, keep=(), remove_illegal=True):\n             else:\n                 return u'&%s;' % m.group(2)\n \n-    return _ent_re.sub(convert_entity, text.decode('utf-8'))\n+    return ent_re.sub(convert_entity, text.decode('utf-8'))\n \n \n def replace_tags(text, token=''):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8f2f3a542e6bca10d6d894544d767e8af2633f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 181 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ Functions for dealing with markup text\n import re\n import htmlentitydefs\n \n-ent_re = re.compile(r'&(#?)([^&;]+);')\n+_ent_re = re.compile(r'&(#?)([^&;]+);')\n _tag_re = re.compile(r'<[a-zA-Z\\/!].*?>', re.DOTALL)\n \n def remove_entities(text, keep=(), remove_illegal=True):\n@@ -45,8 +45,10 @@ def remove_entities(text, keep=(), remove_illegal=True):\n             else:\n                 return u'&%s;' % m.group(2)\n \n-    return ent_re.sub(convert_entity, text.decode('utf-8'))\n+    return _ent_re.sub(convert_entity, text.decode('utf-8'))\n \n+def has_entities(text):\n+    return bool(_ent_re.search(text))\n \n def replace_tags(text, token=''):\n     \"\"\"Replace all markup tags found in the given text by the given token. By\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ae4d62f3839c3d81218a79bab953c5c9316ce4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 144 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,8 +17,8 @@ class Decompressor(object):\n         pass\n     \n     def __init__(self):\n-        self.decompressors = [self.is_tar, self.is_zip,\n-                              self.is_gzip, self.is_bzip2]        \n+        self.decompressors = {'tar': self.is_tar, 'zip': self.is_zip,\n+                              'gz': self.is_gzip, 'bz2': self.is_bzip2}        \n     \n     def is_tar(self, response):\n         try:\n@@ -56,14 +56,17 @@ class Decompressor(object):\n             return False\n         return response.replace(body=ResponseBody(decompressed_body))\n     \n-    def extract(self, response):\n+    def extract_winfo(self, response):\n         self.body = response.body.to_string()\n         self.archive = StringIO()\n         self.archive.write(self.body)\n         \n-        for decompressor in self.decompressors:\n+        for decompressor in self.decompressors.keys():\n             self.archive.seek(0)\n-            ret = decompressor(response)\n-            if ret:\n-                return ret\n-        return response\n+            new_response = self.decompressors[decompressor](response)\n+            if new_response:\n+                return new_response, decompressor\n+        return response, ''\n+        \n+    def extract(self, response):\n+        return self.extract_winfo(response)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f32bae45f2f0f50f5bbcb4ff49f13adb4fd2ae8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 64 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,17 +59,6 @@ class Command(ScrapyCommand):\n         pass\n \n     def get_url(self, url, decompress=False):\n-        #def _get_callback(_response):\n-            #print \"done\"\n-            #if not _response:\n-                #if not opts.loglevel:\n-                    #print 'Nothing downloaded, run with -o DEBUG to see why it failed'\n-                #scrapymanager.stop()\n-                #return\n-            #self.generate_vars(url, response)\n-\n-        #def _errback(_failure):\n-            #print _failure\n \n         print \"Downloading URL...           \",\n         r = Request(url)\n@@ -83,6 +72,7 @@ class Command(ScrapyCommand):\n                     d = Decompressor()\n                     result = d.extract(result)\n                     print \"Done.\"\n+                result.request = r\n                 self.generate_vars(url, result)\n                 return True\n         except Exception, e:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a38c72bb05269a3e39958c2568fc9157f27f9ba9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 245 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,8 @@ def urljoin_rfc(base, ref):\n \n \n _reserved = ';/?:@&=+$|,#' # RFC 2396 (Generic Syntax)\n-_safe_chars = urllib.always_safe + '%' + _reserved\n+_unreserved_marks = \"-_.!~*'()\" #RFC 2396 sec 2.3\n+_safe_chars = urllib.always_safe + '%' + _reserved + _unreserved_marks\n \n def safe_url_string(url, use_encoding='utf8'):\n     \"\"\"Convert a unicode (or utf8 string) object into a legal URL.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#433f35b41700cda65d2be8ae0c7f1f86a0fdf6d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 62 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ class RedirectMiddleware(object):\n                     redirected.body = None\n                     # This is needed to avoid redirection loops with requests that contain dont_filter = True\n                     # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n-                    if isinstance(redirected.dont_filter, int):\n+                    if isinstance(redirected.dont_filter, int) and redirected.dont_filter > 0:\n                         redirected.dont_filter -= 1\n                     else:\n                         redirected.dont_filter = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c755f5a5351c4978a441c7efbc14735ecf9e75f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 72 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,10 +5,14 @@ from scrapy.http import Request, Response\n from scrapy.core.exceptions import HttpException\n from scrapy.utils.url import urljoin_rfc as urljoin\n \n+class RedirectLoop(Exception):\n+    pass\n+\n META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);url=([^\"\\']+)', re.IGNORECASE)\n # some sites use meta-refresh for redirecting to a session expired page, so we\n # restrict automatic redirection to a maximum delay (in number of seconds)\n META_REFRESH_MAXSEC = 100\n+MAX_REDIRECT_LOOP = 10\n \n class RedirectMiddleware(object):\n     def process_exception(self, request, exception, spider):\n@@ -25,7 +29,11 @@ class RedirectMiddleware(object):\n                     redirected.body = None\n                     # This is needed to avoid redirection loops with requests that contain dont_filter = True\n                     # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n-                    if isinstance(redirected.dont_filter, int) and redirected.dont_filter > 0:\n+                    if isinstance(redirected.dont_filter, int):\n+                        if not hasattr(redirected, \"original_dont_filter\"):\n+                            redirected.original_dont_filter = redirected.dont_filter\n+                        if redirected.dont_filter <= -MAX_REDIRECT_LOOP:\n+                            raise RedirectLoop(\"Exited redirect loop with %s consecutive visits to the same url.\" % (redirected.original_dont_filter + MAX_REDIRECT_LOOP) )\n                         redirected.dont_filter -= 1\n                     else:\n                         redirected.dont_filter = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ba4573b132cdb437e91d6b6c791bc4828be2908", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 154 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,8 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(safe_url_string(alessi),\n                          '/ecommerce/oggetto/Te%20%C3%B2/tea-strainer/1273')\n \n+        self.assertEqual(safe_url_string(\"http://clkuk.tradedoubler.com/click?p(28639)a(1424750)g(16962058)epi(59fc2ea58da61c7bce606df72a4a10fb3a0bd2ad)url(http://www.bedworld.net/beds_mattresses/112AURHB--/Aurora_Headboard.html.html)\"), 'http://clkuk.tradedoubler.com/click?p(28639)a(1424750)g(16962058)epi(59fc2ea58da61c7bce606df72a4a10fb3a0bd2ad)url(http://www.bedworld.net/beds_mattresses/112AURHB--/Aurora_Headboard.html.html)')\n+\n     def test_safe_download_url(self):\n         self.assertEqual(safe_download_url('http://www.scrapy.org/../'),\n                          'http://www.scrapy.org/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98f3314bed6090f83995beff3c481964930d8c1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 157 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,7 +35,8 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(safe_url_string(alessi),\n                          '/ecommerce/oggetto/Te%20%C3%B2/tea-strainer/1273')\n \n-        self.assertEqual(safe_url_string(\"http://clkuk.tradedoubler.com/click?p(28639)a(1424750)g(16962058)epi(59fc2ea58da61c7bce606df72a4a10fb3a0bd2ad)url(http://www.bedworld.net/beds_mattresses/112AURHB--/Aurora_Headboard.html.html)\"), 'http://clkuk.tradedoubler.com/click?p(28639)a(1424750)g(16962058)epi(59fc2ea58da61c7bce606df72a4a10fb3a0bd2ad)url(http://www.bedworld.net/beds_mattresses/112AURHB--/Aurora_Headboard.html.html)')\n+        self.assertEqual(safe_url_string(\"http://www.example.com/test?p(29)url(http://www.another.net/page)\"),\n+                                         \"http://www.example.com/test?p(29)url(http://www.another.net/page)\")\n \n     def test_safe_download_url(self):\n         self.assertEqual(safe_download_url('http://www.scrapy.org/../'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ec47301e3fa0c47d1d0fc7187687bd477c22322", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 32 | Churn Cumulative: 390 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.8095238095238095\n\nDIFF:\n@@ -36,15 +36,38 @@ class ExecutionManager(object):\n         requests = self._parse_args(args)\n         self.priorities = self.prioritizer_class(requests.keys())\n \n+        #applies a time (in seconds) between succesive requests crawl\n+        #useful for applications that runs softly distributed in time\n+        self.crawl_delay = opts.get(\"crawl_delay\", 0)\n+        \n+        \n     def crawl(self, *args):\n         \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n \n         requests = self._parse_args(args)\n         # schedule initial requets to be scraped at engine start\n+        \n+        def _issue():\n             for domain in requests or ():\n                 spider = spiders.fromdomain(domain)\n                 priority = self.priorities.get_priority(domain)\n                 for request in requests[domain]:\n+                    yield request, spider, priority\n+\n+        if self.crawl_delay:\n+            gen = _issue()\n+            def _soft_crawl():\n+                try:\n+                    request, spider, priority = gen.next()\n+                    scrapyengine.crawl(request, spider, domain_priority=priority)\n+                    log.msg(\"Delaying %ss the next request.\" % self.crawl_delay)\n+                except StopIteration:\n+                    self.stop()\n+\n+            scrapyengine.addtask(_soft_crawl, self.crawl_delay)\n+\n+        else:\n+            for request, spider, priority in _issue():\n                 scrapyengine.crawl(request, spider, domain_priority=priority)\n \n     def runonce(self, *args, **opts):\n@@ -94,7 +117,7 @@ class ExecutionManager(object):\n             signal.signal(signal.SIGBREAK, sig_handler_terminate)\n \n     def _parse_args(self, args):\n-        \"\"\" Parse crawl arguments and return a tuple of (domains, urls) \"\"\"\n+        \"\"\" Parse crawl arguments and return a dict domains -> [requests] \"\"\"\n         if not args:\n             args = [p.domain_name for p in spiders.enabled]\n \n@@ -133,6 +156,9 @@ class ExecutionManager(object):\n \n         # requests\n         for request in requests:\n+            if request.domain:\n+                spider = spiders.fromdomain(request.domain)\n+            else:\n                 spider = spiders.fromurl(request.url)\n             if not spider:\n                 log.msg('Could not found spider for %s' % request, log.ERROR)\n\n@@ -15,7 +15,7 @@ from scrapy.utils.defer import chain_deferred\n class Request(object):\n     def __init__(self, url, callback=None, context=None, method=None, body=None, headers=None, cookies=None,\n             referer=None, url_encoding='utf-8', link_text='', http_user='', http_pass='', dont_filter=None, \n-            fingerprint_params=None):\n+            fingerprint_params=None, domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n@@ -55,6 +55,8 @@ class Request(object):\n             self.httpauth(http_user, http_pass)\n         self.depth = 0\n         self.link_text = link_text\n+        #allows to directly specify the spider for the request\n+        self.domain = domain\n         \n     def append_callback(self, callback, *args, **kwargs):\n         if isinstance(callback, defer.Deferred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#58a419f45ccd1114ba9bc8036e7b5daf5beeede3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 210 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -139,7 +139,7 @@ class Downloader(object):\n             return\n \n         # Instanciate site specific handling based on info provided by spider\n-        delay = getattr(spider, 'download_delay', None)\n+        delay = getattr(spider, 'download_delay', None) or settings.getint(\"DOWNLOAD_DELAY\", 0)\n         maxcr = getattr(spider, 'max_concurrent_requests', settings.getint('REQUESTS_PER_DOMAIN'))\n         site = SiteDetails(download_delay=delay, max_concurrent_requests=maxcr)\n         self.sites[domain] = site\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e447044fb952f0de948a811e883bbdb4c09def15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 23 | Churn Cumulative: 205 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,38 +36,15 @@ class ExecutionManager(object):\n         requests = self._parse_args(args)\n         self.priorities = self.prioritizer_class(requests.keys())\n         \n-        #applies a time (in seconds) between succesive requests crawl\n-        #useful for applications that runs softly distributed in time\n-        self.crawl_delay = opts.get(\"crawl_delay\", 0)\n-        \n-        \n     def crawl(self, *args):\n         \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n \n         requests = self._parse_args(args)\n         # schedule initial requets to be scraped at engine start\n-        \n-        def _issue():\n         for domain in requests or ():\n             spider = spiders.fromdomain(domain) \n             priority = self.priorities.get_priority(domain) \n             for request in requests[domain]:\n-                    yield request, spider, priority\n-\n-        if self.crawl_delay:\n-            gen = _issue()\n-            def _soft_crawl():\n-                try:\n-                    request, spider, priority = gen.next()\n-                    scrapyengine.crawl(request, spider, domain_priority=priority)\n-                    log.msg(\"Delaying %ss the next request.\" % self.crawl_delay)\n-                except StopIteration:\n-                    self.stop()\n-\n-            scrapyengine.addtask(_soft_crawl, self.crawl_delay)\n-\n-        else:\n-            for request, spider, priority in _issue():\n                 scrapyengine.crawl(request, spider, domain_priority=priority)\n \n     def runonce(self, *args, **opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71c3cea112a9041a61b8bd210b0166098168da3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 159 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,6 +37,8 @@ class UrlUtilsTest(unittest.TestCase):\n \n         self.assertEqual(safe_url_string(\"http://www.example.com/test?p(29)url(http://www.another.net/page)\"),\n                                          \"http://www.example.com/test?p(29)url(http://www.another.net/page)\")\n+        self.assertEqual(safe_url_string(\"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\"),\n+                                         \"http://www.example.com/Brochures_&_Paint_Cards&PageSize=200\")\n                   \n     def test_safe_download_url(self):\n         self.assertEqual(safe_download_url('http://www.scrapy.org/../'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#453714b25256f9e3ca3a6a9728bce57cc3d55cc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 39 | Churn Cumulative: 220 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,3 +59,42 @@ def replace_tags(text, token=''):\n     Always returns a unicode string.\n     \"\"\"\n     return _tag_re.sub(token, text.decode('utf-8'))\n+\n+\n+def remove_comments(text):\n+    \"\"\" Remove HTML Comments. \"\"\"\n+    return re.sub('<!--.*?-->', '', text, re.DOTALL)\n+      \n+def remove_tags(text, which_ones=()):\n+    \"\"\" Remove HTML Tags only. \n+\n+        which_ones -- is a tuple of which tags we want to remove.\n+                      if is empty remove all tags.\n+    \"\"\"\n+    if len(which_ones) > 0:\n+        tags = [ '<%s>|<%s .*?>|</%s>' % (tag,tag,tag) for tag in which_ones ]\n+        reg_exp_remove_tags = '|'.join(tags)\n+    else:\n+        reg_exp_remove_tags = '<.*?>'\n+    re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n+    return re_tags.sub('', text)\n+\n+def remove_tags_with_content(text, which_ones=()):\n+    \"\"\" Remove tags and its content.\n+        \n+        which_ones -- is a tuple of which tags with its content we want to remove.\n+                      if is empty do nothing.\n+    \"\"\"\n+    tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n+    re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n+    return re_tags_remove.sub('', text)\n+\n+def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n+    \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n+\n+        which_ones -- is a tuple of which escape chars we want to remove.\n+                      if is empty do nothing.\n+    \"\"\"\n+    re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n+    return re_escape_chars.sub('', text)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#768a31a483573cc0083f4d25b150dd9d759e7e7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 84 | Churn Cumulative: 349 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import unittest\n \n-from scrapy.utils.markup import remove_entities, replace_tags\n+from scrapy.utils.markup import remove_entities, replace_tags, remove_comments\n+from scrapy.utils.markup import remove_tags_with_content, remove_escape_chars, remove_tags\n \n class UtilsMarkupTest(unittest.TestCase):\n \n@@ -28,7 +29,7 @@ class UtilsMarkupTest(unittest.TestCase):\n \n \n \n-    def test_remove_tags(self):\n+    def test_replace_tags(self):\n         # make sure it always return uncode\n         assert isinstance(replace_tags('no entities'), unicode)\n \n@@ -41,3 +42,72 @@ class UtilsMarkupTest(unittest.TestCase):\n         # multiline tags\n         self.assertEqual(replace_tags('Click <a class=\"one\"\\r\\n href=\"url\">here</a>'),\n                          u'Click here')\n+\n+    def test_remove_comments(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_comments('without comments'), unicode)\n+        assert isinstance(remove_comments('<!-- with comments -->'), unicode)\n+\n+        # text without comments \n+        self.assertEqual(remove_comments(u'text without comments'), u'text without comments')\n+\n+        # text with comments\n+        self.assertEqual(remove_comments(u'<!--text with comments-->'), u'')\n+        self.assertEqual(remove_comments(u'Hello<!--World-->'),u'Hello')\n+\n+    def test_remove_tags(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_tags('no tags'), unicode)\n+        assert isinstance(remove_tags('no tags', which_ones=('p',)), unicode)\n+        assert isinstance(remove_tags('<p>one tag</p>'), unicode)\n+        assert isinstance(remove_tags('<p>one tag</p>', which_ones=('p')), unicode)\n+        assert isinstance(remove_tags('<a>link</a>', which_ones=('b',)), unicode)\n+\n+        # text without tags\n+        self.assertEqual(remove_tags(u'no tags'), u'no tags')\n+        self.assertEqual(remove_tags(u'no tags', which_ones=('p','b',)), u'no tags')\n+\n+        # text with tags\n+        self.assertEqual(remove_tags(u'<p>one p tag</p>'), u'one p tag')\n+        self.assertEqual(remove_tags(u'<p>one p tag</p>', which_ones=('b',)), u'<p>one p tag</p>')\n+\n+        self.assertEqual(remove_tags(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n+                         u'<b>not will removed</b>i will removed')\n+\n+        # text with tags and attributes\n+        self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>'), u'texty')\n+        self.assertEqual(remove_tags(u'<p align=\"center\" class=\"one\">texty</p>', which_ones=('b',)),\n+                         u'<p align=\"center\" class=\"one\">texty</p>')\n+\n+    def test_remove_tags_with_content(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_tags_with_content('no tags'), unicode)\n+        assert isinstance(remove_tags_with_content('no tags', which_ones=('p',)), unicode)\n+        assert isinstance(remove_tags_with_content('<p>one tag</p>', which_ones=('p',)), unicode)\n+        assert isinstance(remove_tags_with_content('<a>link</a>', which_ones=('b',)), unicode)\n+\n+        # text without tags\n+        self.assertEqual(remove_tags_with_content(u'no tags'), u'no tags')\n+        self.assertEqual(remove_tags_with_content(u'no tags', which_ones=('p','b',)), u'no tags')\n+\n+        # text with tags\n+        self.assertEqual(remove_tags_with_content(u'<p>one p tag</p>'), u'<p>one p tag</p>')\n+        self.assertEqual(remove_tags_with_content(u'<p>one p tag</p>', which_ones=('p',)), u'')\n+\n+        self.assertEqual(remove_tags_with_content(u'<b>not will removed</b><i>i will removed</i>', which_ones=('i',)),\n+                         u'<b>not will removed</b>')\n+\n+    def test_remove_escape_chars(self):\n+        # make sure it always return unicode\n+        assert isinstance(remove_escape_chars('no ec'), unicode)\n+        assert isinstance(remove_escape_chars('no ec', which_ones=('\\n','\\t',)), unicode)\n+\n+        # text without escape chars\n+        self.assertEqual(remove_escape_chars(u'no ec'), u'no ec')\n+        self.assertEqual(remove_escape_chars(u'no ec', which_ones=('\\n',)), u'no ec')\n+\n+        # text with escape chars\n+        self.assertEqual(remove_escape_chars(u'escape\\n\\n'), u'escape')\n+        self.assertEqual(remove_escape_chars(u'escape\\n', which_ones=('\\t',)), u'escape\\n')\n+        self.assertEqual(remove_escape_chars(u'escape\\tchars\\n', which_ones=('\\t')), 'escapechars\\n')\n+\n\n@@ -63,7 +63,7 @@ def replace_tags(text, token=''):\n \n def remove_comments(text):\n     \"\"\" Remove HTML Comments. \"\"\"\n-    return re.sub('<!--.*?-->', '', text, re.DOTALL)\n+    return re.sub('<!--.*?-->', '', text.decode('utf-8'), re.DOTALL)\n       \n def remove_tags(text, which_ones=()):\n     \"\"\" Remove HTML Tags only. \n@@ -77,7 +77,7 @@ def remove_tags(text, which_ones=()):\n     else:\n         reg_exp_remove_tags = '<.*?>'\n     re_tags = re.compile(reg_exp_remove_tags, re.DOTALL)\n-    return re_tags.sub('', text)\n+    return re_tags.sub('', text.decode('utf-8'))\n \n def remove_tags_with_content(text, which_ones=()):\n     \"\"\" Remove tags and its content.\n@@ -87,14 +87,14 @@ def remove_tags_with_content(text, which_ones=()):\n     \"\"\"\n     tags = [ '<%s.*?</%s>' % (tag,tag) for tag in which_ones ]\n     re_tags_remove = re.compile('|'.join(tags), re.DOTALL)\n-    return re_tags_remove.sub('', text)\n+    return re_tags_remove.sub('', text.decode('utf-8'))\n \n def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n \n         which_ones -- is a tuple of which escape chars we want to remove.\n-                      if is empty do nothing.\n+                      By default removes \\n, \\t, \\r.\n     \"\"\"\n     re_escape_chars = re.compile('[%s]' % ''.join(which_ones))\n-    return re_escape_chars.sub('', text)\n+    return re_escape_chars.sub('', text.decode('utf-8'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
