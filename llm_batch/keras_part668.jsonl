{"custom_id": "keras#037b85b208af1a81cedf70d722ab2bd4ffe8e1f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 39 | Churn Cumulative: 206 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -84,6 +84,45 @@ def Input(\n     name=None,\n     tensor=None,\n ):\n+    \"\"\"Used to instantiate a Keras tensor.\n+\n+    A Keras tensor is a symbolic tensor-like object, which we augment with\n+    certain attributes that allow us to build a Keras model just by knowing the\n+    inputs and outputs of the model.\n+\n+    For instance, if `a`, `b` and `c` are Keras tensors,\n+    it becomes possible to do:\n+    `model = Model(input=[a, b], output=c)`\n+\n+    Args:\n+        shape: A shape tuple (tuple of integers or `None` objects),\n+            not including the batch size.\n+            For instance, `shape=(32,)` indicates that the expected input\n+            will be batches of 32-dimensional vectors. Elements of this tuple\n+            can be `None`; `None` elements represent dimensions where the shape\n+            is not known and may vary (e.g. sequence length).\n+        batch_size: Optional static batch size (integer).\n+        dtype: The data type expected by the input, as a string\n+            (e.g. `\"float32\"`, `\"int32\"`...)\n+        name: Optional name string for the layer.\n+            Should be unique in a model (do not reuse the same name twice).\n+            It will be autogenerated if it isn't provided.\n+        tensor: Optional existing tensor to wrap into the `Input` layer.\n+            If set, the layer will use this tensor rather\n+            than creating a new placeholder tensor.\n+\n+    Returns:\n+      A Keras tensor.\n+\n+    Example:\n+\n+    ```python\n+    # This is a logistic regression in Keras\n+    x = Input(shape=(32,))\n+    y = Dense(16, activation='softmax')(x)\n+    model = Model(x, y)\n+    ```\n+    \"\"\"\n     layer = InputLayer(\n         shape=shape,\n         batch_size=batch_size,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#273d3be5cd0f1d478aa44d5f6e02cb290b042d80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1022 | Contributors (this commit): 7 | Commits (past 90d): 36 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,13 +11,13 @@ from keras_core.random.seed_generator import make_default_seed\n \n \n def torch_seed_generator(seed):\n-    seed_val, _ = draw_seed(seed)\n+    first_seed, second_seed = draw_seed(seed)\n     device = get_device()\n     if device == \"meta\":\n         # Generator is not supported by the meta device.\n         return None\n     generator = torch.Generator(device=get_device())\n-    generator.manual_seed(int(seed_val))\n+    generator.manual_seed(int(first_seed + second_seed))\n     return generator\n \n \n\n@@ -27,6 +27,22 @@ class InitializersTest(testing.TestCase):\n \n         self.run_class_serialization_test(initializer)\n \n+        # Test that a fixed seed yields the same results each call.\n+        initializer = initializers.RandomNormal(\n+            mean=mean, stddev=stddev, seed=1337\n+        )\n+        values = initializer(shape=shape)\n+        next_values = initializer(shape=shape)\n+        self.assertAllClose(values, next_values)\n+\n+        # Test that a SeedGenerator yields different results each call.\n+        initializer = initializers.RandomNormal(\n+            mean=mean, stddev=stddev, seed=backend.random.SeedGenerator(1337)\n+        )\n+        values = initializer(shape=shape)\n+        next_values = initializer(shape=shape)\n+        self.assertNotAllClose(values, next_values)\n+\n         # Test serialization with SeedGenerator\n         initializer = initializers.RandomNormal(\n             mean=mean, stddev=stddev, seed=backend.random.SeedGenerator(1337)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e0cc00e799dd960806c2dccd00f3204615339ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 52 | Churn Cumulative: 1230 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -215,29 +215,6 @@ def print_summary(\n \n     table = rich.table.Table(*columns, width=line_length, show_lines=True)\n \n-    def get_layer_fields(layer, prefix=\"\"):\n-        output_shape = format_layer_shape(layer)\n-        name = prefix + layer.name\n-        cls_name = layer.__class__.__name__\n-        name = rich.markup.escape(name)\n-        name += f\" ({highlight_symbol(rich.markup.escape(cls_name))})\"\n-\n-        if not hasattr(layer, \"built\"):\n-            params = highlight_number(0)\n-        elif not layer.built:\n-            params = highlight_number(0) + \" (unbuilt)\"\n-        else:\n-            params = highlight_number(f\"{layer.count_params():,}\")\n-\n-        fields = [name, output_shape, params]\n-        if show_trainable:\n-            fields.append(\n-                bold_text(\"Y\", color=34)\n-                if layer.trainable\n-                else bold_text(\"N\", color=9)\n-            )\n-        return fields\n-\n     def get_connections(layer):\n         connections = \"\"\n         for node in layer._inbound_nodes:\n@@ -258,6 +235,31 @@ def print_summary(\n             connections = \"-\"\n         return connections\n \n+    def get_layer_fields(layer, prefix=\"\"):\n+        output_shape = format_layer_shape(layer)\n+        name = prefix + layer.name\n+        cls_name = layer.__class__.__name__\n+        name = rich.markup.escape(name)\n+        name += f\" ({highlight_symbol(rich.markup.escape(cls_name))})\"\n+\n+        if not hasattr(layer, \"built\"):\n+            params = highlight_number(0)\n+        elif not layer.built:\n+            params = highlight_number(0) + \" (unbuilt)\"\n+        else:\n+            params = highlight_number(f\"{layer.count_params():,}\")\n+\n+        fields = [name, output_shape, params]\n+        if not sequential_like:\n+            fields.append(get_connections(layer))\n+        if show_trainable:\n+            fields.append(\n+                bold_text(\"Y\", color=34)\n+                if layer.trainable\n+                else bold_text(\"N\", color=9)\n+            )\n+        return fields\n+\n     def print_layer(layer, nested_level=0):\n         if nested_level:\n             prefix = \"   \" * nested_level + \"└\" + \" \"\n@@ -265,10 +267,6 @@ def print_summary(\n             prefix = \"\"\n \n         fields = get_layer_fields(layer, prefix=prefix)\n-        if not sequential_like:\n-            fields.append(get_connections(layer))\n-        if show_trainable:\n-            fields.append(\"Y\" if layer.trainable else \"N\")\n \n         rows = [fields]\n         if expand_nested and hasattr(layer, \"layers\") and layer.layers:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#169c781be146163b6b074f13c1a096e1c7f6a7e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 1241 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -170,13 +170,13 @@ def print_summary(\n                     break\n \n     if sequential_like:\n-        line_length = line_length or 84\n+        default_line_length = 84\n         positions = positions or [0.45, 0.84, 1.0]\n         # header names for the different log elements\n         header = [\"Layer (type)\", \"Output Shape\", \"Param #\"]\n         alignment = [\"left\", \"left\", \"right\"]\n     else:\n-        line_length = line_length or 108\n+        default_line_length = 108\n         positions = positions or [0.3, 0.56, 0.70, 1.0]\n         # header names for the different log elements\n         header = [\"Layer (type)\", \"Output Shape\", \"Param #\", \"Connected to\"]\n@@ -186,13 +186,16 @@ def print_summary(\n             relevant_nodes += v\n \n     if show_trainable:\n-        line_length += 8\n+        default_line_length += 8\n         positions = [p * 0.88 for p in positions] + [1.0]\n         header.append(\"Trainable\")\n         alignment.append(\"center\")\n \n     # Compute columns widths\n-    line_length = min(line_length, shutil.get_terminal_size().columns - 4)\n+    default_line_length = min(\n+        default_line_length, shutil.get_terminal_size().columns - 4\n+    )\n+    line_length = line_length or default_line_length\n     column_widths = []\n     current = 0\n     for pos in positions:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ecf8da06ac3a7df7b32eacb9868b0db43b9185b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 309 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n import keras_core\n-from keras_core.utils.model_visualization import plot_model\n+from keras_core.utils import plot_model\n \n \n def plot_sequential_model():\n@@ -208,18 +208,18 @@ def plot_nested_functional_model():\n     inputs = keras_core.Input((3,))\n     x = keras_core.layers.Dense(4, activation=\"relu\")(inputs)\n     x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    outputs = keras_core.layers.Dense(4, activation=\"relu\")(x)\n+    outputs = keras_core.layers.Dense(3, activation=\"relu\")(x)\n     inner_model = keras_core.Model(inputs, outputs)\n \n     inputs = keras_core.Input((3,))\n-    x = keras_core.layers.Dense(4, activation=\"relu\", trainable=False)(inputs)\n+    x = keras_core.layers.Dense(3, activation=\"relu\", trainable=False)(inputs)\n     residual = x\n     x = inner_model(x)\n     x += residual\n     residual = x\n     x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n     x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras_core.layers.Dense(3, activation=\"relu\")(x)\n     x += residual\n     x = keras_core.layers.Dropout(0.5)(x)\n     outputs = keras_core.layers.Dense(1, activation=\"sigmoid\")(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e54715a151d0887339224e6d38c0ffde49569712", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,4 +16,4 @@ from keras_core.models import Functional\n from keras_core.models import Model\n from keras_core.models import Sequential\n \n-__version__ = \"0.0.1\"\n+__version__ = \"0.1.0\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c60b47b71d9f8a79797e78948878e7b2a3ca8125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 634 | Contributors (this commit): 6 | Commits (past 90d): 19 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -304,7 +304,7 @@ def convert_to_arrays(arrays, dtype=None):\n             # `torch.Tensor`, as well as any other tensor-like object that has\n             # added numpy support.\n             if hasattr(x, \"__array__\"):\n-                x = np.array(x, dtype=dtype)\n+                x = backend.convert_to_numpy(x).astype(dtype)\n             else:\n                 raise ValueError(\n                     \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60ca4b1799176f7548c5cbf259783f10ffe974b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 173 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n from tensorflow import nest\n \n-from keras_core import backend\n+import keras_core.backend\n from keras_core.layers.layer import Layer\n from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n@@ -22,7 +22,7 @@ class TFDataLayer(Layer):\n \n     def __call__(self, inputs, **kwargs):\n         if backend_utils.in_tf_graph() and not isinstance(\n-            inputs, backend.KerasTensor\n+            inputs, keras_core.backend.KerasTensor\n         ):\n             # We're in a TF graph, e.g. a tf.data pipeline.\n             self.backend.set_backend(\"tensorflow\")\n@@ -47,7 +47,7 @@ class TFDataLayer(Layer):\n \n     @tracking.no_automatic_dependency_tracking\n     def _get_seed_generator(self, backend=None):\n-        if backend is None or backend == self.backend._backend:\n+        if backend is None or backend == keras_core.backend.backend():\n             return self.generator\n         if not hasattr(self, \"_backend_generators\"):\n             self._backend_generators = {}\n\n@@ -2,6 +2,7 @@ import random as python_random\n \n import numpy as np\n \n+import keras_core.backend\n from keras_core.api_export import keras_core_export\n \n \n@@ -30,9 +31,9 @@ class SeedGenerator:\n         if kwargs:\n             raise ValueError(f\"Unrecognized keyword arguments: {kwargs}\")\n         if custom_backend is not None:\n-            backend = custom_backend\n+            self.backend = custom_backend\n         else:\n-            from keras_core import backend\n+            self.backend = keras_core.backend\n \n         if seed is None:\n             seed = make_default_seed()\n@@ -43,9 +44,9 @@ class SeedGenerator:\n \n         def seed_initializer(*args, **kwargs):\n             dtype = kwargs.get(\"dtype\", None)\n-            return backend.convert_to_tensor([seed, 0], dtype=dtype)\n+            return self.backend.convert_to_tensor([seed, 0], dtype=dtype)\n \n-        self.state = backend.Variable(\n+        self.state = self.backend.Variable(\n             seed_initializer,\n             shape=(2,),\n             dtype=\"uint32\",\n@@ -65,7 +66,9 @@ def draw_seed(seed):\n         seed_state = seed.state\n         # Use * 1 to create a copy\n         new_seed_value = seed_state.value * 1\n-        increment = convert_to_tensor(np.array([0, 1]), dtype=\"uint32\")\n+        increment = seed.backend.convert_to_tensor(\n+            np.array([0, 1]), dtype=\"uint32\"\n+        )\n         seed.state.assign(seed_state + increment)\n         return new_seed_value\n     elif isinstance(seed, int):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1a159cce34896e47bd73d9ac387cab6ec611945", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1923 | Contributors (this commit): 9 | Commits (past 90d): 41 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -31,10 +31,10 @@ class Functional(Function, Model):\n     Example:\n \n     ```\n-    inputs = {'x1': keras_core.Input(shape=(10,)),\n-              'x2': keras_core.Input(shape=(1,))}\n+    inputs = {'x1': keras_core.Input(shape=(10,), name='x1'),\n+              'x2': keras_core.Input(shape=(1,), name='x2')}\n     t = keras_core.layers.Dense(1, activation='relu')(inputs['x1'])\n-    outputs = keras_core.layers.Add()([t, inputs['x2'])\n+    outputs = keras_core.layers.Add()([t, inputs['x2']])\n     model = keras_core.Model(inputs, outputs)\n     ```\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33fa6d56273a01a55d8d975318351531f4d83c05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1594 | Contributors (this commit): 7 | Commits (past 90d): 26 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -108,8 +108,6 @@ class Adam(optimizer.Optimizer):\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        beta_1_power = None\n-        beta_2_power = None\n         lr = ops.cast(learning_rate, variable.dtype)\n         gradient = ops.cast(gradient, variable.dtype)\n         local_step = ops.cast(self.iterations + 1, variable.dtype)\n\n@@ -124,8 +124,6 @@ class AdamW(optimizer.Optimizer):\n \n     def update_step(self, gradient, variable, learning_rate):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        beta_1_power = None\n-        beta_2_power = None\n         lr = ops.cast(learning_rate, variable.dtype)\n         gradient = ops.cast(gradient, variable.dtype)\n         local_step = ops.cast(self.iterations + 1, variable.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2365af24fbd78dc743cdca9601408bd52f7409d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1247 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,12 @@ import re\n import shutil\n \n import rich\n+import rich.console\n+import rich.markup\n+\n+# See https://github.com/keras-team/keras-core/issues/448\n+# for below imports\n+import rich.table\n from tensorflow import nest\n \n from keras_core import backend\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13caae72840a0b19ac042c0476f5d613afff00a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 368 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -157,13 +157,7 @@ devices = mesh_utils.create_device_mesh((8,))\n # data will be split along the batch axis\n data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\n # naming axes of the sharded partition\n-data_sharding = NamedSharding(\n-    data_mesh,\n-    P(\n-        \"batch\",\n-    ),\n-)\n-\n+data_sharding = NamedSharding(data_mesh,P(\"batch\",),)\n # all variables will be replicated on all devices\n var_mesh = Mesh(devices, axis_names=(\"_\"))\n # in NamedSharding, axes that are not mentioned are replicated (all axes here)\n@@ -275,7 +269,7 @@ def train_step(train_state, x, y):\n     )\n \n     trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        grads, train_state.trainable_variables, train_state.optimizer_variables\n+        train_state.optimizer_variables, grads, train_state.trainable_variables\n     )\n \n     return loss_value, TrainingState(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f41817b34542366941e976363c342145c9ff0725", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 22 | Files Changed: 3 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 52 | Churn Cumulative: 3715 | Contributors (this commit): 9 | Commits (past 90d): 40 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -157,7 +157,13 @@ devices = mesh_utils.create_device_mesh((8,))\n # data will be split along the batch axis\n data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\n # naming axes of the sharded partition\n-data_sharding = NamedSharding(data_mesh,P(\"batch\",),)\n+data_sharding = NamedSharding(\n+    data_mesh,\n+    P(\n+        \"batch\",\n+    ),\n+)\n+\n # all variables will be replicated on all devices\n var_mesh = Mesh(devices, axis_names=(\"_\"))\n # in NamedSharding, axes that are not mentioned are replicated (all axes here)\n@@ -269,7 +275,7 @@ def train_step(train_state, x, y):\n     )\n \n     trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        train_state.optimizer_variables, grads, train_state.trainable_variables\n+        grads, train_state.trainable_variables, train_state.optimizer_variables\n     )\n \n     return loss_value, TrainingState(\n\n@@ -43,20 +43,16 @@ class TorchTrainer(base_trainer.Trainer):\n \n         # Compute gradients\n         if self.trainable_weights:\n-            # Backpropagation\n-            trainable_weights = [v for v in self.trainable_weights]\n-\n             # Call torch.Tensor.backward() on the loss to compute gradients\n             # for the weights.\n             loss.backward()\n \n+            trainable_weights = self.trainable_weights[:]\n             gradients = [v.value.grad for v in trainable_weights]\n \n             # Update weights\n             with torch.no_grad():\n-                self.optimizer.apply_gradients(\n-                    zip(gradients, trainable_weights)\n-                )\n+                self.optimizer.apply(gradients, trainable_weights)\n         else:\n             warnings.warn(\"The model does not have any trainable weights.\")\n \n\n@@ -193,7 +193,6 @@ class BaseOptimizer:\n \n         `variables` can be provided on the first call to build the optimizer.\n         \"\"\"\n-        grads = list(grads)\n         if len(grads) == 0:\n             # It is possible that the grad is empty. In this case,\n             # `apply_gradients` is a no-op.\n@@ -224,16 +223,15 @@ class BaseOptimizer:\n                 self.built = True\n             self._check_variables_are_known(trainable_variables)\n \n-        grads_and_vars = list(zip(grads, self._trainable_variables))\n-\n         with ops.name_scope(self.name):\n             # Filter empty gradients.\n-            grads_and_vars = self._filter_empty_gradients(grads_and_vars)\n-            if len(list(grads_and_vars)) == 0:\n+            grads, trainable_variables = self._filter_empty_gradients(\n+                grads, trainable_variables\n+            )\n+            if len(list(grads)) == 0:\n                 return\n \n             # Apply clipping and weight decay.\n-            grads, trainable_variables = zip(*grads_and_vars)\n             grads = self._clip_gradients(grads)\n             self._apply_weight_decay(trainable_variables)\n \n@@ -363,19 +361,27 @@ class BaseOptimizer:\n             return self._learning_rate(self.iterations)\n         return self._learning_rate\n \n-    def _filter_empty_gradients(self, grads_and_vars):\n-        filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n+    def _filter_empty_gradients(self, grads, vars):\n+        for grad in grads:\n+            if grad is None:\n+                # Filtering is required.\n+                filtered = [\n+                    (g, v) for g, v in zip(grads, vars) if g is not None\n+                ]\n                 if not filtered:\n                     raise ValueError(\"No gradients provided for any variable.\")\n-        if len(filtered) < len(grads_and_vars):\n-            missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n+                if len(filtered) < len(grads):\n+                    missing_grad_vars = [\n+                        v for g, v in zip(grads, vars) if g is None\n+                    ]\n                     warnings.warn(\n                         \"Gradients do not exist for variables \"\n-                f\"{[v.name for v in missing_grad_vars]} when minimizing the \"\n-                \"loss. If you're using `model.compile()`, did you forget to \"\n-                \"provide a `loss` argument?\"\n+                        f\"{[v.name for v in missing_grad_vars]} when \"\n+                        \"minimizing the loss. If using `model.compile()`, \"\n+                        \"did you forget to provide a `loss` argument?\"\n                     )\n-        return filtered\n+                return zip(*filtered)\n+        return grads, vars\n \n     def _clip_gradients(self, grads):\n         if self.clipnorm and self.clipnorm > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b06d1839e29ece47d2347d4f03325ea8c19ac6d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 275 | Lines Deleted: 30 | Files Changed: 4 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/5 | Churn Δ: 305 | Churn Cumulative: 4020 | Contributors (this commit): 10 | Commits (past 90d): 45 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -157,13 +157,7 @@ devices = mesh_utils.create_device_mesh((8,))\n # data will be split along the batch axis\n data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\n # naming axes of the sharded partition\n-data_sharding = NamedSharding(\n-    data_mesh,\n-    P(\n-        \"batch\",\n-    ),\n-)\n-\n+data_sharding = NamedSharding(data_mesh,P(\"batch\",),)\n # all variables will be replicated on all devices\n var_mesh = Mesh(devices, axis_names=(\"_\"))\n # in NamedSharding, axes that are not mentioned are replicated (all axes here)\n@@ -275,7 +269,7 @@ def train_step(train_state, x, y):\n     )\n \n     trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        grads, train_state.trainable_variables, train_state.optimizer_variables\n+        train_state.optimizer_variables, grads, train_state.trainable_variables\n     )\n \n     return loss_value, TrainingState(\n\n@@ -0,0 +1,253 @@\n+\"\"\"\n+Title: Multi-GPU distributed training with JAX\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2023/07/11\n+Last modified: 2023/07/11\n+Description: Guide to multi-GPU/TPU training for Keras models with JAX.\n+Accelerator: GPU or TPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+There are generally two ways to distribute computation across multiple devices:\n+\n+**Data parallelism**, where a single model gets replicated on multiple devices or\n+multiple machines. Each of them processes different batches of data, then they merge\n+their results. There exist many variants of this setup, that differ in how the different\n+model replicas merge results, in whether they stay in sync at every batch or whether they\n+are more loosely coupled, etc.\n+\n+**Model parallelism**, where different parts of a single model run on different devices,\n+processing a single batch of data together. This works best with models that have a\n+naturally-parallel architecture, such as models that feature multiple branches.\n+\n+This guide focuses on data parallelism, in particular **synchronous data parallelism**,\n+where the different replicas of the model stay in sync after each batch they process.\n+Synchronicity keeps the model convergence behavior identical to what you would see for\n+single-device training.\n+\n+Specifically, this guide teaches you how to use `jax.sharding` APIs to train Keras\n+models, with minimal changes to your code, on multiple GPUs or TPUS (typically 2 to 16)\n+installed on a single machine (single host, multi-device training). This is the\n+most common setup for researchers and small-scale industry workflows.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\n+Let's start by defining the function that creates the model that we will train,\n+and the function that creates the dataset we will train on (MNIST in this case).\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n+import jax\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+\n+from jax.experimental import mesh_utils\n+from jax.sharding import Mesh\n+from jax.sharding import NamedSharding\n+from jax.sharding import PartitionSpec as P\n+\n+\n+def get_model():\n+    # Make a simple convnet with batch normalization and dropout.\n+    inputs = keras.Input(shape=(28, 28, 1))\n+    x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n+    x = keras.layers.Conv2D(\n+        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n+    )(x)\n+    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n+    x = keras.layers.ReLU()(x)\n+    x = keras.layers.Conv2D(\n+        filters=24,\n+        kernel_size=6,\n+        use_bias=False,\n+        strides=2,\n+    )(x)\n+    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n+    x = keras.layers.ReLU()(x)\n+    x = keras.layers.Conv2D(\n+        filters=32,\n+        kernel_size=6,\n+        padding=\"same\",\n+        strides=2,\n+        name=\"large_k\",\n+    )(x)\n+    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n+    x = keras.layers.ReLU()(x)\n+    x = keras.layers.GlobalAveragePooling2D()(x)\n+    x = keras.layers.Dense(256, activation=\"relu\")(x)\n+    x = keras.layers.Dropout(0.5)(x)\n+    outputs = keras.layers.Dense(10)(x)\n+    model = keras.Model(inputs, outputs)\n+    return model\n+\n+\n+def get_datasets():\n+    # Load the data and split it between train and test sets\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+    # Scale images to the [0, 1] range\n+    x_train = x_train.astype(\"float32\")\n+    x_test = x_test.astype(\"float32\")\n+    # Make sure images have shape (28, 28, 1)\n+    x_train = np.expand_dims(x_train, -1)\n+    x_test = np.expand_dims(x_test, -1)\n+    print(\"x_train shape:\", x_train.shape)\n+    print(x_train.shape[0], \"train samples\")\n+    print(x_test.shape[0], \"test samples\")\n+\n+    # Create TF Datasets\n+    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+    eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+    return train_data, eval_data\n+\n+\n+\"\"\"\n+## Single-host, multi-device synchronous training\n+\n+In this setup, you have one machine with several GPUs or TPUs on it (typically 2 to 16).\n+Each device will run a copy of your model (called a **replica**). For simplicity, in\n+what follows, we'll assume we're dealing with 8 GPUs, at no loss of generality.\n+\n+**How it works**\n+\n+At each step of training:\n+\n+- The current batch of data (called **global batch**) is split into 8 different\n+  sub-batches (called **local batches**). For instance, if the global batch has 512\n+  samples, each of the 8 local batches will have 64 samples.\n+- Each of the 8 replicas independently processes a local batch: they run a forward pass,\n+  then a backward pass, outputting the gradient of the weights with respect to the loss of\n+  the model on the local batch.\n+- The weight updates originating from local gradients are efficiently merged across the 8\n+  replicas. Because this is done at the end of every step, the replicas always stay in\n+  sync.\n+\n+In practice, the process of synchronously updating the weights of the model replicas is\n+handled at the level of each individual weight variable. This is done through a using\n+a `jax.sharding.NamedSharding` that is configured to replicate the variables.\n+\n+**How to use it**\n+\n+To do single-host, multi-device synchronous training with a Keras model, you\n+would use the `jax.sharding` features. Here's how it works:\n+\n+- We first create a device mesh using `mesh_utils.create_device_mesh`.\n+- We use `jax.sharding.Mesh`, `jax.sharding.NamedSharding` and\n+  `jax.sharding.PartitionSpec` to define how to partition JAX arrays.\n+    - We specify that we want to replicate the model and optimizer variables\n+      across all devices by using a spec with no axis.\n+    - We specify that we want to shard the data across devices by using a spec\n+      that splits along the batch dimension.\n+- We use `jax.device_put` to replicate the model and optimizer variables across\n+  devices. This happens once at the beginning.\n+- In the training loop, for each batch that we process, we use `jax.device_put`\n+  to split the batch across devices before invoking the train step.\n+\n+Here's the flow, where each step is split into its own utility function:\n+\"\"\"\n+\n+# Config\n+num_epochs = 2\n+batch_size = 64\n+\n+train_data, eval_data = get_datasets()\n+train_data = train_data.batch(batch_size, drop_remainder=True)\n+\n+model = get_model()\n+optimizer = keras.optimizers.Adam(1e-3)\n+loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+\n+# Initialize all state with .build()\n+(one_batch, one_batch_labels) = next(iter(train_data))\n+model.build(one_batch)\n+optimizer.build(model.trainable_variables)\n+\n+\n+# This is the loss function that will be differentiated.\n+# Keras provides a pure functional forward pass: model.stateless_call\n+def compute_loss(trainable_variables, non_trainable_variables, x, y):\n+    y_pred, updated_non_trainable_variables = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x)\n+    loss_value = loss(y, y_pred)\n+    return loss_value, updated_non_trainable_variables\n+\n+\n+# Function to compute gradients\n+compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)\n+\n+\n+# Training step, Keras provides a pure functional optimizer.stateless_apply\n+@jax.jit\n+def train_step(train_state, x, y):\n+    trainable_variables, non_trainable_variables, optimizer_variables = train_state\n+    (loss_value, non_trainable_variables), grads = compute_gradients(\n+        trainable_variables, non_trainable_variables, x, y\n+    )\n+\n+    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n+        optimizer_variables, grads, trainable_variables\n+    )\n+\n+    return loss_value, (trainable_variables, non_trainable_variables, optimizer_variables)\n+\n+\n+# Replicate the model and optimizer variable on all devices\n+def get_replicated_train_state(devices):\n+    # All variables will be replicated on all devices\n+    var_mesh = Mesh(devices, axis_names=('_'))\n+    # In NamedSharding, axes not mentioned are replicated (all axes here)\n+    var_replication = NamedSharding(var_mesh, P())\n+\n+    # Apply the distribution settings to the model variables\n+    trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n+    non_trainable_variables = jax.device_put(model.non_trainable_variables, var_replication)\n+    optimizer_variables = jax.device_put(optimizer.variables, var_replication)\n+\n+    # Combine all state in a tuple\n+    return (trainable_variables, non_trainable_variables, optimizer_variables)\n+\n+\n+num_devices = len(jax.local_devices())\n+print(f\"Running on {num_devices} devices: {jax.local_devices()}\")\n+devices = mesh_utils.create_device_mesh((num_devices,))\n+\n+# Data will be split along the batch axis\n+data_mesh = Mesh(devices, axis_names=('batch',)) # naming axes of the mesh\n+data_sharding = NamedSharding(data_mesh, P('batch',)) # naming axes of the sharded partition\n+\n+# Display data sharding\n+x, y = next(iter(train_data))\n+sharded_x = jax.device_put(x.numpy(), data_sharding)\n+print(\"Data sharding\")\n+jax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [-1, 28*28]))\n+\n+train_state = get_replicated_train_state(devices)\n+\n+# Custom training loop\n+for epoch in range(num_epochs):\n+    data_iter = iter(train_data)\n+    for data in data_iter:\n+        x, y = data\n+        sharded_x = jax.device_put(x.numpy(), data_sharding)\n+        loss_value, train_state = train_step(train_state, sharded_x, y.numpy())\n+    print(\"Epoch\", epoch, \"loss:\", loss_value)\n+\n+# Post-processing model state update to write them back into the model\n+trainable_variables, non_trainable_variables, optimizer_variables = train_state\n+for variable, value in zip(model.trainable_variables, trainable_variables):\n+    variable.assign(value)\n+for variable, value in zip(\n+    model.non_trainable_variables, non_trainable_variables\n+):\n+    variable.assign(value)\n+\n+\"\"\"\n+That's it!\n+\"\"\"\n\n@@ -43,16 +43,20 @@ class TorchTrainer(base_trainer.Trainer):\n \n         # Compute gradients\n         if self.trainable_weights:\n+            # Backpropagation\n+            trainable_weights = [v for v in self.trainable_weights]\n+\n             # Call torch.Tensor.backward() on the loss to compute gradients\n             # for the weights.\n             loss.backward()\n \n-            trainable_weights = self.trainable_weights[:]\n             gradients = [v.value.grad for v in trainable_weights]\n \n             # Update weights\n             with torch.no_grad():\n-                self.optimizer.apply(gradients, trainable_weights)\n+                self.optimizer.apply_gradients(\n+                    zip(gradients, trainable_weights)\n+                )\n         else:\n             warnings.warn(\"The model does not have any trainable weights.\")\n \n\n@@ -193,6 +193,7 @@ class BaseOptimizer:\n \n         `variables` can be provided on the first call to build the optimizer.\n         \"\"\"\n+        grads = list(grads)\n         if len(grads) == 0:\n             # It is possible that the grad is empty. In this case,\n             # `apply_gradients` is a no-op.\n@@ -223,15 +224,16 @@ class BaseOptimizer:\n                 self.built = True\n             self._check_variables_are_known(trainable_variables)\n \n+        grads_and_vars = list(zip(grads, self._trainable_variables))\n+\n         with ops.name_scope(self.name):\n             # Filter empty gradients.\n-            grads, trainable_variables = self._filter_empty_gradients(\n-                grads, trainable_variables\n-            )\n-            if len(list(grads)) == 0:\n+            grads_and_vars = self._filter_empty_gradients(grads_and_vars)\n+            if len(list(grads_and_vars)) == 0:\n                 return\n \n             # Apply clipping and weight decay.\n+            grads, trainable_variables = zip(*grads_and_vars)\n             grads = self._clip_gradients(grads)\n             self._apply_weight_decay(trainable_variables)\n \n@@ -361,27 +363,19 @@ class BaseOptimizer:\n             return self._learning_rate(self.iterations)\n         return self._learning_rate\n \n-    def _filter_empty_gradients(self, grads, vars):\n-        for grad in grads:\n-            if grad is None:\n-                # Filtering is required.\n-                filtered = [\n-                    (g, v) for g, v in zip(grads, vars) if g is not None\n-                ]\n+    def _filter_empty_gradients(self, grads_and_vars):\n+        filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n         if not filtered:\n             raise ValueError(\"No gradients provided for any variable.\")\n-                if len(filtered) < len(grads):\n-                    missing_grad_vars = [\n-                        v for g, v in zip(grads, vars) if g is None\n-                    ]\n+        if len(filtered) < len(grads_and_vars):\n+            missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n             warnings.warn(\n                 \"Gradients do not exist for variables \"\n-                        f\"{[v.name for v in missing_grad_vars]} when \"\n-                        \"minimizing the loss. If using `model.compile()`, \"\n-                        \"did you forget to provide a `loss` argument?\"\n+                f\"{[v.name for v in missing_grad_vars]} when minimizing the \"\n+                \"loss. If you're using `model.compile()`, did you forget to \"\n+                \"provide a `loss` argument?\"\n             )\n-                return zip(*filtered)\n-        return grads, vars\n+        return filtered\n \n     def _clip_gradients(self, grads):\n         if self.clipnorm and self.clipnorm > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32f8bc65128ef521e38e31534fdf847afa512122", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 41 | Churn Cumulative: 793 | Contributors (this commit): 8 | Commits (past 90d): 49 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,7 +11,7 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n \n DYNAMIC_SHAPES_OK = True\n-\n+DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n \n TORCH_DTYPES = {\n     \"float16\": torch.float16,\n@@ -39,20 +39,14 @@ def device_scope(device):\n         global_state.set_global_attribute(\"torch_device\", previous_device)\n \n \n-def get_default_device():\n-    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-\n-\n def get_device():\n     device = global_state.get_global_attribute(\"torch_device\", None)\n     if device is None:\n-        return get_default_device()\n+        return DEFAULT_DEVICE\n     return device\n \n \n def to_torch_dtype(dtype):\n-    if dtype in [value for key, value in TORCH_DTYPES.items()]:\n-        return dtype\n     dtype = standardize_dtype(dtype)\n     dtype = TORCH_DTYPES.get(dtype, None)\n     if dtype is None:\n@@ -114,32 +108,32 @@ class Variable(KerasVariable):\n \n \n def convert_to_tensor(x, dtype=None):\n-    dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n-    device = get_device()\n-    if isinstance(x, int):\n-        dtype = torch.int32\n-    if isinstance(x, float):\n-        dtype = torch.float32\n+    if is_tensor(x):\n+        if dtype is None:\n+            return x\n+        return x.to(to_torch_dtype(dtype))\n     if isinstance(x, Variable):\n         # TorchDynamo has bugs supporting nn.Parameter type check.\n         # Return it directly instead of pass it to the rest of the logic in the\n         # function.\n         return x.value\n-    if is_tensor(x):\n-        if dtype and dtype != x.dtype:\n-            x = x.to(dtype)\n-        return x.to(device)\n-\n+    if isinstance(x, int):\n+        return torch.as_tensor(x, dtype=torch.int32, device=get_device())\n+    if isinstance(x, float):\n+        return torch.as_tensor(x, dtype=torch.float32, device=get_device())\n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n     elif len(x) > 0 and any(isinstance(x1, torch.Tensor) for x1 in x):\n         # Handle list or tuple of torch tensors\n         return torch.stack([convert_to_tensor(x1) for x1 in x])\n-    if isinstance(x, np.ndarray) and x.dtype == np.uint32:\n+    if isinstance(x, np.ndarray):\n+        if x.dtype == np.uint32:\n             # Torch backend does not support uint32.\n             x = x.astype(np.int64)\n-    return torch.as_tensor(x, dtype=dtype, device=device)\n+        dtype = dtype or x.dtype\n+    dtype = to_torch_dtype(dtype)\n+    return torch.as_tensor(x, dtype=dtype, device=get_device())\n \n \n def convert_to_numpy(x):\n@@ -170,6 +164,9 @@ def cast(x, dtype):\n     if isinstance(x, KerasVariable):\n         x = x.value\n     if is_tensor(x):\n+        if x.dtype == dtype:\n+            return x\n+        else:\n             return x.to(dtype)\n     return convert_to_tensor(x, dtype)\n \n@@ -220,7 +217,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 )\n                 return fn(*meta_args, **meta_kwargs)\n         except:\n-            with device_scope(get_default_device()):\n+            with device_scope(DEFAULT_DEVICE):\n                 # If the `\"meta\"` device placement fails, fall back to tracing\n                 # eagerly with tensors on the default device. This will be\n                 # more robust, but more expensive.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7ca57461e823b284e542ebb0b178f10555ac43b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 42 | Churn Cumulative: 3421 | Contributors (this commit): 8 | Commits (past 90d): 38 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,20 +43,16 @@ class TorchTrainer(base_trainer.Trainer):\n \n         # Compute gradients\n         if self.trainable_weights:\n-            # Backpropagation\n-            trainable_weights = [v for v in self.trainable_weights]\n-\n             # Call torch.Tensor.backward() on the loss to compute gradients\n             # for the weights.\n             loss.backward()\n \n+            trainable_weights = self.trainable_weights[:]\n             gradients = [v.value.grad for v in trainable_weights]\n \n             # Update weights\n             with torch.no_grad():\n-                self.optimizer.apply_gradients(\n-                    zip(gradients, trainable_weights)\n-                )\n+                self.optimizer.apply(gradients, trainable_weights)\n         else:\n             warnings.warn(\"The model does not have any trainable weights.\")\n \n\n@@ -193,7 +193,6 @@ class BaseOptimizer:\n \n         `variables` can be provided on the first call to build the optimizer.\n         \"\"\"\n-        grads = list(grads)\n         if len(grads) == 0:\n             # It is possible that the grad is empty. In this case,\n             # `apply_gradients` is a no-op.\n@@ -224,16 +223,15 @@ class BaseOptimizer:\n                 self.built = True\n             self._check_variables_are_known(trainable_variables)\n \n-        grads_and_vars = list(zip(grads, self._trainable_variables))\n-\n         with ops.name_scope(self.name):\n             # Filter empty gradients.\n-            grads_and_vars = self._filter_empty_gradients(grads_and_vars)\n-            if len(list(grads_and_vars)) == 0:\n+            grads, trainable_variables = self._filter_empty_gradients(\n+                grads, trainable_variables\n+            )\n+            if len(list(grads)) == 0:\n                 return\n \n             # Apply clipping and weight decay.\n-            grads, trainable_variables = zip(*grads_and_vars)\n             grads = self._clip_gradients(grads)\n             self._apply_weight_decay(trainable_variables)\n \n@@ -363,19 +361,27 @@ class BaseOptimizer:\n             return self._learning_rate(self.iterations)\n         return self._learning_rate\n \n-    def _filter_empty_gradients(self, grads_and_vars):\n-        filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n+    def _filter_empty_gradients(self, grads, vars):\n+        for grad in grads:\n+            if grad is None:\n+                # Filtering is required.\n+                filtered = [\n+                    (g, v) for g, v in zip(grads, vars) if g is not None\n+                ]\n                 if not filtered:\n                     raise ValueError(\"No gradients provided for any variable.\")\n-        if len(filtered) < len(grads_and_vars):\n-            missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n+                if len(filtered) < len(grads):\n+                    missing_grad_vars = [\n+                        v for g, v in zip(grads, vars) if g is None\n+                    ]\n                     warnings.warn(\n                         \"Gradients do not exist for variables \"\n-                f\"{[v.name for v in missing_grad_vars]} when minimizing the \"\n-                \"loss. If you're using `model.compile()`, did you forget to \"\n-                \"provide a `loss` argument?\"\n+                        f\"{[v.name for v in missing_grad_vars]} when \"\n+                        \"minimizing the loss. If using `model.compile()`, \"\n+                        \"did you forget to provide a `loss` argument?\"\n                     )\n-        return filtered\n+                return zip(*filtered)\n+        return grads, vars\n \n     def _clip_gradients(self, grads):\n         if self.clipnorm and self.clipnorm > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
