{"custom_id": "scrapy#49952a45ae9a77bcbb5d1c340212bb28490ea002", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 433 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -211,6 +211,11 @@ class PprintItemExporter(BaseItemExporter):\n         self.file.write(pprint.pformat(itemdict) + '\\n')\n \n class PythonItemExporter(BaseItemExporter):\n+    \"\"\"The idea behind this exporter is to have a mechanism to serialize items\n+    to built-in python types so any serialization library (like\n+    json, msgpack, binc, etc) can be used on top of it. Its main goal is to\n+    seamless support what BaseItemExporter does plus nested items.\n+    \"\"\"\n \n     def serialize_field(self, field, name, value):\n         serializer = field.get('serializer', self._serialize_value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32b6364bcd1a17bfb6199af06ae613adf69e5058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 155 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,7 @@ from email.MIMEText import MIMEText\n from email.Utils import COMMASPACE, formatdate\n from email import Encoders\n \n-from twisted.internet import defer, reactor\n+from twisted.internet import defer, reactor, ssl\n from twisted.mail.smtp import ESMTPSenderFactory\n \n from scrapy import log\n@@ -19,18 +19,21 @@ from scrapy import log\n class MailSender(object):\n \n     def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n-            smtpuser=None, smtppass=None, smtpport=25, debug=False):\n+            smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n         self.smtphost = smtphost\n         self.smtpport = smtpport\n         self.smtpuser = smtpuser\n         self.smtppass = smtppass\n+        self.smtptls = smtptls\n+        self.smtpssl = smtpssl\n         self.mailfrom = mailfrom\n         self.debug = debug\n \n     @classmethod\n     def from_settings(cls, settings):\n         return cls(settings['MAIL_HOST'], settings['MAIL_FROM'], settings['MAIL_USER'],\n-            settings['MAIL_PASS'], settings.getint('MAIL_PORT'))\n+            settings['MAIL_PASS'], settings.getint('MAIL_PORT'),\n+            settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))\n \n     def send(self, to, subject, body, cc=None, attachs=(), _callback=None):\n         if attachs:\n@@ -91,7 +94,12 @@ class MailSender(object):\n         d = defer.Deferred()\n         factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n             to_addrs, msg, d, heloFallback=True, requireAuthentication=False, \\\n-            requireTransportSecurity=False)\n+            requireTransportSecurity=self.smtptls)\n         factory.noisy = False\n+\n+        if self.smtpssl:\n+            reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n+        else:\n             reactor.connectTCP(self.smtphost, self.smtpport, factory)\n+\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45ff6ec28ac44114b1c8b98e151ff3c21cd9b994", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 3 | Methods Changed: 10 | Complexity Δ (Sum/Max): 9/10 | Churn Δ: 118 | Churn Cumulative: 665 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,7 @@ from scrapy import log\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n+\n class MediaPipeline(object):\n \n     LOG_FAILED_RESULTS = True\n\n@@ -0,0 +1,108 @@\n+import mock\n+import os\n+import time\n+from tempfile import mkdtemp\n+from shutil import rmtree\n+\n+from twisted.trial import unittest\n+from twisted.internet import defer\n+\n+from scrapy.contrib.pipeline.files import FilesPipeline, FSFilesStore\n+from scrapy.item import Item, Field\n+from scrapy.http import Request, Response\n+\n+\n+def _mocked_download_func(request, info):\n+    response = request.meta.get('response')\n+    return response() if callable(response) else response\n+\n+\n+class FilesPipelineTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.pipeline = FilesPipeline(self.tempdir, download_func=_mocked_download_func)\n+        self.pipeline.open_spider(None)\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_file_path(self):\n+        image_path = self.pipeline.file_key\n+        self.assertEqual(image_path(\"https://dev.mydeco.com/mydeco.pdf\"),\n+                         'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n+        self.assertEqual(image_path(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt\"),\n+                         'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')\n+        self.assertEqual(image_path(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc\"),\n+                         'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')\n+        self.assertEqual(image_path(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\"),\n+                         'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n+        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532/\"),\n+                         'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')\n+        self.assertEqual(image_path(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n+\n+    def test_fs_store(self):\n+        assert isinstance(self.pipeline.store, FSFilesStore)\n+        self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n+\n+        key = 'some/image/key.jpg'\n+        path = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n+        self.assertEqual(self.pipeline.store._get_filesystem_path(key), path)\n+\n+    @defer.inlineCallbacks\n+    def test_file_not_expired(self):\n+        item_url = \"http://example.com/file.pdf\"\n+        item = _create_item_with_files(item_url)\n+        patchers = [\n+            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),\n+            mock.patch.object(FSFilesStore, 'stat_image', return_value={\n+                'checksum': 'abc', 'last_modified': time.time()}),\n+            mock.patch.object(FilesPipeline, 'get_media_requests',\n+                              return_value=[_prepare_request_object(item_url)])\n+        ]\n+        map(lambda p: p.start(), patchers)\n+\n+        result = yield self.pipeline.process_item(item, None)\n+        self.assertEqual(result['files'][0]['checksum'], 'abc')\n+\n+        map(lambda p: p.stop(), patchers)\n+\n+    @defer.inlineCallbacks\n+    def test_file_expired(self):\n+        item_url = \"http://example.com/file2.pdf\"\n+        item = _create_item_with_files(item_url)\n+        patchers = [\n+            mock.patch.object(FSFilesStore, 'stat_image', return_value={\n+                'checksum': 'abc',\n+                'last_modified': time.time() - (FilesPipeline.EXPIRES * 60 * 60 * 24 * 2)}),\n+            mock.patch.object(FilesPipeline, 'get_media_requests',\n+                              return_value=[_prepare_request_object(item_url)]),\n+            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True)\n+        ]\n+        map(lambda p: p.start(), patchers)\n+\n+        result = yield self.pipeline.process_item(item, None)\n+        self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n+\n+        map(lambda p: p.stop(), patchers)\n+\n+\n+class ItemWithFiles(Item):\n+    file_urls = Field()\n+    files = Field()\n+\n+\n+def _create_item_with_files(*files):\n+    item = ItemWithFiles()\n+    item['file_urls'] = files\n+    return item\n+\n+\n+def _prepare_request_object(item_url):\n+    return Request(\n+        item_url,\n+        meta={'response': Response(item_url, status=200, body='data')})\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -61,15 +61,6 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(thumbnail_name(\"/tmp/some.name/foo\", name),\n                          'thumbs/50/92dac2a6a2072c5695a5dff1f865b3cb70c657bb.jpg')\n \n-    def test_fs_store(self):\n-        from scrapy.contrib.pipeline.files import FSFilesStore\n-        assert isinstance(self.pipeline.store, FSFilesStore)\n-        self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n-\n-        key = 'some/image/key.jpg'\n-        path = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n-        self.assertEqual(self.pipeline.store._get_filesystem_path(key), path)\n-\n     def test_convert_image(self):\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a9a911fb09316079965b06b8ced6a4b688f06551", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 57 | Files Changed: 5 | Hunks: 27 | Methods Changed: 17 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 117 | Churn Cumulative: 1914 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ import inspect\n import pkg_resources\n \n import scrapy\n-from scrapy.crawler import CrawlerProcess, MultiCrawlerProcess\n+from scrapy.crawler import CrawlerProcess\n from scrapy.xlib import lsprofcalltree\n from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n@@ -138,14 +138,7 @@ def execute(argv=None, settings=None):\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n-    if cmd.multi_crawlers:\n-        process = MultiCrawlerProcess(settings)\n-        cmd.process = process\n-    else:\n-        process = CrawlerProcess(settings)\n-        process.install()\n-        cmd.set_crawler(process)\n-\n+    cmd.crawler_process = CrawlerProcess(settings)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n \n\n@@ -3,17 +3,17 @@ Base class for Scrapy commands\n \"\"\"\n \n import os\n+import warnings\n from optparse import OptionGroup\n from twisted.python import failure\n \n-from scrapy import log\n from scrapy.utils.conf import arglist_to_dict\n-from scrapy.exceptions import UsageError\n+from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n \n class ScrapyCommand(object):\n \n     requires_project = False\n-    multi_crawlers = False\n+    crawler_process = None\n \n     # default settings to be used for this command instead of global defaults\n     default_settings = {}\n@@ -29,9 +29,24 @@ class ScrapyCommand(object):\n \n     @property\n     def crawler(self):\n-        if not self.multi_crawlers and not self._crawler.configured:\n-            log.start_from_crawler(self._crawler)\n-            self._crawler.configure()\n+        warnings.warn(\"Command's default `crawler` is deprecated and will be removed. \"\n+            \"Use `create_crawler` method to instatiate crawlers.\",\n+            ScrapyDeprecationWarning)\n+\n+        if not hasattr(self, '_crawler'):\n+            crawler = self.crawler_process.create_crawler('default')\n+\n+            old_start = crawler.start\n+            self.crawler_process.started = False\n+            def wrapped_start():\n+                if self.crawler_process.started:\n+                    old_start()\n+                else:\n+                    self.crawler_process.started = True\n+                    self.crawler_process.start()\n+            crawler.start = wrapped_start\n+\n+            self.set_crawler(crawler)\n \n         return self._crawler\n \n\n@@ -20,7 +20,6 @@ def _generate(cb):\n \n class Command(ScrapyCommand):\n     requires_project = True\n-    multi_crawlers = True\n     default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n@@ -59,7 +58,7 @@ class Command(ScrapyCommand):\n                 for req in requests:\n                     contract_reqs[spider.name].append(req.callback.__name__)\n             elif requests:\n-                crawler = self.process.create_crawler(spider.name)\n+                crawler = self.crawler_process.create_crawler(spider.name)\n                 crawler.crawl(spider, requests)\n \n         # start checks\n@@ -69,7 +68,7 @@ class Command(ScrapyCommand):\n                 for method in sorted(methods):\n                     print '  * %s' % method\n         else:\n-            self.process.start()\n+            self.crawler_process.start()\n             self.results.printErrors()\n \n     def get_requests(self, spider):\n\n@@ -8,7 +8,6 @@ from scrapy.extension import ExtensionManager\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n-from scrapy.settings import overridden_settings\n from scrapy import log, signals\n \n \n@@ -35,9 +34,8 @@ class Crawler(object):\n     def configure(self):\n         if self.configured:\n             return\n+\n         self.configured = True\n-        d = dict(overridden_settings(self.settings))\n-        log.msg(format=\"Overridden settings: %(settings)r\", settings=d, level=log.DEBUG)\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n@@ -84,11 +82,15 @@ class ProcessMixin(object):\n         install_shutdown_handlers(self._signal_shutdown)\n \n     def start(self):\n+        self.start_crawling()\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n+    def start_crawling(self):\n+        raise NotImplementedError\n+\n     def stop(self):\n         raise NotImplementedError\n \n@@ -113,31 +115,12 @@ class ProcessMixin(object):\n         reactor.callFromThread(self._stop_reactor)\n \n \n-class CrawlerProcess(Crawler, ProcessMixin):\n-    \"\"\" A class to run a single Scrapy crawler in a process\n-    \"\"\"\n-\n-    def __init__(self, *a, **kw):\n-        Crawler.__init__(self, *a, **kw)\n-        ProcessMixin.__init__(self, *a, **kw)\n-        self.signals.connect(self.stop, signals.engine_stopped)\n-\n-    def start(self):\n-        Crawler.start(self)\n-        ProcessMixin.start(self)\n-\n-    def stop(self):\n-        d = Crawler.stop(self)\n-        d.addBoth(self._stop_reactor)\n-        return d\n-\n-\n-class MultiCrawlerProcess(ProcessMixin):\n+class CrawlerProcess(ProcessMixin):\n     \"\"\" A class to run multiple scrapy crawlers in a process sequentially\n     \"\"\"\n \n     def __init__(self, settings):\n-        super(MultiCrawlerProcess, self).__init__(settings)\n+        super(CrawlerProcess, self).__init__(settings)\n \n         self.settings = settings\n         self.crawlers = {}\n@@ -145,16 +128,21 @@ class MultiCrawlerProcess(ProcessMixin):\n \n     def create_crawler(self, name):\n         if name not in self.crawlers:\n-            self.crawlers[name] = Crawler(self.settings)\n+            crawler = Crawler(self.settings)\n+            crawler.configure()\n+\n+            self.crawlers[name] = crawler\n \n         return self.crawlers[name]\n \n-    def start_crawler(self):\n+    def start_crawling(self):\n         name, crawler = self.crawlers.popitem()\n \n-        crawler.sflo = log.start_from_crawler(crawler)\n-        if crawler.sflo:\n-            crawler.signals.connect(crawler.sflo.stop, signals.engine_stopped)\n+        sflo = log.start_from_crawler(crawler)\n+        crawler.install()\n+        crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n+        if sflo:\n+            crawler.signals.connect(sflo.stop, signals.engine_stopped)\n \n         crawler.signals.connect(self.check_done, signals.engine_stopped)\n         crawler.start()\n@@ -163,13 +151,13 @@ class MultiCrawlerProcess(ProcessMixin):\n \n     def check_done(self, **kwargs):\n         if self.crawlers and not self.stopping:\n-            self.start_crawler()\n+            self.start_crawling()\n         else:\n             self._stop_reactor()\n \n     def start(self):\n-        self.start_crawler()\n-        super(MultiCrawlerProcess, self).start()\n+        log.scrapy_info(self.settings)\n+        return super(CrawlerProcess, self).start()\n \n     @defer.inlineCallbacks\n     def stop(self):\n\n@@ -11,6 +11,7 @@ from twisted.python import log\n \n import scrapy\n from scrapy.utils.python import unicode_to_str\n+from scrapy.settings import overridden_settings\n \n # Logging levels\n DEBUG = logging.DEBUG\n@@ -133,17 +134,24 @@ def err(_stuff=None, _why=None, **kw):\n     kw.setdefault('system', 'scrapy')\n     log.err(_stuff, _why, **kw)\n \n-def start_from_crawler(crawler):\n-    settings = crawler.settings\n-    if not settings.getbool('LOG_ENABLED'):\n-        return\n-\n-    sflo = start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n+def start_from_settings(settings, crawler=None):\n+    if settings.getbool('LOG_ENABLED'):\n+        return start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n             settings['LOG_ENCODING'], crawler)\n \n+def scrapy_info(settings):\n+    sflo = start_from_settings(settings)\n+    if sflo:\n         msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n             settings['BOT_NAME']))\n+\n         msg(\"Optional features available: %s\" % \", \".join(scrapy.optional_features),\n             level=DEBUG)\n \n-    return sflo\n+        d = dict(overridden_settings(settings))\n+        msg(format=\"Overridden settings: %(settings)r\", settings=d, level=DEBUG)\n+\n+        sflo.stop()\n+\n+def start_from_crawler(crawler, print_headers=False):\n+    return start_from_settings(crawler.settings, crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a812def59e584512f32f7119d4735db50ab8ea78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 31 | Files Changed: 12 | Hunks: 25 | Methods Changed: 21 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 78 | Churn Cumulative: 1815 | Contributors (this commit): 10 | Commits (past 90d): 19 | Contributors (cumulative): 34 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -34,7 +34,7 @@ class ScrapyCommand(object):\n             ScrapyDeprecationWarning)\n \n         if not hasattr(self, '_crawler'):\n-            crawler = self.crawler_process.create_crawler('default')\n+            crawler = self.crawler_process.create_crawler()\n \n             old_start = crawler.start\n             self.crawler_process.started = False\n\n@@ -16,5 +16,6 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         with MockServer():\n             spider = FollowAllSpider(total=100000)\n-            self.crawler.crawl(spider)\n-            self.crawler.start()\n+            crawler = self.crawler_process.create_crawler()\n+            crawler.crawl(spider)\n+            self.crawler_process.start()\n\n@@ -43,6 +43,8 @@ class Command(ScrapyCommand):\n         elif len(args) > 1:\n             raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n         spname = args[0]\n-        spider = self.crawler.spiders.create(spname, **opts.spargs)\n-        self.crawler.crawl(spider)\n-        self.crawler.start()\n+\n+        crawler = self.crawler_process.create_crawler()\n+        spider = crawler.spiders.create(spname, **opts.spargs)\n+        crawler.crawl(spider)\n+        self.crawler_process.start()\n\n@@ -24,9 +24,11 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         if len(args) != 1:\n             raise UsageError()\n-        editor = self.crawler.settings['EDITOR']\n+\n+        crawler = self.crawler_process.create_crawler()\n+        editor = crawler.settings['EDITOR']\n         try:\n-            spider = self.crawler.spiders.create(args[0])\n+            spider = crawler.spiders.create(args[0])\n         except KeyError:\n             return self._err(\"Spider not found: %s\" % args[0])\n \n\n@@ -47,12 +47,12 @@ class Command(ScrapyCommand):\n         request = Request(args[0], callback=cb, dont_filter=True)\n         request.meta['handle_httpstatus_all'] = True\n \n+        crawler = self.crawler_process.create_crawler()\n         spider = None\n         if opts.spider:\n-            spider = self.crawler.spiders.create(opts.spider)\n+            spider = crawler.spiders.create(opts.spider)\n         else:\n-            spider = create_spider_for_request(self.crawler.spiders, request, \\\n+            spider = create_spider_for_request(crawler.spiders, request, \\\n                 default_spider=BaseSpider('default'))\n-        self.crawler.crawl(spider, [request])\n-        self.crawler.start()\n-\n+        crawler.crawl(spider, [request])\n+        self.crawler_process.start()\n\n@@ -62,7 +62,8 @@ class Command(ScrapyCommand):\n             return\n \n         try:\n-            spider = self.crawler.spiders.create(name)\n+            crawler = self.crawler_process.create_crawler()\n+            spider = crawler.spiders.create(name)\n         except KeyError:\n             pass\n         else:\n\n@@ -9,5 +9,6 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        for s in self.crawler.spiders.list():\n+        crawler = self.crawler_process.create_crawler()\n+        for s in crawler.spiders.list():\n             print s\n\n@@ -125,12 +125,12 @@ class Command(ScrapyCommand):\n     def set_spider(self, url, opts):\n         if opts.spider:\n             try:\n-                self.spider = self.crawler.spiders.create(opts.spider, **opts.spargs)\n+                self.spider = self.pcrawler.spiders.create(opts.spider, **opts.spargs)\n             except KeyError:\n                 log.msg(format='Unable to find spider: %(spider)s',\n                         level=log.ERROR, spider=opts.spider)\n         else:\n-            self.spider = create_spider_for_request(self.crawler.spiders, Request(url), **opts.spargs)\n+            self.spider = create_spider_for_request(self.pcrawler.spiders, Request(url), **opts.spargs)\n             if not self.spider:\n                 log.msg(format='Unable to find spider for: %(url)s',\n                         level=log.ERROR, url=url)\n@@ -139,8 +139,8 @@ class Command(ScrapyCommand):\n         request = Request(url, opts.callback)\n         request = self.prepare_request(request, opts)\n \n-        self.crawler.crawl(self.spider, [request])\n-        self.crawler.start()\n+        self.pcrawler.crawl(self.spider, [request])\n+        self.crawler_process.start()\n \n         if not self.first_response:\n             log.msg(format='No response downloaded for: %(request)s',\n@@ -174,7 +174,7 @@ class Command(ScrapyCommand):\n \n             items, requests = self.run_callback(response, cb)\n             if opts.pipelines:\n-                itemproc = self.crawler.engine.scraper.itemproc\n+                itemproc = self.pcrawler.engine.scraper.itemproc\n                 for item in items:\n                     itemproc.process_item(item, self.spider)\n             self.add_items(depth, items)\n@@ -207,6 +207,7 @@ class Command(ScrapyCommand):\n             url = args[0]\n \n         # prepare spider\n+        self.pcrawler = self.crawler_process.create_crawler()\n         self.set_spider(url, opts)\n \n         if self.spider and opts.depth > 0:\n\n@@ -74,5 +74,6 @@ class Command(ScrapyCommand):\n             raise UsageError(\"No spider found in file: %s\\n\" % filename)\n         spider = spclasses.pop()(**opts.spargs)\n \n-        self.crawler.crawl(spider)\n-        self.crawler.start()\n+        crawler = self.crawler_process.create_crawler()\n+        crawler.crawl(spider)\n+        self.crawler_process.start()\n\n@@ -25,7 +25,8 @@ class Command(ScrapyCommand):\n             help=\"print setting value, intepreted as an float\")\n \n     def run(self, args, opts):\n-        settings = self.crawler.settings\n+        crawler = self.crawler_process.create_crawler()\n+        settings = crawler.settings\n         if opts.get:\n             print settings.get(opts.get)\n         elif opts.getbool:\n\n@@ -8,7 +8,7 @@ from threading import Thread\n \n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n-from scrapy import log\n+\n \n class Command(ScrapyCommand):\n \n@@ -38,15 +38,17 @@ class Command(ScrapyCommand):\n         pass\n \n     def run(self, args, opts):\n+        crawler = self.crawler_process.create_crawler()\n+\n         url = args[0] if args else None\n-        spider = None\n-        if opts.spider:\n-            spider = self.crawler.spiders.create(opts.spider)\n-        shell = Shell(self.crawler, update_vars=self.update_vars, code=opts.code)\n+        spider = crawler.spiders.create(opts.spider) if opts.spider else None\n+\n+        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n         self._start_crawler_thread()\n         shell.start(url=url, spider=spider)\n \n     def _start_crawler_thread(self):\n-        t = Thread(target=self.crawler.start)\n+        self.crawler_process.print_headers()\n+        t = Thread(target=self.crawler_process.start, kwargs={'headers': False})\n         t.daemon = True\n         t.start()\n\n@@ -126,7 +126,7 @@ class CrawlerProcess(ProcessMixin):\n         self.crawlers = {}\n         self.stopping = False\n \n-    def create_crawler(self, name):\n+    def create_crawler(self, name=None):\n         if name not in self.crawlers:\n             crawler = Crawler(self.settings)\n             crawler.configure()\n@@ -155,8 +155,12 @@ class CrawlerProcess(ProcessMixin):\n         else:\n             self._stop_reactor()\n \n-    def start(self):\n+    def print_headers(self):\n         log.scrapy_info(self.settings)\n+\n+    def start(self, headers=True):\n+        if headers:\n+            self.print_headers()\n         return super(CrawlerProcess, self).start()\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96077ccf48a2a434f245db870eb4cf0b74ed278f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 962 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -53,7 +53,7 @@ class ImagesPipeline(FilesPipeline):\n                 buf.seek(0)\n                 checksum = md5sum(buf)\n             width, height = image.size\n-            self.store.persist_image(\n+            self.store.persist_file(\n                 key, buf, info,\n                 meta={'width': width, 'height': height},\n                 headers={'Content-Type': 'image/jpeg'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#902208ca58aa99bacea57488642e0ea4129bd180", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 630 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 0.8\n\nDIFF:\n@@ -26,6 +26,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         crawler = self.crawler_process.create_crawler()\n+        crawler.configure()\n         settings = crawler.settings\n         if opts.get:\n             print settings.get(opts.get)\n\n@@ -19,6 +19,9 @@ class Crawler(object):\n         self.signals = SignalManager(self)\n         self.stats = load_object(settings['STATS_CLASS'])(self)\n \n+        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n+        self.spiders = spman_cls.from_crawler(self)\n+\n         self.scheduled = {}\n \n     def install(self):\n@@ -39,20 +42,25 @@ class Crawler(object):\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n-        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n-        self.spiders = spman_cls.from_crawler(self)\n         self.engine = ExecutionEngine(self, self._spider_closed)\n \n     def crawl(self, spider, requests=None):\n         spider.set_crawler(self)\n-        if requests is None:\n-            requests = spider.start_requests()\n \n         if self.configured and self.engine.running:\n             assert not self.scheduled\n-            return self.engine.open_spider(spider, requests)\n+            return self.schedule(spider, requests)\n         else:\n-            self.scheduled.setdefault(spider, []).extend(requests)\n+            self.scheduled.setdefault(spider, []).append(requests)\n+\n+    def schedule(self, spider, batches=[]):\n+        requests = []\n+        for batch in batches:\n+            if batch is None:\n+                batch = spider.start_requests()\n+            requests.extend(batch)\n+\n+        return self.engine.open_spider(spider, requests)\n \n     def _spider_closed(self, spider=None):\n         if not self.engine.open_spiders:\n@@ -62,8 +70,8 @@ class Crawler(object):\n     def start(self):\n         yield defer.maybeDeferred(self.configure)\n \n-        for spider, requests in self.scheduled.iteritems():\n-            yield self.engine.open_spider(spider, requests)\n+        for spider, batches in self.scheduled.iteritems():\n+            yield self.schedule(spider, batches)\n \n         yield defer.maybeDeferred(self.engine.start)\n \n@@ -128,10 +136,7 @@ class CrawlerProcess(ProcessMixin):\n \n     def create_crawler(self, name=None):\n         if name not in self.crawlers:\n-            crawler = Crawler(self.settings)\n-            crawler.configure()\n-\n-            self.crawlers[name] = crawler\n+            self.crawlers[name] = Crawler(self.settings)\n \n         return self.crawlers[name]\n \n@@ -139,6 +144,7 @@ class CrawlerProcess(ProcessMixin):\n         name, crawler = self.crawlers.popitem()\n \n         sflo = log.start_from_crawler(crawler)\n+        crawler.configure()\n         crawler.install()\n         crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n         if sflo:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7bdeabc0ed00cfaa27287a9ab84b42f893bf0881", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 11 | Methods Changed: 12 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 28 | Churn Cumulative: 720 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,12 +43,13 @@ class Command(ScrapyCommand):\n         url = args[0] if args else None\n         spider = crawler.spiders.create(opts.spider) if opts.spider else None\n \n-        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n+        self.crawler_process.start_crawling()\n         self._start_crawler_thread()\n+\n+        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n         shell.start(url=url, spider=spider)\n \n     def _start_crawler_thread(self):\n-        self.crawler_process.print_headers()\n-        t = Thread(target=self.crawler_process.start, kwargs={'headers': False})\n+        t = Thread(target=self.crawler_process.start_reactor)\n         t.daemon = True\n         t.start()\n\n@@ -91,6 +91,9 @@ class ProcessMixin(object):\n \n     def start(self):\n         self.start_crawling()\n+        self.start_reactor()\n+\n+    def start_reactor(self):\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n@@ -102,7 +105,7 @@ class ProcessMixin(object):\n     def stop(self):\n         raise NotImplementedError\n \n-    def _stop_reactor(self, _=None):\n+    def stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n@@ -120,7 +123,7 @@ class ProcessMixin(object):\n         signame = signal_names[signum]\n         log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n                 level=log.INFO, signame=signame)\n-        reactor.callFromThread(self._stop_reactor)\n+        reactor.callFromThread(self.stop_reactor)\n \n \n class CrawlerProcess(ProcessMixin):\n@@ -140,7 +143,7 @@ class CrawlerProcess(ProcessMixin):\n \n         return self.crawlers[name]\n \n-    def start_crawling(self):\n+    def start_crawler(self):\n         name, crawler = self.crawlers.popitem()\n \n         sflo = log.start_from_crawler(crawler)\n@@ -157,17 +160,13 @@ class CrawlerProcess(ProcessMixin):\n \n     def check_done(self, **kwargs):\n         if self.crawlers and not self.stopping:\n-            self.start_crawling()\n+            self.start_crawler()\n         else:\n-            self._stop_reactor()\n+            self.stop_reactor()\n \n-    def print_headers(self):\n+    def start_crawling(self):\n         log.scrapy_info(self.settings)\n-\n-    def start(self, headers=True):\n-        if headers:\n-            self.print_headers()\n-        return super(CrawlerProcess, self).start()\n+        self.start_crawler()\n \n     @defer.inlineCallbacks\n     def stop(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c092536c5db8d5bf14a1aa366ba45c860eeec5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 478 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -153,5 +153,5 @@ def scrapy_info(settings):\n \n         sflo.stop()\n \n-def start_from_crawler(crawler, print_headers=False):\n+def start_from_crawler(crawler):\n     return start_from_settings(crawler.settings, crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#071172cbd615c7a48733ab6423cbca92b5ff8d08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 641 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookup\n                                    ConnectionRefusedError, ConnectionDone, ConnectError, \\\n                                    ConnectionLost, TCPTimedOutError\n from twisted.internet.defer import TimeoutError as UserTimeoutError\n+from scrapy.xlib.tx._newclient import ResponseFailed\n \n from scrapy import log\n from scrapy.exceptions import NotConfigured\n@@ -33,7 +34,7 @@ class RetryMiddleware(object):\n     # decompress an empty response\n     EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n-                           ConnectionLost, TCPTimedOutError,\n+                           ConnectionLost, TCPTimedOutError, ResponseFailed,\n                            IOError)\n \n     def __init__(self, settings):\n\n@@ -102,7 +102,7 @@ class Drop(Partial):\n \n     def _delayedRender(self, request):\n         request.write(\"this connection will be dropped\\n\")\n-        request.channel.transport.loseConnection()\n+        request.channel.transport.abortConnection()\n         request.finish()\n \n \n\n@@ -90,6 +90,12 @@ class CrawlTestCase(TestCase):\n         yield docrawl(spider)\n         self._assert_retried()\n \n+    @defer.inlineCallbacks\n+    def test_retry_dropped_connection(self):\n+        spider = SimpleSpider(\"http://localhost:8998/drop\")\n+        yield docrawl(spider)\n+        self._assert_retried()\n+\n     def _assert_retried(self):\n         log = get_testlog()\n         self.assertEqual(log.count(\"Retrying\"), 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c64a989ca1da3a666155966f2a499fcc5d27397", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 18 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 45 | Churn Cumulative: 1515 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,15 +18,16 @@ About HTTP errors to consider:\n   indicate server overload, which would be something we want to retry\n \"\"\"\n \n-from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookupError, \\\n-                                   ConnectionRefusedError, ConnectionDone, ConnectError, \\\n-                                   ConnectionLost, TCPTimedOutError\n from twisted.internet.defer import TimeoutError as UserTimeoutError\n-from scrapy.xlib.tx._newclient import ResponseFailed\n+from twisted.internet.error import TimeoutError as ServerTimeoutError, \\\n+        DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, \\\n+        ConnectionLost, TCPTimedOutError\n \n from scrapy import log\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n+from scrapy.xlib.tx import ResponseFailed\n+\n \n class RetryMiddleware(object):\n \n\n@@ -7,17 +7,16 @@ from urlparse import urldefrag\n from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n from twisted.web.http_headers import Headers as TxHeaders\n-from twisted.web.http import PotentialDataLoss\n from twisted.web.iweb import IBodyProducer\n+from twisted.web.http import PotentialDataLoss\n from twisted.internet.error import TimeoutError\n from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \\\n-        ResponseFailed, HTTPConnectionPool, TCP4ClientEndpoint\n+    HTTPConnectionPool, TCP4ClientEndpoint, ResponseFailed\n \n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n-from scrapy import log\n \n \n class HTTP11DownloadHandler(object):\n@@ -145,10 +144,9 @@ class _ResponseReader(protocol.Protocol):\n     def connectionLost(self, reason):\n         if self._finished.called:\n             return\n+\n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n             self._finished.callback((self._txresponse, body, None))\n-        elif reason.check(PotentialDataLoss, ResponseFailed):\n-            self._finished.callback((self._txresponse, body, ['partial']))\n         else:\n             self._finished.errback(reason)\n\n@@ -101,8 +101,12 @@ class Partial(DeferMixin, Resource):\n class Drop(Partial):\n \n     def _delayedRender(self, request):\n+        abort = getarg(request, \"abort\", 0, type=int)\n         request.write(\"this connection will be dropped\\n\")\n+        if abort:\n             request.channel.transport.abortConnection()\n+        else:\n+            request.channel.transport.loseConnection()\n             request.finish()\n \n \n\n@@ -91,8 +91,16 @@ class CrawlTestCase(TestCase):\n         self._assert_retried()\n \n     @defer.inlineCallbacks\n-    def test_retry_dropped_connection(self):\n-        spider = SimpleSpider(\"http://localhost:8998/drop\")\n+    def test_retry_conn_lost(self):\n+        # connection lost after receiving data\n+        spider = SimpleSpider(\"http://localhost:8998/drop?abort=0\")\n+        yield docrawl(spider)\n+        self._assert_retried()\n+\n+    @defer.inlineCallbacks\n+    def test_retry_conn_aborted(self):\n+        # connection lost before receiving data\n+        spider = SimpleSpider(\"http://localhost:8998/drop?abort=1\")\n         yield docrawl(spider)\n         self._assert_retried()\n \n\n@@ -1,14 +1,16 @@\n import unittest\n \n-from twisted.internet.error import TimeoutError as ServerTimeoutError, DNSLookupError, \\\n-                                   ConnectionRefusedError, ConnectionDone, ConnectError, \\\n+from twisted.internet.error import TimeoutError as ServerTimeoutError, \\\n+    DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, \\\n     ConnectionLost\n \n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n+from scrapy.xlib.tx import ResponseFailed\n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n from scrapy.utils.test import get_crawler\n \n+\n class RetryTest(unittest.TestCase):\n     def setUp(self):\n         crawler = get_crawler()\n@@ -62,9 +64,11 @@ class RetryTest(unittest.TestCase):\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_twistederrors(self):\n-        for exc in (ServerTimeoutError, DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, ConnectionLost):\n+        for exc in (ServerTimeoutError, DNSLookupError, ConnectionRefusedError,\n+                    ConnectionDone, ConnectError, ConnectionLost,\n+                    ResponseFailed):\n             req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n-            self._test_retry_exception(req, exc())\n+            self._test_retry_exception(req, exc('foo'))\n \n     def _test_retry_exception(self, req, exception):\n         # first retry\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#efb4f32d802589bd8540452da197704b0dead8a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 91 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,9 @@\n import unittest\n-\n from twisted.internet.error import TimeoutError as ServerTimeoutError, \\\n     DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, \\\n     ConnectionLost\n \n+from scrapy import optional_features\n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n from scrapy.xlib.tx import ResponseFailed\n from scrapy.spider import BaseSpider\n@@ -64,9 +64,13 @@ class RetryTest(unittest.TestCase):\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_twistederrors(self):\n-        for exc in (ServerTimeoutError, DNSLookupError, ConnectionRefusedError,\n-                    ConnectionDone, ConnectError, ConnectionLost,\n-                    ResponseFailed):\n+        exceptions = [ServerTimeoutError, DNSLookupError,\n+                      ConnectionRefusedError, ConnectionDone, ConnectError,\n+                      ConnectionLost]\n+        if 'http11' in optional_features:\n+            exceptions.append(ResponseFailed)\n+\n+        for exc in exceptions:\n             req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n             self._test_retry_exception(req, exc('foo'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c89d8b2984667574dd42b5aa5b74625c06f9f17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 282 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -36,18 +36,18 @@ class FSFilesStore(object):\n     def persist_file(self, key, buf, info, meta=None, headers=None):\n         absolute_path = self._get_filesystem_path(key)\n         self._mkdir(os.path.dirname(absolute_path), info)\n-        with open(absolute_path, 'w') as f:\n+        with open(absolute_path, 'wb') as f:\n             f.write(buf.getvalue())\n \n-    def stat_image(self, key, info):\n+    def stat_file(self, key, info):\n         absolute_path = self._get_filesystem_path(key)\n         try:\n             last_modified = os.path.getmtime(absolute_path)\n         except:  # FIXME: catching everything!\n             return {}\n \n-        with open(absolute_path, 'rb') as imagefile:\n-            checksum = md5sum(imagefile)\n+        with open(absolute_path, 'rb') as f:\n+            checksum = md5sum(f)\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n@@ -77,7 +77,7 @@ class S3FilesStore(object):\n         assert uri.startswith('s3://')\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n-    def stat_image(self, key, info):\n+    def stat_file(self, key, info):\n         def _onsuccess(boto_key):\n             checksum = boto_key.etag.strip('\"')\n             last_modified = boto_key.last_modified\n@@ -190,9 +190,9 @@ class FilesPipeline(MediaPipeline):\n             return {'url': request.url, 'path': key, 'checksum': checksum}\n \n         key = self.file_key(request.url)\n-        dfd = defer.maybeDeferred(self.store.stat_image, key, info)\n+        dfd = defer.maybeDeferred(self.store.stat_file, key, info)\n         dfd.addCallbacks(_onsuccess, lambda _: None)\n-        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_image')\n+        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_file')\n         return dfd\n \n     def media_failed(self, failure, request, info):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02d08722e75e06c2b91246dfa9ac22c8e9b01601", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 699 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -246,7 +246,7 @@ class FilesPipeline(MediaPipeline):\n         spider.crawler.stats.inc_value('file_count', spider=spider)\n         spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)\n \n-    ### Overradiable Interface\n+    ### Overridable Interface\n     def get_media_requests(self, item, info):\n         return [Request(x) for x in item.get('file_urls', [])]\n \n\n@@ -96,7 +96,7 @@ class MediaPipeline(object):\n         for wad in info.waiting.pop(fp):\n             defer_result(result).chainDeferred(wad)\n \n-    ### Overradiable Interface\n+    ### Overridable Interface\n     def media_to_download(self, request, info):\n         \"\"\"Check request before starting download\"\"\"\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
