{"custom_id": "keras#f130fa1bca288cb566b083620ba70cb89e2e5fdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 38 | Files Changed: 4 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 48 | Churn Cumulative: 1866 | Contributors (this commit): 24 | Commits (past 90d): 13 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -128,21 +128,3 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     # for further checks in the caller function\n     return actual_output\n-\n-\n-def keras_test(func):\n-    \"\"\"Function wrapper to clean up after TensorFlow tests.\n-\n-    # Arguments\n-        func: test function to clean up after.\n-\n-    # Returns\n-        A function wrapping the input function.\n-    \"\"\"\n-    @six.wraps(func)\n-    def wrapper(*args, **kwargs):\n-        output = func(*args, **kwargs)\n-        if K.backend() == 'tensorflow' or K.backend() == 'cntk':\n-            K.clear_session()\n-        return output\n-    return wrapper\n\n@@ -4,6 +4,10 @@ from keras import backend as K\n \r\n @pytest.fixture(autouse=True)\r\n def clear_session_after_test():\r\n+    \"\"\"Test wrapper to clean up after TensorFlow and CNTK tests.\r\n+\r\n+    This wrapper runs for all the tests in the keras test suite.\r\n+    \"\"\"\r\n     yield\r\n     if K.backend() == 'tensorflow' or K.backend() == 'cntk':\r\n         K.clear_session()\r\n\n@@ -6,7 +6,6 @@ from keras import backend as K\n from keras.models import Sequential, Model\n from keras.layers import convolutional_recurrent, Input\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras import regularizers\n \n num_row = 3\n@@ -19,7 +18,6 @@ input_num_col = 5\n sequence_len = 2\n \n \n-@keras_test\n def test_convolutional_recurrent():\n \n     for data_format in ['channels_first', 'channels_last']:\n@@ -64,7 +62,6 @@ def test_convolutional_recurrent():\n                                 input_shape=inputs.shape)\n \n \n-@keras_test\n def test_convolutional_recurrent_statefulness():\n \n     data_format = 'channels_last'\n\n@@ -4,7 +4,6 @@ from numpy.testing import assert_allclose\n \n import keras\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras.layers import recurrent\n from keras.layers import embeddings\n from keras.models import Sequential\n@@ -18,26 +17,16 @@ num_samples, timesteps, embedding_dim, units = 2, 5, 4, 3\n embedding_num = 12\n \n \n-def rnn_test(f):\n-    \"\"\"\n-    All the recurrent layers share the same interface,\n-    so we can run through them with a single function.\n-    \"\"\"\n-    f = keras_test(f)\n-    return pytest.mark.parametrize('layer_class', [\n-        recurrent.SimpleRNN,\n+rnn_test = pytest.mark.parametrize('layer_class',\n+                                   [recurrent.SimpleRNN,\n                                     recurrent.GRU,\n-        recurrent.LSTM\n-    ])(f)\n+                                    recurrent.LSTM])\n \n \n-def rnn_cell_test(f):\n-    f = keras_test(f)\n-    return pytest.mark.parametrize('cell_class', [\n-        recurrent.SimpleRNNCell,\n+rnn_cell_test = pytest.mark.parametrize('cell_class',\n+                                        [recurrent.SimpleRNNCell,\n                                          recurrent.GRUCell,\n-        recurrent.LSTMCell\n-    ])(f)\n+                                         recurrent.LSTMCell])\n \n \n @rnn_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#adfec1e8440b3af64b642ea75bdb21bbee70cc80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 85 | Files Changed: 1 | Hunks: 16 | Methods Changed: 11 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 121 | Churn Cumulative: 2020 | Contributors (this commit): 31 | Commits (past 90d): 8 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,6 +17,8 @@ from keras.layers.pooling import MaxPooling2D\n from keras.layers.pooling import GlobalAveragePooling1D\n from keras.layers.pooling import GlobalAveragePooling2D\n from keras.utils.test_utils import get_test_data\n+from keras.utils.generic_utils import to_list\n+from keras.utils.generic_utils import unpack_singleton\n from keras import backend as K\n from keras.utils import np_utils\n try:\n@@ -33,6 +35,22 @@ train_samples = 20\n test_samples = 20\n \n \n+def data_generator(x, y, batch_size):\n+    x = to_list(x)\n+    y = to_list(y)\n+    max_batch_index = len(x[0]) // batch_size\n+    i = 0\n+    while 1:\n+        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]\n+        x_batch = unpack_singleton(x_batch)\n+\n+        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]\n+        y_batch = unpack_singleton(y_batch)\n+        yield x_batch, y_batch\n+        i += 1\n+        i = i % max_batch_index\n+\n+\n # Changing the default arguments of get_test_data.\n def get_data_callbacks(num_train=train_samples,\n                        num_test=test_samples,\n@@ -69,16 +87,7 @@ def test_TerminateOnNaN():\n     assert len(loss) == 1\n     assert loss[0] == np.inf\n \n-    # case 2 fit_generator\n-    def data_generator():\n-        max_batch_index = len(X_train) // batch_size\n-        i = 0\n-        while 1:\n-            yield (X_train[i * batch_size: (i + 1) * batch_size],\n-                   y_train[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            i = i % max_batch_index\n-    history = model.fit_generator(data_generator(),\n+    history = model.fit_generator(data_generator(X_train, y_train, batch_size),\n                                   len(X_train),\n                                   validation_data=(X_test, y_test),\n                                   callbacks=cbks,\n@@ -549,23 +558,6 @@ def test_TensorBoard(tmpdir):\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n-    def data_generator(train):\n-        if train:\n-            max_batch_index = len(X_train) // batch_size\n-        else:\n-            max_batch_index = len(X_test) // batch_size\n-        i = 0\n-        while 1:\n-            if train:\n-                # simulate multi-input/output models\n-                yield (X_train[i * batch_size: (i + 1) * batch_size],\n-                       y_train[i * batch_size: (i + 1) * batch_size])\n-            else:\n-                yield (X_test[i * batch_size: (i + 1) * batch_size],\n-                       y_test[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            i = i % max_batch_index\n-\n     class DummyStatefulMetric(Layer):\n \n         def __init__(self, name='dummy_stateful_metric', **kwargs):\n@@ -609,12 +601,14 @@ def test_TensorBoard(tmpdir):\n               callbacks=callbacks_factory(histogram_freq=0), epochs=2)\n \n     # fit generator without validation data\n-    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n                         callbacks=callbacks_factory(histogram_freq=0,\n                                                     embeddings_freq=0))\n \n     # fit generator with validation data and accuracy\n-    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n                         validation_data=(X_test, y_test),\n                         callbacks=callbacks_factory(histogram_freq=1))\n \n@@ -633,23 +627,6 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n-    def data_generator(train):\n-        if train:\n-            max_batch_index = len(X_train) // batch_size\n-        else:\n-            max_batch_index = len(X_test) // batch_size\n-        i = 0\n-        while 1:\n-            if train:\n-                # simulate multi-input/output models\n-                yield (X_train[i * batch_size: (i + 1) * batch_size],\n-                       y_train[i * batch_size: (i + 1) * batch_size])\n-            else:\n-                yield (X_test[i * batch_size: (i + 1) * batch_size],\n-                       y_test[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            i = i % max_batch_index\n-\n     inp = Input((input_dim,))\n     hidden = Dense(num_hidden, activation='relu')(inp)\n     hidden = Dropout(0.1)(hidden)\n@@ -675,18 +652,22 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n                   callbacks=callbacks_factory(histogram_freq=1), epochs=3)\n     assert 'validation_data must be provided' in str(raised_exception.value)\n \n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    validation_generator = data_generator(X_test, y_test, batch_size)\n+\n     # fit generator without validation data should raise ValueError if\n     # histogram_freq > 0\n     with pytest.raises(ValueError) as raised_exception:\n-        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+        model.fit_generator(train_generator,\n+                            len(X_train), epochs=2,\n                             callbacks=callbacks_factory(histogram_freq=1))\n     assert 'validation_data must be provided' in str(raised_exception.value)\n \n     # fit generator with validation data generator should raise ValueError if\n     # histogram_freq > 0\n     with pytest.raises(ValueError) as raised_exception:\n-        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n-                            validation_data=data_generator(False),\n+        model.fit_generator(train_generator, len(X_train), epochs=2,\n+                            validation_data=validation_generator,\n                             validation_steps=1,\n                             callbacks=callbacks_factory(histogram_freq=1))\n     assert 'validation_data must be provided' in str(raised_exception.value)\n@@ -702,23 +683,6 @@ def test_TensorBoard_multi_input_output(tmpdir):\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n-    def data_generator(train):\n-        if train:\n-            max_batch_index = len(X_train) // batch_size\n-        else:\n-            max_batch_index = len(X_test) // batch_size\n-        i = 0\n-        while 1:\n-            if train:\n-                # simulate multi-input/output models\n-                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n-                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n-            else:\n-                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n-                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n-            i += 1\n-            i = i % max_batch_index\n-\n     inp1 = Input((input_dim, input_dim))\n     inp2 = Input((input_dim, input_dim))\n     inp_3d = add([inp1, inp2])\n@@ -755,13 +719,15 @@ def test_TensorBoard_multi_input_output(tmpdir):\n               validation_data=([X_test] * 2, [y_test] * 2),\n               callbacks=callbacks_factory(histogram_freq=1), epochs=2)\n \n+    train_generator = data_generator([X_train] * 2, [y_train] * 2, batch_size)\n+\n     # fit generator without validation data\n-    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n                         callbacks=callbacks_factory(histogram_freq=0,\n                                                     embeddings_freq=0))\n \n     # fit generator with validation data and accuracy\n-    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n+    model.fit_generator(train_generator, len(X_train), epochs=2,\n                         validation_data=([X_test] * 2, [y_test] * 2),\n                         callbacks=callbacks_factory(histogram_freq=1))\n \n@@ -856,24 +822,9 @@ def test_CallbackValData():\n     model.fit(X_train, y_train, batch_size=batch_size,\n               validation_data=(X_test, y_test), callbacks=[cbk], epochs=1)\n \n-    def data_generator(train):\n-        if train:\n-            max_batch_index = len(X_train) // batch_size\n-        else:\n-            max_batch_index = len(X_test) // batch_size\n-        i = 0\n-        while 1:\n-            if train:\n-                yield (X_train[i * batch_size: (i + 1) * batch_size],\n-                       y_train[i * batch_size: (i + 1) * batch_size])\n-            else:\n-                yield (X_test[i * batch_size: (i + 1) * batch_size],\n-                       y_test[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            i = i % max_batch_index\n-\n     cbk2 = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n-    model.fit_generator(data_generator(True), len(X_train), epochs=1,\n+    train_generator = data_generator(X_train, y_train, batch_size)\n+    model.fit_generator(train_generator, len(X_train), epochs=1,\n                         validation_data=(X_test, y_test),\n                         callbacks=[cbk2])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98465b85d020f1326bcef7632f1261a9a7a84e92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 50 | Files Changed: 7 | Hunks: 40 | Methods Changed: 17 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 129 | Churn Cumulative: 21521 | Contributors (this commit): 177 | Commits (past 90d): 18 | Contributors (cumulative): 244 | DMM Complexity: 0.25\n\nDIFF:\n@@ -552,7 +552,8 @@ class EarlyStopping(Callback):\n                 self.model.stop_training = True\n                 if self.restore_best_weights:\n                     if self.verbose > 0:\n-                        print(\"Restoring model weights from the end of the best epoch\")\n+                        print('Restoring model weights from the end of '\n+                              'the best epoch')\n                     self.model.set_weights(self.best_weights)\n \n     def on_train_end(self, logs=None):\n@@ -577,16 +578,18 @@ class RemoteMonitor(Callback):\n     Events are sent to `root + '/publish/epoch/end/'` by default. Calls are\n     HTTP POST, with a `data` argument which is a\n     JSON-encoded dictionary of event data.\n-    If send_as_json is set to True, the content type of the request will be application/json.\n-    Otherwise the serialized JSON will be send within a form\n+    If send_as_json is set to True, the content type of the request will be\n+    application/json. Otherwise the serialized JSON will be send within a form\n \n     # Arguments\n         root: String; root url of the target server.\n         path: String; path relative to `root` to which the events will be sent.\n-        field: String; JSON field under which the data will be stored. The field is used only if the payload is sent\n-        within a form (i.e. send_as_json is set to False).\n+        field: String; JSON field under which the data will be stored.\n+            The field is used only if the payload is sent within a form\n+            (i.e. send_as_json is set to False).\n         headers: Dictionary; optional custom HTTP headers.\n-        send_as_json: Boolean; whether the request should be send as application/json.\n+        send_as_json: Boolean; whether the request should be send as\n+            application/json.\n     \"\"\"\n \n     def __init__(self,\n@@ -714,7 +717,8 @@ class TensorBoard(Callback):\n         embeddings_data: data to be embedded at layers specified in\n             `embeddings_layer_names`. Numpy array (if the model has a single\n             input) or list of Numpy arrays (if the model has multiple inputs).\n-            Learn [more about embeddings](https://www.tensorflow.org/programmers_guide/embedding)\n+            Learn [more about embeddings]\n+            (https://www.tensorflow.org/programmers_guide/embedding).\n     \"\"\"\n \n     def __init__(self, log_dir='./logs',\n@@ -733,7 +737,8 @@ class TensorBoard(Callback):\n             import tensorflow as tf\n             from tensorflow.contrib.tensorboard.plugins import projector\n         except ImportError:\n-            raise ImportError('You need the TensorFlow module installed to use TensorBoard.')\n+            raise ImportError('You need the TensorFlow module installed to '\n+                              'use TensorBoard.')\n \n         if K.backend() != 'tensorflow':\n             if histogram_freq != 0:\n@@ -784,7 +789,8 @@ class TensorBoard(Callback):\n                         grads = [\n                             grad.values if is_indexed_slices(grad) else grad\n                             for grad in grads]\n-                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n+                        tf.summary.histogram('{}_grad'.format(mapped_weight_name),\n+                                             grads)\n                     if self.write_images:\n                         w_img = tf.squeeze(weight)\n                         shape = K.int_shape(w_img)\n@@ -822,7 +828,8 @@ class TensorBoard(Callback):\n                 if hasattr(layer, 'output'):\n                     if isinstance(layer.output, list):\n                         for i, output in enumerate(layer.output):\n-                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n+                            tf.summary.histogram('{}_out_{}'.format(layer.name, i),\n+                                                 output)\n                     else:\n                         tf.summary.histogram('{}_out'.format(layer.name),\n                                              layer.output)\n@@ -835,7 +842,8 @@ class TensorBoard(Callback):\n             self.writer = tf.summary.FileWriter(self.log_dir)\n \n         if self.embeddings_freq and self.embeddings_data is not None:\n-            self.embeddings_data = standardize_input_data(self.embeddings_data, model.input_names)\n+            self.embeddings_data = standardize_input_data(self.embeddings_data,\n+                                                          model.input_names)\n \n             embeddings_layer_names = self.embeddings_layer_names\n \n@@ -942,8 +950,8 @@ class TensorBoard(Callback):\n                     batch = slice(i, i + step)\n \n                     if type(self.model.input) == list:\n-                        feed_dict = {model_input: embeddings_data[idx][batch]\n-                                     for idx, model_input in enumerate(self.model.input)}\n+                        feed_dict = {_input: embeddings_data[idx][batch]\n+                                     for idx, _input in enumerate(self.model.input)}\n                     else:\n                         feed_dict = {self.model.input: embeddings_data[0][batch]}\n \n@@ -954,7 +962,8 @@ class TensorBoard(Callback):\n \n                     self.sess.run(self.assign_embeddings, feed_dict=feed_dict)\n                     self.saver.save(self.sess,\n-                                    os.path.join(self.log_dir, 'keras_embedding.ckpt'),\n+                                    os.path.join(self.log_dir,\n+                                                 'keras_embedding.ckpt'),\n                                     epoch)\n \n                     i += self.batch_size\n@@ -1088,8 +1097,8 @@ class ReduceLROnPlateau(Callback):\n                         new_lr = max(new_lr, self.min_lr)\n                         K.set_value(self.model.optimizer.lr, new_lr)\n                         if self.verbose > 0:\n-                            print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n-                                  'rate to %s.' % (epoch + 1, new_lr))\n+                            print('\\nEpoch %05d: ReduceLROnPlateau reducing '\n+                                  'learning rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n \n@@ -1161,7 +1170,7 @@ class CSVLogger(Callback):\n \n         if self.model.stop_training:\n             # We set NA so that csv parsers do not fail for this last epoch.\n-            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n+            logs = dict([(k, logs[k] if k in logs else 'NA') for k in self.keys])\n \n         if not self.writer:\n             class CustomDialect(csv.excel):\n\n@@ -40,7 +40,8 @@ class MaxNorm(Constraint):\n             `(rows, cols, input_depth)`.\n \n     # References\n-        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n+          (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n \n     def __init__(self, max_value=2, axis=0):\n\n@@ -86,8 +86,8 @@ def generate_legacy_interface(allowed_positional_args=None,\n                     if i < len(kwargs) - 1:\n                         signature += ', '\n                 signature += ')`'\n-                warnings.warn('Update your `' + object_name +\n-                              '` call to the Keras 2 API: ' + signature, stacklevel=2)\n+                warnings.warn('Update your `' + object_name + '` call to the ' +\n+                              'Keras 2 API: ' + signature, stacklevel=2)\n             return func(*args, **kwargs)\n         wrapper._original_function = func\n         return wrapper\n@@ -127,7 +127,8 @@ def embedding_kwargs_preprocessor(args, kwargs):\n         kwargs.pop('dropout')\n         warnings.warn('The `dropout` argument is no longer support in `Embedding`. '\n                       'You can apply a `keras.layers.SpatialDropout1D` layer '\n-                      'right after the `Embedding` layer to get the same behavior.', stacklevel=3)\n+                      'right after the `Embedding` layer to get the same behavior.',\n+                      stacklevel=3)\n     return args, kwargs, converted\n \n legacy_embedding_support = generate_legacy_interface(\n@@ -378,7 +379,7 @@ def conv3d_args_preprocessor(args, kwargs):\n     if len(args) > 5:\n         raise TypeError('Layer can receive at most 4 positional arguments.')\n     if len(args) == 5:\n-        if isinstance(args[2], int) and isinstance(args[3], int) and isinstance(args[4], int):\n+        if all([isinstance(x, int) for x in args[2:5]]):\n             kernel_size = (args[2], args[3], args[4])\n             args = [args[0], args[1], kernel_size]\n             converted.append(('kernel_size', 'kernel_dim*'))\n@@ -398,21 +399,21 @@ def conv3d_args_preprocessor(args, kwargs):\n             args = [args[0], args[1], kernel_size]\n             converted.append(('kernel_size', 'kernel_dim*'))\n     elif len(args) == 3:\n-        if 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n+        if all([x in kwargs for x in ['kernel_dim2', 'kernel_dim3']]):\n             kernel_size = (args[2],\n                            kwargs.pop('kernel_dim2'),\n                            kwargs.pop('kernel_dim3'))\n             args = [args[0], args[1], kernel_size]\n             converted.append(('kernel_size', 'kernel_dim*'))\n     elif len(args) == 2:\n-        if 'kernel_dim1' in kwargs and 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n+        if all([x in kwargs for x in ['kernel_dim1', 'kernel_dim2', 'kernel_dim3']]):\n             kernel_size = (kwargs.pop('kernel_dim1'),\n                            kwargs.pop('kernel_dim2'),\n                            kwargs.pop('kernel_dim3'))\n             args = [args[0], args[1], kernel_size]\n             converted.append(('kernel_size', 'kernel_dim*'))\n     elif len(args) == 1:\n-        if 'kernel_dim1' in kwargs and 'kernel_dim2' in kwargs and 'kernel_dim3' in kwargs:\n+        if all([x in kwargs for x in ['kernel_dim1', 'kernel_dim2', 'kernel_dim3']]):\n             kernel_size = (kwargs.pop('kernel_dim1'),\n                            kwargs.pop('kernel_dim2'),\n                            kwargs.pop('kernel_dim3'))\n@@ -507,7 +508,8 @@ def zeropadding2d_args_preprocessor(args, kwargs):\n             kwargs['padding'] = ((top_pad, bottom_pad), (left_pad, right_pad))\n             warnings.warn('The `padding` argument in the Keras 2 API no longer'\n                           'accepts dict types. You can now input argument as: '\n-                          '`padding=(top_pad, bottom_pad, left_pad, right_pad)`.', stacklevel=3)\n+                          '`padding=(top_pad, bottom_pad, left_pad, right_pad)`.',\n+                          stacklevel=3)\n     elif len(args) == 2 and isinstance(args[1], dict):\n         if set(args[1].keys()) <= {'top_pad', 'bottom_pad',\n                                    'left_pad', 'right_pad'}:\n@@ -518,7 +520,8 @@ def zeropadding2d_args_preprocessor(args, kwargs):\n             args = (args[0], ((top_pad, bottom_pad), (left_pad, right_pad)))\n             warnings.warn('The `padding` argument in the Keras 2 API no longer'\n                           'accepts dict types. You can now input argument as: '\n-                          '`padding=((top_pad, bottom_pad), (left_pad, right_pad))`', stacklevel=3)\n+                          '`padding=((top_pad, bottom_pad), (left_pad, right_pad))`',\n+                          stacklevel=3)\n     return args, kwargs, converted\n \n legacy_zeropadding2d_support = generate_legacy_interface(\n@@ -583,7 +586,8 @@ def generator_methods_args_preprocessor(args, kwargs):\n                 kwargs['steps_per_epoch'] = samples_per_epoch\n             converted.append(('samples_per_epoch', 'steps_per_epoch'))\n \n-    keras1_args = {'samples_per_epoch', 'val_samples', 'nb_epoch', 'nb_val_samples', 'nb_worker'}\n+    keras1_args = {'samples_per_epoch', 'val_samples',\n+                   'nb_epoch', 'nb_val_samples', 'nb_worker'}\n     if keras1_args.intersection(kwargs.keys()):\n         warnings.warn('The semantics of the Keras 2 argument '\n                       '`steps_per_epoch` is not the same as the '\n\n@@ -135,7 +135,8 @@ class MaxoutDense(Layer):\n                   'nb_feature': self.nb_feature,\n                   'W_regularizer': regularizers.serialize(self.W_regularizer),\n                   'b_regularizer': regularizers.serialize(self.b_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n                   'W_constraint': constraints.serialize(self.W_constraint),\n                   'b_constraint': constraints.serialize(self.b_constraint),\n                   'bias': self.bias,\n@@ -269,7 +270,8 @@ class Highway(Layer):\n                   'activation': activations.serialize(self.activation),\n                   'W_regularizer': regularizers.serialize(self.W_regularizer),\n                   'b_regularizer': regularizers.serialize(self.b_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n                   'W_constraint': constraints.serialize(self.W_constraint),\n                   'b_constraint': constraints.serialize(self.b_constraint),\n                   'bias': self.bias,\n@@ -485,7 +487,8 @@ class Recurrent(Layer):\n         initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n         initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n         initial_state = K.expand_dims(initial_state)  # (samples, 1)\n-        initial_state = K.tile(initial_state, [1, self.units])  # (samples, output_dim)\n+        # (samples, output_dim)\n+        initial_state = K.tile(initial_state, [1, self.units])\n         initial_state = [initial_state for _ in range(len(self.states))]\n         return initial_state\n \n@@ -497,7 +500,8 @@ class Recurrent(Layer):\n         # If there are multiple inputs, then\n         # they should be the main input and `initial_state`\n         # e.g. when loading model from file\n-        if isinstance(inputs, (list, tuple)) and len(inputs) > 1 and initial_state is None:\n+        if (isinstance(inputs, (list, tuple))\n+                and len(inputs) > 1 and initial_state is None):\n             initial_state = inputs[1:]\n             inputs = inputs[0]\n \n@@ -743,7 +747,8 @@ class ConvRecurrent2D(Recurrent):\n         self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n         self.data_format = K.normalize_data_format(data_format)\n-        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2, 'dilation_rate')\n+        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n+                                                        'dilation_rate')\n         self.return_sequences = return_sequences\n         self.go_backwards = go_backwards\n         self.stateful = stateful\n@@ -784,9 +789,10 @@ class ConvRecurrent2D(Recurrent):\n \n         if self.return_state:\n             if self.data_format == 'channels_first':\n-                output_shape = [output_shape] + [(input_shape[0], self.filters, rows, cols) for _ in range(2)]\n+                state_shape = (input_shape[0], self.filters, rows, cols)\n             elif self.data_format == 'channels_last':\n-                output_shape = [output_shape] + [(input_shape[0], rows, cols, self.filters) for _ in range(2)]\n+                state_shape = (input_shape[0], rows, cols, self.filters)\n+            output_shape = [output_shape, state_shape, state_shape]\n \n         return output_shape\n \n\n@@ -45,7 +45,8 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n \n \n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n-    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k), axis=-1)\n+    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),\n+                  axis=-1)\n \n \n # Aliases\n\n@@ -67,8 +67,8 @@ def _clone_functional_model(model, input_tensors=None):\n             # Cache newly created input layer.\n             newly_created_input_layer = input_tensor._keras_history[0]\n             layer_map[layer] = newly_created_input_layer\n-        for original_input_layer, cloned_input_layer in zip(model._input_layers, input_layers):\n-            layer_map[original_input_layer] = cloned_input_layer\n+        for _original, _cloned in zip(model._input_layers, input_layers):\n+            layer_map[_original] = _cloned\n     else:\n         # Make sure that all input tensors come from a Keras layer.\n         # If tensor comes from an input layer: cache the input layer.\n\n@@ -234,7 +234,8 @@ class RMSprop(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n-        - [rmsprop: Divide the gradient by a running average of its recent magnitude](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n+        - [rmsprop: Divide the gradient by a running average of its recent magnitude]\n+          (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n     def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n@@ -301,7 +302,8 @@ class Adagrad(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n-        - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n+        - [Adaptive Subgradient Methods for Online Learning and Stochastic\n+           Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n \n     def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n@@ -371,7 +373,8 @@ class Adadelta(Optimizer):\n         decay: float >= 0. Initial learning rate decay.\n \n     # References\n-        - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n+        - [Adadelta - an adaptive learning rate method]\n+          (https://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n     def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n@@ -446,8 +449,10 @@ class Adam(Optimizer):\n             Beyond\".\n \n     # References\n-        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n-        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n+        - [Adam - A Method for Stochastic Optimization]\n+          (https://arxiv.org/abs/1412.6980v8)\n+        - [On the Convergence of Adam and Beyond]\n+          (https://openreview.net/forum?id=ryQu7f-RZ)\n     \"\"\"\n \n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n@@ -532,7 +537,8 @@ class Adamax(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n-        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n+        - [Adam - A Method for Stochastic Optimization]\n+          (https://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n \n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n@@ -613,7 +619,8 @@ class Nadam(Optimizer):\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n-        - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n+        - [On the importance of initialization and momentum in deep learning]\n+          (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n \n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n@@ -638,10 +645,10 @@ class Nadam(Optimizer):\n         t = K.cast(self.iterations, K.floatx()) + 1\n \n         # Due to the recommendations in [2], i.e. warming momentum schedule\n-        momentum_cache_t = self.beta_1 * (\n-            1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n-        momentum_cache_t_1 = self.beta_1 * (\n-            1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n+        momentum_cache_t = self.beta_1 * (1. - 0.5 * (\n+            K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n+        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (\n+            K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n         m_schedule_new = self.m_schedule * momentum_cache_t\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n@@ -659,7 +666,8 @@ class Nadam(Optimizer):\n             m_t_prime = m_t / (1. - m_schedule_next)\n             v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n             v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n-            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n+            m_t_bar = (1. - momentum_cache_t) * g_prime + (\n+                momentum_cache_t_1 * m_t_prime)\n \n             self.updates.append(K.update(m, m_t))\n             self.updates.append(K.update(v, v_t))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02bc5010a04bb11c8e91835cc9775c8149dec754", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1117 | Contributors (this commit): 20 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -45,7 +45,8 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n \n \n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n-    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),\n+    # If the shape of y_true is (num_samples, 1), flatten to (num_samples,)\n+    return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),\n                   axis=-1)\n \n \n\n@@ -96,9 +96,15 @@ def test_top_k_categorical_accuracy():\n \n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='CNTK backend does not support top_k yet')\n-def test_sparse_top_k_categorical_accuracy():\n-    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n-    y_true = K.variable(np.array([[1], [0]]))\n+@pytest.mark.parametrize('y_pred, y_true', [\n+    # Test correctness if the shape of y_true is (num_samples, 1)\n+    (np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]), np.array([[1], [0]])),\n+    # Test correctness if the shape of y_true is (num_samples,)\n+    (np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]), np.array([1, 0])),\n+])\n+def test_sparse_top_k_categorical_accuracy(y_pred, y_true):\n+    y_pred = K.variable(y_pred)\n+    y_true = K.variable(y_true)\n     success_result = K.eval(\n         metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=3))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4b8049ce41a34d9b1133795d34fb8fa439939c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1356 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -270,7 +270,7 @@ def conv2d_args_preprocessor(args, kwargs):\n     converted = []\n     if len(args) > 4:\n         raise TypeError('Layer can receive at most 3 positional arguments.')\n-    if len(args) == 4:\n+    elif len(args) == 4:\n         if isinstance(args[2], int) and isinstance(args[3], int):\n             new_keywords = ['padding', 'strides', 'data_format']\n             for kwd in new_keywords:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d337273d2e1be823c5e3e1b5e757e2426cba8004", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 5694 | Contributors (this commit): 30 | Commits (past 90d): 37 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -300,8 +300,8 @@ class TestBackend(object):\n     def test_tile(self):\n         shape = (3, 4)\n         arr = np.arange(np.prod(shape)).reshape(shape)\n-        check_single_tensor_operation('tile', arr, BACKENDS, n=[2, 1])\n-        check_single_tensor_operation('tile', (2, 5), BACKENDS, n=[5, 2])\n+        check_single_tensor_operation('tile', arr, WITH_NP, n=[2, 1])\n+        check_single_tensor_operation('tile', (2, 5), WITH_NP, n=[5, 2])\n \n         # test theano shape inference when\n         # input shape has None entries\n\n@@ -402,6 +402,10 @@ def repeat(x, n):\n     return y\n \n \n+def tile(x, n):\n+    return np.tile(x, n)\n+\n+\n def arange(start, stop=None, step=1, dtype='int32'):\n     return np.arange(start, stop, step, dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52e33501a77bd9deed8ce6b355390541f0134cf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 330 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -180,20 +180,22 @@ def test_masked_temporal():\n     The ground-truth best cross-entropy loss should, then be -log(0.5) = 0.69\n \n     '''\n+    np.random.seed(1338)\n+\n     model = Sequential()\n     model.add(layers.Embedding(10, 10, mask_zero=True))\n     model.add(layers.Activation('softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='adam')\n \n-    x = np.random.random_integers(1, 9, (20000, 10))\n+    x = np.random.randint(1, 10, size=(20000, 10))\n     for rowi in range(x.shape[0]):\n-        padding = np.random.random_integers(x.shape[1] / 2)\n+        padding = np.random.randint(0, x.shape[1] / 2 + 1)\n         x[rowi, :padding] = 0\n \n     # 50% of the time the correct output is the input.\n     # The other 50% of the time it's 2 * input % 10\n-    y = (x * np.random.random_integers(1, 2, x.shape)) % 10\n+    y = (x * np.random.randint(1, 3, size=x.shape)) % 10\n     ys = np.zeros((y.size, 10), dtype='int32')\n     for i, target in enumerate(y.flat):\n         ys[i, target] = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8763efb371f355e65473e7c59a8715cb06bdd0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 10242 | Contributors (this commit): 122 | Commits (past 90d): 16 | Contributors (cumulative): 124 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3161,6 +3161,9 @@ def relu(x, alpha=0., max_value=None):\n     # Returns\n         A tensor.\n     \"\"\"\n+    if alpha == 0 and max_value == 6:\n+        return tf.nn.relu6(x)\n+\n     if alpha != 0.:\n         x = tf.nn.leaky_relu(x, alpha)\n     else:\n\n@@ -163,6 +163,19 @@ def test_relu():\n     result = f([test_values])[0]\n     assert_allclose(result, test_values, rtol=1e-05)\n \n+    # Test max_value\n+    test_values = [0.5, 1.5]\n+    f = K.function([x], [activations.relu(x, max_value=1.)])\n+    result = f([test_values])[0]\n+    assert np.max(result) <= 1.\n+\n+    # Test max_value == 6.\n+    test_values = [0.5, 6.]\n+    f = K.function([x], [activations.relu(x, max_value=1.)])\n+    result = f([test_values])[0]\n+    assert np.max(result) <= 6.\n+\n+\n \n def test_elu():\n     x = K.placeholder(ndim=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#810247deaf443f4049a28631557a58742a0ee45f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 165 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -164,19 +164,18 @@ def test_relu():\n     assert_allclose(result, test_values, rtol=1e-05)\n \n     # Test max_value\n-    test_values = [0.5, 1.5]\n+    test_values = np.array([[0.5, 1.5]], dtype=K.floatx())\n     f = K.function([x], [activations.relu(x, max_value=1.)])\n     result = f([test_values])[0]\n     assert np.max(result) <= 1.\n \n     # Test max_value == 6.\n-    test_values = [0.5, 6.]\n+    test_values = np.array([[0.5, 6.]], dtype=K.floatx())\n     f = K.function([x], [activations.relu(x, max_value=1.)])\n     result = f([test_values])[0]\n     assert np.max(result) <= 6.\n \n \n-\n def test_elu():\n     x = K.placeholder(ndim=2)\n     f = K.function([x], [activations.elu(x, 0.5)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#630a3a877c3c96ce39be7b0611570466c8c9f447", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 241 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -102,10 +102,10 @@ y_val = labels[-num_validation_samples:]\n print('Preparing embedding matrix.')\n \n # prepare embedding matrix\n-num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n+num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n for word, i in word_index.items():\n-    if i >= MAX_NUM_WORDS:\n+    if i > MAX_NUM_WORDS:\n         continue\n     embedding_vector = embeddings_index.get(word)\n     if embedding_vector is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3fed40a2d6d6877694a5b034c5ea07e04757c82c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1299 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -539,6 +539,8 @@ class Bidirectional(Wrapper):\n             output = y * y_rev\n         elif self.merge_mode is None:\n             output = [y, y_rev]\n+        else:\n+            raise ValueError('Unrecognized value for argument merge_mode: %s' % (self.merge_mode))\n \n         # Properly set learning phase\n         if (getattr(y, '_uses_learning_phase', False) or\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32fee92df5fa21e3d1ff6375f7a5a923636c311b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2260 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1240,10 +1240,9 @@ class Network(Layer):\n         \"\"\"\n         if not self.built:\n             raise ValueError(\n-                'This model has never been called, thus its weights '\n-                'have not yet been created, so no summary can be displayed. '\n-                'Build the model first '\n-                '(e.g. by calling it on some test data).')\n+                'This model has not yet been built. '\n+                'Build the model first by calling build() or calling fit() with some data. '\n+                'Or specify input_shape or batch_input_shape in the first layer for automatic build. ')\n         return print_layer_summary(self,\n                                    line_length=line_length,\n                                    positions=positions,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9e77534c889e378eda67e6aba10632a9d30d629", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 41 | Files Changed: 7 | Hunks: 36 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 112 | Churn Cumulative: 13876 | Contributors (this commit): 107 | Commits (past 90d): 32 | Contributors (cumulative): 142 | DMM Complexity: 0.25925925925925924\n\nDIFF:\n@@ -22,7 +22,8 @@ def load_data(path='boston_housing.npz', test_split=0.2, seed=113):\n         Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n     \"\"\"\n     assert 0 <= test_split < 1\n-    path = get_file(path,\n+    path = get_file(\n+        path,\n         origin='https://s3.amazonaws.com/keras-datasets/boston_housing.npz',\n         file_hash='f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5')\n     with np.load(path) as f:\n\n@@ -91,9 +91,11 @@ def load_data(path='imdb.npz', num_words=None, skip_top=0,\n     # reserve 'index_from' (=3 by default) characters:\n     # 0 (padding), 1 (start), 2 (OOV)\n     if oov_char is not None:\n-        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs]\n+        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x]\n+              for x in xs]\n     else:\n-        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n+        xs = [[w for w in x if skip_top <= w < num_words]\n+              for x in xs]\n \n     idx = len(x_train)\n     x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n@@ -111,7 +113,8 @@ def get_word_index(path='imdb_word_index.json'):\n     # Returns\n         The word index dictionary.\n     \"\"\"\n-    path = get_file(path,\n+    path = get_file(\n+        path,\n         origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.json',\n         file_hash='bfafd718b763782e994055a2d397834f')\n     with open(path) as f:\n\n@@ -97,7 +97,8 @@ def get_word_index(path='reuters_word_index.json'):\n     # Returns\n         The word index dictionary.\n     \"\"\"\n-    path = get_file(path,\n+    path = get_file(\n+        path,\n         origin='https://s3.amazonaws.com/text-datasets/reuters_word_index.json',\n         file_hash='4d44cc38712099c9e383dc6e5f11a921')\n     f = open(path)\n\n@@ -629,11 +629,13 @@ class Network(Layer):\n                             inbound_layer = node.inbound_layers[j]\n                             node_index = node.node_indices[j]\n                             tensor_index = node.tensor_indices[j]\n-                            shape_key = inbound_layer.name + '_%s_%s' % (node_index, tensor_index)\n+                            shape_key = inbound_layer.name\n+                            shape_key += '_%s_%s' % (node_index, tensor_index)\n                             input_shape = layers_to_output_shapes[shape_key]\n                             input_shapes.append(input_shape)\n \n-                        output_shape = layer.compute_output_shape(unpack_singleton(input_shapes))\n+                        output_shape = layer.compute_output_shape(\n+                            unpack_singleton(input_shapes))\n \n                         output_shapes = to_list(output_shape)\n                         node_index = layer._inbound_nodes.index(node)\n@@ -715,7 +717,8 @@ class Network(Layer):\n                             if has_arg(layer.call, 'mask'):\n                                 if 'mask' not in kwargs:\n                                     kwargs['mask'] = computed_mask\n-                            output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n+                            output_tensors = to_list(\n+                                layer.call(computed_tensor, **kwargs))\n                             output_masks = layer.compute_mask(computed_tensor,\n                                                               computed_mask)\n                             if output_masks is None:\n@@ -741,7 +744,8 @@ class Network(Layer):\n                             else:\n                                 output_masks = to_list(output_masks)\n                         # Apply activity regularizer if any:\n-                        if hasattr(layer, 'activity_regularizer') and layer.activity_regularizer is not None:\n+                        if (hasattr(layer, 'activity_regularizer') and\n+                                layer.activity_regularizer is not None):\n                             with K.name_scope('activity_regularizer'):\n                                 regularization_losses = [\n                                     layer.activity_regularizer(x)\n@@ -753,8 +757,8 @@ class Network(Layer):\n                             raise Exception(\n                                 'Layers should have equal number of output tensors '\n                                 'and output masks. Layer ' + str(layer.name) + ' has'\n-                                ' ' + str(len(output_tensors)) + ' output tensors and'\n-                                ' ' + str(len(output_masks)) + ' output masks.')\n+                                ' ' + str(len(output_tensors)) + ' output tensors '\n+                                'and ' + str(len(output_masks)) + ' output masks.')\n                     # Update model updates and losses:\n                     # Keep track of updates that depend on the inputs\n                     # (e.g. BN updates).\n@@ -770,16 +774,21 @@ class Network(Layer):\n \n                     # Update _keras_shape.\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n-                        input_shapes = unpack_singleton([x._keras_shape for x in computed_tensors])\n+                        input_shapes = unpack_singleton(\n+                            [x._keras_shape for x in computed_tensors])\n                         shapes = to_list(layer.compute_output_shape(input_shapes))\n-                        uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n+                        uses_learning_phase = any(\n+                            [x._uses_learning_phase for x in computed_tensors])\n \n                         for x, s in zip(output_tensors, shapes):\n                             x._keras_shape = s\n-                            x._uses_learning_phase = getattr(x, '_uses_learning_phase', False) or uses_learning_phase\n+                            _u = getattr(x, '_uses_learning_phase', False)\n+                            x._uses_learning_phase = _u or uses_learning_phase\n \n                     # Update tensor_map.\n-                    for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):\n+                    for x, y, mask in zip(reference_output_tensors,\n+                                          output_tensors,\n+                                          output_masks):\n                         tensor_map[str(id(x))] = (y, mask)\n \n         output_tensors = []\n\n@@ -87,7 +87,8 @@ def _serialize_model(model, f, include_optimizer=True):\n \n     model_weights_group = f['model_weights']\n     model_layers = model.layers\n-    model_weights_group['layer_names'] = [layer.name.encode('utf8') for layer in model_layers]\n+    model_weights_group['layer_names'] = [layer.name.encode('utf8')\n+                                          for layer in model_layers]\n     model_weights_group['backend'] = K.backend().encode('utf8')\n     model_weights_group['keras_version'] = str(keras_version).encode('utf8')\n     for layer in model_layers:\n@@ -591,7 +592,7 @@ def preprocess_weights_for_loading(layer, weights,\n                                    original_keras_version=None,\n                                    original_backend=None,\n                                    reshape=False):\n-    \"\"\"Converts layers weights from Keras 1 format to Keras 2 and also weights of CuDNN layers in Keras 2.\n+    \"\"\"Converts layers weights from Keras 1 format to Keras 2.\n \n     # Arguments\n         layer: Layer instance.\n@@ -606,7 +607,7 @@ def preprocess_weights_for_loading(layer, weights,\n         A list of weights values (Numpy arrays).\n     \"\"\"\n     def convert_nested_bidirectional(weights):\n-        \"\"\"Converts layers nested in `Bidirectional` wrapper by `preprocess_weights_for_loading()`.\n+        \"\"\"Converts layers nested in `Bidirectional` wrapper.\n \n         # Arguments\n             weights: List of weights values (Numpy arrays).\n@@ -614,18 +615,20 @@ def preprocess_weights_for_loading(layer, weights,\n             A list of weights values (Numpy arrays).\n         \"\"\"\n         num_weights_per_layer = len(weights) // 2\n-        forward_weights = preprocess_weights_for_loading(layer.forward_layer,\n+        forward_weights = preprocess_weights_for_loading(\n+            layer.forward_layer,\n             weights[:num_weights_per_layer],\n             original_keras_version,\n             original_backend)\n-        backward_weights = preprocess_weights_for_loading(layer.backward_layer,\n+        backward_weights = preprocess_weights_for_loading(\n+            layer.backward_layer,\n             weights[num_weights_per_layer:],\n             original_keras_version,\n             original_backend)\n         return forward_weights + backward_weights\n \n     def convert_nested_time_distributed(weights):\n-        \"\"\"Converts layers nested in `TimeDistributed` wrapper by `preprocess_weights_for_loading()`.\n+        \"\"\"Converts layers nested in `TimeDistributed` wrapper.\n \n         # Arguments\n             weights: List of weights values (Numpy arrays).\n@@ -636,7 +639,7 @@ def preprocess_weights_for_loading(layer, weights,\n             layer.layer, weights, original_keras_version, original_backend)\n \n     def convert_nested_model(weights):\n-        \"\"\"Converts layers nested in `Model` or `Sequential` by `preprocess_weights_for_loading()`.\n+        \"\"\"Converts layers nested in `Model` or `Sequential`.\n \n         # Arguments\n             weights: List of weights values (Numpy arrays).\n@@ -691,7 +694,8 @@ def preprocess_weights_for_loading(layer, weights,\n             if shape[:2] != (layer.kernel_size[0], 1) or shape[3] != layer.filters:\n                 # Legacy shape:\n                 # (filters, input_dim, filter_length, 1)\n-                assert shape[0] == layer.filters and shape[2:] == (layer.kernel_size[0], 1)\n+                assert (shape[0] == layer.filters and\n+                        shape[2:] == (layer.kernel_size[0], 1))\n                 weights[0] = np.transpose(weights[0], (2, 3, 1, 0))\n             weights[0] = weights[0][:, 0, :, :]\n \n@@ -908,14 +912,17 @@ def _convert_rnn_weights(layer, weights):\n     # convert the weights between CuDNNGRU and GRU(reset_after=True)\n     if target_class in ['GRU', 'CuDNNGRU'] and len(weights) == 3:\n         # We can determine the source of the weights from the shape of the bias.\n-        # If there is no bias we skip the conversion since CuDNNGRU always has biases.\n+        # If there is no bias we skip the conversion\n+        # since CuDNNGRU always has biases.\n \n         units = weights[1].shape[0]\n         bias_shape = weights[2].shape\n         n_gates = 3\n \n         def convert_weights(weights, from_cudnn=True):\n-            kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n+            kernels = transform_kernels(weights[0],\n+                                        transpose_input(from_cudnn),\n+                                        n_gates)\n             recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n             biases = np.array(weights[2]).reshape((2, -1) if from_cudnn else -1)\n             return [kernels, recurrent_kernels, biases]\n@@ -954,7 +961,7 @@ def _need_convert_kernel(original_backend):\n \n     The convolution operation is implemented differently in different backends.\n     While TH implements convolution, TF and CNTK implement the correlation operation.\n-    So the channel axis needs to be flipped when we're loading TF weights onto a TH model,\n+    So the channel axis needs to be flipped when TF weights are loaded on a TH model,\n     or vice versa. However, there's no conversion required between TF and CNTK.\n \n     # Arguments\n@@ -1108,9 +1115,10 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n                 reshape=reshape)\n             if len(weight_values) != len(symbolic_weights):\n                 if skip_mismatch:\n-                    warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n-                                  ' due to mismatch in number of weights' +\n-                                  ' ({} vs {}).'.format(len(symbolic_weights), len(weight_values)))\n+                    warnings.warn('Skipping loading of weights for '\n+                                  'layer {}'.format(layer.name) + ' due to mismatch '\n+                                  'in number of weights ({} vs {}).'.format(\n+                                      len(symbolic_weights), len(weight_values)))\n                     continue\n                 else:\n                     raise ValueError('Layer #' + str(k) +\n@@ -1122,11 +1130,12 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n                                      ' element(s).')\n             # Set values.\n             for i in range(len(weight_values)):\n-                if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n+                symbolic_shape = K.int_shape(symbolic_weights[i])\n+                if symbolic_shape != weight_values[i].shape:\n                     if skip_mismatch:\n-                        warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n-                                      ' due to mismatch in shape' +\n-                                      ' ({} vs {}).'.format(\n+                        warnings.warn('Skipping loading of weights for '\n+                                      'layer {}'.format(layer.name) + ' due to '\n+                                      'mismatch in shape ({} vs {}).'.format(\n                                           symbolic_weights[i].shape,\n                                           weight_values[i].shape))\n                         continue\n@@ -1135,7 +1144,7 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n                                          ' (named \"' + layer.name +\n                                          '\"), weight ' +\n                                          str(symbolic_weights[i]) +\n-                                         ' has shape {}'.format(K.int_shape(symbolic_weights[i])) +\n+                                         ' has shape {}'.format(symbolic_shape) +\n                                          ', but the saved weight has shape ' +\n                                          str(weight_values[i].shape) + '.')\n                 else:\n\n@@ -216,7 +216,8 @@ class Model(Network):\n                 target_tensors = [target_tensors]\n             else:\n                 raise TypeError('Expected `target_tensors` to be a tensor, '\n-                                'a list of tensors, or dict of tensors, but got:', target_tensors)\n+                                'a list of tensors, or dict of tensors, but got:',\n+                                target_tensors)\n \n         for i in range(len(self.outputs)):\n             if i in skip_target_indices:\n@@ -382,13 +383,15 @@ class Model(Network):\n                             metric_fn = metrics_module.binary_accuracy\n                         elif metric in ('crossentropy', 'ce'):\n                             metric_fn = metrics_module.binary_crossentropy\n-                    elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n+                    elif (self.loss_functions[i] ==\n+                          losses.sparse_categorical_crossentropy):\n                         # case: categorical accuracy/crossentropy\n                         # with sparse targets\n                         if metric in ('accuracy', 'acc'):\n                             metric_fn = metrics_module.sparse_categorical_accuracy\n                         elif metric in ('crossentropy', 'ce'):\n-                            metric_fn = metrics_module.sparse_categorical_crossentropy\n+                            metric_fn = (\n+                                metrics_module.sparse_categorical_crossentropy)\n                     else:\n                         # case: categorical accuracy/crossentropy\n                         if metric in ('accuracy', 'acc'):\n@@ -570,7 +573,8 @@ class Model(Network):\n               when calling `fit`/etc.\n             - if data tensors: the model is built on top of these tensors.\n               We do not expect any Numpy data to be provided when calling `fit`/etc.\n-          outputs: Optional output tensors (if already computed by running the model).\n+          outputs: Optional output tensors (if already computed by running\n+            the model).\n           training: Boolean or None. Only relevant in symbolic mode. Specifies\n             whether to build the model's graph in inference mode (False), training\n             mode (True), or using the Keras learning phase (None).\n@@ -1284,7 +1288,8 @@ class Model(Network):\n                       use_multiprocessing=False,\n                       shuffle=True,\n                       initial_epoch=0):\n-        \"\"\"Trains the model on data generated batch-by-batch by a Python generator (or an instance of `Sequence`).\n+        \"\"\"Trains the model on data generated batch-by-batch by a Python generator\n+        (or an instance of `Sequence`).\n \n         The generator is run in parallel to the model, for efficiency.\n         For instance, this allows you to do real-time data augmentation\n\n@@ -110,11 +110,13 @@ def fit_generator(model,\n                 # Create an Enqueuer that can be reused\n                 val_data = validation_data\n                 if isinstance(val_data, Sequence):\n-                    val_enqueuer = OrderedEnqueuer(val_data,\n+                    val_enqueuer = OrderedEnqueuer(\n+                        val_data,\n                         use_multiprocessing=use_multiprocessing)\n                     validation_steps = validation_steps or len(val_data)\n                 else:\n-                    val_enqueuer = GeneratorEnqueuer(val_data,\n+                    val_enqueuer = GeneratorEnqueuer(\n+                        val_data,\n                         use_multiprocessing=use_multiprocessing)\n                 val_enqueuer.start(workers=workers,\n                                    max_queue_size=max_queue_size)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
