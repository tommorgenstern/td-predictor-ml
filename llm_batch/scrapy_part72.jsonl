{"custom_id": "scrapy#e82d9d885ef310b2103bac889b6a014170e41e0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 34 | Churn Cumulative: 68 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,15 +4,39 @@ Offsite Spider Middleware\n See documentation in docs/ref/spider-middleware.rst\n \"\"\"\n \n+import re\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.core import signals\n from scrapy.http import Request\n-from scrapy.utils.url import url_is_from_spider\n+from scrapy.utils.httpobj import urlparse_cached\n \n class OffsiteMiddleware(object):\n \n+    def __init__(self):\n+        self.host_regexes = {}\n+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+\n     def process_spider_output(self, response, result, spider):\n         return (x for x in result if not isinstance(x, Request) or \\\n-            self.is_url_from_spider(x.url, spider))\n+            self.should_follow(x, spider))\n \n-    def is_url_from_spider(self, url, spider):\n-        # XXX: should we ask this to the spider manager?\n-        return url_is_from_spider(url, spider)\n+    def should_follow(self, request, spider):\n+        regex = self.host_regexes[spider]\n+        # hostanme can be None for wrong urls (like javascript links)\n+        host = urlparse_cached(request).hostname or ''\n+        return bool(regex.search(host))\n+\n+    def get_host_regex(self, domains):\n+        \"\"\"Override this method to implement a different offsite policy\"\"\"\n+        domains = [d.replace('.', r'\\.') for d in domains]\n+        regex = r'^(|.*\\.)(%s)$' % '|'.join(domains)\n+        return re.compile(regex)\n+\n+    def domain_opened(self, spider):\n+        domains = [spider.domain_name] + spider.extra_domain_names\n+        self.host_regexes[spider] = self.get_host_regex(domains)\n+\n+    def domain_closed(self, spider):\n+        del self.host_regexes[spider]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64ffe6b2d3f839023d48571b2ce8f734115eac5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 4 | Files Changed: 6 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 57 | Churn Cumulative: 297 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,8 +11,9 @@ from twisted.internet import defer\n \n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n+from scrapy.utils.ref import object_ref\n \n-class Request(object):\n+class Request(object_ref):\n \n     __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', '_cache', \\\n         'dont_filter', 'headers', 'cookies', 'deferred', 'priority', \\\n\n@@ -8,8 +8,9 @@ See documentation in docs/ref/request-response.rst\n import copy\n \n from scrapy.http.headers import Headers\n+from scrapy.utils.ref import object_ref\n \n-class Response(object):\n+class Response(object_ref):\n \n     __slots__ = ['url', 'headers', 'status', '_body', 'request', '_meta', \\\n         'flags', '_cache', '__weakref__']\n\n@@ -1,4 +1,6 @@\n-class BaseItem(object):\n+from scrapy.utils.ref import object_ref\n+\n+class BaseItem(object_ref):\n     \"\"\"Base class for all scraped items.\"\"\"\n     pass\n \n\n@@ -15,6 +15,7 @@ from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.stats import stats\n+from scrapy.utils.ref import print_live_refs\n from scrapy.conf import settings\n \n try:\n@@ -32,6 +33,7 @@ telnet_namespace = {\n     'spiders': spiders,\n     'settings': settings,\n     'p': pprint.pprint,\n+    'prefs': print_live_refs,\n     'hpy': hpy,\n }\n \n\n@@ -0,0 +1,42 @@\n+\"\"\"This module provides some functions and classes to record and report live\n+references to object instances, for certain classes\"\"\"\n+\n+import weakref\n+from collections import defaultdict\n+from time import time\n+from operator import itemgetter\n+\n+from scrapy.conf import settings\n+\n+live_refs = defaultdict(weakref.WeakKeyDictionary)\n+\n+class object_ref(object):\n+    \"\"\"Inherit from this class (instead of object) to a keep a record of live\n+    instances\"\"\"\n+\n+    __slots__ = ()\n+\n+    def __new__(cls, *args, **kwargs):\n+        obj = object.__new__(cls)\n+        live_refs[cls][obj] = time()\n+        return obj\n+\n+if not settings.getbool('TRACK_REFS'):\n+    object_ref = object\n+\n+def print_live_refs():\n+    print \"Live References\"\n+    print\n+    now = time()\n+    for cls, wdict in live_refs.iteritems():\n+        if not wdict:\n+            continue\n+        oldest = min(wdict.itervalues())\n+        print \"%-30s %6d   oldest: %ds ago\" % (cls.__name__, len(wdict), \\\n+            now-oldest)\n+\n+def get_oldest(class_name):\n+    for cls, wdict in live_refs.iteritems():\n+        if cls.__name__ == class_name:\n+            if wdict:\n+                return min(wdict.iteritems(), key=itemgetter(1))[0]\n\n@@ -11,9 +11,10 @@ from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml\n from scrapy.xpath.document import Libxml2Document\n from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n+from scrapy.utils.ref import object_ref\n from scrapy.utils.decorator import deprecated\n \n-class XPathSelector(object):\n+class XPathSelector(object_ref):\n \n     def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n         if parent:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00527fc93cf64da05a35d03700028c70214e2729", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 138 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -10,6 +10,7 @@ from scrapy.xpath.factories import xmlDoc_from_html\n class Libxml2Document(object):\n \n     cache = weakref.WeakKeyDictionary()\n+    __slots__ = ['xmlDoc', 'xpathContext']\n \n     def __new__(cls, response, factory=xmlDoc_from_html):\n         cache = cls.cache.setdefault(response, {})\n\n@@ -16,6 +16,8 @@ from scrapy.utils.decorator import deprecated\n \n class XPathSelector(object_ref):\n \n+    __slots__ = ['doc', 'xmlNode', 'response', 'expr']\n+\n     def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n         if parent:\n             self.doc = parent.doc\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#240e06813d157dd012e84550ea154310c218448b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 176 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.xpath.factories import xmlDoc_from_html\n class Libxml2Document(object):\n \n     cache = weakref.WeakKeyDictionary()\n-    __slots__ = ['xmlDoc', 'xpathContext']\n+    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']\n \n     def __new__(cls, response, factory=xmlDoc_from_html):\n         cache = cls.cache.setdefault(response, {})\n\n@@ -16,7 +16,7 @@ from scrapy.utils.decorator import deprecated\n \n class XPathSelector(object_ref):\n \n-    __slots__ = ['doc', 'xmlNode', 'response', 'expr']\n+    __slots__ = ['doc', 'xmlNode', 'expr', '__weakref__']\n \n     def __init__(self, response=None, text=None, node=None, parent=None, expr=None):\n         if parent:\n@@ -31,7 +31,6 @@ class XPathSelector(object_ref):\n             self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n-        self.response = response\n \n     def select(self, xpath):\n         \"\"\"Perform the given XPath query on the current XPathSelector and\n@@ -42,24 +41,15 @@ class XPathSelector(object_ref):\n                 xpath_result = self.doc.xpathContext.xpathEval(xpath)\n             except libxml2.xpathError:\n                 raise ValueError(\"Invalid XPath: %s\" % xpath)\n-            cls = type(self)\n             if hasattr(xpath_result, '__iter__'):\n-                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, \\\n-                    response=self.response) for node in xpath_result])\n+                return XPathSelectorList([self.__class__(node=node, parent=self, \\\n+                    expr=xpath) for node in xpath_result])\n             else:\n-                return XPathSelectorList([cls(node=xpath_result, parent=self, \\\n-                    expr=xpath, response=self.response)])\n+                return XPathSelectorList([self.__class__(node=xpath_result, \\\n+                    parent=self, expr=xpath)])\n         else:\n             return XPathSelectorList([])\n \n-    @deprecated(use_instead='XPathSelector.select')\n-    def __call__(self, xpath):\n-        return self.select(xpath)\n-\n-    @deprecated(use_instead='XPathSelector.select')\n-    def x(self, xpath):\n-        return self.select(xpath)\n-\n     def re(self, regex):\n         \"\"\"Return a list of unicode strings by applying the regex over all\n         current XPath selections, and flattening the results\"\"\"\n@@ -110,6 +100,14 @@ class XPathSelector(object_ref):\n \n     __repr__ = __str__\n \n+    @deprecated(use_instead='XPathSelector.select')\n+    def __call__(self, xpath):\n+        return self.select(xpath)\n+\n+    @deprecated(use_instead='XPathSelector.select')\n+    def x(self, xpath):\n+        return self.select(xpath)\n+\n \n class XPathSelectorList(list):\n     \"\"\"List of XPathSelector objects\"\"\"\n@@ -122,10 +120,6 @@ class XPathSelectorList(list):\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n         return XPathSelectorList(flatten([x.select(xpath) for x in self]))\n \n-    @deprecated(use_instead='XPathSelectorList.select')\n-    def x(self, xpath):\n-        return self.select(xpath)\n-\n     def re(self, regex):\n         \"\"\"Perform the re() method on each XPathSelector of the list, and\n         return the result as a flattened list of unicode strings\"\"\"\n@@ -139,6 +133,10 @@ class XPathSelectorList(list):\n     def extract_unquoted(self):\n         return [x.extract_unquoted() if isinstance(x, XPathSelector) else x for x in self]\n \n+    @deprecated(use_instead='XPathSelectorList.select')\n+    def x(self, xpath):\n+        return self.select(xpath)\n+\n \n class XmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for XML content\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e947e1d45b7d449baf8fe61d08b107e9f20f19d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 4 | Files Changed: 10 | Hunks: 18 | Methods Changed: 4 | Complexity Δ (Sum/Max): 31/12 | Churn Δ: 53 | Churn Cumulative: 445 | Contributors (this commit): 3 | Commits (past 90d): 30 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,8 @@ def _unicode_to_str(string, encoding):\n \n class FormRequest(Request):\n \n+    __slots__ = ()\n+\n     def __init__(self, *args, **kwargs):\n         formdata = kwargs.pop('formdata', None)\n         Request.__init__(self, *args, **kwargs)\n\n@@ -12,6 +12,8 @@ from scrapy.utils.python import memoizemethod\n \n class HtmlResponse(TextResponse):\n \n+    __slots__ = ()\n+\n     _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n \n     _httpequiv_re = _template % ('http-equiv', 'Content-Type')\n\n@@ -12,6 +12,8 @@ from scrapy.utils.python import memoizemethod\n \n class XmlResponse(TextResponse):\n \n+    __slots__ = ()\n+\n     _template = r'''%s\\s*=\\s*[\"']?\\s*%s\\s*[\"']?'''\n     _encoding_re  = _template % ('encoding', r'(?P<charset>[\\w-]+)')\n     XMLDECL_RE  = re.compile(r'<\\?xml\\s.*?%s' % _encoding_re, re.I)\n\n@@ -10,7 +10,7 @@ class Link(object):\n     At the moment, it contains just the url and link text.\n     \"\"\"\n \n-    __slots__ = 'url', 'text'\n+    __slots__ = ['url', 'text']\n \n     def __init__(self, url, text=''):\n         self.url = url\n\n@@ -1,4 +1,5 @@\n import unittest\n+import weakref\n import copy\n \n from scrapy.http import Headers\n@@ -103,3 +104,8 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h1.getlist('header1'), ['value1'])\n         self.assertEqual(h1.getlist('header2'), ['value2', 'value3'])\n \n+    def test_slots(self):\n+        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n+        x = Headers({})\n+        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n+            x.__class__.__name__\n\n@@ -1,5 +1,6 @@\n import unittest\n import cgi\n+import weakref\n from cStringIO import StringIO\n from urlparse import urlparse\n \n@@ -166,6 +167,14 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r4.meta, {})\n         assert r4.dont_filter is False\n \n+    def test_weakref_slots(self):\n+        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n+        for cls in [Request, FormRequest]:\n+            x = cls('http://www.example.com')\n+            weakref.ref(x)\n+            assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n+                x.__class__.__name__\n+\n \n class FormRequestTest(unittest.TestCase):\n \n@@ -299,6 +308,5 @@ class XmlRpcRequestTest(unittest.TestCase):\n         self.assertEqual(r1.body, r2.body)\n \n \n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,4 +1,6 @@\n import unittest\n+import weakref\n+\n from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers\n \n class ResponseTest(unittest.TestCase):\n@@ -201,6 +203,14 @@ class ResponseTest(unittest.TestCase):\n         self._assert_response_values(r5, 'iso-8859-1', body)\n         self._assert_response_values(r6, 'utf-8', body2)\n \n+    def test_weakref_slots(self):\n+        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n+        for cls in [Response, TextResponse, XmlResponse, HtmlResponse]:\n+            x = cls('http://www.example.com')\n+            weakref.ref(x)\n+            assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n+                x.__class__.__name__\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,10 +1,12 @@\n import re\n import unittest\n+import weakref\n \n import libxml2\n \n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, \\\n+    XPathSelector\n from scrapy.xpath.document import Libxml2Document\n from scrapy.utils.test import libxml2debug\n \n@@ -239,6 +241,15 @@ class XPathSelectorTestCase(unittest.TestCase):\n             u'\\n  ',\n             u'\\n  pff\\n'])\n \n+    @libxml2debug\n+    def test_weakref_slots(self):\n+        \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n+        for cls in [XPathSelector, HtmlXPathSelector, XmlXPathSelector]:\n+            x = cls()\n+            weakref.ref(x)\n+            assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n+                x.__class__.__name__\n+\n class Libxml2DocumentTest(unittest.TestCase):\n \n     @libxml2debug\n\n@@ -160,6 +160,9 @@ class SiteNode(object):\n \n \n class CaselessDict(dict):\n+\n+    __slots__ = ()\n+\n     def __init__(self, seq=None):\n         super(CaselessDict, self).__init__()\n         if seq:\n\n@@ -140,9 +140,10 @@ class XPathSelectorList(list):\n \n class XmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for XML content\"\"\"\n+    __slots__ = ()\n     _get_libxml2_doc = staticmethod(xmlDoc_from_xml)\n \n-\n class HtmlXPathSelector(XPathSelector):\n     \"\"\"XPathSelector for HTML content\"\"\"\n+    __slots__ = ()\n     _get_libxml2_doc = staticmethod(xmlDoc_from_html)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c5dad3f89f6a67f6bd517751e2f6e93cc213d65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 26 | Churn Cumulative: 322 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,7 @@ from twisted.internet import defer\n \n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n-from scrapy.utils.ref import object_ref\n+from scrapy.utils.trackref import object_ref\n \n class Request(object_ref):\n \n\n@@ -8,7 +8,7 @@ See documentation in docs/ref/request-response.rst\n import copy\n \n from scrapy.http.headers import Headers\n-from scrapy.utils.ref import object_ref\n+from scrapy.utils.trackref import object_ref\n \n class Response(object_ref):\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.utils.ref import object_ref\n+from scrapy.utils.trackref import object_ref\n \n class BaseItem(object_ref):\n     \"\"\"Base class for all scraped items.\"\"\"\n@@ -23,6 +23,7 @@ class ScrapedItem(BaseItem):\n         Generate the following format so that items can be deserialized\n         easily: ClassName({'attrib': value, ...})\n         \"\"\"\n-        reprdict = dict(items for items in self.__dict__.iteritems() if not items[0].startswith('_'))\n+        reprdict = dict(items for items in self.__dict__.iteritems() \\\n+            if not items[0].startswith('_'))\n         return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n \n\n@@ -15,7 +15,7 @@ from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.stats import stats\n-from scrapy.utils.ref import print_live_refs\n+from scrapy.utils.trackref import print_live_refs\n from scrapy.conf import settings\n \n try:\n\n@@ -1,5 +1,14 @@\n-\"\"\"This module provides some functions and classes to record and report live\n-references to object instances, for certain classes\"\"\"\n+\"\"\"This module provides some functions and classes to record and report\n+references to live object instances.\n+\n+If you want live objects for a particular class to be tracked, you only have to\n+subclass form object_ref (instead of object). Also, remember to turn on\n+tracking by enabling the TRACK_REFS setting.\n+\n+About performance: This library has a minimal performance impact when enabled,\n+and no performance penalty at all when disabled (as object_ref becomes just an\n+alias to object in that case).\n+\"\"\"\n \n import weakref\n from collections import defaultdict\n\n@@ -11,7 +11,7 @@ from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml\n from scrapy.xpath.document import Libxml2Document\n from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n-from scrapy.utils.ref import object_ref\n+from scrapy.utils.trackref import object_ref\n from scrapy.utils.decorator import deprecated\n \n class XPathSelector(object_ref):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3c6d83ad3c39c571e567f957b76bf7fa9177cfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 134 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,6 +82,9 @@ class CsvItemExporter(BaseItemExporter):\n         super(CsvItemExporter, self).__init__()\n         self.csv_writer = csv.writer(*args, **kwargs)\n         if self.include_headers_line:\n+            if not self.fields_to_export:\n+                raise RuntimeError(\"To use include_headers_line you must \" \\\n+                    \"define fields_to_export attribute\")\n             self.csv_writer.writerow(self.fields_to_export)\n \n     def export(self, item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d351c28666552f5b3cbc48d348f600d33879f55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 70 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,7 @@ class OffsiteMiddleware(object):\n     def get_host_regex(self, domains):\n         \"\"\"Override this method to implement a different offsite policy\"\"\"\n         domains = [d.replace('.', r'\\.') for d in domains]\n-        regex = r'^(|.*\\.)(%s)$' % '|'.join(domains)\n+        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)\n \n     def domain_opened(self, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#907fad6da83a20898df1d0cb80ebf36c48488ca5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 553 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,10 +26,12 @@ class ItemLoader(object):\n         self._values = defaultdict(list)\n \n     def add_value(self, field_name, value):\n+        value = arg_to_iter(value)\n         processed_value = self._process_input_value(field_name, value)\n         self._values[field_name] += arg_to_iter(processed_value)\n \n     def replace_value(self, field_name, value):\n+        value = arg_to_iter(value)\n         processed_value = self._process_input_value(field_name, value)\n         self._values[field_name] = arg_to_iter(processed_value)\n \n\n@@ -70,7 +70,30 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_collected_values('name'), [u'Pepe'])\n         self.assertEqual(il.get_output_value('name'), [u'Pepe'])\n \n-    def test_apply_concat_filter(self):\n+    def test_iter_on_input_processor_input(self):\n+        class NameFirstItemLoader(NameItemLoader):\n+            name_in = TakeFirst()\n+\n+        il = NameFirstItemLoader()\n+        il.add_value('name', u'marta')\n+        self.assertEqual(il.get_collected_values('name'), [u'marta'])\n+        il = NameFirstItemLoader()\n+        il.add_value('name', [u'marta', u'jose'])\n+        self.assertEqual(il.get_collected_values('name'), [u'marta'])\n+\n+        il = NameFirstItemLoader()\n+        il.replace_value('name', u'marta')\n+        self.assertEqual(il.get_collected_values('name'), [u'marta'])\n+        il = NameFirstItemLoader()\n+        il.replace_value('name', [u'marta', u'jose'])\n+        self.assertEqual(il.get_collected_values('name'), [u'marta'])\n+\n+        il = NameFirstItemLoader()\n+        il.add_value('name', u'marta')\n+        il.add_value('name', [u'jose', u'pedro'])\n+        self.assertEqual(il.get_collected_values('name'), [u'marta', u'jose'])\n+\n+    def test_map_compose_filter(self):\n         def filter_world(x):\n             return None if x == 'world' else x\n \n@@ -78,7 +101,7 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),\n                          ['HELLO', 'THIS', 'IS', 'SCRAPY'])\n \n-    def test_map_concat_filter_multil(self):\n+    def test_map_compose_filter_multil(self):\n         class TestItemLoader(NameItemLoader):\n             name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])\n \n@@ -121,7 +144,7 @@ class ItemLoaderTest(unittest.TestCase):\n         il.add_value('name', u'marta')\n         self.assertEqual(il.get_output_value('name'), [u'Marta'])\n \n-    def test_empty_map_concat(self):\n+    def test_empty_map_compose(self):\n         class IdentityDefaultedItemLoader(DefaultedItemLoader):\n             name_in = MapCompose()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#314e8dea4303bf8a7d92a484bb9438116c8a08f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 73 | Files Changed: 3 | Hunks: 47 | Methods Changed: 16 | Complexity Δ (Sum/Max): 21/18 | Churn Δ: 176 | Churn Cumulative: 363 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ class BaseItemExporter(object):\n     fields_to_export = None\n     export_empty_fields = False\n \n-    def export(self, item):\n+    def export_item(self, item):\n         raise NotImplementedError\n \n     def serialize(self, field, name, value):\n@@ -43,9 +43,13 @@ class BaseItemExporter(object):\n                     nonempty_fields)\n         return [(k, item.get(k, default_value)) for k in field_iter]\n \n-    def close(self):\n+    def start_exporting(self):\n         pass\n \n+    def finish_exporting(self):\n+        pass\n+\n+\n \n class XmlItemExporter(BaseItemExporter):\n \n@@ -55,15 +59,18 @@ class XmlItemExporter(BaseItemExporter):\n     def __init__(self, file):\n         super(XmlItemExporter, self).__init__()\n         self.xg = XMLGenerator(file)\n+\n+    def start_exporting(self):\n         self.xg.startDocument()\n         self.xg.startElement(self.root_element, {})\n \n-    def export(self, item):\n+    def export_item(self, item):\n         self.xg.startElement(self.item_element, {})\n         for field, value in self._get_fields_to_export(item, default_value=''):\n             self._export_xml_field(item.fields[field], field, value)\n+        self.xg.endElement(self.item_element)\n \n-    def close(self):\n+    def finish_exporting(self):\n         self.xg.endElement(self.root_element)\n         self.xg.endDocument()\n \n@@ -81,15 +88,18 @@ class CsvItemExporter(BaseItemExporter):\n     def __init__(self, *args, **kwargs):\n         super(CsvItemExporter, self).__init__()\n         self.csv_writer = csv.writer(*args, **kwargs)\n+\n+    def start_exporting(self):\n         if self.include_headers_line:\n             if not self.fields_to_export:\n-                raise RuntimeError(\"To use include_headers_line you must \" \\\n-                    \"define fields_to_export attribute\")\n+                raise RuntimeError(\"You must set fields_to_export in order to\" + \\\n+                                   \" use include_headers_line\")\n             self.csv_writer.writerow(self.fields_to_export)\n \n-    def export(self, item):\n+    def export_item(self, item):\n         fields = self._get_fields_to_export(item, default_value='', \\\n             include_empty=True)\n+\n         values = [x[1] for x in fields]\n         self.csv_writer.writerow(values)\n \n@@ -100,7 +110,7 @@ class PickleItemExporter(BaseItemExporter):\n         super(PickleItemExporter, self).__init__()\n         self.pickler = Pickler(*args, **kwargs)\n \n-    def export(self, item):\n+    def export_item(self, item):\n         self.pickler.dump(dict(self._get_fields_to_export(item)))\n \n \n@@ -110,6 +120,6 @@ class PprintItemExporter(BaseItemExporter):\n         super(PprintItemExporter, self).__init__()\n         self.file = file\n \n-    def export(self, item):\n+    def export_item(self, item):\n         itemdict = dict(self._get_fields_to_export(item))\n         self.file.write(pprint.pformat(itemdict) + '\\n')\n\n@@ -14,4 +14,4 @@ class JsonLinesItemExporter(BaseItemExporter):\n \n     def export(self, item):\n         itemdict = dict(self._get_fields_to_export(item))\n-        self.file.write(self.encoder(itemdict) + '\\n')\n+        self.file.write(self.encoder.encode(itemdict) + '\\n')\n\n@@ -5,8 +5,6 @@ from twisted.trial import unittest\n \n from scrapy.newitem import Item, Field\n \n-# FIXME: fix tests\n-\"\"\"\n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n     PickleItemExporter, CsvItemExporter, XmlItemExporter\n \n@@ -15,108 +13,128 @@ class TestItem(Item):\n     age = Field()\n \n \n-class BaseItemExporterTest(unittest.TestCase):\n+class BaseTest(unittest.TestCase):\n+    def setUp(self):\n+        self.i = TestItem(name=u'John', age='22')\n+        self.ie = BaseItemExporter()\n+\n+        self.output = StringIO()\n+\n+class BaseItemExporterTest(BaseTest):\n         \n     def test_export(self):\n-        i = TestItem(name=u'John', age=22)\n+        self.assertRaises(NotImplementedError, self.ie.export_item, self.i)\n \n-        ie = BaseItemExporter()\n-\n-        self.assertRaises(NotImplementedError, ie.export, i)\n-\n-    def test_default_serializer(self):\n-        i = TestItem(name=u'John', age=22)\n-\n-        ie = BaseItemExporter()\n-\n-        self.assertEqual(ie._serialize_field(i.fields['name'], 'name', i['name']), 'John')\n-        self.assertEqual( ie._serialize_field(i.fields['age'], 'age', i['age']), '22')\n+    def test_serialize(self):\n+        self.assertEqual(self.ie.serialize( \\\n+            self.i.fields['name'], 'name', self.i['name']), 'John')\n+        self.assertEqual( \\\n+            self.ie.serialize(self.i.fields['age'], 'age', self.i['age']), '22')\n \n     def test_exporter_custom_serializer(self):\n         class CustomItemExporter(BaseItemExporter):\n-            def serialize_age(self, field, name, value):\n-                return str(value + 1)\n-\n-        i = TestItem(name=u'John', age=22)\n+            def serialize(self, field, name, value):\n+                if name == 'age':\n+                    return str(int(value) + 1)\n+                else:\n+                    return super(CustomItemExporter, self).serialize(field, \\\n+                        name, value)\n \n         ie = CustomItemExporter()\n \n-        self.assertEqual(ie._serialize_field(i.fields['name'], 'name', i['name']), 'John')\n-        self.assertEqual(ie._serialize_field(i.fields['age'], 'age', i['age']), '23')\n+        self.assertEqual( \\\n+            ie.serialize(self.i.fields['name'], 'name', self.i['name']), 'John')\n+        self.assertEqual(\n+            ie.serialize(self.i.fields['age'], 'age', self.i['age']), '23')\n \n     def test_field_custom_serializer(self):\n-        class CustomField(Field):\n-            def serializer(self, field, name, value):\n-                return str(value + 2)\n+        def custom_serializer(value):\n+            return str(int(value) + 2)\n \n         class CustomFieldItem(Item):\n             name = Field()\n-            age = CustomField()\n+            age = Field(serializer=custom_serializer)\n \n-        i = CustomFieldItem(name=u'John', age=22)\n+        i = CustomFieldItem(name=u'John', age='22')\n \n+        self.assertEqual( \\\n+            self.ie.serialize(i.fields['name'], 'name', i['name']), 'John')\n+        self.assertEqual( \\\n+            self.ie.serialize(i.fields['age'], 'age', i['age']), '24')\n+\n+    def test_fields_to_export(self):\n         ie = BaseItemExporter()\n+        ie.fields_to_export = ['name']\n \n-        self.assertEqual(ie._serialize_field(i.fields['name'], 'name', i['name']), 'John')\n-        self.assertEqual(ie._serialize_field(i.fields['age'], 'age', i['age']), '24')\n+        self.assertEqual(ie._get_fields_to_export(self.i), [('name', 'John')])\n \n \n-class PprintItemExporterTest(unittest.TestCase):\n+class PprintItemExporterTest(BaseTest):\n \n     def test_export(self):\n-        i = TestItem(name=u'John', age=22)\n+        ie = PprintItemExporter(self.output)\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n \n-        output = StringIO()\n-        ie = PprintItemExporter(output)\n-        ie.export(i)\n-\n-        self.assertEqual(output.getvalue(), \"{'age': 22, 'name': u'John'}\\n\")\n+        self.assertEqual(self.output.getvalue(), \"{'age': '22', 'name': u'John'}\\n\")\n \n \n-class PickleItemExporterTest(unittest.TestCase):\n+class PickleItemExporterTest(BaseTest):\n         \n     def test_export(self):\n-        i = TestItem(name=u'John', age=22)\n-\n         output = StringIO()\n         ie = PickleItemExporter(output)\n-        ie.export(i)\n+\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n \n         poutput = StringIO()\n         p = Pickler(poutput)\n-        p.dump(dict(i))\n+        p.dump(dict(self.i))\n         \n         self.assertEqual(output.getvalue(), poutput.getvalue())\n \n \n-class CsvItemExporterTest(unittest.TestCase):\n+class CsvItemExporterTest(BaseTest):\n \n     def test_export(self):\n-        i = TestItem(name=u'John', age=22)\n+        ie = CsvItemExporter(self.output)\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n \n-        output = StringIO()\n-        ie = CsvItemExporter(output)\n-        ie.fields_to_export = i.fields.keys()\n-        ie.export(i)\n+        self.assertEqual(self.output.getvalue(), '22,John\\r\\n')\n \n-        self.assertEqual(output.getvalue(), 'age,name\\r\\n22,John\\r\\n')\n+    def test_header(self):\n+        ie = CsvItemExporter(self.output)\n+        ie.include_headers_line = True\n+\n+        self.assertRaises(RuntimeError, ie.start_exporting)\n+\n+        ie.fields_to_export = self.i.fields.keys()\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n+\n+        self.assertEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\r\\n')\n \n \n-class XmlItemExporterTest(unittest.TestCase):\n+class XmlItemExporterTest(BaseTest):\n \n     def test_export(self):\n-        i = TestItem(name=u'John', age=22)\n+        ie = XmlItemExporter(self.output)\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n \n-        output = StringIO()\n-        ie = XmlItemExporter(output)\n-        ie.fields_to_export = i.fields.keys()\n-        ie.export(i)\n+        expected_value = '<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\\n<items><item><age>22</age><name>John</name></item></items>'\n \n-        self.assertEqual(output.getvalue(), \\\n-            '<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\\n<items><item><age>22</age><name>John</name></item>')\n+        self.assertEqual(self.output.getvalue(), expected_value)\n \n \n-class JSONItemExporterTest(unittest.TestCase):\n+class JSONItemExporterTest(BaseTest):\n \n     def setUp(self):\n         try:\n@@ -127,17 +145,19 @@ class JSONItemExporterTest(unittest.TestCase):\n             except ImportError:\n                 raise unittest.SkipTest(\"simplejson module not available\") \n \n+        from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n+\n+        super(JSONItemExporterTest, self).__init__()\n+\n     def test_export(self):\n-        from scrapy.contrib.exporter.jsonexporter import JSONItemExporter\n \n-        output = StringIO()\n-        ie = JSONItemExporter(output)\n-        i = TestItem(name=u'John', age=22)\n-        ie.export(i)\n+        ie = JsonLinesItemExporter(output)\n+        ie.start_exporting()\n+        ie.export_item(self.i)\n+        ie.finish_exporting()\n \n-        self.assertEqual(output.getvalue(), '{\"age\": 22, \"name\": \"John\"}\\n')\n+        self.assertEqual(output.getvalue(), '{\"age\": \"22\", \"name\": \"John\"}\\n')\n \n-\"\"\"\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8504a054cf1ab8e5887561f79101bac9803aad4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 240 | Lines Deleted: 226 | Files Changed: 14 | Hunks: 24 | Methods Changed: 46 | Complexity Δ (Sum/Max): 19/18 | Churn Δ: 466 | Churn Cumulative: 2762 | Contributors (this commit): 4 | Commits (past 90d): 93 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,11 @@\n-# Define here the models for your scraped items\n+from scrapy.item import Item, Field\n \n-from scrapy.item import ScrapedItem\n+class GoogledirItem(Item):\n \n-class GoogledirItem(ScrapedItem):\n+    name = Field()\n+    url = Field()\n+    description = Field()\n \n     def __str__(self):\n-        return \"Google Category: name=%s url=%s\" % (self.name, self.url)\n+        return \"Google Category: name=%s url=%s\" % (self['name'], self['url'])\n \n\n@@ -9,7 +9,7 @@ class FilterWordsPipeline(object):\n \n     def process_item(self, domain, item):\n         for word in self.words_to_filter:\n-            if word in unicode(item.description).lower():\n+            if word in unicode(item['description']).lower():\n                 raise DropItem(\"Contains forbidden word: %s\" % word)\n         else:\n             return item\n\n@@ -25,9 +25,9 @@ class GoogleDirectorySpider(CrawlSpider):\n         for link in links:\n             item = GoogledirItem()\n \n-            item.name = link.select('a/text()').extract()\n-            item.url = link.select('a/@href').extract()\n-            item.description = link.select('font[2]/text()').extract()\n+            item['name'] = link.select('a/text()').extract()\n+            item['url'] = link.select('a/@href').extract()\n+            item['description'] = link.select('font[2]/text()').extract()\n             yield item\n \n SPIDER = GoogleDirectorySpider()\n\n@@ -30,7 +30,7 @@ CONCURRENT_ITEMS = 100\n \n COOKIES_DEBUG = False\n \n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n+DEFAULT_ITEM_CLASS = 'scrapy.item.Item'\n \n DEFAULT_REQUEST_HEADERS = {\n     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/loaders.rst\n \n from collections import defaultdict\n \n-from scrapy.newitem import Item\n+from scrapy.item import Item\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.utils.misc import arg_to_iter\n from .common import wrap_loader_context\n\n@@ -1,3 +1,11 @@\n+\"\"\"\n+Scrapy Item\n+\n+See documentation in docs/topics/item.rst\n+\"\"\"\n+\n+from UserDict import DictMixin\n+\n from scrapy.utils.trackref import object_ref\n \n class BaseItem(object_ref):\n@@ -5,6 +13,74 @@ class BaseItem(object_ref):\n     pass\n \n \n+class Field(dict):\n+    \"\"\"Container of field metadata\"\"\"\n+\n+\n+class _ItemMeta(type):\n+\n+    def __new__(mcs, class_name, bases, attrs):\n+        fields = {}\n+        new_attrs = {}\n+        for n, v in attrs.iteritems():\n+            if isinstance(v, Field):\n+                fields[n] = v\n+            else:\n+                new_attrs[n] = v\n+\n+        cls = type.__new__(mcs, class_name, bases, new_attrs)\n+        cls.fields = cls.fields.copy()\n+        cls.fields.update(fields)\n+        return cls\n+\n+\n+class Item(DictMixin, BaseItem):\n+\n+    __metaclass__ = _ItemMeta\n+\n+    fields = {}\n+\n+    def __init__(self, *args, **kwargs):\n+        self._values = {}\n+        if args or kwargs: # avoid creating dict for most common case\n+            for k, v in dict(*args, **kwargs).iteritems():\n+                self[k] = v\n+\n+    def __getitem__(self, key):\n+        try:\n+            return self._values[key]\n+        except KeyError:\n+            field = self.fields[key]\n+            if 'default' in field:\n+                return field['default']\n+            raise\n+\n+    def __setitem__(self, key, value):\n+        if key in self.fields:\n+            self._values[key] = value\n+        else:\n+            raise KeyError(\"%s does not support field: %s\" % \\\n+                (self.__class__.__name__, key))\n+\n+    def __delitem__(self, key):\n+        del self._values[key]\n+\n+    def __getattr__(self, name):\n+        if name in self.fields:\n+            raise AttributeError(\"Use [%r] to access item field value\" % name)\n+        raise AttributeError(name)\n+\n+    def keys(self):\n+        return self._values.keys()\n+\n+    def __repr__(self):\n+        \"\"\"Generate a representation of this item that can be used to\n+        reconstruct the item by evaluating it\n+        \"\"\"\n+        values = ', '.join('%s=%r' % field for field in self.iteritems())\n+        return \"%s(%s)\" % (self.__class__.__name__, values)\n+\n+\n class ScrapedItem(BaseItem):\n \n     def __init__(self, data=None):\n@@ -12,6 +88,9 @@ class ScrapedItem(BaseItem):\n         A ScrapedItem can be initialised with a dictionary that will be\n         squirted directly into the object.\n         \"\"\"\n+        import warnings\n+        warnings.warn(\"scrapy.item.ScrapedItem is deprecated, use scrapy.item.Item instead\",\n+            DeprecationWarning, stacklevel=2)\n         if isinstance(data, dict):\n             for attr, value in data.iteritems():\n                 setattr(self, attr, value)\n\n@@ -1,69 +0,0 @@\n-from UserDict import DictMixin\n-\n-from scrapy.item import BaseItem\n-\n-class Field(dict):\n-    \"\"\"Container of field metadata\"\"\"\n-\n-class _ItemMeta(type):\n-\n-    def __new__(mcs, class_name, bases, attrs):\n-        fields = {}\n-        new_attrs = {}\n-        for n, v in attrs.iteritems():\n-            if isinstance(v, Field):\n-                fields[n] = v\n-            else:\n-                new_attrs[n] = v\n-\n-        cls = type.__new__(mcs, class_name, bases, new_attrs)\n-        cls.fields = cls.fields.copy()\n-        cls.fields.update(fields)\n-        return cls\n-\n-\n-class Item(DictMixin, BaseItem):\n-\n-    __metaclass__ = _ItemMeta\n-\n-    fields = {}\n-\n-    def __init__(self, *args, **kwargs):\n-        self._values = {}\n-        if args or kwargs: # avoid creating dict for most common case\n-            for k, v in dict(*args, **kwargs).iteritems():\n-                self[k] = v\n-\n-    def __getitem__(self, key):\n-        try:\n-            return self._values[key]\n-        except KeyError:\n-            field = self.fields[key]\n-            if 'default' in field:\n-                return field['default']\n-            raise\n-\n-    def __setitem__(self, key, value):\n-        if key in self.fields:\n-            self._values[key] = value\n-        else:\n-            raise KeyError(\"%s does not support field: %s\" % \\\n-                (self.__class__.__name__, key))\n-\n-    def __delitem__(self, key):\n-        del self._values[key]\n-\n-    def __getattr__(self, name):\n-        if name in self.fields:\n-            raise AttributeError(\"Use [%r] to access item field value\" % name)\n-        raise AttributeError(name)\n-\n-    def keys(self):\n-        return self._values.keys()\n-\n-    def __repr__(self):\n-        \"\"\"Generate a representation of this item that can be used to\n-        reconstruct the item by evaluating it\n-        \"\"\"\n-        values = ', '.join('%s=%r' % field for field in self.iteritems())\n-        return \"%s(%s)\" % (self.__class__.__name__, values)\n\n@@ -3,7 +3,7 @@ from cStringIO import StringIO\n \n from twisted.trial import unittest\n \n-from scrapy.newitem import Item, Field\n+from scrapy.item import Item, Field\n \n from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \\\n     PickleItemExporter, CsvItemExporter, XmlItemExporter\n\n@@ -3,7 +3,7 @@ import unittest\n from scrapy.contrib.loader import ItemLoader, XPathItemLoader\n from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \\\n     Compose, MapCompose\n-from scrapy.newitem import Item, Field\n+from scrapy.item import Item, Field\n from scrapy.xpath import HtmlXPathSelector\n from scrapy.http import HtmlResponse\n \n\n@@ -157,13 +157,13 @@ class EngineTest(unittest.TestCase):\n         # item tests\n         self.assertEqual(2, len(session.itemresp))\n         for item, response in session.itemresp:\n-            self.assertEqual(item.url, response.url)\n-            if 'item1.html' in item.url:\n-                self.assertEqual('Item 1 name', item.name)\n-                self.assertEqual('100', item.price)\n-            if 'item2.html' in item.url:\n-                self.assertEqual('Item 2 name', item.name)\n-                self.assertEqual('200', item.price)\n+            self.assertEqual(item['url'], response.url)\n+            if 'item1.html' in item['url']:\n+                self.assertEqual('Item 1 name', item['name'])\n+                self.assertEqual('100', item['price'])\n+            if 'item2.html' in item['url']:\n+                self.assertEqual('Item 2 name', item['name'])\n+                self.assertEqual('200', item['price'])\n \n     def test_signals(self):\n         \"\"\"\n\n@@ -1,8 +1,136 @@\n import unittest\n \n-from scrapy.item import ScrapedItem\n+from scrapy.item import Item, Field, ScrapedItem\n \n-class ItemTestCase(unittest.TestCase):\n+\n+class ItemTest(unittest.TestCase):\n+\n+    def test_simple(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+        i = TestItem()\n+        i['name'] = u'name'\n+        self.assertEqual(i['name'], u'name')\n+\n+    def test_init(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+        i = TestItem()\n+        self.assertRaises(KeyError, i.__getitem__, 'name')\n+\n+        i2 = TestItem(name=u'john doe')\n+        self.assertEqual(i2['name'], u'john doe')\n+\n+        i3 = TestItem({'name': u'john doe'})\n+        self.assertEqual(i3['name'], u'john doe')\n+\n+        i4 = TestItem(i3)\n+        self.assertEqual(i4['name'], u'john doe')\n+\n+        self.assertRaises(KeyError, TestItem, {'name': u'john doe',\n+                                               'other': u'foo'})\n+\n+    def test_invalid_field(self):\n+        class TestItem(Item):\n+            pass\n+\n+        i = TestItem()\n+        self.assertRaises(KeyError, i.__setitem__, 'field', 'text')\n+        self.assertRaises(KeyError, i.__getitem__, 'field')\n+\n+    def test_default_value(self):\n+        class TestItem(Item):\n+            name = Field(default=u'John')\n+ \n+        i = TestItem()\n+        self.assertEqual(i['name'], u'John')\n+\n+    def test_repr(self):\n+        class TestItem(Item):\n+            name = Field()\n+            number = Field()\n+\n+        i = TestItem()\n+        i['name'] = u'John Doe'\n+        i['number'] = 123\n+        itemrepr = repr(i)\n+        self.assertEqual(itemrepr,\n+                         \"TestItem(name=u'John Doe', number=123)\")\n+\n+        i2 = eval(itemrepr)\n+        self.assertEqual(i2['name'], 'John Doe')\n+        self.assertEqual(i2['number'], 123)\n+\n+    def test_private_attr(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+        i = TestItem()\n+        i._private = 'test'\n+        self.assertEqual(i._private, 'test')\n+\n+    def test_custom_methods(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+            def get_name(self):\n+                return self['name']\n+\n+            def change_name(self, name):\n+                self['name'] = name\n+\n+        i = TestItem()\n+        self.assertRaises(KeyError, i.get_name)\n+        i['name'] = u'lala'\n+        self.assertEqual(i.get_name(), u'lala')\n+        i.change_name(u'other')\n+        self.assertEqual(i.get_name(), 'other')\n+\n+    def test_metaclass(self):\n+        class TestItem(Item):\n+            name = Field()\n+            keys = Field() \n+            values = Field() \n+\n+        i = TestItem()\n+        i['name'] = u'John'\n+        self.assertEqual(i.keys(), ['name'])\n+        self.assertEqual(i.values(), ['John'])\n+\n+        i['keys'] = u'Keys'\n+        i['values'] = u'Values'\n+        self.assertEqual(i.keys(), ['keys', 'values', 'name'])\n+        self.assertEqual(i.values(), [u'Keys', u'Values', u'John'])\n+\n+    def test_metaclass_inheritance(self):\n+        class BaseItem(Item):\n+            name = Field()\n+            keys = Field() \n+            values = Field() \n+\n+        class TestItem(BaseItem):\n+            keys = Field()\n+\n+        i = TestItem()\n+        i['keys'] = 3\n+        self.assertEqual(i.keys(), ['keys'])\n+        self.assertEqual(i.values(), [3])\n+\n+    def test_to_dict(self):\n+        class TestItem(Item):\n+            name = Field()\n+\n+        i = TestItem()\n+        i['name'] = u'John'\n+        self.assertEqual(dict(i), {'name': u'John'})\n+\n+\n+# NOTE: ScrapedItem is deprecated and will be removed in the next stable\n+# release, and so will these tests.\n+\n+class ScrapedItemTestCase(unittest.TestCase):\n \n     def test_item(self):\n         \n\n@@ -1,130 +0,0 @@\n-import datetime\n-import decimal\n-import unittest\n-\n-from scrapy.newitem import Item, Field\n-\n-\n-class NewItemTest(unittest.TestCase):\n-\n-    def test_simple(self):\n-        class TestItem(Item):\n-            name = Field()\n-\n-        i = TestItem()\n-        i['name'] = u'name'\n-        self.assertEqual(i['name'], u'name')\n-\n-    def test_init(self):\n-        class TestItem(Item):\n-            name = Field()\n-\n-        i = TestItem()\n-        self.assertRaises(KeyError, i.__getitem__, 'name')\n-\n-        i2 = TestItem(name=u'john doe')\n-        self.assertEqual(i2['name'], u'john doe')\n-\n-        i3 = TestItem({'name': u'john doe'})\n-        self.assertEqual(i3['name'], u'john doe')\n-\n-        i4 = TestItem(i3)\n-        self.assertEqual(i4['name'], u'john doe')\n-\n-        self.assertRaises(KeyError, TestItem, {'name': u'john doe',\n-                                               'other': u'foo'})\n-\n-    def test_invalid_field(self):\n-        class TestItem(Item):\n-            pass\n-\n-        i = TestItem()\n-        self.assertRaises(KeyError, i.__setitem__, 'field', 'text')\n-        self.assertRaises(KeyError, i.__getitem__, 'field')\n-\n-    def test_default_value(self):\n-        class TestItem(Item):\n-            name = Field(default=u'John')\n- \n-        i = TestItem()\n-        self.assertEqual(i['name'], u'John')\n-\n-    def test_repr(self):\n-        class TestItem(Item):\n-            name = Field()\n-            number = Field()\n-\n-        i = TestItem()\n-        i['name'] = u'John Doe'\n-        i['number'] = 123\n-        itemrepr = repr(i)\n-        self.assertEqual(itemrepr,\n-                         \"TestItem(name=u'John Doe', number=123)\")\n-\n-        i2 = eval(itemrepr)\n-        self.assertEqual(i2['name'], 'John Doe')\n-        self.assertEqual(i2['number'], 123)\n-\n-    def test_private_attr(self):\n-        class TestItem(Item):\n-            name = Field()\n-\n-        i = TestItem()\n-        i._private = 'test'\n-        self.assertEqual(i._private, 'test')\n-\n-    def test_custom_methods(self):\n-        class TestItem(Item):\n-            name = Field()\n-\n-            def get_name(self):\n-                return self['name']\n-\n-            def change_name(self, name):\n-                self['name'] = name\n-\n-        i = TestItem()\n-        self.assertRaises(KeyError, i.get_name)\n-        i['name'] = u'lala'\n-        self.assertEqual(i.get_name(), u'lala')\n-        i.change_name(u'other')\n-        self.assertEqual(i.get_name(), 'other')\n-\n-    def test_metaclass(self):\n-        class TestItem(Item):\n-            name = Field()\n-            keys = Field() \n-            values = Field() \n-\n-        i = TestItem()\n-        i['name'] = u'John'\n-        self.assertEqual(i.keys(), ['name'])\n-        self.assertEqual(i.values(), ['John'])\n-\n-        i['keys'] = u'Keys'\n-        i['values'] = u'Values'\n-        self.assertEqual(i.keys(), ['keys', 'values', 'name'])\n-        self.assertEqual(i.values(), [u'Keys', u'Values', u'John'])\n-\n-    def test_metaclass_inheritance(self):\n-        class BaseItem(Item):\n-            name = Field()\n-            keys = Field() \n-            values = Field() \n-\n-        class TestItem(BaseItem):\n-            keys = Field()\n-\n-        i = TestItem()\n-        i['keys'] = 3\n-        self.assertEqual(i.keys(), ['keys'])\n-        self.assertEqual(i.values(), [3])\n-\n-    def test_to_dict(self):\n-        class TestItem(Item):\n-            name = Field()\n-\n-        i = TestItem()\n-        i['name'] = u'John'\n-        self.assertEqual(dict(i), {'name': u'John'})\n-\n\n@@ -6,10 +6,15 @@ See scrapy/tests/test_engine.py for more info.\n import re\n \n from scrapy.spider import BaseSpider\n-from scrapy.item import ScrapedItem\n+from scrapy.item import Item, Field\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.http import Request\n \n+class TestItem(Item):\n+    name = Field()\n+    url = Field()\n+    price = Field()\n+\n class TestSpider(BaseSpider):\n     domain_name = \"scrapytest.org\"\n     extra_domain_names = [\"localhost\"]\n@@ -27,14 +32,14 @@ class TestSpider(BaseSpider):\n                 yield Request(url=link.url, callback=self.parse_item)\n \n     def parse_item(self, response):\n-        item = ScrapedItem()\n+        item = TestItem()\n         m = self.name_re.search(response.body)\n         if m:\n-            item.name = m.group(1)\n-        item.url = response.url\n+            item['name'] = m.group(1)\n+        item['url'] = response.url\n         m = self.price_re.search(response.body)\n         if m:\n-            item.price = m.group(1)\n+            item['price'] = m.group(1)\n         return [item]\n         \n \n\n@@ -2,7 +2,6 @@ import unittest\n from cStringIO import StringIO\n \n from scrapy.utils.misc import load_object, arg_to_iter\n-from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
