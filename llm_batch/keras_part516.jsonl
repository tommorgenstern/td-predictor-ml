{"custom_id": "keras#81a8a813c39d6f3402b3ff0bcf2ca3541f9f6e9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2041 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -116,13 +116,13 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n           - `\"lower\"`: Text will be lowercased.\n           - `\"strip_punctuation\"`: All punctuation will be removed.\n           - Callable: Inputs will passed to the callable function, which should\n-            standardized and returned.\n+            be standardized and returned.\n       split: Optional specification for splitting the input text. Values can be:\n           - `None`: No splitting.\n           - `\"whitespace\"`: Split on whitespace.\n           - `\"character\"`: Split on each unicode character.\n           - Callable: Standardized inputs will passed to the callable function,\n-            which should split and returned.\n+            which should be split and returned.\n       ngrams: Optional specification for ngrams to create from the\n         possibly-split input text. Values can be None, an integer or tuple of\n         integers; passing an integer will create ngrams up to that integer, and\n@@ -159,11 +159,11 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         max_tokens)` regardless of vocabulary size. Defaults to False.\n       vocabulary: Optional. Either an array of strings or a string path to a\n         text file. If passing an array, can pass a tuple, list, 1D numpy array,\n-        or 1D tensor containing the string vocbulary terms. If passing a file\n+        or 1D tensor containing the string vocabulary terms. If passing a file\n         path, the file should contain one line per term in the vocabulary. If\n         this argument is set, there is no need to `adapt()` the layer.\n       idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n-        1D numpy array, or 1D tensor or the same length as the vocabulary,\n+        1D numpy array, or 1D tensor of the same length as the vocabulary,\n         containing the floating point inverse document frequency weights, which\n         will be multiplied by per sample term counts for the final `tf_idf`\n         weight. If the `vocabulary` argument is set, and `output_mode` is\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09c2eaaf26f301d724125690a38ad404e9a54782", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 12 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 1071 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,8 +17,8 @@\n import timeit\n \n import numpy as np\n-import tensorflow.compat.v2 as tf\n \n+from keras import callbacks\n from keras.benchmarks import distribution_util\n \n \n@@ -72,7 +72,7 @@ def get_keras_examples_metadata(\n     }\n \n \n-class TimerCallBack(tf.keras.callbacks.Callback):\n+class TimerCallBack(callbacks.Callback):\n     \"\"\"Callback for logging time in each epoch or batch.\"\"\"\n \n     def __init__(self):\n\n@@ -21,11 +21,12 @@ To add a new model for memory profile:\n \"\"\"\n \n import numpy as np\n-import tensorflow.compat.v2 as tf\n from absl import app\n from absl import flags\n from absl import logging\n \n+import keras\n+\n try:\n     import memory_profiler\n except ImportError:\n@@ -43,10 +44,10 @@ def _imdb_lstm_model():\n     y_train = np.random.random((2500, 1))\n \n     # IMDB LSTM model.\n-    model = tf.keras.Sequential()\n-    model.add(tf.keras.layers.Embedding(20000, 128))\n-    model.add(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n-    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n+    model = keras.Sequential()\n+    model.add(keras.layers.Embedding(20000, 128))\n+    model.add(keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n+    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n \n     model.compile(\"sgd\", \"mse\")\n     # Warm up the model with one epoch.\n\n@@ -23,6 +23,8 @@ import time\n \n import tensorflow.compat.v2 as tf\n \n+import keras\n+\n \n def save_and_load_benchmark(app):\n     \"\"\"Util for saved model benchmarks.\"\"\"\n@@ -40,7 +42,7 @@ def save_and_load_benchmark(app):\n \n     # Run one untimed iteration of saving/loading.\n     model.save(save_dir, save_format=\"tf\")\n-    tf.keras.models.load_model(save_dir)\n+    keras.models.load_model(save_dir)\n \n     for _ in range(trials):\n         start_time = time.time()\n@@ -48,7 +50,7 @@ def save_and_load_benchmark(app):\n         total_save_time += time.time() - start_time\n \n         start_time = time.time()\n-        tf.keras.models.load_model(save_dir)\n+        keras.models.load_model(save_dir)\n         total_load_time += time.time() - start_time\n \n     save_result = {\n\n@@ -54,14 +54,12 @@ TMP_TEST_DIRNAME = \"keras_test\"\n VERBOSE = True\n INIT_FILE_HEADER = \"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\n # These are symbols that have export issues and that we skip for now.\n-SYMBOLS_TO_SKIP = [\"layer_test\"]\n+SYMBOLS_TO_SKIP = []\n \n \n def copy_keras_codebase(source_dir, target_dir):\n     disallowed = [\n-        \"benchmarks\",\n         \"tools\",\n-        \"tests\",\n         \"integration_test\",\n     ]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89ba3e45094943931d577c9c21194aaec1764f97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 171 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 171 | Churn Cumulative: 986 | Contributors (this commit): 14 | Commits (past 90d): 15 | Contributors (cumulative): 21 | DMM Complexity: 0.5882352941176471\n\nDIFF:\n@@ -32,6 +32,7 @@ from keras.optimizers import adam\n from keras.optimizers import adamax\n from keras.optimizers import adamw\n from keras.optimizers import ftrl\n+from keras.optimizers import lion\n from keras.optimizers import nadam\n from keras.optimizers import optimizer as base_optimizer\n from keras.optimizers import rmsprop\n\n@@ -0,0 +1,160 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Lion optimizer implementation.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.optimizers import optimizer\n+from keras.saving.object_registration import register_keras_serializable\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@register_keras_serializable()\n+@keras_export(\n+    \"keras.optimizers.experimental.Lion\",\n+    \"keras.optimizers.Lion\",\n+    v1=[],\n+)\n+class Lion(optimizer.Optimizer):\n+    r\"\"\"Optimizer that implements the Lion algorithm.\n+\n+    The Lion optimizer is a stochastic-gradient-descent method that uses the\n+    sign operator to control the magnitude of the update, unlike other adaptive\n+    optimizers such as Adam that also rely on second-order moments. This make\n+    Lion more memory-efficient as it only keeps track of the momentum. According\n+    to the authors (see reference), its performance gain over Adam grows with\n+    the training batch size.\n+\n+    Args:\n+      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n+        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        learning rate. Defaults to 0.0001.\n+      beta_1: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The rate\n+        to combine the current gradient and the 1st moment estimate.\n+      beta_2: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 1st moment estimate.\n+    {{base_optimizer_keyword_args}}\n+\n+    References:\n+      - [Chen et al., 2023](http://arxiv.org/abs/2302.06675)\n+      - [Authors implementation](\\\n+          http://github.com/google/automl/tree/master/lion)\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.0001,\n+        beta_1=0.9,\n+        beta_2=0.99,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        jit_compile=True,\n+        name=\"Lion\",\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+            jit_compile=jit_compile,\n+            **kwargs,\n+        )\n+        self._learning_rate = self._build_learning_rate(learning_rate)\n+        self.beta_1 = beta_1\n+        if isinstance(beta_1, (int, float)) and (beta_1 < 0 or beta_1 > 1):\n+            raise ValueError(\"`beta_1` must be between [0, 1].\")\n+        self.beta_2 = beta_2\n+        if isinstance(beta_2, (int, float)) and (beta_2 < 0 or beta_2 > 1):\n+            raise ValueError(\"`beta_2` must be between [0, 1].\")\n+\n+    def build(self, var_list):\n+        super().build(var_list)\n+        if hasattr(self, \"_built\") and self._built:\n+            return\n+        self.momentums = []\n+        for var in var_list:\n+            self.momentums.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"m\"\n+                )\n+            )\n+        self._built = True\n+\n+    def update_step(self, gradient, variable):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        lr = tf.cast(self.learning_rate, variable.dtype)\n+        beta_1 = tf.cast(self.beta_1, variable.dtype)\n+        beta_2 = tf.cast(self.beta_2, variable.dtype)\n+        var_key = self._var_key(variable)\n+        m = self.momentums[self._index_dict[var_key]]\n+\n+        if isinstance(gradient, tf.IndexedSlices):\n+            # Sparse gradients\n+            m_t = m.assign(m * beta_1)\n+            m_scaled_g_values = tf.IndexedSlices(\n+                gradient.values * (1.0 - beta_1), gradient.indices\n+            )\n+            m_t = m_t.scatter_add(m_scaled_g_values)\n+            variable_t = variable.assign_sub(lr * tf.math.sign(m_t))\n+\n+            with tf.control_dependencies([variable_t]):\n+                m_t = m_t.scatter_sub(m_scaled_g_values)\n+                m_t = m_t.assign(m_t * beta_2 / beta_1)\n+                m_scaled_g_values = tf.IndexedSlices(\n+                    gradient.values * (1.0 - beta_2), gradient.indices\n+                )\n+                m_t.scatter_add(m_scaled_g_values)\n+        else:\n+            # Dense gradients\n+            variable_t = variable.assign_sub(\n+                lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))\n+            )\n+            with tf.control_dependencies([variable_t]):\n+                m.assign(m * beta_2 + gradient * (1.0 - beta_2))\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"learning_rate\": self._serialize_hyperparameter(\n+                    self._learning_rate\n+                ),\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+            }\n+        )\n+        return config\n+\n+\n+Lion.__doc__ = Lion.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -10,6 +10,7 @@ from keras.optimizers import adam\n from keras.optimizers import adamax\n from keras.optimizers import adamw\n from keras.optimizers import ftrl\n+from keras.optimizers import lion\n from keras.optimizers import nadam\n from keras.optimizers import rmsprop\n from keras.optimizers import sgd\n@@ -44,6 +45,9 @@ adamw_fn = tf.__internal__.test.combinations.NamedObject(\n ftrl_fn = tf.__internal__.test.combinations.NamedObject(\n     \"ftrl\", lambda: ftrl.Ftrl(0.002)\n )\n+lion_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"lion\", lambda: lion.Lion(0.002)\n+)\n nadam_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentnadam\", lambda: nadam.Nadam(0.002)\n )\n@@ -62,6 +66,7 @@ OPTIMIZER_FN = [\n     adamax_fn,\n     adamw_fn,\n     ftrl_fn,\n+    lion_fn,\n     nadam_fn,\n     rmsprop_fn,\n     sgd_fn,\n\n@@ -18,6 +18,7 @@ from keras.optimizers import adam as adam_new\n from keras.optimizers import adamax as adamax_new\n from keras.optimizers import adamw as adamw_new\n from keras.optimizers import ftrl as ftrl_new\n+from keras.optimizers import lion as lion_new\n from keras.optimizers import nadam as nadam_new\n from keras.optimizers import rmsprop as rmsprop_new\n from keras.optimizers import sgd as sgd_new\n@@ -69,6 +70,9 @@ adamw_new_fn = tf.__internal__.test.combinations.NamedObject(\n ftrl_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalftrl\", lambda: ftrl_new.Ftrl(0.002)\n )\n+lion_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"lion\", lambda: lion_new.Lion(0.002)\n+)\n nadam_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentnadam\", lambda: nadam_new.Nadam(0.002)\n )\n@@ -90,6 +94,7 @@ OPTIMIZER_FN = [\n     adamax_new_fn,\n     adamw_new_fn,\n     ftrl_new_fn,\n+    lion_new_fn,\n     nadam_new_fn,\n     rmsprop_new_fn,\n     sgd_new_fn,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6074929ccdfd2d4889a7c039e8b7163236c3533a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 11 | Churn Cumulative: 171 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,13 +89,16 @@ class Lion(optimizer.Optimizer):\n         )\n         self._learning_rate = self._build_learning_rate(learning_rate)\n         self.beta_1 = beta_1\n-        if isinstance(beta_1, (int, float)) and (beta_1 < 0 or beta_1 > 1):\n-            raise ValueError(\"`beta_1` must be between [0, 1].\")\n         self.beta_2 = beta_2\n-        if isinstance(beta_2, (int, float)) and (beta_2 < 0 or beta_2 > 1):\n-            raise ValueError(\"`beta_2` must be between [0, 1].\")\n \n     def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Lion optimizer has one variable `momentums`.\n+\n+        Args:\n+          var_list: list of model variables to build Lion variables on.\n+        \"\"\"\n         super().build(var_list)\n         if hasattr(self, \"_built\") and self._built:\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2dcab061e20bb584f59892f0289ef91b541b8d03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 636 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,6 +37,7 @@ FLAGS = flags.FLAGS\n flags.DEFINE_string(\"model\", None, \"The model to run memory profiler.\")\n \n \n+def main(_):\n     @memory_profiler.profile\n     def _imdb_lstm_model():\n         \"\"\"LSTM model.\"\"\"\n@@ -53,8 +54,6 @@ def _imdb_lstm_model():\n         # Warm up the model with one epoch.\n         model.fit(x_train, y_train, batch_size=512, epochs=3)\n \n-\n-def main(_):\n     # Add the model for memory profile.\n     models = {\n         \"lstm\": _imdb_lstm_model,\n\n@@ -54,7 +54,7 @@ TMP_TEST_DIRNAME = \"keras_test\"\n VERBOSE = True\n INIT_FILE_HEADER = \"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\n # These are symbols that have export issues and that we skip for now.\n-SYMBOLS_TO_SKIP = []\n+SYMBOLS_TO_SKIP = [\"layer_test\"]\n \n \n def copy_keras_codebase(source_dir, target_dir):\n@@ -235,11 +235,11 @@ def generate_keras_api_files(package_directory, src_directory):\n         root_offset=[\"api\", \"_v1\", \"keras\"],\n     )\n     # Add missing __init__ files in api dirs.\n-    with open(os.path.join(package_directory, \"api\", \"__init__.py\"), \"w\") as f:\n+    with open(os.path.join(package_directory, \"api\", \"__init__.py\"), \"w\"):\n         pass\n-    with open(os.path.join(v1_path, \"__init__.py\"), \"w\") as f:\n+    with open(os.path.join(v1_path, \"__init__.py\"), \"w\"):\n         pass\n-    with open(os.path.join(v2_path, \"__init__.py\"), \"w\") as f:\n+    with open(os.path.join(v2_path, \"__init__.py\"), \"w\"):\n         pass\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#51041a5915c772a67314781c186639184993884a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 198 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -120,27 +120,26 @@ class Lion(optimizer.Optimizer):\n         m = self.momentums[self._index_dict[var_key]]\n \n         if isinstance(gradient, tf.IndexedSlices):\n-            # Sparse gradients\n-            m_t = m.assign(m * beta_1)\n-            m_scaled_g_values = tf.IndexedSlices(\n+            # Sparse gradients (use m as a buffer)\n+            m.assign(m * beta_1)\n+            m.scatter_add(\n+                tf.IndexedSlices(\n                     gradient.values * (1.0 - beta_1), gradient.indices\n                 )\n-            m_t = m_t.scatter_add(m_scaled_g_values)\n-            variable_t = variable.assign_sub(lr * tf.math.sign(m_t))\n-\n-            with tf.control_dependencies([variable_t]):\n-                m_t = m_t.scatter_sub(m_scaled_g_values)\n-                m_t = m_t.assign(m_t * beta_2 / beta_1)\n-                m_scaled_g_values = tf.IndexedSlices(\n-                    gradient.values * (1.0 - beta_2), gradient.indices\n             )\n-                m_t.scatter_add(m_scaled_g_values)\n+            variable.assign_sub(lr * tf.math.sign(m))\n+\n+            m.assign(m * beta_2 / beta_1)\n+            m.scatter_add(\n+                tf.IndexedSlices(\n+                    gradient.values * (1.0 - beta_2 / beta_1), gradient.indices\n+                )\n+            )\n         else:\n             # Dense gradients\n-            variable_t = variable.assign_sub(\n+            variable.assign_sub(\n                 lr * tf.math.sign(m * beta_1 + gradient * (1.0 - beta_1))\n             )\n-            with tf.control_dependencies([variable_t]):\n             m.assign(m * beta_2 + gradient * (1.0 - beta_2))\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d8baa9f6b3691c96816a64c135b43f6864713db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a63bef2ac504d651568c8800a9c1fcdcbd1cb41c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 36 | Files Changed: 8 | Hunks: 28 | Methods Changed: 41 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 19555 | Contributors (this commit): 54 | Commits (past 90d): 35 | Contributors (cumulative): 89 | DMM Complexity: None\n\nDIFF:\n@@ -3499,13 +3499,13 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # Bypass Trackable logic as `__dict__` already contains this info.\n         object.__setattr__(self, \"__dict__\", state)\n \n-    def _save_own_variables(self, store):\n+    def save_own_variables(self, store):\n         \"\"\"Experimental method for saving the state of this layer object.\"\"\"\n         all_vars = self._trainable_weights + self._non_trainable_weights\n         for i, v in enumerate(all_vars):\n             store[f\"{i}\"] = v.numpy()\n \n-    def _load_own_variables(self, store):\n+    def load_own_variables(self, store):\n         \"\"\"Experimental method for loading the state of this layer object.\"\"\"\n         self._update_trackables()\n         all_vars = self._trainable_weights + self._non_trainable_weights\n\n@@ -825,16 +825,16 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         with tf.control_dependencies(lookup_checks):\n             return tf.identity(lookups)\n \n-    def _save_own_variables(self, store):\n+    def save_own_variables(self, store):\n         if self.output_mode == TF_IDF:\n             store[\"idf_weights\"] = self.idf_weights_const.numpy()\n \n-    def _load_own_variables(self, store):\n+    def load_own_variables(self, store):\n         if self.output_mode == TF_IDF:\n             self.idf_weights.assign(store[\"idf_weights\"])\n             self.idf_weights_const = self.idf_weights.value()\n \n-    def _save_assets(self, dir_path):\n+    def save_assets(self, dir_path):\n         if self.input_vocabulary:\n             # Vocab saved in config.\n             # TODO: consider unifying both paths.\n@@ -844,7 +844,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         with open(vocabulary_filepath, \"w\") as f:\n             f.write(\"\\n\".join([str(w) for w in vocabulary]))\n \n-    def _load_assets(self, dir_path):\n+    def load_assets(self, dir_path):\n         if self.input_vocabulary:\n             # Vocab saved in config.\n             # TODO: consider unifying both paths.\n\n@@ -386,7 +386,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n             inputs = tf.cast(inputs, self.compute_dtype)\n         return inputs\n \n-    def _load_own_variables(self, store):\n+    def load_own_variables(self, store):\n         # Ensure that we call finalize_state after variable loading.\n-        super()._load_own_variables(store)\n+        super().load_own_variables(store)\n         self.finalize_state()\n\n@@ -670,14 +670,14 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     def _trackable_saved_model_saver(self):\n         return layer_serialization.VocabularySavedModelSaver(self)\n \n-    def _save_own_variables(self, store):\n-        self._lookup_layer._save_own_variables(store)\n+    def save_own_variables(self, store):\n+        self._lookup_layer.save_own_variables(store)\n \n-    def _load_own_variables(self, store):\n-        self._lookup_layer._load_own_variables(store)\n+    def load_own_variables(self, store):\n+        self._lookup_layer.load_own_variables(store)\n \n-    def _save_assets(self, dir_path):\n-        self._lookup_layer._save_assets(dir_path)\n+    def save_assets(self, dir_path):\n+        self._lookup_layer.save_assets(dir_path)\n \n-    def _load_assets(self, dir_path):\n-        self._lookup_layer._load_assets(dir_path)\n+    def load_assets(self, dir_path):\n+        self._lookup_layer.load_assets(dir_path)\n\n@@ -818,12 +818,12 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                 )\n             variable.assign(weight)\n \n-    def _save_own_variables(self, store):\n+    def save_own_variables(self, store):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n         for i, variable in enumerate(self.variables):\n             store[str(i)] = variable.numpy()\n \n-    def _load_own_variables(self, store):\n+    def load_own_variables(self, store):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n         if len(store.keys()) != len(self.variables):\n             msg = (\n\n@@ -368,11 +368,10 @@ def _save_state(\n     if id(trackable) in visited_trackables:\n         return\n \n-    # TODO(fchollet): better name?\n-    if hasattr(trackable, \"_save_own_variables\") and weights_store:\n-        trackable._save_own_variables(weights_store.make(inner_path))\n-    if hasattr(trackable, \"_save_assets\") and assets_store:\n-        trackable._save_assets(assets_store.make(inner_path))\n+    if hasattr(trackable, \"save_own_variables\") and weights_store:\n+        trackable.save_own_variables(weights_store.make(inner_path))\n+    if hasattr(trackable, \"save_assets\") and assets_store:\n+        trackable.save_assets(assets_store.make(inner_path))\n \n     visited_trackables.add(id(trackable))\n \n@@ -407,10 +406,10 @@ def _load_state(\n     if visited_trackables and id(trackable) in visited_trackables:\n         return\n \n-    if hasattr(trackable, \"_load_own_variables\") and weights_store:\n+    if hasattr(trackable, \"load_own_variables\") and weights_store:\n         if skip_mismatch:\n             try:\n-                trackable._load_own_variables(weights_store.get(inner_path))\n+                trackable.load_own_variables(weights_store.get(inner_path))\n             except Exception as e:\n                 warnings.warn(\n                     f\"Could not load weights in object {trackable}. \"\n@@ -419,12 +418,12 @@ def _load_state(\n                     stacklevel=2,\n                 )\n         else:\n-            trackable._load_own_variables(weights_store.get(inner_path))\n+            trackable.load_own_variables(weights_store.get(inner_path))\n \n-    if hasattr(trackable, \"_load_assets\") and assets_store:\n+    if hasattr(trackable, \"load_assets\") and assets_store:\n         if skip_mismatch:\n             try:\n-                trackable._load_assets(assets_store.get(inner_path))\n+                trackable.load_assets(assets_store.get(inner_path))\n             except Exception as e:\n                 warnings.warn(\n                     f\"Could not load assets in object {trackable}. \"\n@@ -433,7 +432,7 @@ def _load_state(\n                     stacklevel=2,\n                 )\n         else:\n-            trackable._load_assets(assets_store.get(inner_path))\n+            trackable.load_assets(assets_store.get(inner_path))\n \n     if visited_trackables is not None:\n         visited_trackables.add(id(trackable))\n@@ -707,7 +706,6 @@ def _print_h5_file(h5_file, prefix=\"\", action=None):\n \n \n def _print_zip_file(zipfile, action):\n-    # TODO(fchollet): move to debugging logs.\n     io_utils.print_msg(f\"Keras model archive {action}:\")\n     # Same as `ZipFile.printdir()` except for using Keras' printing utility.\n     io_utils.print_msg(\n\n@@ -78,19 +78,19 @@ class LayerWithCustomSaving(MyDense):\n         self.stored_variables = variables_data\n         return super().build(input_shape)\n \n-    def _save_assets(self, inner_path):\n+    def save_assets(self, inner_path):\n         with open(os.path.join(inner_path, \"assets.txt\"), \"w\") as f:\n             f.write(self.assets)\n \n-    def _save_own_variables(self, store):\n+    def save_own_variables(self, store):\n         store[\"variables\"] = self.stored_variables\n \n-    def _load_assets(self, inner_path):\n+    def load_assets(self, inner_path):\n         with open(os.path.join(inner_path, \"assets.txt\"), \"r\") as f:\n             text = f.read()\n         self.assets = text\n \n-    def _load_own_variables(self, store):\n+    def load_own_variables(self, store):\n         self.stored_variables = np.array(store[\"variables\"])\n \n \n\n@@ -765,8 +765,8 @@ class FeatureSpace(base_layer.Layer):\n         \"\"\"\n         saving_lib.save_model(self, filepath)\n \n-    def _save_own_variables(self, store):\n+    def save_own_variables(self, store):\n         return\n \n-    def _load_own_variables(self, store):\n+    def load_own_variables(self, store):\n         return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d8497e7ef6a7ae5529a9af337073a2973594aef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 607 | Files Changed: 3 | Hunks: 2 | Methods Changed: 25 | Complexity Δ (Sum/Max): -62/0 | Churn Δ: 607 | Churn Cumulative: 3202 | Contributors (this commit): 32 | Commits (past 90d): 3 | Contributors (cumulative): 42 | DMM Complexity: 0.2673992673992674\n\nDIFF:\n\n@@ -1,401 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Wrapper for using the Scikit-Learn API with Keras models.\"\"\"\n-\n-\n-import copy\n-import types\n-import warnings\n-\n-import numpy as np\n-\n-from keras import losses\n-from keras.models import Sequential\n-from keras.utils.generic_utils import has_arg\n-from keras.utils.np_utils import to_categorical\n-\n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n-from tensorflow.tools.docs import doc_controls\n-\n-\n-class BaseWrapper:\n-    \"\"\"Base class for the Keras scikit-learn wrapper.\n-\n-    Warning: This class should not be used directly.\n-    Use descendant classes instead.\n-\n-    Args:\n-        build_fn: callable function or class instance\n-        **sk_params: model parameters & fitting parameters\n-\n-    The `build_fn` should construct, compile and return a Keras model, which\n-    will then be used to fit/predict. One of the following\n-    three values could be passed to `build_fn`:\n-    1. A function\n-    2. An instance of a class that implements the `__call__` method\n-    3. None. This means you implement a class that inherits from either\n-    `KerasClassifier` or `KerasRegressor`. The `__call__` method of the\n-    present class will then be treated as the default `build_fn`.\n-\n-    `sk_params` takes both model parameters and fitting parameters. Legal model\n-    parameters are the arguments of `build_fn`. Note that like all other\n-    estimators in scikit-learn, `build_fn` should provide default values for\n-    its arguments, so that you could create the estimator without passing any\n-    values to `sk_params`.\n-\n-    `sk_params` could also accept parameters for calling `fit`, `predict`,\n-    `predict_proba`, and `score` methods (e.g., `epochs`, `batch_size`).\n-    fitting (predicting) parameters are selected in the following order:\n-\n-    1. Values passed to the dictionary arguments of\n-    `fit`, `predict`, `predict_proba`, and `score` methods\n-    2. Values passed to `sk_params`\n-    3. The default values of the `keras.models.Sequential`\n-    `fit`, `predict` methods.\n-\n-    When using scikit-learn's `grid_search` API, legal tunable parameters are\n-    those you could pass to `sk_params`, including fitting parameters.\n-    In other words, you could use `grid_search` to search for the best\n-    `batch_size` or `epochs` as well as the model parameters.\n-    \"\"\"\n-\n-    def __init__(self, build_fn=None, **sk_params):\n-        self.build_fn = build_fn\n-        self.sk_params = sk_params\n-        self.check_params(sk_params)\n-\n-    def check_params(self, params):\n-        \"\"\"Checks for user typos in `params`.\n-\n-        Args:\n-            params: dictionary; the parameters to be checked\n-\n-        Raises:\n-            ValueError: if any member of `params` is not a valid argument.\n-        \"\"\"\n-        legal_params_fns = [\n-            Sequential.fit,\n-            Sequential.predict,\n-            Sequential.evaluate,\n-        ]\n-        if self.build_fn is None:\n-            legal_params_fns.append(self.__call__)\n-        elif not isinstance(\n-            self.build_fn, types.FunctionType\n-        ) and not isinstance(self.build_fn, types.MethodType):\n-            legal_params_fns.append(self.build_fn.__call__)\n-        else:\n-            legal_params_fns.append(self.build_fn)\n-\n-        for params_name in params:\n-            for fn in legal_params_fns:\n-                if has_arg(fn, params_name):\n-                    break\n-            else:\n-                if params_name != \"nb_epoch\":\n-                    raise ValueError(\n-                        f\"{params_name} is not a legal parameter\"\n-                    )  # noqa: E501\n-\n-    def get_params(self, **params):\n-        \"\"\"Gets parameters for this estimator.\n-\n-        Args:\n-            **params: ignored (exists for API compatibility).\n-\n-        Returns:\n-            Dictionary of parameter names mapped to their values.\n-        \"\"\"\n-        res = self.sk_params.copy()\n-        res.update({\"build_fn\": self.build_fn})\n-        return res\n-\n-    def set_params(self, **params):\n-        \"\"\"Sets the parameters of this estimator.\n-\n-        Args:\n-            **params: Dictionary of parameter names mapped to their values.\n-\n-        Returns:\n-            self\n-        \"\"\"\n-        self.check_params(params)\n-        self.sk_params.update(params)\n-        return self\n-\n-    def fit(self, x, y, **kwargs):\n-        \"\"\"Constructs a new model with `build_fn` & fit the model to `(x, y)`.\n-\n-        Args:\n-            x : array-like, shape `(n_samples, n_features)`\n-                Training samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n-                True labels for `x`.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments of `Sequential.fit`\n-\n-        Returns:\n-            history : object\n-                details about the training history at each epoch.\n-        \"\"\"\n-        if self.build_fn is None:\n-            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n-        elif not isinstance(\n-            self.build_fn, types.FunctionType\n-        ) and not isinstance(self.build_fn, types.MethodType):\n-            self.model = self.build_fn(\n-                **self.filter_sk_params(self.build_fn.__call__)\n-            )\n-        else:\n-            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n-\n-        if (\n-            losses.is_categorical_crossentropy(self.model.loss)\n-            and len(y.shape) != 2\n-        ):\n-            y = to_categorical(y)\n-\n-        fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n-        fit_args.update(kwargs)\n-\n-        history = self.model.fit(x, y, **fit_args)\n-\n-        return history\n-\n-    def filter_sk_params(self, fn, override=None):\n-        \"\"\"Filters `sk_params` and returns those in `fn`'s arguments.\n-\n-        Args:\n-            fn : arbitrary function\n-            override: dictionary, values to override `sk_params`\n-\n-        Returns:\n-            res : dictionary containing variables\n-                in both `sk_params` and `fn`'s arguments.\n-        \"\"\"\n-        override = override or {}\n-        res = {}\n-        for name, value in self.sk_params.items():\n-            if has_arg(fn, name):\n-                res.update({name: value})\n-        res.update(override)\n-        return res\n-\n-\n-@keras_export(\"keras.wrappers.scikit_learn.KerasClassifier\")\n-@doc_controls.do_not_generate_docs\n-class KerasClassifier(BaseWrapper):\n-    \"\"\"Implementation of the scikit-learn classifier API for Keras.\n-\n-    DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n-    See https://www.adriangb.com/scikeras/stable/migration.html\n-    for help migrating.\n-    \"\"\"\n-\n-    def __init__(self, build_fn=None, **sk_params):\n-        warnings.warn(\n-            \"KerasClassifier is deprecated, \"\n-            \"use Sci-Keras (https://github.com/adriangb/scikeras) instead. \"\n-            \"See https://www.adriangb.com/scikeras/stable/migration.html \"\n-            \"for help migrating.\",\n-            DeprecationWarning,\n-            stacklevel=2,\n-        )\n-        super().__init__(build_fn, **sk_params)\n-\n-    def fit(self, x, y, **kwargs):\n-        \"\"\"Constructs a new model with `build_fn` & fit the model to `(x, y)`.\n-\n-        Args:\n-            x : array-like, shape `(n_samples, n_features)`\n-                Training samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n-                True labels for `x`.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments of `Sequential.fit`\n-\n-        Returns:\n-            history : object\n-                details about the training history at each epoch.\n-\n-        Raises:\n-            ValueError: In case of invalid shape for `y` argument.\n-        \"\"\"\n-        y = np.array(y)\n-        if len(y.shape) == 2 and y.shape[1] > 1:\n-            self.classes_ = np.arange(y.shape[1])\n-        elif (len(y.shape) == 2 and y.shape[1] == 1) or len(y.shape) == 1:\n-            self.classes_ = np.unique(y)\n-            y = np.searchsorted(self.classes_, y)\n-        else:\n-            raise ValueError(\"Invalid shape for y: \" + str(y.shape))\n-        self.n_classes_ = len(self.classes_)\n-        return super().fit(x, y, **kwargs)\n-\n-    def predict(self, x, **kwargs):\n-        \"\"\"Returns the class predictions for the given test data.\n-\n-        Args:\n-            x: array-like, shape `(n_samples, n_features)`\n-                Test samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments\n-                of `Sequential.predict`.\n-\n-        Returns:\n-            preds: array-like, shape `(n_samples,)`\n-                Class predictions.\n-        \"\"\"\n-        proba = self.model.predict(x, **kwargs)\n-        if proba.shape[-1] > 1:\n-            classes = proba.argmax(axis=-1)\n-        else:\n-            classes = (proba > 0.5).astype(\"int32\")\n-        return self.classes_[classes]\n-\n-    def predict_proba(self, x, **kwargs):\n-        \"\"\"Returns class probability estimates for the given test data.\n-\n-        Args:\n-            x: array-like, shape `(n_samples, n_features)`\n-                Test samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments\n-                of `Sequential.predict`.\n-\n-        Returns:\n-            proba: array-like, shape `(n_samples, n_outputs)`\n-                Class probability estimates.\n-                In the case of binary classification,\n-                to match the scikit-learn API,\n-                will return an array of shape `(n_samples, 2)`\n-                (instead of `(n_sample, 1)` as in Keras).\n-        \"\"\"\n-        probs = self.model.predict(x, **kwargs)\n-\n-        # check if binary classification\n-        if probs.shape[1] == 1:\n-            # first column is probability of class 0 and second is of class 1\n-            probs = np.hstack([1 - probs, probs])\n-        return probs\n-\n-    def score(self, x, y, **kwargs):\n-        \"\"\"Returns the mean accuracy on the given test data and labels.\n-\n-        Args:\n-            x: array-like, shape `(n_samples, n_features)`\n-                Test samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n-                True labels for `x`.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments of `Sequential.evaluate`.\n-\n-        Returns:\n-            score: float\n-                Mean accuracy of predictions on `x` wrt. `y`.\n-\n-        Raises:\n-            ValueError: If the underlying model isn't configured to\n-                compute accuracy. You should pass `metrics=[\"accuracy\"]` to\n-                the `.compile()` method of the model.\n-        \"\"\"\n-        y = np.searchsorted(self.classes_, y)\n-        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n-\n-        loss_name = self.model.loss\n-        if hasattr(loss_name, \"__name__\"):\n-            loss_name = loss_name.__name__\n-        if loss_name == \"categorical_crossentropy\" and len(y.shape) != 2:\n-            y = to_categorical(y)\n-\n-        outputs = self.model.evaluate(x, y, **kwargs)\n-        if not isinstance(outputs, list):\n-            outputs = [outputs]\n-        for name, output in zip(self.model.metrics_names, outputs):\n-            if name in [\"accuracy\", \"acc\"]:\n-                return output\n-        raise ValueError(\n-            \"The model is not configured to compute accuracy. \"\n-            'You should pass `metrics=[\"accuracy\"]` to '\n-            \"the `model.compile()` method.\"\n-        )\n-\n-\n-@keras_export(\"keras.wrappers.scikit_learn.KerasRegressor\")\n-@doc_controls.do_not_generate_docs\n-class KerasRegressor(BaseWrapper):\n-    \"\"\"Implementation of the scikit-learn regressor API for Keras.\n-\n-    DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n-    See https://www.adriangb.com/scikeras/stable/migration.html\n-    for help migrating.\n-    \"\"\"\n-\n-    @doc_controls.do_not_doc_inheritable\n-    def __init__(self, build_fn=None, **sk_params):\n-        warnings.warn(\n-            \"KerasRegressor is deprecated, \"\n-            \"use Sci-Keras (https://github.com/adriangb/scikeras) instead. \"\n-            \"See https://www.adriangb.com/scikeras/stable/migration.html \"\n-            \"for help migrating.\",\n-            DeprecationWarning,\n-            stacklevel=2,\n-        )\n-        super().__init__(build_fn, **sk_params)\n-\n-    def predict(self, x, **kwargs):\n-        \"\"\"Returns predictions for the given test data.\n-\n-        Args:\n-            x: array-like, shape `(n_samples, n_features)`\n-                Test samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments of `Sequential.predict`.\n-\n-        Returns:\n-            preds: array-like, shape `(n_samples,)`\n-                Predictions.\n-        \"\"\"\n-        kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n-        return np.squeeze(self.model.predict(x, **kwargs))\n-\n-    def score(self, x, y, **kwargs):\n-        \"\"\"Returns the mean loss on the given test data and labels.\n-\n-        Args:\n-            x: array-like, shape `(n_samples, n_features)`\n-                Test samples where `n_samples` is the number of samples\n-                and `n_features` is the number of features.\n-            y: array-like, shape `(n_samples,)`\n-                True labels for `x`.\n-            **kwargs: dictionary arguments\n-                Legal arguments are the arguments of `Sequential.evaluate`.\n-\n-        Returns:\n-            score: float\n-                Mean accuracy of predictions on `x` wrt. `y`.\n-        \"\"\"\n-        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n-        loss = self.model.evaluate(x, y, **kwargs)\n-        if isinstance(loss, list):\n-            return -loss[0]\n-        return -loss\n\n@@ -1,206 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for Scikit-learn API wrapper.\"\"\"\n-\n-import warnings\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-\n-import keras\n-from keras.testing_infra import test_utils\n-from keras.wrappers import scikit_learn\n-\n-INPUT_DIM = 5\n-HIDDEN_DIM = 5\n-TRAIN_SAMPLES = 10\n-TEST_SAMPLES = 5\n-NUM_CLASSES = 2\n-BATCH_SIZE = 5\n-EPOCHS = 1\n-\n-\n-def build_fn_clf(hidden_dim):\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(INPUT_DIM, input_shape=(INPUT_DIM,)))\n-    model.add(keras.layers.Activation(\"relu\"))\n-    model.add(keras.layers.Dense(hidden_dim))\n-    model.add(keras.layers.Activation(\"relu\"))\n-    model.add(keras.layers.Dense(NUM_CLASSES))\n-    model.add(keras.layers.Activation(\"softmax\"))\n-    model.compile(\n-        optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n-    )\n-    return model\n-\n-\n-def assert_classification_works(clf):\n-    np.random.seed(42)\n-    (x_train, y_train), (x_test, _) = test_utils.get_test_data(\n-        train_samples=TRAIN_SAMPLES,\n-        test_samples=TEST_SAMPLES,\n-        input_shape=(INPUT_DIM,),\n-        num_classes=NUM_CLASSES,\n-    )\n-\n-    clf.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)\n-\n-    score = clf.score(x_train, y_train, batch_size=BATCH_SIZE)\n-    assert np.isscalar(score) and np.isfinite(score)\n-\n-    preds = clf.predict(x_test, batch_size=BATCH_SIZE)\n-    assert preds.shape == (TEST_SAMPLES,)\n-    for prediction in np.unique(preds):\n-        assert prediction in range(NUM_CLASSES)\n-\n-    proba = clf.predict_proba(x_test, batch_size=BATCH_SIZE)\n-    assert proba.shape == (TEST_SAMPLES, NUM_CLASSES)\n-    assert np.allclose(np.sum(proba, axis=1), np.ones(TEST_SAMPLES))\n-\n-\n-def build_fn_reg(hidden_dim):\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(INPUT_DIM, input_shape=(INPUT_DIM,)))\n-    model.add(keras.layers.Activation(\"relu\"))\n-    model.add(keras.layers.Dense(hidden_dim))\n-    model.add(keras.layers.Activation(\"relu\"))\n-    model.add(keras.layers.Dense(1))\n-    model.add(keras.layers.Activation(\"linear\"))\n-    model.compile(\n-        optimizer=\"sgd\", loss=\"mean_absolute_error\", metrics=[\"accuracy\"]\n-    )\n-    return model\n-\n-\n-def assert_regression_works(reg):\n-    np.random.seed(42)\n-    (x_train, y_train), (x_test, _) = test_utils.get_test_data(\n-        train_samples=TRAIN_SAMPLES,\n-        test_samples=TEST_SAMPLES,\n-        input_shape=(INPUT_DIM,),\n-        num_classes=NUM_CLASSES,\n-    )\n-\n-    reg.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)\n-\n-    score = reg.score(x_train, y_train, batch_size=BATCH_SIZE)\n-    assert np.isscalar(score) and np.isfinite(score)\n-\n-    preds = reg.predict(x_test, batch_size=BATCH_SIZE)\n-    assert preds.shape == (TEST_SAMPLES,)\n-\n-\n-class ScikitLearnAPIWrapperTest(tf.test.TestCase):\n-    def test_classify_build_fn(self):\n-        with self.cached_session():\n-            clf = scikit_learn.KerasClassifier(\n-                build_fn=build_fn_clf,\n-                hidden_dim=HIDDEN_DIM,\n-                batch_size=BATCH_SIZE,\n-                epochs=EPOCHS,\n-            )\n-\n-            assert_classification_works(clf)\n-\n-    def test_classify_class_build_fn(self):\n-        class ClassBuildFnClf:\n-            def __call__(self, hidden_dim):\n-                return build_fn_clf(hidden_dim)\n-\n-        with self.cached_session():\n-            clf = scikit_learn.KerasClassifier(\n-                build_fn=ClassBuildFnClf(),\n-                hidden_dim=HIDDEN_DIM,\n-                batch_size=BATCH_SIZE,\n-                epochs=EPOCHS,\n-            )\n-\n-            assert_classification_works(clf)\n-\n-    def test_classify_inherit_class_build_fn(self):\n-        class InheritClassBuildFnClf(scikit_learn.KerasClassifier):\n-            def __call__(self, hidden_dim):\n-                return build_fn_clf(hidden_dim)\n-\n-        with self.cached_session():\n-            clf = InheritClassBuildFnClf(\n-                build_fn=None,\n-                hidden_dim=HIDDEN_DIM,\n-                batch_size=BATCH_SIZE,\n-                epochs=EPOCHS,\n-            )\n-\n-            assert_classification_works(clf)\n-\n-    def test_regression_build_fn(self):\n-        with self.cached_session():\n-            reg = scikit_learn.KerasRegressor(\n-                build_fn=build_fn_reg,\n-                hidden_dim=HIDDEN_DIM,\n-                batch_size=BATCH_SIZE,\n-                epochs=EPOCHS,\n-            )\n-\n-            assert_regression_works(reg)\n-\n-    def test_regression_class_build_fn(self):\n-        class ClassBuildFnReg:\n-            def __call__(self, hidden_dim):\n-                return build_fn_reg(hidden_dim)\n-\n-        with self.cached_session():\n-            reg = scikit_learn.KerasRegressor(\n-                build_fn=ClassBuildFnReg(),\n-                hidden_dim=HIDDEN_DIM,\n-                batch_size=BATCH_SIZE,\n-                epochs=EPOCHS,\n-            )\n-\n-            assert_regression_works(reg)\n-\n-    def test_regression_inherit_class_build_fn(self):\n-        class InheritClassBuildFnReg(scikit_learn.KerasRegressor):\n-            def __call__(self, hidden_dim):\n-                return build_fn_reg(hidden_dim)\n-\n-        with self.cached_session():\n-            reg = InheritClassBuildFnReg(\n-                build_fn=None,\n-                hidden_dim=HIDDEN_DIM,\n-                batch_size=BATCH_SIZE,\n-                epochs=EPOCHS,\n-            )\n-\n-            assert_regression_works(reg)\n-\n-    def test_regressor_deprecated(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter(\"always\")\n-            scikit_learn.KerasRegressor(build_fn_reg)\n-            assert len(w) == 1\n-            assert issubclass(w[-1].category, DeprecationWarning)\n-            assert \"KerasRegressor is deprecated\" in str(w[-1].message)\n-\n-    def test_classifier_deprecated(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter(\"always\")\n-            scikit_learn.KerasClassifier(build_fn_clf)\n-            assert len(w) == 1\n-            assert issubclass(w[-1].category, DeprecationWarning)\n-            assert \"KerasClassifier is deprecated\" in str(w[-1].message)\n-\n-\n-if __name__ == \"__main__\":\n-    tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
