{"custom_id": "keras#547640587a339c8c10b5eba950080d8cba0266e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 3924 | Contributors (this commit): 34 | Commits (past 90d): 30 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1257,10 +1257,8 @@ class Model(Container):\n                                'Use `model.compile(optimizer, loss)`.')\n \n         output_shapes = []\n-        for output_shape, loss_fn in zip(self.internal_output_shapes, self.loss_functions):\n-            if loss_fn is None:\n-                continue\n-            elif loss_fn.__name__ == 'sparse_categorical_crossentropy':\n+        for output_shape, loss_fn in zip(self._feed_output_shapes, self._feed_loss_fns):\n+            if loss_fn.__name__ == 'sparse_categorical_crossentropy':\n                 output_shapes.append(output_shape[:-1] + (1,))\n             elif getattr(losses, loss_fn.__name__, None) is None:\n                 output_shapes.append(None)\n@@ -1271,7 +1269,7 @@ class Model(Container):\n                                     check_batch_axis=False,\n                                     exception_prefix='model input')\n         y = _standardize_input_data(y, self._feed_output_names,\n-                                    self._feed_output_shapes,\n+                                    output_shapes,\n                                     check_batch_axis=False,\n                                     exception_prefix='model target')\n         sample_weights = _standardize_sample_weights(sample_weight,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8fb8accb055201146a8e1a74268f8eadca3d08fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 3941 | Contributors (this commit): 34 | Commits (past 90d): 29 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -735,12 +735,16 @@ class Model(Container):\n         skip_indices = []\n         self._feed_outputs = []\n         self._feed_output_names = []\n+        self._feed_output_shapes = []\n+        self._feed_loss_fns = []\n         for i in range(len(weighted_losses)):\n             if weighted_losses[i] is None:\n                 skip_indices.append(i)\n             else:\n                 self._feed_outputs.append(self.outputs[i])\n                 self._feed_output_names.append(self.output_names[i])\n+                self._feed_output_shapes.append(self.internal_output_shapes[i])\n+                self._feed_loss_fns.append(self.loss_functions[i])\n \n         # Prepare output masks.\n         masks = self.compute_mask(self.inputs, mask=None)\n@@ -845,6 +849,10 @@ class Model(Container):\n                                           name=name + '_sample_weights'))\n                         sample_weight_modes.append(None)\n         self.sample_weight_modes = sample_weight_modes\n+        self._feed_sample_weight_modes = []\n+        for i in range(len(self.outputs)):\n+            if i not in skip_indices:\n+                self._feed_sample_weight_modes.append(self.sample_weight_modes[i])\n \n         # Prepare targets of model.\n         self.targets = []\n@@ -970,15 +978,6 @@ class Model(Container):\n                 trainable_weights.sort(key=lambda x: x.name)\n         self._collected_trainable_weights = trainable_weights\n \n-        self._feed_sample_weight_modes = []\n-        self._feed_output_shapes = []\n-        self._feed_loss_fns = []\n-        for i in range(len(self.outputs)):\n-            if i not in skip_indices:\n-                self._feed_sample_weight_modes.append(self.sample_weight_modes[i])\n-                self._feed_output_shapes.append(self.internal_output_shapes[i])\n-                self._feed_loss_fns.append(self.loss_functions[i])\n-\n     def _make_train_function(self):\n         if not hasattr(self, 'train_function'):\n             raise RuntimeError('You must compile your model before using it.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5566c9db1d697018bdb3c69b0aed9c9d63fd3783", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1553 | Contributors (this commit): 18 | Commits (past 90d): 29 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -119,6 +119,7 @@ class MinMaxNorm(Constraint):\n             to constrain the weights of each filter tensor of size\n             `(rows, cols, input_depth)`.\n     \"\"\"\n+\n     def __init__(self, min_value=0.0, max_value=1.0, rate=1.0, axis=0):\n         self.min_value = min_value\n         self.max_value = max_value\n\n@@ -164,6 +164,7 @@ class VarianceScaling(Initializer):\n         ValueError: In case of an invalid value for the \"scale\", mode\" or\n           \"distribution\" arguments.\n     \"\"\"\n+\n     def __init__(self, scale=1.0,\n                  mode='fan_in',\n                  distribution='normal',\n\n@@ -32,6 +32,7 @@ class CustomObjectScope(object):\n             # save, load, etc. will recognize custom object by name\n     ```\n     \"\"\"\n+\n     def __init__(self, *args):\n         self.custom_objects = args\n         self.backup = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e90b0713f2e86f9f540b57b4306a77e3f696213b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 116 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 124 | Churn Cumulative: 12023 | Contributors (this commit): 57 | Commits (past 90d): 78 | Contributors (cumulative): 66 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -1810,6 +1810,7 @@ class Container(Layer):\n             A list of loss tensors.\n         \"\"\"\n         losses = []\n+        # Retrieve losses for all internal layers.\n         for layer in self.layers:\n             if hasattr(layer, 'losses'):\n                 if len(layer.inbound_nodes) == 1:\n@@ -1825,6 +1826,9 @@ class Container(Layer):\n                             losses += layer.get_losses_for(inputs)\n                     # Collect unconditional losses.\n                     losses += layer.get_losses_for(None)\n+        # Add any potential unconditional model-level loss.\n+        if hasattr(self, '_per_input_losses'):\n+            losses += self._per_input_losses.get(None, [])\n         return losses\n \n     @property\n\n@@ -1064,7 +1064,15 @@ class Model(Container):\n                 print('Train on %d samples, validate on %d samples' %\n                       (ins[0].shape[0], val_ins[0].shape[0]))\n \n+        if ins and hasattr(ins[0], 'shape'):\n             num_train_samples = ins[0].shape[0]\n+        else:\n+            # May happen if we are running `fit` without Numpy input data,\n+            # i.e. if all inputs to the models are data tensors\n+            # instead of placeholders.\n+            # In that case we will run `fit` over a single batch.\n+            num_train_samples = batch_size\n+            verbose = 2\n         index_array = np.arange(num_train_samples)\n \n         self.history = cbks.History()\n@@ -1213,7 +1221,16 @@ class Model(Container):\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         \"\"\"\n+        if ins and hasattr(ins[0], 'shape'):\n             samples = ins[0].shape[0]\n+        else:\n+            # May happen if we are running `evaluate` without Numpy input data,\n+            # i.e. if all inputs to the models are data tensors\n+            # instead of placeholders.\n+            # In that case we will run `evaluate` over a single batch.\n+            samples = batch_size\n+            verbose = 2\n+\n         outs = []\n         if verbose == 1:\n             progbar = Progbar(target=samples)\n\n@@ -306,10 +306,8 @@ def test_model_with_input_feed_tensor():\n     # Now test a model with a single input\n     # i.e. we don't pass any data to fit the model.\n     a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-\n     a_2 = Dense(4, name='dense_1')(a)\n     a_2 = Dropout(0.5, name='dropout')(a_2)\n-\n     model = Model(a, a_2)\n     model.summary()\n \n@@ -350,9 +348,7 @@ def test_model_with_input_feed_tensor():\n     # Same, without learning phase\n     # i.e. we don't pass any data to fit the model.\n     a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-\n     a_2 = Dense(4, name='dense_1')(a)\n-\n     model = Model(a, a_2)\n     model.summary()\n \n@@ -394,17 +390,14 @@ def test_model_with_input_feed_tensor():\n @keras_test\n def test_model_with_partial_loss():\n     a = Input(shape=(3,), name='input_a')\n-\n     a_2 = Dense(4, name='dense_1')(a)\n     dp = Dropout(0.5, name='dropout')\n     a_3 = dp(a_2)\n-\n     model = Model(a, [a_2, a_3])\n \n     optimizer = 'rmsprop'\n     loss = {'dropout': 'mse'}\n-    model.compile(optimizer, loss, metrics=['mae'],\n-                  sample_weight_mode=None)\n+    model.compile(optimizer, loss, metrics=['mae'])\n \n     input_a_np = np.random.random((10, 3))\n     output_a_np = np.random.random((10, 4))\n@@ -417,6 +410,100 @@ def test_model_with_partial_loss():\n     # evaluate\n     out = model.evaluate(input_a_np, [output_a_np])\n \n+    # Same without dropout.\n+    a = Input(shape=(3,), name='input_a')\n+    a_2 = Dense(4, name='dense_1')(a)\n+    a_3 = Dense(4, name='dense_2')(a_2)\n+    model = Model(a, [a_2, a_3])\n+\n+    optimizer = 'rmsprop'\n+    loss = {'dense_2': 'mse'}\n+    model.compile(optimizer, loss, metrics={'dense_1': 'mae'})\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(input_a_np, output_a_np)\n+    out = model.test_on_batch(input_a_np, output_a_np)\n+    # fit\n+    out = model.fit(input_a_np, [output_a_np])\n+    # evaluate\n+    out = model.evaluate(input_a_np, [output_a_np])\n+\n+\n+@keras_test\n+def test_model_with_external_loss():\n+    # None loss, only regularization loss.\n+    a = Input(shape=(3,), name='input_a')\n+    a_2 = Dense(4, name='dense_1',\n+                kernel_regularizer='l1',\n+                bias_regularizer='l2')(a)\n+    dp = Dropout(0.5, name='dropout')\n+    a_3 = dp(a_2)\n+\n+    model = Model(a, [a_2, a_3])\n+\n+    optimizer = 'rmsprop'\n+    loss = None\n+    model.compile(optimizer, loss, metrics=['mae'])\n+\n+    input_a_np = np.random.random((10, 3))\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(input_a_np, None)\n+    out = model.test_on_batch(input_a_np, None)\n+    # fit\n+    out = model.fit(input_a_np, None)\n+    # evaluate\n+    out = model.evaluate(input_a_np, None)\n+\n+    # No dropout, external loss.\n+    a = Input(shape=(3,), name='input_a')\n+    a_2 = Dense(4, name='dense_1')(a)\n+    a_3 = Dense(4, name='dense_2')(a)\n+\n+    model = Model(a, [a_2, a_3])\n+    model.add_loss(K.mean(a_3 + a_2))\n+\n+    optimizer = 'rmsprop'\n+    loss = None\n+    model.compile(optimizer, loss, metrics=['mae'])\n+\n+    # test train_on_batch\n+    out = model.train_on_batch(input_a_np, None)\n+    out = model.test_on_batch(input_a_np, None)\n+    # fit\n+    out = model.fit(input_a_np, None)\n+    # evaluate\n+    out = model.evaluate(input_a_np, None)\n+\n+    # Test fit with no external data at all.\n+    if K.backend() == 'tensorflow':\n+        import tensorflow as tf\n+\n+        a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n+        a_2 = Dense(4, name='dense_1')(a)\n+        a_2 = Dropout(0.5, name='dropout')(a_2)\n+        model = Model(a, a_2)\n+        model.add_loss(K.mean(a_2))\n+\n+        model.compile(optimizer='rmsprop',\n+                      loss=None,\n+                      metrics=['mean_squared_error'])\n+\n+        # test train_on_batch\n+        out = model.train_on_batch(None, None)\n+        out = model.test_on_batch(None, None)\n+        out = model.predict_on_batch(None)\n+\n+        # test fit\n+        out = model.fit(None, None, epochs=1, batch_size=10)\n+\n+        # test evaluate\n+        out = model.evaluate(None, None, batch_size=10)\n+\n+        # test predict\n+        out = model.predict(None, batch_size=10)\n+        assert out.shape == (10, 4)\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ae158e8b06562ab3db5d55d0f0a1dbda85d8089", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 410 | Lines Deleted: 117 | Files Changed: 7 | Hunks: 70 | Methods Changed: 36 | Complexity Δ (Sum/Max): 27/21 | Churn Δ: 527 | Churn Cumulative: 18474 | Contributors (this commit): 112 | Commits (past 90d): 99 | Contributors (cumulative): 149 | DMM Complexity: 0.9348837209302325\n\nDIFF:\n@@ -98,6 +98,6 @@ else:\n     # Fit the model on the batches generated by datagen.flow().\n     model.fit_generator(datagen.flow(x_train, y_train,\n                                      batch_size=batch_size),\n-                        samples_per_epoch=x_train.shape[0],\n+                        steps_per_epoch=x_train.shape[0] // batch_size,\n                         epochs=epochs,\n                         validation_data=(x_test, y_test))\n\n@@ -228,8 +228,25 @@ class BaseLogger(Callback):\n \n class ProgbarLogger(Callback):\n     \"\"\"Callback that prints metrics to stdout.\n+\n+    # Arguments\n+        count_mode: One of \"steps\" or \"samples\".\n+            Whether the progress bar should\n+            count samples seens or steps (batches) seen.\n+\n+    # Raises\n+        ValueError: In case of invalid `count_mode`.\n     \"\"\"\n \n+    def __init__(self, count_mode='samples'):\n+        super(ProgbarLogger, self).__init__()\n+        if count_mode == 'samples':\n+            self.use_steps = False\n+        elif count_mode == 'steps':\n+            self.use_steps = True\n+        else:\n+            raise ValueError('Unknown `count_mode`: ' + str(count_mode))\n+\n     def on_train_begin(self, logs=None):\n         self.verbose = self.params['verbose']\n         self.epochs = self.params['epochs']\n@@ -237,17 +254,25 @@ class ProgbarLogger(Callback):\n     def on_epoch_begin(self, epoch, logs=None):\n         if self.verbose:\n             print('Epoch %d/%d' % (epoch + 1, self.epochs))\n-            self.progbar = Progbar(target=self.params['samples'],\n+            if self.use_steps:\n+                target = self.params['steps']\n+            else:\n+                target = self.params['samples']\n+            self.target = target\n+            self.progbar = Progbar(target=self.target,\n                                    verbose=self.verbose)\n         self.seen = 0\n \n     def on_batch_begin(self, batch, logs=None):\n-        if self.seen < self.params['samples']:\n+        if self.seen < self.target:\n             self.log_values = []\n \n     def on_batch_end(self, batch, logs=None):\n         logs = logs or {}\n         batch_size = logs.get('size', 0)\n+        if self.use_steps:\n+            self.seen += 1\n+        else:\n             self.seen += batch_size\n \n         for k in self.params['metrics']:\n@@ -256,7 +281,7 @@ class ProgbarLogger(Callback):\n \n         # Skip progbar update for the last batch;\n         # will be handled by on_epoch_end.\n-        if self.verbose and self.seen < self.params['samples']:\n+        if self.verbose and self.seen < self.target:\n             self.progbar.update(self.seen, self.log_values)\n \n     def on_epoch_end(self, epoch, logs=None):\n\n@@ -578,7 +578,6 @@ class GeneratorEnqueuer(object):\n         self._pickle_safe = pickle_safe\n         self._threads = []\n         self._stop_event = None\n-\n         self.queue = None\n \n     def start(self, workers=1, max_q_size=10, wait_time=0.05):\n@@ -1308,9 +1307,18 @@ class Model(Container):\n                                  str(x[0].shape[0]) + ' samples')\n         return x, y, sample_weights\n \n-    def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,\n-            validation_split=0., validation_data=None, shuffle=True,\n-            class_weight=None, sample_weight=None, initial_epoch=0):\n+    def fit(self, x=None,\n+            y=None,\n+            batch_size=32,\n+            epochs=1,\n+            verbose=1,\n+            callbacks=None,\n+            validation_split=0.,\n+            validation_data=None,\n+            shuffle=True,\n+            class_weight=None,\n+            sample_weight=None,\n+            initial_epoch=0):\n         \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n \n         # Arguments\n@@ -1658,11 +1666,17 @@ class Model(Container):\n             return outputs[0]\n         return outputs\n \n-    def fit_generator(self, generator, samples_per_epoch, epochs,\n-                      verbose=1, callbacks=None,\n-                      validation_data=None, num_val_samples=None,\n+    def fit_generator(self, generator,\n+                      steps_per_epoch,\n+                      epochs=1,\n+                      verbose=1,\n+                      callbacks=None,\n+                      validation_data=None,\n+                      validation_steps=None,\n                       class_weight=None,\n-                      max_q_size=10, workers=1, pickle_safe=False,\n+                      max_q_size=10,\n+                      workers=1,\n+                      pickle_safe=False,\n                       initial_epoch=0):\n         \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n \n@@ -1679,8 +1693,11 @@ class Model(Container):\n                 The generator is expected to loop over its data\n                 indefinitely. An epoch finishes when `samples_per_epoch`\n                 samples have been seen by the model.\n-            samples_per_epoch: integer, number of samples to process before\n-                going to the next epoch.\n+            steps_per_epoch: Total number of steps (batches of samples)\n+                to yield from `generator` before declaring one epoch\n+                finished and starting the next epoch. It should typically\n+                be equal to the number of unique samples if your dataset\n+                divided by the batch size.\n             epochs: integer, total number of iterations on the data.\n             verbose: verbosity mode, 0, 1, or 2.\n             callbacks: list of callbacks to be called during training.\n@@ -1688,9 +1705,9 @@ class Model(Container):\n                 - a generator for the validation data\n                 - a tuple (inputs, targets)\n                 - a tuple (inputs, targets, sample_weights).\n-            num_val_samples: only relevant if `validation_data` is a generator.\n-                number of samples to use from validation generator\n-                at the end of every epoch.\n+            validation_steps: Only relevant if `validation_data`\n+                is a generator. Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             max_q_size: maximum size for the generator queue\n@@ -1742,9 +1759,10 @@ class Model(Container):\n         # avoid any explicit version checks\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__'))\n-        if val_gen and not num_val_samples:\n+        if val_gen and not validation_steps:\n             raise ValueError('When using a generator for validation data, '\n-                             'you must specify a value for \"num_val_samples\".')\n+                             'you must specify a value for '\n+                             '`validation_steps`.')\n \n         out_labels = self.metrics_names\n         callback_metrics = out_labels + ['val_' + n for n in out_labels]\n@@ -1753,7 +1771,7 @@ class Model(Container):\n         self.history = cbks.History()\n         callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]\n         if verbose:\n-            callbacks += [cbks.ProgbarLogger()]\n+            callbacks += [cbks.ProgbarLogger(count_mode='steps')]\n         callbacks = cbks.CallbackList(callbacks)\n \n         # it's possible to callback a different model than self:\n@@ -1764,7 +1782,7 @@ class Model(Container):\n         callbacks.set_model(callback_model)\n         callbacks.set_params({\n             'epochs': epochs,\n-            'samples': samples_per_epoch,\n+            'steps': steps_per_epoch,\n             'verbose': verbose,\n             'do_validation': do_validation,\n             'metrics': callback_metrics,\n@@ -1779,8 +1797,8 @@ class Model(Container):\n                 val_x, val_y, val_sample_weight = validation_data\n             else:\n                 raise ValueError('validation_data should be a tuple '\n-                                 '(val_x, val_y, val_sample_weight) '\n-                                 'or (val_x, val_y). Found: ' +\n+                                 '`(val_x, val_y, val_sample_weight)` '\n+                                 'or `(val_x, val_y)`. Found: ' +\n                                  str(validation_data))\n             val_x, val_y, val_sample_weights = self._standardize_user_data(\n                 val_x, val_y, val_sample_weight)\n@@ -1795,9 +1813,9 @@ class Model(Container):\n             callback_model.stop_training = False\n             while epoch < epochs:\n                 callbacks.on_epoch_begin(epoch)\n-                samples_seen = 0\n+                steps_done = 0\n                 batch_index = 0\n-                while samples_seen < samples_per_epoch:\n+                while steps_done < steps_per_epoch:\n                     generator_output = None\n                     while enqueuer.is_running():\n                         if not enqueuer.queue.empty():\n@@ -1807,9 +1825,9 @@ class Model(Container):\n                             time.sleep(wait_time)\n \n                     if not hasattr(generator_output, '__len__'):\n-                        raise ValueError('output of generator should be a tuple '\n-                                         '(x, y, sample_weight) '\n-                                         'or (x, y). Found: ' +\n+                        raise ValueError('output of generator should be '\n+                                         'a tuple `(x, y, sample_weight)` '\n+                                         'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n                     if len(generator_output) == 2:\n                         x, y = generator_output\n@@ -1817,9 +1835,9 @@ class Model(Container):\n                     elif len(generator_output) == 3:\n                         x, y, sample_weight = generator_output\n                     else:\n-                        raise ValueError('output of generator should be a tuple '\n-                                         '(x, y, sample_weight) '\n-                                         'or (x, y). Found: ' +\n+                        raise ValueError('output of generator should be '\n+                                         'a tuple `(x, y, sample_weight)` '\n+                                         'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n                     # build batch logs\n                     batch_logs = {}\n@@ -1844,29 +1862,23 @@ class Model(Container):\n \n                     callbacks.on_batch_end(batch_index, batch_logs)\n \n-                    # construct epoch logs\n+                    # Construct epoch logs.\n                     epoch_logs = {}\n                     batch_index += 1\n-                    samples_seen += batch_size\n+                    steps_done += 1\n \n-                    # epoch finished\n-                    if samples_seen > samples_per_epoch:\n-                        warnings.warn('Epoch comprised more than '\n-                                      '`samples_per_epoch` samples, '\n-                                      'which might affect learning results. '\n-                                      'Set `samples_per_epoch` correctly '\n-                                      'to avoid this warning.')\n-                    if samples_seen >= samples_per_epoch and do_validation:\n+                    # Epoch finished.\n+                    if steps_done >= steps_per_epoch and do_validation:\n                         if val_gen:\n                             val_outs = self.evaluate_generator(\n                                 validation_data,\n-                                num_val_samples,\n+                                validation_steps,\n                                 max_q_size=max_q_size,\n                                 workers=workers,\n                                 pickle_safe=pickle_safe)\n                         else:\n-                            # no need for try/except because\n-                            # data has already been validated\n+                            # No need for try/except because\n+                            # data has already been validated.\n                             val_outs = self.evaluate(\n                                 val_x, val_y,\n                                 batch_size=batch_size,\n@@ -1874,7 +1886,7 @@ class Model(Container):\n                                 verbose=0)\n                         if not isinstance(val_outs, list):\n                             val_outs = [val_outs]\n-                        # same labels assumed\n+                        # Same labels assumed.\n                         for l, o in zip(out_labels, val_outs):\n                             epoch_logs['val_' + l] = o\n \n@@ -1890,7 +1902,7 @@ class Model(Container):\n         callbacks.on_train_end()\n         return self.history\n \n-    def evaluate_generator(self, generator, val_samples,\n+    def evaluate_generator(self, generator, steps,\n                            max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Evaluates the model on a data generator.\n \n@@ -1900,8 +1912,8 @@ class Model(Container):\n         Arguments:\n             generator: Generator yielding tuples (inputs, targets)\n                 or (inputs, targets, sample_weights)\n-            val_samples: Total number of samples to generate from `generator`\n-                before returning.\n+            steps: Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n             max_q_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading\n@@ -1925,18 +1937,17 @@ class Model(Container):\n         \"\"\"\n         self._make_test_function()\n \n-        processed_samples = 0\n+        steps_done = 0\n         wait_time = 0.01\n         all_outs = []\n-        weights = []\n-\n+        batch_sizes = []\n         enqueuer = None\n \n         try:\n             enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n             enqueuer.start(workers=workers, max_q_size=max_q_size)\n \n-            while processed_samples < val_samples:\n+            while steps_done < steps:\n                 generator_output = None\n                 while enqueuer.is_running():\n                     if not enqueuer.queue.empty():\n@@ -1948,7 +1959,8 @@ class Model(Container):\n                 if not hasattr(generator_output, '__len__'):\n                     raise ValueError('output of generator should be a tuple '\n                                      '(x, y, sample_weight) '\n-                                     'or (x, y). Found: ' + str(generator_output))\n+                                     'or (x, y). Found: ' +\n+                                     str(generator_output))\n                 if len(generator_output) == 2:\n                     x, y = generator_output\n                     sample_weight = None\n@@ -1957,20 +1969,20 @@ class Model(Container):\n                 else:\n                     raise ValueError('output of generator should be a tuple '\n                                      '(x, y, sample_weight) '\n-                                     'or (x, y). Found: ' + str(generator_output))\n-\n+                                     'or (x, y). Found: ' +\n+                                     str(generator_output))\n                 outs = self.test_on_batch(x, y, sample_weight=sample_weight)\n \n                 if isinstance(x, list):\n-                    sampless = len(x[0])\n+                    batch_size = len(x[0])\n                 elif isinstance(x, dict):\n-                    sampless = len(list(x.values())[0])\n+                    batch_size = len(list(x.values())[0])\n                 else:\n-                    sampless = len(x)\n+                    batch_size = len(x)\n                 all_outs.append(outs)\n \n-                processed_samples += sampless\n-                weights.append(sampless)\n+                steps_done += 1\n+                batch_sizes.append(batch_size)\n \n         finally:\n             if enqueuer is not None:\n@@ -1978,15 +1990,15 @@ class Model(Container):\n \n         if not isinstance(outs, list):\n             return np.average(np.asarray(all_outs),\n-                              weights=weights)\n+                              weights=batch_sizes)\n         else:\n             averages = []\n             for i in range(len(outs)):\n                 averages.append(np.average([out[i] for out in all_outs],\n-                                           weights=weights))\n+                                           weights=batch_sizes))\n             return averages\n \n-    def predict_generator(self, generator, val_samples,\n+    def predict_generator(self, generator, steps,\n                           max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Generates predictions for the input samples from a data generator.\n \n@@ -1994,13 +2006,13 @@ class Model(Container):\n         `predict_on_batch`.\n \n         # Arguments\n-            generator: generator yielding batches of input samples.\n-            val_samples: total number of samples to generate from `generator`\n-                before returning.\n-            max_q_size: maximum size for the generator queue\n-            workers: maximum number of processes to spin up\n+            generator: Generator yielding batches of input samples.\n+            steps: Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n+            max_q_size: Maximum size for the generator queue.\n+            workers: Maximum number of processes to spin up\n                 when using process based threading\n-            pickle_safe: if True, use process based threading.\n+            pickle_safe: If `True`, use process based threading.\n                 Note that because\n                 this implementation relies on multiprocessing,\n                 you should not pass\n@@ -2017,17 +2029,16 @@ class Model(Container):\n         \"\"\"\n         self._make_predict_function()\n \n-        processed_samples = 0\n+        steps_done = 0\n         wait_time = 0.01\n         all_outs = []\n-\n         enqueuer = None\n \n         try:\n             enqueuer = GeneratorEnqueuer(generator, pickle_safe=pickle_safe)\n             enqueuer.start(workers=workers, max_q_size=max_q_size)\n \n-            while processed_samples < val_samples:\n+            while steps_done < steps:\n                 generator_output = None\n                 while enqueuer.is_running():\n                     if not enqueuer.queue.empty():\n@@ -2043,38 +2054,35 @@ class Model(Container):\n                     elif len(generator_output) == 3:\n                         x, y, sample_weight = generator_output\n                     else:\n-                        raise ValueError('output of generator should be a tuple '\n-                                         '(x, y, sample_weight) '\n-                                         'or (x, y). Found: ' +\n+                        raise ValueError('output of generator should be '\n+                                         'a tuple `(x, y, sample_weight)` '\n+                                         'or `(x, y)`. Found: ' +\n                                          str(generator_output))\n                 else:\n                     x = generator_output\n \n                 outs = self.predict_on_batch(x)\n-\n-                if isinstance(x, list):\n-                    samples = len(x[0])\n-                elif isinstance(x, dict):\n-                    samples = len(list(x.values())[0])\n-                else:\n-                    samples = len(x)\n-\n                 if not isinstance(outs, list):\n                     outs = [outs]\n \n                 if not all_outs:\n                     for out in outs:\n-                        shape = (val_samples,) + out.shape[1:]\n-                        all_outs.append(np.zeros(shape, dtype=K.floatx()))\n+                        all_outs.append([])\n \n                 for i, out in enumerate(outs):\n-                    all_outs[i][processed_samples:(processed_samples + samples)] = out\n-                processed_samples += samples\n+                    all_outs[i].append(out)\n+                steps_done += 1\n \n         finally:\n             if enqueuer is not None:\n                 enqueuer.stop()\n \n         if len(all_outs) == 1:\n-            return all_outs[0]\n-        return all_outs\n+            if steps_done == 1:\n+                return all_outs[0][0]\n+            else:\n+                return np.concatenate(all_outs[0])\n+        if steps_done == 1:\n+            return [out for out in all_outs]\n+        else:\n+            return [np.concatenate(out) for out in all_outs]\n\n@@ -982,11 +982,18 @@ class Sequential(Model):\n         else:\n             return (proba > 0.5).astype('int32')\n \n-    def fit_generator(self, generator, samples_per_epoch, epochs,\n-                      verbose=1, callbacks=None,\n-                      validation_data=None, num_val_samples=None,\n-                      class_weight=None, max_q_size=10, workers=1,\n-                      pickle_safe=False, initial_epoch=0):\n+    def fit_generator(self, generator,\n+                      steps_per_epoch,\n+                      epochs=1,\n+                      verbose=1,\n+                      callbacks=None,\n+                      validation_data=None,\n+                      validation_steps=None,\n+                      class_weight=None,\n+                      max_q_size=10,\n+                      workers=1,\n+                      pickle_safe=False,\n+                      initial_epoch=0):\n         \"\"\"Fits the model on data generated batch-by-batch by a Python generator.\n \n         The generator is run in parallel to the model, for efficiency.\n@@ -1002,8 +1009,11 @@ class Sequential(Model):\n                 The generator is expected to loop over its data\n                 indefinitely. An epoch finishes when `samples_per_epoch`\n                 samples have been seen by the model.\n-            samples_per_epoch: integer, number of samples to process before\n-                going to the next epoch.\n+            steps_per_epoch: Total number of steps (batches of samples)\n+                to yield from `generator` before declaring one epoch\n+                finished and starting the next epoch. It should typically\n+                be equal to the number of unique samples if your dataset\n+                divided by the batch size.\n             epochs: integer, total number of iterations on the data.\n             verbose: verbosity mode, 0, 1, or 2.\n             callbacks: list of callbacks to be called during training.\n@@ -1018,9 +1028,12 @@ class Sequential(Model):\n                 for the class.\n             max_q_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n-            pickle_safe: if True, use process based threading. Note that because\n-                this implementation relies on multiprocessing, you should not pass\n-                non picklable arguments to the generator as they can't be passed\n+            pickle_safe: if True, use process based threading.\n+                Note that because\n+                this implementation relies on multiprocessing,\n+                you should not pass\n+                non picklable arguments to the generator\n+                as they can't be passed\n                 easily to children processes.\n             initial_epoch: epoch at which to start training\n                 (useful for resuming a previous training run)\n@@ -1052,19 +1065,19 @@ class Sequential(Model):\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n         return self.model.fit_generator(generator,\n-                                        samples_per_epoch,\n+                                        steps_per_epoch,\n                                         epochs,\n                                         verbose=verbose,\n                                         callbacks=callbacks,\n                                         validation_data=validation_data,\n-                                        num_val_samples=num_val_samples,\n+                                        validation_steps=validation_steps,\n                                         class_weight=class_weight,\n                                         max_q_size=max_q_size,\n                                         workers=workers,\n                                         pickle_safe=pickle_safe,\n                                         initial_epoch=initial_epoch)\n \n-    def evaluate_generator(self, generator, val_samples,\n+    def evaluate_generator(self, generator, steps,\n                            max_q_size=10, workers=1,\n                            pickle_safe=False):\n         \"\"\"Evaluates the model on a data generator.\n@@ -1075,8 +1088,8 @@ class Sequential(Model):\n         # Arguments\n             generator: Generator yielding tuples (inputs, targets)\n                 or (inputs, targets, sample_weights)\n-            val_samples: Total number of samples to generate from `generator`\n-                before returning.\n+            steps: Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n             max_q_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             pickle_safe: if True, use process based threading.\n@@ -1098,12 +1111,12 @@ class Sequential(Model):\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n         return self.model.evaluate_generator(generator,\n-                                             val_samples,\n+                                             steps,\n                                              max_q_size=max_q_size,\n                                              workers=workers,\n                                              pickle_safe=pickle_safe)\n \n-    def predict_generator(self, generator, val_samples,\n+    def predict_generator(self, generator, steps,\n                           max_q_size=10, workers=1, pickle_safe=False):\n         \"\"\"Generates predictions for the input samples from a data generator.\n \n@@ -1112,8 +1125,8 @@ class Sequential(Model):\n \n         # Arguments\n             generator: generator yielding batches of input samples.\n-            val_samples: total number of samples to generate from `generator`\n-                before returning.\n+            steps: Total number of steps (batches of samples)\n+                to yield from `generator` before stopping.\n             max_q_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             pickle_safe: if True, use process based threading.\n@@ -1127,7 +1140,7 @@ class Sequential(Model):\n         \"\"\"\n         if self.model is None:\n             self.build()\n-        return self.model.predict_generator(generator, val_samples,\n+        return self.model.predict_generator(generator, steps,\n                                             max_q_size=max_q_size,\n                                             workers=workers,\n                                             pickle_safe=pickle_safe)\n\n@@ -150,9 +150,12 @@ def test_model_methods():\n     # test starting from non-zero initial epoch\n     trained_epochs = []\n \n+    # define tracer callback\n     def on_epoch_begin(epoch, logs):\n         trained_epochs.append(epoch)\n+\n     tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin)\n+\n     out = model.fit([input_a_np, input_b_np],\n                     [output_a_np, output_b_np], epochs=5, batch_size=4,\n                     initial_epoch=2, callbacks=[tracker_cb])\n@@ -165,7 +168,7 @@ def test_model_methods():\n         while True:\n             yield ([np.random.random((batch_sz, 3)), np.random.random((batch_sz, 3))],\n                    [np.random.random((batch_sz, 4)), np.random.random((batch_sz, 3))])\n-    out = model.fit_generator(gen_data(4), samples_per_epoch=10, epochs=5,\n+    out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n                               initial_epoch=2, callbacks=[tracker_cb])\n     assert trained_epochs == [2, 3, 4]\n \n\n@@ -81,11 +81,13 @@ def test_sequential_fit_generator():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit_generator(data_generator(True), len(x_train), epochs)\n-    model.fit_generator(data_generator(True), len(x_train), epochs, validation_data=(x_test, y_test))\n-    model.fit_generator(data_generator(True), len(x_train), epochs,\n-                        validation_data=data_generator(False), num_val_samples=batch_size * 3)\n-    model.fit_generator(data_generator(True), len(x_train), epochs, max_q_size=2)\n+    model.fit_generator(data_generator(True), 5, epochs)\n+    model.fit_generator(data_generator(True), 5, epochs,\n+                        validation_data=(x_test, y_test))\n+    model.fit_generator(data_generator(True), 5, epochs,\n+                        validation_data=data_generator(False),\n+                        validation_steps=3)\n+    model.fit_generator(data_generator(True), 5, epochs, max_q_size=2)\n     model.evaluate(x_train, y_train)\n \n \n@@ -120,8 +122,8 @@ def test_sequential():\n \n     loss = model.evaluate(x_test, y_test)\n \n-    prediction = model.predict_generator(data_generator(x_test, y_test), x_test.shape[0], max_q_size=2)\n-    gen_loss = model.evaluate_generator(data_generator(x_test, y_test, 50), x_test.shape[0], max_q_size=2)\n+    prediction = model.predict_generator(data_generator(x_test, y_test), 1, max_q_size=2)\n+    gen_loss = model.evaluate_generator(data_generator(x_test, y_test, 50), 1, max_q_size=2)\n     pred_loss = K.eval(K.mean(losses.get(model.loss)(K.variable(y_test), K.variable(prediction))))\n \n     assert(np.isclose(pred_loss, loss))\n\n@@ -0,0 +1,242 @@\n+from __future__ import print_function\n+import pytest\n+import numpy as np\n+from keras.models import Sequential\n+from keras.layers.core import Dense\n+from keras.utils.test_utils import keras_test\n+\n+\n+@keras_test\n+def test_multiprocessing_training():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+\n+    def custom_generator():\n+        batch_size = 10\n+        n_samples = 50\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=5,\n+                        epochs=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        workers=4,\n+                        pickle_safe=True)\n+\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=5,\n+                        epochs=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        pickle_safe=False)\n+\n+\n+@keras_test\n+def test_multiprocessing_training_fromfile():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+    np.savez('data.npz', **{'data': arr_data, 'labels': arr_labels})\n+\n+    def custom_generator():\n+\n+        batch_size = 10\n+        n_samples = 50\n+\n+        arr = np.load('data.npz')\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr['data'][start: end]\n+            y = arr['labels'][start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=5,\n+                        epochs=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        workers=2,\n+                        pickle_safe=True)\n+\n+    model.fit_generator(custom_generator(),\n+                        steps_per_epoch=5,\n+                        epochs=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        pickle_safe=False)\n+\n+\n+@keras_test\n+def test_multiprocessing_predicting():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+\n+    def custom_generator():\n+        batch_size = 10\n+        n_samples = 50\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            yield X\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+    model.predict_generator(custom_generator(),\n+                            steps=5,\n+                            max_q_size=10,\n+                            workers=2,\n+                            pickle_safe=True)\n+    model.predict_generator(custom_generator(),\n+                            steps=5,\n+                            max_q_size=10,\n+                            pickle_safe=False)\n+\n+\n+@keras_test\n+def test_multiprocessing_evaluating():\n+    arr_data = np.random.randint(0, 256, (50, 2))\n+    arr_labels = np.random.randint(0, 2, 50)\n+\n+    def custom_generator():\n+        batch_size = 10\n+        n_samples = 50\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    model.evaluate_generator(custom_generator(),\n+                             steps=5,\n+                             max_q_size=10,\n+                             workers=2,\n+                             pickle_safe=True)\n+    model.evaluate_generator(custom_generator(),\n+                             steps=5,\n+                             max_q_size=10,\n+                             pickle_safe=False)\n+\n+\n+@keras_test\n+def test_multiprocessing_fit_error():\n+    batch_size = 10\n+    good_batches = 3\n+\n+    def custom_generator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            yield (np.random.randint(batch_size, 256, (50, 2)),\n+                   np.random.randint(batch_size, 2, 50))\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    samples = batch_size * (good_batches + 1)\n+\n+    with pytest.raises(Exception):\n+        model.fit_generator(\n+            custom_generator(), samples, 1,\n+            workers=4, pickle_safe=True,\n+        )\n+\n+    with pytest.raises(Exception):\n+        model.fit_generator(\n+            custom_generator(), samples, 1,\n+            pickle_safe=False,\n+        )\n+\n+\n+@keras_test\n+def test_multiprocessing_evaluate_error():\n+    batch_size = 10\n+    good_batches = 3\n+\n+    def custom_generator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            yield (np.random.randint(batch_size, 256, (50, 2)),\n+                   np.random.randint(batch_size, 2, 50))\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    with pytest.raises(Exception):\n+        model.evaluate_generator(\n+            custom_generator(), good_batches + 1, 1,\n+            workers=4, pickle_safe=True,\n+        )\n+\n+    with pytest.raises(Exception):\n+        model.evaluate_generator(\n+            custom_generator(), good_batches + 1, 1,\n+            pickle_safe=False,\n+        )\n+\n+\n+@keras_test\n+def test_multiprocessing_predict_error():\n+    batch_size = 10\n+    good_batches = 3\n+\n+    def custom_generator():\n+        \"\"\"Raises an exception after a few good batches\"\"\"\n+        for i in range(good_batches):\n+            yield (np.random.randint(batch_size, 256, (50, 2)),\n+                   np.random.randint(batch_size, 2, 50))\n+        raise RuntimeError\n+\n+    model = Sequential()\n+    model.add(Dense(1, input_shape=(2, )))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    with pytest.raises(Exception):\n+        model.predict_generator(\n+            custom_generator(), good_batches + 1, 1,\n+            workers=4, pickle_safe=True,\n+        )\n+\n+    with pytest.raises(Exception):\n+        model.predict_generator(\n+            custom_generator(), good_batches + 1, 1,\n+            pickle_safe=False,\n+        )\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
