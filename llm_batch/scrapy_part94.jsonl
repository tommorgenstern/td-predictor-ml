{"custom_id": "scrapy#1868ede5492219fbd3eca90dc897c4293eae6aa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 17 | Churn Cumulative: 19 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,7 +30,7 @@ import types, weakref\n from scrapy.xlib.pydispatch import saferef, robustapply, errors\n \n __author__ = \"Patrick K. O'Brien <pobrien@orbtech.com>\"\n-__cvsid__ = \"$Id$\"\n+__cvsid__ = \"$Id: dispatcher.py,v 1.1.1.1 2006/07/07 15:59:38 mcfletch Exp $\"\n __version__ = \"$Revision: 1.1.1.1 $\"[11:-2]\n \n try:\n@@ -377,7 +377,12 @@ def _removeReceiver(receiver):\n \t\t# During module cleanup the mapping will be replaced with None\n \t\treturn False\n \tbackKey = id(receiver)\n-\tfor senderkey in sendersBack.get(backKey,()):\n+\ttry:\n+\t\tbackSet = sendersBack.pop(backKey)\n+\texcept KeyError, err:\n+\t\treturn False \n+\telse:\n+\t\tfor senderkey in backSet:\n \t\t\ttry:\n \t\t\t\tsignals = connections[senderkey].keys()\n \t\t\texcept KeyError,err:\n@@ -394,10 +399,6 @@ def _removeReceiver(receiver):\n \t\t\t\t\t\texcept Exception, err:\n \t\t\t\t\t\t\tpass\n \t\t\t\t\t_cleanupConnections(senderkey, signal)\n-\ttry:\n-\t\tdel sendersBack[ backKey ]\n-\texcept KeyError:\n-\t\tpass\n \n def _cleanupConnections(senderkey, signal):\n \t\"\"\"Delete any empty signals for senderkey. Delete senderkey if empty.\"\"\"\n\n@@ -22,7 +22,7 @@ def safeRef(target, onDelete = None):\n \t\t\t\tonDelete=onDelete\n \t\t\t)\n \t\t\treturn reference\n-\tif callable(onDelete):\n+\tif onDelete is not None:\n \t\treturn weakref.ref(target, onDelete)\n \telse:\n \t\treturn weakref.ref( target )\n@@ -120,7 +120,7 @@ class BoundMethodWeakref(object):\n \t\tself.key = self.calculateKey( target )\n \t\tself.weakSelf = weakref.ref(target.im_self, remove)\n \t\tself.weakFunc = weakref.ref(target.im_func, remove)\n-\t\tself.selfName = str(target.im_self)\n+\t\tself.selfName = target.im_self.__class__.__name__\n \t\tself.funcName = str(target.im_func.__name__)\n \tdef calculateKey( cls, target ):\n \t\t\"\"\"Calculate the reference key for this reference\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d42e5fdbac0ad458c3208236616370fecc7c2c5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 220 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,16 +25,16 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n         self.feed(response_text)\n         self.close()\n \n-        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n-\n         ret = []\n         base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n-        for link in links:\n+        for link in self.links:\n             link.url = urljoin_rfc(base_url, link.url, response_encoding)\n             link.url = safe_url_string(link.url, response_encoding)\n             link.text = str_to_unicode(link.text, response_encoding)\n             ret.append(link)\n \n+        ret = unique_list(ret, key=lambda link: link.url) if self.unique else ret\n+\n         return ret\n \n     def extract_links(self, response):\n\n@@ -110,6 +110,13 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n               Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n               Link(url='http://example.com/sample3.html', text=u'sample 3 repetition') ])\n \n+        lx = SgmlLinkExtractor(allow=('sample', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+              ])\n+\n         lx = SgmlLinkExtractor(allow=('sample', ), deny=('3', ))\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n             [ Link(url='http://example.com/sample1.html', text=u''),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf35e09d35205db940261754147b2511aedf484b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 89 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,14 +51,14 @@ class ItemLoader(object):\n     def get_input_processor(self, field_name):\n         proc = getattr(self, '%s_in' % field_name, None)\n         if not proc:\n-            proc = self.item.fields[field_name].get('input_processor', \\\n+            proc = self._get_item_field_attr(field_name, 'input_processor', \\\n                 self.default_input_processor)\n         return proc\n \n     def get_output_processor(self, field_name):\n         proc = getattr(self, '%s_out' % field_name, None)\n         if not proc:\n-            proc = self.item.fields[field_name].get('output_processor', \\\n+            proc = self._get_item_field_attr(field_name, 'output_processor', \\\n                 self.default_output_processor)\n         return proc\n \n@@ -67,6 +67,12 @@ class ItemLoader(object):\n         proc = wrap_loader_context(proc, self.context)\n         return proc(value)\n \n+    def _get_item_field_attr(self, field_name, key, default=None):\n+        if isinstance(self.item, Item):\n+            value = self.item.fields[field_name].get(key, default)\n+        else:\n+            value = default\n+        return value\n \n class XPathItemLoader(ItemLoader):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddfaf6049fa036b7cec4731d8f772c5afdb57ccb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 35 | Churn Cumulative: 124 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,14 +25,43 @@ class ItemLoader(object):\n         self.context = context\n         self._values = defaultdict(list)\n \n-    def add_value(self, field_name, value):\n+    def add_value(self, field_name, value, *processors):\n+        value = self.get_value(value, *processors)\n+        if not value:\n+            return\n+        if not field_name:\n+            for k,v in value.iteritems():\n+                self._add_value(k, v)\n+        else:\n+            self._add_value(field_name, value)\n+\n+    def replace_value(self, field_name, value, *processors):\n+        value = self.get_value(value, *processors)\n+        if not value:\n+            return\n+        if not field_name:\n+            for k,v in value.iteritems():\n+                self._replace_value(k, v)\n+        else:\n+            self._replace_value(field_name, value)\n+\n+    def _add_value(self, field_name, value):\n         value = arg_to_iter(value)\n         processed_value = self._process_input_value(field_name, value)\n+        if processed_value:\n             self._values[field_name] += arg_to_iter(processed_value)\n \n-    def replace_value(self, field_name, value):\n+    def _replace_value(self, field_name, value):\n         self._values.pop(field_name, None)\n-        self.add_value(field_name, value)\n+        self._add_value(field_name, value)\n+\n+    def get_value(self, value, *processors):\n+        for proc in processors:\n+            if value is None:\n+                break\n+            proc = wrap_loader_context(proc, self.context)\n+            value = proc(value)\n+        return value\n \n     def load_item(self):\n         item = self.item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#913b5db242137c85a326aebd63902e49eebb3089", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 143 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,10 +5,12 @@ See documentation in docs/topics/loaders.rst\n \"\"\"\n \n from collections import defaultdict\n+import re\n \n from scrapy.item import Item\n from scrapy.selector import HtmlXPathSelector\n-from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.misc import arg_to_iter, extract_regex\n+from scrapy.utils.python import flatten\n from .common import wrap_loader_context\n from .processor import Identity\n \n@@ -25,8 +27,8 @@ class ItemLoader(object):\n         self.context = context\n         self._values = defaultdict(list)\n \n-    def add_value(self, field_name, value, *processors):\n-        value = self.get_value(value, *processors)\n+    def add_value(self, field_name, value, *processors, **kw):\n+        value = self.get_value(value, *processors, **kw)\n         if not value:\n             return\n         if not field_name:\n@@ -35,8 +37,8 @@ class ItemLoader(object):\n         else:\n             self._add_value(field_name, value)\n \n-    def replace_value(self, field_name, value, *processors):\n-        value = self.get_value(value, *processors)\n+    def replace_value(self, field_name, value, *processors, **kw):\n+        value = self.get_value(value, *processors, **kw)\n         if not value:\n             return\n         if not field_name:\n@@ -55,7 +57,12 @@ class ItemLoader(object):\n         self._values.pop(field_name, None)\n         self._add_value(field_name, value)\n \n-    def get_value(self, value, *processors):\n+    def get_value(self, value, *processors, **kw):\n+        regex = kw.get('re', None)\n+        if regex:\n+            value = arg_to_iter(value)\n+            value = flatten([extract_regex(regex, x) for x in value])\n+\n         for proc in processors:\n             if value is None:\n                 break\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6c315552cabd3a8bd5b966e339d1f1f7866ca76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 528 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,6 +61,9 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_collected_values('name'), [u'Marta', u'Pepe'])\n         self.assertEqual(il.get_output_value('name'), [u'Marta', u'Pepe'])\n \n+        il.add_value(None, u'Jim', lambda x: {'name': x})\n+        self.assertEqual(il.get_collected_values('name'), [u'Marta', u'Pepe', u'Jim'])\n+\n     def test_replace_value(self):\n         il = TestItemLoader()\n         il.replace_value('name', u'marta')\n@@ -70,6 +73,21 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_collected_values('name'), [u'Pepe'])\n         self.assertEqual(il.get_output_value('name'), [u'Pepe'])\n \n+        il.replace_value(None, u'Jim', lambda x: {'name': x})\n+        self.assertEqual(il.get_collected_values('name'), [u'Jim'])\n+\n+    def test_get_value(self):\n+        il = NameItemLoader()\n+        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), unicode.upper))\n+        self.assertEqual([u'foo', u'bar'], il.get_value([u'name:foo', u'name:bar'], re=u'name:(.*)$'))\n+        self.assertEqual(u'foo', il.get_value([u'name:foo', u'name:bar'], TakeFirst(), re=u'name:(.*)$'))\n+\n+        il.add_value('name', [u'name:foo', u'name:bar'], TakeFirst(), re=u'name:(.*)$')\n+        self.assertEqual([u'foo'], il.get_collected_values('name'))\n+        il.replace_value('name', u'name:bar', re=u'name:(.*)$')\n+        self.assertEqual([u'bar'], il.get_collected_values('name'))\n+\n+\n     def test_iter_on_input_processor_input(self):\n         class NameFirstItemLoader(NameItemLoader):\n             name_in = TakeFirst()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd844f690be3221fb813f7de24b365cf5d13ba6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 38 | Churn Cumulative: 709 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -124,13 +124,19 @@ class XPathItemLoader(ItemLoader):\n         context.update(selector=selector, response=response)\n         super(XPathItemLoader, self).__init__(item, **context)\n \n-    def add_xpath(self, field_name, xpath, re=None):\n-        self.add_value(field_name, self._get_values(field_name, xpath, re))\n+    def add_xpath(self, field_name, xpath, *processors, **kw):\n+        values = self._get_values(xpath, **kw)\n+        self.add_value(field_name, values, *processors, **kw)\n \n-    def replace_xpath(self, field_name, xpath, re=None):\n-        self.replace_value(field_name, self._get_values(field_name, xpath, re))\n+    def replace_xpath(self, field_name, xpath, *processors, **kw):\n+        values = self._get_values(xpath, **kw)\n+        self.replace_value(field_name, values, *processors, **kw)\n \n-    def _get_values(self, field_name, xpath, re):\n-        x = self.selector.select(xpath)\n-        return x.re(re) if re else x.extract()\n+    def get_xpath(self, xpath, *processors, **kw):\n+        values = self._get_values(xpath, **kw)\n+        return self.get_value(values, *processors, **kw)\n+\n+    def _get_values(self, xpaths, **kw):\n+        xpaths = arg_to_iter(xpaths)\n+        return flatten([self.selector.select(xpath).extract() for xpath in xpaths])\n \n\n@@ -360,6 +360,24 @@ class XPathItemLoaderTest(unittest.TestCase):\n         l.replace_xpath('name', '//p/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Paragraph'])\n \n+        l.replace_xpath('name', ['//p/text()', '//div/text()'])\n+        self.assertEqual(l.get_output_value('name'), [u'Paragraph', 'Marta'])\n+\n+    def test_get_xpath(self):\n+        l = TestXPathItemLoader(response=self.response)\n+        self.assertEqual(l.get_xpath('//p/text()'), [u'paragraph'])\n+        self.assertEqual(l.get_xpath('//p/text()', TakeFirst()), u'paragraph')\n+        self.assertEqual(l.get_xpath('//p/text()', TakeFirst(), re='pa'), u'pa')\n+\n+        self.assertEqual(l.get_xpath(['//p/text()', '//div/text()']), [u'paragraph', 'marta'])\n+\n+    def test_replace_xpath_multi_fields(self):\n+        l = TestXPathItemLoader(response=self.response)\n+        l.add_xpath(None, '//div/text()', TakeFirst(), lambda x: {'name': x})\n+        self.assertEqual(l.get_output_value('name'), [u'Marta'])\n+        l.replace_xpath(None, '//p/text()', TakeFirst(), lambda x: {'name': x})\n+        self.assertEqual(l.get_output_value('name'), [u'Paragraph'])\n+\n     def test_replace_xpath_re(self):\n         l = TestXPathItemLoader(response=self.response)\n         self.assert_(l.selector)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#849730178403d331e7ba8f62ffbdf78727568388", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 225 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertEqual(list(arg_to_iter(100)), [100])\n         self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])\n         self.assertEqual(list(arg_to_iter([1,2,3])), [1,2,3])\n+        self.assertEqual(list(arg_to_iter({'a':1})), [{'a': 1}])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -9,10 +9,12 @@ from scrapy.utils.markup import remove_entities\n def arg_to_iter(arg):\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n     value, or an iterable.\n+\n+    Exception: if arg is a dict, [arg] will be returned\n     \"\"\"\n     if arg is None:\n         return []\n-    elif hasattr(arg, '__iter__'):\n+    elif not isinstance(arg, dict) and hasattr(arg, '__iter__'):\n         return arg\n     else:\n         return [arg]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f53a723064c44c1f75ab0b7be41bd1c7edbd001", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 550 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,6 +61,10 @@ class ItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_collected_values('name'), [u'Marta', u'Pepe'])\n         self.assertEqual(il.get_output_value('name'), [u'Marta', u'Pepe'])\n \n+        # test add object value\n+        il.add_value('summary', {'key': 1})\n+        self.assertEqual(il.get_collected_values('summary'), [{'key': 1}])\n+\n         il.add_value(None, u'Jim', lambda x: {'name': x})\n         self.assertEqual(il.get_collected_values('name'), [u'Marta', u'Pepe', u'Jim'])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6059221716529927db2d34ddd813ab2ea3b95588", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 612 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -33,6 +33,7 @@ class Compose(object):\n \n     def __init__(self, *functions, **default_loader_context):\n         self.functions = functions\n+        self.stop_on_none = default_loader_context.get('stop_on_none', True)\n         self.default_loader_context = default_loader_context\n     \n     def __call__(self, value, loader_context=None):\n@@ -42,6 +43,8 @@ class Compose(object):\n             context = self.default_loader_context\n         wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n         for func in wrapped_funcs:\n+            if value is None and self.stop_on_none:\n+                break\n             value = func(value)\n         return value\n \n\n@@ -321,6 +321,10 @@ class ProcessorsTest(unittest.TestCase):\n     def test_compose(self):\n         proc = Compose(lambda v: v[0], str.upper)\n         self.assertEqual(proc(['hello', 'world']), 'HELLO')\n+        proc = Compose(str.upper)\n+        self.assertEqual(proc(None), None)\n+        proc = Compose(str.upper, stop_on_none=False)\n+        self.assertRaises(TypeError, proc, None)\n \n     def test_mapcompose(self):\n         filter_world = lambda x: None if x == 'world' else x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f2363afe6f7045dc3b09ff39ef7e80db780462bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 26 | Churn Cumulative: 169 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n         self.unique = unique\n \n     def _extract_links(self, response_text, response_url, response_encoding):\n+        \"\"\" Do the real extraction work \"\"\"\n         self.reset()\n         self.feed(response_text)\n         self.close()\n@@ -33,13 +34,21 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n             link.text = str_to_unicode(link.text, response_encoding)\n             ret.append(link)\n \n-        ret = unique_list(ret, key=lambda link: link.url) if self.unique else ret\n-\n         return ret\n \n+    def _process_links(self, links):\n+        \"\"\" Normalize and filter extracted links\n+\n+        The subclass should override it if neccessary\n+        \"\"\"\n+        links = unique_list(links, key=lambda link: link.url) if self.unique else links\n+        return links\n+\n     def extract_links(self, response):\n         # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url, response.encoding)\n+        links = self._extract_links(response.body, response.url, response.encoding)\n+        links = self._process_links(links)\n+        return links\n \n     def reset(self):\n         FixedSGMLParser.reset(self)\n@@ -93,12 +102,16 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n     def extract_links(self, response):\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n-            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \\\n+            html = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \\\n                 for xpath_expr in self.restrict_xpaths)\n-            links = self._extract_links(html_slice, response.url, response.encoding)\n         else:\n-            links = BaseSgmlLinkExtractor.extract_links(self, response)\n+            html = response.body\n \n+        links = self._extract_links(html, response.url, response.encoding)\n+        links = self._process_links(links)\n+        return links\n+\n+    def _process_links(self, links):\n         links = [link for link in links if _is_valid_url(link.url)]\n \n         if self.allow_res:\n@@ -114,6 +127,7 @@ class SgmlLinkExtractor(BaseSgmlLinkExtractor):\n             for link in links:\n                 link.url = canonicalize_url(link.url)\n \n+        links = BaseSgmlLinkExtractor._process_links(self, links)\n         return links\n \n     def matches(self, url):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cce9c4da492f1a383c5cb9c80f7e07495267021e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 21 | Churn Cumulative: 130 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,12 +6,12 @@ See documentation in docs/topics/spider-middleware.rst\n from scrapy.core.exceptions import IgnoreRequest\n \n \n-class HttpErrorException(IgnoreRequest):\n+class HttpError(IgnoreRequest):\n     \"\"\"A non-200 response was filtered\"\"\"\n \n     def __init__(self, response, *args, **kwargs):\n         self.response = response\n-        super(HttpErrorException, self).__init__(*args, **kwargs)\n+        super(HttpError, self).__init__(*args, **kwargs)\n \n \n class HttpErrorMiddleware(object):\n@@ -25,5 +25,8 @@ class HttpErrorMiddleware(object):\n             allowed_statuses = getattr(spider, 'handle_httpstatus_list', ())\n         if response.status in allowed_statuses:\n             return\n-        raise HttpErrorException(response, 'Ignoring non-200 response')\n+        raise HttpError(response, 'Ignoring non-200 response')\n \n+    def process_spider_exception(self, response, exception, spider):\n+        if isinstance(exception, HttpError):\n+            return []\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n-from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpErrorException\n+from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError\n \n \n class TestHttpErrorMiddleware(TestCase):\n@@ -20,9 +20,17 @@ class TestHttpErrorMiddleware(TestCase):\n     def test_process_spider_input(self):\n         self.assertEquals(None,\n                 self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertRaises(HttpErrorException,\n+        self.assertRaises(HttpError,\n                 self.mw.process_spider_input, self.res404, self.spider)\n \n+    def test_process_spider_exception(self):\n+        self.assertEquals([],\n+                self.mw.process_spider_exception(self.res404, \\\n+                        HttpError(self.res404), self.spider))\n+        self.assertEquals(None,\n+                self.mw.process_spider_exception(self.res404, \\\n+                        Exception(), self.spider))\n+\n     def test_handle_httpstatus_list(self):\n         res = self.res404.copy()\n         res.request = Request('http://scrapytest.org',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c1feb7ae4ae7a7a55b9180cba7a53fd55fbcb32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 268 | Files Changed: 4 | Hunks: 14 | Methods Changed: 22 | Complexity Δ (Sum/Max): -32/0 | Churn Δ: 293 | Churn Cumulative: 960 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: 0.22302158273381295\n\nDIFF:\n@@ -1,24 +0,0 @@\n-\"\"\"\n-A downloader middleware for signing AWS requests just before they get into the\n-downloader. It is important to sign as close to the downloader as possible\n-because Amazon Web Service use timestamps for authentication.\n-\"\"\"\n-\n-import os\n-from time import strftime, gmtime\n-from scrapy.utils.aws import sign_request\n-from scrapy.conf import settings\n-\n-\n-class AWSMiddleware(object):\n-\n-    def __init__(self):\n-        self.access_key = settings['AWS_ACCESS_KEY_ID'] or \\\n-            os.environ.get('AWS_ACCESS_KEY_ID')\n-        self.secret_key = settings['AWS_SECRET_ACCESS_KEY'] or \\\n-            os.environ.get('AWS_SECRET_ACCESS_KEY')\n-\n-    def process_request(self, request, spider):\n-        if request.meta.get('sign_s3_request'):\n-            request.headers['Date'] = strftime(\"%a, %d %b %Y %H:%M:%S GMT\", gmtime())\n-            sign_request(request, self.access_key, self.secret_key)\n\n@@ -14,18 +14,15 @@ import Image\n from cStringIO import StringIO\n from collections import defaultdict\n \n-from twisted.internet import defer\n+from twisted.internet import defer, threads\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import DropItem, NotConfigured, IgnoreRequest\n-from scrapy.spider import BaseSpider\n from scrapy.contrib.pipeline.media import MediaPipeline\n-from scrapy.http import Request\n from scrapy.conf import settings\n \n \n@@ -78,83 +75,47 @@ class FSImagesStore(object):\n             seen.add(dirname)\n \n \n-class _S3AmazonAWSSpider(BaseSpider):\n-    \"\"\"This spider is used for uploading images to Amazon S3\n-\n-    It is basically not a crawling spider like a normal spider is, this spider is\n-    a placeholder that allows us to open a different slot in downloader and use it\n-    for uploads to S3.\n-\n-    The use of another downloader slot for S3 images avoid the effect of normal\n-    spider downloader slot to be affected by requests to a complete different\n-    domain (s3.amazonaws.com).\n-\n-    It means that a spider that uses download_delay or alike is not going to be\n-    delayed even more because it is uploading images to s3.\n-    \"\"\"\n-    name = \"s3.amazonaws.com\"\n-    start_urls = ['http://s3.amazonaws.com/']\n-    max_concurrent_requests = 100\n-\n-\n class S3ImagesStore(object):\n \n-    request_priority = 1000\n+    AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n+    AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n \n     def __init__(self, uri):\n         assert uri.startswith('s3://')\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n-        self._set_custom_spider()\n-\n-    def _set_custom_spider(self):\n-        use_custom_spider = bool(settings['IMAGES_S3STORE_SPIDER'])\n-        if use_custom_spider:\n-            self.s3_spider = _S3AmazonAWSSpider()\n-        else:\n-            self.s3_spider = None\n \n     def stat_image(self, key, info):\n-        def _onsuccess(response):\n-            if response.status == 200:\n-                checksum = response.headers['Etag'].strip('\"')\n-                last_modified = response.headers['Last-Modified']\n+        def _onsuccess(boto_key):\n+            checksum = boto_key.etag.strip('\"')\n+            last_modified = boto_key.last_modified\n             modified_tuple = rfc822.parsedate_tz(last_modified)\n             modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n             return {'checksum': checksum, 'last_modified': modified_stamp}\n \n-        req = self._build_request(key, method='HEAD')\n-        return self._download_request(req, info).addCallback(_onsuccess)\n+        return self._get_boto_key(key).addCallback(_onsuccess)\n+\n+    def _get_boto_bucket(self):\n+        from boto.s3.connection import S3Connection\n+        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY)\n+        return c.get_bucket(self.bucket, validate=False)\n+\n+    def _get_boto_key(self, key):\n+        b = self._get_boto_bucket()\n+        key_name = '%s%s' % (self.prefix, key)\n+        return threads.deferToThread(b.get_key, key_name)\n \n     def persist_image(self, key, image, buf, info):\n         \"\"\"Upload image to S3 storage\"\"\"\n         width, height = image.size\n-        headers = {\n-                'Content-Type': 'image/jpeg',\n-                'X-Amz-Acl': 'public-read',\n-                'X-Amz-Meta-Width': str(width),\n-                'X-Amz-Meta-Height': str(height),\n-                'Cache-Control': 'max-age=172800',\n-                }\n-\n+        headers = {'Cache-Control': 'max-age=172800'} # 2 days of cache\n+        b = self._get_boto_bucket()\n+        key_name = '%s%s' % (self.prefix, key)\n+        k = b.new_key(key_name)\n+        k.set_metadata('width', str(width))\n+        k.set_metadata('height', str(height))\n         buf.seek(0)\n-        req = self._build_request(key, method='PUT', body=buf.read(), headers=headers)\n-        return self._download_request(req, info)\n-\n-    def _build_request(self, key, method, body=None, headers=None):\n-        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket, self.prefix, key)\n-        return Request(url, method=method, body=body, headers=headers, \\\n-                meta={'sign_s3_request': True}, priority=self.request_priority)\n-\n-    def _download_request(self, request, info):\n-        \"\"\"This method is used for HEAD and PUT requests sent to amazon S3\n-\n-        It tries to use a specific spider domain for uploads, or defaults\n-        to current domain spider.\n-        \"\"\"\n-        if self.s3_spider:\n-            # need to use schedule to auto-open domain\n-            return scrapyengine.schedule(request, self.s3_spider)\n-        return scrapyengine.download(request, info.spider)\n+        return threads.deferToThread(k.set_contents_from_file, buf, headers, \\\n+            policy='public-read')\n \n \n class ImagesPipeline(MediaPipeline):\n\n@@ -1,108 +0,0 @@\n-from unittest import TestCase, main\n-\n-from scrapy.utils import aws\n-from scrapy.http import Request\n-\n-# just some random keys. keys are provided by amazon developer guide at\n-# http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n-# and the tests described here are the examples from that manual\n-\n-AWS_ACCESS_KEY_ID = '0PN5J17HBGZHT7JJ3X82'\n-AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n-\n-\n-class ScrapyAWSTest(TestCase):\n-    def test_cannonical_string1(self):\n-        cs = aws.canonical_string('GET', '/johnsmith/photos/puppy.jpg', {\n-            'Host': 'johnsmith.s3.amazonaws.com',\n-            'Date': 'Tue, 27 Mar 2007 19:36:42 +0000',\n-            })\n-        self.assertEqual(cs, \\\n-                '''GET\\n\\n\\nTue, 27 Mar 2007 19:36:42 +0000\\n/johnsmith/photos/puppy.jpg''')\n-\n-    def test_cannonical_string2(self):\n-        cs = aws.canonical_string('PUT', '/johnsmith/photos/puppy.jpg', {\n-            'Content-Type': 'image/jpeg',\n-            'Host': 'johnsmith.s3.amazonaws.com',\n-            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',\n-            'Content-Length': '94328',\n-            })\n-        self.assertEqual(cs, \\\n-                '''PUT\\n\\nimage/jpeg\\nTue, 27 Mar 2007 21:15:45 +0000\\n/johnsmith/photos/puppy.jpg''')\n-\n-    def test_request_signing1(self):\n-        # gets an object from the johnsmith bucket.\n-        req = Request('http://johnsmith.s3.amazonaws.com/photos/puppy.jpg', headers={\n-            'Date': 'Tue, 27 Mar 2007 19:36:42 +0000',\n-            })\n-        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n-        self.assertEqual(req.headers['Authorization'], \\\n-                'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')\n-\n-    def test_request_signing2(self):\n-        # puts an object into the johnsmith bucket.\n-        req = Request('http://johnsmith.s3.amazonaws.com/photos/puppy.jpg', method='PUT', headers={\n-            'Content-Type': 'image/jpeg',\n-            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',\n-            'Content-Length': '94328',\n-            })\n-        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n-        self.assertEqual(req.headers['Authorization'], \\\n-                'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')\n-\n-    def test_request_signing3(self):\n-        # lists the content of the johnsmith bucket.\n-        req = Request('http://johnsmith.s3.amazonaws.com/?prefix=photos&max-keys=50&marker=puppy', \\\n-                method='GET', headers={\n-                    'User-Agent': 'Mozilla/5.0',\n-                    'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',\n-                    })\n-        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n-        self.assertEqual(req.headers['Authorization'], \\\n-                'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')\n-\n-    def test_request_signing4(self):\n-        # fetches the access control policy sub-resource for the 'johnsmith' bucket.\n-        req = Request('http://johnsmith.s3.amazonaws.com/?acl', \\\n-                method='GET', headers={\n-                    'Date': 'Tue, 27 Mar 2007 19:44:46 +0000',\n-                    })\n-        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n-        self.assertEqual(req.headers['Authorization'], \\\n-                'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n-\n-    def test_request_signing5(self):\n-        # deletes an object from the 'johnsmith' bucket using the path-style and Date alternative.\n-        req = Request('http://johnsmith.s3.amazonaws.com/photos/puppy.jpg', \\\n-                method='DELETE', headers={\n-                    'Date': 'Tue, 27 Mar 2007 21:20:27 +0000',\n-                    'x-amz-date': 'Tue, 27 Mar 2007 21:20:26 +0000',\n-                    })\n-        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n-        self.assertEqual(req.headers['Authorization'], \\\n-                'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n-\n-    def test_request_signing6(self):\n-        # uploads an object to a CNAME style virtual hosted bucket with metadata.\n-        req = Request('http://static.johnsmith.net:8080/db-backup.dat.gz', \\\n-                method='PUT', headers={\n-                    'User-Agent': 'curl/7.15.5',\n-                    'Host': 'static.johnsmith.net:8080',\n-                    'Date': 'Tue, 27 Mar 2007 21:06:08 +0000',\n-                    'x-amz-acl': 'public-read',\n-                    'content-type': 'application/x-download',\n-                    'Content-MD5': '4gJE4saaMU4BqNR0kLY+lw==',\n-                    'X-Amz-Meta-ReviewedBy': 'joe@johnsmith.net,jane@johnsmith.net',\n-                    'X-Amz-Meta-FileChecksum': '0x02661779',\n-                    'X-Amz-Meta-ChecksumAlgorithm': 'crc32',\n-                    'Content-Disposition': 'attachment; filename=database.dat',\n-                    'Content-Encoding': 'gzip',\n-                    'Content-Length': '5913339',\n-                    })\n-        aws.sign_request(req, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n-        self.assertEqual(req.headers['Authorization'], \\\n-                'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n-\n-\n-if __name__ == '__main__':\n-    main()\n\n@@ -1,72 +0,0 @@\n-\"\"\"Helper function for working with Amazon Web Services\"\"\"\n-\n-import re\n-import time\n-import hmac\n-import base64\n-import hashlib\n-from urlparse import urlsplit\n-\n-AMAZON_HEADER_PREFIX = 'x-amz-'\n-\n-# generates the aws canonical string for the given parameters\n-def canonical_string(method, path, headers, expires=None):\n-    interesting_headers = {}\n-    for key in headers:\n-        lk = key.lower()\n-        if lk in set(['content-md5', 'content-type', 'date']) \\\n-                or lk.startswith(AMAZON_HEADER_PREFIX):\n-            interesting_headers[lk] = headers[key].strip()\n-\n-    # these keys get empty strings if they don't exist\n-    interesting_headers.setdefault('content-type', '')\n-    interesting_headers.setdefault('content-md5', '')\n-\n-    # just in case someone used this.  it's not necessary in this lib.\n-    if 'x-amz-date' in interesting_headers:\n-        interesting_headers['date'] = ''\n-\n-    # if you're using expires for query string auth, then it trumps date\n-    # (and x-amz-date)\n-    if expires:\n-        interesting_headers['date'] = str(expires)\n-\n-    sorted_header_keys = interesting_headers.keys()\n-    sorted_header_keys.sort()\n-\n-    buf = \"%s\\n\" % method\n-    for key in sorted_header_keys:\n-        if key.startswith(AMAZON_HEADER_PREFIX):\n-            buf += \"%s:%s\\n\" % (key, interesting_headers[key])\n-        else:\n-            buf += \"%s\\n\" % interesting_headers[key]\n-\n-    # don't include anything after the first ? in the resource...\n-    buf += \"%s\" % path.split('?')[0]\n-\n-    # ...unless there is an acl or torrent parameter\n-    if re.search(\"[&?]acl($|=|&)\", path):\n-        buf += \"?acl\"\n-    elif re.search(\"[&?]logging($|=|&)\", path):\n-        buf += \"?logging\"\n-    elif re.search(\"[&?]torrent($|=|&)\", path):\n-        buf += \"?torrent\"\n-    elif re.search(\"[&?]location($|=|&)\", path):\n-        buf += \"?location\"\n-\n-    return buf\n-\n-def sign_request(req, accesskey, secretkey):\n-    if 'Date' not in req.headers:\n-        req.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n-\n-    parsed = urlsplit(req.url)\n-    bucket = parsed.hostname.replace('.s3.amazonaws.com','')\n-    key = '%s?%s' % (parsed.path, parsed.query) if parsed.query else parsed.path\n-    fqkey = '/%s%s' % (bucket, key)\n-\n-    c_string = canonical_string(req.method, fqkey, req.headers)\n-    _hmac = hmac.new(secretkey, digestmod=hashlib.sha1)\n-    _hmac.update(c_string)\n-    b64_hmac = base64.encodestring(_hmac.digest()).strip()\n-    req.headers['Authorization'] = \"AWS %s:%s\" % (accesskey, b64_hmac)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cae22930c8952f91280070bd6407a745826c9854", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 318 | Lines Deleted: 161 | Files Changed: 15 | Hunks: 78 | Methods Changed: 69 | Complexity Δ (Sum/Max): 18/17 | Churn Δ: 479 | Churn Cumulative: 4154 | Contributors (this commit): 6 | Commits (past 90d): 41 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from scrapy import log\n from scrapy.command import ScrapyCommand\n+from scrapy.core.queue import ExecutionQueue\n from scrapy.core.manager import scrapymanager\n from scrapy.conf import settings\n from scrapy.http import Request\n@@ -31,23 +32,25 @@ class Command(ScrapyCommand):\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n+        q = ExecutionQueue()\n         urls, names = self._split_urls_and_names(args)\n         for name in names:\n-            scrapymanager.crawl_spider_name(name)\n+            q.append_spider_name(name)\n \n         if opts.spider:\n             try:\n                 spider = spiders.create(opts.spider)\n                 for url in urls:\n-                    scrapymanager.crawl_url(url, spider)\n+                    q.append_url(url, spider)\n             except KeyError:\n-                log.msg('Could not find spider: %s' % opts.spider, log.ERROR)\n+                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n             for name, urls in self._group_urls_by_spider(urls):\n                 spider = spiders.create(name)\n                 for url in urls:\n-                    scrapymanager.crawl_url(url, spider)\n+                    q.append_url(url, spider)\n \n+        scrapymanager.queue = q\n         scrapymanager.start()\n \n     def _group_urls_by_spider(self, urls):\n\n@@ -28,28 +28,27 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n             help=\"print response HTTP headers instead of body\")\n \n+    def _print_response(self, response, opts):\n+        if opts.headers:\n+            pprint.pprint(response.headers)\n+        else:\n+            print response.body\n+\n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n             return False\n-        responses = [] # to collect downloaded responses\n-        request = Request(args[0], callback=responses.append, dont_filter=True)\n+        cb = lambda x: self._print_response(x, opts)\n+        request = Request(args[0], callback=cb, dont_filter=True)\n \n+        spider = None\n         if opts.spider:\n             try:\n                 spider = spiders.create(opts.spider)\n             except KeyError:\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n-        else:\n-            spider = scrapymanager._create_spider_for_request(request, \\\n-                BaseSpider('default'))\n \n-        scrapymanager.crawl_request(request, spider)\n+        scrapymanager.configure()\n+        scrapymanager.queue.append_request(request, spider, \\\n+            default_spider=BaseSpider('default'))\n         scrapymanager.start()\n \n-        # display response\n-        if responses:\n-            if opts.headers:\n-                pprint.pprint(responses[0].headers)\n-            else:\n-                print responses[0].body\n-\n\n@@ -8,8 +8,6 @@ from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.url import is_url\n from scrapy import log\n \n-from collections import defaultdict\n-\n class Command(ScrapyCommand):\n \n     requires_project = True\n@@ -75,25 +73,20 @@ class Command(ScrapyCommand):\n         if not len(args) == 1 or not is_url(args[0]):\n             return False\n \n-        request = Request(args[0])\n+        responses = [] # to collect downloaded responses\n+        request = Request(args[0], callback=responses.append)\n \n         if opts.spider:\n             try:\n                 spider = spiders.create(opts.spider)\n             except KeyError:\n-                log.msg('Could not find spider: %s' % opts.spider, log.ERROR)\n+                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n                 return\n         else:\n-            spider = scrapymanager._create_spider_for_request(request, \\\n-                log_none=True, log_multiple=True)\n+            spider = spiders.create_for_request(request)\n \n-        if not spider:\n-            return\n-\n-        responses = [] # to collect downloaded responses\n-        request = request.replace(callback=responses.append)\n-\n-        scrapymanager.crawl_request(request, spider)\n+        scrapymanager.configure()\n+        scrapymanager.queue.append_request(request, spider)\n         scrapymanager.start()\n \n         if not responses:\n\n@@ -54,7 +54,7 @@ class Command(ScrapyCommand):\n         module = _import_file(args[0])\n \n         # schedule spider and start engine\n-        scrapymanager.crawl_spider(module.SPIDER)\n+        scrapymanager.queue.append_spider(module.SPIDER)\n         scrapymanager.start()\n \n         if opts.output:\n\n@@ -1,3 +1,4 @@\n+from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n \n@@ -9,4 +10,6 @@ class Command(ScrapyCommand):\n         return \"Start the Scrapy manager but don't run any spider (idle mode)\"\n \n     def run(self, args, opts):\n-        scrapymanager.start(keep_alive=True)\n+        q = KeepAliveExecutionQueue()\n+        scrapymanager.queue = q\n+        scrapymanager.start()\n\n@@ -34,6 +34,27 @@ class TwistedPluginSpiderManager(object):\n         return [name for name, spider in self._spiders.iteritems()\n                 if url_is_from_spider(request.url, spider)]\n \n+    def create_for_request(self, request, default_spider=None, \\\n+            log_none=False, log_multiple=False, **spider_kwargs):\n+        \"\"\"Create a spider to handle the given Request.\n+\n+        This will look for the spiders that can handle the given request (using\n+        find_by_request) and return a (new) Spider if (and only if) there is\n+        only one Spider able to handle the Request.\n+\n+        If multiple spiders (or no spider) are found, it will return the\n+        default_spider passed. It can optionally log if multiple or no spiders\n+        are found.\n+        \"\"\"\n+        snames = self.find_by_request(request)\n+        if len(snames) == 1:\n+            return self.create(snames[0], **spider_kwargs)\n+        if len(snames) > 1 and log_multiple:\n+            log.msg('More than one spider found for: %s' % request, log.ERROR)\n+        if len(snames) == 0 and log_none:\n+            log.msg('Unable to find spider for: %s' % request, log.ERROR)\n+        return default_spider\n+\n     def list(self):\n         \"\"\"Returns list of spiders available.\"\"\"\n         return self._spiders.keys()\n\n@@ -8,7 +8,7 @@ from scrapy.core import signals\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.management.web import banner\n+from scrapy.management.web import banner, webconsole_discover_module\n from scrapy.conf import settings\n \n class Spiderctl(object):\n@@ -20,8 +20,6 @@ class Spiderctl(object):\n         self.finished = set()\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-\n-        from scrapy.management.web import webconsole_discover_module\n         dispatcher.connect(self.webconsole_discover_module, signal=webconsole_discover_module)\n \n     def spider_opened(self, spider):\n@@ -35,7 +33,7 @@ class Spiderctl(object):\n         if wc_request.args:\n             changes = self.webconsole_control(wc_request)\n \n-        self.scheduled = [s.name for s in scrapyengine.spider_scheduler._pending_spiders]\n+        self.scheduled = [s[0].name for s in scrapymanager.queue.spider_requests]\n         self.idle = [d for d in self.enabled_spiders if d not in self.scheduled\n                                                         and d not in self.running\n                                                         and d not in self.finished]\n@@ -126,8 +124,8 @@ class Spiderctl(object):\n         if \"remove_pending_spiders\" in args:\n             removed = []\n             for name in args[\"remove_pending_spiders\"]:\n-                if scrapyengine.spider_scheduler.remove_pending_spider(name):\n-                    removed.append(name)\n+                q = scrapymanager.queue\n+                q.spider_requests = [x for x in q.spider_requests if x[0].name != name]\n             if removed:\n                 s += \"<p>\"\n                 s += \"Removed scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"remove_pending_spiders\"])\n@@ -135,14 +133,14 @@ class Spiderctl(object):\n         if \"add_pending_spiders\" in args:\n             for name in args[\"add_pending_spiders\"]:\n                 if name not in scrapyengine.scheduler.pending_requests:\n-                    scrapymanager.crawl_spider_name(name)\n+                    scrapymanager.queue.append_spider_name(name)\n             s += \"<p>\"\n             s += \"Scheduled spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"add_pending_spiders\"])\n             s += \"</p>\"\n         if \"rerun_finished_spiders\" in args:\n             for name in args[\"rerun_finished_spiders\"]:\n                 if name not in scrapyengine.scheduler.pending_requests:\n-                    scrapymanager.crawl_spider_name(name)\n+                    scrapymanager.queue.append_spider_name(name)\n                 self.finished.remove(name)\n             s += \"<p>\"\n             s += \"Re-scheduled finished spiders: <ul><li>%s</li></ul>\" % \"</li><li>\".join(args[\"rerun_finished_spiders\"])\n\n@@ -183,10 +183,6 @@ class Downloader(object):\n         site.cancel_request_calls()\n         self._process_queue(spider)\n \n-    def has_capacity(self):\n-        \"\"\"Does the downloader have capacity to handle more spiders\"\"\"\n-        return len(self.sites) < self.concurrent_spiders\n-\n     def is_idle(self):\n         return not self.sites\n \n\n@@ -6,7 +6,7 @@ For more information see docs/topics/architecture.rst\n \"\"\"\n from time import time\n \n-from twisted.internet import reactor, task, defer\n+from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n from scrapy.xlib.pydispatch import dispatcher\n \n@@ -27,57 +27,42 @@ class ExecutionEngine(object):\n \n     def __init__(self):\n         self.configured = False\n-        self.keep_alive = False\n         self.closing = {} # dict (spider -> reason) of spiders being closed\n         self.running = False\n         self.killed = False\n         self.paused = False\n         self._next_request_calls = {}\n-        self._mainloop_task = task.LoopingCall(self._mainloop)\n         self._crawled_logline = load_object(settings['LOG_FORMATTER_CRAWLED'])\n \n-    def configure(self):\n+    def configure(self, spider_closed_callback):\n         \"\"\"\n         Configure execution engine with the given scheduling policy and downloader.\n         \"\"\"\n         self.scheduler = load_object(settings['SCHEDULER'])()\n-        self.spider_scheduler = load_object(settings['SPIDER_SCHEDULER'])()\n         self.downloader = Downloader()\n         self.scraper = Scraper(self)\n         self.configured = True\n+        self._spider_closed_callback = spider_closed_callback\n \n     def start(self):\n         \"\"\"Start the execution engine\"\"\"\n-        if self.running:\n-            return\n+        assert not self.running, \"Engine already running\"\n         self.start_time = time()\n         send_catch_log(signal=signals.engine_started, sender=self.__class__)\n-        self._mainloop_task.start(5.0, now=True)\n-        reactor.callWhenRunning(self._mainloop)\n         self.running = True\n \n     def stop(self):\n         \"\"\"Stop the execution engine gracefully\"\"\"\n-        if not self.running:\n-            return\n+        assert self.running, \"Engine not running\"\n         self.running = False\n-        def before_shutdown():\n         dfd = self._close_all_spiders()\n         return dfd.addBoth(lambda _: self._finish_stopping_engine())\n-        reactor.addSystemEventTrigger('before', 'shutdown', before_shutdown)\n-        if self._mainloop_task.running:\n-            self._mainloop_task.stop()\n-        try:\n-            reactor.stop()\n-        except RuntimeError: # raised if already stopped or in shutdown stage\n-            pass\n \n     def kill(self):\n         \"\"\"Forces shutdown without waiting for pending transfers to finish.\n         stop() must have been called first\n         \"\"\"\n-        if self.running:\n-            return\n+        assert not self.running, \"Call engine.stop() before engine.kill()\"\n         self.killed = True\n \n     def pause(self):\n@@ -92,12 +77,6 @@ class ExecutionEngine(object):\n         return self.scheduler.is_idle() and self.downloader.is_idle() and \\\n             self.scraper.is_idle()\n \n-    def next_spider(self):\n-        spider = self.spider_scheduler.next_spider()\n-        if spider:\n-            self.open_spider(spider)\n-            return True\n-\n     def next_request(self, spider, now=False):\n         \"\"\"Scrape the next request for the spider passed.\n \n@@ -163,7 +142,13 @@ class ExecutionEngine(object):\n     def open_spiders(self):\n         return self.downloader.sites.keys()\n \n+    def has_capacity(self):\n+        \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n+        return len(self.downloader.sites) < self.downloader.concurrent_spiders\n+\n     def crawl(self, request, spider):\n+        assert spider in self.open_spiders, \\\n+            \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n         if not request.deferred.callbacks:\n             log.msg(\"Unable to crawl Request with no callback: %s\" % request,\n                 level=log.ERROR, spider=spider)\n@@ -180,25 +165,9 @@ class ExecutionEngine(object):\n     def schedule(self, request, spider):\n         if spider in self.closing:\n             raise IgnoreRequest()\n-        if not self.scheduler.spider_is_open(spider):\n-            self.scheduler.open_spider(spider)\n-            if self.spider_is_closed(spider): # scheduler auto-open\n-                self.spider_scheduler.add_spider(spider)\n         self.next_request(spider)\n         return self.scheduler.enqueue_request(spider, request)\n \n-    def _mainloop(self):\n-        \"\"\"Add more spiders to be scraped if the downloader has the capacity.\n-\n-        If there is nothing else scheduled then stop the execution engine.\n-        \"\"\"\n-        if not self.running or self.paused:\n-            return\n-\n-        while self.running and self.downloader.has_capacity():\n-            if not self.next_spider():\n-                return self._stop_if_idle()\n-\n     def download(self, request, spider):\n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n@@ -241,14 +210,15 @@ class ExecutionEngine(object):\n         return dwld\n \n     def open_spider(self, spider):\n+        assert self.has_capacity(), \"No free spider slots when opening %r\" % \\\n+            spider.name\n         log.msg(\"Spider opened\", spider=spider)\n-        self.next_request(spider)\n-\n+        self.scheduler.open_spider(spider)\n         self.downloader.open_spider(spider)\n         self.scraper.open_spider(spider)\n         stats.open_spider(spider)\n-\n         send_catch_log(signals.spider_opened, sender=self.__class__, spider=spider)\n+        self.next_request(spider)\n \n     def _spider_idle(self, spider):\n         \"\"\"Called when a spider gets idle. This function is called when there\n@@ -270,11 +240,6 @@ class ExecutionEngine(object):\n         if self.spider_is_idle(spider):\n             self.close_spider(spider, reason='finished')\n \n-    def _stop_if_idle(self):\n-        \"\"\"Call the stop method if the system has no outstanding tasks. \"\"\"\n-        if self.is_idle() and not self.keep_alive:\n-            self.stop()\n-\n     def close_spider(self, spider, reason='cancelled'):\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n         if spider not in self.closing:\n@@ -316,7 +281,7 @@ class ExecutionEngine(object):\n         dfd.addErrback(log.err, \"Unhandled error on SpiderManager.close_spider()\",\n             spider=spider)\n         dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n-        reactor.callLater(0, self._mainloop)\n+        reactor.callLater(0, self._spider_closed_callback)\n         return dfd\n \n     def _finish_stopping_engine(self):\n\n@@ -1,28 +1,26 @@\n import signal\n \n-from twisted.internet import reactor\n+from twisted.internet import reactor, defer\n \n+from scrapy.core.engine import scrapyengine\n+from scrapy.core.queue import ExecutionQueue\n from scrapy.extension import extensions\n from scrapy import log\n-from scrapy.http import Request\n-from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n \n class ExecutionManager(object):\n \n     def __init__(self):\n-        self.interrupted = False\n         self.configured = False\n         self.control_reactor = True\n+        self.engine = scrapyengine\n \n-    def configure(self, control_reactor=True):\n+    def configure(self, control_reactor=True, queue=None):\n         self.control_reactor = control_reactor\n         if control_reactor:\n             install_shutdown_handlers(self._signal_shutdown)\n-        reactor.addSystemEventTrigger('before', 'shutdown', scrapyengine.stop)\n \n         if not log.started:\n             log.start()\n@@ -33,68 +31,54 @@ class ExecutionManager(object):\n         log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n             level=log.DEBUG)\n \n-        scrapyengine.configure()\n+        self.queue = queue or ExecutionQueue()\n+        self.engine.configure(self._spider_closed)\n         self.configured = True\n \n-    def crawl_url(self, url, spider=None):\n-        \"\"\"Schedule given url for crawling.\"\"\"\n-        if spider is None:\n-            spider = self._create_spider_for_request(Request(url), log_none=True, \\\n-                log_multiple=True)\n+    @defer.inlineCallbacks\n+    def _start_next_spider(self):\n+        spider, requests = yield defer.maybeDeferred(self.queue.get_next)\n         if spider:\n-            requests = arg_to_iter(spider.make_requests_from_url(url))\n-            self._crawl_requests(requests, spider)\n+            self._start_spider(spider, requests)\n+        if self.engine.has_capacity() and not self._nextcall.active():\n+            self._nextcall = reactor.callLater(self.queue.polling_delay, \\\n+                self._start_next_spider)\n \n-    def crawl_request(self, request, spider=None):\n-        \"\"\"Schedule request for crawling.\"\"\"\n-        assert self.configured, \"Scrapy Manager not yet configured\"\n-        if spider is None:\n-            spider = self._create_spider_for_request(request, log_none=True, \\\n-                log_multiple=True)\n-        if spider:\n-            scrapyengine.crawl(request, spider)\n+    @defer.inlineCallbacks\n+    def _start_spider(self, spider, requests):\n+        \"\"\"Don't call this method. Use self.queue to start new spiders\"\"\"\n+        yield defer.maybeDeferred(self.engine.open_spider, spider)\n+        for request in requests:\n+            self.engine.crawl(request, spider)\n \n-    def crawl_spider_name(self, name):\n-        \"\"\"Schedule given spider by name for crawling.\"\"\"\n-        try:\n-            spider = spiders.create(name)\n-        except KeyError:\n-            log.msg('Could not find spider: %s' % name, log.ERROR)\n-        else:\n-            self.crawl_spider(spider)\n+    @defer.inlineCallbacks\n+    def _spider_closed(self):\n+        if not self.engine.open_spiders:\n+            is_finished = yield defer.maybeDeferred(self.queue.is_finished)\n+            if is_finished:\n+                self.stop()\n+                return\n+        if self.engine.has_capacity():\n+            self._start_next_spider()\n \n-    def crawl_spider(self, spider):\n-        \"\"\"Schedule spider for crawling.\"\"\"\n-        requests = spider.start_requests()\n-        self._crawl_requests(requests, spider)\n-\n-    def _crawl_requests(self, requests, spider):\n-        \"\"\"Shortcut to schedule a list of requests\"\"\"\n-        for req in requests:\n-            self.crawl_request(req, spider)\n-\n-    def start(self, keep_alive=False):\n-        \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n-        scrapyengine.keep_alive = keep_alive\n-        scrapyengine.start()\n+    @defer.inlineCallbacks\n+    def start(self):\n+        yield defer.maybeDeferred(self.engine.start)\n+        self._nextcall = reactor.callLater(0, self._spider_closed)\n+        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         if self.control_reactor:\n             reactor.run(installSignalHandlers=False)\n \n+    @defer.inlineCallbacks\n     def stop(self):\n-        \"\"\"Stop the scrapy server, shutting down the execution engine\"\"\"\n-        self.interrupted = True\n-        scrapyengine.stop()\n-\n-    def _create_spider_for_request(self, request, default=None, log_none=False, \\\n-            log_multiple=False):\n-        spider_names = spiders.find_by_request(request)\n-        if len(spider_names) == 1:\n-            return spiders.create(spider_names[0])\n-        if len(spider_names) > 1 and log_multiple:\n-            log.msg('More than one spider found for: %s' % request, log.ERROR)\n-        if len(spider_names) == 0 and log_none:\n-            log.msg('Could not find spider for: %s' % request, log.ERROR)\n-        return default\n+        if self._nextcall.active():\n+            self._nextcall.cancel()\n+        if self.engine.running:\n+            yield defer.maybeDeferred(self.engine.stop)\n+        try:\n+            reactor.stop()\n+        except RuntimeError: # raised if already stopped or in shutdown stage\n+            pass\n \n     def _signal_shutdown(self, signum, _):\n         signame = signal_names[signum]\n@@ -108,7 +92,7 @@ class ExecutionManager(object):\n         log.msg('Received %s twice, forcing unclean shutdown' % signame, \\\n             level=log.INFO)\n         log.log_level = log.SILENT # disable logging of confusing tracebacks\n-        reactor.callFromThread(scrapyengine.kill)\n+        reactor.callFromThread(self.engine.kill)\n         install_shutdown_handlers(signal.SIG_IGN)\n \n scrapymanager = ExecutionManager()\n\n@@ -0,0 +1,87 @@\n+from twisted.internet import defer\n+\n+from scrapy.http import Request\n+from scrapy.utils.misc import arg_to_iter\n+from scrapy import log\n+from scrapy.spider import spiders\n+\n+\n+class ExecutionQueue(object):\n+\n+    polling_delay = 5\n+\n+    def __init__(self, _spiders=spiders):\n+        self.spider_requests = []\n+        self._spiders = _spiders\n+\n+    def _append_next(self):\n+        \"\"\"Called when there are no more itemsl left in self.spider_requests.\n+        This method is meant to be overriden in subclasses to add new (spider,\n+        requests) tuples to self.spider_requests. It can return a Deferred.\n+        \"\"\"\n+        pass\n+\n+    def get_next(self):\n+        \"\"\"Return a tuple (spider, requests) containing a list of Requests and\n+        the Spider which will be used to crawl those Requests. If there aren't\n+        any more spiders to crawl it must return (None, []).\n+\n+        This method can return a deferred.\n+        \"\"\"\n+        if self.spider_requests:\n+            return self._get_next_now()\n+        d = defer.maybeDeferred(self._append_next)\n+        d.addCallback(lambda _: self._get_next_now())\n+        return d\n+\n+    def _get_next_now(self):\n+        try:\n+            return self.spider_requests.pop(0)\n+        except IndexError:\n+            return (None, [])\n+\n+    def is_finished(self):\n+        \"\"\"Return True if the queue is empty and there won't be any more\n+        spiders to crawl (this is for one-shot runs). If it returns ``False``\n+        Scrapy will keep polling this queue for new requests to scrape\n+        \"\"\"\n+        return not bool(self.spider_requests)\n+\n+    def append_spider(self, spider):\n+        \"\"\"Append a Spider to crawl\"\"\"\n+        requests = spider.start_requests()\n+        self.spider_requests.append((spider, requests))\n+\n+    def append_request(self, request, spider=None, **kwargs):\n+        if spider is None:\n+            spider = self._spiders.create_for_request(request, **kwargs)\n+        if spider:\n+            self.spider_requests.append((spider, [request]))\n+\n+    def append_url(self, url, spider=None, **kwargs):\n+        \"\"\"Append a URL to crawl with the given spider. If the spider is not\n+        given, a spider will be looked up based on the URL\n+        \"\"\"\n+        if spider is None:\n+            spider = self._spiders.create_for_request(Request(url), **kwargs)\n+        if spider:\n+            requests = arg_to_iter(spider.make_requests_from_url(url))\n+            self.spider_requests.append((spider, requests))\n+\n+    def append_spider_name(self, spider_name, **spider_kwargs):\n+        \"\"\"Append a spider to crawl given its name and optional arguments,\n+        which are used to instantiate it. The SpiderManager is used to lookup\n+        the spider\n+        \"\"\"\n+        try:\n+            spider = self._spiders.create(spider_name, **spider_kwargs)\n+        except KeyError:\n+            log.msg('Unable to find spider: %s' % spider_name, log.ERROR)\n+        else:\n+            self.append_spider(spider)\n+\n+\n+class KeepAliveExecutionQueue(ExecutionQueue):\n+\n+    def is_finished(self):\n+        return False\n\n@@ -18,6 +18,7 @@ from scrapy.utils.response import open_in_browser\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n+from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.http import Request, TextResponse\n \n def relevant_var(varname):\n@@ -54,10 +55,11 @@ class Shell(object):\n             url = parse_url(request_or_url)\n             request = Request(url)\n \n-        spider = scrapymanager._create_spider_for_request(request, \\\n-            BaseSpider('default'), log_multiple=True)\n+        spider = spiders.create_for_request(request, BaseSpider('default'), \\\n+            log_multiple=True)\n \n         print \"Fetching %s...\" % request\n+        scrapyengine.open_spider(spider)\n         response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \\\n             request, spider)\n         if response:\n@@ -108,7 +110,8 @@ class Shell(object):\n         signal.signal(signal.SIGINT, signal.SIG_IGN)\n \n         reactor.callInThread(self._console_thread, url)\n-        scrapymanager.start(keep_alive=True)\n+        scrapymanager.queue = KeepAliveExecutionQueue()\n+        scrapymanager.start()\n \n     def inspect_response(self, response):\n         print\n\n@@ -0,0 +1,105 @@\n+import unittest\n+\n+from scrapy.core.queue import ExecutionQueue, KeepAliveExecutionQueue\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+\n+class TestSpider(BaseSpider):\n+\n+    name = \"default\"\n+\n+    def start_requests(self):\n+        return [Request(\"http://www.example.com/1\"), \\\n+            Request(\"http://www.example.com/2\")]\n+\n+    def make_requests_from_url(self, url):\n+        return [Request(url + \"/make1\"), Request(url + \"/make2\")]\n+\n+\n+class TestSpiderManager(object):\n+\n+    def create_for_request(self, request, **kwargs):\n+        return TestSpider('create_for_request', **kwargs)\n+\n+    def create(self, spider_name, **spider_kwargs):\n+        return TestSpider(spider_name, **spider_kwargs)\n+\n+\n+class ExecutionQueueTest(unittest.TestCase):\n+\n+    queue_class = ExecutionQueue\n+\n+    def setUp(self):\n+        self.queue = self.queue_class(_spiders=TestSpiderManager())\n+        self.spider = TestSpider()\n+        self.request = Request('about:none')\n+\n+    def tearDown(self):\n+        del self.queue, self.spider, self.request\n+\n+    def test_is_finished(self):\n+        self.assert_(self.queue.is_finished())\n+        self.queue.append_request(self.request, self.spider)\n+        self.assert_(not self.queue.is_finished())\n+\n+    def test_append_spider(self):\n+        spider = TestSpider()\n+        self.queue.append_spider(spider)\n+        self.assert_(self.queue.spider_requests[0][0] is spider)\n+        self._assert_request_urls(self.queue.spider_requests[0][1],\n+            [\"http://www.example.com/1\", \"http://www.example.com/2\"])\n+\n+    def test_append_request1(self):\n+        spider = TestSpider()\n+        request = Request('about:blank')\n+        self.queue.append_request(request, spider=spider)\n+        self.assert_(self.queue.spider_requests[0][0] is spider)\n+        self.assert_(self.queue.spider_requests[0][1][0] is request)\n+\n+    def test_append_request2(self):\n+        request = Request('about:blank')\n+        self.queue.append_request(request, arg='123')\n+        spider = self.queue.spider_requests[0][0]\n+        self.assert_(spider.name == 'create_for_request')\n+        self.assert_(spider.arg == '123')\n+\n+    def test_append_url(self):\n+        spider = TestSpider()\n+        url = 'http://www.example.com/asd'\n+        self.queue.append_url(url, spider=spider)\n+        self.assert_(self.queue.spider_requests[0][0] is spider)\n+        self._assert_request_urls(self.queue.spider_requests[0][1], \\\n+            ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n+\n+    def test_append_url2(self):\n+        url = 'http://www.example.com/asd'\n+        self.queue.append_url(url, arg='123')\n+        self._assert_request_urls(self.queue.spider_requests[0][1], \\\n+            ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n+        spider = self.queue.spider_requests[0][0]\n+        self.assert_(spider.name == 'create_for_request')\n+        self.assert_(spider.arg == '123')\n+\n+    def test_append_spider_name(self):\n+        self.queue.append_spider_name('test123', arg='123')\n+        spider = self.queue.spider_requests[0][0]\n+        self.assert_(spider.name == 'test123')\n+        self.assert_(spider.arg == '123')\n+\n+    def _assert_request_urls(self, requests, urls):\n+        assert all(isinstance(x, Request) for x in requests)\n+        self.assertEqual([x.url for x in requests], urls)\n+\n+class KeepAliveExecutionQueueTest(ExecutionQueueTest):\n+\n+    queue_class = KeepAliveExecutionQueue\n+\n+    def test_is_finished(self):\n+        self.assert_(not self.queue.is_finished())\n+        self.queue.append_request(self.request, self.spider)\n+        self.assert_(not self.queue.is_finished())\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n+\n\n@@ -97,7 +97,7 @@ class CrawlingSession(object):\n             dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n             scrapymanager.configure()\n-            scrapymanager.crawl_spider(self.spider)\n+            scrapymanager.queue.append_spider(self.spider)\n             scrapymanager.start()\n             self.port.stopListening()\n             self.wasrun = True\n\n@@ -12,11 +12,11 @@ def get_engine_status(engine=None):\n     global_tests = [\n         \"time()-engine.start_time\",\n         \"engine.is_idle()\",\n+        \"engine.has_capacity()\",\n         \"engine.scheduler.is_idle()\",\n         \"len(engine.scheduler.pending_requests)\",\n         \"engine.downloader.is_idle()\",\n         \"len(engine.downloader.sites)\",\n-        \"engine.downloader.has_capacity()\",\n         \"engine.scraper.is_idle()\",\n         \"len(engine.scraper.sites)\",\n     ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
