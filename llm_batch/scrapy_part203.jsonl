{"custom_id": "scrapy#44bd01d51cc0d2008a73e67845922a23d7aee344", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 21 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,24 +60,21 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n                 request.meta.get('handle_httpstatus_all', False)):\n             return response\n \n-        if request.method == 'HEAD':\n-            if response.status in [301, 302, 303, 307] and 'Location' in response.headers:\n-                redirected_url = urljoin(request.url, response.headers['location'])\n+        location = None\n+        if 'Location' in response.headers:\n+            location = response.headers['location']\n+\n+        if location is not None and response.status in [301, 302, 303, 307]:\n+            redirected_url = urljoin(request.url, location)\n+\n+            if response.status in [301, 307] or request.method == 'HEAD':\n                 redirected = request.replace(url=redirected_url)\n                 return self._redirect(redirected, request, spider, response.status)\n-            else:\n-                return response\n \n-        if response.status in [302, 303] and 'Location' in response.headers:\n-            redirected_url = urljoin(request.url, response.headers['location'])\n+            if response.status in [302, 303]:\n                 redirected = self._redirect_request_using_get(request, redirected_url)\n                 return self._redirect(redirected, request, spider, response.status)\n \n-        if response.status in [301, 307] and 'Location' in response.headers:\n-            redirected_url = urljoin(request.url, response.headers['location'])\n-            redirected = request.replace(url=redirected_url)\n-            return self._redirect(redirected, request, spider, response.status)\n-\n         return response\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#defa89913559b543cf7336653e44350481c1c38a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 100 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,7 +62,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n         location = None\n         if 'Location' in response.headers:\n-            location = response.headers['location']\n+            location = response.headers['location'].decode('latin1')\n \n         if location is not None and response.status in [301, 302, 303, 307]:\n             redirected_url = urljoin(request.url, location)\n\n@@ -164,13 +164,13 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n     def test_priority_adjust(self):\n         req = Request('http://a.com')\n-        rsp = HtmlResponse(req.url, body=self._body())\n+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority > req.priority\n \n     def test_meta_refresh(self):\n         req = Request(url='http://example.org')\n-        rsp = HtmlResponse(req.url, body=self._body())\n+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, 'http://example.org/newpage')\n@@ -178,14 +178,16 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def test_meta_refresh_with_high_interval(self):\n         # meta-refresh with high intervals don't trigger redirects\n         req = Request(url='http://example.org')\n-        rsp = HtmlResponse(url='http://example.org', body=self._body(interval=1000))\n+        rsp = HtmlResponse(url='http://example.org',\n+                           body=self._body(interval=1000),\n+                           encoding='utf-8')\n         rsp2 = self.mw.process_response(req, rsp, self.spider)\n         assert rsp is rsp2\n \n     def test_meta_refresh_trough_posted_request(self):\n         req = Request(url='http://example.org', method='POST', body='test',\n                       headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n-        rsp = HtmlResponse(req.url, body=self._body())\n+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n         req2 = self.mw.process_response(req, rsp, self.spider)\n \n         assert isinstance(req2, Request)\n@@ -201,7 +203,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n         req = Request('http://scrapytest.org/max')\n-        rsp = HtmlResponse(req.url, body=self._body())\n+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n@@ -212,7 +214,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def test_ttl(self):\n         self.mw.max_redirect_times = 100\n         req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})\n-        rsp = HtmlResponse(req.url, body=self._body())\n+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n@@ -220,10 +222,10 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n     def test_redirect_urls(self):\n         req1 = Request('http://scrapytest.org/first')\n-        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))\n+        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'), encoding='utf-8')\n         req2 = self.mw.process_response(req1, rsp1, self.spider)\n         assert isinstance(req2, Request), req2\n-        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))\n+        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'), encoding='utf-8')\n         req3 = self.mw.process_response(req2, rsp2, self.spider)\n         assert isinstance(req3, Request), req3\n         self.assertEqual(req2.url, 'http://scrapytest.org/redirected')\n@@ -231,5 +233,6 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n         self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#179c4588ca12d363ab8ce9d2badad6c456d353a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 80 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+# -*- coding: utf-8 -*-\n+\n import unittest\n \n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware\n@@ -150,6 +152,14 @@ class RedirectMiddlewareTest(unittest.TestCase):\n                                                            [404, 301, 302]}))\n         _test_passthrough(Request(url, meta={'handle_httpstatus_all': True}))\n \n+    def test_latin1_location(self):\n+        req = Request('http://scrapytest.org/first')\n+        latin1_path = u'/ação'.encode('latin1')\n+        resp = Response('http://scrapytest.org/first', headers={'Location': latin1_path}, status=302)\n+        req_result = self.mw.process_response(req, resp, self.spider)\n+        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n+        self.assertEquals(perc_encoded_utf8_url, req_result.url)\n+\n \n class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81950f773d588000ef2bbe720caa0c2fc4d3f0df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 126 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from six.moves.urllib.parse import urljoin\n \n from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n+from scrapy.utils.python import to_native_str\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n logger = logging.getLogger(__name__)\n@@ -62,7 +63,8 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n         location = None\n         if 'Location' in response.headers:\n-            location = response.headers['location'].decode('latin1')\n+            # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8\n+            location = to_native_str(response.headers['location'].decode('latin1'))\n \n         if location is not None and response.status in [301, 302, 303, 307]:\n             redirected_url = urljoin(request.url, location)\n\n@@ -154,12 +154,20 @@ class RedirectMiddlewareTest(unittest.TestCase):\n \n     def test_latin1_location(self):\n         req = Request('http://scrapytest.org/first')\n-        latin1_path = u'/ação'.encode('latin1')\n-        resp = Response('http://scrapytest.org/first', headers={'Location': latin1_path}, status=302)\n+        latin1_location = u'/ação'.encode('latin1')  # HTTP historically supports latin1\n+        resp = Response('http://scrapytest.org/first', headers={'Location': latin1_location}, status=302)\n         req_result = self.mw.process_response(req, resp, self.spider)\n         perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n         self.assertEquals(perc_encoded_utf8_url, req_result.url)\n \n+    def test_location_with_wrong_encoding(self):\n+        req = Request('http://scrapytest.org/first')\n+        utf8_location = u'/ação'  # header with wrong encoding (utf-8)\n+        resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)\n+        req_result = self.mw.process_response(req, resp, self.spider)\n+        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%83%C2%A7%C3%83%C2%A3o'\n+        self.assertEquals(perc_encoded_utf8_url, req_result.url)\n+\n \n class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b06d0706e6644187b4c4a6f7ac80e8adbf376907", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 12 | Churn Cumulative: 46 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,24 +61,22 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n                 request.meta.get('handle_httpstatus_all', False)):\n             return response\n \n-        location = None\n-        if 'Location' in response.headers:\n+        allowed_status = (301, 302, 303, 307)\n+        if 'Location' not in response.headers or response.status not in allowed_status:\n+            return response\n+\n         # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8\n         location = to_native_str(response.headers['location'].decode('latin1'))\n \n-        if location is not None and response.status in [301, 302, 303, 307]:\n         redirected_url = urljoin(request.url, location)\n \n-            if response.status in [301, 307] or request.method == 'HEAD':\n+        if response.status in (301, 307) or request.method == 'HEAD':\n             redirected = request.replace(url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n-            if response.status in [302, 303]:\n         redirected = self._redirect_request_using_get(request, redirected_url)\n         return self._redirect(redirected, request, spider, response.status)\n \n-        return response\n-\n \n class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#430e90f4b209af51deec0a85d214dfa3171eaf0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 110 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -177,18 +177,18 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n \n     def _body(self, interval=5, url='http://example.org/newpage'):\n-        return \"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{0};url={1}\"/></head></html>\"\"\"\\\n-                .format(interval, url)\n+        html = u\"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{0};url={1}\"/></head></html>\"\"\"\n+        return html.format(interval, url).encode('utf-8')\n \n     def test_priority_adjust(self):\n         req = Request('http://a.com')\n-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n+        rsp = HtmlResponse(req.url, body=self._body())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority > req.priority\n \n     def test_meta_refresh(self):\n         req = Request(url='http://example.org')\n-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n+        rsp = HtmlResponse(req.url, body=self._body())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, 'http://example.org/newpage')\n@@ -205,7 +205,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def test_meta_refresh_trough_posted_request(self):\n         req = Request(url='http://example.org', method='POST', body='test',\n                       headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n+        rsp = HtmlResponse(req.url, body=self._body())\n         req2 = self.mw.process_response(req, rsp, self.spider)\n \n         assert isinstance(req2, Request)\n@@ -221,7 +221,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n         req = Request('http://scrapytest.org/max')\n-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n+        rsp = HtmlResponse(req.url, body=self._body())\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n@@ -232,7 +232,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n     def test_ttl(self):\n         self.mw.max_redirect_times = 100\n         req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})\n-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')\n+        rsp = HtmlResponse(req.url, body=self._body())\n \n         req = self.mw.process_response(req, rsp, self.spider)\n         assert isinstance(req, Request)\n@@ -240,10 +240,10 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n     def test_redirect_urls(self):\n         req1 = Request('http://scrapytest.org/first')\n-        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'), encoding='utf-8')\n+        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))\n         req2 = self.mw.process_response(req1, rsp1, self.spider)\n         assert isinstance(req2, Request), req2\n-        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'), encoding='utf-8')\n+        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))\n         req3 = self.mw.process_response(req2, rsp2, self.spider)\n         assert isinstance(req3, Request), req3\n         self.assertEqual(req2.url, 'http://scrapytest.org/redirected')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ecbfe4bd6661acd165407e7f1e5abf1f0a2fa31c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 23 | Files Changed: 6 | Hunks: 13 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 31 | Churn Cumulative: 983 | Contributors (this commit): 10 | Commits (past 90d): 24 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,7 @@\n Scrapy - a web crawling and web scraping framework written for Python\n \"\"\"\n \n-__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version',\n+__all__ = ['__version__', 'version_info', 'twisted_version',\n            'Spider', 'Request', 'FormRequest', 'Selector', 'Item', 'Field']\n \n # Scrapy version\n@@ -27,15 +27,8 @@ del warnings\n from . import _monkeypatches\n del _monkeypatches\n \n-# WARNING: optional_features set is deprecated and will be removed soon. Do not use.\n-optional_features = set()\n-# TODO: backwards compatibility, remove for Scrapy 0.20\n-optional_features.add('ssl')\n-\n from twisted import version as _txv\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n-if twisted_version >= (11, 1, 0):\n-    optional_features.add('http11')\n \n # Declare top-level shortcuts\n from scrapy.spiders import Spider\n\n@@ -1,7 +1,7 @@\n-from scrapy import optional_features\n+from scrapy import twisted_version\n from .http10 import HTTP10DownloadHandler\n \n-if 'http11' in optional_features:\n+if twisted_version >= (11, 1, 0):\n     from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n else:\n     HTTPDownloadHandler = HTTP10DownloadHandler\n\n@@ -126,9 +126,6 @@ def log_scrapy_info(settings):\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n \n-    logger.info(\"Optional features available: %(features)s\",\n-                {'features': \", \".join(scrapy.optional_features)})\n-\n     d = dict(overridden_settings(settings))\n     logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n \n\n@@ -27,7 +27,6 @@ from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.settings import Settings\n-from scrapy import optional_features\n from scrapy.utils.test import get_crawler\n from scrapy.exceptions import NotConfigured\n \n@@ -220,7 +219,7 @@ class Http10TestCase(HttpTestCase):\n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n     download_handler_cls = HTTP11DownloadHandler\n-    if 'http11' not in optional_features:\n+    if twisted_version < (11, 1, 0):\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n     def test_download_without_maxsize_limit(self):\n@@ -267,7 +266,7 @@ class Http11TestCase(HttpTestCase):\n \n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n-    if 'http11' not in optional_features:\n+    if twisted_version < (11, 1, 0):\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n     def setUp(self):\n@@ -392,7 +391,7 @@ class Http10ProxyTestCase(HttpProxyTestCase):\n \n class Http11ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = HTTP11DownloadHandler\n-    if 'http11' not in optional_features:\n+    if twisted_version < (11, 1, 0):\n         skip = 'HTTP1.1 not supported in twisted < 11.1.0'\n \n \n\n@@ -4,7 +4,7 @@ from twisted.internet.error import TimeoutError, DNSLookupError, \\\n         ConnectionRefusedError, ConnectionDone, ConnectError, \\\n         ConnectionLost, TCPTimedOutError\n \n-from scrapy import optional_features\n+from scrapy import twisted_version\n from scrapy.downloadermiddlewares.retry import RetryMiddleware\n from scrapy.xlib.tx import ResponseFailed\n from scrapy.spiders import Spider\n@@ -75,7 +75,7 @@ class RetryTest(unittest.TestCase):\n         exceptions = [defer.TimeoutError, TCPTimedOutError, TimeoutError,\n                 DNSLookupError, ConnectionRefusedError, ConnectionDone,\n                 ConnectError, ConnectionLost]\n-        if 'http11' in optional_features:\n+        if twisted_version >= (11, 1, 0): # http11 available\n             exceptions.append(ResponseFailed)\n \n         for exc in exceptions:\n\n@@ -11,10 +11,6 @@ class ToplevelTestCase(TestCase):\n     def test_version_info(self):\n         self.assertIs(type(scrapy.version_info), tuple)\n \n-    def test_optional_features(self):\n-        self.assertIs(type(scrapy.optional_features), set)\n-        self.assertIn('ssl', scrapy.optional_features)\n-\n     def test_request_shortcut(self):\n         from scrapy.http import Request, FormRequest\n         self.assertIs(scrapy.Request, Request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22327ef47bd4b8098baf43d334d7852ae0c42690", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 78 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -27,6 +27,7 @@ IGNORE = ignore_patterns('*.pyc', '.svn')\n class Command(ScrapyCommand):\n \n     requires_project = False\n+    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"<project_name>\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97a52665a0b96c8e2b93303dd5b10276ffac1ed2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 85 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ class ResponseTypes(object):\n         'application/xml': 'scrapy.http.XmlResponse',\n         'application/json': 'scrapy.http.TextResponse',\n         'application/x-json': 'scrapy.http.TextResponse',\n+        'application/json-amazonui-streaming': 'scrapy.http.TextResponse',\n         'application/javascript': 'scrapy.http.TextResponse',\n         'application/x-javascript': 'scrapy.http.TextResponse',\n         'text/xml': 'scrapy.http.XmlResponse',\n\n@@ -36,6 +36,7 @@ class ResponseTypesTest(unittest.TestCase):\n             ('application/xml; charset=UTF-8', XmlResponse),\n             ('application/octet-stream', Response),\n             ('application/x-json; encoding=UTF8;charset=UTF-8', TextResponse),\n+            ('application/json-amazonui-streaming;charset=UTF-8', TextResponse),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c517951a484c25346e96651c34e88105dc0908ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 29 | Churn Cumulative: 694 | Contributors (this commit): 18 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from threading import Thread\n from scrapy.commands import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n+from scrapy.utils.url import add_scheme_if_missing\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n \n@@ -41,6 +42,8 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n+        if url:\n+            url = add_scheme_if_missing(url)\n         spider_loader = self.crawler_process.spider_loader\n \n         spidercls = DefaultSpider\n\n@@ -110,3 +110,11 @@ def escape_ajax(url):\n     if not frag.startswith('!'):\n         return url\n     return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])\n+\n+def add_scheme_if_missing(url):\n+    parser = parse_url(url)\n+    if not parser.scheme:\n+        if not parser.netloc:\n+            parser = parser._replace(netloc=parser.path, path='')\n+        parser = parser._replace(scheme='http')\n+    return parser.geturl() \n\n@@ -4,7 +4,7 @@ import unittest\n import six\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              canonicalize_url)\n+                              canonicalize_url, add_scheme_if_missing)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -73,6 +73,22 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n+    def test_add_scheme_if_missing(self):\n+        self.assertEqual(add_scheme_if_missing('http://www.example.com'),\n+                                               'http://www.example.com')\n+        self.assertEqual(add_scheme_if_missing('http://www.example.com/some/page.html'),\n+                                               'http://www.example.com/some/page.html')\n+        self.assertEqual(add_scheme_if_missing('http://example.com'),\n+                                               'http://example.com')\n+        self.assertEqual(add_scheme_if_missing('www.example.com'),\n+                                               'http://www.example.com')\n+        self.assertEqual(add_scheme_if_missing('example.com'),\n+                                               'http://example.com')\n+        self.assertEqual(add_scheme_if_missing('//example.com'),\n+                                               'http://example.com')\n+        self.assertEqual(add_scheme_if_missing('https://www.example.com'),\n+                                               'https://www.example.com')\n+\n \n class CanonicalizeUrlTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c629eee3e41a4d40f620e3a3f594391735b5a9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 61 | Churn Cumulative: 755 | Contributors (this commit): 18 | Commits (past 90d): 9 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from threading import Thread\n from scrapy.commands import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n-from scrapy.utils.url import add_scheme_if_missing\n+from scrapy.utils.url import add_http_if_no_scheme\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n \n@@ -43,7 +43,7 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         url = args[0] if args else None\n         if url:\n-            url = add_scheme_if_missing(url)\n+            url = add_http_if_no_scheme(url)\n         spider_loader = self.crawler_process.spider_loader\n \n         spidercls = DefaultSpider\n\n@@ -111,10 +111,11 @@ def escape_ajax(url):\n         return url\n     return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])\n \n-def add_scheme_if_missing(url):\n+def add_http_if_no_scheme(url):\n+    \"\"\"Adds http as the default scheme if it is missing from the url\"\"\"\n     parser = parse_url(url)\n-    if not parser.scheme:\n-        if not parser.netloc:\n-            parser = parser._replace(netloc=parser.path, path='')\n-        parser = parser._replace(scheme='http')\n-    return parser.geturl() \n+    if url.startswith('//'):\n+        url = 'http:' + url\n+    elif not parser.scheme or not parser.netloc:\n+        url = 'http://' + url\n+    return url\n\n@@ -4,7 +4,7 @@ import unittest\n import six\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              canonicalize_url, add_scheme_if_missing)\n+                              canonicalize_url, add_http_if_no_scheme)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -73,21 +73,47 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n-    def test_add_scheme_if_missing(self):\n-        self.assertEqual(add_scheme_if_missing('http://www.example.com'),\n+    def test_add_http_if_no_scheme(self):\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com'),\n                                                'http://www.example.com')\n-        self.assertEqual(add_scheme_if_missing('http://www.example.com/some/page.html'),\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page.html'),\n                                                'http://www.example.com/some/page.html')\n-        self.assertEqual(add_scheme_if_missing('http://example.com'),\n+        self.assertEqual(add_http_if_no_scheme('http://example.com'),\n                                                'http://example.com')\n-        self.assertEqual(add_scheme_if_missing('www.example.com'),\n+        self.assertEqual(add_http_if_no_scheme('www.example.com'),\n                                                'http://www.example.com')\n-        self.assertEqual(add_scheme_if_missing('example.com'),\n+        self.assertEqual(add_http_if_no_scheme('example.com'),\n                                                'http://example.com')\n-        self.assertEqual(add_scheme_if_missing('//example.com'),\n+        self.assertEqual(add_http_if_no_scheme('//example.com'),\n                                                'http://example.com')\n-        self.assertEqual(add_scheme_if_missing('https://www.example.com'),\n+        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page.html'),\n+                                               'http://www.example.com/some/page.html')\n+        self.assertEqual(add_http_if_no_scheme('www.example.com:80'),\n+                                               'http://www.example.com:80')\n+        self.assertEqual(add_http_if_no_scheme('www.example.com:80/some/page.html'),\n+                                               'http://www.example.com:80/some/page.html')\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com:80/some/page.html'),\n+                                               'http://www.example.com:80/some/page.html')\n+        self.assertEqual(add_http_if_no_scheme('www.example.com/some/page#frag'),\n+                                               'http://www.example.com/some/page#frag')\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page#frag'),\n+                                               'http://www.example.com/some/page#frag')\n+        self.assertEqual(add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),\n+                                               'http://www.example.com/do?a=1&b=2&c=3')\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),\n+                                               'http://www.example.com/do?a=1&b=2&c=3')\n+        self.assertEqual(add_http_if_no_scheme('username:password@example.com/some/page.html'),\n+                                               'http://username:password@example.com/some/page.html')\n+        self.assertEqual(add_http_if_no_scheme('http://username:password@example.com/some/page.html'),\n+                                               'http://username:password@example.com/some/page.html')\n+        self.assertEqual(add_http_if_no_scheme('username:password@example.com:80/some/part?a=1&b=2&c=3#frag'),\n+                                               'http://username:password@example.com:80/some/part?a=1&b=2&c=3#frag')\n+        self.assertEqual(add_http_if_no_scheme('http://username:password@example.com:80/some/part?a=1&b=2&c=3#frag'),\n+                                               'http://username:password@example.com:80/some/part?a=1&b=2&c=3#frag')\n+        self.assertEqual(add_http_if_no_scheme('https://www.example.com'),\n                                                'https://www.example.com')\n+        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'),\n+                                               'ftp://www.example.com')\n \n \n class CanonicalizeUrlTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d96e767a1baf3b7737440eae0f1d2c5d433f798", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 487 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,11 +111,13 @@ def escape_ajax(url):\n         return url\n     return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])\n \n+\n def add_http_if_no_scheme(url):\n-    \"\"\"Adds http as the default scheme if it is missing from the url\"\"\"\n-    parser = parse_url(url)\n+    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n     if url.startswith('//'):\n         url = 'http:' + url\n-    elif not parser.scheme or not parser.netloc:\n+        return url\n+    parser = parse_url(url)\n+    if not parser.scheme or not parser.netloc:\n         url = 'http://' + url\n     return url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47c8e2ba781e351868305f65b989d4b18f54279f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 42 | Files Changed: 1 | Hunks: 2 | Methods Changed: 27 | Complexity Δ (Sum/Max): 25/25 | Churn Δ: 149 | Churn Cumulative: 266 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,48 +73,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n-    def test_add_http_if_no_scheme(self):\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com'),\n-                                               'http://www.example.com')\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page.html'),\n-                                               'http://www.example.com/some/page.html')\n-        self.assertEqual(add_http_if_no_scheme('http://example.com'),\n-                                               'http://example.com')\n-        self.assertEqual(add_http_if_no_scheme('www.example.com'),\n-                                               'http://www.example.com')\n-        self.assertEqual(add_http_if_no_scheme('example.com'),\n-                                               'http://example.com')\n-        self.assertEqual(add_http_if_no_scheme('//example.com'),\n-                                               'http://example.com')\n-        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page.html'),\n-                                               'http://www.example.com/some/page.html')\n-        self.assertEqual(add_http_if_no_scheme('www.example.com:80'),\n-                                               'http://www.example.com:80')\n-        self.assertEqual(add_http_if_no_scheme('www.example.com:80/some/page.html'),\n-                                               'http://www.example.com:80/some/page.html')\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com:80/some/page.html'),\n-                                               'http://www.example.com:80/some/page.html')\n-        self.assertEqual(add_http_if_no_scheme('www.example.com/some/page#frag'),\n-                                               'http://www.example.com/some/page#frag')\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page#frag'),\n-                                               'http://www.example.com/some/page#frag')\n-        self.assertEqual(add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),\n-                                               'http://www.example.com/do?a=1&b=2&c=3')\n-        self.assertEqual(add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),\n-                                               'http://www.example.com/do?a=1&b=2&c=3')\n-        self.assertEqual(add_http_if_no_scheme('username:password@example.com/some/page.html'),\n-                                               'http://username:password@example.com/some/page.html')\n-        self.assertEqual(add_http_if_no_scheme('http://username:password@example.com/some/page.html'),\n-                                               'http://username:password@example.com/some/page.html')\n-        self.assertEqual(add_http_if_no_scheme('username:password@example.com:80/some/part?a=1&b=2&c=3#frag'),\n-                                               'http://username:password@example.com:80/some/part?a=1&b=2&c=3#frag')\n-        self.assertEqual(add_http_if_no_scheme('http://username:password@example.com:80/some/part?a=1&b=2&c=3#frag'),\n-                                               'http://username:password@example.com:80/some/part?a=1&b=2&c=3#frag')\n-        self.assertEqual(add_http_if_no_scheme('https://www.example.com'),\n-                                               'https://www.example.com')\n-        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'),\n-                                               'ftp://www.example.com')\n-\n \n class CanonicalizeUrlTest(unittest.TestCase):\n \n@@ -229,5 +187,112 @@ class CanonicalizeUrlTest(unittest.TestCase):\n                          \"http://foo.com/AC%2FDC/\")\n \n \n+class AddHttpIfNoScheme(unittest.TestCase):\n+    \n+    def test_add_scheme(self):\n+        self.assertEqual(add_http_if_no_scheme('www.example.com'),\n+                                               'http://www.example.com')\n+\n+    def test_without_subdomain(self):\n+        self.assertEqual(add_http_if_no_scheme('example.com'),\n+                                               'http://example.com')\n+\n+    def test_path(self):\n+        self.assertEqual(add_http_if_no_scheme('www.example.com/some/page.html'),\n+                                               'http://www.example.com/some/page.html')\n+\n+    def test_port(self):\n+        self.assertEqual(add_http_if_no_scheme('www.example.com:80'),\n+                                               'http://www.example.com:80')\n+\n+    def test_fragment(self):\n+        self.assertEqual(add_http_if_no_scheme('www.example.com/some/page#frag'),\n+                                               'http://www.example.com/some/page#frag')\n+\n+    def test_query(self):\n+        self.assertEqual(add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),\n+                                               'http://www.example.com/do?a=1&b=2&c=3')\n+\n+    def test_username_password(self):\n+        self.assertEqual(add_http_if_no_scheme('username:password@www.example.com'),\n+                                               'http://username:password@www.example.com')\n+    \n+    def test_complete_url(self):\n+        self.assertEqual(add_http_if_no_scheme('username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n+                                               'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n+\n+    def test_preserve_http(self):\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com'),\n+                                               'http://www.example.com')\n+\n+    def test_preserve_http_without_subdomain(self):\n+        self.assertEqual(add_http_if_no_scheme('http://example.com'),\n+                                               'http://example.com')\n+\n+    def test_preserve_http_path(self):\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page.html'),\n+                                               'http://www.example.com/some/page.html')\n+\n+    def test_preserve_http_port(self):\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com:80'),\n+                                               'http://www.example.com:80')\n+\n+    def test_preserve_http_fragment(self):\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page#frag'),\n+                                               'http://www.example.com/some/page#frag')\n+\n+    def test_preserve_http_query(self):\n+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),\n+                                               'http://www.example.com/do?a=1&b=2&c=3')\n+\n+    def test_preserve_http_username_password(self):\n+        self.assertEqual(add_http_if_no_scheme('http://username:password@www.example.com'),\n+                                               'http://username:password@www.example.com')\n+\n+    def test_preserve_http_complete_url(self):\n+        self.assertEqual(add_http_if_no_scheme('http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n+                                               'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n+\n+    def test_protocol_relative(self):\n+        self.assertEqual(add_http_if_no_scheme('//www.example.com'),\n+                                               'http://www.example.com')\n+\n+    def test_protocol_relative_without_subdomain(self):\n+        self.assertEqual(add_http_if_no_scheme('//example.com'),\n+                                               'http://example.com')\n+\n+    def test_protocol_relative_path(self):\n+        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page.html'),\n+                                               'http://www.example.com/some/page.html')\n+\n+    def test_protocol_relative_port(self):\n+        self.assertEqual(add_http_if_no_scheme('//www.example.com:80'),\n+                                               'http://www.example.com:80')\n+\n+    def test_protocol_relative_fragment(self):\n+        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page#frag'),\n+                                               'http://www.example.com/some/page#frag')\n+\n+    def test_protocol_relative_query(self):\n+        self.assertEqual(add_http_if_no_scheme('//www.example.com/do?a=1&b=2&c=3'),\n+                                               'http://www.example.com/do?a=1&b=2&c=3')\n+\n+    def test_protocol_relative_username_password(self):\n+        self.assertEqual(add_http_if_no_scheme('//username:password@www.example.com'),\n+                                               'http://username:password@www.example.com')\n+\n+    def test_protocol_relative_complete_url(self):\n+        self.assertEqual(add_http_if_no_scheme('//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),\n+                                               'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')\n+\n+    def test_preserve_https(self):\n+        self.assertEqual(add_http_if_no_scheme('https://www.example.com'),\n+                                               'https://www.example.com')\n+\n+    def test_preserve_ftp(self):\n+        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'),\n+                                               'ftp://www.example.com')\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75cd056223a5a8da87a361aee42a541afcf27553", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 76 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -11,6 +11,8 @@ from scrapy.commands import ScrapyCommand\n \n class Command(ScrapyCommand):\n \n+    default_settings = {'LOG_ENABLED': False}\n+\n     def syntax(self):\n         return \"[-v]\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d523c75f5e354f6e4c28c52517c2e0f392fcb96b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 88 | Files Changed: 4 | Hunks: 5 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/9 | Churn Δ: 184 | Churn Cumulative: 1041 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,88 @@\n+\"\"\"\n+Scrapy Telnet Console extension\n+\n+See documentation in docs/topics/telnetconsole.rst\n+\"\"\"\n+\n+import pprint\n+import logging\n+\n+from twisted.internet import protocol\n+try:\n+    from twisted.conch import manhole, telnet\n+    from twisted.conch.insults import insults\n+    TWISTED_CONCH_AVAILABLE = True\n+except ImportError:\n+    TWISTED_CONCH_AVAILABLE = False\n+\n+from scrapy.exceptions import NotConfigured\n+from scrapy import signals\n+from scrapy.utils.trackref import print_live_refs\n+from scrapy.utils.engine import print_engine_status\n+from scrapy.utils.reactor import listen_tcp\n+\n+try:\n+    import guppy\n+    hpy = guppy.hpy()\n+except ImportError:\n+    hpy = None\n+\n+logger = logging.getLogger(__name__)\n+\n+# signal to update telnet variables\n+# args: telnet_vars\n+update_telnet_vars = object()\n+\n+\n+class TelnetConsole(protocol.ServerFactory):\n+\n+    def __init__(self, crawler):\n+        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n+            raise NotConfigured\n+        if not TWISTED_CONCH_AVAILABLE:\n+            raise NotConfigured\n+        self.crawler = crawler\n+        self.noisy = False\n+        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n+        self.host = crawler.settings['TELNETCONSOLE_HOST']\n+        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n+        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n+    def start_listening(self):\n+        self.port = listen_tcp(self.portrange, self.host, self)\n+        h = self.port.getHost()\n+        logger.debug(\"Telnet console listening on %(host)s:%(port)d\",\n+                     {'host': h.host, 'port': h.port},\n+                     extra={'crawler': self.crawler})\n+\n+    def stop_listening(self):\n+        self.port.stopListening()\n+\n+    def protocol(self):\n+        telnet_vars = self._get_telnet_vars()\n+        return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n+            insults.ServerProtocol, manhole.Manhole, telnet_vars)\n+\n+    def _get_telnet_vars(self):\n+        # Note: if you add entries here also update topics/telnetconsole.rst\n+        telnet_vars = {\n+            'engine': self.crawler.engine,\n+            'spider': self.crawler.engine.spider,\n+            'slot': self.crawler.engine.slot,\n+            'crawler': self.crawler,\n+            'extensions': self.crawler.extensions,\n+            'stats': self.crawler.stats,\n+            'settings': self.crawler.settings,\n+            'est': lambda: print_engine_status(self.crawler.engine),\n+            'p': pprint.pprint,\n+            'prefs': print_live_refs,\n+            'hpy': hpy,\n+            'help': \"This is Scrapy telnet console. For more info see: \" \\\n+                \"http://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n+        }\n+        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n+        return telnet_vars\n\n@@ -120,7 +120,7 @@ EXTENSIONS = {}\n \n EXTENSIONS_BASE = {\n     'scrapy.extensions.corestats.CoreStats': 0,\n-    'scrapy.telnet.TelnetConsole': 0,\n+    'scrapy.extensions.telnet.TelnetConsole': 0,\n     'scrapy.extensions.memusage.MemoryUsage': 0,\n     'scrapy.extensions.memdebug.MemoryDebugger': 0,\n     'scrapy.extensions.closespider.CloseSpider': 0,\n\n@@ -1,88 +1,7 @@\n-\"\"\"\n-Scrapy Telnet Console extension\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.telnet` is deprecated, \"\n+              \"use `scrapy.extensions.telnet` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n \n-See documentation in docs/topics/telnetconsole.rst\n-\"\"\"\n-\n-import pprint\n-import logging\n-\n-from twisted.internet import protocol\n-try:\n-    from twisted.conch import manhole, telnet\n-    from twisted.conch.insults import insults\n-    TWISTED_CONCH_AVAILABLE = True\n-except ImportError:\n-    TWISTED_CONCH_AVAILABLE = False\n-\n-from scrapy.exceptions import NotConfigured\n-from scrapy import signals\n-from scrapy.utils.trackref import print_live_refs\n-from scrapy.utils.engine import print_engine_status\n-from scrapy.utils.reactor import listen_tcp\n-\n-try:\n-    import guppy\n-    hpy = guppy.hpy()\n-except ImportError:\n-    hpy = None\n-\n-logger = logging.getLogger(__name__)\n-\n-# signal to update telnet variables\n-# args: telnet_vars\n-update_telnet_vars = object()\n-\n-\n-class TelnetConsole(protocol.ServerFactory):\n-\n-    def __init__(self, crawler):\n-        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n-            raise NotConfigured\n-        if not TWISTED_CONCH_AVAILABLE:\n-            raise NotConfigured\n-        self.crawler = crawler\n-        self.noisy = False\n-        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n-        self.host = crawler.settings['TELNETCONSOLE_HOST']\n-        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n-        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n-\n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler)\n-\n-    def start_listening(self):\n-        self.port = listen_tcp(self.portrange, self.host, self)\n-        h = self.port.getHost()\n-        logger.debug(\"Telnet console listening on %(host)s:%(port)d\",\n-                     {'host': h.host, 'port': h.port},\n-                     extra={'crawler': self.crawler})\n-\n-    def stop_listening(self):\n-        self.port.stopListening()\n-\n-    def protocol(self):\n-        telnet_vars = self._get_telnet_vars()\n-        return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n-            insults.ServerProtocol, manhole.Manhole, telnet_vars)\n-\n-    def _get_telnet_vars(self):\n-        # Note: if you add entries here also update topics/telnetconsole.rst\n-        telnet_vars = {\n-            'engine': self.crawler.engine,\n-            'spider': self.crawler.engine.spider,\n-            'slot': self.crawler.engine.slot,\n-            'crawler': self.crawler,\n-            'extensions': self.crawler.extensions,\n-            'stats': self.crawler.stats,\n-            'settings': self.crawler.settings,\n-            'est': lambda: print_engine_status(self.crawler.engine),\n-            'p': pprint.pprint,\n-            'prefs': print_live_refs,\n-            'hpy': hpy,\n-            'help': \"This is Scrapy telnet console. For more info see: \" \\\n-                \"http://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n-        }\n-        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n-        return telnet_vars\n+from scrapy.extensions.telnet import *\n\n@@ -138,6 +138,7 @@ DEPRECATION_RULES = [\n     ('scrapy.command.', 'scrapy.commands.'),\n     ('scrapy.dupefilter.', 'scrapy.dupefilters.'),\n     ('scrapy.linkextractor.', 'scrapy.linkextractors.'),\n+    ('scrapy.telnet.', 'scrapy.extensions.telnet.'),\n     ('scrapy.spider.', 'scrapy.spiders.'),\n     ('scrapy.squeue.', 'scrapy.squeues.'),\n     ('scrapy.statscol.', 'scrapy.statscollectors.'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
