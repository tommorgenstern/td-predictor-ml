{"custom_id": "scrapy#667864ae94168a616beabc3930938b28a593633d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 87 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,8 @@\n from __future__ import absolute_import\n import re\n-from twisted.internet import reactor\n+from twisted.internet import reactor, error\n from twisted.internet.defer import Deferred\n+from twisted.python import failure\n from twisted.trial import unittest\n from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n@@ -41,6 +42,26 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n         reactor.callFromThread(deferred.callback, None)\n         return deferred\n \n+    def test_robotstxt_error(self):\n+        crawler = mock.MagicMock()\n+        crawler.settings = Settings()\n+        crawler.settings.set('ROBOTSTXT_OBEY', True)\n+        crawler.engine.download = mock.MagicMock()\n+        err = error.DNSLookupError('Robotstxt address not found')\n+        def return_failure(request, spider):\n+            deferred = Deferred()\n+            reactor.callFromThread(deferred.errback, failure.Failure(err))\n+            return deferred\n+        crawler.engine.download.side_effect = return_failure\n+\n+        middleware = RobotsTxtMiddleware(crawler)\n+        middleware._logerror = mock.MagicMock()\n+        middleware.process_request(Request('http://site.local'), None)\n+        deferred = Deferred()\n+        deferred.addErrback(lambda _: self.assertIsNone(middleware._logerror.assert_any_call()))\n+        reactor.callFromThread(deferred.callback, None)\n+        return deferred\n+\n     def assertNotIgnored(self, request, middleware):\n         spider = None  # not actually used\n         self.assertIsNone(middleware.process_request(request, spider))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5a8f82b06b3c5a426169f7096ab01cea811c752", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 81 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -100,7 +100,7 @@ pygments_style = 'sphinx'\n \n # The theme to use for HTML and HTML Help pages.  See the documentation for\n # a list of builtin themes.\n-html_theme = 'sphinx_rtd_theme'\n+#html_theme = 'sphinx_rtd_theme'\n \n # Theme options are theme-specific and customize the look and feel of a theme\n # further.  For a list of options available for each theme, see the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d00e43f39c4ad82af5af9ffeb6fb35af2642d4aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 83 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -100,7 +100,7 @@ pygments_style = 'sphinx'\n \n # The theme to use for HTML and HTML Help pages.  See the documentation for\n # a list of builtin themes.\n-#html_theme = 'sphinx_rtd_theme'\n+html_theme = 'sphinx_rtd_theme'\n \n # Theme options are theme-specific and customize the look and feel of a theme\n # further.  For a list of options available for each theme, see the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8301d00fee68e958520c0d6008d8b22420bead74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 75 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,23 @@\n+import glob\n import six\n import pytest\n \n-collect_ignore = [\"scrapy/stats.py\", \"scrapy/project.py\"]\n+\n+def _py_files(folder):\n+    return glob.glob(folder + \"/*.py\") + glob.glob(folder + \"/*/*.py\")\n+\n+\n+collect_ignore = [\n+    \"scrapy/conf.py\",\n+    \"scrapy/stats.py\",\n+    \"scrapy/project.py\",\n+    \"scrapy/utils/decorator.py\",\n+    \"scrapy/statscol.py\",\n+    \"scrapy/squeue.py\",\n+    \"scrapy/log.py\",\n+    \"scrapy/dupefilter.py\",\n+] + _py_files(\"scrapy/contrib\") + _py_files(\"scrapy/contrib_exp\")\n+\n \n if six.PY3:\n     for line in open('tests/py3-ignores.txt'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1053d2a2fe3dba51febcb756227f623a16daf70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 64 | Churn Cumulative: 119 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,63 @@\n from docutils.parsers.rst.roles import set_classes\n from docutils import nodes\n+from sphinx.util.compat import Directive\n+from sphinx.util.nodes import make_refnode\n+\n+\n+class settingslist_node(nodes.General, nodes.Element):\n+    pass\n+\n+\n+class SettingsListDirective(Directive):\n+    def run(self):\n+        return [settingslist_node('')]\n+\n+\n+def is_setting_node(node):\n+    return node.tagname == 'pending_xref' and node['reftype'] == 'setting'\n+\n+\n+def collect_scrapy_settings_refs(app, doctree):\n+    env = app.builder.env\n+\n+    if not hasattr(env, 'scrapy_all_settings'):\n+        env.scrapy_all_settings = []\n+\n+    for node in doctree.traverse(is_setting_node):\n+        try:\n+            targetnode = node.parent[node.parent.index(node) - 1]\n+            if not isinstance(targetnode, nodes.target):\n+                raise IndexError\n+        except IndexError:\n+            targetid = \"setting-%d\" % env.new_serialno('setting')\n+            targetnode = nodes.target('', '', ids=[targetid])\n+            node.replace_self([targetnode, node])\n+\n+        env.scrapy_all_settings.append({\n+            'docname': env.docname,\n+            'lineno': node.line,\n+            'node': node.deepcopy(),\n+            'target': targetnode,\n+        })\n+\n+\n+def make_setting_element(setting_data, app, fromdocname):\n+    text = nodes.Text(setting_data['node'].astext())\n+    targetid = ''  # TODO: resolve to a proper id\n+    refnode = make_refnode(app.builder, fromdocname,\n+                           setting_data['docname'], targetid, text)\n+\n+    p = nodes.paragraph()\n+    p.append(refnode)\n+    return p\n+\n+\n+def replace_settingslist_nodes(app, doctree, fromdocname):\n+    env = app.builder.env\n+\n+    for node in doctree.traverse(settingslist_node):\n+        node.replace_self([make_setting_element(d, app, fromdocname)\n+                           for d in env.scrapy_all_settings])\n \n def setup(app):\n     app.add_crossref_type(\n@@ -27,6 +85,12 @@ def setup(app):\n     app.add_role('issue', issue_role)\n     app.add_role('rev', rev_role)\n \n+    app.add_node(settingslist_node)\n+    app.add_directive('settingslist', SettingsListDirective)\n+\n+    app.connect('doctree-read', collect_scrapy_settings_refs)\n+    app.connect('doctree-resolved', replace_settingslist_nodes)\n+\n def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n     ref = 'https://github.com/scrapy/scrapy/blob/master/' + text\n     set_classes(options)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ae05478beea4327f556f21c9c0f954826aff832", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 12 | Churn Cumulative: 407 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -184,7 +184,9 @@ class SelectorList(list):\n \n     def extract_first(self, default=None):\n         for x in self:\n-            return x.extract() or default\n+            return x.extract()\n+        else:\n+            return default\n \n     @deprecated(use_instead='.extract()')\n     def extract_unquoted(self):\n\n@@ -72,6 +72,14 @@ class SelectorTestCase(unittest.TestCase):\n \n         self.assertEqual(sel.xpath('/ul/li[@id=\"doesnt-exist\"]/text()').extract_first(), None)\n \n+    def test_extract_first_default(self):\n+        \"\"\"Test if extract_first() returns default value when no results found\"\"\"\n+        body = '<ul><li id=\"1\">1</li><li id=\"2\">2</li></ul>'\n+        response = TextResponse(url=\"http://example.com\", body=body)\n+        sel = self.sscls(response)\n+\n+        self.assertEqual(sel.xpath('//div/text()').extract_first(default='missing'), 'missing')\n+\n     def test_re_first(self):\n         \"\"\"Test if re_first() returns first matched element\"\"\"\n         body = '<ul><li id=\"1\">1</li><li id=\"2\">2</li></ul>'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#389f6e95c5e5da3221c56ccecf77beaf6434e583", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 9 | Churn Cumulative: 9 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@\n HTMLParser-based link extractor\n \"\"\"\n \n+import warnings\n from HTMLParser import HTMLParser\n from six.moves.urllib.parse import urljoin\n \n@@ -9,12 +10,20 @@ from w3lib.url import safe_url_string\n \n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n \n class HtmlParserLinkExtractor(HTMLParser):\n \n     def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n         HTMLParser.__init__(self)\n \n+        warnings.warn(\n+            \"HtmlParserLinkExtractor is deprecated and will be removed in \"\n+            \"future releases. Please use scrapy.linkextractors.LinkExtractor\",\n+            ScrapyDeprecationWarning\n+        )\n+\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.process_attr = process if callable(process) else lambda v: v\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e59f795d2fb1d3ce9da2d510b7d546b9e034f3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 19 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ class HtmlParserLinkExtractor(HTMLParser):\n         warnings.warn(\n             \"HtmlParserLinkExtractor is deprecated and will be removed in \"\n             \"future releases. Please use scrapy.linkextractors.LinkExtractor\",\n-            ScrapyDeprecationWarning\n+            ScrapyDeprecationWarning, stacklevel=2,\n         )\n \n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n\n@@ -21,7 +21,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         warnings.warn(\n             \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.linkextractors.LinkExtractor\",\n-            ScrapyDeprecationWarning\n+            ScrapyDeprecationWarning, stacklevel=2,\n         )\n         SGMLParser.__init__(self)\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n@@ -104,7 +104,7 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.linkextractors.LinkExtractor\",\n-            ScrapyDeprecationWarning\n+            ScrapyDeprecationWarning, stacklevel=2,\n         )\n \n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fb58a30b731b293c627f650aee3479caca1e201", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 504 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -5,3 +5,4 @@ warnings.warn(\"Module `scrapy.contrib.exporter` is deprecated, \"\n               ScrapyDeprecationWarning, stacklevel=2)\n \n from scrapy.exporters import *\n+from scrapy.exporters import PythonItemExporter\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce735feedde39963d1086ba777d86677107110ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#616aec92c81a4bddf7f8571ae5f95635d183d62b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 16 | Files Changed: 16 | Hunks: 15 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 2565 | Contributors (this commit): 18 | Commits (past 90d): 30 | Contributors (cumulative): 85 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ import pkg_resources\n import scrapy\n from scrapy.crawler import CrawlerProcess\n from scrapy.xlib import lsprofcalltree\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n\n\n@@ -5,7 +5,7 @@ import subprocess\n from six.moves.urllib.parse import urlencode\n \n import scrapy\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.linkextractors import LinkExtractor\n \n \n\n@@ -4,7 +4,7 @@ import sys\n from collections import defaultdict\n from unittest import TextTestRunner, TextTestResult as _TextTestResult\n \n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.contracts import ContractsManager\n from scrapy.utils.misc import load_object\n from scrapy.utils.conf import build_component_list\n\n@@ -1,5 +1,5 @@\n import os\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n \n\n@@ -1,6 +1,6 @@\n import sys, os\n \n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n \n class Command(ScrapyCommand):\n\n@@ -1,7 +1,7 @@\n from __future__ import print_function\n from w3lib.url import is_url\n \n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.exceptions import UsageError\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n\n@@ -7,7 +7,7 @@ from importlib import import_module\n from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n \n\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n \n class Command(ScrapyCommand):\n \n\n@@ -3,7 +3,7 @@ import logging\n \n from w3lib.url import is_url\n \n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils import display\n\n@@ -3,7 +3,7 @@ import os\n from importlib import import_module\n \n from scrapy.utils.spider import iter_spider_classes\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.conf import arglist_to_dict\n \n\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n \n class Command(ScrapyCommand):\n \n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/shell.rst\n \n from threading import Thread\n \n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n\n@@ -7,7 +7,7 @@ from os.path import join, exists, abspath\n from shutil import copytree, ignore_patterns\n \n import scrapy\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n \n\n@@ -5,7 +5,7 @@ import platform\n import twisted\n \n import scrapy\n-from scrapy.command import ScrapyCommand\n+from scrapy.commands import ScrapyCommand\n \n \n class Command(ScrapyCommand):\n\n@@ -1,5 +1,4 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.commands import fetch\n+from scrapy.commands import fetch, ScrapyCommand\n from scrapy.utils.response import open_in_browser\n \n class Command(fetch.Command):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfd40ed57f4e42c85849aaa62b0fe8b18ad69a04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 8 | Churn Cumulative: 289 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,7 @@ collect_ignore = [\n     \"scrapy/squeue.py\",\n     \"scrapy/log.py\",\n     \"scrapy/dupefilter.py\",\n+    \"scrapy/command.py\",\n ] + _py_files(\"scrapy/contrib\") + _py_files(\"scrapy/contrib_exp\")\n \n \n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.command` is deprecated, \"\n+              \"use `scrapy.commands` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.commands import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#896b6dd4812bcb09e197312eda32b82b7460b9ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 10 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +0,0 @@\n-\"\"\"\n-scrapy.linkextractors\n-\n-This package contains a collection of Link Extractors.\n-\n-For more info see docs/topics/link-extractors.rst\n-\"\"\"\n-from .lxmlhtml import LxmlLinkExtractor as LinkExtractor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d72536688fa1b74cfd108202e3303a52947df96a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 64/35 | Churn Δ: 14 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,9 @@\n \"\"\"\n-Common code and definitions used by Link extractors (located in\n-scrapy.linkextractors).\n+scrapy.linkextractors\n+\n+This package contains a collection of Link Extractors.\n+\n+For more info see docs/topics/link-extractors.rst\n \"\"\"\n import re\n from six.moves.urllib.parse import urlparse\n@@ -98,3 +101,6 @@ class FilteringLinkExtractor(object):\n \n     def _extract_links(self, *args, **kwargs):\n         return self.link_extractor._extract_links(*args, **kwargs)\n+\n+# Top-level imports\n+from .lxmlhtml import LxmlLinkExtractor as LinkExtractor\n\n@@ -11,7 +11,7 @@ from scrapy.selector import Selector\n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import unique as unique_list, str_to_unicode\n-from scrapy.linkextractor import FilteringLinkExtractor\n+from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.response import get_base_url\n \n \n\n@@ -8,7 +8,7 @@ from sgmllib import SGMLParser\n from w3lib.url import safe_url_string\n from scrapy.selector import Selector\n from scrapy.link import Link\n-from scrapy.linkextractor import FilteringLinkExtractor\n+from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import unique as unique_list, str_to_unicode\n from scrapy.utils.response import get_base_url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32945242ae230ec05f5cccc16aa33bc6c2326e4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): -35/0 | Churn Δ: 8 | Churn Cumulative: 200 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -17,6 +17,7 @@ collect_ignore = [\n     \"scrapy/log.py\",\n     \"scrapy/dupefilter.py\",\n     \"scrapy/command.py\",\n+    \"scrapy/linkextractor.py\",\n ] + _py_files(\"scrapy/contrib\") + _py_files(\"scrapy/contrib_exp\")\n \n \n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.linkextractor` is deprecated, \"\n+              \"use `scrapy.linkextractors` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.linkextractors import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4926091abdb22cb52a2286dda75f281961aeca8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 6 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +0,0 @@\n-from scrapy.spiders.crawl import CrawlSpider, Rule\n-from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider\n-from scrapy.spiders.sitemap import SitemapSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3f576a816b89260ea675aaf9d2a0140edd69949", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 50 | Files Changed: 48 | Hunks: 51 | Methods Changed: 6 | Complexity Δ (Sum/Max): 137/33 | Churn Δ: 103 | Churn Cumulative: 3585 | Contributors (this commit): 23 | Commits (past 90d): 128 | Contributors (cumulative): 133 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ usage:\n \n \"\"\"\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request\n \n \n\n@@ -45,7 +45,7 @@ if twisted_version >= (11, 1, 0):\n     optional_features.add('http11')\n \n # Declare top-level shortcuts\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request, FormRequest\n from scrapy.selector import Selector\n from scrapy.item import Item, Field\n\n@@ -138,7 +138,7 @@ class CrawlerRunner(object):\n         :param crawler_or_spidercls: already created crawler, or a spider class\n             or spider's name inside the project to create it\n         :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n-            :class:`~scrapy.spider.Spider` subclass or string\n+            :class:`~scrapy.spiders.Spider` subclass or string\n \n         :param list args: arguments to initialize the spider\n \n\n@@ -17,7 +17,7 @@ from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.settings import Settings\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.console import start_python_console\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n\n@@ -111,3 +111,7 @@ spiders = ObsoleteClass(\n     'it with your project settings\"'\n )\n \n+# Top-level imports\n+from scrapy.spiders.crawl import CrawlSpider, Rule\n+from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider\n+from scrapy.spiders.sitemap import SitemapSpider\n\n@@ -9,7 +9,7 @@ import copy\n \n from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n def identity(x):\n     return x\n\n@@ -4,7 +4,7 @@ for scraping from an XML feed.\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.selector import Selector\n\n@@ -1,4 +1,4 @@\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.spider import iterate_spider_output\n \n class InitSpider(Spider):\n\n@@ -1,7 +1,7 @@\n import re\n import logging\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n from scrapy.utils.gz import gunzip, is_gzipped\n\n@@ -3,7 +3,7 @@ import inspect\n \n import six\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.misc import  arg_to_iter\n \n logger = logging.getLogger(__name__)\n@@ -19,7 +19,7 @@ def iter_spider_classes(module):\n     \"\"\"\n     # this needs to be imported here until get rid of the spider manager\n     # singleton in scrapy.spider.spiders\n-    from scrapy.spider import Spider\n+    from scrapy.spiders import Spider\n \n     for obj in six.itervalues(vars(module)):\n         if inspect.isclass(obj) and \\\n\n@@ -27,7 +27,7 @@ def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.settings import Settings\n-    from scrapy.spider import Spider\n+    from scrapy.spiders import Spider\n \n     runner = CrawlerRunner(Settings(settings_dict))\n     return runner._create_crawler(spidercls or Spider)\n\n@@ -5,7 +5,7 @@ Some spiders used for testing and benchmarking\n import time\n from six.moves.urllib.parse import urlencode\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item\n from scrapy.linkextractors import LinkExtractor\n\n@@ -158,7 +158,7 @@ class MySpider(scrapy.Spider):\n         fname = abspath(join(tmpdir, 'myspider.py'))\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n\n@@ -2,7 +2,7 @@ from unittest import TextTestResult\n \n from twisted.trial import unittest\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.contracts import ContractsManager\n\n@@ -24,7 +24,7 @@ from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy import optional_features\n\n@@ -2,7 +2,7 @@ from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n from scrapy.http import Request, Response\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.utils.test import get_crawler\n \n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n import re\n \n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n \n \n\n@@ -1,7 +1,7 @@\n from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from tests import get_testdata\n from scrapy.utils.test import assert_samelines\n \n\n@@ -3,7 +3,7 @@ import six\n \n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.utils.test import get_crawler\n \n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.http import Request\n from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n class TestSpider(Spider):\n     http_user = 'foo'\n\n@@ -8,7 +8,7 @@ from contextlib import contextmanager\n import pytest\n \n from scrapy.http import Response, HtmlResponse, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n\n@@ -3,7 +3,7 @@ from unittest import TestCase\n from os.path import join, abspath, dirname\n from gzip import GzipFile\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware\n from tests import tests_datadir\n\n@@ -5,7 +5,7 @@ from twisted.trial.unittest import TestCase, SkipTest\n from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n spider = Spider('foo')\n \n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, HtmlResponse\n from scrapy.utils.test import get_crawler\n\n@@ -7,7 +7,7 @@ from twisted.internet.error import TimeoutError, DNSLookupError, \\\n from scrapy import optional_features\n from scrapy.downloadermiddlewares.retry import RetryMiddleware\n from scrapy.xlib.tx import ResponseFailed\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request, Response\n from scrapy.utils.test import get_crawler\n \n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.downloadermiddlewares.stats import DownloaderStats\n from scrapy.http import Request, Response\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -1,6 +1,6 @@\n from unittest import TestCase\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request\n from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n from scrapy.utils.test import get_crawler\n\n@@ -22,7 +22,7 @@ from scrapy import signals\n from scrapy.utils.test import get_crawler\n from scrapy.xlib.pydispatch import dispatcher\n from tests import tests_datadir\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n from scrapy.http import Request\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.http import Request, Response\n from scrapy.item import Item, Field\n from scrapy.logformatter import LogFormatter\n\n@@ -6,7 +6,7 @@ from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n \n from scrapy.http import Request, Response\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.request import request_fingerprint\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.utils.signal import disconnect_all\n\n@@ -7,11 +7,10 @@ from testfixtures import LogCapture\n from twisted.trial import unittest\n \n from scrapy import signals\n-from scrapy.spider import Spider, BaseSpider\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.spiders.init import InitSpider\n-from scrapy.spiders import CrawlSpider, Rule, XMLFeedSpider, \\\n+from scrapy.spiders import Spider, BaseSpider, CrawlSpider, Rule, XMLFeedSpider, \\\n     CSVFeedSpider, SitemapSpider\n from scrapy.linkextractors import LinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -116,7 +115,7 @@ class SpiderTest(unittest.TestCase):\n \n     def test_log(self):\n         spider = self.spider_class('example.com')\n-        with mock.patch('scrapy.spider.Spider.logger') as mock_logger:\n+        with mock.patch('scrapy.spiders.Spider.logger') as mock_logger:\n             spider.log('test log msg', 'INFO')\n         mock_logger.log.assert_called_once_with('INFO', 'test log msg')\n \n\n@@ -6,7 +6,7 @@ from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n \n \n-# ugly hack to avoid cyclic imports of scrapy.spider when running this test\n+# ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n # alone\n from scrapy.interfaces import ISpiderLoader\n from scrapy.spiderloader import SpiderLoader\n\n@@ -1,4 +1,4 @@\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n class Spider0(Spider):\n     allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n@@ -1,4 +1,4 @@\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n class Spider1(Spider):\n     name = \"spider1\"\n\n@@ -1,4 +1,4 @@\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n class Spider2(Spider):\n     name = \"spider2\"\n\n@@ -1,4 +1,4 @@\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n class Spider3(Spider):\n     name = \"spider3\"\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.spidermiddlewares.depth import DepthMiddleware\n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.test import get_crawler\n \n\n@@ -7,7 +7,7 @@ from twisted.internet import defer\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.spidermiddlewares.httperror import HttpErrorMiddleware, HttpError\n from scrapy.settings import Settings\n \n\n@@ -3,7 +3,7 @@ from unittest import TestCase\n from six.moves.urllib.parse import urlparse\n \n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.spidermiddlewares.offsite import OffsiteMiddleware\n from scrapy.utils.test import get_crawler\n \n\n@@ -1,7 +1,7 @@\n from unittest import TestCase\n \n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.spidermiddlewares.referer import RefererMiddleware\n \n \n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.http import Response, Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n \n class TestUrlLengthMiddleware(TestCase):\n\n@@ -3,7 +3,7 @@ from datetime import datetime\n from twisted.trial import unittest\n \n from scrapy.extensions.spiderstate import SpiderState\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n \n \n class SpiderStateTest(unittest.TestCase):\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector, DummyStatsCollector\n from scrapy.utils.test import get_crawler\n \n\n@@ -21,7 +21,7 @@ class ToplevelTestCase(TestCase):\n         self.assertIs(scrapy.FormRequest, FormRequest)\n \n     def test_spider_shortcut(self):\n-        from scrapy.spider import Spider\n+        from scrapy.spiders import Spider\n         self.assertIs(scrapy.Spider, Spider)\n \n     def test_selector_shortcut(self):\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.http import Request\n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n class RequestSerializationTest(unittest.TestCase):\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.spider import Spider\n+from scrapy.spiders import Spider\n from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url\n \n __doctests__ = ['scrapy.utils.url']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53fdaa3f70e662b21d209e2094dffba9cd33823f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 8 | Churn Cumulative: 233 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -18,6 +18,7 @@ collect_ignore = [\n     \"scrapy/dupefilter.py\",\n     \"scrapy/command.py\",\n     \"scrapy/linkextractor.py\",\n+    \"scrapy/spider.py\",\n ] + _py_files(\"scrapy/contrib\") + _py_files(\"scrapy/contrib_exp\")\n \n \n\n@@ -0,0 +1,7 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.spider` is deprecated, \"\n+              \"use `scrapy.spiders` instead\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+from scrapy.spiders import *\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5753e498bf4ade06942e853279913375a0b338a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 48 | Churn Cumulative: 167 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,8 +13,24 @@ class SettingsListDirective(Directive):\n         return [settingslist_node('')]\n \n \n-def is_setting_node(node):\n-    return node.tagname == 'pending_xref' and node['reftype'] == 'setting'\n+def is_setting_index(node):\n+    if node.tagname == 'index':\n+        # index entries for setting directives look like:\n+        # [(u'pair', u'SETTING_NAME; setting', u'std:setting-SETTING_NAME', '')]\n+        entry_type, info, refid, _ = node['entries'][0]\n+        return entry_type == 'pair' and info.endswith('; setting')\n+    return False\n+\n+\n+def get_setting_target(node):\n+    # target nodes are placed next to the node in the doc tree\n+    return node.parent[node.parent.index(node) + 1]\n+\n+\n+def get_setting_name_and_refid(node):\n+    \"\"\"Extract setting name from directive index node\"\"\"\n+    entry_type, info, refid, _ = node['entries'][0]\n+    return info.replace('; setting', ''), refid\n \n \n def collect_scrapy_settings_refs(app, doctree):\n@@ -23,29 +39,24 @@ def collect_scrapy_settings_refs(app, doctree):\n     if not hasattr(env, 'scrapy_all_settings'):\n         env.scrapy_all_settings = []\n \n-    for node in doctree.traverse(is_setting_node):\n-        try:\n-            targetnode = node.parent[node.parent.index(node) - 1]\n-            if not isinstance(targetnode, nodes.target):\n-                raise IndexError\n-        except IndexError:\n-            targetid = \"setting-%d\" % env.new_serialno('setting')\n-            targetnode = nodes.target('', '', ids=[targetid])\n-            node.replace_self([targetnode, node])\n+    for node in doctree.traverse(is_setting_index):\n+        targetnode = get_setting_target(node)\n+        assert isinstance(targetnode, nodes.target), \"Next node is not a target\"\n+\n+        setting_name, refid = get_setting_name_and_refid(node)\n \n         env.scrapy_all_settings.append({\n             'docname': env.docname,\n-            'lineno': node.line,\n-            'node': node.deepcopy(),\n-            'target': targetnode,\n+            'setting_name': setting_name,\n+            'refid': refid,\n         })\n \n \n def make_setting_element(setting_data, app, fromdocname):\n-    text = nodes.Text(setting_data['node'].astext())\n-    targetid = ''  # TODO: resolve to a proper id\n     refnode = make_refnode(app.builder, fromdocname,\n-                           setting_data['docname'], targetid, text)\n+                           todocname=setting_data['docname'],\n+                           targetid=setting_data['refid'],\n+                           child=nodes.Text(setting_data['setting_name']))\n \n     p = nodes.paragraph()\n     p.append(refnode)\n@@ -57,7 +68,8 @@ def replace_settingslist_nodes(app, doctree, fromdocname):\n \n     for node in doctree.traverse(settingslist_node):\n         node.replace_self([make_setting_element(d, app, fromdocname)\n-                           for d in env.scrapy_all_settings])\n+                           for d in env.scrapy_all_settings if fromdocname != d['docname']])\n+\n \n def setup(app):\n     app.add_crossref_type(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6884b73e19050c1a6d11228886db51e1787df2bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 184 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,7 @@ from docutils.parsers.rst.roles import set_classes\n from docutils import nodes\n from sphinx.util.compat import Directive\n from sphinx.util.nodes import make_refnode\n+from operator import itemgetter\n \n \n class settingslist_node(nodes.General, nodes.Element):\n@@ -57,18 +58,24 @@ def make_setting_element(setting_data, app, fromdocname):\n                            todocname=setting_data['docname'],\n                            targetid=setting_data['refid'],\n                            child=nodes.Text(setting_data['setting_name']))\n-\n     p = nodes.paragraph()\n-    p.append(refnode)\n-    return p\n+    p += refnode\n+\n+    item = nodes.list_item()\n+    item += p\n+    return item\n \n \n def replace_settingslist_nodes(app, doctree, fromdocname):\n     env = app.builder.env\n \n     for node in doctree.traverse(settingslist_node):\n-        node.replace_self([make_setting_element(d, app, fromdocname)\n-                           for d in env.scrapy_all_settings if fromdocname != d['docname']])\n+        settings_list = nodes.bullet_list()\n+        settings_list.extend([make_setting_element(d, app, fromdocname)\n+                              for d in sorted(env.scrapy_all_settings,\n+                                              key=itemgetter('setting_name'))\n+                              if fromdocname != d['docname']])\n+        node.replace_self(settings_list)\n \n \n def setup(app):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#773ea5a5ef76426dd91a8669542d2602082a5746", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 16 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 39 | Churn Cumulative: 329 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/item.rst\n \n from pprint import pformat\n from collections import MutableMapping\n+\n from abc import ABCMeta\n import six\n \n@@ -27,7 +28,7 @@ class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n \n-        fields = {}\n+        fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)\n\n@@ -120,6 +120,14 @@ class ItemTest(unittest.TestCase):\n         self.assertSortedEqual(list(i.keys()), ['keys', 'values', 'name'])\n         self.assertSortedEqual(list(i.values()), [u'Keys', u'Values', u'John'])\n \n+    def test_metaclass_with_fields_attribute(self):\n+        class TestItem(Item):\n+            fields = {'new': Field(default='X')}\n+\n+        item = TestItem(new=u'New')\n+        self.assertSortedEqual(list(item.keys()), ['new'])\n+        self.assertSortedEqual(list(item.values()), [u'New'])\n+\n     def test_metaclass_inheritance(self):\n         class BaseItem(Item):\n             name = Field()\n@@ -136,41 +144,51 @@ class ItemTest(unittest.TestCase):\n \n     def test_metaclass_multiple_inheritance_simple(self):\n         class A(Item):\n+            fields = {'load': Field(default='A')}\n             save = Field(default='A')\n \n         class B(A): pass\n \n         class C(Item):\n+            fields = {'load': Field(default='C')}\n             save = Field(default='C')\n \n         class D(B, C): pass\n \n-        self.assertEqual(D(save='X')['save'], 'X')\n-        self.assertEqual(D.fields, {'save': {'default': 'A'}})\n+        item = D(save='X', load='Y')\n+        self.assertEqual(item['save'], 'X')\n+        self.assertEqual(item['load'], 'Y')\n+        self.assertEqual(D.fields, {'load': {'default': 'A'},\n+            'save': {'default': 'A'}})\n \n         # D class inverted\n         class E(C, B): pass\n \n         self.assertEqual(E(save='X')['save'], 'X')\n-        self.assertEqual(E.fields, {'save': {'default': 'C'}})\n+        self.assertEqual(E(load='X')['load'], 'X')\n+        self.assertEqual(E.fields, {'load': {'default': 'C'},\n+            'save': {'default': 'C'}})\n \n     def test_metaclass_multiple_inheritance_diamond(self):\n         class A(Item):\n+            fields = {'update': Field(default='A')}\n             save = Field(default='A')\n             load = Field(default='A')\n \n         class B(A): pass\n \n         class C(A):\n+            fields = {'update': Field(default='C')}\n             save = Field(default='C')\n \n         class D(B, C):\n+            fields = {'update': Field(default='D')}\n             load = Field(default='D')\n \n         self.assertEqual(D(save='X')['save'], 'X')\n         self.assertEqual(D(load='X')['load'], 'X')\n         self.assertEqual(D.fields, {'save': {'default': 'C'},\n-            'load': {'default': 'D'}})\n+            'load': {'default': 'D'}, 'update': {'default': 'D'}})\n \n         # D class inverted\n         class E(C, B):\n@@ -179,15 +197,17 @@ class ItemTest(unittest.TestCase):\n         self.assertEqual(E(save='X')['save'], 'X')\n         self.assertEqual(E(load='X')['load'], 'X')\n         self.assertEqual(E.fields, {'save': {'default': 'C'},\n-            'load': {'default': 'E'}})\n+            'load': {'default': 'E'}, 'update': {'default': 'C'}})\n \n     def test_metaclass_multiple_inheritance_without_metaclass(self):\n         class A(Item):\n+            fields = {'load': Field(default='A')}\n             save = Field(default='A')\n \n         class B(A): pass\n \n         class C(object):\n+            fields = {'load': Field(default='C')}\n             not_allowed = Field(default='not_allowed')\n             save = Field(default='C')\n \n@@ -195,14 +215,16 @@ class ItemTest(unittest.TestCase):\n \n         self.assertRaises(KeyError, D, not_allowed='value')\n         self.assertEqual(D(save='X')['save'], 'X')\n-        self.assertEqual(D.fields, {'save': {'default': 'A'}})\n+        self.assertEqual(D.fields, {'save': {'default': 'A'},\n+            'load': {'default': 'A'}})\n \n         # D class inverted\n         class E(C, B): pass\n \n         self.assertRaises(KeyError, E, not_allowed='value')\n         self.assertEqual(E(save='X')['save'], 'X')\n-        self.assertEqual(E.fields, {'save': {'default': 'A'}})\n+        self.assertEqual(E.fields, {'save': {'default': 'A'},\n+            'load': {'default': 'A'}})\n \n     def test_to_dict(self):\n         class TestItem(Item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
