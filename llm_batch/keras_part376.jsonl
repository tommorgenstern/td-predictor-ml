{"custom_id": "keras#58fd473e40e4cebc93de71543672d615a1e5909c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 399 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -239,7 +239,7 @@ class SidecarEvaluator:\n           ]))\n \n       if (self.max_evaluations and\n-          (self.max_evaluations == int(latest_checkpoint.split('-')[-1]))):\n+          (self.max_evaluations <= int(latest_checkpoint.split('-')[-1]))):\n         # Exit the loop because we have evaluated the final checkpoint file.\n         logging.info('Last checkpoint evaluated. SidecarEvaluator stops.')\n         return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#20f89aa01bb9d35512f6b0884f3e7c3490943df0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 7210 | Contributors (this commit): 20 | Commits (past 90d): 13 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -90,8 +90,15 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   A layer is a callable object that takes as input one or more tensors and\n   that outputs one or more tensors. It involves *computation*, defined\n-  in the `call()` method, and a *state* (weight variables), defined\n-  either in the constructor `__init__()` or in the `build()` method.\n+  in the `call()` method, and a *state* (weight variables). State can be\n+  created in various places, at the convenience of the subclass implementer:\n+\n+  * in `__init__()`;\n+  * in the optional `build()` method, which is invoked by the first\n+    `__call__()` to the layer, and supplies the shape(s) of the input(s),\n+    which may not have been known at initialization time;\n+  * in the first invocation of `call()`, with some caveats discussed\n+    below.\n \n   Users will just instantiate a layer and then treat it as a callable.\n \n@@ -134,15 +141,17 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   We recommend that descendants of `Layer` implement the following methods:\n \n-  * `__init__()`: Defines custom layer attributes, and creates layer state\n-    variables that do not depend on input shapes, using `add_weight()`.\n+  * `__init__()`: Defines custom layer attributes, and creates layer weights\n+    that do not depend on input shapes, using `add_weight()`, or other state.\n   * `build(self, input_shape)`: This method can be used to create weights that\n-    depend on the shape(s) of the input(s), using `add_weight()`. `__call__()`\n-    will automatically build the layer (if it has not been built yet) by\n-    calling `build()`.\n+    depend on the shape(s) of the input(s), using `add_weight()`, or other\n+    state. `__call__()` will automatically build the layer (if it has not been\n+    built yet) by calling `build()`.\n   * `call(self, inputs, *args, **kwargs)`: Called in `__call__` after making\n     sure `build()` has been called. `call()` performs the logic of applying the\n-    layer to the input tensors (which should be passed in as argument).\n+    layer to the `inputs`. The first invocation may additionally create state\n+    that could not be conveniently created in `build()`; see its docstring\n+    for details.\n     Two reserved keyword arguments you can optionally use in `call()` are:\n       - `training` (boolean, whether the call is in inference mode or training\n         mode). See more details in [the layer/model subclassing guide](\n@@ -345,7 +354,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # for instance stateful RNNs.\n     self._stateful = False\n     # Indicates whether `build` needs to be called upon layer call, to create\n-    # the layer's weights.\n+    # the layer's weights. (Note that the first call() may also create weights,\n+    # independent of build().)\n     self.built = False\n     # Provides information about which inputs are compatible with the layer.\n     self._input_spec = None\n@@ -460,9 +470,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     This is a method that implementers of subclasses of `Layer` or `Model`\n     can override if they need a state-creation step in-between\n-    layer instantiation and layer call.\n+    layer instantiation and layer call. It is invoked automatically before\n+    the first execution of `call()`.\n \n-    This is typically used to create the weights of `Layer` subclasses.\n+    This is typically used to create the weights of `Layer` subclasses\n+    (at the discretion of the subclass implementer).\n \n     Args:\n       input_shape: Instance of `TensorShape`, or list of instances of\n@@ -483,6 +495,11 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n     method to support masking.\n \n+    The `call()` method may not create state (except in its first invocation,\n+    wrapping the creation of variables or other resources in `tf.init_scope()`).\n+    It is recommended to create state in `__init__()`, or the `build()` method\n+    that is called automatically before `call()` executes the first time.\n+\n     Args:\n       inputs: Input tensor, or dict/list/tuple of input tensors.\n         The first positional `inputs` argument is subject to special rules:\n@@ -786,9 +803,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   def compute_output_shape(self, input_shape):\n     \"\"\"Computes the output shape of the layer.\n \n-    If the layer has not been built, this method will call `build` on the\n-    layer. This assumes that the layer will later be used with inputs that\n-    match the input shape provided here.\n+    This method will cause the layer's state to be built, if that has not\n+    happened before. This requires that the layer will later be used with\n+    inputs that match the input shape provided here.\n \n     Args:\n         input_shape: Shape tuple (tuple of integers)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c80abc967aef464a594851c2d9d5f70b537c8b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 179 | Files Changed: 8 | Hunks: 37 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 255 | Churn Cumulative: 1262 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,15 +14,17 @@\n # ==============================================================================\n \"\"\"Distribution tests for keras.layers.preprocessing.category_encoding.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+# pylint: disable=g-direct-tensorflow-import\n \n import keras\n+from keras import backend\n from keras import keras_parameterized\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.framework import test_util\n \n \n def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n@@ -30,8 +32,8 @@ def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n     dataset = dataset.repeat(repeat)\n   # TPUs currently require fully defined input shapes, drop_remainder ensures\n   # the input will have fully defined shapes.\n-  if isinstance(strategy,\n-                (tf.distribute.experimental.TPUStrategy, tf.compat.v1.distribute.experimental.TPUStrategy)):\n+  if isinstance(strategy, (tf.distribute.experimental.TPUStrategy,\n+                           tf.compat.v1.distribute.experimental.TPUStrategy)):\n     return dataset.batch(batch_size, drop_remainder=True)\n   else:\n     return dataset.batch(batch_size)\n@@ -39,15 +41,20 @@ def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n-        # (b/156783625): Outside compilation failed for eager mode only.\n-        strategy=strategy_combinations.strategies_minus_tpu +\n-        strategy_combinations.multi_worker_mirrored_strategies,\n-        mode=[\"eager\", \"graph\"]))\n+        strategy=strategy_combinations.all_strategies +\n+        strategy_combinations.multi_worker_mirrored_strategies +\n+        strategy_combinations.parameter_server_strategies_single_worker +\n+        strategy_combinations.parameter_server_strategies_multi_worker,\n+        mode=[\"eager\"]))\n class CategoryEncodingDistributionTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_strategy(self, strategy):\n+    if (backend.is_tpu_strategy(strategy) and\n+        not test_util.is_mlir_bridge_enabled()):\n+      self.skipTest(\"TPU tests require MLIR bridge\")\n+\n     input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n     inp_dataset = tf.data.Dataset.from_tensor_slices(input_array)\n     inp_dataset = batch_wrapper(inp_dataset, 2, strategy)\n\n@@ -14,27 +14,29 @@\n # ==============================================================================\n \"\"\"Distribution tests for keras.layers.preprocessing.discretization.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+# pylint: disable=g-direct-tensorflow-import\n \n import keras\n from keras import keras_parameterized\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n-        strategy_combinations.multi_worker_mirrored_strategies,\n-        mode=[\"eager\", \"graph\"]))\n+        strategy_combinations.multi_worker_mirrored_strategies +\n+        strategy_combinations.parameter_server_strategies_single_worker +\n+        strategy_combinations.parameter_server_strategies_multi_worker,\n+        mode=[\"eager\"]))\n class DiscretizationDistributionTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n-  def test_distribution(self, strategy):\n+  def test_strategy(self, strategy):\n     input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n \n     expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]\n\n@@ -14,25 +14,34 @@\n # ==============================================================================\n \"\"\"Tests for keras.layers.preprocessing.hashing.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+# pylint: disable=g-direct-tensorflow-import\n \n import keras\n+from keras import backend\n from keras import keras_parameterized\n-from keras.distribute.strategy_combinations import all_strategies\n+from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.framework import test_util\n \n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n-        distribution=all_strategies,\n-        mode=[\"eager\", \"graph\"]))\n+        strategy=strategy_combinations.all_strategies +\n+        strategy_combinations.multi_worker_mirrored_strategies +\n+        strategy_combinations.parameter_server_strategies_single_worker +\n+        strategy_combinations.parameter_server_strategies_multi_worker,\n+        mode=[\"eager\"]))\n class HashingDistributionTest(keras_parameterized.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n-  def test_distribution(self, distribution):\n+  def test_strategy(self, strategy):\n+    if (backend.is_tpu_strategy(strategy) and\n+        not test_util.is_mlir_bridge_enabled()):\n+      self.skipTest(\"TPU tests require MLIR bridge\")\n+\n     input_data = np.asarray([[\"omar\"], [\"stringer\"], [\"marlo\"], [\"wire\"]])\n     input_dataset = tf.data.Dataset.from_tensor_slices(input_data).batch(\n         2, drop_remainder=True)\n@@ -40,7 +49,7 @@ class HashingDistributionTest(keras_parameterized.TestCase,\n \n     tf.config.set_soft_device_placement(True)\n \n-    with distribution.scope():\n+    with strategy.scope():\n       input_data = keras.Input(shape=(None,), dtype=tf.string)\n       layer = hashing.Hashing(num_bins=2)\n       int_data = layer(input_data)\n@@ -50,4 +59,5 @@ class HashingDistributionTest(keras_parameterized.TestCase,\n \n \n if __name__ == \"__main__\":\n-  tf.test.main()\n+  tf.compat.v1.enable_v2_behavior()\n+  tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -14,15 +14,13 @@\n # ==============================================================================\n \"\"\"Distribution tests for keras.layers.preprocessing.image_preprocessing.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-\n import keras\n from keras import keras_parameterized\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import image_preprocessing\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @tf.__internal__.distribute.combinations.generate(\n@@ -57,7 +55,6 @@ class ImagePreprocessingDistributionTest(\n       cls_layer = keras.layers.Dense(units=1, activation=\"sigmoid\")\n       output = cls_layer(output)\n       model = keras.Model(inputs=input_data, outputs=output)\n-    model.compile(loss=\"binary_crossentropy\")\n     _ = model.predict(image_dataset)\n \n \n\n@@ -14,10 +14,9 @@\n # ==============================================================================\n \"\"\"Distribution tests for keras.layers.preprocessing.index_lookup.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n \n import os\n-import numpy as np\n \n import keras\n from keras import backend\n@@ -25,13 +24,18 @@ from keras import keras_parameterized\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import index_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.framework import test_util\n \n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n-        strategy_combinations.multi_worker_mirrored_strategies,\n-        mode=[\"eager\"]))  # Eager-only, no graph: b/158793009\n+        strategy_combinations.multi_worker_mirrored_strategies +\n+        strategy_combinations.parameter_server_strategies_single_worker +\n+        strategy_combinations.parameter_server_strategies_multi_worker,\n+        mode=[\"eager\"]))\n class IndexLookupDistributionTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n@@ -46,9 +50,9 @@ class IndexLookupDistributionTest(\n     return vocab_path\n \n   def test_strategy(self, strategy):\n-    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.\n-    if backend.is_tpu_strategy(strategy):\n-      self.skipTest(\"This test needs MLIR bridge on TPU.\")\n+    if (backend.is_tpu_strategy(strategy) and\n+        not test_util.is_mlir_bridge_enabled()):\n+      self.skipTest(\"TPU tests require MLIR bridge\")\n \n     vocab_data = [[\n         \"earth\", \"earth\", \"earth\", \"earth\", \"wind\", \"wind\", \"wind\", \"and\",\n@@ -79,9 +83,9 @@ class IndexLookupDistributionTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n   def test_strategy_with_file(self, strategy):\n-    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.\n-    if backend.is_tpu_strategy(strategy):\n-      self.skipTest(\"This test needs MLIR bridge on TPU.\")\n+    if (backend.is_tpu_strategy(strategy) and\n+        not test_util.is_mlir_bridge_enabled()):\n+      self.skipTest(\"TPU tests require MLIR bridge\")\n \n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     vocab_file = self._write_to_temp_file(\"temp\", vocab_data)\n@@ -138,7 +142,6 @@ class IndexLookupDistributionTest(\n       layer.adapt(vocab_dataset)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n-    model.compile(loss=\"mse\")\n     output_dataset = model.predict(input_dataset)\n     self.assertAllEqual(expected_output, output_dataset)\n \n\n@@ -14,15 +14,14 @@\n # ==============================================================================\n \"\"\"Distribution tests for keras.layers.preprocessing.normalization.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n \n import keras\n from keras import keras_parameterized\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import normalization\n from keras.layers.preprocessing import preprocessing_test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n def _get_layer_computation_test_cases():\n@@ -94,7 +93,9 @@ def _get_layer_computation_test_cases():\n     tf.__internal__.test.combinations.times(\n         tf.__internal__.test.combinations.combine(\n             strategy=strategy_combinations.all_strategies +\n-            strategy_combinations.multi_worker_mirrored_strategies,\n+            strategy_combinations.multi_worker_mirrored_strategies +\n+            strategy_combinations.parameter_server_strategies_single_worker +\n+            strategy_combinations.parameter_server_strategies_multi_worker,\n             mode=[\"eager\"]), _get_layer_computation_test_cases()))\n class NormalizationTest(keras_parameterized.TestCase,\n                         preprocessing_test_utils.PreprocessingLayerTest):\n@@ -104,10 +105,8 @@ class NormalizationTest(keras_parameterized.TestCase,\n     input_shape = tuple([None for _ in range(test_data.ndim - 1)])\n     if use_dataset:\n       # Keras APIs expect batched datasets\n-      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(\n-          test_data.shape[0] // 2)\n-      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(\n-          test_data.shape[0] // 2)\n+      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(2)\n+      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(2)\n \n     with strategy.scope():\n       input_data = keras.Input(shape=input_shape)\n\n@@ -1,124 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.normalization.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from absl.testing import parameterized\n-\n-import numpy as np\n-\n-import keras\n-from keras import keras_parameterized\n-from keras.distribute import tpu_strategy_test_utils\n-from keras.layers.preprocessing import normalization\n-from keras.layers.preprocessing import preprocessing_test_utils\n-\n-\n-def _get_layer_computation_test_cases():\n-  test_cases = ({\n-      \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n-      \"axis\": -1,\n-      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_single_element\"\n-  }, {\n-      \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n-      \"axis\": None,\n-      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_single_element_none_axis\"\n-  }, {\n-      \"adapt_data\": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),\n-      \"axis\": None,\n-      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_single_element_none_axis_flat_data\"\n-  }, {\n-      \"adapt_data\":\n-          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],\n-                   np.float32),\n-      \"axis\":\n-          1,\n-      \"test_data\":\n-          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],\n-                   np.float32),\n-      \"expected\":\n-          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],\n-                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],\n-                   np.float32),\n-      \"testcase_name\":\n-          \"3d_internal_axis\"\n-  }, {\n-      \"adapt_data\":\n-          np.array(\n-              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],\n-              np.float32),\n-      \"axis\": (1, 2),\n-      \"test_data\":\n-          np.array(\n-              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],\n-              np.float32),\n-      \"expected\":\n-          np.array(\n-              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],\n-              np.float32),\n-      \"testcase_name\":\n-          \"3d_multiple_axis\"\n-  })\n-\n-  crossed_test_cases = []\n-  # Cross above test cases with use_dataset in (True, False)\n-  for use_dataset in (True, False):\n-    for case in test_cases:\n-      case = case.copy()\n-      if use_dataset:\n-        case[\"testcase_name\"] = case[\"testcase_name\"] + \"_with_dataset\"\n-      case[\"use_dataset\"] = use_dataset\n-      crossed_test_cases.append(case)\n-\n-  return crossed_test_cases\n-\n-\n-@keras_parameterized.run_all_keras_modes(\n-    always_skip_v1=True, always_skip_eager=True)\n-class NormalizationTest(keras_parameterized.TestCase,\n-                        preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  @parameterized.named_parameters(*_get_layer_computation_test_cases())\n-  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,\n-                             expected):\n-    input_shape = tuple([None for _ in range(test_data.ndim - 1)])\n-    if use_dataset:\n-      # Keras APIs expect batched datasets\n-      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(\n-          test_data.shape[0] // 2)\n-      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(\n-          test_data.shape[0] // 2)\n-\n-    strategy = tpu_strategy_test_utils.get_tpu_strategy()\n-\n-    with strategy.scope():\n-      input_data = keras.Input(shape=input_shape)\n-      layer = normalization.Normalization(axis=axis)\n-      layer.adapt(adapt_data)\n-      output = layer(input_data)\n-      model = keras.Model(input_data, output)\n-      output_data = model.predict(test_data)\n-    self.assertAllClose(expected, output_data)\n-\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n@@ -14,9 +14,7 @@\n # ==============================================================================\n \"\"\"Distribution tests for keras.layers.preprocessing.text_vectorization.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+# pylint: disable=g-direct-tensorflow-import\n \n import keras\n from keras import backend\n@@ -24,21 +22,26 @@ from keras import keras_parameterized\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import text_vectorization\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.framework import test_util\n \n \n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n-        strategy_combinations.multi_worker_mirrored_strategies,\n+        strategy_combinations.multi_worker_mirrored_strategies +\n+        strategy_combinations.parameter_server_strategies_single_worker +\n+        strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=[\"eager\"]))\n class TextVectorizationDistributionTest(\n     keras_parameterized.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_distribution_strategy_output(self, strategy):\n-    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.\n-    if backend.is_tpu_strategy(strategy):\n-      self.skipTest(\"This test needs MLIR bridge on TPU.\")\n+    if (backend.is_tpu_strategy(strategy) and\n+        not test_util.is_mlir_bridge_enabled()):\n+      self.skipTest(\"TPU tests require MLIR bridge\")\n \n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#071bcc81a62b2d7756184518599721d0c52851c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 109 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,9 +14,12 @@\n # ==============================================================================\n \"\"\"A simple network to use in tests and examples.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n+from keras.legacy_tf_layers import core\n+from keras.legacy_tf_layers import normalization\n from keras.optimizer_v2 import optimizer_v2\n \n+import tensorflow.compat.v2 as tf\n+\n \n def minimize_loss_example(optimizer, use_bias=False, use_callable_loss=True):\n   \"\"\"Example of non-distribution-aware legacy code.\"\"\"\n@@ -27,7 +30,7 @@ def minimize_loss_example(optimizer, use_bias=False, use_callable_loss=True):\n     # fully defined for TPU.  Remove this when XLA supports dynamic shapes.\n     return dataset.batch(1, drop_remainder=True)\n \n-  layer = tf.compat.v1.layers.Dense(1, use_bias=use_bias)\n+  layer = core.Dense(1, use_bias=use_bias)\n \n   def model_fn(x):\n     \"\"\"A very simple model written by the user.\"\"\"\n@@ -62,9 +65,9 @@ def batchnorm_example(optimizer_fn,\n          for z in range(batch_per_epoch)]).repeat()\n \n   optimizer = optimizer_fn()\n-  batchnorm = tf.compat.v1.layers.BatchNormalization(\n+  batchnorm = normalization.BatchNormalization(\n       renorm=renorm, momentum=momentum, fused=False)\n-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)\n+  layer = core.Dense(1, use_bias=False)\n \n   def model_fn(x):\n     \"\"\"A model that uses batchnorm.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d172769076abd5898e59ba567a135071df26a1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 114 | Files Changed: 9 | Hunks: 64 | Methods Changed: 26 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 218 | Churn Cumulative: 38293 | Contributors (this commit): 100 | Commits (past 90d): 59 | Contributors (cumulative): 151 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1606,7 +1606,8 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     # Test noise shape\n     outputs = backend.dropout(inputs, 0.2, noise_shape=(200, 1))\n     outputs_val = backend.eval(outputs)\n-    self.assertAllClose(outputs_val[2, :], outputs_val[3, :], atol=1e-5)\n+    # Make sure the whole column gets the same dropout\n+    self.assertEqual(np.min(outputs_val[0, :]), np.max(outputs_val[0, :]))\n \n \n class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -3414,6 +3414,42 @@ def _apply_name_scope_on_model_declaration(enable):\n   _is_name_scope_on_model_declaration_enabled = enable\n \n \n+class BaseRandomLayer(Layer):\n+  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, seed=None, force_generator=False, **kwargs):\n+    \"\"\"Initialize the BaseRandomLayer.\n+\n+    Note that the constructor is annotated with\n+    @no_automatic_dependency_tracking. This is to skip the auto\n+    tracking of self._random_generator instance, which is an AutoTrackable.\n+    The backend.RandomGenerator could contain a tf.random.Generator instance\n+    which will have tf.Variable as the internal state. We want to avoid saving\n+    that state into model.weights and checkpoints for backward compatibility\n+    reason. In the meantime, we still need to make them visible to SavedModel\n+    when it is tracing the tf.function for the `call()`.\n+    See _list_extra_dependencies_for_serialization below for more details.\n+\n+    Args:\n+      seed: optional integer, used to create RandomGenerator.\n+      force_generator: boolean, default to False, whether to force the\n+        RandomGenerator to use the code branch of tf.random.Generator.\n+      **kwargs: other keyward arguements that will be passed to the parent class\n+    \"\"\"\n+    super().__init__(**kwargs)\n+    self._random_generator = backend.RandomGenerator(\n+        seed, force_generator=force_generator)\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel only\n+    # (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n+\n+\n # Avoid breaking users who directly import this symbol from this file.\n # TODO(fchollet): remove this.\n InputSpec = input_spec.InputSpec  # pylint:disable=invalid-name\n\n@@ -14,24 +14,24 @@\n # ==============================================================================\n # pylint: disable=protected-access\n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \"\"\"Convolutional-recurrent layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.layers.recurrent import DropoutRNNCellMixin\n from keras.layers.recurrent import RNN\n from keras.utils import conv_utils\n from keras.utils import generic_utils\n from keras.utils import tf_utils\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -209,7 +209,7 @@ class ConvRNN(RNN):\n         shape=(batch_size, None) + input_shape[2:self.rank + 3])\n \n     # allow cell (if layer) to build before we set or validate state_spec\n-    if isinstance(self.cell, Layer):\n+    if isinstance(self.cell, base_layer.Layer):\n       step_input_shape = (input_shape[0],) + input_shape[2:]\n       if constants_shape is not None:\n         self.cell.build([step_input_shape] + constants_shape)\n@@ -397,7 +397,7 @@ class ConvRNN(RNN):\n         backend.set_value(state, value)\n \n \n-class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n+class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the ConvLSTM layer.\n \n   Args:\n\n@@ -16,14 +16,14 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import backend\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export('keras.layers.Dropout')\n-class Dropout(Layer):\n+class Dropout(base_layer.BaseRandomLayer):\n   \"\"\"Applies Dropout to the input.\n \n   The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n@@ -74,7 +74,6 @@ class Dropout(Layer):\n       training mode (adding dropout) or in inference mode (doing nothing).\n   \"\"\"\n \n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n     # Note that the constructor is annotated with\n     # @no_automatic_dependency_tracking. This is to skip the auto\n@@ -85,7 +84,7 @@ class Dropout(Layer):\n     # reason. In the meantime, we still need to make them visible to SavedModel\n     # when it is tracing the tf.function for the `call()`.\n     # See _list_extra_dependencies_for_serialization below for more details.\n-    super(Dropout, self).__init__(**kwargs)\n+    super(Dropout, self).__init__(seed=seed, **kwargs)\n     if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n       raise ValueError(f'Invalid value {rate} received for '\n                        f'`rate`, expected a value between 0 and 1.')\n@@ -93,7 +92,6 @@ class Dropout(Layer):\n     self.noise_shape = noise_shape\n     self.seed = seed\n     self.supports_masking = True\n-    self._random_generator = backend.RandomGenerator(seed)\n \n   def build(self, input_shape):\n     self._random_generator._maybe_init()  # pylint: disable=protected-access\n@@ -134,11 +132,3 @@ class Dropout(Layer):\n     }\n     base_config = super(Dropout, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel only\n-    # (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n\n@@ -17,15 +17,16 @@\n This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n-import tensorflow.compat.v2 as tf\n from keras import backend\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.utils import control_flow_util\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class BaseDenseAttention(Layer):\n+class BaseDenseAttention(base_layer.BaseRandomLayer):\n   \"\"\"Base Attention class for Dense networks.\n \n   This class is suitable for Dense or CNN networks, and not for RNN networks.\n@@ -67,12 +68,12 @@ class BaseDenseAttention(Layer):\n       `[batch_size, Tq, Tv]`.\n   \"\"\"\n \n-  def __init__(self, causal=False, dropout=0.0,\n-               **kwargs):\n+  def __init__(self, causal=False, dropout=0.0, **kwargs):\n     super(BaseDenseAttention, self).__init__(**kwargs)\n     self.causal = causal\n     self.dropout = dropout\n     self.supports_masking = True\n+    self._random_generator = backend.RandomGenerator()\n \n   def _calculate_scores(self, query, key):\n     \"\"\"Calculates attention scores.\n@@ -126,7 +127,7 @@ class BaseDenseAttention(Layer):\n     weights = tf.nn.softmax(scores)\n \n     def dropped_weights():\n-      return tf.nn.dropout(weights, rate=self.dropout)\n+      return self._random_generator.dropout(weights, rate=self.dropout)\n \n     weights = control_flow_util.smart_cond(training, dropped_weights,\n                                            lambda: tf.identity(weights))\n\n@@ -25,34 +25,8 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class BaseRandomLayer(base_layer.Layer):\n-  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, seed=None, **kwargs):\n-    # Note that the constructor is annotated with\n-    # @no_automatic_dependency_tracking. This is to skip the auto\n-    # tracking of self._random_generator instance, which is an AutoTrackable.\n-    # The backend.RandomGenerator could contain a tf.random.Generator instance\n-    # which will have tf.Variable as the internal state. We want to avoid saving\n-    # that state into model.weights and checkpoints for backward compatibility\n-    # reason. In the meantime, we still need to make them visible to SavedModel\n-    # when it is tracing the tf.function for the `call()`.\n-    # See _list_extra_dependencies_for_serialization below for more details.\n-    super().__init__(**kwargs)\n-    self._random_generator = backend.RandomGenerator(seed)\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel\n-    # only (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n-\n-\n @keras_export('keras.layers.GaussianNoise')\n-class GaussianNoise(BaseRandomLayer):\n+class GaussianNoise(base_layer.BaseRandomLayer):\n   \"\"\"Apply additive zero-centered Gaussian noise.\n \n   This is useful to mitigate overfitting\n@@ -108,7 +82,7 @@ class GaussianNoise(BaseRandomLayer):\n \n \n @keras_export('keras.layers.GaussianDropout')\n-class GaussianDropout(BaseRandomLayer):\n+class GaussianDropout(base_layer.BaseRandomLayer):\n   \"\"\"Apply multiplicative 1-centered Gaussian noise.\n \n   As it is a regularization layer, it is only active at training time.\n@@ -164,7 +138,7 @@ class GaussianDropout(BaseRandomLayer):\n \n \n @keras_export('keras.layers.AlphaDropout')\n-class AlphaDropout(BaseRandomLayer):\n+class AlphaDropout(base_layer.BaseRandomLayer):\n   \"\"\"Applies Alpha Dropout to the input.\n \n   Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n\n@@ -189,35 +189,9 @@ class CenterCrop(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-class BaseRandomLayer(base_layer.Layer):\n-  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, seed=None, **kwargs):\n-    # Note that the constructor is annotated with\n-    # @no_automatic_dependency_tracking. This is to skip the auto\n-    # tracking of self._random_generator instance, which is an AutoTrackable.\n-    # The backend.RandomGenerator could contain a tf.random.Generator instance\n-    # which will have tf.Variable as the internal state. We want to avoid saving\n-    # that state into model.weights and checkpoints for backward compatibility\n-    # reason. In the meantime, we still need to make them visible to SavedModel\n-    # when it is tracing the tf.function for the `call()`.\n-    # See _list_extra_dependencies_for_serialization below for more details.\n-    super().__init__(**kwargs)\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel only\n-    # (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n-\n-\n @keras_export('keras.layers.RandomCrop',\n               'keras.layers.experimental.preprocessing.RandomCrop')\n-class RandomCrop(BaseRandomLayer):\n+class RandomCrop(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly crops images during training.\n \n   During training, this layer will randomly choose a location to crop images\n@@ -249,7 +223,8 @@ class RandomCrop(BaseRandomLayer):\n \n   def __init__(self, height, width, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n-    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed)\n+    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed,\n+                                     force_generator=True)\n     self.height = height\n     self.width = width\n     self.seed = seed\n@@ -354,7 +329,7 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n @keras_export('keras.layers.RandomFlip',\n               'keras.layers.experimental.preprocessing.RandomFlip')\n-class RandomFlip(BaseRandomLayer):\n+class RandomFlip(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly flips images during training.\n \n   This layer will flip the images horizontally and or vertically based on the\n@@ -384,7 +359,7 @@ class RandomFlip(BaseRandomLayer):\n                mode=HORIZONTAL_AND_VERTICAL,\n                seed=None,\n                **kwargs):\n-    super(RandomFlip, self).__init__(seed=seed, **kwargs)\n+    super(RandomFlip, self).__init__(seed=seed, force_generator=True, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomFlip').set(True)\n     self.mode = mode\n     if mode == HORIZONTAL:\n@@ -445,7 +420,7 @@ class RandomFlip(BaseRandomLayer):\n # TODO(tanzheny): Add examples, here and everywhere.\n @keras_export('keras.layers.RandomTranslation',\n               'keras.layers.experimental.preprocessing.RandomTranslation')\n-class RandomTranslation(BaseRandomLayer):\n+class RandomTranslation(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly translates images during training.\n \n   This layer will apply random translations to each image during training,\n@@ -507,7 +482,8 @@ class RandomTranslation(BaseRandomLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n-    super(RandomTranslation, self).__init__(seed=seed, **kwargs)\n+    super(RandomTranslation, self).__init__(seed=seed, force_generator=True,\n+                                            **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -771,7 +747,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomRotation',\n               'keras.layers.experimental.preprocessing.RandomRotation')\n-class RandomRotation(BaseRandomLayer):\n+class RandomRotation(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly rotates images during training.\n \n   This layer will apply random rotations to each image, filling empty space\n@@ -827,7 +803,8 @@ class RandomRotation(BaseRandomLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n-    super(RandomRotation, self).__init__(seed=seed, **kwargs)\n+    super(RandomRotation, self).__init__(seed=seed, force_generator=True,\n+                                         **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -897,7 +874,7 @@ class RandomRotation(BaseRandomLayer):\n \n @keras_export('keras.layers.RandomZoom',\n               'keras.layers.experimental.preprocessing.RandomZoom')\n-class RandomZoom(BaseRandomLayer):\n+class RandomZoom(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly zooms images during training.\n \n   This layer will randomly zoom in or out on each axis of an image\n@@ -964,7 +941,7 @@ class RandomZoom(BaseRandomLayer):\n                fill_value=0.0,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n-    super(RandomZoom, self).__init__(seed=seed, **kwargs)\n+    super(RandomZoom, self).__init__(seed=seed, force_generator=True, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -1103,7 +1080,7 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomContrast',\n               'keras.layers.experimental.preprocessing.RandomContrast')\n-class RandomContrast(BaseRandomLayer):\n+class RandomContrast(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n   This layer will randomly adjust the contrast of an image or images by a random\n@@ -1136,7 +1113,8 @@ class RandomContrast(BaseRandomLayer):\n   def __init__(self, factor, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n-    super(RandomContrast, self).__init__(seed=seed, **kwargs)\n+    super(RandomContrast, self).__init__(seed=seed, force_generator=True,\n+                                         **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -1181,7 +1159,7 @@ class RandomContrast(BaseRandomLayer):\n \n @keras_export('keras.layers.RandomHeight',\n               'keras.layers.experimental.preprocessing.RandomHeight')\n-class RandomHeight(BaseRandomLayer):\n+class RandomHeight(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image height during training.\n \n   This layer adjusts the height of a batch of images by a random factor.\n@@ -1222,7 +1200,8 @@ class RandomHeight(BaseRandomLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomHeight').set(True)\n-    super(RandomHeight, self).__init__(seed=seed, **kwargs)\n+    super(RandomHeight, self).__init__(seed=seed, force_generator=True,\n+                                       **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.height_lower = factor[0]\n@@ -1283,7 +1262,7 @@ class RandomHeight(BaseRandomLayer):\n \n @keras_export('keras.layers.RandomWidth',\n               'keras.layers.experimental.preprocessing.RandomWidth')\n-class RandomWidth(BaseRandomLayer):\n+class RandomWidth(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image width during training.\n \n   This layer will randomly adjusts the width of a batch of images of a\n@@ -1324,7 +1303,7 @@ class RandomWidth(BaseRandomLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomWidth').set(True)\n-    super(RandomWidth, self).__init__(seed=seed, **kwargs)\n+    super(RandomWidth, self).__init__(seed=seed, force_generator=True, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.width_lower = factor[0]\n\n@@ -28,7 +28,7 @@ from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.saving.saved_model import layer_serialization\n from keras.utils import control_flow_util\n@@ -45,7 +45,7 @@ RECURRENT_DROPOUT_WARNING_MSG = (\n \n \n @keras_export('keras.layers.StackedRNNCells')\n-class StackedRNNCells(Layer):\n+class StackedRNNCells(base_layer.Layer):\n   \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n \n   Used to implement efficient stacked RNNs.\n@@ -158,7 +158,7 @@ class StackedRNNCells(Layer):\n       return tuple([batch_size] + shape)\n \n     for cell in self.cells:\n-      if isinstance(cell, Layer) and not cell.built:\n+      if isinstance(cell, base_layer.Layer) and not cell.built:\n         with backend.name_scope(cell.name):\n           cell.build(input_shape)\n           cell.built = True\n@@ -196,7 +196,7 @@ class StackedRNNCells(Layer):\n \n \n @keras_export('keras.layers.RNN')\n-class RNN(Layer):\n+class RNN(base_layer.Layer):\n   \"\"\"Base class for recurrent layers.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -587,7 +587,7 @@ class RNN(Layer):\n       step_input_shape = tf.nest.map_structure(get_step_input_shape, input_shape)\n \n     # allow cell (if layer) to build before we set or validate state_spec.\n-    if isinstance(self.cell, Layer) and not self.cell.built:\n+    if isinstance(self.cell, base_layer.Layer) and not self.cell.built:\n       with backend.name_scope(self.cell.name):\n         self.cell.build(step_input_shape)\n         self.cell.built = True\n@@ -1025,7 +1025,7 @@ class RNN(Layer):\n \n \n @keras_export('keras.layers.AbstractRNNCell')\n-class AbstractRNNCell(Layer):\n+class AbstractRNNCell(base_layer.Layer):\n   \"\"\"Abstract object representing an RNN cell.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -1126,6 +1126,8 @@ class DropoutRNNCellMixin:\n       tensor need to dropout.\n     recurrent_dropout: a float number within range [0, 1). The ratio that the\n       recurrent state weights need to dropout.\n+    _random_generator: A backend.RandomGenerator instance, which will be used\n+      to produce outputs based on the inputs and dropout rate.\n   This object will create and cache created dropout masks, and reuse them for\n   the incoming data, so that the same mask is used for every batch input.\n   \"\"\"\n@@ -1181,6 +1183,7 @@ class DropoutRNNCellMixin:\n \n   def _create_dropout_mask(self, inputs, training, count=1):\n     return _generate_dropout_mask(\n+        self._random_generator,\n         tf.ones_like(inputs),\n         self.dropout,\n         training=training,\n@@ -1188,6 +1191,7 @@ class DropoutRNNCellMixin:\n \n   def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n     return _generate_dropout_mask(\n+        self._random_generator,\n         tf.ones_like(inputs),\n         self.recurrent_dropout,\n         training=training,\n@@ -1252,7 +1256,7 @@ class DropoutRNNCellMixin:\n \n \n @keras_export('keras.layers.SimpleRNNCell')\n-class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n+class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for SimpleRNN.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -1711,7 +1715,7 @@ class SimpleRNN(RNN):\n \n \n @keras_export(v1=['keras.layers.GRUCell'])\n-class GRUCell(DropoutRNNCellMixin, Layer):\n+class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the GRU layer.\n \n   Args:\n@@ -2271,7 +2275,7 @@ class GRU(RNN):\n \n \n @keras_export(v1=['keras.layers.LSTMCell'])\n-class LSTMCell(DropoutRNNCellMixin, Layer):\n+class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the LSTM layer.\n \n   Args:\n@@ -2945,9 +2949,9 @@ class LSTM(RNN):\n     return cls(**config)\n \n \n-def _generate_dropout_mask(ones, rate, training=None, count=1):\n+def _generate_dropout_mask(generator, ones, rate, training=None, count=1):\n   def dropped_inputs():\n-    return backend.dropout(ones, rate)\n+    return generator.dropout(ones, rate)\n \n   if count > 1:\n     return [\n\n@@ -12,17 +12,20 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \"\"\"Recurrent layers for TF 2.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n import uuid\n-from tensorflow.python.eager.context import get_device_name\n+\n from keras import activations\n from keras import backend\n+from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.layers import recurrent\n+\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.eager.context import get_device_name\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -201,7 +204,8 @@ class GRUCell(recurrent.GRUCell):\n \n \n @keras_export('keras.layers.GRU', v1=[])\n-class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n+class GRU(recurrent.DropoutRNNCellMixin,\n+          recurrent.GRU, base_layer.BaseRandomLayer):\n   \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -941,7 +945,8 @@ class LSTMCell(recurrent.LSTMCell):\n \n \n @keras_export('keras.layers.LSTM', v1=[])\n-class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n+class LSTM(recurrent.DropoutRNNCellMixin,\n+           recurrent.LSTM, base_layer.BaseRandomLayer):\n   \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
