{"custom_id": "keras#a8d7d04449aedacade10633890f09b0d79634231", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 39 | Files Changed: 1 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 74 | Churn Cumulative: 261 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,9 +3,8 @@ import sys\n import numpy as np\n from numpy.testing import assert_allclose\n \n-from keras.models import Model, Sequential\n-from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed\n-from keras.layers import Input\n+import keras\n+from keras import layers\n from keras import optimizers\n from keras import losses\n from keras import metrics\n@@ -17,10 +16,10 @@ else:\n \n \n def test_sequential_model_pickling():\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(RepeatVector(3))\n-    model.add(TimeDistributed(Dense(3)))\n+    model = keras.Sequential()\n+    model.add(layers.Dense(2, input_shape=(3,)))\n+    model.add(layers.RepeatVector(3))\n+    model.add(layers.TimeDistributed(layers.Dense(3)))\n     model.compile(loss=losses.MSE,\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy],\n@@ -48,14 +47,18 @@ def test_sequential_model_pickling():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-def test_sequential_model_pickling_2():\n+def test_sequential_model_pickling_custom_objects():\n     # test with custom optimizer, loss\n-    custom_opt = optimizers.rmsprop\n-    custom_loss = losses.mse\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(Dense(3))\n-    model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n+    class CustomSGD(optimizers.SGD):\n+        pass\n+\n+    def custom_mse(*args, **kwargs):\n+        return losses.mse(*args, **kwargs)\n+\n+    model = keras.Sequential()\n+    model.add(layers.Dense(2, input_shape=(3,)))\n+    model.add(layers.Dense(3))\n+    model.compile(loss=custom_mse, optimizer=CustomSGD(), metrics=['acc'])\n \n     x = np.random.random((1, 3))\n     y = np.random.random((1, 3))\n@@ -64,6 +67,9 @@ def test_sequential_model_pickling_2():\n     out = model.predict(x)\n \n     state = pickle.dumps(model)\n+\n+    with keras.utils.CustomObjectScope(\n+            {'CustomSGD': CustomSGD, 'custom_mse': custom_mse}):\n         model = pickle.loads(state)\n \n     out2 = model.predict(x)\n@@ -71,11 +77,11 @@ def test_sequential_model_pickling_2():\n \n \n def test_functional_model_pickling():\n-    inputs = Input(shape=(3,))\n-    x = Dense(2)(inputs)\n-    outputs = Dense(3)(x)\n+    inputs = keras.Input(shape=(3,))\n+    x = layers.Dense(2)(inputs)\n+    outputs = layers.Dense(3)(x)\n \n-    model = Model(inputs, outputs)\n+    model = keras.Model(inputs, outputs)\n     model.compile(loss=losses.MSE,\n                   optimizer=optimizers.Adam(),\n                   metrics=[metrics.categorical_accuracy])\n@@ -93,12 +99,12 @@ def test_functional_model_pickling():\n \n \n def test_pickling_multiple_metrics_outputs():\n-    inputs = Input(shape=(5,))\n-    x = Dense(5)(inputs)\n-    output1 = Dense(1, name='output1')(x)\n-    output2 = Dense(1, name='output2')(x)\n+    inputs = keras.Input(shape=(5,))\n+    x = layers.Dense(5)(inputs)\n+    output1 = layers.Dense(1, name='output1')(x)\n+    output2 = layers.Dense(1, name='output2')(x)\n \n-    model = Model(inputs=inputs, outputs=[output1, output2])\n+    model = keras.Model(inputs=inputs, outputs=[output1, output2])\n \n     metrics = {'output1': ['mse', 'binary_accuracy'],\n                'output2': ['mse', 'binary_accuracy']\n@@ -120,32 +126,22 @@ def test_pickling_multiple_metrics_outputs():\n def test_pickling_without_compilation():\n     \"\"\"Test pickling model without compiling.\n     \"\"\"\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(Dense(3))\n+    model = keras.Sequential()\n+    model.add(layers.Dense(2, input_shape=(3,)))\n+    model.add(layers.Dense(3))\n \n     model = pickle.loads(pickle.dumps(model))\n \n \n def test_pickling_right_after_compilation():\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(Dense(3))\n+    model = keras.Sequential()\n+    model.add(layers.Dense(2, input_shape=(3,)))\n+    model.add(layers.Dense(3))\n     model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n     model._make_train_function()\n \n     model = pickle.loads(pickle.dumps(model))\n \n \n-def test_pickling_unused_layers_is_ok():\n-    a = Input(shape=(256, 512, 6))\n-    b = Input(shape=(256, 512, 1))\n-    c = Lambda(lambda x: x[:, :, :, :1])(a)\n-\n-    model = Model(inputs=[a, b], outputs=c)\n-\n-    model = pickle.loads(pickle.dumps(model))\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6dff721a3a8755356b2e89d02ef63ad8ab38ec95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 1138 | Contributors (this commit): 21 | Commits (past 90d): 10 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,10 +34,13 @@ def categorical_accuracy(y_true, y_pred):\n \n \n def sparse_categorical_accuracy(y_true, y_pred):\n-    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)\n-    return K.cast(K.equal(K.flatten(y_true),\n-                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n-                  K.floatx())\n+    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n+    if K.ndim(y_true) == K.ndim(y_pred):\n+        y_true = K.squeeze(y_true, -1)\n+    # convert dense predictions to labels\n+    y_pred_labels = K.argmax(y_pred, axis=-1)\n+    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n+    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n \n \n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n\n@@ -44,11 +44,13 @@ def test_sparse_metrics():\n         assert K.eval(metric(y_a, y_b)).shape == (6,)\n \n \n-def test_sparse_categorical_accuracy_correctness():\n-    y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n-    y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n+@pytest.mark.parametrize('shape', [(6,), (6, 3), (6, 3, 1)])\n+def test_sparse_categorical_accuracy_correctness(shape):\n+    y_a = K.variable(np.random.randint(0, 7, shape), dtype=K.floatx())\n+    y_b_shape = shape + (7,)\n+    y_b = K.variable(np.random.random(y_b_shape), dtype=K.floatx())\n     # use one_hot embedding to convert sparse labels to equivalent dense labels\n-    y_a_dense_labels = K.cast(K.one_hot(K.cast(y_a, dtype='int32'), num_classes=7),\n+    y_a_dense_labels = K.cast(K.one_hot(K.cast(y_a, dtype='int32'), 7),\n                               dtype=K.floatx())\n     sparse_categorical_acc = metrics.sparse_categorical_accuracy(y_a, y_b)\n     categorical_acc = metrics.categorical_accuracy(y_a_dense_labels, y_b)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#161f0a876db4f8aa9d57161c13d366f081369601", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 49 | Files Changed: 8 | Hunks: 23 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 100 | Churn Cumulative: 32953 | Contributors (this commit): 208 | Commits (past 90d): 42 | Contributors (cumulative): 311 | DMM Complexity: None\n\nDIFF:\n@@ -717,8 +717,8 @@ class TensorBoard(Callback):\n         embeddings_data: data to be embedded at layers specified in\n             `embeddings_layer_names`. Numpy array (if the model has a single\n             input) or list of Numpy arrays (if the model has multiple inputs).\n-            Learn [more about embeddings]\n-            (https://www.tensorflow.org/programmers_guide/embedding).\n+            Learn [more about embeddings](\n+            https://www.tensorflow.org/programmers_guide/embedding).\n         update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`, writes\n             the losses and metrics to TensorBoard after each batch. The same\n             applies for `'epoch'`. If using an integer, let's say `10000`,\n\n@@ -40,8 +40,8 @@ class MaxNorm(Constraint):\n             `(rows, cols, input_depth)`.\n \n     # References\n-        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n-          (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n+           http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n \n     def __init__(self, max_value=2, axis=0):\n\n@@ -234,7 +234,8 @@ class Orthogonal(Initializer):\n         seed: A Python integer. Used to seed the random generator.\n \n     # References\n-        Saxe et al., http://arxiv.org/abs/1312.6120\n+        - [Exact solutions to the nonlinear dynamics of learning in deep\n+           linear neural networks](http://arxiv.org/abs/1312.6120)\n     \"\"\"\n \n     def __init__(self, gain=1., seed=None):\n@@ -314,8 +315,7 @@ def lecun_uniform(seed=None):\n         An initializer.\n \n     # References\n-        LeCun 98, Efficient Backprop,\n-        http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n+        - [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n     \"\"\"\n     return VarianceScaling(scale=1.,\n                            mode='fan_in',\n@@ -338,8 +338,8 @@ def glorot_normal(seed=None):\n         An initializer.\n \n     # References\n-        Glorot & Bengio, AISTATS 2010\n-        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n+        - [Understanding the difficulty of training deep feedforward neural\n+           networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n     \"\"\"\n     return VarianceScaling(scale=1.,\n                            mode='fan_avg',\n@@ -362,8 +362,8 @@ def glorot_uniform(seed=None):\n         An initializer.\n \n     # References\n-        Glorot & Bengio, AISTATS 2010\n-        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n+        - [Understanding the difficulty of training deep feedforward neural\n+           networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n     \"\"\"\n     return VarianceScaling(scale=1.,\n                            mode='fan_avg',\n@@ -385,7 +385,8 @@ def he_normal(seed=None):\n         An initializer.\n \n     # References\n-        He et al., http://arxiv.org/abs/1502.01852\n+        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n+           ImageNet Classification](http://arxiv.org/abs/1502.01852)\n     \"\"\"\n     return VarianceScaling(scale=2.,\n                            mode='fan_in',\n@@ -430,7 +431,8 @@ def he_uniform(seed=None):\n         An initializer.\n \n     # References\n-        He et al., http://arxiv.org/abs/1502.01852\n+        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n+           ImageNet Classification](http://arxiv.org/abs/1502.01852)\n     \"\"\"\n     return VarianceScaling(scale=2.,\n                            mode='fan_in',\n\n@@ -35,8 +35,8 @@ class LeakyReLU(Layer):\n         alpha: float >= 0. Negative slope coefficient.\n \n     # References\n-        - [Rectifier Nonlinearities Improve Neural Network Acoustic Models]\n-          (https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)\n+        - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](\n+           https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)\n     \"\"\"\n \n     def __init__(self, alpha=0.3, **kwargs):\n@@ -209,8 +209,8 @@ class ThresholdedReLU(Layer):\n         theta: float >= 0. Threshold location of activation.\n \n     # References\n-        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features]\n-          (https://arxiv.org/abs/1402.3337)\n+        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](\n+           https://arxiv.org/abs/1402.3337)\n     \"\"\"\n \n     def __init__(self, theta=1.0, **kwargs):\n\n@@ -273,8 +273,8 @@ class Conv1D(_Conv):\n             the output has the same length as the original input.\n             Useful when modeling temporal data where the model\n             should not violate the temporal order. See\n-            [WaveNet: A Generative Model for Raw Audio, section 2.1]\n-            (https://arxiv.org/abs/1609.03499).\n+            [WaveNet: A Generative Model for Raw Audio, section 2.1](\n+            https://arxiv.org/abs/1609.03499).\n         data_format: A string,\n             one of `\"channels_last\"` (default) or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs.\n@@ -727,10 +727,10 @@ class Conv2DTranspose(Conv2D):\n         ```\n \n     # References\n-        - [A guide to convolution arithmetic for deep learning]\n-          (https://arxiv.org/abs/1603.07285v1)\n-        - [Deconvolutional Networks]\n-          (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n+        - [A guide to convolution arithmetic for deep learning](\n+           https://arxiv.org/abs/1603.07285v1)\n+        - [Deconvolutional Networks](\n+           http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     \"\"\"\n \n     @interfaces.legacy_deconv2d_support\n@@ -1002,10 +1002,10 @@ class Conv3DTranspose(Conv3D):\n         ```\n \n     # References\n-        - [A guide to convolution arithmetic for deep learning]\n-          (https://arxiv.org/abs/1603.07285v1)\n-        - [Deconvolutional Networks]\n-          (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n+        - [A guide to convolution arithmetic for deep learning](\n+           https://arxiv.org/abs/1603.07285v1)\n+        - [Deconvolutional Networks](\n+           http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     \"\"\"\n \n     def __init__(self, filters,\n\n@@ -95,8 +95,8 @@ class Dropout(Layer):\n         seed: A Python integer to use as random seed.\n \n     # References\n-        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n-          (http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n+           http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n     \"\"\"\n     @interfaces.legacy_dropout_support\n     def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n@@ -159,8 +159,8 @@ class SpatialDropout1D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks]\n-          (https://arxiv.org/abs/1411.4280)\n+        - [Efficient Object Localization Using Convolutional Networks](\n+           https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n     @interfaces.legacy_spatialdropout1d_support\n@@ -205,8 +205,8 @@ class SpatialDropout2D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks]\n-          (https://arxiv.org/abs/1411.4280)\n+        - [Efficient Object Localization Using Convolutional Networks](\n+           https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n     @interfaces.legacy_spatialdropoutNd_support\n@@ -254,8 +254,8 @@ class SpatialDropout3D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks]\n-          (https://arxiv.org/abs/1411.4280)\n+        - [Efficient Object Localization Using Convolutional Networks](\n+           https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n     @interfaces.legacy_spatialdropoutNd_support\n\n@@ -74,8 +74,8 @@ class GaussianDropout(Layer):\n         Same shape as input.\n \n     # References\n-        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n-          (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n+           http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n \n     @interfaces.legacy_gaussiandropout_support\n\n@@ -234,8 +234,8 @@ class RMSprop(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n-        - [rmsprop: Divide the gradient by a running average of its recent magnitude]\n-          (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n+        - [rmsprop: Divide the gradient by a running average of its recent magnitude\n+           ](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     \"\"\"\n \n     def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,\n@@ -373,8 +373,8 @@ class Adadelta(Optimizer):\n         decay: float >= 0. Initial learning rate decay.\n \n     # References\n-        - [Adadelta - an adaptive learning rate method]\n-          (https://arxiv.org/abs/1212.5701)\n+        - [Adadelta - an adaptive learning rate method](\n+           https://arxiv.org/abs/1212.5701)\n     \"\"\"\n \n     def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,\n@@ -449,10 +449,10 @@ class Adam(Optimizer):\n             Beyond\".\n \n     # References\n-        - [Adam - A Method for Stochastic Optimization]\n-          (https://arxiv.org/abs/1412.6980v8)\n-        - [On the Convergence of Adam and Beyond]\n-          (https://openreview.net/forum?id=ryQu7f-RZ)\n+        - [Adam - A Method for Stochastic Optimization](\n+           https://arxiv.org/abs/1412.6980v8)\n+        - [On the Convergence of Adam and Beyond](\n+           https://openreview.net/forum?id=ryQu7f-RZ)\n     \"\"\"\n \n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n@@ -537,8 +537,8 @@ class Adamax(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n \n     # References\n-        - [Adam - A Method for Stochastic Optimization]\n-          (https://arxiv.org/abs/1412.6980v8)\n+        - [Adam - A Method for Stochastic Optimization](\n+           https://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n \n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n@@ -619,8 +619,8 @@ class Nadam(Optimizer):\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n-        - [On the importance of initialization and momentum in deep learning]\n-          (http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n+        - [On the importance of initialization and momentum in deep learning](\n+           http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n \n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02c1f8865684a055416a93bdc8a7bd07c988a09b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 9 | Churn Cumulative: 331 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,17 +28,16 @@ all_sparse_metrics = [\n ]\n \n \n-def test_metrics():\n+@pytest.mark.parametrize('metric', all_metrics)\n+def test_metrics(metric):\n     y_a = K.variable(np.random.random((6, 7)))\n     y_b = K.variable(np.random.random((6, 7)))\n-    for metric in all_metrics:\n     output = metric(y_a, y_b)\n-        print(metric.__name__)\n     assert K.eval(output).shape == (6,)\n \n \n-def test_sparse_metrics():\n-    for metric in all_sparse_metrics:\n+@pytest.mark.parametrize('metric', all_sparse_metrics)\n+def test_sparse_metrics(metric):\n     y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n     y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n     assert K.eval(metric(y_a, y_b)).shape == (6,)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#874f6db89d70b3c0221034943b2a427d88b63e81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 24 | Churn Cumulative: 5186 | Contributors (this commit): 31 | Commits (past 90d): 32 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -911,15 +911,15 @@ class TestBackend(object):\n                         np.log(np.sum(np.exp(x_np), axis=axis, keepdims=keepdims)),\n                         rtol=1e-5)\n \n+    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+                        reason='The optimization is applied only with TensorFlow.')\n     def test_logsumexp_optim(self):\n         '''\n         Check if optimization works.\n         '''\n-        for k in [KTF]:\n         x_np = np.array([1e+4, 1e-4])\n-            assert_allclose(k.eval(k.logsumexp(k.variable(x_np), axis=0)),\n-                            1e4,\n-                            rtol=1e-5)\n+        result = K.eval(K.logsumexp(K.variable(x_np), axis=0))\n+        assert_allclose(result, 1e4, rtol=1e-5)\n \n     def test_switch(self):\n         # scalar\n@@ -1441,10 +1441,10 @@ class TestBackend(object):\n             check_single_tensor_operation('spatial_2d_padding', x_shape, BACKENDS,\n                                           padding=padding, data_format=data_format)\n         # Check handling of dynamic shapes.\n-            for k in [KTF, KTH]:\n-                x = k.placeholder(shape=(1, None, None, 1))\n-                y = k.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n-                assert k.int_shape(y) == (1, None, None, 1)\n+        if K in [KTF, KTH]:\n+            x = K.placeholder(shape=(1, None, None, 1))\n+            y = K.spatial_2d_padding(x, padding=padding, data_format='channels_last')\n+            assert K.int_shape(y) == (1, None, None, 1)\n \n         # Test invalid use cases\n         xval = np.random.random(x_shape)\n@@ -1463,10 +1463,10 @@ class TestBackend(object):\n             check_single_tensor_operation('spatial_3d_padding', x_shape, BACKENDS,\n                                           padding=padding, data_format=data_format)\n         # Check handling of dynamic shapes.\n-            for k in [KTF, KTH]:\n-                x = k.placeholder(shape=(1, None, None, None, 1))\n-                y = k.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n-                assert k.int_shape(y) == (1, None, None, None, 1)\n+        if K in [KTF, KTH]:\n+            x = K.placeholder(shape=(1, None, None, None, 1))\n+            y = K.spatial_3d_padding(x, padding=padding, data_format='channels_last')\n+            assert K.int_shape(y) == (1, None, None, None, 1)\n \n         # Test invalid use cases\n         xval = np.random.random(x_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2d1dea24c8b369b3a4411f0ede9518cef1db7db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 48 | Churn Cumulative: 5905 | Contributors (this commit): 31 | Commits (past 90d): 51 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -202,15 +202,15 @@ class TestBackend(object):\n         check_two_tensor_operation('dot', (4, 2), (5, 2, 3), WITH_NP)\n \n         check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n-                                   BACKENDS, cntk_two_dynamicity=True, axes=(2, 2))\n+                                   WITH_NP, cntk_two_dynamicity=True, axes=(2, 2))\n         check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3),\n-                                   BACKENDS, cntk_two_dynamicity=True, axes=(2, 1))\n+                                   WITH_NP, cntk_two_dynamicity=True, axes=(2, 1))\n         check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3),\n-                                   BACKENDS, cntk_two_dynamicity=True, axes=(1, 1))\n+                                   WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n         check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n-                                   BACKENDS, cntk_two_dynamicity=True, axes=1)\n+                                   WITH_NP, cntk_two_dynamicity=True, axes=1)\n         check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n-                                   BACKENDS, cntk_two_dynamicity=True, axes=(1, 1))\n+                                   WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n \n         check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n         check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n\n@@ -474,6 +474,44 @@ def dot(x, y):\n     return np.dot(x, y)\n \n \n+def batch_dot(x, y, axes=None):\n+    if isinstance(axes, int):\n+        axes = (axes, axes)\n+    if axes is None:\n+        # behaves like tf.batch_matmul as default\n+        axes = [x.ndim - 1, y.ndim - 2]\n+    if any([isinstance(a, (list, tuple)) for a in axes]):\n+        raise ValueError('Multiple target dimensions are not supported. ' +\n+                         'Expected: None, int, (int, int), ' +\n+                         'Provided: ' + str(axes))\n+    if x.ndim > y.ndim:\n+        diff = x.ndim - y.ndim\n+        y = np.reshape(y, np.concatenate([np.shape(y), [1] * diff], axis=0))\n+    else:\n+        diff = 0\n+    if ndim(x) == 2 and ndim(y) == 2:\n+        if axes[0] == axes[1]:\n+            out = np.sum(np.multiply(x, y), axes[0])\n+        else:\n+            out = np.sum(np.multiply(np.transpose(x, [1, 0]), y), axes[1])\n+    else:\n+        out = np.tensordot(x, y, axes=axes)\n+        for axis in [axes[0]]:\n+            axis_list = np.arange(len(out.shape) - 1).tolist()\n+            axis_list.insert(0, axis_list.pop(axis))\n+            out = np.transpose(np.diagonal(out, axis1=0, axis2=axis),\n+                               tuple(axis_list))\n+    if diff:\n+        if x.ndim > y.ndim:\n+            idx = x.ndim + y.ndim - 3\n+        else:\n+            idx = x.ndim - 1\n+        out = np.squeeze(out, tuple(range(idx, idx + diff)))\n+    if ndim(out) == 1:\n+        out = expand_dims(out, 1)\n+    return out\n+\n+\n def transpose(x):\n     return np.transpose(x)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f2b261bc2555773bd88cbbeda976f98e244d02c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 11780 | Contributors (this commit): 127 | Commits (past 90d): 62 | Contributors (cumulative): 158 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,7 +54,6 @@ def parse_stories(lines, only_supporting=False):\n         if '\\t' in line:\n             q, a, supporting = line.split('\\t')\n             q = tokenize(q)\n-            substory = None\n             if only_supporting:\n                 # Only select the related substory\n                 supporting = map(int, supporting.split())\n\n@@ -98,7 +98,6 @@ def parse_stories(lines, only_supporting=False):\n         if '\\t' in line:\n             q, a, supporting = line.split('\\t')\n             q = tokenize(q)\n-            substory = None\n             if only_supporting:\n                 # Only select the related substory\n                 supporting = map(int, supporting.split())\n\n@@ -130,8 +130,6 @@ class Capsule(Layer):\n         b = K.zeros_like(hat_inputs[:, :, :, 0])\n         for i in range(self.routings):\n             c = softmax(b, 1)\n-            if K.backend() == 'theano':\n-                o = K.sum(o, axis=1)\n             o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n             if i < self.routings - 1:\n                 b = K.batch_dot(o, hat_inputs, [2, 3])\n\n@@ -885,8 +885,6 @@ class TensorBoard(Callback):\n \n             self.saver = tf.train.Saver(list(embeddings_vars.values()))\n \n-            embeddings_metadata = {}\n-\n             if not isinstance(self.embeddings_metadata, str):\n                 embeddings_metadata = self.embeddings_metadata\n             else:\n\n@@ -596,7 +596,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                 self.queue.task_done()\n                 if inputs is not None:\n                     yield inputs\n-        except Exception as e:\n+        except Exception:\n             self.stop()\n             six.reraise(*sys.exc_info())\n \n\n@@ -1601,7 +1601,6 @@ class TestBackend(object):\n              [0.0, 0.0, 0.0, 0.0],  # t=4 (ignored)\n              [0.0, 0.0, 0.0, 0.0]],  # t=5 (ignored)\n             dtype=np.float32)\n-        input_log_prob_matrix_0 = np.log(input_prob_matrix_0)\n \n         seq_len_1 = 5\n         # dimensions are time x depth\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#590aa832680641218b233b22fa8b3bd7fdc0287e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 524 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -111,7 +111,7 @@ def cast_to_floatx(x):\n \n \n def image_data_format():\n-    \"\"\"Returns the default image data format convention ('channels_first' or 'channels_last').\n+    \"\"\"Returns the default image data format convention.\n \n     # Returns\n         A string, either `'channels_first'` or `'channels_last'`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f12e96d8bfa22c2303ed0482544fb0ab6721a10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 5914 | Contributors (this commit): 31 | Commits (past 90d): 54 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -316,9 +316,9 @@ class TestBackend(object):\n         ref = np.arange(np.prod(shape)).reshape(shape)\n         inds = [1, 3, 7, 9]\n         t_list = [k.gather(k.variable(ref), k.variable(inds, dtype='int32'))\n-                  for k in BACKENDS]\n+                  for k in WITH_NP]\n         z_list = [k.eval(k.gather(k.variable(ref), k.variable(inds, dtype='int32')))\n-                  for k in BACKENDS]\n+                  for k in WITH_NP]\n \n         assert_list_pairwise(z_list)\n         assert_list_keras_shape(t_list, z_list)\n\n@@ -447,6 +447,10 @@ def batch_flatten(x):\n     return np.reshape(x, (x.shape[0], -1))\n \n \n+def gather(reference, indices):\n+    return reference[indices]\n+\n+\n def eval(x):\n     return x\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#885ba629acba8128fd40b0af92c960b99ac5b827", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 5217 | Contributors (this commit): 31 | Commits (past 90d): 36 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -754,7 +754,7 @@ class TestBackend(object):\n         assert_allclose(last_y1, last_y2, atol=1e-05)\n         assert_allclose(y1, y2, atol=1e-05)\n \n-    def legacy_test_rnn(self):\n+    def test_legacy_rnn(self):\n         # implement a simple RNN\n         num_samples = 4\n         input_dim = 5\n@@ -849,7 +849,7 @@ class TestBackend(object):\n         for m_s, u_m_s, k in zip(state_list[4], state_list[5], BACKENDS):\n             assert_allclose(m_s, u_m_s, atol=1e-04)\n \n-    def legacy_test_rnn_no_states(self):\n+    def test_legacy_rnn_no_states(self):\n         # implement a simple RNN without states\n         input_dim = 8\n         output_dim = 4\n@@ -1137,7 +1137,7 @@ class TestBackend(object):\n             padding=padding, data_format=data_format, pool_mode=pool_mode,\n             cntk_dynamicity=True)\n \n-    def legacy_test_conv1d(self):\n+    def test_legacy_conv1d(self):\n         # channels_last input shape: (n, length, input_depth)\n         input_shape = (4, 8, 2)\n         kernel_shape = (3, 2, 3)\n@@ -1147,7 +1147,7 @@ class TestBackend(object):\n                                        strides=strides,\n                                        data_format='channels_last')\n \n-    def legacy_test_conv2d(self):\n+    def test_legacy_conv2d(self):\n         # TF kernel shape: (rows, cols, input_depth, depth)\n         # channels_first input shape: (n, input_depth, rows, cols)\n         for (input_shape, kernel_shape, data_format) in [\n@@ -1158,7 +1158,7 @@ class TestBackend(object):\n                                        BACKENDS, cntk_dynamicity=True,\n                                        data_format=data_format)\n \n-    def legacy_test_depthwise_conv_2d(self):\n+    def test_legacy_depthwise_conv_2d(self):\n         # TF kernel shape: (rows, cols, input_depth, depth_multiplier)\n         # channels_first input shape: (n, input_depth, rows, cols)\n         for (input_shape, kernel_shape, data_format) in [\n@@ -1170,7 +1170,7 @@ class TestBackend(object):\n                                        BACKENDS, cntk_dynamicity=True,\n                                        data_format=data_format)\n \n-    def legacy_test_conv3d(self):\n+    def test_legacy_conv3d(self):\n         # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n         # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3, input_depth)\n         # TH kernel shape: (depth, input_depth, x, y, z)\n@@ -1210,7 +1210,7 @@ class TestBackend(object):\n                 padding=padding, data_format=data_format))\n         assert_allclose(y1, y2, atol=1e-05)\n \n-    def legacy_test_pool2d(self):\n+    def test_legacy_pool2d(self):\n         check_single_tensor_operation('pool2d', (5, 10, 12, 3),\n                                       BACKENDS, cntk_dynamicity=True,\n                                       pool_size=(2, 2), strides=(1, 1), padding='valid')\n@@ -1232,7 +1232,7 @@ class TestBackend(object):\n                                       pool_size=(3, 3), strides=(1, 1),\n                                       padding='same', pool_mode='avg')\n \n-    def legacy_test_pool3d(self):\n+    def test_legacy_pool3d(self):\n         check_single_tensor_operation('pool3d', (5, 10, 12, 5, 3),\n                                       BACKENDS, cntk_dynamicity=True,\n                                       pool_size=(2, 2, 2), strides=(1, 1, 1), padding='valid')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d5d4a6368e45cb9939acb04d089cb46adbca32a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 27 | Churn Cumulative: 5957 | Contributors (this commit): 31 | Commits (past 90d): 57 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -331,19 +331,20 @@ class TestBackend(object):\n             y = K.gather(x, indices)\n             assert y._keras_shape == (5, 6, 3, 4)\n \n-    def test_value_manipulation(self):\n+    @pytest.mark.parametrize('function_name',\n+                             ['get_value', 'count_params',\n+                              'int_shape', 'get_variable_shape'])\n+    def test_value_manipulation(self, function_name):\n         val = np.random.random((4, 2))\n-        for function_name in ['get_value', 'count_params',\n-                              'int_shape', 'get_variable_shape']:\n         v_list = [getattr(k, function_name)(k.variable(val))\n-                      for k in BACKENDS]\n+                  for k in WITH_NP]\n \n         if function_name == 'get_value':\n             assert_list_pairwise(v_list)\n         else:\n             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n \n-        # print_tensor\n+    def test_print_tensor(self):\n         check_single_tensor_operation('print_tensor', (), WITH_NP)\n         check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n         check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n\n@@ -455,6 +455,22 @@ def eval(x):\n     return x\n \n \n+def get_value(x):\n+    return x\n+\n+\n+def count_params(x):\n+    return x.size\n+\n+\n+def int_shape(x):\n+    return x.shape\n+\n+\n+def get_variable_shape(x):\n+    return int_shape(x)\n+\n+\n def dtype(x):\n     return x.dtype.name\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e15533e6c725dca8c37a861aacb13ef149789433", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 27 | Churn Cumulative: 5984 | Contributors (this commit): 31 | Commits (past 90d): 59 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1427,9 +1427,9 @@ class TestBackend(object):\n \n     def test_temporal_padding(self):\n         check_single_tensor_operation('temporal_padding', (4, 3, 3),\n-                                      BACKENDS)\n+                                      WITH_NP)\n         check_single_tensor_operation('temporal_padding', (2, 3, 4),\n-                                      BACKENDS, padding=(1, 2))\n+                                      WITH_NP, padding=(1, 2))\n \n     def test_spatial_2d_padding(self):\n         padding = ((1, 2), (2, 1))\n@@ -1439,7 +1439,7 @@ class TestBackend(object):\n                 x_shape = (1, 3) + shape\n             else:\n                 x_shape = (1,) + shape + (3,)\n-            check_single_tensor_operation('spatial_2d_padding', x_shape, BACKENDS,\n+            check_single_tensor_operation('spatial_2d_padding', x_shape, WITH_NP,\n                                           padding=padding, data_format=data_format)\n         # Check handling of dynamic shapes.\n         if K in [KTF, KTH]:\n@@ -1461,7 +1461,7 @@ class TestBackend(object):\n                 x_shape = (1, 3) + shape\n             else:\n                 x_shape = (1,) + shape + (3,)\n-            check_single_tensor_operation('spatial_3d_padding', x_shape, BACKENDS,\n+            check_single_tensor_operation('spatial_3d_padding', x_shape, WITH_NP,\n                                           padding=padding, data_format=data_format)\n         # Check handling of dynamic shapes.\n         if K in [KTF, KTH]:\n\n@@ -7,6 +7,7 @@ import numpy as np\n import scipy.signal as signal\n import scipy as sp\n from keras.backend import floatx\n+from keras.utils.generic_utils import transpose_shape\n \n \n def normalize_conv(func):\n@@ -431,6 +432,24 @@ def repeat(x, n):\n     return y\n \n \n+def temporal_padding(x, padding=(1, 1)):\n+    return np.pad(x, [(0, 0), padding, (0, 0)], mode='constant')\n+\n+\n+def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n+    all_dims_padding = ((0, 0),) + padding + ((0, 0),)\n+    all_dims_padding = transpose_shape(all_dims_padding, data_format,\n+                                       spatial_axes=(1, 2))\n+    return np.pad(x, all_dims_padding, mode='constant')\n+\n+\n+def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n+    all_dims_padding = ((0, 0),) + padding + ((0, 0),)\n+    all_dims_padding = transpose_shape(all_dims_padding, data_format,\n+                                       spatial_axes=(1, 2, 3))\n+    return np.pad(x, all_dims_padding, mode='constant')\n+\n+\n def tile(x, n):\n     return np.tile(x, n)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc6f00fa0d831e8583a5285f032cf583d4980c4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 228 | Lines Deleted: 109 | Files Changed: 1 | Hunks: 65 | Methods Changed: 34 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 337 | Churn Cumulative: 5573 | Contributors (this commit): 32 | Commits (past 90d): 39 | Contributors (cumulative): 32 | DMM Complexity: 0.90625\n\nDIFF:\n@@ -33,8 +33,12 @@ except ImportError:\n     KTH = None\n     warnings.warn('Could not import the Theano backend')\n \n-\n-WITH_NP = [KTH if K.backend() == 'theano' else KC if K.backend() == 'cntk' else KTF, KNP]\n+if K.backend() == 'theano':\n+    WITH_NP = [KTH, KNP]\n+elif K.backend() == 'cntk':\n+    WITH_NP = [KC, KNP]\n+else:\n+    WITH_NP = [KTF, KNP]\n \n \n def check_dtype(var, dtype):\n@@ -67,7 +71,11 @@ def parse_shape_or_val(shape_or_val):\n         return shape_or_val, np.random.random(shape_or_val).astype(np.float32) - 0.5\n \n \n-def assert_list_pairwise(z_list, shape=True, allclose=True, itself=False, atol=1e-05):\n+def assert_list_pairwise(z_list,\n+                         shape=True,\n+                         allclose=True,\n+                         itself=False,\n+                         atol=1e-05):\n     for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n         if shape:\n             assert z1.shape == z2.shape\n@@ -85,7 +93,10 @@ def assert_list_keras_shape(t_list, z_list):\n                     assert t._keras_shape[i] == z.shape[i]\n \n \n-def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, **kwargs):\n+def check_single_tensor_operation(function_name,\n+                                  x_shape_or_val,\n+                                  backend_list,\n+                                  **kwargs):\n     shape_or_val = kwargs.pop('shape_or_val', True)\n     assert_value_equality = kwargs.pop('assert_value_equality', True)\n     cntk_dynamicity = kwargs.pop('cntk_dynamicity', False)\n@@ -113,8 +124,11 @@ def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, *\n     assert_list_keras_shape(t_list, z_list)\n \n \n-def check_two_tensor_operation(function_name, x_shape_or_val,\n-                               y_shape_or_val, backend_list, **kwargs):\n+def check_two_tensor_operation(function_name,\n+                               x_shape_or_val,\n+                               y_shape_or_val,\n+                               backend_list,\n+                               **kwargs):\n     concat_args = kwargs.pop('concat_args', False)\n     cntk_dynamicity = kwargs.pop('cntk_dynamicity', False)\n     cntk_two_dynamicity = kwargs.pop('cntk_two_dynamicity', False)\n@@ -150,9 +164,12 @@ def check_two_tensor_operation(function_name, x_shape_or_val,\n     assert_list_keras_shape(t_list, z_list)\n \n \n-def check_composed_tensor_operations(first_function_name, first_function_args,\n-                                     second_function_name, second_function_args,\n-                                     input_shape, backend_list):\n+def check_composed_tensor_operations(first_function_name,\n+                                     first_function_args,\n+                                     second_function_name,\n+                                     second_function_args,\n+                                     input_shape,\n+                                     backend_list):\n     val = np.random.random(input_shape) - 0.5\n \n     z_list = []\n@@ -186,16 +203,20 @@ class TestBackend(object):\n         check_single_tensor_operation('eye', 3, WITH_NP, shape_or_val=False)\n \n     def test_ones(self):\n-        check_single_tensor_operation('ones', (3, 5, 10, 8), WITH_NP, shape_or_val=False)\n+        check_single_tensor_operation('ones', (3, 5, 10, 8),\n+                                      WITH_NP, shape_or_val=False)\n \n     def test_zeros(self):\n-        check_single_tensor_operation('zeros', (3, 5, 10, 8), WITH_NP, shape_or_val=False)\n+        check_single_tensor_operation('zeros', (3, 5, 10, 8),\n+                                      WITH_NP, shape_or_val=False)\n \n     def test_ones_like(self):\n-        check_single_tensor_operation('ones_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)\n+        check_single_tensor_operation('ones_like', (3, 5, 10, 8),\n+                                      WITH_NP, shape_or_val=True)\n \n     def test_zeros_like(self):\n-        check_single_tensor_operation('zeros_like', (3, 5, 10, 8), WITH_NP, shape_or_val=True)\n+        check_single_tensor_operation('zeros_like', (3, 5, 10, 8),\n+                                      WITH_NP, shape_or_val=True)\n \n     def test_linear_operations(self):\n         check_two_tensor_operation('dot', (4, 2), (2, 4), WITH_NP)\n@@ -220,10 +241,12 @@ class TestBackend(object):\n     def test_random_variables(self):\n         check_single_tensor_operation('random_uniform_variable', (2, 3), WITH_NP,\n                                       low=0., high=1.,\n-                                      shape_or_val=False, assert_value_equality=False)\n+                                      shape_or_val=False,\n+                                      assert_value_equality=False)\n         check_single_tensor_operation('random_normal_variable', (2, 3), WITH_NP,\n                                       mean=0., scale=1.,\n-                                      shape_or_val=False, assert_value_equality=False)\n+                                      shape_or_val=False,\n+                                      assert_value_equality=False)\n \n     @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Not supported.')\n     def test_batch_dot_shape(self):\n@@ -361,7 +384,8 @@ class TestBackend(object):\n \n         check_single_tensor_operation('mean', (4, 2), WITH_NP)\n         check_single_tensor_operation('mean', (4, 2), WITH_NP, axis=1, keepdims=True)\n-        check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=-1, keepdims=True)\n+        check_single_tensor_operation('mean', (4, 2, 3),\n+                                      WITH_NP, axis=-1, keepdims=True)\n         check_single_tensor_operation('mean', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n         check_single_tensor_operation('var', (4, 2), WITH_NP)\n@@ -374,7 +398,8 @@ class TestBackend(object):\n         # check_single_tensor_operation('std', (4, 2, 3), BACKENDS, axis=[1, -1])\n \n         check_single_tensor_operation('logsumexp', (4, 2), WITH_NP)\n-        check_single_tensor_operation('logsumexp', (4, 2), WITH_NP, axis=1, keepdims=True)\n+        check_single_tensor_operation('logsumexp', (4, 2),\n+                                      WITH_NP, axis=1, keepdims=True)\n         check_single_tensor_operation('logsumexp', (4, 2, 3), WITH_NP, axis=[1, -1])\n \n         check_single_tensor_operation('prod', (4, 2), WITH_NP)\n@@ -612,11 +637,14 @@ class TestBackend(object):\n \n         kwargs_list = [\n             {'go_backwards': False, 'mask': None},\n-            {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': False, 'mask': None, 'unroll': True,\n+             'input_length': timesteps},\n             {'go_backwards': True, 'mask': None},\n-            {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': True, 'mask': None, 'unroll': True,\n+             'input_length': timesteps},\n             {'go_backwards': False, 'mask': mask_k},\n-            {'go_backwards': False, 'mask': mask_k, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': False, 'mask': mask_k, 'unroll': True,\n+             'input_length': timesteps},\n         ]\n \n         for (i, kwargs) in enumerate(kwargs_list):\n@@ -646,7 +674,8 @@ class TestBackend(object):\n                 assert_allclose(y1, y2, atol=1e-05)\n                 assert_allclose(h1, h2, atol=1e-05)\n             else:\n-                assert_allclose(last_output_list[i - 1], last_output_list[i], atol=1e-05)\n+                assert_allclose(last_output_list[i - 1], last_output_list[i],\n+                                atol=1e-05)\n                 assert_allclose(outputs_list[i - 1], outputs_list[i], atol=1e-05)\n                 assert_allclose(state_list[i - 1], state_list[i], atol=1e-05)\n \n@@ -682,11 +711,14 @@ class TestBackend(object):\n \n         kwargs_list = [\n             {'go_backwards': False, 'mask': None},\n-            {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': False, 'mask': None, 'unroll': True,\n+             'input_length': timesteps},\n             {'go_backwards': True, 'mask': None},\n-            {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': True, 'mask': None, 'unroll': True,\n+             'input_length': timesteps},\n             {'go_backwards': False, 'mask': mask_k},\n-            {'go_backwards': False, 'mask': mask_k, 'unroll': True, 'input_length': timesteps},\n+            {'go_backwards': False, 'mask': mask_k, 'unroll': True,\n+             'input_length': timesteps},\n         ]\n \n         for (i, kwargs) in enumerate(kwargs_list):\n@@ -721,7 +753,8 @@ class TestBackend(object):\n                 assert_allclose(h11, h21, atol=1e-05)\n                 assert_allclose(h12, h22, atol=1e-05)\n             else:\n-                assert_allclose(last_output_list[i - 1], last_output_list[i], atol=1e-05)\n+                assert_allclose(last_output_list[i - 1], last_output_list[i],\n+                                atol=1e-05)\n                 assert_allclose(outputs_list[i - 1], outputs_list[i], atol=1e-05)\n                 assert_allclose(state_list[i - 1][0], state_list[i][0], atol=1e-05)\n                 assert_allclose(state_list[i - 1][1], state_list[i][1], atol=1e-05)\n@@ -762,8 +795,10 @@ class TestBackend(object):\n         output_dim = 3\n         timesteps = 6\n \n-        input_val = np.random.random((num_samples, timesteps, input_dim)).astype(np.float32)\n-        init_state_val = np.random.random((num_samples, output_dim)).astype(np.float32)\n+        input_val = np.random.random(\n+            (num_samples, timesteps, input_dim)).astype(np.float32)\n+        init_state_val = np.random.random(\n+            (num_samples, output_dim)).astype(np.float32)\n         W_i_val = np.random.random((input_dim, output_dim)).astype(np.float32)\n         W_o_val = np.random.random((output_dim, output_dim)).astype(np.float32)\n         np_mask = np.random.randint(2, size=(num_samples, timesteps))\n@@ -793,11 +828,14 @@ class TestBackend(object):\n \n             kwargs_list = [\n                 {'go_backwards': False, 'mask': None},\n-                {'go_backwards': False, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+                {'go_backwards': False, 'mask': None, 'unroll': True,\n+                 'input_length': timesteps},\n                 {'go_backwards': True, 'mask': None},\n-                {'go_backwards': True, 'mask': None, 'unroll': True, 'input_length': timesteps},\n+                {'go_backwards': True, 'mask': None, 'unroll': True,\n+                 'input_length': timesteps},\n                 {'go_backwards': False, 'mask': mask},\n-                {'go_backwards': False, 'mask': mask, 'unroll': True, 'input_length': timesteps},\n+                {'go_backwards': False, 'mask': mask, 'unroll': True,\n+                 'input_length': timesteps},\n             ]\n \n             for (i, kwargs) in enumerate(kwargs_list):\n@@ -995,21 +1033,28 @@ class TestBackend(object):\n         check_single_tensor_operation('softmax', (4, 5, 3), WITH_NP, axis=1)\n         check_single_tensor_operation('softmax', (4, 5, 3, 10), WITH_NP, axis=2)\n \n-        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n+        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2),\n+                                   WITH_NP, from_logits=True)\n         # cross_entropy call require the label is a valid probability distribution,\n         # otherwise it is garbage in garbage out...\n-        # due to the algo difference, we can't guarantee CNTK has the same result on the garbage input.\n+        # due to the algo difference, we can't guarantee CNTK has the same result\n+        # on the garbage input.\n         # so create a separate test case for valid label input\n         if K.backend() != 'cntk':\n-            check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=True)\n+            check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2),\n+                                       WITH_NP, from_logits=True)\n         xval = np.asarray([[0.26157712, 0.0432167], [-0.43380741, 0.30559841],\n-                           [0.20225059, -0.38956559], [-0.13805378, 0.08506755]], dtype=np.float32)\n+                           [0.20225059, -0.38956559], [-0.13805378, 0.08506755]],\n+                          dtype=np.float32)\n         yval = np.asarray([[0.46221867, 0.53778133], [0.51228984, 0.48771016],\n-                           [0.64916514, 0.35083486], [0.47028078, 0.52971922]], dtype=np.float32)\n-        check_two_tensor_operation('categorical_crossentropy', yval, xval,\n-                                   WITH_NP, cntk_two_dynamicity=True, from_logits=True)\n-        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n-        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), WITH_NP, from_logits=False)\n+                           [0.64916514, 0.35083486], [0.47028078, 0.52971922]],\n+                          dtype=np.float32)\n+        check_two_tensor_operation('categorical_crossentropy', yval, xval, WITH_NP,\n+                                   cntk_two_dynamicity=True, from_logits=True)\n+        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2),\n+                                   WITH_NP, from_logits=False)\n+        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2),\n+                                   WITH_NP, from_logits=False)\n \n         check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=-1)\n         check_single_tensor_operation('l2_normalize', (4, 3), WITH_NP, axis=1)\n@@ -1022,7 +1067,8 @@ class TestBackend(object):\n         predictions = np.random.random((batch_size, num_classes)).astype('float32')\n         targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n \n-        # (k == 0 or k > num_classes) does not raise an error but just return an unmeaningful tensor.\n+        # (k == 0 or k > num_classes) does not raise an error\n+        # but just return an unmeaningful tensor.\n         for k in range(num_classes + 1):\n             z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n                                         b.variable(targets, dtype='int32'), k))\n@@ -1033,7 +1079,8 @@ class TestBackend(object):\n         # randomly set half of the predictions to an identical value\n         num_identical = num_classes // 2\n         for i in range(batch_size):\n-            idx_identical = np.random.choice(num_classes, size=num_identical, replace=False)\n+            idx_identical = np.random.choice(num_classes,\n+                                             size=num_identical, replace=False)\n             predictions[i, idx_identical] = predictions[i, 0]\n         targets = np.zeros(batch_size, dtype='int32')\n \n@@ -1069,8 +1116,13 @@ class TestBackend(object):\n             ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9),\n              'same', 'channels_first'),\n         ])\n-    def test_conv_transpose(self, op, input_shape, kernel_shape, output_shape,\n-                            padding, data_format):\n+    def test_conv_transpose(self,\n+                            op,\n+                            input_shape,\n+                            kernel_shape,\n+                            output_shape,\n+                            padding,\n+                            data_format):\n         check_two_tensor_operation(\n             op, input_shape, kernel_shape, WITH_NP,\n             output_shape=output_shape, padding=padding, data_format=data_format,\n@@ -1078,16 +1130,26 @@ class TestBackend(object):\n \n     @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                         reason='cntk only supports dilated conv on GPU')\n-    @pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [\n+    @pytest.mark.parametrize(\n+        'op,input_shape,kernel_shape,padding,data_format,dilation_rate', [\n             ('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2),\n             ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2),\n-        ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)),\n-        ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)),\n-        ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)),\n-        ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2)),\n+            ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2),\n+             'same', 'channels_last', (2, 2)),\n+            ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4),\n+             'valid', 'channels_first', (2, 2)),\n+            ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4),\n+             'valid', 'channels_last', (2, 2, 2)),\n+            ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4),\n+             'same', 'channels_first', (2, 2, 2)),\n         ])\n-    def test_dilated_conv(self, op, input_shape, kernel_shape, padding,\n-                          data_format, dilation_rate):\n+    def test_dilated_conv(self,\n+                          op,\n+                          input_shape,\n+                          kernel_shape,\n+                          padding,\n+                          data_format,\n+                          dilation_rate):\n         check_two_tensor_operation(\n             op, input_shape, kernel_shape, WITH_NP,\n             padding=padding, data_format=data_format,\n@@ -1096,14 +1158,21 @@ class TestBackend(object):\n     @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                         reason='cntk only supports dilated conv transpose on GPU')\n     @pytest.mark.parametrize(\n-        'op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [\n+        'op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate',\n+        [\n             ('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2),\n              'same', 'channels_last', (2, 2)),\n             ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9),\n              'same', 'channels_first', (2, 2)),\n         ])\n-    def test_dilated_conv_transpose(self, op, input_shape, kernel_shape, output_shape,\n-                                    padding, data_format, dilation_rate):\n+    def test_dilated_conv_transpose(self,\n+                                    op,\n+                                    input_shape,\n+                                    kernel_shape,\n+                                    output_shape,\n+                                    padding,\n+                                    data_format,\n+                                    dilation_rate):\n         check_two_tensor_operation(\n             op, input_shape, kernel_shape, WITH_NP, output_shape=output_shape,\n             padding=padding, data_format=data_format, dilation_rate=dilation_rate,\n@@ -1115,23 +1184,44 @@ class TestBackend(object):\n         ('depthwise_conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'),\n         ('depthwise_conv2d', (1, 7, 6, 3), (3, 3, 3, 4), 'same', 'channels_last'),\n     ])\n-    def test_depthwise_conv(self, op, input_shape, kernel_shape, padding, data_format):\n+    def test_depthwise_conv(self,\n+                            op,\n+                            input_shape,\n+                            kernel_shape,\n+                            padding,\n+                            data_format):\n         check_two_tensor_operation(\n             op, input_shape, kernel_shape, WITH_NP,\n             padding=padding, data_format=data_format,\n             cntk_dynamicity=True)\n \n-    @pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [\n-        ('pool2d', (2, 3, 7, 7), (3, 3), (1, 1), 'same', 'channels_first', 'avg'),\n-        ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1), 'valid', 'channels_first', 'max'),\n-        ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1), 'valid', 'channels_last', 'avg'),\n-        ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1), 'same', 'channels_last', 'max'),\n-        ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1), 'same', 'channels_first', 'avg'),\n-        ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1), 'valid', 'channels_first', 'max'),\n-        ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1), 'valid', 'channels_last', 'avg'),\n-        ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1), 'same', 'channels_last', 'max'),\n+    @pytest.mark.parametrize(\n+        'op,input_shape,pool_size,strides,padding,data_format,pool_mode', [\n+            ('pool2d', (2, 3, 7, 7), (3, 3), (1, 1),\n+             'same', 'channels_first', 'avg'),\n+            ('pool2d', (3, 3, 8, 5), (2, 3), (1, 1),\n+             'valid', 'channels_first', 'max'),\n+            ('pool2d', (2, 9, 5, 3), (3, 2), (1, 1),\n+             'valid', 'channels_last', 'avg'),\n+            ('pool2d', (3, 6, 7, 3), (3, 3), (1, 1),\n+             'same', 'channels_last', 'max'),\n+            ('pool3d', (2, 3, 7, 7, 7), (3, 3, 3), (1, 1, 1),\n+             'same', 'channels_first', 'avg'),\n+            ('pool3d', (3, 3, 8, 5, 9), (2, 3, 2), (1, 1, 1),\n+             'valid', 'channels_first', 'max'),\n+            ('pool3d', (2, 8, 9, 5, 3), (3, 2, 3), (1, 1, 1),\n+             'valid', 'channels_last', 'avg'),\n+            ('pool3d', (3, 5, 6, 7, 3), (3, 3, 3), (1, 1, 1),\n+             'same', 'channels_last', 'max'),\n         ])\n-    def test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n+    def test_pool(self,\n+                  op,\n+                  input_shape,\n+                  pool_size,\n+                  strides,\n+                  padding,\n+                  data_format,\n+                  pool_mode):\n         check_single_tensor_operation(\n             op, input_shape, WITH_NP,\n             pool_size=pool_size, strides=strides,\n@@ -1154,7 +1244,8 @@ class TestBackend(object):\n         for (input_shape, kernel_shape, data_format) in [\n             ((2, 3, 4, 5), (2, 2, 3, 4), 'channels_first'),\n             ((2, 3, 5, 6), (4, 3, 3, 4), 'channels_first'),\n-                ((1, 6, 5, 3), (3, 3, 3, 2), 'channels_last')]:\n+            ((1, 6, 5, 3), (3, 3, 3, 2), 'channels_last')\n+        ]:\n             check_two_tensor_operation('conv2d', input_shape, kernel_shape,\n                                        BACKENDS, cntk_dynamicity=True,\n                                        data_format=data_format)\n@@ -1165,7 +1256,8 @@ class TestBackend(object):\n         for (input_shape, kernel_shape, data_format) in [\n             ((2, 3, 4, 5), (2, 2, 3, 4), 'channels_first'),\n             ((2, 3, 5, 6), (4, 3, 3, 4), 'channels_first'),\n-                ((1, 6, 5, 3), (3, 3, 3, 2), 'channels_last')]:\n+            ((1, 6, 5, 3), (3, 3, 3, 2), 'channels_last')\n+        ]:\n             check_two_tensor_operation('depthwise_conv2d',\n                                        input_shape, kernel_shape,\n                                        BACKENDS, cntk_dynamicity=True,\n@@ -1179,12 +1271,14 @@ class TestBackend(object):\n         for (input_shape, kernel_shape, data_format) in [\n             ((2, 3, 4, 5, 4), (2, 2, 2, 3, 4), 'channels_first'),\n             ((2, 3, 5, 4, 6), (3, 2, 4, 3, 4), 'channels_first'),\n-                ((1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'channels_last')]:\n+            ((1, 2, 2, 2, 1), (2, 2, 2, 1, 1), 'channels_last')\n+        ]:\n             check_two_tensor_operation('conv3d', input_shape, kernel_shape,\n                                        BACKENDS, cntk_dynamicity=True,\n                                        data_format=data_format)\n \n-    @pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [\n+    @pytest.mark.parametrize(\n+        'op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [\n             ('separable_conv1d', (2, 8, 2), (3,), 1, 'same', 'channels_last'),\n             ('separable_conv1d', (1, 8, 2), (3,), 2, 'valid', 'channels_last'),\n             ('separable_conv2d', (2, 3, 4, 5), (3, 3), 1, 'same', 'channels_first'),\n@@ -1192,11 +1286,22 @@ class TestBackend(object):\n             ('separable_conv2d', (1, 6, 5, 3), (3, 4), 1, 'valid', 'channels_last'),\n             ('separable_conv2d', (1, 7, 6, 3), (3, 3), 2, 'same', 'channels_last'),\n         ])\n-    def test_separable_conv(self, op, input_shape, kernel_shape, depth_multiplier, padding, data_format):\n-        input_depth = input_shape[1] if data_format == 'channels_first' else input_shape[-1]\n+    def test_separable_conv(self,\n+                            op,\n+                            input_shape,\n+                            kernel_shape,\n+                            depth_multiplier,\n+                            padding,\n+                            data_format):\n+        if data_format == 'channels_first':\n+            input_depth = input_shape[1]\n+        else:\n+            input_depth = input_shape[-1]\n         _, x = parse_shape_or_val(input_shape)\n-        _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n-        _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n+        _, depthwise = parse_shape_or_val(kernel_shape +\n+                                          (input_depth, depth_multiplier))\n+        _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) +\n+                                          (input_depth * depth_multiplier, 7))\n         y1 = KNP.separable_conv(x, depthwise, pointwise,\n                                 padding=padding, data_format=data_format)\n         if K.backend() == 'cntk':\n@@ -1211,57 +1316,60 @@ class TestBackend(object):\n                 padding=padding, data_format=data_format))\n         assert_allclose(y1, y2, atol=1e-05)\n \n-    def test_legacy_pool2d(self):\n-        check_single_tensor_operation('pool2d', (5, 10, 12, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+    def legacy_test_pool2d(self):\n+        check_single_tensor_operation(\n+            'pool2d', (5, 10, 12, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 2), strides=(1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool2d', (5, 9, 11, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool2d', (5, 9, 11, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 2), strides=(1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool2d', (5, 9, 11, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool2d', (5, 9, 11, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 2), strides=(1, 1), pool_mode='avg')\n \n-        check_single_tensor_operation('pool2d', (5, 9, 11, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool2d', (5, 9, 11, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 3), strides=(1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool2d', (2, 7, 7, 5),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool2d', (2, 7, 7, 5), BACKENDS, cntk_dynamicity=True,\n             pool_size=(3, 3), strides=(1, 1),\n             padding='same', pool_mode='avg')\n \n-    def test_legacy_pool3d(self):\n-        check_single_tensor_operation('pool3d', (5, 10, 12, 5, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+    def legacy_test_pool3d(self):\n+        check_single_tensor_operation(\n+            'pool3d', (5, 10, 12, 5, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 2, 2), strides=(1, 1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool3d', (5, 9, 11, 5, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 2, 2), strides=(1, 1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool3d', (5, 9, 11, 5, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 2, 2), strides=(1, 1, 1), pool_mode='avg')\n \n-        check_single_tensor_operation('pool3d', (5, 9, 11, 5, 3),\n-                                      BACKENDS, cntk_dynamicity=True,\n+        check_single_tensor_operation(\n+            'pool3d', (5, 9, 11, 5, 3), BACKENDS, cntk_dynamicity=True,\n             pool_size=(2, 3, 2), strides=(1, 1, 1), padding='valid')\n \n-        check_single_tensor_operation('pool3d', (2, 6, 6, 6, 3), [KTH, KTF], pool_size=(3, 3, 3),\n-                                      strides=(1, 1, 1), padding='same', pool_mode='avg')\n+        check_single_tensor_operation(\n+            'pool3d', (2, 6, 6, 6, 3), [KTH, KTF],\n+            pool_size=(3, 3, 3), strides=(1, 1, 1), padding='same', pool_mode='avg')\n \n     def test_random_normal(self):\n         # test standard normal as well as a normal with a different set of parameters\n         for mean, std in [(0., 1.), (-10., 5.)]:\n-            rand = K.eval(K.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n+            rand = K.eval(K.random_normal((300, 200),\n+                                          mean=mean, stddev=std, seed=1337))\n             assert rand.shape == (300, 200)\n             assert np.abs(np.mean(rand) - mean) < std * 0.015\n             assert np.abs(np.std(rand) - std) < std * 0.015\n \n-            # test that random_normal also generates different values when used within a function\n+            # test that random_normal also generates different values when used\n+            # within a function\n             r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n             samples = np.array([K.eval(r) for _ in range(200)])\n             assert np.abs(np.mean(samples) - mean) < std * 0.015\n@@ -1301,7 +1409,8 @@ class TestBackend(object):\n         std = 1.\n         min_val = -2.\n         max_val = 2.\n-        rand = K.eval(K.truncated_normal((300, 200), mean=mean, stddev=std, seed=1337))\n+        rand = K.eval(K.truncated_normal((300, 200),\n+                                         mean=mean, stddev=std, seed=1337))\n         assert rand.shape == (300, 200)\n         assert np.abs(np.mean(rand) - mean) < 0.015\n         assert np.max(rand) <= max_val\n@@ -1352,7 +1461,8 @@ class TestBackend(object):\n                                    strides=(2, 2), dilation_rate=(1, 2))\n \n     def test_pooling_invalid_use(self):\n-        for (input_shape, pool_size) in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n+        for (input_shape, pool_size) in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)],\n+                                            [(2, 2), (2, 2, 2)]):\n             x = K.variable(np.random.random(input_shape))\n             if len(pool_size) == 2:\n                 with pytest.raises(ValueError):\n@@ -1512,12 +1622,12 @@ class TestBackend(object):\n             xth = KTH.variable(x_val)\n             xtf = KTF.variable(x_val)\n             xc = KC.placeholder(x_shape)\n-            zth, _, _ = KTH.normalize_batch_in_training(xth, None, None,\n-                                                        reduction_axes='per-activation')\n-            ztf, _, _ = KTF.normalize_batch_in_training(xtf, None, None,\n-                                                        reduction_axes=[0, 1, 2, 3])\n-            zc, _, _ = KC.normalize_batch_in_training(xc, None, None,\n-                                                      reduction_axes=[0, 1, 2, 3])\n+            zth, _, _ = KTH.normalize_batch_in_training(\n+                xth, None, None, reduction_axes='per-activation')\n+            ztf, _, _ = KTF.normalize_batch_in_training(\n+                xtf, None, None, reduction_axes=[0, 1, 2, 3])\n+            zc, _, _ = KC.normalize_batch_in_training(\n+                xc, None, None, reduction_axes=[0, 1, 2, 3])\n             zth = KTH.eval(zth)\n             ztf = KTF.eval(ztf)\n             zc = KC.function([xc], [zc])([x_val])[0]\n@@ -1558,8 +1668,12 @@ class TestBackend(object):\n         k_inputs = K.variable(inputs, dtype=\"float32\")\n         k_input_lens = K.variable(input_lens, dtype=\"int32\")\n         k_label_lens = K.variable(label_lens, dtype=\"int32\")\n-        res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n-        assert_allclose(res[0, :] if K.backend() == 'theano' else res[:, 0], ref, atol=1e-05)\n+        res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens,\n+                                      k_label_lens))\n+        if K.backend() == 'theano':\n+            assert_allclose(res[0, :], ref, atol=1e-05)\n+        else:\n+            assert_allclose(res[:, 0], ref, atol=1e-05)\n \n         # test when batch_size = 1, that is, one sample only\n         # get only first sample from above test case\n@@ -1584,8 +1698,12 @@ class TestBackend(object):\n         k_inputs = K.variable(inputs, dtype=\"float32\")\n         k_input_lens = K.variable(input_lens, dtype=\"int32\")\n         k_label_lens = K.variable(label_lens, dtype=\"int32\")\n-        res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens, k_label_lens))\n-        assert_allclose(res[0, :] if K.backend() == 'theano' else res[:, 0], ref, atol=1e-05)\n+        res = K.eval(K.ctc_batch_cost(k_labels, k_inputs, k_input_lens,\n+                                      k_label_lens))\n+        if K.backend() == 'theano':\n+            assert_allclose(res[0, :], ref, atol=1e-05)\n+        else:\n+            assert_allclose(res[:, 0], ref, atol=1e-05)\n \n     @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Test adapted from tensorflow.')\n@@ -1762,7 +1880,8 @@ class TestBackend(object):\n \n             k_s_d = k.eval(k_s)\n \n-            k_d = k.eval(k.concatenate([k.variable(x_dense_1), k.variable(x_dense_2)]))\n+            k_d = k.eval(k.concatenate([k.variable(x_dense_1),\n+                                        k.variable(x_dense_2)]))\n \n             assert k_s_d.shape == k_d.shape\n             assert_allclose(k_s_d, k_d, atol=1e-05)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
