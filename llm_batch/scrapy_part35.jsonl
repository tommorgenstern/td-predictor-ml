{"custom_id": "scrapy#7e640da4332180477534fb242a53a5fedabb133d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 19 | Files Changed: 6 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 30 | Churn Cumulative: 1440 | Contributors (this commit): 3 | Commits (past 90d): 38 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,7 +23,7 @@ class DownloaderStats(object):\n         stats.incpath('_global/downloader/request_count')\n         stats.incpath('%s/downloader/request_count' % spider.domain_name)\n         stats.incpath('%s/downloader/request_method_count/%s' % (spider.domain_name, request.method))\n-        reqlen = len(request)\n+        reqlen = len(request.httprepr())\n         stats.incpath('%s/downloader/request_bytes' % spider.domain_name, reqlen)\n         stats.incpath('_global/downloader/request_bytes', reqlen)\n \n@@ -43,6 +43,6 @@ class DownloaderStats(object):\n         stats.incpath('_global/downloader/response_count')\n         stats.incpath('%s/downloader/response_count' % domain)\n         stats.incpath('%s/downloader/response_status_count/%s' % (domain, response.status))\n-        reslen = len(response)\n+        reslen = len(response.httprepr())\n         stats.incpath('%s/downloader/response_bytes' % domain, reslen)\n         stats.incpath('_global/downloader/response_bytes', reslen)\n\n@@ -86,7 +86,7 @@ class ItemSamplerMiddleware(object):\n     def process_spider_input(self, response, spider):\n         if stats.getpath(\"%s/items_sampled\" % spider.domain_name) >= items_per_domain:\n             return []\n-        elif max_response_size and max_response_size > len(response):  \n+        elif max_response_size and max_response_size > len(response.httprepr()):  \n             return []\n \n     def process_spider_output(self, response, result, spider):\n\n@@ -68,10 +68,6 @@ class Request(object):\n         else:\n             return \"<%s %s>\" % (self.method, self.url)\n \n-    def __len__(self):\n-        \"\"\"Return raw HTTP request size\"\"\"\n-        return len(self.to_string())\n-\n     def __repr__(self):\n         d = {\n             'method': self.method,\n@@ -95,7 +91,7 @@ class Request(object):\n         new.context = self.context # requests shares same context dictionary\n         return new\n \n-    def to_string(self):\n+    def httprepr(self):\n         \"\"\" Return raw HTTP request representation (as string). This is\n         provided only for reference since it's not the actual stream of bytes\n         that will be send when performing the request (that's controlled by\n\n@@ -52,10 +52,6 @@ class Response(object):\n         else:\n             return \"<%d %s>\" % (self.status, self.url)\n \n-    def __len__(self):\n-        \"\"\"Return raw HTTP response size\"\"\"\n-        return len(self.to_string())\n-\n     def copy(self):\n         \"\"\"Create a new Response based on the current one\"\"\"\n         return self.replace()\n@@ -78,7 +74,7 @@ class Response(object):\n         new.meta = self.meta.copy()\n         return new\n \n-    def to_string(self):\n+    def httprepr(self):\n         \"\"\"\n         Return raw HTTP response representation (as string). This is provided\n         only for reference, since it's not the exact stream of bytes that was\n\n@@ -105,12 +105,12 @@ class RequestTest(unittest.TestCase):\n \n         assert type(r2) is CustomRequest\n \n-    def test_to_string(self):\n+    def test_httprepr(self):\n         r1 = Request(\"http://www.example.com\")\n-        self.assertEqual(r1.to_string(), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n+        self.assertEqual(r1.httprepr(), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n \n         r1 = Request(\"http://www.example.com\", method='POST', headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(r1.to_string(), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n+        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -41,12 +41,12 @@ class ResponseTest(unittest.TestCase):\n \n         assert type(r2) is CustomResponse\n \n-    def test_to_string(self):\n+    def test_httprepr(self):\n         r1 = Response('example.com', \"http://www.example.com\")\n-        self.assertEqual(r1.to_string(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n+        self.assertEqual(r1.httprepr(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n \n         r1 = Response('example.com', \"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n-        self.assertEqual(r1.to_string(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n+        self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n         \n class ResponseBodyTest(unittest.TestCase):\n     unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ecc6808e01ffd7338bd22a9c2999b972bbd4d56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 481 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -31,10 +31,10 @@ class HistoryMiddleware(object):\n             d = datetime.now() - last_checked\n             if d.days < self.MIN_CHECK_DAYS:\n                 raise IgnoreRequest(\"Not scraping %s (scraped %s ago)\" % (request.url, d))\n-            request.context['history_response_version'] = version\n+            request.meta['history_response_version'] = version\n \n     def process_response(self, request, response, spider):\n-        version = request.context.get('history_response_version')\n+        version = request.meta.get('history_response_version')\n         if version == self.get_version(response):\n             del request.content['history_response_version']\n             hist = self.historydata.version_info(domain, version)\n\n@@ -17,7 +17,7 @@ from scrapy.utils.defer import chain_deferred\n \n class Request(object):\n \n-    def __init__(self, url, callback=None, context=None, method='GET',\n+    def __init__(self, url, callback=None, method='GET',\n         body=None, headers=None, cookies=None,\n         url_encoding='utf-8', dont_filter=None, domain=None):\n \n@@ -40,8 +40,6 @@ class Request(object):\n         self.cookies = cookies or {}\n         # request headers\n         self.headers = Headers(headers or {}, encoding=url_encoding)\n-        # persistent context across requests\n-        self.context = context or {}\n         # dont_filter be filtered by scheduler\n         self.dont_filter = dont_filter\n         #allows to directly specify the spider for the request\n@@ -75,20 +73,18 @@ class Request(object):\n             'headers': self.headers,\n             'cookies': self.cookies,\n             'body': self.body,\n-            'context': self.context\n             }\n         return \"%s(%s)\" % (self.__class__.__name__, repr(d))\n \n     def copy(self):\n-        \"\"\"Clone request except `context` attribute\"\"\"\n+        \"\"\"Return a new request cloned from this one\"\"\"\n         new = copy.copy(self)\n         new.cache = {}\n         for att in self.__dict__:\n-            if att not in ['cache', 'context', 'url', 'deferred']:\n+            if att not in ['cache', 'url', 'deferred']:\n                 value = getattr(self, att)\n                 setattr(new, att, copy.copy(value))\n         new.deferred = defer.Deferred()\n-        new.context = self.context # requests shares same context dictionary\n         return new\n \n     def httprepr(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#654b49c86e79d571190e60ba28c8c551582bba28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 52 | Churn Cumulative: 1329 | Contributors (this commit): 2 | Commits (past 90d): 35 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ from scrapy.utils.defer import chain_deferred\n class Request(object):\n \n     def __init__(self, url, callback=None, method='GET',\n-        body=None, headers=None, cookies=None,\n+        body=None, headers=None, cookies=None, meta=None,\n         url_encoding='utf-8', dont_filter=None, domain=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n@@ -45,7 +45,7 @@ class Request(object):\n         #allows to directly specify the spider for the request\n         self.domain = domain\n \n-        self.meta = {}\n+        self.meta = {} if meta is None else dict(meta)\n         self.cache = {}\n         \n     def append_callback(self, callback, *args, **kwargs):\n@@ -73,6 +73,7 @@ class Request(object):\n             'headers': self.headers,\n             'cookies': self.cookies,\n             'body': self.body,\n+            'meta': self.meta,\n             }\n         return \"%s(%s)\" % (self.__class__.__name__, repr(d))\n \n\n@@ -19,7 +19,7 @@ class Response(object):\n \n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    def __init__(self, domain, url, status=200, headers=None, body=None):\n+    def __init__(self, domain, url, status=200, headers=None, body=None, meta=None):\n         self.domain = domain\n         self.url = Url(url)\n         self.headers = Headers(headers or {})\n@@ -32,7 +32,7 @@ class Response(object):\n             self.body = None\n         self.cached = False\n         self.request = None\n-        self.meta = {}\n+        self.meta = {} if meta is None else dict(meta)\n         self.cache = {}\n \n     def headers_encoding(self):\n\n@@ -1,9 +1,29 @@\n import unittest\n-from scrapy.http import Request, Headers\n+from scrapy.http import Request, Headers, Url\n from scrapy.core.scheduler import GroupFilter\n \n class RequestTest(unittest.TestCase):\n \n+    def test_init(self):\n+        r = Request(\"http://www.example.com\")\n+        assert isinstance(r.url, Url)\n+        self.assertEqual(r.url, \"http://www.example.com\")\n+        self.assertEqual(r.method, \"GET\")\n+\n+        assert isinstance(r.headers, Headers)\n+        self.assertEqual(r.headers, {})\n+        self.assertEqual(r.meta, {})\n+\n+        meta = {\"lala\": \"lolo\"}\n+        headers = {\"caca\": \"coco\"}\n+        body = \"a body\"\n+        r = Request(\"http://www.example.com\", meta=meta, headers=headers, body=\"a body\")\n+\n+        assert r.meta is not meta\n+        self.assertEqual(r.meta, meta)\n+        assert r.headers is not headers\n+        self.assertEqual(r.headers[\"caca\"], \"coco\")\n+\n     def test_groupfilter(self):\n         k1 = \"id1\"\n         k2 = \"id1\"\n\n@@ -1,5 +1,5 @@\n import unittest\n-from scrapy.http import Response\n+from scrapy.http import Response, Headers, Url\n from scrapy.http.response import _ResponseBody\n \n class ResponseTest(unittest.TestCase):\n@@ -15,6 +15,25 @@ class ResponseTest(unittest.TestCase):\n         # test presence of all optional parameters\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', headers={}, status=200, body=None), Response))\n \n+        r = Response(\"domain.com\", \"http://www.example.com\")\n+        assert isinstance(r.url, Url)\n+        self.assertEqual(r.url, \"http://www.example.com\")\n+        self.assertEqual(r.status, 200)\n+\n+        assert isinstance(r.headers, Headers)\n+        self.assertEqual(r.headers, {})\n+        self.assertEqual(r.meta, {})\n+\n+        meta = {\"lala\": \"lolo\"}\n+        headers = {\"caca\": \"coco\"}\n+        body = \"a body\"\n+        r = Response(\"example.com\", \"http://www.example.com\", meta=meta, headers=headers, body=\"a body\")\n+\n+        assert r.meta is not meta\n+        self.assertEqual(r.meta, meta)\n+        assert r.headers is not headers\n+        self.assertEqual(r.headers[\"caca\"], \"coco\")\n+\n     def test_copy(self):\n         \"\"\"Test Response copy\"\"\"\n         \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db91d2687156b564fac6fa2928074e2170f32aef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 55 | Files Changed: 17 | Hunks: 49 | Methods Changed: 41 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 108 | Churn Cumulative: 3368 | Contributors (this commit): 3 | Commits (past 90d): 91 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -160,7 +160,7 @@ class Cache(object):\n         headers = Headers(responseheaders)\n         status = metadata['status']\n \n-        response = CachedResponse(domain=domain, url=url, headers=headers, status=status, body=responsebody)\n+        response = CachedResponse(url=url, headers=headers, status=status, body=responsebody)\n         return response\n \n     def store(self, domain, key, request, response):\n\n@@ -49,7 +49,7 @@ def download_http(request, spider):\n         body = body or ''\n         status = int(factory.status)\n         headers = Headers(factory.response_headers)\n-        r = Response(domain=spider.domain_name, url=request.url, status=status, headers=headers, body=body)\n+        r = Response(url=request.url, status=status, headers=headers, body=body)\n         signals.send_catch_log(signal=signals.request_uploaded, sender='download_http', request=request, spider=spider)\n         signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)\n         return r\n@@ -81,5 +81,5 @@ def download_file(request, spider) :\n     \"\"\"Return a deferred for a file download.\"\"\"\n     filepath = request.url.split(\"file://\")[1]\n     with open(filepath) as f:\n-        response = Response(domain=spider.domain_name, url=request.url, body=f.read())\n+        response = Response(url=request.url, body=f.read())\n     return defer_succeed(response)\n\n@@ -7,7 +7,6 @@ See documentation in docs/ref/request-response.rst\n \n import re\n import copy\n-from types import NoneType\n \n from twisted.web.http import RESPONSES\n from BeautifulSoup import UnicodeDammit\n@@ -19,8 +18,7 @@ class Response(object):\n \n     _ENCODING_RE = re.compile(r'charset=([\\w-]+)', re.I)\n \n-    def __init__(self, domain, url, status=200, headers=None, body=None, meta=None):\n-        self.domain = domain\n+    def __init__(self, url, status=200, headers=None, body=None, meta=None):\n         self.url = Url(url)\n         self.headers = Headers(headers or {})\n         self.status = status\n@@ -43,8 +41,8 @@ class Response(object):\n                 return encoding.group(1)\n \n     def __repr__(self):\n-        return \"Response(domain=%s, url=%s, headers=%s, status=%s, body=%s)\" % \\\n-                (repr(self.domain), repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n+        return \"Response(url=%s, headers=%s, status=%s, body=%s)\" % \\\n+                (repr(self.url), repr(self.headers), repr(self.status), repr(self.body))\n \n     def __str__(self):\n         if self.status == 200:\n@@ -56,7 +54,7 @@ class Response(object):\n         \"\"\"Create a new Response based on the current one\"\"\"\n         return self.replace()\n \n-    def replace(self, domain=None, url=None, status=None, headers=None, body=None):\n+    def replace(self, url=None, status=None, headers=None, body=None):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n \n@@ -64,8 +62,7 @@ class Response(object):\n \n         >>> newresp = oldresp.replace(body=\"New body\")\n         \"\"\"\n-        new = self.__class__(domain=domain or self.domain,\n-                             url=url or self.url,\n+        new = self.__class__(url=url or self.url,\n                              status=status or self.status,\n                              headers=headers or copy.deepcopy(self.headers),\n                              body=body)\n\n@@ -42,7 +42,7 @@ class AdaptorsTestCase(unittest.TestCase):\n     def get_selector(self, domain, url, sample_filename, headers=None, selector=HtmlXPathSelector):\n         sample_filename = os.path.join(self.samplesdir, sample_filename)\n         body = file(sample_filename).read()\n-        response = Response(domain=domain, url=url, headers=Headers(headers), status=200, body=body)\n+        response = Response(url=url, headers=Headers(headers), status=200, body=body)\n         return selector(response)\n \n     def test_extract(self):\n@@ -124,7 +124,7 @@ class AdaptorsTestCase(unittest.TestCase):\n                            <a onclick=\"javascript: opensomething('dummy/my_html2.html');\">something2</a>\n                          </div>\n                        </body></html>\"\"\"\n-        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=test_data)\n+        sample_response = Response('http://foobar.com/dummy', body=test_data)\n         sample_xsel = XmlXPathSelector(sample_response)\n         sample_adaptor = adaptors.ExtractImages(response=sample_response)\n \n\n@@ -11,7 +11,7 @@ class ResponseSoupTest(unittest.TestCase):\n         ResponseSoup()\n \n     def test_response_soup(self):\n-        r1 = Response('example.com', 'http://www.example.com', body='')\n+        r1 = Response('http://www.example.com', body='')\n \n         soup1 = r1.getsoup()\n         soup2 = r1.getsoup()\n@@ -22,7 +22,7 @@ class ResponseSoupTest(unittest.TestCase):\n         assert soup1 is soup2\n \n     def test_response_soup_caching(self):\n-        r1 = Response('example.com', 'http://www.example.com', body='')\n+        r1 = Response('http://www.example.com', body='')\n         soup1 = r1.getsoup()\n         r2 = r1.copy()\n         soup2 = r1.getsoup()\n\n@@ -152,7 +152,6 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual(404, response.status)\n             if session.getpath(response.url) == '/redirect':\n                 self.assertEqual(302, response.status)\n-            self.assertEqual(response.domain, spider.domain_name)\n \n     def test_item_data(self):\n         \"\"\"\n\n@@ -5,6 +5,9 @@ from scrapy.core.scheduler import GroupFilter\n class RequestTest(unittest.TestCase):\n \n     def test_init(self):\n+        # Request requires url in the constructor\n+        self.assertRaises(Exception, Request)\n+\n         r = Request(\"http://www.example.com\")\n         assert isinstance(r.url, Url)\n         self.assertEqual(r.url, \"http://www.example.com\")\n\n@@ -5,17 +5,16 @@ from scrapy.http.response import _ResponseBody\n class ResponseTest(unittest.TestCase):\n \n     def test_init(self):\n-        # Response requires domain and url\n+        # Response requires url in the consturctor\n         self.assertRaises(Exception, Response)\n-        self.assertRaises(Exception, Response, 'example.com')\n-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/'), Response))\n+        self.assertTrue(isinstance(Response('http://example.com/'), Response))\n         # body can be str or None but not ResponseBody\n-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body=None), Response))\n-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body='body'), Response))\n+        self.assertTrue(isinstance(Response('http://example.com/', body=None), Response))\n+        self.assertTrue(isinstance(Response('http://example.com/', body='body'), Response))\n         # test presence of all optional parameters\n-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', headers={}, status=200, body=None), Response))\n+        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=None), Response))\n \n-        r = Response(\"domain.com\", \"http://www.example.com\")\n+        r = Response(\"http://www.example.com\")\n         assert isinstance(r.url, Url)\n         self.assertEqual(r.url, \"http://www.example.com\")\n         self.assertEqual(r.status, 200)\n@@ -27,7 +26,7 @@ class ResponseTest(unittest.TestCase):\n         meta = {\"lala\": \"lolo\"}\n         headers = {\"caca\": \"coco\"}\n         body = \"a body\"\n-        r = Response(\"example.com\", \"http://www.example.com\", meta=meta, headers=headers, body=\"a body\")\n+        r = Response(\"http://www.example.com\", meta=meta, headers=headers, body=\"a body\")\n \n         assert r.meta is not meta\n         self.assertEqual(r.meta, meta)\n@@ -37,7 +36,7 @@ class ResponseTest(unittest.TestCase):\n     def test_copy(self):\n         \"\"\"Test Response copy\"\"\"\n         \n-        r1 = Response('example.com', \"http://www.example.com\")\n+        r1 = Response(\"http://www.example.com\")\n         r1.meta['foo'] = 'bar'\n         r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n@@ -55,16 +54,16 @@ class ResponseTest(unittest.TestCase):\n         class CustomResponse(Response):\n             pass\n \n-        r1 = CustomResponse('example.com', 'http://www.example.com')\n+        r1 = CustomResponse('http://www.example.com')\n         r2 = r1.copy()\n \n         assert type(r2) is CustomResponse\n \n     def test_httprepr(self):\n-        r1 = Response('example.com', \"http://www.example.com\")\n+        r1 = Response(\"http://www.example.com\")\n         self.assertEqual(r1.httprepr(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n \n-        r1 = Response('example.com', \"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n+        r1 = Response(\"http://www.example.com\", status=404, headers={\"Content-type\": \"text/html\"}, body=\"Some body\")\n         self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\\r\\n')\n         \n class ResponseBodyTest(unittest.TestCase):\n\n@@ -35,7 +35,7 @@ class ResponseLibxml2DocTest(TestCase):\n         scrapymanager.configure()\n \n         self.body_content = 'test problematic \\x00 body'\n-        response = Response('example.com', 'http://example.com/catalog/product/blabla-123',\n+        response = Response('http://example.com/catalog/product/blabla-123',\n                             headers={'Content-Type': 'text/plain; charset=utf-8'}, body=self.body_content)\n         response.getlibxml2doc()\n \n\n@@ -15,7 +15,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         <p><a href=\"../othercat.html\">Other category</a></p>\n         <p><a href=\"/\" /></p>\n         </body></html>\"\"\"\n-        response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n+        response = Response(\"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n         self.assertEqual(lx.extract_links(response),\n@@ -28,7 +28,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n         html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n         <body><p><a href=\"item/12.html\">Item 12</a></p>\n         </body></html>\"\"\"\n-        response = Response(\"example.org\", \"http://example.org/somepage/index.html\", body=html)\n+        response = Response(\"http://example.org/somepage/index.html\", body=html)\n \n         lx = LinkExtractor()  # default: tag=a, attr=href\n         self.assertEqual(lx.extract_links(response),\n@@ -37,10 +37,10 @@ class LinkExtractorTestCase(unittest.TestCase):\n     def test_extraction_encoding(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'linkextractor_noenc.html'), 'r').read()\n-        response_utf8 = Response(url='http://example.com/utf8', domain='example.com', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n-        response_noenc = Response(url='http://example.com/noenc', domain='example.com', body=body)\n+        response_utf8 = Response(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n+        response_noenc = Response(url='http://example.com/noenc', body=body)\n         body = open(os.path.join(base_path, 'linkextractor_latin1.html'), 'r').read()\n-        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=body)\n+        response_latin1 = Response(url='http://example.com/latin1', body=body)\n \n         lx = LinkExtractor()\n         self.assertEqual(lx.extract_links(response_utf8),\n@@ -67,7 +67,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n         body = open(os.path.join(base_path, 'regex_linkextractor.html'), 'r').read()\n-        self.response = Response(url='http://example.com/index', domain='example.com', body=body)\n+        self.response = Response(url='http://example.com/index', body=body)\n \n     def test_urls_type(self):\n         '''Test that the resulting urls are regular strings and not a unicode objects'''\n@@ -146,7 +146,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n #    def setUp(self):\n #        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n #        body = open(os.path.join(base_path 'image_linkextractor.html'), 'r').read()\n-#        self.response = Response(url='http://example.com/index', domain='example.com', body=body)\n+#        self.response = Response(url='http://example.com/index', body=body)\n \n #    def test_urls_type(self):\n #        '''Test that the resulting urls are regular strings and not a unicode objects'''\n\n@@ -16,7 +16,7 @@ def setUp():\n         fd = open(os.path.join(datadir, 'feed-sample1.' + format), 'r')\n         body = fd.read()\n         fd.close()\n-        test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n+        test_responses[format] = Response('http://foo.com/bar', body=body)\n     return uncompressed_body, test_responses\n \n class ScrapyDecompressionTest(TestCase):\n\n@@ -12,8 +12,8 @@ class RetryTest(unittest.TestCase):\n         self.spider = spiders.fromdomain('scrapytest.org')\n \n     def test_process_exception(self):\n-        exception_404 = (Request('http://www.scrapytest.org/404'), HttpException('404', None, Response('scrapytest.org', 'http://www.scrapytest.org/404', body='')), self.spider)\n-        exception_503 = (Request('http://www.scrapytest.org/503'), HttpException('503', None, Response('scrapytest.org', 'http://www.scrapytest.org/503', body='')), self.spider)\n+        exception_404 = (Request('http://www.scrapytest.org/404'), HttpException('404', None, Response('http://www.scrapytest.org/404', body='')), self.spider)\n+        exception_503 = (Request('http://www.scrapytest.org/503'), HttpException('503', None, Response('http://www.scrapytest.org/503', body='')), self.spider)\n \n         mw = RetryMiddleware()\n         mw.retry_times = 1\n\n@@ -20,7 +20,7 @@ class UtilsXmlTestCase(unittest.TestCase):\n               </product>\\\n             </products>\"\"\"\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        response = Response(url=\"http://example.com\", body=body)\n         attrs = []\n         for x in xmliter(response, 'product'):\n             attrs.append((x.x(\"@id\").extract(), x.x(\"name/text()\").extract(), x.x(\"./type/text()\").extract()))\n@@ -53,7 +53,7 @@ class UtilsXmlTestCase(unittest.TestCase):\n                 </channel>\n             </rss>\n         \"\"\"\n-        response = Response(domain='mydummycompany.com', url='http://mydummycompany.com', body=body)\n+        response = Response(url='http://mydummycompany.com', body=body)\n         my_iter = xmliter(response, 'item')\n \n         node = my_iter.next()\n@@ -83,7 +83,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     def test_iterator_defaults(self):\n         body = open(self.sample_feed_path).read()\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        response = Response(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n         result = [row for row in csv]\n@@ -101,7 +101,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     def test_iterator_delimiter(self):\n         body = open(self.sample_feed_path).read().replace(',', '\\t')\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        response = Response(url=\"http://example.com/\", body=body)\n         csv = csviter(response, delimiter='\\t')\n \n         self.assertEqual([row for row in csv],\n@@ -114,7 +114,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         sample = open(self.sample_feed_path).read().splitlines()\n         headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        response = Response(url=\"http://example.com/\", body=body)\n         csv = csviter(response, headers=headers)\n \n         self.assertEqual([row for row in csv],\n@@ -127,7 +127,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         body = open(self.sample_feed_path).read()\n         body = '\\n'.join((body, 'a,b', 'a,b,c,d'))\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        response = Response(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n         self.assertEqual([row for row in csv],\n@@ -139,7 +139,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     def test_iterator_exception(self):\n         body = open(self.sample_feed_path).read()\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com/\", body=body)\n+        response = Response(url=\"http://example.com/\", body=body)\n         iter = csviter(response)\n         iter.next()\n         iter.next()\n\n@@ -3,7 +3,7 @@ from scrapy.http.response import Response\n from scrapy.utils.response import body_or_str\n \n class ResponseUtilsTest(unittest.TestCase):\n-    dummy_response = Response(domain='example.org', url='http://example.org/', body='dummy_response')\n+    dummy_response = Response(url='http://example.org/', body='dummy_response')\n \n     def test_input(self):\n         self.assertTrue(isinstance(body_or_str(self.dummy_response), basestring))\n\n@@ -19,7 +19,7 @@ class XPathTestCase(unittest.TestCase):\n     def test_selector_simple(self):\n         \"\"\"Simple selector tests\"\"\"\n         body = \"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        response = Response(url=\"http://example.com\", body=body)\n         xpath = HtmlXPathSelector(response)\n \n         xl = xpath.x('//input')\n@@ -75,7 +75,7 @@ class XPathTestCase(unittest.TestCase):\n                     </div>\n                   </body>\"\"\"\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        response = Response(url=\"http://example.com\", body=body)\n         x = HtmlXPathSelector(response)\n \n         divtwo = x.x('//div[@class=\"two\"]')\n@@ -100,7 +100,7 @@ class XPathTestCase(unittest.TestCase):\n                   </div>\n \n                \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        response = Response(url=\"http://example.com\", body=body)\n         x = HtmlXPathSelector(response)\n \n         name_re = re.compile(\"Name: (\\w+)\")\n@@ -131,7 +131,7 @@ class XPathTestCase(unittest.TestCase):\n         </test>\n         \"\"\"\n \n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        response = Response(url=\"http://example.com\", body=body)\n         x = XmlXPathSelector(response)\n         \n         x.register_namespace(\"somens\", \"http://scrapy.org\")\n@@ -149,7 +149,7 @@ class XPathTestCase(unittest.TestCase):\n     <p:SecondTestTag><material/><price>90</price><p:name>Dried Rose</p:name></p:SecondTestTag>\n </BrowseNode>\n         \"\"\"\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", body=body)\n+        response = Response(url=\"http://example.com\", body=body)\n         x = XmlXPathSelector(response)\n \n         x.register_namespace(\"xmlns\", \"http://webservices.amazon.com/AWSECommerceService/2005-10-05\")\n@@ -176,7 +176,7 @@ class XPathTestCase(unittest.TestCase):\n         html_utf8 = html.encode(encoding)\n \n         headers = {'Content-Type': ['text/html; charset=utf-8']}\n-        response = Response(domain=\"example.com\", url=\"http://example.com\", headers=headers, body=html_utf8)\n+        response = Response(url=\"http://example.com\", headers=headers, body=html_utf8)\n         x = HtmlXPathSelector(response)\n         self.assertEquals(x.x(\"//span[@id='blank']/text()\").extract(),\n                           [u'\\xa3'])\n\n@@ -9,7 +9,7 @@ class ResponseLibxml2Test(unittest.TestCase):\n         ResponseLibxml2()\n \n     def test_response_libxml2_caching(self):\n-        r1 = Response('example.com', 'http://www.example.com', body='<html><head></head><body></body></html>')\n+        r1 = Response('http://www.example.com', body='<html><head></head><body></body></html>')\n         r2 = r1.copy()\n \n         doc1 = r1.getlibxml2doc()\n\n@@ -28,7 +28,7 @@ class XPathSelector(object):\n                 self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n-            response = Response(domain=None, url=None, body=unicode_to_str(text))\n+            response = Response(url=None, body=unicode_to_str(text))\n             self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#314bbabb30dc70298e59c2979ec0a9311ece5baa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 621 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -129,9 +129,6 @@ class ExecutionManager(object):\n \n         # requests\n         for request in requests:\n-            if request.domain:\n-                spider = spiders.fromdomain(request.domain)\n-            else:\n             spider = spiders.fromurl(request.url)\n             if not spider:\n                 log.msg('Could not find spider for %s' % request, log.ERROR)\n\n@@ -19,7 +19,7 @@ class Request(object):\n \n     def __init__(self, url, callback=None, method='GET',\n         body=None, headers=None, cookies=None, meta=None,\n-        url_encoding='utf-8', dont_filter=None, domain=None):\n+        url_encoding='utf-8', dont_filter=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n@@ -42,8 +42,6 @@ class Request(object):\n         self.headers = Headers(headers or {}, encoding=url_encoding)\n         # dont_filter be filtered by scheduler\n         self.dont_filter = dont_filter\n-        #allows to directly specify the spider for the request\n-        self.domain = domain\n \n         self.meta = {} if meta is None else dict(meta)\n         self.cache = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c9f7257a211490bc63a0a0cf1e4e2db86521d7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 15 | Files Changed: 4 | Hunks: 18 | Methods Changed: 14 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 81 | Churn Cumulative: 1453 | Contributors (this commit): 2 | Commits (past 90d): 43 | Contributors (cumulative): 7 | DMM Complexity: 0.5675675675675675\n\nDIFF:\n@@ -17,9 +17,8 @@ from scrapy.utils.defer import chain_deferred\n \n class Request(object):\n \n-    def __init__(self, url, callback=None, method='GET',\n-        body=None, headers=None, cookies=None, meta=None,\n-        url_encoding='utf-8', dont_filter=None):\n+    def __init__(self, url, callback=None, method='GET', headers=None, body=None, \n+                 cookies=None, meta=None, url_encoding='utf-8', dont_filter=None):\n \n         self.encoding = url_encoding  # this one has to be set first\n         self.set_url(url)\n@@ -69,14 +68,14 @@ class Request(object):\n             'method': self.method,\n             'url': self.url,\n             'headers': self.headers,\n-            'cookies': self.cookies,\n             'body': self.body,\n+            'cookies': self.cookies,\n             'meta': self.meta,\n             }\n         return \"%s(%s)\" % (self.__class__.__name__, repr(d))\n \n     def copy(self):\n-        \"\"\"Return a new request cloned from this one\"\"\"\n+        \"\"\"Return a copy a of this Request\"\"\"\n         new = copy.copy(self)\n         new.cache = {}\n         for att in self.__dict__:\n@@ -86,6 +85,17 @@ class Request(object):\n         new.deferred = defer.Deferred()\n         return new\n \n+    def replace(self, url=None, method=None, headers=None, body=None, cookies=None, meta=None):\n+        \"\"\"Create a new Request with the same attributes except for those\n+        given new values.\n+        \"\"\"\n+        return self.__class__(url=url or self.url,\n+                              method=method or self.method,\n+                              headers=headers or copy.deepcopy(self.headers),\n+                              body=body or self.body,\n+                              cookies=cookies or self.cookies,\n+                              meta=meta or self.meta)\n+\n     def httprepr(self):\n         \"\"\" Return raw HTTP request representation (as string). This is\n         provided only for reference since it's not the actual stream of bytes\n\n@@ -51,24 +51,21 @@ class Response(object):\n             return \"<%d %s>\" % (self.status, self.url)\n \n     def copy(self):\n-        \"\"\"Create a new Response based on the current one\"\"\"\n+        \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n \n-    def replace(self, url=None, status=None, headers=None, body=None):\n+    def replace(self, url=None, status=None, headers=None, body=None, meta=None):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n-\n-        Example:\n-\n-        >>> newresp = oldresp.replace(body=\"New body\")\n         \"\"\"\n         new = self.__class__(url=url or self.url,\n                              status=status or self.status,\n                              headers=headers or copy.deepcopy(self.headers),\n-                             body=body)\n+                             meta=meta or self.meta)\n         if body is None:\n             new.body = copy.deepcopy(self.body)\n-        new.meta = self.meta.copy()\n+        else:\n+            new.body = _ResponseBody(body, self.headers_encoding())\n         return new\n \n     def httprepr(self):\n@@ -184,3 +181,6 @@ class _ResponseBody(object):\n     def __len__(self):\n         return len(self._content)\n \n+    def __eq__(self, other):\n+        return self._content == other._content and self.declared_encoding == other.declared_encoding\n+\n\n@@ -105,7 +105,10 @@ class RequestTest(unittest.TestCase):\n     def test_copy(self):\n         \"\"\"Test Request copy\"\"\"\n         \n-        r1 = Request(\"http://www.example.com\")\n+        def somecallback():\n+            pass\n+\n+        r1 = Request(\"http://www.example.com\", callback=somecallback)\n         r1.meta['foo'] = 'bar'\n         r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n@@ -113,10 +116,18 @@ class RequestTest(unittest.TestCase):\n         assert r1.cache\n         assert not r2.cache\n \n+        assert r1.deferred is not r2.deferred\n+\n         # make sure meta dict is shallow copied\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n \n+        # make sure headers attribute is shallow copied\n+        assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n+        self.assertEqual(r1.headers, r2.headers)\n+\n+        # Request.body can be identical since it's an immutable object (str)\n+\n     def test_copy_inherited_classes(self):\n         \"\"\"Test Request children copies preserve their class\"\"\"\n \n@@ -128,6 +139,16 @@ class RequestTest(unittest.TestCase):\n \n         assert type(r2) is CustomRequest\n \n+    def test_replace(self):\n+        \"\"\"Test Request.replace() method\"\"\"\n+        hdrs = Headers({\"key\": \"value\"})\n+        r1 = Request(\"http://www.example.com\")\n+        r2 = r1.replace(method=\"POST\", body=\"New body\", headers=hdrs)\n+        self.assertEqual(r1.url, r2.url)\n+        self.assertEqual((r1.method, r2.method), (\"GET\", \"POST\"))\n+        self.assertEqual((r1.body, r2.body), (None, \"New body\"))\n+        self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n+\n     def test_httprepr(self):\n         r1 = Request(\"http://www.example.com\")\n         self.assertEqual(r1.httprepr(), 'GET http://www.example.com HTTP/1.1\\r\\nHost: www.example.com\\r\\n\\r\\n')\n\n@@ -36,7 +36,7 @@ class ResponseTest(unittest.TestCase):\n     def test_copy(self):\n         \"\"\"Test Response copy\"\"\"\n         \n-        r1 = Response(\"http://www.example.com\")\n+        r1 = Response(\"http://www.example.com\", body=\"Some body\")\n         r1.meta['foo'] = 'bar'\n         r1.cache['lala'] = 'lolo'\n         r2 = r1.copy()\n@@ -48,6 +48,14 @@ class ResponseTest(unittest.TestCase):\n         assert r1.meta is not r2.meta, \"meta must be a shallow copy, not identical\"\n         self.assertEqual(r1.meta, r2.meta)\n \n+        # make sure headers attribute is shallow copied\n+        assert r1.headers is not r2.headers, \"headers must be a shallow copy, not identical\"\n+        self.assertEqual(r1.headers, r2.headers)\n+\n+        # make sure body is shallow copied\n+        assert r1.body is not r2.body, \"body must be a shallow copy, not identical\"\n+        self.assertEqual(r1.body, r2.body)\n+\n     def test_copy_inherited_classes(self):\n         \"\"\"Test Response children copies preserve their class\"\"\"\n \n@@ -59,6 +67,18 @@ class ResponseTest(unittest.TestCase):\n \n         assert type(r2) is CustomResponse\n \n+    def test_replace(self):\n+        \"\"\"Test Response.replace() method\"\"\"\n+        hdrs = Headers({\"key\": \"value\"})\n+        r1 = Response(\"http://www.example.com\")\n+        r2 = r1.replace(status=301, body=\"New body\", headers=hdrs)\n+        assert r1.body is None\n+        assert isinstance(r2.body, _ResponseBody)\n+        self.assertEqual(r1.url, r2.url)\n+        self.assertEqual((r1.status, r2.status), (200, 301))\n+        self.assertEqual((r1.body, r2.body.to_string()), (None, \"New body\"))\n+        self.assertEqual((r1.headers, r2.headers), ({}, hdrs))\n+\n     def test_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n         self.assertEqual(r1.httprepr(), 'HTTP/1.1 200 OK\\r\\n\\r\\n')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a413fc3bce19cbf204c68d977b74409ff5701639", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 20 | Methods Changed: 13 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 84 | Churn Cumulative: 976 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,3 +17,13 @@ sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), \"xli\n # monkey patches to fix external library issues\n from scrapy.patches import monkeypatches\n monkeypatches.apply_patches()\n+\n+# optional_features is a set containing Scrapy optional features\n+optional_features = set()\n+\n+try:\n+    import OpenSSL\n+except ImportError:\n+    pass\n+else:\n+    optional_features.add('ssl')\n\n@@ -3,7 +3,6 @@ from __future__ import with_statement\n import os\n import hashlib\n import datetime\n-import urlparse\n import cPickle as pickle\n from pydispatch import dispatcher\n \n@@ -74,8 +73,7 @@ class CacheMiddleware(object):\n             self.cache.store(domain, key, request, exception.response)\n \n def is_cacheable(request):\n-    scheme, _, _, _, _ = urlparse.urlsplit(request.url)\n-    return scheme in ['http', 'https']\n+    return request.url.scheme in ['http', 'https']\n \n \n class Cache(object):\n@@ -172,7 +170,7 @@ class Cache(object):\n                 'url':request.url,\n                 'method': request.method,\n                 'status': response.status,\n-                'domain': response.domain,\n+                'domain': domain,\n                 'timestamp': datetime.datetime.utcnow(),\n             }\n \n\n@@ -10,42 +10,56 @@ from twisted.web.client import HTTPClientFactory\n from twisted.internet import defer, reactor\n from twisted.web import error as web_error\n \n+try:\n+    from twisted.internet import ssl\n+except ImportError:\n+    pass\n+\n+from scrapy import optional_features\n from scrapy.core import signals\n from scrapy.http import Request, Response, Headers\n-from scrapy.core.exceptions import UsageError, HttpException\n+from scrapy.core.exceptions import UsageError, HttpException, NotSupported\n from scrapy.utils.defer import defer_succeed\n from scrapy.conf import settings\n \n from scrapy.core.downloader.dnscache import DNSCache\n \n+default_timeout = settings.getint('DOWNLOAD_TIMEOUT')\n+default_agent = settings.get('USER_AGENT')\n+ssl_supported = 'ssl' in optional_features\n+\n # Cache for dns lookups.\n dnscache = DNSCache()\n \n def download_any(request, spider):\n-    u = urlparse.urlparse(request.url)\n-    if u.scheme == 'file':\n-        return download_file(request, spider)\n-    elif u.scheme in ('http', 'https'):\n+    scheme = request.url.scheme\n+    if scheme == 'http':\n         return download_http(request, spider)\n+    elif scheme == 'https':\n+        if ssl_supported:\n+            return download_https(request, spider)\n         else:\n-        raise UsageError(\"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, request.url))\n+            raise NotSupported(\"HTTPS not supported: install pyopenssl library\")\n+    elif request.url.scheme == 'file':\n+        return download_file(request, spider)\n+    else:\n+        raise NotSupported(\"Unsupported URL scheme '%s' in: <%s>\" % (request.url.scheme, request.url))\n \n-def download_http(request, spider):\n-    \"\"\"This functions handles http/https downloads\"\"\"\n+def create_factory(request, spider):\n+    \"\"\"Return HTTPClientFactory for the given Request\"\"\"\n     url = urlparse.urldefrag(request.url)[0]\n \n-    agent = request.headers.get('user-agent', settings.get('USER_AGENT'))\n-    request.headers.pop('user-agent', None)  # remove user-agent if already present\n-    factory = HTTPClientFactory(url=str(url), # never pass unicode urls to twisted\n+    agent = request.headers.pop('user-agent', default_agent)\n+    factory = HTTPClientFactory(url=url, # never pass unicode urls to twisted\n                                 method=request.method,\n                                 postdata=request.body,\n                                 headers=request.headers,\n                                 agent=agent,\n                                 cookies=request.cookies,\n-                                timeout=getattr(spider, \"download_timeout\", None) or settings.getint('DOWNLOAD_TIMEOUT'),\n+                                timeout=getattr(spider, \"download_timeout\", default_timeout),\n                                 followRedirect=False)\n \n-    def _response(body):\n+    def _create_response(body):\n         body = body or ''\n         status = int(factory.status)\n         headers = Headers(factory.response_headers)\n@@ -55,26 +69,33 @@ def download_http(request, spider):\n         return r\n \n     def _on_success(body):\n-        return _response(body)\n+        return _create_response(body)\n \n     def _on_error(_failure):\n         ex = _failure.value\n         if isinstance(ex, web_error.Error): # HttpException\n-            raise HttpException(ex.status, ex.message, _response(ex.response))\n+            raise HttpException(ex.status, ex.message, _create_response(ex.response))\n         return _failure\n \n     factory.noisy = False\n     factory.deferred.addCallbacks(_on_success, _on_error)\n+    return factory\n \n-    u = urlparse.urlparse(request.url)\n-    ip = dnscache.get(u.hostname)\n-    port = u.port\n-    if u.scheme == 'https' :\n-        from twisted.internet import ssl\n+def download_http(request, spider):\n+    \"\"\"Return a deferred for the HTTP download\"\"\"\n+    factory = create_factory(request, spider)\n+    ip = dnscache.get(request.url.hostname)\n+    port = request.url.port\n+    reactor.connectTCP(ip, port or 80, factory)\n+    return factory.deferred\n+\n+def download_https(request, spider):\n+    \"\"\"Return a deferred for the HTTPS download\"\"\"\n+    factory = create_factory(request, spider)\n+    ip = dnscache.get(request.url.hostname)\n+    port = request.url.port\n     contextFactory = ssl.ClientContextFactory()\n     reactor.connectSSL(ip, port or 443, factory, contextFactory)\n-    else:\n-        reactor.connectTCP(ip, port or 80, factory)\n     return factory.deferred\n \n def download_file(request, spider) :\n\n@@ -13,6 +13,9 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r.url, \"http://www.example.com\")\n         self.assertEqual(r.method, \"GET\")\n \n+        r.url = \"http://www.example.com/other\"\n+        assert isinstance(r.url, Url)\n+\n         assert isinstance(r.headers, Headers)\n         self.assertEqual(r.headers, {})\n         self.assertEqual(r.meta, {})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
