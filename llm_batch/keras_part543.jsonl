{"custom_id": "keras#aae25f518f4a8c4d365172670fa193487c5c4089", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 41 | Churn Cumulative: 62 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import numpy as np\n \n from keras_core import testing\n-from keras_core.backend import keras_tensor\n+from keras_core import backend\n from keras_core.layers.layer import Layer\n \n \n@@ -13,7 +13,7 @@ class LayerTest(testing.TestCase):\n                     return x\n                 return x + 1\n \n-        x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n+        x = backend.KerasTensor(shape=(2, 3), name=\"x\")\n         with self.assertRaisesRegex(\n             ValueError, \"Only input tensors may be passed as\"\n         ):\n@@ -21,3 +21,40 @@ class LayerTest(testing.TestCase):\n \n         # This works\n         SomeLayer()(x, bool_arg=True)\n+\n+    def test_rng_seed_tracking(self):\n+        class RNGLayer(Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.seed_gen = backend.random.SeedGenerator(seed=1337)\n+\n+            def call(self, x):\n+                return backend.random.dropout(x, rate=0.5, seed=self.seed_gen)\n+            \n+        layer = RNGLayer()\n+        self.assertEqual(layer.variables, [layer.seed_gen.state])\n+        self.assertAllClose(layer.variables[0], [1337, 0])\n+        layer(np.ones((3, 4)))\n+        self.assertAllClose(layer.variables[0], [1337, 1])\n+\n+        # Test tracking in list attributes.\n+        class RNGListLayer(Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.seed_gens = []\n+                self.seed_gens.append(backend.random.SeedGenerator(seed=1))\n+                self.seed_gens.append(backend.random.SeedGenerator(seed=10))\n+\n+            def call(self, x):\n+                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[0])\n+                x = backend.random.dropout(x, rate=0.5, seed=self.seed_gens[1])\n+                return x\n+        \n+        layer = RNGListLayer()\n+        self.assertEqual(layer.variables, [layer.seed_gens[0].state, layer.seed_gens[1].state])\n+        self.assertAllClose(layer.variables[0], [1, 0])\n+        self.assertAllClose(layer.variables[1], [10, 0])\n+        layer(np.ones((3, 4)))\n+        self.assertAllClose(layer.variables[0], [1, 1])\n+        self.assertAllClose(layer.variables[1], [10, 1])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbcd6eee8a8877e0c226cd2fd862ccfb6599f278", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 46 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,7 @@ class ConstraintsTest(testing.TestCase):\n     def test_non_neg(self):\n         constraint_fn = constraints.NonNeg()\n         output = constraint_fn(get_example_array())\n+        output = np.array(output)\n         self.assertTrue((np.min(output, axis=1) >= 0.0).all())\n \n     def test_unit_norm(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fc825fcbdfafc8c33af865a3588741004157e35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 224/184 | Churn Δ: 62 | Churn Cumulative: 6828 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: 0.7931034482758621\n\nDIFF:\n@@ -1148,7 +1148,9 @@ class BatchNormalizationBase(Layer):\n                 keepdims=keep_dims,\n             )\n \n-    def _sync_calculate_mean_and_var(self, x, axes, keep_dims, mask=None):\n+    def _sync_calculate_mean_and_var(\n+        self, x, reduction_axes, keep_dims, mask=None\n+    ):\n         with backend.name_scope(\"moments\"):\n             # The dynamic range of fp16 is too limited to support the collection\n             # of sufficient statistics. As a workaround we simply perform the\n@@ -1159,7 +1161,7 @@ class BatchNormalizationBase(Layer):\n \n             if not replica_ctx:\n                 return self._no_sync_calculate_mean_and_var(\n-                    x, axes, keep_dims, mask=mask\n+                    x, reduction_axes, keep_dims, mask=mask\n                 )\n \n             if mask is not None:\n@@ -1168,13 +1170,20 @@ class BatchNormalizationBase(Layer):\n                     mask_weights, axis=-1, name=\"mask_weights_broadcasted\"\n                 )\n                 y *= mask_weights\n+                local_count = tf.broadcast_to(\n+                    mask_weights, tf.shape(y), name=\"count\"\n+                )\n+            else:\n+                local_count = tf.ones_like(y, name=\"count\")\n \n-            local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n+            local_sum = tf.reduce_sum(y, axis=reduction_axes, keepdims=True)\n             local_squared_sum = tf.reduce_sum(\n-                tf.square(y), axis=axes, keepdims=True\n+                tf.square(y), axis=reduction_axes, keepdims=True\n+            )\n+            local_count = tf.reduce_sum(\n+                local_count, axis=reduction_axes, keepdims=True\n             )\n \n-            batch_size = tf.cast(tf.shape(y)[axes[0]], tf.float32)\n             # TODO(b/163099951): batch the all-reduces once we sort out the\n             # ordering issue for NCCL. We don't have a mechanism to launch\n             # NCCL in the same order in each replica nowadays, so we limit\n@@ -1185,21 +1194,17 @@ class BatchNormalizationBase(Layer):\n             y_squared_sum = replica_ctx.all_reduce(\n                 tf.distribute.ReduceOp.SUM, local_squared_sum\n             )\n-            global_batch_size = replica_ctx.all_reduce(\n-                tf.distribute.ReduceOp.SUM, batch_size\n+            count_sum = replica_ctx.all_reduce(\n+                tf.distribute.ReduceOp.SUM, local_count\n             )\n \n-            axes_vals = [(tf.shape(y))[axes[i]] for i in range(1, len(axes))]\n-            multiplier = tf.cast(tf.reduce_prod(axes_vals), tf.float32)\n-            multiplier = multiplier * global_batch_size\n-\n-            mean = y_sum / multiplier\n-            y_squared_mean = y_squared_sum / multiplier\n+            mean = y_sum / count_sum\n+            y_squared_mean = y_squared_sum / count_sum\n             # var = E(x^2) - E(x)^2\n             variance = y_squared_mean - tf.square(mean)\n             if not keep_dims:\n-                mean = tf.squeeze(mean, axes)\n-                variance = tf.squeeze(variance, axes)\n+                mean = tf.squeeze(mean, reduction_axes)\n+                variance = tf.squeeze(variance, reduction_axes)\n             if x.dtype == tf.float16:\n                 return (\n                     tf.cast(mean, tf.float16),\n\n@@ -267,6 +267,33 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         self.assertAllEqual(model.layers[2].moving_mean, [1.5, 5.0])\n         self.assertAllEqual(model.layers[2].moving_variance, [0.25, 0.0])\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_sync_batchnorm_with_mask(self):\n+        padded_data = np.array(\n+            [[[1, 5], [2, 5], [0, 0], [0, 0]] for _ in range(10)],\n+            dtype=\"float32\",\n+        )  # Pad value of 0\n+        strategy = tf.distribute.MirroredStrategy([\"CPU:0\"])\n+        distributed_data = strategy.distribute_datasets_from_function(\n+            dataset_fn=lambda _: tf.data.Dataset.from_tensors(\n+                (padded_data, padded_data)\n+            ).repeat(),\n+            options=None,\n+        )\n+        with strategy.scope():\n+            inputs = keras.layers.Input((None, 2))\n+            masked = keras.layers.Masking()(inputs)\n+            normed = keras.layers.BatchNormalization(\n+                momentum=0.0, synchronized=True\n+            )(masked)\n+            model = keras.models.Model(inputs, normed)\n+        # MirroredStrategy will be very slow when run eagerly.\n+        model.compile(\"rmsprop\", \"mse\", run_eagerly=False)\n+        model.fit(distributed_data, steps_per_epoch=1, epochs=5)\n+\n+        self.assertAllEqual(model.layers[2].moving_mean, [1.5, 5.0])\n+        self.assertAllEqual(model.layers[2].moving_variance, [0.25, 0.0])\n+\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_eager_batchnorm_in_custom_model_call_with_tf_function(self):\n         class MyModel(keras.Model):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b8dd458b48f7b3161f72502dc64435e4a79c6db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 22 | Files Changed: 3 | Hunks: 20 | Methods Changed: 13 | Complexity Δ (Sum/Max): 15/14 | Churn Δ: 116 | Churn Cumulative: 790 | Contributors (this commit): 3 | Commits (past 90d): 25 | Contributors (cumulative): 6 | DMM Complexity: 0.2631578947368421\n\nDIFF:\n@@ -1,4 +1,8 @@\n+import contextlib\n+import warnings\n+\n import tensorflow as tf\n+from tensorflow.python.eager import context as tf_context\n \n from keras_core import callbacks as callbacks_module\n from keras_core import optimizers as optimizers_module\n@@ -41,19 +45,25 @@ class Trainer(base_trainer.Trainer):\n         if self.train_function is not None and not force:\n             return self.train_function\n \n-        def step_function(data):\n+        def one_step_on_data(data):\n             \"\"\"Runs a single training step.\"\"\"\n             return self.train_step(data)\n \n-        if self.jit_compile:\n-            train_function = tf.function(\n-                step_function, jit_compile=True, reduce_retracing=True\n+        if not self.run_eagerly and self.jit_compile:\n+            one_step_on_data = tf.function(\n+                one_step_on_data, jit_compile=True, reduce_retracing=True\n             )\n-        elif not self.run_eagerly:\n-            train_function = tf.function(step_function, reduce_retracing=True)\n-        else:\n-            train_function = step_function\n \n+        def one_step_on_iterator(iterator):\n+            data = next(iterator)\n+            return one_step_on_data(data)\n+\n+        if not self.run_eagerly:\n+            train_function = tf.function(\n+                one_step_on_iterator, reduce_retracing=True\n+            )\n+        else:\n+            train_function = one_step_on_iterator\n         self.train_function = train_function\n \n     def make_test_function(self, force=False):\n@@ -105,7 +115,7 @@ class Trainer(base_trainer.Trainer):\n             ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n \n         # Create an iterator that yields batches for one epoch.\n-        epoch_iterator = EpochIterator(\n+        epoch_iterator = TFEpochIterator(\n             x=x,\n             y=y,\n             sample_weight=sample_weight,\n@@ -135,9 +145,10 @@ class Trainer(base_trainer.Trainer):\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n-            for step, batch in epoch_iterator.enumerate_epoch(return_type=\"tf\"):\n+            with epoch_iterator.catch_stop_iteration():\n+                for step, iterator in epoch_iterator.enumerate_epoch():\n                     callbacks.on_train_batch_begin(step)\n-                logs = self.train_function(batch)\n+                    logs = self.train_function(iterator)\n                     callbacks.on_train_batch_end(step, logs)\n                     if self.stop_training:\n                         break\n@@ -216,3 +227,53 @@ class Trainer(base_trainer.Trainer):\n                 pass\n             result[key] = value\n         return result\n+\n+\n+class TFEpochIterator(EpochIterator):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._steps_seen = 0\n+\n+    def enumerate_epoch(self):\n+        if self.steps_per_epoch:\n+            if not self._current_iterator:\n+                self._current_iterator = iter(\n+                    self.data_adapter.get_tf_dataset()\n+                )\n+            for step in range(self.steps_per_epoch):\n+                yield step, self._current_iterator\n+        else:\n+            iterator = iter(self.data_adapter.get_tf_dataset())\n+            if self.num_batches:\n+                for step in range(self.num_batches):\n+                    yield step, iterator\n+            else:\n+                step = -1\n+                while True:\n+                    step += 1\n+                    self._steps_seen = step + 1\n+                    yield step, iterator\n+        self.data_adapter.on_epoch_end()\n+\n+    def tf_sync(self):\n+        tf_context.async_wait()\n+\n+    @contextlib.contextmanager\n+    def catch_stop_iteration(self):\n+        \"\"\"Catches errors when an iterator runs out of data.\"\"\"\n+        try:\n+            yield\n+            self.tf_sync()\n+        except (StopIteration, tf.errors.OutOfRangeError):\n+            if self._num_batches is None:\n+                self._num_batches = self._steps_seen\n+            warnings.warn(\n+                \"Your input ran out of data; interrupting training. \"\n+                \"Make sure that your dataset or generator can generate \"\n+                \"at least `steps_per_epoch * epochs` batches. \"\n+                \"You may need to use the `.repeat()` \"\n+                \"function when building your dataset.\",\n+                stacklevel=2,\n+            )\n+            self._current_iterator = None\n+            self.data_adapter.on_epoch_end()\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n from keras_core import backend\n+from keras_core import testing\n from keras_core.layers.layer import Layer\n \n \n@@ -51,10 +51,12 @@ class LayerTest(testing.TestCase):\n                 return x\n \n         layer = RNGListLayer()\n-        self.assertEqual(layer.variables, [layer.seed_gens[0].state, layer.seed_gens[1].state])\n+        self.assertEqual(\n+            layer.variables,\n+            [layer.seed_gens[0].state, layer.seed_gens[1].state],\n+        )\n         self.assertAllClose(layer.variables[0], [1, 0])\n         self.assertAllClose(layer.variables[1], [10, 0])\n         layer(np.ones((3, 4)))\n         self.assertAllClose(layer.variables[0], [1, 1])\n         self.assertAllClose(layer.variables[1], [10, 1])\n-\n\n@@ -97,6 +97,7 @@ class EpochIterator:\n             raise ValueError(\n                 f\"Unrecognized data type: x={x} (of type {type(x)})\"\n             )\n+        self._num_batches = self.data_adapter.num_batches\n \n     def _get_iterator(self, return_type):\n         if return_type not in (\"np\", \"tf\"):\n@@ -118,19 +119,27 @@ class EpochIterator:\n                 try:\n                     data = next(self._current_iterator)\n                     yield step, data\n-                except StopIteration:\n+                except (StopIteration, tf.errors.OutOfRangeError):\n                     warnings.warn(\n-                        \"The dataset ran out of data before the end of the epoch. \"\n-                        \"When passing `steps_per_epoch` \"\n-                        \"(or otherwise `validation_steps` in `fit()` or `steps` in `evaluate()`), \"\n-                        \"make sure that your dataset size (number of batches) is divisible \"\n-                        \"by `steps_per_epoch`.\"\n+                        \"Your input ran out of data; interrupting epoch. \"\n+                        \"Make sure that your dataset or generator can generate \"\n+                        \"at least `steps_per_epoch * epochs` batches. \"\n+                        \"You may need to use the `.repeat()` \"\n+                        \"function when building your dataset.\",\n+                        stacklevel=2,\n                     )\n-                    self._current_iterator = self._get_iterator(return_type)\n+                    self._current_iterator = None\n         else:\n             for step, data in enumerate(self._get_iterator(return_type)):\n                 yield step, data\n+            if not self._num_batches:\n+                # Infer the number of batches returned by the data_adater.\n+                # Assumed static.\n+                self._num_batches = step + 1\n+        self.data_adapter.on_epoch_end()\n \n     @property\n     def num_batches(self):\n-        return self.data_adapter.num_batches\n+        # Either copied from the data_adapter, or\n+        # inferred at the end of an iteration.\n+        return self._num_batches\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f24539c28873b97313c4dadc0f0524897509fbbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 84 | Churn Cumulative: 930 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -735,6 +735,7 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose(ref_out, out, atol=1e-6)\n \n \n+# This custom class lacks custom object registration.\n class CustomRNN(keras.layers.Layer):\n     def __init__(self, units):\n         super(CustomRNN, self).__init__()\n@@ -756,6 +757,61 @@ class CustomRNN(keras.layers.Layer):\n         return self.classifier(features)\n \n \n+# This class is properly registered with a `get_config()` method.\n+# However, since it does not subclass keras.layers.Layer, it lacks\n+# `from_config()` for deserialization.\n+@keras.utils.register_keras_serializable()\n+class GrowthFactor:\n+    def __init__(self, factor):\n+        self.factor = factor\n+\n+    def __call__(self, inputs):\n+        return inputs * self.factor\n+\n+    def get_config(self):\n+        return {\"factor\": self.factor}\n+\n+\n+@keras.utils.register_keras_serializable(package=\"Complex\")\n+class FactorLayer(keras.layers.Layer):\n+    def __init__(self, factor):\n+        super().__init__()\n+        self.factor = factor\n+\n+    def call(self, x):\n+        return x * self.factor\n+\n+    def get_config(self):\n+        return {\"factor\": self.factor}\n+\n+\n+# This custom model does not explicitly deserialize the layers it includes\n+# in its `get_config`. Explicit deserialization in a `from_config` override\n+# or `__init__` is needed here, or an error will be thrown at loading time.\n+@keras.utils.register_keras_serializable(package=\"Complex\")\n+class ComplexModel(keras.layers.Layer):\n+    def __init__(self, first_layer, second_layer=None, **kwargs):\n+        super().__init__(**kwargs)\n+        self.first_layer = first_layer\n+        if second_layer is not None:\n+            self.second_layer = second_layer\n+        else:\n+            self.second_layer = keras.layers.Dense(8)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"first_layer\": self.first_layer,\n+                \"second_layer\": self.second_layer,\n+            }\n+        )\n+        return config\n+\n+    def call(self, inputs):\n+        return self.first_layer(self.second_layer(inputs))\n+\n+\n @test_utils.run_v2_only\n class SavingV3BattleTest(tf.test.TestCase, parameterized.TestCase):\n     def test_custom_model_without_registration_error(self):\n@@ -778,6 +834,34 @@ class SavingV3BattleTest(tf.test.TestCase, parameterized.TestCase):\n             model.save(temp_filepath)\n             _ = keras.models.load_model(temp_filepath)\n \n+    def test_custom_object_without_from_config(self):\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"custom_fn_model.keras\"\n+        )\n+\n+        inputs = keras.Input(shape=(4, 4))\n+        outputs = keras.layers.Dense(1, activation=GrowthFactor(0.5))(inputs)\n+        model = keras.Model(inputs, outputs)\n+\n+        model.save(temp_filepath)\n+\n+        with self.assertRaisesRegex(\n+            TypeError, \"Unable to reconstruct an instance\"\n+        ):\n+            _ = keras.models.load_model(temp_filepath)\n+\n+    def test_complex_model_without_explicit_deserialization(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"complex_model.keras\")\n+\n+        inputs = keras.Input((32,))\n+        outputs = ComplexModel(first_layer=FactorLayer(0.5))(inputs)\n+        model = keras.Model(inputs, outputs)\n+\n+        model.save(temp_filepath)\n+\n+        with self.assertRaisesRegex(TypeError, \"object is not callable\"):\n+            _ = keras.models.load_model(temp_filepath)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c0ff911dfa7c7acba794f3ad0a3a727b4c6283f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 127 | Files Changed: 6 | Hunks: 32 | Methods Changed: 13 | Complexity Δ (Sum/Max): 4/5 | Churn Δ: 288 | Churn Cumulative: 1261 | Contributors (this commit): 3 | Commits (past 90d): 30 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,3 +3,8 @@ from keras_core.constraints.constraints import MaxNorm\n from keras_core.constraints.constraints import MinMaxNorm\n from keras_core.constraints.constraints import NonNeg\n from keras_core.constraints.constraints import UnitNorm\n+\n+\n+def get(identifier):\n+    # temporary shim.\n+    return identifier\n\n@@ -1,3 +1,25 @@\n-from keras_core.initializers.constant_initializers import *\n+from keras_core.initializers.constant_initializers import Ones\n+from keras_core.initializers.constant_initializers import Zeros\n from keras_core.initializers.initializer import Initializer\n-from keras_core.initializers.random_initializers import *\n+from keras_core.initializers.random_initializers import GlorotNormal\n+from keras_core.initializers.random_initializers import GlorotUniform\n+from keras_core.initializers.random_initializers import HeNormal\n+from keras_core.initializers.random_initializers import HeUniform\n+from keras_core.initializers.random_initializers import LecunNormal\n+from keras_core.initializers.random_initializers import LecunUniform\n+from keras_core.initializers.random_initializers import RandomNormal\n+from keras_core.initializers.random_initializers import RandomUniform\n+from keras_core.initializers.random_initializers import VarianceScaling\n+\n+\n+def get(identifier):\n+    # Temporary shim\n+    if identifier == \"zeros\":\n+        return Zeros()\n+    if identifier == \"ones\":\n+        return Ones()\n+    if identifier == \"glorot_uniform\":\n+        return GlorotUniform()\n+    if identifier == \"glorot_normal\":\n+        return GlorotNormal()\n+    return identifier\n\n@@ -1,8 +1,10 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n from keras_core.backend import standardize_dtype\n from keras_core.initializers.initializer import Initializer\n-from keras_core.operations import numpy as knp\n \n \n+@keras_core_export(\"keras_core.initializers.Zeros\")\n class Zeros(Initializer):\n     \"\"\"Initializer that generates tensors initialized to 0.\n \n@@ -17,7 +19,7 @@ class Zeros(Initializer):\n     >>> layer = Dense(3, kernel_initializer=initializer)\n     \"\"\"\n \n-    def __call__(self, shape, dtype=None, **kwargs):\n+    def __call__(self, shape, dtype=None):\n         \"\"\"Returns a tensor object initialized as specified by the initializer.\n \n         Args:\n@@ -29,9 +31,10 @@ class Zeros(Initializer):\n             **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n-        return knp.zeros(shape, dtype=dtype)\n+        return ops.zeros(shape, dtype=dtype)\n \n \n+@keras_core_export(\"keras_core.initializers.Ones\")\n class Ones(Initializer):\n     \"\"\"Initializer that generates tensors initialized to 1.\n \n@@ -48,7 +51,7 @@ class Ones(Initializer):\n     >>> layer = Dense(3, kernel_initializer=initializer)\n     \"\"\"\n \n-    def __call__(self, shape, dtype=None, **kwargs):\n+    def __call__(self, shape, dtype=None):\n         \"\"\"Returns a tensor object initialized as specified by the initializer.\n \n         Args:\n@@ -60,4 +63,4 @@ class Ones(Initializer):\n             **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n-        return knp.ones(shape, dtype=dtype)\n+        return ops.ones(shape, dtype=dtype)\n\n@@ -45,13 +45,12 @@ class Initializer:\n     fine.\n     \"\"\"\n \n-    def __call__(self, shape, dtype=None, **kwargs):\n+    def __call__(self, shape, dtype=None):\n         \"\"\"Returns a tensor object initialized as specified by the initializer.\n \n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor.\n-            **kwargs: Additional keyword arguments.\n         \"\"\"\n         raise NotImplementedError(\n             \"Initializer subclasses must implement the `__call__()` method.\"\n@@ -84,17 +83,3 @@ class Initializer:\n             An `Initializer` instance.\n         \"\"\"\n         return cls(**config)\n-\n-    def _warn_reuse(self):\n-        if getattr(self, \"_used\", False):\n-            if getattr(self, \"seed\", None) is None:\n-                warnings.warn(\n-                    f\"The initializer {self.__class__.__name__} is unseeded \"\n-                    \"and being called multiple times, which will return \"\n-                    \"identical values each time (even if the initializer is \"\n-                    \"unseeded). Please update your code to provide a seed to \"\n-                    \"the initializer, or avoid using the same initializer \"\n-                    \"instance more than once.\"\n-                )\n-        else:\n-            self._used = True\n\n@@ -1,9 +1,111 @@\n import math\n \n+from keras_core.api_export import keras_core_export\n from keras_core.backend import random\n from keras_core.initializers.initializer import Initializer\n \n \n+@keras_core_export(\"keras_core.initializers.RandomNormal\")\n+class RandomNormal(Initializer):\n+    \"\"\"Random normal initializer.\n+\n+    Draws samples from a normal distribution for given parameters.\n+\n+    Examples:\n+\n+    >>> # Standalone usage:\n+    >>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n+    >>> values = initializer(shape=(2, 2))\n+\n+    >>> # Usage in a Keras layer:\n+    >>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n+    >>> layer = Dense(3, kernel_initializer=initializer)\n+\n+    Args:\n+        mean: A python scalar or a scalar keras tensor. Mean of the random values to\n+            generate.\n+        stddev: A python scalar or a scalar keras tensor. Standard deviation of the\n+           random values to generate.\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n+\n+    def __init__(self, mean=0.0, stddev=1.0, seed=None):\n+        self.mean = mean\n+        self.stddev = stddev\n+        self.seed = seed or random.make_default_seed()\n+        super().__init__()\n+\n+    def __call__(self, shape, dtype=None):\n+        return random.normal(\n+            shape=shape,\n+            mean=self.mean,\n+            stddev=self.stddev,\n+            seed=self.seed,\n+            dtype=dtype,\n+        )\n+\n+    def get_config(self):\n+        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": self.seed}\n+\n+\n+@keras_core_export(\"keras_core.initializers.RandomUniform\")\n+class RandomUniform(Initializer):\n+    \"\"\"Random uniform initializer.\n+\n+    Draws samples from a uniform distribution for given parameters.\n+\n+    Examples:\n+\n+    >>> # Standalone usage:\n+    >>> initializer = RandomUniform(minval=0.0, maxval=1.0)\n+    >>> values = initializer(shape=(2, 2))\n+\n+    >>> # Usage in a Keras layer:\n+    >>> initializer = RandomUniform(minval=0.0, maxval=1.0)\n+    >>> layer = Dense(3, kernel_initializer=initializer)\n+\n+    Args:\n+        minval: A python scalar or a scalar keras tensor. Lower bound of the range of\n+            random values to generate (inclusive).\n+        maxval: A python scalar or a scalar keras tensor. Upper bound of the range of\n+            random values to generate (exclusive).\n+        seed: A Python integer or instance of\n+            `keras_core.backend.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.backend.SeedGenerator`.\n+    \"\"\"\n+\n+    def __init__(self, minval=0.0, maxval=1.0, seed=None):\n+        self.minval = minval\n+        self.maxval = maxval\n+        self.seed = seed or random.make_default_seed()\n+        super().__init__()\n+\n+    def __call__(self, shape, dtype=None):\n+        return random.uniform(\n+            shape=shape,\n+            minval=self.minval,\n+            maxval=self.maxval,\n+            seed=self.seed,\n+            dtype=dtype,\n+        )\n+\n+    def get_config(self):\n+        return {\"minval\": self.minval, \"maxval\": self.maxval, \"seed\": self.seed}\n+\n+\n+@keras_core_export(\"keras_core.initializers.VarianceScaling\")\n class VarianceScaling(Initializer):\n     \"\"\"Initializer that adapts its scale to the shape of its input tensors.\n \n@@ -80,7 +182,7 @@ class VarianceScaling(Initializer):\n         self.distribution = distribution\n         self.seed = seed or random.make_default_seed()\n \n-    def __call__(self, shape, dtype=None, **kwargs):\n+    def __call__(self, shape, dtype=None):\n         \"\"\"Returns a tensor object initialized as specified by the initializer.\n \n         Args:\n@@ -89,7 +191,6 @@ class VarianceScaling(Initializer):\n                 supported. If not specified, `tf.keras.backend.floatx()` is used,\n                 which default to `float32` unless you configured it otherwise (via\n                 `tf.keras.backend.set_floatx(float_dtype)`)\n-            **kwargs: Additional keyword arguments.\n         \"\"\"\n         scale = self.scale\n         fan_in, fan_out = compute_fans(shape)\n@@ -124,6 +225,7 @@ class VarianceScaling(Initializer):\n         }\n \n \n+@keras_core_export(\"keras_core.initializers.GlorotUniform\")\n class GlorotUniform(VarianceScaling):\n     \"\"\"The Glorot uniform initializer, also called Xavier uniform initializer.\n \n@@ -151,7 +253,7 @@ class GlorotUniform(VarianceScaling):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n \n-    References:\n+    Reference:\n \n     - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n     \"\"\"\n@@ -165,6 +267,7 @@ class GlorotUniform(VarianceScaling):\n         return {\"seed\": self.seed}\n \n \n+@keras_core_export(\"keras_core.initializers.GlorotNormal\")\n class GlorotNormal(VarianceScaling):\n     \"\"\"The Glorot normal initializer, also called Xavier normal initializer.\n \n@@ -193,7 +296,8 @@ class GlorotNormal(VarianceScaling):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n \n-    References:\n+    Reference:\n+\n     - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n     \"\"\"\n \n@@ -209,6 +313,7 @@ class GlorotNormal(VarianceScaling):\n         return {\"seed\": self.seed}\n \n \n+@keras_core_export(\"keras_core.initializers.LecunNormal\")\n class LecunNormal(VarianceScaling):\n     \"\"\"Lecun normal initializer.\n \n@@ -240,7 +345,8 @@ class LecunNormal(VarianceScaling):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n \n-    References:\n+    Reference:\n+\n     - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n     \"\"\"\n \n@@ -253,6 +359,7 @@ class LecunNormal(VarianceScaling):\n         return {\"seed\": self.seed}\n \n \n+@keras_core_export(\"keras_core.initializers.LecunUniform\")\n class LecunUniform(VarianceScaling):\n     \"\"\"Lecun uniform initializer.\n \n@@ -280,7 +387,8 @@ class LecunUniform(VarianceScaling):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n \n-    References:\n+    Reference:\n+\n     - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n     \"\"\"\n \n@@ -293,6 +401,7 @@ class LecunUniform(VarianceScaling):\n         return {\"seed\": self.seed}\n \n \n+@keras_core_export(\"keras_core.initializers.HeNormal\")\n class HeNormal(VarianceScaling):\n     \"\"\"He normal initializer.\n \n@@ -321,6 +430,7 @@ class HeNormal(VarianceScaling):\n             of `keras_core.backend.SeedGenerator`.\n \n     Reference:\n+\n     - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n     \"\"\"\n \n@@ -333,6 +443,7 @@ class HeNormal(VarianceScaling):\n         return {\"seed\": self.seed}\n \n \n+@keras_core_export(\"keras_core.initializers.HeUniform\")\n class HeUniform(VarianceScaling):\n     \"\"\"He uniform variance scaling initializer.\n \n@@ -361,6 +472,7 @@ class HeUniform(VarianceScaling):\n             of `keras_core.backend.SeedGenerator`.\n \n     Reference:\n+\n     - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n     \"\"\"\n \n@@ -399,101 +511,3 @@ def compute_fans(shape):\n         fan_in = shape[-2] * receptive_field_size\n         fan_out = shape[-1] * receptive_field_size\n     return int(fan_in), int(fan_out)\n-\n-\n-class RandomNormal(Initializer):\n-    \"\"\"Random normal initializer.\n-\n-    Draws samples from a normal distribution for given parameters.\n-\n-    Examples:\n-\n-    >>> # Standalone usage:\n-    >>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n-    >>> values = initializer(shape=(2, 2))\n-\n-    >>> # Usage in a Keras layer:\n-    >>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n-    >>> layer = Dense(3, kernel_initializer=initializer)\n-\n-    Args:\n-        mean: A python scalar or a scalar keras tensor. Mean of the random values to\n-            generate.\n-        stddev: A python scalar or a scalar keras tensor. Standard deviation of the\n-           random values to generate.\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n-\n-    def __init__(self, mean=0.0, stddev=1.0, seed=None):\n-        self.mean = mean\n-        self.stddev = stddev\n-        self.seed = seed or random.make_default_seed()\n-        super().__init__()\n-\n-    def __call__(self, shape, dtype=None, **kwargs):\n-        return random.normal(\n-            shape=shape,\n-            mean=self.mean,\n-            stddev=self.stddev,\n-            seed=self.seed,\n-            dtype=dtype,\n-        )\n-\n-    def get_config(self):\n-        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": self.seed}\n-\n-\n-class RandomUniform(Initializer):\n-    \"\"\"Random uniform initializer.\n-\n-    Draws samples from a uniform distribution for given parameters.\n-\n-    Examples:\n-\n-    >>> # Standalone usage:\n-    >>> initializer = RandomUniform(minval=0.0, maxval=1.0)\n-    >>> values = initializer(shape=(2, 2))\n-\n-    >>> # Usage in a Keras layer:\n-    >>> initializer = RandomUniform(minval=0.0, maxval=1.0)\n-    >>> layer = Dense(3, kernel_initializer=initializer)\n-\n-    Args:\n-        minval: A python scalar or a scalar keras tensor. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: A python scalar or a scalar keras tensor. Upper bound of the range of\n-            random values to generate (exclusive).\n-        seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n-    \"\"\"\n-\n-    def __init__(self, minval=0.0, maxval=1.0, seed=None):\n-        self.minval = minval\n-        self.maxval = maxval\n-        self.seed = seed or random.make_default_seed()\n-        super().__init__()\n-\n-    def __call__(self, shape, dtype=None, **kwargs):\n-        return random.uniform(\n-            shape=shape,\n-            minval=self.minval,\n-            maxval=self.maxval,\n-            seed=self.seed,\n-            dtype=dtype,\n-        )\n-\n-    def get_config(self):\n-        return {\"minval\": self.minval, \"maxval\": self.maxval, \"seed\": self.seed}\n\n@@ -3,3 +3,8 @@ from keras_core.regularizers.regularizers import L1L2\n from keras_core.regularizers.regularizers import L2\n from keras_core.regularizers.regularizers import OrthogonalRegularizer\n from keras_core.regularizers.regularizers import Regularizer\n+\n+\n+def get(identifier):\n+    # temporary shim.\n+    return identifier\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46c186ff4607fec31c085163b8958ccf998f5b5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5237 | Contributors (this commit): 12 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -1165,7 +1165,7 @@ class BatchNormalizationBase(Layer):\n                 )\n \n             if mask is not None:\n-                mask_weights = tf.cast(mask, tf.float32, name=\"mask_weights\")\n+                mask_weights = tf.cast(mask, y.dtype, name=\"mask_weights\")\n                 mask_weights = tf.expand_dims(\n                     mask_weights, axis=-1, name=\"mask_weights_broadcasted\"\n                 )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fa38c107166b26e36ee16219b3b37e9ab014616", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 64 | Files Changed: 5 | Hunks: 28 | Methods Changed: 15 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 162 | Churn Cumulative: 2060 | Contributors (this commit): 2 | Commits (past 90d): 33 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,17 @@ from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n class Trainer(base_trainer.Trainer):\n+\n+    def stateless_compute_loss_and_updates(\n+            self, trainable_variables, non_trainable_variables, x, y, sample_weight\n+        ):\n+            y_pred, non_trainable_variables = self.stateless_call(\n+                trainable_variables, non_trainable_variables, x\n+            )\n+\n+            loss = self.compute_loss(x, y, y_pred, sample_weight)\n+            return loss, (y_pred, non_trainable_variables)\n+\n     def fit(\n         self,\n         x=None,\n@@ -77,6 +88,9 @@ class Trainer(base_trainer.Trainer):\n                 self.compute_metrics(x, y, y_pred, sample_weight)\n                 self.reset_metrics()\n                 break\n+        if not self.optimizer.built:\n+            # Build optimizer\n+            self.optimizer.build(self.trainable_variables)\n \n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n@@ -90,33 +104,9 @@ class Trainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        self.stop_training = False\n-        # self.make_train_function()\n-        callbacks.on_train_begin()\n-        training_logs = None\n-        logs = None\n+        grad_fn = jax.value_and_grad(self.stateless_compute_loss_and_updates, has_aux=True)\n         \n-        trainable_variables = self.trainable_variables\n-        if not self.optimizer.built:\n-            self.optimizer.build(trainable_variables)\n-        non_trainable_variables = self.non_trainable_variables\n-        optimizer_variables = self.optimizer.variables\n-        metrics_variables = self.metrics_variables\n-\n-        def compute_loss_and_updates(\n-            trainable_variables, non_trainable_variables, x, y\n-        ):\n-            y_pred, non_trainable_variables = self.stateless_call(\n-                trainable_variables, non_trainable_variables, x\n-            )\n-\n-            loss = self._compile_loss(y, y_pred)\n-            return loss, (y_pred, non_trainable_variables)\n-\n-        grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n-\n-        @jax.jit\n-        def train_step(state, data):\n+        def _train_step(state, data):\n             (\n                 trainable_variables,\n                 non_trainable_variables,\n@@ -127,7 +117,7 @@ class Trainer(base_trainer.Trainer):\n                 data\n             )\n             (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n-                trainable_variables, non_trainable_variables, x, y\n+                trainable_variables, non_trainable_variables, x, y, sample_weight\n             )\n \n             (\n@@ -164,9 +154,25 @@ class Trainer(base_trainer.Trainer):\n             )\n             return logs, state\n         \n+        if not self.run_eagerly and not self.jit_compile:\n+            @jax.jit\n+            def train_step(state, data):\n+                return _train_step(state, data)\n+        else:\n+            train_step = _train_step\n+        \n+        self.stop_training = False\n+        callbacks.on_train_begin()\n+\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n+\n+            trainable_variables = self.trainable_variables\n+            non_trainable_variables = self.non_trainable_variables\n+            optimizer_variables = self.optimizer.variables\n+            metrics_variables = self.metrics_variables\n+\n             for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n@@ -268,13 +274,3 @@ class Trainer(base_trainer.Trainer):\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n         raise NotImplementedError\n\\ No newline at end of file\n-\n-    def _process_logs(self, logs):\n-        result = {}\n-        for key, value in logs.items():\n-            try:\n-                value = float(value)\n-            except:\n-                pass\n-            result[key] = value\n-        return result\n\n@@ -24,9 +24,10 @@ class Trainer(base_trainer.Trainer):\n             loss = self.compute_loss(\n                 x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n             )\n+        self._loss_tracker.update_state(loss)\n \n         # Compute gradients\n-        # TODO: move value conversion to the optimizer\n+        # TODO: move value conversion to TF\n         trainable_weights = [v.value for v in self.trainable_weights]\n         gradients = tape.gradient(loss, trainable_weights)\n \n@@ -218,16 +219,6 @@ class Trainer(base_trainer.Trainer):\n     ):\n         raise NotImplementedError\n \n-    def _process_logs(self, logs):\n-        result = {}\n-        for key, value in logs.items():\n-            try:\n-                value = float(value)\n-            except:\n-                pass\n-            result[key] = value\n-        return result\n-\n \n class TFEpochIterator(EpochIterator):\n     def __init__(self, *args, **kwargs):\n\n@@ -1,27 +1,53 @@\n from keras_core import activations\n+from keras_core import constraints\n from keras_core import initializers\n from keras_core import operations as ops\n+from keras_core import regularizers\n from keras_core.layers.layer import Layer\n \n \n class Dense(Layer):\n-    def __init__(self, units, activation=None, use_bias=True, name=None):\n-        # TODO: support all other arguments.\n+    def __init__(\n+        self,\n+        units,\n+        activation=None,\n+        use_bias=True,\n+        kernel_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        kernel_regularizer=None,\n+        bias_regularizer=None,\n+        activity_regularizer=None,\n+        kernel_constraint=None,\n+        bias_constraint=None,\n+        name=None,\n+    ):\n         super().__init__(name=name)\n         self.units = units\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n+        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+\n+        if activity_regularizer:\n+            # TODO\n+            raise ValueError(\"activity_regularizer not yet supported.\")\n \n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n         self.kernel = self.add_weight(\n             shape=(input_dim, self.units),\n-            initializer=initializers.GlorotUniform(),\n+            initializer=self.kernel_initializer,\n+            regularizer=self.kernel_regularizer,\n         )\n         if self.use_bias:\n             self.bias = self.add_weight(\n                 shape=(self.units,),\n-                initializer=initializers.Zeros(),\n+                initializer=self.bias_initializer,\n+                regularizer=self.bias_regularizer,\n             )\n \n     def call(self, inputs):\n\n@@ -149,7 +149,6 @@ class Trainer:\n             total_loss = losses[0]\n         else:\n             total_loss = ops.sum(losses)\n-        self._loss_tracker.update_state(total_loss)\n         return total_loss\n \n     def compute_metrics(self, x, y, y_pred, sample_weight=None):\n@@ -288,3 +287,13 @@ class Trainer:\n     def compile_from_config(self):\n         # TODO\n         raise NotImplementedError\n+\n+    def _process_logs(self, logs):\n+        result = {}\n+        for key, value in logs.items():\n+            try:\n+                value = float(value)\n+            except:\n+                pass\n+            result[key] = value\n+        return result\n\\ No newline at end of file\n\n@@ -1,32 +1,38 @@\n import numpy as np\n-import pytest\n \n from keras_core import backend\n+from keras_core import initializers\n from keras_core import layers\n from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n from keras_core import testing\n+\n+if backend.backend() == \"jax\":\n+    from keras_core.backend.jax.trainer import Trainer\n+else:\n     from keras_core.backend.tensorflow.trainer import Trainer\n \n \n # A model is just a layer mixed in with a Trainer.\n-class TFModel(layers.Dense, Trainer):\n+class ExampleModel(layers.Dense, Trainer):\n     def __init__(self, units):\n-        layers.Dense.__init__(self, units=units)\n+        layers.Dense.__init__(\n+            self,\n+            units=units,\n+            use_bias=False,\n+            kernel_initializer=initializers.Ones(),\n+        )\n         Trainer.__init__(self)\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\", reason=\"Target the TF backend only.\"\n-)\n-class TestTFTrainer(testing.TestCase):\n+class TestTrainer(testing.TestCase):\n     def _test_basic_flow(self, run_eagerly, jit_compile):\n-        model = TFModel(units=3)\n-        x = np.random.random((100, 4))\n-        y = np.random.random((100, 3))\n+        model = ExampleModel(units=3)\n+        x = np.ones((100, 4))\n+        y = np.zeros((100, 3))\n         batch_size = 16\n-        epochs = 10\n+        epochs = 3\n \n         model.compile(\n             optimizer=optimizers.SGD(),\n@@ -36,6 +42,12 @@ class TestTFTrainer(testing.TestCase):\n             jit_compile=jit_compile,\n         )\n         history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertIn(\"mean_squared_error\", history)\n+        self.assertAllClose(\n+            history[\"mean_squared_error\"], [13.938, 9.547, 6.539], atol=1e-2\n+        )\n \n     def test_basic_flow_eager(self):\n         self._test_basic_flow(run_eagerly=True, jit_compile=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
