{"custom_id": "keras#a90af6f22eb13bc2647a57751c02d86cac5254f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 69 | Files Changed: 2 | Hunks: 52 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 189 | Churn Cumulative: 17022 | Contributors (this commit): 78 | Commits (past 90d): 32 | Contributors (cumulative): 90 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -13,7 +13,8 @@ from .pooling import MaxPooling1D, MaxPooling2D, MaxPooling3D\n \n \n class Convolution1D(Layer):\n-    \"\"\"Convolution operator for filtering neighborhoods of one-dimensional inputs.\n+    \"\"\"Convolution operator for filtering neighborhoods of 1-D inputs.\n+\n     When using this layer as the first layer in a model,\n     either provide the keyword argument `input_dim`\n     (int, e.g. 128 for sequences of 128-dimensional vectors),\n@@ -39,16 +40,18 @@ class Convolution1D(Layer):\n             (dimensionality of the output).\n         filter_length: The extension (spatial or temporal) of each filter.\n         init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n-            or alternatively, Theano function to use for weights initialization.\n-            This parameter is only relevant if you don't pass a `weights` argument.\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization.\n+            This parameter is only relevant\n+            if you don't pass a `weights` argument.\n         activation: name of activation function to use\n             (see [activations](../activations.md)),\n             or alternatively, elementwise Theano function.\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n+        border_mode: 'valid', 'same' or 'full'\n+            ('full' requires the Theano backend).\n         subsample_length: factor by which to subsample output.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n@@ -81,7 +84,8 @@ class Convolution1D(Layer):\n     def __init__(self, nb_filter, filter_length,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_regularizer=None, b_regularizer=None,\n+                 activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, input_dim=None, input_length=None, **kwargs):\n \n@@ -174,7 +178,8 @@ class Convolution1D(Layer):\n \n \n class AtrousConvolution1D(Convolution1D):\n-    \"\"\"Atrous Convolution operator for filtering neighborhoods of one-dimensional inputs.\n+    \"\"\"Atrous Convolution operator for filtering neighborhoods of 1-D inputs.\n+\n     A.k.a dilated convolution or convolution with holes.\n     When using this layer as the first layer in a model,\n     either provide the keyword argument `input_dim`\n@@ -185,14 +190,18 @@ class AtrousConvolution1D(Convolution1D):\n     # Example\n \n     ```python\n-        # apply an atrous convolution 1d with atrous rate 2 of length 3 to a sequence with 10 timesteps,\n+        # apply an atrous convolution 1d\n+        # with atrous rate 2 of length 3 to a sequence with 10 timesteps,\n         # with 64 output filters\n         model = Sequential()\n-        model.add(AtrousConvolution1D(64, 3, atrous_rate=2, border_mode='same', input_shape=(10, 32)))\n+        model.add(AtrousConvolution1D(64, 3, atrous_rate=2,\n+                                      border_mode='same',\n+                                      input_shape=(10, 32)))\n         # now model.output_shape == (None, 10, 64)\n \n         # add a new atrous conv1d on top\n-        model.add(AtrousConvolution1D(32, 3, atrous_rate=2, border_mode='same'))\n+        model.add(AtrousConvolution1D(32, 3, atrous_rate=2,\n+                                      border_mode='same'))\n         # now model.output_shape == (None, 10, 32)\n     ```\n \n@@ -201,16 +210,18 @@ class AtrousConvolution1D(Convolution1D):\n             (dimensionality of the output).\n         filter_length: The extension (spatial or temporal) of each filter.\n         init: name of initialization function for the weights of the layer\n-            (see [initializations](../initializations.md)),\n-            or alternatively, Theano function to use for weights initialization.\n-            This parameter is only relevant if you don't pass a `weights` argument.\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization.\n+            This parameter is only relevant\n+            if you don't pass a `weights` argument.\n         activation: name of activation function to use\n             (see [activations](../activations.md)),\n             or alternatively, elementwise Theano function.\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n+        border_mode: 'valid', 'same' or 'full'\n+            ('full' requires the Theano backend).\n         subsample_length: factor by which to subsample output.\n         atrous_rate: Factor for kernel dilation. Also called filter_dilation\n             elsewhere.\n@@ -245,7 +256,8 @@ class AtrousConvolution1D(Convolution1D):\n     def __init__(self, nb_filter, filter_length,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample_length=1, atrous_rate=1,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_regularizer=None, b_regularizer=None,\n+                 activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, **kwargs):\n \n@@ -254,7 +266,8 @@ class AtrousConvolution1D(Convolution1D):\n \n         self.atrous_rate = int(atrous_rate)\n \n-        super(AtrousConvolution1D, self).__init__(nb_filter, filter_length,\n+        super(AtrousConvolution1D, self).__init__(\n+            nb_filter, filter_length,\n             init=init, activation=activation,\n             weights=weights, border_mode=border_mode,\n             subsample_length=subsample_length,\n@@ -291,6 +304,7 @@ class AtrousConvolution1D(Convolution1D):\n \n class Convolution2D(Layer):\n     \"\"\"Convolution operator for filtering windows of two-dimensional inputs.\n+\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers, does not include the sample axis),\n@@ -301,7 +315,9 @@ class Convolution2D(Layer):\n     ```python\n         # apply a 3x3 convolution with 64 output filters on a 256x256 image:\n         model = Sequential()\n-        model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(3, 256, 256)))\n+        model.add(Convolution2D(64, 3, 3,\n+                                border_mode='same',\n+                                input_shape=(3, 256, 256)))\n         # now model.output_shape == (None, 64, 256, 256)\n \n         # add a 3x3 convolution on top, with 32 output filters:\n@@ -324,7 +340,8 @@ class Convolution2D(Layer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n+        border_mode: 'valid', 'same' or 'full'\n+            ('full' requires the Theano backend).\n         subsample: tuple of length 2. Factor by which to subsample output.\n             Also called strides elsewhere.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n@@ -362,7 +379,8 @@ class Convolution2D(Layer):\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1), dim_ordering='default',\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_regularizer=None, b_regularizer=None,\n+                 activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, **kwargs):\n         if dim_ordering == 'default':\n@@ -476,12 +494,14 @@ class Convolution2D(Layer):\n \n \n class Deconvolution2D(Convolution2D):\n-    \"\"\"Transposed convolution operator for filtering windows of two-dimensional inputs.\n-    The need for transposed convolutions generally arises from the desire\n-    to use a transformation going in the opposite direction of a normal convolution,\n-    i.e., from something that has the shape of the output of some convolution\n-    to something that has the shape of its input\n-    while maintaining a connectivity pattern that is compatible with said convolution. [1]\n+    \"\"\"Transposed convolution operator for filtering windows of 2-D inputs.\n+\n+    The need for transposed convolutions generally arises from the desire to\n+    use a transformation going in the opposite direction\n+    of a normal convolution, i.e., from something that has the shape\n+    of the output of some convolution to something that has the shape\n+    of its input while maintaining a connectivity pattern\n+    that is compatible with said convolution.\n \n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n@@ -494,10 +514,14 @@ class Deconvolution2D(Convolution2D):\n     # Examples\n \n     ```python\n-        # apply a 3x3 transposed convolution with stride 1x1 and 3 output filters on a 12x12 image:\n+        # apply a 3x3 transposed convolution\n+        # with stride 1x1 and 3 output filters on a 12x12 image:\n         model = Sequential()\n-        model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14), border_mode='valid', input_shape=(3, 12, 12)))\n-        # Note that you will have to change the output_shape depending on the backend used.\n+        model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14),\n+                                  border_mode='valid',\n+                                  input_shape=(3, 12, 12)))\n+        # Note that you will have to change\n+        # the output_shape depending on the backend used.\n \n         # we can predict with the model and print the shape of the array.\n         dummy_input = np.ones((32, 3, 12, 12))\n@@ -508,9 +532,13 @@ class Deconvolution2D(Convolution2D):\n         # Theano CPU: (None, 3, 14, 14)\n         # TensorFlow: (None, 14, 14, 3)\n \n-        # apply a 3x3 transposed convolution with stride 2x2 and 3 output filters on a 12x12 image:\n+        # apply a 3x3 transposed convolution\n+        # with stride 2x2 and 3 output filters on a 12x12 image:\n         model = Sequential()\n-        model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25), subsample=(2, 2), border_mode='valid', input_shape=(3, 12, 12)))\n+        model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25),\n+                                  subsample=(2, 2),\n+                                  border_mode='valid',\n+                                  input_shape=(3, 12, 12)))\n         model.summary()\n \n         # we can predict with the model and print the shape of the array.\n@@ -528,7 +556,8 @@ class Deconvolution2D(Convolution2D):\n         nb_row: Number of rows in the transposed convolution kernel.\n         nb_col: Number of columns in the transposed convolution kernel.\n         output_shape: Output shape of the transposed convolution operation.\n-            tuple of integers (nb_samples, nb_filter, nb_output_rows, nb_output_cols)\n+            tuple of integers\n+            `(nb_samples, nb_filter, nb_output_rows, nb_output_cols)`.\n             Formula for calculation of the output shape [1], [2]:\n                 o = s (i - 1) + a + k - 2p, \\quad a \\in \\{0, \\ldots, s - 1\\}\n                 where:\n@@ -539,8 +568,9 @@ class Deconvolution2D(Convolution2D):\n                     a - user-specified quantity used to distinguish between\n                         the s different possible output sizes.\n              Because a is not specified explicitly and Theano and Tensorflow\n-             use different values, it is better to use a dummy input and observe\n-             the actual output shape of a layer as specified in the examples.\n+             use different values, it is better to use\n+             a dummy input and observe the actual output shape of\n+             a layer as specified in the examples.\n         init: name of initialization function for the weights of the layer\n             (see [initializations](../initializations.md)), or alternatively,\n             Theano function to use for weights initialization.\n@@ -552,7 +582,8 @@ class Deconvolution2D(Convolution2D):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n+        border_mode: 'valid', 'same' or 'full'\n+            ('full' requires the Theano backend).\n         subsample: tuple of length 2. Factor by which to oversample output.\n             Also called strides elsewhere.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n@@ -570,7 +601,8 @@ class Deconvolution2D(Convolution2D):\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"tf\".\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n \n     # Input shape\n         4D tensor with shape:\n@@ -658,7 +690,8 @@ class Deconvolution2D(Convolution2D):\n \n \n class AtrousConvolution2D(Convolution2D):\n-    \"\"\"Atrous Convolution operator for filtering windows of two-dimensional inputs.\n+    \"\"\"Atrous Convolution operator for filtering windows of 2-D inputs.\n+\n     A.k.a dilated convolution or convolution with holes.\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n@@ -668,10 +701,14 @@ class AtrousConvolution2D(Convolution2D):\n     # Examples\n \n     ```python\n-        # apply a 3x3 convolution with atrous rate 2x2 and 64 output filters on a 256x256 image:\n+        # apply a 3x3 convolution with atrous rate 2x2\n+        # and 64 output filters on a 256x256 image:\n         model = Sequential()\n-        model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2), border_mode='valid', input_shape=(3, 256, 256)))\n-        # now the actual kernel size is dilated from 3x3 to 5x5 (3+(3-1)*(2-1)=5)\n+        model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2),\n+                                      border_mode='valid',\n+                                      input_shape=(3, 256, 256)))\n+        # now the actual kernel size is dilated\n+        # from 3x3 to 5x5 (3+(3-1)*(2-1)=5)\n         # thus model.output_shape == (None, 64, 252, 252)\n     ```\n \n@@ -690,7 +727,8 @@ class AtrousConvolution2D(Convolution2D):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n+        border_mode: 'valid', 'same' or 'full'\n+            ('full' requires the Theano backend).\n         subsample: tuple of length 2. Factor by which to subsample output.\n             Also called strides elsewhere.\n         atrous_rate: tuple of length 2. Factor for kernel dilation.\n@@ -710,7 +748,8 @@ class AtrousConvolution2D(Convolution2D):\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"tf\".\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n \n     # Input shape\n         4D tensor with shape:\n@@ -733,7 +772,8 @@ class AtrousConvolution2D(Convolution2D):\n                  init='glorot_uniform', activation=None, weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  atrous_rate=(1, 1), dim_ordering='default',\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_regularizer=None, b_regularizer=None,\n+                 activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, **kwargs):\n         if dim_ordering == 'default':\n@@ -1033,6 +1073,7 @@ class SeparableConvolution2D(Layer):\n \n class Convolution3D(Layer):\n     \"\"\"Convolution operator for filtering windows of three-dimensional inputs.\n+\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers, does not include the sample axis),\n@@ -1054,10 +1095,12 @@ class Convolution3D(Layer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of Numpy arrays to set as initial weights.\n-        border_mode: 'valid', 'same' or 'full'. ('full' requires the Theano backend.)\n+        border_mode: 'valid', 'same' or 'full'\n+            ('full' requires the Theano backend).\n         subsample: tuple of length 3. Factor by which to subsample output.\n             Also called strides elsewhere.\n-            Note: 'subsample' is implemented by slicing the output of conv3d with strides=(1,1,1).\n+            Note: 'subsample' is implemented by slicing\n+            the output of conv3d with strides=(1,1,1).\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n@@ -1073,7 +1116,8 @@ class Convolution3D(Layer):\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"tf\".\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n \n     # Input shape\n         5D tensor with shape:\n@@ -1223,7 +1267,7 @@ class Convolution3D(Layer):\n \n \n class UpSampling1D(Layer):\n-    \"\"\"Repeat each temporal step `length` times along the time axis.\n+    \"\"\"Repeats each temporal step `length` times along the time axis.\n \n     # Arguments\n         length: integer. Upsampling factor.\n@@ -1255,7 +1299,7 @@ class UpSampling1D(Layer):\n \n \n class UpSampling2D(Layer):\n-    \"\"\"Repeat the rows and columns of the data\n+    \"\"\"Repeats the rows and columns of the data\n     by size[0] and size[1] respectively.\n \n     # Arguments\n@@ -1319,7 +1363,7 @@ class UpSampling2D(Layer):\n \n \n class UpSampling3D(Layer):\n-    \"\"\"Repeat the first, second and third dimension of the data\n+    \"\"\"Repeats the first, second and third dimension of the data\n     by size[0], size[1] and size[2] respectively.\n \n     # Arguments\n@@ -1402,10 +1446,10 @@ class ZeroPadding1D(Layer):\n             If any key is missing, default value of 0 will be used for the missing key.\n \n     # Input shape\n-        3D tensor with shape (samples, axis_to_pad, features)\n+        3D tensor with shape `(samples, axis_to_pad, features)`\n \n     # Output shape\n-        3D tensor with shape (samples, padded_axis, features)\n+        3D tensor with shape `(samples, padded_axis, features)`\n     \"\"\"\n \n     def __init__(self, padding=1, **kwargs):\n@@ -1575,11 +1619,11 @@ class ZeroPadding3D(Layer):\n \n     # Input shape\n         5D tensor with shape:\n-        (samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)\n+        `(samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)`\n \n     # Output shape\n         5D tensor with shape:\n-        (samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)\n+        `(samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)`\n     \"\"\"\n \n     def __init__(self, padding=(1, 1, 1), dim_ordering='default', **kwargs):\n@@ -1634,10 +1678,10 @@ class Cropping1D(Layer):\n             the cropping dimension (axis 1).\n \n     # Input shape\n-        3D tensor with shape (samples, axis_to_crop, features)\n+        3D tensor with shape `(samples, axis_to_crop, features)`\n \n     # Output shape\n-        3D tensor with shape (samples, cropped_axis, features)\n+        3D tensor with shape `(samples, cropped_axis, features)`\n     \"\"\"\n \n     def __init__(self, cropping=(1, 1), **kwargs):\n@@ -1689,11 +1733,11 @@ class Cropping2D(Layer):\n \n     # Input shape\n         4D tensor with shape:\n-        (samples, depth, first_axis_to_crop, second_axis_to_crop)\n+        `(samples, depth, first_axis_to_crop, second_axis_to_crop)`\n \n     # Output shape\n         4D tensor with shape:\n-        (samples, depth, first_cropped_axis, second_cropped_axis)\n+        `(samples, depth, first_cropped_axis, second_cropped_axis)`\n \n     # Examples\n \n@@ -1777,11 +1821,11 @@ class Cropping3D(Layer):\n \n     # Input shape\n         5D tensor with shape:\n-        (samples, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)\n+        `(samples, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)`\n \n     # Output shape\n         5D tensor with shape:\n-        (samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)\n+        `(samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)`\n \n     \"\"\"\n \n\n@@ -64,7 +64,9 @@ class Masking(Layer):\n \n \n class Dropout(Layer):\n-    \"\"\"Applies Dropout to the input. Dropout consists in randomly setting\n+    \"\"\"Applies Dropout to the input.\n+\n+    Dropout consists in randomly setting\n     a fraction `p` of input units to 0 at each update during training time,\n     which helps prevent overfitting.\n \n@@ -554,7 +556,8 @@ class Lambda(Layer):\n             If a tuple, it only specifies the first dimension onward;\n                  sample dimension is assumed either the same as the input:\n                  `output_shape = (input_shape[0], ) + output_shape`\n-                 or, the input is `None` and the sample dimension is also `None`:\n+                 or, the input is `None` and\n+                 the sample dimension is also `None`:\n                  `output_shape = (None, ) + output_shape`\n             If a function, it specifies the entire shape as a function of the\n             input shape: `output_shape = f(input_shape)`\n@@ -602,9 +605,12 @@ class Lambda(Layer):\n                     return K.int_shape(x)\n             # Otherwise, we default to the input shape.\n             warnings.warn('`output_shape` argument not specified for layer {} '\n-                          'and cannot be automatically inferred with the Theano backend. '\n-                          'Defaulting to output shape `{}` (same as input shape). '\n-                          'If the expected output shape is different, specify it via the `output_shape` argument.'\n+                          'and cannot be automatically inferred '\n+                          'with the Theano backend. '\n+                          'Defaulting to output shape `{}` '\n+                          '(same as input shape). '\n+                          'If the expected output shape is different, '\n+                          'specify it via the `output_shape` argument.'\n                           .format(self.name, input_shape))\n             return input_shape\n         elif isinstance(self._output_shape, (tuple, list)):\n@@ -1249,12 +1255,13 @@ class TimeDistributedDense(Layer):\n             if hasattr(K, 'int_shape'):\n                 input_length = K.int_shape(x)[1]\n                 if not input_length:\n-                    raise ValueError(\n-                        'Layer ' + self.name +\n-                        ' requires to know the length of its input, '\n-                        'but it could not be inferred automatically. '\n-                        'Specify it manually by passing an input_shape '\n-                        'argument to the first layer in your model.')\n+                    raise ValueError('Layer ' + self.name +\n+                                     ' requires to know the length '\n+                                     'of its input, but it could not '\n+                                     'be inferred automatically. '\n+                                     'Specify it manually by passing '\n+                                     'an input_shape argument to '\n+                                     'the first layer in your model.')\n             else:\n                 input_length = K.shape(x)[1]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f5455e29efa4579eebbf894e97ee53cd1257529", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 18 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 19860 | Contributors (this commit): 127 | Commits (past 90d): 93 | Contributors (cumulative): 173 | DMM Complexity: None\n\nDIFF:\n@@ -1923,6 +1923,7 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n # Note that tensorflow's native CTC code is significantly\n # faster than this\n \n+\n def ctc_interleave_blanks(Y):\n     Y_ = T.alloc(-1, Y.shape[0] * 2 + 1)\n     Y_ = T.set_subtensor(Y_[T.arange(Y.shape[0]) * 2 + 1], Y)\n\n@@ -42,6 +42,7 @@ class InputSpec(object):\n     A None entry in a shape is compatible with any dimension,\n     a None shape is compatible with any shape.\n     \"\"\"\n+\n     def __init__(self, dtype=None, shape=None, ndim=None):\n         if isinstance(ndim, str):\n             if '+' not in ndim:\n@@ -99,6 +100,7 @@ class Node(object):\n         A.outbound_nodes\n         B.inbound_nodes\n     \"\"\"\n+\n     def __init__(self, outbound_layer,\n                  inbound_layers, node_indices, tensor_indices,\n                  input_tensors, output_tensors,\n@@ -282,6 +284,7 @@ class Layer(object):\n         create_input_layer()\n         assert_input_compatibility()\n     \"\"\"\n+\n     def __init__(self, **kwargs):\n         # These properties should have been set\n         # by the child class, as appropriate.\n@@ -1052,6 +1055,7 @@ class InputLayer(Layer):\n             is meant to be sparse.\n         name: Name of the layer (string).\n     \"\"\"\n+\n     def __init__(self, input_shape=None, batch_input_shape=None,\n                  input_dtype=None, input_tensor=None, sparse=False, name=None):\n         self.input_spec = None\n@@ -1253,6 +1257,7 @@ class Merge(Layer):\n             if merge mode is a lambda/function). If the latter case, it should\n             take as input a list of masks and return a single mask.\n     \"\"\"\n+\n     def __init__(self, layers=None, mode='sum', concat_axis=-1,\n                  dot_axes=-1, output_shape=None, output_mask=None,\n                  arguments=None, node_indices=None, tensor_indices=None,\n@@ -1722,6 +1727,7 @@ class Container(Layer):\n     # Class Methods\n         from_config\n     \"\"\"\n+\n     def __init__(self, input, output, name=None):\n         # Handle name argument.\n         if not name:\n\n@@ -249,6 +249,7 @@ class Sequential(Model):\n             model.add(Dense(32))\n         ```\n     \"\"\"\n+\n     def __init__(self, layers=None, name=None):\n         self.layers = []  # stack of layers\n         self.model = None  # internal Model instance\n\n@@ -56,6 +56,7 @@ class Optimizer(object):\n         clipvalue: float >= 0. Gradients will be clipped\n             when their absolute value exceeds this value.\n     \"\"\"\n+\n     def __init__(self, **kwargs):\n         allowed_kwargs = {'clipnorm', 'clipvalue'}\n         for k in kwargs:\n@@ -134,6 +135,7 @@ class SGD(Optimizer):\n         decay: float >= 0. Learning rate decay over each update.\n         nesterov: boolean. Whether to apply Nesterov momentum.\n     \"\"\"\n+\n     def __init__(self, lr=0.01, momentum=0., decay=0.,\n                  nesterov=False, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n@@ -199,6 +201,7 @@ class RMSprop(Optimizer):\n         epsilon: float >= 0. Fuzz factor.\n         decay: float >= 0. Learning rate decay over each update.\n     \"\"\"\n+\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n@@ -257,6 +260,7 @@ class Adagrad(Optimizer):\n     # References\n         - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     \"\"\"\n+\n     def __init__(self, lr=0.01, epsilon=1e-8, decay=0., **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n         self.lr = K.variable(lr)\n@@ -312,6 +316,7 @@ class Adadelta(Optimizer):\n     # References\n         - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n     \"\"\"\n+\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, decay=0.,\n                  **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n@@ -379,6 +384,7 @@ class Adam(Optimizer):\n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n+\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adam, self).__init__(**kwargs)\n@@ -448,6 +454,7 @@ class Adamax(Optimizer):\n     # References\n         - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n     \"\"\"\n+\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, decay=0., **kwargs):\n         super(Adamax, self).__init__(**kwargs)\n@@ -523,6 +530,7 @@ class Nadam(Optimizer):\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n         - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n     \"\"\"\n+\n     def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                  epsilon=1e-8, schedule_decay=0.004, **kwargs):\n         super(Nadam, self).__init__(**kwargs)\n\n@@ -238,6 +238,7 @@ class ImageDataGenerator(object):\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"th\".\n     \"\"\"\n+\n     def __init__(self,\n                  featurewise_center=False,\n                  samplewise_center=False,\n\n@@ -40,6 +40,7 @@ def one_hot(text, n, filters=base_filter(), lower=True, split=\" \"):\n \n \n class Tokenizer(object):\n+\n     def __init__(self, nb_words=None, filters=base_filter(),\n                  lower=True, split=' ', char_level=False):\n         \"\"\"The class allows to vectorize a text corpus, by turning each\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#309f586424ea229fa9f6fa963e9345b4dbd22849", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 1 | Files Changed: 8 | Hunks: 21 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 37 | Churn Cumulative: 3869 | Contributors (this commit): 39 | Commits (past 90d): 24 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -41,6 +41,7 @@ class CharacterTable(object):\n     + Decode the one hot integer representation to their character output\n     + Decode a vector of probabilities to their character output\n     '''\n+\n     def __init__(self, chars, maxlen):\n         self.chars = sorted(set(chars))\n         self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n\n@@ -45,6 +45,7 @@ class Antirectifier(Layer):\n         with twice less parameters yet with comparable\n         classification accuracy as an equivalent ReLU-based network.\n     '''\n+\n     def get_output_shape_for(self, input_shape):\n         shape = list(input_shape)\n         assert len(shape) == 2  # only valid for 2D tensors\n\n@@ -20,6 +20,8 @@ img_height = 128\n layer_name = 'block5_conv1'\n \n # util function to convert a tensor into a valid image\n+\n+\n def deprocess_image(x):\n     # normalize tensor: center on 0., ensure std is 0.1\n     x -= x.mean()\n\n@@ -61,6 +61,8 @@ saved_settings = {\n settings = saved_settings['dreamy']\n \n # util function to open, resize and format pictures into appropriate tensors\n+\n+\n def preprocess_image(image_path):\n     img = load_img(image_path, target_size=(img_width, img_height))\n     img = img_to_array(img)\n@@ -69,6 +71,8 @@ def preprocess_image(image_path):\n     return img\n \n # util function to convert a tensor into a valid image\n+\n+\n def deprocess_image(x):\n     if K.image_dim_ordering() == 'th':\n         x = x.reshape((3, img_width, img_height))\n@@ -101,6 +105,8 @@ print('Model loaded.')\n layer_dict = dict([(layer.name, layer) for layer in model.layers])\n \n # continuity loss util function\n+\n+\n def continuity_loss(x):\n     assert K.ndim(x) == 4\n     if K.image_dim_ordering() == 'th':\n@@ -146,6 +152,8 @@ else:\n     outputs.append(grads)\n \n f_outputs = K.function([dream], outputs)\n+\n+\n def eval_loss_and_grads(x):\n     x = x.reshape((1,) + img_size)\n     outs = f_outputs([x])\n@@ -162,7 +170,10 @@ def eval_loss_and_grads(x):\n # \"loss\" and \"grads\". This is done because scipy.optimize\n # requires separate functions for loss and gradients,\n # but computing them separately would be inefficient.\n+\n+\n class Evaluator(object):\n+\n     def __init__(self):\n         self.loss_value = None\n         self.grad_values = None\n\n@@ -34,6 +34,7 @@ X_test /= 255\n y_train = np_utils.to_categorical(y_train, nb_classes)\n y_test = np_utils.to_categorical(y_test, nb_classes)\n \n+\n def make_model(dense_layer_sizes, nb_filters, nb_conv, nb_pool):\n     '''Creates model comprised of 2 convolutional layers followed by dense layers\n \n\n@@ -93,6 +93,8 @@ img_nrows = 400\n img_ncols = int(width * img_nrows / height)\n \n # util function to open, resize and format pictures into appropriate tensors\n+\n+\n def preprocess_image(image_path):\n     img = load_img(image_path, target_size=(img_nrows, img_ncols))\n     img = img_to_array(img)\n@@ -101,6 +103,8 @@ def preprocess_image(image_path):\n     return img\n \n # util function to convert a tensor into a valid image\n+\n+\n def deprocess_image(x):\n     if K.image_dim_ordering() == 'th':\n         x = x.reshape((3, img_nrows, img_ncols))\n@@ -144,6 +148,8 @@ outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n # first we need to define 4 util functions\n \n # the gram matrix of an image tensor (feature-wise outer product)\n+\n+\n def gram_matrix(x):\n     assert K.ndim(x) == 3\n     if K.image_dim_ordering() == 'th':\n@@ -158,6 +164,8 @@ def gram_matrix(x):\n # It is based on the gram matrices (which capture style) of\n # feature maps from the style reference image\n # and from the generated image\n+\n+\n def style_loss(style, combination):\n     assert K.ndim(style) == 3\n     assert K.ndim(combination) == 3\n@@ -170,11 +178,15 @@ def style_loss(style, combination):\n # an auxiliary loss function\n # designed to maintain the \"content\" of the\n # base image in the generated image\n+\n+\n def content_loss(base, combination):\n     return K.sum(K.square(combination - base))\n \n # the 3rd loss function, total variation loss,\n # designed to keep the generated image locally coherent\n+\n+\n def total_variation_loss(x):\n     assert K.ndim(x) == 4\n     if K.image_dim_ordering() == 'th':\n@@ -215,6 +227,7 @@ else:\n \n f_outputs = K.function([combination_image], outputs)\n \n+\n def eval_loss_and_grads(x):\n     if K.image_dim_ordering() == 'th':\n         x = x.reshape((1, 3, img_nrows, img_ncols))\n@@ -234,7 +247,10 @@ def eval_loss_and_grads(x):\n # \"loss\" and \"grads\". This is done because scipy.optimize\n # requires separate functions for loss and gradients,\n # but computing them separately would be inefficient.\n+\n+\n class Evaluator(object):\n+\n     def __init__(self):\n         self.loss_value = None\n         self.grads_values = None\n\n@@ -100,6 +100,7 @@ deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n \n+\n def vae_loss(x, x_decoded_mean):\n     # NOTE: binary_crossentropy expects a batch_size by dim\n     # for x and x_decoded_mean, so we MUST flatten these!\n\n@@ -126,7 +126,9 @@ class Optimizer(object):\n \n \n class SGD(Optimizer):\n-    \"\"\"Stochastic gradient descent, with support for momentum,\n+    \"\"\"Stochastic gradient descent optimizer.\n+\n+    Includes support for momentum,\n     learning rate decay, and Nesterov momentum.\n \n     # Arguments\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c10945f53a3274748510aa9e7c120f22e9b5f01a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1885 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,7 @@ from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n from keras.utils.test_utils import keras_test\n \n+\n @keras_test\n def test_get_updates_for():\n     a = Input(shape=(2,))\n\n@@ -96,6 +96,7 @@ def test_ModelCheckpoint():\n     os.remove(filepath.format(epoch=1))\n     os.remove(filepath.format(epoch=3))\n \n+\n def test_EarlyStopping():\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n                                                          nb_test=test_samples,\n\n@@ -49,6 +49,7 @@ def test_clasify_build_fn():\n \n def test_clasify_class_build_fn():\n     class ClassBuildFnClf(object):\n+\n         def __call__(self, hidden_dims):\n             return build_fn_clf(hidden_dims)\n \n@@ -61,6 +62,7 @@ def test_clasify_class_build_fn():\n \n def test_clasify_inherit_class_build_fn():\n     class InheritClassBuildFnClf(KerasClassifier):\n+\n         def __call__(self, hidden_dims):\n             return build_fn_clf(hidden_dims)\n \n@@ -110,6 +112,7 @@ def test_regression_build_fn():\n \n def test_regression_class_build_fn():\n     class ClassBuildFnReg(object):\n+\n         def __call__(self, hidden_dims):\n             return build_fn_reg(hidden_dims)\n \n@@ -122,6 +125,7 @@ def test_regression_class_build_fn():\n \n def test_regression_inherit_class_build_fn():\n     class InheritClassBuildFnReg(KerasRegressor):\n+\n         def __call__(self, hidden_dims):\n             return build_fn_reg(hidden_dims)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f4734cbf1044cb5eb921f8e834423c3f528d0d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 17 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 74 | Churn Cumulative: 331 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -25,7 +25,16 @@ def base_filter():\n \n \n def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n-    \"\"\"prune: sequence of characters to filter out\n+    \"\"\"Converts a text to a sequence of word indices.\n+\n+    # Arguments\n+        text: Input text (string).\n+        filters: Sequence of characters to filter out.\n+        lower: Whether to convert the input to lowercase.\n+        split: Sentence split marker (string).\n+\n+    # Returns\n+        A list of integer word indices.\n     \"\"\"\n     if lower:\n         text = text.lower()\n@@ -35,15 +44,17 @@ def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n \n \n def one_hot(text, n, filters=base_filter(), lower=True, split=\" \"):\n-    seq = text_to_word_sequence(text, filters=filters, lower=lower, split=split)\n+    seq = text_to_word_sequence(text,\n+                                filters=filters,\n+                                lower=lower,\n+                                split=split)\n     return [(abs(hash(w)) % (n - 1) + 1) for w in seq]\n \n \n class Tokenizer(object):\n+    \"\"\"Text tokenization utility class.\n \n-    def __init__(self, nb_words=None, filters=base_filter(),\n-                 lower=True, split=' ', char_level=False):\n-        \"\"\"The class allows to vectorize a text corpus, by turning each\n+    This class allows to vectorize a text corpus, by turning each\n     text into either a sequence of integers (each integer being the index\n     of a token in a dictionary) or into a vector where the coefficient\n     for each token could be binary, based on word count, based on tf-idf...\n@@ -66,6 +77,9 @@ class Tokenizer(object):\n \n     `0` is a reserved index that won't be assigned to any word.\n     \"\"\"\n+\n+    def __init__(self, nb_words=None, filters=base_filter(),\n+                 lower=True, split=' ', char_level=False):\n         self.word_counts = {}\n         self.word_docs = {}\n         self.filters = filters\n@@ -76,7 +90,9 @@ class Tokenizer(object):\n         self.char_level = char_level\n \n     def fit_on_texts(self, texts):\n-        \"\"\"Required before using texts_to_sequences or texts_to_matrix\n+        \"\"\"Updates internal vocabulary based on a list of texts.\n+\n+        Required before using `texts_to_sequences` or `texts_to_matrix`.\n \n         # Arguments\n             texts: can be a list of strings,\n@@ -85,7 +101,10 @@ class Tokenizer(object):\n         self.document_count = 0\n         for text in texts:\n             self.document_count += 1\n-            seq = text if self.char_level else text_to_word_sequence(text, self.filters, self.lower, self.split)\n+            seq = text if self.char_level else text_to_word_sequence(text,\n+                                                                     self.filters,\n+                                                                     self.lower,\n+                                                                     self.split)\n             for w in seq:\n                 if w in self.word_counts:\n                     self.word_counts[w] += 1\n@@ -108,8 +127,14 @@ class Tokenizer(object):\n             self.index_docs[self.word_index[w]] = c\n \n     def fit_on_sequences(self, sequences):\n-        \"\"\"Required before using sequences_to_matrix\n-        (if fit_on_texts was never called)\n+        \"\"\"Updates internal vocabulary based on a list of sequences.\n+\n+        Required before using `sequences_to_matrix`\n+        (if `fit_on_texts` was never called).\n+\n+        # Arguments\n+            sequences: A list of sequence.\n+                A \"sequence\" is a list of integer word indices.\n         \"\"\"\n         self.document_count = len(sequences)\n         self.index_docs = {}\n@@ -123,10 +148,15 @@ class Tokenizer(object):\n \n     def texts_to_sequences(self, texts):\n         \"\"\"Transforms each text in texts in a sequence of integers.\n+\n         Only top \"nb_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n \n-        Returns a list of sequences.\n+        # Arguments\n+            texts: A list of texts (strings).\n+\n+        # Returns\n+            A list of sequences.\n         \"\"\"\n         res = []\n         for vect in self.texts_to_sequences_generator(texts):\n@@ -138,14 +168,18 @@ class Tokenizer(object):\n         Only top \"nb_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n \n-        Yields individual sequences.\n+        # Arguments\n+            texts: A list of texts (strings).\n \n-        # Arguments:\n-            texts: list of strings.\n+        # Yields\n+            Yields individual sequences.\n         \"\"\"\n         nb_words = self.nb_words\n         for text in texts:\n-            seq = text if self.char_level else text_to_word_sequence(text, self.filters, self.lower, self.split)\n+            seq = text if self.char_level else text_to_word_sequence(text,\n+                                                                     self.filters,\n+                                                                     self.lower,\n+                                                                     self.split)\n             vect = []\n             for w in seq:\n                 i = self.word_index.get(w)\n@@ -160,9 +194,12 @@ class Tokenizer(object):\n         \"\"\"Convert a list of texts to a Numpy matrix,\n         according to some vectorization mode.\n \n-        # Arguments:\n+        # Arguments\n             texts: list of strings.\n-            modes: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n+            modes: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n+\n+        # Returns\n+            A Numpy matrix.\n         \"\"\"\n         sequences = self.texts_to_sequences(texts)\n         return self.sequences_to_matrix(sequences, mode=mode)\n@@ -171,10 +208,13 @@ class Tokenizer(object):\n         \"\"\"Converts a list of sequences into a Numpy matrix,\n         according to some vectorization mode.\n \n-        # Arguments:\n+        # Arguments\n             sequences: list of sequences\n                 (a sequence is a list of integer word indices).\n             modes: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n+\n+        # Returns\n+            A Numpy matrix.\n         \"\"\"\n         if not self.nb_words:\n             if self.word_index:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ab4b647f82faa640c6919c8539106b445259220", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 5193 | Contributors (this commit): 36 | Commits (past 90d): 30 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -406,7 +406,7 @@ class Layer(object):\n                    constraint=None):\n         \"\"\"Adds a weight variable to the layer.\n \n-        # Arguments:\n+        # Arguments\n             shape: The shape tuple of the weight.\n             initializer: An Initializer instance (callable).\n             trainable: A boolean, whether the weight should\n\n@@ -36,7 +36,7 @@ def make_tuple(*args):\n \n \n def func_dump(func):\n-    \"\"\"Serialize user defined function.\n+    \"\"\"Serializes user defined function.\n     \"\"\"\n     code = marshal.dumps(func.__code__).decode('raw_unicode_escape')\n     defaults = func.__defaults__\n@@ -48,7 +48,8 @@ def func_dump(func):\n \n \n def func_load(code, defaults=None, closure=None, globs=None):\n-    \"\"\"Deserialize user defined function.\"\"\"\n+    \"\"\"Deserializes user defined function.\n+    \"\"\"\n     if isinstance(code, (tuple, list)):  # unpack previous dump\n         code, defaults, closure = code\n     code = marshal.loads(code.encode('raw_unicode_escape'))\n@@ -61,14 +62,14 @@ def func_load(code, defaults=None, closure=None, globs=None):\n \n \n class Progbar(object):\n+    \"\"\"Displays a progress bar.\n \n-    def __init__(self, target, width=30, verbose=1, interval=0.01):\n-        \"\"\"Dislays a progress bar.\n-\n-        # Arguments:\n+    # Arguments\n         target: Total number of steps expected.\n         interval: Minimum visual progress update interval (in seconds).\n     \"\"\"\n+\n+    def __init__(self, target, width=30, verbose=1, interval=0.01):\n         self.width = width\n         self.target = target\n         self.sum_values = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
